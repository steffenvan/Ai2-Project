<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012751">
<title confidence="0.9985975">
Accurate Cross-lingual Projection between Count-based Word Vectors
by Exploiting Translatable Context Pairs
</title>
<author confidence="0.990283">
Shonosuke Ishiwatari4, Nobuhiro KajiQ°, Naoki Yoshinaga*°,
</author>
<affiliation confidence="0.991191333333333">
♣ Graduate School of Information Science and Technology, the University of Tokyo
Q Institute of Industrial Science, the University of Tokyo
d National Institute of Information and Communications Technology, Japan
</affiliation>
<email confidence="0.942596">
{ishiwatari, kaji, ynaga}@tkl.iis.u-tokyo.ac.jp
</email>
<author confidence="0.762737">
Masashi Toyoda*, Masaru Kitsuregawa**
</author>
<affiliation confidence="0.9941105">
Q Institute of Industrial Science, the University of Tokyo
♠ National Institute of Informatics, Japan
</affiliation>
<email confidence="0.988125">
{toyoda, kitsure}@tkl.iis.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.996524" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997988">
We propose a method that learns a cross-
lingual projection of word representations
from one language into another. Our
method utilizes translatable context pairs
as bonus terms of the objective function.
In the experiments, our method outper-
formed existing methods in three language
pairs, (English, Spanish), (Japanese, Chi-
nese) and (English, Japanese), without us-
ing any additional supervisions.
</bodyText>
<sectionHeader confidence="0.9984" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999731458333334">
Vector-based representations of word meanings,
hereafter word vectors, have been widely used in
a variety of NLP applications including synonym
detection (Baroni et al., 2014), paraphrase detec-
tion (Erk and Pad´o, 2008), and dialogue analy-
sis (Kalchbrenner and Blunsom, 2013). The ba-
sic idea behind those representation methods is
the distributional hypothesis (Harris, 1954; Firth,
1957) that similar words are likely to co-occur
with similar context words.
A problem with the word vectors is that they
are not meant for capturing the similarity between
words in different languages, i.e., translation pairs
such as “gato” and “cat.” The meaning represen-
tations of such word pairs are usually dissimilar,
because the vast majority of the context words are
from the same language as the target words (e.g.,
Spanish for “gato” and English for “cat”). This
prevents using word vectors in multi-lingual appli-
cations such as cross-lingual information retrieval
and machine translation.
Several approaches have been made so far to
address this problem (Fung, 1998; Klementiev et
al., 2012; Mikolov et al., 2013b). In particular,
Mikolov et al. (2013b) recently explored learning
a linear transformation between word vectors of
different languages from a small amount of train-
ing data, i.e., a set of bilingual word pairs.
This study explores incorporating prior knowl-
edge about the correspondence between dimen-
sions of word vectors to learn more accurate trans-
formation, when using count-based word vectors
(Baroni et al., 2014). Since the dimensions of
count-based word vectors are explicitly associated
with context words, we can partially be aware of
the cross-lingual correspondence between the di-
mensions of word vectors by diverting the training
data. Also, word surface forms present noisy yet
useful clues on the correspondence when targeting
the language pairs that have exchanged their vo-
cabulary (e.g., “cocktail” in English and “c´octel”
in Spanish). Although apparently useful, how to
exploit such knowledge within the learning frame-
work has not been addressed so far.
We evaluated the proposed method in three lan-
guage pairs. Compared with baselines including
a method that uses vectors learned by neural net-
works, our method gave better results.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998323">
Neural networks (Mikolov et al., 2013a; Bengio
et al., 2003) have recently gained much attention
as a way of inducing word vectors. Although the
scope of our study is currently limited to the count-
based word vectors, our experiment demonstrated
that the proposed method performs significantly
better than strong baselines including neural net-
works. This suggests that count-based word vec-
tors have a great advantage when learning a cross-
lingual projection. As a future work, we are also
</bodyText>
<page confidence="0.96859">
300
</page>
<note confidence="0.609191">
Proceedings of the 19th Conference on Computational Language Learning, pages 300–304,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999947230769231">
interested in extending the method presented here
to apply word vectors learned by neural networks.
There are also methods that directly inducing
meaning representations shared by different lan-
guages (Klementiev et al., 2012; Lauly et al.,
2014; Xiao and Guo, 2014; Hermann and Blun-
som, 2014; Faruqui and Dyer, 2014; Gouws and
Søgaard, 2015), rather than learning transforma-
tion between different languages (Fung, 1998;
N ikolov et al., 2013b; Dinu and Baroni, 2014).
However, the former approach is unable to handle
words not appearing in the training data, unlike the
latter approach.
</bodyText>
<sectionHeader confidence="0.998846" genericHeader="method">
3 Proposed Method
</sectionHeader>
<subsectionHeader confidence="0.999986">
3.1 Learning cross-lingual projection
</subsectionHeader>
<bodyText confidence="0.999968166666667">
We begin by introducing the previous method of
learning a linear transformation from word vectors
in one language into another, which are hereafter
referred to as source and target language.
Suppose we have a training data of n examples
{(x1, z1), (x2, z2),... (xn, zn)}, where xi is the
count-based vector representation of a word in the
source language (e.g., “gato”), and zi is the word
vector of its translation in the target language (e.g.,
“cat”). Then, we seek for a translation matrix, W,
such that Wxi approximates zi, by solving the
following optimization problem.
</bodyText>
<equation confidence="0.922501">
kWxi − zik2 + λ2 kWk2.(1)
</equation>
<bodyText confidence="0.999992333333333">
The second term is the L2 regularizer. Although
the regularization term does not appear in the orig-
inal formalization (N ikolov et al., 2013b), we take
this as a starting point of our investigation because
the regularizer can prevent over-fitting and gener-
ally helps learn better models.
</bodyText>
<subsectionHeader confidence="0.999917">
3.2 Exploiting translatable context pairs
</subsectionHeader>
<bodyText confidence="0.999988666666667">
Within the learning framework above, we propose
exploiting the fact that dimensions of count-based
word vectors are associated with context words,
and some dimensions in the source language are
translations of those in the target language.
For illustration purpose, suppose count-based
word vectors of Spanish and English. The Span-
ish word vectors would have dimensions associ-
ated with context words such as “amigo,” “comer,”
“importante,” while the dimensions of the English
word vectors are associated with “eat,” “run,”
“small” and “importance,” and so on. Since,
for example, “friend” is a English translation of
“amigo,” the Spanish dimension associated with
“amigo” is likely to be mapped to the English di-
mension associated with “friend.” Such knowl-
edge about the cross-lingual correspondence be-
tween dimensions is considered beneficial for
learning accurate translation matrix.
We take two approaches to obtaining such cor-
respondence. Firstly, since we have already as-
sumed that a small amount of training data is avail-
able for training the translation matrix, it can also
be used for finding the correspondence between
dimensions (referred to as Dtrain). Note that it is
natural that some words in a language have many
translations in another language. Thus, for ex-
ample, Dtrain may include (“amigo”, “friend”),
(“amigo”, “fan”) and (“amigo”, “supporter”).
Secondly, since languages have evolved over
the years while often deriving or borrowing words
(or concepts) from those in other languages, those
words have similar or even the same spelling.
We take advantage of this to find the correspon-
dence between dimensions. We specifically define
function DIST(r, s) that measures the surface-level
similarity, and regard all context word pairs (r, s)
having smaller distance than a threshold1 as trans-
latable ones (referred to as Dsim).
</bodyText>
<equation confidence="0.658718">
Levenshtein(r, s)
DIST(r, s)=
min(len(r), len(s))
</equation>
<bodyText confidence="0.999961">
where function Levenshtein(r, s) represents the
Levenshtein distance between the two words, and
len(r) represents the length of the word.
</bodyText>
<subsectionHeader confidence="0.998052">
3.3 New objective function
</subsectionHeader>
<bodyText confidence="0.999905714285714">
We incorporate the knowledge about the corre-
spondence between the dimensions into the learn-
ing framework. Since the correspondence ob-
tained by the methods presented above can be
noisy, we want to treat it as a soft constraint. This
consideration leads us to develop the following
new objective function:
</bodyText>
<equation confidence="0.870098333333333">
kWxi − zik2 + λ2kWk2
�−βtrain �wjk − βsim wjk.
(j,k)EDtrain (j,k)EDsim
</equation>
<bodyText confidence="0.977539">
The third and fourth terms are newly added to
guide the learning process to strengthen wjk when
</bodyText>
<footnote confidence="0.762953">
1The threshold was fixed to 0.5.
</footnote>
<equation confidence="0.445707375">
W*=argmin
W
n
i=1
W*=argmin
W
n
i=1
</equation>
<page confidence="0.985506">
301
</page>
<bodyText confidence="0.9998865">
k-th dimension in the source language corre-
sponds to j-th dimension in the target language.
Dtrain and Dsim are sets of dimension pairs found
by the two methods. Qtrain and Qsim are param-
eters representing the strength of the new terms,
and are tuned on held-out development data.
</bodyText>
<subsectionHeader confidence="0.909225">
3.4 Optimization
</subsectionHeader>
<bodyText confidence="0.9999602">
We use Pegasos algorithm (Shalev-Shwartz et al.,
2011), an instance of the stochastic gradient de-
scent (Bottou, 2004), to optimize the new objec-
tive. Given T-th learning sample (xT, zT), we up-
date translation matrix W as follows:
</bodyText>
<equation confidence="0.892830833333333">
W ←W − •/TVET(W)
where •/T represents the learning rate and is set to
•/T = λT , and VET(W) is the gradient which is
1
calculated from T-th sample (xT, zT):
2(W xT − zT)xTT − QtrainA − QsimB + λW .
</equation>
<bodyText confidence="0.9780215">
A and B are gradients corresponding to the two
new terms. A is a matrix in which alk = 1 if
(j, k) E Dtrain otherwise 0. B is defined simi-
larly.
</bodyText>
<sectionHeader confidence="0.999528" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99977975">
We evaluate our method on translation among
word vectors in four languages: English (En),
Spanish (Es), Japanese (Jp) and Chinese (Cn). We
have chosen three language pairs: (En, Es), (Jp,
Cn) and (En, Jp), for the translation, so that we
can examine the impact of each type of translat-
able context pairs integrated into the learning ob-
jective.
</bodyText>
<subsectionHeader confidence="0.975617">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.996254777777778">
First, we prepared source text in the four lan-
guages from Wikipedia2 dumps following (Baroni
et al., 2014). We extracted plain text from the
XML dumps by using wp2txt.3 Since words are
concatenated in Japanese and Chinese, we used
MeCab4 and Stanford Word Segmenter5 to tok-
enize the text. Since inflection occurs in English,
Spanish, and Japanese, we used Stanford POS tag-
ger,6 Pattern,7 and MeCab to lemmatize the text.
</bodyText>
<footnote confidence="0.999921333333333">
2http://dumps.wikimedia.org/
3https://github.com/yohasebe/wp2txt/
4http://taku910.github.io/mecab/
5http://nlp.stanford.edu/software/segmenter.shtml
6http://nlp.stanford.edu/software/tagger.shtml
7http://www.clips.ua.ac.be/pages/pattern
</footnote>
<bodyText confidence="0.999636489361702">
Next, we induced count-based word vectors
from the obtained text. We considered context
windows of five words to both sides of the tar-
get word. The function words are then excluded
from the extracted context words. Since the count
vectors are very high-dimensional and sparse, we
selected top-10k frequent words as contexts words
(in other words, the number of dimensions of the
word vectors). We converted the counts into pos-
itive point-wise mutual information (Church and
Hanks, 1990) and normalized the resulting vectors
to remove the bias that is introduced by the differ-
ence of the word frequency.
Then, we compiled a seed bilingual dictionary
(a set of bilingual word pairs) for each language
pair that is used to learn and evaluate the transla-
tion matrix. We utilized cross-lingual synsets in
the Open Multilingual Wordnet8 to obtain bilin-
gual pairs.
Since our method aims to be used in expand-
ing bilingual dictionaries, we designed datasets as-
suming such a situation. Considering that more
frequent words are likely to be registered in a dic-
tionary, we sorted words in the source language
by frequency and used the top-11k words and
their translations in the target language as a train-
ing/development data, and used the subsequent 1k
words and their translations as a test data.
We have compared our method with the follow-
ing three methods:
Baseline learns a translation matrix using Eq. 1
for the same count-based word vectors as the
proposed method. Comparison between the
proposed method and this method reveals the
impact of incorporating the cross-lingual cor-
respondences between dimensions.
CBOW learns a translation matrix using Eq. 1
for word vectors learned by a neural net-
work (specifically, continuous bag-of-words
(CBOW)) (Mikolov et al., 2013b). Compari-
son between this method and the above base-
line reveals the impact of the vector repre-
sentation. Note that the CBOW-based word
vectors take rare context words as well as
the top-10k frequent words into account. We
used word2vec9 to obtain the vectors for each
language.10 Since Mikolov et al. (2013b)
</bodyText>
<footnote confidence="0.9992675">
8http://compling.hss.ntu.edu.sg/omw/
9https://code.google.com/p/word2vec/
10The threshold of sub-sampling of words was set to 1e-3
to reduce the effect of very frequent words, e.g., “a” or “the.”
</footnote>
<page confidence="0.995894">
302
</page>
<tableCaption confidence="0.999888">
Table 1: Experimental results: the accuracy of the translation.
</tableCaption>
<table confidence="0.999579375">
Testset Baseline CBOW Direct Mapping Proposed,/o surface Proposed
P@1 P@5 P@1 P@5 P@1 P@5 P@1 P@5 P@1 P@5
Es → En 0.1% 0.5% 7.5% 22.0% 45.7% 61.1% 46.6% 62.4% 54.7% 67.6%
Es ← En 0.1% 0.6% 7.1% 18.9% 11.9% 26.1% 28.7% 45.7% 31.3% 49.6%
Jp → Cn 0.6% 1.6% 5.4% 13.8% 9.3% 22.2% 11.1% 26.2% 15.5% 34.0%
Jp ← Cn 0.3% 1.2% 2.9% 11.3% 11.6% 26.8% 7.8% 21.6% 13.1% 27.9%
En → Jp 0.3% 1.1% 4.9% 13.3% 5.4% 13.9% 18.5% 36.4% 19.3% 37.1%
En ← Jp 0.2% 1.0% 6.5% 19.1% 22.3% 37.4% 32.3% 51.0% 32.5% 51.9%
</table>
<bodyText confidence="0.999801">
reported the accurate translation can be ob-
tained when the vectors in the source lan-
guage is 2-4x larger than that in the target
language, we prepared m-dimensional (m =
100, 200, 300) vectors for the target language
and n-dimensional (n = 2m, 3m, 4m) vec-
tors for the source language, and optimized
their combinations on the development data.
Direct Mapping exploits the training data to map
each dimension in a word vector in the source
language to the corresponding dimension in
a word vector in the target language, refer-
ring to the bilingual pairs in the training data
(Fung, 1998). To deal with words that have
more than one translation, we weighted each
translation by a reciprocal rank of its fre-
quency among the translations in the target
language, as in (Prochasson et al., 2009).
Note that all methods, including the proposed
methods, use the same amount of supervision
(training data) and thereby they are completely
comparable with each other.
Evaluation procedure For each word vector in
the source language, we translate it into the target
language and evaluate the quality of the translation
as in (Mikolov et al., 2013b): i) measure the cosine
similarity between the resulting word vector and
all the vectors in the test data (in the target lan-
guage), ii) next choose the top-n (n = 1, 5) word
vectors that have the highest similarity against the
resulting vector, and iii) then examine whether the
chosen vectors include the correct one.
</bodyText>
<subsectionHeader confidence="0.580973">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.995321285714286">
Table 1 shows results of the translation between
word vectors in each language pair. Proposed sig-
nificantly improved the translation quality against
Baseline, and performed the best among all of
the methods. Although the use of CBOW-based
word vectors (CBOW) has improved the trans-
lation quality against Baseline, the performance
</bodyText>
<figureCaption confidence="0.909741">
Figure 1: The impact of the size of training data
(Es → En).
</figureCaption>
<bodyText confidence="0.998091444444444">
gain is smaller than that obtained by our new ob-
jective. Proposed,/o surface uses only the training
data to find translatable context pairs by setting
Qsim = 0. Thus, its advantage over Direct Map-
ping confirms the importance of learning a trans-
lation matrix. In addition, the greater advantage of
Proposed over Proposed,/o surface in the transla-
tion between (En, Es) or (Jp, Cn) conforms to our
expectation that surface-level similarity is more
useful for translation between the language pairs
which have often exchanged their vocabulary.
Figure 1 shows P@1 (Es → En) plotted against
the size of training data. Remember that the train-
ing data is not only used to learn a translation ma-
trix in the methods other than Direct Mapping but
also is used to map dimensions in Direct Mapping
and the proposed methods. Proposed performs
the best among all methods regardless the size of
training data. Comparison between Direct Map-
ping and Proposed,/o surface reveals that learning
a translation matrix is not always effective when
the size of the training data is small, since it may
be suffered from over-fitting (the size of the trans-
lation matrix is too large for the size of training
data). We can see that surface-level similarity is
beneficial especially when the size of training data
is small.
</bodyText>
<page confidence="0.998975">
303
</page>
<sectionHeader confidence="0.998449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999995363636364">
We have proposed the use of prior knowledge
in accurately translating word vectors. We have
specifically exploited two types of translatable
context pairs, which are taken from the training
data and guessed by surface-level similarity, to de-
sign a new objective function in learning the trans-
lation matrix. Experimental results confirmed that
our method significantly improved the translation
among word vectors in four languages, and the ad-
vantage was greater than that obtained by the use
of a word vector learned by a neural network.
</bodyText>
<sectionHeader confidence="0.996984" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9579605">
This work was partially supported by JSPS KAK-
ENHI Grant Number 25280111.
</bodyText>
<sectionHeader confidence="0.998261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99990025">
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
ofACL, pages 238–247.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
L´eon Bottou. 2004. Stochastic learning. In Ad-
vanced lectures on machine learning, pages 146–
168. Springer.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.
Georgiana Dinu and Marco Baroni. 2014. How to
make words with vectors: Phrase generation in dis-
tributional semantics. In Proceedings of ACL, pages
624–633.
Katrin Erk and Sebastian Pad´o. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, pages 897–906.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proceedings of EACL, pages 462–
471.
John R. Firth. 1957. A synopsis of linguistic theory.
Studies in Linguistic Analysis, pages 1–32.
Pascale Fung. 1998. A statistical view on bilingual
lexicon extraction: from parallel corpora to non-
parallel corpora. In Machine Translation and the
Information Soup, pages 1–17. Springer.
Stephan Gouws and Anders Søgaard. 2015. Simple
task-specific bilingual word embeddings. In Pro-
ceedings of NAACL-HLT, pages 1386–1390.
Zellig S. Harris. 1954. Distributional structure. Word,
10:146–162.
Karl Moritz Hermann and Phil Blunsom. 2014. Multi-
lingual models for compositional distributed seman-
tics. In Proceedings of ACL, pages 58–68.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse com-
positionality. In Proceedings of ACL Workshop on
Continuous Vector Space Models and their Compo-
sitionality, pages 119–126.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of COLING,
pages 1459–1474.
Stanislas Lauly, Hugo Larochelle, Mitesh Khapra,
Balaraman Ravindran, Vikas C Raykar, and Amrita
Saha. 2014. An autoencoder approach to learn-
ing bilingual word representations. In Advances in
NIPS, pages 1853–1861.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013b. Exploiting similarities among languages for
machine translation. arXiv preprint.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Proceedings of MT Summit XII, pages 284–291.
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro,
and Andrew Cotter. 2011. Pegasos: Primal esti-
mated sub-gradient solver for SVM. Mathematical
programming, 127(1):3–30.
Min Xiao and Yuhong Guo. 2014. Distributed word
representation learning for cross-lingual dependency
parsing. In Proceedings of CoNLL, pages 119–129.
</reference>
<page confidence="0.99933">
304
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.529132">
<title confidence="0.9888965">Accurate Cross-lingual Projection between Count-based Word by Exploiting Translatable Context Pairs</title>
<author confidence="0.993219">Nobuhiro Naoki</author>
<affiliation confidence="0.996602333333333">School of Information Science and Technology, the University of of Industrial Science, the University of Institute of Information and Communications Technology,</affiliation>
<email confidence="0.859883">kaji,</email>
<author confidence="0.635634">Masaru</author>
<affiliation confidence="0.966218">of Industrial Science, the University of Institute of Informatics,</affiliation>
<abstract confidence="0.997909636363636">We propose a method that learns a crosslingual projection of word representations from one language into another. Our method utilizes translatable context pairs as bonus terms of the objective function. In the experiments, our method outperformed existing methods in three language pairs, (English, Spanish), (Japanese, Chinese) and (English, Japanese), without using any additional supervisions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="1204" citStr="Baroni et al., 2014" startWordPosition="154" endWordPosition="157">}@tkl.iis.u-tokyo.ac.jp Abstract We propose a method that learns a crosslingual projection of word representations from one language into another. Our method utilizes translatable context pairs as bonus terms of the objective function. In the experiments, our method outperformed existing methods in three language pairs, (English, Spanish), (Japanese, Chinese) and (English, Japanese), without using any additional supervisions. 1 Introduction Vector-based representations of word meanings, hereafter word vectors, have been widely used in a variety of NLP applications including synonym detection (Baroni et al., 2014), paraphrase detection (Erk and Pad´o, 2008), and dialogue analysis (Kalchbrenner and Blunsom, 2013). The basic idea behind those representation methods is the distributional hypothesis (Harris, 1954; Firth, 1957) that similar words are likely to co-occur with similar context words. A problem with the word vectors is that they are not meant for capturing the similarity between words in different languages, i.e., translation pairs such as “gato” and “cat.” The meaning representations of such word pairs are usually dissimilar, because the vast majority of the context words are from the same lang</context>
<context position="2552" citStr="Baroni et al., 2014" startWordPosition="363" endWordPosition="366">lications such as cross-lingual information retrieval and machine translation. Several approaches have been made so far to address this problem (Fung, 1998; Klementiev et al., 2012; Mikolov et al., 2013b). In particular, Mikolov et al. (2013b) recently explored learning a linear transformation between word vectors of different languages from a small amount of training data, i.e., a set of bilingual word pairs. This study explores incorporating prior knowledge about the correspondence between dimensions of word vectors to learn more accurate transformation, when using count-based word vectors (Baroni et al., 2014). Since the dimensions of count-based word vectors are explicitly associated with context words, we can partially be aware of the cross-lingual correspondence between the dimensions of word vectors by diverting the training data. Also, word surface forms present noisy yet useful clues on the correspondence when targeting the language pairs that have exchanged their vocabulary (e.g., “cocktail” in English and “c´octel” in Spanish). Although apparently useful, how to exploit such knowledge within the learning framework has not been addressed so far. We evaluated the proposed method in three lang</context>
<context position="9482" citStr="Baroni et al., 2014" startWordPosition="1480" endWordPosition="1483"> A and B are gradients corresponding to the two new terms. A is a matrix in which alk = 1 if (j, k) E Dtrain otherwise 0. B is defined similarly. 4 Experiments We evaluate our method on translation among word vectors in four languages: English (En), Spanish (Es), Japanese (Jp) and Chinese (Cn). We have chosen three language pairs: (En, Es), (Jp, Cn) and (En, Jp), for the translation, so that we can examine the impact of each type of translatable context pairs integrated into the learning objective. 4.1 Setup First, we prepared source text in the four languages from Wikipedia2 dumps following (Baroni et al., 2014). We extracted plain text from the XML dumps by using wp2txt.3 Since words are concatenated in Japanese and Chinese, we used MeCab4 and Stanford Word Segmenter5 to tokenize the text. Since inflection occurs in English, Spanish, and Japanese, we used Stanford POS tagger,6 Pattern,7 and MeCab to lemmatize the text. 2http://dumps.wikimedia.org/ 3https://github.com/yohasebe/wp2txt/ 4http://taku910.github.io/mecab/ 5http://nlp.stanford.edu/software/segmenter.shtml 6http://nlp.stanford.edu/software/tagger.shtml 7http://www.clips.ua.ac.be/pages/pattern Next, we induced count-based word vectors from t</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings ofACL, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="3360" citStr="Bengio et al., 2003" startWordPosition="489" endWordPosition="492">d vectors by diverting the training data. Also, word surface forms present noisy yet useful clues on the correspondence when targeting the language pairs that have exchanged their vocabulary (e.g., “cocktail” in English and “c´octel” in Spanish). Although apparently useful, how to exploit such knowledge within the learning framework has not been addressed so far. We evaluated the proposed method in three language pairs. Compared with baselines including a method that uses vectors learned by neural networks, our method gave better results. 2 Related Work Neural networks (Mikolov et al., 2013a; Bengio et al., 2003) have recently gained much attention as a way of inducing word vectors. Although the scope of our study is currently limited to the countbased word vectors, our experiment demonstrated that the proposed method performs significantly better than strong baselines including neural networks. This suggests that count-based word vectors have a great advantage when learning a crosslingual projection. As a future work, we are also 300 Proceedings of the 19th Conference on Computational Language Learning, pages 300–304, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L´eon Bottou</author>
</authors>
<title>Stochastic learning.</title>
<date>2004</date>
<booktitle>In Advanced lectures on machine learning,</booktitle>
<pages>146--168</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="8552" citStr="Bottou, 2004" startWordPosition="1307" endWordPosition="1308">wjk. (j,k)EDtrain (j,k)EDsim The third and fourth terms are newly added to guide the learning process to strengthen wjk when 1The threshold was fixed to 0.5. W*=argmin W n i=1 W*=argmin W n i=1 301 k-th dimension in the source language corresponds to j-th dimension in the target language. Dtrain and Dsim are sets of dimension pairs found by the two methods. Qtrain and Qsim are parameters representing the strength of the new terms, and are tuned on held-out development data. 3.4 Optimization We use Pegasos algorithm (Shalev-Shwartz et al., 2011), an instance of the stochastic gradient descent (Bottou, 2004), to optimize the new objective. Given T-th learning sample (xT, zT), we update translation matrix W as follows: W ←W − •/TVET(W) where •/T represents the learning rate and is set to •/T = λT , and VET(W) is the gradient which is 1 calculated from T-th sample (xT, zT): 2(W xT − zT)xTT − QtrainA − QsimB + λW . A and B are gradients corresponding to the two new terms. A is a matrix in which alk = 1 if (j, k) E Dtrain otherwise 0. B is defined similarly. 4 Experiments We evaluate our method on translation among word vectors in four languages: English (En), Spanish (Es), Japanese (Jp) and Chinese </context>
</contexts>
<marker>Bottou, 2004</marker>
<rawString>L´eon Bottou. 2004. Stochastic learning. In Advanced lectures on machine learning, pages 146– 168. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<booktitle>Computational linguistics,</booktitle>
<pages>16--1</pages>
<contexts>
<context position="10520" citStr="Church and Hanks, 1990" startWordPosition="1616" endWordPosition="1619">http://nlp.stanford.edu/software/segmenter.shtml 6http://nlp.stanford.edu/software/tagger.shtml 7http://www.clips.ua.ac.be/pages/pattern Next, we induced count-based word vectors from the obtained text. We considered context windows of five words to both sides of the target word. The function words are then excluded from the extracted context words. Since the count vectors are very high-dimensional and sparse, we selected top-10k frequent words as contexts words (in other words, the number of dimensions of the word vectors). We converted the counts into positive point-wise mutual information (Church and Hanks, 1990) and normalized the resulting vectors to remove the bias that is introduced by the difference of the word frequency. Then, we compiled a seed bilingual dictionary (a set of bilingual word pairs) for each language pair that is used to learn and evaluate the translation matrix. We utilized cross-lingual synsets in the Open Multilingual Wordnet8 to obtain bilingual pairs. Since our method aims to be used in expanding bilingual dictionaries, we designed datasets assuming such a situation. Considering that more frequent words are likely to be registered in a dictionary, we sorted words in the sourc</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Marco Baroni</author>
</authors>
<title>How to make words with vectors: Phrase generation in distributional semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>624--633</pages>
<contexts>
<context position="4426" citStr="Dinu and Baroni, 2014" startWordPosition="652" endWordPosition="655">f the 19th Conference on Computational Language Learning, pages 300–304, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics interested in extending the method presented here to apply word vectors learned by neural networks. There are also methods that directly inducing meaning representations shared by different languages (Klementiev et al., 2012; Lauly et al., 2014; Xiao and Guo, 2014; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), rather than learning transformation between different languages (Fung, 1998; N ikolov et al., 2013b; Dinu and Baroni, 2014). However, the former approach is unable to handle words not appearing in the training data, unlike the latter approach. 3 Proposed Method 3.1 Learning cross-lingual projection We begin by introducing the previous method of learning a linear transformation from word vectors in one language into another, which are hereafter referred to as source and target language. Suppose we have a training data of n examples {(x1, z1), (x2, z2),... (xn, zn)}, where xi is the count-based vector representation of a word in the source language (e.g., “gato”), and zi is the word vector of its translation in the </context>
</contexts>
<marker>Dinu, Baroni, 2014</marker>
<rawString>Georgiana Dinu and Marco Baroni. 2014. How to make words with vectors: Phrase generation in distributional semantics. In Proceedings of ACL, pages 624–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>897--906</pages>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of EMNLP, pages 897–906.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Improving vector space word representations using multilingual correlation.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>462--471</pages>
<contexts>
<context position="4275" citStr="Faruqui and Dyer, 2014" startWordPosition="629" endWordPosition="632"> suggests that count-based word vectors have a great advantage when learning a crosslingual projection. As a future work, we are also 300 Proceedings of the 19th Conference on Computational Language Learning, pages 300–304, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics interested in extending the method presented here to apply word vectors learned by neural networks. There are also methods that directly inducing meaning representations shared by different languages (Klementiev et al., 2012; Lauly et al., 2014; Xiao and Guo, 2014; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), rather than learning transformation between different languages (Fung, 1998; N ikolov et al., 2013b; Dinu and Baroni, 2014). However, the former approach is unable to handle words not appearing in the training data, unlike the latter approach. 3 Proposed Method 3.1 Learning cross-lingual projection We begin by introducing the previous method of learning a linear transformation from word vectors in one language into another, which are hereafter referred to as source and target language. Suppose we have a training data of n examples {(x1, z1), (x2, z2),... (xn, zn)}, </context>
</contexts>
<marker>Faruqui, Dyer, 2014</marker>
<rawString>Manaal Faruqui and Chris Dyer. 2014. Improving vector space word representations using multilingual correlation. In Proceedings of EACL, pages 462– 471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<title>A synopsis of linguistic theory. Studies in Linguistic Analysis,</title>
<date>1957</date>
<pages>1--32</pages>
<contexts>
<context position="1417" citStr="Firth, 1957" startWordPosition="187" endWordPosition="188">tive function. In the experiments, our method outperformed existing methods in three language pairs, (English, Spanish), (Japanese, Chinese) and (English, Japanese), without using any additional supervisions. 1 Introduction Vector-based representations of word meanings, hereafter word vectors, have been widely used in a variety of NLP applications including synonym detection (Baroni et al., 2014), paraphrase detection (Erk and Pad´o, 2008), and dialogue analysis (Kalchbrenner and Blunsom, 2013). The basic idea behind those representation methods is the distributional hypothesis (Harris, 1954; Firth, 1957) that similar words are likely to co-occur with similar context words. A problem with the word vectors is that they are not meant for capturing the similarity between words in different languages, i.e., translation pairs such as “gato” and “cat.” The meaning representations of such word pairs are usually dissimilar, because the vast majority of the context words are from the same language as the target words (e.g., Spanish for “gato” and English for “cat”). This prevents using word vectors in multi-lingual applications such as cross-lingual information retrieval and machine translation. Severa</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John R. Firth. 1957. A synopsis of linguistic theory. Studies in Linguistic Analysis, pages 1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
</authors>
<title>A statistical view on bilingual lexicon extraction: from parallel corpora to nonparallel corpora.</title>
<date>1998</date>
<booktitle>In Machine Translation and the Information Soup,</booktitle>
<pages>1--17</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2087" citStr="Fung, 1998" startWordPosition="293" endWordPosition="294">ext words. A problem with the word vectors is that they are not meant for capturing the similarity between words in different languages, i.e., translation pairs such as “gato” and “cat.” The meaning representations of such word pairs are usually dissimilar, because the vast majority of the context words are from the same language as the target words (e.g., Spanish for “gato” and English for “cat”). This prevents using word vectors in multi-lingual applications such as cross-lingual information retrieval and machine translation. Several approaches have been made so far to address this problem (Fung, 1998; Klementiev et al., 2012; Mikolov et al., 2013b). In particular, Mikolov et al. (2013b) recently explored learning a linear transformation between word vectors of different languages from a small amount of training data, i.e., a set of bilingual word pairs. This study explores incorporating prior knowledge about the correspondence between dimensions of word vectors to learn more accurate transformation, when using count-based word vectors (Baroni et al., 2014). Since the dimensions of count-based word vectors are explicitly associated with context words, we can partially be aware of the cross</context>
<context position="4378" citStr="Fung, 1998" startWordPosition="645" endWordPosition="646"> work, we are also 300 Proceedings of the 19th Conference on Computational Language Learning, pages 300–304, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics interested in extending the method presented here to apply word vectors learned by neural networks. There are also methods that directly inducing meaning representations shared by different languages (Klementiev et al., 2012; Lauly et al., 2014; Xiao and Guo, 2014; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), rather than learning transformation between different languages (Fung, 1998; N ikolov et al., 2013b; Dinu and Baroni, 2014). However, the former approach is unable to handle words not appearing in the training data, unlike the latter approach. 3 Proposed Method 3.1 Learning cross-lingual projection We begin by introducing the previous method of learning a linear transformation from word vectors in one language into another, which are hereafter referred to as source and target language. Suppose we have a training data of n examples {(x1, z1), (x2, z2),... (xn, zn)}, where xi is the count-based vector representation of a word in the source language (e.g., “gato”), and </context>
<context position="13450" citStr="Fung, 1998" startWordPosition="2109" endWordPosition="2110">% 32.3% 51.0% 32.5% 51.9% reported the accurate translation can be obtained when the vectors in the source language is 2-4x larger than that in the target language, we prepared m-dimensional (m = 100, 200, 300) vectors for the target language and n-dimensional (n = 2m, 3m, 4m) vectors for the source language, and optimized their combinations on the development data. Direct Mapping exploits the training data to map each dimension in a word vector in the source language to the corresponding dimension in a word vector in the target language, referring to the bilingual pairs in the training data (Fung, 1998). To deal with words that have more than one translation, we weighted each translation by a reciprocal rank of its frequency among the translations in the target language, as in (Prochasson et al., 2009). Note that all methods, including the proposed methods, use the same amount of supervision (training data) and thereby they are completely comparable with each other. Evaluation procedure For each word vector in the source language, we translate it into the target language and evaluate the quality of the translation as in (Mikolov et al., 2013b): i) measure the cosine similarity between the re</context>
</contexts>
<marker>Fung, 1998</marker>
<rawString>Pascale Fung. 1998. A statistical view on bilingual lexicon extraction: from parallel corpora to nonparallel corpora. In Machine Translation and the Information Soup, pages 1–17. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Anders Søgaard</author>
</authors>
<title>Simple task-specific bilingual word embeddings.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>1386--1390</pages>
<contexts>
<context position="4301" citStr="Gouws and Søgaard, 2015" startWordPosition="633" endWordPosition="636">ed word vectors have a great advantage when learning a crosslingual projection. As a future work, we are also 300 Proceedings of the 19th Conference on Computational Language Learning, pages 300–304, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics interested in extending the method presented here to apply word vectors learned by neural networks. There are also methods that directly inducing meaning representations shared by different languages (Klementiev et al., 2012; Lauly et al., 2014; Xiao and Guo, 2014; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), rather than learning transformation between different languages (Fung, 1998; N ikolov et al., 2013b; Dinu and Baroni, 2014). However, the former approach is unable to handle words not appearing in the training data, unlike the latter approach. 3 Proposed Method 3.1 Learning cross-lingual projection We begin by introducing the previous method of learning a linear transformation from word vectors in one language into another, which are hereafter referred to as source and target language. Suppose we have a training data of n examples {(x1, z1), (x2, z2),... (xn, zn)}, where xi is the count-base</context>
</contexts>
<marker>Gouws, Søgaard, 2015</marker>
<rawString>Stephan Gouws and Anders Søgaard. 2015. Simple task-specific bilingual word embeddings. In Proceedings of NAACL-HLT, pages 1386–1390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--146</pages>
<contexts>
<context position="1403" citStr="Harris, 1954" startWordPosition="185" endWordPosition="186">s of the objective function. In the experiments, our method outperformed existing methods in three language pairs, (English, Spanish), (Japanese, Chinese) and (English, Japanese), without using any additional supervisions. 1 Introduction Vector-based representations of word meanings, hereafter word vectors, have been widely used in a variety of NLP applications including synonym detection (Baroni et al., 2014), paraphrase detection (Erk and Pad´o, 2008), and dialogue analysis (Kalchbrenner and Blunsom, 2013). The basic idea behind those representation methods is the distributional hypothesis (Harris, 1954; Firth, 1957) that similar words are likely to co-occur with similar context words. A problem with the word vectors is that they are not meant for capturing the similarity between words in different languages, i.e., translation pairs such as “gato” and “cat.” The meaning representations of such word pairs are usually dissimilar, because the vast majority of the context words are from the same language as the target words (e.g., Spanish for “gato” and English for “cat”). This prevents using word vectors in multi-lingual applications such as cross-lingual information retrieval and machine trans</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S. Harris. 1954. Distributional structure. Word, 10:146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual models for compositional distributed semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>58--68</pages>
<contexts>
<context position="4251" citStr="Hermann and Blunsom, 2014" startWordPosition="624" endWordPosition="628">uding neural networks. This suggests that count-based word vectors have a great advantage when learning a crosslingual projection. As a future work, we are also 300 Proceedings of the 19th Conference on Computational Language Learning, pages 300–304, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics interested in extending the method presented here to apply word vectors learned by neural networks. There are also methods that directly inducing meaning representations shared by different languages (Klementiev et al., 2012; Lauly et al., 2014; Xiao and Guo, 2014; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), rather than learning transformation between different languages (Fung, 1998; N ikolov et al., 2013b; Dinu and Baroni, 2014). However, the former approach is unable to handle words not appearing in the training data, unlike the latter approach. 3 Proposed Method 3.1 Learning cross-lingual projection We begin by introducing the previous method of learning a linear transformation from word vectors in one language into another, which are hereafter referred to as source and target language. Suppose we have a training data of n examples {(x1, z1), </context>
</contexts>
<marker>Hermann, Blunsom, 2014</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2014. Multilingual models for compositional distributed semantics. In Proceedings of ACL, pages 58–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent convolutional neural networks for discourse compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>119--126</pages>
<contexts>
<context position="1304" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="169" endWordPosition="172">n of word representations from one language into another. Our method utilizes translatable context pairs as bonus terms of the objective function. In the experiments, our method outperformed existing methods in three language pairs, (English, Spanish), (Japanese, Chinese) and (English, Japanese), without using any additional supervisions. 1 Introduction Vector-based representations of word meanings, hereafter word vectors, have been widely used in a variety of NLP applications including synonym detection (Baroni et al., 2014), paraphrase detection (Erk and Pad´o, 2008), and dialogue analysis (Kalchbrenner and Blunsom, 2013). The basic idea behind those representation methods is the distributional hypothesis (Harris, 1954; Firth, 1957) that similar words are likely to co-occur with similar context words. A problem with the word vectors is that they are not meant for capturing the similarity between words in different languages, i.e., translation pairs such as “gato” and “cat.” The meaning representations of such word pairs are usually dissimilar, because the vast majority of the context words are from the same language as the target words (e.g., Spanish for “gato” and English for “cat”). This prevents using word </context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent convolutional neural networks for discourse compositionality. In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality, pages 119–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ivan Titov</author>
<author>Binod Bhattarai</author>
</authors>
<title>Inducing crosslingual distributed representations of words.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1459--1474</pages>
<contexts>
<context position="2112" citStr="Klementiev et al., 2012" startWordPosition="295" endWordPosition="298"> problem with the word vectors is that they are not meant for capturing the similarity between words in different languages, i.e., translation pairs such as “gato” and “cat.” The meaning representations of such word pairs are usually dissimilar, because the vast majority of the context words are from the same language as the target words (e.g., Spanish for “gato” and English for “cat”). This prevents using word vectors in multi-lingual applications such as cross-lingual information retrieval and machine translation. Several approaches have been made so far to address this problem (Fung, 1998; Klementiev et al., 2012; Mikolov et al., 2013b). In particular, Mikolov et al. (2013b) recently explored learning a linear transformation between word vectors of different languages from a small amount of training data, i.e., a set of bilingual word pairs. This study explores incorporating prior knowledge about the correspondence between dimensions of word vectors to learn more accurate transformation, when using count-based word vectors (Baroni et al., 2014). Since the dimensions of count-based word vectors are explicitly associated with context words, we can partially be aware of the cross-lingual correspondence b</context>
<context position="4184" citStr="Klementiev et al., 2012" startWordPosition="612" endWordPosition="615">d method performs significantly better than strong baselines including neural networks. This suggests that count-based word vectors have a great advantage when learning a crosslingual projection. As a future work, we are also 300 Proceedings of the 19th Conference on Computational Language Learning, pages 300–304, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics interested in extending the method presented here to apply word vectors learned by neural networks. There are also methods that directly inducing meaning representations shared by different languages (Klementiev et al., 2012; Lauly et al., 2014; Xiao and Guo, 2014; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), rather than learning transformation between different languages (Fung, 1998; N ikolov et al., 2013b; Dinu and Baroni, 2014). However, the former approach is unable to handle words not appearing in the training data, unlike the latter approach. 3 Proposed Method 3.1 Learning cross-lingual projection We begin by introducing the previous method of learning a linear transformation from word vectors in one language into another, which are hereafter referred to as source and target </context>
</contexts>
<marker>Klementiev, Titov, Bhattarai, 2012</marker>
<rawString>Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations of words. In Proceedings of COLING, pages 1459–1474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanislas Lauly</author>
<author>Hugo Larochelle</author>
<author>Mitesh Khapra</author>
<author>Balaraman Ravindran</author>
<author>Vikas C Raykar</author>
<author>Amrita Saha</author>
</authors>
<title>An autoencoder approach to learning bilingual word representations.</title>
<date>2014</date>
<booktitle>In Advances in NIPS,</booktitle>
<pages>1853--1861</pages>
<contexts>
<context position="4204" citStr="Lauly et al., 2014" startWordPosition="616" endWordPosition="619">cantly better than strong baselines including neural networks. This suggests that count-based word vectors have a great advantage when learning a crosslingual projection. As a future work, we are also 300 Proceedings of the 19th Conference on Computational Language Learning, pages 300–304, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics interested in extending the method presented here to apply word vectors learned by neural networks. There are also methods that directly inducing meaning representations shared by different languages (Klementiev et al., 2012; Lauly et al., 2014; Xiao and Guo, 2014; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), rather than learning transformation between different languages (Fung, 1998; N ikolov et al., 2013b; Dinu and Baroni, 2014). However, the former approach is unable to handle words not appearing in the training data, unlike the latter approach. 3 Proposed Method 3.1 Learning cross-lingual projection We begin by introducing the previous method of learning a linear transformation from word vectors in one language into another, which are hereafter referred to as source and target language. Suppose we</context>
</contexts>
<marker>Lauly, Larochelle, Khapra, Ravindran, Raykar, Saha, 2014</marker>
<rawString>Stanislas Lauly, Hugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas C Raykar, and Amrita Saha. 2014. An autoencoder approach to learning bilingual word representations. In Advances in NIPS, pages 1853–1861.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at ICLR.</booktitle>
<contexts>
<context position="2134" citStr="Mikolov et al., 2013" startWordPosition="299" endWordPosition="302">ctors is that they are not meant for capturing the similarity between words in different languages, i.e., translation pairs such as “gato” and “cat.” The meaning representations of such word pairs are usually dissimilar, because the vast majority of the context words are from the same language as the target words (e.g., Spanish for “gato” and English for “cat”). This prevents using word vectors in multi-lingual applications such as cross-lingual information retrieval and machine translation. Several approaches have been made so far to address this problem (Fung, 1998; Klementiev et al., 2012; Mikolov et al., 2013b). In particular, Mikolov et al. (2013b) recently explored learning a linear transformation between word vectors of different languages from a small amount of training data, i.e., a set of bilingual word pairs. This study explores incorporating prior knowledge about the correspondence between dimensions of word vectors to learn more accurate transformation, when using count-based word vectors (Baroni et al., 2014). Since the dimensions of count-based word vectors are explicitly associated with context words, we can partially be aware of the cross-lingual correspondence between the dimensions </context>
<context position="11797" citStr="Mikolov et al., 2013" startWordPosition="1824" endWordPosition="1827">eir translations in the target language as a training/development data, and used the subsequent 1k words and their translations as a test data. We have compared our method with the following three methods: Baseline learns a translation matrix using Eq. 1 for the same count-based word vectors as the proposed method. Comparison between the proposed method and this method reveals the impact of incorporating the cross-lingual correspondences between dimensions. CBOW learns a translation matrix using Eq. 1 for word vectors learned by a neural network (specifically, continuous bag-of-words (CBOW)) (Mikolov et al., 2013b). Comparison between this method and the above baseline reveals the impact of the vector representation. Note that the CBOW-based word vectors take rare context words as well as the top-10k frequent words into account. We used word2vec9 to obtain the vectors for each language.10 Since Mikolov et al. (2013b) 8http://compling.hss.ntu.edu.sg/omw/ 9https://code.google.com/p/word2vec/ 10The threshold of sub-sampling of words was set to 1e-3 to reduce the effect of very frequent words, e.g., “a” or “the.” 302 Table 1: Experimental results: the accuracy of the translation. Testset Baseline CBOW Dir</context>
<context position="13999" citStr="Mikolov et al., 2013" startWordPosition="2197" endWordPosition="2200">age, referring to the bilingual pairs in the training data (Fung, 1998). To deal with words that have more than one translation, we weighted each translation by a reciprocal rank of its frequency among the translations in the target language, as in (Prochasson et al., 2009). Note that all methods, including the proposed methods, use the same amount of supervision (training data) and thereby they are completely comparable with each other. Evaluation procedure For each word vector in the source language, we translate it into the target language and evaluate the quality of the translation as in (Mikolov et al., 2013b): i) measure the cosine similarity between the resulting word vector and all the vectors in the test data (in the target language), ii) next choose the top-n (n = 1, 5) word vectors that have the highest similarity against the resulting vector, and iii) then examine whether the chosen vectors include the correct one. 4.2 Results Table 1 shows results of the translation between word vectors in each language pair. Proposed significantly improved the translation quality against Baseline, and performed the best among all of the methods. Although the use of CBOW-based word vectors (CBOW) has impr</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of Workshop at ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation. arXiv preprint.</title>
<date>2013</date>
<contexts>
<context position="2134" citStr="Mikolov et al., 2013" startWordPosition="299" endWordPosition="302">ctors is that they are not meant for capturing the similarity between words in different languages, i.e., translation pairs such as “gato” and “cat.” The meaning representations of such word pairs are usually dissimilar, because the vast majority of the context words are from the same language as the target words (e.g., Spanish for “gato” and English for “cat”). This prevents using word vectors in multi-lingual applications such as cross-lingual information retrieval and machine translation. Several approaches have been made so far to address this problem (Fung, 1998; Klementiev et al., 2012; Mikolov et al., 2013b). In particular, Mikolov et al. (2013b) recently explored learning a linear transformation between word vectors of different languages from a small amount of training data, i.e., a set of bilingual word pairs. This study explores incorporating prior knowledge about the correspondence between dimensions of word vectors to learn more accurate transformation, when using count-based word vectors (Baroni et al., 2014). Since the dimensions of count-based word vectors are explicitly associated with context words, we can partially be aware of the cross-lingual correspondence between the dimensions </context>
<context position="11797" citStr="Mikolov et al., 2013" startWordPosition="1824" endWordPosition="1827">eir translations in the target language as a training/development data, and used the subsequent 1k words and their translations as a test data. We have compared our method with the following three methods: Baseline learns a translation matrix using Eq. 1 for the same count-based word vectors as the proposed method. Comparison between the proposed method and this method reveals the impact of incorporating the cross-lingual correspondences between dimensions. CBOW learns a translation matrix using Eq. 1 for word vectors learned by a neural network (specifically, continuous bag-of-words (CBOW)) (Mikolov et al., 2013b). Comparison between this method and the above baseline reveals the impact of the vector representation. Note that the CBOW-based word vectors take rare context words as well as the top-10k frequent words into account. We used word2vec9 to obtain the vectors for each language.10 Since Mikolov et al. (2013b) 8http://compling.hss.ntu.edu.sg/omw/ 9https://code.google.com/p/word2vec/ 10The threshold of sub-sampling of words was set to 1e-3 to reduce the effect of very frequent words, e.g., “a” or “the.” 302 Table 1: Experimental results: the accuracy of the translation. Testset Baseline CBOW Dir</context>
<context position="13999" citStr="Mikolov et al., 2013" startWordPosition="2197" endWordPosition="2200">age, referring to the bilingual pairs in the training data (Fung, 1998). To deal with words that have more than one translation, we weighted each translation by a reciprocal rank of its frequency among the translations in the target language, as in (Prochasson et al., 2009). Note that all methods, including the proposed methods, use the same amount of supervision (training data) and thereby they are completely comparable with each other. Evaluation procedure For each word vector in the source language, we translate it into the target language and evaluate the quality of the translation as in (Mikolov et al., 2013b): i) measure the cosine similarity between the resulting word vector and all the vectors in the test data (in the target language), ii) next choose the top-n (n = 1, 5) word vectors that have the highest similarity against the resulting vector, and iii) then examine whether the chosen vectors include the correct one. 4.2 Results Table 1 shows results of the translation between word vectors in each language pair. Proposed significantly improved the translation quality against Baseline, and performed the best among all of the methods. Although the use of CBOW-based word vectors (CBOW) has impr</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine translation. arXiv preprint.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Prochasson</author>
<author>Emmanuel Morin</author>
<author>Kyo Kageura</author>
</authors>
<title>Anchor points for bilingual lexicon extraction from small comparable corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of MT Summit XII,</booktitle>
<pages>284--291</pages>
<contexts>
<context position="13653" citStr="Prochasson et al., 2009" startWordPosition="2142" endWordPosition="2145"> (m = 100, 200, 300) vectors for the target language and n-dimensional (n = 2m, 3m, 4m) vectors for the source language, and optimized their combinations on the development data. Direct Mapping exploits the training data to map each dimension in a word vector in the source language to the corresponding dimension in a word vector in the target language, referring to the bilingual pairs in the training data (Fung, 1998). To deal with words that have more than one translation, we weighted each translation by a reciprocal rank of its frequency among the translations in the target language, as in (Prochasson et al., 2009). Note that all methods, including the proposed methods, use the same amount of supervision (training data) and thereby they are completely comparable with each other. Evaluation procedure For each word vector in the source language, we translate it into the target language and evaluate the quality of the translation as in (Mikolov et al., 2013b): i) measure the cosine similarity between the resulting word vector and all the vectors in the test data (in the target language), ii) next choose the top-n (n = 1, 5) word vectors that have the highest similarity against the resulting vector, and iii</context>
</contexts>
<marker>Prochasson, Morin, Kageura, 2009</marker>
<rawString>Emmanuel Prochasson, Emmanuel Morin, and Kyo Kageura. 2009. Anchor points for bilingual lexicon extraction from small comparable corpora. In Proceedings of MT Summit XII, pages 284–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Nathan Srebro</author>
<author>Andrew Cotter</author>
</authors>
<title>Pegasos: Primal estimated sub-gradient solver for SVM.</title>
<date>2011</date>
<booktitle>Mathematical programming,</booktitle>
<pages>127--1</pages>
<contexts>
<context position="8489" citStr="Shalev-Shwartz et al., 2011" startWordPosition="1295" endWordPosition="1298">e following new objective function: kWxi − zik2 + λ2kWk2 �−βtrain �wjk − βsim wjk. (j,k)EDtrain (j,k)EDsim The third and fourth terms are newly added to guide the learning process to strengthen wjk when 1The threshold was fixed to 0.5. W*=argmin W n i=1 W*=argmin W n i=1 301 k-th dimension in the source language corresponds to j-th dimension in the target language. Dtrain and Dsim are sets of dimension pairs found by the two methods. Qtrain and Qsim are parameters representing the strength of the new terms, and are tuned on held-out development data. 3.4 Optimization We use Pegasos algorithm (Shalev-Shwartz et al., 2011), an instance of the stochastic gradient descent (Bottou, 2004), to optimize the new objective. Given T-th learning sample (xT, zT), we update translation matrix W as follows: W ←W − •/TVET(W) where •/T represents the learning rate and is set to •/T = λT , and VET(W) is the gradient which is 1 calculated from T-th sample (xT, zT): 2(W xT − zT)xTT − QtrainA − QsimB + λW . A and B are gradients corresponding to the two new terms. A is a matrix in which alk = 1 if (j, k) E Dtrain otherwise 0. B is defined similarly. 4 Experiments We evaluate our method on translation among word vectors in four la</context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, Cotter, 2011</marker>
<rawString>Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. 2011. Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical programming, 127(1):3–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Xiao</author>
<author>Yuhong Guo</author>
</authors>
<title>Distributed word representation learning for cross-lingual dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>119--129</pages>
<contexts>
<context position="4224" citStr="Xiao and Guo, 2014" startWordPosition="620" endWordPosition="623">trong baselines including neural networks. This suggests that count-based word vectors have a great advantage when learning a crosslingual projection. As a future work, we are also 300 Proceedings of the 19th Conference on Computational Language Learning, pages 300–304, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics interested in extending the method presented here to apply word vectors learned by neural networks. There are also methods that directly inducing meaning representations shared by different languages (Klementiev et al., 2012; Lauly et al., 2014; Xiao and Guo, 2014; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), rather than learning transformation between different languages (Fung, 1998; N ikolov et al., 2013b; Dinu and Baroni, 2014). However, the former approach is unable to handle words not appearing in the training data, unlike the latter approach. 3 Proposed Method 3.1 Learning cross-lingual projection We begin by introducing the previous method of learning a linear transformation from word vectors in one language into another, which are hereafter referred to as source and target language. Suppose we have a training dat</context>
</contexts>
<marker>Xiao, Guo, 2014</marker>
<rawString>Min Xiao and Yuhong Guo. 2014. Distributed word representation learning for cross-lingual dependency parsing. In Proceedings of CoNLL, pages 119–129.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>