<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000065">
<title confidence="0.982002">
Inferring Selectional Preferences from Part-Of-Speech N-grams
</title>
<author confidence="0.826463">
Hyeju Jang and Jack Mostow
</author>
<affiliation confidence="0.703030333333333">
Project LISTEN (www.cs.cmu.edu/~listen), School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.990172">
hyejuj@cs.cmu.edu, mostow@cs.cmu.edu
</email>
<sectionHeader confidence="0.984598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901470588235">
We present the PONG method to compute
selectional preferences using part-of-speech
(POS) N-grams. From a corpus labeled with
grammatical dependencies, PONG learns the
distribution of word relations for each POS
N-gram. From the much larger but unlabeled
Google N-grams corpus, PONG learns the
distribution of POS N-grams for a given pair
of words. We derive the probability that one
word has a given grammatical relation to the
other. PONG estimates this probability by
combining both distributions, whether or not
either word occurs in the labeled corpus.
PONG achieves higher average precision on
16 relations than a state-of-the-art baseline in
a pseudo-disambiguation task, but lower
coverage and recall.
</bodyText>
<sectionHeader confidence="0.992524" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99857455">
Selectional preferences specify plausible fillers
for the arguments of a predicate, e.g., celebrate.
Can you celebrate a birthday? Sure. Can you
celebrate a pencil? Arguably yes: Today the
Acme Pencil Factory celebrated its one-billionth
pencil. However, such a contrived example is
unnatural because unlike birthday, pencil lacks a
strong association with celebrate. How can we
compute the degree to which birthday or pencil
is a plausible and typical object of celebrate?
Formally, we are interested in computing the
probability Pr(r  |t, R), where (as Table 1
specifies), t is a target word such as celebrate, r
is a word possibly related to it, such as birthday
or pencil, and R is a possible relation between
them, whether a semantic role such as the agent
of an action, or a grammatical dependency such
as the object of a verb. We call t the “target”
because originally it referred to a vocabulary
word targeted for instruction, and r its “relative.”
</bodyText>
<table confidence="0.984018285714286">
Notation Description
R a relation between words
t a target word
r, r&apos; possible relatives of t
g a word N-gram
gi and gj ith and jth words of g
p the POS N-gram of g
</table>
<tableCaption confidence="0.999954">
Table 1: Notation used throughout this paper
</tableCaption>
<bodyText confidence="0.99849184">
Previous work on selectional preferences has
used them primarily for natural language analytic
tasks such as word sense disambiguation (Resnik,
1997), dependency parsing (Zhou et al., 2011),
and semantic role labeling (Gildea and Jurafsky,
2002). However, selectional preferences can
also apply to natural language generation tasks
such as sentence generation and question
generation. For generation tasks, choosing the
right word to express a specified argument of a
relation requires knowing its connotations – that
is, its selectional preferences. Therefore, it is
useful to know selectional preferences for many
different relations. Such knowledge could have
many uses. In education, they could help teach
word connotations. In machine learning they
could help computers learn languages. In
machine translation, they could help generate
more natural wording.
This paper introduces a method named PONG
(for Part-Of-Speech N-Grams) to compute
selectional preferences for many different
relations by combining part-of-speech
information and Google N-grams. PONG
achieves higher precision on a pseudo-
</bodyText>
<note confidence="0.858004666666667">
377
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 377–386,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.996982285714286">
disambiguation task than the best previous model
(Erk et al., 2010), but lower coverage.
The paper is organized as follows. Section 2
describes the relations for which we compute
selectional preferences. Section 3 describes
PONG. Section 4 evaluates PONG. Section 5
relates PONG to prior work. Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.983673" genericHeader="method">
2 Relations Used
</sectionHeader>
<bodyText confidence="0.998879173913044">
Selectional preferences characterize constraints
on the arguments of predicates. Selectional
preferences for semantic roles (such as agent and
patient) are generally more informative than for
grammatical dependencies (such as subject and
object). For example, consider these
semantically equivalent but grammatically
distinct sentences:
Pat opened the door.
The door was opened by Pat.
In both sentences the agent of opened, namely
Pat, must be capable of opening something – an
informative constraint on Pat. In contrast,
knowing that the grammatical subject of opened
is Pat in the first sentence and the door in the
second sentence tells us only that they are nouns.
Despite this limitation, selectional preferences
for grammatical dependencies are still useful, for
a number of reasons. First, in practice they
approximate semantic role labels. For instance,
typically the grammatical subject of opened is its
agent. Second, grammatical dependencies can be
extracted by parsers, which tend to be more
accurate than current semantic role labelers.
Third, the number of different grammatical
dependencies is large enough to capture diverse
relations, but not so large as to have sparse data
for individual relations. Thus in this paper, we
use grammatical dependencies as relations.
A parse tree determines the basic grammatical
dependencies between the words in a sentence.
For instance, in the parse of Pat opened the door,
the verb opened has Pat as its subject and door
as its object, and door has the as its determiner.
Besides these basic dependencies, we use two
additional types of dependencies.
Composing two basic dependencies yields a
collapsed dependency (de Marneffe and Manning,
2008). For example, consider this sentence:
The airplane flies in the sky.
Here sky is the prepositional object of in, which
is the head of a prepositional phrase attached to
flies. Composing these two dependencies yields
the collapsed dependency prep_in between flies
and sky, which captures an important semantic
relation between these two content words: sky is
the location where flies occurs. Other function
words yield different collapsed dependencies.
For example, consider these two sentences:
The airplane flies over the ocean.
The airplane flies and lands.
Collapsed dependencies for the first sentence
include prep_over between flies and ocean,
which characterizes their relative vertical
position, and conj_and between flies and lands,
which links two actions that an airplane can
perform. As these examples illustrate, collapsing
dependencies involving prepositions and
conjunctions can yield informative dependencies
between content words.
Besides collapsed dependencies, PONG infers
inverse dependencies. Inverse selectional
preferences are selectional preferences of
arguments for their predicates, such as a
preference of a subject or object for its verb.
They capture semantic regularities such as the set
of verbs that an agent can perform, which tend to
outnumber the possible agents for a verb (Erk et
al., 2010).
</bodyText>
<sectionHeader confidence="0.968705" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999981636363636">
To compute selectional preferences, PONG
combines information from a limited corpus
labeled with the grammatical dependencies
described in Section 2, and a much larger
unlabeled corpus. The key idea is to abstract
word sequences labeled with grammatical
relations into POS N-grams, in order to learn a
mapping from POS N-grams to those relations.
For instance, PONG abstracts the parsed
sentence Pat opened the door as NN VB DT NN,
with the first and last NN as the subject and
object of the VB. To estimate the distribution of
POS N-grams containing particular target and
relative words, PONG POS-tags Google N-
grams (Franz and Brants, 2006).
Section 3.1 derives PONG’s probabilistic
model for combining information from labeled
and unlabeled corpora. Section 3.2 and Section
3.3 describe how PONG estimates probabilities
from each corpus. Section 3.4 discusses a
sparseness problem revealed during probability
estimation, and how we address it in PONG.
</bodyText>
<subsectionHeader confidence="0.99698">
3.1 Probabilistic model
</subsectionHeader>
<bodyText confidence="0.999967">
We quantify the selectional preference for a
relative r to instantiate a relation R of a target t as
the probability Pr(r  |t, R), estimated as follows.
By the definition of conditional probability:
</bodyText>
<page confidence="0.884084">
378
</page>
<bodyText confidence="0.999732818181818">
We care only about the relative probability of
different r for fixed t and R, so we rewrite it as:
We calculate the denominator freq(t) as the
number of N-grams in the Google N-gram
corpus that contain t, and the numerator freq(t, r)
as the number of N-grams containing both t and r.
To estimate the factor Pr(R  |r, t) directly from
a corpus of text labeled with grammatical
relations, it would be trivial to count how often a
word r bears relation R to target word t.
However, the results would be limited to the
words in the corpus, and many relation
frequencies would be estimated sparsely or
missing altogether; t or r might not even occur.
Instead, we abstract each word in the corpus as
its part-of-speech (POS) label. Thus we abstract
The big boy ate meat as DT JJ NN VB NN. We
call this sequence of POS tags a POS N-gram.
We use POS N-grams to predict word relations.
For instance, we predict that in any word
sequence with this POS N-gram, the JJ will
modify (amod) the first NN, and the second NN
will be the direct object (dobj) of the VB.
This prediction is not 100% reliable. For
example, the initial 5-gram of The big boy ate
meat pie has the same POS 5-gram as before.
However, the dobj of its VB (ate) is not the
second NN (meat), but the subsequent NN (pie).
Thus POS N-grams predict word relations only
in a probabilistic sense.
To transform Pr(R  |r, t) into a form we can
estimate, we first apply the definition of
conditional probability:
</bodyText>
<equation confidence="0.947427">
Pr(R|t, r) Pr(R, t, r)
Pr(t, r)
</equation>
<bodyText confidence="0.9996615">
To estimate the numerator Pr(R, t, r), we first
marginalize over the POS N-gram p:
</bodyText>
<equation confidence="0.998787">
Pr(R, t, r, p)
Pr( , )
t r
</equation>
<bodyText confidence="0.997446">
We expand the numerator using the chain rule:
Cancelling the common factor yields:
We approximate the first term Pr(R  |p, t, r) as
Pr(R  |p), based on the simplifying assumption
that R is conditionally independent of t and r,
given p. In other words, we assume that given a
POS N-gram, the target and relative words t and
r give no additional information about the
probability of a relation. However, their
respective positions i and j in the POS N-gram p
matter, so we condition the probability on them:
</bodyText>
<equation confidence="0.809886">
Pr(R I P, t, r) Pr(R I P, i, j)
</equation>
<bodyText confidence="0.998252384615385">
Summing over their possible positions, we get
As Figure 1 shows, we estimate Pr(R  |p, i, j) by
abstracting the labeled corpus into POS N-grams.
We estimate Pr(p  |t = gi, r = gj) based on the
frequency of partially lexicalized POS N-grams
like DT JJ:red NN:hat VB NN among Google N-
grams with t and r in the specified positions.
Sections 3.2 and 3.3 describe how we estimate
Pr(R  |p, i, j) and Pr(p  |t = gi, r = gj), respectively.
Note that PONG estimates relative rather than
absolute probabilities. Therefore it cannot (and
does not) compare them against a fixed threshold
to make decisions about selectional preferences.
</bodyText>
<subsectionHeader confidence="0.999793">
3.2 Mapping POS I-grams to relations
</subsectionHeader>
<bodyText confidence="0.999898909090909">
To estimate Pr(R  |p, i, j), we use the Penn
Treebank Wall Street Journal (WSJ) corpus,
which is labeled with grammatical relations
using the Stanford dependency parser (Klein and
Manning, 2003).
To estimate the probability Pr(R  |p, i, j) of a
relation R between a target at position i and a
relative at position j in a POS N-gram p, we
compute what fraction of the word N-grams g
with POS N-gram p have relation R between
some target t and relative r at positions i and j:
</bodyText>
<equation confidence="0.651757666666667">
Pr(R  |p, i, j)
freq(g s.t.POS(g) p relation(gi, gj) R)
freq(g s.t.POS(g) p relation(gi, gj))
</equation>
<subsectionHeader confidence="0.996323">
3.3 Estimating POS I-gram distributions
</subsectionHeader>
<bodyText confidence="0.791251142857143">
Given a target and relative, we need to estimate
their distribution of POS N-grams and positions.
We use the chain rule:
Pr(R I r, t) Pr(r I t) Pr(t)
and notice that t is held constant:
Pr(R I r, t) Pr(r I t)
We estimate the second factor as follows:
</bodyText>
<figure confidence="0.813211">
p
379
</figure>
<figureCaption confidence="0.999906">
Figure 1: Overview of PONG.
</figureCaption>
<bodyText confidence="0.9015216">
From the labeled corpus, PONG extracts abstract mappings from POS N-grams to relations.
From the unlabeled corpus, PONG estimates POS N-gram probability given a target and relative.
freq(g s.t. g t g r)
i j
A labeled corpus is too sparse for this purpose,
so we use the much larger unlabeled Google N-
grams corpus (Franz and Brants, 2006).
The probability that an N-gram with target t at
position i and relative r at position j will have the
POS N-gram p is:
</bodyText>
<equation confidence="0.745217">
Pr(p  |t gi , r gj )
freq(g s.t.POS(g) p,gi t,gj r))
</equation>
<bodyText confidence="0.9999505">
To compute this ratio, we first use a well-
indexed table to efficiently retrieve all N-grams
with words t and r at the specified positions. We
then obtain their POS N-grams from the Stanford
POS tagger (Toutanova et al., 2003), and count
how many of them have the POS N-gram p.
</bodyText>
<subsectionHeader confidence="0.99324">
3.4 Reducing POS I-gram sparseness
</subsectionHeader>
<bodyText confidence="0.999929333333333">
We abstract word N-grams into POS N-grams to
address the sparseness of the labeled corpus, but
even the POS N-grams can be sparse. For n=5,
the rarer ones occur too sparsely (if at all) in our
labeled corpus to estimate their frequency.
To address this issue, we use a coarser POS
tag set than the Penn Treebank POS tag set. As
Table 2 shows, we merge tags for adjectives,
nouns, adverbs, and verbs into four coarser tags.
</bodyText>
<table confidence="0.9987618">
Coarse Original
ADJ JJ, JJR, JJS
ADVERB RB, RBR, RBS
NOUN NN, NNS, NNP, NNPS
VERB VB, VBD, VBG, VBN, VBP, VBZ
</table>
<tableCaption confidence="0.999184">
Table 2: Coarser POS tag set used in PONG
</tableCaption>
<bodyText confidence="0.9996825625">
To gauge the impact of the coarser POS tags,
we calculated Pr(r  |t, R) for 76 test instances
used in an earlier unpublished study by Liu Liu,
a former Project LISTEN graduate student. Each
instance consists of two randomly chosen words
in the WSJ corpus labeled with a grammatical
relation. Coarse POS tags increased coverage of
this pilot set – that is, the fraction of instances for
which PONG computes a probability – from 69%
to 92%.
Using the universal tag set (Petrov et al., 2011)
as an even coarser tag set is an interesting future
direction, especially for other languages. Its
smaller size (12 tags vs. our 23) should reduce
data sparseness, but increase the risk of over-
generalization.
</bodyText>
<sectionHeader confidence="0.998028" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999791777777778">
To evaluate PONG, we use a standard pseudo-
disambiguation task, detailed in Section 4.1.
Section 4.2 describes our test set. Section 4.3
lists the metrics we evaluate on this test set.
Section 4.4 describes the baselines we compare
PONG against on these metrics, and Section 4.5
describes the relations we compare them on.
Section 4.6 reports our results. Section 4.7
analyzes sources of error.
</bodyText>
<subsectionHeader confidence="0.99023">
4.1 Evaluation task
</subsectionHeader>
<bodyText confidence="0.999970923076923">
The pseudo-disambiguation task (Gale et al.,
1992; Schutze, 1992) is as follows: given a
target word t, a relation R, a relative r, and a
random distracter r&apos;, prefer either r or r&apos;,
whichever is likelier to have relation R to word t.
This evaluation does not use a threshold: just
prefer whichever word is likelier according to the
model being evaluated. If the model assigns only
one of the words a probability, prefer it, based on
the assumption that the unknown probability of
the other word is lower. If the model assigns the
same probability to both words, or no probability
to either word, do not prefer either word.
</bodyText>
<page confidence="0.514651">
380
</page>
<subsectionHeader confidence="0.9949">
4.2 Test set
</subsectionHeader>
<bodyText confidence="0.99985852631579">
As a source of evaluation data, we used the
British National Corpus (BNC). As a common
test corpus for all the methods we evaluated, we
selected one half of BNC by sorting filenames
alphabetically and using the odd-numbered files.
We used the other half of BNC as a training
corpus for the baseline methods we compared
PONG to.
A test set for the pseudo-disambiguation task
task consists of tuples of the form (R, t, r, r&apos;). To
construct a test set, we adapted the process used
by Rooth et al. (1999) and Erk et al. (2010).
First, we chose 100 (R, t) pairs for each
relation R at random from the test corpus. Rooth
et al. (1999) and Erk et al. (2010) chose such
pairs from a training corpus to ensure that it
contained the target t. In contrast, choosing pairs
from an unseen test corpus includes target words
whether or not they occur in the training corpus.
To obtain a sample stratified by frequency,
rather than skewed heavily toward high-
frequency pairs, Erk et al. (2010) drew (R, t)
pairs from each of five frequency bands in the
entire British National Corpus (BNC): 50-100
occurrences; 101-200; 201-500; 500-1000; and
more than 1000. However, we use only half of
BNC as our test corpus, so to obtain a
comparable test set, we drew 20 (R, t) pairs from
each of the corresponding frequency bands in
that half: 26-50 occurrences; 51-100; 101-250;
251-500; and more than 500.
For each chosen (R, t) pair, we drew a separate
(R, t, r) triple from each of six frequency bands:
1-25 occurrences; 26-50; 51-100; 101-250; 251-
500; and more than 500. We necessarily omitted
frequency bands that contained no such triples.
We filtered out triples where r did not have the
most frequent part of speech for the relation R.
For example, this filter would exclude the triple
(dobj, celebrate, the) because a direct object is
most frequently a noun, but the is a determiner.
Then, like Erk et al. (2010), we paired the
relative r in each (R, t, r) triple with a distracter r&apos;
with the same (most frequent) part of speech as
the relative r, yielding the test tuple (R, t, r, r&apos;).
Rooth et al. (1999) restricted distracter
candidates to words with between 30 and 3,000
occurrences in BNC; accordingly, we chose only
distracters with between 15 and 1,500
occurrences in our test corpus. We selected r&apos;
from these candidates randomly, with probability
proportional to their frequency in the test corpus.
Like Rooth et al. (1999), we excluded as
distracters any actual relatives, i.e. candidates r&apos;
where the test corpus contained the triple (R, t, r&apos;).
Table 3 shows the resulting number of (R, t, r, r&apos;)
test tuples for each relation.
</bodyText>
<table confidence="0.999885">
Relation R # tuples for R # tuples for RT
advmod 121 131
amod 162 128
conj and 155 151
dobj 145 167
nn 173 158
nsubj 97 124
prep of 144 153
xcomp 139 140
</table>
<tableCaption confidence="0.999705">
Table 3: Test set size for each relation
</tableCaption>
<subsectionHeader confidence="0.994495">
4.3 Metrics
</subsectionHeader>
<bodyText confidence="0.999942909090909">
We report four evaluation metrics: precision,
coverage, recall, and F-score. Precision (called
“accuracy” in some papers on selectional
preferences) is the percentage of all covered
tuples where the original relative r is preferred.
Coverage is the percentage of tuples for which
the model prefers r to r&apos; or vice versa. Recall is
the percentage of all tuples where the original
relative is preferred, i.e., precision times
coverage. F-score is the harmonic mean of
precision and recall.
</bodyText>
<subsectionHeader confidence="0.997983">
4.4 Baselines
</subsectionHeader>
<bodyText confidence="0.999864380952381">
We compare PONG to two baseline methods.
EPP is a state-of-the-art model for which Erk
et al. (2010) reported better performance than
both Resnik’s (1996) WordNet model and
Rooth’s (1999) EM clustering model. EPP
computes selectional preferences using
distributional similarity, based on the assumption
that relatives are likely to appear in the same
contexts as relatives seen in the training corpus.
EPP computes the similarity of a potential
relative’s vector space representation to relatives
in the training corpus.
EPP has various options for its vector space
representation, similarity measure, weighting
scheme, generalization space, and whether to use
PCA. In re-implementing EPP, we chose the
options that performed best according to Erk et al.
(2010), with one exception. To save work, we
chose not to use PCA, which Erk et al. (2010)
described as performing only slightly better in
the dependency-based space.
</bodyText>
<page confidence="0.672176">
381
</page>
<table confidence="0.999350888888889">
Relation Target Relative Description
advmod verb adverb Adverbial modifier
amod noun adjective Adjective modifier
conj and noun noun Conjunction with “and”
dobj verb noun Direct object
nn noun noun Noun compound modifier
nsubj verb noun Nominal subject
prep of noun noun Prepositional modifier
xcomp verb verb Open clausal complement
</table>
<tableCaption confidence="0.974331666666667">
Table 4: Relations tested in the pseudo-disambiguation experiment.
Relation names and descriptions are from de Marneffe and Manning (2008) except for prep_of.
Target and relative POS are the most frequent POS pairs for the relations in our labeled WSJ corpus.
</tableCaption>
<table confidence="0.999987315789474">
Relation Precision (%) Coverage (%) Recall (%) F-score (%)
PONG EPP DEP PONG EPP DEP PONG EPP DEP PONG EPP DEP
advmod 78.7 - 98.6 72.1 - 69.2 56.7 - 68.3 65.9 - 80.7
advmodT 89.0 71.0 97.4 69.5 100 59.5 61.8 71.0 58.0 73.0 71.0 72.7
amod 78.8 - 99.0 90.1 - 61.1 71.0 - 60.5 74.7 - 75.1
amodT 84.1 74.0 97.3 83.6 99.2 57.0 70.3 73.4 55.5 76.6 73.7 70.6
conj_and 77.2 74.2 100 73.6 100 52.3 56.8 74.2 52.3 65.4 74.2 68.6
conj_andT 80.5 70.2 97.3 74.8 100 49.7 60.3 70.2 48.3 68.9 70.2 64.6
dobj 87.2 80.0 97.7 80.7 100 60.0 70.3 80.0 58.6 77.9 80.0 73.3
dobjT 89.6 80.2 98.1 92.2 100 64.1 82.6 80.2 62.9 86.0 80.2 76.6
nn 86.7 73.8 97.2 95.3 99.4 63.0 82.7 73.4 61.3 84.6 73.6 75.2
nnT 83.8 79.7 99.0 93.7 100 60.8 78.5 79.7 60.1 81.0 79.7 74.8
nsubj 76.1 77.3 100 69.1 100 42.3 52.6 77.3 42.3 62.2 77.3 59.4
nsubjT 78.5 66.9 95.0 86.3 100 48.4 67.7 66.9 46.0 72.7 66.9 62.0
prep_of 88.4 77.8 98.4 84.0 100 44.4 74.3 77.8 43.8 80.3 77.8 60.6
prep_ofT 79.2 76.5 97.4 81.7 100 50.3 64.7 76.5 49.0 71.2 76.5 65.2
xcomp 84.0 61.9 95.3 85.6 100 61.2 71.9 61.9 58.3 77.5 61.9 72.3
xcompT 86.4 78.6 98.9 89.3 100 63.6 77.1 78.6 62.9 81.5 78.6 76.9
average 83.0 74.4 97.9 82.6 99.9 56.7 68.7 74.4 55.5 75.0 74.4 70.5
</table>
<tableCaption confidence="0.92572">
Table 5: Coverage, Precision, Recall, and F-score for various relations; RT is the inverse of relation R.
PONG uses POS N-grams, EPP uses distributional similarity, and DEP uses dependency parses.
</tableCaption>
<bodyText confidence="0.867428">
To score a potential relative r0, EPP uses this
formula:
</bodyText>
<equation confidence="0.903247">
SelprefR t (r0) wtR,t (r) sim(r0, r)
r Seenargs(R,t) ZR,t
</equation>
<bodyText confidence="0.959680230769231">
Here sim(r0, r) is the nGCM similarity defined
below between vector space representations of r0
and a relative r seen in the training data:
i
The weight function wtr,t(a) is analogous to
inverse document frequency in Information
Retrieval.
DEP, our second baseline method, runs the
Stanford dependency parser to label the training
corpus with grammatical relations, and uses their
frequencies to predict selectional preferences.
To do the pseudo-disambiguation task, DEP
compares the frequencies of (R, t, r) and (R, t, r&apos;).
</bodyText>
<subsectionHeader confidence="0.992627">
4.5 Relations tested
</subsectionHeader>
<bodyText confidence="0.9654333">
To test PONG, EPP, and DEP, we chose the
most frequent eight relations between content
words in the WSJ corpus, which occur over
10,000 times and are described in Table 4. We
also tested their inverse relations. However, EPP
does not compute selectional preferences for
adjective and adverb as relatives. For this reason,
we did not test EPP on advmod and amod
relations with adverbs and adjectives as relatives.
where
</bodyText>
<figure confidence="0.981430533333333">
a
2
abi
n
1
simnGCM
n a a &apos;
i (a, a&apos;) exp( ( i )2 )
&apos;
b b
i
1
a
a
382
</figure>
<subsectionHeader confidence="0.987263">
4.6 Experimental results
</subsectionHeader>
<bodyText confidence="0.999770357142857">
Table 5 displays results for all 16 relations. To
compute statistical significance conservatively in
comparing methods, we used paired t-tests with
N = 16 relations.
PONG’s precision was significantly better
than EPP (p&lt;0.001) but worse than DEP
(p&lt;0.0001). Still, PONG’s high precision
validates its underlying assumption that POS N-
grams strongly predict grammatical
dependencies.
On coverage and recall, EPP beat PONG,
which beat DEP (p&lt;0.0001). PONG’s F-score
was higher, but not significantly, than EPP’s
(p&gt;0.5) or DEP’s (p&gt;0.02).
</bodyText>
<subsectionHeader confidence="0.995782">
4.7 Error analysis
</subsectionHeader>
<bodyText confidence="0.998555095238095">
In the pseudo-disambiguation task of choosing
which of two words is related to a target, PONG
makes errors of coverage (preferring neither
word) and precision (preferring the wrong word).
Coverage errors, which occurred 17.4% of the
time on average, arose only when PONG failed
to estimate a probability for either word. PONG
fails to score a potential relative r of a target t
with a specified relation R if the labeled corpus
has no POS N-grams that (a) map to R, (b)
contain the POS of t and r, and (c) match Google
word N-grams with t and r at those positions.
Every relation has at least one POS N-gram that
maps to it, so condition (a) never fails. PONG
uses the most frequent POS of t and r, and we
believe that condition (b) never fails. However,
condition (c) can and does fail when t and r do
not co-occur in any Google N-grams, at least that
match a POS N-gram that can map to relation R.
For example, oversee and diet do not co-occur in
any Google N-grams, so PONG cannot score diet
as a potential dobj of oversee.
Precision errors, which occur 17% of the time
on average, arose when (a) PONG scored the
distracter but failed to score the true relative, or
(b) scored them both but preferred the distracter.
Case (a) accounted for 44.62% of the errors on
the covered test tuples.
One likely cause of errors in case (b) is over-
generalization when PONG abstracts a word N-
gram labeled with a relation by mapping its POS
N-gram to that relation. In particular, the coarse
POS tag set may discard too much information.
Another likely cause of errors is probabilities
estimated poorly due to sparse data. The
probability of a relation for a POS N-gram rare in
the training corpus is likely to be inaccurate. So
is the probability of a POS N-gram for rare co-
occurrences of a target and relative in Google
word N-grams. Using a smaller tag set may
reduce the sparse data problem but increase the
risk of over-generalization.
</bodyText>
<sectionHeader confidence="0.897906" genericHeader="method">
5 Relation to Prior Work
</sectionHeader>
<bodyText confidence="0.999960659574468">
In predicting selectional preferences, a key
issue is generalization. Our DEP baseline simply
counts co-occurrences of target and relative
words in a corpus to predict selectional
preferences, but only for words seen in the
corpus. Prior work, summarized in
Table 6, has therefore tried to infer the similarity
of unseen relatives to seen relatives. To illustrate,
consider the problem of inducing that the direct
objects of celebrate tend to be days or events.
Resnik (1996) combined WordNet with a
labeled corpus to model the probability that
relatives of a predicate belong to a particular
conceptual class. This method could notice, for
example, that the direct objects of celebrate tend
to belong to the conceptual class event. Thus it
could prefer anniversary or occasion as the
object of celebrate even if unseen in its training
corpus. However, this method depends strongly
on the WordNet taxonomy.
Rather than use linguistic resources such as
WordNet, Rooth et al. (1999) and Wald et al.
(2008) induced semantically annotated
subcategorization frames from unlabeled corpora.
They modeled semantic classes as hidden
variables, which they estimated using EM-based
clustering. Ritter (2010) computed selectional
preferences by using unsupervised topic models
such as LinkLDA, which infers semantic classes
of words automatically instead of requiring a pre-
defined set of classes as input.
The contexts in which a linguistic unit occurs
provide information about its meaning. Erk
(2007) and Erk et al. (2010) modeled the
contexts of a word as the distribution of words
that co-occur with it. They calculated the
semantic similarity of two words as the similarity
of their context distributions according to various
measures. Erk et al. (2010) reported the state-of-
the-art method we used as our EPP baseline.
In contrast to prior work that explored various
solutions to the generalization problem, we don’t
so much solve this problem as circumvent it.
Instead of generalizing from a training corpus
directly to unseen words, PONG abstracts a word
N-gram to a POS N-gram and maps it to the
relations that the word N-gram is labeled with.
</bodyText>
<page confidence="0.70534">
383
</page>
<table confidence="0.997106377777778">
Reference Relation to Lexical Primary corpus Generalization Method
target resource (labeled) &amp; corpus
information (unlabeled) &amp;
used information used
Resnik, Verb-object Senses in Target, relative, none Information
1996 Verb-subject WordNet and relation in a theoretic
Adjective-noun noun parsed, partially model
Modifier-head taxonomy sense-tagged
Head-modifier corpus (Brown
corpus)
Rooth et Verb-object none Target, relative, none EM-based
al., 1999 Verb-subject and relation in a clustering
parsed corpus
(parsed BNC)
Ritter, Verb-subject none Subject-verb- none LDA model
2010 Verb-object object tuples
Subject-verb- from 500 million
object web-pages
Erk, 2007 Predicate and none Target, relative, Words and their Similarity
Semantic roles and relation in a relations in a model based
semantic role parsed corpus on word co-
labeled corpus (BNC) occurrence
(FrameNet)
Erk et al., SYN option: none Target, relative, Two options: Similarity
2010 Verb-subject and relation in WORDSPACE: model using
Verb-object, and SYN option: a an unlabeled vector space
their inverse parsed corpus corpus (BNC) representation
relations (parsed BNC) DEPSPACE: of words
SEM option: SEM option: a Words and their
verb and semantic role subject and object
semantic roles labeled corpus relations in a
that have nouns (FrameNet) parsed corpus
as their headword (parsed BNC)
in a primary
corpus, and their
inverse relations
Zhou et Any (relations none Counts of words none PMI
al., 2011 not distinguished) in Web or (Pointwise
Google N-gram Mutual
Information)
This paper All grammatical none POS N-gram POS N-gram Combine both
dependencies in a distribution for distribution for POS N-gram
parsed corpus, relations in target and relative distributions
and their inverse parsed WSJ in Google N-gram
relations corpus
</table>
<tableCaption confidence="0.999907">
Table 6: Comparison with prior methods to compute selectional preferences
</tableCaption>
<bodyText confidence="0.999921333333333">
To compute selectional preferences, whether the
words are in the training corpus or not, PONG
applies these abstract mappings to word N-grams
in the much larger Google N-grams corpus.
Some prior work on selectional preferences
has used POS N-grams and a large unlabeled
corpus. The most closely related work we found
was by Gormley et al. (2011). They used
patterns in POS N-grams to generate test data for
their selectional preferences model, but not to
infer preferences. Zhou et al. (2011) identified
selectional preferences of one word for another
</bodyText>
<page confidence="0.799992">
384
</page>
<bodyText confidence="0.999823">
by using Pointwise Mutual Information (PMI)
(Fano, 1961) to check whether they co-occur
more frequently in a large corpus than predicted
by their unigram frequencies. However, their
method did not distinguish among different
relations.
</bodyText>
<sectionHeader confidence="0.998211" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999935">
This paper describes, derives, and evaluates
PONG, a novel probabilistic model of selectional
preferences. PONG uses a labeled corpus to map
POS N-grams to grammatical relations. It
combines this mapping with probabilities
estimated from a much larger POS-tagged but
unlabeled Google N-grams corpus.
We tested PONG on the eight most common
relations in the WSJ corpus, and their inverses –
more relations than evaluated in prior work.
Compared to the state-of-the-art EPP baseline
(Erk et al., 2010), PONG averaged higher
precision but lower coverage and recall.
Compared to the DEP baseline, PONG averaged
lower precision but higher coverage and recall.
All these differences were substantial (p &lt; 0.001).
Compared to both baselines, PONG’s average F-
score was higher, though not significantly.
Some directions for future work include: First,
improve PONG by incorporating models of
lexical similarity explored in prior work. Second,
use the universal tag set to extend PONG to other
languages, or to perform better in English. Third,
in place of grammatical relations, use rich,
diverse semantic roles, while avoiding sparsity.
Finally, use selectional preferences to teach word
connotations by using various relations to
generate example sentences or useful questions.
</bodyText>
<sectionHeader confidence="0.997138" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999878375">
The research reported here was supported by the
Institute of Education Sciences, U.S. Department
of Education, through Grant R305A080157. The
opinions expressed are those of the authors and
do not necessarily represent the views of the
Institute or the U.S. Department of Education.
We thank the helpful reviewers and Katrin Erk
for her generous assistance.
</bodyText>
<sectionHeader confidence="0.994328" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999962826923077">
de Marneffe, M.-C. and Manning, C.D. 2008.
Stanford Typed Dependencies Manual.
http://nlp.stanford.edu/software/dependencies_man
ual.pdf, Stanford University, Stanford, CA.
Erk, K. 2007. A Simple, Similarity-Based Model for
Selectional Preferences. In Proceedings of the 45th
Annual Meeting of the Association of
Computational Linguistics, Prague, Czech
Republic, June, 2007, 216-223.
Erk, K., Padó, S. and Padó, U. 2010. A Flexible,
Corpus-Driven Model of Regular and Inverse
Selectional Preferences. Computational Linguistics
36(4), 723-763.
Fano, R. 1961. Transmission O F Information: A
Statistical Theory of Communications. MIT
Press, Cambridge, MA.
Franz, A. and Brants, T. 2006. All Our N-Gram Are
Belong to You.
Gale, W.A., Church, K.W. and Yarowsky, D. 1992.
Work on Statistical Methods for Word Sense
Disambiguation. In Proceedings of the AAAI Fall
Symposium on Probabilistic Approaches to Natural
Language, Cambridge, MA, October 23–25, 1992,
54-60.
Gildea, D. and Jurafsky, D. 2002. Automatic Labeling
of Semantic Roles. Computational Linguistics
28(3), 245-288.
Gormley, M.R., Dredze, M., Durme, B.V. and Eisner,
J. 2011. Shared Components Topic Models with
Application to Selectional Preference, NIPS
Workshop on Learning Semantics Sierra Nevada,
Spain.
im Walde, S.S., Hying, C., Scheible, C. and Schmid,
H. 2008. Combining Em Training and the Mdl
Principle for an Automatic Verb Classification
Incorporating Selectional Preferences. In
Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics,
Columbus, OH, 2008, 496-504.
Klein, D. and Manning, C.D. 2003. Accurate
Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for
Computational Linguistics, Sapporo, Japan, July 7-
12, 2003, E.W. HINRICHS and D. ROTH, Eds.
Petrov, S., Das, D. and McDonald, R.T. 2011. A
Universal Part-of-Speech Tagset. ArXiv
1104.2086.
Resnik, P. 1996. Selectional Constraints: An
Information-Theoretic Model and Its
Computational Realization. Cognition 61, 127-159.
Resnik, P. 1997. Selectional Preference and Sense
Disambiguation. In ACL SIGLEX Workshop on
</reference>
<page confidence="0.641617">
385
</page>
<reference confidence="0.9997745625">
Tagging Text with Lexical Semantics: Why, What,
and How, Washington, DC, April 4-5, 1997, 52-57.
Ritter, A., Mausam and Etzioni, O. 2010. A Latent
Dirichlet Allocation Method for Selectional
Preferences. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics, Uppsala, Sweden, 2010, 424-434.
Rooth, M., Riezler, S., Prescher, D., Carroll, G. and
Beil, F. 1999. Inducing a Semantically Annotated
Lexicon Via Em-Based Clustering. In Proceedings
of the 37th Annual Meeting of the Association for
Computational Linguistics on Computational
Linguistics, College Park, MD, 1999, Association
for Computational Linguistics, 104-111.
Schutze, H. 1992. Context Space. In Proceedings of
the AAAI Fall Symposium on Intelligent
Probabilistic Approaches to Natural Language,
Cambridge, MA, 1992, 113-120.
Toutanova, K., Klein, D., Manning, C. and Singer, Y.
2003. Feature-Rich Part-of-Speech Tagging with a
Cyclic Dependency Network. In Proceedings of the
Human Language Technology Conference and
Annual Meeting of the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL), Edmonton, Canada, 2003, 252–
259.
Zhou, G., Zhao, J., Liu, K. and Cai, L. 2011.
Exploiting Web-Derived Selectional Preference to
Improve Statistical Dependency Parsing. In
Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics,
Portland, OR, 2011, 1556–1565.
</reference>
<page confidence="0.943815">
386
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.965749">
<title confidence="0.999857">Inferring Selectional Preferences from Part-Of-Speech N-grams</title>
<author confidence="0.999461">Hyeju Jang</author>
<author confidence="0.999461">Jack</author>
<affiliation confidence="0.9996725">LISTEN(www.cs.cmu.edu/~listen),School of Computer Carnegie Mellon</affiliation>
<address confidence="0.992364">Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.998507222222222">We present the PONG method to compute selectional preferences using part-of-speech (POS) N-grams. From a corpus labeled with grammatical dependencies, PONG learns the distribution of word relations for each POS N-gram. From the much larger but unlabeled Google N-grams corpus, PONG learns the distribution of POS N-grams for a given pair of words. We derive the probability that one word has a given grammatical relation to the other. PONG estimates this probability by combining both distributions, whether or not either word occurs in the labeled corpus. PONG achieves higher average precision on 16 relations than a state-of-the-art baseline in a pseudo-disambiguation task, but lower coverage and recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M-C de Marneffe</author>
<author>C D Manning</author>
</authors>
<title>Stanford Typed Dependencies Manual. http://nlp.stanford.edu/software/dependencies_man ual.pdf,</title>
<date>2008</date>
<institution>Stanford University,</institution>
<location>Stanford, CA.</location>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>de Marneffe, M.-C. and Manning, C.D. 2008. Stanford Typed Dependencies Manual. http://nlp.stanford.edu/software/dependencies_man ual.pdf, Stanford University, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
</authors>
<title>A Simple, Similarity-Based Model for Selectional Preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>216--223</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="26291" citStr="Erk (2007)" startWordPosition="4412" endWordPosition="4413">y on the WordNet taxonomy. Rather than use linguistic resources such as WordNet, Rooth et al. (1999) and Wald et al. (2008) induced semantically annotated subcategorization frames from unlabeled corpora. They modeled semantic classes as hidden variables, which they estimated using EM-based clustering. Ritter (2010) computed selectional preferences by using unsupervised topic models such as LinkLDA, which infers semantic classes of words automatically instead of requiring a predefined set of classes as input. The contexts in which a linguistic unit occurs provide information about its meaning. Erk (2007) and Erk et al. (2010) modeled the contexts of a word as the distribution of words that co-occur with it. They calculated the semantic similarity of two words as the similarity of their context distributions according to various measures. Erk et al. (2010) reported the state-ofthe-art method we used as our EPP baseline. In contrast to prior work that explored various solutions to the generalization problem, we don’t so much solve this problem as circumvent it. Instead of generalizing from a training corpus directly to unseen words, PONG abstracts a word N-gram to a POS N-gram and maps it to th</context>
<context position="27610" citStr="Erk, 2007" startWordPosition="4609" endWordPosition="4610">ion Method target resource (labeled) &amp; corpus information (unlabeled) &amp; used information used Resnik, Verb-object Senses in Target, relative, none Information 1996 Verb-subject WordNet and relation in a theoretic Adjective-noun noun parsed, partially model Modifier-head taxonomy sense-tagged Head-modifier corpus (Brown corpus) Rooth et Verb-object none Target, relative, none EM-based al., 1999 Verb-subject and relation in a clustering parsed corpus (parsed BNC) Ritter, Verb-subject none Subject-verb- none LDA model 2010 Verb-object object tuples Subject-verb- from 500 million object web-pages Erk, 2007 Predicate and none Target, relative, Words and their Similarity Semantic roles and relation in a relations in a model based semantic role parsed corpus on word colabeled corpus (BNC) occurrence (FrameNet) Erk et al., SYN option: none Target, relative, Two options: Similarity 2010 Verb-subject and relation in WORDSPACE: model using Verb-object, and SYN option: a an unlabeled vector space their inverse parsed corpus corpus (BNC) representation relations (parsed BNC) DEPSPACE: of words SEM option: SEM option: a Words and their verb and semantic role subject and object semantic roles labeled corp</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Erk, K. 2007. A Simple, Similarity-Based Model for Selectional Preferences. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, Prague, Czech Republic, June, 2007, 216-223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
<author>S Padó</author>
<author>U Padó</author>
</authors>
<title>A Flexible, Corpus-Driven Model of Regular and Inverse Selectional Preferences.</title>
<date>2010</date>
<journal>Computational Linguistics</journal>
<volume>36</volume>
<issue>4</issue>
<pages>723--763</pages>
<contexts>
<context position="3527" citStr="Erk et al., 2010" startWordPosition="530" endWordPosition="533"> computers learn languages. In machine translation, they could help generate more natural wording. This paper introduces a method named PONG (for Part-Of-Speech N-Grams) to compute selectional preferences for many different relations by combining part-of-speech information and Google N-grams. PONG achieves higher precision on a pseudo377 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 377–386, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics disambiguation task than the best previous model (Erk et al., 2010), but lower coverage. The paper is organized as follows. Section 2 describes the relations for which we compute selectional preferences. Section 3 describes PONG. Section 4 evaluates PONG. Section 5 relates PONG to prior work. Section 6 concludes. 2 Relations Used Selectional preferences characterize constraints on the arguments of predicates. Selectional preferences for semantic roles (such as agent and patient) are generally more informative than for grammatical dependencies (such as subject and object). For example, consider these semantically equivalent but grammatically distinct sentences</context>
<context position="6819" citStr="Erk et al., 2010" startWordPosition="1025" endWordPosition="1028">nj_and between flies and lands, which links two actions that an airplane can perform. As these examples illustrate, collapsing dependencies involving prepositions and conjunctions can yield informative dependencies between content words. Besides collapsed dependencies, PONG infers inverse dependencies. Inverse selectional preferences are selectional preferences of arguments for their predicates, such as a preference of a subject or object for its verb. They capture semantic regularities such as the set of verbs that an agent can perform, which tend to outnumber the possible agents for a verb (Erk et al., 2010). 3 Method To compute selectional preferences, PONG combines information from a limited corpus labeled with the grammatical dependencies described in Section 2, and a much larger unlabeled corpus. The key idea is to abstract word sequences labeled with grammatical relations into POS N-grams, in order to learn a mapping from POS N-grams to those relations. For instance, PONG abstracts the parsed sentence Pat opened the door as NN VB DT NN, with the first and last NN as the subject and object of the VB. To estimate the distribution of POS N-grams containing particular target and relative words, </context>
<context position="15381" citStr="Erk et al. (2010)" startWordPosition="2551" endWordPosition="2554">y to both words, or no probability to either word, do not prefer either word. 380 4.2 Test set As a source of evaluation data, we used the British National Corpus (BNC). As a common test corpus for all the methods we evaluated, we selected one half of BNC by sorting filenames alphabetically and using the odd-numbered files. We used the other half of BNC as a training corpus for the baseline methods we compared PONG to. A test set for the pseudo-disambiguation task task consists of tuples of the form (R, t, r, r&apos;). To construct a test set, we adapted the process used by Rooth et al. (1999) and Erk et al. (2010). First, we chose 100 (R, t) pairs for each relation R at random from the test corpus. Rooth et al. (1999) and Erk et al. (2010) chose such pairs from a training corpus to ensure that it contained the target t. In contrast, choosing pairs from an unseen test corpus includes target words whether or not they occur in the training corpus. To obtain a sample stratified by frequency, rather than skewed heavily toward highfrequency pairs, Erk et al. (2010) drew (R, t) pairs from each of five frequency bands in the entire British National Corpus (BNC): 50-100 occurrences; 101-200; 201-500; 500-1000; </context>
<context position="16755" citStr="Erk et al. (2010)" startWordPosition="2792" endWordPosition="2795">onding frequency bands in that half: 26-50 occurrences; 51-100; 101-250; 251-500; and more than 500. For each chosen (R, t) pair, we drew a separate (R, t, r) triple from each of six frequency bands: 1-25 occurrences; 26-50; 51-100; 101-250; 251- 500; and more than 500. We necessarily omitted frequency bands that contained no such triples. We filtered out triples where r did not have the most frequent part of speech for the relation R. For example, this filter would exclude the triple (dobj, celebrate, the) because a direct object is most frequently a noun, but the is a determiner. Then, like Erk et al. (2010), we paired the relative r in each (R, t, r) triple with a distracter r&apos; with the same (most frequent) part of speech as the relative r, yielding the test tuple (R, t, r, r&apos;). Rooth et al. (1999) restricted distracter candidates to words with between 30 and 3,000 occurrences in BNC; accordingly, we chose only distracters with between 15 and 1,500 occurrences in our test corpus. We selected r&apos; from these candidates randomly, with probability proportional to their frequency in the test corpus. Like Rooth et al. (1999), we excluded as distracters any actual relatives, i.e. candidates r&apos; where the</context>
<context position="18293" citStr="Erk et al. (2010)" startWordPosition="3059" endWordPosition="3062">on 4.3 Metrics We report four evaluation metrics: precision, coverage, recall, and F-score. Precision (called “accuracy” in some papers on selectional preferences) is the percentage of all covered tuples where the original relative r is preferred. Coverage is the percentage of tuples for which the model prefers r to r&apos; or vice versa. Recall is the percentage of all tuples where the original relative is preferred, i.e., precision times coverage. F-score is the harmonic mean of precision and recall. 4.4 Baselines We compare PONG to two baseline methods. EPP is a state-of-the-art model for which Erk et al. (2010) reported better performance than both Resnik’s (1996) WordNet model and Rooth’s (1999) EM clustering model. EPP computes selectional preferences using distributional similarity, based on the assumption that relatives are likely to appear in the same contexts as relatives seen in the training corpus. EPP computes the similarity of a potential relative’s vector space representation to relatives in the training corpus. EPP has various options for its vector space representation, similarity measure, weighting scheme, generalization space, and whether to use PCA. In re-implementing EPP, we chose t</context>
<context position="26313" citStr="Erk et al. (2010)" startWordPosition="4415" endWordPosition="4418">t taxonomy. Rather than use linguistic resources such as WordNet, Rooth et al. (1999) and Wald et al. (2008) induced semantically annotated subcategorization frames from unlabeled corpora. They modeled semantic classes as hidden variables, which they estimated using EM-based clustering. Ritter (2010) computed selectional preferences by using unsupervised topic models such as LinkLDA, which infers semantic classes of words automatically instead of requiring a predefined set of classes as input. The contexts in which a linguistic unit occurs provide information about its meaning. Erk (2007) and Erk et al. (2010) modeled the contexts of a word as the distribution of words that co-occur with it. They calculated the semantic similarity of two words as the similarity of their context distributions according to various measures. Erk et al. (2010) reported the state-ofthe-art method we used as our EPP baseline. In contrast to prior work that explored various solutions to the generalization problem, we don’t so much solve this problem as circumvent it. Instead of generalizing from a training corpus directly to unseen words, PONG abstracts a word N-gram to a POS N-gram and maps it to the relations that the w</context>
<context position="30119" citStr="Erk et al., 2010" startWordPosition="4994" endWordPosition="4997">icted by their unigram frequencies. However, their method did not distinguish among different relations. 6 Conclusion This paper describes, derives, and evaluates PONG, a novel probabilistic model of selectional preferences. PONG uses a labeled corpus to map POS N-grams to grammatical relations. It combines this mapping with probabilities estimated from a much larger POS-tagged but unlabeled Google N-grams corpus. We tested PONG on the eight most common relations in the WSJ corpus, and their inverses – more relations than evaluated in prior work. Compared to the state-of-the-art EPP baseline (Erk et al., 2010), PONG averaged higher precision but lower coverage and recall. Compared to the DEP baseline, PONG averaged lower precision but higher coverage and recall. All these differences were substantial (p &lt; 0.001). Compared to both baselines, PONG’s average Fscore was higher, though not significantly. Some directions for future work include: First, improve PONG by incorporating models of lexical similarity explored in prior work. Second, use the universal tag set to extend PONG to other languages, or to perform better in English. Third, in place of grammatical relations, use rich, diverse semantic ro</context>
</contexts>
<marker>Erk, Padó, Padó, 2010</marker>
<rawString>Erk, K., Padó, S. and Padó, U. 2010. A Flexible, Corpus-Driven Model of Regular and Inverse Selectional Preferences. Computational Linguistics 36(4), 723-763.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fano</author>
</authors>
<title>Transmission O F Information: A Statistical Theory of Communications.</title>
<date>1961</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="29427" citStr="Fano, 1961" startWordPosition="4891" endWordPosition="4892">o compute selectional preferences, whether the words are in the training corpus or not, PONG applies these abstract mappings to word N-grams in the much larger Google N-grams corpus. Some prior work on selectional preferences has used POS N-grams and a large unlabeled corpus. The most closely related work we found was by Gormley et al. (2011). They used patterns in POS N-grams to generate test data for their selectional preferences model, but not to infer preferences. Zhou et al. (2011) identified selectional preferences of one word for another 384 by using Pointwise Mutual Information (PMI) (Fano, 1961) to check whether they co-occur more frequently in a large corpus than predicted by their unigram frequencies. However, their method did not distinguish among different relations. 6 Conclusion This paper describes, derives, and evaluates PONG, a novel probabilistic model of selectional preferences. PONG uses a labeled corpus to map POS N-grams to grammatical relations. It combines this mapping with probabilities estimated from a much larger POS-tagged but unlabeled Google N-grams corpus. We tested PONG on the eight most common relations in the WSJ corpus, and their inverses – more relations th</context>
</contexts>
<marker>Fano, 1961</marker>
<rawString>Fano, R. 1961. Transmission O F Information: A Statistical Theory of Communications. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Franz</author>
<author>T Brants</author>
</authors>
<date>2006</date>
<journal>All Our N-Gram Are Belong</journal>
<note>to You.</note>
<contexts>
<context position="7471" citStr="Franz and Brants, 2006" startWordPosition="1132" endWordPosition="1135">onal preferences, PONG combines information from a limited corpus labeled with the grammatical dependencies described in Section 2, and a much larger unlabeled corpus. The key idea is to abstract word sequences labeled with grammatical relations into POS N-grams, in order to learn a mapping from POS N-grams to those relations. For instance, PONG abstracts the parsed sentence Pat opened the door as NN VB DT NN, with the first and last NN as the subject and object of the VB. To estimate the distribution of POS N-grams containing particular target and relative words, PONG POS-tags Google Ngrams (Franz and Brants, 2006). Section 3.1 derives PONG’s probabilistic model for combining information from labeled and unlabeled corpora. Section 3.2 and Section 3.3 describe how PONG estimates probabilities from each corpus. Section 3.4 discusses a sparseness problem revealed during probability estimation, and how we address it in PONG. 3.1 Probabilistic model We quantify the selectional preference for a relative r to instantiate a relation R of a target t as the probability Pr(r |t, R), estimated as follows. By the definition of conditional probability: 378 We care only about the relative probability of different r fo</context>
<context position="12034" citStr="Franz and Brants, 2006" startWordPosition="1952" endWordPosition="1955"> Given a target and relative, we need to estimate their distribution of POS N-grams and positions. We use the chain rule: Pr(R I r, t) Pr(r I t) Pr(t) and notice that t is held constant: Pr(R I r, t) Pr(r I t) We estimate the second factor as follows: p 379 Figure 1: Overview of PONG. From the labeled corpus, PONG extracts abstract mappings from POS N-grams to relations. From the unlabeled corpus, PONG estimates POS N-gram probability given a target and relative. freq(g s.t. g t g r) i j A labeled corpus is too sparse for this purpose, so we use the much larger unlabeled Google Ngrams corpus (Franz and Brants, 2006). The probability that an N-gram with target t at position i and relative r at position j will have the POS N-gram p is: Pr(p |t gi , r gj ) freq(g s.t.POS(g) p,gi t,gj r)) To compute this ratio, we first use a wellindexed table to efficiently retrieve all N-grams with words t and r at the specified positions. We then obtain their POS N-grams from the Stanford POS tagger (Toutanova et al., 2003), and count how many of them have the POS N-gram p. 3.4 Reducing POS I-gram sparseness We abstract word N-grams into POS N-grams to address the sparseness of the labeled corpus, but even the POS N-grams</context>
</contexts>
<marker>Franz, Brants, 2006</marker>
<rawString>Franz, A. and Brants, T. 2006. All Our N-Gram Are Belong to You.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
<author>D Yarowsky</author>
</authors>
<title>Work on Statistical Methods for Word Sense Disambiguation.</title>
<date>1992</date>
<booktitle>In Proceedings of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language,</booktitle>
<pages>54--60</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="14267" citStr="Gale et al., 1992" startWordPosition="2348" endWordPosition="2351">ially for other languages. Its smaller size (12 tags vs. our 23) should reduce data sparseness, but increase the risk of overgeneralization. 4 Evaluation To evaluate PONG, we use a standard pseudodisambiguation task, detailed in Section 4.1. Section 4.2 describes our test set. Section 4.3 lists the metrics we evaluate on this test set. Section 4.4 describes the baselines we compare PONG against on these metrics, and Section 4.5 describes the relations we compare them on. Section 4.6 reports our results. Section 4.7 analyzes sources of error. 4.1 Evaluation task The pseudo-disambiguation task (Gale et al., 1992; Schutze, 1992) is as follows: given a target word t, a relation R, a relative r, and a random distracter r&apos;, prefer either r or r&apos;, whichever is likelier to have relation R to word t. This evaluation does not use a threshold: just prefer whichever word is likelier according to the model being evaluated. If the model assigns only one of the words a probability, prefer it, based on the assumption that the unknown probability of the other word is lower. If the model assigns the same probability to both words, or no probability to either word, do not prefer either word. 380 4.2 Test set As a sou</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, W.A., Church, K.W. and Yarowsky, D. 1992. Work on Statistical Methods for Word Sense Disambiguation. In Proceedings of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, Cambridge, MA, October 23–25, 1992, 54-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<volume>28</volume>
<issue>3</issue>
<pages>245--288</pages>
<contexts>
<context position="2391" citStr="Gildea and Jurafsky, 2002" startWordPosition="369" endWordPosition="372">atical dependency such as the object of a verb. We call t the “target” because originally it referred to a vocabulary word targeted for instruction, and r its “relative.” Notation Description R a relation between words t a target word r, r&apos; possible relatives of t g a word N-gram gi and gj ith and jth words of g p the POS N-gram of g Table 1: Notation used throughout this paper Previous work on selectional preferences has used them primarily for natural language analytic tasks such as word sense disambiguation (Resnik, 1997), dependency parsing (Zhou et al., 2011), and semantic role labeling (Gildea and Jurafsky, 2002). However, selectional preferences can also apply to natural language generation tasks such as sentence generation and question generation. For generation tasks, choosing the right word to express a specified argument of a relation requires knowing its connotations – that is, its selectional preferences. Therefore, it is useful to know selectional preferences for many different relations. Such knowledge could have many uses. In education, they could help teach word connotations. In machine learning they could help computers learn languages. In machine translation, they could help generate more</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, D. and Jurafsky, D. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics 28(3), 245-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Gormley</author>
<author>M Dredze</author>
<author>B V Durme</author>
<author>J Eisner</author>
</authors>
<title>Shared Components Topic Models with Application to Selectional Preference,</title>
<date>2011</date>
<booktitle>NIPS Workshop on Learning Semantics Sierra</booktitle>
<location>Nevada,</location>
<contexts>
<context position="29160" citStr="Gormley et al. (2011)" startWordPosition="4848" endWordPosition="4851"> Combine both dependencies in a distribution for distribution for POS N-gram parsed corpus, relations in target and relative distributions and their inverse parsed WSJ in Google N-gram relations corpus Table 6: Comparison with prior methods to compute selectional preferences To compute selectional preferences, whether the words are in the training corpus or not, PONG applies these abstract mappings to word N-grams in the much larger Google N-grams corpus. Some prior work on selectional preferences has used POS N-grams and a large unlabeled corpus. The most closely related work we found was by Gormley et al. (2011). They used patterns in POS N-grams to generate test data for their selectional preferences model, but not to infer preferences. Zhou et al. (2011) identified selectional preferences of one word for another 384 by using Pointwise Mutual Information (PMI) (Fano, 1961) to check whether they co-occur more frequently in a large corpus than predicted by their unigram frequencies. However, their method did not distinguish among different relations. 6 Conclusion This paper describes, derives, and evaluates PONG, a novel probabilistic model of selectional preferences. PONG uses a labeled corpus to map</context>
</contexts>
<marker>Gormley, Dredze, Durme, Eisner, 2011</marker>
<rawString>Gormley, M.R., Dredze, M., Durme, B.V. and Eisner, J. 2011. Shared Components Topic Models with Application to Selectional Preference, NIPS Workshop on Learning Semantics Sierra Nevada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>im Walde</author>
<author>S S Hying</author>
<author>C Scheible</author>
<author>C</author>
<author>H Schmid</author>
</authors>
<title>Combining Em Training and the Mdl Principle for an Automatic Verb Classification Incorporating Selectional Preferences.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>496--504</pages>
<location>Columbus, OH,</location>
<marker>Walde, Hying, Scheible, C, Schmid, 2008</marker>
<rawString>im Walde, S.S., Hying, C., Scheible, C. and Schmid, H. 2008. Combining Em Training and the Mdl Principle for an Automatic Verb Classification Incorporating Selectional Preferences. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, Columbus, OH, 2008, 496-504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sapporo, Japan,</location>
<contexts>
<context position="10999" citStr="Klein and Manning, 2003" startWordPosition="1759" endWordPosition="1762">S N-grams like DT JJ:red NN:hat VB NN among Google Ngrams with t and r in the specified positions. Sections 3.2 and 3.3 describe how we estimate Pr(R |p, i, j) and Pr(p |t = gi, r = gj), respectively. Note that PONG estimates relative rather than absolute probabilities. Therefore it cannot (and does not) compare them against a fixed threshold to make decisions about selectional preferences. 3.2 Mapping POS I-grams to relations To estimate Pr(R |p, i, j), we use the Penn Treebank Wall Street Journal (WSJ) corpus, which is labeled with grammatical relations using the Stanford dependency parser (Klein and Manning, 2003). To estimate the probability Pr(R |p, i, j) of a relation R between a target at position i and a relative at position j in a POS N-gram p, we compute what fraction of the word N-grams g with POS N-gram p have relation R between some target t and relative r at positions i and j: Pr(R |p, i, j) freq(g s.t.POS(g) p relation(gi, gj) R) freq(g s.t.POS(g) p relation(gi, gj)) 3.3 Estimating POS I-gram distributions Given a target and relative, we need to estimate their distribution of POS N-grams and positions. We use the chain rule: Pr(R I r, t) Pr(r I t) Pr(t) and notice that t is held constant: P</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, D. and Manning, C.D. 2003. Accurate Unlexicalized Parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, Sapporo, Japan, July 7-12, 2003, E.W. HINRICHS and D. ROTH, Eds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Das</author>
<author>R T McDonald</author>
</authors>
<title>A Universal Part-of-Speech Tagset. ArXiv 1104.2086.</title>
<date>2011</date>
<contexts>
<context position="13581" citStr="Petrov et al., 2011" startWordPosition="2238" endWordPosition="2241"> JJ, JJR, JJS ADVERB RB, RBR, RBS NOUN NN, NNS, NNP, NNPS VERB VB, VBD, VBG, VBN, VBP, VBZ Table 2: Coarser POS tag set used in PONG To gauge the impact of the coarser POS tags, we calculated Pr(r |t, R) for 76 test instances used in an earlier unpublished study by Liu Liu, a former Project LISTEN graduate student. Each instance consists of two randomly chosen words in the WSJ corpus labeled with a grammatical relation. Coarse POS tags increased coverage of this pilot set – that is, the fraction of instances for which PONG computes a probability – from 69% to 92%. Using the universal tag set (Petrov et al., 2011) as an even coarser tag set is an interesting future direction, especially for other languages. Its smaller size (12 tags vs. our 23) should reduce data sparseness, but increase the risk of overgeneralization. 4 Evaluation To evaluate PONG, we use a standard pseudodisambiguation task, detailed in Section 4.1. Section 4.2 describes our test set. Section 4.3 lists the metrics we evaluate on this test set. Section 4.4 describes the baselines we compare PONG against on these metrics, and Section 4.5 describes the relations we compare them on. Section 4.6 reports our results. Section 4.7 analyzes s</context>
</contexts>
<marker>Petrov, Das, McDonald, 2011</marker>
<rawString>Petrov, S., Das, D. and McDonald, R.T. 2011. A Universal Part-of-Speech Tagset. ArXiv 1104.2086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selectional Constraints: An Information-Theoretic Model and Its Computational Realization.</title>
<date>1996</date>
<journal>Cognition</journal>
<volume>61</volume>
<pages>127--159</pages>
<contexts>
<context position="25276" citStr="Resnik (1996)" startWordPosition="4259" endWordPosition="4260">-grams. Using a smaller tag set may reduce the sparse data problem but increase the risk of over-generalization. 5 Relation to Prior Work In predicting selectional preferences, a key issue is generalization. Our DEP baseline simply counts co-occurrences of target and relative words in a corpus to predict selectional preferences, but only for words seen in the corpus. Prior work, summarized in Table 6, has therefore tried to infer the similarity of unseen relatives to seen relatives. To illustrate, consider the problem of inducing that the direct objects of celebrate tend to be days or events. Resnik (1996) combined WordNet with a labeled corpus to model the probability that relatives of a predicate belong to a particular conceptual class. This method could notice, for example, that the direct objects of celebrate tend to belong to the conceptual class event. Thus it could prefer anniversary or occasion as the object of celebrate even if unseen in its training corpus. However, this method depends strongly on the WordNet taxonomy. Rather than use linguistic resources such as WordNet, Rooth et al. (1999) and Wald et al. (2008) induced semantically annotated subcategorization frames from unlabeled </context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>Resnik, P. 1996. Selectional Constraints: An Information-Theoretic Model and Its Computational Realization. Cognition 61, 127-159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selectional Preference and Sense Disambiguation.</title>
<date>1997</date>
<booktitle>In ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How,</booktitle>
<pages>52--57</pages>
<location>Washington, DC,</location>
<contexts>
<context position="2295" citStr="Resnik, 1997" startWordPosition="357" endWordPosition="358">on between them, whether a semantic role such as the agent of an action, or a grammatical dependency such as the object of a verb. We call t the “target” because originally it referred to a vocabulary word targeted for instruction, and r its “relative.” Notation Description R a relation between words t a target word r, r&apos; possible relatives of t g a word N-gram gi and gj ith and jth words of g p the POS N-gram of g Table 1: Notation used throughout this paper Previous work on selectional preferences has used them primarily for natural language analytic tasks such as word sense disambiguation (Resnik, 1997), dependency parsing (Zhou et al., 2011), and semantic role labeling (Gildea and Jurafsky, 2002). However, selectional preferences can also apply to natural language generation tasks such as sentence generation and question generation. For generation tasks, choosing the right word to express a specified argument of a relation requires knowing its connotations – that is, its selectional preferences. Therefore, it is useful to know selectional preferences for many different relations. Such knowledge could have many uses. In education, they could help teach word connotations. In machine learning </context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Resnik, P. 1997. Selectional Preference and Sense Disambiguation. In ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How, Washington, DC, April 4-5, 1997, 52-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>Mausam</author>
<author>O Etzioni</author>
</authors>
<title>A Latent Dirichlet Allocation Method for Selectional Preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>424--434</pages>
<location>Uppsala,</location>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Ritter, A., Mausam and Etzioni, O. 2010. A Latent Dirichlet Allocation Method for Selectional Preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, 2010, 424-434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rooth</author>
<author>S Riezler</author>
<author>D Prescher</author>
<author>G Carroll</author>
<author>F Beil</author>
</authors>
<title>Inducing a Semantically Annotated Lexicon Via Em-Based Clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, College Park, MD, 1999, Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="15359" citStr="Rooth et al. (1999)" startWordPosition="2546" endWordPosition="2549">igns the same probability to both words, or no probability to either word, do not prefer either word. 380 4.2 Test set As a source of evaluation data, we used the British National Corpus (BNC). As a common test corpus for all the methods we evaluated, we selected one half of BNC by sorting filenames alphabetically and using the odd-numbered files. We used the other half of BNC as a training corpus for the baseline methods we compared PONG to. A test set for the pseudo-disambiguation task task consists of tuples of the form (R, t, r, r&apos;). To construct a test set, we adapted the process used by Rooth et al. (1999) and Erk et al. (2010). First, we chose 100 (R, t) pairs for each relation R at random from the test corpus. Rooth et al. (1999) and Erk et al. (2010) chose such pairs from a training corpus to ensure that it contained the target t. In contrast, choosing pairs from an unseen test corpus includes target words whether or not they occur in the training corpus. To obtain a sample stratified by frequency, rather than skewed heavily toward highfrequency pairs, Erk et al. (2010) drew (R, t) pairs from each of five frequency bands in the entire British National Corpus (BNC): 50-100 occurrences; 101-20</context>
<context position="16950" citStr="Rooth et al. (1999)" startWordPosition="2831" endWordPosition="2834">bands: 1-25 occurrences; 26-50; 51-100; 101-250; 251- 500; and more than 500. We necessarily omitted frequency bands that contained no such triples. We filtered out triples where r did not have the most frequent part of speech for the relation R. For example, this filter would exclude the triple (dobj, celebrate, the) because a direct object is most frequently a noun, but the is a determiner. Then, like Erk et al. (2010), we paired the relative r in each (R, t, r) triple with a distracter r&apos; with the same (most frequent) part of speech as the relative r, yielding the test tuple (R, t, r, r&apos;). Rooth et al. (1999) restricted distracter candidates to words with between 30 and 3,000 occurrences in BNC; accordingly, we chose only distracters with between 15 and 1,500 occurrences in our test corpus. We selected r&apos; from these candidates randomly, with probability proportional to their frequency in the test corpus. Like Rooth et al. (1999), we excluded as distracters any actual relatives, i.e. candidates r&apos; where the test corpus contained the triple (R, t, r&apos;). Table 3 shows the resulting number of (R, t, r, r&apos;) test tuples for each relation. Relation R # tuples for R # tuples for RT advmod 121 131 amod 162 </context>
<context position="25781" citStr="Rooth et al. (1999)" startWordPosition="4338" endWordPosition="4341">te, consider the problem of inducing that the direct objects of celebrate tend to be days or events. Resnik (1996) combined WordNet with a labeled corpus to model the probability that relatives of a predicate belong to a particular conceptual class. This method could notice, for example, that the direct objects of celebrate tend to belong to the conceptual class event. Thus it could prefer anniversary or occasion as the object of celebrate even if unseen in its training corpus. However, this method depends strongly on the WordNet taxonomy. Rather than use linguistic resources such as WordNet, Rooth et al. (1999) and Wald et al. (2008) induced semantically annotated subcategorization frames from unlabeled corpora. They modeled semantic classes as hidden variables, which they estimated using EM-based clustering. Ritter (2010) computed selectional preferences by using unsupervised topic models such as LinkLDA, which infers semantic classes of words automatically instead of requiring a predefined set of classes as input. The contexts in which a linguistic unit occurs provide information about its meaning. Erk (2007) and Erk et al. (2010) modeled the contexts of a word as the distribution of words that co</context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>Rooth, M., Riezler, S., Prescher, D., Carroll, G. and Beil, F. 1999. Inducing a Semantically Annotated Lexicon Via Em-Based Clustering. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, College Park, MD, 1999, Association for Computational Linguistics, 104-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schutze</author>
</authors>
<title>Context Space.</title>
<date>1992</date>
<booktitle>In Proceedings of the AAAI Fall Symposium on Intelligent Probabilistic Approaches to Natural Language,</booktitle>
<pages>113--120</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="14283" citStr="Schutze, 1992" startWordPosition="2352" endWordPosition="2353">guages. Its smaller size (12 tags vs. our 23) should reduce data sparseness, but increase the risk of overgeneralization. 4 Evaluation To evaluate PONG, we use a standard pseudodisambiguation task, detailed in Section 4.1. Section 4.2 describes our test set. Section 4.3 lists the metrics we evaluate on this test set. Section 4.4 describes the baselines we compare PONG against on these metrics, and Section 4.5 describes the relations we compare them on. Section 4.6 reports our results. Section 4.7 analyzes sources of error. 4.1 Evaluation task The pseudo-disambiguation task (Gale et al., 1992; Schutze, 1992) is as follows: given a target word t, a relation R, a relative r, and a random distracter r&apos;, prefer either r or r&apos;, whichever is likelier to have relation R to word t. This evaluation does not use a threshold: just prefer whichever word is likelier according to the model being evaluated. If the model assigns only one of the words a probability, prefer it, based on the assumption that the unknown probability of the other word is lower. If the model assigns the same probability to both words, or no probability to either word, do not prefer either word. 380 4.2 Test set As a source of evaluatio</context>
</contexts>
<marker>Schutze, 1992</marker>
<rawString>Schutze, H. 1992. Context Space. In Proceedings of the AAAI Fall Symposium on Intelligent Probabilistic Approaches to Natural Language, Cambridge, MA, 1992, 113-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>252--259</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="12432" citStr="Toutanova et al., 2003" startWordPosition="2028" endWordPosition="2031">s, PONG estimates POS N-gram probability given a target and relative. freq(g s.t. g t g r) i j A labeled corpus is too sparse for this purpose, so we use the much larger unlabeled Google Ngrams corpus (Franz and Brants, 2006). The probability that an N-gram with target t at position i and relative r at position j will have the POS N-gram p is: Pr(p |t gi , r gj ) freq(g s.t.POS(g) p,gi t,gj r)) To compute this ratio, we first use a wellindexed table to efficiently retrieve all N-grams with words t and r at the specified positions. We then obtain their POS N-grams from the Stanford POS tagger (Toutanova et al., 2003), and count how many of them have the POS N-gram p. 3.4 Reducing POS I-gram sparseness We abstract word N-grams into POS N-grams to address the sparseness of the labeled corpus, but even the POS N-grams can be sparse. For n=5, the rarer ones occur too sparsely (if at all) in our labeled corpus to estimate their frequency. To address this issue, we use a coarser POS tag set than the Penn Treebank POS tag set. As Table 2 shows, we merge tags for adjectives, nouns, adverbs, and verbs into four coarser tags. Coarse Original ADJ JJ, JJR, JJS ADVERB RB, RBR, RBS NOUN NN, NNS, NNP, NNPS VERB VB, VBD,</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Toutanova, K., Klein, D., Manning, C. and Singer, Y. 2003. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of the Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), Edmonton, Canada, 2003, 252– 259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Zhao</author>
<author>K Liu</author>
<author>L Cai</author>
</authors>
<title>Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1556--1565</pages>
<location>Portland, OR,</location>
<contexts>
<context position="2335" citStr="Zhou et al., 2011" startWordPosition="361" endWordPosition="364"> role such as the agent of an action, or a grammatical dependency such as the object of a verb. We call t the “target” because originally it referred to a vocabulary word targeted for instruction, and r its “relative.” Notation Description R a relation between words t a target word r, r&apos; possible relatives of t g a word N-gram gi and gj ith and jth words of g p the POS N-gram of g Table 1: Notation used throughout this paper Previous work on selectional preferences has used them primarily for natural language analytic tasks such as word sense disambiguation (Resnik, 1997), dependency parsing (Zhou et al., 2011), and semantic role labeling (Gildea and Jurafsky, 2002). However, selectional preferences can also apply to natural language generation tasks such as sentence generation and question generation. For generation tasks, choosing the right word to express a specified argument of a relation requires knowing its connotations – that is, its selectional preferences. Therefore, it is useful to know selectional preferences for many different relations. Such knowledge could have many uses. In education, they could help teach word connotations. In machine learning they could help computers learn language</context>
<context position="29307" citStr="Zhou et al. (2011)" startWordPosition="4872" endWordPosition="4875">nverse parsed WSJ in Google N-gram relations corpus Table 6: Comparison with prior methods to compute selectional preferences To compute selectional preferences, whether the words are in the training corpus or not, PONG applies these abstract mappings to word N-grams in the much larger Google N-grams corpus. Some prior work on selectional preferences has used POS N-grams and a large unlabeled corpus. The most closely related work we found was by Gormley et al. (2011). They used patterns in POS N-grams to generate test data for their selectional preferences model, but not to infer preferences. Zhou et al. (2011) identified selectional preferences of one word for another 384 by using Pointwise Mutual Information (PMI) (Fano, 1961) to check whether they co-occur more frequently in a large corpus than predicted by their unigram frequencies. However, their method did not distinguish among different relations. 6 Conclusion This paper describes, derives, and evaluates PONG, a novel probabilistic model of selectional preferences. PONG uses a labeled corpus to map POS N-grams to grammatical relations. It combines this mapping with probabilities estimated from a much larger POS-tagged but unlabeled Google N-g</context>
</contexts>
<marker>Zhou, Zhao, Liu, Cai, 2011</marker>
<rawString>Zhou, G., Zhao, J., Liu, K. and Cai, L. 2011. Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, Portland, OR, 2011, 1556–1565.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>