<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.998581">
A MARKOV LANGUAGE LEARNING MODEL
FOR FINITE PARAMETER SPACES
</title>
<author confidence="0.988232">
Partha Niyogi and Robert C. Berwick
</author>
<affiliation confidence="0.9847875">
Center for Biological and Computational Learning
Massachusetts Institute of Technology
</affiliation>
<address confidence="0.8270705">
E25-201
Cambridge, MA 02139, USA
</address>
<email confidence="0.993374">
Internet: pn©ai.mit.edu, berwick@ai.mit.edu
</email>
<sectionHeader confidence="0.996583" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999778222222222">
This paper shows how to formally characterize lan-
guage learning in a finite parameter space as a Markov
structure. Important new language learning results fol-
low directly: explicitly calculated sample complexity
learning times under different input distribution as-
stunptions (including CHILDES database language in-
put) and learning regimes. We also briefly describe a
new way to formally model (rapid) diachronic syntax
change.
</bodyText>
<sectionHeader confidence="0.999014666666667" genericHeader="keywords">
BACKGROUND MOTIVATION:
TRIGGERS AND LANGUAGE
ACQUISITION
</sectionHeader>
<bodyText confidence="0.9999301">
Recently, several researchers, including Gibson and
Wexler (1994), henceforth GW, Dresher and Kaye
(1990); and Clark and Roberts (1993) have modeled
language learning in a (finite) space whose grammars
are characterized by a finite number of parameters or n-
length Boolean-valued vectors. Many current linguistic
theories now employ such parametric models explicitly
or in spirit, including Lexical-Functional Grammar and
versions of HPSG, besides GB variants.
With all such models, key questions about sample
complexity, convergence time, and alternative model-
ing assumptions are difficult to assess without a pre-
cise mathematical formalization. Previous research has
usually addressed only the question of convergence in
the limit without probing the equally important ques-
tion of sample complexity: it is of not much use that a
learner can acquire a language if sample complexity is
extraordinarily high, hence psychologically implausible.
This remains a relatively undeveloped area of language
learning theory. The current paper aims to fill that
gap. We choose as a starting point the GW Triggering
Learning Algorithm (TLA). Our central result is that
the performance of this algorithm and others like it is
completely modeled by a Markov chain. We explore
the basic computational consequences of this, including
some surprising results about sample complexity and
convergence time, the dominance of random walk over
gradient ascent, and the applicability of these results to
actual child language acquisition and possibly language
change.
Background. Following Gold (1967) the basic frame-
work is that of identification in the limit. We assume
some familiarity with Gold&apos;s assumptions. The learner
receives an (infinite) sequence of (positive) example sen-
tences from some target language. After each, the
learner either (i) stays in the same state; or (ii) moves
to a new state (change its parameter settings). If after
some finite number of examples the learner converges to
the correct target language and never changes its guess,
then it has correctly identified the target language in
the limit; otherwise, it fails.
In the GW model (and others) the learner obeys two
additional fundamental constraints: (1) the single-value
constraint—the learner can change only 1 parameter
value each step; and (2) the greediness constraint—if
the learner is given a positive example it cannot recog-
nize and changes one parameter value, finding that it
can accept the example, then the learner retains that
new value. The TLA essentially simulates this; see Gib-
son and Wexler (1994) for details.
</bodyText>
<sectionHeader confidence="0.916289" genericHeader="method">
THE MARKOV FORMULATION
</sectionHeader>
<bodyText confidence="0.999750666666666">
Previous parameter models leave open key questions ad-
dressable by a more precise formalization as a Markov
chain. The correspondence is direct. Each point i in the
Markov space is a possible parameter setting. Transi-
tions between states stand for probabilities b that the
learner will move from hypothesis state i to state j.
As we show below, given a distribution over L(G), we
can calculate the actual b&apos;s themselves. Thus, we can
picture the TLA learning space as a directed, labeled
graph V with 2&apos; vertices. See figure 1 for an example in
a 3-parameter systern.1 We can now use Markov theory
to describe TLA parameter spaces, as in Isaacson and
</bodyText>
<footnote confidence="0.9992214">
1GW construct an identical transition diagram in the
description of their computer program for calculating lo-
cal maxima. However, this diagram is not explicitly pre-
sented as a Markov structure and does not include transition
probabilities.
</footnote>
<page confidence="0.997852">
171
</page>
<bodyText confidence="0.94363325">
Madsen (1976). By the single value hypothesis, the sys-
tem can only move 1 Hamming bit at a time, either to-
ward the target language or 1 bit away. Surface strings
can force the learner from one hypothesis state to an-
other. For instance, if state i corresponds to a gram-
mar that generates a language that is a proper subset
of another grammar hypothesis j, there can never be
a transition from j to i, and there must be one from i
to j. Once we reach the target grammar there is noth-
ing that can move the learner from this state, since all
remaining positive evidence will not cause the learner
to change its hypothesis: an Absorbing State (AS)
in the Markov literature. Clearly, one can conclude at
once the following important learnability result:
Theorem 1 Given a Markov chain C corresponding to
a GW TLA learner, 3 exactly 1 AS (corresponding to
the target grammar/language) if C is learnable.
Proof. By assumption, C is learnable. Now assume
for sake of contradiction that there is not exactly one
AS. Then there must be either 0 AS or &gt; 1 AS. In the
first case, by the definition of an absorbing state, there
is no hypothesis in which the learner will remain for-
ever. Therefore C is not learnable, a contradiction. In
the second case, without loss of generality, assume there
are exactly two absorbing states, the first S correspond-
ing to the target parameter setting, and the second S&apos;
corresponding to some other setting. By the definition
of an absorbing state, in the limit C will with some
nonzero probability enter S&apos;, and never exit S&apos;. Then
C is not learnable, a contradiction. Hence our assump-
tion that there is not exactly 1 AS must be false.
Assume that there exists exactly 1 AS i in the
Markov chain M. Then, by the definition of an absorb-
ing state, after some number of steps n, no matter what
the starting state, M will end up in state i, correspond-
ing to the target grammar. 1
Corollary 0.1 Given a Markov chain corresponding to
a (finite) family of grammars in a GW learning sys-
tem, if there exist 2 or more AS, then that family is not
learnable.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="method">
DERIVATION OF TRANSITION
PROBABILITIES FOR THE
MARKOV TLA STRUCTURE
</sectionHeader>
<bodyText confidence="0.998500555555556">
We now derive the transition probabilities for the
Markov TLA structure, the key to establishing sam-
ple complexity results. Let the target language Lt be
L, = {si, s2, s3, ...} and Pa probability distribution on
these strings. Suppose the learner is in a state corre-
sponding to language L. With probability P(s), it
receives a string si. There are two cases given current
parameter settings.
Case I. The learner can syntactically analyze the re-
ceived string si. Then parameter values are unchanged.
This is so only when si E Ls. The probability of re-
maining in the state s is P(s).
Case II. The learner cannot syntactically analyze
the string. Then si t% L3; the learner is in state s,
and has n neighboring states (Hamming distance of 1).
The learner picks one of these uniformly at random. If
ni of these neighboring states correspond to languages
which contain si and the learner picks any one of them
(with probability n3/n), it stays in that state. If the
learner picks any of the other states (with probability
(n — n2)/n) then it remains in state s. Note that n2
could take values between 0 and n. Thus the probability
that the learner remains in states is P(s2)((n— ni)/n).
The probability of moving to each of the other ni states
is P(si)(niln).
The probability that the learner will remain in its
original state s is the sum of the probabilities of these
</bodyText>
<equation confidence="0.7279235">
two cases:P•)+
Es3EL.(s 3 ,--.33a.(1— 11, iin)P(si)•
</equation>
<bodyText confidence="0.9724596">
To compute the transition probability from s to
k, note that this transition will occur with proba-
bility 1/n for all the strings 53 E Lk but not in
L3. These strings occur with probability P(s) each
and so the transition probability is:P[s k]
</bodyText>
<equation confidence="0.853244833333333">
Es,EL,,,s,e&apos;L.,3,ELk(110P(sj)•
Summing over all strings .92 E (Ltfl Lk)\Ls (set dif-
ference) it is easy to see that si E (Ltfl Lk)\L, &lt;=&gt; si E
(Lt n Lk)\(Lt n L3). Rewriting, we have P[s le]
Now we can compute
Es,E(LinLk)\(LtoL,
</equation>
<bodyText confidence="0.997599">
the transition probabilities between any two states.
Thus the self-transition probability can be given as,
</bodyText>
<equation confidence="0.615479">
P[s = 1—is a neighboring state of s P[s k].
Example.
</equation>
<bodyText confidence="0.999304333333333">
Consider the 3-parameter natural language system de-
scribed by Gibson and Wexler (1994), designed to cover
basic word orders (X-bar structures) plus the verb-
second phenomena of Germanic languages. Its binary
parameters are: (1) Spec(ifier) initial (0) or final (1);
(2) Compl(ement) initial (0) or final (1); and Verb Sec-
ond (V2) does not exist (0) or does exist (1). Possi-
ble &amp;quot;words&amp;quot; in this language include S(ubject), V(erb),
O(bject), D(irect) O(bject), Adv(erb) phrase, and so
forth. Given these alternatives, Gibson and Wexler
(1994) show that there are 12 possible surface strings
for each (—V2) grammar and 18 possible surface strings
for each (+V2) grammar, restricted to unembedded or
&amp;quot;degree-0&amp;quot; examples for reasons of psychological plau-
sibility (see Gibson and Wexler for discussion). For in-
stance, the parameter setting [0 I 01, Specifier initial,
Complement final, and —V2, works out to the possi-
ble basic English surface phrase order of Subject—Verb—
Object (SVO).
As in figure 1 below, suppose the SVO (&amp;quot;English&amp;quot;,
setting #5,[0 1 0]) is the target grammar. The figure&apos;s
shaded rings represent increasing Hamming distances
from the target. Each labeled circle is a Markov state.
Surrounding the bulls-eye target are the 3 other param-
eter arrays that differ from [0 1 0] by one binary digit:
e.g., [0, 0, 0], or Spec-first, Comp-first, —V2, basic order
SOV or &amp;quot;Japanese&amp;quot;.
</bodyText>
<page confidence="0.99492">
172
</page>
<figureCaption confidence="0.981492">
Figure 1: The 8 parameter settings in the GW example, shown as a Markov structure, with transition probabilities
</figureCaption>
<bodyText confidence="0.76662725">
omitted. Directed arrows between circles (states) represent possible nonzero (possible learner) transitions. The target
grammar (in this case, number 5, setting [0 1 0]), lies at dead center. Around it are the three settings that differ
from the target by exactly one binary digit; surrounding those are the 3 hypotheses two binary digits away from the
target; the third ring out contains the single hypothesis that differs from the target by 3 binary digits.
</bodyText>
<figure confidence="0.720559">
sink
1_ 1 11
target:5
ec lst,tomp final, —VgA
0011
</figure>
<page confidence="0.992522">
173
</page>
<bodyText confidence="0.999968451612903">
Plainly there are exactly 2 absorbing states in this
Markov chain. One is the target grammar (by defini-
tion); the other is state 2. State 4 is also a sink that
leads only to state 4 or state 2. GW call these two
nontarget states local maxima because local gradient
ascent will converge to these without reaching the de-
sired target. Hence this system is not learnable. More
importantly though, in addition to these local maxima,
we show (see below) that there are other states (not
detected in GW or described by Clark) from which the
learner will never reach the target with (high) positive
probability. Example: we show that if the learner starts
at hypothesis VOS—V2, then with probability 0.33 in
the limit, the learner will never converge to the SVO
target. Crucially, we must use set differences to build
the Markov figure straightforwardly, as indicated in the
next section. In short, while it is possible to reach &amp;quot;En-
glish&amp;quot;from some source languages like &amp;quot;Japanese,&amp;quot; this
is not possible for other starting points (exactly 4 other
initial states).
It is easy to imagine alternatives to the TLA that
avoid the local maxima problem. As it stands the
learner only changes a parameter setting if that change
allows the learner to analyze the sentence it could not
analyze before. If we relax this condition so that under
unanalyzability the learner picks a random parameter
to change, then the problem with local maxima disap-
pears, because there can be only 1 Absorbing State, the
target grammar. All other states have exit arcs. Thus,
by our main theorem, such a system is learnable. We
discuss other alternatives below.
</bodyText>
<sectionHeader confidence="0.9995965" genericHeader="method">
CONVERGENCE TIMES FOR THE
MARKOV CHAIN MODEL
</sectionHeader>
<bodyText confidence="0.9999332">
Perhaps the most significant advantage of the Markov
chain formulation is that one can calculate the number
of examples needed to acquire a language. Recall it
is not enough to demonstrate convergence in the limit;
learning must also be feasible. This is particularly true
in the case of finite parameter spaces where convergence
might not be as much of a problem as feasibility. Fortu-
nately, given the transition matrix of a Markov chain,
the problem of how long it takes to converge has been
well studied.
</bodyText>
<sectionHeader confidence="0.9997595" genericHeader="method">
SOME TRANSITION MATRICES AND
THEIR CONVERGENCE CURVES
</sectionHeader>
<bodyText confidence="0.999687">
Consider the example in the previous section. The tar-
get grammar is SVO—V2 (grammar #5 in GW). For
simplicity, assume a uniform distribution on L5. Then
the probability of a particular string si in L5 is 1/12 be-
cause there are 12 (degree-0) strings in L5. We directly
compute the transition matrix (0 entries elsewhere):
</bodyText>
<table confidence="0.997326769230769">
L2 L3 L4 L5 L6 L7 L8
Li 2 T 3
L2 1
L3 3 1
4
1
L4 12 12
L5 1
1 5
L6 6
1
18 igg
LE; 12 36 9
</table>
<bodyText confidence="0.988894363636364">
States 2 and 5 are absorbing; thus this chain contains
local maxima. Also, state 4 exits only to either itself
or to state 2, hence is also a local maximum. If T is
the transition probability matrix of a chain, then the
corresponding i, j element of Trn is the probability that
the learner moves from state i to state j in 771 steps.
For learnability to hold irrespective starting state, the
probability of reaching state 5 should approach 1 as in
goes to infinity, i.e., column 5 of T&apos; should contain all
l&apos;s, and O&apos;s elsewhere. Direct computation shows this
to be false:
</bodyText>
<figure confidence="0.506105090909091">
LI. L2 L3 L4 L5 L6 L7 L8
2
3
L2 1
L3 1 2
3 3
L4 1
L5 1
L6 1
L7 1
L8 1
</figure>
<bodyText confidence="0.999777">
We see that if the learner starts out in states 2 or 4,
it will certainly end up in state 2 in the limit. These
two states correspond to local maxima grammars in the
GW framework. We also see that if the learner starts
in states 5 through 8, it will certainly converge in the
limit to the target grammar.
States 1 and 3 are much more interesting, and con-
stitute new results about this parameterization. If the
learner starts in either of these states, it reaches the
target grammar with probability 2/3 and state 2 with
probability 1/3. Thus, local maxima are not the only
problem for parameter space learnability. To our knowl-
edge, GW and other researchers have focused exclu-
sively on local maxima. However, while it is true that
states 2 and 4 will, with probability 1, not converge to
the target grammar, it is also true that states 1 and
3 will not converge to the target, with probability 1/3.
Thus, the number of &amp;quot;bad&amp;quot; initial hypotheses is signif-
icantly larger than realized generally (in fact, 12 out of
56 of the possible source-target grammar pairs in the 3-
parameter system). This difference is again due to the
new probabilistic framework introduced in the current
paper.
</bodyText>
<page confidence="0.99257">
174
</page>
<bodyText confidence="0.9995826">
Figure 2 shows a plot of the quantity p(m)
min{pi(m)} as a function of in, the number of exam-
ples. Here pi denotes the probability of being in state 1
at the end of in examples in the case where the learner
started in state i. Naturally we want
</bodyText>
<equation confidence="0.927896">
liM pi ( nl) 7-- 1
111-■ 00
</equation>
<bodyText confidence="0.999265">
and for this example this is indeed the case. The next
figure shows a plot of the following quantity as a func-
tion of in, the number of examples.
</bodyText>
<equation confidence="0.973941">
p(m) = min{pi(m)}
</equation>
<bodyText confidence="0.999954931034483">
The quantity p(m) is easy to interpret. Thus p(m)
0.95 means that for every initial state of the learner
the probability that it is in the target state after m
examples is at least 0.95. Further there is one initial
state (the worst initial state with respect to the target,
which in our example is L8) for which this probability
is exactly 0.95. We find on looking at the curve that
the learner converges with high probability within 100
to 200 (degree-0) example sentences, a psychologically
plausible number.
We can now compare the convergence time of TLA to
other algorithms. Perhaps the simplest is random walk:
start the learner at a random point in the 3-parameter
space, and then, if an input sentence cannot be ana-
lyzed, move 1-bit randomly from state to state. Note
that this regime cannot suffer from the local maxima
problem, since there is always some finite probability of
exiting a non-target state.
Computing the convergence curves for a random walk
algorithm (RWA) on the 8 state space, we find that the
convergence times are actually faster than for the TLA;
see figure 2. Since the RWA is also superior in that it
does not suffer from the same local maxima problem
as TLA, the conceptual support for the TLA is by no
means clear. Of course, it may be that the TLA has
empirical support, in the sense of independent evidence
that children do use this procedure (given by the pat-
tern of their errors, etc.), but this evidence is lacking,
as far as we know.
</bodyText>
<sectionHeader confidence="0.9924245" genericHeader="method">
DISTRIBUTIONAL ASSUMPTIONS:
PART I
</sectionHeader>
<bodyText confidence="0.971675230769231">
In the earlier section we assumed that the data was uni-
formly distributed. We computed the transition matrix
for a particular target language and showed that con-
vergence times were of the order of 100-200 samples. In
this section we show that the convergence times depend
crucially upon the distribution. In particular we can
choose a distribution which will make the convergence
time as large as we want. Thus the distribution-free
convergence time for the 3-parameter system is infinite.
As before, we consider the situation where the target
language is L1. There are no local maxima problems
for this choice. We begin by letting the distribution be
parametrized by the variables a, b, c, d where
</bodyText>
<equation confidence="0.9952594">
a = P (A = {Adv(erb)Phrase V S})
b = P(B = {Adv V 0 5, Adv Aux V S})
c = P(C = {Adv V 01 02 5, Adv Aux V 0 S,
Adv Aux V 01 02 S})
d = P(D = {V S})
</equation>
<bodyText confidence="0.953050274509804">
Thus each of the sets A, B ,C and D contain different
degree-0 sentences of Li. Clearly the probability of the
set Li \{AUBOCUD} is 1 — (a+b+ c+ d). The elements
of each defined subset of Li are equally likely with re-
spect to each other. Setting positive values for a, b, c, d
such that a + b c d &lt; 1 now defines a unique prob-
ability for each degree(0) sentence in Li. For example,
the probability of AdvV OS is b/2, the probability of
AdvAuxV OS is c/3, that of VOS is (1— (a+b+c+ d))/6
and so on; see figure 3. We can now obtain the tran-
sition matrix corresponding to this distribution. If we
compare this matrix with that obtained with a uniform
distribution on the sentences of Li in the earlier section.
This matrix has non-zero elements (transition proba-
bilities) exactly where the earlier matrix had non-zero
elements. However, the value of each transition prob-
ability now depends upon a, b, c, and d. In particular
if we choose a = 1/12,b = 2/12,c = 3/12,d = 1/12
(this is equivalent to assuming a uniform distribution)
we obtain the appropriate transition matrix as before.
Looking more closely at the general transition matrix,
we see that the transition probability from state 2 to
state 1 is (1 — (a -I- b + c))/3. Clearly if we make a
arbitrarily close to 1, then this transition probability
is arbitrarily close to 0 so that the number of samples
needed to converge can be made arbitrarily large. Thus
choosing large values for a and small values for b will
result in large convergence times.
This means that the sample complexity cannot be
bounded in a distribution-free sense, because by choos-
ing a highly unfavorable distribution the sample com-
plexity can be made as high as possible. For example,
we now give the convergence curves calculated for dif-
ferent choices of a, b, c, d. We see that for a uniform
distribution the convergence occurs within 200 sam-
ples. By choosing a distribution with a = 0.9999 and
b = c = d = 0.000001, the convergence time can be
pushed up to as much as 50 million samples. (Of course,
this distribution is presumably not psychologically re-
alistic.) For a = 0.99,b = c = d = 0.0001, the sample
complexity is on the order of 100, 000 positive examples.
Remark. The preceding calculation provides a worst-
case convergence time. We can also calculate average
convergence times using standard results from Markov
chain theory (see Isaacson and Madsen, 1976), as in
table 2. These support our previous results.
There are also well-known convergence theorems de-
rived from a consideration of the eigenvalues of the
transition matrix. We state without proof a conver-
gence result for transition matrices stated in terms of
its eigenvalues.
</bodyText>
<page confidence="0.998921">
175
</page>
<tableCaption confidence="0.953706">
Table 1: Complete list of problem states, i.e., all combinations of starting grammar and target grammar which result
in non-learnability of the target. The items marked with an asterisk are those listed in the original paper by Gibson
and Wexler (1994).
</tableCaption>
<table confidence="0.9528785">
Initial Grammar Target Grammar State of Initial Grammar Probability of Not
(Markov Structure) Converging to Target
(SVO-V2) (OVS-V2) Not Sink 0.5
(SVO+V2)* (OVS-V2) Sink 1.0
(SOV-V2) (OVS-V2) Not Sink 0.15
(S0V+V2)* (OVS-V2) Sink 1.0
(VOS-V2) (SVO-V2) Not Sink 0.33
(V0S+V2)* (SVO-V2) Sink 1.0
(OVS-V2) (SVO-V2) Not Sink 0.33
(OVS+V2)* (SVO-V2) Not Sink 1.0
(VOS-V2) (SOV-V2) Not Sink 0.33
(VOS-1-V2)* (SOV-V2) Sink 1.0
(OVS-V2) (SOV-V2) Not Sink 0.08
(OVS+V2)* (SOV-V2) Sink 1.0
lOo 200 300
Number of examples (m)
</table>
<figureCaption confidence="0.83298">
Figure 2: Convergence as a function of number of examples. The probability of converging to the target state after
</figureCaption>
<footnote confidence="0.836070666666667">
171 examples is plotted against in. The data from the target is assumed to be distributed uniformly over degree-0
sentences. The solid line represents TLA convergence times and the dotted line is a random walk learning algorithm
(RWA) which actually converges faster than the TLA in this case.
</footnote>
<page confidence="0.968731">
400
176
</page>
<figure confidence="0.9925065">
10 20 30 4:1
Log(Number of Samples)
</figure>
<figureCaption confidence="0.923513">
Figure 3: Rates of convergence for TLA with L1 as the target language for different distributions. The probability
of converging to the target after m samples is plotted against log(m). The three curves show how unfavorable
distributions can increase convergence times. The dashed line assumes uniform distribution and is the same curve
as plotted in figure 2.
</figureCaption>
<tableCaption confidence="0.647267666666667">
Table 2: Mean and standard deviation convergence times to target 5 (English) given different distributions over
the target language, and a uniform distribution over initial states. The first distribution is uniform over the target
language; the other distributions alter the value of a as discussed in the main text.
</tableCaption>
<table confidence="0.997798666666667">
Learning Mean abs. Std. Dev.
scenario time of abs. time
TLA (uniform) 34.8 22.3
TLA (a = 0.99) 45000 33000
TLA (a = 0.9999) 4.5 x 106 3.3 x 106
RW 9.6 10.1
</table>
<page confidence="0.991209">
177
</page>
<bodyText confidence="0.959856">
Theorem 2 Let T be an n x n transition matrix with
n linearly independent left eigenvectors xn cor-
responding to eigenvalues , , Ari . Let xo (an n-
dimensional vector) represent the starting probability of
being in each state of the chain and ir be the limiting
probability of being in each state. Then after k transi-
tions, the probability of being in each state xoTk can be
described by
</bodyText>
<equation confidence="0.775293">
n 72
xoTk —7r 114 E A/ixoyixi ii&lt; max lAiik xoyixi
2&lt;i&lt;n
1.1 1.2
</equation>
<bodyText confidence="0.9999206">
where the yi &apos;s are the right eigenvectors of T.
This theorem bounds the convergence rate to the
limiting distribution ir (in cases where there is only
one absorption state, 7r will have a 1 corresponding to
that state and 0 everywhere else). Using this result
we bound the rates of convergence (in terms of num-
ber k of samples). It should be plain that these results
could be used to establish standard errors and confi-
dence bounds on convergence times in the usual way,
another advantage of our new approach; see table 3.
</bodyText>
<sectionHeader confidence="0.9517795" genericHeader="method">
DISTRIBUTIONAL ASSUMPTIONS,
PART II
</sectionHeader>
<bodyText confidence="0.999983833333333">
The Markov model also allows us to easily determine
the effect of distributional changes in the input. This
is important for either computer or child acquisition
studies, since we can use corpus distributions to com-
pute convergence times in advance. For instance, it
can be easily shown that convergence times depend cru-
cially upon the distribution chosen (so in particular the
TLA learning model does not follow any distribution-
free PAC results). Specifically, we can choose a distribu-
tion that will make the convergence time as large as we
want. For example, in the situation where the target
language is L1, we can increase the convergence time
arbitrarily by increasing the probability of the string
{Adv(verb) V S}. By choosing a more unfavorable dis-
tribution the convergence time can be pushed up to as
much as 50 million samples. While not surprising in it-
self, the specificity of the model allows us to be precise
about the required sample size.
</bodyText>
<sectionHeader confidence="0.986192" genericHeader="method">
CHILDES DISTRIBUTIONS
</sectionHeader>
<bodyText confidence="0.999799295454546">
It is of interest to examine the fidelity of the model us-
ing real language distributions, namely, the CHILDES
database. We have carried out preliminary direct ex-
periments using the CHILDES caretaker English input
to &amp;quot;Nina&amp;quot; and German input to &amp;quot;Katrin&amp;quot;; these consist
of 43,612 and 632 sentences each, respectively. We note,
following well-known results by psycholinguists, that
both corpuses contain a much higher percentage of aux-
inversion and wh-questions than &amp;quot;ordinary&amp;quot; text (e.g.,
the LOB): 25,890 questions, and 11, 775 wh-questions;
201 and 99 in the German corpus; but only 2,506 ques-
tions or 3.7% out of 53,495 LOB sentences.
To test convergence, an implemented system using a
newer version of deMarcken&apos;s partial parser (see deMar-
cken, 1990) analyzed each degree-0 or degree-1 sentence
as falling into one of the input patterns SVO, S Aux V,
etc., as appropriate for the target language. Sentences
not parsable into these patterns were discarded (pre-
sumably &amp;quot;too complex&amp;quot; in some sense following a tradi-
tion established by many other researchers; see Wexler
and Culicover (1980) for details). Some examples of
caretaker inputs follow:
this is a book ? what do you see in the book?
how many rabbits?
what is the rabbit doing? (...)
is he hopping? oh . and what is he playing with?
red mir doch nicht alles nach !
ja , die schwatzen auch immer alles nach (...)
When run through the TLA, we discover that tcione-
vergence falls roughly along the TLA convergence m
displayed in figure 1—roughly 100 examples to asymp-
tote. Thus, the feasibility of the basic model is con-
firmed by actual caretaker input, at least in this simple
case, for both English and German. We are contin-
uing to explore this model with other languages and
distributional assumptions. However, there is one very
important new complication that must be taken into
account: we have found that one must (obviously) add
patterns to cover the predominance of auxiliary inver-
sions and wh-questions. However, that largely begs the
question of whether the language is verb-second or not.
Thus, as far as we can tell, we have not yet arrived at
a satisfactory parameter-setting account for V2 acqui-
sition.
</bodyText>
<sectionHeader confidence="0.9999555" genericHeader="method">
VARIANTS OF THE LEARNING
MODEL AND EXTENSIONS
</sectionHeader>
<bodyText confidence="0.9999098">
The Markov formulation allows one to more easily ex-
plore algorithm variants. Besides the TLA, we consider
the possible three simple learning algorithm regimes by
dropping either or both of the Single Value and Greed-
iness constraints. The key result is that almost any
other regime works faster than local gradient ascent and
avoids problems with local maxima. See figure 4 for a
representative result. Thus, most interestingly, param-
eterized language learning appears particularly robust
under algorithmic changes.
</bodyText>
<sectionHeader confidence="0.989476" genericHeader="method">
EXTENSIONS, DIACHRONIC
CHANGE AND CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.999868">
We remark here that the &amp;quot;batch&amp;quot; phonological param-
eter learning system of Dresher and Kaye (1990) is sus-
ceptible to a more direct PAC-type analysis, since their
system sets parameters in an &amp;quot;off-line&amp;quot; mode. We state
without proof some results that can be given in such
cases.
</bodyText>
<page confidence="0.998865">
178
</page>
<tableCaption confidence="0.99875">
Table 3: Convergence rates derived from eigenvalue calculations.
</tableCaption>
<table confidence="0.6234802">
Learning scenario Rate of Convergence
TLA (uniform) 0(0.94k)
TLA(a = 0.99) OW — 10-4)k)
TLA(a = 0.9999) OW —
RW 0(0.89k)
</table>
<figure confidence="0.9461375">
20 40 60 80 160
Number of samples
</figure>
<figureCaption confidence="0.85812575">
Figure 4: Convergence rates for different learning algorithms when L1 is the target language. The curve with the
slowest rate (large dashes) represents the TLA, the one with the fastest rate (small dashes) is the Random Walk
(RWA) with no greediness or single value constraints. Random walks with exactly one of the greediness and single
value constraints have performances in between.
</figureCaption>
<page confidence="0.993734">
179
</page>
<bodyText confidence="0.993605909090909">
Theorem 3 If the learner draws more than M =
in(1/(1—bt))
ln(1/6) samples, then it will identify the tar-
get with confidence greater than 1 — 8. ( Here bt =
P(Lt \u Li)).
Finally, the Markov model also points to an intrigu-
ing new model for syntactic change. One simply has to
introduce two or more target languages that emit posi-
tive example strings with (probably different) frequen-
cies: each corresponding to difference language sources.
If the model is run as before, then there can be a large
probability for a learner to converge to a state different
from the highest frequency emitting target state: that
is, the learner can acquire a different parameter setting,
for example, a —V2 setting, even in a predominantly
+V2 environment. This is of course one of the histor-
ical changes that occurred in the development of En-
glish. Space does not permit us to explore all the con-
sequences of this new Markov model; we remark here
that once again we can compute convergence times and
stability under different distributions of target frequen-
cies, combining it with the usual dynamical models of
genotype fixation. In this case, the interesting result is
that the TLA actually boosts diachronic change by or-
ders of magnitude, since as observed earlier, it can per-
mit the learner to arrive at a different convergent state
even when there is just one target language emitter. In
contrast, the local maxima targets are stable, and never
undergo change. Whether this powerful &amp;quot;boost&amp;quot; effect
plays a role in diachronic change remains a topic for fu-
ture investigation. As far as we know, the possibility for
formally modeling the kind of saltation indicated by the
Markov model has not been noted previously and has
only been vaguely stated by authors such as Lightfoot
(1990).
In conclusion, by introducing a formal mathematical
model for language acquisition, we can provide rigor-
ous results on parameter learning, algorithmic varia-
tion, sample complexity, and diachronic syntax change.
These results are of interest for corpus-based acquisition
and investigations of child acquisition, as well as point-
ing the way to a more rigorous bridge between modern
computational learning theory and computational lin-
guistics.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.999933571428571">
We would like to thank Ken Wexler, Ted Gibson, and
an anonymous ACL reviewer for valuable discussions
and comments on this work. Dr. Leonardo Topa pro-
vided invaluable programming assistance. All residual
errors are ours. This research is supported by NSF
grant 9217041-ASC and ARPA under the HPCC pro-
gram.
</bodyText>
<sectionHeader confidence="0.999779" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999616681818182">
Clark, Robin and Roberts, Ian (1993). &amp;quot;A Compu-
tational Model of Language Learnability and Lan-
guage Change.&amp;quot; Linguistic Inquiry, 24(2):299-345.
deMarcken, Carl (1990). &amp;quot;Parsing the LOB Corpus.&amp;quot;
Proceedings of the 25th Annual Meeting of the As-
sociation for Computational Linguistics. Pitts-
burgh, PA: Association for Computational Linguis-
tics, 243-251.
Dresher, Elan and Kaye, Jonathan (1990). &amp;quot;A Compu-
tational Learning Model For Metrical Phonology.&amp;quot;
Cognition, 34(0:137-195.
Gibson, Edward and Wexler, Kenneth (1994). &amp;quot;Trig-
gers.&amp;quot; Linguistic Inquiry, to appear.
Gold, E.M. (1967). &amp;quot;Language Identification in the
Limit.&amp;quot; Information and Control, 10(4): 447-474.
Isaacson, David and Masden, John (1976). Markov
Chains. New York: John Wiley.
Lightfoot, David (1990). How to Set Parameters. Cam-
bridge, MA: MIT Press.
Wexler, Kenneth and Culicover, Peter (1980). Formal
Principles of Language Acquisition. Cambridge,
MA: MIT Press.
</reference>
<page confidence="0.997748">
180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000046">
<title confidence="0.99942">A MARKOV LANGUAGE LEARNING MODEL FOR FINITE PARAMETER SPACES</title>
<author confidence="0.999563">Niyogi C Berwick</author>
<affiliation confidence="0.9998495">Center for Biological and Computational Learning Massachusetts Institute of Technology</affiliation>
<address confidence="0.986655">E25-201 Cambridge, MA 02139, USA</address>
<email confidence="0.997567">Internet:pn©ai.mit.edu,berwick@ai.mit.edu</email>
<abstract confidence="0.976187495327102">paper shows how to lanlearning in a finite parameter space as a new language learning results follow directly: explicitly calculated sample complexity learning times under different input distribution asstunptions (including CHILDES database language input) and learning regimes. We also briefly describe a way to formally model (rapid) change. BACKGROUND MOTIVATION: TRIGGERS AND LANGUAGE ACQUISITION Recently, several researchers, including Gibson and Wexler (1994), henceforth GW, Dresher and Kaye (1990); and Clark and Roberts (1993) have modeled language learning in a (finite) space whose grammars are characterized by a finite number of parameters or nlength Boolean-valued vectors. Many current linguistic theories now employ such parametric models explicitly or in spirit, including Lexical-Functional Grammar and versions of HPSG, besides GB variants. With all such models, key questions about sample complexity, convergence time, and alternative modeling assumptions are difficult to assess without a precise mathematical formalization. Previous research has usually addressed only the question of convergence in the limit without probing the equally important question of sample complexity: it is of not much use that a learner can acquire a language if sample complexity is extraordinarily high, hence psychologically implausible. This remains a relatively undeveloped area of language learning theory. The current paper aims to fill that gap. We choose as a starting point the GW Triggering Learning Algorithm (TLA). Our central result is that the performance of this algorithm and others like it is by a Markov chain. We explore the basic computational consequences of this, including some surprising results about sample complexity and convergence time, the dominance of random walk over gradient ascent, and the applicability of these results to actual child language acquisition and possibly language change. Background. Following Gold (1967) the basic framework is that of identification in the limit. We assume some familiarity with Gold&apos;s assumptions. The learner receives an (infinite) sequence of (positive) example sentences from some target language. After each, the learner either (i) stays in the same state; or (ii) moves to a new state (change its parameter settings). If after some finite number of examples the learner converges to the correct target language and never changes its guess, then it has correctly identified the target language in the limit; otherwise, it fails. In the GW model (and others) the learner obeys two fundamental constraints: (1) the can change only 1 parameter each step; and (2) the constraint—if the learner is given a positive example it cannot recognize and changes one parameter value, finding that it can accept the example, then the learner retains that new value. The TLA essentially simulates this; see Gibson and Wexler (1994) for details. THE MARKOV FORMULATION Previous parameter models leave open key questions addressable by a more precise formalization as a Markov The correspondence is direct. Each point the Markov space is a possible parameter setting. Transibetween states stand for probabilities the will move from hypothesis state i to state we show below, given a distribution over calculate the actual Thus, we can picture the TLA learning space as a directed, labeled 2&apos; vertices. See figure 1 for an example in 3-parameter We can now use Markov theory to describe TLA parameter spaces, as in Isaacson and construct an identical transition diagram in the description of their computer program for calculating local maxima. However, this diagram is not explicitly presented as a Markov structure and does not include transition probabilities. 171 Madsen (1976). By the single value hypothesis, the syscan only move 1 Hamming bit at a time, either totarget language or 1 bit away. Surface strings can force the learner from one hypothesis state to an- For instance, if state to a grammar that generates a language that is a proper subset another grammar hypothesis can never be transition from there must be one from we reach the target grammar there is nothing that can move the learner from this state, since all remaining positive evidence will not cause the learner change its hypothesis: an State in the Markov literature. Clearly, one can conclude at once the following important learnability result: 1 a Markov chain C corresponding to GW TLA learner, exactly AS (corresponding to the target grammar/language) if C is learnable. assumption, learnable. Now assume for sake of contradiction that there is not exactly one AS. Then there must be either 0 AS or &gt; 1 AS. In the first case, by the definition of an absorbing state, there is no hypothesis in which the learner will remain for- Therefore not learnable, a contradiction. In the second case, without loss of generality, assume there exactly two absorbing states, the first correspondto the target parameter setting, and the second corresponding to some other setting. By the definition an absorbing state, in the limit with some probability enter never exit not learnable, a contradiction. Hence our assumption that there is not exactly 1 AS must be false. Assume that there exists exactly 1 AS i in the chain by the definition of an absorbing state, after some number of steps n, no matter what starting state, end up in state corresponding to the target grammar. 1 0.1 a Markov chain corresponding to a (finite) family of grammars in a GW learning system, if there exist 2 or more AS, then that family is not learnable. DERIVATION OF TRANSITION PROBABILITIES FOR THE MARKOV TLA STRUCTURE We now derive the transition probabilities for the Markov TLA structure, the key to establishing samcomplexity results. Let the target language be = s3, ...} probability distribution on these strings. Suppose the learner is in a state correto language probability a string are two cases given current parameter settings. learner can syntactically analyze the restring parameter values are unchanged. is so only when E probability of rein the state learner cannot syntactically analyze string. Then learner is in state and has n neighboring states (Hamming distance of 1). The learner picks one of these uniformly at random. If ni of these neighboring states correspond to languages which contain si and the learner picks any one of them probability it stays in that state. If the learner picks any of the other states (with probability — n2)/n) then it remains in state that could take values between 0 and n. Thus the probability the learner remains in probability of moving to each of the other The probability that the learner will remain in its state the sum of the probabilities of these 3 iin)P(si)• compute the transition probability from that this transition will occur with proba- 1/n for all the strings E Lk not in strings occur with probability P(s) each so the transition probability k] over all strings .92 (Ltfl (set difit is easy to see that (Ltfl &lt;=&gt; si n we have le] Now we can compute the transition probabilities between any two states. Thus the self-transition probability can be given as, = a neighboring state of s Example. Consider the 3-parameter natural language system described by Gibson and Wexler (1994), designed to cover basic word orders (X-bar structures) plus the verbsecond phenomena of Germanic languages. Its binary are: (1) Spec(ifier) initial (0) or final (2) Compl(ement) initial (0) or final (1); and Verb Second (V2) does not exist (0) or does exist (1). Possible &amp;quot;words&amp;quot; in this language include S(ubject), V(erb), O(bject), D(irect) O(bject), Adv(erb) phrase, and so forth. Given these alternatives, Gibson and Wexler (1994) show that there are 12 possible surface strings for each (—V2) grammar and 18 possible surface strings for each (+V2) grammar, restricted to unembedded or &amp;quot;degree-0&amp;quot; examples for reasons of psychological plausibility (see Gibson and Wexler for discussion). For inthe parameter setting [0 I Specifier initial, Complement final, and —V2, works out to the possible basic English surface phrase order of Subject—Verb— Object (SVO). As in figure 1 below, suppose the SVO (&amp;quot;English&amp;quot;, 1 0]) is the target grammar. The figure&apos;s shaded rings represent increasing Hamming distances from the target. Each labeled circle is a Markov state. Surrounding the bulls-eye target are the 3 other parameter arrays that differ from [0 1 0] by one binary digit: e.g., [0, 0, 0], or Spec-first, Comp-first, —V2, basic order or 172 Figure 1: The 8 parameter settings in the GW example, shown as a Markov structure, with transition probabilities omitted. Directed arrows between circles (states) represent possible nonzero (possible learner) transitions. The target grammar (in this case, number 5, setting [0 1 0]), lies at dead center. Around it are the three settings that differ from the target by exactly one binary digit; surrounding those are the 3 hypotheses two binary digits away from the target; the third ring out contains the single hypothesis that differs from the target by 3 binary digits. sink 1_ 1 11 target:5 ec lst,tomp final, —VgA 0011 173 Plainly there are exactly 2 absorbing states in this Markov chain. One is the target grammar (by definithe other is state 2. State 4 is also a leads only to state 4 or state 2. GW call these two states maxima local gradient ascent will converge to these without reaching the detarget. Hence this system is More importantly though, in addition to these local maxima, show (see below) that there are (not detected in GW or described by Clark) from which the learner will never reach the target with (high) positive probability. Example: we show that if the learner starts at hypothesis VOS—V2, then with probability 0.33 in the limit, the learner will never converge to the SVO target. Crucially, we must use set differences to build the Markov figure straightforwardly, as indicated in the next section. In short, while it is possible to reach &amp;quot;English&amp;quot;from some source languages like &amp;quot;Japanese,&amp;quot; this is not possible for other starting points (exactly 4 other initial states). It is easy to imagine alternatives to the TLA that avoid the local maxima problem. As it stands the learner only changes a parameter setting if that change allows the learner to analyze the sentence it could not analyze before. If we relax this condition so that under unanalyzability the learner picks a random parameter to change, then the problem with local maxima disappears, because there can be only 1 Absorbing State, the target grammar. All other states have exit arcs. Thus, our main theorem, such a system We discuss other alternatives below. CONVERGENCE TIMES FOR THE MARKOV CHAIN MODEL Perhaps the most significant advantage of the Markov chain formulation is that one can calculate the number of examples needed to acquire a language. Recall it is not enough to demonstrate convergence in the limit; must also be is particularly true in the case of finite parameter spaces where convergence might not be as much of a problem as feasibility. Fortunately, given the transition matrix of a Markov chain, the problem of how long it takes to converge has been well studied. SOME TRANSITION MATRICES AND THEIR CONVERGENCE CURVES Consider the example in the previous section. The target grammar is SVO—V2 (grammar #5 in GW). For assume a uniform distribution on probability of a particular string in 1/12 bethere are 12 (degree-0) strings in directly compute the transition matrix (0 entries elsewhere): L2 L3 L4 L5 L6 L7 L8 Li 2 T 3 L2 1 3 1 4 1 L4 12 12 L5 1 1 5 L6 6 1 18 LE; 12 36 9 States 2 and 5 are absorbing; thus this chain contains local maxima. Also, state 4 exits only to either itself to state 2, hence is also a local maximum. If the transition probability matrix of a chain, then the j of the probability that learner moves from state state For learnability to hold irrespective starting state, the probability of reaching state 5 should approach 1 as in to infinity, i.e., column 5 of contain all l&apos;s, and O&apos;s elsewhere. Direct computation shows this to be false: L3 L5 L7 2 3 L2 1 L3 1 2 3 3 L4 1 L5 1 L6 1 L7 1 L8 1 We see that if the learner starts out in states 2 or 4, will up in state 2 in the limit. These two states correspond to local maxima grammars in the GW framework. We also see that if the learner starts states 5 through 8, it will in the limit to the target grammar. States 1 and 3 are much more interesting, and constitute new results about this parameterization. If the learner starts in either of these states, it reaches the target grammar with probability 2/3 and state 2 with 1/3. Thus, local maxima are only problem for parameter space learnability. To our knowledge, GW and other researchers have focused exclusively on local maxima. However, while it is true that states 2 and 4 will, with probability 1, not converge to target grammar, it is that states 1 and 3 will not converge to the target, with probability 1/3. Thus, the number of &amp;quot;bad&amp;quot; initial hypotheses is significantly larger than realized generally (in fact, 12 out of 56 of the possible source-target grammar pairs in the 3parameter system). This difference is again due to the new probabilistic framework introduced in the current paper. 174 2 shows a plot of the quantity as a function of number of exam- Here the probability of being in state 1 at the end of in examples in the case where the learner in state we want pi ( nl) 1 111-■ 00 and for this example this is indeed the case. The next figure shows a plot of the following quantity as a function of in, the number of examples. = quantity easy to interpret. Thus 0.95 means that for every initial state of the learner the probability that it is in the target state after m examples is at least 0.95. Further there is one initial state (the worst initial state with respect to the target, in our example is for which this probability is exactly 0.95. We find on looking at the curve that the learner converges with high probability within 100 to 200 (degree-0) example sentences, a psychologically plausible number. We can now compare the convergence time of TLA to other algorithms. Perhaps the simplest is random walk: start the learner at a random point in the 3-parameter space, and then, if an input sentence cannot be analyzed, move 1-bit randomly from state to state. Note that this regime cannot suffer from the local maxima problem, since there is always some finite probability of exiting a non-target state. Computing the convergence curves for a random walk algorithm (RWA) on the 8 state space, we find that the convergence times are actually faster than for the TLA; see figure 2. Since the RWA is also superior in that it does not suffer from the same local maxima problem as TLA, the conceptual support for the TLA is by no means clear. Of course, it may be that the TLA has empirical support, in the sense of independent evidence that children do use this procedure (given by the pattern of their errors, etc.), but this evidence is lacking, as far as we know. DISTRIBUTIONAL ASSUMPTIONS: PART I In the earlier section we assumed that the data was uniformly distributed. We computed the transition matrix for a particular target language and showed that convergence times were of the order of 100-200 samples. In this section we show that the convergence times depend crucially upon the distribution. In particular we can choose a distribution which will make the convergence time as large as we want. Thus the distribution-free convergence time for the 3-parameter system is infinite. As before, we consider the situation where the target is are no local maxima problems for this choice. We begin by letting the distribution be by the variables a, c, d = (A = V S}) = P(B = V 0 5, Adv Aux V S}) = V 01 02 5, Adv Aux V 0 S, Adv Aux V 01 02 S}) = P(D = {V each of the sets B ,C different sentences of Clearly the probability of the \{AUBOCUD} 1 (a+b+ c+ d). elements each defined subset of are equally likely with reto each other. Setting positive values for a, c, that a + c d 1 now defines a unique probfor each degree(0) sentence in For example, probability of OS b/2, the probability of OS is that of (1— and so on; see figure 3. We can now obtain the transition matrix corresponding to this distribution. If we compare this matrix with that obtained with a uniform distribution on the sentences of Li in the earlier section. This matrix has non-zero elements (transition probabilities) exactly where the earlier matrix had non-zero elements. However, the value of each transition probnow depends upon a, c, particular we choose = = = (this is equivalent to assuming a uniform distribution) we obtain the appropriate transition matrix as before. Looking more closely at the general transition matrix, we see that the transition probability from state 2 to 1 is (1 — (a -I- + Clearly if we make a arbitrarily close to 1, then this transition probability is arbitrarily close to 0 so that the number of samples needed to converge can be made arbitrarily large. Thus large values for a and small values for result in large convergence times. This means that the sample complexity cannot be bounded in a distribution-free sense, because by choosing a highly unfavorable distribution the sample complexity can be made as high as possible. For example, we now give the convergence curves calculated for difchoices of a, c, d. see that for a uniform distribution the convergence occurs within 200 samples. By choosing a distribution with a = 0.9999 and = c = d = the convergence time can be pushed up to as much as 50 million samples. (Of course, this distribution is presumably not psychologically re- For a = = c = d = the sample complexity is on the order of 100, 000 positive examples. preceding calculation provides a worstconvergence time. We can also calculate convergence times using standard results from Markov chain theory (see Isaacson and Madsen, 1976), as in table 2. These support our previous results. There are also well-known convergence theorems derived from a consideration of the eigenvalues of the transition matrix. We state without proof a convergence result for transition matrices stated in terms of its eigenvalues. 175 Table 1: Complete list of problem states, i.e., all combinations of starting grammar and target grammar which result in non-learnability of the target. The items marked with an asterisk are those listed in the original paper by Gibson</abstract>
<note confidence="0.800679782608696">and Wexler (1994). Initial Grammar Target Grammar State of Initial Grammar (Markov Structure) Probability of Not Converging to Target (SVO-V2) (OVS-V2) Not Sink 0.5 (SVO+V2)* (OVS-V2) Sink 1.0 (SOV-V2) (OVS-V2) Not Sink 0.15 (S0V+V2)* (OVS-V2) Sink 1.0 (VOS-V2) (SVO-V2) Not Sink 0.33 (V0S+V2)* (SVO-V2) Sink 1.0 (OVS-V2) (SVO-V2) Not Sink 0.33 (OVS+V2)* (SVO-V2) Not Sink 1.0 (VOS-V2) (SOV-V2) Not Sink 0.33 (VOS-1-V2)* (SOV-V2) Sink 1.0 (OVS-V2) (SOV-V2) Not Sink 0.08 (OVS+V2)* (SOV-V2) Sink 1.0 300 Number of examples (m) Figure 2: Convergence as a function of number of examples. The probability of converging to the target state after is plotted against in. The data from the target is assumed to be distributed uniformly over degree-0 The solid line represents times and the dotted line is a random walk learning algorithm which actually converges the TLA in this case. 400 176</note>
<phone confidence="0.662057">20 30</phone>
<abstract confidence="0.986812344086022">Log(Number of Samples) 3: Rates of convergence for TLA with as the target language for different distributions. The probability of converging to the target after m samples is plotted against log(m). The three curves show how unfavorable distributions can increase convergence times. The dashed line assumes uniform distribution and is the same curve as plotted in figure 2. Table 2: Mean and standard deviation convergence times to target 5 (English) given different distributions over the target language, and a uniform distribution over initial states. The first distribution is uniform over the target language; the other distributions alter the value of a as discussed in the main text. Learning Mean abs. Std. Dev. scenario time of abs. time TLA (uniform) 34.8 22.3 45000 33000 TLA (a = 0.9999) x x RW 9.6 10.1 177 2 T be an n transition matrix with independent left eigenvectors corto eigenvalues , . (an dimensional vector) represent the starting probability of being in each state of the chain and ir be the limiting probability of being in each state. Then after k transithe probability of being in each state can be described by 114 A/ixoyixi max xoyixi 2&lt;i&lt;n 1.1 1.2 the are the right eigenvectors of T. This theorem bounds the convergence rate to the limiting distribution ir (in cases where there is only one absorption state, 7r will have a 1 corresponding to that state and 0 everywhere else). Using this result we bound the rates of convergence (in terms of numsamples). be plain that these results could be used to establish standard errors and confidence bounds on convergence times in the usual way, another advantage of our new approach; see table 3. DISTRIBUTIONAL ASSUMPTIONS, PART II The Markov model also allows us to easily determine the effect of distributional changes in the input. This is important for either computer or child acquisition studies, since we can use corpus distributions to compute convergence times in advance. For instance, it can be easily shown that convergence times depend crucially upon the distribution chosen (so in particular the TLA learning model does not follow any distributionfree PAC results). Specifically, we can choose a distribution that will make the convergence time as large as we want. For example, in the situation where the target is we can increase the convergence time arbitrarily by increasing the probability of the string {Adv(verb) V S}. By choosing a more unfavorable distribution the convergence time can be pushed up to as much as 50 million samples. While not surprising in itself, the specificity of the model allows us to be precise about the required sample size. CHILDES DISTRIBUTIONS It is of interest to examine the fidelity of the model using real language distributions, namely, the CHILDES database. We have carried out preliminary direct experiments using the CHILDES caretaker English input to &amp;quot;Nina&amp;quot; and German input to &amp;quot;Katrin&amp;quot;; these consist of 43,612 and 632 sentences each, respectively. We note, following well-known results by psycholinguists, that both corpuses contain a much higher percentage of auxinversion and wh-questions than &amp;quot;ordinary&amp;quot; text (e.g., the LOB): 25,890 questions, and 11, 775 wh-questions; 201 and 99 in the German corpus; but only 2,506 questions or 3.7% out of 53,495 LOB sentences. To test convergence, an implemented system using a newer version of deMarcken&apos;s partial parser (see deMarcken, 1990) analyzed each degree-0 or degree-1 sentence as falling into one of the input patterns SVO, S Aux V, etc., as appropriate for the target language. Sentences not parsable into these patterns were discarded (presumably &amp;quot;too complex&amp;quot; in some sense following a tradition established by many other researchers; see Wexler and Culicover (1980) for details). Some examples of caretaker inputs follow: this is a book ? what do you see in the book? how many rabbits? what is the rabbit doing? (...) is he hopping? oh . and what is he playing with? red mir doch nicht alles nach ! ja , die schwatzen auch immer alles nach (...) run through the TLA, we discover that vergence falls roughly along the TLA convergence m displayed in figure 1—roughly 100 examples to asymptote. Thus, the feasibility of the basic model is confirmed by actual caretaker input, at least in this simple case, for both English and German. We are continuing to explore this model with other languages and distributional assumptions. However, there is one very important new complication that must be taken into account: we have found that one must (obviously) add patterns to cover the predominance of auxiliary inversions and wh-questions. However, that largely begs the question of whether the language is verb-second or not. Thus, as far as we can tell, we have not yet arrived at a satisfactory parameter-setting account for V2 acquisition. VARIANTS OF THE LEARNING MODEL AND EXTENSIONS The Markov formulation allows one to more easily explore algorithm variants. Besides the TLA, we consider the possible three simple learning algorithm regimes by dropping either or both of the Single Value and Greedconstraints. The key result is that any works faster than local gradient ascent and avoids problems with local maxima. See figure 4 for a representative result. Thus, most interestingly, parameterized language learning appears particularly robust under algorithmic changes. EXTENSIONS, DIACHRONIC CHANGE AND CONCLUSIONS We remark here that the &amp;quot;batch&amp;quot; phonological parameter learning system of Dresher and Kaye (1990) is susceptible to a more direct PAC-type analysis, since their system sets parameters in an &amp;quot;off-line&amp;quot; mode. We state without proof some results that can be given in such cases. 178 Table 3: Convergence rates derived from eigenvalue calculations. Learning scenario Rate of Convergence TLA (uniform) TLA(a = 0.99) — TLA(a = 0.9999) OW — RW 60 80 160 Number of samples 4: Convergence rates for different learning algorithms when is the target language. The curve with the slowest rate (large dashes) represents the TLA, the one with the fastest rate (small dashes) is the Random Walk (RWA) with no greediness or single value constraints. Random walks with exactly one of the greediness and single value constraints have performances in between. 179 3 the learner draws more than M = ln(1/6) then it will identify the with confidence greater than 1 — 8. ( Here = \u Li)). Finally, the Markov model also points to an intriguing new model for syntactic change. One simply has to introduce two or more target languages that emit positive example strings with (probably different) frequencies: each corresponding to difference language sources. If the model is run as before, then there can be a large probability for a learner to converge to a state different from the highest frequency emitting target state: that is, the learner can acquire a different parameter setting, for example, a —V2 setting, even in a predominantly +V2 environment. This is of course one of the historical changes that occurred in the development of English. Space does not permit us to explore all the consequences of this new Markov model; we remark here that once again we can compute convergence times and stability under different distributions of target frequencies, combining it with the usual dynamical models of genotype fixation. In this case, the interesting result is that the TLA actually boosts diachronic change by orders of magnitude, since as observed earlier, it can permit the learner to arrive at a different convergent state when there is just language emitter. In contrast, the local maxima targets are stable, and never undergo change. Whether this powerful &amp;quot;boost&amp;quot; effect plays a role in diachronic change remains a topic for future investigation. As far as we know, the possibility for formally modeling the kind of saltation indicated by the Markov model has not been noted previously and has only been vaguely stated by authors such as Lightfoot (1990). In conclusion, by introducing a formal mathematical model for language acquisition, we can provide rigorous results on parameter learning, algorithmic variation, sample complexity, and diachronic syntax change. These results are of interest for corpus-based acquisition and investigations of child acquisition, as well as pointing the way to a more rigorous bridge between modern computational learning theory and computational linguistics. ACKNOWLEDGMENTS We would like to thank Ken Wexler, Ted Gibson, and an anonymous ACL reviewer for valuable discussions and comments on this work. Dr. Leonardo Topa provided invaluable programming assistance. All residual errors are ours. This research is supported by NSF grant 9217041-ASC and ARPA under the HPCC program.</abstract>
<note confidence="0.902709217391304">REFERENCES Clark, Robin and Roberts, Ian (1993). &amp;quot;A Computational Model of Language Learnability and Lan- Change.&amp;quot; Inquiry, deMarcken, Carl (1990). &amp;quot;Parsing the LOB Corpus.&amp;quot; Proceedings of the 25th Annual Meeting of the Asfor Computational Linguistics. Pittsburgh, PA: Association for Computational Linguistics, 243-251. Dresher, Elan and Kaye, Jonathan (1990). &amp;quot;A Computational Learning Model For Metrical Phonology.&amp;quot; Gibson, Edward and Wexler, Kenneth (1994). &amp;quot;Trig- Inquiry, appear. Gold, E.M. (1967). &amp;quot;Language Identification in the and Control, 447-474. David and Masden, John (1976). York: John Wiley. David (1990). to Set Parameters. Cambridge, MA: MIT Press. Kenneth and Culicover, Peter (1980). of Language Acquisition. MA: MIT Press. 180</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robin Clark</author>
<author>Ian Roberts</author>
</authors>
<title>A Computational Model of Language Learnability and Language Change.&amp;quot; Linguistic Inquiry,</title>
<date>1993</date>
<pages>24--2</pages>
<contexts>
<context position="890" citStr="Clark and Roberts (1993)" startWordPosition="117" endWordPosition="120">act This paper shows how to formally characterize language learning in a finite parameter space as a Markov structure. Important new language learning results follow directly: explicitly calculated sample complexity learning times under different input distribution asstunptions (including CHILDES database language input) and learning regimes. We also briefly describe a new way to formally model (rapid) diachronic syntax change. BACKGROUND MOTIVATION: TRIGGERS AND LANGUAGE ACQUISITION Recently, several researchers, including Gibson and Wexler (1994), henceforth GW, Dresher and Kaye (1990); and Clark and Roberts (1993) have modeled language learning in a (finite) space whose grammars are characterized by a finite number of parameters or nlength Boolean-valued vectors. Many current linguistic theories now employ such parametric models explicitly or in spirit, including Lexical-Functional Grammar and versions of HPSG, besides GB variants. With all such models, key questions about sample complexity, convergence time, and alternative modeling assumptions are difficult to assess without a precise mathematical formalization. Previous research has usually addressed only the question of convergence in the limit wit</context>
</contexts>
<marker>Clark, Roberts, 1993</marker>
<rawString>Clark, Robin and Roberts, Ian (1993). &amp;quot;A Computational Model of Language Learnability and Language Change.&amp;quot; Linguistic Inquiry, 24(2):299-345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl deMarcken</author>
</authors>
<title>Parsing the LOB Corpus.&amp;quot;</title>
<date>1990</date>
<journal>Association for Computational Linguistics,</journal>
<booktitle>Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>243--251</pages>
<location>Pittsburgh, PA:</location>
<contexts>
<context position="25211" citStr="deMarcken, 1990" startWordPosition="4319" endWordPosition="4321"> out preliminary direct experiments using the CHILDES caretaker English input to &amp;quot;Nina&amp;quot; and German input to &amp;quot;Katrin&amp;quot;; these consist of 43,612 and 632 sentences each, respectively. We note, following well-known results by psycholinguists, that both corpuses contain a much higher percentage of auxinversion and wh-questions than &amp;quot;ordinary&amp;quot; text (e.g., the LOB): 25,890 questions, and 11, 775 wh-questions; 201 and 99 in the German corpus; but only 2,506 questions or 3.7% out of 53,495 LOB sentences. To test convergence, an implemented system using a newer version of deMarcken&apos;s partial parser (see deMarcken, 1990) analyzed each degree-0 or degree-1 sentence as falling into one of the input patterns SVO, S Aux V, etc., as appropriate for the target language. Sentences not parsable into these patterns were discarded (presumably &amp;quot;too complex&amp;quot; in some sense following a tradition established by many other researchers; see Wexler and Culicover (1980) for details). Some examples of caretaker inputs follow: this is a book ? what do you see in the book? how many rabbits? what is the rabbit doing? (...) is he hopping? oh . and what is he playing with? red mir doch nicht alles nach ! ja , die schwatzen auch immer</context>
</contexts>
<marker>deMarcken, 1990</marker>
<rawString>deMarcken, Carl (1990). &amp;quot;Parsing the LOB Corpus.&amp;quot; Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics. Pittsburgh, PA: Association for Computational Linguistics, 243-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elan Dresher</author>
<author>Jonathan Kaye</author>
</authors>
<title>A Computational Learning Model For Metrical Phonology.&amp;quot;</title>
<date>1990</date>
<journal>Cognition,</journal>
<pages>34--0</pages>
<contexts>
<context position="860" citStr="Dresher and Kaye (1990)" startWordPosition="112" endWordPosition="115">edu, berwick@ai.mit.edu Abstract This paper shows how to formally characterize language learning in a finite parameter space as a Markov structure. Important new language learning results follow directly: explicitly calculated sample complexity learning times under different input distribution asstunptions (including CHILDES database language input) and learning regimes. We also briefly describe a new way to formally model (rapid) diachronic syntax change. BACKGROUND MOTIVATION: TRIGGERS AND LANGUAGE ACQUISITION Recently, several researchers, including Gibson and Wexler (1994), henceforth GW, Dresher and Kaye (1990); and Clark and Roberts (1993) have modeled language learning in a (finite) space whose grammars are characterized by a finite number of parameters or nlength Boolean-valued vectors. Many current linguistic theories now employ such parametric models explicitly or in spirit, including Lexical-Functional Grammar and versions of HPSG, besides GB variants. With all such models, key questions about sample complexity, convergence time, and alternative modeling assumptions are difficult to assess without a precise mathematical formalization. Previous research has usually addressed only the question o</context>
<context position="27335" citStr="Dresher and Kaye (1990)" startWordPosition="4667" endWordPosition="4670">o more easily explore algorithm variants. Besides the TLA, we consider the possible three simple learning algorithm regimes by dropping either or both of the Single Value and Greediness constraints. The key result is that almost any other regime works faster than local gradient ascent and avoids problems with local maxima. See figure 4 for a representative result. Thus, most interestingly, parameterized language learning appears particularly robust under algorithmic changes. EXTENSIONS, DIACHRONIC CHANGE AND CONCLUSIONS We remark here that the &amp;quot;batch&amp;quot; phonological parameter learning system of Dresher and Kaye (1990) is susceptible to a more direct PAC-type analysis, since their system sets parameters in an &amp;quot;off-line&amp;quot; mode. We state without proof some results that can be given in such cases. 178 Table 3: Convergence rates derived from eigenvalue calculations. Learning scenario Rate of Convergence TLA (uniform) 0(0.94k) TLA(a = 0.99) OW — 10-4)k) TLA(a = 0.9999) OW — RW 0(0.89k) 20 40 60 80 160 Number of samples Figure 4: Convergence rates for different learning algorithms when L1 is the target language. The curve with the slowest rate (large dashes) represents the TLA, the one with the fastest rate (small</context>
</contexts>
<marker>Dresher, Kaye, 1990</marker>
<rawString>Dresher, Elan and Kaye, Jonathan (1990). &amp;quot;A Computational Learning Model For Metrical Phonology.&amp;quot; Cognition, 34(0:137-195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
<author>Kenneth Wexler</author>
</authors>
<title>Triggers.&amp;quot; Linguistic Inquiry,</title>
<date>1994</date>
<note>to appear.</note>
<contexts>
<context position="820" citStr="Gibson and Wexler (1994)" startWordPosition="106" endWordPosition="109">ridge, MA 02139, USA Internet: pn©ai.mit.edu, berwick@ai.mit.edu Abstract This paper shows how to formally characterize language learning in a finite parameter space as a Markov structure. Important new language learning results follow directly: explicitly calculated sample complexity learning times under different input distribution asstunptions (including CHILDES database language input) and learning regimes. We also briefly describe a new way to formally model (rapid) diachronic syntax change. BACKGROUND MOTIVATION: TRIGGERS AND LANGUAGE ACQUISITION Recently, several researchers, including Gibson and Wexler (1994), henceforth GW, Dresher and Kaye (1990); and Clark and Roberts (1993) have modeled language learning in a (finite) space whose grammars are characterized by a finite number of parameters or nlength Boolean-valued vectors. Many current linguistic theories now employ such parametric models explicitly or in spirit, including Lexical-Functional Grammar and versions of HPSG, besides GB variants. With all such models, key questions about sample complexity, convergence time, and alternative modeling assumptions are difficult to assess without a precise mathematical formalization. Previous research h</context>
<context position="3331" citStr="Gibson and Wexler (1994)" startWordPosition="490" endWordPosition="494">es the learner converges to the correct target language and never changes its guess, then it has correctly identified the target language in the limit; otherwise, it fails. In the GW model (and others) the learner obeys two additional fundamental constraints: (1) the single-value constraint—the learner can change only 1 parameter value each step; and (2) the greediness constraint—if the learner is given a positive example it cannot recognize and changes one parameter value, finding that it can accept the example, then the learner retains that new value. The TLA essentially simulates this; see Gibson and Wexler (1994) for details. THE MARKOV FORMULATION Previous parameter models leave open key questions addressable by a more precise formalization as a Markov chain. The correspondence is direct. Each point i in the Markov space is a possible parameter setting. Transitions between states stand for probabilities b that the learner will move from hypothesis state i to state j. As we show below, given a distribution over L(G), we can calculate the actual b&apos;s themselves. Thus, we can picture the TLA learning space as a directed, labeled graph V with 2&apos; vertices. See figure 1 for an example in a 3-parameter syste</context>
<context position="8534" citStr="Gibson and Wexler (1994)" startWordPosition="1405" endWordPosition="1408">probability 1/n for all the strings 53 E Lk but not in L3. These strings occur with probability P(s) each and so the transition probability is:P[s k] Es,EL,,,s,e&apos;L.,3,ELk(110P(sj)• Summing over all strings .92 E (Ltfl Lk)\Ls (set difference) it is easy to see that si E (Ltfl Lk)\L, &lt;=&gt; si E (Lt n Lk)\(Lt n L3). Rewriting, we have P[s le] Now we can compute Es,E(LinLk)\(LtoL, the transition probabilities between any two states. Thus the self-transition probability can be given as, P[s = 1—is a neighboring state of s P[s k]. Example. Consider the 3-parameter natural language system described by Gibson and Wexler (1994), designed to cover basic word orders (X-bar structures) plus the verbsecond phenomena of Germanic languages. Its binary parameters are: (1) Spec(ifier) initial (0) or final (1); (2) Compl(ement) initial (0) or final (1); and Verb Second (V2) does not exist (0) or does exist (1). Possible &amp;quot;words&amp;quot; in this language include S(ubject), V(erb), O(bject), D(irect) O(bject), Adv(erb) phrase, and so forth. Given these alternatives, Gibson and Wexler (1994) show that there are 12 possible surface strings for each (—V2) grammar and 18 possible surface strings for each (+V2) grammar, restricted to unembe</context>
<context position="20665" citStr="Gibson and Wexler (1994)" startWordPosition="3559" endWordPosition="3562">ence times using standard results from Markov chain theory (see Isaacson and Madsen, 1976), as in table 2. These support our previous results. There are also well-known convergence theorems derived from a consideration of the eigenvalues of the transition matrix. We state without proof a convergence result for transition matrices stated in terms of its eigenvalues. 175 Table 1: Complete list of problem states, i.e., all combinations of starting grammar and target grammar which result in non-learnability of the target. The items marked with an asterisk are those listed in the original paper by Gibson and Wexler (1994). Initial Grammar Target Grammar State of Initial Grammar Probability of Not (Markov Structure) Converging to Target (SVO-V2) (OVS-V2) Not Sink 0.5 (SVO+V2)* (OVS-V2) Sink 1.0 (SOV-V2) (OVS-V2) Not Sink 0.15 (S0V+V2)* (OVS-V2) Sink 1.0 (VOS-V2) (SVO-V2) Not Sink 0.33 (V0S+V2)* (SVO-V2) Sink 1.0 (OVS-V2) (SVO-V2) Not Sink 0.33 (OVS+V2)* (SVO-V2) Not Sink 1.0 (VOS-V2) (SOV-V2) Not Sink 0.33 (VOS-1-V2)* (SOV-V2) Sink 1.0 (OVS-V2) (SOV-V2) Not Sink 0.08 (OVS+V2)* (SOV-V2) Sink 1.0 lOo 200 300 Number of examples (m) Figure 2: Convergence as a function of number of examples. The probability of conve</context>
</contexts>
<marker>Gibson, Wexler, 1994</marker>
<rawString>Gibson, Edward and Wexler, Kenneth (1994). &amp;quot;Triggers.&amp;quot; Linguistic Inquiry, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Gold</author>
</authors>
<title>Language Identification in the Limit.&amp;quot;</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>4</issue>
<pages>447--474</pages>
<contexts>
<context position="2332" citStr="Gold (1967)" startWordPosition="332" endWordPosition="333">y undeveloped area of language learning theory. The current paper aims to fill that gap. We choose as a starting point the GW Triggering Learning Algorithm (TLA). Our central result is that the performance of this algorithm and others like it is completely modeled by a Markov chain. We explore the basic computational consequences of this, including some surprising results about sample complexity and convergence time, the dominance of random walk over gradient ascent, and the applicability of these results to actual child language acquisition and possibly language change. Background. Following Gold (1967) the basic framework is that of identification in the limit. We assume some familiarity with Gold&apos;s assumptions. The learner receives an (infinite) sequence of (positive) example sentences from some target language. After each, the learner either (i) stays in the same state; or (ii) moves to a new state (change its parameter settings). If after some finite number of examples the learner converges to the correct target language and never changes its guess, then it has correctly identified the target language in the limit; otherwise, it fails. In the GW model (and others) the learner obeys two a</context>
</contexts>
<marker>Gold, 1967</marker>
<rawString>Gold, E.M. (1967). &amp;quot;Language Identification in the Limit.&amp;quot; Information and Control, 10(4): 447-474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Isaacson</author>
<author>John Masden</author>
</authors>
<title>Markov Chains.</title>
<date>1976</date>
<publisher>John Wiley.</publisher>
<location>New York:</location>
<marker>Isaacson, Masden, 1976</marker>
<rawString>Isaacson, David and Masden, John (1976). Markov Chains. New York: John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lightfoot</author>
</authors>
<title>How to Set Parameters.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="29895" citStr="Lightfoot (1990)" startWordPosition="5102" endWordPosition="5103"> that the TLA actually boosts diachronic change by orders of magnitude, since as observed earlier, it can permit the learner to arrive at a different convergent state even when there is just one target language emitter. In contrast, the local maxima targets are stable, and never undergo change. Whether this powerful &amp;quot;boost&amp;quot; effect plays a role in diachronic change remains a topic for future investigation. As far as we know, the possibility for formally modeling the kind of saltation indicated by the Markov model has not been noted previously and has only been vaguely stated by authors such as Lightfoot (1990). In conclusion, by introducing a formal mathematical model for language acquisition, we can provide rigorous results on parameter learning, algorithmic variation, sample complexity, and diachronic syntax change. These results are of interest for corpus-based acquisition and investigations of child acquisition, as well as pointing the way to a more rigorous bridge between modern computational learning theory and computational linguistics. ACKNOWLEDGMENTS We would like to thank Ken Wexler, Ted Gibson, and an anonymous ACL reviewer for valuable discussions and comments on this work. Dr. Leonardo</context>
</contexts>
<marker>Lightfoot, 1990</marker>
<rawString>Lightfoot, David (1990). How to Set Parameters. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Wexler</author>
<author>Peter Culicover</author>
</authors>
<title>Formal Principles of Language Acquisition.</title>
<date>1980</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="25548" citStr="Wexler and Culicover (1980)" startWordPosition="4372" endWordPosition="4375">n &amp;quot;ordinary&amp;quot; text (e.g., the LOB): 25,890 questions, and 11, 775 wh-questions; 201 and 99 in the German corpus; but only 2,506 questions or 3.7% out of 53,495 LOB sentences. To test convergence, an implemented system using a newer version of deMarcken&apos;s partial parser (see deMarcken, 1990) analyzed each degree-0 or degree-1 sentence as falling into one of the input patterns SVO, S Aux V, etc., as appropriate for the target language. Sentences not parsable into these patterns were discarded (presumably &amp;quot;too complex&amp;quot; in some sense following a tradition established by many other researchers; see Wexler and Culicover (1980) for details). Some examples of caretaker inputs follow: this is a book ? what do you see in the book? how many rabbits? what is the rabbit doing? (...) is he hopping? oh . and what is he playing with? red mir doch nicht alles nach ! ja , die schwatzen auch immer alles nach (...) When run through the TLA, we discover that tcionevergence falls roughly along the TLA convergence m displayed in figure 1—roughly 100 examples to asymptote. Thus, the feasibility of the basic model is confirmed by actual caretaker input, at least in this simple case, for both English and German. We are continuing to e</context>
</contexts>
<marker>Wexler, Culicover, 1980</marker>
<rawString>Wexler, Kenneth and Culicover, Peter (1980). Formal Principles of Language Acquisition. Cambridge, MA: MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>