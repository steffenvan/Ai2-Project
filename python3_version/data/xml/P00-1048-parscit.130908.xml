<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.768950666666667">
Hidden Markov Model-Based Korean Part-of-Speech Tagging
Considering High Agglutinativity, Word-Spacing, and Lexical
Correlativity
</note>
<author confidence="0.946402">
Sang-Zoo Lee and Jun-ichi Tsujii
</author>
<affiliation confidence="0.997873">
Department of Information Science
University of Tokyo
</affiliation>
<address confidence="0.705116333333333">
Hongo 7-3-1, Bunkyo-ku
Tokyo 113-0033, Japan
{ lee,tsujii} ©is. s. u-tokyo. ac. jp
</address>
<sectionHeader confidence="0.963924" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99971537037037">
In this paper we present hidden
Markov models for Korean part-of-
speech tagging, which consider Ko-
rean characteristics such as high
agglutinativity, word-spacing, and
high lexical correlativity. In order
ot consider rich information in con-
texts, the models adopt a less strict
Markov assumption. In the models,
sparse-data problem is very serious
and their parameters tend to be esti-
mated unreliably because they have
a large number of parameters. To
overcome sparse-data problem, our
model uses a simplified version of
the well-known back-off smoothing
method. To mitigate unreliable esti-
mation problem, our models assume
joint independence instead of con-
ditional independence because joint
probabilities have the same degree of
estimation reliability. Experimental
results show that models with rich
contexts perform even better than
standard HMMs and that joint in-
dependent assumption is effective in
some models.
</bodyText>
<sectionHeader confidence="0.998818" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.966225">
Korean is an highly agglutinative language
which has word-spacing orthography. It
makes Korean part-of-speech (POS) tagging
different from English POS tagging. Gener-
ally English POS tagging can be regarded as
a process in which a proper POS tag is as-
signed to each word in texts. However, in Ko-
</bodyText>
<author confidence="0.891323">
Hae-Chang Rim
</author>
<affiliation confidence="0.996983">
Department of Computer Science
Korea University
</affiliation>
<address confidence="0.8894025">
1 5-Ga Anam-Dong, Seongbuk-Gu
Seoul 136-701, Korea
</address>
<email confidence="0.987574">
rim@nlp.korea.ac.kr
</email>
<bodyText confidence="0.99993043902439">
rean POS tagging, each word is tagged with a
proper combination of categories and lexical
forms of morphemes(Lee et al., 1999) because
Korean words can be freely formed by aggluti-
nating morphemes and so the number of cate-
gories of Korean words can be (theoretically)
infinite.
Over a decade, many works for Korean
POS tagging have used a wide range of ma-
chine learning techniques such as a hidden
Markov model (HMM) (Lee, 1995) (Kim et
al., 1998), a maximum entropy model (Kang,
1998), transformation rules (Lim, 1997), a de-
cision tree (Lee et al., 1999), discriminative
learning (Kim et al., 1995), a fuzzy net (Kim
et al., 1993), a neural network (Lee, 1994),
and so on.
In this paper we propose hidden Markov
models for Korean POS tagging, which adopt
a less strict Markov assumption(Cinlar, 1975)
to consider rich contexts and which consider
Korean characteristics such as high agglutina-
tivity, word-spacing, and high lexical correla-
tivity. In the models, sparse-data problem is
very serious because they have a large num-
ber of parameters. To overcome sparse-data
problem, our model uses a simplified version
of the well-known back-off smoothing method.
If the parameters are very specific like lexi-
calized ones, they tend to have very different
estimation reliability, making the Markov as-
sumption implausible. To mitigate this prob-
lem, our models assume joint independence
between random variables instead of condi-
tional independence because joint probabili-
ties have the same degree of estimation reli-
ability. Experimental results for the KUNLP
corpus (Lee et al., 1999) show that models
with rich contexts perform even better than
standard HMMs and that joint independent
assumption is effective in some models.
</bodyText>
<sectionHeader confidence="0.825975" genericHeader="method">
2 Lexical correlativity of Korean
</sectionHeader>
<bodyText confidence="0.985405230769231">
In Korean, the same word form can be made
from different morpheme sequences with the
same tag sequence. For instance, a word
form Na-New n can correspond to two differ-
ent morpheme sequences with the same tag
sequence, NalV(=to sprout)+Neun/E(=case
marker) and NagV(=to fiy)+Neun/El. We
call this ambiguity &amp;quot;homo-categorial&amp;quot; ambi-
guity.
Usually homo-categorial ambiguity is not
easy to resolve without consulting lexical in-
formation in contexts. For example, Na-Neun
is tagged with Na1V+NeunIE in &amp;quot;SSag-i Na-
</bodyText>
<construct confidence="0.722660666666667">
Neun Jung-i-Da (= A bud is sprouting)&amp;quot; and
with Nal1V+NeunIE in &amp;quot;Sae-Ga Na-Neun
Jung-i-Da (= A bird is flying)&amp;quot;. Because
</construct>
<bodyText confidence="0.995217533333333">
these sentences have the same tag context
&amp;quot;N±P V±E N±I±E&amp;quot;2, they cannot be dis-
criminated by considering only POS tag in-
formation in contexts. Moreover, although
two lexical probabilities, Pr(Na I V) and
Pr(Nal I V), are considered, the word can
not be correctly tagged since the tag with
larger probability is always selected in both
sentences.
However, such ambiguity can be resolved
by referring lexical relations in contexts. For
example, Na-New n can be correctly tagged if
we consider lexical relations between SSag-Gi
and Na-Neun and between Sae-Ga and Na-
Neun.
</bodyText>
<sectionHeader confidence="0.9665765" genericHeader="method">
3 HMM-based Korean POS
tagging
</sectionHeader>
<bodyText confidence="0.995768714285714">
Figure 1 shows a morpheme-unit lattice struc-
ture of a Korean sentence, &amp;quot;Neo-New n Hal Su
iss-Da.&amp;quot; , where each node has a morpheme
and its POS tag and where the sequence con-
nected by bold lines indicates the most likely
sequence. Because Korean has word-spacing
orthography, transitions between nodes can
</bodyText>
<footnote confidence="0.941723333333333">
1 V denotes a verbal stem, and E a verbal ending.
2N denotes a noun, P a postposition, and la cop-
ula.
</footnote>
<equation confidence="0.877043">
$/$
Neo/NNP NeolIVV
Neun/PX Neun/EFD
Ha//NNCG Ha//NNBU Ha/VV Ha/VX
Su/NN ....
issIVJ i88/VX
Da/E*FF----
./SS.
$/$
A morpheme-unit lattice
&amp;quot;NeoNeun Hal Su issDa.&amp;quot;(= You can do it.)
</equation>
<bodyText confidence="0.9997786">
be distinguished by a word boundary. Tran-
sitions across a word boundary, which are de-
picted by a solid line, are distinguished from
transitions within a word, which are depicted
by a dotted line.
</bodyText>
<subsectionHeader confidence="0.999449">
3.1 Standard word-unit model
</subsectionHeader>
<bodyText confidence="0.996588454545455">
We basically follow the notation of (Char-
niak et al., 1993) to describe Bayesian
models. In this paper, we assume that
{w1, w2, , ww} is a set of words,
{ t1, t2, , CI is a set of POS tags,
a sequence of random variables =
W1 W2 . . . Wn is a sentence of n words,
a sequence of random variables T1,n =
T1 T2 . . . Tn is a sequence of n word cat-
egories. Because each of random variables
W can take as its value any of the words
in the vocabulary, we denote the value of W,
by w, and a particular sequence of values for
W3 (i &lt; j) by w3. In a similar way, we
denote the value of T, by t, and a particu-
lar sequence of values for T% (i &lt; j) by 4,3.
For generality, terms w3 and 4,3 (i &gt; ,j) are
defined as being empty.
The purpose of Bayesian models for POS
tagging is to find the most likely sequence of
POS tags for a given sequence of words, as
follows:
</bodyText>
<figureCaption confidence="0.918133">
Figure 1:
</figureCaption>
<bodyText confidence="0.458444">
of
</bodyText>
<equation confidence="0.996425666666667">
- argmaxPr(Ti.,,,, = ti,nI Wi, = wi,n) (1)
- argmaxPr(ti,,,,,I wl,n) (2)
Pr(tl,n, Wl,n)
= argmax
Pr(wl,n)
- argmax wl,n) (3)
</equation>
<bodyText confidence="0.997629333333333">
Eqn. 1 becomes Eqn. 2 because reference to
the random variables themselves can be omit-
ted. Eqn. 2 is then transformed into Eqn. 3
since Pr(wi,n) is constant for all ti,n.
Then, the probability Pr(ti,n, wi,n) is bro-
ken down into Eqn. 4 by using the chain rule.
</bodyText>
<equation confidence="0.9744322">
Pr(tl,n, Wl,n) — x Pr(uh t1,, w1,-1)
H Pr(t I ti.,-,-1)
1,w1 (4)
i=1
However, it is either implausible or impossible
</equation>
<bodyText confidence="0.893477142857143">
to compute Pr(t, wi,,_1) and Pr(w, I
wi,,_1) in Eqn. 4.
The standard HMM simplifies them by
making the following two strict Markov as-
sumption (conditional independence), Eqn. 5
and Eqn. 6, to get a more tractable form,
Eqn. 7.
</bodyText>
<equation confidence="0.99956375">
Pr(t• Pr(t I ti-K,i-1) (5)
Pr(wi Pr(w I ti) (6)
Pr(ti,n,wi,n) P.--; H x Pr(wi I ti)
( Pr(ti I ti-K,i-1) (7)
</equation>
<bodyText confidence="0.925299947368421">
The standard HMM assumes that the proba-
bility of a current tag t, conditionally depends
on only the previous K tags and that
the probability of a current word w, condi-
tionally depends on only the current tag3.
Generally, the standard HMM has a limita-
tion that it can not solve complicated ambi-
guities because it does not consider rich con-
texts. To overcome this limitation, the stan-
dard HMM should be extended so that it can
consult rich information in contexts.
Moreover, the standard word-unit model
can not be used effectively for tagging highly
agglutinative languages like Korean. There-
fore, the word-unit model should be trans-
formed into a morpheme-unit model.
3Usually, K is determined as 1 (bigram as in (Char-
niak et al., 1993)) or 2 (trigram as in (Merialdo,
1991)).
</bodyText>
<subsectionHeader confidence="0.982498">
3.2 Extended morpheme-unit model
</subsectionHeader>
<bodyText confidence="0.99993375">
Bayesian models for morpheme-unit tagging
find the most likely sequence of morphemes
and corresponding tags for a given sequence
of words, as follows:
</bodyText>
<equation confidence="0.977382333333333">
T(wi,n) = argmax Pr(ci,u, rni,uI wi,n) (8)
argmax Pr(ci,u, P2,u mi,) (9)
c1, ,m
</equation>
<bodyText confidence="0.9999091875">
In the above equations, u(&gt; n) denotes the
number of morphemes in a sequence corre-
sponding the given word sequence, c denotes a
morpheme-unit tag, m denotes a morpheme,
and p denotes a type of transition from the
previous tag to the current tag. p can have
one of two values, &amp;quot;#&amp;quot; denoting a transition
across a word boundary and &amp;quot;+&amp;quot; denoting a
transition within a word. Because it is diffi-
cult to calculate Eqn. 8, the word sequence
term wi,n is usually ignored as in Eqn. 9. In-
stead, we introduce p in Eqn. 9 to consider
word-spacing4.
The probability Pr(ci,u,p2,u, mi,) is also
broken down into Eqn. 10 by using the chain
rule.
</bodyText>
<equation confidence="0.93313975">
Pr(C1,u,P2,u,n11,u)
= H x Pr(
j=1
u Pr(ci,Pi
</equation>
<bodyText confidence="0.983577857142857">
ni, (10)
Because Eqn. 10 is not easy to compute, it is
simplified by making a Markov assumption to
get a more tractable form.
An extended HMM for morpheme-unit tag-
ging can be defined by making a less strict
Markov assumption, as follows:
</bodyText>
<equation confidence="0.9936145">
A(C[s](K,J), [s](LJ)) Pr(c1,u,P2,u, mi,)
H Pr(Ci[, pi] I )
Ci-K,i-1[,Pi-K +1,i-1],
i=1 X Pr(m I ci—LAPi—L+1,i], mi—i,i—i)
</equation>
<bodyText confidence="0.93842">
In a model A(C[s](K,J), [cL,i)), the proba-
bility of the current morpheme tag c, condi-
tionally depends on both the previous K tags
</bodyText>
<footnote confidence="0.9433465">
4Most previous HMM-based Korean taggers except
(Kim et al., 1998) did not consider word-spacing.
</footnote>
<bodyText confidence="0.999877833333333">
(optionally, the types of their tran-
sition p,_K+1,,_1) and the previous J mor-
phemes m,_ and the probability of the
current morpheme n1,,, conditionally depends
on the current tag and the previous L tags
c,_4, (optionally, the types of their transi-
tion pi-L+1,%) and the previous I morphemes
m,_/,,_1. In experiments, we set K as 1 or
2, J as 0 or K, L as 1 or 2, and / as 0 or
L. If J and I are zero, the above models are
non-lexicalized models. Otherwise, they are
lexicalized models.
For example, the extended model
A (C,(2,2) , M(2,2)), where word-spacing is
considered only in the tag probabilities, cal-
culate the probability of a node &amp;quot;Su/NNBG&amp;quot;
of the most likely sequence in Figure 1 as
follows:
</bodyText>
<equation confidence="0.5821655">
Pr(NNBG,#IVV,EFD,±,Ha,l)
x Pr(Su I VV, EFD,NNBG,Ha,l)
</equation>
<sectionHeader confidence="0.529208" genericHeader="method">
4 Parameter estimation
</sectionHeader>
<bodyText confidence="0.999926125">
The extended models have a large number
of parameters, as compared to the standard
models. Therefore, they must suffer from
both sparse-data problem and unreliable es-
timation problem. The models adopt a sim-
plified back-off smoothing technique as a so-
lution to the first problem, and joint indepen-
dence assumption as a solution to the second.
</bodyText>
<subsectionHeader confidence="0.989846">
4.1 Simplified back-off smoothing
</subsectionHeader>
<bodyText confidence="0.999012">
In supervised learning, the simpliest pa-
rameter estimation is the maximum like-
lihood(ML) estimation(Duda et al., 1973)
which maximizes the probability of a train-
ing set. The ML estimate of morpheme tag
(K+1)-gram probability, PrML(c I
is calculated as follows:
</bodyText>
<equation confidence="0.988808333333333">
Fq(c,_K,,)
Pr (c I Ci_K,i_i) = (12)
ML r
</equation>
<bodyText confidence="0.993968133333333">
where the function Fq(x) returns the fre-
quency of x in the training set. When using
the ML estimation, data sparseness is even
more serious in the extended models than in
the standard models because the former has
even more parameters than the latter.
In (Chen, 1996), where various smoothing
techniques was tested for a language model by
using the perplexity measure, it was reported
that the back-off smoothing method(Katz,
1987) performs better on a small traning set
than other methods. In the back-off smooth-
ing, the smoothed probability of tag (K+1)-
gram Pr sBo (c%I cz_K,%_1) is calculated as fol-
lows:
</bodyText>
<equation confidence="0.786419">
Pr (c I c,—K,i—i) —
SBO
{dr PrmL (ci I ci—K,i—i) if r &gt;0
a(ci_i-c,i—i)PrsBo(c, ci—K+1,i-1)if r = 0
where r = Fq(ci—K,i), r* = ± 1) nr+1
nr
r* (r+i)xnr+i
</equation>
<bodyText confidence="0.999929733333333">
In the equation above, nr denotes the num-
ber of (K+1)-gram whose frequency is r,
and the coefficient dr is called the discount
ratio, which reflects the Good-Turing es-
timate(Good, 1953)5. Eqn. 13 says that
PrsBo(c I c,_K,%_1) is under-estimated by
dr than its maximum likelihood estimate, if
r &gt; 0, or is backed off by its smoothing term
PrsBo(c I cz—K+1,%-1) in proportion to the
value of the function a(c101) of its condi-
tional term CiK,il, if r = 0.
However, because Eqn. 13 requires compli-
cated computation in a(q_10_1), we simplify
it to get a function of the frequency of a con-
ditional term, as follows:
</bodyText>
<equation confidence="0.989191">
a(Fq(ci_K,i-i) = f) =
x E[Fq(ci_K,i-i) =
A
ET=0 = f]
</equation>
<bodyText confidence="0.97381875">
where
In Eqn. 14, the range of f is bucketed into 7
regions such as f = 0, 1, 2, 3, 4, 5 and f &gt; 6
In (Katz, 1987) dr = 1 if r &gt; 5.
</bodyText>
<equation confidence="0.994004555555556">
1 (r+1)Xnr+i
711
dr =
(14)
A = 1 Eci_K,j,r &gt;0 PrmL (ci I ci—K,i— 1)
E[Fq(ci—K,i—i) = f] =
Pr (c,
SBO
(13)
</equation>
<bodyText confidence="0.995157571428571">
since it is also difficult to compute this equa-
tion for all possible values of f.
In the formalism of the simplified back-
off smoothing, each probability whose ML
estimate is zero is backed off by its cor-
responding smoothing term. In experi-
ments, the smoothing term of Pr sBo (c%[, Pd
</bodyText>
<equation confidence="0.758750333333333">
Ci-K,i-1[, Pi- K+1,i-1], is determined
as follows:
PrsBo(ci[, Pi] I
Pr SBO(ci[, Pi] I
PrSBO(ci[, Pi] I
PrAD (Ci)
</equation>
<bodyText confidence="0.938846625">
The smoothing term of Pr SBO(m%
rni—i,i— 1) is determined
as follows:
In the equations above, the unigram proba-
bilities are calculated by using the additive
smoothing with 6 = 10-2, which is chosen
through experiments. The equation for the
additive smoothing(Chen, 1996) is as follows:
</bodyText>
<equation confidence="0.999663">
AD (Fq(c,_K,,) + 6)
Pr(c, = Fq(c,_K,,) + 6
</equation>
<subsectionHeader confidence="0.75775">
4.2 Joint independence
</subsectionHeader>
<bodyText confidence="0.999990428571429">
The parameters of an HMM may have differ-
ent degree of statistical reliability because pa-
rameter reliability depends on the frequency
of conditional term. For example, let a corpus
consist of 1 million words and let the follow-
ing parameters be extracted from the corpus
by using the maximum likelihood estimation.
</bodyText>
<equation confidence="0.970516">
Pr(a) = 0.01 Pr(d I a) = 0.1
Pr(b) = 0.001 Pr(d I b) = 0.1
Pr(c) = 0.0001 Pr(d I c) = 0.1
</equation>
<bodyText confidence="0.999471416666667">
In this case, three conditional probabilities,
Pr(d I a), Pr(d I b), and Pr(d I c) are all 0.1
but Pr(d I a) is statistically more reliable than
others because its sample size (10,000 words
= 1 million x Pr(a)) is bigger than others. Ac-
tually, this problem becomes very serious in
extended models, even though parameters of
the models are seen in the training corpus.
To consider such statistical reliability of a
probability estimate, we introduce the con-
cept of weighting Markov assumption, as fol-
lows:
</bodyText>
<equation confidence="0.99896">
Pr(Ci Ci,i_1,7711,i_i)
Pr(Ci Ci-K ,i-1, rni-J,i-1) (15)
X W (Ci-K,i-1,rni-J,i-1)
Pr(771i I
Pr(m, I ci—L,i,mi—I,i—i) (16)
x
</equation>
<bodyText confidence="0.99980575">
If the probability function, Pr, is used as
the weight function, W, the equations above
become equations assuming joint indepen-
dence between random variables as follows:
</bodyText>
<equation confidence="0.97847825">
Pr(c, I ci,,_1,777,1,,_1)
Pr(c,,
Pr(771i I
Pr(m,, ci—L,i,
</equation>
<bodyText confidence="0.99998675">
The equations above assume that the proba-
bility of the current morpheme tag c, jointly
depends on both the previous K tags
and the previous J words m,_ and that
the probability of the current word m jointly
depends on the current tag and the previous L
tags c,_4, and the previous I words m,_/,,_1.
If a Bayesian model assumes joint indepen-
dence, we call it a joint independence model
(JIM).
Actually, using the probability function as
the weight function is mathematically incor-
rect and implausible. For example, while the
sum of probabilities of all sentences with the
same length becomes 1.0 in an HMM, it be-
comes naturally less than 1.0 in a JIM. There-
fore, JIMs should not be used in calculating
the probability of a sentence. However, if we
want to find the most likely sequence for each
sentence and the joint probability of each pa-
</bodyText>
<equation confidence="0.923750875">
if K &gt; 1,
J &gt; 1
if K &gt; 1,
J = 1
K&gt; 1,
J = 0
K = 1,
J = 0
</equation>
<figure confidence="0.947714357142857">
if
if
ci-L+1,i
PrSBO(rni )
rni-I+1,i-1
Ci-L,i
Pr SBO(nli I r )
Ci-L+1,i )
[, Pti-L+2,i]
if L &gt;1,1&gt;1
if L &gt; 1,1 = 1
if L &gt; 1,1 = 0
if L = 0,1 = 0
PrSBO(m% I
</figure>
<equation confidence="0.7103195">
Pr AD(7711)
d)(C[s](K,J), [s](L,/,) pr(ci,u,p2,u,mi,u)
H Prcei[,pi],
rni-J, )
i-1 (19)
i=1 x Pr(mi , ci—L,i [, Pi—L+1,i], rni—i,i— 1)
</equation>
<bodyText confidence="0.9944366">
rameter is regarded as a score, JIMs work
well.
By replacing corresponding parameters, an
extended morpheme-unit HMM can be trans-
formed into the corresponding JIM, which is
defined as follows:
In the extended JIM, (C8(2,2), M(2,2)), the
probability of a node &amp;quot;Su/NNBG&amp;quot; of the most
likely sequence in Figure 1 is calculated as
follows:
</bodyText>
<equation confidence="0.9099965">
Pr(NNBG,#,VV,EFD,±, Ha, 1)
x Pr(Su, VV, EFD,NNBG, Ha, 1)
</equation>
<bodyText confidence="0.999889666666667">
The parameters of a JIM are estimated by
using the parameters of the corresponding
HMM as follows:
</bodyText>
<sectionHeader confidence="0.998841" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999411829787234">
In experiments, we used the KUNLP corpus
which consists of 167,115 words and 15,211
sentences and is tagged with 65 POS tags. It
was segmented into two parts, the training
set of 90% and the test set of 10%, in the
way that each sentence in the test set was
extracted from every 10 sentence. In the same
way, we made 10-fold data set for 10-fold cross
validation.
In order to morphologically analyze each
word, we used the Korean morphological ana-
lyzer (Lee, 1999) which is consistent with the
KUNLP corpus. By using the morphologi-
cal analyzer, the average number of possible
analyses per word becomes 3.41.
Figure 2-5 illustrate graphs showing the
average accuracy rates of HMMs and JIMs,
without considering word-spacing, with con-
sidering word-spacing only in the lexical prob-
abilities, with considering word-spacing only
in the tag probabilities, and with considering
word-spacing in both the tag and lexical prob-
abilities, respectively. Here, labels in x-axis
specify models in the way that KI:}1 denotes
A(C[s](K,J), M[s](L,I)) or d)(C[8](K,J), MH(L,I)).
The models are arranged by the ascending or-
der of theoretical number of parameters. The
first two models are standard models and the
others are extended models. The average ac-
curacy rates beyond the range of each graph
are intentionally omitted.
In these figures, we can observe that the
simplified back-off smoothing technique mit-
igates sparse-data problems in both HMMs
and JIMs. As expected, JIMs achieves
higher accuracy than the corresponding
HMMs in some extended models consult-
ing rich contexts. Consulting word-spacing
makes slight improvement in some of both
HMMs and JIMs. It is statistically signifi-
cant with confidence 99that the best model,
A(C8(2,2), M8(2,2)) (96.97%), is better than
any other models including the previous stan-
dard model A (C(1,0) , M(0,0)) (94.95%) (Lee,
1995), the previous model A (C8(1,0), M(0,0))
(94.96%) (Kim et al., 1998), and the best
JIM, A(C(1,1), 8(1,1)) (96.95%).
</bodyText>
<sectionHeader confidence="0.999268" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99994675">
We have presented the extended HMMs for
Korean POS tagging, which assume joint in-
dependence between random variables, which
are based on the morpheme-unit lattice struc-
ture, and which consider word-spacing and
rich information in contexts. In the models,
a simplified version of back-off smoothing is
used to mitigate data sparseness problem.
From the experimental results, we have ob-
served that extended models achieved even
better results than the standard models in
case of both HMMs and JIMs, that the simpli-
</bodyText>
<equation confidence="0.707423142857143">
X PrAD(
rni-J,i-i
Pr(mi, =
Pr(mi I ci—LA,Pi—L+1,i], rni—i,i—i)
x Pr(ci—L,i [,Pi—L+1,i], mi—i,i—i)
Fq(ci_K,i) +
) _
</equation>
<table confidence="0.672342375">
PrsBo(CiL PiJ, mi_ j,i—i
,
PrsB0(cd, Ai I mi_
)
Pr (ci_K,i) —
AD (Fq(ci—K,i) + 5)
10 2,0 1,0 2,0 1,1 1,1 1,0 2,0 1,1 1,0 2,0 1,1 2,2 2,2 2,2 2,2 1,0 2,0 1,1 2,2
00 0,0 1,0 1,0 0,0 1,0 2,0 2,0 2,0 1,1 1,1 1,1 0,0 1,0 2,0 1,1 2,2 2,2 2,2 22
</table>
<figureCaption confidence="0.792701">
Figure 2: Without considering word-spacing
</figureCaption>
<figure confidence="0.723783">
1 0 2,0 1,0 2,0 1,1 1,1 1,0 2,0 1,1 1,0 2,0 1,1 2,2 2,2 2,2 2,2 1,0 2,0 1,1 2,2
00 0,0 1,0 1,0 0,0 1,0 2,0 2,0 2,0 1,1 1,1 1,1 0,0 1,0 2,0 1,1 2,2 2,2 2,2 22
</figure>
<figureCaption confidence="0.992947">
Figure 3: With considering word-spacing only in the lexical probabilities
</figureCaption>
<figure confidence="0.99727575">
97.0
HMM .x. -
JIM -E-
96.5
96.0
95.5
95.0
97,0
96.5
96.0
95.5
95.0
</figure>
<bodyText confidence="0.988445384615385">
fled back-off smoothing technique mitigated
data sparseness quite effectively, that consult-
ing word-spacing made slight improvement of
accuracy, and that some extended JIMs out-
performed the corresponding HMMs.
Now, we are implementing and evaluat-
ing various smoothing techniques in order to
find more effective smoothing technique for
HMM/JIM-based Korean POS tagging. And
also, we are trying to apply JIMs to different
areas such as information extraction in the
bio-molecular domain, noun phrase chunck-
ing, and so on.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.94226262962963">
E. Charniak, C. Hendrickson, N. Jacobson, and
M. Perkowitz. 1993. Equations for Part-of-
Speech Tagging. In Proc. of the 11th Nat&apos;l
Conf. on Artificial Intelligence(AAAI-93), 784-
789.
for Natural Language. Doctoral Dissertation,
Harvard University, USA.
E. Cinlar. 1975. Introduction to Stochastic Pro-
cesses. Prentice-Hall, New Jersey.
R. 0. Duda and R. E. Hart. 1973. Pattern Clas-
sification and Scene Analysis. John Wiley.
I. J. Good. 1953. &amp;quot;The Population Frequencies of
Species and the Estimation of Population Pa-
rameters,&amp;quot; In Biometrika, 40(3-4):237-264.
S. M. Katz. 1987. Estimation of Probabilities
from Sparse Data for the Language Model Com-
ponent of a Speech Recognizer. In IEEE Trans-
actions on Acoustics, Speech and Signal Pro-
cessing(ASSP), 35(3) :400-401.
I.-H. Kang, J.-H. Kim, and G.-C. Kim. 1998. Ko-
rean Part-of-Speech Tagging Using Maximum
Entropy Model. In Proc. of the lath National
Conference on Korean Information Processing,
9-14.
J.-H. Kim, et al. 1993. Korean Part-of-Speech
Tagging by Using a Fuzzy net. In Proc. of the
S. F. Chen. 1996. Building Probabilistic Models
</reference>
<figure confidence="0.8087645">
1 0 2,0 1,0 2,0 1,1 1,1 1,0 2,0 1,1 1,0 2,0 1,1 2,2 2,2 2,2 2,2 1,0 2,0 1,1 2,2
00 0,0 1,0 1,0 0,0 1,0 2,0 2,0 2,0 1,1 1,1 1,1 0,0 1,0 2,0 1,1 2,2 2,2 2,2 22
</figure>
<figureCaption confidence="0.991841">
Figure 4: With considering word-spacing only in the tag probabilities
</figureCaption>
<figure confidence="0.968846">
1 0 2,0 1,0 2,0 1,1 1,1 1,0 2,0 1,1 1,0 2,0 1,1 2,2 2,2 2,2 2,2 1,0 2,0 1,1 2,2
00 0,0 1,0 1,0 0,0 1,0 2,0 2,0 2,0 1,1 1,1 1,1 0,0 1,0 2,0 1,1 2,2 2,2 2,2 22
</figure>
<figureCaption confidence="0.995406">
Figure 5: With considering word-spacing in both the tag and lexical probabilities
</figureCaption>
<figure confidence="0.998846">
HMM .x. - x
JIM -El-
97.0
HMM .x. - x
JIM -El-
96.5
96.0
95.5
95.0
97,0
96.5
96.0
95.5
95.0
</figure>
<reference confidence="0.993215111111111">
5th National Conference on Korean Informa-
tion Processing, 593-603.
J.-H. Kim and G.-C. Kim. 1995. Discriminative
Learning in Part-of-Speech Tagging. In Proc.
of the National Conference on Korean Infor-
mation Science Society, Spring, 627-630.
J.-D. Kim, S.-Z. Lee, and Rim. 1998. A
Morpheme-Unit POS Tagging Model Consid-
ering Word-Spacing. In Proc. of the 10th Na-
tional Conference on Korean Information Pro-
cessing, 3-8.
S.-J. Lee. 1994. Prediction and Disambiguation
of Korean Word Category by Using a Neural
Network. Doctoral Dissertation, Seoul National
University, Korea.
S.-H. Lee. 1995. Korean POS Tagging System
Considering Unknown Words. Master Thesis,
Korea Advanced Institute of Science and Tech-
nology(KAIST), Korea.
S.-Z. Lee, J.-D. Kim, W.-H. Ryu, and W-
C. Rim. 1999. A Part-of-Speech Tagging
Model Using Lexical Rules Based on Corpus
Statistics. In Proc. of the International Con-
ference on Computer Processing of Oriental
Languages (ICCPOL- 99), 385-390.
S.-Z. Lee. 1999. New Statistical Models for Au-
tomatic POS Tagging. Doctoral Dissertation,
Korea University, Korea.
Lim. 1997. Korean Part-of-Speech Tagging
by Using Lingusitic Knowledge and Statistical
Information. Doctoral Dissertation, Korea Uni-
versity, Korea.
B. Merialdo. 1991. Tagging Text with a Prob-
abilistic Model. In Proc. of the International
Conference on Acoustic, Speech and Signal
Processing (I CA S SP- 91), 809-812.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.444040">
<title confidence="0.989355333333333">Hidden Markov Model-Based Korean Part-of-Speech Tagging Considering High Agglutinativity, Word-Spacing, and Lexical Correlativity</title>
<author confidence="0.999759">Sang-Zoo Lee</author>
<author confidence="0.999759">Jun-ichi Tsujii</author>
<affiliation confidence="0.9999045">Department of Information Science University of Tokyo</affiliation>
<address confidence="0.636895">Hongo 7-3-1, Bunkyo-ku Tokyo 113-0033, Japan</address>
<email confidence="0.528025">lee©is.s.u-tokyo.ac.jp</email>
<email confidence="0.528025">tsujii©is.s.u-tokyo.ac.jp</email>
<abstract confidence="0.999607107142857">In this paper we present hidden Markov models for Korean part-ofspeech tagging, which consider Korean characteristics such as high agglutinativity, word-spacing, and high lexical correlativity. In order ot consider rich information in contexts, the models adopt a less strict Markov assumption. In the models, sparse-data problem is very serious and their parameters tend to be estimated unreliably because they have a large number of parameters. To overcome sparse-data problem, our model uses a simplified version of the well-known back-off smoothing method. To mitigate unreliable estimation problem, our models assume joint independence instead of conditional independence because joint probabilities have the same degree of estimation reliability. Experimental results show that models with rich contexts perform even better than standard HMMs and that joint independent assumption is effective in some models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>C Hendrickson</author>
<author>N Jacobson</author>
<author>M Perkowitz</author>
</authors>
<title>Equations for Part-ofSpeech Tagging.</title>
<date>1993</date>
<booktitle>In Proc. of the 11th Nat&apos;l Conf. on Artificial Intelligence(AAAI-93),</booktitle>
<pages>784--789</pages>
<contexts>
<context position="5515" citStr="Charniak et al., 1993" startWordPosition="860" endWordPosition="864">word-spacing orthography, transitions between nodes can 1 V denotes a verbal stem, and E a verbal ending. 2N denotes a noun, P a postposition, and la copula. $/$ Neo/NNP NeolIVV Neun/PX Neun/EFD Ha//NNCG Ha//NNBU Ha/VV Ha/VX Su/NN .... issIVJ i88/VX Da/E*FF---- ./SS. $/$ A morpheme-unit lattice &amp;quot;NeoNeun Hal Su issDa.&amp;quot;(= You can do it.) be distinguished by a word boundary. Transitions across a word boundary, which are depicted by a solid line, are distinguished from transitions within a word, which are depicted by a dotted line. 3.1 Standard word-unit model We basically follow the notation of (Charniak et al., 1993) to describe Bayesian models. In this paper, we assume that {w1, w2, , ww} is a set of words, { t1, t2, , CI is a set of POS tags, a sequence of random variables = W1 W2 . . . Wn is a sentence of n words, a sequence of random variables T1,n = T1 T2 . . . Tn is a sequence of n word categories. Because each of random variables W can take as its value any of the words in the vocabulary, we denote the value of W, by w, and a particular sequence of values for W3 (i &lt; j) by w3. In a similar way, we denote the value of T, by t, and a particular sequence of values for T% (i &lt; j) by 4,3. For generality</context>
<context position="7894" citStr="Charniak et al., 1993" startWordPosition="1315" endWordPosition="1319">gs and that the probability of a current word w, conditionally depends on only the current tag3. Generally, the standard HMM has a limitation that it can not solve complicated ambiguities because it does not consider rich contexts. To overcome this limitation, the standard HMM should be extended so that it can consult rich information in contexts. Moreover, the standard word-unit model can not be used effectively for tagging highly agglutinative languages like Korean. Therefore, the word-unit model should be transformed into a morpheme-unit model. 3Usually, K is determined as 1 (bigram as in (Charniak et al., 1993)) or 2 (trigram as in (Merialdo, 1991)). 3.2 Extended morpheme-unit model Bayesian models for morpheme-unit tagging find the most likely sequence of morphemes and corresponding tags for a given sequence of words, as follows: T(wi,n) = argmax Pr(ci,u, rni,uI wi,n) (8) argmax Pr(ci,u, P2,u mi,) (9) c1, ,m In the above equations, u(&gt; n) denotes the number of morphemes in a sequence corresponding the given word sequence, c denotes a morpheme-unit tag, m denotes a morpheme, and p denotes a type of transition from the previous tag to the current tag. p can have one of two values, &amp;quot;#&amp;quot; denoting a tran</context>
</contexts>
<marker>Charniak, Hendrickson, Jacobson, Perkowitz, 1993</marker>
<rawString>E. Charniak, C. Hendrickson, N. Jacobson, and M. Perkowitz. 1993. Equations for Part-ofSpeech Tagging. In Proc. of the 11th Nat&apos;l Conf. on Artificial Intelligence(AAAI-93), 784-789.</rawString>
</citation>
<citation valid="false">
<institution>for Natural Language. Doctoral Dissertation, Harvard University, USA.</institution>
<marker></marker>
<rawString>for Natural Language. Doctoral Dissertation, Harvard University, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Cinlar</author>
</authors>
<title>Introduction to Stochastic Processes.</title>
<date>1975</date>
<publisher>Prentice-Hall,</publisher>
<location>New Jersey.</location>
<contexts>
<context position="2467" citStr="Cinlar, 1975" startWordPosition="377" endWordPosition="378">orphemes and so the number of categories of Korean words can be (theoretically) infinite. Over a decade, many works for Korean POS tagging have used a wide range of machine learning techniques such as a hidden Markov model (HMM) (Lee, 1995) (Kim et al., 1998), a maximum entropy model (Kang, 1998), transformation rules (Lim, 1997), a decision tree (Lee et al., 1999), discriminative learning (Kim et al., 1995), a fuzzy net (Kim et al., 1993), a neural network (Lee, 1994), and so on. In this paper we propose hidden Markov models for Korean POS tagging, which adopt a less strict Markov assumption(Cinlar, 1975) to consider rich contexts and which consider Korean characteristics such as high agglutinativity, word-spacing, and high lexical correlativity. In the models, sparse-data problem is very serious because they have a large number of parameters. To overcome sparse-data problem, our model uses a simplified version of the well-known back-off smoothing method. If the parameters are very specific like lexicalized ones, they tend to have very different estimation reliability, making the Markov assumption implausible. To mitigate this problem, our models assume joint independence between random variab</context>
</contexts>
<marker>Cinlar, 1975</marker>
<rawString>E. Cinlar. 1975. Introduction to Stochastic Processes. Prentice-Hall, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duda</author>
<author>R E Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>John Wiley.</publisher>
<marker>Duda, Hart, 1973</marker>
<rawString>R. 0. Duda and R. E. Hart. 1973. Pattern Classification and Scene Analysis. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I J Good</author>
</authors>
<title>The Population Frequencies of Species and the Estimation of Population Parameters,&amp;quot;</title>
<date>1953</date>
<journal>In Biometrika,</journal>
<pages>40--3</pages>
<contexts>
<context position="11866" citStr="Good, 1953" startWordPosition="1994" endWordPosition="1996">y using the perplexity measure, it was reported that the back-off smoothing method(Katz, 1987) performs better on a small traning set than other methods. In the back-off smoothing, the smoothed probability of tag (K+1)- gram Pr sBo (c%I cz_K,%_1) is calculated as follows: Pr (c I c,—K,i—i) — SBO {dr PrmL (ci I ci—K,i—i) if r &gt;0 a(ci_i-c,i—i)PrsBo(c, ci—K+1,i-1)if r = 0 where r = Fq(ci—K,i), r* = ± 1) nr+1 nr r* (r+i)xnr+i In the equation above, nr denotes the number of (K+1)-gram whose frequency is r, and the coefficient dr is called the discount ratio, which reflects the Good-Turing estimate(Good, 1953)5. Eqn. 13 says that PrsBo(c I c,_K,%_1) is under-estimated by dr than its maximum likelihood estimate, if r &gt; 0, or is backed off by its smoothing term PrsBo(c I cz—K+1,%-1) in proportion to the value of the function a(c101) of its conditional term CiK,il, if r = 0. However, because Eqn. 13 requires complicated computation in a(q_10_1), we simplify it to get a function of the frequency of a conditional term, as follows: a(Fq(ci_K,i-i) = f) = x E[Fq(ci_K,i-i) = A ET=0 = f] where In Eqn. 14, the range of f is bucketed into 7 regions such as f = 0, 1, 2, 3, 4, 5 and f &gt; 6 In (Katz, 1987) dr = 1 </context>
</contexts>
<marker>Good, 1953</marker>
<rawString>I. J. Good. 1953. &amp;quot;The Population Frequencies of Species and the Estimation of Population Parameters,&amp;quot; In Biometrika, 40(3-4):237-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer. In</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing(ASSP),</journal>
<volume>35</volume>
<issue>3</issue>
<pages>400--401</pages>
<contexts>
<context position="11349" citStr="Katz, 1987" startWordPosition="1902" endWordPosition="1903">ch maximizes the probability of a training set. The ML estimate of morpheme tag (K+1)-gram probability, PrML(c I is calculated as follows: Fq(c,_K,,) Pr (c I Ci_K,i_i) = (12) ML r where the function Fq(x) returns the frequency of x in the training set. When using the ML estimation, data sparseness is even more serious in the extended models than in the standard models because the former has even more parameters than the latter. In (Chen, 1996), where various smoothing techniques was tested for a language model by using the perplexity measure, it was reported that the back-off smoothing method(Katz, 1987) performs better on a small traning set than other methods. In the back-off smoothing, the smoothed probability of tag (K+1)- gram Pr sBo (c%I cz_K,%_1) is calculated as follows: Pr (c I c,—K,i—i) — SBO {dr PrmL (ci I ci—K,i—i) if r &gt;0 a(ci_i-c,i—i)PrsBo(c, ci—K+1,i-1)if r = 0 where r = Fq(ci—K,i), r* = ± 1) nr+1 nr r* (r+i)xnr+i In the equation above, nr denotes the number of (K+1)-gram whose frequency is r, and the coefficient dr is called the discount ratio, which reflects the Good-Turing estimate(Good, 1953)5. Eqn. 13 says that PrsBo(c I c,_K,%_1) is under-estimated by dr than its maximum </context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>S. M. Katz. 1987. Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer. In IEEE Transactions on Acoustics, Speech and Signal Processing(ASSP), 35(3) :400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I-H Kang</author>
<author>J-H Kim</author>
<author>G-C Kim</author>
</authors>
<title>Korean Part-of-Speech Tagging Using Maximum Entropy Model.</title>
<date>1998</date>
<booktitle>In Proc. of the lath National Conference on Korean Information Processing,</booktitle>
<pages>9--14</pages>
<marker>Kang, Kim, Kim, 1998</marker>
<rawString>I.-H. Kang, J.-H. Kim, and G.-C. Kim. 1998. Korean Part-of-Speech Tagging Using Maximum Entropy Model. In Proc. of the lath National Conference on Korean Information Processing, 9-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-H Kim</author>
</authors>
<title>Korean Part-of-Speech Tagging by Using a Fuzzy net.</title>
<date>1993</date>
<booktitle>In Proc. of the</booktitle>
<pages>593--603</pages>
<marker>Kim, 1993</marker>
<rawString>J.-H. Kim, et al. 1993. Korean Part-of-Speech Tagging by Using a Fuzzy net. In Proc. of the S. F. Chen. 1996. Building Probabilistic Models 5th National Conference on Korean Information Processing, 593-603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-H Kim</author>
<author>G-C Kim</author>
</authors>
<title>Discriminative Learning in Part-of-Speech Tagging.</title>
<date>1995</date>
<booktitle>In Proc. of the National Conference on Korean Information Science</booktitle>
<pages>627--630</pages>
<publisher>Society, Spring,</publisher>
<marker>Kim, Kim, 1995</marker>
<rawString>J.-H. Kim and G.-C. Kim. 1995. Discriminative Learning in Part-of-Speech Tagging. In Proc. of the National Conference on Korean Information Science Society, Spring, 627-630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>S-Z Lee</author>
<author>Rim</author>
</authors>
<title>A Morpheme-Unit POS Tagging Model Considering Word-Spacing.</title>
<date>1998</date>
<booktitle>In Proc. of the 10th National Conference on Korean Information Processing,</booktitle>
<pages>3--8</pages>
<contexts>
<context position="2113" citStr="Kim et al., 1998" startWordPosition="316" endWordPosition="319"> to each word in texts. However, in KoHae-Chang Rim Department of Computer Science Korea University 1 5-Ga Anam-Dong, Seongbuk-Gu Seoul 136-701, Korea rim@nlp.korea.ac.kr rean POS tagging, each word is tagged with a proper combination of categories and lexical forms of morphemes(Lee et al., 1999) because Korean words can be freely formed by agglutinating morphemes and so the number of categories of Korean words can be (theoretically) infinite. Over a decade, many works for Korean POS tagging have used a wide range of machine learning techniques such as a hidden Markov model (HMM) (Lee, 1995) (Kim et al., 1998), a maximum entropy model (Kang, 1998), transformation rules (Lim, 1997), a decision tree (Lee et al., 1999), discriminative learning (Kim et al., 1995), a fuzzy net (Kim et al., 1993), a neural network (Lee, 1994), and so on. In this paper we propose hidden Markov models for Korean POS tagging, which adopt a less strict Markov assumption(Cinlar, 1975) to consider rich contexts and which consider Korean characteristics such as high agglutinativity, word-spacing, and high lexical correlativity. In the models, sparse-data problem is very serious because they have a large number of parameters. To</context>
<context position="9429" citStr="Kim et al., 1998" startWordPosition="1580" endWordPosition="1583"> the chain rule. Pr(C1,u,P2,u,n11,u) = H x Pr( j=1 u Pr(ci,Pi ni, (10) Because Eqn. 10 is not easy to compute, it is simplified by making a Markov assumption to get a more tractable form. An extended HMM for morpheme-unit tagging can be defined by making a less strict Markov assumption, as follows: A(C[s](K,J), [s](LJ)) Pr(c1,u,P2,u, mi,) H Pr(Ci[, pi] I ) Ci-K,i-1[,Pi-K +1,i-1], i=1 X Pr(m I ci—LAPi—L+1,i], mi—i,i—i) In a model A(C[s](K,J), [cL,i)), the probability of the current morpheme tag c, conditionally depends on both the previous K tags 4Most previous HMM-based Korean taggers except (Kim et al., 1998) did not consider word-spacing. (optionally, the types of their transition p,_K+1,,_1) and the previous J morphemes m,_ and the probability of the current morpheme n1,,, conditionally depends on the current tag and the previous L tags c,_4, (optionally, the types of their transition pi-L+1,%) and the previous I morphemes m,_/,,_1. In experiments, we set K as 1 or 2, J as 0 or K, L as 1 or 2, and / as 0 or L. If J and I are zero, the above models are non-lexicalized models. Otherwise, they are lexicalized models. For example, the extended model A (C,(2,2) , M(2,2)), where word-spacing is consid</context>
<context position="18203" citStr="Kim et al., 1998" startWordPosition="3106" endWordPosition="3109"> In these figures, we can observe that the simplified back-off smoothing technique mitigates sparse-data problems in both HMMs and JIMs. As expected, JIMs achieves higher accuracy than the corresponding HMMs in some extended models consulting rich contexts. Consulting word-spacing makes slight improvement in some of both HMMs and JIMs. It is statistically significant with confidence 99that the best model, A(C8(2,2), M8(2,2)) (96.97%), is better than any other models including the previous standard model A (C(1,0) , M(0,0)) (94.95%) (Lee, 1995), the previous model A (C8(1,0), M(0,0)) (94.96%) (Kim et al., 1998), and the best JIM, A(C(1,1), 8(1,1)) (96.95%). 6 Conclusion We have presented the extended HMMs for Korean POS tagging, which assume joint independence between random variables, which are based on the morpheme-unit lattice structure, and which consider word-spacing and rich information in contexts. In the models, a simplified version of back-off smoothing is used to mitigate data sparseness problem. From the experimental results, we have observed that extended models achieved even better results than the standard models in case of both HMMs and JIMs, that the simpliX PrAD( rni-J,i-i Pr(mi, = </context>
</contexts>
<marker>Kim, Lee, Rim, 1998</marker>
<rawString>J.-D. Kim, S.-Z. Lee, and Rim. 1998. A Morpheme-Unit POS Tagging Model Considering Word-Spacing. In Proc. of the 10th National Conference on Korean Information Processing, 3-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-J Lee</author>
</authors>
<title>Prediction and Disambiguation of Korean Word Category by Using a Neural Network. Doctoral Dissertation,</title>
<date>1994</date>
<institution>Seoul National University,</institution>
<contexts>
<context position="2327" citStr="Lee, 1994" startWordPosition="354" endWordPosition="355">r combination of categories and lexical forms of morphemes(Lee et al., 1999) because Korean words can be freely formed by agglutinating morphemes and so the number of categories of Korean words can be (theoretically) infinite. Over a decade, many works for Korean POS tagging have used a wide range of machine learning techniques such as a hidden Markov model (HMM) (Lee, 1995) (Kim et al., 1998), a maximum entropy model (Kang, 1998), transformation rules (Lim, 1997), a decision tree (Lee et al., 1999), discriminative learning (Kim et al., 1995), a fuzzy net (Kim et al., 1993), a neural network (Lee, 1994), and so on. In this paper we propose hidden Markov models for Korean POS tagging, which adopt a less strict Markov assumption(Cinlar, 1975) to consider rich contexts and which consider Korean characteristics such as high agglutinativity, word-spacing, and high lexical correlativity. In the models, sparse-data problem is very serious because they have a large number of parameters. To overcome sparse-data problem, our model uses a simplified version of the well-known back-off smoothing method. If the parameters are very specific like lexicalized ones, they tend to have very different estimation</context>
</contexts>
<marker>Lee, 1994</marker>
<rawString>S.-J. Lee. 1994. Prediction and Disambiguation of Korean Word Category by Using a Neural Network. Doctoral Dissertation, Seoul National University, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-H Lee</author>
</authors>
<title>Korean POS Tagging System Considering Unknown Words.</title>
<date>1995</date>
<booktitle>Master Thesis, Korea Advanced Institute of Science and Technology(KAIST),</booktitle>
<contexts>
<context position="2094" citStr="Lee, 1995" startWordPosition="314" endWordPosition="315"> is assigned to each word in texts. However, in KoHae-Chang Rim Department of Computer Science Korea University 1 5-Ga Anam-Dong, Seongbuk-Gu Seoul 136-701, Korea rim@nlp.korea.ac.kr rean POS tagging, each word is tagged with a proper combination of categories and lexical forms of morphemes(Lee et al., 1999) because Korean words can be freely formed by agglutinating morphemes and so the number of categories of Korean words can be (theoretically) infinite. Over a decade, many works for Korean POS tagging have used a wide range of machine learning techniques such as a hidden Markov model (HMM) (Lee, 1995) (Kim et al., 1998), a maximum entropy model (Kang, 1998), transformation rules (Lim, 1997), a decision tree (Lee et al., 1999), discriminative learning (Kim et al., 1995), a fuzzy net (Kim et al., 1993), a neural network (Lee, 1994), and so on. In this paper we propose hidden Markov models for Korean POS tagging, which adopt a less strict Markov assumption(Cinlar, 1975) to consider rich contexts and which consider Korean characteristics such as high agglutinativity, word-spacing, and high lexical correlativity. In the models, sparse-data problem is very serious because they have a large numbe</context>
<context position="18135" citStr="Lee, 1995" startWordPosition="3097" endWordPosition="3098">tes beyond the range of each graph are intentionally omitted. In these figures, we can observe that the simplified back-off smoothing technique mitigates sparse-data problems in both HMMs and JIMs. As expected, JIMs achieves higher accuracy than the corresponding HMMs in some extended models consulting rich contexts. Consulting word-spacing makes slight improvement in some of both HMMs and JIMs. It is statistically significant with confidence 99that the best model, A(C8(2,2), M8(2,2)) (96.97%), is better than any other models including the previous standard model A (C(1,0) , M(0,0)) (94.95%) (Lee, 1995), the previous model A (C8(1,0), M(0,0)) (94.96%) (Kim et al., 1998), and the best JIM, A(C(1,1), 8(1,1)) (96.95%). 6 Conclusion We have presented the extended HMMs for Korean POS tagging, which assume joint independence between random variables, which are based on the morpheme-unit lattice structure, and which consider word-spacing and rich information in contexts. In the models, a simplified version of back-off smoothing is used to mitigate data sparseness problem. From the experimental results, we have observed that extended models achieved even better results than the standard models in ca</context>
</contexts>
<marker>Lee, 1995</marker>
<rawString>S.-H. Lee. 1995. Korean POS Tagging System Considering Unknown Words. Master Thesis, Korea Advanced Institute of Science and Technology(KAIST), Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rim</author>
</authors>
<title>A Part-of-Speech Tagging Model Using Lexical Rules Based on Corpus Statistics.</title>
<date>1999</date>
<booktitle>In Proc. of the International Conference on Computer Processing of Oriental Languages (ICCPOL-</booktitle>
<volume>99</volume>
<pages>385--390</pages>
<marker>Rim, 1999</marker>
<rawString>S.-Z. Lee, J.-D. Kim, W.-H. Ryu, and WC. Rim. 1999. A Part-of-Speech Tagging Model Using Lexical Rules Based on Corpus Statistics. In Proc. of the International Conference on Computer Processing of Oriental Languages (ICCPOL- 99), 385-390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-Z Lee</author>
</authors>
<title>New Statistical Models for Automatic POS Tagging. Doctoral Dissertation,</title>
<date>1999</date>
<institution>Korea University,</institution>
<contexts>
<context position="16745" citStr="Lee, 1999" startWordPosition="2885" endWordPosition="2886">1) x Pr(Su, VV, EFD,NNBG, Ha, 1) The parameters of a JIM are estimated by using the parameters of the corresponding HMM as follows: 5 Experiments In experiments, we used the KUNLP corpus which consists of 167,115 words and 15,211 sentences and is tagged with 65 POS tags. It was segmented into two parts, the training set of 90% and the test set of 10%, in the way that each sentence in the test set was extracted from every 10 sentence. In the same way, we made 10-fold data set for 10-fold cross validation. In order to morphologically analyze each word, we used the Korean morphological analyzer (Lee, 1999) which is consistent with the KUNLP corpus. By using the morphological analyzer, the average number of possible analyses per word becomes 3.41. Figure 2-5 illustrate graphs showing the average accuracy rates of HMMs and JIMs, without considering word-spacing, with considering word-spacing only in the lexical probabilities, with considering word-spacing only in the tag probabilities, and with considering word-spacing in both the tag and lexical probabilities, respectively. Here, labels in x-axis specify models in the way that KI:}1 denotes A(C[s](K,J), M[s](L,I)) or d)(C[8](K,J), MH(L,I)). The </context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>S.-Z. Lee. 1999. New Statistical Models for Automatic POS Tagging. Doctoral Dissertation, Korea University, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lim</author>
</authors>
<title>Korean Part-of-Speech Tagging by Using Lingusitic Knowledge and Statistical Information. Doctoral Dissertation,</title>
<date>1997</date>
<institution>Korea University,</institution>
<contexts>
<context position="2185" citStr="Lim, 1997" startWordPosition="328" endWordPosition="329">nce Korea University 1 5-Ga Anam-Dong, Seongbuk-Gu Seoul 136-701, Korea rim@nlp.korea.ac.kr rean POS tagging, each word is tagged with a proper combination of categories and lexical forms of morphemes(Lee et al., 1999) because Korean words can be freely formed by agglutinating morphemes and so the number of categories of Korean words can be (theoretically) infinite. Over a decade, many works for Korean POS tagging have used a wide range of machine learning techniques such as a hidden Markov model (HMM) (Lee, 1995) (Kim et al., 1998), a maximum entropy model (Kang, 1998), transformation rules (Lim, 1997), a decision tree (Lee et al., 1999), discriminative learning (Kim et al., 1995), a fuzzy net (Kim et al., 1993), a neural network (Lee, 1994), and so on. In this paper we propose hidden Markov models for Korean POS tagging, which adopt a less strict Markov assumption(Cinlar, 1975) to consider rich contexts and which consider Korean characteristics such as high agglutinativity, word-spacing, and high lexical correlativity. In the models, sparse-data problem is very serious because they have a large number of parameters. To overcome sparse-data problem, our model uses a simplified version of th</context>
</contexts>
<marker>Lim, 1997</marker>
<rawString>Lim. 1997. Korean Part-of-Speech Tagging by Using Lingusitic Knowledge and Statistical Information. Doctoral Dissertation, Korea University, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging Text with a Probabilistic Model.</title>
<date>1991</date>
<booktitle>In Proc. of the International Conference on Acoustic, Speech and Signal Processing (I CA S SP-</booktitle>
<volume>91</volume>
<pages>809--812</pages>
<contexts>
<context position="7932" citStr="Merialdo, 1991" startWordPosition="1325" endWordPosition="1326"> w, conditionally depends on only the current tag3. Generally, the standard HMM has a limitation that it can not solve complicated ambiguities because it does not consider rich contexts. To overcome this limitation, the standard HMM should be extended so that it can consult rich information in contexts. Moreover, the standard word-unit model can not be used effectively for tagging highly agglutinative languages like Korean. Therefore, the word-unit model should be transformed into a morpheme-unit model. 3Usually, K is determined as 1 (bigram as in (Charniak et al., 1993)) or 2 (trigram as in (Merialdo, 1991)). 3.2 Extended morpheme-unit model Bayesian models for morpheme-unit tagging find the most likely sequence of morphemes and corresponding tags for a given sequence of words, as follows: T(wi,n) = argmax Pr(ci,u, rni,uI wi,n) (8) argmax Pr(ci,u, P2,u mi,) (9) c1, ,m In the above equations, u(&gt; n) denotes the number of morphemes in a sequence corresponding the given word sequence, c denotes a morpheme-unit tag, m denotes a morpheme, and p denotes a type of transition from the previous tag to the current tag. p can have one of two values, &amp;quot;#&amp;quot; denoting a transition across a word boundary and &amp;quot;+&amp;quot; </context>
</contexts>
<marker>Merialdo, 1991</marker>
<rawString>B. Merialdo. 1991. Tagging Text with a Probabilistic Model. In Proc. of the International Conference on Acoustic, Speech and Signal Processing (I CA S SP- 91), 809-812.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>