<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009920">
<title confidence="0.999731">
A Hierarchical Pitman-Yor Process HMM
for Unsupervised Part of Speech Induction
</title>
<author confidence="0.99761">
Phil Blunsom Trevor Cohn
</author>
<affiliation confidence="0.9998215">
Department of Computer Science Department of Computer Science
University of Oxford University of Sheffield
</affiliation>
<email confidence="0.991518">
Phil.Blunsom@cs.ox.ac.uk T.Cohn@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.99724" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999780133333333">
In this work we address the problem of
unsupervised part-of-speech induction
by bringing together several strands of
research into a single model. We develop a
novel hidden Markov model incorporating
sophisticated smoothing using a hierarchical
Pitman-Yor processes prior, providing an
elegant and principled means of incorporating
lexical characteristics. Central to our
approach is a new type-based sampling
algorithm for hierarchical Pitman-Yor models
in which we track fractional table counts.
In an empirical evaluation we show that our
model consistently out-performs the current
state-of-the-art across 10 languages.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994214872727273">
Unsupervised part-of-speech (PoS) induction has
long been a central challenge in computational
linguistics, with applications in human language
learning and for developing portable language
processing systems. Despite considerable research
effort, progress in fully unsupervised PoS induction
has been slow and modern systems barely improve
over the early Brown et al. (1992) approach
(Christodoulopoulos et al., 2010). One popular
means of improving tagging performance is to
include supervision in the form of a tag dictionary
or similar, however this limits portability and
also comprimises any cognitive conclusions. In
this paper we present a novel approach to fully
unsupervised PoS induction which uniformly
outperforms the existing state-of-the-art across all
our corpora in 10 different languages. Moreover, the
performance of our unsupervised model approaches
865
that of many existing semi-supervised systems,
despite our method not receiving any human input.
In this paper we present a Bayesian hidden
Markov model (HMM) which uses a non-parametric
prior to infer a latent tagging for a sequence of
words. HMMs have been popular for unsupervised
PoS induction from its very beginnings (Brown
et al., 1992), and justifiably so, as the most
discriminating feature for deciding a word’s PoS is
its local syntactic context.
Our work brings together several strands of
research including Bayesian non-parametric HMMs
(Goldwater and Griffiths, 2007), Pitman-Yor
language models (Teh, 2006b; Goldwater et
al., 2006b), tagging constraints over word types
(Brown et al., 1992) and the incorporation of
morphological features (Clark, 2003). The result
is a non-parametric Bayesian HMM which avoids
overfitting, contains no free parameters, and
exhibits good scaling properties. Our model uses
a hierarchical Pitman-Yor process (PYP) prior to
affect sophisicated smoothing over the transition
and emission distributions. This allows the
modelling of sub-word structure, thereby capturing
tag-specific morphological variation. Unlike many
existing approaches, our model is a principled
generative model and does not include any hand
tuned language specific features.
Inspired by previous successful approaches
(Brown et al., 1992), we develop a new type-
level inference procedure in the form of an
MCMC sampler with an approximate method for
incorporating the complex dependencies that arise
between jointly sampled events. Our experimental
evaluation demonstrates that our model, particularly
when restricted to a single tag per type, produces
</bodyText>
<note confidence="0.9619705">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 865–874,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9954005">
state-of-the-art results across a range of corpora and
languages.
</bodyText>
<sectionHeader confidence="0.990691" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999644047619047">
Past research in unsupervised PoS induction has
largely been driven by two different motivations: a
task based perspective which has focussed on induc-
ing word classes to improve various applications,
and a linguistic perspective where the aim is to
induce classes which correspond closely to anno-
tated part-of-speech corpora. Early work was firmly
situtated in the task-based setting of improving gen-
eralisation in language models. Brown et al. (1992)
presented a simple first-order HMM which restricted
word types to always be generated from the same
class. Though PoS induction was not their aim, this
restriction is largely validated by empirical analysis
of treebanked data, and moreover conveys the sig-
nificant advantage that all the tags for a given word
type can be updated at the same time, allowing very
efficient inference using the exchange algorithm.
This model has been popular for language mod-
elling and bilingual word alignment, and an imple-
mentation with improved inference called mkcls
(Och, 1999)1 has become a standard part of statis-
tical machine translation systems.
The HMM ignores orthographic information,
which is often highly indicative of a word’s part-
of-speech, particularly so in morphologically rich
languages. For this reason Clark (2003) extended
Brown et al. (1992)’s HMM by incorporating a
character language model, allowing the modelling
of limited morphology. Our work draws from these
models, in that we develop a HMM with a one
class per tag restriction and include a character
level language model. In contrast to these previous
works which use the maximum likelihood estimate,
we develop a Bayesian model with a rich prior for
smoothing the parameter estimates, allowing us to
move to a trigram model.
A number of researchers have investigated a semi-
supervised PoS induction task in which a tag dictio-
nary or similar data is supplied a priori (Smith and
Eisner, 2005; Haghighi and Klein, 2006; Goldwater
and Griffiths, 2007; Toutanova and Johnson, 2008;
Ravi and Knight, 2009). These systems achieve
</bodyText>
<footnote confidence="0.978766">
1Available from http://fjoch.com/mkcls.html.
</footnote>
<bodyText confidence="0.999904891304348">
much higher accuracy than fully unsupervised sys-
tems, though it is unclear whether the tag dictionary
assumption has real world application. We focus
solely on the fully unsupervised scenario, which we
believe is more practical for text processing in new
languages and domains.
Recent work on unsupervised PoS induction has
focussed on encouraging sparsity in the emission
distributions in order to match empirical distribu-
tions derived from treebank data (Goldwater and
Griffiths, 2007; Johnson, 2007; Gao and Johnson,
2008). These authors took a Bayesian approach
using a Dirichlet prior to encourage sparse distri-
butions over the word types emitted from each tag.
Conversely, Ganchev et al. (2010) developed a tech-
nique to optimize the more desirable reverse prop-
erty of the word types having a sparse posterior dis-
tribution over tags. Recently Lee et al. (2010) com-
bined the one class per word type constraint (Brown
et al., 1992) in a HMM with a Dirichlet prior to
achieve both forms of sparsity. However this work
approximated the derivation of the Gibbs sampler
(omitting the interdependence between events when
sampling from a collapsed model), resulting in a
model which underperformed Brown et al. (1992)’s
one-class HMM.
Our work also seeks to enforce both forms of
sparsity, by developing an algorithm for type-level
inference under the one class constraint. This work
differs from previous Bayesian models in that we
explicitly model a complex backoff path using a
hierachical prior, such that our model jointly infers
distributions over tag trigrams, bigrams and uni-
grams and whole words and their character level
representation. This smoothing is critical to ensure
adequate generalisation from small data samples.
Research in language modelling (Teh, 2006b;
Goldwater et al., 2006a) and parsing (Cohn et
al., 2010) has shown that models employing
Pitman-Yor priors can significantly outperform the
more frequently used Dirichlet priors, especially
where complex hierarchical relationships exist
between latent variables. In this work we apply
these advances to unsupervised PoS tagging,
developing a HMM smoothed using a Pitman-Yor
process prior.
</bodyText>
<page confidence="0.999031">
866
</page>
<sectionHeader confidence="0.999573" genericHeader="method">
3 The PYP-HMM
</sectionHeader>
<bodyText confidence="0.999906666666667">
We develop a trigram hidden Markov model which
models the joint probability of a sequence of latent
tags, t, and words, w, as
</bodyText>
<equation confidence="0.994772">
L+1
Pθ(t, w) = Pθ(tl|tl−1, tl−2)Pθ(wl|tl) ,
l=1
</equation>
<bodyText confidence="0.999768757575758">
where L = |w |= |t |and t0 = t−1 = tL+1 = $ are
assigned a sentinel value to denote the start or end of
the sentence. A key decision in formulating such a
model is the smoothing of the tag trigram and emis-
sion distributions, which would otherwise be too dif-
ficult to estimate from small datasets. Prior work
in unsupervised PoS induction has employed simple
smoothing techniques, such as additive smoothing
or Dirichlet priors (Goldwater and Griffiths, 2007;
Johnson, 2007), however this body of work has over-
looked recent advances in smoothing methods used
for language modelling (Teh, 2006b; Goldwater et
al., 2006b). Here we build upon previous work by
developing a PoS induction model smoothed with
a sophisticated non-parametric prior. Our model
uses a hierarchical Pitman-Yor process prior for both
the transition and emission distributions, encoding
a backoff path from complex distributions to suc-
cesssively simpler ones. The use of complex dis-
tributions (e.g., over tag trigrams) allows for rich
expressivity when sufficient evidence is available,
while the hierarchy affords a means of backing off
to simpler and more easily estimated distributions
otherwise. The PYP has been shown to generate
distributions particularly well suited to modelling
language (Teh, 2006a; Goldwater et al., 2006b), and
has been shown to be a generalisation of Kneser-Ney
smoothing, widely recognised as the best smoothing
method for language modelling (Chen and Good-
man, 1996).
The model is depicted in the plate diagram in Fig-
ure 1. At its centre is a standard trigram HMM,
which generates a sequence of tags and words,
</bodyText>
<equation confidence="0.9815425">
tl|tl−1, tl−2, T ∼ Ttl−1,tl−2
wl|tl, E ∼ Etl .
</equation>
<figureCaption confidence="0.964184">
Figure 1: Plate diagram representation of the trigram
HMM. The indexes i and j range over the set of tags
and k ranges over the set of characters. Hyper-parameters
have been omitted from the figure for clarity.
</figureCaption>
<bodyText confidence="0.998968333333333">
The trigram transition distribution, Tij, is drawn
from a hierarchical PYP prior which backs off to a
bigram Bj and then a unigram U distribution,
</bodyText>
<equation confidence="0.977837">
Tij|aT,bT,Bj ∼ PYP(aT,bT,Bj)
Bj|aB, bB, U ∼ PYP(aB, bB, U)
U|aU, bU ∼ PYP(aU, bU, Uniform) ,
</equation>
<bodyText confidence="0.999882166666667">
where the prior over U has as its base distribition a
uniform distribution over the set of tags, while the
priors for Bj and Tij back off by discarding an item
of context. This allows the modelling of trigram
tag sequences, while smoothing these estimates with
their corresponding bigram and unigram distribu-
tions. The degree of smoothing is regulated by
the hyper-parameters a and b which are tied across
each length of n-gram; these hyper-parameters are
inferred during training, as described in 3.1.
The tag-specific emission distributions, Ej, are
also drawn from a PYP prior,
</bodyText>
<equation confidence="0.629219">
Ej|aE, bE, C ∼ PYP(aE, bE, Cj) .
</equation>
<bodyText confidence="0.999960777777778">
We consider two different settings for the base distri-
bution Cj: 1) a simple uniform distribution over the
vocabulary (denoted HMM for the experiments in
section 4); and 2) a character-level language model
(denoted HMM+LM). In many languages morpho-
logical regularities correlate strongly with a word’s
part-of-speech (e.g., suffixes in English), which we
hope to capture using a basic character language
model. This model was inspired by Clark (2003)
</bodyText>
<figure confidence="0.998847769230769">
Dj
U
Bj
Tij
Cjk
t1
t2
t3
Ej
w1
w2
w3
...
</figure>
<page confidence="0.541849">
867
</page>
<figureCaption confidence="0.9976245">
Figure 2: The conditioning structure of the hierarchical
PYP with an embedded character language models.
</figureCaption>
<bodyText confidence="0.9655798">
who applied a character level distribution to the sin-
gle class HMM (Brown et al., 1992). We formu-
late the character-level language model as a bigram
model over the character sequence comprising word
wl,
</bodyText>
<equation confidence="0.993288333333333">
wlk|wlk−1, tl,C - Ctlwlk−1
Cjk|aC, bC, Dj - PYP(aC, bC, Dj)
Dj|aD, bD - PYP(aD, bD, Uniform) ,
</equation>
<bodyText confidence="0.999765923076923">
where k indexes the characters in the word and,
in a slight abuse of notation, the character itself,
w0 and is set to a special sentinel value denoting
the start of the sentence (ditto for a final end of
sentence marker) and the uniform base distribution
ranges over the set of characters. We expect that
the HMM+LM model will outperform the uniform
HMM as it can capture many consistent morpholog-
ical affixes and thereby better distinguish between
different parts-of-speech. The HMM+LM is shown
in Figure 2, illustrating the decomposition of the tag
sequence into n-grams and a word into its compo-
nent character bigrams.
</bodyText>
<subsectionHeader confidence="0.999187">
3.1 Training
</subsectionHeader>
<bodyText confidence="0.999345625">
In order to induce a tagging under this model we
use Gibbs sampling, a Markov chain Monte Carlo
(MCMC) technique for drawing samples from the
posterior distribution over the tag sequences given
observed word sequences. We present two different
sampling strategies: First, a simple Gibbs sampler
which randomly samples an update to a single tag
given all other tags; and second, a type-level sam-
pler which updates all tags for a given word under a
one-tag-per-word-type constraint. In order to extract
a single tag sequence to test our model against the
gold standard we find the tag at each site with maxi-
mum marginal probability in the sample set.
Following standard practice, we perform
inference using a collapsed sampler whereby
the model parameters U, B, T, E and C are
marginalised out. After marginalisation the
posterior distribution under a PYP prior is described
by a variant of the Chinese Restaurant Process
(CRP). The CRP is based around the analogy of
a restaurant with an infinite number of tables,
with customers entering one at a time and seating
themselves at a table. The choice of table is
governed by
</bodyText>
<equation confidence="0.99582675">
P(zl = k|z−l) = { n k −a 1 GkGK− (1)
l−1+b - -
x−a+b k = K− + 1
l−1+b
</equation>
<bodyText confidence="0.9999784">
where zl is the table chosen by the lth customer, z−l
is the seating arrangement of the l - 1 previous cus-
tomers, n−k is the number of customers in z−l who
are seated at table k, K− = K(z−l) is the total num-
ber of tables in z−l, and z1 = 1 by definition. The
arrangement of customers at tables defines a cluster-
ing which exhibits a power-law behavior controlled
by the hyperparameters a and b.
To complete the restaurant analogy, a dish is then
served to each table which is shared by all the cus-
tomers seated there. This corresponds to a draw
from the base distribution, which in our case ranges
over tags for the transition distribution, and words
for the observation distribution. Overall the PYP
leads to a distribution of the form
</bodyText>
<equation confidence="0.9984704">
PT(tl = i|z−l,t−l) = 1 − + bT X (2)
n
h
� �
n− hi � K− hiaT + �K− h aT + bT � P B(i|z−l, t−l) ,
</equation>
<bodyText confidence="0.992612222222222">
illustrating the trigram transition distribution, where
t−l are all previous tags, h = (tl−2, tl−1) is the con-
ditioning bigram, n−hi is the count of the trigram hi
in t−l, n−h the total count over all trigrams beginning
with h, K−hi the number of tables served dish i and
PB(·) is the base distribution, in this case the bigram
distribution.
A hierarchy of PYPs can be formed by making the
base distribution of a PYP another PYP, following a
</bodyText>
<figure confidence="0.933223333333333">
The big dog
5 23 23 7
b r o w n
</figure>
<page confidence="0.975818">
868
</page>
<bodyText confidence="0.993602387755102">
semantics whereby whenever a customer sits at an 2010), though one must be careful to manage the
empty table in a restaurant, a new customer is also dependencies between multiple draws from the pos-
said to enter the restaurant for its base distribution. terior.
That is, each table at one level is equivalent to a cus- The dependency on table counts in the conditional
tomer at the next deeper level, creating the invari- distributions complicates the process of drawing
ants: K−hi = n−ui and K−ui = n−i , where u = tl−1 samples for both our models. In the non-hierarchical
indicates the unigram backoff context of h. The model (Goldwater and Griffiths, 2007) these
recursion terminates at the lowest level where the dependencies can easily be accounted for by
base distribution is static. The hierarchical setting incrementing customer counts when such a
allows for the modelling of elaborate backoff paths dependence occurs. In our model we would need to
from rich and complex structure to successively sim- sum over all possible table assignments that result
pler structures. in the same tagging, at all levels in the hierarchy:
Gibbs samplers Both our Gibbs samplers perform tag trigrams, bigrams and unigrams; and also words,
the same calculation of conditional tag distributions, character bigrams and character unigrams. To avoid
and involve first decrementing all trigrams and emis- this rather onerous marginalisation2 we instead use
sions affected by a sampling action, and then rein- expected table counts to calculate the conditional
troducing the trigrams one at a time, conditioning distributions for sampling. Unfortunately we
their probabilities on the updated counts and table know of no efficient algorithm for calculating the
configurations as we progress. expected table counts, so instead develop a novel
The first local Gibbs sampler (PYP-HMM) approximation
updates a single tag assignment at a time, in a En+1 [Ki] ^ En [Ki] +
similar fashion to Goldwater and Griffiths (2007). (aUEn [K] + bU)P0(i) (3)
Changing one tag affects three trigrams, with (n − En [Ki] bU) + (aUEn [K] + bU)P0(i)
posterior where Ki is the number of tables for the tag uni-
P(tl|z−l, t−l, w) a P(tl±2, wl|z−l±2, t−l±2) , gram i of which there are n + 1 occurrences, En [ ]
where l±2 denotes the range l−2,l−1,l,l+1,l+2. denotes an expectation after observing n items and
The joint distribution over the three trigrams con- En [K] = Ej En [Kj]. This formulation defines
tained in tl±2 can be calculated using the PYP for- a simple recurrence starting with the first customer
mulation. This calculation is complicated by the fact seated at a table, E1 [Ki] = 1, and as each subse-
that these events are not independent; the counts of quent customer arrives we fractionally assign them
one trigram can affect the probability of later ones, to a new table based on their conditional probability
and moreover, the table assignment for the trigram of sitting alone. These fractional counts are then
may also affect the bigram and unigram counts, of carried forward for subsequent customers.
particular import when the same tag occurs twice in This approximation is tight for small n, and there-
a row such as in Figure 2. fore it should be effective in the case of the local
Many HMMs used for inducing word classes for Gibbs sampler where only three trigrams are being
language modelling include the restriction that all resampled. For the type based resampling where
occurrences of a word type always appear with the large numbers of n are involved (consider resam-
same class throughout the corpus (Brown et al., pling the), this approximation can deviate from the
1992; Och, 1999; Clark, 2003). Our second sampler actual value due to errors accumulated in the recur-
(PYP-1HMM) restricts inference to taggings which sion. Figure 3 illustrates a simulation demonstrating
adhere to this one tag per type restriction. This that the approximation is a close match for small a
restriction permits efficient inference techniques in and n but underestimates the true value for high a
which all tags of all occurrences of a word type are
updated in parallel. Similar techniques have been
used for models with Dirichlet priors (Liang et al.,
869
2Marginalisation is intractable in general, i.e. for the 1HMM
where many sites are sampled jointly.
</bodyText>
<figure confidence="0.9873205">
0 20 40 60 80 100
number of customers
</figure>
<figureCaption confidence="0.9545534">
Figure 3: Simulation comparing the expected table
count (solid lines) versus the approximation under Eq. 3
(dashed lines) for various values of a. This data was gen-
erated from a single PYP with b = 1, P0(i) = 4 and
n = 100 customers which all share the same tag.
</figureCaption>
<bodyText confidence="0.992642619047619">
and n. The approximation was much less sensitive
to the choice of b (not shown).
To resample a sequence of trigrams we start by
removing their counts from the current restaurant
configuration (resulting in z_). For each tag we
simulate adding back the trigrams one at a time,
calculating their probability under the given z_ plus
the fractional table counts accumulated by Equation
3. We then calculate the expected table count con-
tribution from this trigram and add it to the accu-
mulated counts. The fractional table count from the
trigram then results in a fractional customer entering
the bigram restaurant, and so on down to unigrams.
At each level we must update the expected counts
before moving on to the next trigram. After per-
forming this process for all trigrams under consider-
ation and for all tags, we then normalise the resulting
tag probabilities and sample an outcome. Once a
tag has been sampled, we then add all the trigrams
to the restaurants sampling their tables assignments
explicitly (which are no longer fractional), recorded
in z. Because we do not marginalise out the table
counts and our expectations are only approximate,
this sampler will be biased. We leave to future work
properly accounting for this bias, e.g., by devising a
Metropolis Hastings acceptance test.
Sampling hyperparameters We treat the
hyper-parameters {(ax, bx) , x E (U, B, T, E, C)}
as random variables in our model and infer their
values. We place prior distributions on the PYP
discount ax and concentration bx hyperparamters
and sample their values using a slice sampler. For
the discount parameters we employ a uniform
Beta distribution (ax — Beta(1,1)), and for
the concentration parameters we use a vague
gamma prior (bx — Gamma(10, 0.1)). All the
hyper-parameters are resampled after every 5th
sample of the corpus.
The result of this hyperparameter inference is that
there are no user tunable parameters in the model,
an important feature that we believe helps explain its
consistently high performance across test settings.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999970518518518">
We perform experiments with a range of corpora
to both investigate the properties of our proposed
models and inference algorithms, as well as to estab-
lish their robustness across languages and domains.
For our core English experiments we report results
on the entire Penn. Treebank (Marcus et al., 1993),
while for other languages we use the corpora made
available for the CoNLL-X Shared Task (Buchholz
and Marsi, 2006). We report results using the many-
to-one (M-1) and v-measure (VM) metrics consid-
ered best by the evaluation of Christodoulopoulos
et al. (2010). M-1 measures the accuracy of the
model after mapping each predicted class to its most
frequent corresponding tag, while VM is a variant
of the F-measure which uses conditional entropy
analogies of precision and recall. The log-posterior
for the HMM sampler levels off after a few hundred
samples, so we report results after five hundred. The
1HMM sampler converges more quickly so we use
two hundred samples for these models. All reported
results are the mean of three sampling runs.
An important detail for any unsupervised
learning algorithm is its initialisation. We used
slightly different initialisation for each of our
inference strategies. For the unrestricted HMM we
randomly assigned each word token to a class. For
the restricted 1HMM we use a similar initialiser to
</bodyText>
<figure confidence="0.993557">
a=0.9
a=0.8
a=0.5
a=0.1
2 4 6 8 10 12
expected tables
</figure>
<page confidence="0.864428">
870
</page>
<table confidence="0.9839054375">
Frequency
Model M-1 VM
Prototype meta-model (CGS10) 76.1 68.8
MEMM (BBDK10) 75.5 -
mkcls (Och, 1999) 73.7 65.6
MLE 1HMM-LM (Clark, 2003)∗ 71.2 65.5
BHMM (GG07) 63.2 56.2
PR (Ganchev et al., 2010)∗ 62.5 54.8
Trigram PYP-HMM 69.8 62.6
Trigram PYP-1HMM 76.0 68.0
Trigram PYP-1HMM-LM 77.5 69.7
Bigram PYP-HMM 66.9 59.2
Bigram PYP-1HMM 72.9 65.9
Trigram DP-HMM 68.1 60.0
Trigram DP-1HMM 76.0 68.0
Trigram DP-1HMM-LM 76.8 69.8
</table>
<tableCaption confidence="0.998721">
Table 1: WSJ performance comparing previous work
</tableCaption>
<bodyText confidence="0.865256">
to our own model. The columns display the many-to-1
accuracy and the V measure, both averaged over 5 inde-
pendent runs. Our model was run with the local sampler
(HMM), the type-level sampler (1HMM) and also with
the character LM (1HMM-LM). Also shown are results
using Dirichlet Process (DP) priors by fixing a = 0. The
system abbreviations are CGS10 (Christodoulopoulos et
al., 2010), BBDK10 (Berg-Kirkpatrick et al., 2010) and
GG07 (Goldwater and Griffiths, 2007). Starred entries
denote results reported in CGS10.
Clark (2003), assigning each of the k most frequent
word types to its own class, and then randomly
dividing the rest of the types between the classes.
As a baseline we report the performance of
mkcls (Och, 1999) on all test corpora. This model
seems not to have been evaluated in prior work on
unsupervised PoS tagging, which is surprising given
its consistently good performance.
First we present our results on the most frequently
reported evaluation, the WSJ sections of the Penn.
Treebank, along with a number of state-of-the-art
results previously reported (Table 1). All of these
models are allowed 45 tags, the same number of tags
as in the gold-standard. The performance of our
models is strong, particularly the 1HMM. We also
see that incorporating a character language model
(1HMM-LM) leads to further gains in performance,
improving over the best reported scores under both
M-1 and VM. We have omitted the results for the
HMM-LM as experimentation showed that the local
Gibbs sampler became hopelessly stuck, failing to
</bodyText>
<figure confidence="0.931294">
0 10 20 30 40 50
Tags sorted by frequency
</figure>
<figureCaption confidence="0.960849333333333">
Figure 4: Sorted frequency of tags for WSJ. The gold
standard distribution follows a steep exponential curve
while the induced model distributions are more uniform.
</figureCaption>
<bodyText confidence="0.999927903225806">
mix due to the model’s deep structure (its peak per-
formance was Pz� 55%).
To evaluate the effectiveness of the PYP prior we
include results using a Dirichlet Process prior (DP).
We see that for all models the use of the PYP pro-
vides some gain for the HMM, but diminishes for
the 1HMM. This is perhaps a consequence of the
expected table count approximation for the type-
sampled PYP-1HMM: the DP relies less on the table
counts than the PYP.
If we restrict the model to bigrams we see
a considerable drop in performance. Note that
the bigram PYP-HMM outperforms the closely
related BHMM (the main difference being that
we smooth tag bigrams with unigrams). It is also
interesting to compare the bigram PYP-1HMM to
the closely related model of Lee et al. (2010). That
model incorrectly assumed independence of the
conditional sampling distributions, resulting in a
accuracy of 66.4%, well below that of our model.
Figures 4 and 5 provide insight into the behavior
of the sampling algorithms. The former shows that
both our models and mkcls induce a more uniform
distribution over tags than specified by the treebank.
It is unclear whether it is desirable for models to
exhibit behavior closer to the treebank, which ded-
icates separate tags to very infrequent phenomena
while lumping the large range of noun types into
a single category. The graph in Figure 5 shows
that the type-based 1HMM sampler finds a good
tagging extremely quickly and then sticks with it,
</bodyText>
<figure confidence="0.982424764705882">
18 x 104
16
14
12
10
8
4
2
0
6
Gold tag distribution
1HMM
1HMM−LM
MKCLS
871
M−1 Accuracy (%)
Number of samples
</figure>
<figureCaption confidence="0.992317">
Figure 5: M-1 accuracy vs. number of samples.
</figureCaption>
<figure confidence="0.998969791666667">
’’
‘‘
$
PRP$
POS
MD
VBP
VBG
PRP
VBNSYM
UH
VBZ
TO #
P$
CC
FW
VB DT
VBDP
RBS
RB
EX
CD
RB
’’
‘‘
$
PRP$
POS
MD
VBP
VBG
PRP
VBN
VBZ
TO
CC
VB
VBD
RB
CD
.
,
NNS
JJ
DT
NNP
IN
NN
</figure>
<figureCaption confidence="0.9974818">
Figure 6: Cooccurence between frequent gold (y-axis)
and predicted (x-axis) tags, comparing mkcls (top) and
PYP-1HMM-LM (bottom). Both axes are sorted in terms
of frequency. Darker shades indicate more frequent cooc-
curence and columns represent the induced tags.
</figureCaption>
<bodyText confidence="0.99996578125">
save for the occasional step change demonstrated by
the 1HMM-LM line. The locally sampled model is
far slower to converge, rising slowly and plateauing
well below the other models.
In Figure 6 we compare the distributions over
WSJ tags for mkcls and the PYP-1HMM-LM. On
the macro scale we can see that our model induces a
sparser distribution. With closer inspection we can
identify particular improvements our model makes.
In the first column for mkcls and the third column
for our model we can see similar classes with sig-
nificant counts for DTs and PRPs, indicating a class
that the models may be using to represent the start
of sentences (informed by start transitions or capi-
talisation). This column exemplifies the sparsity of
the PYP model’s posterior.
We continue our evaluation on the CoNLL
multilingual corpora (Table 2). These results show
a highly consistent story of performance for our
models across diverse corpora. In all cases the
PYP-1HMM outperforms the PYP-HMM, which
are both outperformed by the PYP-1HMM-LM.
The character language model provides large
gains in performance on a number of corpora,
in particular those with rich morphology (Arabic
+5%, Portuguese +5%, Spanish +4%). We again
note the strong performance of the mkcls model,
significantly beating recently published state-of-the-
art results for both Dutch and Swedish. Overall our
best model (PYP-1HMM-LM) outperforms both
the state-of-the-art, where previous work exists, as
well as mkcls consistently across all languages.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99990685">
The hidden Markov model, originally developed by
Brown et al. (1992), continues to be an effective
modelling structure for PoS induction. We have
combined hierarchical Bayesian priors with a tri-
gram HMM and character language model to pro-
duce a model with consistently state-of-the-art per-
formance across corpora in ten languages. How-
ever our analysis indicates that there is still room for
improvement, particularly in model formulation and
developing effective inference algorithms.
Induced tags have already proven their usefulness
in applications such as Machine Translation, thus it
will prove interesting as to whether the improve-
ments seen from our models can lead to gains in
downstream tasks. The continued successes of mod-
els combining hierarchical Pitman-Yor priors with
expressive graphical models attests to this frame-
work’s enduring attraction, we foresee continued
interest in applying this technique to other NLP
tasks.
</bodyText>
<figure confidence="0.996340347826087">
80
70
60
50
40
30
20P
R10
B− 0
50
100 150
PYP−1HMM
PYP−1HMM−LM
PYP−HMM
PYP−HMM−LM
B .
NNSRBR ,
JJS
JJ B
DT R P
NNP W NPS
IN P
NN R JJR
</figure>
<page confidence="0.982228">
872
</page>
<table confidence="0.9993644">
Language mkcls EMM 1EMM 1EMM-LM Best pub. Tokens Tag types
Arabic 58.5 57.1 62.7 67.5 - 54,379 20
Bulgarian 66.8 67.8 69.7 73.2 - 190,217 54
Czech 59.6 62.0 66.3 70.1 - 1,249,408 12c
Danish 62.7 69.9 73.9 76.2 66.7* 94,386 25
Dutch 64.3 66.6 68.7 70.4 67.31 195,069 13c
Hungarian 54.3 65.9 69.0 73.0 - 131,799 43
Portuguese 68.5 72.1 73.5 78.5 75.3* 206,678 22
Spanish 63.8 71.6 74.7 78.8 73.2* 89,334 47
Swedish 64.3 66.6 67.0 68.6 60.61 191,467 41
</table>
<tableCaption confidence="0.952442">
Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published
result (*Berg-Kirkpatrick et al. (2010) and †Lee et al. (2010)). This data was taken from the CoNLL-X shared task
training sets, resulting in listed corpus sizes. Fine PoS tags were used for evaluation except for items marked with
which used the coarse tags. For each language the systems were trained to produce the same number of tags as the
gold standard.
</tableCaption>
<sectionHeader confidence="0.998238" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99965184375">
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero, and Dan Klein. 2010. Painless unsu-
pervised learning with features. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 582–590, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18:467–479, December.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning, CoNLL-X ’06, pages 149–
164, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th annual meeting
on Association for Computational Linguistics, pages
310–318, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 575–584, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth Annual Meeting of the
European Association for Computational Linguistics
(EACL), pages 59–66.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research, pages 3053–3096.
Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, 99:2001–2049, August.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
bayesian estimators for unsupervised hidden markov
model pos taggers. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ’08, pages 344–352, Morristown, NJ,
USA. Association for Computational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proc. of the 45th Annual Meeting of the ACL
(ACL-2007), pages 744–751, Prague, Czech Republic,
June.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006a. Contextual dependencies in unsupervised
word segmentation. In Proc. of the 44th Annual Meet-
ing of the ACL and 21st International Conference
on Computational Linguistics (COLING/ACL-2006),
Sydney.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006b. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Sch¨olkopf, and J. Platt, editors, Advances in Neural
</reference>
<page confidence="0.990024">
873
</page>
<reference confidence="0.9998066">
Information Processing Systems 18, pages 459–466.
MIT Press, Cambridge, MA.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 320–
327, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Mark Johnson. 2007. Why doesnt EM find good
HMM POS-taggers? In Proc. of the 2007 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2007), pages 296–305, Prague, Czech
Republic.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’10, pages 853–861, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
P. Liang, M. I. Jordan, and D. Klein. 2010. Type-based
MCMC. In North American Association for Compu-
tational Linguistics (NAACL).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19(2):313–330.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, pages 71–76,
Morristown, NJ, USA. Association for Computational
Linguistics.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of the Joint Conferenceof the 47th Annual Meet-
ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natu-
ral Language Processing of the Asian Federation of
Natural Language Processing (ACL-IJCNLP), pages
504–512.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 354–362, Ann Arbor, Michigan, June.
Y. W. Teh. 2006a. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
985–992.
Yee Whye Teh. 2006b. A hierarchical bayesian language
model based on pitman-yor processes. In Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, ACL-44, pages
985–992, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Kristina Toutanova and Mark Johnson. 2008. A bayesian
lda-based model for semi-supervised part-of-speech
tagging. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 1521–1528. MIT Press,
Cambridge, MA.
</reference>
<page confidence="0.998767">
874
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.773450">
<title confidence="0.9996685">A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</title>
<author confidence="0.999954">Phil Blunsom Trevor Cohn</author>
<affiliation confidence="0.999837">Department of Computer Science Department of Computer Science University of Oxford University of Sheffield</affiliation>
<email confidence="0.783946">Phil.Blunsom@cs.ox.ac.ukT.Cohn@dcs.shef.ac.uk</email>
<abstract confidence="0.9992110625">In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>582--590</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 582–590, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Comput. Linguist.,</journal>
<pages>18--467</pages>
<contexts>
<context position="1282" citStr="Brown et al. (1992)" startWordPosition="167" endWordPosition="170">based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages. 1 Introduction Unsupervised part-of-speech (PoS) induction has long been a central challenge in computational linguistics, with applications in human language learning and for developing portable language processing systems. Despite considerable research effort, progress in fully unsupervised PoS induction has been slow and modern systems barely improve over the early Brown et al. (1992) approach (Christodoulopoulos et al., 2010). One popular means of improving tagging performance is to include supervision in the form of a tag dictionary or similar, however this limits portability and also comprimises any cognitive conclusions. In this paper we present a novel approach to fully unsupervised PoS induction which uniformly outperforms the existing state-of-the-art across all our corpora in 10 different languages. Moreover, the performance of our unsupervised model approaches 865 that of many existing semi-supervised systems, despite our method not receiving any human input. In t</context>
<context position="3137" citStr="Brown et al., 1992" startWordPosition="434" endWordPosition="437">logical features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approaches, our model is a principled generative model and does not include any hand tuned language specific features. Inspired by previous successful approaches (Brown et al., 1992), we develop a new typelevel inference procedure in the form of an MCMC sampler with an approximate method for incorporating the complex dependencies that arise between jointly sampled events. Our experimental evaluation demonstrates that our model, particularly when restricted to a single tag per type, produces Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 865–874, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics state-of-the-art results across a range of corpora and languages. 2 Background Past research in u</context>
<context position="5016" citStr="Brown et al. (1992)" startWordPosition="717" endWordPosition="720"> and moreover conveys the significant advantage that all the tags for a given word type can be updated at the same time, allowing very efficient inference using the exchange algorithm. This model has been popular for language modelling and bilingual word alignment, and an implementation with improved inference called mkcls (Och, 1999)1 has become a standard part of statistical machine translation systems. The HMM ignores orthographic information, which is often highly indicative of a word’s partof-speech, particularly so in morphologically rich languages. For this reason Clark (2003) extended Brown et al. (1992)’s HMM by incorporating a character language model, allowing the modelling of limited morphology. Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate, we develop a Bayesian model with a rich prior for smoothing the parameter estimates, allowing us to move to a trigram model. A number of researchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supplied a priori (Smith and Eisn</context>
<context position="6737" citStr="Brown et al., 1992" startWordPosition="990" endWordPosition="993">tion has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). These authors took a Bayesian approach using a Dirichlet prior to encourage sparse distributions over the word types emitted from each tag. Conversely, Ganchev et al. (2010) developed a technique to optimize the more desirable reverse property of the word types having a sparse posterior distribution over tags. Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)’s one-class HMM. Our work also seeks to enforce both forms of sparsity, by developing an algorithm for type-level inference under the one class constraint. This work differs from previous Bayesian models in that we explicitly model a complex backoff path using a hierachical prior, such that our model jointly infers di</context>
<context position="11569" citStr="Brown et al., 1992" startWordPosition="1775" endWordPosition="1778"> simple uniform distribution over the vocabulary (denoted HMM for the experiments in section 4); and 2) a character-level language model (denoted HMM+LM). In many languages morphological regularities correlate strongly with a word’s part-of-speech (e.g., suffixes in English), which we hope to capture using a basic character language model. This model was inspired by Clark (2003) Dj U Bj Tij Cjk t1 t2 t3 Ej w1 w2 w3 ... 867 Figure 2: The conditioning structure of the hierarchical PYP with an embedded character language models. who applied a character level distribution to the single class HMM (Brown et al., 1992). We formulate the character-level language model as a bigram model over the character sequence comprising word wl, wlk|wlk−1, tl,C - Ctlwlk−1 Cjk|aC, bC, Dj - PYP(aC, bC, Dj) Dj|aD, bD - PYP(aD, bD, Uniform) , where k indexes the characters in the word and, in a slight abuse of notation, the character itself, w0 and is set to a special sentinel value denoting the start of the sentence (ditto for a final end of sentence marker) and the uniform base distribution ranges over the set of characters. We expect that the HMM+LM model will outperform the uniform HMM as it can capture many consistent m</context>
<context position="28848" citStr="Brown et al. (1992)" startWordPosition="4702" endWordPosition="4705">e PYP-HMM, which are both outperformed by the PYP-1HMM-LM. The character language model provides large gains in performance on a number of corpora, in particular those with rich morphology (Arabic +5%, Portuguese +5%, Spanish +4%). We again note the strong performance of the mkcls model, significantly beating recently published state-of-theart results for both Dutch and Swedish. Overall our best model (PYP-1HMM-LM) outperforms both the state-of-the-art, where previous work exists, as well as mkcls consistently across all languages. 5 Discussion The hidden Markov model, originally developed by Brown et al. (1992), continues to be an effective modelling structure for PoS induction. We have combined hierarchical Bayesian priors with a trigram HMM and character language model to produce a model with consistently state-of-the-art performance across corpora in ten languages. However our analysis indicates that there is still room for improvement, particularly in model formulation and developing effective inference algorithms. Induced tags have already proven their usefulness in applications such as Machine Translation, thus it will prove interesting as to whether the improvements seen from our models can l</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Classbased n-gram models of natural language. Comput. Linguist., 18:467–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06,</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="22005" citStr="Buchholz and Marsi, 2006" startWordPosition="3560" endWordPosition="3563"> hyperparameter inference is that there are no user tunable parameters in the model, an important feature that we believe helps explain its consistently high performance across test settings. 4 Experiments We perform experiments with a range of corpora to both investigate the properties of our proposed models and inference algorithms, as well as to establish their robustness across languages and domains. For our core English experiments we report results on the entire Penn. Treebank (Marcus et al., 1993), while for other languages we use the corpora made available for the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We report results using the manyto-one (M-1) and v-measure (VM) metrics considered best by the evaluation of Christodoulopoulos et al. (2010). M-1 measures the accuracy of the model after mapping each predicted class to its most frequent corresponding tag, while VM is a variant of the F-measure which uses conditional entropy analogies of precision and recall. The log-posterior for the HMM sampler levels off after a few hundred samples, so we report results after five hundred. The 1HMM sampler converges more quickly so we use two hundred samples for these models. All reported results are the </context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06, pages 149– 164, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9624" citStr="Chen and Goodman, 1996" startWordPosition="1440" endWordPosition="1444">ons, encoding a backoff path from complex distributions to successsively simpler ones. The use of complex distributions (e.g., over tag trigrams) allows for rich expressivity when sufficient evidence is available, while the hierarchy affords a means of backing off to simpler and more easily estimated distributions otherwise. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006a; Goldwater et al., 2006b), and has been shown to be a generalisation of Kneser-Ney smoothing, widely recognised as the best smoothing method for language modelling (Chen and Goodman, 1996). The model is depicted in the plate diagram in Figure 1. At its centre is a standard trigram HMM, which generates a sequence of tags and words, tl|tl−1, tl−2, T ∼ Ttl−1,tl−2 wl|tl, E ∼ Etl . Figure 1: Plate diagram representation of the trigram HMM. The indexes i and j range over the set of tags and k ranges over the set of characters. Hyper-parameters have been omitted from the figure for clarity. The trigram transition distribution, Tij, is drawn from a hierarchical PYP prior which backs off to a bigram Bj and then a unigram U distribution, Tij|aT,bT,Bj ∼ PYP(aT,bT,Bj) Bj|aB, bB, U ∼ PYP(aB</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 310–318, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Two decades of unsupervised POS induction: How far have we come?</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>575--584</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="1325" citStr="Christodoulopoulos et al., 2010" startWordPosition="172" endWordPosition="175">ierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages. 1 Introduction Unsupervised part-of-speech (PoS) induction has long been a central challenge in computational linguistics, with applications in human language learning and for developing portable language processing systems. Despite considerable research effort, progress in fully unsupervised PoS induction has been slow and modern systems barely improve over the early Brown et al. (1992) approach (Christodoulopoulos et al., 2010). One popular means of improving tagging performance is to include supervision in the form of a tag dictionary or similar, however this limits portability and also comprimises any cognitive conclusions. In this paper we present a novel approach to fully unsupervised PoS induction which uniformly outperforms the existing state-of-the-art across all our corpora in 10 different languages. Moreover, the performance of our unsupervised model approaches 865 that of many existing semi-supervised systems, despite our method not receiving any human input. In this paper we present a Bayesian hidden Mark</context>
<context position="22148" citStr="Christodoulopoulos et al. (2010)" startWordPosition="3583" endWordPosition="3586">its consistently high performance across test settings. 4 Experiments We perform experiments with a range of corpora to both investigate the properties of our proposed models and inference algorithms, as well as to establish their robustness across languages and domains. For our core English experiments we report results on the entire Penn. Treebank (Marcus et al., 1993), while for other languages we use the corpora made available for the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We report results using the manyto-one (M-1) and v-measure (VM) metrics considered best by the evaluation of Christodoulopoulos et al. (2010). M-1 measures the accuracy of the model after mapping each predicted class to its most frequent corresponding tag, while VM is a variant of the F-measure which uses conditional entropy analogies of precision and recall. The log-posterior for the HMM sampler levels off after a few hundred samples, so we report results after five hundred. The 1HMM sampler converges more quickly so we use two hundred samples for these models. All reported results are the mean of three sampling runs. An important detail for any unsupervised learning algorithm is its initialisation. We used slightly different init</context>
<context position="23838" citStr="Christodoulopoulos et al., 2010" startWordPosition="3859" endWordPosition="3862">M 69.8 62.6 Trigram PYP-1HMM 76.0 68.0 Trigram PYP-1HMM-LM 77.5 69.7 Bigram PYP-HMM 66.9 59.2 Bigram PYP-1HMM 72.9 65.9 Trigram DP-HMM 68.1 60.0 Trigram DP-1HMM 76.0 68.0 Trigram DP-1HMM-LM 76.8 69.8 Table 1: WSJ performance comparing previous work to our own model. The columns display the many-to-1 accuracy and the V measure, both averaged over 5 independent runs. Our model was run with the local sampler (HMM), the type-level sampler (1HMM) and also with the character LM (1HMM-LM). Also shown are results using Dirichlet Process (DP) priors by fixing a = 0. The system abbreviations are CGS10 (Christodoulopoulos et al., 2010), BBDK10 (Berg-Kirkpatrick et al., 2010) and GG07 (Goldwater and Griffiths, 2007). Starred entries denote results reported in CGS10. Clark (2003), assigning each of the k most frequent word types to its own class, and then randomly dividing the rest of the types between the classes. As a baseline we report the performance of mkcls (Och, 1999) on all test corpora. This model seems not to have been evaluated in prior work on unsupervised PoS tagging, which is surprising given its consistently good performance. First we present our results on the most frequently reported evaluation, the WSJ secti</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2010</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2010. Two decades of unsupervised POS induction: How far have we come? In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 575–584, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth Annual Meeting of the European Association for Computational Linguistics (EACL),</booktitle>
<pages>59--66</pages>
<contexts>
<context position="2548" citStr="Clark, 2003" startWordPosition="356" endWordPosition="357">MM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justifiably so, as the most discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al., 1992) and the incorporation of morphological features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approaches, our model is a principled generative model and does not include any hand tuned language specific features. Inspired by previous successful approaches (Brown et al., 1992), we develo</context>
<context position="4987" citStr="Clark (2003)" startWordPosition="714" endWordPosition="715">is of treebanked data, and moreover conveys the significant advantage that all the tags for a given word type can be updated at the same time, allowing very efficient inference using the exchange algorithm. This model has been popular for language modelling and bilingual word alignment, and an implementation with improved inference called mkcls (Och, 1999)1 has become a standard part of statistical machine translation systems. The HMM ignores orthographic information, which is often highly indicative of a word’s partof-speech, particularly so in morphologically rich languages. For this reason Clark (2003) extended Brown et al. (1992)’s HMM by incorporating a character language model, allowing the modelling of limited morphology. Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate, we develop a Bayesian model with a rich prior for smoothing the parameter estimates, allowing us to move to a trigram model. A number of researchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supp</context>
<context position="11331" citStr="Clark (2003)" startWordPosition="1732" endWordPosition="1733">are inferred during training, as described in 3.1. The tag-specific emission distributions, Ej, are also drawn from a PYP prior, Ej|aE, bE, C ∼ PYP(aE, bE, Cj) . We consider two different settings for the base distribution Cj: 1) a simple uniform distribution over the vocabulary (denoted HMM for the experiments in section 4); and 2) a character-level language model (denoted HMM+LM). In many languages morphological regularities correlate strongly with a word’s part-of-speech (e.g., suffixes in English), which we hope to capture using a basic character language model. This model was inspired by Clark (2003) Dj U Bj Tij Cjk t1 t2 t3 Ej w1 w2 w3 ... 867 Figure 2: The conditioning structure of the hierarchical PYP with an embedded character language models. who applied a character level distribution to the single class HMM (Brown et al., 1992). We formulate the character-level language model as a bigram model over the character sequence comprising word wl, wlk|wlk−1, tl,C - Ctlwlk−1 Cjk|aC, bC, Dj - PYP(aC, bC, Dj) Dj|aD, bD - PYP(aD, bD, Uniform) , where k indexes the characters in the word and, in a slight abuse of notation, the character itself, w0 and is set to a special sentinel value denoting</context>
<context position="18597" citStr="Clark, 2003" startWordPosition="2996" endWordPosition="2997">ustomers. particular import when the same tag occurs twice in This approximation is tight for small n, and therea row such as in Figure 2. fore it should be effective in the case of the local Many HMMs used for inducing word classes for Gibbs sampler where only three trigrams are being language modelling include the restriction that all resampled. For the type based resampling where occurrences of a word type always appear with the large numbers of n are involved (consider resamsame class throughout the corpus (Brown et al., pling the), this approximation can deviate from the 1992; Och, 1999; Clark, 2003). Our second sampler actual value due to errors accumulated in the recur(PYP-1HMM) restricts inference to taggings which sion. Figure 3 illustrates a simulation demonstrating adhere to this one tag per type restriction. This that the approximation is a close match for small a restriction permits efficient inference techniques in and n but underestimates the true value for high a which all tags of all occurrences of a word type are updated in parallel. Similar techniques have been used for models with Dirichlet priors (Liang et al., 869 2Marginalisation is intractable in general, i.e. for the 1</context>
<context position="23121" citStr="Clark, 2003" startWordPosition="3745" endWordPosition="3746">erges more quickly so we use two hundred samples for these models. All reported results are the mean of three sampling runs. An important detail for any unsupervised learning algorithm is its initialisation. We used slightly different initialisation for each of our inference strategies. For the unrestricted HMM we randomly assigned each word token to a class. For the restricted 1HMM we use a similar initialiser to a=0.9 a=0.8 a=0.5 a=0.1 2 4 6 8 10 12 expected tables 870 Frequency Model M-1 VM Prototype meta-model (CGS10) 76.1 68.8 MEMM (BBDK10) 75.5 - mkcls (Och, 1999) 73.7 65.6 MLE 1HMM-LM (Clark, 2003)∗ 71.2 65.5 BHMM (GG07) 63.2 56.2 PR (Ganchev et al., 2010)∗ 62.5 54.8 Trigram PYP-HMM 69.8 62.6 Trigram PYP-1HMM 76.0 68.0 Trigram PYP-1HMM-LM 77.5 69.7 Bigram PYP-HMM 66.9 59.2 Bigram PYP-1HMM 72.9 65.9 Trigram DP-HMM 68.1 60.0 Trigram DP-1HMM 76.0 68.0 Trigram DP-1HMM-LM 76.8 69.8 Table 1: WSJ performance comparing previous work to our own model. The columns display the many-to-1 accuracy and the V measure, both averaged over 5 independent runs. Our model was run with the local sampler (HMM), the type-level sampler (1HMM) and also with the character LM (1HMM-LM). Also shown are results usin</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark. 2003. Combining distributional and morphological information for part of speech induction. In Proceedings of the tenth Annual Meeting of the European Association for Computational Linguistics (EACL), pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
<author>Sharon Goldwater</author>
</authors>
<title>Inducing tree-substitution grammars.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3053--3096</pages>
<contexts>
<context position="7633" citStr="Cohn et al., 2010" startWordPosition="1128" endWordPosition="1131">2)’s one-class HMM. Our work also seeks to enforce both forms of sparsity, by developing an algorithm for type-level inference under the one class constraint. This work differs from previous Bayesian models in that we explicitly model a complex backoff path using a hierachical prior, such that our model jointly infers distributions over tag trigrams, bigrams and unigrams and whole words and their character level representation. This smoothing is critical to ensure adequate generalisation from small data samples. Research in language modelling (Teh, 2006b; Goldwater et al., 2006a) and parsing (Cohn et al., 2010) has shown that models employing Pitman-Yor priors can significantly outperform the more frequently used Dirichlet priors, especially where complex hierarchical relationships exist between latent variables. In this work we apply these advances to unsupervised PoS tagging, developing a HMM smoothed using a Pitman-Yor process prior. 866 3 The PYP-HMM We develop a trigram hidden Markov model which models the joint probability of a sequence of latent tags, t, and words, w, as L+1 Pθ(t, w) = Pθ(tl|tl−1, tl−2)Pθ(wl|tl) , l=1 where L = |w |= |t |and t0 = t−1 = tL+1 = $ are assigned a sentinel value t</context>
</contexts>
<marker>Cohn, Blunsom, Goldwater, 2010</marker>
<rawString>Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. Journal of Machine Learning Research, pages 3053–3096.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jo˜ao Grac¸a</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>99--2001</pages>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 99:2001–2049, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mark Johnson</author>
</authors>
<title>A comparison of bayesian estimators for unsupervised hidden markov model pos taggers.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>344--352</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6328" citStr="Gao and Johnson, 2008" startWordPosition="920" endWordPosition="923">2008; Ravi and Knight, 2009). These systems achieve 1Available from http://fjoch.com/mkcls.html. much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). These authors took a Bayesian approach using a Dirichlet prior to encourage sparse distributions over the word types emitted from each tag. Conversely, Ganchev et al. (2010) developed a technique to optimize the more desirable reverse property of the word types having a sparse posterior distribution over tags. Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampli</context>
</contexts>
<marker>Gao, Johnson, 2008</marker>
<rawString>Jianfeng Gao and Mark Johnson. 2008. A comparison of bayesian estimators for unsupervised hidden markov model pos taggers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 344–352, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the ACL (ACL-2007),</booktitle>
<pages>744--751</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2362" citStr="Goldwater and Griffiths, 2007" startWordPosition="328" endWordPosition="331">ormance of our unsupervised model approaches 865 that of many existing semi-supervised systems, despite our method not receiving any human input. In this paper we present a Bayesian hidden Markov model (HMM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justifiably so, as the most discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al., 1992) and the incorporation of morphological features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approac</context>
<context position="5681" citStr="Goldwater and Griffiths, 2007" startWordPosition="827" endWordPosition="830">language model, allowing the modelling of limited morphology. Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate, we develop a Bayesian model with a rich prior for smoothing the parameter estimates, allowing us to move to a trigram model. A number of researchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supplied a priori (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Ravi and Knight, 2009). These systems achieve 1Available from http://fjoch.com/mkcls.html. much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffit</context>
<context position="8615" citStr="Goldwater and Griffiths, 2007" startWordPosition="1289" endWordPosition="1292"> develop a trigram hidden Markov model which models the joint probability of a sequence of latent tags, t, and words, w, as L+1 Pθ(t, w) = Pθ(tl|tl−1, tl−2)Pθ(wl|tl) , l=1 where L = |w |= |t |and t0 = t−1 = tL+1 = $ are assigned a sentinel value to denote the start or end of the sentence. A key decision in formulating such a model is the smoothing of the tag trigram and emission distributions, which would otherwise be too difficult to estimate from small datasets. Prior work in unsupervised PoS induction has employed simple smoothing techniques, such as additive smoothing or Dirichlet priors (Goldwater and Griffiths, 2007; Johnson, 2007), however this body of work has overlooked recent advances in smoothing methods used for language modelling (Teh, 2006b; Goldwater et al., 2006b). Here we build upon previous work by developing a PoS induction model smoothed with a sophisticated non-parametric prior. Our model uses a hierarchical Pitman-Yor process prior for both the transition and emission distributions, encoding a backoff path from complex distributions to successsively simpler ones. The use of complex distributions (e.g., over tag trigrams) allows for rich expressivity when sufficient evidence is available, </context>
<context position="15575" citStr="Goldwater and Griffiths, 2007" startWordPosition="2497" endWordPosition="2500"> customer sits at an 2010), though one must be careful to manage the empty table in a restaurant, a new customer is also dependencies between multiple draws from the possaid to enter the restaurant for its base distribution. terior. That is, each table at one level is equivalent to a cus- The dependency on table counts in the conditional tomer at the next deeper level, creating the invari- distributions complicates the process of drawing ants: K−hi = n−ui and K−ui = n−i , where u = tl−1 samples for both our models. In the non-hierarchical indicates the unigram backoff context of h. The model (Goldwater and Griffiths, 2007) these recursion terminates at the lowest level where the dependencies can easily be accounted for by base distribution is static. The hierarchical setting incrementing customer counts when such a allows for the modelling of elaborate backoff paths dependence occurs. In our model we would need to from rich and complex structure to successively sim- sum over all possible table assignments that result pler structures. in the same tagging, at all levels in the hierarchy: Gibbs samplers Both our Gibbs samplers perform tag trigrams, bigrams and unigrams; and also words, the same calculation of cond</context>
<context position="16915" citStr="Goldwater and Griffiths (2007)" startWordPosition="2702" endWordPosition="2705">ll trigrams and emis- this rather onerous marginalisation2 we instead use sions affected by a sampling action, and then rein- expected table counts to calculate the conditional troducing the trigrams one at a time, conditioning distributions for sampling. Unfortunately we their probabilities on the updated counts and table know of no efficient algorithm for calculating the configurations as we progress. expected table counts, so instead develop a novel The first local Gibbs sampler (PYP-HMM) approximation updates a single tag assignment at a time, in a En+1 [Ki] ^ En [Ki] + similar fashion to Goldwater and Griffiths (2007). (aUEn [K] + bU)P0(i) (3) Changing one tag affects three trigrams, with (n − En [Ki] bU) + (aUEn [K] + bU)P0(i) posterior where Ki is the number of tables for the tag uniP(tl|z−l, t−l, w) a P(tl±2, wl|z−l±2, t−l±2) , gram i of which there are n + 1 occurrences, En [ ] where l±2 denotes the range l−2,l−1,l,l+1,l+2. denotes an expectation after observing n items and The joint distribution over the three trigrams con- En [K] = Ej En [Kj]. This formulation defines tained in tl±2 can be calculated using the PYP for- a simple recurrence starting with the first customer mulation. This calculation is</context>
<context position="23919" citStr="Goldwater and Griffiths, 2007" startWordPosition="3870" endWordPosition="3873"> 66.9 59.2 Bigram PYP-1HMM 72.9 65.9 Trigram DP-HMM 68.1 60.0 Trigram DP-1HMM 76.0 68.0 Trigram DP-1HMM-LM 76.8 69.8 Table 1: WSJ performance comparing previous work to our own model. The columns display the many-to-1 accuracy and the V measure, both averaged over 5 independent runs. Our model was run with the local sampler (HMM), the type-level sampler (1HMM) and also with the character LM (1HMM-LM). Also shown are results using Dirichlet Process (DP) priors by fixing a = 0. The system abbreviations are CGS10 (Christodoulopoulos et al., 2010), BBDK10 (Berg-Kirkpatrick et al., 2010) and GG07 (Goldwater and Griffiths, 2007). Starred entries denote results reported in CGS10. Clark (2003), assigning each of the k most frequent word types to its own class, and then randomly dividing the rest of the types between the classes. As a baseline we report the performance of mkcls (Och, 1999) on all test corpora. This model seems not to have been evaluated in prior work on unsupervised PoS tagging, which is surprising given its consistently good performance. First we present our results on the most frequently reported evaluation, the WSJ sections of the Penn. Treebank, along with a number of state-of-the-art results previo</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully bayesian approach to unsupervised part-of-speech tagging. In Proc. of the 45th Annual Meeting of the ACL (ACL-2007), pages 744–751, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006),</booktitle>
<location>Sydney.</location>
<contexts>
<context position="2426" citStr="Goldwater et al., 2006" startWordPosition="337" endWordPosition="340">emi-supervised systems, despite our method not receiving any human input. In this paper we present a Bayesian hidden Markov model (HMM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justifiably so, as the most discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al., 1992) and the incorporation of morphological features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approaches, our model is a principled generative model and does not inc</context>
<context position="7599" citStr="Goldwater et al., 2006" startWordPosition="1122" endWordPosition="1125">which underperformed Brown et al. (1992)’s one-class HMM. Our work also seeks to enforce both forms of sparsity, by developing an algorithm for type-level inference under the one class constraint. This work differs from previous Bayesian models in that we explicitly model a complex backoff path using a hierachical prior, such that our model jointly infers distributions over tag trigrams, bigrams and unigrams and whole words and their character level representation. This smoothing is critical to ensure adequate generalisation from small data samples. Research in language modelling (Teh, 2006b; Goldwater et al., 2006a) and parsing (Cohn et al., 2010) has shown that models employing Pitman-Yor priors can significantly outperform the more frequently used Dirichlet priors, especially where complex hierarchical relationships exist between latent variables. In this work we apply these advances to unsupervised PoS tagging, developing a HMM smoothed using a Pitman-Yor process prior. 866 3 The PYP-HMM We develop a trigram hidden Markov model which models the joint probability of a sequence of latent tags, t, and words, w, as L+1 Pθ(t, w) = Pθ(tl|tl−1, tl−2)Pθ(wl|tl) , l=1 where L = |w |= |t |and t0 = t−1 = tL+1 =</context>
<context position="9459" citStr="Goldwater et al., 2006" startWordPosition="1415" endWordPosition="1418">ion model smoothed with a sophisticated non-parametric prior. Our model uses a hierarchical Pitman-Yor process prior for both the transition and emission distributions, encoding a backoff path from complex distributions to successsively simpler ones. The use of complex distributions (e.g., over tag trigrams) allows for rich expressivity when sufficient evidence is available, while the hierarchy affords a means of backing off to simpler and more easily estimated distributions otherwise. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006a; Goldwater et al., 2006b), and has been shown to be a generalisation of Kneser-Ney smoothing, widely recognised as the best smoothing method for language modelling (Chen and Goodman, 1996). The model is depicted in the plate diagram in Figure 1. At its centre is a standard trigram HMM, which generates a sequence of tags and words, tl|tl−1, tl−2, T ∼ Ttl−1,tl−2 wl|tl, E ∼ Etl . Figure 1: Plate diagram representation of the trigram HMM. The indexes i and j range over the set of tags and k ranges over the set of characters. Hyper-parameters have been omitted from the figure for clarity. The trigram transition distribut</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Tom Griffiths, and Mark Johnson. 2006a. Contextual dependencies in unsupervised word segmentation. In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006), Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators. In</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 18,</booktitle>
<pages>459--466</pages>
<editor>Y. Weiss, B. Sch¨olkopf, and J. Platt, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2426" citStr="Goldwater et al., 2006" startWordPosition="337" endWordPosition="340">emi-supervised systems, despite our method not receiving any human input. In this paper we present a Bayesian hidden Markov model (HMM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justifiably so, as the most discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al., 1992) and the incorporation of morphological features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approaches, our model is a principled generative model and does not inc</context>
<context position="7599" citStr="Goldwater et al., 2006" startWordPosition="1122" endWordPosition="1125">which underperformed Brown et al. (1992)’s one-class HMM. Our work also seeks to enforce both forms of sparsity, by developing an algorithm for type-level inference under the one class constraint. This work differs from previous Bayesian models in that we explicitly model a complex backoff path using a hierachical prior, such that our model jointly infers distributions over tag trigrams, bigrams and unigrams and whole words and their character level representation. This smoothing is critical to ensure adequate generalisation from small data samples. Research in language modelling (Teh, 2006b; Goldwater et al., 2006a) and parsing (Cohn et al., 2010) has shown that models employing Pitman-Yor priors can significantly outperform the more frequently used Dirichlet priors, especially where complex hierarchical relationships exist between latent variables. In this work we apply these advances to unsupervised PoS tagging, developing a HMM smoothed using a Pitman-Yor process prior. 866 3 The PYP-HMM We develop a trigram hidden Markov model which models the joint probability of a sequence of latent tags, t, and words, w, as L+1 Pθ(t, w) = Pθ(tl|tl−1, tl−2)Pθ(wl|tl) , l=1 where L = |w |= |t |and t0 = t−1 = tL+1 =</context>
<context position="9459" citStr="Goldwater et al., 2006" startWordPosition="1415" endWordPosition="1418">ion model smoothed with a sophisticated non-parametric prior. Our model uses a hierarchical Pitman-Yor process prior for both the transition and emission distributions, encoding a backoff path from complex distributions to successsively simpler ones. The use of complex distributions (e.g., over tag trigrams) allows for rich expressivity when sufficient evidence is available, while the hierarchy affords a means of backing off to simpler and more easily estimated distributions otherwise. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006a; Goldwater et al., 2006b), and has been shown to be a generalisation of Kneser-Ney smoothing, widely recognised as the best smoothing method for language modelling (Chen and Goodman, 1996). The model is depicted in the plate diagram in Figure 1. At its centre is a standard trigram HMM, which generates a sequence of tags and words, tl|tl−1, tl−2, T ∼ Ttl−1,tl−2 wl|tl, E ∼ Etl . Figure 1: Plate diagram representation of the trigram HMM. The indexes i and j range over the set of tags and k ranges over the set of characters. Hyper-parameters have been omitted from the figure for clarity. The trigram transition distribut</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Tom Griffiths, and Mark Johnson. 2006b. Interpolating between types and tokens by estimating power-law generators. In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 459–466. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>320--327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5650" citStr="Haghighi and Klein, 2006" startWordPosition="823" endWordPosition="826">incorporating a character language model, allowing the modelling of limited morphology. Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate, we develop a Bayesian model with a rich prior for smoothing the parameter estimates, allowing us to move to a trigram model. A number of researchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supplied a priori (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Ravi and Knight, 2009). These systems achieve 1Available from http://fjoch.com/mkcls.html. much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treeb</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 320– 327, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesnt EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proc. of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP-2007),</booktitle>
<pages>296--305</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6304" citStr="Johnson, 2007" startWordPosition="918" endWordPosition="919">a and Johnson, 2008; Ravi and Knight, 2009). These systems achieve 1Available from http://fjoch.com/mkcls.html. much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). These authors took a Bayesian approach using a Dirichlet prior to encourage sparse distributions over the word types emitted from each tag. Conversely, Ganchev et al. (2010) developed a technique to optimize the more desirable reverse property of the word types having a sparse posterior distribution over tags. Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence be</context>
<context position="8631" citStr="Johnson, 2007" startWordPosition="1293" endWordPosition="1294">v model which models the joint probability of a sequence of latent tags, t, and words, w, as L+1 Pθ(t, w) = Pθ(tl|tl−1, tl−2)Pθ(wl|tl) , l=1 where L = |w |= |t |and t0 = t−1 = tL+1 = $ are assigned a sentinel value to denote the start or end of the sentence. A key decision in formulating such a model is the smoothing of the tag trigram and emission distributions, which would otherwise be too difficult to estimate from small datasets. Prior work in unsupervised PoS induction has employed simple smoothing techniques, such as additive smoothing or Dirichlet priors (Goldwater and Griffiths, 2007; Johnson, 2007), however this body of work has overlooked recent advances in smoothing methods used for language modelling (Teh, 2006b; Goldwater et al., 2006b). Here we build upon previous work by developing a PoS induction model smoothed with a sophisticated non-parametric prior. Our model uses a hierarchical Pitman-Yor process prior for both the transition and emission distributions, encoding a backoff path from complex distributions to successsively simpler ones. The use of complex distributions (e.g., over tag trigrams) allows for rich expressivity when sufficient evidence is available, while the hierar</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesnt EM find good HMM POS-taggers? In Proc. of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP-2007), pages 296–305, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Simple type-level unsupervised pos tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>853--861</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6668" citStr="Lee et al. (2010)" startWordPosition="977" endWordPosition="980">in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). These authors took a Bayesian approach using a Dirichlet prior to encourage sparse distributions over the word types emitted from each tag. Conversely, Ganchev et al. (2010) developed a technique to optimize the more desirable reverse property of the word types having a sparse posterior distribution over tags. Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)’s one-class HMM. Our work also seeks to enforce both forms of sparsity, by developing an algorithm for type-level inference under the one class constraint. This work differs from previous Bayesian models in that we explicitly model a complex backoff </context>
<context position="25967" citStr="Lee et al. (2010)" startWordPosition="4216" endWordPosition="4219">sing a Dirichlet Process prior (DP). We see that for all models the use of the PYP provides some gain for the HMM, but diminishes for the 1HMM. This is perhaps a consequence of the expected table count approximation for the typesampled PYP-1HMM: the DP relies less on the table counts than the PYP. If we restrict the model to bigrams we see a considerable drop in performance. Note that the bigram PYP-HMM outperforms the closely related BHMM (the main difference being that we smooth tag bigrams with unigrams). It is also interesting to compare the bigram PYP-1HMM to the closely related model of Lee et al. (2010). That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%, well below that of our model. Figures 4 and 5 provide insight into the behavior of the sampling algorithms. The former shows that both our models and mkcls induce a more uniform distribution over tags than specified by the treebank. It is unclear whether it is desirable for models to exhibit behavior closer to the treebank, which dedicates separate tags to very infrequent phenomena while lumping the large range of noun types into a single category. The graph in Figure 5 sh</context>
</contexts>
<marker>Lee, Haghighi, Barzilay, 2010</marker>
<rawString>Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2010. Simple type-level unsupervised pos tagging. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 853–861, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Type-based MCMC.</title>
<date>2010</date>
<booktitle>In North American Association for Computational Linguistics (NAACL).</booktitle>
<marker>Liang, Jordan, Klein, 2010</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2010. Type-based MCMC. In North American Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="21889" citStr="Marcus et al., 1993" startWordPosition="3541" endWordPosition="3544">amma(10, 0.1)). All the hyper-parameters are resampled after every 5th sample of the corpus. The result of this hyperparameter inference is that there are no user tunable parameters in the model, an important feature that we believe helps explain its consistently high performance across test settings. 4 Experiments We perform experiments with a range of corpora to both investigate the properties of our proposed models and inference algorithms, as well as to establish their robustness across languages and domains. For our core English experiments we report results on the entire Penn. Treebank (Marcus et al., 1993), while for other languages we use the corpora made available for the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We report results using the manyto-one (M-1) and v-measure (VM) metrics considered best by the evaluation of Christodoulopoulos et al. (2010). M-1 measures the accuracy of the model after mapping each predicted class to its most frequent corresponding tag, while VM is a variant of the F-measure which uses conditional entropy analogies of precision and recall. The log-posterior for the HMM sampler levels off after a few hundred samples, so we report results after five hundred. T</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1999</date>
<booktitle>In Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>71--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4733" citStr="Och, 1999" startWordPosition="677" endWordPosition="678">isation in language models. Brown et al. (1992) presented a simple first-order HMM which restricted word types to always be generated from the same class. Though PoS induction was not their aim, this restriction is largely validated by empirical analysis of treebanked data, and moreover conveys the significant advantage that all the tags for a given word type can be updated at the same time, allowing very efficient inference using the exchange algorithm. This model has been popular for language modelling and bilingual word alignment, and an implementation with improved inference called mkcls (Och, 1999)1 has become a standard part of statistical machine translation systems. The HMM ignores orthographic information, which is often highly indicative of a word’s partof-speech, particularly so in morphologically rich languages. For this reason Clark (2003) extended Brown et al. (1992)’s HMM by incorporating a character language model, allowing the modelling of limited morphology. Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate</context>
<context position="18583" citStr="Och, 1999" startWordPosition="2994" endWordPosition="2995">ubsequent customers. particular import when the same tag occurs twice in This approximation is tight for small n, and therea row such as in Figure 2. fore it should be effective in the case of the local Many HMMs used for inducing word classes for Gibbs sampler where only three trigrams are being language modelling include the restriction that all resampled. For the type based resampling where occurrences of a word type always appear with the large numbers of n are involved (consider resamsame class throughout the corpus (Brown et al., pling the), this approximation can deviate from the 1992; Och, 1999; Clark, 2003). Our second sampler actual value due to errors accumulated in the recur(PYP-1HMM) restricts inference to taggings which sion. Figure 3 illustrates a simulation demonstrating adhere to this one tag per type restriction. This that the approximation is a close match for small a restriction permits efficient inference techniques in and n but underestimates the true value for high a which all tags of all occurrences of a word type are updated in parallel. Similar techniques have been used for models with Dirichlet priors (Liang et al., 869 2Marginalisation is intractable in general, </context>
<context position="23085" citStr="Och, 1999" startWordPosition="3739" endWordPosition="3740">ive hundred. The 1HMM sampler converges more quickly so we use two hundred samples for these models. All reported results are the mean of three sampling runs. An important detail for any unsupervised learning algorithm is its initialisation. We used slightly different initialisation for each of our inference strategies. For the unrestricted HMM we randomly assigned each word token to a class. For the restricted 1HMM we use a similar initialiser to a=0.9 a=0.8 a=0.5 a=0.1 2 4 6 8 10 12 expected tables 870 Frequency Model M-1 VM Prototype meta-model (CGS10) 76.1 68.8 MEMM (BBDK10) 75.5 - mkcls (Och, 1999) 73.7 65.6 MLE 1HMM-LM (Clark, 2003)∗ 71.2 65.5 BHMM (GG07) 63.2 56.2 PR (Ganchev et al., 2010)∗ 62.5 54.8 Trigram PYP-HMM 69.8 62.6 Trigram PYP-1HMM 76.0 68.0 Trigram PYP-1HMM-LM 77.5 69.7 Bigram PYP-HMM 66.9 59.2 Bigram PYP-1HMM 72.9 65.9 Trigram DP-HMM 68.1 60.0 Trigram DP-1HMM 76.0 68.0 Trigram DP-1HMM-LM 76.8 69.8 Table 1: WSJ performance comparing previous work to our own model. The columns display the many-to-1 accuracy and the V measure, both averaged over 5 independent runs. Our model was run with the local sampler (HMM), the type-level sampler (1HMM) and also with the character LM (1</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz Josef Och. 1999. An efficient method for determining bilingual word classes. In Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, pages 71–76, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP),</booktitle>
<pages>504--512</pages>
<contexts>
<context position="5734" citStr="Ravi and Knight, 2009" startWordPosition="835" endWordPosition="838">. Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate, we develop a Bayesian model with a rich prior for smoothing the parameter estimates, allowing us to move to a trigram model. A number of researchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supplied a priori (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Ravi and Knight, 2009). These systems achieve 1Available from http://fjoch.com/mkcls.html. much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Thes</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP), pages 504–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>354--362</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="5624" citStr="Smith and Eisner, 2005" startWordPosition="819" endWordPosition="822"> et al. (1992)’s HMM by incorporating a character language model, allowing the modelling of limited morphology. Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate, we develop a Bayesian model with a rich prior for smoothing the parameter estimates, allowing us to move to a trigram model. A number of researchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supplied a priori (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Ravi and Knight, 2009). These systems achieve 1Available from http://fjoch.com/mkcls.html. much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distri</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 354–362, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>985--992</pages>
<contexts>
<context position="2401" citStr="Teh, 2006" startWordPosition="335" endWordPosition="336">y existing semi-supervised systems, despite our method not receiving any human input. In this paper we present a Bayesian hidden Markov model (HMM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justifiably so, as the most discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al., 1992) and the incorporation of morphological features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approaches, our model is a principled generati</context>
<context position="7574" citStr="Teh, 2006" startWordPosition="1120" endWordPosition="1121"> in a model which underperformed Brown et al. (1992)’s one-class HMM. Our work also seeks to enforce both forms of sparsity, by developing an algorithm for type-level inference under the one class constraint. This work differs from previous Bayesian models in that we explicitly model a complex backoff path using a hierachical prior, such that our model jointly infers distributions over tag trigrams, bigrams and unigrams and whole words and their character level representation. This smoothing is critical to ensure adequate generalisation from small data samples. Research in language modelling (Teh, 2006b; Goldwater et al., 2006a) and parsing (Cohn et al., 2010) has shown that models employing Pitman-Yor priors can significantly outperform the more frequently used Dirichlet priors, especially where complex hierarchical relationships exist between latent variables. In this work we apply these advances to unsupervised PoS tagging, developing a HMM smoothed using a Pitman-Yor process prior. 866 3 The PYP-HMM We develop a trigram hidden Markov model which models the joint probability of a sequence of latent tags, t, and words, w, as L+1 Pθ(t, w) = Pθ(tl|tl−1, tl−2)Pθ(wl|tl) , l=1 where L = |w |= </context>
<context position="9434" citStr="Teh, 2006" startWordPosition="1413" endWordPosition="1414">a PoS induction model smoothed with a sophisticated non-parametric prior. Our model uses a hierarchical Pitman-Yor process prior for both the transition and emission distributions, encoding a backoff path from complex distributions to successsively simpler ones. The use of complex distributions (e.g., over tag trigrams) allows for rich expressivity when sufficient evidence is available, while the hierarchy affords a means of backing off to simpler and more easily estimated distributions otherwise. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006a; Goldwater et al., 2006b), and has been shown to be a generalisation of Kneser-Ney smoothing, widely recognised as the best smoothing method for language modelling (Chen and Goodman, 1996). The model is depicted in the plate diagram in Figure 1. At its centre is a standard trigram HMM, which generates a sequence of tags and words, tl|tl−1, tl−2, T ∼ Ttl−1,tl−2 wl|tl, E ∼ Etl . Figure 1: Plate diagram representation of the trigram HMM. The indexes i and j range over the set of tags and k ranges over the set of characters. Hyper-parameters have been omitted from the figure for clarity. The tri</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Y. W. Teh. 2006a. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 985–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical bayesian language model based on pitman-yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>985--992</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2401" citStr="Teh, 2006" startWordPosition="335" endWordPosition="336">y existing semi-supervised systems, despite our method not receiving any human input. In this paper we present a Bayesian hidden Markov model (HMM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justifiably so, as the most discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al., 1992) and the incorporation of morphological features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approaches, our model is a principled generati</context>
<context position="7574" citStr="Teh, 2006" startWordPosition="1120" endWordPosition="1121"> in a model which underperformed Brown et al. (1992)’s one-class HMM. Our work also seeks to enforce both forms of sparsity, by developing an algorithm for type-level inference under the one class constraint. This work differs from previous Bayesian models in that we explicitly model a complex backoff path using a hierachical prior, such that our model jointly infers distributions over tag trigrams, bigrams and unigrams and whole words and their character level representation. This smoothing is critical to ensure adequate generalisation from small data samples. Research in language modelling (Teh, 2006b; Goldwater et al., 2006a) and parsing (Cohn et al., 2010) has shown that models employing Pitman-Yor priors can significantly outperform the more frequently used Dirichlet priors, especially where complex hierarchical relationships exist between latent variables. In this work we apply these advances to unsupervised PoS tagging, developing a HMM smoothed using a Pitman-Yor process prior. 866 3 The PYP-HMM We develop a trigram hidden Markov model which models the joint probability of a sequence of latent tags, t, and words, w, as L+1 Pθ(t, w) = Pθ(tl|tl−1, tl−2)Pθ(wl|tl) , l=1 where L = |w |= </context>
<context position="9434" citStr="Teh, 2006" startWordPosition="1413" endWordPosition="1414">a PoS induction model smoothed with a sophisticated non-parametric prior. Our model uses a hierarchical Pitman-Yor process prior for both the transition and emission distributions, encoding a backoff path from complex distributions to successsively simpler ones. The use of complex distributions (e.g., over tag trigrams) allows for rich expressivity when sufficient evidence is available, while the hierarchy affords a means of backing off to simpler and more easily estimated distributions otherwise. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006a; Goldwater et al., 2006b), and has been shown to be a generalisation of Kneser-Ney smoothing, widely recognised as the best smoothing method for language modelling (Chen and Goodman, 1996). The model is depicted in the plate diagram in Figure 1. At its centre is a standard trigram HMM, which generates a sequence of tags and words, tl|tl−1, tl−2, T ∼ Ttl−1,tl−2 wl|tl, E ∼ Etl . Figure 1: Plate diagram representation of the trigram HMM. The indexes i and j range over the set of tags and k ranges over the set of characters. Hyper-parameters have been omitted from the figure for clarity. The tri</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006b. A hierarchical bayesian language model based on pitman-yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 985–992, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Mark Johnson</author>
</authors>
<title>A bayesian lda-based model for semi-supervised part-of-speech tagging.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 20,</booktitle>
<pages>1521--1528</pages>
<editor>In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5710" citStr="Toutanova and Johnson, 2008" startWordPosition="831" endWordPosition="834">delling of limited morphology. Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate, we develop a Bayesian model with a rich prior for smoothing the parameter estimates, allowing us to move to a trigram model. A number of researchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supplied a priori (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Ravi and Knight, 2009). These systems achieve 1Available from http://fjoch.com/mkcls.html. much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao </context>
</contexts>
<marker>Toutanova, Johnson, 2008</marker>
<rawString>Kristina Toutanova and Mark Johnson. 2008. A bayesian lda-based model for semi-supervised part-of-speech tagging. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1521–1528. MIT Press, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>