<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001065">
<title confidence="0.9967175">
Parsing Syntactic and Semantic Dependencies with
Two Single-Stage Maximum Entropy Models *
</title>
<author confidence="0.998024">
Hai Zhao and Chunyu Kit
</author>
<affiliation confidence="0.999088">
Department of Chinese, Translation and Linguistics
City University of Hong Kong
</affiliation>
<address confidence="0.998953">
83 Tat Chee Avenue, Kowloon, Hong Kong, China
</address>
<email confidence="0.998173">
haizhao@cityu.edu.hk, ctckit@cityu.edu.hk
</email>
<sectionHeader confidence="0.995626" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999815">
This paper describes our system to carry
out the joint parsing of syntactic and se-
mantic dependencies for our participation
in the shared task of CoNLL-2008. We il-
lustrate that both syntactic parsing and se-
mantic parsing can be transformed into a
word-pair classification problem and im-
plemented as a single-stage system with
the aid of maximum entropy modeling.
Our system ranks the fourth in the closed
track for the task with the following per-
formance on the WSJ+Brown test set:
81.44% labeled macro F1 for the overall
task, 86.66% labeled attachment for syn-
tactic dependencies, and 76.16% labeled
F1 for semantic dependencies.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937846153846">
The joint parsing of syntactic and semantic depen-
dencies introduced by the shared task of CoNLL-
08 is more complicated than syntactic dependency
parsing or semantic role labeling alone (Surdeanu
et al., 2008). For semantic parsing, in particu-
lar, a dependency-based representation is given but
the predicates involved are unknown, and we also
have nominal predicates besides the verbal ones.
All these bring about more difficulties for learning.
This paper presents our research for participation
in the CoNLL-2008 shared task, with a highlight
on our strategy to select learning framework and
features for maximum entropy learning.
</bodyText>
<footnote confidence="0.970846">
This study is supported by CERG grant 9040861 (CityU
1318/03H) and CityU Strategic Research Grant 7002037.
@ 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999333333333333">
The rest of the paper is organized as follows.
The next section presents the technical details of
our system and Section 3 its evaluation results.
Section 4 looks into a few issues concerning our
forthcoming work for this shared task, and Section
5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.987549" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999976857142857">
For the sake of efficiency, we opt for the maximum
entropy model with Gaussian prior as our learning
model for both the syntactic and semantic depen-
dency parsing. Our implementation of the model
adopts L-BFGS algorithm for parameter optimiza-
tion as usual (Liu and Nocedal, 1989). No addi-
tional feature selection techniques are applied.
Our system consists of three components to deal
with syntactic and semantic dependency parsing
and word sense determination, respectively. Both
parsing is formulated as a single-stage word-pair
classification problem, and the latter is carried out
by a search through the NomBank (Meyers et al.,
2004) or the PropBank (Palmer et al., 2005)1.
</bodyText>
<subsectionHeader confidence="0.996881">
2.1 Syntactic Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999978416666667">
We use a shift-reduce scheme to implement syn-
tactic dependency parsing as in (Nivre, 2003). It
takes a step-wised, history- or transition-based ap-
proach. It is basically a word-by-word method
with a projective constraint. In each step, the clas-
sifier checks a word pair, e.g., TOP, the top of a
stack for processed words, and, NEXT, the first
word in the unprocessed word sequence, in order
to determine if a dependent label should be as-
signed to them. Besides two arc-building actions,
a shift action and a reduce action are also defined
to meet the projective constraint, as follows.
</bodyText>
<footnote confidence="0.993712">
1These two dictionaries that we used are downloaded from
CoNLL-2008 official website.
</footnote>
<page confidence="0.943888">
203
</page>
<reference confidence="0.2445925">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 203–207
Manchester, August 2008
</reference>
<figure confidence="0.7275125">
Table 2: Features for Syntactic Parsing
Notation
Meaning
Clique in the top of stack
The first clique below the top of stack, etc.
The first (second) clique in the unprocessed
sequence, etc.
Dependent label
Head
Leftmost child
Rightmost child
Right nearest child
Word form
Word lemma
Predicted PoS tag
Split Y , which may be form, lemma or pos.
</figure>
<figureCaption confidence="0.518905">
’s, e.g., ‘s.dprel’ means dependent label
of the clique in the top of stack
</figureCaption>
<bodyText confidence="0.777967333333333">
Feature combination, i.e., ‘s.pos/i.pos’
means s.pos and i.pos together as a
feature function.
</bodyText>
<figure confidence="0.979287075">
The current predicate candidate
The current argument candidate
s
s−1,...
i, i+1,...
dprel
h
lm
rm
rn
form
lemma
pos
sp Y
.
/
p
a
Basic
Extension
itself, its previous two and next two Ys, and
all bigrams within the five-clique window,
(x is s or i, and Y is form, lemma or pos.)
(x is s or i, and Y is form, lemma or pos.)
(x is s or s−1 and Y is pos, sp lemma
or sp pos)
s.h.sp form
s.dprel
s.lm.dprel
s.rn.dprel
i.lm.sp pos
s.lm.dprel/s.dprel
s.lm.sp pos/s.sp pos
s.h.sp pos/s.sp pos
x.sp posirootscore (x is s or i.)
s.sp pos/i.sp posIpairscore
s.curroot.sp pos/i.sp pos
x.sp Y
x.Y
x.Y/i.Y
</figure>
<tableCaption confidence="0.971845">
Table 1: Feature Notations
</tableCaption>
<listItem confidence="0.9999015">
1. Left-arc: Add an arc from NEXT to TOP and
pop the stack.
2. Right-arc: Add an arc from TOP to NEXT
and push NEXT onto the stack.
3. Reduce: Pop TOP from the stack.
4. Shift: Push NEXT onto the stack.
</listItem>
<bodyText confidence="0.9999852">
We implement a left-to-right arc-eager parsing
model in a way that the parser scan through an in-
put sequence from left to right and the right depen-
dents are attached to their heads as soon as possible
(Hall et al., 2007). To construct a single-stage sys-
tem, we extend the left-/right-arc actions to their
correspondent multi-label actions as necessary. In-
cluding 32 left-arc and 66 right-arc actions, alto-
gether a 100-class problem is yielded for the pars-
ing action classification for this shared task.
Since only projective sequences can be handled
by the shift-reduce scheme, we apply the pseudo-
projective transformation introduced by (Nivre and
Nilsson, 2005) to projectivize those non-projective
sequences. Our statistics show that only 7.6% se-
quences and less than 1% dependencies in the cor-
pus provided for training are non-projective. Thus,
we use a simplified strategy to projectivize an input
sequence. Firstly, we simply replace the head of a
non-projective dependency by its original head’s
head but without any additional dependent label
encoding for the purpose of deprojectivizing the
output during decoding. Secondly, if the above
standard projectivization step cannot eliminate all
non-projective dependencies in a sequence, then
the word with the shortest sequence (rather than
dependent tree) distance to the original head will
be chosen as the head of a non-projective depen-
dency. In practice, the above two-step projectiviza-
tion procedure can eliminate all non-projective de-
pendencies in all sequences. Our purpose here is to
provide as much data as possible for training, and
only projective sequences are input for training and
output for decoding.
While memory-based and margin-based learn-
ing approaches such as support vector machines
are popularly applied to shift-reduce parsing, our
work provides evidence that the maximum en-
tropy model can achieve a comparative perfor-
mance with the aid of a suitable feature set. With
feature notations in Table 1, we use a feature set as
shown in Table 2 for syntactic parsing.
Here, we explain ‘rootscore’, ‘pairscore’ and
curroot in Table 2. Both rootscore and pairscore
return the log frequency for an event in the training
corpus. The former counts a given split PoS occur-
ring as ROOT, and the latter two split PoS’s com-
bination associated with a dependency label. The
feature curroot returns the root of a partial parsing
tree that includes a specified node.
</bodyText>
<subsectionHeader confidence="0.998324">
2.2 Semantic Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999981142857143">
Assuming no predicates overtly known, we keep
using a word-pair classifier to perform semantic
parsing through a single-stage processing. Specif-
ically, we specify the first word in a word pair as
a predicate candidate (i.e., a semantic head, and
noted as p in our feature representation) and the
next as an argument candidate (i.e., a semantic de-
</bodyText>
<page confidence="0.9887">
204
</page>
<table confidence="0.998935242424243">
Basic Extension
x.sp Y itself, its previous and next cliques, and
x.sp pos all bigrams within the three-clique window.
x.Y (Y is form or lemma.)a
p.Y/i.Y itself, its previous and next two cliques, and
all bigrams within the five-clique window.
(Y is form, lemma or pos.)
(Y is sp lemma or sp pos.)
a is the same as p
x.is Verb or Noun
bankAdvice
b a.h.sp form
x.dprel
x.lm.dprel
p.rm.dprel
p.lm.sp pos
a.lm.dprel/a.dprel
a.lm.sp pos/a.sp pos
a.sp Y/a.dprel (Y is lemma or pos.)
x.sp lemma/x.h.sp form
p.sp lemma/p.h.sp pos
p.sp pos/p.dprel
a.preddirc
p.voice/a.preddird
x.posSeqe
x.dprelSeqf
a.dpTreeLevelg
a/p|dpRelation
a/p|SharedPosPath
a/p|SharedDprelPath
a/p|x.posPath
a/p|x.dprelPath
a/p|dprelPath
</table>
<tableCaption confidence="0.524912">
ax is p or a throughout the whole table.
</tableCaption>
<bodyText confidence="0.9021871">
bThis and the following features are all concerned with a
known syntactic dependency tree.
cpreddir: the direction to the current predicate candidate.
dvoice: if the syntactic head of p is be and p is not ended
with -ing, then p is passive.
eposSeq: PoS tag sequence of all syntactic children
fdprelSeq: syntactic dependent label sequence of all syn-
tactic children
gdpTreeLevel: the level in the syntactic parse tree, counted
from the leaf node.
</bodyText>
<tableCaption confidence="0.940324">
Table 3: Features for Semantic Parsing
</tableCaption>
<bodyText confidence="0.9995434375">
pendent, and noted as a). We do not differenti-
ate between nominal and verbal predicates and our
system handles them in in exactly the same way.
If decoding outputs show that no arguments can
be found for a predicate candidate in the decoding
output, then this candidate will be naturally dis-
carded from the output predicate list.
When no constraint available, however, all word
pairs in the an input sequence must be considered,
leading to very poor efficiency in computation for
no gain in effectiveness. Thus, the training sample
needs to be pruned properly.
For predicate, only nouns and verbs are consid-
ered possible candidates. That is, all words with-
out a split PoS in these two categories are filtered
out. Many prepositions are also marked as pred-
icate in the training corpus, but their arguments’
roles are ‘SU’, which are not counted the official
evaluation.
For argument, a dependency version of the prun-
ing algorithm in (Xue and Palmer, 2004) is used to
find, in an iterative way, the current syntactic head
and its siblings in a parse tree in a constituent-
based representation. In this representation, the
head of a phrase governs all its sisters in the tree,
as illustrated in the conversion of constituents to
dependencies in (Lin, 1995). In our implementa-
tion, the following equivalent algorithm is applied
to select argument candidates from a syntactic de-
pendency parse tree.
Initialization: Set the given predicate candi-
date as the current node;
</bodyText>
<listItem confidence="0.84334875">
(1) The current node and all of its syntactic chil-
dren are selected as argument candidates.
(2) Reset the current node to its syntactic head
and repeat step (1) until the root is reached.
</listItem>
<bodyText confidence="0.999972857142857">
This algorithm can cover 98.5% arguments while
reducing about 60% of the training samples, ac-
cording to our statistics. However, this is achieved
at the price of including a syntactic parse tree as
part of the input for semantic parsing.
The feature set listed in Table 3 is adopted for
our semantic parsing, some of which are borrowed
from (Hacioglu, 2004). Among them, dpTreeRela-
tion returns the relationship of a and p in a syntac-
tic parse tree. Its possible values include parent,
sibling, child, uncle, grand parent
etc. Note that there is always a path to the ROOT in
the syntactic parse tree for either a or p. Along the
common part of these two paths, SharedDprelPath
returns the sequence of dependent labels collected
from each node, and SharedPosPath returns the
corresponding sequence of PoS tags. x.dprelPath
and x.posPath return the PoS tag sequence from x
to the beginnings of SharedDprelPath and Shared-
PosPath, respectively. a/p|dprelPath returns the
concatenation of a.dprelPath and p.dprelPath.
We may have an example to show how the fea-
ture bankAdvice works. Firstly, the current pro-
cessed semantic role labels and argument candi-
date direction are checked. Specifically, they are
the arguments AO and Al that have been marked
before the predicate candidate p and the current ar-
gument identification direction after p. Secondly,
</bodyText>
<page confidence="0.997196">
205
</page>
<table confidence="0.9995146">
UAS LAS Label-Acc.
Development 88.78 85.85 91.14
WSJ 89.86 87.52 92.47
Brown 85.03 79.83 86.71
WSJ+Brown 89.32 86.66 91.83
</table>
<tableCaption confidence="0.99657">
Table 4: The Results of Syntactic Parsing (%)
</tableCaption>
<table confidence="0.999971222222222">
Data Precision Recall F-score
Development 79.76 72.25 75.82
Label. WSJ 80.57 74.97 77.67
Brown 66.28 61.29 63.69
WSJ+Brown 79.03 73.49 76.16
Development 89.58 81.15 85.16
Unlab. WSJ 89.48 83.26 86.26
Brown 83.14 76.88 79.89
WSJ+Brown 88.79 82.57 85.57
</table>
<tableCaption confidence="0.999781">
Table 5: The Results of Semantic Parsing (%)
</tableCaption>
<bodyText confidence="0.999882875">
each example2 of p in NomBank or PropBank that
depends on the split PoS tag of p is checked if
it partially matches the current processed role la-
bels. If a unique example exists in this form, e.g.,
Before:A0-A1; After:A3, then this feature
returns A3 as feature value. If no matched or mul-
tiple matched examples exist, then this feature re-
turns a default value.
</bodyText>
<subsectionHeader confidence="0.999578">
2.3 Word Sense Determination
</subsectionHeader>
<bodyText confidence="0.9999515">
The shared task of CoNLL-2008 for word sense
disambiguation task is to determine the sense of an
output predicate. Our system carries out this task
by searching for a right example in the given Nom-
Bank or PropBank. The semantic role set scheme
of each example for an output predicate is checked.
If a scheme is found to match the output seman-
tic role set of a predicate, then the corresponding
sense for the first match is chosen; otherwise the
system outputs ‘01’ as the default sense.
</bodyText>
<sectionHeader confidence="0.973713" genericHeader="method">
3 Evaluation Results
</sectionHeader>
<bodyText confidence="0.999944166666667">
Our evaluation is carried out on a 64-bit ubuntu
Linux installed server with double dual-core AMD
Opteron processors of 2.8GHz and 8GB memory.
The full training set for CoNLL-2008 is used to
train the maximum entropy model. The training
for the syntactic parser costs about 200 hours and
</bodyText>
<footnote confidence="0.8636665">
2The term “example” means a chunk in NomBank
or PropBank, which demonstrates how semantic roles
occur around a specified predicate. For example, for
a sense item of the predicate access in PropBank,
we first have &lt;arg n=&amp;quot;0&amp;quot;&gt;a computer&lt;/arg&gt;
&lt;rel&gt;access&lt;/rel&gt; &lt;arg n=&amp;quot;1&amp;quot;&gt;its
memory&lt;/arg&gt;, and then a role set scheme for this
sense as Before:A0;After:A1.
</footnote>
<table confidence="0.999959647058824">
Data Precision Recall F-score
Development 82.80 79.05 80.88
Label. WSJ 84.05 81.25 82.62
Macro Brown 73.05 70.56 71.78
WSJ+Brown 82.85 80.08 81.44
Development 89.18 84.97 87.02
Unlab. WSJ 89.67 86.56 88.09
Macro Brown 84.08 80.96 82.49
WSJ+Brown 89.06 85.94 87.47
Development 83.69 80.71 82.17
Label. WSJ 85.07 82.88 83.96
Micro Brown 75.14 73.09 74.10
WSJ+Brown 83.98 81.80 82.88
Development 89.06 85.90 87.45
Unlab. WSJ 89.72 87.42 88.56
Micro Brown 84.38 82.07 83.21
WSJ+Brown 89.14 86.83 87.97
</table>
<tableCaption confidence="0.989216">
Table 6: Overall Scores (%)
</tableCaption>
<bodyText confidence="0.9983785">
4.1GB memory and that for the semantic parser
costs about 170 hours and 4.9GB memory. The
running time in each case is the sum of all running
time for all threads involved. When a parallel opti-
mization technique is applied to speedup the train-
ing, the time can be reduced to about 1/3.5 of the
above.
The official evaluation results for our system are
presented in Tables 4, 5 and 6. Following the
official guideline of CoNLL-2008, we use unla-
beled attachment score (UAS), labeled attachment
score (LAS) and label accuracy to assess the per-
formance of syntactic dependency parsing. For
semantic parsing, the unlabeled scores metric the
identification performance and the labeled scores
the overall performance of semantic labeling.
</bodyText>
<sectionHeader confidence="0.995996" genericHeader="method">
4 To Do
</sectionHeader>
<bodyText confidence="0.9976576">
Although we are unable to follow our plan to do
more than what we have done for this shared task,
because of the inadequate computational resource
and limited time, we have a number of techniques
in our anticipation to bring in further performance
improvement.
While expecting to accomplish the joint infer-
ence of syntactic and semantic parsing, we only
have time to complete a system with the former to
enhance the latter. But we did have experiments in
the early stage of our work to show that a syntactic
dependency parser can make use of available se-
mantic dependency information to enhance its per-
formance by 0.5-1% 3.
Most errors in our syntactic parsing are related
</bodyText>
<footnote confidence="0.988972">
3We used the outputs of a semantic parser, either predicted
or gold-standard, as features for syntactic parsing.
</footnote>
<page confidence="0.997796">
206
</page>
<bodyText confidence="0.9999787">
to the dependencies of comma and prepositions.
We need to take care of them, for PP attachment
is also crucial to the success of semantic parsing.
Extra effort is paid, as illustrated in previous work
such as (Xue and Palmer, 2004), to handle such
cases, especially when a PP is involved. We find in
our data that about 1% arguments occur as a grand-
child of a predicate through PP attachment.
Syntactic parsing contributes crucially to the
overall performance of the joint parsing by pro-
viding a solid basis for further semantic parsing.
Thus there is reason to believe that improvement
of syntactic dependency parsing can be more in-
fluential than that of semantic parsing to the overall
improvement. Only one model was used for syn-
tactic parsing in our system, in contrast to the exist-
ing work using an ensemble technique for further
performance enhancement, e.g., (Hall et al., 2007).
Again, the latter means much more computational
cost should be taken.
Though it was not done before submission dead-
line, we also tried to enhance the semantic parsing
with some more sophisticated inputs from the syn-
tactic parsing. One is predicted syntactic parsed
tree input that may be created by cross-validation
rather than the gold-standard syntactic input that
our submitted semantic parser was actually trained
on. Another is the n-best outputs of the syntactic
parser. However, only the single-best output of the
syntactic parser was actually used.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999989714285714">
As presented in the above sections, our system to
participate in the CoNLL-2008 shared task is im-
plemented as two single-stage maximum entropy
learning. We have tackled both syntactic and se-
mantic parsing as a word-pair classification prob-
lem. Despite the simplicity of this approach, our
system has produced promising results.
</bodyText>
<sectionHeader confidence="0.997513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99997025">
We wish to thank Dr. Wenliang Chen of NICT,
Japan for helpful discussions on dependency pars-
ing, and two anonymous reviewers for their valu-
able comments.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873705882353">
Hacioglu, Kadri. 2004. Semantic role labeling us-
ing dependency trees. In Proceedings of the 20th
international conference on Computational Linguis-
tics (COLING-2004), pages 1273–1276, Geneva,
Switzerland, August 23rd-27th.
Hall, Johan, Jens Nilsson, Joakim Nivre,
G¨ulsen Eryiˇgit, Be´ata Megyesi, Mattias Nils-
son, and Markus Saers. 2007. Single malt or
blended? a study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 933–939,
Prague, Czech, June.
Lin, Dekang. 1995. A dependency-based method for
evaluating broad-coverage parser. In Proceedings
of the Fourteenth International Joint Conference on
Artificial Intelligence (IJCAI-95), pages 1420–1425,
Montr´eal, Qu´ebec, Canada, August 20-25.
Liu, Dong C. and Jorge Nocedal. 1989. On the lim-
ited memory bfgs method for large scale optimiza-
tion. Mathematical Programming, 45:503–528.
Meyers, Adam, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The nombank project:
An interim report. In Proceedings of HLT/NAACL
Workshop on Frontiers in Corpus Annotation, pages
24–31, Boston, Massachusetts, USA, May 6.
Nivre, Joakim and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL-2005), pages 99–106, Ann
Arbor, Michigan, USA, June 25-30.
Nivre, Joakim. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT 03), pages 149–160, Nancy, France, April 23-
25.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Lluis M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
Xue, Nianwen and Martha Palmer. 2004. Cal-
ibrating features for semantic role labeling. In
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2004), pages 88–94,
Barcelona, Spain, July 25-26.
</reference>
<page confidence="0.99802">
207
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928752">
<title confidence="0.997639">Parsing Syntactic and Semantic Dependencies with Single-Stage Maximum Entropy Models</title>
<author confidence="0.996731">Hai Zhao</author>
<author confidence="0.996731">Chunyu</author>
<affiliation confidence="0.9982005">Department of Chinese, Translation and City University of Hong</affiliation>
<address confidence="0.992844">83 Tat Chee Avenue, Kowloon, Hong Kong,</address>
<email confidence="0.988426">haizhao@cityu.edu.hk,ctckit@cityu.edu.hk</email>
<abstract confidence="0.997038294117647">This paper describes our system to carry out the joint parsing of syntactic and semantic dependencies for our participation in the shared task of CoNLL-2008. We illustrate that both syntactic parsing and semantic parsing can be transformed into a word-pair classification problem and implemented as a single-stage system with the aid of maximum entropy modeling. Our system ranks the fourth in the closed track for the task with the following performance on the WSJ+Brown test set: 81.44% labeled macro F1 for the overall task, 86.66% labeled attachment for syntactic dependencies, and 76.16% labeled F1 for semantic dependencies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>CoNLL</author>
</authors>
<date>2008</date>
<booktitle>Proceedings of the 12th Conference on Computational Natural Language Learning,</booktitle>
<pages>203--207</pages>
<location>Manchester,</location>
<marker>CoNLL, 2008</marker>
<rawString>CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 203–207 Manchester, August 2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
</authors>
<title>Semantic role labeling using dependency trees.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics (COLING-2004),</booktitle>
<pages>1273--1276</pages>
<location>Geneva, Switzerland,</location>
<marker>Hacioglu, 2004</marker>
<rawString>Hacioglu, Kadri. 2004. Semantic role labeling using dependency trees. In Proceedings of the 20th international conference on Computational Linguistics (COLING-2004), pages 1273–1276, Geneva, Switzerland, August 23rd-27th.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Joakim Nivre</author>
<author>G¨ulsen Eryiˇgit</author>
<author>Be´ata Megyesi</author>
<author>Mattias Nilsson</author>
<author>Markus Saers</author>
</authors>
<title>Single malt or blended? a study in multilingual parser optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>933--939</pages>
<location>Prague, Czech,</location>
<marker>Hall, Nilsson, Nivre, Eryiˇgit, Megyesi, Nilsson, Saers, 2007</marker>
<rawString>Hall, Johan, Jens Nilsson, Joakim Nivre, G¨ulsen Eryiˇgit, Be´ata Megyesi, Mattias Nilsson, and Markus Saers. 2007. Single malt or blended? a study in multilingual parser optimization. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 933–939, Prague, Czech, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A dependency-based method for evaluating broad-coverage parser.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95),</booktitle>
<pages>1420--1425</pages>
<location>Montr´eal, Qu´ebec, Canada,</location>
<marker>Lin, 1995</marker>
<rawString>Lin, Dekang. 1995. A dependency-based method for evaluating broad-coverage parser. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), pages 1420–1425, Montr´eal, Qu´ebec, Canada, August 20-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory bfgs method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>45--503</pages>
<contexts>
<context position="2424" citStr="Liu and Nocedal, 1989" startWordPosition="364" endWordPosition="367">s.org/licenses/by-nc-sa/3.0/). Some rights reserved. The rest of the paper is organized as follows. The next section presents the technical details of our system and Section 3 its evaluation results. Section 4 looks into a few issues concerning our forthcoming work for this shared task, and Section 5 concludes the paper. 2 System Description For the sake of efficiency, we opt for the maximum entropy model with Gaussian prior as our learning model for both the syntactic and semantic dependency parsing. Our implementation of the model adopts L-BFGS algorithm for parameter optimization as usual (Liu and Nocedal, 1989). No additional feature selection techniques are applied. Our system consists of three components to deal with syntactic and semantic dependency parsing and word sense determination, respectively. Both parsing is formulated as a single-stage word-pair classification problem, and the latter is carried out by a search through the NomBank (Meyers et al., 2004) or the PropBank (Palmer et al., 2005)1. 2.1 Syntactic Dependency Parsing We use a shift-reduce scheme to implement syntactic dependency parsing as in (Nivre, 2003). It takes a step-wised, history- or transition-based approach. It is basical</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Liu, Dong C. and Jorge Nocedal. 1989. On the limited memory bfgs method for large scale optimization. Mathematical Programming, 45:503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>The nombank project: An interim report.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL Workshop on Frontiers in Corpus Annotation,</booktitle>
<pages>24--31</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="2783" citStr="Meyers et al., 2004" startWordPosition="417" endWordPosition="420">fficiency, we opt for the maximum entropy model with Gaussian prior as our learning model for both the syntactic and semantic dependency parsing. Our implementation of the model adopts L-BFGS algorithm for parameter optimization as usual (Liu and Nocedal, 1989). No additional feature selection techniques are applied. Our system consists of three components to deal with syntactic and semantic dependency parsing and word sense determination, respectively. Both parsing is formulated as a single-stage word-pair classification problem, and the latter is carried out by a search through the NomBank (Meyers et al., 2004) or the PropBank (Palmer et al., 2005)1. 2.1 Syntactic Dependency Parsing We use a shift-reduce scheme to implement syntactic dependency parsing as in (Nivre, 2003). It takes a step-wised, history- or transition-based approach. It is basically a word-by-word method with a projective constraint. In each step, the classifier checks a word pair, e.g., TOP, the top of a stack for processed words, and, NEXT, the first word in the unprocessed word sequence, in order to determine if a dependent label should be assigned to them. Besides two arc-building actions, a shift action and a reduce action are </context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Meyers, Adam, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The nombank project: An interim report. In Proceedings of HLT/NAACL Workshop on Frontiers in Corpus Annotation, pages 24–31, Boston, Massachusetts, USA, May 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudoprojective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL-2005),</booktitle>
<pages>99--106</pages>
<location>Ann Arbor, Michigan, USA,</location>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Nivre, Joakim and Jens Nilsson. 2005. Pseudoprojective dependency parsing. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL-2005), pages 99–106, Ann Arbor, Michigan, USA, June 25-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03),</booktitle>
<pages>149--160</pages>
<location>Nancy, France,</location>
<marker>Nivre, 2003</marker>
<rawString>Nivre, Joakim. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03), pages 149–160, Nancy, France, April 23-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="2821" citStr="Palmer et al., 2005" startWordPosition="424" endWordPosition="427">opy model with Gaussian prior as our learning model for both the syntactic and semantic dependency parsing. Our implementation of the model adopts L-BFGS algorithm for parameter optimization as usual (Liu and Nocedal, 1989). No additional feature selection techniques are applied. Our system consists of three components to deal with syntactic and semantic dependency parsing and word sense determination, respectively. Both parsing is formulated as a single-stage word-pair classification problem, and the latter is carried out by a search through the NomBank (Meyers et al., 2004) or the PropBank (Palmer et al., 2005)1. 2.1 Syntactic Dependency Parsing We use a shift-reduce scheme to implement syntactic dependency parsing as in (Nivre, 2003). It takes a step-wised, history- or transition-based approach. It is basically a word-by-word method with a projective constraint. In each step, the classifier checks a word pair, e.g., TOP, the top of a stack for processed words, and, NEXT, the first word in the unprocessed word sequence, in order to determine if a dependent label should be assigned to them. Besides two arc-building actions, a shift action and a reduce action are also defined to meet the projective co</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluis M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Surdeanu, Mihai, Richard Johansson, Adam Meyers, Lluis M`arquez, and Joakim Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP-2004),</booktitle>
<pages>88--94</pages>
<location>Barcelona, Spain,</location>
<marker>Xue, Palmer, 2004</marker>
<rawString>Xue, Nianwen and Martha Palmer. 2004. Calibrating features for semantic role labeling. In 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP-2004), pages 88–94, Barcelona, Spain, July 25-26.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>