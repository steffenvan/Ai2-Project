<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.624514">
Using Synonym Relations In Chinese Collocation Extraction
</title>
<author confidence="0.99251">
Wanyin Li
</author>
<affiliation confidence="0.811053">
Department of Computing,
The Hong Kong Polytechnic University,
</affiliation>
<address confidence="0.372019">
Hung Hom, Kowloon, Hong Kong
</address>
<email confidence="0.983539">
cswyli@comp.polyu.edu.hk
</email>
<author confidence="0.984742">
Qin Lu
</author>
<affiliation confidence="0.803822">
Department of Computing,
The Hong Kong Polytechnic University,
</affiliation>
<address confidence="0.363892">
Hung Hom, Kowloon, Hong Kong
</address>
<email confidence="0.956921">
csluqin @comp.polyu.edu.hk
</email>
<author confidence="0.997774">
Ruifeng Xu
</author>
<affiliation confidence="0.998086">
Department of Computing, The Hong Kong Polytechnic University,
</affiliation>
<address confidence="0.539741">
Hung Hom, Kowloon, Hong Kong
</address>
<email confidence="0.893289">
csrfxu@comp.polyu.edu.hk
</email>
<bodyText confidence="0.994470962962963">
“ ” rather than “ ”,
Abstract “ ”rather than “ ”.
A challenging task in Chinese collocation
extraction is to improve both the precision and
recall rate. Most lexical statistical methods
including Xtract face the problem of unable to
extract collocations with lower frequencies than
a given threshold. This paper presents a method
where HowNet is used to find synonyms using a
similarity function. Based on such synonym
information, we have successfully extracted
synonymous collocations which normally cannot
be extracted using the lexical statistical
approach. We applied synonyms mapping to
each headword to extract more synonymous
word bi-grams. Our evaluation over 60MB
tagged corpus shows that we can extract
synonymous collocations that occur with very
low frequency, sometimes even for collocations
that occur only once in the training set.
Comparing to a collocation extraction system
based on Xtract, we have reached the precision
rate of 43% on word bi-grams for a set of 9
headwords, almost 50% improvement from
precision rate of 30% in the Xtract system.
Furthermore, it improves the recall rate of word
bi-gram collocation extraction by 30%.
</bodyText>
<sectionHeader confidence="0.999194" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989801280701755">
A Chinese collocation is a recurrent and
conventional expression of words which holds
syntactic and semantic relations. A widely adopted
definition given by Benson (Benson 1990) stated
that “a collocation is an arbitrary and recurrent
word combination.” For example, we say “warm
greetings” rather than “hot greetings”, “broad
daylight” rather than “bright daylight”. Similarly,
” are three nouns
with similar meanings, however, we say
Study in collocation extraction using lexical
statistics has gained some insights to the issues
faced in collocation extraction (Church and Hanks
1990, Smadja 1993, Choueka 1993, Lin 1998). As
the lexical statistical approach is developed based
on the “recurrence” property of collocations, only
collocations with reasonably good recurrence can
be extracted. Collocations with low occurrence
frequency cannot be extracted, thus affecting the
recall rate. The precision rate using the lexical
statistics approach can reach around 60% if both
word bi-gram extraction and n-gram extractions
are taking into account (Smadja 1993, Lin 1997
and Lu et al. 2003). The low precision rate is
mainly due to the low precision rate of word bi-
gram extractions as only about 30% - 40%
precision rate can be achieved for word bi-grams.
In this paper, we propose a different approach to
find collocations with low recurrences. The main
idea is to make use of synonym relations to extract
synonymous collocations. Lin (Lin 1997)
described a distributional hypothesis that if two
words have similar set of collocations, they are
probably similar. In HowNet, Liu Qun (Liu et al.
2002) defined the word similarity as two words
that can substitute each other in the context and
keep the sentence consistent in syntax and
semantic structure. That means, naturally, two
similar words are very close to each other and they
can be used in place of the other in certain context.
For example, we may either say “ ”or “ ”
as and are semantically close to each
other. We apply this lexical phenomenal after the
lexical statistics based extractor to find the low
frequency synonymous collocations, thus
increasing recall rate.
in Chinese “
” “
” “
The rest of this paper is organized as follows.
Section 2 describes related existing collocation
extraction techniques based on both lexical
statistics and synonymous collocation. Section 3
describes our approach on collocation extraction.
Section 4 evaluates the proposed method. Section 5
draws our conclusion and presents possible future
work.
</bodyText>
<sectionHeader confidence="0.999606" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999958129411765">
Methods have proposed to extract collocations
based on lexical statistics. Choueka (Choueka
1993) applied quantitative selection criteria based
on frequency threshold to extract adjacent n-grams
(including bi-grams). Church and Hanks (Church
and Hanks 1990) employed mutual information to
extract both adjacent and distant bi-grams that tend
to co-occur within a fixed-size window. But the
method did not extend to extract n-grams. Smadja
(Smadja 1993) proposed a statistical model by
measuring the spread of the distribution of co-
occurring pairs of words with higher strength. This
method successfully extracted both adjacent and
distant bi-grams and n-grams. However, the
method failed to extract bi-grams with lower
frequency. The precision rate on bi-grams
collocation is very low, only around high 20% and
low 30%. Even though, it is difficult to measure
recall rate in collocation extraction (almost no
report on recall estimation), It is understood that
low occurrence collocations cannot be extracted.
Our research group has further applied the Xtract
system to Chinese (Lu et al. 2003) by adjusting the
parameters to optimize the algorithm for Chinese
and a new weighted algorithm was developed
based on mutual information to acquire word bi-
grams with one higher frequency word and one
lower frequency word. The result has achieved an
estimated 5% improvement in recall rate and a
15% improvement in precision comparing to the
Xtract system.
All of the above techniques do not take
advantage of the wide range of lexical resources
available including synonym information. Pearce
(Pearce 2001) presented a collocation extraction
technique that relies on a mapping from a word to
its synonyms for each of its senses. The underlying
intuitions is that if the difference between the
occurrence counts of one synonyms pair with
respect to a particular word was at least two, then
this was deemed sufficient to consider them as a
collocation. To apply this approach, knowledge in
word (concept) semantics and relations to other
words must be available such as the use of
WordNet. Dagan (Dagan 1997) applied similarity-
based smoothing method to solve the problem of
data sparseness in statistical natural language
processing. The experiments conducted in his later
works showed that this method achieved much
better results than back-off smoothing methods in
word sense disambiguation. Similarly, Hua Wu
(Wu and Zhou 2003) applied synonyms
relationship between two different languages to
automatically acquire English synonymous
collocation. This is the first time that the concept
synonymous collocation is proposed. A side
intuition raised here is that nature language is full
of synonymous collocations. As many of them
have low occurrences, they are failed to be
retrieved by lexical statistical methods. Even
though there are Chinese synonym dictionaries,
such as ( Tong Yi Ci Lin), the
dictionaries lack structured knowledge and
synonyms are too loosely defined to be used for
collocation extraction.
HowNet developed by Dong et al (Dong and
Dong 1999) is the best publicly available resource
on Chinese semantics. By making use of semantic
similarities of words, synonyms can be defined by
the closeness of their related concepts and the
closeness can be calculated. In Section 3, we
present our method to extract synonyms from
HowNet and using synonym relations to further
extract collocations.
Sun (Sun 1997) did a preliminary Quantitative
analysis on Chinese collocations based on their
arbitrariness, recurrence and the syntax structure.
The purpose of this study is to help differentiate if
a collocation is true or not according to the
quantitative factors. By observing the existence of
synonyms information in natural language use, we
consider it possible to identify different types of
collocations using more semantic and syntactic
information available. We discuss the basic ideas
in section 5..
</bodyText>
<sectionHeader confidence="0.980893" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.99733975">
Our method of extracting Chinese collocations
consists of three steps.
Step 1: Take the output of any lexical statistical
algorithm which extracts word bi-gram
collocations. The data is then sorted
according to each headword , Wh, with its co-
word, Wc, listed.
Step 2: For each headword Wh used to extract bi-
grams, we acquire its synonyms based on a
similarity function using HowNet. Any word
in HowNet having similarity value over a
threshold value is chosen as a synonym
headword Ws for additional extractions.
Step 3: For each synonym headword, Ws, and the
co-word Wc of Wh, as its synonym, if the bi-
gram (Ws , Wc) is not in the output of the
lexical statistical algorithm in Step one, take
this bi-gram (Ws , Wc) as a collocation if the
pair co-occurs in the corpus by additional
search to the corpus.
</bodyText>
<subsectionHeader confidence="0.999861">
3.1 Structure of HowNet
</subsectionHeader>
<bodyText confidence="0.994729041666667">
Different from WordNet or other synonyms
dictionary, HowNet describes words as a set of
concepts and each concept is described by a
set of primitives . The following lists for the
word , one of its corresponding concepts
In the above record, DEF is where the primitives
are specified. DEF contains up to four types of
primitives: the basic independent primitive
, the other independent
primitive , the relation primitive
, and the symbol primitive ,
where the basic independent primitive and the
other independent primitive are used to indicate the
semantics of a concept and the others are used to
indicate syntactical relationships. The similarity
model described in the next subsection will
consider both of these relationships.
The primitives are linked by a hierarchical tree
to indicate the parent-child relationships of the
primitives as shown in the following example:
This hierarchical structure provides a way to link
one concept with any other concept in HowNet,
and the closeness of concepts can be simulated by
the distance between two concepts.
</bodyText>
<subsectionHeader confidence="0.995368">
3.2 Similarity Model Based on HowNet
</subsectionHeader>
<bodyText confidence="0.999885454545455">
Liu Qun (Liu 2002) defined word similarity as
two words which can substitute each other in the
same context and still maintain the sentence
consistent syntactically and semantically. This is
very close to our definition of synonyms. Thus we
directly used their similarity function, which is
stated as follows.
A word in HowNet is defined as a set of
concepts and each concept is represented by
primitives. Thus, HowNet can be described by W,
a collection of n words, as:
</bodyText>
<equation confidence="0.9561632">
W = { w1, w2, ... wn}Each word wi is, in
turn, described by a set of concepts S as:
Wi = { Si1, Si2,...Six},
And, each concept Si is, in turn, described by a
set of primitives:
Si = { pi1, pi2 ...piy }
For each word pair, w1 and w2, the similarity
function is defined by
(w1 ,w2)= max Sim(S1i,S2j
i=1..n, j=1 ...m
</equation>
<bodyText confidence="0.999569">
where S1i is the list of concepts associated with W1
and S2j is the list of concepts associated with W2.
As any concept Si is presented by its primitives,
the similarity of primitives for any p1, and p2 of
the same type, can be expressed by the following
formula:
</bodyText>
<equation confidence="0.9870568">
Sim(p1,p2) = α )+α (2)
p
p2
(
1 ,
</equation>
<bodyText confidence="0.963645">
where
is an adjustable parameter set to 1.6,
and Dis(p1,
is the path length between
and
based on the semantic tree structure. The above
formula where
is a constant does not indicate
explicitly the fact that the depth of a pair of nodes
in the tree affects their similarity. For two pairs of
nodes
and
p4) with the same distance,
the deeper the depth is, the more commonly shared
ancestros they would have which should be
semantically closer to each other. In following two
tree structures, the pair of nodes
in the left
tree should be more similar than
</bodyText>
<equation confidence="0.946681272727273">
α
p2)
p1
p2
α
(p1 ,p2)
(p3
(p1,p2)
(p3 , p4) in the
right tree.
root
</equation>
<page confidence="0.704427">
P3
P4
</page>
<figure confidence="0.914053333333333">
Sim
)
1
) (
Dis
root
</figure>
<page confidence="0.875751">
p2
p1
</page>
<bodyText confidence="0.998123">
To indicate this observation, α is modified as a
function of tree depths of the nodes using the
formula α =min(d(p1), d(p2)) . Consequently, the
formula (2) is rewritten as formular (28) during the
experiment.
</bodyText>
<equation confidence="0.8214025">
d (p1 ), d (p 2 )) (28)
))
</equation>
<bodyText confidence="0.999960333333333">
where d(pi) is the depth of node pi in the tree . The
comparison of calculating the word similarity by
applying the formula (2) and (28) is shown in
Section 4.4.
Based on the DEF description in HowNet,
different primitive types play different roles only
some are directly related to semantics. To make
use of both the semantic and syntactic information
included in HowNet to describe a word, the
similarity of two concepts should take into
consideration of all primitive types with weighted
considerations and thus the formula is defined as
</bodyText>
<equation confidence="0.980491">
4 i
Sim (S1,S2)=∑β i∏Simj(p1j,p2j) (3)
</equation>
<bodyText confidence="0.999941333333333">
where βi is a weighting factor given in (Liu
2002) with the sum of β1 + β2 + β3 + β4 being 1,
and β1 &gt; β2 &gt; β3 &gt; β4. The distribution of the
weighting factors is given for each concept a priori
in HowNet to indicate the importance of primitive
pi in defining the corresponding concept S.
</bodyText>
<subsectionHeader confidence="0.998842">
3.3 Collocation Extraction
</subsectionHeader>
<bodyText confidence="0.99995">
In order to extract collocations from a corpus,
and to obtain result for Step 1 of our algorithm, we
used the collocation extraction algorithm
developed by the research group at the Hong Kong
Polytechnic University(Lu et al. 2003). The
extraction of bi-gram collocation is based on the
English Xtract(Smaja 1993) with improvements.
Based on the three Steps mentioned earlier, we will
present the extractions in each step in the
subsections.
</bodyText>
<subsectionHeader confidence="0.639738">
3.3.1 Bi-gram Extraction
</subsectionHeader>
<bodyText confidence="0.999428916666667">
Based on the lexical statistical model proposed
by Smadja in Xtract on extracting English
collocations, an improved algorithm was
developed for Chinese collocation by our research
group and the system is called CXtract. For easy of
understanding, we will explain the algorithm
briefly here. According to Xtract, word
cooccurence is denoted by a tripplet (wh, wi, d)
where wh is a given headword, wi is a co-word
appeared in the corpus in a distance d within the
window of [-5, 5]. The frequency fi of the co-word
wi in the window of [-5, 5] is defined as:
</bodyText>
<equation confidence="0.937165166666667">
5
fi f i j
= (4)
,
∑−=
j 5
</equation>
<bodyText confidence="0.968395333333333">
where fi, j is the frequency of the co-word at distance
j in the corpus within the window. The average
frequency of fi , denoted by f i, is given by
</bodyText>
<equation confidence="0.9165584">
5
fi f i j
= /1 0 (5)
∑=− ,
j 5
</equation>
<bodyText confidence="0.94135">
Then, the average frequency f , and the standard
deviation σ are defined by
</bodyText>
<equation confidence="0.9988362">
1 ∑ n 2
; σ = ( )
f i f
− (6)
n i=1
</equation>
<bodyText confidence="0.9279475">
The Strength of the co-occurrence for the pair
(wh, wi,), denoted by ki, is defined by
</bodyText>
<equation confidence="0.9905136">
k f i f
−
=
i
σ
</equation>
<bodyText confidence="0.994851">
Furthermore, the Spread of (wh, wi,),, denoted as
Ui, which characterizes the distribution of wi
around wh is define as:
</bodyText>
<equation confidence="0.9783902">
∑ , −
(f f )2
i j i
U = ;
i
</equation>
<bodyText confidence="0.998683333333333">
To eliminate the bi-grams with unlikely co-
occurrence, the following sets of threshold values
is defined:
</bodyText>
<equation confidence="0.988427">
f f
−
C k i
1: = ≥ K
i
σ
C2: Ui ≥ U0 (10)
C f i j ≥ f i + K ⋅ Ui
3: , ( 1 ) (11)
</equation>
<bodyText confidence="0.998244875">
However, the above statistical model given by
Smadja fails to extract the bi-grams with a much
higher frequency of wh but a relatively low
frequency word of wi,, For example, in the bi-
gram , freq ( ) is much lower than the
freq ( ). Therefore, we further defined a
weighted mutual information to extract this kind of
bi-grams:
</bodyText>
<equation confidence="0.985881666666667">
(w , )
w
h i ≥ R0
f w
( )
i
</equation>
<bodyText confidence="0.998989666666667">
As a result, the system should return a list of
triplets (wh, wi, d), where (wh, wi,) is considered
collocations.
</bodyText>
<equation confidence="0.563923866666667">
i
1
=1 j=
,
Sim
p2
(p
)
1
min(
=
,p2)+ min(d(p1),d(p
2
1
Dis
(p
1
n
f
fi
∑=
n i 1
, (7)
(8)
10
0 (9)
=
f
Ri
, (12)
</equation>
<subsectionHeader confidence="0.762228">
3.3.2 Synonyms Set
</subsectionHeader>
<bodyText confidence="0.9839035">
For each given headword wh, before taking it as
an input to extract its bi-grams directly, we fist
apply the similarity formula described in Equation
(1) to generate a set of synonyms headwords Wsyn:
</bodyText>
<equation confidence="0.988559">
Wsyn = {ws: Sim(wh,ws) &gt; θ} (13)
</equation>
<bodyText confidence="0.995459052631579">
Where 0 &lt;θ &lt;1 is an algorithm parameter which
is adjusted based on experience. We set it as 0.85
from the experiment because we would like to
balance the strength of the synonyms relationship
and the coverage of the synonyms set. The setting
of the parameter θ &lt; 0.85 weaks the similarity
strength of the extracted synonyms. For example,
for a given collocation “ ”, that is unlikely
to include the candidates “
”, we
hope to include the candidate synonymous
”
,
”. We will show the test of θ in the
section 4.2.
This synonyms headwords set provides the
possibility to extract the synonymous collocation
with the lower frequency that failed to be extracted
by lexical statistic.
</bodyText>
<subsectionHeader confidence="0.765375">
3.3.3 Synonymous Collocations
</subsectionHeader>
<bodyText confidence="0.999935739130435">
A phenomenal among the collocations in natural
language is that there are many synonymous
collocations exist. For example, ‘switch on light’
and ‘turn on light’, “
Due to the domain specification of the corpus,
some of the synonymous collocations may fail to
be extracted by the lexical statistic model because
of their lower frequency. Based on this
observation, this paper takes a further step. The
basic idea is for a bi-gram collocation (wh, wc, d )
we select the synonyms ws of wh with the
maximum similarity respect to all the concepts
contained by wh, we deem (ws, wc, d ) as a
collocation if its occurrence is greater than 1 in the
corpus. There are similar works discussed by
Pearce (Pearce 2001). .
For a given collocation (ws, wc,, d), if ws E Wsyn,
then we deem the triple (ws, wc,, d) as a
synonymous collocation with respect to the
collocation (wh, wc,, d) if the co-occurrence of (ws,
wc, , d) in the corpus is greater than one. Therefore,
we define the collection of synonymous
collocations Csyn as:
</bodyText>
<equation confidence="0.944712">
Csyn = {(ws,wc,d) : Freq(ws,w,d) &gt; 1} (14)
</equation>
<bodyText confidence="0.930437">
where ws E Wsyn.
</bodyText>
<sectionHeader confidence="0.991972" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9913435">
The performance of collocation is normally
evaluated by precision and recall as defined below.
</bodyText>
<equation confidence="0.707599666666667">
numberof correct ExtractedCollocatio ns
recall= (16)
total numberof actual Collocations
</equation>
<bodyText confidence="0.99968125">
To evaluate the performance of our approach, we
conducted a set of experiments based on 9 selected
headwords. A baseline system using only lexical
statistics given in 3.3.1 is used to get a set of
baseline data called Set A. The output using our
algorithm is called Set B. Results are checked by
hand for validation on what is true collocation and
what is not a true collocation.
</bodyText>
<tableCaption confidence="0.989942333333333">
Table 2. Sample table for the bi-grams that are
not true collocations
Table 1. Sample table for the true collocation
</tableCaption>
<equation confidence="0.672369666666667">
with headword “ ”
”, “ ”,
“ ”. On the other hand, by setting the
</equation>
<bodyText confidence="0.971041">
parameter θ &gt; 0.85 will limit the coverage of the
synonyms set and hence lose valuable synonyms.
For example, for a given bi-gram “
</bodyText>
<figure confidence="0.7270015">
collocations such as “
“ ”,
“
” and “ ”.
precision =
numberof correct Extracted Collocations (15)
total number of extracted
Collocations
</figure>
<tableCaption confidence="0.7204102">
Table 1 shows samples of extracted word bi-grams
using our algorithm that are considered
synonymous collocations for the headword “
Table 2 shows extracted bi-grams by our algorithm
that are not considered true collocations.
</tableCaption>
<subsectionHeader confidence="0.999492">
4.1 Test Set
</subsectionHeader>
<bodyText confidence="0.999685846153846">
Our experiment is based on a corpus of six
months tagged People Daily with 11 millions
number of words. For word bi-gram extractions,
we consider only content words, thus headwords
are selected from noun, verb and adjective only.
For evaluation purpose, we selected randomly 3
nouns, 3 verbs and 3 adjectives with frequency of
low, medium and high. Thus, in Step 1 of the
algorithm, 9 headwords were used to extract bi-
gram collocations from the corpus, and 253 pairs
of collocations were extracted. Evaluation by hand
has identified 77 true collocations in Set A test set.
The overall precision rate is 30% (see Table 3).
</bodyText>
<table confidence="0.999270714285714">
Noun+Verb
+Adjective
Headword 9
Extracted Bi-grams 253
True collocations using 77
lexical statistics only
Precision rate 30%
</table>
<tableCaption confidence="0.999963">
Table 3: Statistics in test set for set A
</tableCaption>
<bodyText confidence="0.999009416666667">
Using Step 2 of our algorithm, where 0=0.85 is
used, we have obtained 55 synonym headwords
(include the 9 headwords). Out of these 55
synonyms, 614 bi-gram pairs were then extracted
from the lexical statistics based algorithm, in
which 179 are consider true collocations. Then, by
applying Step 3 of our algorithm, we extracted an
additional 201 bi-gram pairs, among them, 178 are
considered true collocations. Therefore, using our
algorithm, the overall precision rate has achieved
43%, an improvement of almost 50%. The data is
summarized in Table 4.
</bodyText>
<table confidence="0.9765583">
n., v, and adj.
Synonyms headword 55
Bi-grams (lexical statistics) 614
Non-synonym collocations 179
(lexical statistics only)
Extracted synonym 201
collocations Step 2
True synonym collocations 178
using Step 2
Overall precision rate 43%
</table>
<tableCaption confidence="0.999939">
Table 4: Statistics in test set for mode B
</tableCaption>
<subsectionHeader confidence="0.94112">
4.2 The choice of θ
</subsectionHeader>
<bodyText confidence="0.999670727272727">
We also conducted a set of experiments to
choose the best value for the similarity function’s
threshold 0. We tested the best value of 0 with both
the precision rate and the estimated recall rate
using the so called remainder bi-grams. The
remainder bi-grams is the total number of bi-grams
extracted by the algorithm. When precision goes
up, the size of the result is smaller, which in a way
is an indicator of less recalled collocations. Figure
1 shows the precision rate and the estimated recall
rate in testing the value of 0.
</bodyText>
<figureCaption confidence="0.998388">
Figure 1. Precision Rate vs. value of 0
</figureCaption>
<bodyText confidence="0.913382">
From Figure 1, it is obvious that at 0=0.85 the
recall rate starts to drop more drastically without
much incentive for precision.
</bodyText>
<table confidence="0.9996204">
Extracted Bi- Extracted
grams using Synonyms
lexical Collocations
statistics using Step 2
(1.2,1.4,12) 465 328
(1.4,1.4,12) 457 304
(1.4,1.6,12) 394 288
(1.2,1.2,12) 513 382
(1.2,1.2,14) 503 407
(1.2,1.2,16) 481 413
</table>
<tableCaption confidence="0.99885">
Table 5: Value of (K0, K1, U0).
</tableCaption>
<subsectionHeader confidence="0.97799">
4.3 The test of (K0, K1, U0)
</subsectionHeader>
<bodyText confidence="0.9971519375">
The original threshold for CXtract is (1.2, 1.2, 12)
for the parameters (K0, K1, U0). However, with
synonyms collocations, we have also conducted
some experiments to see whether the parameters
should be adjusted. Table 5 shows the statistics to
test the value of (K0, K1, U0). The similarity
threshold 0 was fixed at 0.85 throughout the
experiments.
”.
The experimental shows that varying the value of
(k0, kj) does not bring any benefit to our algorithm.
However, increasing the value of u0 did improve
the extraction of synonymous collocations. Figure
2 shows that U0 =14 is a good trade-off for the
precision rate and the remainder Bi-grams. The basic
meaning behind the result is reasonable. According to
Smadja, U0 defined in the formula (8) represents the
co-occurrence distribution of the candidate
collocation (wh, wc) in the position of d (-5 ≤ d ≤
5). For a true collocation (wh, wc,, d), its co-
occurrence in the position d is much higher than in
other positions which leads to a peak in the co-
occurrence distribution. Therefore, it is selected by
the statistical algorithm based on the formula (10).
Based on the physical meaning behind, one way to
improve the precision rate is to increase the value of
the threshold U0. A side effect to an increased value
of U0 is that the recall is decreased because some
true collocations do not meet the condition of co-
occurrence greater than U0. Step 2 of the new
algorithm regains some true collocations lost
because of a higher U0. in Step 1.
</bodyText>
<figureCaption confidence="0.998543">
Figure 2. Precision Rate vs. Value of U0
</figureCaption>
<bodyText confidence="0.9779634">
4.4 The comparison of similarity calculation
based on formula (2) and (2ª)
Table 6 shows the similarity value given by
formula (2) where α is a constant given the value
1.6 and by formula (2ª) where α is replaced by a
function of the depths of the nodes. Results show
that (2ª) is more fine tuned and reflects the nature
of the data better. For example, and
are more similar than and .
and are much similar but not the same.
</bodyText>
<tableCaption confidence="0.823438">
Table 6: comparison of similarity calculation
</tableCaption>
<sectionHeader confidence="0.959092" genericHeader="conclusions">
5 Conclusion and Further Work
</sectionHeader>
<bodyText confidence="0.95549702173913">
In this paper, we have presented a method to
extract bi-gram collocations using lexical statistics
model with synonyms information. Our method
reaches the precision rate of 43% for the tested data.
Comparing to the precision of 30% using lexical
statistics only, our improvement is close to 50%. In
additional, the recall improved 30%. The contribution
is that we have made use of synonym information
which is plentiful in the natural language use and it
works well to supplement the shortcomings of lexical
statistical method.
Manning claimed that the lack of valid
substitution for a synonym is a characteristics of
collocations in general (Manning and Schutze
1999). To extend our work, we consider the use of
synonym information can be further applied to
help identify collocations of different types.
Our preliminary study has suggested that
collocation can be classified into 4 types:
Type 0 Collocation: Fully fixed collocation
which include some idioms, proverbs and sayings
such as “ ” “ ” and so on.
Type 1 Collocation: Fixed collocation in which
the appearance of one word implies the co-
occurrence of another one such as “
” “
”
” “
Type 2 Collocation: Strong collocation which
allows very limited substitution of the components,
for example, “ ”, “ ”,
” and so on.
Type 3 Collocation: Normal collocation which
allows more substitution of the components,
however a limitation is still required. For example,
“
“
“”
.
”.
By using synonym information and define
substitutability, we can validate whether
collocations are fixed collocations, strong
collocations with very limited substitutions, or
general collocations that can be substituted more
freely.
</bodyText>
<sectionHeader confidence="0.998876" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999777857142857">
Our great thanks to Dr. Liu Qun of the Chinese
Language Research Center of Peking University for
letting us share their data structure in the Synonyms
Similarity Calculation. This work is partially
supported by the Hong Kong Polytechnic
University (Project Code A-P203) and CERG
Grant (Project code 5087/01E)
</bodyText>
<sectionHeader confidence="0.999286" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999955019230769">
M. Benson, 1990. Collocations and General
Purpose Dictionaries. International Journal of
Lexicography, 3(1): 23-35
Y. Choueka, 1993. Looking for Needles in a
Haystack or Locating Interesting Collocation
Expressions in Large Textual Database.
Proceedings of RIAO Conference on User-
oriented Content-based Text and Image
Handling: 21-24, Cambridge.
K. Church, and P. Hanks, 1990. Word Association
Norms, Mutual Information,and Lexicography.
Computational Linguistics, 6(1): 22-29.
I. Dagan, L. Lee, and F. Pereira. 1997. Similarity-
based method for word sense disambiguation.
Proceedings of the 35th Annual Meeting of
ACL: 56-63, Madrid, Spain.
Z. D. Dong and Q. Dong. 1999. Hownet,
http://www.keenage.com
D. K. Lin, 1997. Using Syntactic Dependency as
Local Context to Resolve Word Sense Ambiguity.
Proceedings of ACL/EACL-97: 64-71, Madrid,
Spain
Q. Liu, 2002. The Word Similarity Calculation on
&lt;&lt;HowNet&gt;&gt;. Proceedings of 3rd Conference
on Chinese lexicography, TaiBei
Q. Lu, Y. Li, and R. F. Xu, 2003. Improving Xtract
for Chinese Collocation Extraction. Proceedings
of IEEE International Conference on Natural
Language Processing and Knowledge
Engineering, Beijing
C. D. Manning and H. Schutze, 1999. Foundations
of Statistical Natural Language Processing. The
MIT Press, Cambridge, Massachusetts
D. Pearce, 2001. Synonymy in Collocation
Extraction. Proceedings of NAACL&apos;01
Workshop on Wordnet and Other Lexical
Resources: Applications, Extensions and
Customizations
F. Smadja, 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1): 143-
177
H. Wu, and M. Zhou, 2003. Synonymous
Collocation Extraction Using Translation
Information. Proceeding of the 41st Annual
Meeting of ACL
D. K. Lin, 1998. Extracting collocations from text
corpora. In Proc. First Workshop on
Computational Terminology, Montreal, Canada.
M. S. Sun, C. N. Huang and J. Fang, 1997.
Preliminary Study on Quantitative Study on
Chinese Collocations. ZhongGuoYuWen, No.1,
29-38, (in Chinese).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.048263">
<title confidence="0.82277175">Using Synonym Relations In Chinese Collocation Extraction Wanyin Department of The Hong Kong Polytechnic</title>
<author confidence="0.978392">Hung Hom</author>
<author confidence="0.978392">Hong Kowloon</author>
<email confidence="0.935642">cswyli@comp.polyu.edu.hk</email>
<author confidence="0.859874">Qin</author>
<affiliation confidence="0.888991">Department of</affiliation>
<title confidence="0.798049">The Hong Kong Polytechnic</title>
<author confidence="0.86456">Hung Hom</author>
<author confidence="0.86456">Hong Kowloon</author>
<email confidence="0.483194">csluqin@comp.polyu.edu.hk</email>
<author confidence="0.996912">Ruifeng Xu</author>
<affiliation confidence="0.999999">Department of Computing, The Hong Kong Polytechnic University,</affiliation>
<address confidence="0.879151">Hung Hom, Kowloon, Hong Kong</address>
<email confidence="0.966794">csrfxu@comp.polyu.edu.hk</email>
<author confidence="0.354902">rather than</author>
<abstract confidence="0.995006576923077">rather than “ ”. A challenging task in Chinese collocation extraction is to improve both the precision and recall rate. Most lexical statistical methods including Xtract face the problem of unable to extract collocations with lower frequencies than a given threshold. This paper presents a method where HowNet is used to find synonyms using a similarity function. Based on such synonym information, we have successfully extracted synonymous collocations which normally cannot be extracted using the lexical statistical approach. We applied synonyms mapping to each headword to extract more synonymous word bi-grams. Our evaluation over 60MB tagged corpus shows that we can extract synonymous collocations that occur with very low frequency, sometimes even for collocations that occur only once in the training set. Comparing to a collocation extraction system based on Xtract, we have reached the precision rate of 43% on word bi-grams for a set of 9 headwords, almost 50% improvement from precision rate of 30% in the Xtract system. Furthermore, it improves the recall rate of word bi-gram collocation extraction by 30%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Benson</author>
</authors>
<title>Collocations and General Purpose Dictionaries.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>1</issue>
<pages>23--35</pages>
<contexts>
<context position="1786" citStr="Benson 1990" startWordPosition="263" endWordPosition="264">ns that occur with very low frequency, sometimes even for collocations that occur only once in the training set. Comparing to a collocation extraction system based on Xtract, we have reached the precision rate of 43% on word bi-grams for a set of 9 headwords, almost 50% improvement from precision rate of 30% in the Xtract system. Furthermore, it improves the recall rate of word bi-gram collocation extraction by 30%. 1 Introduction A Chinese collocation is a recurrent and conventional expression of words which holds syntactic and semantic relations. A widely adopted definition given by Benson (Benson 1990) stated that “a collocation is an arbitrary and recurrent word combination.” For example, we say “warm greetings” rather than “hot greetings”, “broad daylight” rather than “bright daylight”. Similarly, ” are three nouns with similar meanings, however, we say Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction (Church and Hanks 1990, Smadja 1993, Choueka 1993, Lin 1998). As the lexical statistical approach is developed based on the “recurrence” property of collocations, only collocations with reasonably good recurrence </context>
</contexts>
<marker>Benson, 1990</marker>
<rawString>M. Benson, 1990. Collocations and General Purpose Dictionaries. International Journal of Lexicography, 3(1): 23-35</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Choueka</author>
</authors>
<title>Looking for Needles in a Haystack or Locating Interesting Collocation Expressions in Large Textual Database.</title>
<date>1993</date>
<booktitle>Proceedings of RIAO Conference on Useroriented Content-based Text and Image Handling:</booktitle>
<pages>21--24</pages>
<location>Cambridge.</location>
<contexts>
<context position="2222" citStr="Choueka 1993" startWordPosition="327" endWordPosition="328"> Chinese collocation is a recurrent and conventional expression of words which holds syntactic and semantic relations. A widely adopted definition given by Benson (Benson 1990) stated that “a collocation is an arbitrary and recurrent word combination.” For example, we say “warm greetings” rather than “hot greetings”, “broad daylight” rather than “bright daylight”. Similarly, ” are three nouns with similar meanings, however, we say Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction (Church and Hanks 1990, Smadja 1993, Choueka 1993, Lin 1998). As the lexical statistical approach is developed based on the “recurrence” property of collocations, only collocations with reasonably good recurrence can be extracted. Collocations with low occurrence frequency cannot be extracted, thus affecting the recall rate. The precision rate using the lexical statistics approach can reach around 60% if both word bi-gram extraction and n-gram extractions are taking into account (Smadja 1993, Lin 1997 and Lu et al. 2003). The low precision rate is mainly due to the low precision rate of word bigram extractions as only about 30% - 40% precisi</context>
<context position="4221" citStr="Choueka 1993" startWordPosition="647" endWordPosition="648">nomenal after the lexical statistics based extractor to find the low frequency synonymous collocations, thus increasing recall rate. in Chinese “ ” “ ” “ The rest of this paper is organized as follows. Section 2 describes related existing collocation extraction techniques based on both lexical statistics and synonymous collocation. Section 3 describes our approach on collocation extraction. Section 4 evaluates the proposed method. Section 5 draws our conclusion and presents possible future work. 2 Related Work Methods have proposed to extract collocations based on lexical statistics. Choueka (Choueka 1993) applied quantitative selection criteria based on frequency threshold to extract adjacent n-grams (including bi-grams). Church and Hanks (Church and Hanks 1990) employed mutual information to extract both adjacent and distant bi-grams that tend to co-occur within a fixed-size window. But the method did not extend to extract n-grams. Smadja (Smadja 1993) proposed a statistical model by measuring the spread of the distribution of cooccurring pairs of words with higher strength. This method successfully extracted both adjacent and distant bi-grams and n-grams. However, the method failed to extrac</context>
</contexts>
<marker>Choueka, 1993</marker>
<rawString>Y. Choueka, 1993. Looking for Needles in a Haystack or Locating Interesting Collocation Expressions in Large Textual Database. Proceedings of RIAO Conference on Useroriented Content-based Text and Image Handling: 21-24, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<date>1990</date>
<journal>Word Association Norms, Mutual Information,and Lexicography. Computational Linguistics,</journal>
<volume>6</volume>
<issue>1</issue>
<pages>22--29</pages>
<contexts>
<context position="2195" citStr="Church and Hanks 1990" startWordPosition="321" endWordPosition="324"> extraction by 30%. 1 Introduction A Chinese collocation is a recurrent and conventional expression of words which holds syntactic and semantic relations. A widely adopted definition given by Benson (Benson 1990) stated that “a collocation is an arbitrary and recurrent word combination.” For example, we say “warm greetings” rather than “hot greetings”, “broad daylight” rather than “bright daylight”. Similarly, ” are three nouns with similar meanings, however, we say Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction (Church and Hanks 1990, Smadja 1993, Choueka 1993, Lin 1998). As the lexical statistical approach is developed based on the “recurrence” property of collocations, only collocations with reasonably good recurrence can be extracted. Collocations with low occurrence frequency cannot be extracted, thus affecting the recall rate. The precision rate using the lexical statistics approach can reach around 60% if both word bi-gram extraction and n-gram extractions are taking into account (Smadja 1993, Lin 1997 and Lu et al. 2003). The low precision rate is mainly due to the low precision rate of word bigram extractions as o</context>
<context position="4381" citStr="Church and Hanks 1990" startWordPosition="666" endWordPosition="669"> “ The rest of this paper is organized as follows. Section 2 describes related existing collocation extraction techniques based on both lexical statistics and synonymous collocation. Section 3 describes our approach on collocation extraction. Section 4 evaluates the proposed method. Section 5 draws our conclusion and presents possible future work. 2 Related Work Methods have proposed to extract collocations based on lexical statistics. Choueka (Choueka 1993) applied quantitative selection criteria based on frequency threshold to extract adjacent n-grams (including bi-grams). Church and Hanks (Church and Hanks 1990) employed mutual information to extract both adjacent and distant bi-grams that tend to co-occur within a fixed-size window. But the method did not extend to extract n-grams. Smadja (Smadja 1993) proposed a statistical model by measuring the spread of the distribution of cooccurring pairs of words with higher strength. This method successfully extracted both adjacent and distant bi-grams and n-grams. However, the method failed to extract bi-grams with lower frequency. The precision rate on bi-grams collocation is very low, only around high 20% and low 30%. Even though, it is difficult to measu</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K. Church, and P. Hanks, 1990. Word Association Norms, Mutual Information,and Lexicography. Computational Linguistics, 6(1): 22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>L Lee</author>
<author>F Pereira</author>
</authors>
<title>Similaritybased method for word sense disambiguation.</title>
<date>1997</date>
<booktitle>Proceedings of the 35th Annual Meeting of ACL:</booktitle>
<pages>56--63</pages>
<location>Madrid,</location>
<marker>Dagan, Lee, Pereira, 1997</marker>
<rawString>I. Dagan, L. Lee, and F. Pereira. 1997. Similaritybased method for word sense disambiguation. Proceedings of the 35th Annual Meeting of ACL: 56-63, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z D Dong</author>
<author>Q Dong</author>
</authors>
<date>1999</date>
<note>Hownet, http://www.keenage.com</note>
<contexts>
<context position="7183" citStr="Dong and Dong 1999" startWordPosition="1105" endWordPosition="1108"> relationship between two different languages to automatically acquire English synonymous collocation. This is the first time that the concept synonymous collocation is proposed. A side intuition raised here is that nature language is full of synonymous collocations. As many of them have low occurrences, they are failed to be retrieved by lexical statistical methods. Even though there are Chinese synonym dictionaries, such as ( Tong Yi Ci Lin), the dictionaries lack structured knowledge and synonyms are too loosely defined to be used for collocation extraction. HowNet developed by Dong et al (Dong and Dong 1999) is the best publicly available resource on Chinese semantics. By making use of semantic similarities of words, synonyms can be defined by the closeness of their related concepts and the closeness can be calculated. In Section 3, we present our method to extract synonyms from HowNet and using synonym relations to further extract collocations. Sun (Sun 1997) did a preliminary Quantitative analysis on Chinese collocations based on their arbitrariness, recurrence and the syntax structure. The purpose of this study is to help differentiate if a collocation is true or not according to the quantitat</context>
</contexts>
<marker>Dong, Dong, 1999</marker>
<rawString>Z. D. Dong and Q. Dong. 1999. Hownet, http://www.keenage.com</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Lin</author>
</authors>
<title>Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity.</title>
<date>1997</date>
<booktitle>Proceedings of ACL/EACL-97:</booktitle>
<pages>64--71</pages>
<location>Madrid, Spain</location>
<contexts>
<context position="2679" citStr="Lin 1997" startWordPosition="393" endWordPosition="394">ction using lexical statistics has gained some insights to the issues faced in collocation extraction (Church and Hanks 1990, Smadja 1993, Choueka 1993, Lin 1998). As the lexical statistical approach is developed based on the “recurrence” property of collocations, only collocations with reasonably good recurrence can be extracted. Collocations with low occurrence frequency cannot be extracted, thus affecting the recall rate. The precision rate using the lexical statistics approach can reach around 60% if both word bi-gram extraction and n-gram extractions are taking into account (Smadja 1993, Lin 1997 and Lu et al. 2003). The low precision rate is mainly due to the low precision rate of word bigram extractions as only about 30% - 40% precision rate can be achieved for word bi-grams. In this paper, we propose a different approach to find collocations with low recurrences. The main idea is to make use of synonym relations to extract synonymous collocations. Lin (Lin 1997) described a distributional hypothesis that if two words have similar set of collocations, they are probably similar. In HowNet, Liu Qun (Liu et al. 2002) defined the word similarity as two words that can substitute each oth</context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>D. K. Lin, 1997. Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity. Proceedings of ACL/EACL-97: 64-71, Madrid, Spain</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Liu</author>
</authors>
<title>The Word Similarity Calculation on &lt;&lt;HowNet&gt;&gt;.</title>
<date>2002</date>
<booktitle>Proceedings of 3rd Conference on Chinese lexicography, TaiBei</booktitle>
<contexts>
<context position="9997" citStr="Liu 2002" startWordPosition="1568" endWordPosition="1569">ependent primitive are used to indicate the semantics of a concept and the others are used to indicate syntactical relationships. The similarity model described in the next subsection will consider both of these relationships. The primitives are linked by a hierarchical tree to indicate the parent-child relationships of the primitives as shown in the following example: This hierarchical structure provides a way to link one concept with any other concept in HowNet, and the closeness of concepts can be simulated by the distance between two concepts. 3.2 Similarity Model Based on HowNet Liu Qun (Liu 2002) defined word similarity as two words which can substitute each other in the same context and still maintain the sentence consistent syntactically and semantically. This is very close to our definition of synonyms. Thus we directly used their similarity function, which is stated as follows. A word in HowNet is defined as a set of concepts and each concept is represented by primitives. Thus, HowNet can be described by W, a collection of n words, as: W = { w1, w2, ... wn}Each word wi is, in turn, described by a set of concepts S as: Wi = { Si1, Si2,...Six}, And, each concept Si is, in turn, desc</context>
<context position="12584" citStr="Liu 2002" startWordPosition="2046" endWordPosition="2047">pth of node pi in the tree . The comparison of calculating the word similarity by applying the formula (2) and (28) is shown in Section 4.4. Based on the DEF description in HowNet, different primitive types play different roles only some are directly related to semantics. To make use of both the semantic and syntactic information included in HowNet to describe a word, the similarity of two concepts should take into consideration of all primitive types with weighted considerations and thus the formula is defined as 4 i Sim (S1,S2)=∑β i∏Simj(p1j,p2j) (3) where βi is a weighting factor given in (Liu 2002) with the sum of β1 + β2 + β3 + β4 being 1, and β1 &gt; β2 &gt; β3 &gt; β4. The distribution of the weighting factors is given for each concept a priori in HowNet to indicate the importance of primitive pi in defining the corresponding concept S. 3.3 Collocation Extraction In order to extract collocations from a corpus, and to obtain result for Step 1 of our algorithm, we used the collocation extraction algorithm developed by the research group at the Hong Kong Polytechnic University(Lu et al. 2003). The extraction of bi-gram collocation is based on the English Xtract(Smaja 1993) with improvements. Bas</context>
</contexts>
<marker>Liu, 2002</marker>
<rawString>Q. Liu, 2002. The Word Similarity Calculation on &lt;&lt;HowNet&gt;&gt;. Proceedings of 3rd Conference on Chinese lexicography, TaiBei</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Lu</author>
<author>Y Li</author>
<author>R F Xu</author>
</authors>
<title>Improving Xtract for Chinese Collocation Extraction.</title>
<date>2003</date>
<booktitle>Proceedings of IEEE International Conference on Natural Language Processing and Knowledge Engineering,</booktitle>
<location>Beijing</location>
<contexts>
<context position="2699" citStr="Lu et al. 2003" startWordPosition="396" endWordPosition="399">exical statistics has gained some insights to the issues faced in collocation extraction (Church and Hanks 1990, Smadja 1993, Choueka 1993, Lin 1998). As the lexical statistical approach is developed based on the “recurrence” property of collocations, only collocations with reasonably good recurrence can be extracted. Collocations with low occurrence frequency cannot be extracted, thus affecting the recall rate. The precision rate using the lexical statistics approach can reach around 60% if both word bi-gram extraction and n-gram extractions are taking into account (Smadja 1993, Lin 1997 and Lu et al. 2003). The low precision rate is mainly due to the low precision rate of word bigram extractions as only about 30% - 40% precision rate can be achieved for word bi-grams. In this paper, we propose a different approach to find collocations with low recurrences. The main idea is to make use of synonym relations to extract synonymous collocations. Lin (Lin 1997) described a distributional hypothesis that if two words have similar set of collocations, they are probably similar. In HowNet, Liu Qun (Liu et al. 2002) defined the word similarity as two words that can substitute each other in the context an</context>
<context position="5218" citStr="Lu et al. 2003" startWordPosition="796" endWordPosition="799">el by measuring the spread of the distribution of cooccurring pairs of words with higher strength. This method successfully extracted both adjacent and distant bi-grams and n-grams. However, the method failed to extract bi-grams with lower frequency. The precision rate on bi-grams collocation is very low, only around high 20% and low 30%. Even though, it is difficult to measure recall rate in collocation extraction (almost no report on recall estimation), It is understood that low occurrence collocations cannot be extracted. Our research group has further applied the Xtract system to Chinese (Lu et al. 2003) by adjusting the parameters to optimize the algorithm for Chinese and a new weighted algorithm was developed based on mutual information to acquire word bigrams with one higher frequency word and one lower frequency word. The result has achieved an estimated 5% improvement in recall rate and a 15% improvement in precision comparing to the Xtract system. All of the above techniques do not take advantage of the wide range of lexical resources available including synonym information. Pearce (Pearce 2001) presented a collocation extraction technique that relies on a mapping from a word to its syn</context>
<context position="13079" citStr="Lu et al. 2003" startWordPosition="2134" endWordPosition="2137">thus the formula is defined as 4 i Sim (S1,S2)=∑β i∏Simj(p1j,p2j) (3) where βi is a weighting factor given in (Liu 2002) with the sum of β1 + β2 + β3 + β4 being 1, and β1 &gt; β2 &gt; β3 &gt; β4. The distribution of the weighting factors is given for each concept a priori in HowNet to indicate the importance of primitive pi in defining the corresponding concept S. 3.3 Collocation Extraction In order to extract collocations from a corpus, and to obtain result for Step 1 of our algorithm, we used the collocation extraction algorithm developed by the research group at the Hong Kong Polytechnic University(Lu et al. 2003). The extraction of bi-gram collocation is based on the English Xtract(Smaja 1993) with improvements. Based on the three Steps mentioned earlier, we will present the extractions in each step in the subsections. 3.3.1 Bi-gram Extraction Based on the lexical statistical model proposed by Smadja in Xtract on extracting English collocations, an improved algorithm was developed for Chinese collocation by our research group and the system is called CXtract. For easy of understanding, we will explain the algorithm briefly here. According to Xtract, word cooccurence is denoted by a tripplet (wh, wi, d</context>
</contexts>
<marker>Lu, Li, Xu, 2003</marker>
<rawString>Q. Lu, Y. Li, and R. F. Xu, 2003. Improving Xtract for Chinese Collocation Extraction. Proceedings of IEEE International Conference on Natural Language Processing and Knowledge Engineering, Beijing</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schutze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts</location>
<contexts>
<context position="23842" citStr="Manning and Schutze 1999" startWordPosition="4061" endWordPosition="4064">o extract bi-gram collocations using lexical statistics model with synonyms information. Our method reaches the precision rate of 43% for the tested data. Comparing to the precision of 30% using lexical statistics only, our improvement is close to 50%. In additional, the recall improved 30%. The contribution is that we have made use of synonym information which is plentiful in the natural language use and it works well to supplement the shortcomings of lexical statistical method. Manning claimed that the lack of valid substitution for a synonym is a characteristics of collocations in general (Manning and Schutze 1999). To extend our work, we consider the use of synonym information can be further applied to help identify collocations of different types. Our preliminary study has suggested that collocation can be classified into 4 types: Type 0 Collocation: Fully fixed collocation which include some idioms, proverbs and sayings such as “ ” “ ” and so on. Type 1 Collocation: Fixed collocation in which the appearance of one word implies the cooccurrence of another one such as “ ” “ ” ” “ Type 2 Collocation: Strong collocation which allows very limited substitution of the components, for example, “ ”, “ ”, ” an</context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>C. D. Manning and H. Schutze, 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pearce</author>
</authors>
<title>Synonymy in Collocation Extraction.</title>
<date>2001</date>
<booktitle>Proceedings of NAACL&apos;01 Workshop on Wordnet and Other Lexical Resources: Applications, Extensions and Customizations</booktitle>
<contexts>
<context position="5725" citStr="Pearce 2001" startWordPosition="879" endWordPosition="880">cannot be extracted. Our research group has further applied the Xtract system to Chinese (Lu et al. 2003) by adjusting the parameters to optimize the algorithm for Chinese and a new weighted algorithm was developed based on mutual information to acquire word bigrams with one higher frequency word and one lower frequency word. The result has achieved an estimated 5% improvement in recall rate and a 15% improvement in precision comparing to the Xtract system. All of the above techniques do not take advantage of the wide range of lexical resources available including synonym information. Pearce (Pearce 2001) presented a collocation extraction technique that relies on a mapping from a word to its synonyms for each of its senses. The underlying intuitions is that if the difference between the occurrence counts of one synonyms pair with respect to a particular word was at least two, then this was deemed sufficient to consider them as a collocation. To apply this approach, knowledge in word (concept) semantics and relations to other words must be available such as the use of WordNet. Dagan (Dagan 1997) applied similaritybased smoothing method to solve the problem of data sparseness in statistical nat</context>
<context position="16879" citStr="Pearce 2001" startWordPosition="2879" endWordPosition="2880">locations exist. For example, ‘switch on light’ and ‘turn on light’, “ Due to the domain specification of the corpus, some of the synonymous collocations may fail to be extracted by the lexical statistic model because of their lower frequency. Based on this observation, this paper takes a further step. The basic idea is for a bi-gram collocation (wh, wc, d ) we select the synonyms ws of wh with the maximum similarity respect to all the concepts contained by wh, we deem (ws, wc, d ) as a collocation if its occurrence is greater than 1 in the corpus. There are similar works discussed by Pearce (Pearce 2001). . For a given collocation (ws, wc,, d), if ws E Wsyn, then we deem the triple (ws, wc,, d) as a synonymous collocation with respect to the collocation (wh, wc,, d) if the co-occurrence of (ws, wc, , d) in the corpus is greater than one. Therefore, we define the collection of synonymous collocations Csyn as: Csyn = {(ws,wc,d) : Freq(ws,w,d) &gt; 1} (14) where ws E Wsyn. 4 Evaluation The performance of collocation is normally evaluated by precision and recall as defined below. numberof correct ExtractedCollocatio ns recall= (16) total numberof actual Collocations To evaluate the performance of ou</context>
</contexts>
<marker>Pearce, 2001</marker>
<rawString>D. Pearce, 2001. Synonymy in Collocation Extraction. Proceedings of NAACL&apos;01 Workshop on Wordnet and Other Lexical Resources: Applications, Extensions and Customizations</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
</authors>
<title>Retrieving collocations from text:</title>
<date>1993</date>
<journal>Xtract. Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>143--177</pages>
<contexts>
<context position="2208" citStr="Smadja 1993" startWordPosition="325" endWordPosition="326">ntroduction A Chinese collocation is a recurrent and conventional expression of words which holds syntactic and semantic relations. A widely adopted definition given by Benson (Benson 1990) stated that “a collocation is an arbitrary and recurrent word combination.” For example, we say “warm greetings” rather than “hot greetings”, “broad daylight” rather than “bright daylight”. Similarly, ” are three nouns with similar meanings, however, we say Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction (Church and Hanks 1990, Smadja 1993, Choueka 1993, Lin 1998). As the lexical statistical approach is developed based on the “recurrence” property of collocations, only collocations with reasonably good recurrence can be extracted. Collocations with low occurrence frequency cannot be extracted, thus affecting the recall rate. The precision rate using the lexical statistics approach can reach around 60% if both word bi-gram extraction and n-gram extractions are taking into account (Smadja 1993, Lin 1997 and Lu et al. 2003). The low precision rate is mainly due to the low precision rate of word bigram extractions as only about 30%</context>
<context position="4576" citStr="Smadja 1993" startWordPosition="698" endWordPosition="699">r approach on collocation extraction. Section 4 evaluates the proposed method. Section 5 draws our conclusion and presents possible future work. 2 Related Work Methods have proposed to extract collocations based on lexical statistics. Choueka (Choueka 1993) applied quantitative selection criteria based on frequency threshold to extract adjacent n-grams (including bi-grams). Church and Hanks (Church and Hanks 1990) employed mutual information to extract both adjacent and distant bi-grams that tend to co-occur within a fixed-size window. But the method did not extend to extract n-grams. Smadja (Smadja 1993) proposed a statistical model by measuring the spread of the distribution of cooccurring pairs of words with higher strength. This method successfully extracted both adjacent and distant bi-grams and n-grams. However, the method failed to extract bi-grams with lower frequency. The precision rate on bi-grams collocation is very low, only around high 20% and low 30%. Even though, it is difficult to measure recall rate in collocation extraction (almost no report on recall estimation), It is understood that low occurrence collocations cannot be extracted. Our research group has further applied the</context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>F. Smadja, 1993. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1): 143-177</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wu</author>
<author>M Zhou</author>
</authors>
<title>Synonymous Collocation Extraction Using Translation Information.</title>
<date>2003</date>
<booktitle>Proceeding of the 41st Annual Meeting of ACL</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="6547" citStr="Wu and Zhou 2003" startWordPosition="1008" endWordPosition="1011">of one synonyms pair with respect to a particular word was at least two, then this was deemed sufficient to consider them as a collocation. To apply this approach, knowledge in word (concept) semantics and relations to other words must be available such as the use of WordNet. Dagan (Dagan 1997) applied similaritybased smoothing method to solve the problem of data sparseness in statistical natural language processing. The experiments conducted in his later works showed that this method achieved much better results than back-off smoothing methods in word sense disambiguation. Similarly, Hua Wu (Wu and Zhou 2003) applied synonyms relationship between two different languages to automatically acquire English synonymous collocation. This is the first time that the concept synonymous collocation is proposed. A side intuition raised here is that nature language is full of synonymous collocations. As many of them have low occurrences, they are failed to be retrieved by lexical statistical methods. Even though there are Chinese synonym dictionaries, such as ( Tong Yi Ci Lin), the dictionaries lack structured knowledge and synonyms are too loosely defined to be used for collocation extraction. HowNet develope</context>
</contexts>
<marker>Wu, Zhou, 2003</marker>
<rawString>H. Wu, and M. Zhou, 2003. Synonymous Collocation Extraction Using Translation Information. Proceeding of the 41st Annual Meeting of ACL D. K. Lin, 1998. Extracting collocations from text corpora. In Proc. First Workshop on Computational Terminology, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Sun</author>
<author>C N Huang</author>
<author>J Fang</author>
</authors>
<date>1997</date>
<booktitle>Preliminary Study on Quantitative Study on Chinese Collocations. ZhongGuoYuWen, No.1,</booktitle>
<pages>29--38</pages>
<note>(in Chinese).</note>
<marker>Sun, Huang, Fang, 1997</marker>
<rawString>M. S. Sun, C. N. Huang and J. Fang, 1997. Preliminary Study on Quantitative Study on Chinese Collocations. ZhongGuoYuWen, No.1, 29-38, (in Chinese).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>