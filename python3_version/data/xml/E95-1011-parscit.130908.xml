<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997878">
A Tractable Extension of Linear Indexed Grammars
</title>
<author confidence="0.977563">
Bill Keller and David Weir
</author>
<affiliation confidence="0.994173">
School of Cognitive and Computing Sciences
University of Sussex
</affiliation>
<address confidence="0.8895565">
Falmer, Brighton BN1 9QH
UK
</address>
<email confidence="0.911928">
bill.keller/david.weirOcogs.sussex.ac.uk
</email>
<sectionHeader confidence="0.981476" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99864125">
Vijay-Shanker and Weir (1993) show
that Linear Indexed Grammars (LIG) can
be processed in polynomial time by ex-
ploiting constraints which make possible
the extensive use of structure-sharing.
This paper describes a formalism that
is more powerful than LIG, but which
can also be processed in polynomial time
using similar techniques. The formal-
ism, which we refer to as Partially Lin-
ear PATR (PLPATR) manipulates feature
structures rather than stacks.
</bodyText>
<sectionHeader confidence="0.995584" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999721923076924">
Unification-based grammar formalisms can be
viewed as generalizations of Context-Free Gram-
mars (CFG) where the nonterminal symbols are
replaced by an infinite domain of feature struc-
tures. Much of their popularity stems from the
way in which syntactic generalization may be el-
egantly stated by means of constraints amongst
features and their values. Unfortunately, the ex-
pressivity of these formalisms can have undesir-
able consequences for their processing. In naive
implementations of unification grammar parsers,
feature structures play the same role as nonter-
minals in standard context-free grammar parsers.
Potentially large feature structures are stored at
intermediate steps in the computation, so that
the space requirements of the algorithm are ex-
pensive. Furthermore, the need to perform non-
destructive unification means that a large propor-
tion of the processing time is spent copying feature
structures.
One approach to this problem is to refine pars-
ing algorithms by developing techniques such as
restrictions, structure-sharing, and lazy unifica-
tion that reduce the amount of structure that is
stored and hence the need for copying of features
structures (Shieber, 1985; Pereira, 1985; Kart-
tunen and Kay, 1985; Wroblewski, 1987; Gerde-
mann, 1989; Godden, 1990; Kogure, 1990; Emele,
1991; Tomabechi, 1991; Harrison and Ellison,
1992)). While these techniques can yield signifi-
cant improvements in performance, the generality
of unification-based grammar formalisms means
that there are still cases where expensive process-
ing is unavoidable. This approach does not ad-
dress the fundamental issue of the tradeoff be-
tween the descriptive capacity of a formalism and
its computational power.
In this paper we identify a set of constraints
that can be placed on unification-based grammar
formalisms in order to guarantee the existence of
polynomial time parsing algorithms. Our choice
of constraints is motivated by showing how they
generalize constraints inherent in Linear Indexed
Grammar (LIG). We begin by describing how con-
straints inherent in LIG admit tractable process-
ing algorithms and then consider how these con-
straints can be generalized to a formalism that
manipulates trees rather than stacks. The con-
straints that we identify for the tree-based sys-
tem can be regarded equally well as constraints
on unification-based grammar formalisms such as
PATR (Shieber, 1984).
</bodyText>
<sectionHeader confidence="0.815953" genericHeader="method">
2 From Stacks to Trees
</sectionHeader>
<bodyText confidence="0.99935123076923">
An Indexed Grammar (IG) can be viewed as a CFG
in which each nonterminal is associated with a
stack of indices. Productions specify not only how
nonterminals can be rewritten but also how their
associated stacks are modified. LIG, which were
first described by Gazdar (1988), are constrained
such that stacks are passed from the mother to at
most a single daughter.
For LIG, the size of the domain of nontermi-
nals and associated stacks (the analogue of the
nonterminals in CFG) is not bound by the gram-
mar. However, Vijay-Shanker and Weir (1993)
demonstrate that polynomial time performance
</bodyText>
<page confidence="0.997865">
75
</page>
<bodyText confidence="0.985256829268292">
can be achieved through the use of structure-
sharing made possible by constraints in the way
that LIG use stacks. Although stacks of un-
bounded size can arise during a derivation, it is
not possible for a LIG to specify that two depen-
dent, unbounded stacks must appear at distinct
places in the derivation tree. Structure-sharing
can therefore be used effectively because check-
ing the applicability of rules at each step in the
derivation involves the comparison of structures
of limited size.
Our goal is to generalize the constraints inher-
ent in LIG to a formalism that manipulates fea-
ture structures rather than stacks. As a guid-
ing heuristic we will avoid formalisms that gen-
erate tree sets with an unbounded number of un-
bounded, dependent branches. It appears that the
structure-sharing techniques used with LIG cannot
be generalized in a straightforward way to such
formalisms.
Suppose that we generalize LIG to allow the
stack to be passed from the mother to two daugh-
ters. If this is done recursion can be used to pro-
duce an unbounded number of unbounded, depen-
dent branches. An alternative is to allow an un-
bounded stack to be shared between two (or more)
daughters but not with the mother. Thus, rules
may mention more than one unbounded stack, but
the stack associated with the mother is still asso-
ciated with at most one daughter. We refer to
this extension as Partially Linear Indexed Gram-
mars (PLIG).
Example 1 The PLIG with the following produc-
tions generates the language
fanbrnendni In,m&gt;11
and the tree set shown in Figure 1. Because a sin-
gle PLIG production may mention more than one
unbounded stack, variables (x, y) are introduced to
distinguish between them. The notation A[xol is
used to denote the nonterminal A associated with
any stack whose top symbol is o-.
</bodyText>
<equation confidence="0.842959">
A[x] aA[xo-], A[x] B [MC [x] D [y] ,
B[xcr] bB[x], B[o] b,
C[xo] cC[x], C[a] --+ c,
D[xcd --+ dD[x], D[o] d.
</equation>
<footnote confidence="0.30416975">
Example 2 A PLIG with the following produc-
tions generates the k-copy language over {a,b}* ,
i.e., the language
{wk IwE {a,b}*
</footnote>
<figure confidence="0.999330111111111">
a A[cri
/111
a A[crn-11
a
C[O&apos;n]
b
B[am] c C[o-n-1]
/1
b B[o] c C[a]
</figure>
<figureCaption confidence="0.999867">
Figure 1: Tree set for { a&amp;quot;bm cn dm I n , m &gt; 1 }
</figureCaption>
<bodyText confidence="0.361902125">
where k &gt; 1.
SO A[x] . A[x], AD A,
k copies
A[xcri] a A[x], A[so-2] b A[x].
Example 3 PLIG can &amp;quot;count&amp;quot; to any fixed k, i.e.,
a PLIG with the following productions generates the
language
where k &gt; 1.
</bodyText>
<equation confidence="0.508566666666667">
SO -4 Ai[x]. • • Ak [X],
Ai[x AiD --&gt; A,
A k[X ak A k[X], A kl] -+ A.
</equation>
<bodyText confidence="0.99713875">
In PLIG, stacks shared amongst siblings cannot
be passed to the mother. As a consequence, there
is no possibility that recursion can be used to in-
crease the number of dependent branches. In fact,
the number of dependent branches is bounded by
the length of the right-hand-side of productions.
By the same token, however, PLIG may only gen-
erate structural descriptions in which dependent
</bodyText>
<figure confidence="0.939056090909091">
D[o-m]
d D [Cfml
d D[o]
76
771 kroi 772
Aft,,] S2 [Cr ( Trt , r„)]
a A[rn -i] B[rn] S3 [r,,]
a AFrii b B[rn—C[rn]
a b B[r] c C Ern — 1]
c C[r1]
where and ri+i = o2(Ti)
</figure>
<figureCaption confidence="0.995663">
Figure 2: Tree set for { anbncn n &gt; 1 }
</figureCaption>
<bodyText confidence="0.996231823529412">
branches begin at nodes that are siblings of one
another. Note that the tree shown in Figure 2
is unobtainable because the branch rooted at rh.
is dependent on more than one of the branches
originating at its sibling qz.
This limitation can be overcome by moving to
a formalism that manipulates trees rather than
stacks. We consider an extension of CFG in which
each nonterminal A is associated with a tree T.
Productions now specify how the tree associated
with the mother is related to the trees associ-
ated with the daughters. We denote trees with
first order terms. For example, the following pro-
duction requires that the and y subtrees of the
mother&apos;s tree are shared with the B and C daugh-
ters, respectively. In addition, the daughters have
in common the subtree z.
</bodyText>
<equation confidence="0.4459345">
Akro(x, B[o-i(x , z)]
C[o-2(y, z)]
</equation>
<bodyText confidence="0.929047090909091">
There is a need to incorporate some kind of
generalized notion of linearity into such a system.
Corresponding to the linearity restriction in LIG
we require that any part of the mother&apos;s tree is
passed to at most one daughter. Corresponding
to the partial linearity of PLIG, we permit subtrees
that are not shared with the mother to be shared
amongst the daughters. Under these conditions,
the tree set shown in Figure 2 can be generated.
current
q state
</bodyText>
<figureCaption confidence="0.961016">
Figure 3: Encoding a Turing Machine
</figureCaption>
<bodyText confidence="0.999912923076923">
The nodes qi and 772 share the tree rn, which oc-
curs twice at the node 772. At 7/2 the two copies of
are distributed across the daughters.
The formalism as currently described can be
used to simulate arbitrary Turing Machine com-
putations. To see this, note that an instanta-
neous description of a Turing Machine can be en-
coded with a tree as shown in Figure 3. Moves
of the Turing Machine can be simulated by unary
productions. The following production may be
glossed: &amp;quot;if in state q and scanning the symbol X,
then change state to q&apos;, write the symbol Y and
move left&amp;quot; 1.
</bodyText>
<equation confidence="0.742022">
A[q(W (x), X, y)] -+ A[q&apos; (x ,W,Y (y))]
</equation>
<bodyText confidence="0.9999721">
One solution to this problem is to prevent a sin-
gle daughter sharing more than one of its subtrees
with the mother. However, we do not impose this
restriction because it still leaves open the possi-
bility of generating trees in which every branch
has the same length, thus violating the condition
that trees have at most a bounded number of un-
bounded, dependent branches. Figure 4 shows
how a set of such trees can be generated by il-
lustrating the effect of the following production.
</bodyText>
<construct confidence="0.79505525">
A[o-(cr(x , y), o-(x&apos; , y&apos;))] A[o-(z , x)]
A[o-(z , y)]
A[o-(z , x&apos;)}
A[o-(z , y&apos;)]
</construct>
<bodyText confidence="0.9977235">
To see this, assume (by induction) that all four
of the daughter nonterminals are associated with
the full binary tree of height i (ri). All four of
these trees are constrained to be equal by the
production given above, which requires that they
have identical left (i.e. z) subtrees (these subtrees
must be the full binary tree 73_1). Passing the
right subtrees (x, y, x&apos; and y&apos;) to the mother as
shown allows the construction of a full binary tree
with height i 1 (ri÷1). This can be repeated an
&apos;There will be a set of such productions for each
tape symbol W.
</bodyText>
<figure confidence="0.986583111111111">
current
symbol
last
nonblank
symbol
first
nonblank ai
symbol
77
A a A. a A a A a
A A A Ti
5 1 5 2 5 3 5 E
Aa
B (75
D 0-8
A ai
0-2
07
</figure>
<figureCaption confidence="0.970645">
Figure 5: A PLTG local tree
</figureCaption>
<figure confidence="0.771536">
- 1 = U
</figure>
<figureCaption confidence="0.999269">
Figure 4: Building full binary trees
</figureCaption>
<bodyText confidence="0.9813245">
unbounded number of times so that all full binary
trees are produced.
To overcome both of these problems we impose
the following additional constraint on the produc-
tions of a grammar. We require that subtrees of
the mother that are passed to daughters that share
subtrees with one another must appear as siblings
in the mother&apos;s tree. Note that this condition rules
out the production responsible for building full bi-
nary trees since the x, y, x&apos; and y&apos; subtrees are not
siblings in the mother&apos;s tree despite the fact that
all of the daughters share a common subtree z.
Moreover, since a daughter shares subtrees with
itself, a special case of the condition is that sub-
trees occurring within some daughter can only ap-
pear as siblings in the mother. This condition also
rules out the Turing Machine simulation. We refer
to this formalism as Partially Linear Tree Gram-
mars (PLTG). As a further illustration of the con-
straints places on shared subtrees, Figure 5 shows
a local tree that could appear in a derivation tree.
This local tree is licensed by the following produc-
tion which respects all of the constraints on PLTG
productions.
</bodyText>
<equation confidence="0.97577675">
A[a-i(a-2(xi, x2, x3), cra(x4, 0-4))]
B[crs(x5, x5, xi)]
C[cr5(c7, x4)]
D[a8(x2, x3, x5)]
</equation>
<bodyText confidence="0.956474888888889">
Note that in Figure 5 the daughter nodes labelled
B and D share a common subtree and the sub-
trees shared between the mother and the B and D
daughters appear as siblings in the tree associated
with the mother.
Example 4 The PLTG with the following produc-
tions generates the language
{ an bn cn n &gt; 1 }
and the tree set shown in Figure 2.
</bodyText>
<equation confidence="0.973576">
Sl[ao] A[x] S2 [°-(X1 X)])
S2 [0.(X, —&gt; B[x] S3 [Y] ,
S3 [X] —*
A[cr2(x)] aA[x],
B[cr2(x)] b8[x],
C[cr2(x)] cC[x],
</equation>
<bodyText confidence="0.98034525">
Example 5 The PLTG with the following produc-
tions generates the language of strings consisting
of k copies of strings of matching parenthesis, i.e.,
the language
{ wk Ito D
where k &gt; 1 and D is the set of strings in {(,)}*
that have balanced brackets, i.e, the Dyck language
over{(,)}.
</bodyText>
<equation confidence="0.397699">
A[x] A[x],
k copies
A[o-i(x)] (A[x]), A[0-2(x, y)] A[x]il[Y]•
</equation>
<sectionHeader confidence="0.647958" genericHeader="method">
3 Trees to Feature Structures
</sectionHeader>
<bodyText confidence="0.999151">
Finally, we note that acyclic feature structures
without re-entrancy can be viewed as trees with
branches labelled by feature names and atomic
values only found at leaf nodes (interior nodes
</bodyText>
<page confidence="0.991374">
78
</page>
<bodyText confidence="0.999842083333333">
being unlabelled). Based on this observation,
we can consider the constraints we have formu-
lated for the tree system PLTG as constraints on
a unification-based grammar formalism such as
PATR. We will call this system Partially Linear
PATR (PLPATR). Having made the move from trees
to feature structures, we consider the possibility
of re-entrancy in PLPATR.
Note that the feature structure at the root
of a PLPATR derivation tree will not involve re-
entrancy. However, for the following reasons we
believe that this does not constitute as great a
limitation as it might appear. In unification-based
grammar, the feature structure associated with
the root of the tree is often regarded as the struc-
ture that has been derived from the input (i.e.,
the output of a parser). As a consequence there
is a tendency to use the grammar rules to accu-
mulate a single, large feature structure giving a
complete encoding of the analysis. To do this, un-
bounded feature information is passed up the tree
in a way that violates the constraints developed in
this paper. Rather than giving such prominence
to the root feature structure, we suggest that the
entire derivation tree should be seen as the object
that is derived from the input, i.e., this is what
the parser returns. Because feature structures as-
sociated with all nodes in the tree are available,
feature information need only be passed up the
tree when it is required in order to establish de-
pendencies within the derivation tree. When this
approach is taken, there may be less need for re-
entrancy in the root feature structure. Further-
more, re-entrancy in the form of shared feature
structures within and across nodes will be found
in PLPATR (see for example Figure 5).
</bodyText>
<sectionHeader confidence="0.996021" genericHeader="method">
4 Generative Capacity
</sectionHeader>
<bodyText confidence="0.987359432432433">
LIG are more powerful than CFG and are known to
be weakly equivalent to Tree Adjoining Grammar,
Combinatory Categorial Grammar, and Head
Grammar (Vijay-Shanker and Weir, 1994). PLIG
are more powerful than LIG since they can gener-
ate the k-copy language for any fixed k (see Exam-
ple 2). Slightly more generally, PLIG can generate
the language
{wkIwER}
for any k &gt; 1 and regular language R. We be-
lieve that the language involving copies of strings
of matching brackets described in Example 5 can-
not be generated by PLIG but, as shown in Exani-
ple 5, it can be generated by PLTG and therefore
PLPATR. Slightly more generally, PLTG can gener-
ate the language
{ wk tv E L
for any k &gt; 1 and context-free language L. It
appears that the class of languages generated by
PLTG is included in those languages generated by
Linear Context-Free Rewriting Systems (Vijay-
Shanker et al., 1987) since the construction in-
volved in a proof of this underlies the recognition
algorithm discussed in the next section.
As is the case for the tree sets of IG, LIG and
Tree Adjoining Grammar, the tree sets generated
by PLTG have path sets that are context-free lan-
guages. In other words, the set of all strings la-
belling root to frontier paths of derivation trees
is a context-free language. While the tree sets
of LIG and Tree Adjoining Grammars have inde-
pendent branches, PLTG tree sets exhibit depen-
dent branches, where the number of dependent
branches in any tree is bounded by the grammar.
Note that the number of dependent branches in
the tree sets of IG is not bounded by the grammar
(e.g., they generate sets of all full binary trees).
</bodyText>
<sectionHeader confidence="0.963331" genericHeader="method">
5 Tractable Recognition
</sectionHeader>
<bodyText confidence="0.999925448275862">
In this section we outline the main ideas un-
derlying a polynomial time recognition algorithm
for PLPATR that generalizes the CKY algorithm
(Kasami, 1965; Younger, 1967). The key to this
algorithm is the use of structure sharing tech-
niques similar to those used to process LIG effi-
ciently (Vijay-Shanker and Weir, 1993). To un-
derstand how these techniques are applied in the
case of PLPATR, it is therefore helpful to consider
first the somewhat simpler case of LIG.
The CKY algorithm is a bottom-up recognition
algorithm for CFG. For a given grammar G and
input string al ... an the algorithm constructs an
array P, having n&apos; elements, where element P[i, j]
stores all and only those nonterminals of G that
derive the substring ai ..ai. A naive adapta-
tion of this algorithm for LIG recognition would
involve storing a set of nonterminals and their as-
sociated stacks. But since stack length is at least
proportional to the length of the input string,
the resultant algorithm would exhibit exponen-
tial space and time complexity in the worst case.
Vijay-Shanker and Weir (1993) showed that the
behaviour of the naive algorithm can be improved
upon. In LIG derivations the application of a rule
cannot depend on more than a bounded portion
of the top of the stack. Thus, rather than storing
the whole of the potentially unbounded stack in
a particular array entry, it suffices to store just
</bodyText>
<page confidence="0.992077">
79
</page>
<figure confidence="0.9939785">
A ao-ol
aq
</figure>
<figureCaption confidence="0.99903">
Figure 6: &amp;quot;Context-Freeness&apos; in LIG derivations
</figureCaption>
<bodyText confidence="0.974574208333333">
a bounded portion together with a pointer to the
residue.
Consider Figure 6. Tree (a) shows a LIG deriva-
tion of the substring ai ai from the object
A[acrul. In this derivation tree, the node labelled
B[aol is a distinguished descendant of the root2
and is the first point below A[acrol at which the
top symbol (u) of the (unbounded) stack au is ex-
posed. This node is called the ierminator of the
node labelled A[au]. It is not difficult to show that
only that portion of the derivation below the ter-
minator node is dependent on more than the top
of the stack au. It follows that for any stack cei o-,
if there is a derivation of the substring ap aq
from 13[aio] (see tree (b)), then there is a cor-
responding derivation of ai ai from A[a&apos;aul
(see tree (c)). This captures the sense in which
LIG derivations exhibit &amp;quot;context-freeness&amp;quot;. Effi-
cient storage of stacks can therefore be achieved
by storing in P[i, just that bounded amount of
information (nonterminal plus top of stack) rele-
vant to rule application, together with a pointer to
any entry in P[p, g] representing a subderivation
from an object B[a&apos;cr].
</bodyText>
<footnote confidence="0.980881333333333">
2The stack aa associated with B is &amp;quot;inherited&amp;quot;
from the stack associated with A at the root of the
tree.
</footnote>
<bodyText confidence="0.99954403030303">
Before describing how we adapt this technique
to the case of PLPATR we discuss the sense in
which PLPATR derivations exhibit a &amp;quot;context-
freeness&amp;quot; property. The constraints on PLPATR
which we have identified in this paper ensure that
these feature values can be manipulated indepen-
dently of one another and that they behave in
a stack-like way. As a consequence, the storage
technique used effectively for LIG recognition may
be generalized to the case of PLPATR.
Suppose that we have the derived tree shown
in Figure 7 where the nodes at the root of the
subtrees 1-1 and 72 are the so-called f-terminator
and g-terminator of the tree&apos;s root, respectively.
Roughly speaking, the f-terminator of a node is
the node from which it gets the value for the fea-
ture f Because of the constraints on the form
of PLPATR productions, the derivations between
the root of 7 and these terminators cannot in gen-
eral depend on more than a bounded part of the
feature structures m and 12 1. At the root of the
figure the feature structures 1T1 and 2 have been
expanded to show the extent of the dependency in
this example. In this case, the value of the feature
f in F1-1 must be a, whereas, the feature g is not
fixed. Furthermore, the value of the feature g in
must be b, whereas, the feature f is not fixed.
This means, for example, that the applicability of
the productions used on the path from the root
of to the root of T depends on the feature f in
111 having the value a but does not depend on the
value of the feature g in 11 1. Note that in this tree
the value of the feature g in 1 is
</bodyText>
<equation confidence="0.985527666666667">
[ f
=
g : F3
</equation>
<bodyText confidence="0.929292">
and the value of the feature f in
</bodyText>
<equation confidence="0.986352333333333">
[ f : F4 1
F2-
g : d
</equation>
<figureCaption confidence="0.5570788">
Suppose that, in addition to the tree shown in
Figure 7 the grammar generates the pair of trees
shown in Figure 8. Notice that while the feature
structures at the root of T3 and T4 are not compat-
ible with 111 and r21, they do agree with respect
to those parts that are fully expanded at T S root
node. The &amp;quot;context-freeness&amp;quot; of PLPATR means
that given the three trees shown in Figures 7 and 8
the tree shown in Figure 9 will also be generated
by the grammar.
</figureCaption>
<bodyText confidence="0.99768575">
This gives us a means of efficiently storing the
potentially unbounded feature structures associ-
ated with nodes in a derivation tree (derived fea-
ture structures). By analogy with the situation for
</bodyText>
<figure confidence="0.945972214285714">
terminator
ai
1 21
is
2 1
80
a
a,
aq
a,
fa
f
9 • F3
: c
</figure>
<figureCaption confidence="0.966804">
Figure 7: Terminators in PLPATR
</figureCaption>
<figure confidence="0.997417375">
g :
a
f : a
g [ gf Fc I I
f dF
9 •
[ [ g
a aq a,. U3
</figure>
<figureCaption confidence="0.962875">
Figure 8: Compatible subderivations
</figureCaption>
<figure confidence="0.974798">
TI
a
aP a ar a,
</figure>
<figureCaption confidence="0.999932">
Figure 9: Alternative derivation
</figureCaption>
<bodyText confidence="0.999968568181818">
LIG, derived feature structures can be viewed as
consisting of a bounded part (relevant to rule ap-
plication) plus unbounded information about the
values of features. For each feature, we store in
the recognition array a bounded amount of in-
formation about its value locally, together with a
pointer to a further array element. Entries in this
element of the recognition array that are compat-
ible (i.e. unifiable) with the bounded, local infor-
mation correspond to different possible values for
the feature. For example, we can use a single en-
try in the recognition array to store the fact that
all of the feature structures that can appear at the
root of the trees in Figure 9 derive the substring
ai aj. This entry would be underspecified, for
example, the value of feature [1 would be spec-
ified to be any feature stored in the array entry
for the substring ap aq whose feature f had the
value a.
However, this is not the end of the story. In con-
trast to LIG, PLPATR licenses structure sharing on
the right hand side of productions. That is, par-
tial linearity permits feature values to be shared
between daughters where they are not also shared
with the mother. But in that case, it appears
that checking the applicability of a production at
some point in a derivation must entail the com-
parison of structures of unbounded size. In fact,
this is not so. The PLPATR recognition algorithm
employs a second array (called the compatibility
array), which encodes information about the com-
patibility of derived feature structures. Tuples of
compatible derived feature structures are stored
in the compatibility array using exactly the same
approach used to store feature structures in the
main recognition array. The presence of a tuple
in the compatibility array (the indices of which
encode which input substrings are spanned) in-
dicates the existence of derivations of compatible
feature structures. Due to the &amp;quot;context-freeness&amp;quot;
of PLPATR, new entries can be added to the com-
patibility array in a bottom-up manner based on
existing entries without the need to reconstruct
complete feature structures.
</bodyText>
<sectionHeader confidence="0.999405" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999986777777778">
In considering ways of extending LIG, this paper
has introduced the notion of partial linearity and
shown how it can be manifested in the form of
a constrained unification-based grammar formal-
ism. We have explored examples of the kinds of
tree sets and string languages that this system can
generate. We have also briefly outlined the sense
in which partial linearity gives rise to &amp;quot;context-
freeness&amp;quot; in derivations and sketched how this can
</bodyText>
<figure confidence="0.7023535">
[ f : a
g F
[ f :
g
</figure>
<page confidence="0.991324">
81
</page>
<bodyText confidence="0.9503855">
be exploited in order to obtain a tractable recog-
nition algorithm.
</bodyText>
<sectionHeader confidence="0.996232" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.989932666666667">
We thank Roger Evans, Gerald Gazdar, Aravind
Joshi, Bernard Lang, Fernando Pereira, Mark
Steedman and K. Vijay-Shanker for their help.
</bodyText>
<sectionHeader confidence="0.99803" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902177419355">
Martin Emele. 1991. Unification with lazy non-
redundant copying. In 29th meeting Assoc.
Comput Ling., pages 323-330, Berkeley, CA.
G. Gazdar. 1988. Applicability of indexed gram-
mars to natural languages. In U. Reyle and
C. Rohrer, editors, Natural Language Parsing
and Linguistic Theories, pages 69-94. D. Rei-
del, Dordrecht, Holland.
Dale Gerdemann. 1989. Using restrictions to
optimize unification parsing. In International
Workshop of Parsing Technologies, pages 8-
17, Pittsburgh, PA.
Kurt Godden. 1990. Lazy unification. In 28&apos;h
meeting Assoc. Compd. Ling., pages 180-187,
Pittsburgh, PA.
S. P. Harrison and T. M. Ellison. 1992. Restric-
tion and termination in parsing with feature-
theoretic grammars. Computational Linguis-
tics, 18(0519-531.
L. Karttunen and M. Kay. 1985. Structure shar-
ing with binary trees. In 23&apos;h meeting Assoc.
Comput Ling., pages 133-136.
T. Kasami. 1965. An efficient recognition and
syntax algorithm for context-free languages.
Technical Report AF-CRL-65-758, Air Force
Cambridge Research Laboratory, Bedford, MA.
Kiyoshi Kogure. 1990. Strategic lazy incremental
copy graph unification. In 13&apos;1 International
Conference on Comput Ling., pages 223-228,
Helsinki.
F. C. N. Pereira. 1985. A structure-sharing
representation for unification-based grammar
formalisms. In 23&apos;h meeting Assoc. Comput.
Ling., pages 137-144.
S. M. Shieber. 1984. The design of a computer
language for linguistic information. In 10&apos;h
International Conference on Comput. Ling.,
pages 363-366.
S. M. Shieber. 1985. Using restriction to ex-
tend parsing algorithms for complex-feature-
based formalisms. In 23rd meeting Assoc. Com-
put. Ling., pages 82-93.
Hideto Tomabechi. 1991. Quasi-destructive
graph unification. In 29&amp;quot; meeting Assoc. Com-
put. Ling., pages 315-322, Berkeley, CA.
K. Vijay-Shanker and D. J. Weir. 1993. Parsing
some constrained grammar formalisms. Com-
putational Linguistics, 19(0591-636.
K. Vijay-Shanker and D. J. Weir. 1994. The
equivalence of four extensions of context-free
grammars. Math. Syst. Theory, 27:511-546.
K. Vijay-Shanker, D. J. Weir, and A. K. Joshi.
1987. Characterizing structural descriptions
produced by various grammatical formalisms.
In 25&apos;h meeting Assoc. Comput. Ling., pages
104-111.
David Wroblewski. 1987. Nondestructive graph
unification. In 6&apos;h National Conference on Arti-
ficial Intelligence, pages 582-587, Seattle, WA.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information
and Control, 10(2):189-208.
</reference>
<page confidence="0.999127">
82
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.874212">
<title confidence="0.999678">A Tractable Extension of Linear Indexed Grammars</title>
<author confidence="0.999978">Bill Keller</author>
<author confidence="0.999978">David Weir</author>
<affiliation confidence="0.9999375">School of Cognitive and Computing Sciences University of Sussex</affiliation>
<address confidence="0.9405205">Falmer, Brighton BN1 9QH UK</address>
<email confidence="0.997334">bill.keller/david.weirOcogs.sussex.ac.uk</email>
<abstract confidence="0.999149">Vijay-Shanker and Weir (1993) show Linear Indexed Grammars be processed in polynomial time by exploiting constraints which make possible the extensive use of structure-sharing. This paper describes a formalism that more powerful than which can also be processed in polynomial time using similar techniques. The formalism, which we refer to as Partially Lin- (PLPATR) feature structures rather than stacks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Martin Emele</author>
</authors>
<title>Unification with lazy nonredundant copying.</title>
<date>1991</date>
<booktitle>In 29th meeting Assoc. Comput Ling.,</booktitle>
<pages>323--330</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="1982" citStr="Emele, 1991" startWordPosition="293" endWordPosition="294">e computation, so that the space requirements of the algorithm are expensive. Furthermore, the need to perform nondestructive unification means that a large proportion of the processing time is spent copying feature structures. One approach to this problem is to refine parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures (Shieber, 1985; Pereira, 1985; Karttunen and Kay, 1985; Wroblewski, 1987; Gerdemann, 1989; Godden, 1990; Kogure, 1990; Emele, 1991; Tomabechi, 1991; Harrison and Ellison, 1992)). While these techniques can yield significant improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable. This approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power. In this paper we identify a set of constraints that can be placed on unification-based grammar formalisms in order to guarantee the existence of polynomial time parsing algorithms. Our choice of const</context>
</contexts>
<marker>Emele, 1991</marker>
<rawString>Martin Emele. 1991. Unification with lazy nonredundant copying. In 29th meeting Assoc. Comput Ling., pages 323-330, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Applicability of indexed grammars to natural languages.</title>
<date>1988</date>
<booktitle>Natural Language Parsing and Linguistic Theories,</booktitle>
<pages>69--94</pages>
<editor>In U. Reyle and C. Rohrer, editors,</editor>
<location>Dordrecht, Holland.</location>
<contexts>
<context position="3367" citStr="Gazdar (1988)" startWordPosition="509" endWordPosition="510">ble processing algorithms and then consider how these constraints can be generalized to a formalism that manipulates trees rather than stacks. The constraints that we identify for the tree-based system can be regarded equally well as constraints on unification-based grammar formalisms such as PATR (Shieber, 1984). 2 From Stacks to Trees An Indexed Grammar (IG) can be viewed as a CFG in which each nonterminal is associated with a stack of indices. Productions specify not only how nonterminals can be rewritten but also how their associated stacks are modified. LIG, which were first described by Gazdar (1988), are constrained such that stacks are passed from the mother to at most a single daughter. For LIG, the size of the domain of nonterminals and associated stacks (the analogue of the nonterminals in CFG) is not bound by the grammar. However, Vijay-Shanker and Weir (1993) demonstrate that polynomial time performance 75 can be achieved through the use of structuresharing made possible by constraints in the way that LIG use stacks. Although stacks of unbounded size can arise during a derivation, it is not possible for a LIG to specify that two dependent, unbounded stacks must appear at distinct p</context>
</contexts>
<marker>Gazdar, 1988</marker>
<rawString>G. Gazdar. 1988. Applicability of indexed grammars to natural languages. In U. Reyle and C. Rohrer, editors, Natural Language Parsing and Linguistic Theories, pages 69-94. D. Reidel, Dordrecht, Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dale Gerdemann</author>
</authors>
<title>Using restrictions to optimize unification parsing.</title>
<date>1989</date>
<booktitle>In International Workshop of Parsing Technologies,</booktitle>
<pages>8--17</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1941" citStr="Gerdemann, 1989" startWordPosition="286" endWordPosition="288">ctures are stored at intermediate steps in the computation, so that the space requirements of the algorithm are expensive. Furthermore, the need to perform nondestructive unification means that a large proportion of the processing time is spent copying feature structures. One approach to this problem is to refine parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures (Shieber, 1985; Pereira, 1985; Karttunen and Kay, 1985; Wroblewski, 1987; Gerdemann, 1989; Godden, 1990; Kogure, 1990; Emele, 1991; Tomabechi, 1991; Harrison and Ellison, 1992)). While these techniques can yield significant improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable. This approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power. In this paper we identify a set of constraints that can be placed on unification-based grammar formalisms in order to guarantee the existence of polynomial tim</context>
</contexts>
<marker>Gerdemann, 1989</marker>
<rawString>Dale Gerdemann. 1989. Using restrictions to optimize unification parsing. In International Workshop of Parsing Technologies, pages 8-17, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Godden</author>
</authors>
<title>Lazy unification. In 28&apos;h meeting Assoc.</title>
<date>1990</date>
<journal>Compd. Ling.,</journal>
<pages>180--187</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1955" citStr="Godden, 1990" startWordPosition="289" endWordPosition="290"> at intermediate steps in the computation, so that the space requirements of the algorithm are expensive. Furthermore, the need to perform nondestructive unification means that a large proportion of the processing time is spent copying feature structures. One approach to this problem is to refine parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures (Shieber, 1985; Pereira, 1985; Karttunen and Kay, 1985; Wroblewski, 1987; Gerdemann, 1989; Godden, 1990; Kogure, 1990; Emele, 1991; Tomabechi, 1991; Harrison and Ellison, 1992)). While these techniques can yield significant improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable. This approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power. In this paper we identify a set of constraints that can be placed on unification-based grammar formalisms in order to guarantee the existence of polynomial time parsing algo</context>
</contexts>
<marker>Godden, 1990</marker>
<rawString>Kurt Godden. 1990. Lazy unification. In 28&apos;h meeting Assoc. Compd. Ling., pages 180-187, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Harrison</author>
<author>T M Ellison</author>
</authors>
<title>Restriction and termination in parsing with featuretheoretic grammars.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--0519</pages>
<contexts>
<context position="2028" citStr="Harrison and Ellison, 1992" startWordPosition="297" endWordPosition="300">ce requirements of the algorithm are expensive. Furthermore, the need to perform nondestructive unification means that a large proportion of the processing time is spent copying feature structures. One approach to this problem is to refine parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures (Shieber, 1985; Pereira, 1985; Karttunen and Kay, 1985; Wroblewski, 1987; Gerdemann, 1989; Godden, 1990; Kogure, 1990; Emele, 1991; Tomabechi, 1991; Harrison and Ellison, 1992)). While these techniques can yield significant improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable. This approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power. In this paper we identify a set of constraints that can be placed on unification-based grammar formalisms in order to guarantee the existence of polynomial time parsing algorithms. Our choice of constraints is motivated by showing how they genera</context>
</contexts>
<marker>Harrison, Ellison, 1992</marker>
<rawString>S. P. Harrison and T. M. Ellison. 1992. Restriction and termination in parsing with featuretheoretic grammars. Computational Linguistics, 18(0519-531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Karttunen</author>
<author>M Kay</author>
</authors>
<title>Structure sharing with binary trees. In 23&apos;h meeting Assoc. Comput Ling.,</title>
<date>1985</date>
<pages>133--136</pages>
<contexts>
<context position="1906" citStr="Karttunen and Kay, 1985" startWordPosition="279" endWordPosition="283">mar parsers. Potentially large feature structures are stored at intermediate steps in the computation, so that the space requirements of the algorithm are expensive. Furthermore, the need to perform nondestructive unification means that a large proportion of the processing time is spent copying feature structures. One approach to this problem is to refine parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures (Shieber, 1985; Pereira, 1985; Karttunen and Kay, 1985; Wroblewski, 1987; Gerdemann, 1989; Godden, 1990; Kogure, 1990; Emele, 1991; Tomabechi, 1991; Harrison and Ellison, 1992)). While these techniques can yield significant improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable. This approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power. In this paper we identify a set of constraints that can be placed on unification-based grammar formalisms in order to guaran</context>
</contexts>
<marker>Karttunen, Kay, 1985</marker>
<rawString>L. Karttunen and M. Kay. 1985. Structure sharing with binary trees. In 23&apos;h meeting Assoc. Comput Ling., pages 133-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kasami</author>
</authors>
<title>An efficient recognition and syntax algorithm for context-free languages.</title>
<date>1965</date>
<tech>Technical Report AF-CRL-65-758,</tech>
<institution>Air Force Cambridge Research Laboratory,</institution>
<location>Bedford, MA.</location>
<contexts>
<context position="15712" citStr="Kasami, 1965" startWordPosition="2732" endWordPosition="2733">gs labelling root to frontier paths of derivation trees is a context-free language. While the tree sets of LIG and Tree Adjoining Grammars have independent branches, PLTG tree sets exhibit dependent branches, where the number of dependent branches in any tree is bounded by the grammar. Note that the number of dependent branches in the tree sets of IG is not bounded by the grammar (e.g., they generate sets of all full binary trees). 5 Tractable Recognition In this section we outline the main ideas underlying a polynomial time recognition algorithm for PLPATR that generalizes the CKY algorithm (Kasami, 1965; Younger, 1967). The key to this algorithm is the use of structure sharing techniques similar to those used to process LIG efficiently (Vijay-Shanker and Weir, 1993). To understand how these techniques are applied in the case of PLPATR, it is therefore helpful to consider first the somewhat simpler case of LIG. The CKY algorithm is a bottom-up recognition algorithm for CFG. For a given grammar G and input string al ... an the algorithm constructs an array P, having n&apos; elements, where element P[i, j] stores all and only those nonterminals of G that derive the substring ai ..ai. A naive adaptat</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>T. Kasami. 1965. An efficient recognition and syntax algorithm for context-free languages. Technical Report AF-CRL-65-758, Air Force Cambridge Research Laboratory, Bedford, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoshi Kogure</author>
</authors>
<title>Strategic lazy incremental copy graph unification.</title>
<date>1990</date>
<booktitle>In 13&apos;1 International Conference on Comput Ling.,</booktitle>
<pages>223--228</pages>
<location>Helsinki.</location>
<contexts>
<context position="1969" citStr="Kogure, 1990" startWordPosition="291" endWordPosition="292">te steps in the computation, so that the space requirements of the algorithm are expensive. Furthermore, the need to perform nondestructive unification means that a large proportion of the processing time is spent copying feature structures. One approach to this problem is to refine parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures (Shieber, 1985; Pereira, 1985; Karttunen and Kay, 1985; Wroblewski, 1987; Gerdemann, 1989; Godden, 1990; Kogure, 1990; Emele, 1991; Tomabechi, 1991; Harrison and Ellison, 1992)). While these techniques can yield significant improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable. This approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power. In this paper we identify a set of constraints that can be placed on unification-based grammar formalisms in order to guarantee the existence of polynomial time parsing algorithms. Our ch</context>
</contexts>
<marker>Kogure, 1990</marker>
<rawString>Kiyoshi Kogure. 1990. Strategic lazy incremental copy graph unification. In 13&apos;1 International Conference on Comput Ling., pages 223-228, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
</authors>
<title>A structure-sharing representation for unification-based grammar formalisms.</title>
<date>1985</date>
<booktitle>In 23&apos;h meeting Assoc. Comput. Ling.,</booktitle>
<pages>137--144</pages>
<contexts>
<context position="1881" citStr="Pereira, 1985" startWordPosition="277" endWordPosition="278">ntext-free grammar parsers. Potentially large feature structures are stored at intermediate steps in the computation, so that the space requirements of the algorithm are expensive. Furthermore, the need to perform nondestructive unification means that a large proportion of the processing time is spent copying feature structures. One approach to this problem is to refine parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures (Shieber, 1985; Pereira, 1985; Karttunen and Kay, 1985; Wroblewski, 1987; Gerdemann, 1989; Godden, 1990; Kogure, 1990; Emele, 1991; Tomabechi, 1991; Harrison and Ellison, 1992)). While these techniques can yield significant improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable. This approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power. In this paper we identify a set of constraints that can be placed on unification-based grammar form</context>
</contexts>
<marker>Pereira, 1985</marker>
<rawString>F. C. N. Pereira. 1985. A structure-sharing representation for unification-based grammar formalisms. In 23&apos;h meeting Assoc. Comput. Ling., pages 137-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>The design of a computer language for linguistic information.</title>
<date>1984</date>
<booktitle>In 10&apos;h International Conference on Comput. Ling.,</booktitle>
<pages>363--366</pages>
<contexts>
<context position="3068" citStr="Shieber, 1984" startWordPosition="458" endWordPosition="459">fication-based grammar formalisms in order to guarantee the existence of polynomial time parsing algorithms. Our choice of constraints is motivated by showing how they generalize constraints inherent in Linear Indexed Grammar (LIG). We begin by describing how constraints inherent in LIG admit tractable processing algorithms and then consider how these constraints can be generalized to a formalism that manipulates trees rather than stacks. The constraints that we identify for the tree-based system can be regarded equally well as constraints on unification-based grammar formalisms such as PATR (Shieber, 1984). 2 From Stacks to Trees An Indexed Grammar (IG) can be viewed as a CFG in which each nonterminal is associated with a stack of indices. Productions specify not only how nonterminals can be rewritten but also how their associated stacks are modified. LIG, which were first described by Gazdar (1988), are constrained such that stacks are passed from the mother to at most a single daughter. For LIG, the size of the domain of nonterminals and associated stacks (the analogue of the nonterminals in CFG) is not bound by the grammar. However, Vijay-Shanker and Weir (1993) demonstrate that polynomial t</context>
</contexts>
<marker>Shieber, 1984</marker>
<rawString>S. M. Shieber. 1984. The design of a computer language for linguistic information. In 10&apos;h International Conference on Comput. Ling., pages 363-366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>Using restriction to extend parsing algorithms for complex-featurebased formalisms.</title>
<date>1985</date>
<booktitle>In 23rd meeting Assoc. Comput. Ling.,</booktitle>
<pages>82--93</pages>
<contexts>
<context position="1866" citStr="Shieber, 1985" startWordPosition="275" endWordPosition="276"> in standard context-free grammar parsers. Potentially large feature structures are stored at intermediate steps in the computation, so that the space requirements of the algorithm are expensive. Furthermore, the need to perform nondestructive unification means that a large proportion of the processing time is spent copying feature structures. One approach to this problem is to refine parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures (Shieber, 1985; Pereira, 1985; Karttunen and Kay, 1985; Wroblewski, 1987; Gerdemann, 1989; Godden, 1990; Kogure, 1990; Emele, 1991; Tomabechi, 1991; Harrison and Ellison, 1992)). While these techniques can yield significant improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable. This approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power. In this paper we identify a set of constraints that can be placed on unification-bas</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>S. M. Shieber. 1985. Using restriction to extend parsing algorithms for complex-featurebased formalisms. In 23rd meeting Assoc. Comput. Ling., pages 82-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideto Tomabechi</author>
</authors>
<title>Quasi-destructive graph unification. In 29&amp;quot; meeting Assoc.</title>
<date>1991</date>
<journal>Comput. Ling.,</journal>
<pages>315--322</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="1999" citStr="Tomabechi, 1991" startWordPosition="295" endWordPosition="296">, so that the space requirements of the algorithm are expensive. Furthermore, the need to perform nondestructive unification means that a large proportion of the processing time is spent copying feature structures. One approach to this problem is to refine parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures (Shieber, 1985; Pereira, 1985; Karttunen and Kay, 1985; Wroblewski, 1987; Gerdemann, 1989; Godden, 1990; Kogure, 1990; Emele, 1991; Tomabechi, 1991; Harrison and Ellison, 1992)). While these techniques can yield significant improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable. This approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power. In this paper we identify a set of constraints that can be placed on unification-based grammar formalisms in order to guarantee the existence of polynomial time parsing algorithms. Our choice of constraints is motivat</context>
</contexts>
<marker>Tomabechi, 1991</marker>
<rawString>Hideto Tomabechi. 1991. Quasi-destructive graph unification. In 29&amp;quot; meeting Assoc. Comput. Ling., pages 315-322, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
</authors>
<title>Parsing some constrained grammar formalisms.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--0591</pages>
<contexts>
<context position="3638" citStr="Vijay-Shanker and Weir (1993)" startWordPosition="555" endWordPosition="558">ation-based grammar formalisms such as PATR (Shieber, 1984). 2 From Stacks to Trees An Indexed Grammar (IG) can be viewed as a CFG in which each nonterminal is associated with a stack of indices. Productions specify not only how nonterminals can be rewritten but also how their associated stacks are modified. LIG, which were first described by Gazdar (1988), are constrained such that stacks are passed from the mother to at most a single daughter. For LIG, the size of the domain of nonterminals and associated stacks (the analogue of the nonterminals in CFG) is not bound by the grammar. However, Vijay-Shanker and Weir (1993) demonstrate that polynomial time performance 75 can be achieved through the use of structuresharing made possible by constraints in the way that LIG use stacks. Although stacks of unbounded size can arise during a derivation, it is not possible for a LIG to specify that two dependent, unbounded stacks must appear at distinct places in the derivation tree. Structure-sharing can therefore be used effectively because checking the applicability of rules at each step in the derivation involves the comparison of structures of limited size. Our goal is to generalize the constraints inherent in LIG t</context>
<context position="15878" citStr="Vijay-Shanker and Weir, 1993" startWordPosition="2758" endWordPosition="2761">endent branches, PLTG tree sets exhibit dependent branches, where the number of dependent branches in any tree is bounded by the grammar. Note that the number of dependent branches in the tree sets of IG is not bounded by the grammar (e.g., they generate sets of all full binary trees). 5 Tractable Recognition In this section we outline the main ideas underlying a polynomial time recognition algorithm for PLPATR that generalizes the CKY algorithm (Kasami, 1965; Younger, 1967). The key to this algorithm is the use of structure sharing techniques similar to those used to process LIG efficiently (Vijay-Shanker and Weir, 1993). To understand how these techniques are applied in the case of PLPATR, it is therefore helpful to consider first the somewhat simpler case of LIG. The CKY algorithm is a bottom-up recognition algorithm for CFG. For a given grammar G and input string al ... an the algorithm constructs an array P, having n&apos; elements, where element P[i, j] stores all and only those nonterminals of G that derive the substring ai ..ai. A naive adaptation of this algorithm for LIG recognition would involve storing a set of nonterminals and their associated stacks. But since stack length is at least proportional to </context>
</contexts>
<marker>Vijay-Shanker, Weir, 1993</marker>
<rawString>K. Vijay-Shanker and D. J. Weir. 1993. Parsing some constrained grammar formalisms. Computational Linguistics, 19(0591-636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
</authors>
<title>The equivalence of four extensions of context-free grammars.</title>
<date>1994</date>
<journal>Math. Syst. Theory,</journal>
<pages>27--511</pages>
<contexts>
<context position="14094" citStr="Vijay-Shanker and Weir, 1994" startWordPosition="2443" endWordPosition="2446">res associated with all nodes in the tree are available, feature information need only be passed up the tree when it is required in order to establish dependencies within the derivation tree. When this approach is taken, there may be less need for reentrancy in the root feature structure. Furthermore, re-entrancy in the form of shared feature structures within and across nodes will be found in PLPATR (see for example Figure 5). 4 Generative Capacity LIG are more powerful than CFG and are known to be weakly equivalent to Tree Adjoining Grammar, Combinatory Categorial Grammar, and Head Grammar (Vijay-Shanker and Weir, 1994). PLIG are more powerful than LIG since they can generate the k-copy language for any fixed k (see Example 2). Slightly more generally, PLIG can generate the language {wkIwER} for any k &gt; 1 and regular language R. We believe that the language involving copies of strings of matching brackets described in Example 5 cannot be generated by PLIG but, as shown in Exaniple 5, it can be generated by PLTG and therefore PLPATR. Slightly more generally, PLTG can generate the language { wk tv E L for any k &gt; 1 and context-free language L. It appears that the class of languages generated by PLTG is include</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1994</marker>
<rawString>K. Vijay-Shanker and D. J. Weir. 1994. The equivalence of four extensions of context-free grammars. Math. Syst. Theory, 27:511-546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
<author>A K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms. In 25&apos;h meeting Assoc.</title>
<date>1987</date>
<journal>Comput. Ling.,</journal>
<pages>104--111</pages>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shanker, D. J. Weir, and A. K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In 25&apos;h meeting Assoc. Comput. Ling., pages 104-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Wroblewski</author>
</authors>
<title>Nondestructive graph unification.</title>
<date>1987</date>
<booktitle>In 6&apos;h National Conference on Artificial Intelligence,</booktitle>
<pages>582--587</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="1924" citStr="Wroblewski, 1987" startWordPosition="284" endWordPosition="285">large feature structures are stored at intermediate steps in the computation, so that the space requirements of the algorithm are expensive. Furthermore, the need to perform nondestructive unification means that a large proportion of the processing time is spent copying feature structures. One approach to this problem is to refine parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures (Shieber, 1985; Pereira, 1985; Karttunen and Kay, 1985; Wroblewski, 1987; Gerdemann, 1989; Godden, 1990; Kogure, 1990; Emele, 1991; Tomabechi, 1991; Harrison and Ellison, 1992)). While these techniques can yield significant improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable. This approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power. In this paper we identify a set of constraints that can be placed on unification-based grammar formalisms in order to guarantee the existence </context>
</contexts>
<marker>Wroblewski, 1987</marker>
<rawString>David Wroblewski. 1987. Nondestructive graph unification. In 6&apos;h National Conference on Artificial Intelligence, pages 582-587, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages</title>
<date>1967</date>
<booktitle>in time n3. Information and Control,</booktitle>
<pages>10--2</pages>
<contexts>
<context position="15728" citStr="Younger, 1967" startWordPosition="2734" endWordPosition="2735">oot to frontier paths of derivation trees is a context-free language. While the tree sets of LIG and Tree Adjoining Grammars have independent branches, PLTG tree sets exhibit dependent branches, where the number of dependent branches in any tree is bounded by the grammar. Note that the number of dependent branches in the tree sets of IG is not bounded by the grammar (e.g., they generate sets of all full binary trees). 5 Tractable Recognition In this section we outline the main ideas underlying a polynomial time recognition algorithm for PLPATR that generalizes the CKY algorithm (Kasami, 1965; Younger, 1967). The key to this algorithm is the use of structure sharing techniques similar to those used to process LIG efficiently (Vijay-Shanker and Weir, 1993). To understand how these techniques are applied in the case of PLPATR, it is therefore helpful to consider first the somewhat simpler case of LIG. The CKY algorithm is a bottom-up recognition algorithm for CFG. For a given grammar G and input string al ... an the algorithm constructs an array P, having n&apos; elements, where element P[i, j] stores all and only those nonterminals of G that derive the substring ai ..ai. A naive adaptation of this algo</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>D. H. Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10(2):189-208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>