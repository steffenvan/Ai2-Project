<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000050">
<title confidence="0.997057">
Additive Neural Networks for Statistical Machine Translation
</title>
<author confidence="0.999472">
Lemao Liu&apos;, Taro Watanabe2, Eiichiro Sumita2, Tiejun Zhao&apos;
</author>
<affiliation confidence="0.998509666666667">
&apos;School of Computer Science and Technology
Harbin Institute of Technology (HIT), Harbin, China
2National Institute of Information and Communication Technology (NICT)
</affiliation>
<address confidence="0.922633">
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
</address>
<email confidence="0.949853">
{lmliu  |tjzhao}@mtlab.hit.edu.cn
{taro.watanabe  |eiichiro.sumita}@nict.go.jp
</email>
<sectionHeader confidence="0.99712" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999451083333333">
Most statistical machine translation
(SMT) systems are modeled using a log-
linear framework. Although the log-linear
model achieves success in SMT, it still
suffers from some limitations: (1) the
features are required to be linear with
respect to the model itself; (2) features
cannot be further interpreted to reach
their potential. A neural network is
a reasonable method to address these
pitfalls. However, modeling SMT with a
neural network is not trivial, especially
when taking the decoding efficiency
into consideration. In this paper, we
propose a variant of a neural network, i.e.
additive neural networks, for SMT to go
beyond the log-linear translation model.
In addition, word embedding is employed
as the input to the neural network, which
encodes each word as a feature vector.
Our model outperforms the log-linear
translation models with/without embed-
ding features on Chinese-to-English and
Japanese-to-English translation tasks.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998914462962963">
Recently, great progress has been achieved in
SMT, especially since Och and Ney (2002) pro-
posed the log-linear model: almost all the state-
of-the-art SMT systems are based on the log-linear
model. Its most important advantage is that arbi-
trary features can be added to the model. Thus,
it casts complex translation between a pair of lan-
guages as feature engineering, which facilitates re-
search and development for SMT.
Regardless of how successful the log-linear
model is in SMT, it still has some shortcomings.
This joint work was done while the first author visited
NICT.
On the one hand, features are required to be lin-
ear with respect to the objective of the translation
model (Nguyen et al., 2007), but it is not guaran-
teed that the potential features be linear with the
model. This induces modeling inadequacy (Duh
and Kirchhoff, 2008), in which the translation per-
formance may not improve, or may even decrease,
after one integrates additional features into the
model. On the other hand, it cannot deeply in-
terpret its surface features, and thus can not ef-
ficiently develop the potential of these features.
What may happen is that a feature p does initially
not improve the translation performance, but after
a nonlinear operation, e.g. log(p), it does. The
reason is not because this feature is useless but the
model does not efficiently interpret and represent
it. Situations such as this confuse explanations for
feature designing, since it is unclear whether such
a feature contributes to a translation or not.
A neural network (Bishop, 1995) is a reason-
able method to overcome the above shortcomings.
However, it should take constraints, e.g. the de-
coding efficiency, into account in SMT. Decod-
ing in SMT is considered as the expansion of
translation states and it is handled by a heuris-
tic search (Koehn, 2004a). In the search pro-
cedure, frequent computation of the model score
is needed for the search heuristic function, which
will be challenged by the decoding efficiency for
the neural network based translation model. Fur-
ther, decoding with non-local (or state-dependent)
features, such as a language model, is also a prob-
lem. Actually, even for the (log-) linear model,
efficient decoding with the language model is not
trivial (Chiang, 2007).
In this paper, we propose a variant of neural net-
works, i.e. additive neural networks (see Section
3 for details), for SMT. It consists of two com-
ponents: a linear component which captures non-
local (or state dependent) features and a non-linear
component (i.e., neural nework) which encodes lo-
</bodyText>
<page confidence="0.967683">
791
</page>
<note confidence="0.916211">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 791–801,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.801156166666667">
✚ ❸ X
❐ ✟❭
X
X
X over the last years
friendly cooperation
</figure>
<figureCaption confidence="0.62738575">
Figure 1: A bilingual tree with two synchronous rules, r1 : X -+ (❐⑥ ✟❭; friendly cooperation)
and r2 : X -+ (✚+❡ ❸ X; X over the last years). The inside rectangle denotes the partial derivation
d1 = Ir1} with the partial translation e1 =“friendly cooperation”, and the outside rectangle denotes the
derivation d2 = Ir1, r2} with the translation e2=“friendly cooperation over the last years”.
</figureCaption>
<bodyText confidence="0.999675222222223">
cal (or state independent) features. Compared with
the log-linear model, it has more powerful expres-
sive abilities and can deeply interpret and repre-
sent features with hidden units in neural networks.
Moreover, our method is simple to implement and
its decoding efficiency is comparable to that of the
log-linear model. We also integrate word embed-
ding into the model by representing each word as
a feature vector (Collobert and Weston, 2008).
Because of the thousands of parameters and the
non-convex objective in our model, efficient train-
ing is not simple. We propose an efficient train-
ing methodology: we apply the mini-batch conju-
gate sub-gradient algorithm (Le et al., 2011) to ac-
celerate the training; we also propose pre-training
and post-training methods to avoid poor local min-
ima. The biggest contribution of this paper is that
it goes beyond the log-linear model and proposes a
non-linear translation model instead of re-ranking
model (Duh and Kirchhoff, 2008; Sokolov et al.,
2012).
On both Chinese-to-English and Japanese-to-
English translation tasks, experiment results show
that our model can leverage the shortcomings suf-
fered by the log-linear model, and thus achieves
significant improvements over the log-linear based
translation.
a collection of synchronous rules for Hiero
grammar (Chiang, 2005), or phrase pairs in
Moses (Koehn et al., 2007); h(f, e, d) =
(h1(f, e, d), h2(f, e, d), ··· , hK(f, e, d))&gt; is a
K-dimensional feature vector defined on the tu-
ple (f, e, d); W = (w1, w2, ··· , wK)&gt; is a K-
dimensional weight vector of h, i.e., the parame-
ters of the model, and it can be tuned by the toolkit
MERT (Och, 2003). Different from Brown’s
generative model (Brown et al., 1993), the log-
linear model does not assume strong indepen-
dency holds, and allows arbitrary features to be
integrated into the model easily. In other words,
it can transform complex language translation into
feature engineering: it can achieve high translation
performance if reasonable features are chosen and
appropriate parameters are assigned for the weight
vector.
</bodyText>
<subsectionHeader confidence="0.999843">
2.2 Decoding By Search
</subsectionHeader>
<bodyText confidence="0.999773333333333">
Given a source sentence f and a weight W, de-
coding finds the best translation candidate eˆ via
the programming problem:
</bodyText>
<equation confidence="0.96515025">
(ˆe, ˆd) = arg max P(e,d|f;W)
e,d
= arg max {W&gt; · h(f, e, d)}. (2)
e,d
</equation>
<sectionHeader confidence="0.995117" genericHeader="method">
2 Log-linear Model, Revisited
</sectionHeader>
<subsectionHeader confidence="0.932306">
2.1 Log-linear Translation Model
</subsectionHeader>
<bodyText confidence="0.9995495">
Och and Ney (2002) proposed the log-linear trans-
lation model, which can be formalized as follows:
</bodyText>
<equation confidence="0.978232333333333">
exp {W&gt; · h(f,e, d)}
P(e,d|f;W) = (1)
Ee,,d, exp {W&gt; · h(f, e0, d0)}
</equation>
<bodyText confidence="0.999669285714286">
where f denotes the source sentence, and
e(e0) denotes its translation candidate; d(d0)
is a derivation over the pair
Since the range of (e, d) is exponential with re-
spect to the size of f, the exact decoding is in-
tractable and an inexact strategy such as beam
search is used instead in practice.
The idea of search for decoding can be shown
in Figure 1: it encodes each search state as a
partial translation together with its derivation, e.g.
(e1, d1); it consequently expands the states from
the initial (empty) state to the end state (e2, d2)
according to the translation rules r1 and r2. Dur-
ing the state expansion process, the score wz ·
</bodyText>
<equation confidence="0.480795">
(f, e), i.e.,
</equation>
<page confidence="0.990488">
792
</page>
<bodyText confidence="0.999714583333333">
hi(f, e, d) for a partial translation is calculated re-
peatedly. In the log-linear model, if hi(f, e, d) is
a local feature, the calculation of its score wi ·
hi(f, e, d) has a substructure, and thus it can be
calculated with dynamic programming which ac-
celerates its decoding. For the non-local features
such as the language model, Chiang (2007) pro-
posed a cube-pruning method for efficient decod-
ing. The main reason why cube-pruning works is
that the translation model is linear and the model
score for the language model is approximately
monotonic (Chiang, 2007).
</bodyText>
<sectionHeader confidence="0.973822" genericHeader="method">
3 Additive Neural Networks
</sectionHeader>
<subsectionHeader confidence="0.993519">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999971764705882">
Although the log-linear model has achieved great
progress for SMT, it still suffers from some pit-
falls: it requires features be linear with the model
and it can not interpret and represent features
deeply. The neural network model is a reason-
able method to overcome these pitfalls. However,
the neural network based machine translation is far
from easy.
As mentioned in Section 2, the decoding proce-
dure performs an expansion of translation states.
Firstly, let us consider a simple case in neural net-
work based translation where all the features in the
translation model are independent of the transla-
tion state, i.e. all the components of the vector
h(f, e, d) are local features. In this way, we can
easily define the following translation model with
a single-layer neural network:
</bodyText>
<equation confidence="0.975545">
S(f, e, d; W, M, B) =
WT · σ(M · h(f, e, d) + B), (3)
</equation>
<bodyText confidence="0.965616">
where M E Ru,K is a matrix, and B E Ru is a
vector, i.e. bias; σ is a single-layer neural network
with u hidden units, i.e. an element wise sigmoid
function sigmoid(x) = 1/(1 + exp(−x)). For
consistent description in the rest, we also represent
Eq. (3) as a function of a feature vector h, i.e.
S(h; W, M, B) = WT · σ(M · h + B).
Now let us consider the search procedure with
the model in Eq. (3) using Figure 1 as our ex-
ample. Suppose the current translation state is en-
coded as (e1, d1), which is expanded into (e2, d2)
using the rule r2 (d2 = d1 U {r2}). Since h is
state-independent, h(f, e2, d2) = h(f, e1, d1) +
h(r2). However, since S(f, e, d; W, M, B) is non-
decomposable as a linear model, there is no sub-
structure for calculating S(f, e2, d2; W, M, B),
and one has to re-calculate it via Eq. (3) even
if the score of S(f, e1, d1; M, B) for its previous
state (e1, d1) is available. When the size of the pa-
rameter (W, M, B) is relatively large, it will be a
challenge for the decoding efficiency.
In order to keep the substructure property,
S(f, e2, d2; W, M, B) should be represented as
F (S(f, e1, d1; W, M, B); S(h(r2); M, B)) by a
function F. For simplicity, we suppose that the
additive property holds in F, and then we can ob-
tain a new translation model via the following re-
cursive equation:
</bodyText>
<equation confidence="0.9827305">
S(f,e2,d2;W,M,B) = S(f, e1, d1; W,M,B)
+ S(h(r2); W, M, B). (4)
</equation>
<bodyText confidence="0.999538529411764">
Since the above model is defined only on lo-
cal features, it ignores the contributions from non-
local features. Actually, existing works empir-
ically show that some non-local features, espe-
cially language model, contribute greatly to ma-
chine translation.
Scoring for non-local features such as a n-
gram language model is not easily done. In log-
linear translation model, Chiang (2007) proposed
a cube-pruning method for scoring the language
model. The premise of cube-pruning is that the
language model score is approximately monotonic
(Chiang, 2007). However, if scoring the language
model with a neural network, this premise is diffi-
cult to hold. Therefore, one of the solutions is to
preserve a linear model for scoring the language
model directly.
</bodyText>
<subsectionHeader confidence="0.996595">
3.2 Definition
</subsectionHeader>
<bodyText confidence="0.999957">
According to the above analysis, we propose
a variant of a neural network model for ma-
chine translation, and we call it Additive Neural
Networks or AdNN for short.
The AdNN model is a combination of a lin-
ear model and a neural network: non-local fea-
tures, e.g. LM, are linearly modeled for the cube-
pruning strategy, and local features are modeled
by the neural network for deep interpretation and
representation. Formally, the AdNN based transla-
tion model is discriminative but non-probabilistic,
and it can be defined as follows:
</bodyText>
<equation confidence="0.880492">
S(f, e, d; θ) = WT · h(f, e, d)+
� W&apos;T · σ(M · h&apos;(r) + B), (5)
rEd
</equation>
<page confidence="0.974608">
793
</page>
<bodyText confidence="0.964595636363636">
where h and h&apos; are feature vectors with dimension
K and K&apos; respectively, and each component of h&apos;
is a local feature which can be defined on a rule
r : X -+ (α, -y); 0 = (W, W&apos;, M, B) is the model
parameters with M E R&apos;&amp;quot;K&apos;. In this paper, we
focus on a single-layer neural network for its sim-
plicity, and one can similarly define σ as a multi-
layer neural network.
Again for the example shown in Figure 1, the
model score defined in Eq. (5) for the pair (e2, d2)
can be represented as follows:
</bodyText>
<equation confidence="0.947278">
S(f, e2, d2; 0) = WT · h(f, e2, d2)+
W&apos;T·σ(M·h&apos;(r1)+B)+W&apos;T·σ(M·h&apos;(r2)+B).
</equation>
<bodyText confidence="0.9987536">
Eq. (5) is similar to both additive models (Buja
et al., 1989) and generalized additive neural net-
works (Potts, 1999): it consists of many additive
terms, and each term is either a linear or a non-
linear (a neural network) model. That is the rea-
son why our model is called “additive neural net-
works”. Of course, our model still has some dif-
ferences from both of them. Firstly, our model is
decomposable with respect to rules instead of the
component variables. Secondly, some of its addi-
tive terms share the same parameters (M, B).
There are also strong relationships between
AdNN and the log-linear model. If we consider
the parameters (M, B) as constant and σ(M ·
h&apos;(r) + B) as a new feature vector, then AdNN is
reduced to a log-linear model. Since both (M, B)
and (W, W&apos;) are parameters in AdNN, our model
can jointly learn the feature σ(M · h&apos;(r) + B) and
tune the weight (W, W&apos;) of the log-linear model
together. That is different from most works un-
der the log-linear translation framework, which
firstly learn features or sub-models and then tune
the log-linear model including the learned features
in two separate steps. By joint training, AdNN can
learn the features towards the translation evalua-
tion metric, which is the main advantage of our
model over the log-linear model.
In this paper, we apply our AdNN model to hi-
erarchical phrase based translation, and it can be
similarly applied to phrase-based or syntax-based
translation. Similar to Hiero (Chiang, 2005), the
feature vector h in Eq. (5) includes 8 default fea-
tures, which consist of translation probabilities,
lexical translation probabilities, word penalty, glue
rule penalty, synchronous rule penalty and lan-
guage model. These default features are included
because they empirically perform well in the log-
linear model. For the local feature vector h&apos; in Eq
(5), we employ word embedding features as de-
scribed in the following subsection.
</bodyText>
<subsectionHeader confidence="0.997078">
3.3 Word Embedding features for AdNN
</subsectionHeader>
<bodyText confidence="0.999970473684211">
Word embedding can relax the sparsity introduced
by the lexicalization in NLP, and it improves the
systems for many tasks such as language model,
named entity recognition, and parsing (Collobert
and Weston, 2008; Turian et al., 2010; Collobert,
2011). Here, we propose embedding features for
rules in SMT by combining word embeddings.
Firstly, we will define the embedding for the
source side α of a rule r : X -+ (α, -y). Let
VS be the vocabulary in the source language with
size VS ; R&amp;quot;|VS |be the word embedding matrix,
each column of which is the word embedding (n-
dimensional vector) for the corresponding word in
VS; and maxSource be the maximal length of α
for all rules. We further assume that the α for all
rules share the same length as maxSource; other-
wise, we add maxSource − α words “NULL”
to the end of α to obtain a new α. We define the
embedding of α as the concatenation of the word
embedding of each word in α. In particular, for
the non-terminal in α, we define its word embed-
ding as the vector whose components are 0.1; and
we define the word embedding of “NULL” as 0.
Then, we similarly define the embedding for the
target side of a rule, given an embedding matrix
for the target vocabulary. Finally, we define the
embedding of a rule as the concatenation of the
embedding of its source and target sides.
In this paper, we apply the word embedding ma-
trices from the RNNLM toolkit (Mikolov et al.,
2010) with the default settings: we train two RNN
language models on the source and target sides of
training corpus, respectively, and then we obtain
two matrices as their by-products1. It would be
potentially better to train the word embedding ma-
trix from a much larger corpus as (Collobert and
Weston, 2008), and we will leave this as a future
task.
</bodyText>
<subsectionHeader confidence="0.976176">
3.4 Decoding
</subsectionHeader>
<bodyText confidence="0.967579">
Substituting the P(e, d f; W) in Eq. (2) with
S(f, e, d; 0) in Eq. (5), we can obtain its corre-
1In the RNNLM toolkit, the default dimension for word
embedding is n = 30. In our experiments, the maximal
length of α and γ are 5 and 12 respectively. Thus the di-
mension for h&apos; is K&apos; = 30 × (5 + 12) = 510.
</bodyText>
<page confidence="0.988148">
794
</page>
<bodyText confidence="0.976058">
sponding decoding formula:
</bodyText>
<equation confidence="0.930441">
(ˆe, ˆd) = arg max 5(f, e, d; 0).
e,d
</equation>
<bodyText confidence="0.999829857142857">
Given the model parameter 0 = (W, W&apos;, M, B), if
we consider (M, B) as constant and Q(M ·h&apos;(r)+
B) as an additional feature vector besides h, then
Eq. (5) goes back to being a log-linear model with
parameter (W, W&apos;). In this way, the decoding for
AdNN can share the same search strategy and cube
pruning method as the log-linear model.
</bodyText>
<sectionHeader confidence="0.991913" genericHeader="method">
4 Training Method
</sectionHeader>
<subsectionHeader confidence="0.983269">
4.1 Training Objective
</subsectionHeader>
<bodyText confidence="0.999986823529412">
For the log-linear model, there are various tun-
ing methods, e.g. MERT (Och, 2003), MIRA
(Watanabe et al., 2007; Chiang et al., 2008), PRO
(Hopkins and May, 2011) and so on, which itera-
tively optimize a weight such that, after re-ranking
a k-best list of a given development set with this
weight, the loss of the resulting 1-best list is mini-
mal. In the extreme, if the k-best list consists only
of a pair of translations ((e*, d*), (e&apos;, d&apos;)), the de-
sirable weight should satisfy the assertion: if the
BLEU score of e* is greater than that of e&apos;, then
the model score of (e*, d*) with this weight will
be also greater than that of (e&apos;, d&apos;). In this paper,
a pair (e*, e&apos;) for a source sentence f is called as
a preference pair for f. Following PRO, we define
the following objective function under the max-
margin framework to optimize the AdNN model:
</bodyText>
<equation confidence="0.998437666666667">
2 110112 + A
1 �
N
f
with
S(·) = max {5(f, e&apos;, d&apos;; 0) − 5(f, e*, d*; 0) + 1, 01
</equation>
<bodyText confidence="0.99878">
where f is a source sentence in a given devel-
opment set, and ((e*, d*), (e&apos;, d&apos;)) is a preference
pair for f; N is the number of all preference pairs;
A &gt; 0 is a regularizer.
</bodyText>
<subsectionHeader confidence="0.975902">
4.2 Optimization Algorithm
</subsectionHeader>
<bodyText confidence="0.960851529411765">
Since there are thousands of parameters in Eq. (6)
and the tuning in SMT will minimize Eq. (6) re-
peatedly, efficient and scalable optimization meth-
ods are required. Following Le et al. (2011),
we apply the mini-batch Conjugate Sub-Gradient
(mini-batch CSG) method to minimize Eq. (6).
Compared with the sub-gradient descent, mini-
batch CSG has some advantages: (1) it can ac-
celerate the calculation of the sub-gradient since
it calculates the sub-gradient on a subset of pref-
erence pairs (i.e. mini-batch) instead of all of the
preference pairs; (2) it reduces the number of iter-
ations since it employs the conjugate information
besides the sub-gradient. Algorithm 1 shows the
procedure to minimize Eq. (6).
Algorithm 1 Mini-batch conjugate subgradient
Input: 01, T, CGIter, batch-size, k-best-list
</bodyText>
<listItem confidence="0.9869152">
1: for all t such that 1 &lt; t &lt; T do
2: Sample mini-batch preference pairs with
size batch-size from k-best-list
3: Calculate some quantities for CG, e.g.
training objective Obj, subgradient A, ac-
cording to Eq. (6) defined over the sampled
preference pairs
4: 0t+1 = CG(0t, Obj, A, CGIter)
5: end for
Output: 0T+1
</listItem>
<bodyText confidence="0.999801090909091">
In detail, line 2 in Algorithm 1 firstly fol-
lows PRO to sample a set of preference pairs
from k-best-list, and then uniformly samples
batch-size pairs from the preference pair set. Line
3 calculates some quantities for CG, and Line 4
calls a CG optimizer 2 and obtains 0t+1. At the
end of the algorithm, it returns the result 0T+1. In
this work, we set the maximum number of CG iter-
ations, CGIter, to a small number, which means
0t+1 will be returned within CGIter iterations be-
fore the CG converges, for faster learning.
</bodyText>
<subsectionHeader confidence="0.998686">
4.3 Pre-Training and Post-Training
</subsectionHeader>
<bodyText confidence="0.999881454545454">
Since Eq. (6) is non-linear, there are many local
minimal solutions. Actually, this problem is inher-
ent and is one many works based on the neural net-
work for other NLP tasks such as language model
and parsing, also suffer from. And these works
empirically show that some pre-training methods,
which provide a reasonable initial solution, can
improve the performance. Observing the structure
of Eq. (5) and the relationships between our model
and a log-linear model, we propose the following
simple pre-training method.
</bodyText>
<footnote confidence="0.9348834">
2In implementation, we call the CG toolkit (Hager and
Zhang, 2006), which requires overloading objective and sub-
gradient functions. For easier description, we substitute over-
loading functions and transform the value of functions in the
pseudo-code.
</footnote>
<table confidence="0.6753595">
� S(f, e*, d*, e&apos;, d&apos;; 0), (6)
e*,d*,e&apos;,d&apos;
</table>
<page confidence="0.991097">
795
</page>
<bodyText confidence="0.95136716">
If we set W&apos; = 0, the model defined in Eq. (5)
can be regarded as a log-linear model with features
h. Therefore, we pre-train W using MERT or PRO
by holding W&apos; = 0, and use (W, W&apos; = 0, M, B)
as an initializer3 for Algorithm 1.
Although the above pre-training would provide
a reasonable solution, Algorithm 1 may still fall
into local minima. We also propose a post-training
method: after obtaining a solution with Algorithm
1, we modify this solution slightly to get a new
solution. The idea of the post-training method is
similar to that of the pre-training method. Suppose
0 = (W, W&apos;, M, B) be the solution obtained from
Algorithm 1. If we consider both M and B to be
constant, the Eq. (5) goes back to the log-linear
model whose features are (h, σ(M · h&apos; + B)) and
parameters are (W, W&apos;). Again, we train the pa-
rameters (W, W&apos;) with MERT or PRO and get the
new parameters ( W¯, W¯&apos;). Therefore, we can set
0 = ( W¯, W¯&apos;, M, B) as the final solution for Eq.
(6). The advantage of post-training is that it op-
timizes a convex programming derived from the
original nonlinear (non-convex) programming in
Eq. (6), and thus it may decrease the risk of poor
local optima.
</bodyText>
<subsectionHeader confidence="0.8183585">
4.4 Training Algorithm
Algorithm 2 Training Algorithm
</subsectionHeader>
<bodyText confidence="0.9897705">
Input: MaxIter, a dev set, parameters (e.g. A )
for Algorithm 1
</bodyText>
<listItem confidence="0.9815196">
1: Pre-train to obtain 01 = (W, W&apos; = 0, M, B)
as the initial parameter
2: for all i such that 1 ≤ i ≤ MaxIter do
3: Decode with 0i on the dev set and merge all
k-best-lists
4: Run Algorithm 1 based on the merged k-
best-list to obtain 0i+1
5: end for
6: Post-train based on 0MaxIter+1 to obtain 0
Output: 0
</listItem>
<bodyText confidence="0.983420315789473">
The whole training for the AdNN model is sum-
marized in Algorithm 2. Given a development set,
we first run pre-training to obtain an initial param-
eter 01 for Algorithm 1 in line 1. Secondly, it it-
eratively performs decoding and optimization for
MaxIter times in the loop from line 2 to line 5: it
decodes with the parameter 0i and merges all the
3To avoid the symmetry in the solution, we sample a very
small (M, B) from the gaussian distribution in practice in-
stead of setting (M, B) = 0.
k-best-lists in line 3; and it then runs Algorithm 1
to optimize 0i+1. Thirdly, it runs the post-training
to get the result 0 based on 0MaxIter+1.
Of course, we can run post-training after run-
ning Algorithm 1 at each iteration i. However,
since each pass of post-training (e.g. PRO) takes
several hours because of multiple decoding times,
we run it only once, at the end of the iterations
instead.
</bodyText>
<sectionHeader confidence="0.997281" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.963111">
5.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999989567567568">
We conduct our experiments on the Chinese-to-
English and Japanese-to-English translation tasks.
For the Chinese-to-English task, the training data
is the FBIS corpus (news domain) with about
240k sentence pairs; the development set is the
NIST02 evaluation data; the development test set
is NIST05; and the test datasets are NIST06, and
NIST08. For the Japanese-to-English task, the
training data with 300k sentence pairs is from the
NTCIR-patent task (Fujii et al., 2010); the devel-
opment set, development test set, and two test sets
are averagely extracted from a given development
set with 4000 sentences, and these four datasets
are called test1, test2, test3 and test4, respectively.
We run GIZA++ (Och and Ney, 2000) on the
training corpus in both directions (Koehn et al.,
2003) to obtain the word alignment for each sen-
tence pair. Using the SRILM Toolkits (Stolcke,
2002) with modified Kneser-Ney smoothing, we
train a 4-gram language model for the Chinese-to-
English task on the Xinhua portion of the English
Gigaword corpus and a 4-gram language model
for the Japanese-to-English task on the target side
of its training data. In our experiments, the transla-
tion performances are measured by case-sensitive
BLEU4 metric4 (Papineni et al., 2002). The sig-
nificance testing is performed by paired bootstrap
re-sampling (Koehn, 2004b).
We use an in-house developed hierarchical
phrase-based translation (Chiang, 2005) for our
baseline system, which shares the similar setting
as Hiero (Chiang, 2005), e.g. beam-size=100, k-
best-size=100, and is denoted as L-Hiero to em-
phasize its log-linear model. We tune L-Hiero
with two methods MERT and PRO implemented
in the Moses toolkit. On the same experiment set-
tings, the performance of L-Hiero is comparable
</bodyText>
<footnote confidence="0.989768">
4We use mteval-v13a.pl as the evaluation tool(Ref.
http://www.itl.nist.gov/iad/mig/tests/mt/2008/scoring.html).
</footnote>
<page confidence="0.995187">
796
</page>
<figure confidence="0.710496333333333">
Seconds/Sent
L-Hiero 1.77
AdNN-Hiero-E 1.88
</figure>
<tableCaption confidence="0.9509495">
Table 1: The decoding time comparison on
NIST05 between L-Hiero and AdNN-Hiero-E.
</tableCaption>
<bodyText confidence="0.995346333333333">
to that of Moses: on the NIST05 test set, L-Hiero
achieves 25.1 BLEU scores and Moses achieves
24.8. Further, we integrate the embedding fea-
tures (See Section 3.3) into the log-linear model
along with the default features as L-Hiero, which
is called L-Hiero-E. Since L-Hiero-E has hun-
dreds of features, we use PRO as its tuning toolkit.
AdNN-Hiero-E is our implementation of the
AddNN model with embedding features, as dis-
cussed in Section 3, and it shares the same
codebase and settings as L-Hiero. We adopt
the following setting for training AdNN-Hiero-
E: u=10; batch-size=1000 and CGiter=3, as re-
ferred in (Le et al., 2011), and T=200 in Algo-
rithm 1; the pre-training and post-training meth-
ods as PRO; the regularizer A in Eq. (6) as 10
and 30, and MaxIter as 16 and 20 in Algorithm
2, for Chinese-to-English and Japanese-to-English
tasks, respectively. Although there are several pa-
rameters in AdNN which may limit its practica-
bility, according to many of our internal studies,
most parameters are insensitive to AdNN except
A and MaxIter, which are common in other tun-
ing toolkits such as MIRA and can be tuned5 on a
development test dataset.
Since both MERT and PRO tuning toolkits in-
volve randomness in their implementations, all
BLEU scores reported in the experiments are the
average of five tuning runs, as suggested by Clark
et al. (2011) for fairer comparisons. For AdNN,
we report the averaged scores of five post-training
runs, but both pre-training and training are per-
formed only once.
</bodyText>
<subsectionHeader confidence="0.980492">
5.2 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999985">
As discussed in Section 3, our AdNN-Hiero-E
shares the same decoding strategy and pruning
method as L-Hiero. When compared with L-
Hiero, decoding for AdNN-Hiero-E only needs
additional computational times for the features in
the hidden units, i.e. Q(M · h&apos;(r) + B). Since
</bodyText>
<footnote confidence="0.989657">
5For easier tuning, we tuned these two parameters on a
given development test set without post-training in Algorithm
2.
</footnote>
<table confidence="0.9882235">
Chinese-to-English
NIST05 NIST06 NIST08
L-Hiero MERT 25.10+ 24.46+ 17.42+
PRO 25.57+ 25.27+ 18.33+
L-Hiero-E PRO 24.80+ 24.46+ 18.20+
AdNN-Hiero-E 26.37 25.93 19.42
Japanese-to-English
test2 test3 test4
L-Hiero MERT 24.35+ 25.62+ 23.68+
PRO 24.38+ 25.55+ 23.66+
L-Hiero-E PRO 24.47+ 25.86+ 24.03+
AdNN-Hiero-E 25.14 26.32 24.45
</table>
<tableCaption confidence="0.8261575">
Table 2: The BLEU comparisons between AdNN-
Hiero-E and Log-linear translation models on
</tableCaption>
<bodyText confidence="0.96790775">
the Chinese-to-English and Japanese-to-English
tasks. + means the comparison is significant over
AdNN-Hiero-E with p &lt; 0.05.
these features are not dependent on the transla-
tion states, they are computed and saved to mem-
ory when loading the translation model. During
decoding, we just look up these scores instead
of re-calculating them on the fly. Therefore, the
decoding efficiency of AdNN-Hiero-E is almost
the same as that of L-Hiero. As shown in Table
1 the average decoding time for L-Hiero is 1.77
seconds/sentence while that for AdNN-Hiero-E is
1.88 seconds/sentence on the NIST05 test set.
Word embedding features can improve the per-
formance on other NLP tasks (Turian et al., 2010),
but its effect on log-linear based SMT is not as ex-
pected. As shown in Table 2, L-Hiero-E gains lit-
tle over L-Hiero for the Japanese-to-English task,
and even decreases the performance over L-Hiero
for the Chinese-to-English task. These results fur-
ther prove our claim in Section 1, i.e. the log-
linear model requires the features to be linear with
the model and thus limits its expressive abilities.
However, after the single-layer non-linear opera-
tor (sigmoid functions) on the embedding features
for deep interpretation and representation, AdNN-
Hiero-E gains improvements over both L-Hiero
and L-Hiero-E, as depicted in Table 2. In detail,
for the Chinese-to-English task, AdNN-Hiero-E
improves more than 0.6 BLEU scores over L-
Hiero on both test sets: the gains over L-Hiero
tuned with PRO are 0.66 and 1.09 on NIST06 and
NIST08, respectively, and the gains over L-Hiero
tuned with MERT are even more. Similar re-
sults are achieved on the Japanese-to-English task.
AdNN-Hiero-E gains about 0.7 BLEU scores on
</bodyText>
<page confidence="0.993222">
797
</page>
<table confidence="0.9702743">
Chinese-to-English
NIST05 NIST06 NIST08
L-Hiero -E 25.57+ 25.27+ 18.33+
AdNN-Hiero 26.37 25.93 19.42
AdNN-Hiero- D 26.21 26.07 19.54
Japanese-to-English
test3 test4
25.55 23.66
26.32+ 24.45+
25.46 23.73
</table>
<tableCaption confidence="0.988022">
Table 3: The effect of different feature setting on
AdNN model. + means the comparison is signifi-
cant over AdNN-Hiero-D with p &lt; 0.05.
</tableCaption>
<bodyText confidence="0.991233103448276">
both test sets.
In addition, to investigate the effect of differ-
ent feature settings on AdNN, we alternatively de-
sign another setting for h&apos; in Eq. (5): we use
the default features for both h&apos; and h. In partic-
ular, the language model of a rule for h&apos; is lo-
cally calculated without the contexts out of the
rule as described in (Chiang, 2007). We call the
AdNN model with this setting AdNN-Hiero-D6.
Although there are serious overlaps between h and
h&apos; for AdNN-Hiero-D which may limit its gener-
alization abilities, as shown in Table 3, it is still
comparable to L-Hiero on the Japanese-to-English
task, and significantly outperforms L-Hiero on the
Chinese-to-English translation task. To investigate
the reason why the gains for AdNN-Hiero-D on
the two different translation tasks differ, we cal-
culate the perplexities between the target side of
training data and test datasets on both translation
tasks. We find that the perplexity of the 4-gram
language model for the Chinese-to-English task is
321.73, but that for the Japanese-to-English task
is only 81.48. Based on these similarity statistics,
we conjecture that the log-linear model does not
fit well for difficult translation tasks (e.g. transla-
tion task on the news domain). The problem seems
to be resolved by simply alternating feature repre-
sentations through non-linear models, i.e. AddN-
Hiero-D, even with single-layer networks.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.969584">
Neural networks have achieved widespread at-
tentions in many NLP tasks, e.g. the language
</bodyText>
<footnote confidence="0.492864666666667">
6All its parameters are shared with AdNN-Hiero-E except
A and MaxIter, which are tuned on the development test
datasets.
</footnote>
<bodyText confidence="0.999781191489362">
model (Bengio et al., 2003); POS, Chunking,
NER, and SRL (Collobert and Weston, 2008);
Parsing (Collobert and Weston, 2008; Socher et
al., 2011); and Machine transliteration (Deselaers
et al., 2009). Our work is, of course, highly mo-
tivated by these works. Unlike these works, we
propose a variant neural network, i.e. additive neu-
ral networks, starting from SMT itself and taking
both of the model definition and its inference (de-
coding) together into account.
Our variant of neural network, AdNN, is highly
related to both additive models (Buja et al., 1989)
and generalized additive neural networks (Potts,
1999; Waal and Toit, 2007), in which an additive
term is either a linear model or a neural network.
Unlike additive models and generalized additive
neural networks, our model is decomposable with
respect to translation rules rather than its compo-
nent variables considering the decoding efficiency
of machine translation; and it allows its additive
terms of neural networks to share the same param-
eters for a compact structure to avoid sparsity.
The idea of the neural network in machine
translation has already been pioneered in previ-
ous works. Casta˜no et al. (1997) introduced a neu-
ral network for example-based machine transla-
tion. In particular, Son et al. (2012) and Schwenk
(2012) employed a neural network to model the
phrase translation probability on the rule level
(α, y) instead of the bilingual sentence level (f, e)
as in Eq. (5), and thus they did not go beyond the
log-linear model for SMT.
There are also works which exploit non-linear
models in SMT. Duh and Kirchhoff (2008) pro-
posed a boosting re-ranking algorithm using
MERT as a week learner to improve the model’s
expressive abilities; Sokolov et al. (2012) simi-
larly proposed a boosting re-ranking method from
the ranking perspective rather than the classifica-
tion perspective. Instead of considering the re-
ranking task in SMT, Xiao et al. (2010) employed
a boosting method for the system combination in
SMT. Unlike their post-processing models (either
a re-ranking or a system combination model) in
SMT, we propose a non-linear translation model
which can be easily incorporated into the existing
SMT framework.
</bodyText>
<sectionHeader confidence="0.99707" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9987725">
In this paper, we go beyond the log-linear model
for SMT and propose a novel AdNN based trans-
</bodyText>
<figure confidence="0.991125666666667">
test2
24.38
25.14+
AdNN-Hiero- D 24.42
L-Hiero
AdNN-Hiero -E
</figure>
<page confidence="0.990103">
798
</page>
<bodyText confidence="0.999950347826087">
lation model. Our model overcomes some of the
shortcomings suffered by the log-linear model:
linearity and the lack of deep interpretation and
representation in features. One advantage of our
model is that it jointly learns features and tunes
the translation model and thus learns features to-
wards the translation evaluation metric. Addi-
tionally, the decoding of our model is as efficient
as that of the log-linear model. For Chinese-to-
English and Japanese-to-English translation tasks,
our model significantly outperforms the log-linear
model, with the help of word embedding.
We plan to explore more work on the additive
neural networks in the future. For example, we
will train word embedding matrices for source and
target languages from a larger corpus, and take
into consideration the bilingual information, for
instance, word alignment; the multi-layer neural
network within the additive neural networks will
be also investigated in addition to the single-layer
neural network; and we will test our method on
other translation tasks with larger training data as
well.
</bodyText>
<sectionHeader confidence="0.998384" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999973923076923">
We would like to thank our colleagues in both
HIT and NICT for insightful discussions, Tomas
Mikolov for the helpful discussion about the word
embedding in RNNLM, and three anonymous re-
viewers for many invaluable comments and sug-
gestions to improve our paper. This work is
supported by National Natural Science Founda-
tion of China (61173073, 61100093, 61073130,
61272384), the Key Project of the National
High Technology Research and Development Pro-
gram of China (2011AA01A207), and the Fun-
damental Research Funds for Central Universities
(HIT.NSRIF.2013065).
</bodyText>
<sectionHeader confidence="0.999" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998127125">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3:1137–1155,
March.
Christopher M. Bishop. 1995. Neural Networks for
Pattern Recognition. Oxford University Press, Inc.,
New York, NY, USA.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19:263–311,
June.
Andreas Buja, Trevor Hastie, and Robert Tibshirani.
1989. Linear smoothers and additive models. The
Annals of Statistics, 17:453–510.
M. Asuncin Casta˜no, Francisco Casacuberta, and En-
rique Vidal. 1997. Machine translation using neu-
ral networks and finite-state models. In TMI, pages
160–167.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. of EMNLP. ACL.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ’05, pages 263–
270, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201–228, June.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: controlling for opti-
mizer instability. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ’11, pages 176–181, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, ICML.
R. Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In AISTATS.
Thomas Deselaers, Saˇsa Hasan, Oliver Bender, and
Hermann Ney. 2009. A deep learning approach
to machine transliteration. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, StatMT ’09, pages 233–241, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Kevin Duh and Katrin Kirchhoff. 2008. Beyond log-
linear models: Boosted minimum error rate training
for n-best re-ranking. In Proceedings of ACL-08:
HLT, Short Papers, pages 37–40, Columbus, Ohio,
June. Association for Computational Linguistics.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2010. Overview of the patent
translation task at the ntcir-8 workshop. In In
Proceedings of the 8th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-lingual Information Access, pages 293–
302.
</reference>
<page confidence="0.993384">
799
</page>
<reference confidence="0.998142254545455">
William W. Hager and Hongchao Zhang. 2006. Algo-
rithm 851: Cg descent, a conjugate gradient method
with guaranteed descent. ACM Trans. Math. Softw.,
32(1):113–137, March.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL. ACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philipp Koehn. 2004a. Pharaoh: A beam search de-
coder for phrase-based statistical machine transla-
tion models. In AMTA.
Philipp Koehn. 2004b. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP.
ACL.
Quoc V. Le, Jiquan Ngiam, Adam Coates, Ahbik
Lahiri, Bobby Prochnow, and Andrew Y. Ng. 2011.
On optimization methods for deep learning. In
ICML, pages 265–272.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock´y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048.
Patrick Nguyen, Milind Mahajan, and Xiaodong He.
2007. Training non-parametric features for statisti-
cal machine translation. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
pages 72–79, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of
the 38th Annual Meeting on Association for Com-
putational Linguistics, ACL ’00, pages 440–447,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 295–302,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
William J. E. Potts. 1999. Generalized additive neural
networks. In Proceedings of the fifth ACM SIGKDD
international conference on Knowledge discovery
and data mining, KDD ’99, pages 194–200, New
York, NY, USA. ACM.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine trans-
lation. In Proceedings of the 24th International
Conference on Computational Linguistics, COLING
’12, Mumbai, India. Association for Computational
Linguistics.
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011. Parsing Natural
Scenes and Natural Language with Recursive Neural
Networks. In Proceedings of the 26th International
Conference on Machine Learning (ICML).
A. Sokolov, G. Wisniewski, and F. Yvon. 2012. Non-
linear n-best list reranking with few features. In
AMTA, San Diego, USA.
Le Hai Son, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, NAACL HLT ’12, pages 39–
48, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 384–394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
D. A. de Waal and J. V. du Toit. 2007. Generalized ad-
ditive models from a neural network perspective. In
Proceedings of the Seventh IEEE International Con-
ference on Data Mining Workshops, ICDMW ’07,
pages 265–270, Washington, DC, USA. IEEE Com-
puter Society.
</reference>
<page confidence="0.963946">
800
</page>
<reference confidence="0.9980972">
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 764–773, Prague, Czech Republic,
June. Association for Computational Linguistics.
Tong Xiao, Jingbo Zhu, Muhua Zhu, and Huizhen
Wang. 2010. Boosting-based system combina-
tion for machine translation. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 739–748,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.998325">
801
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.403533">
<title confidence="0.999848">Additive Neural Networks for Statistical Machine Translation</title>
<author confidence="0.998288">Taro Eiichiro Tiejun</author>
<affiliation confidence="0.999291333333333">of Computer Science and Harbin Institute of Technology (HIT), Harbin, Institute of Information and Communication Technology</affiliation>
<address confidence="0.994637">3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto,</address>
<email confidence="0.6370365">||</email>
<abstract confidence="0.99896472">Most statistical machine translation (SMT) systems are modeled using a loglinear framework. Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interpreted to reach their potential. A neural network is a reasonable method to address these pitfalls. However, modeling SMT with a neural network is not trivial, especially when taking the decoding efficiency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embedding features on Chinese-to-English and Japanese-to-English translation tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1137</pages>
<contexts>
<context position="31132" citStr="Bengio et al., 2003" startWordPosition="5293" endWordPosition="5296">apanese-to-English task is only 81.48. Based on these similarity statistics, we conjecture that the log-linear model does not fit well for difficult translation tasks (e.g. translation task on the news domain). The problem seems to be resolved by simply alternating feature representations through non-linear models, i.e. AddNHiero-D, even with single-layer networks. 6 Related Work Neural networks have achieved widespread attentions in many NLP tasks, e.g. the language 6All its parameters are shared with AdNN-Hiero-E except A and MaxIter, which are tuned on the development test datasets. model (Bengio et al., 2003); POS, Chunking, NER, and SRL (Collobert and Weston, 2008); Parsing (Collobert and Weston, 2008; Socher et al., 2011); and Machine transliteration (Deselaers et al., 2009). Our work is, of course, highly motivated by these works. Unlike these works, we propose a variant neural network, i.e. additive neural networks, starting from SMT itself and taking both of the model definition and its inference (decoding) together into account. Our variant of neural network, AdNN, is highly related to both additive models (Buja et al., 1989) and generalized additive neural networks (Potts, 1999; Waal and To</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Neural Networks for Pattern Recognition.</title>
<date>1995</date>
<publisher>Oxford University Press, Inc.,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2935" citStr="Bishop, 1995" startWordPosition="447" endWordPosition="448">additional features into the model. On the other hand, it cannot deeply interpret its surface features, and thus can not efficiently develop the potential of these features. What may happen is that a feature p does initially not improve the translation performance, but after a nonlinear operation, e.g. log(p), it does. The reason is not because this feature is useless but the model does not efficiently interpret and represent it. Situations such as this confuse explanations for feature designing, since it is unclear whether such a feature contributes to a translation or not. A neural network (Bishop, 1995) is a reasonable method to overcome the above shortcomings. However, it should take constraints, e.g. the decoding efficiency, into account in SMT. Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search (Koehn, 2004a). In the search procedure, frequent computation of the model score is needed for the search heuristic function, which will be challenged by the decoding efficiency for the neural network based translation model. Further, decoding with non-local (or state-dependent) features, such as a language model, is also a problem. Actually</context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>Christopher M. Bishop. 1995. Neural Networks for Pattern Recognition. Oxford University Press, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<contexts>
<context position="6281" citStr="Brown et al., 1993" startWordPosition="998" endWordPosition="1001">s show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1(f, e, d), h2(f, e, d), ··· , hK(f, e, d))&gt; is a K-dimensional feature vector defined on the tuple (f, e, d); W = (w1, w2, ··· , wK)&gt; is a Kdimensional weight vector of h, i.e., the parameters of the model, and it can be tuned by the toolkit MERT (Och, 2003). Different from Brown’s generative model (Brown et al., 1993), the loglinear model does not assume strong independency holds, and allows arbitrary features to be integrated into the model easily. In other words, it can transform complex language translation into feature engineering: it can achieve high translation performance if reasonable features are chosen and appropriate parameters are assigned for the weight vector. 2.2 Decoding By Search Given a source sentence f and a weight W, decoding finds the best translation candidate eˆ via the programming problem: (ˆe, ˆd) = arg max P(e,d|f;W) e,d = arg max {W&gt; · h(f, e, d)}. (2) e,d 2 Log-linear Model, Re</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Comput. Linguist., 19:263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Buja</author>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
</authors>
<title>Linear smoothers and additive models. The Annals of Statistics,</title>
<date>1989</date>
<pages>17--453</pages>
<contexts>
<context position="12560" citStr="Buja et al., 1989" startWordPosition="2131" endWordPosition="2134"> h&apos; are feature vectors with dimension K and K&apos; respectively, and each component of h&apos; is a local feature which can be defined on a rule r : X -+ (α, -y); 0 = (W, W&apos;, M, B) is the model parameters with M E R&apos;&amp;quot;K&apos;. In this paper, we focus on a single-layer neural network for its simplicity, and one can similarly define σ as a multilayer neural network. Again for the example shown in Figure 1, the model score defined in Eq. (5) for the pair (e2, d2) can be represented as follows: S(f, e2, d2; 0) = WT · h(f, e2, d2)+ W&apos;T·σ(M·h&apos;(r1)+B)+W&apos;T·σ(M·h&apos;(r2)+B). Eq. (5) is similar to both additive models (Buja et al., 1989) and generalized additive neural networks (Potts, 1999): it consists of many additive terms, and each term is either a linear or a nonlinear (a neural network) model. That is the reason why our model is called “additive neural networks”. Of course, our model still has some differences from both of them. Firstly, our model is decomposable with respect to rules instead of the component variables. Secondly, some of its additive terms share the same parameters (M, B). There are also strong relationships between AdNN and the log-linear model. If we consider the parameters (M, B) as constant and σ(M</context>
<context position="31665" citStr="Buja et al., 1989" startWordPosition="5379" endWordPosition="5382">axIter, which are tuned on the development test datasets. model (Bengio et al., 2003); POS, Chunking, NER, and SRL (Collobert and Weston, 2008); Parsing (Collobert and Weston, 2008; Socher et al., 2011); and Machine transliteration (Deselaers et al., 2009). Our work is, of course, highly motivated by these works. Unlike these works, we propose a variant neural network, i.e. additive neural networks, starting from SMT itself and taking both of the model definition and its inference (decoding) together into account. Our variant of neural network, AdNN, is highly related to both additive models (Buja et al., 1989) and generalized additive neural networks (Potts, 1999; Waal and Toit, 2007), in which an additive term is either a linear model or a neural network. Unlike additive models and generalized additive neural networks, our model is decomposable with respect to translation rules rather than its component variables considering the decoding efficiency of machine translation; and it allows its additive terms of neural networks to share the same parameters for a compact structure to avoid sparsity. The idea of the neural network in machine translation has already been pioneered in previous works. Casta</context>
</contexts>
<marker>Buja, Hastie, Tibshirani, 1989</marker>
<rawString>Andreas Buja, Trevor Hastie, and Robert Tibshirani. 1989. Linear smoothers and additive models. The Annals of Statistics, 17:453–510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Asuncin Casta˜no</author>
<author>Francisco Casacuberta</author>
<author>Enrique Vidal</author>
</authors>
<title>Machine translation using neural networks and finite-state models.</title>
<date>1997</date>
<booktitle>In TMI,</booktitle>
<pages>160--167</pages>
<marker>Casta˜no, Casacuberta, Vidal, 1997</marker>
<rawString>M. Asuncin Casta˜no, Francisco Casacuberta, and Enrique Vidal. 1997. Machine translation using neural networks and finite-state models. In TMI, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="17108" citStr="Chiang et al., 2008" startWordPosition="2944" endWordPosition="2947">ension for h&apos; is K&apos; = 30 × (5 + 12) = 510. 794 sponding decoding formula: (ˆe, ˆd) = arg max 5(f, e, d; 0). e,d Given the model parameter 0 = (W, W&apos;, M, B), if we consider (M, B) as constant and Q(M ·h&apos;(r)+ B) as an additional feature vector besides h, then Eq. (5) goes back to being a log-linear model with parameter (W, W&apos;). In this way, the decoding for AdNN can share the same search strategy and cube pruning method as the log-linear model. 4 Training Method 4.1 Training Objective For the log-linear model, there are various tuning methods, e.g. MERT (Och, 2003), MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011) and so on, which iteratively optimize a weight such that, after re-ranking a k-best list of a given development set with this weight, the loss of the resulting 1-best list is minimal. In the extreme, if the k-best list consists only of a pair of translations ((e*, d*), (e&apos;, d&apos;)), the desirable weight should satisfy the assertion: if the BLEU score of e* is greater than that of e&apos;, then the model score of (e*, d*) with this weight will be also greater than that of (e&apos;, d&apos;). In this paper, a pair (e*, e&apos;) for a source sentence f is called as a preference pair for f.</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proc. of EMNLP. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5896" citStr="Chiang, 2005" startWordPosition="921" endWordPosition="922">ning; we also propose pre-training and post-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1(f, e, d), h2(f, e, d), ··· , hK(f, e, d))&gt; is a K-dimensional feature vector defined on the tuple (f, e, d); W = (w1, w2, ··· , wK)&gt; is a Kdimensional weight vector of h, i.e., the parameters of the model, and it can be tuned by the toolkit MERT (Och, 2003). Different from Brown’s generative model (Brown et al., 1993), the loglinear model does not assume strong independency holds, and allows arbitrary features to be integrated into the model easily. In other words, it can transform complex language translation into feature engin</context>
<context position="13973" citStr="Chiang, 2005" startWordPosition="2377" endWordPosition="2378">ne the weight (W, W&apos;) of the log-linear model together. That is different from most works under the log-linear translation framework, which firstly learn features or sub-models and then tune the log-linear model including the learned features in two separate steps. By joint training, AdNN can learn the features towards the translation evaluation metric, which is the main advantage of our model over the log-linear model. In this paper, we apply our AdNN model to hierarchical phrase based translation, and it can be similarly applied to phrase-based or syntax-based translation. Similar to Hiero (Chiang, 2005), the feature vector h in Eq. (5) includes 8 default features, which consist of translation probabilities, lexical translation probabilities, word penalty, glue rule penalty, synchronous rule penalty and language model. These default features are included because they empirically perform well in the loglinear model. For the local feature vector h&apos; in Eq (5), we employ word embedding features as described in the following subsection. 3.3 Word Embedding features for AdNN Word embedding can relax the sparsity introduced by the lexicalization in NLP, and it improves the systems for many tasks such</context>
<context position="24538" citStr="Chiang, 2005" startWordPosition="4247" endWordPosition="4248">gnment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004b). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) for our baseline system, which shares the similar setting as Hiero (Chiang, 2005), e.g. beam-size=100, kbest-size=100, and is denoted as L-Hiero to emphasize its log-linear model. We tune L-Hiero with two methods MERT and PRO implemented in the Moses toolkit. On the same experiment settings, the performance of L-Hiero is comparable 4We use mteval-v13a.pl as the evaluation tool(Ref. http://www.itl.nist.gov/iad/mig/tests/mt/2008/scoring.html). 796 Seconds/Sent L-Hiero 1.77 AdNN-Hiero-E 1.88 Table 1: The decoding time comparison on NIST05 between L-Hiero and AdNN-Hiero-E. to that of Moses: on th</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 263– 270, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="3643" citStr="Chiang, 2007" startWordPosition="563" endWordPosition="564">, e.g. the decoding efficiency, into account in SMT. Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search (Koehn, 2004a). In the search procedure, frequent computation of the model score is needed for the search heuristic function, which will be challenged by the decoding efficiency for the neural network based translation model. Further, decoding with non-local (or state-dependent) features, such as a language model, is also a problem. Actually, even for the (log-) linear model, efficient decoding with the language model is not trivial (Chiang, 2007). In this paper, we propose a variant of neural networks, i.e. additive neural networks (see Section 3 for details), for SMT. It consists of two components: a linear component which captures nonlocal (or state dependent) features and a non-linear component (i.e., neural nework) which encodes lo791 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 791–801, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ✚  X ❐ ✟❭ X X X over the last years friendly cooperation Figure 1: A bilingual tree with two synchronous rules, r</context>
<context position="8095" citStr="Chiang (2007)" startWordPosition="1316" endWordPosition="1317">ial translation together with its derivation, e.g. (e1, d1); it consequently expands the states from the initial (empty) state to the end state (e2, d2) according to the translation rules r1 and r2. During the state expansion process, the score wz · (f, e), i.e., 792 hi(f, e, d) for a partial translation is calculated repeatedly. In the log-linear model, if hi(f, e, d) is a local feature, the calculation of its score wi · hi(f, e, d) has a substructure, and thus it can be calculated with dynamic programming which accelerates its decoding. For the non-local features such as the language model, Chiang (2007) proposed a cube-pruning method for efficient decoding. The main reason why cube-pruning works is that the translation model is linear and the model score for the language model is approximately monotonic (Chiang, 2007). 3 Additive Neural Networks 3.1 Motivation Although the log-linear model has achieved great progress for SMT, it still suffers from some pitfalls: it requires features be linear with the model and it can not interpret and represent features deeply. The neural network model is a reasonable method to overcome these pitfalls. However, the neural network based machine translation i</context>
<context position="10946" citStr="Chiang (2007)" startWordPosition="1840" endWordPosition="1841">) by a function F. For simplicity, we suppose that the additive property holds in F, and then we can obtain a new translation model via the following recursive equation: S(f,e2,d2;W,M,B) = S(f, e1, d1; W,M,B) + S(h(r2); W, M, B). (4) Since the above model is defined only on local features, it ignores the contributions from nonlocal features. Actually, existing works empirically show that some non-local features, especially language model, contribute greatly to machine translation. Scoring for non-local features such as a ngram language model is not easily done. In loglinear translation model, Chiang (2007) proposed a cube-pruning method for scoring the language model. The premise of cube-pruning is that the language model score is approximately monotonic (Chiang, 2007). However, if scoring the language model with a neural network, this premise is difficult to hold. Therefore, one of the solutions is to preserve a linear model for scoring the language model directly. 3.2 Definition According to the above analysis, we propose a variant of a neural network model for machine translation, and we call it Additive Neural Networks or AdNN for short. The AdNN model is a combination of a linear model and</context>
<context position="29830" citStr="Chiang, 2007" startWordPosition="5094" endWordPosition="5095">18.33+ AdNN-Hiero 26.37 25.93 19.42 AdNN-Hiero- D 26.21 26.07 19.54 Japanese-to-English test3 test4 25.55 23.66 26.32+ 24.45+ 25.46 23.73 Table 3: The effect of different feature setting on AdNN model. + means the comparison is significant over AdNN-Hiero-D with p &lt; 0.05. both test sets. In addition, to investigate the effect of different feature settings on AdNN, we alternatively design another setting for h&apos; in Eq. (5): we use the default features for both h&apos; and h. In particular, the language model of a rule for h&apos; is locally calculated without the contexts out of the rule as described in (Chiang, 2007). We call the AdNN model with this setting AdNN-Hiero-D6. Although there are serious overlaps between h and h&apos; for AdNN-Hiero-D which may limit its generalization abilities, as shown in Table 3, it is still comparable to L-Hiero on the Japanese-to-English task, and significantly outperforms L-Hiero on the Chinese-to-English translation task. To investigate the reason why the gains for AdNN-Hiero-D on the two different translation tasks differ, we calculate the perplexities between the target side of training data and test datasets on both translation tasks. We find that the perplexity of the 4</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33(2):201–228, June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>176--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="26463" citStr="Clark et al. (2011)" startWordPosition="4554" endWordPosition="4557">(6) as 10 and 30, and MaxIter as 16 and 20 in Algorithm 2, for Chinese-to-English and Japanese-to-English tasks, respectively. Although there are several parameters in AdNN which may limit its practicability, according to many of our internal studies, most parameters are insensitive to AdNN except A and MaxIter, which are common in other tuning toolkits such as MIRA and can be tuned5 on a development test dataset. Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al. (2011) for fairer comparisons. For AdNN, we report the averaged scores of five post-training runs, but both pre-training and training are performed only once. 5.2 Results and Analysis As discussed in Section 3, our AdNN-Hiero-E shares the same decoding strategy and pruning method as L-Hiero. When compared with LHiero, decoding for AdNN-Hiero-E only needs additional computational times for the features in the hidden units, i.e. Q(M · h&apos;(r) + B). Since 5For easier tuning, we tuned these two parameters on a given development test set without post-training in Algorithm 2. Chinese-to-English NIST05 NIST0</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 176–181, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning, ICML.</booktitle>
<contexts>
<context position="5022" citStr="Collobert and Weston, 2008" startWordPosition="787" endWordPosition="790">h the partial translation e1 =“friendly cooperation”, and the outside rectangle denotes the derivation d2 = Ir1, r2} with the translation e2=“friendly cooperation over the last years”. cal (or state independent) features. Compared with the log-linear model, it has more powerful expressive abilities and can deeply interpret and represent features with hidden units in neural networks. Moreover, our method is simple to implement and its decoding efficiency is comparable to that of the log-linear model. We also integrate word embedding into the model by representing each word as a feature vector (Collobert and Weston, 2008). Because of the thousands of parameters and the non-convex objective in our model, efficient training is not simple. We propose an efficient training methodology: we apply the mini-batch conjugate sub-gradient algorithm (Le et al., 2011) to accelerate the training; we also propose pre-training and post-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEngl</context>
<context position="14658" citStr="Collobert and Weston, 2008" startWordPosition="2482" endWordPosition="2485">es, which consist of translation probabilities, lexical translation probabilities, word penalty, glue rule penalty, synchronous rule penalty and language model. These default features are included because they empirically perform well in the loglinear model. For the local feature vector h&apos; in Eq (5), we employ word embedding features as described in the following subsection. 3.3 Word Embedding features for AdNN Word embedding can relax the sparsity introduced by the lexicalization in NLP, and it improves the systems for many tasks such as language model, named entity recognition, and parsing (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011). Here, we propose embedding features for rules in SMT by combining word embeddings. Firstly, we will define the embedding for the source side α of a rule r : X -+ (α, -y). Let VS be the vocabulary in the source language with size VS ; R&amp;quot;|VS |be the word embedding matrix, each column of which is the word embedding (ndimensional vector) for the corresponding word in VS; and maxSource be the maximal length of α for all rules. We further assume that the α for all rules share the same length as maxSource; otherwise, we add maxSource − α words “NULL” to the en</context>
<context position="16173" citStr="Collobert and Weston, 2008" startWordPosition="2760" endWordPosition="2763">milarly define the embedding for the target side of a rule, given an embedding matrix for the target vocabulary. Finally, we define the embedding of a rule as the concatenation of the embedding of its source and target sides. In this paper, we apply the word embedding matrices from the RNNLM toolkit (Mikolov et al., 2010) with the default settings: we train two RNN language models on the source and target sides of training corpus, respectively, and then we obtain two matrices as their by-products1. It would be potentially better to train the word embedding matrix from a much larger corpus as (Collobert and Weston, 2008), and we will leave this as a future task. 3.4 Decoding Substituting the P(e, d f; W) in Eq. (2) with S(f, e, d; 0) in Eq. (5), we can obtain its corre1In the RNNLM toolkit, the default dimension for word embedding is n = 30. In our experiments, the maximal length of α and γ are 5 and 12 respectively. Thus the dimension for h&apos; is K&apos; = 30 × (5 + 12) = 510. 794 sponding decoding formula: (ˆe, ˆd) = arg max 5(f, e, d; 0). e,d Given the model parameter 0 = (W, W&apos;, M, B), if we consider (M, B) as constant and Q(M ·h&apos;(r)+ B) as an additional feature vector besides h, then Eq. (5) goes back to being </context>
<context position="31190" citStr="Collobert and Weston, 2008" startWordPosition="5302" endWordPosition="5305">se similarity statistics, we conjecture that the log-linear model does not fit well for difficult translation tasks (e.g. translation task on the news domain). The problem seems to be resolved by simply alternating feature representations through non-linear models, i.e. AddNHiero-D, even with single-layer networks. 6 Related Work Neural networks have achieved widespread attentions in many NLP tasks, e.g. the language 6All its parameters are shared with AdNN-Hiero-E except A and MaxIter, which are tuned on the development test datasets. model (Bengio et al., 2003); POS, Chunking, NER, and SRL (Collobert and Weston, 2008); Parsing (Collobert and Weston, 2008; Socher et al., 2011); and Machine transliteration (Deselaers et al., 2009). Our work is, of course, highly motivated by these works. Unlike these works, we propose a variant neural network, i.e. additive neural networks, starting from SMT itself and taking both of the model definition and its inference (decoding) together into account. Our variant of neural network, AdNN, is highly related to both additive models (Buja et al., 1989) and generalized additive neural networks (Potts, 1999; Waal and Toit, 2007), in which an additive term is either a linear mo</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning, ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>In AISTATS.</booktitle>
<contexts>
<context position="14697" citStr="Collobert, 2011" startWordPosition="2490" endWordPosition="2491">exical translation probabilities, word penalty, glue rule penalty, synchronous rule penalty and language model. These default features are included because they empirically perform well in the loglinear model. For the local feature vector h&apos; in Eq (5), we employ word embedding features as described in the following subsection. 3.3 Word Embedding features for AdNN Word embedding can relax the sparsity introduced by the lexicalization in NLP, and it improves the systems for many tasks such as language model, named entity recognition, and parsing (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011). Here, we propose embedding features for rules in SMT by combining word embeddings. Firstly, we will define the embedding for the source side α of a rule r : X -+ (α, -y). Let VS be the vocabulary in the source language with size VS ; R&amp;quot;|VS |be the word embedding matrix, each column of which is the word embedding (ndimensional vector) for the corresponding word in VS; and maxSource be the maximal length of α for all rules. We further assume that the α for all rules share the same length as maxSource; otherwise, we add maxSource − α words “NULL” to the end of α to obtain a new α. We define the</context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>R. Collobert. 2011. Deep learning for efficient discriminative parsing. In AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Deselaers</author>
<author>Saˇsa Hasan</author>
<author>Oliver Bender</author>
<author>Hermann Ney</author>
</authors>
<title>A deep learning approach to machine transliteration.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09,</booktitle>
<pages>233--241</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="31303" citStr="Deselaers et al., 2009" startWordPosition="5318" endWordPosition="5321">e.g. translation task on the news domain). The problem seems to be resolved by simply alternating feature representations through non-linear models, i.e. AddNHiero-D, even with single-layer networks. 6 Related Work Neural networks have achieved widespread attentions in many NLP tasks, e.g. the language 6All its parameters are shared with AdNN-Hiero-E except A and MaxIter, which are tuned on the development test datasets. model (Bengio et al., 2003); POS, Chunking, NER, and SRL (Collobert and Weston, 2008); Parsing (Collobert and Weston, 2008; Socher et al., 2011); and Machine transliteration (Deselaers et al., 2009). Our work is, of course, highly motivated by these works. Unlike these works, we propose a variant neural network, i.e. additive neural networks, starting from SMT itself and taking both of the model definition and its inference (decoding) together into account. Our variant of neural network, AdNN, is highly related to both additive models (Buja et al., 1989) and generalized additive neural networks (Potts, 1999; Waal and Toit, 2007), in which an additive term is either a linear model or a neural network. Unlike additive models and generalized additive neural networks, our model is decomposab</context>
</contexts>
<marker>Deselaers, Hasan, Bender, Ney, 2009</marker>
<rawString>Thomas Deselaers, Saˇsa Hasan, Oliver Bender, and Hermann Ney. 2009. A deep learning approach to machine transliteration. In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09, pages 233–241, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Beyond loglinear models: Boosted minimum error rate training for n-best re-ranking.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>37--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2223" citStr="Duh and Kirchhoff, 2008" startWordPosition="329" endWordPosition="332">dvantage is that arbitrary features can be added to the model. Thus, it casts complex translation between a pair of languages as feature engineering, which facilitates research and development for SMT. Regardless of how successful the log-linear model is in SMT, it still has some shortcomings. This joint work was done while the first author visited NICT. On the one hand, features are required to be linear with respect to the objective of the translation model (Nguyen et al., 2007), but it is not guaranteed that the potential features be linear with the model. This induces modeling inadequacy (Duh and Kirchhoff, 2008), in which the translation performance may not improve, or may even decrease, after one integrates additional features into the model. On the other hand, it cannot deeply interpret its surface features, and thus can not efficiently develop the potential of these features. What may happen is that a feature p does initially not improve the translation performance, but after a nonlinear operation, e.g. log(p), it does. The reason is not because this feature is useless but the model does not efficiently interpret and represent it. Situations such as this confuse explanations for feature designing,</context>
<context position="5551" citStr="Duh and Kirchhoff, 2008" startWordPosition="871" endWordPosition="874">ding into the model by representing each word as a feature vector (Collobert and Weston, 2008). Because of the thousands of parameters and the non-convex objective in our model, efficient training is not simple. We propose an efficient training methodology: we apply the mini-batch conjugate sub-gradient algorithm (Le et al., 2011) to accelerate the training; we also propose pre-training and post-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1(f, e, d), h2(f, e, d), ··· , hK(f, e, d))&gt; is a K-dimensional feature vector defined on the tuple (f, e, d); W = (w1, w2, ··· , wK)&gt; is a Kdimensional weight vector of h, i.e., the paramete</context>
<context position="32706" citStr="Duh and Kirchhoff (2008)" startWordPosition="5550" endWordPosition="5553">works to share the same parameters for a compact structure to avoid sparsity. The idea of the neural network in machine translation has already been pioneered in previous works. Casta˜no et al. (1997) introduced a neural network for example-based machine translation. In particular, Son et al. (2012) and Schwenk (2012) employed a neural network to model the phrase translation probability on the rule level (α, y) instead of the bilingual sentence level (f, e) as in Eq. (5), and thus they did not go beyond the log-linear model for SMT. There are also works which exploit non-linear models in SMT. Duh and Kirchhoff (2008) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model’s expressive abilities; Sokolov et al. (2012) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective. Instead of considering the reranking task in SMT, Xiao et al. (2010) employed a boosting method for the system combination in SMT. Unlike their post-processing models (either a re-ranking or a system combination model) in SMT, we propose a non-linear translation model which can be easily incorporated into the existing SMT framework. 7 Conc</context>
</contexts>
<marker>Duh, Kirchhoff, 2008</marker>
<rawString>Kevin Duh and Katrin Kirchhoff. 2008. Beyond loglinear models: Boosted minimum error rate training for n-best re-ranking. In Proceedings of ACL-08: HLT, Short Papers, pages 37–40, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Masao Utiyama</author>
<author>Mikio Yamamoto</author>
<author>Takehito Utsuro</author>
</authors>
<title>Overview of the patent translation task at the ntcir-8 workshop. In</title>
<date>2010</date>
<booktitle>In Proceedings of the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-lingual Information Access,</booktitle>
<pages>293--302</pages>
<contexts>
<context position="23589" citStr="Fujii et al., 2010" startWordPosition="4097" endWordPosition="4100">s because of multiple decoding times, we run it only once, at the end of the iterations instead. 5 Experiments and Results 5.1 Experimental Setting We conduct our experiments on the Chinese-toEnglish and Japanese-to-English translation tasks. For the Chinese-to-English task, the training data is the FBIS corpus (news domain) with about 240k sentence pairs; the development set is the NIST02 evaluation data; the development test set is NIST05; and the test datasets are NIST06, and NIST08. For the Japanese-to-English task, the training data with 300k sentence pairs is from the NTCIR-patent task (Fujii et al., 2010); the development set, development test set, and two test sets are averagely extracted from a given development set with 4000 sentences, and these four datasets are called test1, test2, test3 and test4, respectively. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Jap</context>
</contexts>
<marker>Fujii, Utiyama, Yamamoto, Utsuro, 2010</marker>
<rawString>Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and Takehito Utsuro. 2010. Overview of the patent translation task at the ntcir-8 workshop. In In Proceedings of the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-lingual Information Access, pages 293– 302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Hager</author>
<author>Hongchao Zhang</author>
</authors>
<title>Algorithm 851: Cg descent, a conjugate gradient method with guaranteed descent.</title>
<date>2006</date>
<journal>ACM Trans. Math. Softw.,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="20360" citStr="Hager and Zhang, 2006" startWordPosition="3516" endWordPosition="3519">. 4.3 Pre-Training and Post-Training Since Eq. (6) is non-linear, there are many local minimal solutions. Actually, this problem is inherent and is one many works based on the neural network for other NLP tasks such as language model and parsing, also suffer from. And these works empirically show that some pre-training methods, which provide a reasonable initial solution, can improve the performance. Observing the structure of Eq. (5) and the relationships between our model and a log-linear model, we propose the following simple pre-training method. 2In implementation, we call the CG toolkit (Hager and Zhang, 2006), which requires overloading objective and subgradient functions. For easier description, we substitute overloading functions and transform the value of functions in the pseudo-code. � S(f, e*, d*, e&apos;, d&apos;; 0), (6) e*,d*,e&apos;,d&apos; 795 If we set W&apos; = 0, the model defined in Eq. (5) can be regarded as a log-linear model with features h. Therefore, we pre-train W using MERT or PRO by holding W&apos; = 0, and use (W, W&apos; = 0, M, B) as an initializer3 for Algorithm 1. Although the above pre-training would provide a reasonable solution, Algorithm 1 may still fall into local minima. We also propose a post-train</context>
</contexts>
<marker>Hager, Zhang, 2006</marker>
<rawString>William W. Hager and Hongchao Zhang. 2006. Algorithm 851: Cg descent, a conjugate gradient method with guaranteed descent. ACM Trans. Math. Softw., 32(1):113–137, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="17137" citStr="Hopkins and May, 2011" startWordPosition="2949" endWordPosition="2952">(5 + 12) = 510. 794 sponding decoding formula: (ˆe, ˆd) = arg max 5(f, e, d; 0). e,d Given the model parameter 0 = (W, W&apos;, M, B), if we consider (M, B) as constant and Q(M ·h&apos;(r)+ B) as an additional feature vector besides h, then Eq. (5) goes back to being a log-linear model with parameter (W, W&apos;). In this way, the decoding for AdNN can share the same search strategy and cube pruning method as the log-linear model. 4 Training Method 4.1 Training Objective For the log-linear model, there are various tuning methods, e.g. MERT (Och, 2003), MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011) and so on, which iteratively optimize a weight such that, after re-ranking a k-best list of a given development set with this weight, the loss of the resulting 1-best list is minimal. In the extreme, if the k-best list consists only of a pair of translations ((e*, d*), (e&apos;, d&apos;)), the desirable weight should satisfy the assertion: if the BLEU score of e* is greater than that of e&apos;, then the model score of (e*, d*) with this weight will be also greater than that of (e&apos;, d&apos;). In this paper, a pair (e*, e&apos;) for a source sentence f is called as a preference pair for f. Following PRO, we define the</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="23902" citStr="Koehn et al., 2003" startWordPosition="4149" endWordPosition="4152">(news domain) with about 240k sentence pairs; the development set is the NIST02 evaluation data; the development test set is NIST05; and the test datasets are NIST06, and NIST08. For the Japanese-to-English task, the training data with 300k sentence pairs is from the NTCIR-patent task (Fujii et al., 2010); the development set, development test set, and two test sets are averagely extracted from a given development set with 4000 sentences, and these four datasets are called test1, test2, test3 and test4, respectively. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004b). We use an in-house developed hierarchical phr</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT-NAACL. ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5943" citStr="Koehn et al., 2007" startWordPosition="928" endWordPosition="931">st-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1(f, e, d), h2(f, e, d), ··· , hK(f, e, d))&gt; is a K-dimensional feature vector defined on the tuple (f, e, d); W = (w1, w2, ··· , wK)&gt; is a Kdimensional weight vector of h, i.e., the parameters of the model, and it can be tuned by the toolkit MERT (Och, 2003). Different from Brown’s generative model (Brown et al., 1993), the loglinear model does not assume strong independency holds, and allows arbitrary features to be integrated into the model easily. In other words, it can transform complex language translation into feature engineering: it can achieve high translation perform</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="3204" citStr="Koehn, 2004" startWordPosition="494" endWordPosition="495">fter a nonlinear operation, e.g. log(p), it does. The reason is not because this feature is useless but the model does not efficiently interpret and represent it. Situations such as this confuse explanations for feature designing, since it is unclear whether such a feature contributes to a translation or not. A neural network (Bishop, 1995) is a reasonable method to overcome the above shortcomings. However, it should take constraints, e.g. the decoding efficiency, into account in SMT. Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search (Koehn, 2004a). In the search procedure, frequent computation of the model score is needed for the search heuristic function, which will be challenged by the decoding efficiency for the neural network based translation model. Further, decoding with non-local (or state-dependent) features, such as a language model, is also a problem. Actually, even for the (log-) linear model, efficient decoding with the language model is not trivial (Chiang, 2007). In this paper, we propose a variant of neural networks, i.e. additive neural networks (see Section 3 for details), for SMT. It consists of two components: a li</context>
<context position="24453" citStr="Koehn, 2004" startWordPosition="4237" endWordPosition="4238"> the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004b). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) for our baseline system, which shares the similar setting as Hiero (Chiang, 2005), e.g. beam-size=100, kbest-size=100, and is denoted as L-Hiero to emphasize its log-linear model. We tune L-Hiero with two methods MERT and PRO implemented in the Moses toolkit. On the same experiment settings, the performance of L-Hiero is comparable 4We use mteval-v13a.pl as the evaluation tool(Ref. http://www.itl.nist.gov/iad/mig/tests/mt/2008/scoring.html). 796 Seconds/Sent L-Hiero 1.77 AdNN-Hiero-E 1.88 Table 1: The decodin</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004a. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="3204" citStr="Koehn, 2004" startWordPosition="494" endWordPosition="495">fter a nonlinear operation, e.g. log(p), it does. The reason is not because this feature is useless but the model does not efficiently interpret and represent it. Situations such as this confuse explanations for feature designing, since it is unclear whether such a feature contributes to a translation or not. A neural network (Bishop, 1995) is a reasonable method to overcome the above shortcomings. However, it should take constraints, e.g. the decoding efficiency, into account in SMT. Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search (Koehn, 2004a). In the search procedure, frequent computation of the model score is needed for the search heuristic function, which will be challenged by the decoding efficiency for the neural network based translation model. Further, decoding with non-local (or state-dependent) features, such as a language model, is also a problem. Actually, even for the (log-) linear model, efficient decoding with the language model is not trivial (Chiang, 2007). In this paper, we propose a variant of neural networks, i.e. additive neural networks (see Section 3 for details), for SMT. It consists of two components: a li</context>
<context position="24453" citStr="Koehn, 2004" startWordPosition="4237" endWordPosition="4238"> the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004b). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) for our baseline system, which shares the similar setting as Hiero (Chiang, 2005), e.g. beam-size=100, kbest-size=100, and is denoted as L-Hiero to emphasize its log-linear model. We tune L-Hiero with two methods MERT and PRO implemented in the Moses toolkit. On the same experiment settings, the performance of L-Hiero is comparable 4We use mteval-v13a.pl as the evaluation tool(Ref. http://www.itl.nist.gov/iad/mig/tests/mt/2008/scoring.html). 796 Seconds/Sent L-Hiero 1.77 AdNN-Hiero-E 1.88 Table 1: The decodin</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004b. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le</author>
<author>Jiquan Ngiam</author>
<author>Adam Coates</author>
<author>Ahbik Lahiri</author>
<author>Bobby Prochnow</author>
<author>Andrew Y Ng</author>
</authors>
<title>On optimization methods for deep learning.</title>
<date>2011</date>
<booktitle>In ICML,</booktitle>
<pages>265--272</pages>
<contexts>
<context position="5260" citStr="Le et al., 2011" startWordPosition="825" endWordPosition="828">r model, it has more powerful expressive abilities and can deeply interpret and represent features with hidden units in neural networks. Moreover, our method is simple to implement and its decoding efficiency is comparable to that of the log-linear model. We also integrate word embedding into the model by representing each word as a feature vector (Collobert and Weston, 2008). Because of the thousands of parameters and the non-convex objective in our model, efficient training is not simple. We propose an efficient training methodology: we apply the mini-batch conjugate sub-gradient algorithm (Le et al., 2011) to accelerate the training; we also propose pre-training and post-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. a collection of synchronous ru</context>
<context position="18298" citStr="Le et al. (2011)" startWordPosition="3175" endWordPosition="3178">s a preference pair for f. Following PRO, we define the following objective function under the maxmargin framework to optimize the AdNN model: 2 110112 + A 1 � N f with S(·) = max {5(f, e&apos;, d&apos;; 0) − 5(f, e*, d*; 0) + 1, 01 where f is a source sentence in a given development set, and ((e*, d*), (e&apos;, d&apos;)) is a preference pair for f; N is the number of all preference pairs; A &gt; 0 is a regularizer. 4.2 Optimization Algorithm Since there are thousands of parameters in Eq. (6) and the tuning in SMT will minimize Eq. (6) repeatedly, efficient and scalable optimization methods are required. Following Le et al. (2011), we apply the mini-batch Conjugate Sub-Gradient (mini-batch CSG) method to minimize Eq. (6). Compared with the sub-gradient descent, minibatch CSG has some advantages: (1) it can accelerate the calculation of the sub-gradient since it calculates the sub-gradient on a subset of preference pairs (i.e. mini-batch) instead of all of the preference pairs; (2) it reduces the number of iterations since it employs the conjugate information besides the sub-gradient. Algorithm 1 shows the procedure to minimize Eq. (6). Algorithm 1 Mini-batch conjugate subgradient Input: 01, T, CGIter, batch-size, k-bes</context>
<context position="25740" citStr="Le et al., 2011" startWordPosition="4430" endWordPosition="4433">f Moses: on the NIST05 test set, L-Hiero achieves 25.1 BLEU scores and Moses achieves 24.8. Further, we integrate the embedding features (See Section 3.3) into the log-linear model along with the default features as L-Hiero, which is called L-Hiero-E. Since L-Hiero-E has hundreds of features, we use PRO as its tuning toolkit. AdNN-Hiero-E is our implementation of the AddNN model with embedding features, as discussed in Section 3, and it shares the same codebase and settings as L-Hiero. We adopt the following setting for training AdNN-HieroE: u=10; batch-size=1000 and CGiter=3, as referred in (Le et al., 2011), and T=200 in Algorithm 1; the pre-training and post-training methods as PRO; the regularizer A in Eq. (6) as 10 and 30, and MaxIter as 16 and 20 in Algorithm 2, for Chinese-to-English and Japanese-to-English tasks, respectively. Although there are several parameters in AdNN which may limit its practicability, according to many of our internal studies, most parameters are insensitive to AdNN except A and MaxIter, which are common in other tuning toolkits such as MIRA and can be tuned5 on a development test dataset. Since both MERT and PRO tuning toolkits involve randomness in their implementa</context>
</contexts>
<marker>Le, Ngiam, Coates, Lahiri, Prochnow, Ng, 2011</marker>
<rawString>Quoc V. Le, Jiquan Ngiam, Adam Coates, Ahbik Lahiri, Bobby Prochnow, and Andrew Y. Ng. 2011. On optimization methods for deep learning. In ICML, pages 265–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock´y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock´y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Nguyen</author>
<author>Milind Mahajan</author>
<author>Xiaodong He</author>
</authors>
<title>Training non-parametric features for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>72--79</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2084" citStr="Nguyen et al., 2007" startWordPosition="306" endWordPosition="309">2002) proposed the log-linear model: almost all the stateof-the-art SMT systems are based on the log-linear model. Its most important advantage is that arbitrary features can be added to the model. Thus, it casts complex translation between a pair of languages as feature engineering, which facilitates research and development for SMT. Regardless of how successful the log-linear model is in SMT, it still has some shortcomings. This joint work was done while the first author visited NICT. On the one hand, features are required to be linear with respect to the objective of the translation model (Nguyen et al., 2007), but it is not guaranteed that the potential features be linear with the model. This induces modeling inadequacy (Duh and Kirchhoff, 2008), in which the translation performance may not improve, or may even decrease, after one integrates additional features into the model. On the other hand, it cannot deeply interpret its surface features, and thus can not efficiently develop the potential of these features. What may happen is that a feature p does initially not improve the translation performance, but after a nonlinear operation, e.g. log(p), it does. The reason is not because this feature is</context>
</contexts>
<marker>Nguyen, Mahajan, He, 2007</marker>
<rawString>Patrick Nguyen, Milind Mahajan, and Xiaodong He. 2007. Training non-parametric features for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 72–79, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00,</booktitle>
<pages>440--447</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="23839" citStr="Och and Ney, 2000" startWordPosition="4138" endWordPosition="4141">Chinese-to-English task, the training data is the FBIS corpus (news domain) with about 240k sentence pairs; the development set is the NIST02 evaluation data; the development test set is NIST05; and the test datasets are NIST06, and NIST08. For the Japanese-to-English task, the training data with 300k sentence pairs is from the NTCIR-patent task (Fujii et al., 2010); the development set, development test set, and two test sets are averagely extracted from a given development set with 4000 sentences, and these four datasets are called test1, test2, test3 and test4, respectively. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-samplin</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00, pages 440–447, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>295--302</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1469" citStr="Och and Ney (2002)" startWordPosition="201" endWordPosition="204">a neural network is not trivial, especially when taking the decoding efficiency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embedding features on Chinese-to-English and Japanese-to-English translation tasks. 1 Introduction Recently, great progress has been achieved in SMT, especially since Och and Ney (2002) proposed the log-linear model: almost all the stateof-the-art SMT systems are based on the log-linear model. Its most important advantage is that arbitrary features can be added to the model. Thus, it casts complex translation between a pair of languages as feature engineering, which facilitates research and development for SMT. Regardless of how successful the log-linear model is in SMT, it still has some shortcomings. This joint work was done while the first author visited NICT. On the one hand, features are required to be linear with respect to the objective of the translation model (Nguye</context>
<context position="6940" citStr="Och and Ney (2002)" startWordPosition="1106" endWordPosition="1109">ong independency holds, and allows arbitrary features to be integrated into the model easily. In other words, it can transform complex language translation into feature engineering: it can achieve high translation performance if reasonable features are chosen and appropriate parameters are assigned for the weight vector. 2.2 Decoding By Search Given a source sentence f and a weight W, decoding finds the best translation candidate eˆ via the programming problem: (ˆe, ˆd) = arg max P(e,d|f;W) e,d = arg max {W&gt; · h(f, e, d)}. (2) e,d 2 Log-linear Model, Revisited 2.1 Log-linear Translation Model Och and Ney (2002) proposed the log-linear translation model, which can be formalized as follows: exp {W&gt; · h(f,e, d)} P(e,d|f;W) = (1) Ee,,d, exp {W&gt; · h(f, e0, d0)} where f denotes the source sentence, and e(e0) denotes its translation candidate; d(d0) is a derivation over the pair Since the range of (e, d) is exponential with respect to the size of f, the exact decoding is intractable and an inexact strategy such as beam search is used instead in practice. The idea of search for decoding can be shown in Figure 1: it encodes each search state as a partial translation together with its derivation, e.g. (e1, d1</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 295–302, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="6219" citStr="Och, 2003" startWordPosition="991" endWordPosition="992">panese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1(f, e, d), h2(f, e, d), ··· , hK(f, e, d))&gt; is a K-dimensional feature vector defined on the tuple (f, e, d); W = (w1, w2, ··· , wK)&gt; is a Kdimensional weight vector of h, i.e., the parameters of the model, and it can be tuned by the toolkit MERT (Och, 2003). Different from Brown’s generative model (Brown et al., 1993), the loglinear model does not assume strong independency holds, and allows arbitrary features to be integrated into the model easily. In other words, it can transform complex language translation into feature engineering: it can achieve high translation performance if reasonable features are chosen and appropriate parameters are assigned for the weight vector. 2.2 Decoding By Search Given a source sentence f and a weight W, decoding finds the best translation candidate eˆ via the programming problem: (ˆe, ˆd) = arg max P(e,d|f;W) e</context>
<context position="17057" citStr="Och, 2003" startWordPosition="2937" endWordPosition="2938">γ are 5 and 12 respectively. Thus the dimension for h&apos; is K&apos; = 30 × (5 + 12) = 510. 794 sponding decoding formula: (ˆe, ˆd) = arg max 5(f, e, d; 0). e,d Given the model parameter 0 = (W, W&apos;, M, B), if we consider (M, B) as constant and Q(M ·h&apos;(r)+ B) as an additional feature vector besides h, then Eq. (5) goes back to being a log-linear model with parameter (W, W&apos;). In this way, the decoding for AdNN can share the same search strategy and cube pruning method as the log-linear model. 4 Training Method 4.1 Training Objective For the log-linear model, there are various tuning methods, e.g. MERT (Och, 2003), MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011) and so on, which iteratively optimize a weight such that, after re-ranking a k-best list of a given development set with this weight, the loss of the resulting 1-best list is minimal. In the extreme, if the k-best list consists only of a pair of translations ((e*, d*), (e&apos;, d&apos;)), the desirable weight should satisfy the assertion: if the BLEU score of e* is greater than that of e&apos;, then the model score of (e*, d*) with this weight will be also greater than that of (e&apos;, d&apos;). In this paper, a pair (e*, e&apos;) for a sour</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="24369" citStr="Papineni et al., 2002" startWordPosition="4223" endWordPosition="4226">ts are called test1, test2, test3 and test4, respectively. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004b). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) for our baseline system, which shares the similar setting as Hiero (Chiang, 2005), e.g. beam-size=100, kbest-size=100, and is denoted as L-Hiero to emphasize its log-linear model. We tune L-Hiero with two methods MERT and PRO implemented in the Moses toolkit. On the same experiment settings, the performance of L-Hiero is comparable 4We use mteval-v13a.pl as the evaluation tool(Ref. http://www.itl.nist.gov/iad/mig/tests/mt/2008</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William J E Potts</author>
</authors>
<title>Generalized additive neural networks.</title>
<date>1999</date>
<booktitle>In Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’99,</booktitle>
<pages>194--200</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="12615" citStr="Potts, 1999" startWordPosition="2141" endWordPosition="2142"> and each component of h&apos; is a local feature which can be defined on a rule r : X -+ (α, -y); 0 = (W, W&apos;, M, B) is the model parameters with M E R&apos;&amp;quot;K&apos;. In this paper, we focus on a single-layer neural network for its simplicity, and one can similarly define σ as a multilayer neural network. Again for the example shown in Figure 1, the model score defined in Eq. (5) for the pair (e2, d2) can be represented as follows: S(f, e2, d2; 0) = WT · h(f, e2, d2)+ W&apos;T·σ(M·h&apos;(r1)+B)+W&apos;T·σ(M·h&apos;(r2)+B). Eq. (5) is similar to both additive models (Buja et al., 1989) and generalized additive neural networks (Potts, 1999): it consists of many additive terms, and each term is either a linear or a nonlinear (a neural network) model. That is the reason why our model is called “additive neural networks”. Of course, our model still has some differences from both of them. Firstly, our model is decomposable with respect to rules instead of the component variables. Secondly, some of its additive terms share the same parameters (M, B). There are also strong relationships between AdNN and the log-linear model. If we consider the parameters (M, B) as constant and σ(M · h&apos;(r) + B) as a new feature vector, then AdNN is red</context>
<context position="31719" citStr="Potts, 1999" startWordPosition="5388" endWordPosition="5389">el (Bengio et al., 2003); POS, Chunking, NER, and SRL (Collobert and Weston, 2008); Parsing (Collobert and Weston, 2008; Socher et al., 2011); and Machine transliteration (Deselaers et al., 2009). Our work is, of course, highly motivated by these works. Unlike these works, we propose a variant neural network, i.e. additive neural networks, starting from SMT itself and taking both of the model definition and its inference (decoding) together into account. Our variant of neural network, AdNN, is highly related to both additive models (Buja et al., 1989) and generalized additive neural networks (Potts, 1999; Waal and Toit, 2007), in which an additive term is either a linear model or a neural network. Unlike additive models and generalized additive neural networks, our model is decomposable with respect to translation rules rather than its component variables considering the decoding efficiency of machine translation; and it allows its additive terms of neural networks to share the same parameters for a compact structure to avoid sparsity. The idea of the neural network in machine translation has already been pioneered in previous works. Casta˜no et al. (1997) introduced a neural network for exam</context>
</contexts>
<marker>Potts, 1999</marker>
<rawString>William J. E. Potts. 1999. Generalized additive neural networks. In Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’99, pages 194–200, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space translation models for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics, COLING ’12,</booktitle>
<institution>India. Association for Computational Linguistics.</institution>
<location>Mumbai,</location>
<contexts>
<context position="32401" citStr="Schwenk (2012)" startWordPosition="5498" endWordPosition="5499"> model or a neural network. Unlike additive models and generalized additive neural networks, our model is decomposable with respect to translation rules rather than its component variables considering the decoding efficiency of machine translation; and it allows its additive terms of neural networks to share the same parameters for a compact structure to avoid sparsity. The idea of the neural network in machine translation has already been pioneered in previous works. Casta˜no et al. (1997) introduced a neural network for example-based machine translation. In particular, Son et al. (2012) and Schwenk (2012) employed a neural network to model the phrase translation probability on the rule level (α, y) instead of the bilingual sentence level (f, e) as in Eq. (5), and thus they did not go beyond the log-linear model for SMT. There are also works which exploit non-linear models in SMT. Duh and Kirchhoff (2008) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model’s expressive abilities; Sokolov et al. (2012) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective. Instead of considering the reranking</context>
</contexts>
<marker>Schwenk, 2012</marker>
<rawString>Holger Schwenk. 2012. Continuous space translation models for phrase-based statistical machine translation. In Proceedings of the 24th International Conference on Computational Linguistics, COLING ’12, Mumbai, India. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff Chiung-Yu Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing Natural Scenes and Natural Language with Recursive Neural Networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 26th International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="31249" citStr="Socher et al., 2011" startWordPosition="5311" endWordPosition="5314">does not fit well for difficult translation tasks (e.g. translation task on the news domain). The problem seems to be resolved by simply alternating feature representations through non-linear models, i.e. AddNHiero-D, even with single-layer networks. 6 Related Work Neural networks have achieved widespread attentions in many NLP tasks, e.g. the language 6All its parameters are shared with AdNN-Hiero-E except A and MaxIter, which are tuned on the development test datasets. model (Bengio et al., 2003); POS, Chunking, NER, and SRL (Collobert and Weston, 2008); Parsing (Collobert and Weston, 2008; Socher et al., 2011); and Machine transliteration (Deselaers et al., 2009). Our work is, of course, highly motivated by these works. Unlike these works, we propose a variant neural network, i.e. additive neural networks, starting from SMT itself and taking both of the model definition and its inference (decoding) together into account. Our variant of neural network, AdNN, is highly related to both additive models (Buja et al., 1989) and generalized additive neural networks (Potts, 1999; Waal and Toit, 2007), in which an additive term is either a linear model or a neural network. Unlike additive models and general</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng, and Christopher D. Manning. 2011. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In Proceedings of the 26th International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sokolov</author>
<author>G Wisniewski</author>
<author>F Yvon</author>
</authors>
<title>Nonlinear n-best list reranking with few features.</title>
<date>2012</date>
<booktitle>In AMTA,</booktitle>
<location>San Diego, USA.</location>
<contexts>
<context position="5574" citStr="Sokolov et al., 2012" startWordPosition="875" endWordPosition="878">presenting each word as a feature vector (Collobert and Weston, 2008). Because of the thousands of parameters and the non-convex objective in our model, efficient training is not simple. We propose an efficient training methodology: we apply the mini-batch conjugate sub-gradient algorithm (Le et al., 2011) to accelerate the training; we also propose pre-training and post-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1(f, e, d), h2(f, e, d), ··· , hK(f, e, d))&gt; is a K-dimensional feature vector defined on the tuple (f, e, d); W = (w1, w2, ··· , wK)&gt; is a Kdimensional weight vector of h, i.e., the parameters of the model, and it</context>
<context position="32843" citStr="Sokolov et al. (2012)" startWordPosition="5572" endWordPosition="5575">ady been pioneered in previous works. Casta˜no et al. (1997) introduced a neural network for example-based machine translation. In particular, Son et al. (2012) and Schwenk (2012) employed a neural network to model the phrase translation probability on the rule level (α, y) instead of the bilingual sentence level (f, e) as in Eq. (5), and thus they did not go beyond the log-linear model for SMT. There are also works which exploit non-linear models in SMT. Duh and Kirchhoff (2008) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model’s expressive abilities; Sokolov et al. (2012) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective. Instead of considering the reranking task in SMT, Xiao et al. (2010) employed a boosting method for the system combination in SMT. Unlike their post-processing models (either a re-ranking or a system combination model) in SMT, we propose a non-linear translation model which can be easily incorporated into the existing SMT framework. 7 Conclusion and Future Work In this paper, we go beyond the log-linear model for SMT and propose a novel AdNN based transtest2 24.38 25.14+ Ad</context>
</contexts>
<marker>Sokolov, Wisniewski, Yvon, 2012</marker>
<rawString>A. Sokolov, G. Wisniewski, and F. Yvon. 2012. Nonlinear n-best list reranking with few features. In AMTA, San Diego, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Hai Son</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12,</booktitle>
<pages>39--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="32382" citStr="Son et al. (2012)" startWordPosition="5493" endWordPosition="5496">erm is either a linear model or a neural network. Unlike additive models and generalized additive neural networks, our model is decomposable with respect to translation rules rather than its component variables considering the decoding efficiency of machine translation; and it allows its additive terms of neural networks to share the same parameters for a compact structure to avoid sparsity. The idea of the neural network in machine translation has already been pioneered in previous works. Casta˜no et al. (1997) introduced a neural network for example-based machine translation. In particular, Son et al. (2012) and Schwenk (2012) employed a neural network to model the phrase translation probability on the rule level (α, y) instead of the bilingual sentence level (f, e) as in Eq. (5), and thus they did not go beyond the log-linear model for SMT. There are also works which exploit non-linear models in SMT. Duh and Kirchhoff (2008) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model’s expressive abilities; Sokolov et al. (2012) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective. Instead of consid</context>
</contexts>
<marker>Son, Allauzen, Yvon, 2012</marker>
<rawString>Le Hai Son, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous space translation models with neural networks. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12, pages 39– 48, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="23996" citStr="Stolcke, 2002" startWordPosition="4167" endWordPosition="4168">e development test set is NIST05; and the test datasets are NIST06, and NIST08. For the Japanese-to-English task, the training data with 300k sentence pairs is from the NTCIR-patent task (Fujii et al., 2010); the development set, development test set, and two test sets are averagely extracted from a given development set with 4000 sentences, and these four datasets are called test1, test2, test3 and test4, respectively. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004b). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) for our baseline system, which shares the similar setting</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14679" citStr="Turian et al., 2010" startWordPosition="2486" endWordPosition="2489">tion probabilities, lexical translation probabilities, word penalty, glue rule penalty, synchronous rule penalty and language model. These default features are included because they empirically perform well in the loglinear model. For the local feature vector h&apos; in Eq (5), we employ word embedding features as described in the following subsection. 3.3 Word Embedding features for AdNN Word embedding can relax the sparsity introduced by the lexicalization in NLP, and it improves the systems for many tasks such as language model, named entity recognition, and parsing (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011). Here, we propose embedding features for rules in SMT by combining word embeddings. Firstly, we will define the embedding for the source side α of a rule r : X -+ (α, -y). Let VS be the vocabulary in the source language with size VS ; R&amp;quot;|VS |be the word embedding matrix, each column of which is the word embedding (ndimensional vector) for the corresponding word in VS; and maxSource be the maximal length of α for all rules. We further assume that the α for all rules share the same length as maxSource; otherwise, we add maxSource − α words “NULL” to the end of α to obtain a ne</context>
<context position="28137" citStr="Turian et al., 2010" startWordPosition="4813" endWordPosition="4816">arison is significant over AdNN-Hiero-E with p &lt; 0.05. these features are not dependent on the translation states, they are computed and saved to memory when loading the translation model. During decoding, we just look up these scores instead of re-calculating them on the fly. Therefore, the decoding efficiency of AdNN-Hiero-E is almost the same as that of L-Hiero. As shown in Table 1 the average decoding time for L-Hiero is 1.77 seconds/sentence while that for AdNN-Hiero-E is 1.88 seconds/sentence on the NIST05 test set. Word embedding features can improve the performance on other NLP tasks (Turian et al., 2010), but its effect on log-linear based SMT is not as expected. As shown in Table 2, L-Hiero-E gains little over L-Hiero for the Japanese-to-English task, and even decreases the performance over L-Hiero for the Chinese-to-English task. These results further prove our claim in Section 1, i.e. the loglinear model requires the features to be linear with the model and thus limits its expressive abilities. However, after the single-layer non-linear operator (sigmoid functions) on the embedding features for deep interpretation and representation, AdNNHiero-E gains improvements over both L-Hiero and L-H</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 384–394, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A de Waal</author>
<author>J V du Toit</author>
</authors>
<title>Generalized additive models from a neural network perspective.</title>
<date>2007</date>
<booktitle>In Proceedings of the Seventh IEEE International Conference on Data Mining Workshops, ICDMW ’07,</booktitle>
<pages>265--270</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<marker>de Waal, Toit, 2007</marker>
<rawString>D. A. de Waal and J. V. du Toit. 2007. Generalized additive models from a neural network perspective. In Proceedings of the Seventh IEEE International Conference on Data Mining Workshops, ICDMW ’07, pages 265–270, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>764--773</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="17086" citStr="Watanabe et al., 2007" startWordPosition="2940" endWordPosition="2943">pectively. Thus the dimension for h&apos; is K&apos; = 30 × (5 + 12) = 510. 794 sponding decoding formula: (ˆe, ˆd) = arg max 5(f, e, d; 0). e,d Given the model parameter 0 = (W, W&apos;, M, B), if we consider (M, B) as constant and Q(M ·h&apos;(r)+ B) as an additional feature vector besides h, then Eq. (5) goes back to being a log-linear model with parameter (W, W&apos;). In this way, the decoding for AdNN can share the same search strategy and cube pruning method as the log-linear model. 4 Training Method 4.1 Training Objective For the log-linear model, there are various tuning methods, e.g. MERT (Och, 2003), MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011) and so on, which iteratively optimize a weight such that, after re-ranking a k-best list of a given development set with this weight, the loss of the resulting 1-best list is minimal. In the extreme, if the k-best list consists only of a pair of translations ((e*, d*), (e&apos;, d&apos;)), the desirable weight should satisfy the assertion: if the BLEU score of e* is greater than that of e&apos;, then the model score of (e*, d*) with this weight will be also greater than that of (e&apos;, d&apos;). In this paper, a pair (e*, e&apos;) for a source sentence f is called as a </context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 764–773, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Xiao</author>
<author>Jingbo Zhu</author>
<author>Muhua Zhu</author>
<author>Huizhen Wang</author>
</authors>
<title>Boosting-based system combination for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>739--748</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="33033" citStr="Xiao et al. (2010)" startWordPosition="5602" endWordPosition="5605">ural network to model the phrase translation probability on the rule level (α, y) instead of the bilingual sentence level (f, e) as in Eq. (5), and thus they did not go beyond the log-linear model for SMT. There are also works which exploit non-linear models in SMT. Duh and Kirchhoff (2008) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model’s expressive abilities; Sokolov et al. (2012) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective. Instead of considering the reranking task in SMT, Xiao et al. (2010) employed a boosting method for the system combination in SMT. Unlike their post-processing models (either a re-ranking or a system combination model) in SMT, we propose a non-linear translation model which can be easily incorporated into the existing SMT framework. 7 Conclusion and Future Work In this paper, we go beyond the log-linear model for SMT and propose a novel AdNN based transtest2 24.38 25.14+ AdNN-Hiero- D 24.42 L-Hiero AdNN-Hiero -E 798 lation model. Our model overcomes some of the shortcomings suffered by the log-linear model: linearity and the lack of deep interpretation and rep</context>
</contexts>
<marker>Xiao, Zhu, Zhu, Wang, 2010</marker>
<rawString>Tong Xiao, Jingbo Zhu, Muhua Zhu, and Huizhen Wang. 2010. Boosting-based system combination for machine translation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 739–748, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>