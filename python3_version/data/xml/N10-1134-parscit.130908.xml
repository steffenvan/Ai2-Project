<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.996846">
Multi-document Summarization via
Budgeted Maximization of Submodular Functions
</title>
<author confidence="0.997016">
Hui Lin
</author>
<affiliation confidence="0.9977185">
Dept. of Electrical Engineering
University of Washington
</affiliation>
<address confidence="0.951353">
Seattle, WA 98195, USA
</address>
<email confidence="0.999437">
hlin@ee.washington.edu
</email>
<author confidence="0.992794">
Jeff Bilmes
</author>
<affiliation confidence="0.9981565">
Dept. of Electrical Engineering
University of Washington
</affiliation>
<address confidence="0.783664">
Seattle, WA 98195, USA
</address>
<email confidence="0.999417">
bilmes@ee.washington.edu
</email>
<sectionHeader confidence="0.994812" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999375363636363">
We treat the text summarization problem as
maximizing a submodular function under a
budget constraint. We show, both theoretically
and empirically, a modified greedy algorithm
can efficiently solve the budgeted submodu-
lar maximization problem near-optimally, and
we derive new approximation bounds in do-
ing so. Experiments on DUC’04 task show
that our approach is superior to the best-
performing method from the DUC’04 evalu-
ation on ROUGE-1 scores.
</bodyText>
<sectionHeader confidence="0.998777" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958596491228">
Automatically generating summaries from large text
corpora has long been studied in both information
retrieval and natural language processing. There
are several types of text summarization tasks. For
example, if an input query is given, the generated
summary can be query-specific, and otherwise it is
generic. Also, the number of documents to be sum-
marized can vary from one to many. The constituent
sentences of a summary, moreover, might be formed
in a variety of different ways — summarization can
be conducted using either extraction or abstraction,
the former selects only sentences from the origi-
nal document set, whereas the latter involves natu-
ral language generation. In this paper, we address
the problem of generic extractive summaries from
clusters of related documents, commonly known as
multi-document summarization.
In extractive text summarization, textual units
(e.g., sentences) from a document set are extracted
to form a summary, where grammaticality is as-
sured at the local level. Finding the optimal sum-
mary can be viewed as a combinatorial optimiza-
tion problem which is NP-hard to solve (McDon-
ald, 2007). One of the standard methods for
this problem is called Maximum Marginal Rele-
vance (MMR) (Dang, 2005)(Carbonell and Gold-
stein, 1998), where a greedy algorithm selects the
most relevant sentences, and at the same time avoids
redundancy by removing sentences that are too sim-
ilar to already selected ones. One major problem
of MMR is that it is non-optimal because the deci-
sion is made based on the scores at the current it-
eration. McDonald (2007) proposed to replace the
greedy search of MMR with a globally optimal for-
mulation, where the basic MMR framework can be
expressed as a knapsack packing problem, and an
integer linear program (ILP) solver can be used to
maximize the resulting objective function. ILP Al-
gorithms, however, can sometimes either be expen-
sive for large scale problems or themselves might
only be heuristic without associated theoretical ap-
proximation guarantees.
In this paper, we study graph-based approaches
for multi-document summarization. Indeed, several
graph-based methods have been proposed for extrac-
tive summarization in the past. Erkan and Radev
(2004) introduced a stochastic graph-based method,
LexRank, for computing the relative importance of
textual units for multi-document summarization. In
LexRank the importance of sentences is computed
based on the concept of eigenvector centrality in
the graph representation of sentences. Mihalcea and
Tarau also proposed an eigenvector centrality algo-
rithm on weighted graphs for document summariza-
tion (Mihalcea and Tarau, 2004). Mihalcea et al.
later applied Google’s PageRank (Brin and Page,
1998) to natural language processing tasks ranging
</bodyText>
<page confidence="0.941493">
912
</page>
<note confidence="0.7552175">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 912–920,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999874516129032">
from automatic keyphrase extraction and word sense
disambiguation, to extractive summarization (Mi-
halcea et al., 2004; Mihalcea, 2004). Recent work
in (Lin et al., 2009) presents a graph-based approach
where an undirected weighted graph is built for the
document to be summarized, and vertices represent
the candidate sentences and edge weights represent
the similarity between sentences. The summary ex-
traction procedure is done by maximizing a submod-
ular set function under a cardinality constraint.
Inspired by (Lin et al., 2009), we perform summa-
rization by maximizing submodular functions under
a budget constraint. A budget constraint is natural
in summarization task as the length of the summary
is often restricted. The length (byte budget) limita-
tion represents the real world scenario where sum-
maries are displayed using only limited computer
screen real estate. In practice, the candidate tex-
tual/linguistic units might not have identical costs
(e.g., sentence lengths vary). Since a cardinality
constraint is a special case (a budget constraint with
unity costs), our approach is more general than (Lin
et al., 2009). Moreover, we propose a modified
greedy algorithm (Section 4) and both theoretically
(Section 4.1) and empirically (Section 5.1) show that
the algorithm solves the problem near-optimally,
thanks to submodularity. Regarding summarization
performance, experiments on DUC’04 task show
that our approach is superior to the best-performing
method in DUC’04 evaluation on ROUGE-1 scores
(Section 5).
</bodyText>
<sectionHeader confidence="0.882287" genericHeader="introduction">
2 Background on Submodularity
</sectionHeader>
<bodyText confidence="0.9805115">
Consider a set function f : 2V —* R, which maps
subsets S C V of a finite ground set V to real num-
bers. f(�) is called normalized if f(0) = 0, and
is monotone if f(S) G f(T) whenever S C T.
f(�) is called submodular (Lovasz, 1983) if for any
S, T C V , we have
</bodyText>
<equation confidence="0.925057">
f(S U T) + f(S n T) G f(S) + f(T). (1)
</equation>
<bodyText confidence="0.985405609756097">
An equivalent definition of submodularity is the
property of diminishing returns, well-known in the
field of economics. That is, f(�) is submodular if for
any R C S C V and s E V \ S,
f(S U {s}) − f(S) G f(R U {s}) − f(R). (2)
Eqn. 2 states that the “value” of s never increases
in the contexts of ever larger sets, exactly the prop-
erty of diminishing returns. This phenomenon arises
naturally in many other contexts as well. For ex-
ample, the Shannon entropy function is submodu-
lar in the set of random variables. Submodular-
ity, moreover, is a discrete analog of convexity (Lo-
vasz, 1983). As convexity makes continuous func-
tions more amenable to optimization, submodular-
ity plays an essential role in combinatorial optimiza-
tion.
Many combinatorial optimization problems can
be solved optimally or near-optimally in polynomial
time only when the underlying function is submod-
ular. It has been shown that any submodular func-
tion can be minimized in polynomial time (Schri-
jver, 2000)(Iwata et al., 2001). Maximization of sub-
modular functions, however, is an NP-complete op-
timization problem but fortunately, some submod-
ular maximization problems can be solved near-
optimally. A famous result is that the maximization
of a monotone submodular function under a cardi-
nality constraint can be solved using a greedy al-
gorithm (Nemhauser et al., 1978) within a constant
factor (0.63) of being optimal. A constant-factor ap-
proximation algorithm has also been obtained for
maximizing monotone submodular function with a
knapsack constraint (see Section 4.2). Feige et.al.
(2007) studied unconstrained maximization of a ar-
bitrary submodular functions (not necessarily mono-
tone). Kawahara et.al. (2009) proposed a cutting-
plane method for optimally maximizing a submod-
ular set function under a cardinality constraint, and
Lee et.al. (2009) studied non-monotone submodu-
lar maximization under matroid and knapsack con-
straints.
</bodyText>
<sectionHeader confidence="0.978258" genericHeader="method">
3 Problem Setup
</sectionHeader>
<bodyText confidence="0.992576">
In this paper, we study the problem of maximizing a
submodular function under budget constraint, stated
formally below:
</bodyText>
<equation confidence="0.959799666666667">
max f(S) : j: cZ G B (3)
S⊆V
Z∈S
</equation>
<bodyText confidence="0.993636">
where V is the ground set of all linguistic units (e.g.,
sentences) in the document, S is the extracted sum-
mary (a subset of V ), cZ is the non-negative cost of
</bodyText>
<page confidence="0.998241">
913
</page>
<bodyText confidence="0.999367833333333">
selecting unit i and B is our budget, and submodular
function f(·) scores the summary quality.
The budgeted constraint arises naturally since of-
ten the summary must be length limited as men-
tioned above. In particular, the budget B could be
the maximum number of words allowed in any sum-
mary, or alternatively the maximum number of bytes
of any summary, where ci would then be either num-
ber of words or the number of bytes in sentence i.
To benefit from submodular optimization, the
objective function measuring the summary quality
must be submodular. In general, there are two ways
to apply submodular optimization to any application
domain. One way is to force submodularity on an
application, leading to an artificial and poorly per-
forming objective function even if it can be opti-
mized well. The alternative is to address applica-
tions where submodularity naturally applies. We are
fortunate in that, like convexity in the continuous do-
main, submodularity seems to arise naturally in a va-
riety of discrete domains, and as we will see below,
extractive summarization is one of them. As men-
tioned in Section 1, our approach is graph-based,
not only because a graph is a natural representation
of the relationships and interactions between textual
units, but also because many submodular functions
are well defined on a graph and can naturally be used
in measuring the summary quality.
Suppose certain pairs (i, j) with i, j ∈ V are sim-
ilar and the similarity of i and j is measured by a
non-negative value wi,j. We can represent the en-
tire document with a weighted graph (V, E), with
non-negative weights wi,j associated with each edge
ei,j, e ∈ E. One well-known graph-based submod-
ular function that measures the similarity of S to the
remainder V \ S is the graph-cut function:
</bodyText>
<equation confidence="0.9994875">
fcut(S) = ∑ ∑ wi,j. (4)
iEV \S jES
</equation>
<bodyText confidence="0.999856166666667">
In multi-document summarization, redundancy is a
particularly important issue since textual units from
different documents might convey the same infor-
mation. A high quality (small and meaningful) sum-
mary should not only be informative about the re-
mainder but also be compact (non-redundant). Typ-
ically, this goal is expressed as a combination of
maximizing the information coverage and minimiz-
ing the redundancy (as used in MMR (Carbonell and
Goldstein, 1998)). Inspired by this, we use the fol-
lowing objective by combining a λ-weighted penalty
term with the graph cut function:
</bodyText>
<equation confidence="0.9994865">
fMMR(S) = ∑ ∑ ∑wi,j − λ wi,j, λ ≥ 0.
iEV \S jES i,jES:i7�j
</equation>
<bodyText confidence="0.9998967">
Luckily, this function is still submodular as both the
graph cut function and the redundancy term are sub-
modular. Neither objective, however, is monotone,
something we address in Theorem 3. Although sim-
ilar to the MMR objective function, our approach is
different since 1) ours is graph-based and 2) we for-
malize the problem as submodular function maxi-
mization under the budget constraint where a simple
greedy algorithm can solve the problem guaranteed
near-optimally.
</bodyText>
<sectionHeader confidence="0.984854" genericHeader="method">
4 Algorithms
</sectionHeader>
<construct confidence="0.367087">
Algorithm 1 Modified greedy algorithm
</construct>
<listItem confidence="0.9950608">
1: G ← ∅
2: U ← V
3: while U ≠ ∅ do
4: k ← arg maxℓEU f(Gu(Ce)r f(G)
5: G ← G ∪ {k} if ∑iEG ci + ck ≤ Band
f(G ∪ {k}) − f(G) ≥ 0
6: U ← U \ {k}
7: end while
8: v* ← arg maxvEV,cv«3 f({v})
9: return Gf = arg maxSE{{v*},G} f(S)
</listItem>
<bodyText confidence="0.999083352941176">
Inspired by (Khuller et al., 1999), we propose
Algorithm 1 to solve Eqn. (3). The algorithm se-
quentially finds unit k with the largest ratio of ob-
jective function gain to scaled cost, i.e., (f(G ∪
{ℓ}) − f(G))/crℓ, where r &gt; 0 is the scaling factor.
If adding k increases the objective function value
while not violating the budget constraint, it is then
selected and otherwise bypassed. After the sequen-
tial selection, set G is compared to the within-budget
singleton with the largest objective value, and the
larger of the two becomes the final output.
The essential aspect of a greedy algorithm is
the design of the greedy heuristic. As discussed
in (Khuller et al., 1999), a heuristic that greedily se-
lects the k that maximizes (f(G ∪ {k}) − f(G))/ck
has an unbounded approximation factor. For ex-
ample, let V = {a, b}, f({a}) = 1, f({b}) = p,
</bodyText>
<page confidence="0.990208">
914
</page>
<bodyText confidence="0.999440740740741">
ca = 1, cb = p + 1, and 13 = p + 1. The solution
obtained by the greedy heuristic is {a} with objec-
tive function value 1, while the true optimal objec-
tive function value is p. The approximation factor
for this example is then p and therefore unbounded.
We address this issue by the following two mod-
ifications to the naive greedy algorithms. The first
one is the final step (line 8 and 9) in Algorithm 1
where set G and singletons are compared. This step
ensures that we could obtain a constant approxima-
tion factor for r = 1 (see the proof in the Appendix).
The second modification is that we introduce a
scaling factor r to adjust the scale of the cost. Sup-
pose, in the above example, we scale the cost as
ca = 1r, cb = (p+1)r, then selecting a or b depends
also on the scale r, and we might get the optimal so-
lution using a appropriate r. Indeed, the objective
function values and the costs might be uncalibrated
since they might measure different units. E.g., it is
hard to say if selecting a sentence of 15 words with
an objective function gain of 2 is better than select-
ing sentence of 10 words with gain of 1. Scaling
can potentially alleviate this mismatch (i.e., we can
adjust r on development set). Interestingly, our the-
oretical analysis of the performance guarantee of the
algorithm also gives us guidance about how to scale
the cost for a particular problem (see Section 4.1).
</bodyText>
<subsectionHeader confidence="0.999925">
4.1 Analysis of performance guarantee
</subsectionHeader>
<bodyText confidence="0.997989888888889">
Although Algorithm 1 is essentially a simple greedy
strategy, we show that it solves Eqn. (3) globally and
near-optimally, by exploiting the structure of sub-
modularity. As far as we know, this is a new result
for submodular optimization, not previously stated
or published before.
Theorem 1. For normalized monotone submodular
function f(�), Algorithm 1 with r = 1 has a constant
approximation factor as follows:
</bodyText>
<equation confidence="0.8506625">
( )
f(Gf) &gt; 1 − e−2) f(S∗), (5)
</equation>
<bodyText confidence="0.969296315789474">
where S∗ is an optimal solution.
Proof. See Appendix.
Note that an α-approximation algorithm for an
optimization problem is a polynomial-time algo-
rithm that for all instances of the problem produces
a solution whose value is within a factor of α of the
value of the an optimal solution. So Theorem 1 ba-
sically states that the solution found by Algorithm 1
can be at least as good as (1 − 1/,1e)f(S∗) �
0.39f(S∗) even in the worst case. A constant ap-
proximation bound is good since it is true for all in-
stances of the problem, and we always know how
good the algorithm is guaranteed to be without any
extra computation. For r =� 1, we resort to instance-
dependent bound where the approximation can be
easily computed per problem instance.
Theorem 2. With normalized monotone submodu-
lar f(�), for i = 1, ... , |G|, let vi be the ith unit
added into G and Gi is the set after adding vi. When
</bodyText>
<equation confidence="0.9847106">
0 &lt; r &lt; 1,
(f(Gi) &gt; 1 −∏i(1 − 13r|S*|1−r) f(S∗)
k=1
l (6)
&gt; 1 − ∏ (1 − 13r|Vi1−r I f(S∗) (7)
k=1 /
and when r &gt; 1,
( i
f (Gi) &gt; 1 −∏(1 − cvk (13 ) r) f (S∗). (8)
k=1
</equation>
<bodyText confidence="0.964269666666667">
Proof. See Appendix.
Theorem 2 gives bounds for a specific instance of
the problem. Eqn. (6) requires the size |S∗|, which
is unknown, requiring us to estimate an upper bound
of the cardinality of the optimal set S∗. Obviously,
|S∗ |&lt; |V |, giving us Eqn. (7). A tighter upper
bound is obtained, however, by sorting the costs.
That is, let c[1], c[2],... , c[|V |] be the sorted sequence
of costs in nondecreasing order, giving |S∗ |&lt; m
where ∑m−1
k=1 c[i] &lt; 13 and ∑mk=1 c[i] &gt; 13. In this
case, the computation cost for the bound estimation
is O(|V  |log |V |), which is quite feasible.
Note that both Theorem 1 and 2 are for mono-
tone submodular functions while our practical ob-
jective function, i.e. fMMR, is not guaranteed every-
where monotone. However, our theoretical results
still holds for fMMR with high probability in prac-
tice. Intuitively, in summarization tasks, the sum-
mary is usually small compared to the ground set
size (|S |« |V |). When |S |is small, fMMR is
</bodyText>
<page confidence="0.993208">
915
</page>
<bodyText confidence="0.953418307692308">
monotone and our theoretical results still hold. Pre-
cisely, assume that all edge weights are bounded:
wi,j E [0, 1] (which is the case for cosine simi-
larity between non-negative vectors). Also assume
that edges weights are independently identically dis-
tributed with mean µ, i.e. E(wi,j) = µ. Given a
budget 13, assume the maximum possible size of a
solution is K. Let α = 2λ + 1, and β = 2K − 1.
Notice that β « |V  |for our summarization task. We
have the following theorem:
Theorem 3. Algorithm 1 solves the summarization
problem near-optimally (i.e. Theorem 1 and Theo-
rem 2 hold) with high probability of at least
</bodyText>
<equation confidence="0.992898">
1 − exp { —2(|V  |− (α + 1)β)2µ2 + lnK}
ll IV |+ (α2 − 1)β JJJ
</equation>
<bodyText confidence="0.934832">
Proof. Omitted due to space limitation.
</bodyText>
<sectionHeader confidence="0.977177" genericHeader="method">
4.2 Related work
</sectionHeader>
<bodyText confidence="0.999913357142857">
Algorithms for maximizing submodular function
under budget constraint (Eqn. (3)) have been stud-
ied before. Krause (2005) generalized the work by
Khuller et al.(1999) on budgeted maximum cover
problem to the submodular framework, and showed
a 12(1 − 1/e)-approximation algorithm. The algo-
rithm in (Krause and Guestrin, 2005) and (Khuller
et al., 1999) is actually a special case of Algorithm 1
when r = 1, and Theorem 1 gives a better bound
(i.e., (1 − 1/.\/e) &gt; 12(1 − 1/e)) in this case. There
is also a greedy algorithm with partial enumerations
(Sviridenko, 2004; Krause and Guestrin, 2005) fac-
tor (1 − 1/e). This algorithm, however, is too com-
putationally expensive and thus not practical for real
world applications (the computation cost is O(|V |5)
in general). When each unit has identical cost, the
budget constraint reduces to cardinality constraint
where a greedy algorithm is known to be a (1−1/e)-
approximation algorithm (Nemhauser et al., 1978)
which is the best that can be achieved in polyno-
mial time (Feige, 1998) if P =� NP. Recent work
(Takamura and Okumura, 2009) applied the maxi-
mum coverage problem to text summarization (with-
out apparently being aware that their objective is
submodular) and studied a similar algorithm to ours
when r = 1 and for the non-penalized graph-cut
function. This problem, however, is a special case
of constrained submodular function maximization.
</bodyText>
<sectionHeader confidence="0.998938" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999939205882353">
We evaluated our approach on the data set of
DUC’04 (2004) with the setting of task 2, which
is a multi-document summarization task on English
news articles. In this task, 50 document clusters
are given, each of which consists of 10 documents.
For each document cluster, a short multi-document
summary is to be generated. The summary should
not be longer than 665 bytes including spaces and
punctuation, as required in the DUC’04 evaluation.
We used DUC’03 as our development set. All docu-
ments were segmented into sentences using a script
distributed by DUC. ROUGE version 1.5.5 (Lin,
2004), which is widely used in the study of summa-
rization, was used to evaluate summarization perfor-
mance in our experiments 1. We focus on ROUGE-
1 (unigram) F-measure scores since it has demon-
strated strong correlation with human annotation
(Lin, 2004).
The basic textual/linguistic units we consider in
our experiments are sentences. For each document
cluster, sentences in all the documents of this cluster
forms the ground set V . We built semantic graphs
for each document cluster based on cosine similar-
ity, where cosine similarity is computed based on
the TF-IDF (term frequency, inverse document fre-
quency) vectors for the words in the sentences. The
cosine similarity measures the similarity between
sentences, i.e., wi,j.
Here the IDF values were calculated using all the
document clusters. The weighted graph was built
by connecting vertices (corresponding to sentences)
with weight wi,j &gt; 0. Any unconnected vertex was
removed from the graph, which is equivalent to pre-
excluding certain sentences from the summary.
</bodyText>
<subsectionHeader confidence="0.998585">
5.1 Comparison with exact solution
</subsectionHeader>
<bodyText confidence="0.999903777777778">
In this section, we empirically show that Algo-
rithm 1 works near-optimally in practice. To deter-
mine how much accuracy is lost due to approxima-
tions, we compared our approximation algorithms
with an exact solution. The exact solutions were ob-
tained by Integer Linear Programming (ILP). Solv-
ing arbitrary ILP is an NP-hard problem. If the size
of the problem is not too large, we can sometimes
find the exact solution within a manageable time
</bodyText>
<footnote confidence="0.899557">
1Options used: -a -c 95 -b 665 -m -n 4 -w 1.2
</footnote>
<page confidence="0.994882">
916
</page>
<bodyText confidence="0.89875225">
using a branch-and-bound method. In our experi-
ments, MOSEK was used as our ILP solver.
We formalize Eqn. (3) as an ILP by introducing
indicator (binary) variables xi,j, yi,j, i ≠ j and zi
</bodyText>
<equation confidence="0.801565666666667">
for i, j ∈ V . In particular, zi = 1 indicates that
unit i is selected, i.e., i ∈ 5, xi,j = 1 indicates that
i ∈ 5 but j ∈/ 5, and yi,j = 1 indicates both i and
</equation>
<bodyText confidence="0.929674333333333">
j are selected. Adding constraints to ensure a valid
solution, we have the following ILP formulation for
Eqn. (3) with objective function fMMR(5):
</bodyText>
<figure confidence="0.968734294117647">
Objective function value
140
120
100
80
60
40
20
0
exact solution
op mal
r=0
r=0.5
r=1
r=1.5
� � wi,jyi,j 0 2 4 6 8 10 12
max wi,jxi,j − A number of sentences in the summary
</figure>
<equation confidence="0.933445833333333">
i̸�j,i,jEV i̸�j,i,jEV
�subject to: cizi ≤ B,
iEV
xi,j − zi ≤ 0, xi,j + zj ≤ 1, zi − zj − xi,j ≤ 0,
yi,j − zi ≤ 0, yi,j − zj ≤ 0, zi + zj − yi,j ≤ 1,
xi,j, yi,j, zi ∈ {0, 1}, ∀i ≠ j, i, j ∈ V
</equation>
<bodyText confidence="0.999823928571429">
Note that the number of variables in the ILP for-
mulation is O(|V |2). For a document cluster with
hundreds of candidate textual units, the scale of the
problem easily grows involving tens of thousands
of variables, making the problem very expensive to
solve. For instance, solving the ILP exactly on a
document cluster with 182 sentences (as used in Fig-
ure 1) took about 17 hours while our Algorithm 1
finished in less than 0.01 seconds.
We tested both approximate and exact algorithms
on DUC’03 data where 60 document clusters were
used (30 TDT document clusters and 30 TREC doc-
ument clusters), each of which contains 10 docu-
ments on average. The true approximation factor
was computed by dividing the objective function
value found by Algorithm 1 over the optimal ob-
jective function value (found by ILP). The average
approximation factors over the 58 document clus-
ters (ILP on 2 of the 60 document clusters failed to
finish) are shown in Table 1, along with other statis-
tics. On average Algorithm 1 finds a solution that is
over 90% as good as the optimal solution for many
different r values, which backs up our claim that
the modified greedy algorithm solves the problem
near-optimally, even occasionally optimally (Figure
1 shows one such example).
The higher objective function value does not al-
ways indicate higher ROUGE-1 score. Indeed,
</bodyText>
<figureCaption confidence="0.97324275">
Figure 1: Application of Algorithm 1 when summariz-
ing document cluster d30001t in the DUC’04 dataset with
summary size limited to 665 bytes. The objective func-
tion was fMMR with A = 2. The plots show the achieved
objective function as the number of selected sentences
grows. The plots stop when in each case adding more
sentences violates the budget. Algorithm 1 with r = 1
found the optimal solution exactly.
</figureCaption>
<bodyText confidence="0.999981909090909">
rather than directly optimizing ROUGE, we opti-
mize a surrogate submodular function that indicates
the quality of a summary. Optimality in the submod-
ular function does not necessary indicate optimality
in ROUGE score. Nevertheless, we will show that
our approach outperforms several other approaches
in terms of ROUGE. We note that ROUGE is itself
a surrogate for true human-judged summary quality,
it might possibly be that fMMR is a still better surro-
gate — we do not consider this possibility further in
this work, however.
</bodyText>
<subsectionHeader confidence="0.999643">
5.2 Summarization Results
</subsectionHeader>
<bodyText confidence="0.999986888888889">
We used DUC’03 (as above) for our development
set to investigate how r and A relate to the ROUGE-
1 score. From Figure 2, the best performance is
achieved with r = 0.3, A = 4. Using these settings,
we applied our approach to the DUC’04 task. The
results, along with the results of other approaches,
are shown in Table 2. All the results in Table 2 are
presented as ROUGE-1 F-measure scores. 2
We compared our approach to two other well-
</bodyText>
<footnote confidence="0.98155475">
2When the evaluation was done in 2004, ROUGE was still in
revision 1.2.1, so we re-evaluated the DUC’04 submissions us-
ing ROUGE v1.5.5 and the numbers are slightly different from
the those reported officially.
</footnote>
<page confidence="0.995258">
917
</page>
<tableCaption confidence="0.997637">
Table 1: Comparison of Algorithm 1 to exact algorithms
</tableCaption>
<bodyText confidence="0.9818755">
on DUC’03 dataset. All the numbers shown in the ta-
ble are the average statistics (mean/std). The “true” ap-
proximation factor is the ratio of objective function value
found by Algorithm 1 over the ILP-derived true-optimal
objective value, and the approximation bounds were esti-
mated using Theorem 2.
</bodyText>
<table confidence="0.983364090909091">
Approx. factor ROUGE-1
(%)
true bound
exact 1.00 - 33.60/5.05
r = 0.0 0.65/0.15 &gt;0.19/0.08 33.50/5.94
r = 0.1 0.71/0.15 &gt;0.24/0.08 33.68/6.03
r = 0.3 0.88/0.11 &gt;0.37/0.06 34.77/5.49
r = 0.5 0.96/0.04 &gt;0.48/0.05 34.33/5.94
r = 0.7 0.98/0.02 &gt;0.56/0.05 34.08/5.41
r = 1.0 0.98/0.02 &gt;0.65/0.04 33.32/5.14
r = 1.2 0.97/0.02 &gt;0.48/0.05 32.54/4.69
</table>
<bodyText confidence="0.952075222222222">
Wilcoxon signed rank test at level p &lt; 0.05. Our
approach also outperforms the best system (Conroy
et al., 2004), peer code 65 in the DUC’04 evalua-
tion although not as significant (p &lt; 0.08). The rea-
son might be that DUC’03 is a poor representation
of DUC’04 — indeed, by varying r and λ over the
ranges 0 &lt; r &lt; 0.2 and 5 &lt; λ &lt; 9 respectively, the
DUC’04 ROUGE-1 scores were all &gt; 38.8% with
the best DUC’04 score being 39.3%.
</bodyText>
<tableCaption confidence="0.955273">
Table 2: ROUGE-1 F-measure results (%)
</tableCaption>
<table confidence="0.9982894">
Method ROUGE-1 score
peer65 (best system in DUC04) 37.94
peer104 (LexRank) 37.12
PageRank 35.37
Submodular (r = 0.3, λ = 4) 38.39
</table>
<sectionHeader confidence="0.997251" genericHeader="conclusions">
6 Appendix
</sectionHeader>
<bodyText confidence="0.9880068">
We analyze the performance guarantee of Algorithm 1.
We use the following notation: S∗ is the optimal solu-
tion; Gf is the final solution obtained by Algorithm 1;
G is the solution obtained by the greedy heuristic (line
1 to 7 in Algorithm 1); vi is the ith unit added to G,
i = 1, ... , |G|; Gi is the set obtained by greedy algorithm
after adding vi (i.e., Gi = Uik=1{vk}, for i = 1, ... , |G|,
with G0 = 0 and G|G |= G); f(·) : 2V → R is a
monotone submodular function; and ρk(S) is the gain of
adding k to S, i.e., f(S U {k}) − f(S).
</bodyText>
<figure confidence="0.9556564">
Lemma 1. VX, Y C V,
35.0%
34.5%
ROUGE-1 F-measure
34.0%
33.5%
33.0%
32.5%
32.0%
r=0
r=0.3
r=0.5
r=0.7
r=1
0 5 10 15
</figure>
<figureCaption confidence="0.997092">
Figure 2: Different combinations of r and λ for fMMR � ρk(Y ). (9)
related to ROUGE-1 score on DUC’03 task 1. f(X) &lt; f(Y ) +
k∈X\Y
</figureCaption>
<bodyText confidence="0.9996675">
known graph-based approaches, LexRank and
PageRank. LexRank was one of the participat-
ing system in DUC’04, with peer code 104. For
PageRank, we implemented the recursive graph-
based ranking algorithm ourselves. The importance
of sentences was estimated in an iterative way as
in (Brin and Page, 1998)(Mihalcea et al., 2004).
Sentences were then selected based on their impor-
tance rankings until the budget constraint was vi-
olated. The graphs used for PageRank were ex-
actly the graphs in our submodular approaches (i.e.,
an undirected graph). In both cases, submodu-
lar summarization achieves better ROUGE-1 scores.
The improvement is statistically significant by the
</bodyText>
<note confidence="0.57509">
Proof. See (Nemhauser et al., 1978)
</note>
<construct confidence="0.238273">
Lemma 2. For i = 1, ... , |G|, when 0 &lt; r &lt; 1,
</construct>
<equation confidence="0.957502705882353">
1?T I CY�k I1-T
f (S∗) − f (Gi−1) &lt; 13 S (f(Gi) − f(Gi−1)),
cr
vi
and when r &gt; 1,
( &apos; )r
f(S∗) − f(Gi−1) &lt; (f(Gi) − f(Gi−1))
cvi
Proof. Based on line 4 of Algorithm 1, we have
Vu E S∗ \ Gi−1, ρu(Gi−1)
cru
ρvi(Gi−1)
&lt;
r
cvi
.
918
Thus when 0 ≤ r ≤ 1,
∑
ρu(Gi−1) ≤ ρvi(Gi−1) cr
cr u
vi u∈S∗\Gi−1
(∑
≤ ρvi(Gi−1) u∈S∗\Gi−1 cu
|S∗ \ Gi−1|
cr |S∗ \ Gi−1|
vi

≤ ρvi(Gi−1)  ∑
|S∗|1−r cu
cr vi u∈S∗\Gi−1
≤ ρvi(Gi−1) |S∗|1−rBr,
cr
vi
</equation>
<bodyText confidence="0.937660666666667">
where the second inequality is due to the concavity of
g(x) = xr, x &gt; 0, 0 ≤ r ≤ 1. The last inequality uses
the fact that ∑u∈S∗ cu ≤ B. Similarly, when r ≥ 1,
</bodyText>
<equation confidence="0.905433">
919 Proof. Consider the following two cases:
∑
ρu(Gi−1) ≤ ρvi(Gi−1) cr
cr u
vi u∈S∗\Gi−1
  r
 ∑ cu 
u∈S∗\Gi−1
Br.
</equation>
<bodyText confidence="0.973433714285714">
Applying Lemma 1, i.e., let X = S∗ and Y = Gi−1, the
lemma immediately follows.
The following is a proof of Theorem 2.
Proof. Obviously, the theorem is true when i = 1 by
applying Lemma 2.
Assume that the theorem is true for i −1, 2 ≤ i ≤ |G|,
we show that it also holds for i. When 0 ≤ r ≤ 1,
</bodyText>
<equation confidence="0.997948633333333">
f(Gi) = f(Gi−1) + (f(Gi) − f(Gi−1))
cr
≥ f(Gi−1) + Br|S∗|1−r (f(S∗) − f(Gi−1))
vi
( )
1 − cr vi f(Gi−1) + cr vi
= Br|S∗|1−r f(S∗)
Br|S∗|1−r
i−1
≥ (1 −Br|cS∗iv|1−r) 1 −∏(1 −Br|cvk1−r)
i
r
f (S
) +
1
f (S
)
v
c
∗
Br|
∗|
−r
∗
( ( ))
∏i 1 − cr vk
= 1 − f(S∗).
Br|S∗|1−r
k=1
1 can
</equation>
<bodyText confidence="0.815307333333333">
≥
be proven similarly.
Now we are ready to prove Theorem 1.
</bodyText>
<figure confidence="0.997449410596026">
∑
u∈S∗\Gi−1
)r
 r

∑
u∈S∗\Gi−1
ρvi(Gi−1)
≤
cr
vi
ρvi(Gi−1)
≤
cr
vi
k=1
&gt; 1 f (S
). Then it
)
f
})) &gt; 2 f (S
) due line
V, we have
We
consider the following two sub-cases, namely Case 2.1
then we know that
G
at most one unit in
\ G since otherwise we will have
cv &gt; B. By assumption, we have
\ G)
\ G) +
G)
which implies
G)
Thus we have
G)
∃v∈
f({v})
∗
f
≥
{v
∗
∀v∈
f({v})≤1�f(S∗).
∑v∈G
≤1�B,
∀v∈/
1�B
∈/
S∗
∑v∈S∗
f(S∗
≤�f(S∗). Submodularity of f(·) gives us:
1f(S∗
f(S∗∩
≥f(S∗),
f(S∗∩
≥1�f(S∗).
f(Gf)≥f(G)≥f(S∗∩
≥2f(S∗),1
∑v∈G cv &gt; 1�B, for 0 ≤ r ≤ 1, using
Theorem 2, we have
 )
(
∏ |G |1 − cr vk
f(G) ≥ 1 −  f(S∗)
Br|S∗|1−r
Case 1:
is guaranteed that f (G
9 of Algorithm 1.
Case 2:

≥ 1 −
V such that
k=1

1 −
and Case 2.2:
Case 2.1: If
cv
G, cv &gt;
since otherwise we can add a v
into G to increase the objective function value without
violating the budget constraint. This implies that there is
S
1G
(
≥
1 −|
∗|r−
|
1 −
2r|G|r2 21G T
) f(S∗)
where the second inequality follows from monotonicity.
Case 2.2: If
Lagrange multipliers) that for a1, ... ,
II8+ such that
an∈
∑ni=1 ai = α, function
achieves its minimum of 1
(1
when a1 =
=
=
for
&gt; 0. The last inequality follows
from
1
x.
In all cases, we ha(ve / l
f(G
)
min{ 2,1
e 212sGi)r 11
In particular, when r = 1, we obtain the constan
−
−β/nr)n
···
an
α/n
α,β
e−x≥
−
f
≥
−
f(S∗)
t approx-
imation factor, i.e.
The case when r
∏ |G|
k=1
crvk|S∗|r−1
2r(∑|G|)r
k=1 cvk
1
e
10s
) f (S
)
−
≥
1−
−
∗
∗
 
 f(S∗)
</figure>
<bodyText confidence="0.707241">
where the third inequality uses the fact (provable using
( )
</bodyText>
<equation confidence="0.7478892">
∏n 1 −βari
1 −αr
i=1
( )
f(Gf) ≥ 1 − e− 1 2 f(S∗)
</equation>
<sectionHeader confidence="0.994009" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999830333333333">
This work is supported by an ONR MURI grant
(No. N000140510388), the Companions project
(IST programme under EC grant IST-FP6-034434),
and the National Science Foundation under grant
IIS-0535100. We also wish to thank the anonymous
reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.998891" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999916022222223">
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer networks
and ISDN systems, 30(1-7):107–117.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proc. of SIGIR.
J.M. Conroy, J.D. Schlesinger, J. Goldstein, and D.P.
O’leary. 2004. Left-brain/right-brain multi-document
summarization. In Proceedings of the Document Un-
derstanding Conference (DUC 2004).
H.T. Dang. 2005. Overview of DUC 2005. In Proceed-
ings of the Document Understanding Conference.
2004. Document understanding conferences (DUC).
http://www-nlpir.nist.gov/projects/duc/index.html.
G. Erkan and D.R. Radev. 2004. LexRank: Graph-
based Lexical Centrality as Salience in Text Summa-
rization. Journal of Artificial Intelligence Research,
22:457–479.
U. Feige, V. Mirrokni, and J. Vondrak. 2007. Maximiz-
ing non-monotone submodular functions. In Proceed-
ings of 48th Annual IEEE Symposium on Foundations
of Computer Science (FOCS).
U. Feige. 1998. A threshold of ln n for approximating set
cover. Journal of the ACM (JACM), 45(4):634–652.
G. Goel, , C. Karande, P. Tripathi, and L. Wang.
2009. Approximability of Combinatorial Problems
with Multi-agent Submodular Cost Functions. FOCS.
S. Iwata, L. Fleischer, and S. Fujishige. 2001. A
combinatorial strongly polynomial algorithm for min-
imizing submodular functions. Journal of the ACM,
48(4):761–777.
Yoshinobu Kawahara, Kiyohito Nagano, Koji Tsuda, and
Jeff Bilmes. 2009. Submodularity cuts and appli-
cations. In Neural Information Processing Society
(NIPS), Vancouver, Canada, December.
S. Khuller, A. Moss, and J. Naor. 1999. The budgeted
maximum coverage problem. Information Processing
Letters, 70(1):39–45.
A. Krause and C. Guestrin. 2005. A note on the bud-
geted maximization of submodular functions. Techni-
cal Rep. No. CMU-CALD-05, 103.
J. Lee, V.S. Mirrokni, V. Nagarajan, and M. Sviridenko.
2009. Non-monotone submodular maximization un-
der matroid and knapsack constraints. In Proceedings
of the 41st annual ACM symposium on Symposium on
theory of computing, pages 323–332. ACM New York,
NY, USA.
Hui Lin, Jeff Bilmes, and Shasha Xie. 2009. Graph-
based submodular selection for extractive summariza-
tion. In Proc. IEEE Automatic Speech Recognition
and Understanding (ASRU), Merano, Italy, December.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop.
L. Lovasz. 1983. Submodular functions and convexity.
Mathematical programming-The state of the art,(eds.
A. Bachem, M. Grotschel and B. Korte) Springer,
pages 235–257.
R. McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. Lecture
Notes in Computer Science, 4425:557.
R. Mihalcea and P. Tarau. 2004. TextRank: bringing or-
der into texts. In Proceedings of EMNLP, Barcelona,
Spain.
R. Mihalcea, P. Tarau, and E. Figa. 2004. PageRank on
semantic networks, with application to word sense dis-
ambiguation. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING-
04).
R. Mihalcea. 2004. Graph-based ranking algorithms for
sentence extraction, applied to text summarization. In
Proceedings of the ACL 2004 (companion volume).
2006. Mosek.
G.L. Nemhauser, L.A. Wolsey, and M.L. Fisher. 1978.
An analysis of approximations for maximizing sub-
modular set functions I. Mathematical Programming,
14(1):265–294.
A. Schrijver. 2000. A combinatorial algorithm mini-
mizing submodular functions in strongly polynomial
time. Journal of Combinatorial Theory, Series B,
80(2):346–355.
M. Sviridenko. 2004. A note on maximizing a submod-
ular set function subject to a knapsack constraint. Op-
erations Research Letters, 32(1):41–43.
H. Takamura and M. Okumura. 2009. Text summariza-
tion model based on maximum coverage problem and
its variant. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 781–789. Association for
Computational Linguistics.
</reference>
<page confidence="0.996945">
920
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.915768">
<title confidence="0.999303">Multi-document Summarization Budgeted Maximization of Submodular Functions</title>
<author confidence="0.999941">Hui Lin</author>
<affiliation confidence="0.9997665">Dept. of Electrical Engineering University of Washington</affiliation>
<address confidence="0.999721">Seattle, WA 98195,</address>
<email confidence="0.999519">hlin@ee.washington.edu</email>
<author confidence="0.980152">Jeff</author>
<affiliation confidence="0.999784">Dept. of Electrical University of</affiliation>
<address confidence="0.998667">Seattle, WA 98195,</address>
<email confidence="0.999769">bilmes@ee.washington.edu</email>
<abstract confidence="0.99466525">We treat the text summarization problem as maximizing a submodular function under a budget constraint. We show, both theoretically and empirically, a modified greedy algorithm can efficiently solve the budgeted submodular maximization problem near-optimally, and we derive new approximation bounds in doing so. Experiments on DUC’04 task show that our approach is superior to the bestperforming method from the DUC’04 evaluation on ROUGE-1 scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual Web search engine.</title>
<date>1998</date>
<booktitle>Computer networks and ISDN systems,</booktitle>
<pages>30--1</pages>
<contexts>
<context position="3489" citStr="Brin and Page, 1998" startWordPosition="522" endWordPosition="525">tion. Indeed, several graph-based methods have been proposed for extractive summarization in the past. Erkan and Radev (2004) introduced a stochastic graph-based method, LexRank, for computing the relative importance of textual units for multi-document summarization. In LexRank the importance of sentences is computed based on the concept of eigenvector centrality in the graph representation of sentences. Mihalcea and Tarau also proposed an eigenvector centrality algorithm on weighted graphs for document summarization (Mihalcea and Tarau, 2004). Mihalcea et al. later applied Google’s PageRank (Brin and Page, 1998) to natural language processing tasks ranging 912 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 912–920, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics from automatic keyphrase extraction and word sense disambiguation, to extractive summarization (Mihalcea et al., 2004; Mihalcea, 2004). Recent work in (Lin et al., 2009) presents a graph-based approach where an undirected weighted graph is built for the document to be summarized, and vertices represent the candidate sentences and edge weights represe</context>
<context position="26281" citStr="Brin and Page, 1998" startWordPosition="4526" endWordPosition="4529">unction; and ρk(S) is the gain of adding k to S, i.e., f(S U {k}) − f(S). Lemma 1. VX, Y C V, 35.0% 34.5% ROUGE-1 F-measure 34.0% 33.5% 33.0% 32.5% 32.0% r=0 r=0.3 r=0.5 r=0.7 r=1 0 5 10 15 Figure 2: Different combinations of r and λ for fMMR � ρk(Y ). (9) related to ROUGE-1 score on DUC’03 task 1. f(X) &lt; f(Y ) + k∈X\Y known graph-based approaches, LexRank and PageRank. LexRank was one of the participating system in DUC’04, with peer code 104. For PageRank, we implemented the recursive graphbased ranking algorithm ourselves. The importance of sentences was estimated in an iterative way as in (Brin and Page, 1998)(Mihalcea et al., 2004). Sentences were then selected based on their importance rankings until the budget constraint was violated. The graphs used for PageRank were exactly the graphs in our submodular approaches (i.e., an undirected graph). In both cases, submodular summarization achieves better ROUGE-1 scores. The improvement is statistically significant by the Proof. See (Nemhauser et al., 1978) Lemma 2. For i = 1, ... , |G|, when 0 &lt; r &lt; 1, 1?T I CY�k I1-T f (S∗) − f (Gi−1) &lt; 13 S (f(Gi) − f(Gi−1)), cr vi and when r &gt; 1, ( &apos; )r f(S∗) − f(Gi−1) &lt; (f(Gi) − f(Gi−1)) cvi Proof. Based on line 4</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>S. Brin and L. Page. 1998. The anatomy of a large-scale hypertextual Web search engine. Computer networks and ISDN systems, 30(1-7):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proc. of SIGIR.</booktitle>
<contexts>
<context position="2040" citStr="Carbonell and Goldstein, 1998" startWordPosition="299" endWordPosition="303">he latter involves natural language generation. In this paper, we address the problem of generic extractive summaries from clusters of related documents, commonly known as multi-document summarization. In extractive text summarization, textual units (e.g., sentences) from a document set are extracted to form a summary, where grammaticality is assured at the local level. Finding the optimal summary can be viewed as a combinatorial optimization problem which is NP-hard to solve (McDonald, 2007). One of the standard methods for this problem is called Maximum Marginal Relevance (MMR) (Dang, 2005)(Carbonell and Goldstein, 1998), where a greedy algorithm selects the most relevant sentences, and at the same time avoids redundancy by removing sentences that are too similar to already selected ones. One major problem of MMR is that it is non-optimal because the decision is made based on the scores at the current iteration. McDonald (2007) proposed to replace the greedy search of MMR with a globally optimal formulation, where the basic MMR framework can be expressed as a knapsack packing problem, and an integer linear program (ILP) solver can be used to maximize the resulting objective function. ILP Algorithms, however, </context>
<context position="10108" citStr="Carbonell and Goldstein, 1998" startWordPosition="1611" endWordPosition="1614">∈ E. One well-known graph-based submodular function that measures the similarity of S to the remainder V \ S is the graph-cut function: fcut(S) = ∑ ∑ wi,j. (4) iEV \S jES In multi-document summarization, redundancy is a particularly important issue since textual units from different documents might convey the same information. A high quality (small and meaningful) summary should not only be informative about the remainder but also be compact (non-redundant). Typically, this goal is expressed as a combination of maximizing the information coverage and minimizing the redundancy (as used in MMR (Carbonell and Goldstein, 1998)). Inspired by this, we use the following objective by combining a λ-weighted penalty term with the graph cut function: fMMR(S) = ∑ ∑ ∑wi,j − λ wi,j, λ ≥ 0. iEV \S jES i,jES:i7�j Luckily, this function is still submodular as both the graph cut function and the redundancy term are submodular. Neither objective, however, is monotone, something we address in Theorem 3. Although similar to the MMR objective function, our approach is different since 1) ours is graph-based and 2) we formalize the problem as submodular function maximization under the budget constraint where a simple greedy algorithm </context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Conroy</author>
<author>J D Schlesinger</author>
<author>J Goldstein</author>
<author>D P O’leary</author>
</authors>
<title>Left-brain/right-brain multi-document summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the Document Understanding Conference (DUC</booktitle>
<marker>Conroy, Schlesinger, Goldstein, O’leary, 2004</marker>
<rawString>J.M. Conroy, J.D. Schlesinger, J. Goldstein, and D.P. O’leary. 2004. Left-brain/right-brain multi-document summarization. In Proceedings of the Document Understanding Conference (DUC 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2005</date>
<booktitle>In Proceedings of the Document Understanding Conference.</booktitle>
<note>Document understanding conferences (DUC). http://www-nlpir.nist.gov/projects/duc/index.html.</note>
<contexts>
<context position="2009" citStr="Dang, 2005" startWordPosition="298" endWordPosition="299">t, whereas the latter involves natural language generation. In this paper, we address the problem of generic extractive summaries from clusters of related documents, commonly known as multi-document summarization. In extractive text summarization, textual units (e.g., sentences) from a document set are extracted to form a summary, where grammaticality is assured at the local level. Finding the optimal summary can be viewed as a combinatorial optimization problem which is NP-hard to solve (McDonald, 2007). One of the standard methods for this problem is called Maximum Marginal Relevance (MMR) (Dang, 2005)(Carbonell and Goldstein, 1998), where a greedy algorithm selects the most relevant sentences, and at the same time avoids redundancy by removing sentences that are too similar to already selected ones. One major problem of MMR is that it is non-optimal because the decision is made based on the scores at the current iteration. McDonald (2007) proposed to replace the greedy search of MMR with a globally optimal formulation, where the basic MMR framework can be expressed as a knapsack packing problem, and an integer linear program (ILP) solver can be used to maximize the resulting objective func</context>
</contexts>
<marker>Dang, 2005</marker>
<rawString>H.T. Dang. 2005. Overview of DUC 2005. In Proceedings of the Document Understanding Conference. 2004. Document understanding conferences (DUC). http://www-nlpir.nist.gov/projects/duc/index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>D R Radev</author>
</authors>
<title>LexRank: Graphbased Lexical Centrality as Salience in Text Summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>22--457</pages>
<contexts>
<context position="2994" citStr="Erkan and Radev (2004)" startWordPosition="452" endWordPosition="455">e the greedy search of MMR with a globally optimal formulation, where the basic MMR framework can be expressed as a knapsack packing problem, and an integer linear program (ILP) solver can be used to maximize the resulting objective function. ILP Algorithms, however, can sometimes either be expensive for large scale problems or themselves might only be heuristic without associated theoretical approximation guarantees. In this paper, we study graph-based approaches for multi-document summarization. Indeed, several graph-based methods have been proposed for extractive summarization in the past. Erkan and Radev (2004) introduced a stochastic graph-based method, LexRank, for computing the relative importance of textual units for multi-document summarization. In LexRank the importance of sentences is computed based on the concept of eigenvector centrality in the graph representation of sentences. Mihalcea and Tarau also proposed an eigenvector centrality algorithm on weighted graphs for document summarization (Mihalcea and Tarau, 2004). Mihalcea et al. later applied Google’s PageRank (Brin and Page, 1998) to natural language processing tasks ranging 912 Human Language Technologies: The 2010 Annual Conference</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G. Erkan and D.R. Radev. 2004. LexRank: Graphbased Lexical Centrality as Salience in Text Summarization. Journal of Artificial Intelligence Research, 22:457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Feige</author>
<author>V Mirrokni</author>
<author>J Vondrak</author>
</authors>
<title>Maximizing non-monotone submodular functions.</title>
<date>2007</date>
<booktitle>In Proceedings of 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS).</booktitle>
<marker>Feige, Mirrokni, Vondrak, 2007</marker>
<rawString>U. Feige, V. Mirrokni, and J. Vondrak. 2007. Maximizing non-monotone submodular functions. In Proceedings of 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Feige</author>
</authors>
<title>A threshold of ln n for approximating set cover.</title>
<date>1998</date>
<journal>Journal of the ACM (JACM),</journal>
<volume>45</volume>
<issue>4</issue>
<contexts>
<context position="17557" citStr="Feige, 1998" startWordPosition="2983" endWordPosition="2984"> gives a better bound (i.e., (1 − 1/.\/e) &gt; 12(1 − 1/e)) in this case. There is also a greedy algorithm with partial enumerations (Sviridenko, 2004; Krause and Guestrin, 2005) factor (1 − 1/e). This algorithm, however, is too computationally expensive and thus not practical for real world applications (the computation cost is O(|V |5) in general). When each unit has identical cost, the budget constraint reduces to cardinality constraint where a greedy algorithm is known to be a (1−1/e)- approximation algorithm (Nemhauser et al., 1978) which is the best that can be achieved in polynomial time (Feige, 1998) if P =� NP. Recent work (Takamura and Okumura, 2009) applied the maximum coverage problem to text summarization (without apparently being aware that their objective is submodular) and studied a similar algorithm to ours when r = 1 and for the non-penalized graph-cut function. This problem, however, is a special case of constrained submodular function maximization. 5 Experiments We evaluated our approach on the data set of DUC’04 (2004) with the setting of task 2, which is a multi-document summarization task on English news articles. In this task, 50 document clusters are given, each of which </context>
</contexts>
<marker>Feige, 1998</marker>
<rawString>U. Feige. 1998. A threshold of ln n for approximating set cover. Journal of the ACM (JACM), 45(4):634–652.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Karande</author>
<author>P Tripathi</author>
<author>L Wang</author>
</authors>
<title>Approximability of Combinatorial Problems with Multi-agent Submodular Cost Functions.</title>
<date>2009</date>
<publisher>FOCS.</publisher>
<marker>Karande, Tripathi, Wang, 2009</marker>
<rawString>G. Goel, , C. Karande, P. Tripathi, and L. Wang. 2009. Approximability of Combinatorial Problems with Multi-agent Submodular Cost Functions. FOCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Iwata</author>
<author>L Fleischer</author>
<author>S Fujishige</author>
</authors>
<title>A combinatorial strongly polynomial algorithm for minimizing submodular functions.</title>
<date>2001</date>
<journal>Journal of the ACM,</journal>
<volume>48</volume>
<issue>4</issue>
<contexts>
<context position="6590" citStr="Iwata et al., 2001" startWordPosition="1030" endWordPosition="1033">es naturally in many other contexts as well. For example, the Shannon entropy function is submodular in the set of random variables. Submodularity, moreover, is a discrete analog of convexity (Lovasz, 1983). As convexity makes continuous functions more amenable to optimization, submodularity plays an essential role in combinatorial optimization. Many combinatorial optimization problems can be solved optimally or near-optimally in polynomial time only when the underlying function is submodular. It has been shown that any submodular function can be minimized in polynomial time (Schrijver, 2000)(Iwata et al., 2001). Maximization of submodular functions, however, is an NP-complete optimization problem but fortunately, some submodular maximization problems can be solved nearoptimally. A famous result is that the maximization of a monotone submodular function under a cardinality constraint can be solved using a greedy algorithm (Nemhauser et al., 1978) within a constant factor (0.63) of being optimal. A constant-factor approximation algorithm has also been obtained for maximizing monotone submodular function with a knapsack constraint (see Section 4.2). Feige et.al. (2007) studied unconstrained maximizatio</context>
</contexts>
<marker>Iwata, Fleischer, Fujishige, 2001</marker>
<rawString>S. Iwata, L. Fleischer, and S. Fujishige. 2001. A combinatorial strongly polynomial algorithm for minimizing submodular functions. Journal of the ACM, 48(4):761–777.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshinobu Kawahara</author>
<author>Kiyohito Nagano</author>
<author>Koji Tsuda</author>
<author>Jeff Bilmes</author>
</authors>
<title>Submodularity cuts and applications.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Society (NIPS),</booktitle>
<location>Vancouver, Canada,</location>
<marker>Kawahara, Nagano, Tsuda, Bilmes, 2009</marker>
<rawString>Yoshinobu Kawahara, Kiyohito Nagano, Koji Tsuda, and Jeff Bilmes. 2009. Submodularity cuts and applications. In Neural Information Processing Society (NIPS), Vancouver, Canada, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Khuller</author>
<author>A Moss</author>
<author>J Naor</author>
</authors>
<title>The budgeted maximum coverage problem.</title>
<date>1999</date>
<journal>Information Processing Letters,</journal>
<volume>70</volume>
<issue>1</issue>
<contexts>
<context position="11069" citStr="Khuller et al., 1999" startWordPosition="1796" endWordPosition="1799">ing we address in Theorem 3. Although similar to the MMR objective function, our approach is different since 1) ours is graph-based and 2) we formalize the problem as submodular function maximization under the budget constraint where a simple greedy algorithm can solve the problem guaranteed near-optimally. 4 Algorithms Algorithm 1 Modified greedy algorithm 1: G ← ∅ 2: U ← V 3: while U ≠ ∅ do 4: k ← arg maxℓEU f(Gu(Ce)r f(G) 5: G ← G ∪ {k} if ∑iEG ci + ck ≤ Band f(G ∪ {k}) − f(G) ≥ 0 6: U ← U \ {k} 7: end while 8: v* ← arg maxvEV,cv«3 f({v}) 9: return Gf = arg maxSE{{v*},G} f(S) Inspired by (Khuller et al., 1999), we propose Algorithm 1 to solve Eqn. (3). The algorithm sequentially finds unit k with the largest ratio of objective function gain to scaled cost, i.e., (f(G ∪ {ℓ}) − f(G))/crℓ, where r &gt; 0 is the scaling factor. If adding k increases the objective function value while not violating the budget constraint, it is then selected and otherwise bypassed. After the sequential selection, set G is compared to the within-budget singleton with the largest objective value, and the larger of the two becomes the final output. The essential aspect of a greedy algorithm is the design of the greedy heuristi</context>
<context position="16877" citStr="Khuller et al., 1999" startWordPosition="2864" endWordPosition="2867">g theorem: Theorem 3. Algorithm 1 solves the summarization problem near-optimally (i.e. Theorem 1 and Theorem 2 hold) with high probability of at least 1 − exp { —2(|V |− (α + 1)β)2µ2 + lnK} ll IV |+ (α2 − 1)β JJJ Proof. Omitted due to space limitation. 4.2 Related work Algorithms for maximizing submodular function under budget constraint (Eqn. (3)) have been studied before. Krause (2005) generalized the work by Khuller et al.(1999) on budgeted maximum cover problem to the submodular framework, and showed a 12(1 − 1/e)-approximation algorithm. The algorithm in (Krause and Guestrin, 2005) and (Khuller et al., 1999) is actually a special case of Algorithm 1 when r = 1, and Theorem 1 gives a better bound (i.e., (1 − 1/.\/e) &gt; 12(1 − 1/e)) in this case. There is also a greedy algorithm with partial enumerations (Sviridenko, 2004; Krause and Guestrin, 2005) factor (1 − 1/e). This algorithm, however, is too computationally expensive and thus not practical for real world applications (the computation cost is O(|V |5) in general). When each unit has identical cost, the budget constraint reduces to cardinality constraint where a greedy algorithm is known to be a (1−1/e)- approximation algorithm (Nemhauser et al</context>
</contexts>
<marker>Khuller, Moss, Naor, 1999</marker>
<rawString>S. Khuller, A. Moss, and J. Naor. 1999. The budgeted maximum coverage problem. Information Processing Letters, 70(1):39–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Krause</author>
<author>C Guestrin</author>
</authors>
<title>A note on the budgeted maximization of submodular functions.</title>
<date>2005</date>
<tech>Technical Rep. No. CMU-CALD-05,</tech>
<pages>103</pages>
<contexts>
<context position="16850" citStr="Krause and Guestrin, 2005" startWordPosition="2859" endWordPosition="2862">ation task. We have the following theorem: Theorem 3. Algorithm 1 solves the summarization problem near-optimally (i.e. Theorem 1 and Theorem 2 hold) with high probability of at least 1 − exp { —2(|V |− (α + 1)β)2µ2 + lnK} ll IV |+ (α2 − 1)β JJJ Proof. Omitted due to space limitation. 4.2 Related work Algorithms for maximizing submodular function under budget constraint (Eqn. (3)) have been studied before. Krause (2005) generalized the work by Khuller et al.(1999) on budgeted maximum cover problem to the submodular framework, and showed a 12(1 − 1/e)-approximation algorithm. The algorithm in (Krause and Guestrin, 2005) and (Khuller et al., 1999) is actually a special case of Algorithm 1 when r = 1, and Theorem 1 gives a better bound (i.e., (1 − 1/.\/e) &gt; 12(1 − 1/e)) in this case. There is also a greedy algorithm with partial enumerations (Sviridenko, 2004; Krause and Guestrin, 2005) factor (1 − 1/e). This algorithm, however, is too computationally expensive and thus not practical for real world applications (the computation cost is O(|V |5) in general). When each unit has identical cost, the budget constraint reduces to cardinality constraint where a greedy algorithm is known to be a (1−1/e)- approximation</context>
</contexts>
<marker>Krause, Guestrin, 2005</marker>
<rawString>A. Krause and C. Guestrin. 2005. A note on the budgeted maximization of submodular functions. Technical Rep. No. CMU-CALD-05, 103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
<author>V S Mirrokni</author>
<author>V Nagarajan</author>
<author>M Sviridenko</author>
</authors>
<title>Non-monotone submodular maximization under matroid and knapsack constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of the 41st annual ACM symposium on Symposium on theory of computing,</booktitle>
<pages>323--332</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA.</location>
<marker>Lee, Mirrokni, Nagarajan, Sviridenko, 2009</marker>
<rawString>J. Lee, V.S. Mirrokni, V. Nagarajan, and M. Sviridenko. 2009. Non-monotone submodular maximization under matroid and knapsack constraints. In Proceedings of the 41st annual ACM symposium on Symposium on theory of computing, pages 323–332. ACM New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
<author>Shasha Xie</author>
</authors>
<title>Graphbased submodular selection for extractive summarization.</title>
<date>2009</date>
<booktitle>In Proc. IEEE Automatic Speech Recognition and Understanding (ASRU),</booktitle>
<location>Merano, Italy,</location>
<contexts>
<context position="3906" citStr="Lin et al., 2009" startWordPosition="581" endWordPosition="584"> Tarau also proposed an eigenvector centrality algorithm on weighted graphs for document summarization (Mihalcea and Tarau, 2004). Mihalcea et al. later applied Google’s PageRank (Brin and Page, 1998) to natural language processing tasks ranging 912 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 912–920, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics from automatic keyphrase extraction and word sense disambiguation, to extractive summarization (Mihalcea et al., 2004; Mihalcea, 2004). Recent work in (Lin et al., 2009) presents a graph-based approach where an undirected weighted graph is built for the document to be summarized, and vertices represent the candidate sentences and edge weights represent the similarity between sentences. The summary extraction procedure is done by maximizing a submodular set function under a cardinality constraint. Inspired by (Lin et al., 2009), we perform summarization by maximizing submodular functions under a budget constraint. A budget constraint is natural in summarization task as the length of the summary is often restricted. The length (byte budget) limitation represent</context>
</contexts>
<marker>Lin, Bilmes, Xie, 2009</marker>
<rawString>Hui Lin, Jeff Bilmes, and Shasha Xie. 2009. Graphbased submodular selection for extractive summarization. In Proc. IEEE Automatic Speech Recognition and Understanding (ASRU), Merano, Italy, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop.</booktitle>
<contexts>
<context position="18530" citStr="Lin, 2004" startWordPosition="3141" endWordPosition="3142">ion. 5 Experiments We evaluated our approach on the data set of DUC’04 (2004) with the setting of task 2, which is a multi-document summarization task on English news articles. In this task, 50 document clusters are given, each of which consists of 10 documents. For each document cluster, a short multi-document summary is to be generated. The summary should not be longer than 665 bytes including spaces and punctuation, as required in the DUC’04 evaluation. We used DUC’03 as our development set. All documents were segmented into sentences using a script distributed by DUC. ROUGE version 1.5.5 (Lin, 2004), which is widely used in the study of summarization, was used to evaluate summarization performance in our experiments 1. We focus on ROUGE1 (unigram) F-measure scores since it has demonstrated strong correlation with human annotation (Lin, 2004). The basic textual/linguistic units we consider in our experiments are sentences. For each document cluster, sentences in all the documents of this cluster forms the ground set V . We built semantic graphs for each document cluster based on cosine similarity, where cosine similarity is computed based on the TF-IDF (term frequency, inverse document fr</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lovasz</author>
</authors>
<title>Submodular functions and convexity. Mathematical programming-The state of the art,(eds.</title>
<date>1983</date>
<pages>235--257</pages>
<editor>A. Bachem, M. Grotschel and B. Korte</editor>
<publisher>Springer,</publisher>
<contexts>
<context position="5520" citStr="Lovasz, 1983" startWordPosition="838" endWordPosition="839">lgorithm (Section 4) and both theoretically (Section 4.1) and empirically (Section 5.1) show that the algorithm solves the problem near-optimally, thanks to submodularity. Regarding summarization performance, experiments on DUC’04 task show that our approach is superior to the best-performing method in DUC’04 evaluation on ROUGE-1 scores (Section 5). 2 Background on Submodularity Consider a set function f : 2V —* R, which maps subsets S C V of a finite ground set V to real numbers. f(�) is called normalized if f(0) = 0, and is monotone if f(S) G f(T) whenever S C T. f(�) is called submodular (Lovasz, 1983) if for any S, T C V , we have f(S U T) + f(S n T) G f(S) + f(T). (1) An equivalent definition of submodularity is the property of diminishing returns, well-known in the field of economics. That is, f(�) is submodular if for any R C S C V and s E V \ S, f(S U {s}) − f(S) G f(R U {s}) − f(R). (2) Eqn. 2 states that the “value” of s never increases in the contexts of ever larger sets, exactly the property of diminishing returns. This phenomenon arises naturally in many other contexts as well. For example, the Shannon entropy function is submodular in the set of random variables. Submodularity, m</context>
</contexts>
<marker>Lovasz, 1983</marker>
<rawString>L. Lovasz. 1983. Submodular functions and convexity. Mathematical programming-The state of the art,(eds. A. Bachem, M. Grotschel and B. Korte) Springer, pages 235–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>4425--557</pages>
<contexts>
<context position="1907" citStr="McDonald, 2007" startWordPosition="280" endWordPosition="282">ed using either extraction or abstraction, the former selects only sentences from the original document set, whereas the latter involves natural language generation. In this paper, we address the problem of generic extractive summaries from clusters of related documents, commonly known as multi-document summarization. In extractive text summarization, textual units (e.g., sentences) from a document set are extracted to form a summary, where grammaticality is assured at the local level. Finding the optimal summary can be viewed as a combinatorial optimization problem which is NP-hard to solve (McDonald, 2007). One of the standard methods for this problem is called Maximum Marginal Relevance (MMR) (Dang, 2005)(Carbonell and Goldstein, 1998), where a greedy algorithm selects the most relevant sentences, and at the same time avoids redundancy by removing sentences that are too similar to already selected ones. One major problem of MMR is that it is non-optimal because the decision is made based on the scores at the current iteration. McDonald (2007) proposed to replace the greedy search of MMR with a globally optimal formulation, where the basic MMR framework can be expressed as a knapsack packing pr</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>R. McDonald. 2007. A study of global inference algorithms in multi-document summarization. Lecture Notes in Computer Science, 4425:557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>TextRank: bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="3418" citStr="Mihalcea and Tarau, 2004" startWordPosition="511" endWordPosition="514"> In this paper, we study graph-based approaches for multi-document summarization. Indeed, several graph-based methods have been proposed for extractive summarization in the past. Erkan and Radev (2004) introduced a stochastic graph-based method, LexRank, for computing the relative importance of textual units for multi-document summarization. In LexRank the importance of sentences is computed based on the concept of eigenvector centrality in the graph representation of sentences. Mihalcea and Tarau also proposed an eigenvector centrality algorithm on weighted graphs for document summarization (Mihalcea and Tarau, 2004). Mihalcea et al. later applied Google’s PageRank (Brin and Page, 1998) to natural language processing tasks ranging 912 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 912–920, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics from automatic keyphrase extraction and word sense disambiguation, to extractive summarization (Mihalcea et al., 2004; Mihalcea, 2004). Recent work in (Lin et al., 2009) presents a graph-based approach where an undirected weighted graph is built for the document to be summarized, </context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>R. Mihalcea and P. Tarau. 2004. TextRank: bringing order into texts. In Proceedings of EMNLP, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
<author>E Figa</author>
</authors>
<title>PageRank on semantic networks, with application to word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING04).</booktitle>
<contexts>
<context position="3854" citStr="Mihalcea et al., 2004" startWordPosition="571" endWordPosition="575">y in the graph representation of sentences. Mihalcea and Tarau also proposed an eigenvector centrality algorithm on weighted graphs for document summarization (Mihalcea and Tarau, 2004). Mihalcea et al. later applied Google’s PageRank (Brin and Page, 1998) to natural language processing tasks ranging 912 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 912–920, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics from automatic keyphrase extraction and word sense disambiguation, to extractive summarization (Mihalcea et al., 2004; Mihalcea, 2004). Recent work in (Lin et al., 2009) presents a graph-based approach where an undirected weighted graph is built for the document to be summarized, and vertices represent the candidate sentences and edge weights represent the similarity between sentences. The summary extraction procedure is done by maximizing a submodular set function under a cardinality constraint. Inspired by (Lin et al., 2009), we perform summarization by maximizing submodular functions under a budget constraint. A budget constraint is natural in summarization task as the length of the summary is often restr</context>
<context position="26304" citStr="Mihalcea et al., 2004" startWordPosition="4529" endWordPosition="4532"> the gain of adding k to S, i.e., f(S U {k}) − f(S). Lemma 1. VX, Y C V, 35.0% 34.5% ROUGE-1 F-measure 34.0% 33.5% 33.0% 32.5% 32.0% r=0 r=0.3 r=0.5 r=0.7 r=1 0 5 10 15 Figure 2: Different combinations of r and λ for fMMR � ρk(Y ). (9) related to ROUGE-1 score on DUC’03 task 1. f(X) &lt; f(Y ) + k∈X\Y known graph-based approaches, LexRank and PageRank. LexRank was one of the participating system in DUC’04, with peer code 104. For PageRank, we implemented the recursive graphbased ranking algorithm ourselves. The importance of sentences was estimated in an iterative way as in (Brin and Page, 1998)(Mihalcea et al., 2004). Sentences were then selected based on their importance rankings until the budget constraint was violated. The graphs used for PageRank were exactly the graphs in our submodular approaches (i.e., an undirected graph). In both cases, submodular summarization achieves better ROUGE-1 scores. The improvement is statistically significant by the Proof. See (Nemhauser et al., 1978) Lemma 2. For i = 1, ... , |G|, when 0 &lt; r &lt; 1, 1?T I CY�k I1-T f (S∗) − f (Gi−1) &lt; 13 S (f(Gi) − f(Gi−1)), cr vi and when r &gt; 1, ( &apos; )r f(S∗) − f(Gi−1) &lt; (f(Gi) − f(Gi−1)) cvi Proof. Based on line 4 of Algorithm 1, we hav</context>
</contexts>
<marker>Mihalcea, Tarau, Figa, 2004</marker>
<rawString>R. Mihalcea, P. Tarau, and E. Figa. 2004. PageRank on semantic networks, with application to word sense disambiguation. In Proceedings of the 20th International Conference on Computational Linguistics (COLING04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Graph-based ranking algorithms for sentence extraction, applied to text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL</booktitle>
<publisher>Mosek.</publisher>
<contexts>
<context position="3871" citStr="Mihalcea, 2004" startWordPosition="576" endWordPosition="577">tation of sentences. Mihalcea and Tarau also proposed an eigenvector centrality algorithm on weighted graphs for document summarization (Mihalcea and Tarau, 2004). Mihalcea et al. later applied Google’s PageRank (Brin and Page, 1998) to natural language processing tasks ranging 912 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 912–920, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics from automatic keyphrase extraction and word sense disambiguation, to extractive summarization (Mihalcea et al., 2004; Mihalcea, 2004). Recent work in (Lin et al., 2009) presents a graph-based approach where an undirected weighted graph is built for the document to be summarized, and vertices represent the candidate sentences and edge weights represent the similarity between sentences. The summary extraction procedure is done by maximizing a submodular set function under a cardinality constraint. Inspired by (Lin et al., 2009), we perform summarization by maximizing submodular functions under a budget constraint. A budget constraint is natural in summarization task as the length of the summary is often restricted. The length</context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>R. Mihalcea. 2004. Graph-based ranking algorithms for sentence extraction, applied to text summarization. In Proceedings of the ACL 2004 (companion volume). 2006. Mosek.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G L Nemhauser</author>
<author>L A Wolsey</author>
<author>M L Fisher</author>
</authors>
<title>An analysis of approximations for maximizing submodular set functions I.</title>
<date>1978</date>
<booktitle>Mathematical Programming,</booktitle>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="6931" citStr="Nemhauser et al., 1978" startWordPosition="1083" endWordPosition="1086">imization. Many combinatorial optimization problems can be solved optimally or near-optimally in polynomial time only when the underlying function is submodular. It has been shown that any submodular function can be minimized in polynomial time (Schrijver, 2000)(Iwata et al., 2001). Maximization of submodular functions, however, is an NP-complete optimization problem but fortunately, some submodular maximization problems can be solved nearoptimally. A famous result is that the maximization of a monotone submodular function under a cardinality constraint can be solved using a greedy algorithm (Nemhauser et al., 1978) within a constant factor (0.63) of being optimal. A constant-factor approximation algorithm has also been obtained for maximizing monotone submodular function with a knapsack constraint (see Section 4.2). Feige et.al. (2007) studied unconstrained maximization of a arbitrary submodular functions (not necessarily monotone). Kawahara et.al. (2009) proposed a cuttingplane method for optimally maximizing a submodular set function under a cardinality constraint, and Lee et.al. (2009) studied non-monotone submodular maximization under matroid and knapsack constraints. 3 Problem Setup In this paper, </context>
<context position="17485" citStr="Nemhauser et al., 1978" startWordPosition="2967" endWordPosition="2970">r et al., 1999) is actually a special case of Algorithm 1 when r = 1, and Theorem 1 gives a better bound (i.e., (1 − 1/.\/e) &gt; 12(1 − 1/e)) in this case. There is also a greedy algorithm with partial enumerations (Sviridenko, 2004; Krause and Guestrin, 2005) factor (1 − 1/e). This algorithm, however, is too computationally expensive and thus not practical for real world applications (the computation cost is O(|V |5) in general). When each unit has identical cost, the budget constraint reduces to cardinality constraint where a greedy algorithm is known to be a (1−1/e)- approximation algorithm (Nemhauser et al., 1978) which is the best that can be achieved in polynomial time (Feige, 1998) if P =� NP. Recent work (Takamura and Okumura, 2009) applied the maximum coverage problem to text summarization (without apparently being aware that their objective is submodular) and studied a similar algorithm to ours when r = 1 and for the non-penalized graph-cut function. This problem, however, is a special case of constrained submodular function maximization. 5 Experiments We evaluated our approach on the data set of DUC’04 (2004) with the setting of task 2, which is a multi-document summarization task on English new</context>
<context position="26682" citStr="Nemhauser et al., 1978" startWordPosition="4587" endWordPosition="4590">rticipating system in DUC’04, with peer code 104. For PageRank, we implemented the recursive graphbased ranking algorithm ourselves. The importance of sentences was estimated in an iterative way as in (Brin and Page, 1998)(Mihalcea et al., 2004). Sentences were then selected based on their importance rankings until the budget constraint was violated. The graphs used for PageRank were exactly the graphs in our submodular approaches (i.e., an undirected graph). In both cases, submodular summarization achieves better ROUGE-1 scores. The improvement is statistically significant by the Proof. See (Nemhauser et al., 1978) Lemma 2. For i = 1, ... , |G|, when 0 &lt; r &lt; 1, 1?T I CY�k I1-T f (S∗) − f (Gi−1) &lt; 13 S (f(Gi) − f(Gi−1)), cr vi and when r &gt; 1, ( &apos; )r f(S∗) − f(Gi−1) &lt; (f(Gi) − f(Gi−1)) cvi Proof. Based on line 4 of Algorithm 1, we have Vu E S∗ \ Gi−1, ρu(Gi−1) cru ρvi(Gi−1) &lt; r cvi . 918 Thus when 0 ≤ r ≤ 1, ∑ ρu(Gi−1) ≤ ρvi(Gi−1) cr cr u vi u∈S∗\Gi−1 (∑ ≤ ρvi(Gi−1) u∈S∗\Gi−1 cu |S∗ \ Gi−1| cr |S∗ \ Gi−1| vi  ≤ ρvi(Gi−1)  ∑ |S∗|1−r cu cr vi u∈S∗\Gi−1 ≤ ρvi(Gi−1) |S∗|1−rBr, cr vi where the second inequality is due to the concavity of g(x) = xr, x &gt; 0, 0 ≤ r ≤ 1. The last inequality uses the fact that ∑u∈</context>
</contexts>
<marker>Nemhauser, Wolsey, Fisher, 1978</marker>
<rawString>G.L. Nemhauser, L.A. Wolsey, and M.L. Fisher. 1978. An analysis of approximations for maximizing submodular set functions I. Mathematical Programming, 14(1):265–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Schrijver</author>
</authors>
<title>A combinatorial algorithm minimizing submodular functions in strongly polynomial time.</title>
<date>2000</date>
<journal>Journal of Combinatorial Theory, Series B,</journal>
<volume>80</volume>
<issue>2</issue>
<contexts>
<context position="6570" citStr="Schrijver, 2000" startWordPosition="1028" endWordPosition="1030">s phenomenon arises naturally in many other contexts as well. For example, the Shannon entropy function is submodular in the set of random variables. Submodularity, moreover, is a discrete analog of convexity (Lovasz, 1983). As convexity makes continuous functions more amenable to optimization, submodularity plays an essential role in combinatorial optimization. Many combinatorial optimization problems can be solved optimally or near-optimally in polynomial time only when the underlying function is submodular. It has been shown that any submodular function can be minimized in polynomial time (Schrijver, 2000)(Iwata et al., 2001). Maximization of submodular functions, however, is an NP-complete optimization problem but fortunately, some submodular maximization problems can be solved nearoptimally. A famous result is that the maximization of a monotone submodular function under a cardinality constraint can be solved using a greedy algorithm (Nemhauser et al., 1978) within a constant factor (0.63) of being optimal. A constant-factor approximation algorithm has also been obtained for maximizing monotone submodular function with a knapsack constraint (see Section 4.2). Feige et.al. (2007) studied uncon</context>
</contexts>
<marker>Schrijver, 2000</marker>
<rawString>A. Schrijver. 2000. A combinatorial algorithm minimizing submodular functions in strongly polynomial time. Journal of Combinatorial Theory, Series B, 80(2):346–355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sviridenko</author>
</authors>
<title>A note on maximizing a submodular set function subject to a knapsack constraint.</title>
<date>2004</date>
<journal>Operations Research Letters,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="17092" citStr="Sviridenko, 2004" startWordPosition="2907" endWordPosition="2908"> Omitted due to space limitation. 4.2 Related work Algorithms for maximizing submodular function under budget constraint (Eqn. (3)) have been studied before. Krause (2005) generalized the work by Khuller et al.(1999) on budgeted maximum cover problem to the submodular framework, and showed a 12(1 − 1/e)-approximation algorithm. The algorithm in (Krause and Guestrin, 2005) and (Khuller et al., 1999) is actually a special case of Algorithm 1 when r = 1, and Theorem 1 gives a better bound (i.e., (1 − 1/.\/e) &gt; 12(1 − 1/e)) in this case. There is also a greedy algorithm with partial enumerations (Sviridenko, 2004; Krause and Guestrin, 2005) factor (1 − 1/e). This algorithm, however, is too computationally expensive and thus not practical for real world applications (the computation cost is O(|V |5) in general). When each unit has identical cost, the budget constraint reduces to cardinality constraint where a greedy algorithm is known to be a (1−1/e)- approximation algorithm (Nemhauser et al., 1978) which is the best that can be achieved in polynomial time (Feige, 1998) if P =� NP. Recent work (Takamura and Okumura, 2009) applied the maximum coverage problem to text summarization (without apparently be</context>
</contexts>
<marker>Sviridenko, 2004</marker>
<rawString>M. Sviridenko. 2004. A note on maximizing a submodular set function subject to a knapsack constraint. Operations Research Letters, 32(1):41–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Takamura</author>
<author>M Okumura</author>
</authors>
<title>Text summarization model based on maximum coverage problem and its variant.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>781--789</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17610" citStr="Takamura and Okumura, 2009" startWordPosition="2991" endWordPosition="2994">/e) &gt; 12(1 − 1/e)) in this case. There is also a greedy algorithm with partial enumerations (Sviridenko, 2004; Krause and Guestrin, 2005) factor (1 − 1/e). This algorithm, however, is too computationally expensive and thus not practical for real world applications (the computation cost is O(|V |5) in general). When each unit has identical cost, the budget constraint reduces to cardinality constraint where a greedy algorithm is known to be a (1−1/e)- approximation algorithm (Nemhauser et al., 1978) which is the best that can be achieved in polynomial time (Feige, 1998) if P =� NP. Recent work (Takamura and Okumura, 2009) applied the maximum coverage problem to text summarization (without apparently being aware that their objective is submodular) and studied a similar algorithm to ours when r = 1 and for the non-penalized graph-cut function. This problem, however, is a special case of constrained submodular function maximization. 5 Experiments We evaluated our approach on the data set of DUC’04 (2004) with the setting of task 2, which is a multi-document summarization task on English news articles. In this task, 50 document clusters are given, each of which consists of 10 documents. For each document cluster, </context>
</contexts>
<marker>Takamura, Okumura, 2009</marker>
<rawString>H. Takamura and M. Okumura. 2009. Text summarization model based on maximum coverage problem and its variant. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 781–789. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>