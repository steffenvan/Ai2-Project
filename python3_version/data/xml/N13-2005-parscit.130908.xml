<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009851">
<title confidence="0.9979975">
From Language to Family and Back: Native Language and Language
Family Identification from English Text
</title>
<author confidence="0.988383">
Ariel Stolerman Aylin Caliskan Islam Rachel Greenstadt
</author>
<affiliation confidence="0.99427">
Dept. of Computer Science
Drexel University
</affiliation>
<address confidence="0.800332">
Philadelphia, PA
</address>
<email confidence="0.999635">
{ams573,ac993,greenie}@cs.drexel.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999502764705883">
Revealing an anonymous author’s traits from
text is a well-researched area. In this paper we
aim to identify the native language and lan-
guage family of a non-native English author,
given his/her English writings. We extract fea-
tures from the text based on prior work, and
extend or modify it to construct different fea-
ture sets, and use support vector machines for
classification. We show that native language
identification accuracy can be improved by up
to 6.43% for a 9-class task, depending on the
feature set, by introducing a novel method to
incorporate language family information. In
addition we show that introducing grammar-
based features improves accuracy of both na-
tive language and language family identifica-
tion.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999988924528302">
Mining text for features to infer characteristics on
its author is an important research field. One au-
thor property that has been researched is native lan-
guage, extracted from the author’s writing in a non-
native language. Learning the native language of an
anonymous author can assist in profiling criminals
or terrorists, and may also undermine the privacy of
legitimate anonymous authors by helping to unveil
their identity.
Influences of native language (L1) on second lan-
guage (L2), referred as the L1-L2 transfer effect, is
seen in writing and can be utilized to identify na-
tive language. In this paper we examine aspects of
a broader class – the language family to which the
native language of an author belongs. In the rest of
the paper native language and native language fam-
ily will be referred as L1 and LF, respectively.
First, we examine the correct classification rates
of LF compared to L1. As L1 is a subset of LF,
the number of L1 classes is greater than or equal to
the number of corresponding LF classes. Therefore,
higher LF classification accuracy can be achieved
trivially by taking the family of the attributed L1
in a L1 classification task. This can be helpful in
cases where high accuracy is preferred over reso-
lution. We introduce a novel, improved method
that achieves higher correct classification rate for LF
identification, compared to the trivial method.
Our main contribution is showing that L1 identi-
fication accuracy can be increased by incorporating
family information via LF identification.
We use stylometric analysis and machine learn-
ing techniques to identify L1 and LF. We conduct a
series of experiments by mining English text written
by non-native English authors for linguistic features.
We use 4 different feature sets detailed in section 3.
We evaluate the accuracy of our results by examin-
ing the true-positive rate.
The novelty of our work is in exploring the LF-
L2 transfer effect using stylometric methods, and
expanding L1 identification methods accordingly.
Increasing the state-of-the-art correct classification
rate for L1 detection is not our main goal. Instead,
we introduce concepts to increase achieved accuracy
by incorporating LF knowledge into the classifica-
tion process.
The next section (2) provides background and
prior work. Section 3 describes the experimental
setup. In section 4 we describe the different experi-
ments that were performed, followed by results and
evaluation. We finalize with discussion on the given
results (section 5), followed by conclusions and di-
rections for future research (section 6).
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99983725">
Literature includes work on extracting demographic
and psychological traits from different data formats,
such as speech and text samples. Native language
and accent identification from speech can be found
</bodyText>
<page confidence="0.97358">
32
</page>
<note confidence="0.9105155">
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 32–39,
Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999421254545455">
in (Choueiter et al., 2008; Tomokiyo and Jones,
2001). Identifying an author’s native language from
L2 text, which is English in most cases, is the closest
problem to our work.
Introductory studies in the area identified the writ-
ten or spoken language itself, focusing on telephone
dialogue corpora (Ahmed et al., 2004; Zissman,
1993). Further studies focused on extracting specific
information from text or speech after identifying the
language being used. Wanneroy et al. (1999) inves-
tigated how non-native speech deteriorated language
identification and used acoustic adaptation to im-
prove it. Choueiter et al. (2008) classified different
foreign accented English speech samples by using a
combination of heteroscedastic LDA and maximum
mutual information training. Tomokiyo and Jones
(2001) characterized part-of-speech sequences and
showed that Naive Bayes classification can be used
to identify non-native utterances of English.
The first work that utilized stylometric methods
for native language attribution is introduced by Kop-
pel et al. (2005a; 2005b). They explored frequencies
of sets of features, and used them with multi-linear
support vector machines to classify text by author’s
native language. They used a set of features con-
sisted of function words, letter n-grams, errors and
idiosyncrasies, and experimented on a dataset of au-
thors of five different native languages taken from
ICLEv1 (Granger et al., 2002), reaching to 80.2%
accuracy. Tsur and Rappoport (2007) revisited Kop-
pel’s work using only the 200 most frequent charac-
ter bigrams, and achieved 65.6% accuracy, with only
a small degradation when removing dominant words
or function words.
Brooke and Hirst (2012) presented a method of
utilizing native language corpora for identifying na-
tive language in non-native texts. They used word-
by-word translation of large native language corpora
to create sets of second language forms that are pos-
sible results of language transfer, later used in unsu-
pervised classification. They achieved results above
random chance for L1 identification, however insuf-
ficiently accurate.
More related work can be found in (Estival et
al., 2007; van Halteren, 2008; Carrio-Pastor, 2009;
Golcher and Reznicek, 2009; Wong and Dras, 2009;
Wong et al., 2011; Brooke and Hirst, 2011; Ahn,
2011). The work mentioned above and our approach
both utilize the L1-L2 transfer effect to gain infor-
mation about an author’s native language. Gibbons
(2009) proved the impact of native language fam-
ily’s typological properties on L2. As far as we
know, our work is the first to combine stylometry
and native language family’s effect on L2, utilized
for L1 identification.
</bodyText>
<sectionHeader confidence="0.988909" genericHeader="method">
3 Experimental Setting
</sectionHeader>
<subsectionHeader confidence="0.998307">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.99998796875">
We use the ICLEv2 (Granger et al., 2009) corpus
that contains English documents written by interme-
diate to advanced international learners of English,
with language backgrounds of 16 mother-tongues.
The first version of the corpus was used in signif-
icant previous work (Koppel et al., 2005a; Koppel
et al., 2005b; Tsur and Rappoport, 2007). They re-
ported that they were able to use 258 documents of
sizes 500-1000 words for each language they used.
We use version 2 of the corpus and restrict all doc-
uments in our experiments to those with 500-1000
words as well. However, we found that constraining
our documents to these lengths allows us to use only
133-146 documents per language. We conduct a se-
ries of experiments with different sub-corpora con-
structed of documents representing 11 native lan-
guages out of the 16 available in the corpus. The
native languages we used are: Bulgarian, Czech,
Dutch, French, German, Italian, Norwegian, Polish,
Russian, Spanish and Swedish, all Indo-European
languages. These languages represent 3 language-
families in a coarse partition: Germanic, Slavic and
Romance, which are used as the LF class in the ex-
periments to follow. All sub-corpora configurations
are detailed in section 4.
Since we are looking at a set of languages from
both L1 and LF aspects, we maintained only the
sub-corpora that allowed a sufficient amount of lan-
guages in each represented family, i.e. 3 languages
in each of the Germanic, Slavic and Romance fam-
ilies. Therefore we removed 5 of the 16 available
languages in the corpus.
</bodyText>
<subsectionHeader confidence="0.999772">
3.2 Feature Selection
</subsectionHeader>
<bodyText confidence="0.999737769230769">
Koppel et al. represented each document in their ex-
periment as a 1,035-dimensional feature vector: 400
function words, 200 most frequent letter n-grams,
185 misspellings and syntactic errors and 250 rare
POS bigrams. The 250 rare POS bigrams are the
least common bigrams extracted from the Brown
Corpus (Francis and Kucera, 1983), and their ap-
pearances are considered to be erroneous or non-
standard.
In our experiments we used 4 different feature
sets, partially based on that used by Koppel et al. We
used the authorship attribution tool JStylo (McDon-
ald et al., 2012) for feature extraction. The feature
</bodyText>
<page confidence="0.995193">
33
</page>
<bodyText confidence="0.999908886363636">
sets are the following:
Basic: includes the 400 most frequent function
words, 200 most frequent letter bigrams, 250 rare
POS bigrams and 300 most frequent spelling errors.
The 400 most frequent function words were taken
from a list of 512 function words used in the orig-
inal experiments by Koppel et al. For the 200 let-
ter n-grams, we chose bigrams, as they are shown
to be effective for the task in previous research.
The 250 rare POS bigrams were extracted from the
Brown Corpus using the POS tagger in (Toutanova
et al., 2003). Finally, we simplified the error types
by considering only misspelled words, based on
a list of 5,753 common misspellings, constructed
from Wikipedia common misspellings and those
used in (Abbasi and Chen, 2008). We ignored any
misspellings with 0-1 appearances across the entire
sub-corpus. Since many of the rare POS bigrams
and misspellings had no appearances, the effective
vector lengths vary between 653-870 features.
Extended: identical to the former, with the addi-
tion of the 200 most frequent POS bigrams across
the entire sub-corpus used for each experiment.
These syntactic features were selected as an addi-
tional representation of grammatical structures in
the text.
There are several methods for natural language
classification, including genetic, typological and
areal (Campbell and Poser, 2008). We consider
the typological classification that uses structural fea-
tures to compare similarities between languages and
classify them into families. Therefore we chose
grammatical evidence in L2 as features that may rep-
resent similar transfer effects among languages in
the same family.
Grammatical: constructed only from the 200
most frequent POS bigrams, representing the gram-
matical level of the text.
InfoGain: We used the 200 features with the high-
est information gain extracted from the extended
feature set using Weka (Hall et al., 2009), calculated
for any given feature by measuring the expected re-
duction in entropy caused by partitioning the test in-
stances according to that feature.
</bodyText>
<subsectionHeader confidence="0.997886">
3.3 Classification
</subsectionHeader>
<bodyText confidence="0.9999715">
We trained a SMO (Platt, 1998) SVM classifier with
polynomial kernel, chosen as SVMs are used exten-
sively in prior work and ours outperformed other
methods tested, including decision trees, nearest-
neighbors, Bayesian and logistic regression classi-
fiers.
</bodyText>
<sectionHeader confidence="0.971771" genericHeader="method">
4 Experimental Variations and Evaluation
</sectionHeader>
<bodyText confidence="0.999971714285714">
We conducted 3 different experiments using vari-
ous sub-corpora and the 4 feature sets described in
the previous sections, with L1 and LF classification
tasks. We evaluated the results by using the true-
positive rate to capture accuracy. Following is a de-
tailed description of the different variations and re-
sults.
</bodyText>
<subsectionHeader confidence="0.998997">
4.1 9-Class Languages, 3-Class Families
</subsectionHeader>
<bodyText confidence="0.999678285714286">
Setup: We compared 9-L1 identification with the
corresponding 3-LF identification, using datasets
constructed of the sub-corpus containing all 11 lan-
guages mentioned before. For the 9-L1 task we
randomly sampled documents of 9 languages, 3 for
each of the Germanic, Slavic and Romance language
families, in order to maintain the same number of
languages per family in every experiment. We con-
structed 16 different 9-L1 sets, choosing 3 out of 4
Germanic languages, 3 out of 4 Slavic languages and
the only 3 Romance languages available. In each of
the 16 experiments we used the same number of doc-
uments per language, varying between 133-146.
In order to compare results with LF identification,
we conducted 3 sets of experiments, each containing
16 3-LF experiments, corresponding to the 16 that
were performed for L1 identification.
First, we ran the trivial experiment of attributing
the family of the predicted language resulted from
the L1 identification experiments. This method is
denoted as the trivial method.
Next, we ran the same experiments conducted for
L1, with the only difference of using LF as the class
rather than L1. As a result of that configuration,
each experiment also contained the same number
of documents per language family, varying between
399-438. This method is denoted as the standalone
method (as it is a standalone experiment, indepen-
dent of L1 classification results).
Lastly, we ran experiments combining the stan-
dalone and trivial approaches. We hypothesize that
if L1 is attributed with high confidence, so is the LF
of that attributed L1, however if the confidence level
decreases, a standalone LF experiment achieves bet-
ter results. We ran the L1 identification experiments
and set a threshold as the averaged probability of the
predicted class across the entire test set, based on the
class probability distribution outputted by the SVM
classifier. To obtain proper probability estimates,
we fit logistic regression models to the outputs of
the SVM. Every instance classified with probability
above the threshold was attributed the family using
</bodyText>
<page confidence="0.995441">
34
</page>
<bodyText confidence="0.926023666666667">
the trivial method, and every instance below – using
the standalone method. This method is denoted as
the combined method.
Results: We averaged the results of all 16 L1 iden-
tification experiments, and those of the 3 sets of 16
LF identification experiments. See figure 1.
</bodyText>
<table confidence="0.916022875">
100 L1 (9)
90 LF trivial (3)
80 LF standalone (3)
70 LF combined (3)
60 L1 Random
50 LF Random
40
30
</table>
<figure confidence="0.74176">
20
10
0
Basic Extended Grammar InfoGain
</figure>
<figureCaption confidence="0.999218333333333">
Figure 1: Accuracy for 9-class L1 and 3-class LF iden-
tification. The combined method for LF outperforms the
other two.
</figureCaption>
<bodyText confidence="0.999965823529412">
The accuracy for L1 identification was 67.78%,
65.64%, 59.34% and 44.02% for the extended, ba-
sic, InfoGain and grammatical feature sets, respec-
tively.
Out of the 3 LF identification experiment sets,
the combined method achieved the best accuracy:
90.57%, 86.24%, 86.2% and 85.29% for the Info-
Gain, grammatical, extended and basic feature sets,
respectively. These results support our hypothesis.
The trivial method achieved better results than the
standalone method for the basic and extended fea-
ture sets: 78.33% and 79.87% for the first, 74.53%
and 77.24% for the latter. For the grammatical and
InfoGain feature sets, the standalone performed bet-
ter than the trivial: 63.61% and 76.02% for the first,
63.1% and 73.94% for the latter.
Since the L1 identification experiments have more
classes than the LF experiments, the random chance
varies between them: 11.11% for L1 and 33.33%
for LF. Although the absolute accuracy for LF is
consistently higher than for L1, if we subtract the
corresponding random chance values to obtain “ef-
fective” accuracy, in most cases L1 is more accu-
rate than LF. The LF combined method is the only
one out of the 3 LF methods that exceeds the effec-
tive accuracy of L1, for the grammatical and Info-
Gain feature sets. Combined with the standard (non-
effective) results, it appears that the InfoGain feature
set with the LF combined method achieves the high-
est accuracy with the most added knowledge over
random classification, across all tasks and feature
sets. It is also notable that the smallest difference
between L1 and LF identification accuracy is seen
for the grammatical feature set. See figure 2.
</bodyText>
<figure confidence="0.473258">
100 L1 (9)
90 LF trivial (3)
80 LF standalone (3)
70 LF combined (3)
60
50
40
30
20
10
0
Basic Extended Grammar InfoGain
</figure>
<figureCaption confidence="0.897869">
Figure 2: Effective accuracy for 9-L1 and 3-LF identifi-
cation. Accuracy for L1 exceeds most accuracy results
for LF, except for the combined method on the grammat-
ical and InfoGain feature sets.
</figureCaption>
<subsectionHeader confidence="0.992385">
4.2 3-Class Languages, 3-Class Families
</subsectionHeader>
<bodyText confidence="0.999697133333333">
Setup: In order to have the same random-chance
baseline for both L1 and LF tasks, we compared
3-L1 with 3-LF identification, using the same sub-
corpus as before.
For L1 we constructed 9 experiments, in each ran-
domly sampling 3 languages from 1, 2 and 3 differ-
ent language families (3 experiments each). The rea-
son for this choice is that as more families are used,
the farther the chosen languages are from one an-
other. Therefore the choice above is intended to bal-
ance the effect of LF in those experiments. We used
133 documents per language for all experiments.
For LF we constructed 2 sets of 9 experiments,
in order to examine the notion that languages in the
same family have more family-distinguishable com-
monalities as opposed to random sets of languages.
In the first, for each of the experiments we randomly
created 3 sets of languages to be considered as fam-
ilies. We randomly sampled documents from all
11 languages to construct sets for the 3 randomly-
generated families used as classes. Here we also
maintained 133 documents per language family. In
the second we ran a similar configuration, only using
the actual language families.
Results: The averaged accuracy for L1 was 84.23%,
82.29%, 81.67% and 66.97% for the extended, In-
foGain, basic and grammatical feature sets, respec-
tively. These results consistently outperformed the
results of both sets of LF experiments. See figure 3.
The accuracy attained for actual language families
</bodyText>
<page confidence="0.990687">
35
</page>
<figure confidence="0.996606083333333">
100 L1 (3)
90 LF (3)
80 Random LF (3)
70 Random
60
50
40
30
20
10
0
Basic Extended Grammar InfoGain
</figure>
<figureCaption confidence="0.975077">
Figure 3: Accuracy for 3-L1, 3-LF and 3-randomly-
generated families identification. Using the original fam-
ilies achieves the highest accuracy for LF identification.
</figureCaption>
<bodyText confidence="0.9996694">
was 72.43%, 70.09%, 68.72% and 56.55% for the
extended, basic, InfoGain and grammatical feature
sets, respectively, which consistently outperformed
that of the randomly-generated families: 61.46%,
60.01%, 58.81% and 48.67%. This shows that par-
titioning the languages into sets by their actual fam-
ily achieves the highest accuracy for LF identifica-
tion. As in the previous experiment, the difference
in accuracy between L1 and LF identification was
the smallest with the grammatical feature set.
</bodyText>
<subsectionHeader confidence="0.997866">
4.3 9-Class Languages, Reclassify by Family
</subsectionHeader>
<bodyText confidence="0.994387194444444">
Setup: We wanted to examine whether LF classifi-
cation can improve L1 classification. In this exper-
iment we conducted the same 16 9-L1 experiments
from section 4.1. We then set a threshold as in the
combined method in section 4.1, such that each clas-
sified instance with predicted probability less than
that threshold is treated as misclassified. For all
allegedly-misclassified instances we attributed the
family they belong to, using various methods de-
tailed later. As last step we reclassified those in-
stances using a training set constructed only of the
3 languages in the family they were classified as,
and considered these results as L1 classification-
correction for those instances. We measured the
overall change in accuracy.
The entire 16 10-fold cross-validation experi-
ments were conducted 3 times, each with a different
method for LF attribution for the instances below the
threshold: 1) The standalone method – running LF
identification task over all those instances, using the
same training set (with families as classes rather than
languages), 2) The trivial method – using the family
of the predicted language of those instances, and 3)
Random – randomly selecting the family.
Results: We averaged the results of all 16 L1 exper-
iments for each of the 3 LF attribution methods and
each of the 4 feature sets used.
We measured the net fix in accuracy (added num-
ber of correctly classified instances, taking into ac-
count corrected classifications and new misclassifi-
cations). For all feature sets, LF attribution using
the standalone method yielded the highest fix rate,
followed by LF attribution using the trivial method.
The randomly attributed family method consistently
yielded negative fix rate (i.e. reduced overall accu-
racy). See figure 4.
</bodyText>
<figure confidence="0.961033333333333">
100 L1 w/o fix
90 LF standalone
80 LF trivial
70 Random LF
60
50
40
30
20
10
0
Basic Extended Grammar InfoGain
</figure>
<figureCaption confidence="0.9938934">
Figure 4: Accuracy for L1 identification without fix
and with fixing using LF attribution by the standalone
method, trivial method and random selection of family.
The standalone method yields the highest net fix in L1
classification accuracy.
</figureCaption>
<bodyText confidence="0.9999771875">
The extended feature set yielded the best results.
Starting at a baseline of 67.17% for L1 identifica-
tion without any fix, the true-positive rates obtained
for this feature set were 70.9% and 68.05% for at-
tributing LF by the standalone and the trivial meth-
ods, respectively. The increase in accuracy is statis-
tically significant (p &lt; 0.01). The random family
attribution method yielded a decrease in accuracy to
66.35%.
It is notable that although yielding best results
for the extended feature set, the standalone method
achieved higher increase in accuracy in some of the
other feature sets. The increase rates for this method
were: 6.43%, 4.48%, 3.73% and 3.67% for the Info-
Gain, grammatical, extended and basic feature sets,
respectively.
</bodyText>
<sectionHeader confidence="0.99907" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999647">
The first notable result is seen in experiment 4.1,
where using the combined method for LF identifi-
cation derives higher accuracy than both the trivial
and the standalone methods. This may suggest that
when L1 is predicted with high confidence, LF is
</bodyText>
<page confidence="0.995809">
36
</page>
<bodyText confidence="0.999990349206349">
predicted well, but when the confidence level is low,
it is better to run standalone LF classification. Since
the combined method uses the best of the two others,
it outperforms both.
The most important result is seen in experiment
4.3, where L1 identification is improved by up to
6.43% in accuracy for 9-L1 classification by in-
troducing information about the language family,
thus providing a smaller set of language classes in
which the actual language is more likely to be found.
Attributing LF by standalone experiments yielded
higher L1 classification accuracy than attributing it
by the family of the predicted language. This out-
come seemingly contradicts the results seen in sec-
tion 4.1, where the latter LF attribution method out-
performed the first. However, this only supports the
idea suggested above regarding the threshold, that
the family of the attributed L1 is the actual family
with higher probability than LF attributed by a stan-
dalone experiment, only when L1 is attributed with
high confidence (i.e. above the selected threshold).
The results in sections 4.1 and 4.2 suggest that
all 4 feature sets achieve better accuracy for L1 than
for LF (standalone) classification. We believe this is
since for L1 we try to distinguish between individual
languages as they transfer to English. However, LF
identification necessitates finding features that inter-
sect between languages in a particular family, and
distinguish well between different families as they
are transferred to English. This makes LF identifi-
cation a more difficult task.
The results obtained for randomly generated fam-
ilies in sections 4.2 and 4.3, which are consistently
lower than using the actual families, suggest that the
contribution of using the latter yields the best perfor-
mance. That is, languages in the same family have
more commonalities distinguishing them from other
families, than random sets of languages have.
Looking at the results using the different feature
sets, in most cases the extended feature set out-
performed the rest. This shows that adding gram-
matical features increases accuracy for both L1 and
LF. Furthermore, in all experiments using only the
grammatical features achieved a rather good accu-
racy (significantly higher than random chance), con-
sidering that we used only 200 of these features.
This supports the notion that grammatical features
are useful for both L1 and LF identification.
Another interesting notion regarding the gram-
matical feature set is seen in the portion these fea-
tures consist of the InfoGain feature set for the ex-
periments of section 4.2: 33.05% for L1 and 57.16%
for LF. This suggests that the grammatical level of
the text has greater significance for identifying LF
compared to L1. When analyzing the portion lexical
features consist of the InfoGain feature set, an oppo-
site trend is seen: function words and letter bigrams
consist 29.94% and 33.94% of the features for L1, as
opposed to 17.44% and 23.55% for LF, respectively.
This suggests that the lexical level of the text is bet-
ter for L1 detection than for LF detection. Although
less significant, the same trend is seen with spelling
errors: 3% for L1 and 1.83% for LF.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999987769230769">
The main conclusion is that when trying to gain in-
formation about the native language of an English
text author, integrating family identification can in-
crease the total accuracy, using the method intro-
duced in section 4.3, where all low-confidence clas-
sifications are reapplied within a smaller set of can-
didates – languages within the family attributed to
those instances using a standalone experiment.
Furthermore, when dealing with a large number
of L1 classes, higher accuracy can be attained by
reducing the level of specification to language fami-
lies, which can be obtained with high accuracy using
the combined method presented in this paper that in-
tegrates both the trivial LF by predicted L1 and LF
by standalone experiment methods using the average
confidence level as threshold.
In addition, using the most frequent POS bigrams,
which represent the grammatical level of the text, is
shown to increase accuracy in both L1 and LF identi-
fication tasks, especially for the latter. Using lexical
features as function words and character bigrams is
helpful especially for L1 identification.
We suggest several directions for future work.
First, trying new feature sets that may capture other
similarities between languages in the same family.
For instance, since languages in the same family
tend to share basic vocabulary, it may have some
level of transfer to L2 that could be captured by a
synonym-based classifier. For instance, “verde” in
Spanish and “vert” in French may be translated to
“verdant”, whereas “gr¨un” in German and “groen”
in Dutch may be translated to “green”.
In addition, we can further explore the notion
of increasing accuracy by applying knowledge of a
broader class on the task applied in other stylometry-
based information extraction tasks. For instance, us-
ing wide age ranges as the broader class for classi-
fying age of anonymous authors, or personality pro-
totypes for personality type identification.
</bodyText>
<page confidence="0.999182">
37
</page>
<sectionHeader confidence="0.993761" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992145576923077">
Ahmed Abbasi and Hsinchun Chen. 2008. Writeprints:
A stylometric approach to identity-level identification
and similarity detection in cyberspace. ACM Trans.
Inf. Syst., 26(2):7:1–7:29, April.
Bashir Ahmed, Sung-Hyuk Cha, and Charles Tappert.
2004. Language identification from text using n-gram
based cumulative frequency addition. Proc. CSIS Re-
search Day, May.
Charles S. Ahn. 2011. Automatically detecting authors’
native language. Thesis, Naval Postgraduate School,
March.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ’cheap’ learner corpora. In The 2011
Conference of Learner Corpus Research (LCR2011).
Julian Brooke and Graeme Hirst. 2012. Measur-
ing interlanguage: Native language identification with
l1-influence metrics. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Ugur Dogan, Bente Maegaard, Joseph Mari-
ani, Jan Odijk, and Stelios Piperidis, editors, Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC’12), Istanbul,
Turkey, may. European Language Resources Associa-
tion (ELRA).
Lyle Campbell and William J. Poser. 2008. Language
Classification: History and Method. Cambridge Uni-
versity Press.
Maria Luisa Carrio-Pastor. 2009. Contrasting specific
english corpora: Language variation. International
Journal of English Studies, Special Issue, pages 221–
233.
Ghinwa F. Choueiter, Geoffrey Zweig, and Patrick
Nguyen. 2008. An empirical study of automatic ac-
cent classification. In ICASSP, pages 4265–4268.
Dominique Estival, Tanja Gaustad, Son B. Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for english emails. In 10th Conference of the Pacific
Association for Computational Linguistics (PACLING
2007), pages 262–272.
Winthrop Nelson Francis and Henry Kucera. 1983. Fre-
quency Analysis ofEnglish Usage: Lexicon and Gram-
mar. Houghton Mifflin.
Erin Elizabeth Gibbons. 2009. The effects of second lan-
guage experience on typologically similar and dissimi-
lar third language. Thesis, Brigham Young University,
Center for Language Studies.
Felix Golcher and Marc Reznicek. 2009. Stylome-
try and the interplay of topic and l1 in the different
annotation layers in the falko corpus. In Humboldt-
Universitat zu Berlin, QITL-4. [Online: Stand 2012-
03-22T16:09:09Z].
Sylvaine Granger, Estelle Dagneaux, and Fanny Meunier.
2002. International Corpus of Learner English : Ver-
sion 1 ; Handbook and CD-ROM. Pr. Univ. de Lou-
vain, Louvain-la-Neuve.
Sylvaine Granger, Estelle Dagneaux, Magali Paquot, and
Fanny Meunier. 2009. The International Corpus of
Learner English, Version 2: Handbook and CD-Rom.
Pr. Univ. de Louvain, Louvain-la-Neuve.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10–18, November.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005a.
Automatically determining an anonymous author’s na-
tive language. In Proceedings of the 2005 IEEE inter-
national conference on Intelligence and Security In-
formatics, ISI’05, pages 209–217, Berlin, Heidelberg.
Springer-Verlag.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005b.
Determining an author’s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining, KDD ’05, pages 624–628, New
York, NY, USA. ACM.
Andrew McDonald, Sadia Afroz, Aylin Caliskan, Ariel
Stolerman, and Rachel Greenstadt. 2012. Use
fewer instances of the letter “i”: Toward writing style
anonymization. July.
J. Platt. 1998. Fast training of support vector ma-
chines using sequential minimal optimization. In
B. Schoelkopf, C. Burges, and A. Smola, editors, Ad-
vances in Kernel Methods - Support Vector Learning.
MIT Press.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You’re not from ’round here, are you?: naive bayes
detection of non-native utterance text. In Proceedings
of the second meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language technologies, NAACL ’01, pages 1–8,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Hu-
man Language Technology Conference (HLT-NAACL
2003).
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Com-
putational Language Acquisition, CACLA ’07, pages
9–16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.985295">
38
</page>
<reference confidence="0.999818689655173">
Hans van Halteren. 2008. Source language markers in
europarl translations. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
- Volume 1, COLING ’08, pages 937–944, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
R. Wanneroy, E. Bilinski, C. Barras, M. Adda-Decker,
and E. Geoffrois. 1999. Acoustic-phonetic modeling
of non-native speech for language identification. In
Proceedings of the ESCA-NATO Workshop on Multi-
Lingual Interoperability in Speech Technology (MIST),
The Netherlands.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop 2009, pages 53–61, Sydney, Aus-
tralia, December.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2011. Topic modeling for native language identifi-
cation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 115–
124, Canberra, Australia, December.
Marc A. Zissman. 1993. Automatic language identifica-
tion using gaussian mixture and hidden markov mod-
els. In Proceedings of the 1993 IEEE international
conference on Acoustics, speech, and signal process-
ing: speech processing - Volume II, ICASSP’93, pages
399–402, Washington, DC, USA. IEEE Computer So-
ciety.
</reference>
<page confidence="0.999528">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.322984">
<title confidence="0.9858585">From Language to Family and Back: Native Language and Family Identification from English Text</title>
<author confidence="0.999648">Ariel Stolerman Aylin Caliskan Islam Rachel Greenstadt</author>
<affiliation confidence="0.740192333333333">Dept. of Computer Drexel Philadelphia,</affiliation>
<abstract confidence="0.9896635">Revealing an anonymous author’s traits from text is a well-researched area. In this paper we aim to identify the native language and language family of a non-native English author, given his/her English writings. We extract features from the text based on prior work, and extend or modify it to construct different feature sets, and use support vector machines for classification. We show that native language identification accuracy can be improved by up to 6.43% for a 9-class task, depending on the feature set, by introducing a novel method to incorporate language family information. In addition we show that introducing grammarbased features improves accuracy of both native language and language family identification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmed Abbasi</author>
<author>Hsinchun Chen</author>
</authors>
<title>Writeprints: A stylometric approach to identity-level identification and similarity detection in cyberspace.</title>
<date>2008</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="9571" citStr="Abbasi and Chen, 2008" startWordPosition="1513" endWordPosition="1516"> POS bigrams and 300 most frequent spelling errors. The 400 most frequent function words were taken from a list of 512 function words used in the original experiments by Koppel et al. For the 200 letter n-grams, we chose bigrams, as they are shown to be effective for the task in previous research. The 250 rare POS bigrams were extracted from the Brown Corpus using the POS tagger in (Toutanova et al., 2003). Finally, we simplified the error types by considering only misspelled words, based on a list of 5,753 common misspellings, constructed from Wikipedia common misspellings and those used in (Abbasi and Chen, 2008). We ignored any misspellings with 0-1 appearances across the entire sub-corpus. Since many of the rare POS bigrams and misspellings had no appearances, the effective vector lengths vary between 653-870 features. Extended: identical to the former, with the addition of the 200 most frequent POS bigrams across the entire sub-corpus used for each experiment. These syntactic features were selected as an additional representation of grammatical structures in the text. There are several methods for natural language classification, including genetic, typological and areal (Campbell and Poser, 2008). </context>
</contexts>
<marker>Abbasi, Chen, 2008</marker>
<rawString>Ahmed Abbasi and Hsinchun Chen. 2008. Writeprints: A stylometric approach to identity-level identification and similarity detection in cyberspace. ACM Trans. Inf. Syst., 26(2):7:1–7:29, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bashir Ahmed</author>
<author>Sung-Hyuk Cha</author>
<author>Charles Tappert</author>
</authors>
<title>Language identification from text using n-gram based cumulative frequency addition.</title>
<date>2004</date>
<booktitle>Proc. CSIS Research Day,</booktitle>
<contexts>
<context position="4276" citStr="Ahmed et al., 2004" startWordPosition="667" endWordPosition="670">gical traits from different data formats, such as speech and text samples. Native language and accent identification from speech can be found 32 Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 32–39, Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics in (Choueiter et al., 2008; Tomokiyo and Jones, 2001). Identifying an author’s native language from L2 text, which is English in most cases, is the closest problem to our work. Introductory studies in the area identified the written or spoken language itself, focusing on telephone dialogue corpora (Ahmed et al., 2004; Zissman, 1993). Further studies focused on extracting specific information from text or speech after identifying the language being used. Wanneroy et al. (1999) investigated how non-native speech deteriorated language identification and used acoustic adaptation to improve it. Choueiter et al. (2008) classified different foreign accented English speech samples by using a combination of heteroscedastic LDA and maximum mutual information training. Tomokiyo and Jones (2001) characterized part-of-speech sequences and showed that Naive Bayes classification can be used to identify non-native uttera</context>
</contexts>
<marker>Ahmed, Cha, Tappert, 2004</marker>
<rawString>Bashir Ahmed, Sung-Hyuk Cha, and Charles Tappert. 2004. Language identification from text using n-gram based cumulative frequency addition. Proc. CSIS Research Day, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles S Ahn</author>
</authors>
<title>Automatically detecting authors’ native language. Thesis,</title>
<date>2011</date>
<location>Naval Postgraduate School,</location>
<contexts>
<context position="6254" citStr="Ahn, 2011" startWordPosition="966" endWordPosition="967">012) presented a method of utilizing native language corpora for identifying native language in non-native texts. They used wordby-word translation of large native language corpora to create sets of second language forms that are possible results of language transfer, later used in unsupervised classification. They achieved results above random chance for L1 identification, however insufficiently accurate. More related work can be found in (Estival et al., 2007; van Halteren, 2008; Carrio-Pastor, 2009; Golcher and Reznicek, 2009; Wong and Dras, 2009; Wong et al., 2011; Brooke and Hirst, 2011; Ahn, 2011). The work mentioned above and our approach both utilize the L1-L2 transfer effect to gain information about an author’s native language. Gibbons (2009) proved the impact of native language family’s typological properties on L2. As far as we know, our work is the first to combine stylometry and native language family’s effect on L2, utilized for L1 identification. 3 Experimental Setting 3.1 Corpus We use the ICLEv2 (Granger et al., 2009) corpus that contains English documents written by intermediate to advanced international learners of English, with language backgrounds of 16 mother-tongues. </context>
</contexts>
<marker>Ahn, 2011</marker>
<rawString>Charles S. Ahn. 2011. Automatically detecting authors’ native language. Thesis, Naval Postgraduate School, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Brooke</author>
<author>Graeme Hirst</author>
</authors>
<title>Native language detection with ’cheap’ learner corpora.</title>
<date>2011</date>
<booktitle>In The 2011 Conference of Learner Corpus Research (LCR2011).</booktitle>
<contexts>
<context position="6242" citStr="Brooke and Hirst, 2011" startWordPosition="962" endWordPosition="965">rds. Brooke and Hirst (2012) presented a method of utilizing native language corpora for identifying native language in non-native texts. They used wordby-word translation of large native language corpora to create sets of second language forms that are possible results of language transfer, later used in unsupervised classification. They achieved results above random chance for L1 identification, however insufficiently accurate. More related work can be found in (Estival et al., 2007; van Halteren, 2008; Carrio-Pastor, 2009; Golcher and Reznicek, 2009; Wong and Dras, 2009; Wong et al., 2011; Brooke and Hirst, 2011; Ahn, 2011). The work mentioned above and our approach both utilize the L1-L2 transfer effect to gain information about an author’s native language. Gibbons (2009) proved the impact of native language family’s typological properties on L2. As far as we know, our work is the first to combine stylometry and native language family’s effect on L2, utilized for L1 identification. 3 Experimental Setting 3.1 Corpus We use the ICLEv2 (Granger et al., 2009) corpus that contains English documents written by intermediate to advanced international learners of English, with language backgrounds of 16 moth</context>
</contexts>
<marker>Brooke, Hirst, 2011</marker>
<rawString>Julian Brooke and Graeme Hirst. 2011. Native language detection with ’cheap’ learner corpora. In The 2011 Conference of Learner Corpus Research (LCR2011).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Julian Brooke</author>
<author>Graeme Hirst</author>
</authors>
<title>Measuring interlanguage: Native language identification with l1-influence metrics.</title>
<date>2012</date>
<booktitle>Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors,</editor>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="5648" citStr="Brooke and Hirst (2012)" startWordPosition="870" endWordPosition="873"> explored frequencies of sets of features, and used them with multi-linear support vector machines to classify text by author’s native language. They used a set of features consisted of function words, letter n-grams, errors and idiosyncrasies, and experimented on a dataset of authors of five different native languages taken from ICLEv1 (Granger et al., 2002), reaching to 80.2% accuracy. Tsur and Rappoport (2007) revisited Koppel’s work using only the 200 most frequent character bigrams, and achieved 65.6% accuracy, with only a small degradation when removing dominant words or function words. Brooke and Hirst (2012) presented a method of utilizing native language corpora for identifying native language in non-native texts. They used wordby-word translation of large native language corpora to create sets of second language forms that are possible results of language transfer, later used in unsupervised classification. They achieved results above random chance for L1 identification, however insufficiently accurate. More related work can be found in (Estival et al., 2007; van Halteren, 2008; Carrio-Pastor, 2009; Golcher and Reznicek, 2009; Wong and Dras, 2009; Wong et al., 2011; Brooke and Hirst, 2011; Ahn,</context>
</contexts>
<marker>Brooke, Hirst, 2012</marker>
<rawString>Julian Brooke and Graeme Hirst. 2012. Measuring interlanguage: Native language identification with l1-influence metrics. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyle Campbell</author>
<author>William J Poser</author>
</authors>
<title>Language Classification: History and Method.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10169" citStr="Campbell and Poser, 2008" startWordPosition="1601" endWordPosition="1604"> in (Abbasi and Chen, 2008). We ignored any misspellings with 0-1 appearances across the entire sub-corpus. Since many of the rare POS bigrams and misspellings had no appearances, the effective vector lengths vary between 653-870 features. Extended: identical to the former, with the addition of the 200 most frequent POS bigrams across the entire sub-corpus used for each experiment. These syntactic features were selected as an additional representation of grammatical structures in the text. There are several methods for natural language classification, including genetic, typological and areal (Campbell and Poser, 2008). We consider the typological classification that uses structural features to compare similarities between languages and classify them into families. Therefore we chose grammatical evidence in L2 as features that may represent similar transfer effects among languages in the same family. Grammatical: constructed only from the 200 most frequent POS bigrams, representing the grammatical level of the text. InfoGain: We used the 200 features with the highest information gain extracted from the extended feature set using Weka (Hall et al., 2009), calculated for any given feature by measuring the exp</context>
</contexts>
<marker>Campbell, Poser, 2008</marker>
<rawString>Lyle Campbell and William J. Poser. 2008. Language Classification: History and Method. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Luisa Carrio-Pastor</author>
</authors>
<title>Contrasting specific english corpora: Language variation.</title>
<date>2009</date>
<journal>International Journal of English Studies, Special Issue,</journal>
<pages>221--233</pages>
<contexts>
<context position="6150" citStr="Carrio-Pastor, 2009" startWordPosition="948" endWordPosition="949">65.6% accuracy, with only a small degradation when removing dominant words or function words. Brooke and Hirst (2012) presented a method of utilizing native language corpora for identifying native language in non-native texts. They used wordby-word translation of large native language corpora to create sets of second language forms that are possible results of language transfer, later used in unsupervised classification. They achieved results above random chance for L1 identification, however insufficiently accurate. More related work can be found in (Estival et al., 2007; van Halteren, 2008; Carrio-Pastor, 2009; Golcher and Reznicek, 2009; Wong and Dras, 2009; Wong et al., 2011; Brooke and Hirst, 2011; Ahn, 2011). The work mentioned above and our approach both utilize the L1-L2 transfer effect to gain information about an author’s native language. Gibbons (2009) proved the impact of native language family’s typological properties on L2. As far as we know, our work is the first to combine stylometry and native language family’s effect on L2, utilized for L1 identification. 3 Experimental Setting 3.1 Corpus We use the ICLEv2 (Granger et al., 2009) corpus that contains English documents written by inte</context>
</contexts>
<marker>Carrio-Pastor, 2009</marker>
<rawString>Maria Luisa Carrio-Pastor. 2009. Contrasting specific english corpora: Language variation. International Journal of English Studies, Special Issue, pages 221– 233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ghinwa F Choueiter</author>
<author>Geoffrey Zweig</author>
<author>Patrick Nguyen</author>
</authors>
<title>An empirical study of automatic accent classification.</title>
<date>2008</date>
<booktitle>In ICASSP,</booktitle>
<pages>4265--4268</pages>
<contexts>
<context position="3984" citStr="Choueiter et al., 2008" startWordPosition="620" endWordPosition="623"> the different experiments that were performed, followed by results and evaluation. We finalize with discussion on the given results (section 5), followed by conclusions and directions for future research (section 6). 2 Related Work Literature includes work on extracting demographic and psychological traits from different data formats, such as speech and text samples. Native language and accent identification from speech can be found 32 Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 32–39, Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics in (Choueiter et al., 2008; Tomokiyo and Jones, 2001). Identifying an author’s native language from L2 text, which is English in most cases, is the closest problem to our work. Introductory studies in the area identified the written or spoken language itself, focusing on telephone dialogue corpora (Ahmed et al., 2004; Zissman, 1993). Further studies focused on extracting specific information from text or speech after identifying the language being used. Wanneroy et al. (1999) investigated how non-native speech deteriorated language identification and used acoustic adaptation to improve it. Choueiter et al. (2008) class</context>
</contexts>
<marker>Choueiter, Zweig, Nguyen, 2008</marker>
<rawString>Ghinwa F. Choueiter, Geoffrey Zweig, and Patrick Nguyen. 2008. An empirical study of automatic accent classification. In ICASSP, pages 4265–4268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominique Estival</author>
<author>Tanja Gaustad</author>
<author>Son B Pham</author>
<author>Will Radford</author>
<author>Ben Hutchinson</author>
</authors>
<title>Author profiling for english emails.</title>
<date>2007</date>
<booktitle>In 10th Conference of the Pacific Association for Computational Linguistics (PACLING</booktitle>
<pages>262--272</pages>
<contexts>
<context position="6109" citStr="Estival et al., 2007" startWordPosition="941" endWordPosition="944"> frequent character bigrams, and achieved 65.6% accuracy, with only a small degradation when removing dominant words or function words. Brooke and Hirst (2012) presented a method of utilizing native language corpora for identifying native language in non-native texts. They used wordby-word translation of large native language corpora to create sets of second language forms that are possible results of language transfer, later used in unsupervised classification. They achieved results above random chance for L1 identification, however insufficiently accurate. More related work can be found in (Estival et al., 2007; van Halteren, 2008; Carrio-Pastor, 2009; Golcher and Reznicek, 2009; Wong and Dras, 2009; Wong et al., 2011; Brooke and Hirst, 2011; Ahn, 2011). The work mentioned above and our approach both utilize the L1-L2 transfer effect to gain information about an author’s native language. Gibbons (2009) proved the impact of native language family’s typological properties on L2. As far as we know, our work is the first to combine stylometry and native language family’s effect on L2, utilized for L1 identification. 3 Experimental Setting 3.1 Corpus We use the ICLEv2 (Granger et al., 2009) corpus that c</context>
</contexts>
<marker>Estival, Gaustad, Pham, Radford, Hutchinson, 2007</marker>
<rawString>Dominique Estival, Tanja Gaustad, Son B. Pham, Will Radford, and Ben Hutchinson. 2007. Author profiling for english emails. In 10th Conference of the Pacific Association for Computational Linguistics (PACLING 2007), pages 262–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Winthrop Nelson Francis</author>
<author>Henry Kucera</author>
</authors>
<title>Frequency Analysis ofEnglish Usage: Lexicon and Grammar.</title>
<date>1983</date>
<publisher>Houghton Mifflin.</publisher>
<contexts>
<context position="8549" citStr="Francis and Kucera, 1983" startWordPosition="1339" endWordPosition="1342">m both L1 and LF aspects, we maintained only the sub-corpora that allowed a sufficient amount of languages in each represented family, i.e. 3 languages in each of the Germanic, Slavic and Romance families. Therefore we removed 5 of the 16 available languages in the corpus. 3.2 Feature Selection Koppel et al. represented each document in their experiment as a 1,035-dimensional feature vector: 400 function words, 200 most frequent letter n-grams, 185 misspellings and syntactic errors and 250 rare POS bigrams. The 250 rare POS bigrams are the least common bigrams extracted from the Brown Corpus (Francis and Kucera, 1983), and their appearances are considered to be erroneous or nonstandard. In our experiments we used 4 different feature sets, partially based on that used by Koppel et al. We used the authorship attribution tool JStylo (McDonald et al., 2012) for feature extraction. The feature 33 sets are the following: Basic: includes the 400 most frequent function words, 200 most frequent letter bigrams, 250 rare POS bigrams and 300 most frequent spelling errors. The 400 most frequent function words were taken from a list of 512 function words used in the original experiments by Koppel et al. For the 200 lett</context>
</contexts>
<marker>Francis, Kucera, 1983</marker>
<rawString>Winthrop Nelson Francis and Henry Kucera. 1983. Frequency Analysis ofEnglish Usage: Lexicon and Grammar. Houghton Mifflin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erin Elizabeth Gibbons</author>
</authors>
<title>The effects of second language experience on typologically similar and dissimilar third language. Thesis,</title>
<date>2009</date>
<institution>Brigham Young University, Center for Language Studies.</institution>
<contexts>
<context position="6406" citStr="Gibbons (2009)" startWordPosition="990" endWordPosition="991">of large native language corpora to create sets of second language forms that are possible results of language transfer, later used in unsupervised classification. They achieved results above random chance for L1 identification, however insufficiently accurate. More related work can be found in (Estival et al., 2007; van Halteren, 2008; Carrio-Pastor, 2009; Golcher and Reznicek, 2009; Wong and Dras, 2009; Wong et al., 2011; Brooke and Hirst, 2011; Ahn, 2011). The work mentioned above and our approach both utilize the L1-L2 transfer effect to gain information about an author’s native language. Gibbons (2009) proved the impact of native language family’s typological properties on L2. As far as we know, our work is the first to combine stylometry and native language family’s effect on L2, utilized for L1 identification. 3 Experimental Setting 3.1 Corpus We use the ICLEv2 (Granger et al., 2009) corpus that contains English documents written by intermediate to advanced international learners of English, with language backgrounds of 16 mother-tongues. The first version of the corpus was used in significant previous work (Koppel et al., 2005a; Koppel et al., 2005b; Tsur and Rappoport, 2007). They repor</context>
</contexts>
<marker>Gibbons, 2009</marker>
<rawString>Erin Elizabeth Gibbons. 2009. The effects of second language experience on typologically similar and dissimilar third language. Thesis, Brigham Young University, Center for Language Studies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Golcher</author>
<author>Marc Reznicek</author>
</authors>
<title>Stylometry and the interplay of topic and l1 in the different annotation layers in the falko corpus.</title>
<date>2009</date>
<booktitle>In HumboldtUniversitat zu Berlin, QITL-4. [Online: Stand</booktitle>
<pages>2012--03</pages>
<contexts>
<context position="6178" citStr="Golcher and Reznicek, 2009" startWordPosition="950" endWordPosition="953">only a small degradation when removing dominant words or function words. Brooke and Hirst (2012) presented a method of utilizing native language corpora for identifying native language in non-native texts. They used wordby-word translation of large native language corpora to create sets of second language forms that are possible results of language transfer, later used in unsupervised classification. They achieved results above random chance for L1 identification, however insufficiently accurate. More related work can be found in (Estival et al., 2007; van Halteren, 2008; Carrio-Pastor, 2009; Golcher and Reznicek, 2009; Wong and Dras, 2009; Wong et al., 2011; Brooke and Hirst, 2011; Ahn, 2011). The work mentioned above and our approach both utilize the L1-L2 transfer effect to gain information about an author’s native language. Gibbons (2009) proved the impact of native language family’s typological properties on L2. As far as we know, our work is the first to combine stylometry and native language family’s effect on L2, utilized for L1 identification. 3 Experimental Setting 3.1 Corpus We use the ICLEv2 (Granger et al., 2009) corpus that contains English documents written by intermediate to advanced interna</context>
</contexts>
<marker>Golcher, Reznicek, 2009</marker>
<rawString>Felix Golcher and Marc Reznicek. 2009. Stylometry and the interplay of topic and l1 in the different annotation layers in the falko corpus. In HumboldtUniversitat zu Berlin, QITL-4. [Online: Stand 2012-03-22T16:09:09Z].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvaine Granger</author>
<author>Estelle Dagneaux</author>
<author>Fanny Meunier</author>
</authors>
<date>2002</date>
<booktitle>International Corpus of Learner English : Version 1 ; Handbook and CD-ROM. Pr. Univ. de Louvain,</booktitle>
<location>Louvain-la-Neuve.</location>
<contexts>
<context position="5386" citStr="Granger et al., 2002" startWordPosition="829" endWordPosition="832">d part-of-speech sequences and showed that Naive Bayes classification can be used to identify non-native utterances of English. The first work that utilized stylometric methods for native language attribution is introduced by Koppel et al. (2005a; 2005b). They explored frequencies of sets of features, and used them with multi-linear support vector machines to classify text by author’s native language. They used a set of features consisted of function words, letter n-grams, errors and idiosyncrasies, and experimented on a dataset of authors of five different native languages taken from ICLEv1 (Granger et al., 2002), reaching to 80.2% accuracy. Tsur and Rappoport (2007) revisited Koppel’s work using only the 200 most frequent character bigrams, and achieved 65.6% accuracy, with only a small degradation when removing dominant words or function words. Brooke and Hirst (2012) presented a method of utilizing native language corpora for identifying native language in non-native texts. They used wordby-word translation of large native language corpora to create sets of second language forms that are possible results of language transfer, later used in unsupervised classification. They achieved results above ra</context>
</contexts>
<marker>Granger, Dagneaux, Meunier, 2002</marker>
<rawString>Sylvaine Granger, Estelle Dagneaux, and Fanny Meunier. 2002. International Corpus of Learner English : Version 1 ; Handbook and CD-ROM. Pr. Univ. de Louvain, Louvain-la-Neuve.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvaine Granger</author>
<author>Estelle Dagneaux</author>
<author>Magali Paquot</author>
<author>Fanny Meunier</author>
</authors>
<date>2009</date>
<booktitle>The International Corpus of Learner English, Version 2: Handbook and CD-Rom. Pr. Univ. de Louvain,</booktitle>
<location>Louvain-la-Neuve.</location>
<contexts>
<context position="6695" citStr="Granger et al., 2009" startWordPosition="1037" endWordPosition="1040">n be found in (Estival et al., 2007; van Halteren, 2008; Carrio-Pastor, 2009; Golcher and Reznicek, 2009; Wong and Dras, 2009; Wong et al., 2011; Brooke and Hirst, 2011; Ahn, 2011). The work mentioned above and our approach both utilize the L1-L2 transfer effect to gain information about an author’s native language. Gibbons (2009) proved the impact of native language family’s typological properties on L2. As far as we know, our work is the first to combine stylometry and native language family’s effect on L2, utilized for L1 identification. 3 Experimental Setting 3.1 Corpus We use the ICLEv2 (Granger et al., 2009) corpus that contains English documents written by intermediate to advanced international learners of English, with language backgrounds of 16 mother-tongues. The first version of the corpus was used in significant previous work (Koppel et al., 2005a; Koppel et al., 2005b; Tsur and Rappoport, 2007). They reported that they were able to use 258 documents of sizes 500-1000 words for each language they used. We use version 2 of the corpus and restrict all documents in our experiments to those with 500-1000 words as well. However, we found that constraining our documents to these lengths allows us</context>
</contexts>
<marker>Granger, Dagneaux, Paquot, Meunier, 2009</marker>
<rawString>Sylvaine Granger, Estelle Dagneaux, Magali Paquot, and Fanny Meunier. 2009. The International Corpus of Learner English, Version 2: Handbook and CD-Rom. Pr. Univ. de Louvain, Louvain-la-Neuve.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="10714" citStr="Hall et al., 2009" startWordPosition="1685" endWordPosition="1688">tion, including genetic, typological and areal (Campbell and Poser, 2008). We consider the typological classification that uses structural features to compare similarities between languages and classify them into families. Therefore we chose grammatical evidence in L2 as features that may represent similar transfer effects among languages in the same family. Grammatical: constructed only from the 200 most frequent POS bigrams, representing the grammatical level of the text. InfoGain: We used the 200 features with the highest information gain extracted from the extended feature set using Weka (Hall et al., 2009), calculated for any given feature by measuring the expected reduction in entropy caused by partitioning the test instances according to that feature. 3.3 Classification We trained a SMO (Platt, 1998) SVM classifier with polynomial kernel, chosen as SVMs are used extensively in prior work and ours outperformed other methods tested, including decision trees, nearestneighbors, Bayesian and logistic regression classifiers. 4 Experimental Variations and Evaluation We conducted 3 different experiments using various sub-corpora and the 4 feature sets described in the previous sections, with L1 and L</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: an update. SIGKDD Explor. Newsl., 11(1):10–18, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Kfir Zigdon</author>
</authors>
<title>Automatically determining an anonymous author’s native language.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 IEEE international conference on Intelligence and Security Informatics, ISI’05,</booktitle>
<pages>209--217</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="5010" citStr="Koppel et al. (2005" startWordPosition="769" endWordPosition="773">the language being used. Wanneroy et al. (1999) investigated how non-native speech deteriorated language identification and used acoustic adaptation to improve it. Choueiter et al. (2008) classified different foreign accented English speech samples by using a combination of heteroscedastic LDA and maximum mutual information training. Tomokiyo and Jones (2001) characterized part-of-speech sequences and showed that Naive Bayes classification can be used to identify non-native utterances of English. The first work that utilized stylometric methods for native language attribution is introduced by Koppel et al. (2005a; 2005b). They explored frequencies of sets of features, and used them with multi-linear support vector machines to classify text by author’s native language. They used a set of features consisted of function words, letter n-grams, errors and idiosyncrasies, and experimented on a dataset of authors of five different native languages taken from ICLEv1 (Granger et al., 2002), reaching to 80.2% accuracy. Tsur and Rappoport (2007) revisited Koppel’s work using only the 200 most frequent character bigrams, and achieved 65.6% accuracy, with only a small degradation when removing dominant words or f</context>
<context position="6944" citStr="Koppel et al., 2005" startWordPosition="1075" endWordPosition="1078">sfer effect to gain information about an author’s native language. Gibbons (2009) proved the impact of native language family’s typological properties on L2. As far as we know, our work is the first to combine stylometry and native language family’s effect on L2, utilized for L1 identification. 3 Experimental Setting 3.1 Corpus We use the ICLEv2 (Granger et al., 2009) corpus that contains English documents written by intermediate to advanced international learners of English, with language backgrounds of 16 mother-tongues. The first version of the corpus was used in significant previous work (Koppel et al., 2005a; Koppel et al., 2005b; Tsur and Rappoport, 2007). They reported that they were able to use 258 documents of sizes 500-1000 words for each language they used. We use version 2 of the corpus and restrict all documents in our experiments to those with 500-1000 words as well. However, we found that constraining our documents to these lengths allows us to use only 133-146 documents per language. We conduct a series of experiments with different sub-corpora constructed of documents representing 11 native languages out of the 16 available in the corpus. The native languages we used are: Bulgarian, </context>
</contexts>
<marker>Koppel, Schler, Zigdon, 2005</marker>
<rawString>Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005a. Automatically determining an anonymous author’s native language. In Proceedings of the 2005 IEEE international conference on Intelligence and Security Informatics, ISI’05, pages 209–217, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Kfir Zigdon</author>
</authors>
<title>Determining an author’s native language by mining a text for errors.</title>
<date>2005</date>
<booktitle>In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, KDD ’05,</booktitle>
<pages>624--628</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5010" citStr="Koppel et al. (2005" startWordPosition="769" endWordPosition="773">the language being used. Wanneroy et al. (1999) investigated how non-native speech deteriorated language identification and used acoustic adaptation to improve it. Choueiter et al. (2008) classified different foreign accented English speech samples by using a combination of heteroscedastic LDA and maximum mutual information training. Tomokiyo and Jones (2001) characterized part-of-speech sequences and showed that Naive Bayes classification can be used to identify non-native utterances of English. The first work that utilized stylometric methods for native language attribution is introduced by Koppel et al. (2005a; 2005b). They explored frequencies of sets of features, and used them with multi-linear support vector machines to classify text by author’s native language. They used a set of features consisted of function words, letter n-grams, errors and idiosyncrasies, and experimented on a dataset of authors of five different native languages taken from ICLEv1 (Granger et al., 2002), reaching to 80.2% accuracy. Tsur and Rappoport (2007) revisited Koppel’s work using only the 200 most frequent character bigrams, and achieved 65.6% accuracy, with only a small degradation when removing dominant words or f</context>
<context position="6944" citStr="Koppel et al., 2005" startWordPosition="1075" endWordPosition="1078">sfer effect to gain information about an author’s native language. Gibbons (2009) proved the impact of native language family’s typological properties on L2. As far as we know, our work is the first to combine stylometry and native language family’s effect on L2, utilized for L1 identification. 3 Experimental Setting 3.1 Corpus We use the ICLEv2 (Granger et al., 2009) corpus that contains English documents written by intermediate to advanced international learners of English, with language backgrounds of 16 mother-tongues. The first version of the corpus was used in significant previous work (Koppel et al., 2005a; Koppel et al., 2005b; Tsur and Rappoport, 2007). They reported that they were able to use 258 documents of sizes 500-1000 words for each language they used. We use version 2 of the corpus and restrict all documents in our experiments to those with 500-1000 words as well. However, we found that constraining our documents to these lengths allows us to use only 133-146 documents per language. We conduct a series of experiments with different sub-corpora constructed of documents representing 11 native languages out of the 16 available in the corpus. The native languages we used are: Bulgarian, </context>
</contexts>
<marker>Koppel, Schler, Zigdon, 2005</marker>
<rawString>Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005b. Determining an author’s native language by mining a text for errors. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, KDD ’05, pages 624–628, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McDonald</author>
<author>Sadia Afroz</author>
<author>Aylin Caliskan</author>
<author>Ariel Stolerman</author>
<author>Rachel Greenstadt</author>
</authors>
<title>Use fewer instances of the letter “i”: Toward writing style anonymization.</title>
<date>2012</date>
<contexts>
<context position="8789" citStr="McDonald et al., 2012" startWordPosition="1380" endWordPosition="1384">lable languages in the corpus. 3.2 Feature Selection Koppel et al. represented each document in their experiment as a 1,035-dimensional feature vector: 400 function words, 200 most frequent letter n-grams, 185 misspellings and syntactic errors and 250 rare POS bigrams. The 250 rare POS bigrams are the least common bigrams extracted from the Brown Corpus (Francis and Kucera, 1983), and their appearances are considered to be erroneous or nonstandard. In our experiments we used 4 different feature sets, partially based on that used by Koppel et al. We used the authorship attribution tool JStylo (McDonald et al., 2012) for feature extraction. The feature 33 sets are the following: Basic: includes the 400 most frequent function words, 200 most frequent letter bigrams, 250 rare POS bigrams and 300 most frequent spelling errors. The 400 most frequent function words were taken from a list of 512 function words used in the original experiments by Koppel et al. For the 200 letter n-grams, we chose bigrams, as they are shown to be effective for the task in previous research. The 250 rare POS bigrams were extracted from the Brown Corpus using the POS tagger in (Toutanova et al., 2003). Finally, we simplified the er</context>
</contexts>
<marker>McDonald, Afroz, Caliskan, Stolerman, Greenstadt, 2012</marker>
<rawString>Andrew McDonald, Sadia Afroz, Aylin Caliskan, Ariel Stolerman, and Rachel Greenstadt. 2012. Use fewer instances of the letter “i”: Toward writing style anonymization. July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
</authors>
<title>Fast training of support vector machines using sequential minimal optimization.</title>
<date>1998</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In B. Schoelkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10914" citStr="Platt, 1998" startWordPosition="1719" endWordPosition="1720">m into families. Therefore we chose grammatical evidence in L2 as features that may represent similar transfer effects among languages in the same family. Grammatical: constructed only from the 200 most frequent POS bigrams, representing the grammatical level of the text. InfoGain: We used the 200 features with the highest information gain extracted from the extended feature set using Weka (Hall et al., 2009), calculated for any given feature by measuring the expected reduction in entropy caused by partitioning the test instances according to that feature. 3.3 Classification We trained a SMO (Platt, 1998) SVM classifier with polynomial kernel, chosen as SVMs are used extensively in prior work and ours outperformed other methods tested, including decision trees, nearestneighbors, Bayesian and logistic regression classifiers. 4 Experimental Variations and Evaluation We conducted 3 different experiments using various sub-corpora and the 4 feature sets described in the previous sections, with L1 and LF classification tasks. We evaluated the results by using the truepositive rate to capture accuracy. Following is a detailed description of the different variations and results. 4.1 9-Class Languages,</context>
</contexts>
<marker>Platt, 1998</marker>
<rawString>J. Platt. 1998. Fast training of support vector machines using sequential minimal optimization. In B. Schoelkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Mayfield Tomokiyo</author>
<author>Rosie Jones</author>
</authors>
<title>You’re not from ’round here, are you?: naive bayes detection of non-native utterance text.</title>
<date>2001</date>
<booktitle>In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, NAACL ’01,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4011" citStr="Tomokiyo and Jones, 2001" startWordPosition="624" endWordPosition="627">ts that were performed, followed by results and evaluation. We finalize with discussion on the given results (section 5), followed by conclusions and directions for future research (section 6). 2 Related Work Literature includes work on extracting demographic and psychological traits from different data formats, such as speech and text samples. Native language and accent identification from speech can be found 32 Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 32–39, Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics in (Choueiter et al., 2008; Tomokiyo and Jones, 2001). Identifying an author’s native language from L2 text, which is English in most cases, is the closest problem to our work. Introductory studies in the area identified the written or spoken language itself, focusing on telephone dialogue corpora (Ahmed et al., 2004; Zissman, 1993). Further studies focused on extracting specific information from text or speech after identifying the language being used. Wanneroy et al. (1999) investigated how non-native speech deteriorated language identification and used acoustic adaptation to improve it. Choueiter et al. (2008) classified different foreign acc</context>
</contexts>
<marker>Tomokiyo, Jones, 2001</marker>
<rawString>Laura Mayfield Tomokiyo and Rosie Jones. 2001. You’re not from ’round here, are you?: naive bayes detection of non-native utterance text. In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, NAACL ’01, pages 1–8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Human Language Technology Conference (HLT-NAACL</booktitle>
<contexts>
<context position="9358" citStr="Toutanova et al., 2003" startWordPosition="1481" endWordPosition="1484">orship attribution tool JStylo (McDonald et al., 2012) for feature extraction. The feature 33 sets are the following: Basic: includes the 400 most frequent function words, 200 most frequent letter bigrams, 250 rare POS bigrams and 300 most frequent spelling errors. The 400 most frequent function words were taken from a list of 512 function words used in the original experiments by Koppel et al. For the 200 letter n-grams, we chose bigrams, as they are shown to be effective for the task in previous research. The 250 rare POS bigrams were extracted from the Brown Corpus using the POS tagger in (Toutanova et al., 2003). Finally, we simplified the error types by considering only misspelled words, based on a list of 5,753 common misspellings, constructed from Wikipedia common misspellings and those used in (Abbasi and Chen, 2008). We ignored any misspellings with 0-1 appearances across the entire sub-corpus. Since many of the rare POS bigrams and misspellings had no appearances, the effective vector lengths vary between 653-870 features. Extended: identical to the former, with the addition of the 200 most frequent POS bigrams across the entire sub-corpus used for each experiment. These syntactic features were</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Human Language Technology Conference (HLT-NAACL 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Using classifier features for studying the effect of native language on the choice of written second language words.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, CACLA ’07,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5441" citStr="Tsur and Rappoport (2007)" startWordPosition="837" endWordPosition="840">yes classification can be used to identify non-native utterances of English. The first work that utilized stylometric methods for native language attribution is introduced by Koppel et al. (2005a; 2005b). They explored frequencies of sets of features, and used them with multi-linear support vector machines to classify text by author’s native language. They used a set of features consisted of function words, letter n-grams, errors and idiosyncrasies, and experimented on a dataset of authors of five different native languages taken from ICLEv1 (Granger et al., 2002), reaching to 80.2% accuracy. Tsur and Rappoport (2007) revisited Koppel’s work using only the 200 most frequent character bigrams, and achieved 65.6% accuracy, with only a small degradation when removing dominant words or function words. Brooke and Hirst (2012) presented a method of utilizing native language corpora for identifying native language in non-native texts. They used wordby-word translation of large native language corpora to create sets of second language forms that are possible results of language transfer, later used in unsupervised classification. They achieved results above random chance for L1 identification, however insufficient</context>
<context position="6994" citStr="Tsur and Rappoport, 2007" startWordPosition="1083" endWordPosition="1086">hor’s native language. Gibbons (2009) proved the impact of native language family’s typological properties on L2. As far as we know, our work is the first to combine stylometry and native language family’s effect on L2, utilized for L1 identification. 3 Experimental Setting 3.1 Corpus We use the ICLEv2 (Granger et al., 2009) corpus that contains English documents written by intermediate to advanced international learners of English, with language backgrounds of 16 mother-tongues. The first version of the corpus was used in significant previous work (Koppel et al., 2005a; Koppel et al., 2005b; Tsur and Rappoport, 2007). They reported that they were able to use 258 documents of sizes 500-1000 words for each language they used. We use version 2 of the corpus and restrict all documents in our experiments to those with 500-1000 words as well. However, we found that constraining our documents to these lengths allows us to use only 133-146 documents per language. We conduct a series of experiments with different sub-corpora constructed of documents representing 11 native languages out of the 16 available in the corpus. The native languages we used are: Bulgarian, Czech, Dutch, French, German, Italian, Norwegian, </context>
</contexts>
<marker>Tsur, Rappoport, 2007</marker>
<rawString>Oren Tsur and Ari Rappoport. 2007. Using classifier features for studying the effect of native language on the choice of written second language words. In Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, CACLA ’07, pages 9–16, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
</authors>
<title>Source language markers in europarl translations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08,</booktitle>
<pages>937--944</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>van Halteren, 2008</marker>
<rawString>Hans van Halteren. 2008. Source language markers in europarl translations. In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08, pages 937–944, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wanneroy</author>
<author>E Bilinski</author>
<author>C Barras</author>
<author>M Adda-Decker</author>
<author>E Geoffrois</author>
</authors>
<title>Acoustic-phonetic modeling of non-native speech for language identification.</title>
<date>1999</date>
<booktitle>In Proceedings of the ESCA-NATO Workshop on MultiLingual Interoperability in Speech Technology (MIST), The</booktitle>
<location>Netherlands.</location>
<contexts>
<context position="4438" citStr="Wanneroy et al. (1999)" startWordPosition="690" endWordPosition="693">f the NAACL HLT 2013 Student Research Workshop, pages 32–39, Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics in (Choueiter et al., 2008; Tomokiyo and Jones, 2001). Identifying an author’s native language from L2 text, which is English in most cases, is the closest problem to our work. Introductory studies in the area identified the written or spoken language itself, focusing on telephone dialogue corpora (Ahmed et al., 2004; Zissman, 1993). Further studies focused on extracting specific information from text or speech after identifying the language being used. Wanneroy et al. (1999) investigated how non-native speech deteriorated language identification and used acoustic adaptation to improve it. Choueiter et al. (2008) classified different foreign accented English speech samples by using a combination of heteroscedastic LDA and maximum mutual information training. Tomokiyo and Jones (2001) characterized part-of-speech sequences and showed that Naive Bayes classification can be used to identify non-native utterances of English. The first work that utilized stylometric methods for native language attribution is introduced by Koppel et al. (2005a; 2005b). They explored fre</context>
</contexts>
<marker>Wanneroy, Bilinski, Barras, Adda-Decker, Geoffrois, 1999</marker>
<rawString>R. Wanneroy, E. Bilinski, C. Barras, M. Adda-Decker, and E. Geoffrois. 1999. Acoustic-phonetic modeling of non-native speech for language identification. In Proceedings of the ESCA-NATO Workshop on MultiLingual Interoperability in Speech Technology (MIST), The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Contrastive analysis and native language identification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>53--61</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="6199" citStr="Wong and Dras, 2009" startWordPosition="954" endWordPosition="957">n removing dominant words or function words. Brooke and Hirst (2012) presented a method of utilizing native language corpora for identifying native language in non-native texts. They used wordby-word translation of large native language corpora to create sets of second language forms that are possible results of language transfer, later used in unsupervised classification. They achieved results above random chance for L1 identification, however insufficiently accurate. More related work can be found in (Estival et al., 2007; van Halteren, 2008; Carrio-Pastor, 2009; Golcher and Reznicek, 2009; Wong and Dras, 2009; Wong et al., 2011; Brooke and Hirst, 2011; Ahn, 2011). The work mentioned above and our approach both utilize the L1-L2 transfer effect to gain information about an author’s native language. Gibbons (2009) proved the impact of native language family’s typological properties on L2. As far as we know, our work is the first to combine stylometry and native language family’s effect on L2, utilized for L1 identification. 3 Experimental Setting 3.1 Corpus We use the ICLEv2 (Granger et al., 2009) corpus that contains English documents written by intermediate to advanced international learners of En</context>
</contexts>
<marker>Wong, Dras, 2009</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive analysis and native language identification. In Proceedings of the Australasian Language Technology Association Workshop 2009, pages 53–61, Sydney, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
<author>Mark Johnson</author>
</authors>
<title>Topic modeling for native language identification.</title>
<date>2011</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>115--124</pages>
<location>Canberra, Australia,</location>
<contexts>
<context position="6218" citStr="Wong et al., 2011" startWordPosition="958" endWordPosition="961">ords or function words. Brooke and Hirst (2012) presented a method of utilizing native language corpora for identifying native language in non-native texts. They used wordby-word translation of large native language corpora to create sets of second language forms that are possible results of language transfer, later used in unsupervised classification. They achieved results above random chance for L1 identification, however insufficiently accurate. More related work can be found in (Estival et al., 2007; van Halteren, 2008; Carrio-Pastor, 2009; Golcher and Reznicek, 2009; Wong and Dras, 2009; Wong et al., 2011; Brooke and Hirst, 2011; Ahn, 2011). The work mentioned above and our approach both utilize the L1-L2 transfer effect to gain information about an author’s native language. Gibbons (2009) proved the impact of native language family’s typological properties on L2. As far as we know, our work is the first to combine stylometry and native language family’s effect on L2, utilized for L1 identification. 3 Experimental Setting 3.1 Corpus We use the ICLEv2 (Granger et al., 2009) corpus that contains English documents written by intermediate to advanced international learners of English, with languag</context>
</contexts>
<marker>Wong, Dras, Johnson, 2011</marker>
<rawString>Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson. 2011. Topic modeling for native language identification. In Proceedings of the Australasian Language Technology Association Workshop 2011, pages 115– 124, Canberra, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc A Zissman</author>
</authors>
<title>Automatic language identification using gaussian mixture and hidden markov models.</title>
<date>1993</date>
<booktitle>In Proceedings of the 1993 IEEE international conference on Acoustics, speech, and signal processing: speech processing - Volume II, ICASSP’93,</booktitle>
<pages>399--402</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="4292" citStr="Zissman, 1993" startWordPosition="671" endWordPosition="672">fferent data formats, such as speech and text samples. Native language and accent identification from speech can be found 32 Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 32–39, Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics in (Choueiter et al., 2008; Tomokiyo and Jones, 2001). Identifying an author’s native language from L2 text, which is English in most cases, is the closest problem to our work. Introductory studies in the area identified the written or spoken language itself, focusing on telephone dialogue corpora (Ahmed et al., 2004; Zissman, 1993). Further studies focused on extracting specific information from text or speech after identifying the language being used. Wanneroy et al. (1999) investigated how non-native speech deteriorated language identification and used acoustic adaptation to improve it. Choueiter et al. (2008) classified different foreign accented English speech samples by using a combination of heteroscedastic LDA and maximum mutual information training. Tomokiyo and Jones (2001) characterized part-of-speech sequences and showed that Naive Bayes classification can be used to identify non-native utterances of English.</context>
</contexts>
<marker>Zissman, 1993</marker>
<rawString>Marc A. Zissman. 1993. Automatic language identification using gaussian mixture and hidden markov models. In Proceedings of the 1993 IEEE international conference on Acoustics, speech, and signal processing: speech processing - Volume II, ICASSP’93, pages 399–402, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>