<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.047328">
<title confidence="0.998096">
A Maximum Entropy Approach to Identifying Sentence
Boundaries
</title>
<author confidence="0.983826">
Jeffrey C. Reynar and Adwait Ratnaparkhi*
</author>
<affiliation confidence="0.925638">
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, Pennsylvania. USA
</affiliation>
<email confidence="0.865494">
{jcreynar, adwait}.0.unagi.cis.upenn.edu
</email>
<sectionHeader confidence="0.994814" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998848">
We present a trainable model for identify-
ing sentence boundaries in raw text. Given
a corpus annotated with sentence bound-
aries, our model learns to classify each oc-
currence of., ?, and !as either a valid or in-
valid sentence boundary. The training pro-
cedure requires no hand-crafted rules, lex-
ica, part-of-speech tags, or domain-specific
information. The model can therefore be
trained easily on any genre of English, and
should be trainable on any other Roman-
alphabet language. Performance is compa-
rable to or better than the performance of
similar systems, but we emphasize the sim-
plicity of retraining for new domains.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.967663826086957">
The task of identifying sentence boundaries in text
has not received as much attention as it deserves.
Many freely available natural language processing
tools require their input to be divided into sentences,
but make no mention of how to accomplish this (e.g.
(Brill, 1994; Collins, 1996)). Others perform the
division implicitly without discussing performance
(e.g. (Cutting et al., 1992)).
On first glance, it may appear that using a short
list, of sentence-final punctuation marks, such as .,
?, and !, is sufficient. However, these punctua-
tion marks are not used exclusively to mark sen-
tence breaks. For example, embedded quotations
may contain any of the sentence-ending punctua-
tion marks and . is used as a decimal point, in e-
mail addresses, to indicate ellipsis and in abbrevia-
tions. Both ! and ? are somewhat less ambiguous
*The authors would like to acknowledge the support
of ARPA grant N66001-94-C-6043, ARO grant DAAH04-
94-G-0426 and NSF grant SBR89-20230.
but appear in proper names and may be used mul-
tiple times for emphasis to mark a single sentence
boundary.
Lexically-based rules could be written and excep-
tion lists used to disambiguate the difficult cases
described above. However, the lists will never be
exhaustive, and multiple rules may interact badly
since punctuation marks exhibit absorption proper-
ties. Sites which logically should be marked with
multiple punctuation marks will often only have one
((Nunberg, 1990) as summarized in (White, 1995)).
For example, a sentence-ending abbreviation will
most likely not be followed by an additional period
if the abbreviation already contains one (e.g. note
that D.0 is followed by only a single . in The presi-
dent lives in Washington, D.C.).
As a result, we believe that manually writing rules
is not a good approach. Instead, we present a solu-
tion based on a maximum entropy model which re-
quires a few hints about what. information to use and
a corpus annotated with sentence boundaries. The
model trains easily and performs comparably to sys-
tems that require vastly more information. Training
on 39441 sentences takes 18 minutes on a Sun Ultra
Sparc and disambiguating the boundaries in a single
Wall Street Journal article requires only 1.4 seconds.
</bodyText>
<sectionHeader confidence="0.995809" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.961817416666667">
To our knowledge, there have been few papers about
identifying sentence boundaries. The most recent
work will be described in (Palmer and Hearst, To
appear). There is also a less detailed description of
Palmer and Hearst&apos;s system, SATZ, in (Palmer and
Hearst, 1994).&apos; The SATZ architecture uses either
a decision tree or a neural network to disambiguate
sentence boundaries. The neural network achieves
98.5% accuracy on a corpus of Wall Street Journal
We recommend these articles for a more compre-
hensive review of sentence-boundary identification work
than we will be able to provide here.
</bodyText>
<page confidence="0.995737">
16
</page>
<bodyText confidence="0.999529375">
articles using a lexicon which includes part-of-speech
(POS) tag information. By increasing the quantity
ol training data and decreasing the size of their test
corpus, Palmer and Hearst achieved performance of
!)8.9% with the neural network. They obtained simi-
lar results using the decision tree. All the results we
will present for our algorithms are on their initial,
larger test corpus.
In (Riley, 1989), Riley describes a decision-tree
based approach to the problem. His performance on
I he Brown corpus is 99.8%, using a model learned
from a. corpus of 25 million words. Liberman and
Church suggest in (Liberma.n and Church, 1992)
that a. system could be quickly built to divide
newswire text into sentences with a nearly negligible
error rate, but do not actually build such a system.
</bodyText>
<sectionHeader confidence="0.98906" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.999994958333334">
We present two systems for identifying sentence
boundaries. One is targeted at high performance
and uses some knowledge about the structure of En-
glish financial newspaper text which may not be ap-
plicable to text from other genres or in other lan-
guages. The other system uses no domain-specific
knowledge and is aimed at being portable across En-
glish text genres and Roman alphabet languages.
Potential sentence boundaries are identified by
scanning the text for sequences of characters sep-
arated by whitespace (tokens) containing one of the
symbols !, . or ?. We use information about the to-
ken containing the potential sentence boundary, as
well as contextual information about the tokens im-
mediately to the left and to the right. We also con-
ducted tests using wider contexts, but performance
did not improve.
We call the token containing the symbol which
marks a putative sentence boundary the Candidate.
&apos;Hie portion of the Candidate preceding the poten-
tial sentence boundary is called the Prefix and the
portion following it is called the Suffix. The system
that focused on maximizing performance used the
following hints, or contextual &amp;quot;templates&amp;quot;:
</bodyText>
<listItem confidence="0.999683">
• The Prefix
• The Suffix
• The presence of particular characters in the Pre-
fix or Suffix
• Whether the Candidate is an honorific (e.g.
Ms., Dr., Gen.)
• Whether the Candidate is a corporate designa-
tor (e.g. Corp. „9. p. A., L. L. C.)
• Features of the word left of the Candidate
• Features of the word right of the Candidate
</listItem>
<bodyText confidence="0.989553">
The templates specify only the form of the in-
formation. The exact information used by the
maximum entropy model for the potential sentence
boundary marked by . in Corp. in Example 1 would
be: PreviousWordIsCapitalized, Prefix= Corp, Suf-
fix=NULL, PrefixFeature=CorporateDesignator.
</bodyText>
<listItem confidence="0.898318">
(1) ANLP Corp. chairman Dr. Smith resigned.
</listItem>
<bodyText confidence="0.9994">
The highly portable system uses only the identity
of the Candidate and its neighboring words, and a
list of abbreviations induced from the training data.2
Specifically, the &amp;quot;templates&amp;quot; used are:
</bodyText>
<listItem confidence="0.99184825">
• The Prefix
• The Suffix
• Whether the Prefix or Suffix is on the list of
induced abbreviations
• The word left of the Candidate
• The word right of the Candidate
• Whether the word to the left or right of the
Candidate is on the list of induced abbreviations
</listItem>
<bodyText confidence="0.999937727272727">
The information this model would use for Exam-
ple 1 would be: PreviousWord=ANLP, Following-
Word=chairmon, Prefix=Corp, Suffix=NULL, Pre-
fixFeature=InducedAbbreviation.
The abbreviation list is automatically produced
from the training data, and the contextual ques-
tions are also automatically generated by scanning
the training data. with question templates. As a. re-
sult, no hand-crafted rules or lists are required by
the highly portable system and it can be easily re-
trained for other languages or text genres.
</bodyText>
<sectionHeader confidence="0.994422" genericHeader="method">
4 Maximum Entropy
</sectionHeader>
<bodyText confidence="0.994387">
The model used here for sentence-boundary de-
tection is based on the maximum entropy model
used for POS tagging in (Ratna.parkhi, 1996). For
each potential sentence boundary token (., ?, and
!), we estimate a. joint, probability distribution p
of the token and its surrounding context, both of
which are denoted by c, occurring as an actual
sentence boundary. The distribution is given by:
p(b, c) = Ir „,,,.f-(b„c), where b e no, yes}, where
</bodyText>
<footnote confidence="0.792677333333333">
2 A token in the training data is considered an abbre-
viation if it is preceded and followed by whitespace, and
it contains a . that is not a sentence boundary.
</footnote>
<page confidence="0.998192">
17
</page>
<bodyText confidence="0.9999815">
the cri&apos;s are the unknown parameters of the model,
and where each aj corresponds to a fi, or a feature.
Thus the probability of seeing an actual sentence
boundary in the context c is given by p(yes, c).
The contextual information deemed useful for
sentence-boundary detection, which. we described
earlier, must be encoded using features. For exam-
ple, a useful feature might be:
</bodyText>
<equation confidence="0.990097666666667">
1 if Prefix(c) = Mr &amp; b. = no
,C1 =
0 otherwise
</equation>
<bodyText confidence="0.999939909090909">
This feature will allow the model to discover that the
period at the end of the word Mr. seldom occurs as
a sentence boundary. Therefore the parameter cor-
responding to this feature will hopefully boost the
probability p(no, c) if the Prefix is Mr. The param-
eters are chosen to maximize the likelihood of the
training data, using the Generalized Iterative Scaling
(Darroch and Ratcliff, 1972) algorithm.
The model also can be viewed under the Maxi-
mum Entropy framework, in which we choose a dis-
tribution p that maximizes the entropy H (p)
</bodyText>
<equation confidence="0.995743333333333">
H (p) = —E p(b, c) logp(b, c)
under the following constraints:
Ep(b,c)fj(b,c) = /3(b , c) f i(b, c), 1 &lt;j &lt; k
</equation>
<bodyText confidence="0.9999757">
where /:5(b, c) is the observed distribution of sentence-
boundaries and contexts in the training data. As a
result, the model in practice tends not to commit
towards a particular outcome (yes or no) unless it
has seen sufficient evidence for that outcome; it is
maximally uncertain beyond meeting the evidence.
All experiments use a simple decision rule to clas-
sify each potential sentence boundary: a potential
sentence boundary is an actual sentence boundary if
and only if p(yesic) &gt; .5, where
</bodyText>
<equation confidence="0.55439">
p(yes, c) p(no, c)
</equation>
<bodyText confidence="0.997376">
and where c is the context including the potential
sentence boundary.
</bodyText>
<sectionHeader confidence="0.982127" genericHeader="method">
5 System Performance
</sectionHeader>
<bodyText confidence="0.95475775">
We trained our system on 39441 sentences (898737
words) of Wall Street Journal text from sections
00 through 24 of the second release of the Penn
Treebank3 (Marcus, Santorini, and Marcinkiewicz,
</bodyText>
<footnote confidence="0.900461666666667">
3We did not train on files which overlapped with
Palmer and Hearst&apos;s test data, namely sections 03, 04,
05 and 06.
</footnote>
<table confidence="0.998870166666667">
WSJ Brown
Sentences 20478 51672
Candidate P. Marks 32173 61282
Accuracy 98.8% 97.9%
False Positives 201 750
False Negatives 171 506
</table>
<tableCaption confidence="0.999966">
Table 1: Our best performance on two corpora.
</tableCaption>
<bodyText confidence="0.999836166666667">
1993). We corrected punctuation mistakes and er-
roneous sentence boundaries in the training data.
Performance figures for our best performing system,
which used a hand-crafted list of honorifics and cor-
porate designators, are shown in Table 1. The first
test set, WSJ, is Palmer and Hearst&apos;s initial test
data and the second is the entire Brown corpus. We
present the Brown corpus performance to show the
importance of training on the genre of text. on which
testing will be performed. Table 1 also shows the
number of sentences in each corpus, the number of
candidate punctuation marks, the accuracy over po-
tential sentence boundaries, the number of false posi-
tives and the number of false negatives. Performance
on the WSJ corpus was, as we expected, higher than
performance on the Brown corpus since we trained
the model on financial newspaper text.
Possibly more significant. than the system&apos;s per-
formance is its portability to new domains and lan-
guages. A trimmed down system which used no
information except that derived from the training
corpus performs nearly as well, and requires no re-
sources other than a training corpus. Its perfor-
mance on the same two corpora is shown in Table 2.
</bodyText>
<table confidence="0.99916775">
Test Accuracy False False
Corpus Positives Negatives
WSJ 98.0% 396 245
Brown 97.5% 1260 265
</table>
<tableCaption confidence="0.91824">
Table 2: Performance on the same two corpora using
the highly portable system.
</tableCaption>
<bodyText confidence="0.998437">
Since 39441 training sentences is considerably
more than might exist in a new domain or a lan-
guage other than English, we experimented with the
quantity of training data required to maintain per-
formance. Table 3 shows performance on the WSJ
corpus as a. function of training set size using the best
performing system and the more portable system.
As can seen from the table, performance degrades
a.s the quantity of training data decreases, but even
</bodyText>
<equation confidence="0.993869">
p(yes, c)
p(yesic) =
</equation>
<page confidence="0.996826">
18
</page>
<table confidence="0.99523975">
Number of sentences in training corpus
500 1000 2000 4000 8000 16000 39441
Best performing 97.6% 98.4% 98.0% 98.4% 98.3% 98.3% 98.8%
Highly portable 96.5% 97.3% 97.3% 97.6% 97.6% 97.8% 98.0%
</table>
<tableCaption confidence="0.999893">
Table 3: Performance on Wall Street Journal test data as a. function of training set size for both systems.
</tableCaption>
<bodyText confidence="0.9999448">
with only 500 example sentences performance is bet-
er than the baselines of 64.00/0 if a. sentence bound-
ary is guessed at every potential site and 78.4%, if
only token-final instances of sentence-ending punc-
tuation are assumed to be boundaries.
</bodyText>
<sectionHeader confidence="0.999558" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999991347826087">
We have described an approach to identifying sen-
tence boundaries which performs comparably to
other state-of-the-art systems that require vastly
more resources. For example, Riley&apos;s performance
on the Brown corpus is higher than ours, but his sys-
tem is trained on the Brown corpus and uses thirty
times as much data as our system. Also, Palmer
L Hearst&apos;s system requires POS tag information,
which limits its use to those genres or languages for
which there are either POS tag lexica or POS tag
annotated corpora that could be used to train auto-
matic taggers. In comparison, our system does not
require POS ta.gs or any supporting resources be-
yond the sentence-boundary annotated corpus. It
is therefore easy and inexpensive to retrain this sys-
tem for different genres of text in English and text in
other Roman-alphabet languages. Furthermore, we
showed tha.t a small training corpus is sufficient for
good performance, and we estimate that annotating
enough data to achieve good performance would re-
quire only several hours of work, in comparison to
the many hours required to generate POS tag and
lexical probabilities.
</bodyText>
<sectionHeader confidence="0.999016" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999946">
We would like to thank David Palmer for giving us
the test data he and Marti Hearst used for their
sentence detection experiments. We would also like
to thank the anonymous reviewers for their helpful
insights.
</bodyText>
<sectionHeader confidence="0.998634" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992586326530612">
Brill, Eric. 1994. Some advances in transformation-
based part-of-speech tagging. In Proceedings of
the Twelfth National Conference on Artificial In-
telligence, volume 1, pages 722-727.
Collins, Michael. 1996. A new statistical parser
I ased on bigram lexical dependencies. In Proceed-
ings of t&amp; 34&amp;quot; Annual Meeting of the Association
for Computational Linguistics, June.
Cutting, Doug, .1 ulian Kupiec, .1 an Pedersen, and
Penelope Sibun. 1992. A practical part-of-speech
tagger. In Proceedings of the Third Conference on
Applied Natural Language Processing, pages 133-
140, Trento, Italy, April.
Darroch, J. N. and D. Ratcliff. 1972. Generalized
Iterative Scaling for Log-Linear Models. The An-
nals of Mathematical Statistics, 43(5):1470-1480.
Liberman, Mark Y. and Kenneth W. Church. 1992.
Text analysis and word pronunciation in text-to-
speech synthesis. In Sadaoki Ruin and M. Mohan
Sondi, editors, Advances in Speech Signal Process-
ing. Marcel Dekker, Incorporated, New York.
Marcus, Mitchell, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treeba.nk. Computa-
tional Linguistics, 19(2)313-330.
Nunberg, Geoffrey. 1990. The Linguistics of Punc-
tuation. Number 18 in CSLI Lecture Notes. Uni-
versity of Chicago Press.
Palmer, David D. and Marti A. Hearst. 1994. Adap-
tive sentence boundary disambiguation. In Pro-
ceedings of the 1994 conference on Applied Natu-
ral Language Processing (ANLP), Stuttgart, Ger-
many, October.
Palmer, David D. and Marti A. Hearst. To appear.
Adaptive multilingual sentence boundary disam-
biguation. Computational Linguistics.
Ratnaparkhi, Adwait. 1996. A maximum entropy
model for part-of-speech tagging. In Conference
on. Empirical Methods in Natural Language Pro-
cessing, pages 133-142, University of Pennsylva-
nia, May 17-18.
Riley, Michael D. 1989. Some applications of
tree-based modelling to speech and language. In
DARPA Speech and Language Technology Work-
shop, pages 339-352, Cape Cod, Massachusetts.
White, Michael. 1995. Presenting punctuation. In
Proceedings of the Fifth European Workshop on
Natural Language Generation, pages 107-125, Lei-
den, The Netherlands.
</reference>
<page confidence="0.999333">
19
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.917608">
<title confidence="0.9711055">A Maximum Entropy Approach to Identifying Sentence Boundaries</title>
<author confidence="0.999839">Jeffrey C Reynar</author>
<author confidence="0.999839">Adwait Ratnaparkhi</author>
<affiliation confidence="0.9999055">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.999568">Philadelphia, Pennsylvania. USA</address>
<email confidence="0.997839">jcreynar.0.unagi.cis.upenn.edu</email>
<email confidence="0.997839">adwait.0.unagi.cis.upenn.edu</email>
<abstract confidence="0.9985170625">We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of., ?, and !as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Some advances in transformationbased part-of-speech tagging.</title>
<date>1994</date>
<booktitle>In Proceedings of the Twelfth National Conference on Artificial Intelligence,</booktitle>
<volume>1</volume>
<pages>722--727</pages>
<contexts>
<context position="1170" citStr="Brill, 1994" startWordPosition="175" endWordPosition="176">art-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains. 1 Introduction The task of identifying sentence boundaries in text has not received as much attention as it deserves. Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (Brill, 1994; Collins, 1996)). Others perform the division implicitly without discussing performance (e.g. (Cutting et al., 1992)). On first glance, it may appear that using a short list, of sentence-final punctuation marks, such as ., ?, and !, is sufficient. However, these punctuation marks are not used exclusively to mark sentence breaks. For example, embedded quotations may contain any of the sentence-ending punctuation marks and . is used as a decimal point, in email addresses, to indicate ellipsis and in abbreviations. Both ! and ? are somewhat less ambiguous *The authors would like to acknowledge t</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>Brill, Eric. 1994. Some advances in transformationbased part-of-speech tagging. In Proceedings of the Twelfth National Conference on Artificial Intelligence, volume 1, pages 722-727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser I ased on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of t&amp; 34&amp;quot; Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="1186" citStr="Collins, 1996" startWordPosition="177" endWordPosition="178"> tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains. 1 Introduction The task of identifying sentence boundaries in text has not received as much attention as it deserves. Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (Brill, 1994; Collins, 1996)). Others perform the division implicitly without discussing performance (e.g. (Cutting et al., 1992)). On first glance, it may appear that using a short list, of sentence-final punctuation marks, such as ., ?, and !, is sufficient. However, these punctuation marks are not used exclusively to mark sentence breaks. For example, embedded quotations may contain any of the sentence-ending punctuation marks and . is used as a decimal point, in email addresses, to indicate ellipsis and in abbreviations. Both ! and ? are somewhat less ambiguous *The authors would like to acknowledge the support of AR</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Collins, Michael. 1996. A new statistical parser I ased on bigram lexical dependencies. In Proceedings of t&amp; 34&amp;quot; Annual Meeting of the Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Cutting</author>
</authors>
<title>A practical part-of-speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<volume>1</volume>
<pages>133--140</pages>
<location>Trento, Italy,</location>
<marker>Cutting, 1992</marker>
<rawString>Cutting, Doug, .1 ulian Kupiec, .1 an Pedersen, and Penelope Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, pages 133-140, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized Iterative Scaling for Log-Linear Models. The Annals of Mathematical Statistics,</title>
<date>1972</date>
<pages>43--5</pages>
<contexts>
<context position="8753" citStr="Darroch and Ratcliff, 1972" startWordPosition="1427" endWordPosition="1430">y p(yes, c). The contextual information deemed useful for sentence-boundary detection, which. we described earlier, must be encoded using features. For example, a useful feature might be: 1 if Prefix(c) = Mr &amp; b. = no ,C1 = 0 otherwise This feature will allow the model to discover that the period at the end of the word Mr. seldom occurs as a sentence boundary. Therefore the parameter corresponding to this feature will hopefully boost the probability p(no, c) if the Prefix is Mr. The parameters are chosen to maximize the likelihood of the training data, using the Generalized Iterative Scaling (Darroch and Ratcliff, 1972) algorithm. The model also can be viewed under the Maximum Entropy framework, in which we choose a distribution p that maximizes the entropy H (p) H (p) = —E p(b, c) logp(b, c) under the following constraints: Ep(b,c)fj(b,c) = /3(b , c) f i(b, c), 1 &lt;j &lt; k where /:5(b, c) is the observed distribution of sentenceboundaries and contexts in the training data. As a result, the model in practice tends not to commit towards a particular outcome (yes or no) unless it has seen sufficient evidence for that outcome; it is maximally uncertain beyond meeting the evidence. All experiments use a simple deci</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>Darroch, J. N. and D. Ratcliff. 1972. Generalized Iterative Scaling for Log-Linear Models. The Annals of Mathematical Statistics, 43(5):1470-1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Y Liberman</author>
<author>Kenneth W Church</author>
</authors>
<title>Text analysis and word pronunciation in text-tospeech synthesis.</title>
<date>1992</date>
<booktitle>In Sadaoki Ruin</booktitle>
<editor>and M. Mohan Sondi, editors,</editor>
<publisher>Marcel Dekker, Incorporated,</publisher>
<location>New York.</location>
<marker>Liberman, Church, 1992</marker>
<rawString>Liberman, Mark Y. and Kenneth W. Church. 1992. Text analysis and word pronunciation in text-tospeech synthesis. In Sadaoki Ruin and M. Mohan Sondi, editors, Advances in Speech Signal Processing. Marcel Dekker, Incorporated, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treeba.nk.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, Mitchell, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treeba.nk. Computational Linguistics, 19(2)313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Nunberg</author>
</authors>
<title>The Linguistics of Punctuation.</title>
<date>1990</date>
<journal>Number</journal>
<volume>18</volume>
<publisher>University of Chicago Press.</publisher>
<note>in CSLI Lecture Notes.</note>
<contexts>
<context position="2342" citStr="Nunberg, 1990" startWordPosition="361" endWordPosition="362">s *The authors would like to acknowledge the support of ARPA grant N66001-94-C-6043, ARO grant DAAH04- 94-G-0426 and NSF grant SBR89-20230. but appear in proper names and may be used multiple times for emphasis to mark a single sentence boundary. Lexically-based rules could be written and exception lists used to disambiguate the difficult cases described above. However, the lists will never be exhaustive, and multiple rules may interact badly since punctuation marks exhibit absorption properties. Sites which logically should be marked with multiple punctuation marks will often only have one ((Nunberg, 1990) as summarized in (White, 1995)). For example, a sentence-ending abbreviation will most likely not be followed by an additional period if the abbreviation already contains one (e.g. note that D.0 is followed by only a single . in The president lives in Washington, D.C.). As a result, we believe that manually writing rules is not a good approach. Instead, we present a solution based on a maximum entropy model which requires a few hints about what. information to use and a corpus annotated with sentence boundaries. The model trains easily and performs comparably to systems that require vastly mo</context>
</contexts>
<marker>Nunberg, 1990</marker>
<rawString>Nunberg, Geoffrey. 1990. The Linguistics of Punctuation. Number 18 in CSLI Lecture Notes. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Palmer</author>
<author>Marti A Hearst</author>
</authors>
<title>Adaptive sentence boundary disambiguation.</title>
<date>1994</date>
<booktitle>In Proceedings of the 1994 conference on Applied Natural Language Processing (ANLP),</booktitle>
<location>Stuttgart, Germany,</location>
<contexts>
<context position="3406" citStr="Palmer and Hearst, 1994" startWordPosition="536" endWordPosition="539">bout what. information to use and a corpus annotated with sentence boundaries. The model trains easily and performs comparably to systems that require vastly more information. Training on 39441 sentences takes 18 minutes on a Sun Ultra Sparc and disambiguating the boundaries in a single Wall Street Journal article requires only 1.4 seconds. 2 Previous Work To our knowledge, there have been few papers about identifying sentence boundaries. The most recent work will be described in (Palmer and Hearst, To appear). There is also a less detailed description of Palmer and Hearst&apos;s system, SATZ, in (Palmer and Hearst, 1994).&apos; The SATZ architecture uses either a decision tree or a neural network to disambiguate sentence boundaries. The neural network achieves 98.5% accuracy on a corpus of Wall Street Journal We recommend these articles for a more comprehensive review of sentence-boundary identification work than we will be able to provide here. 16 articles using a lexicon which includes part-of-speech (POS) tag information. By increasing the quantity ol training data and decreasing the size of their test corpus, Palmer and Hearst achieved performance of !)8.9% with the neural network. They obtained similar result</context>
</contexts>
<marker>Palmer, Hearst, 1994</marker>
<rawString>Palmer, David D. and Marti A. Hearst. 1994. Adaptive sentence boundary disambiguation. In Proceedings of the 1994 conference on Applied Natural Language Processing (ANLP), Stuttgart, Germany, October.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David D Palmer</author>
<author>Marti A Hearst</author>
</authors>
<title>To appear. Adaptive multilingual sentence boundary disambiguation. Computational Linguistics.</title>
<marker>Palmer, Hearst, </marker>
<rawString>Palmer, David D. and Marti A. Hearst. To appear. Adaptive multilingual sentence boundary disambiguation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Conference on. Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<institution>University of Pennsylvania,</institution>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi, Adwait. 1996. A maximum entropy model for part-of-speech tagging. In Conference on. Empirical Methods in Natural Language Processing, pages 133-142, University of Pennsylvania, May 17-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael D Riley</author>
</authors>
<title>Some applications of tree-based modelling to speech and language.</title>
<date>1989</date>
<booktitle>In DARPA Speech and Language Technology Workshop,</booktitle>
<pages>339--352</pages>
<location>Cape Cod, Massachusetts.</location>
<contexts>
<context position="4142" citStr="Riley, 1989" startWordPosition="654" endWordPosition="655">work achieves 98.5% accuracy on a corpus of Wall Street Journal We recommend these articles for a more comprehensive review of sentence-boundary identification work than we will be able to provide here. 16 articles using a lexicon which includes part-of-speech (POS) tag information. By increasing the quantity ol training data and decreasing the size of their test corpus, Palmer and Hearst achieved performance of !)8.9% with the neural network. They obtained similar results using the decision tree. All the results we will present for our algorithms are on their initial, larger test corpus. In (Riley, 1989), Riley describes a decision-tree based approach to the problem. His performance on I he Brown corpus is 99.8%, using a model learned from a. corpus of 25 million words. Liberman and Church suggest in (Liberma.n and Church, 1992) that a. system could be quickly built to divide newswire text into sentences with a nearly negligible error rate, but do not actually build such a system. 3 Our Approach We present two systems for identifying sentence boundaries. One is targeted at high performance and uses some knowledge about the structure of English financial newspaper text which may not be applica</context>
</contexts>
<marker>Riley, 1989</marker>
<rawString>Riley, Michael D. 1989. Some applications of tree-based modelling to speech and language. In DARPA Speech and Language Technology Workshop, pages 339-352, Cape Cod, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>Presenting punctuation.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fifth European Workshop on Natural Language Generation,</booktitle>
<pages>107--125</pages>
<location>Leiden, The Netherlands.</location>
<contexts>
<context position="2373" citStr="White, 1995" startWordPosition="366" endWordPosition="367">owledge the support of ARPA grant N66001-94-C-6043, ARO grant DAAH04- 94-G-0426 and NSF grant SBR89-20230. but appear in proper names and may be used multiple times for emphasis to mark a single sentence boundary. Lexically-based rules could be written and exception lists used to disambiguate the difficult cases described above. However, the lists will never be exhaustive, and multiple rules may interact badly since punctuation marks exhibit absorption properties. Sites which logically should be marked with multiple punctuation marks will often only have one ((Nunberg, 1990) as summarized in (White, 1995)). For example, a sentence-ending abbreviation will most likely not be followed by an additional period if the abbreviation already contains one (e.g. note that D.0 is followed by only a single . in The president lives in Washington, D.C.). As a result, we believe that manually writing rules is not a good approach. Instead, we present a solution based on a maximum entropy model which requires a few hints about what. information to use and a corpus annotated with sentence boundaries. The model trains easily and performs comparably to systems that require vastly more information. Training on 394</context>
</contexts>
<marker>White, 1995</marker>
<rawString>White, Michael. 1995. Presenting punctuation. In Proceedings of the Fifth European Workshop on Natural Language Generation, pages 107-125, Leiden, The Netherlands.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>