<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007621">
<title confidence="0.987562666666667">
Learning Correlations between Linguistic Indicators and Semantic
Constraints:
Reuse of Context-Dependent Descriptions of Entities
</title>
<author confidence="0.989278">
Dragomir R. Radev
</author>
<affiliation confidence="0.9961765">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.990966">
New York, NY 10027
</address>
<email confidence="0.95311">
radevAcs.columbia.edu
</email>
<sectionHeader confidence="0.996472" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989857142857">
This paper presents the results of a study on the
semantic constraints imposed on lexical choice
by certain contextual indicators. We show how
such indicators are computed and how correla-
tions between them and the choice of a noun
phrase description of a named entity can be au-
tomatically established using supervised learn-
ing. Based on this correlation, we have devel-
oped a technique for automatic lexical choice of
descriptions of entities in text generation. We
discuss the underlying relationship between the
pragmatics of choosing an appropriate descrip-
tion that serves a specific purpose in the auto-
matically generated text and the semantics of
the description itself. We present our work in
the framework of the more general concept of
reuse of linguistic structures that are automati-
cally extracted from large corpora. We present
a formal evaluation of our approach and we con-
clude with some thoughts on potential applica-
tions of our method.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999502339285714">
Human writers constantly make deliberate deci-
sions about picking a particular way of express-
ing a certain concept. These decisions are made
based on the topic of the text and the effect that
the writer wants to achieve. Such contextual
and pragmatic constraints are obvious to ex-
perienced writers who produce context-specific
text without much effort. However, in order for
a computer to produce text in a similar way,
either these constraints have to be added man-
ually by an expert or the system must be able
to acquire them in an automatic way.
An example related to the lexical choice of
an appropriate nominal description of a person
should make the above clear. Even though it
seems intuitive that Bill Clinton should always
be described with the NP &amp;quot;U. S. president&amp;quot; or a
variation thereof, it turns out that many other
descriptions appear in on-line news stories that
characterize him in light of the topic of the arti-
cle. For example, an article from 1996 on elec-
tions uses &amp;quot;Bill Clinton, the democratic pres-
idential candidate&amp;quot;, while a 1997 article on a
false bomb alert in Little Rock, Ark. uses &amp;quot;Bill
Clinton, an Arkansas native&amp;quot;.
This paper presents the results of a study of
the correlation between named entities (people,
places, or organizations) and noun phrases used
to describe them in a corpus.
Intuitively, the use of a description is based on
a deliberate decision on the part of the author
of a piece of text. A writer is likely to select a
description that puts the entity in the context
of the rest of the article.
It is known that the distribution of words in
a document is related to its topic (Salton and
McGill, 1983). We have developed related tech-
niques for approximating pragmatic constraints
using words that appear in the immediate con-
text of the entity.
We will show that context influences the
choice of a description, as do several other lin-
guistic indicators. Each of the indicators by it-
self doesn&apos;t provide enough empirical data that
distinguishes among all descriptions that are re-
lated to a an entity. However, a carefully se-
lected combination of such indicators provides
enough information in order pick an appropriate
description with more than 80% accuracy.
Section 2 describes how we can automatically
obtain enough constraints on the usage of de-
scriptions. In Section 3, we show how such con-
structions are related to language reuse.
In Section 4 we describe our experimental
setup and the algorithms that we have designed.
Section 5 includes a description of our results.
</bodyText>
<page confidence="0.994131">
1072
</page>
<bodyText confidence="0.999261666666667">
In Section 6 we discuss some possible exten-
sions to our study and we provide some thoughts
about possible uses of our framework.
</bodyText>
<sectionHeader confidence="0.945417" genericHeader="introduction">
2 Problem Description
</sectionHeader>
<bodyText confidence="0.611062923076923">
Let&apos;s define the relation Description° f (E) to
be the one between a named entity E and a
noun phrase, D, describing the named entity.
In the example shown in Table 1, there are two
entity-description pairs.
Description0 f (&amp;quot;Tareq Aziz&amp;quot;) = &amp;quot;Iraq&apos;s
Deputy Prime Minister&amp;quot;
Description0 f (&amp;quot;Richard Butler&amp;quot;) = &amp;quot;Chief
U.N. arms inspector&amp;quot;
Chief U.N. arms inspector Richard Butler
met Iraq&apos;s Deputy Prime Minister Tareq Aziz
Monday after rejecting Iraqi attempts to set
deadlines for finishing his work.
</bodyText>
<figureCaption confidence="0.991372">
Figure 1: Sample sentence containing two
entity-description pairs.
</figureCaption>
<bodyText confidence="0.994387688888889">
Each entity appearing in a text can have mul-
tiple descriptions (up to several dozen) associ-
ated with it.
We call the set of all descriptions related to
the same entity in a corpus, a profile of that
entity. Profiles for a large number of entities
were compiled using our earlier system, PRO-
FILE (Radev and McKeown, 1997). It turns
out that there is a large variety in the size of
the profile (number of distinct descriptions) for
different entities. Table 1 shows a subset of the
profile for Ung Huot, the former foreign minister
of Cambodia, who was elected prime minister at
some point of time during the run of our exper-
iment. A few sample semantic features of the
descriptions in Table 1 are shown as separate
columns.
We used information extraction techniques to
collect entities and descriptions from a corpus
and analyzed their lexical and semantic proper-
ties.
We have processed 178 MB1 of newswire
and analyzed the use of descriptions related
to 11,504 entities. Even though PROFILE ex-
tracts other entities in addition to people (e.g.,
&apos;The corpus contains 19,473 news stories that cover
the period October 1, 1997 - January 9, 1998 that were
available through PROFILE.
places and organizations), we have restricted
our analysis to names of people only. We claim,
however, that a large portion of our findings re-
late to the other types of entities as well.
We have investigated 35,206 tuples, consist-
ing of an entity, a description, an article ID,
and the position (sentence number) in the arti-
cle in which the entity-description pair occurs.
Since there are 11,504 distinct entities, we had
on average 3.06 distinct descriptions per entity
(DDPE). Table 2 shows the distribution of
DDPE values across the corpus. Notice that a
large number of entities (9,053 out of the 11,504)
have a single description. These are not as in-
teresting for our analysis as the remaining 2,451
entities that have DDPE values between 2 and
24.
</bodyText>
<figureCaption confidence="0.940275">
Figure 2: Number of distinct descriptions per
entity (log-log scale)
</figureCaption>
<sectionHeader confidence="0.940872" genericHeader="method">
3 Language Reuse in Text
Generation
</sectionHeader>
<bodyText confidence="0.999752846153846">
Text generation usually involves lexical choice -
that is, choosing one way of referring to an en-
tity over another. Lexical choice refers to a vari-
ety of decisions that have to made in text gener-
ation. For example, picking one among several
equivalent (or newly equivalent) constructions
is a form of lexical choice (e.g., &amp;quot;The Utah Jazz
handed the Boston Celtics a defeat&amp;quot; vs. &amp;quot;The
Utah Jazz defeated the Boston Celtics&amp;quot; (Robin,
1994)). We are interested in a different aspect
of the problem: namely learning the rules that
can be used for automatically selecting an ap-
propriate description of an entity in a specific
</bodyText>
<figure confidence="0.580587">
1
10&apos; 10&apos;
X Numb/br 01 &amp;Mind deariplbm (DDPE)
</figure>
<page confidence="0.559204">
1073
</page>
<table confidence="0.9934574">
Description Semantic categories
addressing country male new political post seniority
a senior member X
Cambodia&apos;s X
Cambodian foreign minister X X
co-premier X
first prime minister X
foreign minister X
His Excellency X
Mr. X X X
new co-premier X X
new first prime minister X X
newly-appointed first prime minister X
premier X
prime minister
</table>
<tableCaption confidence="0.9991">
Table 1: Profile of Ung Huot
</tableCaption>
<table confidence="0.518735625">
DDPE count DDPE count DDPE count
1 9,053 8 27 15 4
2 1,481 9 26 16 2
3 472 10 12 17 2
4 182 11 10 18 1
5 112 12 8 19 1
6 74 13 2 24 1
7 31 14 3
</table>
<tableCaption confidence="0.947766">
Table 2: Number of distinct descriptions per entity (DDPE).
</tableCaption>
<bodyText confidence="0.99933124">
context.
To be feasible and scaleable, a technique for
solving a particular case of the problem of lex-
ical choice must involve automated learning. It
is also useful if the technique can specify enough
constraints on the text to be generated so that
the number of possible surface realizations that
match the semantic constraints is reduced sig-
nificantly. The easiest case in which lexical
choice can be made is when the full surface
structure can be used, and when it has been au-
tomatically extracted from a corpus. Of course,
the constraints on the use of the structure in the
generated text have to be reasonably similar to
the ones in the source text.
We have found that a natural application for
the analysis of entity-description pairs is lan-
guage reuse, which includes techniques of ex-
tracting shallow structure from a corpus and
applying that structure to computer-generated
texts.
Language reuse involves two components: a
source text written by a human and a target
text, that is to be automatically generated by
a computer, partially making use of structures
reused from the source text. The source text
is the one from which particular surface struc-
tures are extracted automatically, along with
the appropriate syntactic, semantic, and prag-
matic constraints under which they are used.
Some examples of language reuse include col-
location analysis (Smadja, 1993), the use of
entire factual sentences extracted from cor-
pora (e.g., &amp;quot;Toy Story&apos; is the Academy Award
winning animated film developed by Pixar&amp;quot;),
and summarization using sentence extraction
(Paice, 1990; Kupiec et al., 1995). In the case
of summarization through sentence extraction,
the target text has the additional property of
being a subtext of the source text. Other tech-
niques that can be broadly categorized as lan-
guage reuse are learning relations from on-line
texts (Mitchell, 1997) and answering natural
language questions using an on-line encyclope-
dia (Kupiec, 1993).
Stydying the concept of language reuse is re-
warding because it allows generation systems to
leverage on texts written by humans and their
deliberate choice of words, facts, structure.
We mentioned that for language reuse to take
</bodyText>
<page confidence="0.987678">
1074
</page>
<bodyText confidence="0.99993378125">
place, the generation system has to use the same
surface structure in the same syntactic, seman-
tic, and pragmatic context as the source text
from which it was extracted. Obviously, all of
this information is typically not available to a
generation system. There are some special cases
in which most of it can be automatically com-
puted.
Descriptions of entities are a particular in-
stance of a surface structure that can be reused
relatively easily. Syntactic constraints related
to the use of descriptions are modest - since de-
scriptions are always noun phrases that appear
as either pre-modifiers or appositions2, they are
quite flexibly usable in any generated text in
which an entity can be modified with an ap-
propriate description. We will show in the rest
of the paper how the requisite semantic (i.e.,
&amp;quot;what is the meaning of the description to pick&amp;quot;)
and pragmatic constraints (i.e., &amp;quot;what purpose
does using the description achieve?&amp;quot;) can be ex-
tracted automatically.
Given a profile like the one shown in Table 1,
and an appropriate set of semantic constraints
(columns 2-7 of the table), the generation com-
ponent needs to perform a profile lookup and
select a row (description) that satisfies most or
all semantic constraints. For example, if the se-
mantic constraints specify that the description
has to include the country and the political po-
sition of Ung Huot, the most appropriate de-
scription is &amp;quot;Cambodian foreign minister&amp;quot;.
</bodyText>
<sectionHeader confidence="0.99942" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.991877162790698">
In our experiments, we have used two widely
available tools - WordNet and Ripper.
WordNet (Miller et al., 1990) is an on-line
hierarchical lexical database which contains se-
mantic information about English words (in-
cluding hypernymy relations which we use in
our system). We use chains of hypernyms when
we need to approximate the usage of a particu-
lar word in a description using its ancestor and
sibling nodes in WordNet. Particularly useful
for our application are the synset offsets of the
words in a description. The synset offset is a
number that uniquely identifies a concept node
(synset) in the WordNet hierarchy. Figure 3
shows that the synset offset for the concept &amp;quot;ad-
ministrator, decision maker&amp;quot; is &amp;quot;{07063507} &amp;quot;,
2We haven&apos;t included relative clauses in our study.
while its hypernym, &amp;quot;head, chief, top dog&amp;quot; has
a synset offset of &amp;quot;{ 07311393} &amp;quot;.
Ripper (Cohen, 1995) is an algorithm that
learns rules from example tuples in a relation.
Attributes in the tuples can be integers (e.g.,
length of an article, in words), sets (e.g., se-
mantic features), or bags (e.g., words that ap-
pear in a sentence or document). We use Rip-
per to learn rules that correlate context and
other linguistic indicators with the semantics
of the description being extracted and subse-
quently reused. It is important to notice that
Ripper is designed to learn rules that classify
data into atomic classes (e.g., &amp;quot;good&amp;quot;, &amp;quot;aver-
age&amp;quot;, and &amp;quot;bad&amp;quot;). We had to modify its al-
gorithm in order to classify data into sets of
atoms. For example, a rule can have the form
&amp;quot;if CONDITION then A07063762} 1028643261
{00017954}.13. This rule states that if a certain
&amp;quot;CONDITION&amp;quot; (which is a function of the in-
dicators related to the description) is met, then
the description is likely to contain words that
are semantically related to the three WordNet
nodes R07063762} {02864326} 100017954H.
The stages of our experiments are described
in detail in the remainder of this section.
</bodyText>
<subsectionHeader confidence="0.998473">
4.1 Semantic tagging of descriptions
</subsectionHeader>
<bodyText confidence="0.9999925">
Our system, PROFILE, processes WWW-
accessible newswire on a round-the-clock basis
and extracts entities (people, places, and orga-
nizations) along with related descriptions. The
extraction grammar, developed in CREP (Du-
ford, 1993), covers a variety of pre-modifier and
appositional noun phrases.
For each word wi in a description, we use a
version of WordNet to extract the synset offset
of the immediate parent of wi.
</bodyText>
<subsectionHeader confidence="0.999668">
4.2 Finding linguistic cues
</subsectionHeader>
<bodyText confidence="0.999431666666667">
Initially, we were interested in discovering rules
manually and then validating them using the
learning algorithm. However, the task proved
(nearly) impossible considering the sheer size
of the corpus. One possible rule that we hy-
pothesized and wanted to verify empirically at
this stage was parallelism. This linguistically-
motivated rule states that in a sentence with
a parallel structure (consider, for instance, the
</bodyText>
<footnote confidence="0.8653225">
&apos;These offsets correspond to the WordNet nodes
&amp;quot;manager&amp;quot;, &amp;quot;internet&amp;quot;, and &amp;quot;group&amp;quot;
</footnote>
<page confidence="0.982694">
1075
</page>
<table confidence="0.92621">
DIRECTOR: {07063762} director, manager, managing director
{07063507} administrator, decision maker
{07311393} head, chief, top dog
{06950891} leader
{00004123} person, individual, someone, somebody, mortal, human, soul
{00002086} life form, organism, being, living thing
{00001740} entity, something
</table>
<figureCaption confidence="0.996056">
Figure 3: Hypernym chain of &amp;quot;director&amp;quot; in WordNet, showing synset offsets.
</figureCaption>
<bodyText confidence="0.998608133333333">
sentence fragment &amp;quot;... Alija Izetbegovic, a Mus-
lim, Kresimir Zubak, a Croat, and Momcilo
Krajisnik, a Serb...&amp;quot;) all entities involved have
similar descriptions. However, rules at such a
detailed syntactic level take too long to process
on a 180 MB corpus and, further, no more than
a handful of such rules can be discovered manu-
ally. As a result, we made a decision to extract
all indicators automatically. We would also like
to note that using syntactic information on such
a large corpus doesn&apos;t appear particularly fea-
sible. We limited therefore our investigation
to lexical, semantic, and contextual indicators
only. The following subsection describes the at-
tributes used.
</bodyText>
<subsectionHeader confidence="0.9843755">
4.3 Extracting linguistic cues
automatically
</subsectionHeader>
<bodyText confidence="0.996053">
The list of indicators that we use in our system
are the following:
</bodyText>
<listItem confidence="0.972943555555556">
• Context: (using a window of size 4, ex-
cluding the actual description used, but
not the entity itself) - e.g., &amp;quot;rclinton&apos;
&apos;clinton&apos; &apos;counsel&apos; &apos;counsel&apos; &apos;decision&apos; &apos;deci-
sion&apos; &apos;gore&apos; &apos;gore&apos; &apos;md&apos; &apos;md&apos; &apos;index&apos; &apos;news&apos;
&apos;november&apos; wednesday1&amp;quot; is a bag of words
found near the description of Bill Clinton
in the training corpus.
• Length of the article: - an integer.
• Name of the entity: - e.g., &amp;quot;Bill Clin-
ton&amp;quot;.
• Profile: The entire profile related to a per-
son (all descriptions of that person that are
found in the training corpus).
• Synset Offsets: - the WordNet node num-
bers of all words (and their parents)) that
appear in the profile associated with the
entity that we want to describe.
</listItem>
<subsectionHeader confidence="0.999873">
4.4 Applying machine learning method
</subsectionHeader>
<bodyText confidence="0.9999448">
To learn rules, we ran Ripper on 90% (10,353)
of the entities in the entire corpus. We kept the
remaining 10% (or 1,151 entities) for evaluation.
Sample rules discovered by the system are
shown in Table 3.
</bodyText>
<sectionHeader confidence="0.99886" genericHeader="evaluation">
5 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.999958222222222">
We have performed a standard evaluation of the
precision and recall that our system achieves in
selecting a description. Table 4 shows our re-
sults under two sets of parameters.
Precision and recall are based on how well the
system predicts a set of semantic constraints.
Precision (or 13) is defined to be the number of
matches divided by the number of elements in
the predicted set. Recall (or R) is the number
of matches divided by the number of elements
in the correct set. If, for example, the system
predicts [A] [B] [C], but the set of constraints
on the actual description is [B./ [DI, we would
compute that P = 33.3% and R = 50.0%. Ta-
ble 4 reports the average values of P and R for
all training examples4.
Selecting appropriate descriptions based on
our algorithm is feasible even though the val-
ues of precision and recall obtained may seem
only moderately high. The reason for this is
that the problem that we are trying to solve is
underspecified. That is, in the same context,
more than one description can be potentially
used. Mutually interchangeable descriptions in-
clude synonyms and near synonyms (&amp;quot;leader&amp;quot;
vs. &amp;quot;chief) or pairs of descriptions of different
generality (U.S. president vs. president). This
</bodyText>
<footnote confidence="0.99960525">
4We run Ripper in a so-called &amp;quot;noise-free mode&amp;quot;,
which causes the condition parts of the rules it discovers
to be mutually exclusive and therefore, the values of P
and R on the training data are both 100%.
</footnote>
<page confidence="0.949885">
1076
</page>
<table confidence="0.99925775">
Rule Decision
IF CONTEXT - inflation {09613349} (politician)
IF PROFILES - detective AND CONTEXT - agency {07485319} (policeman)
IF CONTEXT - celine {07032298} (north_american)
</table>
<tableCaption confidence="0.957879">
Table 3: Sample rules discovered by the system.
</tableCaption>
<table confidence="0.998937583333333">
word nodes only word and parent nodes
Training set size Precision Recall Precision Recall
500 64.29% 2.86% 78.57% 2.86%
1,000 71.43% 2.86% 85.71% 2.86%
2,000 42.86% 40.71% 67.86% 62.14%
5,000 59.33% 48.40% 64.67% 53.73%
10,000 69.72% 45.04% 74.44% 59.32%
15,000 76.24% 44.02% 73.39% 53.17%
20,000 76.25% 49.91% 79.08% 58.70%
25,000 83.37% 52.26% 82.39% 57.49%
30,000 80.14% 50.55% 82.77% 57.66%
50,000 83.13% 58.54% 88.87% 63.39%
</table>
<tableCaption confidence="0.9793105">
Table 4: Values for precision and recall using word nodes only (left) and both word and parent
nodes (right).
</tableCaption>
<bodyText confidence="0.99145895">
type of evaluation requires the availability of hu-
man judges.
There are two parts to the evaluation: how
well does the system performs in selecting se-
mantic features (WordNet nodes) and how well
it works in constraining the choice of a descrip-
tion. To select a description, our system does a
lookup in the profile for a possible description
that satisfies most semantic constraints (e.g., we
select a row in Table 1 based on constraints on
the columns).
Our system depends crucially on the multiple
components that we use. For example, the shal-
low CREP grammar that is used in extracting
entities and descriptions often fails to extract
good descriptions, mostly due to incorrect PP
attachment. We have also had problems from
the part-of-speech tagger and, as a result, we
occasionally incorrectly extract word sequences
that do not represent descriptions.
</bodyText>
<sectionHeader confidence="0.997724" genericHeader="evaluation">
6 Applications and Future Work
</sectionHeader>
<bodyText confidence="0.999904">
We should note that PROFILE is part of a
large system for information retrieval and sum-
marization of news through information extrac-
tion and symbolic text generation (McKeown
and Radev, 1995). We intend to use PROFILE
to improve lexical choice in the summary gen-
eration component, especially when producing
user-centered summaries or summary updates
(Radev and McKeown, 1998 to appear). There
are two particularly appealing cases - (1) when
the extraction component has failed to extract a
description and (2) when the user model (user&apos;s
interests, knowledge of the entity and personal
preferences for sources of information and for ei-
ther conciseness or verbosity) dictates that a de-
scription should be used even when one doesn&apos;t
appear in the texts being summarized.
A second potentially interesting application
involves using the data and rules extracted by
PROFILE for language regeneration. In (Radev
and McKeown, 1998 to appear) we show how
the conversion of extracted descriptions into
components of a generation grammar allows for
flexible (re)generation of new descriptions that
don&apos;t appear in the source text. For example,
a description can be replaced by a more general
one, two descriptions can be combined to form
a single one, or one long description can be de-
constructed into its components, some of which
can be reused as new descriptions.
We are also interested in investigating an-
other idea - that of predicting the use of a de-
scription of an entity even when the correspond-
ing profile doesn&apos;t contain any description at all,
or when it contains only descriptions that con-
tain words that are not directly related to the
words predicted by the rules of PROFILE. In
this case, if the system predicts a semantic cat-
</bodyText>
<page confidence="0.989466">
1077
</page>
<bodyText confidence="0.999945818181818">
egory that doesn&apos;t match any of the descriptions
in a specific profile, two things can be done: (1)
if there is a single description in the profile, to
pick that one, and (2) if there is more than one
description, pick the one whose semantic vector
is closest to the predicted semantic vector.
Finally, the profile extractor will be used as
part of a large-scale, automatically generated
Who&apos;s who site which will be accessible both
by users through a Web interface and by NLP
systems through a client-server API.
</bodyText>
<sectionHeader confidence="0.998866" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9998934">
In this paper, we showed that context and other
linguistic indicators correlate with the choice
of a particular noun phrase to describe an en-
tity. Using machine learning techniques from a
very large corpus, we automatically extracted
a large set of rules that predict the choice of a
description out of an entity profile. We showed
that high-precision automatic prediction of an
appropriate description in a specific context is
possible.
</bodyText>
<sectionHeader confidence="0.999299" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999726714285714">
This material is based upon work supported by
the National Science Foundation under Grants
No. IRI-96-19124, IRI-96-18797, and CDA-96-
25374, as well as a grant from Columbia Uni-
versity&apos;s Strategic Initiative Fund sponsored by
the Provost&apos;s Office. Any opinions, findings,
and conclusions or recommendations expressed
in this material are those of the author(s) and
do not necessarily reflect the views of the Na-
tional Science Foundation.
The author is grateful to the following
people for their comments and suggestions:
Kathy McKeown, Vasileios Hatzivassiloglou,
and Hongyan Jing.
</bodyText>
<sectionHeader confidence="0.999623" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999953898305085">
William W. Cohen. 1995. Fast effective rule
induction. In Proc. 12th International Con-
ference on Machine Learning, pages 115-123.
Morgan Kaufmann.
Darrin Duford. 1993. CREP: a regular
expression-matching textual corpus tool.
Technical Report CUCS-005-93, Columbia
University.
Julian M. Kupiec, Jan Pedersen, and Francine
Chen. 1995. A trainable document summa-
rizer. In Proceedings, 18th Annual Interna-
tional ACM SIGIR Conference on Research
and Development in Information Retrieval,
pages 68-73, Seattle, Washington, July.
Julian M. Kupiec. 1993. MURAX: A robust
linguistic approach for question answering us-
ing an on-line encyclopedia. In Proceedings,
16th Annual International ACM SIGIR Con-
ference on Research and Development in In-
formation Retrieval.
Kathleen R. McKeown and Dragomir R. Radev.
1995. Generating summaries of multiple news
articles. In Proceedings, 18th Annual Interna-
tional ACM SIGIR Conference on Research
and Development in Information Retrieval,
pages 74-82, Seattle, Washington, July.
George A. Miller, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine J.
Miller. 1990. Introduction to WordNet: An
on-line lexical database. International Jour-
nal of Lexicography (special issue), 3(4):235—
312.
Tom M. Mitchell. 1997. Does machine learning
really work? Al Magazine, 18(3).
Chris Paice. 1990. Constructing literature
abstracts by computer: Techniques and
prospects. Information Processing and Man-
agement, 26:171-186.
Dragomir R. Radev and Kathleen R. McKe-
own. 1997. Building a generation knowledge
source using internet-accessible newswire. In
Proceedings of the 5th Conference on Ap-
plied Natural Language Processing, Washing-
ton, DC, April.
Dragomir R. Radev and Kathleen R. McK-
eown. 1998, to appear. Generating natu-
ral language summaries from multiple on-line
sources. Computational Linguistics.
Jacques Robin. 1994. Revision-Based Gener-
ation of Natural Language Summaries Pro-
viding Historical Background. Ph.D. the-
sis, Computer Science Department, Columbia
University.
G. Salton and M.J. McGill. 1983. Introduction
to Modern Information Retrieval. Computer
Series. McGraw Hill, New York.
Frank Smadja. 1993. Retrieving collocations
from text: Xtract. Computational Linguis-
tics, 19(1):143-177, March.
</reference>
<page confidence="0.995884">
1078
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970871">
<title confidence="0.997118333333333">Learning Correlations between Linguistic Indicators and Semantic Constraints: Reuse of Context-Dependent Descriptions of Entities</title>
<author confidence="0.999746">Dragomir R Radev</author>
<affiliation confidence="0.9998925">Department of Computer Science Columbia University</affiliation>
<address confidence="0.999525">New York, NY 10027</address>
<email confidence="0.99965">radevAcs.columbia.edu</email>
<abstract confidence="0.998979409090909">This paper presents the results of a study on the semantic constraints imposed on lexical choice by certain contextual indicators. We show how such indicators are computed and how correlations between them and the choice of a noun phrase description of a named entity can be automatically established using supervised learning. Based on this correlation, we have developed a technique for automatic lexical choice of descriptions of entities in text generation. We discuss the underlying relationship between the pragmatics of choosing an appropriate description that serves a specific purpose in the automatically generated text and the semantics of the description itself. We present our work in the framework of the more general concept of reuse of linguistic structures that are automatically extracted from large corpora. We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>William W Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>In Proc. 12th International Conference on Machine Learning,</booktitle>
<pages>115--123</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="12259" citStr="Cohen, 1995" startWordPosition="2043" endWordPosition="2044">). We use chains of hypernyms when we need to approximate the usage of a particular word in a description using its ancestor and sibling nodes in WordNet. Particularly useful for our application are the synset offsets of the words in a description. The synset offset is a number that uniquely identifies a concept node (synset) in the WordNet hierarchy. Figure 3 shows that the synset offset for the concept &amp;quot;administrator, decision maker&amp;quot; is &amp;quot;{07063507} &amp;quot;, 2We haven&apos;t included relative clauses in our study. while its hypernym, &amp;quot;head, chief, top dog&amp;quot; has a synset offset of &amp;quot;{ 07311393} &amp;quot;. Ripper (Cohen, 1995) is an algorithm that learns rules from example tuples in a relation. Attributes in the tuples can be integers (e.g., length of an article, in words), sets (e.g., semantic features), or bags (e.g., words that appear in a sentence or document). We use Ripper to learn rules that correlate context and other linguistic indicators with the semantics of the description being extracted and subsequently reused. It is important to notice that Ripper is designed to learn rules that classify data into atomic classes (e.g., &amp;quot;good&amp;quot;, &amp;quot;average&amp;quot;, and &amp;quot;bad&amp;quot;). We had to modify its algorithm in order to classify</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>William W. Cohen. 1995. Fast effective rule induction. In Proc. 12th International Conference on Machine Learning, pages 115-123. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darrin Duford</author>
</authors>
<title>CREP: a regular expression-matching textual corpus tool.</title>
<date>1993</date>
<tech>Technical Report CUCS-005-93,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="13593" citStr="Duford, 1993" startWordPosition="2259" endWordPosition="2261">his rule states that if a certain &amp;quot;CONDITION&amp;quot; (which is a function of the indicators related to the description) is met, then the description is likely to contain words that are semantically related to the three WordNet nodes R07063762} {02864326} 100017954H. The stages of our experiments are described in detail in the remainder of this section. 4.1 Semantic tagging of descriptions Our system, PROFILE, processes WWWaccessible newswire on a round-the-clock basis and extracts entities (people, places, and organizations) along with related descriptions. The extraction grammar, developed in CREP (Duford, 1993), covers a variety of pre-modifier and appositional noun phrases. For each word wi in a description, we use a version of WordNet to extract the synset offset of the immediate parent of wi. 4.2 Finding linguistic cues Initially, we were interested in discovering rules manually and then validating them using the learning algorithm. However, the task proved (nearly) impossible considering the sheer size of the corpus. One possible rule that we hypothesized and wanted to verify empirically at this stage was parallelism. This linguisticallymotivated rule states that in a sentence with a parallel st</context>
</contexts>
<marker>Duford, 1993</marker>
<rawString>Darrin Duford. 1993. CREP: a regular expression-matching textual corpus tool. Technical Report CUCS-005-93, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian M Kupiec</author>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings, 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>68--73</pages>
<location>Seattle, Washington,</location>
<contexts>
<context position="9329" citStr="Kupiec et al., 1995" startWordPosition="1562" endWordPosition="1565"> text, that is to be automatically generated by a computer, partially making use of structures reused from the source text. The source text is the one from which particular surface structures are extracted automatically, along with the appropriate syntactic, semantic, and pragmatic constraints under which they are used. Some examples of language reuse include collocation analysis (Smadja, 1993), the use of entire factual sentences extracted from corpora (e.g., &amp;quot;Toy Story&apos; is the Academy Award winning animated film developed by Pixar&amp;quot;), and summarization using sentence extraction (Paice, 1990; Kupiec et al., 1995). In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. Other techniques that can be broadly categorized as language reuse are learning relations from on-line texts (Mitchell, 1997) and answering natural language questions using an on-line encyclopedia (Kupiec, 1993). Stydying the concept of language reuse is rewarding because it allows generation systems to leverage on texts written by humans and their deliberate choice of words, facts, structure. We mentioned that for language reuse to take 1074 place, the</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian M. Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings, 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 68-73, Seattle, Washington, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian M Kupiec</author>
</authors>
<title>MURAX: A robust linguistic approach for question answering using an on-line encyclopedia.</title>
<date>1993</date>
<booktitle>In Proceedings, 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="9683" citStr="Kupiec, 1993" startWordPosition="1619" endWordPosition="1620"> collocation analysis (Smadja, 1993), the use of entire factual sentences extracted from corpora (e.g., &amp;quot;Toy Story&apos; is the Academy Award winning animated film developed by Pixar&amp;quot;), and summarization using sentence extraction (Paice, 1990; Kupiec et al., 1995). In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. Other techniques that can be broadly categorized as language reuse are learning relations from on-line texts (Mitchell, 1997) and answering natural language questions using an on-line encyclopedia (Kupiec, 1993). Stydying the concept of language reuse is rewarding because it allows generation systems to leverage on texts written by humans and their deliberate choice of words, facts, structure. We mentioned that for language reuse to take 1074 place, the generation system has to use the same surface structure in the same syntactic, semantic, and pragmatic context as the source text from which it was extracted. Obviously, all of this information is typically not available to a generation system. There are some special cases in which most of it can be automatically computed. Descriptions of entities are</context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>Julian M. Kupiec. 1993. MURAX: A robust linguistic approach for question answering using an on-line encyclopedia. In Proceedings, 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Dragomir R Radev</author>
</authors>
<title>Generating summaries of multiple news articles.</title>
<date>1995</date>
<booktitle>In Proceedings, 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>74--82</pages>
<location>Seattle, Washington,</location>
<contexts>
<context position="19714" citStr="McKeown and Radev, 1995" startWordPosition="3248" endWordPosition="3251">ur system depends crucially on the multiple components that we use. For example, the shallow CREP grammar that is used in extracting entities and descriptions often fails to extract good descriptions, mostly due to incorrect PP attachment. We have also had problems from the part-of-speech tagger and, as a result, we occasionally incorrectly extract word sequences that do not represent descriptions. 6 Applications and Future Work We should note that PROFILE is part of a large system for information retrieval and summarization of news through information extraction and symbolic text generation (McKeown and Radev, 1995). We intend to use PROFILE to improve lexical choice in the summary generation component, especially when producing user-centered summaries or summary updates (Radev and McKeown, 1998 to appear). There are two particularly appealing cases - (1) when the extraction component has failed to extract a description and (2) when the user model (user&apos;s interests, knowledge of the entity and personal preferences for sources of information and for either conciseness or verbosity) dictates that a description should be used even when one doesn&apos;t appear in the texts being summarized. A second potentially i</context>
</contexts>
<marker>McKeown, Radev, 1995</marker>
<rawString>Kathleen R. McKeown and Dragomir R. Radev. 1995. Generating summaries of multiple news articles. In Proceedings, 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 74-82, Seattle, Washington, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography (special issue),</journal>
<volume>3</volume>
<issue>4</issue>
<pages>312</pages>
<contexts>
<context position="11489" citStr="Miller et al., 1990" startWordPosition="1914" endWordPosition="1917">xtracted automatically. Given a profile like the one shown in Table 1, and an appropriate set of semantic constraints (columns 2-7 of the table), the generation component needs to perform a profile lookup and select a row (description) that satisfies most or all semantic constraints. For example, if the semantic constraints specify that the description has to include the country and the political position of Ung Huot, the most appropriate description is &amp;quot;Cambodian foreign minister&amp;quot;. 4 Experimental Setup In our experiments, we have used two widely available tools - WordNet and Ripper. WordNet (Miller et al., 1990) is an on-line hierarchical lexical database which contains semantic information about English words (including hypernymy relations which we use in our system). We use chains of hypernyms when we need to approximate the usage of a particular word in a description using its ancestor and sibling nodes in WordNet. Particularly useful for our application are the synset offsets of the words in a description. The synset offset is a number that uniquely identifies a concept node (synset) in the WordNet hierarchy. Figure 3 shows that the synset offset for the concept &amp;quot;administrator, decision maker&amp;quot; is</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990. Introduction to WordNet: An on-line lexical database. International Journal of Lexicography (special issue), 3(4):235— 312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom M Mitchell</author>
</authors>
<title>Does machine learning really work?</title>
<date>1997</date>
<journal>Al Magazine,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="9597" citStr="Mitchell, 1997" startWordPosition="1607" endWordPosition="1608">pragmatic constraints under which they are used. Some examples of language reuse include collocation analysis (Smadja, 1993), the use of entire factual sentences extracted from corpora (e.g., &amp;quot;Toy Story&apos; is the Academy Award winning animated film developed by Pixar&amp;quot;), and summarization using sentence extraction (Paice, 1990; Kupiec et al., 1995). In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. Other techniques that can be broadly categorized as language reuse are learning relations from on-line texts (Mitchell, 1997) and answering natural language questions using an on-line encyclopedia (Kupiec, 1993). Stydying the concept of language reuse is rewarding because it allows generation systems to leverage on texts written by humans and their deliberate choice of words, facts, structure. We mentioned that for language reuse to take 1074 place, the generation system has to use the same surface structure in the same syntactic, semantic, and pragmatic context as the source text from which it was extracted. Obviously, all of this information is typically not available to a generation system. There are some special</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Tom M. Mitchell. 1997. Does machine learning really work? Al Magazine, 18(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Paice</author>
</authors>
<title>Constructing literature abstracts by computer: Techniques and prospects.</title>
<date>1990</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>26--171</pages>
<contexts>
<context position="9307" citStr="Paice, 1990" startWordPosition="1560" endWordPosition="1561"> and a target text, that is to be automatically generated by a computer, partially making use of structures reused from the source text. The source text is the one from which particular surface structures are extracted automatically, along with the appropriate syntactic, semantic, and pragmatic constraints under which they are used. Some examples of language reuse include collocation analysis (Smadja, 1993), the use of entire factual sentences extracted from corpora (e.g., &amp;quot;Toy Story&apos; is the Academy Award winning animated film developed by Pixar&amp;quot;), and summarization using sentence extraction (Paice, 1990; Kupiec et al., 1995). In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. Other techniques that can be broadly categorized as language reuse are learning relations from on-line texts (Mitchell, 1997) and answering natural language questions using an on-line encyclopedia (Kupiec, 1993). Stydying the concept of language reuse is rewarding because it allows generation systems to leverage on texts written by humans and their deliberate choice of words, facts, structure. We mentioned that for language reuse t</context>
</contexts>
<marker>Paice, 1990</marker>
<rawString>Chris Paice. 1990. Constructing literature abstracts by computer: Techniques and prospects. Information Processing and Management, 26:171-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Building a generation knowledge source using internet-accessible newswire.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<location>Washington, DC,</location>
<contexts>
<context position="4774" citStr="Radev and McKeown, 1997" startWordPosition="783" endWordPosition="786">inister&amp;quot; Description0 f (&amp;quot;Richard Butler&amp;quot;) = &amp;quot;Chief U.N. arms inspector&amp;quot; Chief U.N. arms inspector Richard Butler met Iraq&apos;s Deputy Prime Minister Tareq Aziz Monday after rejecting Iraqi attempts to set deadlines for finishing his work. Figure 1: Sample sentence containing two entity-description pairs. Each entity appearing in a text can have multiple descriptions (up to several dozen) associated with it. We call the set of all descriptions related to the same entity in a corpus, a profile of that entity. Profiles for a large number of entities were compiled using our earlier system, PROFILE (Radev and McKeown, 1997). It turns out that there is a large variety in the size of the profile (number of distinct descriptions) for different entities. Table 1 shows a subset of the profile for Ung Huot, the former foreign minister of Cambodia, who was elected prime minister at some point of time during the run of our experiment. A few sample semantic features of the descriptions in Table 1 are shown as separate columns. We used information extraction techniques to collect entities and descriptions from a corpus and analyzed their lexical and semantic properties. We have processed 178 MB1 of newswire and analyzed t</context>
</contexts>
<marker>Radev, McKeown, 1997</marker>
<rawString>Dragomir R. Radev and Kathleen R. McKeown. 1997. Building a generation knowledge source using internet-accessible newswire. In Proceedings of the 5th Conference on Applied Natural Language Processing, Washington, DC, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Kathleen R McKeown</author>
</authors>
<title>to appear. Generating natural language summaries from multiple on-line sources. Computational Linguistics.</title>
<date>1998</date>
<contexts>
<context position="19897" citStr="Radev and McKeown, 1998" startWordPosition="3275" endWordPosition="3278">od descriptions, mostly due to incorrect PP attachment. We have also had problems from the part-of-speech tagger and, as a result, we occasionally incorrectly extract word sequences that do not represent descriptions. 6 Applications and Future Work We should note that PROFILE is part of a large system for information retrieval and summarization of news through information extraction and symbolic text generation (McKeown and Radev, 1995). We intend to use PROFILE to improve lexical choice in the summary generation component, especially when producing user-centered summaries or summary updates (Radev and McKeown, 1998 to appear). There are two particularly appealing cases - (1) when the extraction component has failed to extract a description and (2) when the user model (user&apos;s interests, knowledge of the entity and personal preferences for sources of information and for either conciseness or verbosity) dictates that a description should be used even when one doesn&apos;t appear in the texts being summarized. A second potentially interesting application involves using the data and rules extracted by PROFILE for language regeneration. In (Radev and McKeown, 1998 to appear) we show how the conversion of extracted</context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>Dragomir R. Radev and Kathleen R. McKeown. 1998, to appear. Generating natural language summaries from multiple on-line sources. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Robin</author>
</authors>
<title>Revision-Based Generation of Natural Language Summaries Providing Historical Background.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Department, Columbia University.</institution>
<contexts>
<context position="6926" citStr="Robin, 1994" startWordPosition="1147" endWordPosition="1148">as the remaining 2,451 entities that have DDPE values between 2 and 24. Figure 2: Number of distinct descriptions per entity (log-log scale) 3 Language Reuse in Text Generation Text generation usually involves lexical choice - that is, choosing one way of referring to an entity over another. Lexical choice refers to a variety of decisions that have to made in text generation. For example, picking one among several equivalent (or newly equivalent) constructions is a form of lexical choice (e.g., &amp;quot;The Utah Jazz handed the Boston Celtics a defeat&amp;quot; vs. &amp;quot;The Utah Jazz defeated the Boston Celtics&amp;quot; (Robin, 1994)). We are interested in a different aspect of the problem: namely learning the rules that can be used for automatically selecting an appropriate description of an entity in a specific 1 10&apos; 10&apos; X Numb/br 01 &amp;Mind deariplbm (DDPE) 1073 Description Semantic categories addressing country male new political post seniority a senior member X Cambodia&apos;s X Cambodian foreign minister X X co-premier X first prime minister X foreign minister X His Excellency X Mr. X X X new co-premier X X new first prime minister X X newly-appointed first prime minister X premier X prime minister Table 1: Profile of Ung </context>
</contexts>
<marker>Robin, 1994</marker>
<rawString>Jacques Robin. 1994. Revision-Based Generation of Natural Language Summaries Providing Historical Background. Ph.D. thesis, Computer Science Department, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval. Computer Series.</title>
<date>1983</date>
<publisher>McGraw Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="2859" citStr="Salton and McGill, 1983" startWordPosition="469" endWordPosition="472">ial candidate&amp;quot;, while a 1997 article on a false bomb alert in Little Rock, Ark. uses &amp;quot;Bill Clinton, an Arkansas native&amp;quot;. This paper presents the results of a study of the correlation between named entities (people, places, or organizations) and noun phrases used to describe them in a corpus. Intuitively, the use of a description is based on a deliberate decision on the part of the author of a piece of text. A writer is likely to select a description that puts the entity in the context of the rest of the article. It is known that the distribution of words in a document is related to its topic (Salton and McGill, 1983). We have developed related techniques for approximating pragmatic constraints using words that appear in the immediate context of the entity. We will show that context influences the choice of a description, as do several other linguistic indicators. Each of the indicators by itself doesn&apos;t provide enough empirical data that distinguishes among all descriptions that are related to a an entity. However, a carefully selected combination of such indicators provides enough information in order pick an appropriate description with more than 80% accuracy. Section 2 describes how we can automaticall</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M.J. McGill. 1983. Introduction to Modern Information Retrieval. Computer Series. McGraw Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
</authors>
<title>Retrieving collocations from text:</title>
<date>1993</date>
<journal>Xtract. Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="9106" citStr="Smadja, 1993" startWordPosition="1530" endWordPosition="1531">, which includes techniques of extracting shallow structure from a corpus and applying that structure to computer-generated texts. Language reuse involves two components: a source text written by a human and a target text, that is to be automatically generated by a computer, partially making use of structures reused from the source text. The source text is the one from which particular surface structures are extracted automatically, along with the appropriate syntactic, semantic, and pragmatic constraints under which they are used. Some examples of language reuse include collocation analysis (Smadja, 1993), the use of entire factual sentences extracted from corpora (e.g., &amp;quot;Toy Story&apos; is the Academy Award winning animated film developed by Pixar&amp;quot;), and summarization using sentence extraction (Paice, 1990; Kupiec et al., 1995). In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. Other techniques that can be broadly categorized as language reuse are learning relations from on-line texts (Mitchell, 1997) and answering natural language questions using an on-line encyclopedia (Kupiec, 1993). Stydying the concept </context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>Frank Smadja. 1993. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143-177, March.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>