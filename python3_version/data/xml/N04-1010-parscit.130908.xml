<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000830">
<title confidence="0.988947">
Acquiring Hyponymy Relations from Web Documents
</title>
<author confidence="0.992415">
Keiji Shinzato Kentaro Torisawa
</author>
<affiliation confidence="0.996422">
School of Information Science,
Japan Advanced Institute of Science and Technology (JAIST)
</affiliation>
<address confidence="0.965633">
1-1 Asahidai, Tatsunokuchi, Nomi-gun, Ishikawa, 923-1292 JAPAN
</address>
<email confidence="0.999462">
{skeiji,torisawa}@jaist.ac.jp
</email>
<sectionHeader confidence="0.995648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949625">
This paper describes an automatic method for
acquiring hyponymy relations from HTML
documents on the WWW. Hyponymy relations
can play a crucial role in various natural lan-
guage processing systems. Most existing ac-
quisition methods for hyponymy relations rely
on particular linguistic patterns, such as “NP
such as NP”. Our method, however, does not
use such linguistic patterns, and we expect
that our procedure can be applied to a wide
range of expressions for which existing meth-
ods cannot be used. Our acquisition algo-
rithm uses clues such as itemization or listing
in HTML documents and statistical measures
such as document frequencies and verb-noun
co-occurrences.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999329888888889">
The goal of this work is to become able to automatically
acquire hyponymy relations for a wide range of words
or phrases from HTML documents on the WWW. We do
not use particular lexicosyntactic patterns, as previous at-
tempts have (Hearst, 1992; Caraballo, 1999; Imasumi,
2001; Fleischman et al., 2003; Morin and Jacquemin,
2003; Ando et al., 2003). The frequencies of use for such
lexicosyntactic patterns are relatively low, and there can
be many words or phrases that do not appear in such pat-
terns even if we look at a large number of texts. The effort
of searching for other clues indicating hyponymy rela-
tions is thus significant. We try to acquire hyponymy re-
lations by combining three different types of clue obtain-
able from a wide range of words or phrases. The first type
of clue is inclusion in itemizations or lists found in typi-
cal HTML documents on the WWW. The second consists
of statistical measures such as the document frequency
(df) and the inverse document frequency (idf), which are
</bodyText>
<listItem confidence="0.9993345">
• Car Specification
• Toyota
• Honda
• Nissan
</listItem>
<figureCaption confidence="0.99679">
Figure 1: An example of itemization
</figureCaption>
<bodyText confidence="0.999085391304348">
popular in the IR literature. The third is verb-noun co-
occurrence in normal corpora.
In our acquisition, we made the following assumptions.
Assumption A Expressions included in the same item-
ization or listing in an HTML document are likely
to have a common hypernym.
Assumption B Given a set of hyponyms that have a
common hypernym, the hypernym appears in many
documents that include the hyponyms.
Assumption C Hyponyms and their hypernyms are se-
mantically similar.
Our acquisition process computes a common hyper-
nym for expressions in the same itemizations. It pro-
ceeds as follows. First, we download a large number
of HTML documents from the WWW and extract a set
of natural language expressions that are listed in the
same itemized region of documents. Consider the item-
ization in Fig. 1. We extract the set of expressions,
{Toyota, Honda, Nissan} from it. From Assumption A,
we can treat these expressions as candidates of hyponyms
that have a common hypernym such as “company”. We
call such expressions in the same itemization hyponym
candidates. Particularly, a set of the hyponym candi-
dates extracted from a single itemization or list is called
a hyponym candidate set (HCS). For the example docu-
ment, we would treat Toyota, Honda, and Nissan as hy-
ponym candidates, and regard them as members of the
same HCS.
We then download documents that include at least one
hyponym candidate by using an existing search engine,
and pick up a noun that appears in the documents and
that has the largest score. The score was designed so
that words appearing in many downloaded documents are
highly ranked, according to Assumption B. We call the
selected noun a hypernym candidate for the given hy-
ponym candidates.
Note that if we download documents including “Toy-
ota” or “Honda”, many will include the word “company”,
which is a true hypernym of Toyota. However, words
which are not hypernyms, but which are closely associ-
ated with Toyota or Honda (e.g., “price”) will also be in-
cluded in many of the downloaded documents. The next
step of our procedure is designed to exclude such non-
hypernyms according to Assumption C. We compute the
similarity between hypernym candidates and hyponym
candidates in an HCS, and eliminate the HCS and its hy-
pernym candidate from the output if they are not seman-
tically similar. For instance, if the previous step of our
procedure produces “price” as a hypernym candidate for
Toyota and Honda, then the hypernym candidate and the
hyponym candidates are removed from the output. We
empirically show that this helps to improve overall preci-
sion.
Finally, we further elaborate computed hypernym can-
didates by using additional heuristic rules. Though we
admit that these rules are rather ad hoc, they worked well
in our experiments.
We have tested the effectiveness of our methods
through a series of experiments in which we used HTML
documents downloaded from actual web sites. We ob-
served that our method can find a significant number of
hypernyms that (at least some of) alternative hypernym
acquisition procedures cannot acquire, at least, when only
a rather small amount of texts are available.
In this paper, Section 2 describes our acquisition al-
gorithm. Section 3 gives our experimental results which
we obtained using Japanese HTML documents, and Sec-
tion 4 discusses the benefit obtained through our method
based on a comparison with alternative methods.
</bodyText>
<sectionHeader confidence="0.96435" genericHeader="method">
2 Acquisition Algorithm
</sectionHeader>
<bodyText confidence="0.996972777777778">
Our acquisition algorithm consists of four steps, as ex-
plained in this section.
Step 1 Extraction of hyponym candidates from itemized
expressions in HTML documents.
Step 2 Selection of a hypernym candidate with respect
to df and idf.
Step 3 Ranking of hypernym candidates and HCSs
based on semantic similarities between hypernym
and hyponym candidates.
</bodyText>
<figure confidence="0.998401625">
&lt;UL&gt;
&lt;LI&gt;Car Specification&lt;/LI&gt;
&lt;UL&gt;
&lt;LI&gt;Toyota&lt;/LI&gt;
&lt;LI&gt;Honda&lt;/LI&gt;
&lt;LI&gt;Nissan&lt;/LI&gt;
&lt;/UL&gt;
&lt;/UL&gt;
</figure>
<figureCaption confidence="0.999383">
Figure 2: An example of HTML documents
</figureCaption>
<bodyText confidence="0.979029666666667">
Step 4 Application of a few additional heuristics to elab-
orate computed hypernym candidates and hyponym
candidates.
</bodyText>
<subsectionHeader confidence="0.987465">
2.1 Step 1: Extraction of hyponym candidates
</subsectionHeader>
<bodyText confidence="0.998001068965517">
The objective of Step 1 is to extract an HCS, which is a set
of hyponym candidates that may have a common hyper-
nym, from the itemizations or lists in HTML documents.
Many methods can be used to do this. Our approach is
a simple one. Each expression in an HTML document
can be associated with a path, which specifies both the
HTML tags that enclose the expression and the order of
the tags. Consider the HTML document in Figure 2. The
expression “Car Specification” is enclosed by the tags
&lt;LI&gt;,&lt;/LI&gt; and &lt;UL&gt;,&lt;/UL&gt;. If we sort these tags
according to their nesting order, we obtain a path (UL, LI)
and this path specifies the information regarding the place
of the expression. We write ((UL, LI), Car Specification)
if (UL, LI) is a path for the expression “Car Specifica-
tion”. We can then obtain the following paths for the ex-
pressions from the document.
((UL, LI), Car Specification),
((UL, UL, LI), Toyota),
((UL, UL, LI), Honda),
((UL, UL, LI), Nissan)
Basically, our method extracts the set of expressions
associated with the same path as an HCS 1. In the above
example, we can obtain the HCS {Toyota, Honda, · · ·}.
We extract an itemization only when its size is n and
3 &lt; n &lt; 20. This is because the processing of large
itemizations (particularly the downloading of the related
documents) is time-consuming, and small itemizations
are often used to obtain a proper layout in HTML doc-
uments2.
</bodyText>
<footnote confidence="0.9378608">
1We actually need to distinguish different occurrences of the
tags in some cases to prevent distinct itemizations from being
recognized as a single itemization.
2We found some words that are often inserted into an item-
ization but do not have common semantic properties with other
</footnote>
<bodyText confidence="0.9230558">
items in the same itemization, during the experiments using a
development set. “&apos;7,/�&apos; (links)” and “&apos;SAL7&apos; (help)” are ex-
amples of such words. We prepared a list of such words con-
sisting of 70 items, and removed them from the HCSs obtained
in Step 1.
</bodyText>
<subsectionHeader confidence="0.9627065">
2.2 Step 2: Selection of a hypernym candidate by df
and idf
</subsectionHeader>
<bodyText confidence="0.999987">
In Step 1, we can obtain a set of hyponym candidates,
an HCS, that may have a common hypernym. In Step
2, we select a common hypernym candidate for an HCS.
First, we prepare two sets of documents. We randomly
select a large number of HTML documents and download
them. We call this set of documents a global document
set. We assume this document set indicates the general
tendencies of word frequencies. Then we download the
documents including each hyponym candidate in a given
HCS. This document set is called a local document set,
and we use it to know the strength of the association of
nouns with the hyponym candidates.
Let us denote a given HCS as C, a local document
set obtained from all the items in C as LD(C), and a
global document set as G. We also assume that N is
a set of words, which can be candidates of hypernym3.
A hypernym candidate, denoted as h(C), for C is ob-
tained through the following formula, where df (n, D) is
the number of documents that include a noun n in a doc-
ument set D.
</bodyText>
<equation confidence="0.984600333333333">
h(C) = argmaXn∈N{df (n, LD(C)) · idf (n, G)}
idf (n, G) = log |G|
df (n, G)
</equation>
<bodyText confidence="0.976912228571428">
The score has a large value for a noun that appears in a
large number of documents in the local document set and
is found in a relatively small number of documents in the
global document set.
In general, nouns strongly associated with many items
in a given HCS tend to be selected through the above for-
mula. Since hyponym candidates tend to share a common
semantic property, and their hypernym is one of the words
strongly associated with the common property, the hyper-
nym is likely to be picked up through the above formula.
Note that a process of generalization is performed auto-
matically by treating all the hyponym candidates in an
HCS simultaneously. That is, words strongly connected
with only one hyponym candidate (for instance, “Lexus”
for Toyota) have relatively low score values since we ob-
tain statistical measures from all the local document sets
for all the hyponym candidates in an HCS.
Nevertheless, this scoring method is a weak method in
one sense. There could be many non-hypernyms that are
3In our experiments, N is a set consisting of 37,639 words,
each of which appeared more than 500 times in 33 years of
Japanese newspaper articles (Yomiuri newspaper 1987-2001,
Mainichi newspaper 1991-1999 and Nikkei newspaper 1983-
1990; 3.01 GB in total). We excluded 116 nouns that we ob-
served never be hypernyms from N. An example of such noun
is “僕 (I)”. We found them in the experiments using a develop-
ment set.
strongly associated with many of the hyponym candidates
(for instance, “price” for Toyota and Honda). Such non-
hypernyms are dealt with in the next step.
An evident alternative to this method is to use
tf (n, LD(C)), which is the frequency of a noun n in the
local document set, instead of df (n, LD(C)). We tried
using this method in our experiments, but it produced less
accurate results, as we show in Section 3.
</bodyText>
<subsectionHeader confidence="0.9536965">
2.3 Step 3: Ranking of hypernym candidates and
HCSs by semantic similarity
</subsectionHeader>
<bodyText confidence="0.999548948717949">
Thus, our procedure can produce pairs consisting of a
hypernym candidate and an HCS, which are denoted by
{hh(C1), C1i, hh(C2), C2i, ··· , hh(Cm), Cmi}.
Here, C1, · · · , Cm are HCSs, and h(Ci) is a common hy-
pernym candidate for hyponym candidates in an HCS Ci.
In Step 3, our procedure ranks these pairs by using the
semantic similarity between h(Ci) and the items in Ci.
The final output of our procedure is the top k pairs in this
ranking after some heuristic rules are applied to it in Step
4. In other words, the procedure discards the remaining
m − k pairs in the ranking because they tend to include
erroneous hypernyms.
As mentioned, we cannot exclude non-hypernyms that
are strongly associated with hyponym candidates from
the hypernym candidate obtained by h(C). For exam-
ple, the value of h(C) may be a non-hypernym “price”,
rather than “company”, when C = {Toyota, Honda}.
The objective of Step 3 is to exclude such non-hypernyms
from the output of our procedure. We expect such non-
hypernyms to have relatively low semantic similarities to
the hyponym candidates, while the behavior of true hy-
pernyms should be semantically similar to the hyponyms.
If we rank the pairs of hypernym candidates and HCSs
according to their semantic similarities, the low ranked
pairs are likely to have an erroneous hypernym candidate.
We can then obtain relatively precise hypernyms by dis-
carding the low ranked pairs.
The similarities are computed through the following
steps. First, we parse all the texts in the local document
set, and check the argument positions of verbs where hy-
ponym candidates appear. (To parse texts, we use a down-
graded version of an existing parser (Kanayama et al.,
2000) throughout this work.) Let us denote the frequency
of the hyponym candidates in an HCS C occupying an
argument position p of a verb v as fhypo(C, p, v). As-
sume that all possible argument positions are denoted as
{p1, · · · , pl} and all the verbs as {v1, · · · , vm}. We then
define the co-occurrence vector of hyponym candidates
as follows.
</bodyText>
<equation confidence="0.79382">
hypov(C) = hfhypo(C, p1, v1), fhypo(C,p2, v1), ··· ,
fhypo(C, pl−1, vm), fhypo(C, pl, vm)i
</equation>
<bodyText confidence="0.9869365">
In the same way, we can define the co-occurrence vec-
tor of a hypernym candidate n.
</bodyText>
<equation confidence="0.99852">
hyperv(n) = (f(n,p1,v1),··· ,f(n,pt,vm))
</equation>
<bodyText confidence="0.9997231">
Here, f(n, p, v) is the frequency of a noun n occupying
an argument position p of a verb v obtained from the pars-
ing results of a large number of documents - 33 years of
Japanese newspaper articles (Yomiuri newspaper 1987-
2001, Mainichi newspaper 1991-1999, and Nikkei news-
paper 1990-1998; 3.01 GB in total) - in our experimental
setting.
The semantic similarities between hyponym candi-
dates in C and a hypernym candidate n are then computed
by a cosine measure between the vectors:
</bodyText>
<equation confidence="0.7208132">
hypov(C) · hyperv(n)
sim(n, C) = |hypov(C)||hyperv(n)|
Our procedure sorts the hypernym-HCS pairs
{(h(Ci), Ci)Jmi=1 using the value
sim(h(Ci), Ci) · df (h(Ci), LD(Ci)) · idf (h(Ci), G)
</equation>
<bodyText confidence="0.999783">
Note that we consider not only the similarity but also the
df · idf score used in Step 2 in the sorting.
An evident alternative to the above method is the al-
gorithm that re-ranks the top j hypernym candidates ob-
tained by df · idf for a given HCS by using the same
score. However, we found no significant improvement
when this alternative was used in our experiments, as we
later explain.
</bodyText>
<subsectionHeader confidence="0.985418">
2.4 Step 4: Application of other heuristic rules
</subsectionHeader>
<bodyText confidence="0.987814454545454">
The procedure described up to now can produce a hyper-
nym for hyponym candidates with a certain precision. We
found, though, that we can improve accuracy by using a
few more heuristic rules, which are listed below.
Rule 1 If the number of documents that include a hyper-
nym candidate is less than the sum of the numbers of
the documents that include an item in the HCS, then
discard both the hypernym candidate and the HCS
from the output.
Rule 2 If a hypernym candidate appears as substrings of
an item in its HCS and it is not a suffix of the item,
then discard both the hypernym candidate and the
HCS from the output. If a hypernym candidate is
a suffix of its hyponym candidate, then half of the
members of an HCS must have the hypernym can-
didate as their suffixes. Otherwise, discard both the
hypernym candidate and its HCS from the output.
Rule 3 If a hypernym candidate is an expression belong-
ing to the category ofplace names, then replace it by
“place name”.
In general, we can expect that a hypernym is used in
a wider range of contexts than those of its hyponyms,
and that the number of documents including the hyper-
nym candidate should be larger than the number of web
documents including hyponym candidates. This justifies
Rule 1. We use the hit counts given by an existing search
engine as the number of documents including an expres-
sion.
As for Rule 2, note that Japanese is a head
final language, and a semantic head of a com-
plex noun phrase is the last noun. Consider
the following two Japanese complex nouns.
amerika-eiga / nihon-eiga
</bodyText>
<equation confidence="0.975277">
(American) (movie) / (Japanese) (movie)
</equation>
<bodyText confidence="0.99987490625">
Apparently an American movie is a kind of movie as is
a Japanese movie. There are many multi-word expres-
sions whose hypernyms are their suffixes, and if some
expressions share a common suffix, it is likely to be their
hypernym. However, if a hypernym candidate appears in
a position other than as a suffix of a hyponym candidate,
the hypernym candidate is likely to be an erroneous one.
In addition, if a hypernym candidate is a common suffix
of only a small portion of an HCS, then the HCS tends
not to have semantic uniformity, and such a hypernym
candidate should be eliminated from the output. (We em-
pirically determined “one-half” as a threshold in our ex-
periments on the development set.)
As for Rule 3, in our experiments on a development set,
we found that our procedure could not provide precise hy-
pernyms for place names such as “Kyoto” and “Tokyo”.
In the case of Kyoto and Tokyo, our procedure produced
“Japan” as a hypernym candidate. Although “Japan” is
consistent with most of our assumptions regarding hy-
pernyms, it is a holonym of Kyoto and Tokyo, but their
hypernym. In general, when a set of place names is given
as an HCS, the procedure tends to produce the name of
the region or area that includes all the places designated
by the hyponym candidates. We then added the rule to re-
place such place names by the expression “place name,”
which is a true hypernym in many of such cases 4.
Recall that we obtained the ranked pairs of an HCS and
its common hypernym in Step 3. By applying the above
rules, some pairs are removed from the ranked pairs, or
are modified. For some given integer k, the top k pairs of
the obtained ranked pairs become the final output of our
procedure, as mentioned before.
</bodyText>
<sectionHeader confidence="0.997567" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.799125666666667">
We downloaded about 8.71 x 105 HTML documents
(10.4 GB with HTML tags), and extracted 9.02 x 104
HCSs through the method described in Section 2.1. We
</bodyText>
<footnote confidence="0.932057333333333">
4To judge if a hypernym candidate is a place name, we
used the output of a morphological analyzer (Matsumoto et al.,
1993).
</footnote>
<figure confidence="0.993981869565217">
Proposed Method (Step 4)
Step 3
Step 2
Step 2 (tf)
100
80
60
40
20
0
100
80
60
40
20
0
Proposed Method
Re-ranking of top 2 hypernym candidates
Re-ranking of top 3 hypernym candidates
Re-ranking of top 4 hypernym candidates
Re-ranking of top 5 hypernym candidates
0 500 1000 1500 2000 2500 0 500 1000 1500 2000 2500
# of hypernym hyponym pairs # of hypernym hyponym pairs
</figure>
<figureCaption confidence="0.99933">
Figure 3: Contribution of each step Figure 5: Contribution of re-ranking
</figureCaption>
<figure confidence="0.9940795">
0 500 1000 1500 2000 2500
# of hypernym hyponym pairs
</figure>
<figureCaption confidence="0.999997">
Figure 4: Contribution of each step and rule
</figureCaption>
<bodyText confidence="0.990868045454546">
randomly picked 2,000 HCSs from among the extracted
HCS as our test set. The test set contained 13,790 hy-
ponym candidates. (Besides these HCSs, we used a de-
velopment set consisting of about 4,000 HCSs to develop
our algorithm.) For each single hyponym candidate, we
downloaded the top 100 documents in the ranking pro-
duced by a search engine5 as a local document set if the
engine found more than 100 documents. Otherwise, all
the documents were downloaded. (Note that a local doc-
ument set for an HCS may contain more than 100 doc-
uments.) As a global document set, we used the down-
loaded 1.00 × 106 HTML documents (1.26 GB without
HTML tags).
Fig. 3 shows the accuracy of hypernyms
obtained after Steps 2, 3, and 4. We as-
sumed each step produced the sorted pairs of
an HCS and a hypernym, which are denoted by
{hh(C1), C1i, hh(C2), C2i, ··· , hh(Cm), Cmi}. The
sorting was done by the score sim(h(Ci), Ci) ·
df (h(Ci), LD(Ci)) · idf (h(Ci), G) after Steps 3 and
4, as described before, while the output of Step 2 was
sorted by the df · idf score. In addition, we assumed
</bodyText>
<footnote confidence="0.988512">
5The search engine “goo”. (http://www.goo.ne.jp)
</footnote>
<bodyText confidence="0.999720526315789">
each step produced only the top 200 pairs in the sorted
pairs. (Since the output of Step 4 is the final output, this
means that we also assumed that only the top 200 pairs
of a hypernym and an HCS would be produced as final
output with our procedure. In other words, the remaining
1,800 (=2,000-200) pairs were discarded. )
The resulting hypernyms were checked by the authors
according to the definition of the hypernym given in
Miller et al., 1990, i.e., we checked if the expression “a
hyponym candidate is a kind of a hypernym candidate.”
is acceptable. Then, we computed the precision, which is
the ratio of the correct hypernym-hyponym pairs against
all the pairs obtained from the top n pairs of an HCS and
its hypernym candidate. The x-axis of the graph indicates
the number of hypernym-hyponym pairs obtained from
the top n pairs of an HCS and its hypernym candidate,
while the y-axis indicates the precision.
More precisely, the curve for Step i plots the following
points, where 1 ≤ j ≤ 200.
</bodyText>
<equation confidence="0.998861666666667">
�j k=1 correct(Ck, h(Ck))
|Ck|, i
Ek=1 |Ck|
</equation>
<bodyText confidence="0.9985465625">
correct(Ck, h(Ck)) indicates the number of hyponym
candidates in Ck that are true hyponyms of h(Ck). Note
that after Step 4, the precision reached about 75% for 701
hyponym candidates, which was slightly more than 5%
of all the given hyponym candidates. For 1398 hyponym
candidates (about 10% of all the candidates), the preci-
sion was about 61%.
Another important point is that “Step 2 (tf)” in the
graph refers to an alternative to our Step 2 procedure; i.e.,
the Step 2 procedure in which df (h(C), LD(C)) was re-
placed by tf (h(C), LD(C)). One can see the Step 2 pro-
cedure with df works better than that with tf .
Table 1 shows some examples of the acquired HCSs
and their common hypernyms. Recall that a common suf-
fix of an HCS is a good candidate to be a hypernym. The
examples were taken from cases where a common suffix
</bodyText>
<figure confidence="0.989858461538461">
100
80
60
40
20
0
Proposed Method
-Step 3
-Step 4
-Rule 1
-Rule 2
-Rule 3
� j
</figure>
<equation confidence="0.915741555555555">
h
k=1
hypernym「hyponym」,hyponym .* 以外の .* hypernym,
hyponym .* など (、 |の)? hypernym,
hyponym .* のような .* hypernym,
hyponym .* に似た .* hypernym,
hyponym .* と (い 1 言) う .* hypernym,
hyponym .* と呼ばれる .* hypernym,
hyponym .* (ら I たち) .* hypernym
</equation>
<bodyText confidence="0.48393">
The hypernym and hyponym may be bracketed by「」or “”.
</bodyText>
<figureCaption confidence="0.992658">
Figure 6: lexicosyntactic patterns
</figureCaption>
<bodyText confidence="0.999814238095238">
of an HCS was not produced as a hypernym. This list
is actually the output of Step 3, and shows which HCSs
and their hypernym candidates were eliminated/modified
from the output in Step 4 and which rule was fired to
eliminate/modify them.
Next, we eliminated some steps from the whole pro-
cedure. Figure 4 shows the accuracy when one of the
steps was eliminated from the procedure. “-Step X” or
“-Rule X” refers to the accuracies obtained through the
procedure from which step X or rule X were eliminated.
Note that both graphs indicate that every step and rule
contributed to the improvement of the precision.
Figure 5 compares our method and an alternative
method, which was the algorithm that re-ranks the top j
hypernym candidates for a given HCS by using the score
sim(h, C) · df (h, LD(C)) · idf (h, G), where h is a hy-
pernym candidate, in Step 3. (Recall that our algorithm
uses the score only for sorting pairs of HCSs and their hy-
pernym. In other words, we do not re-rank the hypernym
candidates for a single HCS.) We found no significant im-
provement when the alternative was used.
</bodyText>
<sectionHeader confidence="0.867839" genericHeader="method">
4 Comparison with alternative methods
</sectionHeader>
<bodyText confidence="0.999354594202899">
We have shown that our assumptions are effective for ac-
quiring hypernyms. However, there are other alternative
methods applicable under our settings. We evaluated the
followings methods and compared the results with those
of our procedure.
Alternative 1 Compute the non-null suffixes that are
shared by the maximum number of hyponym can-
didates, and regard the longest as a hypernym can-
didate.
Alternative 2 Extract hypernyms for hyponym candi-
dates by looking at the captions or titles of the item-
izations from which hyponym candidates are ex-
tracted.
Alternative 3 Extract hypernyms by using lexicosyntac-
tic patterns.
Alternative 4 Combinations of Alternative 1-3.
The evaluation method for Alternative 1 and Alterna-
tive 2 is the same as the one for our method. We simply
judged if the produced hypernyms are acceptable or not.
But we used different evaluation method for the other
alternatives. We checked if the correct hypernyms pro-
duced by our method can be found by these alternatives.
This is simply for the sake of easiness of the evaluation.
Note that we evaluated Alternative 1 and Alternative 2 in
the second evaluation scheme when they are combined
and are used as a part of Alternative 4.
More detailed explanations on the alternative methods
are given below.
Alternative 1 Recall that Japanese is a head final lan-
guage, and we have explained that common suffixes of
hyponym candidates are good candidates to be common
hyponyms. Alternative 1 computes a hypernym candidate
according to this principle.
Alternative 2 This method uses the captions of the
itemizations, which are likely to contain a hypernym of
the items in the itemization. We manually found cap-
tions or titles that are in the position such that they can
explain the content of the itemization, and picked up the
caption closest to the itemization and the second closest
to it. Then, we checked if the picked-up captions included
the proper hypernyms. Note that the precision obtained
by this method is just an upper bound of real performance
because we do not have a method to extract hypernyms
from captions at least at the current stage of our research.
Alternative 3 We prepared the lexicosyntactic patterns
in Fig. 6, which are similar to the ones used in the pre-
vious studies of hypernym acquisition in Japanese (Ima-
sumi, 2001; Ando et al., 2003). One difference from the
previous studies was that we used a regular expression
instead of a parser. This may have caused some errors,
but our patterns were more generous than those used in
the previous studies, and did not miss the expressions
matched to the patterns from the previous studies. In
other words, the accuracy obtained with our patterns was
an upper bound on the performance obtained by the previ-
ous proposal. Another difference was that the procedure
was given correct pairs of a hypernym and a hyponym
computed beforehand using our proposed method, and it
only checked whether given pairs could be found by us-
ing the lexicosyntactic patterns from given texts. In other
words, this alternative method checked if the lexicosyn-
tactic patterns could find the hypernym-hyponym pairs
successfully obtained by our procedure. The texts used
were local document sets from which our procedure com-
puted a hypernym candidate. If our procedure has better
figures than this method, this means that our procedure
can produce hypernyms that cannot be acquired by pat-
terns, at least, from a rather small number of texts (i.e., a
maximum of 100 documents per hyponym candidate).
</bodyText>
<figure confidence="0.9964455">
100
Proposed Method
Alternative 1
Alternative 2
Alternative 3
Alternative 4
60
40
20
0
0 500 1000 1500 2000
# of hypernym hyponym pairs
</figure>
<figureCaption confidence="0.99997">
Figure 7: Comparison with alternative methods
</figureCaption>
<bodyText confidence="0.999956">
that our method could find a significant number of hy-
ponymy relations that alternative methods could not, at
least when the amount of documents used was rather
small.
The first goal of our future work is to further improve
the precision of our method. One possible approach will
be to combine our methods with alternative techniques,
which were actually examined in our experiments. Our
second goal is to extend our method so that it can han-
dle multi-word hypernyms. Currently, our method pro-
duces just “company” as a hypernym of “Toyota”. If we
can obtain a multi-word hypernym such as “automobile
manufacturer,” it can provide more useful information to
various types of natural language processing systems.
</bodyText>
<page confidence="0.595514">
80
</page>
<bodyText confidence="0.991760357142857">
Alternative 4 We also compared our procedure with
the combination of all the above methods: Alternative
4. Again, we checked whether the combination could
find the correct hypernym-hyponym pairs provided by
our method. The difference between the precision of our
method and that of Alternative 4 reflects the number of
hypernym-hyponym pairs that our method could acquire
and that Alternative 4 could not. We assumed that for a
given HCS a hypernym was successfully acquired if one
of the above methods could find the correct hypernym. In
other words, the performance of Alternative 4 would be
achieved only when there were a technique to combine
the output of the above methods in an optimal way.
Figure 7 shows the comparison between our procedure
and the alternative methods. We plotted the graph as-
suming the pairs of hypernym candidates and hyponym
candidates were sorted in the same order as the order ob-
tained by our procedure6. The results suggest that our
method can acquire a significant number of hypernyms
that the alternative methods cannot obtain, when we gave
rather small amount of texts, a maximum of 100 docu-
ments per hyponym candidate, as in our current experi-
mental settings. There is possibility that the difference,
particularly the difference from the peformance of Alter-
native 3, becomes smaller when we give more texts to the
alternative methods. But the comparison in such settings
is actually a difficult task because of the time required for
downloading. It is our possible future work.
</bodyText>
<sectionHeader confidence="0.857463" genericHeader="conclusions">
5 Concluding Remarks and Future Work
</sectionHeader>
<bodyText confidence="0.999558">
We have proposed a method for acquiring hyponymy re-
lations from Web documents, and have shown its effec-
tiveness through experimental results. We also showed
</bodyText>
<footnote confidence="0.99653775">
6More precisely, we sorted only the hyponym candidates in
the order used by our procedure for sorting, and attached the
hypernym candidates produced by each alternative to the hy-
ponym candidates.
</footnote>
<sectionHeader confidence="0.992179" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997693972972973">
Maya Ando, Satoshi Sekine, and Shun Ishizaki. 2003.
Automatic extraction of hyponyms from newspaper us-
ing lexicosyntactic patterns. In IPSJSIG Technical Re-
port 2003-NL-157, pages 77–82. in Japanese.
Sharon A. Caraballo. 1999. Automatic construction of
a hypernym-labeled noun hierarchy from text. In Pro-
ceedings of 37th Annual Meeting of the Association for
Computational Linguistics, pages 120–126.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online ques-
tion answering: Answering questions before they are
asked. In Proceedings of the 41st Annural Meeting of
the Association for Computational Linguistics, pages
1–7.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th International Conference on Computational Lin-
guistics, pages 539–545.
Kyosuke Imasumi. 2001. Automatic acquisition of hy-
ponymy relations from coordinated noun phrases and
appositions. Master’s thesis, Kyushu Institute of Tech-
nology.
Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsuishi,
and Jun’ichi Tsujii. 2000. A hybrid Japanese parser
with hand-crafted grammar and statistics. In Proceed-
ings of COLING 2000, pages 411–417.
Yuji Matsumoto, Sadao Kurohashi, Takehito Utsuro, Hi-
roshi Taeki, and Makoto Nagao. 1993. Japanese
Morphological Analyzer JUMAN user’s manual. in
Japanese.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to wordnet: An on-line lexical database.
Journal ofLexicography, 3(4):235–244.
Emmanuel Morin and Christian Jacquemin. 2003. Auto-
matic acquisition and expansion of hypernym links. In
Computer and the Humanities 2003. forthcoming.
</reference>
<tableCaption confidence="0.996312">
Table 1: Examples of the acquired pairs of a hypernym candidate and HCS.
</tableCaption>
<table confidence="0.974387826923077">
Rank Hyponym candidate sets Hypernyms Rank Fired Hypernyms
by obtained in by Rules obtained in
Step4 Step3 Step3 Step4
1 2 3
29 殺人 (murder)*, 放火 (arson)*, 強姦 (rape)*, 侵入盗 (burglary)*, 犯罪 68 – – – 犯罪
侵入強盗 (burgle robbery)*, 非侵入盗 (theft without breaking-in)*, (crime) (crime)
非侵入強盗 (robbery without breaking-in)*
69 モスクワ (Moscow)*, キエフ (Kiev)*, タシケント (Tashkend)*, ロシア 169 – – + 地名
ミンスク (Minsk)*, トビリシ (Tbilisi)*, ドゥシャンベ (Dushanbe)*, (Russia) (place
ビシュケク (Bishkek)*, アスタナ (Astana)*, キシニョフ (Kishinev)*, name)
エレバン (Erevan)*, バクー (Baku)*, アシハバード (Ashkhabad)*
78 セギノール (Seguignol)*, 藤井康雄 (Yasuo Fujii)*, 選手 196 – – – 選手
五島裕二 (Yuji Goshima)*, 玉木朋孝 (Tomotaka Tamaki)*, (player) (player)
福留宏紀 (Hiroki Fukutome)*, 平野恵一 (Keiichi Hirano)*,
シェルドン (Sheldon)*, 塩谷和彦 (Kazuhiko Shiotani)*,
(These are baseball players.)
81 ワイヤレスカード (wirelesscard), 小電力セキュリティ(security), 無線 200 – – – 無線
市民ラジオ (radio), 特定小電力機器 (a kind of instrument), (wireless) (wireless)
PHS陸上移動局 (a kind of department)
116 イワウメ (Diapensia lapponica)*, チシマザサ (Sasa kurilensis), 花 280 – – – 花
キバナ シ ャ ク ナ ゲ (Rhododendron aureum)*, (flower) (flower)
ヤマナ ル コ ユ リ (Polygonatum lasianthum)*
127 シイタケ (shiitake mushroom)*, サンゴハリタケ (Hericium ramosum)*, キノコ 306 – – – キノコ
サンコタケ (Pseudocolus schellenbergiae)*, (mash- (mash-
シロイボカサタケ (Rhodophyllus murraii)*, room) room)
シロオニタケ (Amanita virgineoides Bas)*,
サンコタケ (Pseudocolus schellenbergiae)*
139 音楽 (music), 映画 (movie), マンガ (cartoon), サイト 324 – – – サイト
出会い (encounter), 芸能人 (artiste) (web site) (web site)
150 芥川竜之介 (Ryunosuke Akutagawa), 鷹野つぎ (Tsugi Takano), 作品 343 – – – 作品
若山牧水 (Bokusui Wakayama), 梶井基次郎 (Motojiro Kajii), (work) (work)
徳冨蘆花 (Roka Tokutomi), 宮本百合子 (Yuriko Miyamoto),
夏目漱石 (Soseki Natume), 田中貢太郎 (Kantaro Tanaka),
国木田独歩 (Doppo Kunikida), 夢野久作 (Kyusaku Yumeno),
ブレイクウィリアム (William Blake), 菊池寛 (Kan Kikuchi),
夢野久作海若藍平 (parse error)
(These are novelists.)
172 メーデー (May Day), クリスマス (Christmas Day), イースター (Easter), 日本 391 – – + 地名
新年 (the New Year), 万聖節 (All Saints’ Day), 主顕節 (Epifania), (Japan) (place
解放記念日 (Emancipation Day), 聖母受胎祭 (Immacolata concezione), name)
聖ステファノの日 (Stefano’s Day), 聖母昇天祭 (Ferragosto)
(These are national holidays in Italy.)
184 おかあさん (mother)*, 暖流 (warm current)*, 浮雲 (cloud drift)*, 映画 416 – – – 映画
青空娘 (blue sky girl)*, 美貌に罪あり (beauty has guilt)* (movie) (movie)
(These are Japanese movies.)
– 銀河群 (group of galaxies), 構成メンバー (member), 銀河 10 – + –
アンドロメダ銀河 (Andromeda Galaxy)*, 銀河系 (The Galaxy)*, (galaxy)
局部銀河群 (local group of galaxies)
– ブラジル (Brazil), フィリピン (Philippine), 韓国 (Korea), 日本 80 + – + –
インド (India), アメリカ (U.S.A.), タイ (Thailand), (Japan)
中国 (China), ペルー (Peru), オーストラリア (Australia),
アルゼンチン (Argentina), スペイン (Spain)
</table>
<bodyText confidence="0.7244495">
‘*’ indicates a hyponym candidate that is a true hyponym of the provided hypernym candidate.
‘+’ in the “Fired Rules” column indicates a firing rule, while ‘–’ specifies the rule that doesn’t fire.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.913216">
<title confidence="0.999656">Acquiring Hyponymy Relations from Web Documents</title>
<author confidence="0.9965">Keiji Shinzato Kentaro</author>
<affiliation confidence="0.9969725">School of Information Japan Advanced Institute of Science and Technology</affiliation>
<address confidence="0.995214">1-1 Asahidai, Tatsunokuchi, Nomi-gun, Ishikawa, 923-1292</address>
<abstract confidence="0.995482705882353">This paper describes an automatic method for acquiring hyponymy relations from HTML documents on the WWW. Hyponymy relations can play a crucial role in various natural language processing systems. Most existing acquisition methods for hyponymy relations rely particular linguistic patterns, such as as Our method, however, does not use such linguistic patterns, and we expect that our procedure can be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Maya Ando</author>
<author>Satoshi Sekine</author>
<author>Shun Ishizaki</author>
</authors>
<title>Automatic extraction of hyponyms from newspaper using lexicosyntactic patterns.</title>
<date>2003</date>
<booktitle>In IPSJSIG Technical Report 2003-NL-157,</booktitle>
<pages>77--82</pages>
<note>in Japanese.</note>
<contexts>
<context position="1305" citStr="Ando et al., 2003" startWordPosition="192" endWordPosition="195"> be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences. 1 Introduction The goal of this work is to become able to automatically acquire hyponymy relations for a wide range of words or phrases from HTML documents on the WWW. We do not use particular lexicosyntactic patterns, as previous attempts have (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003). The frequencies of use for such lexicosyntactic patterns are relatively low, and there can be many words or phrases that do not appear in such patterns even if we look at a large number of texts. The effort of searching for other clues indicating hyponymy relations is thus significant. We try to acquire hyponymy relations by combining three different types of clue obtainable from a wide range of words or phrases. The first type of clue is inclusion in itemizations or lists found in typical HTML documents on the WWW. The second consists of statistical measures such as the document frequency (</context>
<context position="25454" citStr="Ando et al., 2003" startWordPosition="4414" endWordPosition="4417">n such that they can explain the content of the itemization, and picked up the caption closest to the itemization and the second closest to it. Then, we checked if the picked-up captions included the proper hypernyms. Note that the precision obtained by this method is just an upper bound of real performance because we do not have a method to extract hypernyms from captions at least at the current stage of our research. Alternative 3 We prepared the lexicosyntactic patterns in Fig. 6, which are similar to the ones used in the previous studies of hypernym acquisition in Japanese (Imasumi, 2001; Ando et al., 2003). One difference from the previous studies was that we used a regular expression instead of a parser. This may have caused some errors, but our patterns were more generous than those used in the previous studies, and did not miss the expressions matched to the patterns from the previous studies. In other words, the accuracy obtained with our patterns was an upper bound on the performance obtained by the previous proposal. Another difference was that the procedure was given correct pairs of a hypernym and a hyponym computed beforehand using our proposed method, and it only checked whether given</context>
</contexts>
<marker>Ando, Sekine, Ishizaki, 2003</marker>
<rawString>Maya Ando, Satoshi Sekine, and Shun Ishizaki. 2003. Automatic extraction of hyponyms from newspaper using lexicosyntactic patterns. In IPSJSIG Technical Report 2003-NL-157, pages 77–82. in Japanese.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
</authors>
<title>Automatic construction of a hypernym-labeled noun hierarchy from text.</title>
<date>1999</date>
<booktitle>In Proceedings of 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>120--126</pages>
<contexts>
<context position="1218" citStr="Caraballo, 1999" startWordPosition="180" endWordPosition="181">however, does not use such linguistic patterns, and we expect that our procedure can be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences. 1 Introduction The goal of this work is to become able to automatically acquire hyponymy relations for a wide range of words or phrases from HTML documents on the WWW. We do not use particular lexicosyntactic patterns, as previous attempts have (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003). The frequencies of use for such lexicosyntactic patterns are relatively low, and there can be many words or phrases that do not appear in such patterns even if we look at a large number of texts. The effort of searching for other clues indicating hyponymy relations is thus significant. We try to acquire hyponymy relations by combining three different types of clue obtainable from a wide range of words or phrases. The first type of clue is inclusion in itemizations or lists found in typical HTML documents o</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>Sharon A. Caraballo. 1999. Automatic construction of a hypernym-labeled noun hierarchy from text. In Proceedings of 37th Annual Meeting of the Association for Computational Linguistics, pages 120–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Eduard Hovy</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>Offline strategies for online question answering: Answering questions before they are asked.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annural Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="1258" citStr="Fleischman et al., 2003" startWordPosition="184" endWordPosition="187">istic patterns, and we expect that our procedure can be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences. 1 Introduction The goal of this work is to become able to automatically acquire hyponymy relations for a wide range of words or phrases from HTML documents on the WWW. We do not use particular lexicosyntactic patterns, as previous attempts have (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003). The frequencies of use for such lexicosyntactic patterns are relatively low, and there can be many words or phrases that do not appear in such patterns even if we look at a large number of texts. The effort of searching for other clues indicating hyponymy relations is thus significant. We try to acquire hyponymy relations by combining three different types of clue obtainable from a wide range of words or phrases. The first type of clue is inclusion in itemizations or lists found in typical HTML documents on the WWW. The second consists of statis</context>
</contexts>
<marker>Fleischman, Hovy, Echihabi, 2003</marker>
<rawString>Michael Fleischman, Eduard Hovy, and Abdessamad Echihabi. 2003. Offline strategies for online question answering: Answering questions before they are asked. In Proceedings of the 41st Annural Meeting of the Association for Computational Linguistics, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="1201" citStr="Hearst, 1992" startWordPosition="178" endWordPosition="179">. Our method, however, does not use such linguistic patterns, and we expect that our procedure can be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences. 1 Introduction The goal of this work is to become able to automatically acquire hyponymy relations for a wide range of words or phrases from HTML documents on the WWW. We do not use particular lexicosyntactic patterns, as previous attempts have (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003). The frequencies of use for such lexicosyntactic patterns are relatively low, and there can be many words or phrases that do not appear in such patterns even if we look at a large number of texts. The effort of searching for other clues indicating hyponymy relations is thus significant. We try to acquire hyponymy relations by combining three different types of clue obtainable from a wide range of words or phrases. The first type of clue is inclusion in itemizations or lists found in typical</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th International Conference on Computational Linguistics, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyosuke Imasumi</author>
</authors>
<title>Automatic acquisition of hyponymy relations from coordinated noun phrases and appositions.</title>
<date>2001</date>
<tech>Master’s thesis,</tech>
<institution>Kyushu Institute of Technology.</institution>
<contexts>
<context position="1233" citStr="Imasumi, 2001" startWordPosition="182" endWordPosition="183"> use such linguistic patterns, and we expect that our procedure can be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences. 1 Introduction The goal of this work is to become able to automatically acquire hyponymy relations for a wide range of words or phrases from HTML documents on the WWW. We do not use particular lexicosyntactic patterns, as previous attempts have (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003). The frequencies of use for such lexicosyntactic patterns are relatively low, and there can be many words or phrases that do not appear in such patterns even if we look at a large number of texts. The effort of searching for other clues indicating hyponymy relations is thus significant. We try to acquire hyponymy relations by combining three different types of clue obtainable from a wide range of words or phrases. The first type of clue is inclusion in itemizations or lists found in typical HTML documents on the WWW. The </context>
<context position="25434" citStr="Imasumi, 2001" startWordPosition="4411" endWordPosition="4413"> in the position such that they can explain the content of the itemization, and picked up the caption closest to the itemization and the second closest to it. Then, we checked if the picked-up captions included the proper hypernyms. Note that the precision obtained by this method is just an upper bound of real performance because we do not have a method to extract hypernyms from captions at least at the current stage of our research. Alternative 3 We prepared the lexicosyntactic patterns in Fig. 6, which are similar to the ones used in the previous studies of hypernym acquisition in Japanese (Imasumi, 2001; Ando et al., 2003). One difference from the previous studies was that we used a regular expression instead of a parser. This may have caused some errors, but our patterns were more generous than those used in the previous studies, and did not miss the expressions matched to the patterns from the previous studies. In other words, the accuracy obtained with our patterns was an upper bound on the performance obtained by the previous proposal. Another difference was that the procedure was given correct pairs of a hypernym and a hyponym computed beforehand using our proposed method, and it only c</context>
</contexts>
<marker>Imasumi, 2001</marker>
<rawString>Kyosuke Imasumi. 2001. Automatic acquisition of hyponymy relations from coordinated noun phrases and appositions. Master’s thesis, Kyushu Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama</author>
<author>Kentaro Torisawa</author>
<author>Yutaka Mitsuishi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A hybrid Japanese parser with hand-crafted grammar and statistics.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>411--417</pages>
<contexts>
<context position="12789" citStr="Kanayama et al., 2000" startWordPosition="2163" endWordPosition="2166">es, while the behavior of true hypernyms should be semantically similar to the hyponyms. If we rank the pairs of hypernym candidates and HCSs according to their semantic similarities, the low ranked pairs are likely to have an erroneous hypernym candidate. We can then obtain relatively precise hypernyms by discarding the low ranked pairs. The similarities are computed through the following steps. First, we parse all the texts in the local document set, and check the argument positions of verbs where hyponym candidates appear. (To parse texts, we use a downgraded version of an existing parser (Kanayama et al., 2000) throughout this work.) Let us denote the frequency of the hyponym candidates in an HCS C occupying an argument position p of a verb v as fhypo(C, p, v). Assume that all possible argument positions are denoted as {p1, · · · , pl} and all the verbs as {v1, · · · , vm}. We then define the co-occurrence vector of hyponym candidates as follows. hypov(C) = hfhypo(C, p1, v1), fhypo(C,p2, v1), ··· , fhypo(C, pl−1, vm), fhypo(C, pl, vm)i In the same way, we can define the co-occurrence vector of a hypernym candidate n. hyperv(n) = (f(n,p1,v1),··· ,f(n,pt,vm)) Here, f(n, p, v) is the frequency of a nou</context>
</contexts>
<marker>Kanayama, Torisawa, Mitsuishi, Tsujii, 2000</marker>
<rawString>Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsuishi, and Jun’ichi Tsujii. 2000. A hybrid Japanese parser with hand-crafted grammar and statistics. In Proceedings of COLING 2000, pages 411–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
</authors>
<title>Sadao Kurohashi, Takehito Utsuro, Hiroshi Taeki, and Makoto Nagao.</title>
<date>1993</date>
<note>Japanese Morphological Analyzer JUMAN user’s manual. in Japanese.</note>
<marker>Matsumoto, 1993</marker>
<rawString>Yuji Matsumoto, Sadao Kurohashi, Takehito Utsuro, Hiroshi Taeki, and Makoto Nagao. 1993. Japanese Morphological Analyzer JUMAN user’s manual. in Japanese.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<title>Introduction to wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>Journal ofLexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="20169" citStr="Miller et al., 1990" startWordPosition="3492" endWordPosition="3495"> Steps 3 and 4, as described before, while the output of Step 2 was sorted by the df · idf score. In addition, we assumed 5The search engine “goo”. (http://www.goo.ne.jp) each step produced only the top 200 pairs in the sorted pairs. (Since the output of Step 4 is the final output, this means that we also assumed that only the top 200 pairs of a hypernym and an HCS would be produced as final output with our procedure. In other words, the remaining 1,800 (=2,000-200) pairs were discarded. ) The resulting hypernyms were checked by the authors according to the definition of the hypernym given in Miller et al., 1990, i.e., we checked if the expression “a hyponym candidate is a kind of a hypernym candidate.” is acceptable. Then, we computed the precision, which is the ratio of the correct hypernym-hyponym pairs against all the pairs obtained from the top n pairs of an HCS and its hypernym candidate. The x-axis of the graph indicates the number of hypernym-hyponym pairs obtained from the top n pairs of an HCS and its hypernym candidate, while the y-axis indicates the precision. More precisely, the curve for Step i plots the following points, where 1 ≤ j ≤ 200. �j k=1 correct(Ck, h(Ck)) |Ck|, i Ek=1 |Ck| co</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990. Introduction to wordnet: An on-line lexical database. Journal ofLexicography, 3(4):235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Morin</author>
<author>Christian Jacquemin</author>
</authors>
<title>Automatic acquisition and expansion of hypernym links.</title>
<date>2003</date>
<booktitle>In Computer and the Humanities 2003. forthcoming.</booktitle>
<contexts>
<context position="1285" citStr="Morin and Jacquemin, 2003" startWordPosition="188" endWordPosition="191">pect that our procedure can be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences. 1 Introduction The goal of this work is to become able to automatically acquire hyponymy relations for a wide range of words or phrases from HTML documents on the WWW. We do not use particular lexicosyntactic patterns, as previous attempts have (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003). The frequencies of use for such lexicosyntactic patterns are relatively low, and there can be many words or phrases that do not appear in such patterns even if we look at a large number of texts. The effort of searching for other clues indicating hyponymy relations is thus significant. We try to acquire hyponymy relations by combining three different types of clue obtainable from a wide range of words or phrases. The first type of clue is inclusion in itemizations or lists found in typical HTML documents on the WWW. The second consists of statistical measures such as the </context>
</contexts>
<marker>Morin, Jacquemin, 2003</marker>
<rawString>Emmanuel Morin and Christian Jacquemin. 2003. Automatic acquisition and expansion of hypernym links. In Computer and the Humanities 2003. forthcoming.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>