<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.966257">
Learning 5000 Relational Extractors
</title>
<author confidence="0.99898">
Raphael Hoffmann, Congle Zhang, Daniel S. Weld
</author>
<affiliation confidence="0.9954675">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.90408">
Seattle, WA-98195, USA
</address>
<email confidence="0.999495">
{raphaelh,clzhang,weld}@cs.washington.edu
</email>
<sectionHeader confidence="0.993913" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951588235294">
Many researchers are trying to use information
extraction (IE) to create large-scale knowl-
edge bases from natural language text on the
Web. However, the primary approach (su-
pervised learning of relation-specific extrac-
tors) requires manually-labeled training data
for each relation and doesn’t scale to the thou-
sands of relations encoded in Web text.
This paper presents LUCHS, a self-supervised,
relation-specific IE system which learns 5025
relations — more than an order of magnitude
greater than any previous approach — with an
average F1 score of 61%. Crucial to LUCHS’s
performance is an automated system for dy-
namic lexicon learning, which allows it to
learn accurately from heuristically-generated
training data, which is often noisy and sparse.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994002">
Information extraction (IE), the process of gen-
erating relational data from natural-language text,
has gained popularity for its potential applications
in Web search, question answering and other tasks.
Two main approaches have been attempted:
</bodyText>
<listItem confidence="0.9363476">
• Supervised learning of relation-specific ex-
tractors (e.g., (Freitag, 1998)), and
• “Open” IE — self-supervised learning of
unlexicalized, relation-independent extractors
(e.g., Textrunner (Banko et al., 2007)).
</listItem>
<bodyText confidence="0.994851414634147">
Unfortunately, both methods have problems.
Supervised approaches require manually-labeled
training data for each relation and hence can’t
scale to handle the thousands of relations encoded
in Web text. Open extraction is more scalable,
but has lower precision and recall. Furthermore,
open extraction doesn’t canonicalize relations, so
any application using the output must deal with
homonymy and synonymy.
A third approach, sometimes refered to as weak
supervision, is to heuristically match values from
a database to text, thus generating a set of train-
ing data for self-supervised learning of relation-
specific extractors (Craven and Kumlien, 1999).
With the Kylin system (Wu and Weld, 2007) ap-
plied this idea to Wikipedia by matching values
of an article’s infobox1 attributes to corresponding
sentences in the article, and suggested that their
approach could extract thousands of relations (Wu
et al., 2008). Unfortunately, however, they never
tested the idea on more than a dozen relations. In-
deed, no one has demonstrated a practical way to
extract more than about one hundred relations.
We note that Wikipedia’s infobox ‘ontology’ is
a particularly interesting target for extraction. As a
by-product of thousands of contributors, it is broad
in coverage and growing quickly. Unfortunately,
the schemata are surprisingly noisy and most are
sparsely populated; challenging conditions for ex-
traction.
This paper presents LUCHS, an autonomous,
self-supervised system, which learns 5025 rela-
tional extractors — an order of magnitude greater
than any previous effort. Like Kylin, LUCHS cre-
ates training data by matching Wikipedia attribute
values with corresponding sentences, but by itself,
this method was insufficient for accurate extrac-
tion of most relations. Thus, LUCHS introduces
a new technique, dynamic lexicon features, which
dramatically improves performance when learning
from sparse data and that way enables scalability.
</bodyText>
<subsectionHeader confidence="0.98745">
1.1 Dynamic Lexicon Features
</subsectionHeader>
<bodyText confidence="0.993395333333333">
Figure 1 summarizes the architecture of LUCHS.
At the highest level, LUCHS’s offline training pro-
cess resembles that of Kylin. Wikipedia pages
</bodyText>
<footnote confidence="0.9976288">
1A sizable fraction of Wikipedia articles have associated
infoboxes — relational summaries of the key aspects of the
subject of the article. For example, the infobox for Alan Tur-
ing’s Wikipedia page lists the values of 10 attributes, includ-
ing his birthdate, nationality and doctoral advisor.
</footnote>
<page confidence="0.927221">
286
</page>
<note confidence="0.9877275">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 286–295,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999062">
Figure 1: Architecture of LUCHS. In order to
</figureCaption>
<bodyText confidence="0.970730344827586">
handle sparsity in its heuristically-generated train-
ing data, LUCHS generates custom lexicon features
when learning each relational extractor.
containing infoboxes are used to train a classi-
fier that can predict the appropriate schema for
pages missing infoboxes. Additionally, the val-
ues of infobox attributes are compared with article
sentences to heuristically generate training data.
LUCHS’s major innovation is a feature-generation
process, which starts by harvesting HTML lists
from a 5B document Web crawl, discarding 98%
to create a set of 49M semantically-relevant lists.
When learning an extractor for relation R, LUCHS
extracts seed phrases from R’s training data and
uses a semi-supervised learning algorithm to cre-
ate several relation-specific lexicons at different
points on a precision-recall spectrum. These lex-
icons form Boolean features which, along with
lexical and dependency parser-based features, are
used to produce a CRF extractor for each relation
— one which performs much better than lexicon-
free extraction on sparse training data.
At runtime, LUCHS feeds pages to the article
classfier, which predicts which infobox schema
is most appropriate for extraction. Then a small
set of relation-specific extractors are applied to
each sentence, outputting tuples. Our experiments
demonstrate a high F1 score, 61%, across the 5025
relational extractors learned.
</bodyText>
<subsectionHeader confidence="0.853513">
1.2 Summary
</subsectionHeader>
<bodyText confidence="0.972486">
This paper makes several contributions:
</bodyText>
<listItem confidence="0.971642444444444">
• We present LUCHS, a self-supervised IE sys-
tem capable of learning more than an order
of magnitude more relation-specific extractors
than previous systems.
• We describe the construction and use of dy-
namic lexicon features, a novel technique, that
enables hyper-lexicalized extractors which
cope effectively with sparse training data.
• We evaluate the overall end-to-end perfor-
mance of LUCHS, showing an F1 score of 61%
when extracting relations from randomly se-
lected Wikipedia pages.
• We present a comprehensive set of additional
experiments, evaluating LUCHS’s individual
components, measuring the effect of dynamic
lexicon features, testing sensitivity to varying
amounts of training data, and categorizing the
types of relations LUCHS can extract.
</listItem>
<sectionHeader confidence="0.590259" genericHeader="introduction">
2 Heuristic Generation of Training Data
</sectionHeader>
<bodyText confidence="0.999921727272728">
Wikipedia is an ideal starting point for our long-
term goal of creating a massive knowledge base of
extracted facts for two reasons. First, it is com-
prehensive, containing a diverse body of content
with significant depth. Perhaps more importantly,
Wikipedia’s structure facilitates self-supervised
extraction. Infoboxes are short, manually-created
tabular summaries of many articles’ key facts —
effectively defining a relational schema for that
class of entity. Since the same facts are often ex-
pressed in both article and ontology, matching val-
ues of the ontology to the article can deliver valu-
able, though noisy, training data.
For example, the Wikipedia article on “Jerry Se-
infeld” contains the sentence “Seinfeld was born
in Brooklyn, New York.” and the article’s infobox
contains the attribute “birth place = Brooklyn”.
By matching the attribute’s value “Brooklyn” to
the sentence, we can heuristically generate train-
ing data for a birth place extractor. This data is
noisy; some attributes will not find matches, while
others will find many co-incidental matches.
</bodyText>
<sectionHeader confidence="0.8755" genericHeader="method">
3 Learning Extractors
</sectionHeader>
<bodyText confidence="0.999795928571429">
We first assume that each Wikipedia infobox at-
tribute corresponds to a unique relation (but see
Section 5.6) for which we would like to learn a
specific extractor. A major challenge with such
an approach is scalability. Running a relation-
specific extractor for each of Wikipedia’s 34,000
unique infobox attributes on each of Wikipedia’s
50 million sentences would require 1.7 trillion ex-
tractor executions.
We therefore choose a hierarchical approach
that combines both article classifiers and rela-
tion extractors. For each infobox schema, LUCHS
trains a classifier that predicts if an article is likely
to contain that schema. Only when an article
</bodyText>
<figure confidence="0.998998722222222">
Extraction
Learning
Pages Classified Pages Tuples
Training Data Training Data
Classifier
Learner
Article
Classifier
Matcher Harvester
CRF
Learner
Lexicons
Extractor
Extracto
Filtered Lists
Lexicon
Learner
WWW
</figure>
<page confidence="0.992676">
287
</page>
<bodyText confidence="0.998560771929825">
is likely to contain a schema, does LUCHS run
that schema’s relation extractors. To extract in-
fobox attributes from all of Wikipedia, LUCHS
now needs orders of magnitude fewer executions.
While this approach does not propagate infor-
mation from extractors back to article classifiers,
experiments confirm that our article classifiers
nonetheless deliver accurate results (Section 5.2),
reducing the potential benefit of joint inference. In
addition, our approach reduces the need for extrac-
tors to keep track of the larger context, thus sim-
plifying the extraction problem.
We briefly summarize article classification: We
use a linear, multi-class classifier with six kinds of
features: words in the article title, words in the
first sentence, words in the first sentence which
are direct objects to the verb ‘to be’, article sec-
tion headers, Wikipedia categories, and their an-
cestor categories. We use the voted perceptron al-
gorithm (Freund and Schapire, 1999) for training.
More challenging are the attribute extractors,
which we wish to be simple, fast, and able to well
capture local dependencies. We use a linear-chain
conditional random field (CRF) — an undirected
graphical model connecting a sequence of input
and output random variables, x = (x0, ... , xT)
and y = (y0, ... , yT) (Lafferty et al., 2001). In-
put variables are assigned words w. The states
of output variables represent discrete labels l, e.g.
Argi-of-Relj and Other. In our case, variables
are connected in a chain, following the first-order
Markov assumption. We train to maximize condi-
tional likelihood of output variables given an input
probability distribution. The CRF models p(y1x)
are represented with a log-linear distribution
where feature functions, f, encode sufficient
statistics of (x, y), T is the length of the sequence,
K is the number of feature functions, and Ak are
parameters representing feature weights, which
we learn during training. Z(x) is a partition func-
tion used to normalize the probabilities to 1. Fea-
ture functions allow complex, overlapping global
features with lookahead.
Common techniques for learning the weights Ak
include numeric optimization algorithms such as
stochastic gradient descent or L-BFGS. In our ex-
periments, we again use the simpler and more effi-
cient voted-perceptron algorithm (Collins, 2002).
The linear-chain layout enables efficient interence
using the dynamic programming-based Viterbi al-
gorithm (Lafferty et al., 2001).
We evaluate nine kinds of Boolean features:
Words For each input word w we introduce fea-
ture fww (yt−1, yt, x, t) := ✶[xt=w].
State Transitions For each transition be-
tween output labels li, lj we add feature
ftran
</bodyText>
<equation confidence="0.595613">
li,lj (yt−1, yt, x, t) := ✶[yt−1=li∧yt=lj].
</equation>
<bodyText confidence="0.8876">
Word Contextualization For parameters p and
s we add features fprev
</bodyText>
<equation confidence="0.687589">
w (yt−1, yt, x, t) :=
</equation>
<bodyText confidence="0.978156666666667">
✶[wE{xt−p,...,xt−1}] and fwsub (yt−1, yt, x, t) :=
✶[wE{xt+1,...,xt+3}] which capture a window of
words appearing before and after each position t.
Capitalization We add feature
fcap(yt−1, yt, x, t) := ✶[xtis capitalized].
Digits We add feature fdig(yt−1, yt, x, t) :=
✶[xtis digits].
Dependencies We set fdep(yt−1, yt, x, t) to the
lemmatized sequence of words from xt to the root
of the dependency tree, computed using the Stan-
ford parser (Marneffe et al., 2006).
First Sentence We set ffs(yt−1, yt, x, t) :=
✶[xtin first sentence of article].
Gaussians For numeric attributes, we fit a Gaus-
sian (p, Q) and add feature fgau
</bodyText>
<equation confidence="0.4578385">
i (yt−1, yt, x, t) :=
✶[|xt−µ|&lt;iσ] for parameters i.
</equation>
<bodyText confidence="0.995565">
Lexicons For non-numeric attributes, and for a
lexicon l, i.e. a set of related words, we add fea-
ture flex
l (yt−1, yt, x, t) := ✶[xtEl]. Lexicons are
explained in the following section.
</bodyText>
<sectionHeader confidence="0.886294" genericHeader="method">
4 Extraction with Lexicons
</sectionHeader>
<bodyText confidence="0.999980416666667">
It is often possible to group words that are likely
to be assigned similar labels, even if many of these
words do not appear in our training set. The ob-
tained lexicons then provide an elegant way to im-
prove the generalization ability of an extractor, es-
pecially when only little training data is available.
However, there is a danger of overfitting, which
we discuss in Section 4.2.4.
The next section explains how we mine the Web
to obtain a large corpus of quality lists. Then Sec-
tion 4.2 presents our semi-supervised algorithm
for learning semantic lexicons from these lists.
</bodyText>
<equation confidence="0.999771666666667">
T K
1
p(y1x) =
� �
Z(x) exp Akfk(yt−1, yt, x, t)
t=1 k=1
</equation>
<page confidence="0.991615">
288
</page>
<subsectionHeader confidence="0.999468">
4.1 Harvesting Lists from the Web
</subsectionHeader>
<bodyText confidence="0.999953956521739">
Domain-independence requires access to an ex-
tremely large number of lists, but our tight in-
tegration of lexicon acquisition and CRF learn-
ing requires that relevant lists be accessed instan-
taneously. Approaches using search engines or
wrappers at query time (Etzioni et al., 2004; Wang
and Cohen, 2008) are too slow; we must extract
and index lists prior to learning.
We begin with a 5 billion page Web crawl.
LUCHS can be combined with any list harvesting
technique, but we choose a simple approach, ex-
tracting lists defined by HTML &lt;ul&gt; or &lt;ol&gt;
tags. The set of lists obtained in this way is ex-
tremely noisy — many lists comprise navigation
bars, tag sets, spam links, or a series of long text
paragraphs. This is consistent with the observation
that less than 2% of Web tables are relational (Ca-
farella et al., 2008).
We therefore apply a series of filtering steps.
We remove lists of only one or two items, lists
containing long phrases, and duplicate lists from
the same host. After filtering we obtain 49 million
lists, containing 56 million unique phrases.
</bodyText>
<subsectionHeader confidence="0.997594">
4.2 Semi-Supervised Learning of Lexicons
</subsectionHeader>
<bodyText confidence="0.934454">
While training a CRF extractor for a given rela-
tion, LUCHS uses its corpus of lists to automati-
cally generate a set of semantic lexicons — spe-
cific to that relation. The technique proceeds in
three steps, which have been engineered to run ex-
tremely quickly:
</bodyText>
<listItem confidence="0.998846166666667">
1. Seed phrases are extracted from the labeled
training set.
2. A learning algorithm expands the seed
phrases into a set of lexicons.
3. The semantic lexicons are added as features
to the CRF learning algorithm.
</listItem>
<subsectionHeader confidence="0.991601">
4.2.1 Extracting Seed Phrases
</subsectionHeader>
<bodyText confidence="0.999863888888889">
For each training sentence LUCHS first identifies
subsequences of labeled words, and for each such
labeled subsequence, LUCHS creates one or more
seed phrases p. Typically, a set of seeds con-
sists precisely of the labeled subsequences. How-
ever, if the labeled subsequences are long and have
substructure, e.g., ‘San Remo, Italy’, our system
splits at the separator token, and creates additional
seed sets from prefixes and postfixes.
</bodyText>
<subsectionHeader confidence="0.482276">
4.2.2 From Seeds to Lexicons
</subsectionHeader>
<bodyText confidence="0.999566214285714">
To expand a set of seeds into a lexicon, LUCHS
must identify relevant lists in the corpus. Rele-
vancy can be computed by defining a similarity be-
tween lists using the vector-space model. Specifi-
cally, let L denote the corpus of lists, and P be the
set of unique phrases from L. Each list l0 E L can
be represented as a vector of weighted phrases p E
P appearing on the list, l0 = (l0p1l0p ... l0p|�|). Fol-
lowing the notion of inverse document frequency,
a phrase’s weight is inversely proportional to the
number of lists containing the phrase. Popular
phrases which appear on many lists thus receive
a small weight, whereas rare phrases are weighted
higher:
</bodyText>
<equation confidence="0.791272">
1
|ll E L|p E l}|
</equation>
<bodyText confidence="0.989153333333333">
Unlike the vector space model for documents, we
ignore term frequency, since the vast majority of
lists in our corpus don’t contain duplicates. This
vector representation supports the simple cosine
definition of list similarity, which for lists l0, l1 E
L is defined as
</bodyText>
<equation confidence="0.994389">
l0 · l1
simcos �=
</equation>
<bodyText confidence="0.999784807692308">
Intuitively, two lists are similar if they have many
overlapping phrases, the phrases are not too com-
mon, and the lists don’t contain many other
phrases. By representing the seed set as another
vector, we can find similar lists, hopefully contain-
ing related phrases. We then create a semantic lex-
icon by collecting phrases from a range of related
lists.
For example, one lexicon may be created as the
union of all phrases on lists that have non-zero
similarity to the seed list. Unfortunately, due to
the noisy nature of the Web lists such a lexicon
may be very large and may contain many irrele-
vant phrases. We expect that lists with higher sim-
ilarity are more likely to contain phrases which are
related to our seeds; hence, by varying the sim-
ilarity threshold one may produce lexicons rep-
resenting different compromises between lexicon
precision and recall. Not knowing which lexicon
will be most useful to the extractors, LUCHS gen-
erates several and lets the extractors learn appro-
priate weights.
However, since list similarities vary depending
on the seeds, fixed thresholds are not an option. If
#similarlists denotes the number of lists that have
non-zero similarity to the seed list and #lexicons
</bodyText>
<figure confidence="0.726271">
=
0
lA
IIl0IIIIl1II.
</figure>
<page confidence="0.9397">
289
</page>
<bodyText confidence="0.926514666666667">
the total number of lexicons we want to generate,
LUCHS sets lexicon i E {0, ... , #lexicons − 11
to be the union of prases on the
</bodyText>
<subsubsectionHeader confidence="0.411741">
#similarlistsz/#lexicons
</subsubsectionHeader>
<bodyText confidence="0.725396">
most similar lists.2
</bodyText>
<subsubsectionHeader confidence="0.55694">
4.2.3 Efficiently Creating Lexicons
</subsubsectionHeader>
<bodyText confidence="0.997927133333333">
We create lexicons from lists that are similar to
our seed vector, so we only consider lists that have
at least one phrase in common. Importantly, our
index structures allow LUCHS to select the rele-
vant lists efficiently. For each seed, LUCHS re-
trieves the set of containing lists as a sorted se-
quence of list identifiers. These sequences are
then merged yielding a sequence of list identifiers
with associated seed-hit counts. Precomputed list
lengths and inverse document frequencies are also
retrieved from indices, allowing efficient compu-
tation of similarity. The worst case complexity is
O(log(S)SK) where S is the number of seeds and
K the maximum number of lists to consider per
seed.
</bodyText>
<subsubsectionHeader confidence="0.588016">
4.2.4 Preventing Lexicon Overfitting
</subsubsectionHeader>
<bodyText confidence="0.999975944444445">
Finally, we integrate the acquired semantic lexi-
cons as features into the CRF. Although Section 3
discussed how to use lexicons as CRF features,
there are some subtleties. Recall that the lexi-
cons were created from seeds extracted from the
training set. If we now train the CRF on the same
examples that generated the lexicon features, then
the CRF will likely overfit, and weight the lexicon
features too highly!
Before training, we therefore split the training
set into k partitions. For each example in a par-
tition we assign features based on lexicons gener-
ated from only the k−1 remaining partitions. This
avoids overfitting and ensures that we will not per-
form much worse than without lexicon features.
When we apply the CRF to our test set, we use the
lexicons based on all k partitions. We refer to this
technique as cross-training.
</bodyText>
<sectionHeader confidence="0.999712" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9997086">
We start by evaluating end-to-end performance of
LUCHS when applied to Wikipedia text, then an-
alyze the characteristics of its components. Our
experiments use the 10/2008 English Wikipedia
dump.
</bodyText>
<footnote confidence="0.9879255">
2For practical reasons, we exclude the case i = #lexicons
in our experiments.
</footnote>
<figure confidence="0.994889875">
1.0
0.8
0.6
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0
recall
</figure>
<figureCaption confidence="0.992945">
Figure 2: Precision / recall curve for end-to-end
system performance on 100 random articles.
</figureCaption>
<subsectionHeader confidence="0.978299">
5.1 Overall Extraction Performance
</subsectionHeader>
<bodyText confidence="0.968649361111111">
To evaluate the end-to-end performance of
LUCHS, we test the pipeline which first classifies
incoming pages, activating a small set of extrac-
tors on the text. To ensure adequate training and
test data, we limit ourselves to infobox classes
with at least ten instances; there exist 1,583 such
classes, together comprising 981,387 articles. We
only consider the first ten sentences for each ar-
ticle, and we only consider 5025 attributes.3 We
create a test set by sampling 100 articles ran-
domly; these articles are not used to train article
classifiers or extractors. Each test article is then
automatically classified, and a random attribute
of the predicted schema is selected for extraction.
Gold labels for the selected attribute and article are
created manually by a human judge and compared
to the token-level predictions from the extractors
which are trainined on the remaining articles with
heuristic matches.
Overall, LUCHS reaches a precision of .55 at a
recall of .68, giving an F1-score of .61 (Figure 2).
Analyzing the errors in more detail, we find that in
11 of 100 cases an article was incorrectly classi-
fied. We note that in at least two of these cases the
predicted class could also be considered correct.
For example, instead of Infobox Minor Planet the
extractor predicted Infobox Planet.
On five of the selected attributes the extrac-
tor failed because the attributes could be consid-
ered unlearnable: The flexibility of Wikipedia’s
infobox system allows contributors to introduce
attributes for formatting, for example defining el-
3Attributes were selected to have at least 10 heuristic
matches, to have 10% of values covered by matches, and 10%
of articles with attribute in infobox covered by matches.
precision
</bodyText>
<page confidence="0.905628">
290
</page>
<bodyText confidence="0.9997711">
ement order. In the future we wish to train LUCHS
to ignore this type of attribute.
We also compared the heuristic matches con-
tained in the selected 100 articles to the gold stan-
dard: The matches reach a precision of .90 at a
recall of .33, giving an F1-score of .48. So while
most heuristic matches hit mentions of attribute
values, many other mentions go unmatched. Man-
ual analysis shows that these values are often miss-
ing from an infobox, are formatted differently, or
are inconsistent to what is stated in the article.
So why did the low recall of the heuristic
matches not adversely affect recall of our extrac-
tors? For most articles, an attribute can be as-
signed a single unique value. When training an
attribute extractor, only articles that contained a
heuristic match for that attribute were considered,
thus avoiding many cases of unmatched mentions.
Subsequent experiments evaluate the perfor-
mance of LUCHS components in more detail.
</bodyText>
<subsectionHeader confidence="0.99626">
5.2 Article Classification
</subsectionHeader>
<bodyText confidence="0.999992181818182">
The first step in LUCHS’s run-time pipeline is de-
termining which infobox schemata are most likely
to be found in a given article. To test this, we ran-
domly split our 981,387 articles into 4/5 for train-
ing and 1/5 for testing, and train a single multi-
class classifier. For this experiment, we use the
original infobox class of an article as its gold la-
bel. We compute the accuracy of the prediction at
.92. Since some classes can be considered inter-
changeable, this number represents a lower bound
on performance.
</bodyText>
<subsectionHeader confidence="0.995059">
5.3 Factors Affecting Extraction Accuracy
</subsectionHeader>
<bodyText confidence="0.9999384">
We now evaluate attribute extraction assuming
perfect article classification. To keep training time
manageable, we sample 100 articles for training
and 100 articles for testing4 for each of 100 ran-
dom attributes. We again only consider the first
ten sentences of each article, and we only con-
sider articles that have heuristic matches with the
attribute. We measure F1-score at a token-level,
taking the heuristic matches as ground-truth.
We first test the performance of extractors
trained using our basic features (Section 3)5, not
including lexicons and Gaussians. We begin us-
ing word features and obtain a token-level F1-
score of .311 for text and .311 for numeric at-
tributes. Adding any of our additional features
</bodyText>
<footnote confidence="0.999499666666667">
4These numbers are smaller for attributes with less train-
ing data available, but the same split is maintained.
5For contextualization features we choose p, s = 5.
</footnote>
<table confidence="0.999709">
Features F1-Score
Text attributes
Baseline .491
Baseline + Lexicons w/o CT .367
Baseline + Lexicons .545
Numeric attributes
Baseline .586
Baseline + Gaussians w/o CT .623
Baseline + Gaussians .627
</table>
<tableCaption confidence="0.780716333333333">
Table 1: Impact of Lexicon and Gaussian features.
Cross-Training (CT) is essential to improve per-
formance.
</tableCaption>
<bodyText confidence="0.999596666666667">
improves these scores, but the relative improve-
ments vary: For both text and numeric attributes,
contextualization and dependency features deliver
the largest improvement. We then iteratively add
the feature with largest improvement until no fur-
ther improvement is observed. We finally obtain
an F1-score of .491 for text and .586 for numeric
attributes. For text attributes the extractor uses
word, contextualization, first sentence, capitaliza-
tion, and digit features; for numeric attributes the
extractor uses word, contextualization, digit, first
sentence, and dependency features. We use these
extractors as a baseline to evaluate our lexicon and
Gaussian features.
Varying the size of the training sets affects re-
sults: Taking more articles raises the F1-score, but
taking more sentences per article reduces it. This
is because Wikipedia articles often summarize a
topic in the first few paragraphs and later discuss
related topics, necessitating reference resolution
which we plan to add in future work.
</bodyText>
<subsectionHeader confidence="0.990747">
5.4 Lexicon and Gaussian Features
</subsectionHeader>
<bodyText confidence="0.999901133333333">
We next study how our distribution features6 im-
pact the quality of the baseline extractors (Table
1). Without cross-training we observe a reduction
in performance, due to overfitting. Cross-training
avoids this, and substantially improves results over
the baseline. While cross-training is particularly
critical for lexicon features, it is less needed for
Gaussians where only two parameters, mean and
deviation, are fitted to the training set.
The relative improvements depend on the num-
ber of available training examples (Table 2). Lex-
icon and Gaussian features especially benefit ex-
tractors for sparse attributes. Here we can also see
that the improvements are mainly due to increases
in recall.
</bodyText>
<footnote confidence="0.984306">
6We set the number of lexicon and Gaussian features to 4.
</footnote>
<page confidence="0.989172">
291
</page>
<table confidence="0.999967555555556">
# Train F1-B F1-LUCHS AF1 APr ARe
Text attributes
10 .379 .439 +16% +10% +20%
25 .447 .504 +13% +7% +20%
100 .491 .545 +11% +5% +17%
Numeric attributes
10 .484 .531 +10% +4% +13%
25 .552 .596 +8% +4% +10%
100 .586 .627 +7% +5% +8%
</table>
<tableCaption confidence="0.991143">
Table 2: Lexicon and Gaussian features greatly ex-
</tableCaption>
<bodyText confidence="0.712177">
pand F1 score (F1-LUCHS) over the baseline (F1-
B), in particular for attributes with few training ex-
amples. Gains are mainly due to increased recall.
</bodyText>
<subsectionHeader confidence="0.995103">
5.5 Scaling to All of Wikipedia
</subsectionHeader>
<bodyText confidence="0.99999284">
Finally, we take our best extractors and run them
on all 5025 attributes, again assuming perfect ar-
ticle classification and using heuristic matches as
gold-standard. Figure 3 shows the distribution of
obtained F1 scores. 810 text attributes and 328 nu-
meric attributes reach a score of 0.80 or higher.
The performance depends on the number of
available training examples, and that number is
governed by a long-tailed distribution. For ex-
ample, 61% of the attributes in our set have 50
or fewer examples, 36% have 20 or fewer. Inter-
estingly, the number of training examples had a
smaller effect on performance than expected. Fig-
ure 4 shows the correlation between these vari-
ables. Lexicon and Gaussian features enables ac-
ceptable performance even for sparse attributes.
Averaging across all attributes we obtain F1
scores of 0.56 and 0.60 for textual and numeric
values respectively. We note that these scores
assume that all attributes are equally important,
weighting rare attributes just like common ones.
If we weight scores by the number of attribute in-
stances, we obtain F1 scores of 0.64 (textual) and
0.78 (numeric). In each case, precision is slightly
higher than recall.
</bodyText>
<subsectionHeader confidence="0.998771">
5.6 Towards an Attribute Ontology
</subsectionHeader>
<bodyText confidence="0.9999832">
The true promise of relation-specific extractors
comes when an ontology ties the system together.
By learning a probabilistic model of selectional
preferences, one can use joint inference to improve
extraction accuracy. One can also answer scien-
tific questions, such as “How many of the learned
Wikipedia attributes are distinct?” It is clear that
many duplicates exist due to collaborative sloppi-
ness, but semantic similarity is a matter of opinion
and an exact answer is impossible.
</bodyText>
<figure confidence="0.9527495">
0% 20% 40% 60% 80% 100%
# Attributes
</figure>
<figureCaption confidence="0.999541666666667">
Figure 3: F1 scores among attributes, ranked by
score. 810 text attributes (20%) and 328 numeric
attributes (31%) had an F1-score of .80 or higher.
</figureCaption>
<figure confidence="0.98935">
Numeric attr.
Text attr.
0.0
0 20 40 60 80 100
# Training Examples
</figure>
<figureCaption confidence="0.741644">
Figure 4: Average F1 score by number of training
examples. While more training data helps, even
sparse attributes reach acceptable performance.
</figureCaption>
<bodyText confidence="0.999936888888889">
Nevertheless, we clustered the textual attributes
in several ways. First, we cleaned the attribute
names heuristically and performed spell check.
The “distance” between two attributes was calcu-
lated with a combination of edit distance and IR
metrics with Wordnet synonyms; then hierarchical
agglomerative clustering was performed. We man-
ually assigned names to the clusters and cleaned
them, splitting and joining as needed. The result is
too crude to be called an ontology, but we continue
its elaboration. There are a total of 3962 attributes
grouped in about 1282 clusters (not yet counting
attributes with numerical values); the largest clus-
ter, location, has 115 similar attributes. Figure 5
shows the confusion matrix between attributes in
the biggest clusters; the shade of the i, jth pixel
indicates the F1 score achieved by training on in-
stances of attribute i and testing on attribute j.
</bodyText>
<figure confidence="0.994416357142857">
Numeric attr. (1063)
Text attr. (3962)
0.6
0.4
0.2
0.0
1.0
0.8
F1 Score
0.8
Average F1 Score
0.6
0.4
0.2
</figure>
<page confidence="0.837177">
292
</page>
<figureCaption confidence="0.935831">
Figure 5: Confusion matrix for extractor accuracy
</figureCaption>
<bodyText confidence="0.952483">
training on one attribute then testing on another.
Note the extraction similarity between title and
full-name, as well as between dates of birth and
death. Space constraints allow us to show only
1000 of LUCHS’s 5025 extracted attributes, those
in the largest clusters.
</bodyText>
<sectionHeader confidence="0.999787" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.996829690140845">
Large-scale extraction A popular approach to IE
is supervised learning of relation-specific extrac-
tors (Freitag, 1998). Open IE, self-supervised
learning of unlexicalized, relation-independent ex-
tractors (Banko et al., 2007), is a more scalable
approach, but suffers from lower precision and
recall, and doesn’t canonicalize the relations. A
third approach, weak supervision, performs self-
supervised learning of relation-specific extractors
from noisy training data, heuristically generated
by matching database values to text. (Craven and
Kumlien, 1999; Hirschman et al., 2002) apply this
technique to the biological domain, and (Mintz
et al., 2009) apply it to 102 relations from Free-
base. LUCHS differs from these approaches in that
its “database” – the set of infobox values – itself
is noisy, contains many more relations, and has
few instances per relation. Whereas the existing
approaches focus on syntactic extraction patterns,
LUCHS focuses on lexical information enhanced
by dynamic lexicon learning.
Extraction from Wikipedia Wikipedia has
become an interesting target for extraction.
(Suchanek et al., 2008) build a knowledgebase
from Wikipedia’s semi-structured data. (Wang et
al., 2007) propose a semisupervised positive-only
learning technique. Although that extracts from
text, its reliance on hyperlinks and other semi-
structured data limits extraction. (Wu and Weld,
2007; Wu et al., 2008)’s systems generate train-
ing data similar to LUCHS, but were only on a few
infobox classes. In contrast, LUCHS shows that
the idea scales to more than 5000 relations, but
that additional techniques, such as dynamic lexi-
con learning, are necessary to deal with sparsity.
Extraction with lexicons While lexicons have
been commonly used for IE (Cohen and Sarawagi,
2004; Agichtein and Ganti, 2004; Bellare and Mc-
Callum, 2007), many approaches assume that lex-
icons are clean and are supplied by a user before
training. Other approaches (Talukdar et al., 2006;
Miller et al., 2004; Riloff, 1993) learn lexicons
automatically from distributional patterns in text.
(Wang et al., 2009) learns lexicons from Web lists
for query tagging. LUCHS differs from these ap-
proaches in that it is not limited to a small set of
well-defined relations. Rather than creating large
lexicons of common entities, LUCHS attempts to
efficiently instantiate a series of lexicons from a
small set of seeds to bias extractors of sparse at-
tributes. Crucual to LUCHS’s different setting is
also the need to avoid overfitting.
Set expansion A large amount of work has
looked at automatically generating sets of related
items. Starting with a set of seed terms, (Etzioni
et al., 2004) extract lists by learning wrappers for
Web pages containing those terms. (Wang and Co-
hen, 2007; Wang and Cohen, 2008) extend the
idea, computing term relatedness through a ran-
dom walk algorithm that takes into account seeds,
documents, wrappers and mentions. Other ap-
proaches include Bayesian methods (Ghahramani
and Heller, 2005) and graph label propagation al-
gorithms (Talukdar et al., 2008; Bengio et al.,
2006). The goal of set expansion techniques is
to generate high precision sets of related items;
hence, these techniques are evaluated based on
lexicon precision and recall. For LUCHS, which is
evaluated based on the quality of an extractor us-
ing the lexicons, lexicon precision is not important
– as long as it does not confuse the extractor.
</bodyText>
<sectionHeader confidence="0.999565" genericHeader="method">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999539222222222">
We envision a Web-scale machine reading system
which simultaneously learns ontologies and ex-
tractors, and we believe that LUCHS’s approach
of leveraging noisy semi-structured information
(such as lists or formatting templates) is a key to-
wards this goal. For future work, we plan to en-
hance LUCHS in two major ways.
First, we note that a big weakness is that the
system currently only works for Wikipedia pages.
</bodyText>
<page confidence="0.9943">
293
</page>
<bodyText confidence="0.999935368421053">
For example, LUCHS assumes that each page cor-
responds to exactly one schema and that the sub-
ject of relations on a page are the same. Also,
LUCHS makes predictions on a token basis, thus
sometimes failing to recognize larger segments.
To remove these limitations we plan to add a
deeper linguistic analysis, making better use of
parse and dependency information and including
coreference resolution. We also plan to employ
relation-independent Open extraction techniques,
e.g. as suggested in (Wu and Weld, 2008) (retrain-
ing).
Second, we note that LUCHS’s performance
may benefit substantially from an attribute ontol-
ogy. As we showed in Section 5.6, LUCHS’s cur-
rent extractors can also greatly facilitate learning
a full attribute ontology. We therefore plan to in-
terleave extractor learning and ontology inference,
hence jointly learning ontology and extractors.
</bodyText>
<sectionHeader confidence="0.998352" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999933761904762">
Many researchers are trying to use IE to cre-
ate large-scale knowledge bases from natural lan-
guage text on the Web, but existing relation-
specific techniques do not scale to the thousands
of relations encoded in Web text – while relation-
independent techniques suffer from lower preci-
sion and recall, and do not canonicalize the rela-
tions. This paper shows that – with new techniques
– self-supervised learning of relation-specific ex-
tractors from Wikipedia infoboxes does scale.
In particular, we present LUCHS, a self-
supervised IE system capable of learning more
than an order of magnitude more relation-specific
extractors than previous systems. LUCHS uses
dynamic lexicon features that enable hyper-
lexicalized extractors which cope effectively with
sparse training data. We show an overall perfor-
mance of 61% F1 score, and present experiments
evaluating LUCHS’s individual components.
Datasets generated in this work are available to
the community7.
</bodyText>
<sectionHeader confidence="0.998579" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.799831666666667">
We thank Jesse Davis, Oren Etzioni, Andrey Kolobov,
Mausam, Fei Wu, and the anonymous reviewers for helpful
comments and suggestions.
</bodyText>
<footnote confidence="0.908852333333333">
This material is based upon work supported by a WRF /
TJ Cable Professorship, a gift from Google and by the Air
Force Research Laboratory (AFRL) under prime contract no.
FA8750-09-C-0181. Any opinions, findings, and conclusion
or recommendations expressed in this material are those of
7http://www.cs.washington.edu/ai/iwp
</footnote>
<note confidence="0.7503175">
the author(s) and do not necessarily reflect the view of the
Air Force Research Laboratory (AFRL).
</note>
<sectionHeader confidence="0.957858" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997570272727273">
Eugene Agichtein and Venkatesh Ganti. 2004. Mining refer-
ence tables for automatic text segmentation. In Proceed-
ings of the Tenth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD-2004),
pages 20–29.
S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary G. Ives. 2007.
Dbpedia: A nucleus for a web of open data. In Proceed-
ings of the 6th International Semantic Web Conference
and 2nd Asian Semantic Web Conference (ISWC/ASWC-
2007), pages 722–735.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of the
20th International Joint Conference on Artificial Intelli-
gence (IJCAI-2007), pages 2670–2676.
Kedar Bellare and Andrew McCallum. 2007. Learning ex-
tractors from unlabeled text using relevant databases. In
Sixth International Workshop on Information Integration
on the Web.
Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux.
2006. Label propagation and quadratic criterion. In
Olivier Chapelle, Bernhard Sch¨olkopf, and Alexander
Zien, editors, Semi-Supervised Learning, pages 193–216.
MIT Press.
Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang, Eu-
gene Wu, and Yang Zhang. 2008. Webtables: exploring
the power of tables on the web. Proceedings of the In-
ternational Conference on Very Large Databases (VLDB-
2008), 1(1):538–549.
Andrew Carlson, Justin Betteridge, Estevam R. Hruschka Jr.,
and Tom M. Mitchell. 2009a. Coupling semi-supervised
learning of categories and relations. In NAACL HLT 2009
Workskop on Semi-supervised Learning for Natural Lan-
guage Processing.
Andrew Carlson, Scott Gaffney, and Flavian Vasile. 2009b.
Learning a named entity tagger from gazetteers with the
partial perceptron. In AAAI Spring Symposium on Learn-
ing by Reading and Learning to Read.
William W. Cohen and Sunita Sarawagi. 2004. Exploiting
dictionaries in named entity extraction: combining semi-
markov extraction processes and data integration methods.
In Proceedings of the Tenth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining
(KDD-2004), pages 89–98.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Process-
ing (EMNLP-2002).
Mark Craven and Johan Kumlien. 1999. Constructing bi-
ological knowledge bases by extracting information from
text sources. In Proceedings of the Seventh International
Conference on Intelligent Systems for Molecular Biology
(ISMB-1999), pages 77–86.
</reference>
<page confidence="0.976872">
294
</page>
<reference confidence="0.999734104761905">
Benjamin Van Durme and Marius Pasca. 2008. Finding cars,
goddesses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extraction.
In Proceedings of the Twenty-Third AAAI Conference on
Artificial Intelligence (AAAI-2008), pages 1243–1248.
Oren Etzioni, Michael J. Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S.
Weld, and Alexander Yates. 2004. Methods for domain-
independent information extraction from the web: An ex-
perimental comparison. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence (AAAI-
2004), pages 391–398.
Dayne Freitag. 1998. Toward general-purpose learning for
information extraction. In Proceedings of the 17th inter-
national conference on Computational linguistics, pages
404–408. Association for Computational Linguistics.
Yoav Freund and Robert E. Schapire. 1999. Large margin
classification using the perceptron algorithm. Machine
Learning, 37(3):277–296.
Zoubin Ghahramani and Katherine A. Heller. 2005.
Bayesian sets. In Neural Information Processing Systems
(NIPS-2005).
Lynette Hirschman, Alexander A. Morgan, and Alexander S.
Yeh. 2002. Rutabaga by any other name: extracting
biological names. Journal of Biomedical Informatics,
35(4):247–259.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of the Eighteenth International Conference
on Machine Learning (ICML-2001), pages 282–289.
Marie-Catherine De Marneffe, Bill Maccartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the
fifth international conference on Language Resources and
Evaluation (LREC-2006).
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004.
Name tagging with word clusters and discriminative train-
ing. In Proceedings of the Human Language Technology
Conference of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL-2004).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky.
2009. Distant supervision for relation extraction without
labeled data. In The Annual Meeting of the Association
for Computational Linguistics (ACL-2009).
Marius Pasca. 2009. Outclassing wikipedia in open-domain
information extraction: Weakly-supervised acquisition of
attributes over conceptual hierarchies. In Proceedings
of the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-2009),
pages 639–647.
Ellen Riloff. 1993. Automatically constructing a dictionary
for information extraction tasks. In Proceedings of the
11th National Conference on Artificial Intelligence (AAAI-
1993), pages 811–816.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2008. Yago: A large ontology from wikipedia and word-
net. Elsevier Journal of Web Semantics, 6(3):203–217.
Fabian M. Suchanek, Mauro Sozio, and Gerhard Weikum.
2009. Sofie: A self-organizing framework for informa-
tion extraction. In Proceedings of the 18th International
Conference on World Wide Web (WWW-2009).
Partha Pratim Talukdar, Thorsten Brants, Mark Liberman,
and Fernando Pereira. 2006. A context pattern induction
method for named entity extraction. In The Tenth Confer-
ence on Natural Language Learning (CoNLL-X-2006).
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly-supervised acquisition of labeled
class instances using graph random walks. In EMNLP,
pages 582–590.
Richard C. Wang and William W. Cohen. 2007. Language-
independent set expansion of named entities using the
web. In Proceedings of the 7th IEEE International Con-
ference on Data Mining (ICDM-2007), pages 342–350.
Richard C. Wang and William W. Cohen. 2008. Iterative set
expansion of named entities using the web. In Proceed-
ings of the 8th IEEE International Conference on Data
Mining (ICDM-2008).
Gang Wang, Yong Yu, and Haiping Zhu. 2007. Pore:
Positive-only relation extraction from wikipedia text.
In Proceedings of the 6th International Semantic Web
Conference and 2nd Asian Semantic Web Conference
(ISWC/ASWC-2007), pages 580–594.
Ye-Yi Wang, Raphael Hoffmann, Xiao Li, and Alex Acero.
2009. Semi-supervised acquisition of semantic classes –
from the web and for the web. In International Confer-
ence on Information and Knowledge Management (CIKM-
2009), pages 37–46.
Fei Wu and Daniel S. Weld. 2007. Autonomously seman-
tifying wikipedia. In Proceedings of the International
Conference on Information and Knowledge Management
(CIKM-2007), pages 41–50.
Fei Wu and Daniel S. Weld. 2008. Automatically refin-
ing the wikipedia infobox ontology. In Proceedings of
the 17th International Conference on World Wide Web
(WWW-2008), pages 635–644.
Fei Wu and Daniel S. Weld. 2010. Open information ex-
traction using wikipedia. In The Annual Meeting of the
Association for Computational Linguistics (ACL-2010).
Fei Wu, Raphael Hoffmann, and Daniel S. Weld. 2008. In-
formation extraction from wikipedia: moving down the
long tail. In Proceedings of the 14th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data
Mining (KDD-2008), pages 731–739.
</reference>
<page confidence="0.998559">
295
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968277">
<title confidence="0.999838">Learning 5000 Relational Extractors</title>
<author confidence="0.999997">Raphael Hoffmann</author>
<author confidence="0.999997">Congle Zhang</author>
<author confidence="0.999997">Daniel S Weld</author>
<affiliation confidence="0.9997505">Computer Science &amp; Engineering University of Washington</affiliation>
<address confidence="0.9998">Seattle, WA-98195, USA</address>
<abstract confidence="0.998194944444444">Many researchers are trying to use information extraction (IE) to create large-scale knowledge bases from natural language text on the Web. However, the primary approach (supervised learning of relation-specific extractors) requires manually-labeled training data for each relation and doesn’t scale to the thousands of relations encoded in Web text. paper presents a self-supervised, relation-specific IE system which learns 5025 relations — more than an order of magnitude greater than any previous approach — with an F1 score of 61%. Crucial to is an automated system for dylexicon which allows it to learn accurately from heuristically-generated training data, which is often noisy and sparse.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Venkatesh Ganti</author>
</authors>
<title>Mining reference tables for automatic text segmentation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004),</booktitle>
<pages>20--29</pages>
<contexts>
<context position="31092" citStr="Agichtein and Ganti, 2004" startWordPosition="4939" endWordPosition="4942"> data. (Wang et al., 2007) propose a semisupervised positive-only learning technique. Although that extracts from text, its reliance on hyperlinks and other semistructured data limits extraction. (Wu and Weld, 2007; Wu et al., 2008)’s systems generate training data similar to LUCHS, but were only on a few infobox classes. In contrast, LUCHS shows that the idea scales to more than 5000 relations, but that additional techniques, such as dynamic lexicon learning, are necessary to deal with sparsity. Extraction with lexicons While lexicons have been commonly used for IE (Cohen and Sarawagi, 2004; Agichtein and Ganti, 2004; Bellare and McCallum, 2007), many approaches assume that lexicons are clean and are supplied by a user before training. Other approaches (Talukdar et al., 2006; Miller et al., 2004; Riloff, 1993) learn lexicons automatically from distributional patterns in text. (Wang et al., 2009) learns lexicons from Web lists for query tagging. LUCHS differs from these approaches in that it is not limited to a small set of well-defined relations. Rather than creating large lexicons of common entities, LUCHS attempts to efficiently instantiate a series of lexicons from a small set of seeds to bias extracto</context>
</contexts>
<marker>Agichtein, Ganti, 2004</marker>
<rawString>Eugene Agichtein and Venkatesh Ganti. 2004. Mining reference tables for automatic text segmentation. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004), pages 20–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Christian Bizer</author>
<author>Georgi Kobilarov</author>
<author>Jens Lehmann</author>
<author>Richard Cyganiak</author>
<author>Zachary G Ives</author>
</authors>
<title>Dbpedia: A nucleus for a web of open data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th International Semantic Web Conference and 2nd Asian Semantic Web Conference (ISWC/ASWC2007),</booktitle>
<pages>722--735</pages>
<marker>Auer, Bizer, Kobilarov, Lehmann, Cyganiak, Ives, 2007</marker>
<rawString>S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary G. Ives. 2007. Dbpedia: A nucleus for a web of open data. In Proceedings of the 6th International Semantic Web Conference and 2nd Asian Semantic Web Conference (ISWC/ASWC2007), pages 722–735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-2007),</booktitle>
<pages>2670--2676</pages>
<contexts>
<context position="1436" citStr="Banko et al., 2007" startWordPosition="197" endWordPosition="200">system for dynamic lexicon learning, which allows it to learn accurately from heuristically-generated training data, which is often noisy and sparse. 1 Introduction Information extraction (IE), the process of generating relational data from natural-language text, has gained popularity for its potential applications in Web search, question answering and other tasks. Two main approaches have been attempted: • Supervised learning of relation-specific extractors (e.g., (Freitag, 1998)), and • “Open” IE — self-supervised learning of unlexicalized, relation-independent extractors (e.g., Textrunner (Banko et al., 2007)). Unfortunately, both methods have problems. Supervised approaches require manually-labeled training data for each relation and hence can’t scale to handle the thousands of relations encoded in Web text. Open extraction is more scalable, but has lower precision and recall. Furthermore, open extraction doesn’t canonicalize relations, so any application using the output must deal with homonymy and synonymy. A third approach, sometimes refered to as weak supervision, is to heuristically match values from a database to text, thus generating a set of training data for self-supervised learning of r</context>
<context position="29517" citStr="Banko et al., 2007" startWordPosition="4702" endWordPosition="4705">2 0.0 1.0 0.8 F1 Score 0.8 Average F1 Score 0.6 0.4 0.2 292 Figure 5: Confusion matrix for extractor accuracy training on one attribute then testing on another. Note the extraction similarity between title and full-name, as well as between dates of birth and death. Space constraints allow us to show only 1000 of LUCHS’s 5025 extracted attributes, those in the largest clusters. 6 Related Work Large-scale extraction A popular approach to IE is supervised learning of relation-specific extractors (Freitag, 1998). Open IE, self-supervised learning of unlexicalized, relation-independent extractors (Banko et al., 2007), is a more scalable approach, but suffers from lower precision and recall, and doesn’t canonicalize the relations. A third approach, weak supervision, performs selfsupervised learning of relation-specific extractors from noisy training data, heuristically generated by matching database values to text. (Craven and Kumlien, 1999; Hirschman et al., 2002) apply this technique to the biological domain, and (Mintz et al., 2009) apply it to 102 relations from Freebase. LUCHS differs from these approaches in that its “database” – the set of infobox values – itself is noisy, contains many more relatio</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-2007), pages 2670–2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kedar Bellare</author>
<author>Andrew McCallum</author>
</authors>
<title>Learning extractors from unlabeled text using relevant databases.</title>
<date>2007</date>
<booktitle>In Sixth International Workshop on Information Integration on the Web.</booktitle>
<contexts>
<context position="31121" citStr="Bellare and McCallum, 2007" startWordPosition="4943" endWordPosition="4947">propose a semisupervised positive-only learning technique. Although that extracts from text, its reliance on hyperlinks and other semistructured data limits extraction. (Wu and Weld, 2007; Wu et al., 2008)’s systems generate training data similar to LUCHS, but were only on a few infobox classes. In contrast, LUCHS shows that the idea scales to more than 5000 relations, but that additional techniques, such as dynamic lexicon learning, are necessary to deal with sparsity. Extraction with lexicons While lexicons have been commonly used for IE (Cohen and Sarawagi, 2004; Agichtein and Ganti, 2004; Bellare and McCallum, 2007), many approaches assume that lexicons are clean and are supplied by a user before training. Other approaches (Talukdar et al., 2006; Miller et al., 2004; Riloff, 1993) learn lexicons automatically from distributional patterns in text. (Wang et al., 2009) learns lexicons from Web lists for query tagging. LUCHS differs from these approaches in that it is not limited to a small set of well-defined relations. Rather than creating large lexicons of common entities, LUCHS attempts to efficiently instantiate a series of lexicons from a small set of seeds to bias extractors of sparse attributes. Cruc</context>
</contexts>
<marker>Bellare, McCallum, 2007</marker>
<rawString>Kedar Bellare and Andrew McCallum. 2007. Learning extractors from unlabeled text using relevant databases. In Sixth International Workshop on Information Integration on the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Olivier Delalleau</author>
<author>Nicolas Le Roux</author>
</authors>
<title>Label propagation and quadratic criterion.</title>
<date>2006</date>
<booktitle>Semi-Supervised Learning,</booktitle>
<pages>193--216</pages>
<editor>In Olivier Chapelle, Bernhard Sch¨olkopf, and Alexander Zien, editors,</editor>
<publisher>MIT Press.</publisher>
<marker>Bengio, Delalleau, Le Roux, 2006</marker>
<rawString>Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. 2006. Label propagation and quadratic criterion. In Olivier Chapelle, Bernhard Sch¨olkopf, and Alexander Zien, editors, Semi-Supervised Learning, pages 193–216. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Cafarella</author>
<author>Alon Y Halevy</author>
<author>Daisy Zhe Wang</author>
<author>Eugene Wu</author>
<author>Yang Zhang</author>
</authors>
<title>Webtables: exploring the power of tables on the web.</title>
<date>2008</date>
<booktitle>Proceedings of the International Conference on Very Large Databases (VLDB2008),</booktitle>
<pages>1--1</pages>
<contexts>
<context position="13444" citStr="Cafarella et al., 2008" startWordPosition="2072" endWordPosition="2076">ly. Approaches using search engines or wrappers at query time (Etzioni et al., 2004; Wang and Cohen, 2008) are too slow; we must extract and index lists prior to learning. We begin with a 5 billion page Web crawl. LUCHS can be combined with any list harvesting technique, but we choose a simple approach, extracting lists defined by HTML &lt;ul&gt; or &lt;ol&gt; tags. The set of lists obtained in this way is extremely noisy — many lists comprise navigation bars, tag sets, spam links, or a series of long text paragraphs. This is consistent with the observation that less than 2% of Web tables are relational (Cafarella et al., 2008). We therefore apply a series of filtering steps. We remove lists of only one or two items, lists containing long phrases, and duplicate lists from the same host. After filtering we obtain 49 million lists, containing 56 million unique phrases. 4.2 Semi-Supervised Learning of Lexicons While training a CRF extractor for a given relation, LUCHS uses its corpus of lists to automatically generate a set of semantic lexicons — specific to that relation. The technique proceeds in three steps, which have been engineered to run extremely quickly: 1. Seed phrases are extracted from the labeled training </context>
</contexts>
<marker>Cafarella, Halevy, Wang, Wu, Zhang, 2008</marker>
<rawString>Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang. 2008. Webtables: exploring the power of tables on the web. Proceedings of the International Conference on Very Large Databases (VLDB2008), 1(1):538–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Coupling semi-supervised learning of categories and relations.</title>
<date>2009</date>
<booktitle>In NAACL HLT 2009 Workskop on Semi-supervised Learning for Natural Language Processing.</booktitle>
<marker>Carlson, Betteridge, Jr, Mitchell, 2009</marker>
<rawString>Andrew Carlson, Justin Betteridge, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2009a. Coupling semi-supervised learning of categories and relations. In NAACL HLT 2009 Workskop on Semi-supervised Learning for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Scott Gaffney</author>
<author>Flavian Vasile</author>
</authors>
<title>Learning a named entity tagger from gazetteers with the partial perceptron.</title>
<date>2009</date>
<booktitle>In AAAI Spring Symposium on Learning by Reading and Learning to Read.</booktitle>
<marker>Carlson, Gaffney, Vasile, 2009</marker>
<rawString>Andrew Carlson, Scott Gaffney, and Flavian Vasile. 2009b. Learning a named entity tagger from gazetteers with the partial perceptron. In AAAI Spring Symposium on Learning by Reading and Learning to Read.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Sunita Sarawagi</author>
</authors>
<title>Exploiting dictionaries in named entity extraction: combining semimarkov extraction processes and data integration methods.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004),</booktitle>
<pages>89--98</pages>
<contexts>
<context position="31065" citStr="Cohen and Sarawagi, 2004" startWordPosition="4935" endWordPosition="4938">ikipedia’s semi-structured data. (Wang et al., 2007) propose a semisupervised positive-only learning technique. Although that extracts from text, its reliance on hyperlinks and other semistructured data limits extraction. (Wu and Weld, 2007; Wu et al., 2008)’s systems generate training data similar to LUCHS, but were only on a few infobox classes. In contrast, LUCHS shows that the idea scales to more than 5000 relations, but that additional techniques, such as dynamic lexicon learning, are necessary to deal with sparsity. Extraction with lexicons While lexicons have been commonly used for IE (Cohen and Sarawagi, 2004; Agichtein and Ganti, 2004; Bellare and McCallum, 2007), many approaches assume that lexicons are clean and are supplied by a user before training. Other approaches (Talukdar et al., 2006; Miller et al., 2004; Riloff, 1993) learn lexicons automatically from distributional patterns in text. (Wang et al., 2009) learns lexicons from Web lists for query tagging. LUCHS differs from these approaches in that it is not limited to a small set of well-defined relations. Rather than creating large lexicons of common entities, LUCHS attempts to efficiently instantiate a series of lexicons from a small se</context>
</contexts>
<marker>Cohen, Sarawagi, 2004</marker>
<rawString>William W. Cohen and Sunita Sarawagi. 2004. Exploiting dictionaries in named entity extraction: combining semimarkov extraction processes and data integration methods. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004), pages 89–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-2002).</booktitle>
<contexts>
<context position="10578" citStr="Collins, 2002" startWordPosition="1577" endWordPosition="1578">ion where feature functions, f, encode sufficient statistics of (x, y), T is the length of the sequence, K is the number of feature functions, and Ak are parameters representing feature weights, which we learn during training. Z(x) is a partition function used to normalize the probabilities to 1. Feature functions allow complex, overlapping global features with lookahead. Common techniques for learning the weights Ak include numeric optimization algorithms such as stochastic gradient descent or L-BFGS. In our experiments, we again use the simpler and more efficient voted-perceptron algorithm (Collins, 2002). The linear-chain layout enables efficient interence using the dynamic programming-based Viterbi algorithm (Lafferty et al., 2001). We evaluate nine kinds of Boolean features: Words For each input word w we introduce feature fww (yt−1, yt, x, t) := ✶[xt=w]. State Transitions For each transition between output labels li, lj we add feature ftran li,lj (yt−1, yt, x, t) := ✶[yt−1=li∧yt=lj]. Word Contextualization For parameters p and s we add features fprev w (yt−1, yt, x, t) := ✶[wE{xt−p,...,xt−1}] and fwsub (yt−1, yt, x, t) := ✶[wE{xt+1,...,xt+3}] which capture a window of words appearing befor</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology (ISMB-1999),</booktitle>
<pages>77--86</pages>
<contexts>
<context position="2089" citStr="Craven and Kumlien, 1999" startWordPosition="292" endWordPosition="295">ds have problems. Supervised approaches require manually-labeled training data for each relation and hence can’t scale to handle the thousands of relations encoded in Web text. Open extraction is more scalable, but has lower precision and recall. Furthermore, open extraction doesn’t canonicalize relations, so any application using the output must deal with homonymy and synonymy. A third approach, sometimes refered to as weak supervision, is to heuristically match values from a database to text, thus generating a set of training data for self-supervised learning of relationspecific extractors (Craven and Kumlien, 1999). With the Kylin system (Wu and Weld, 2007) applied this idea to Wikipedia by matching values of an article’s infobox1 attributes to corresponding sentences in the article, and suggested that their approach could extract thousands of relations (Wu et al., 2008). Unfortunately, however, they never tested the idea on more than a dozen relations. Indeed, no one has demonstrated a practical way to extract more than about one hundred relations. We note that Wikipedia’s infobox ‘ontology’ is a particularly interesting target for extraction. As a by-product of thousands of contributors, it is broad i</context>
<context position="29846" citStr="Craven and Kumlien, 1999" startWordPosition="4747" endWordPosition="4750">25 extracted attributes, those in the largest clusters. 6 Related Work Large-scale extraction A popular approach to IE is supervised learning of relation-specific extractors (Freitag, 1998). Open IE, self-supervised learning of unlexicalized, relation-independent extractors (Banko et al., 2007), is a more scalable approach, but suffers from lower precision and recall, and doesn’t canonicalize the relations. A third approach, weak supervision, performs selfsupervised learning of relation-specific extractors from noisy training data, heuristically generated by matching database values to text. (Craven and Kumlien, 1999; Hirschman et al., 2002) apply this technique to the biological domain, and (Mintz et al., 2009) apply it to 102 relations from Freebase. LUCHS differs from these approaches in that its “database” – the set of infobox values – itself is noisy, contains many more relations, and has few instances per relation. Whereas the existing approaches focus on syntactic extraction patterns, LUCHS focuses on lexical information enhanced by dynamic lexicon learning. Extraction from Wikipedia Wikipedia has become an interesting target for extraction. (Suchanek et al., 2008) build a knowledgebase from Wikipe</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology (ISMB-1999), pages 77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Marius Pasca</author>
</authors>
<title>Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (AAAI-2008),</booktitle>
<pages>1243--1248</pages>
<marker>Van Durme, Pasca, 2008</marker>
<rawString>Benjamin Van Durme and Marius Pasca. 2008. Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction. In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (AAAI-2008), pages 1243–1248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael J Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Methods for domainindependent information extraction from the web: An experimental comparison.</title>
<date>2004</date>
<booktitle>In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI2004),</booktitle>
<pages>391--398</pages>
<contexts>
<context position="12904" citStr="Etzioni et al., 2004" startWordPosition="1974" endWordPosition="1977">r of overfitting, which we discuss in Section 4.2.4. The next section explains how we mine the Web to obtain a large corpus of quality lists. Then Section 4.2 presents our semi-supervised algorithm for learning semantic lexicons from these lists. T K 1 p(y1x) = � � Z(x) exp Akfk(yt−1, yt, x, t) t=1 k=1 288 4.1 Harvesting Lists from the Web Domain-independence requires access to an extremely large number of lists, but our tight integration of lexicon acquisition and CRF learning requires that relevant lists be accessed instantaneously. Approaches using search engines or wrappers at query time (Etzioni et al., 2004; Wang and Cohen, 2008) are too slow; we must extract and index lists prior to learning. We begin with a 5 billion page Web crawl. LUCHS can be combined with any list harvesting technique, but we choose a simple approach, extracting lists defined by HTML &lt;ul&gt; or &lt;ol&gt; tags. The set of lists obtained in this way is extremely noisy — many lists comprise navigation bars, tag sets, spam links, or a series of long text paragraphs. This is consistent with the observation that less than 2% of Web tables are relational (Cafarella et al., 2008). We therefore apply a series of filtering steps. We remove </context>
<context position="31949" citStr="Etzioni et al., 2004" startWordPosition="5080" endWordPosition="5083">ributional patterns in text. (Wang et al., 2009) learns lexicons from Web lists for query tagging. LUCHS differs from these approaches in that it is not limited to a small set of well-defined relations. Rather than creating large lexicons of common entities, LUCHS attempts to efficiently instantiate a series of lexicons from a small set of seeds to bias extractors of sparse attributes. Crucual to LUCHS’s different setting is also the need to avoid overfitting. Set expansion A large amount of work has looked at automatically generating sets of related items. Starting with a set of seed terms, (Etzioni et al., 2004) extract lists by learning wrappers for Web pages containing those terms. (Wang and Cohen, 2007; Wang and Cohen, 2008) extend the idea, computing term relatedness through a random walk algorithm that takes into account seeds, documents, wrappers and mentions. Other approaches include Bayesian methods (Ghahramani and Heller, 2005) and graph label propagation algorithms (Talukdar et al., 2008; Bengio et al., 2006). The goal of set expansion techniques is to generate high precision sets of related items; hence, these techniques are evaluated based on lexicon precision and recall. For LUCHS, which</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2004</marker>
<rawString>Oren Etzioni, Michael J. Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2004. Methods for domainindependent information extraction from the web: An experimental comparison. In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI2004), pages 391–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Toward general-purpose learning for information extraction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>404--408</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1302" citStr="Freitag, 1998" startWordPosition="182" endWordPosition="183">f magnitude greater than any previous approach — with an average F1 score of 61%. Crucial to LUCHS’s performance is an automated system for dynamic lexicon learning, which allows it to learn accurately from heuristically-generated training data, which is often noisy and sparse. 1 Introduction Information extraction (IE), the process of generating relational data from natural-language text, has gained popularity for its potential applications in Web search, question answering and other tasks. Two main approaches have been attempted: • Supervised learning of relation-specific extractors (e.g., (Freitag, 1998)), and • “Open” IE — self-supervised learning of unlexicalized, relation-independent extractors (e.g., Textrunner (Banko et al., 2007)). Unfortunately, both methods have problems. Supervised approaches require manually-labeled training data for each relation and hence can’t scale to handle the thousands of relations encoded in Web text. Open extraction is more scalable, but has lower precision and recall. Furthermore, open extraction doesn’t canonicalize relations, so any application using the output must deal with homonymy and synonymy. A third approach, sometimes refered to as weak supervisi</context>
<context position="29411" citStr="Freitag, 1998" startWordPosition="4691" endWordPosition="4692">nstances of attribute i and testing on attribute j. Numeric attr. (1063) Text attr. (3962) 0.6 0.4 0.2 0.0 1.0 0.8 F1 Score 0.8 Average F1 Score 0.6 0.4 0.2 292 Figure 5: Confusion matrix for extractor accuracy training on one attribute then testing on another. Note the extraction similarity between title and full-name, as well as between dates of birth and death. Space constraints allow us to show only 1000 of LUCHS’s 5025 extracted attributes, those in the largest clusters. 6 Related Work Large-scale extraction A popular approach to IE is supervised learning of relation-specific extractors (Freitag, 1998). Open IE, self-supervised learning of unlexicalized, relation-independent extractors (Banko et al., 2007), is a more scalable approach, but suffers from lower precision and recall, and doesn’t canonicalize the relations. A third approach, weak supervision, performs selfsupervised learning of relation-specific extractors from noisy training data, heuristically generated by matching database values to text. (Craven and Kumlien, 1999; Hirschman et al., 2002) apply this technique to the biological domain, and (Mintz et al., 2009) apply it to 102 relations from Freebase. LUCHS differs from these a</context>
</contexts>
<marker>Freitag, 1998</marker>
<rawString>Dayne Freitag. 1998. Toward general-purpose learning for information extraction. In Proceedings of the 17th international conference on Computational linguistics, pages 404–408. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="9220" citStr="Freund and Schapire, 1999" startWordPosition="1361" endWordPosition="1364">nonetheless deliver accurate results (Section 5.2), reducing the potential benefit of joint inference. In addition, our approach reduces the need for extractors to keep track of the larger context, thus simplifying the extraction problem. We briefly summarize article classification: We use a linear, multi-class classifier with six kinds of features: words in the article title, words in the first sentence, words in the first sentence which are direct objects to the verb ‘to be’, article section headers, Wikipedia categories, and their ancestor categories. We use the voted perceptron algorithm (Freund and Schapire, 1999) for training. More challenging are the attribute extractors, which we wish to be simple, fast, and able to well capture local dependencies. We use a linear-chain conditional random field (CRF) — an undirected graphical model connecting a sequence of input and output random variables, x = (x0, ... , xT) and y = (y0, ... , yT) (Lafferty et al., 2001). Input variables are assigned words w. The states of output variables represent discrete labels l, e.g. Argi-of-Relj and Other. In our case, variables are connected in a chain, following the first-order Markov assumption. We train to maximize condi</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>Katherine A Heller</author>
</authors>
<title>Bayesian sets.</title>
<date>2005</date>
<booktitle>In Neural Information Processing Systems (NIPS-2005).</booktitle>
<contexts>
<context position="32280" citStr="Ghahramani and Heller, 2005" startWordPosition="5131" endWordPosition="5134">om a small set of seeds to bias extractors of sparse attributes. Crucual to LUCHS’s different setting is also the need to avoid overfitting. Set expansion A large amount of work has looked at automatically generating sets of related items. Starting with a set of seed terms, (Etzioni et al., 2004) extract lists by learning wrappers for Web pages containing those terms. (Wang and Cohen, 2007; Wang and Cohen, 2008) extend the idea, computing term relatedness through a random walk algorithm that takes into account seeds, documents, wrappers and mentions. Other approaches include Bayesian methods (Ghahramani and Heller, 2005) and graph label propagation algorithms (Talukdar et al., 2008; Bengio et al., 2006). The goal of set expansion techniques is to generate high precision sets of related items; hence, these techniques are evaluated based on lexicon precision and recall. For LUCHS, which is evaluated based on the quality of an extractor using the lexicons, lexicon precision is not important – as long as it does not confuse the extractor. 7 Future Work We envision a Web-scale machine reading system which simultaneously learns ontologies and extractors, and we believe that LUCHS’s approach of leveraging noisy semi</context>
</contexts>
<marker>Ghahramani, Heller, 2005</marker>
<rawString>Zoubin Ghahramani and Katherine A. Heller. 2005. Bayesian sets. In Neural Information Processing Systems (NIPS-2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Alexander A Morgan</author>
<author>Alexander S Yeh</author>
</authors>
<title>Rutabaga by any other name: extracting biological names.</title>
<date>2002</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="29871" citStr="Hirschman et al., 2002" startWordPosition="4751" endWordPosition="4754">hose in the largest clusters. 6 Related Work Large-scale extraction A popular approach to IE is supervised learning of relation-specific extractors (Freitag, 1998). Open IE, self-supervised learning of unlexicalized, relation-independent extractors (Banko et al., 2007), is a more scalable approach, but suffers from lower precision and recall, and doesn’t canonicalize the relations. A third approach, weak supervision, performs selfsupervised learning of relation-specific extractors from noisy training data, heuristically generated by matching database values to text. (Craven and Kumlien, 1999; Hirschman et al., 2002) apply this technique to the biological domain, and (Mintz et al., 2009) apply it to 102 relations from Freebase. LUCHS differs from these approaches in that its “database” – the set of infobox values – itself is noisy, contains many more relations, and has few instances per relation. Whereas the existing approaches focus on syntactic extraction patterns, LUCHS focuses on lexical information enhanced by dynamic lexicon learning. Extraction from Wikipedia Wikipedia has become an interesting target for extraction. (Suchanek et al., 2008) build a knowledgebase from Wikipedia’s semi-structured dat</context>
</contexts>
<marker>Hirschman, Morgan, Yeh, 2002</marker>
<rawString>Lynette Hirschman, Alexander A. Morgan, and Alexander S. Yeh. 2002. Rutabaga by any other name: extracting biological names. Journal of Biomedical Informatics, 35(4):247–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning (ICML-2001),</booktitle>
<pages>282--289</pages>
<contexts>
<context position="9571" citStr="Lafferty et al., 2001" startWordPosition="1422" endWordPosition="1425">ds in the article title, words in the first sentence, words in the first sentence which are direct objects to the verb ‘to be’, article section headers, Wikipedia categories, and their ancestor categories. We use the voted perceptron algorithm (Freund and Schapire, 1999) for training. More challenging are the attribute extractors, which we wish to be simple, fast, and able to well capture local dependencies. We use a linear-chain conditional random field (CRF) — an undirected graphical model connecting a sequence of input and output random variables, x = (x0, ... , xT) and y = (y0, ... , yT) (Lafferty et al., 2001). Input variables are assigned words w. The states of output variables represent discrete labels l, e.g. Argi-of-Relj and Other. In our case, variables are connected in a chain, following the first-order Markov assumption. We train to maximize conditional likelihood of output variables given an input probability distribution. The CRF models p(y1x) are represented with a log-linear distribution where feature functions, f, encode sufficient statistics of (x, y), T is the length of the sequence, K is the number of feature functions, and Ak are parameters representing feature weights, which we lea</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning (ICML-2001), pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill Maccartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the fifth international conference on Language Resources and Evaluation (LREC-2006).</booktitle>
<marker>De Marneffe, Maccartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the fifth international conference on Language Resources and Evaluation (LREC-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL-2004).</booktitle>
<contexts>
<context position="31274" citStr="Miller et al., 2004" startWordPosition="4970" endWordPosition="4973">action. (Wu and Weld, 2007; Wu et al., 2008)’s systems generate training data similar to LUCHS, but were only on a few infobox classes. In contrast, LUCHS shows that the idea scales to more than 5000 relations, but that additional techniques, such as dynamic lexicon learning, are necessary to deal with sparsity. Extraction with lexicons While lexicons have been commonly used for IE (Cohen and Sarawagi, 2004; Agichtein and Ganti, 2004; Bellare and McCallum, 2007), many approaches assume that lexicons are clean and are supplied by a user before training. Other approaches (Talukdar et al., 2006; Miller et al., 2004; Riloff, 1993) learn lexicons automatically from distributional patterns in text. (Wang et al., 2009) learns lexicons from Web lists for query tagging. LUCHS differs from these approaches in that it is not limited to a small set of well-defined relations. Rather than creating large lexicons of common entities, LUCHS attempts to efficiently instantiate a series of lexicons from a small set of seeds to bias extractors of sparse attributes. Crucual to LUCHS’s different setting is also the need to avoid overfitting. Set expansion A large amount of work has looked at automatically generating sets </context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative training. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In The Annual Meeting of the Association for Computational Linguistics (ACL-2009).</booktitle>
<contexts>
<context position="29943" citStr="Mintz et al., 2009" startWordPosition="4763" endWordPosition="4766">r approach to IE is supervised learning of relation-specific extractors (Freitag, 1998). Open IE, self-supervised learning of unlexicalized, relation-independent extractors (Banko et al., 2007), is a more scalable approach, but suffers from lower precision and recall, and doesn’t canonicalize the relations. A third approach, weak supervision, performs selfsupervised learning of relation-specific extractors from noisy training data, heuristically generated by matching database values to text. (Craven and Kumlien, 1999; Hirschman et al., 2002) apply this technique to the biological domain, and (Mintz et al., 2009) apply it to 102 relations from Freebase. LUCHS differs from these approaches in that its “database” – the set of infobox values – itself is noisy, contains many more relations, and has few instances per relation. Whereas the existing approaches focus on syntactic extraction patterns, LUCHS focuses on lexical information enhanced by dynamic lexicon learning. Extraction from Wikipedia Wikipedia has become an interesting target for extraction. (Suchanek et al., 2008) build a knowledgebase from Wikipedia’s semi-structured data. (Wang et al., 2007) propose a semisupervised positive-only learning t</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In The Annual Meeting of the Association for Computational Linguistics (ACL-2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
</authors>
<title>Outclassing wikipedia in open-domain information extraction: Weakly-supervised acquisition of attributes over conceptual hierarchies.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2009),</booktitle>
<pages>639--647</pages>
<marker>Pasca, 2009</marker>
<rawString>Marius Pasca. 2009. Outclassing wikipedia in open-domain information extraction: Weakly-supervised acquisition of attributes over conceptual hierarchies. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2009), pages 639–647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically constructing a dictionary for information extraction tasks.</title>
<date>1993</date>
<booktitle>In Proceedings of the 11th National Conference on Artificial Intelligence (AAAI1993),</booktitle>
<pages>811--816</pages>
<contexts>
<context position="31289" citStr="Riloff, 1993" startWordPosition="4974" endWordPosition="4975"> 2007; Wu et al., 2008)’s systems generate training data similar to LUCHS, but were only on a few infobox classes. In contrast, LUCHS shows that the idea scales to more than 5000 relations, but that additional techniques, such as dynamic lexicon learning, are necessary to deal with sparsity. Extraction with lexicons While lexicons have been commonly used for IE (Cohen and Sarawagi, 2004; Agichtein and Ganti, 2004; Bellare and McCallum, 2007), many approaches assume that lexicons are clean and are supplied by a user before training. Other approaches (Talukdar et al., 2006; Miller et al., 2004; Riloff, 1993) learn lexicons automatically from distributional patterns in text. (Wang et al., 2009) learns lexicons from Web lists for query tagging. LUCHS differs from these approaches in that it is not limited to a small set of well-defined relations. Rather than creating large lexicons of common entities, LUCHS attempts to efficiently instantiate a series of lexicons from a small set of seeds to bias extractors of sparse attributes. Crucual to LUCHS’s different setting is also the need to avoid overfitting. Set expansion A large amount of work has looked at automatically generating sets of related item</context>
</contexts>
<marker>Riloff, 1993</marker>
<rawString>Ellen Riloff. 1993. Automatically constructing a dictionary for information extraction tasks. In Proceedings of the 11th National Conference on Artificial Intelligence (AAAI1993), pages 811–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A large ontology from wikipedia and wordnet.</title>
<date>2008</date>
<journal>Elsevier Journal of Web Semantics,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="30412" citStr="Suchanek et al., 2008" startWordPosition="4834" endWordPosition="4837">ching database values to text. (Craven and Kumlien, 1999; Hirschman et al., 2002) apply this technique to the biological domain, and (Mintz et al., 2009) apply it to 102 relations from Freebase. LUCHS differs from these approaches in that its “database” – the set of infobox values – itself is noisy, contains many more relations, and has few instances per relation. Whereas the existing approaches focus on syntactic extraction patterns, LUCHS focuses on lexical information enhanced by dynamic lexicon learning. Extraction from Wikipedia Wikipedia has become an interesting target for extraction. (Suchanek et al., 2008) build a knowledgebase from Wikipedia’s semi-structured data. (Wang et al., 2007) propose a semisupervised positive-only learning technique. Although that extracts from text, its reliance on hyperlinks and other semistructured data limits extraction. (Wu and Weld, 2007; Wu et al., 2008)’s systems generate training data similar to LUCHS, but were only on a few infobox classes. In contrast, LUCHS shows that the idea scales to more than 5000 relations, but that additional techniques, such as dynamic lexicon learning, are necessary to deal with sparsity. Extraction with lexicons While lexicons hav</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2008</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. Yago: A large ontology from wikipedia and wordnet. Elsevier Journal of Web Semantics, 6(3):203–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Mauro Sozio</author>
<author>Gerhard Weikum</author>
</authors>
<title>Sofie: A self-organizing framework for information extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th International Conference on World Wide Web (WWW-2009).</booktitle>
<marker>Suchanek, Sozio, Weikum, 2009</marker>
<rawString>Fabian M. Suchanek, Mauro Sozio, and Gerhard Weikum. 2009. Sofie: A self-organizing framework for information extraction. In Proceedings of the 18th International Conference on World Wide Web (WWW-2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Thorsten Brants</author>
<author>Mark Liberman</author>
<author>Fernando Pereira</author>
</authors>
<title>A context pattern induction method for named entity extraction.</title>
<date>2006</date>
<booktitle>In The Tenth Conference on Natural Language Learning (CoNLL-X-2006).</booktitle>
<contexts>
<context position="31253" citStr="Talukdar et al., 2006" startWordPosition="4966" endWordPosition="4969">ctured data limits extraction. (Wu and Weld, 2007; Wu et al., 2008)’s systems generate training data similar to LUCHS, but were only on a few infobox classes. In contrast, LUCHS shows that the idea scales to more than 5000 relations, but that additional techniques, such as dynamic lexicon learning, are necessary to deal with sparsity. Extraction with lexicons While lexicons have been commonly used for IE (Cohen and Sarawagi, 2004; Agichtein and Ganti, 2004; Bellare and McCallum, 2007), many approaches assume that lexicons are clean and are supplied by a user before training. Other approaches (Talukdar et al., 2006; Miller et al., 2004; Riloff, 1993) learn lexicons automatically from distributional patterns in text. (Wang et al., 2009) learns lexicons from Web lists for query tagging. LUCHS differs from these approaches in that it is not limited to a small set of well-defined relations. Rather than creating large lexicons of common entities, LUCHS attempts to efficiently instantiate a series of lexicons from a small set of seeds to bias extractors of sparse attributes. Crucual to LUCHS’s different setting is also the need to avoid overfitting. Set expansion A large amount of work has looked at automatic</context>
</contexts>
<marker>Talukdar, Brants, Liberman, Pereira, 2006</marker>
<rawString>Partha Pratim Talukdar, Thorsten Brants, Mark Liberman, and Fernando Pereira. 2006. A context pattern induction method for named entity extraction. In The Tenth Conference on Natural Language Learning (CoNLL-X-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Joseph Reisinger</author>
<author>Marius Pasca</author>
<author>Deepak Ravichandran</author>
<author>Rahul Bhagat</author>
<author>Fernando Pereira</author>
</authors>
<title>Weakly-supervised acquisition of labeled class instances using graph random walks.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>582--590</pages>
<contexts>
<context position="32342" citStr="Talukdar et al., 2008" startWordPosition="5141" endWordPosition="5144">ual to LUCHS’s different setting is also the need to avoid overfitting. Set expansion A large amount of work has looked at automatically generating sets of related items. Starting with a set of seed terms, (Etzioni et al., 2004) extract lists by learning wrappers for Web pages containing those terms. (Wang and Cohen, 2007; Wang and Cohen, 2008) extend the idea, computing term relatedness through a random walk algorithm that takes into account seeds, documents, wrappers and mentions. Other approaches include Bayesian methods (Ghahramani and Heller, 2005) and graph label propagation algorithms (Talukdar et al., 2008; Bengio et al., 2006). The goal of set expansion techniques is to generate high precision sets of related items; hence, these techniques are evaluated based on lexicon precision and recall. For LUCHS, which is evaluated based on the quality of an extractor using the lexicons, lexicon precision is not important – as long as it does not confuse the extractor. 7 Future Work We envision a Web-scale machine reading system which simultaneously learns ontologies and extractors, and we believe that LUCHS’s approach of leveraging noisy semi-structured information (such as lists or formatting templates</context>
</contexts>
<marker>Talukdar, Reisinger, Pasca, Ravichandran, Bhagat, Pereira, 2008</marker>
<rawString>Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca, Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira. 2008. Weakly-supervised acquisition of labeled class instances using graph random walks. In EMNLP, pages 582–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard C Wang</author>
<author>William W Cohen</author>
</authors>
<title>Languageindependent set expansion of named entities using the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th IEEE International Conference on Data Mining (ICDM-2007),</booktitle>
<pages>342--350</pages>
<contexts>
<context position="32044" citStr="Wang and Cohen, 2007" startWordPosition="5095" endWordPosition="5099">g. LUCHS differs from these approaches in that it is not limited to a small set of well-defined relations. Rather than creating large lexicons of common entities, LUCHS attempts to efficiently instantiate a series of lexicons from a small set of seeds to bias extractors of sparse attributes. Crucual to LUCHS’s different setting is also the need to avoid overfitting. Set expansion A large amount of work has looked at automatically generating sets of related items. Starting with a set of seed terms, (Etzioni et al., 2004) extract lists by learning wrappers for Web pages containing those terms. (Wang and Cohen, 2007; Wang and Cohen, 2008) extend the idea, computing term relatedness through a random walk algorithm that takes into account seeds, documents, wrappers and mentions. Other approaches include Bayesian methods (Ghahramani and Heller, 2005) and graph label propagation algorithms (Talukdar et al., 2008; Bengio et al., 2006). The goal of set expansion techniques is to generate high precision sets of related items; hence, these techniques are evaluated based on lexicon precision and recall. For LUCHS, which is evaluated based on the quality of an extractor using the lexicons, lexicon precision is not</context>
</contexts>
<marker>Wang, Cohen, 2007</marker>
<rawString>Richard C. Wang and William W. Cohen. 2007. Languageindependent set expansion of named entities using the web. In Proceedings of the 7th IEEE International Conference on Data Mining (ICDM-2007), pages 342–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard C Wang</author>
<author>William W Cohen</author>
</authors>
<title>Iterative set expansion of named entities using the web.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th IEEE International Conference on Data Mining (ICDM-2008).</booktitle>
<contexts>
<context position="12927" citStr="Wang and Cohen, 2008" startWordPosition="1978" endWordPosition="1981">h we discuss in Section 4.2.4. The next section explains how we mine the Web to obtain a large corpus of quality lists. Then Section 4.2 presents our semi-supervised algorithm for learning semantic lexicons from these lists. T K 1 p(y1x) = � � Z(x) exp Akfk(yt−1, yt, x, t) t=1 k=1 288 4.1 Harvesting Lists from the Web Domain-independence requires access to an extremely large number of lists, but our tight integration of lexicon acquisition and CRF learning requires that relevant lists be accessed instantaneously. Approaches using search engines or wrappers at query time (Etzioni et al., 2004; Wang and Cohen, 2008) are too slow; we must extract and index lists prior to learning. We begin with a 5 billion page Web crawl. LUCHS can be combined with any list harvesting technique, but we choose a simple approach, extracting lists defined by HTML &lt;ul&gt; or &lt;ol&gt; tags. The set of lists obtained in this way is extremely noisy — many lists comprise navigation bars, tag sets, spam links, or a series of long text paragraphs. This is consistent with the observation that less than 2% of Web tables are relational (Cafarella et al., 2008). We therefore apply a series of filtering steps. We remove lists of only one or tw</context>
<context position="32067" citStr="Wang and Cohen, 2008" startWordPosition="5100" endWordPosition="5103">these approaches in that it is not limited to a small set of well-defined relations. Rather than creating large lexicons of common entities, LUCHS attempts to efficiently instantiate a series of lexicons from a small set of seeds to bias extractors of sparse attributes. Crucual to LUCHS’s different setting is also the need to avoid overfitting. Set expansion A large amount of work has looked at automatically generating sets of related items. Starting with a set of seed terms, (Etzioni et al., 2004) extract lists by learning wrappers for Web pages containing those terms. (Wang and Cohen, 2007; Wang and Cohen, 2008) extend the idea, computing term relatedness through a random walk algorithm that takes into account seeds, documents, wrappers and mentions. Other approaches include Bayesian methods (Ghahramani and Heller, 2005) and graph label propagation algorithms (Talukdar et al., 2008; Bengio et al., 2006). The goal of set expansion techniques is to generate high precision sets of related items; hence, these techniques are evaluated based on lexicon precision and recall. For LUCHS, which is evaluated based on the quality of an extractor using the lexicons, lexicon precision is not important – as long as</context>
</contexts>
<marker>Wang, Cohen, 2008</marker>
<rawString>Richard C. Wang and William W. Cohen. 2008. Iterative set expansion of named entities using the web. In Proceedings of the 8th IEEE International Conference on Data Mining (ICDM-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gang Wang</author>
<author>Yong Yu</author>
<author>Haiping Zhu</author>
</authors>
<title>Pore: Positive-only relation extraction from wikipedia text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th International Semantic Web Conference and 2nd Asian Semantic Web Conference (ISWC/ASWC-2007),</booktitle>
<pages>580--594</pages>
<contexts>
<context position="30493" citStr="Wang et al., 2007" startWordPosition="4845" endWordPosition="4848">ly this technique to the biological domain, and (Mintz et al., 2009) apply it to 102 relations from Freebase. LUCHS differs from these approaches in that its “database” – the set of infobox values – itself is noisy, contains many more relations, and has few instances per relation. Whereas the existing approaches focus on syntactic extraction patterns, LUCHS focuses on lexical information enhanced by dynamic lexicon learning. Extraction from Wikipedia Wikipedia has become an interesting target for extraction. (Suchanek et al., 2008) build a knowledgebase from Wikipedia’s semi-structured data. (Wang et al., 2007) propose a semisupervised positive-only learning technique. Although that extracts from text, its reliance on hyperlinks and other semistructured data limits extraction. (Wu and Weld, 2007; Wu et al., 2008)’s systems generate training data similar to LUCHS, but were only on a few infobox classes. In contrast, LUCHS shows that the idea scales to more than 5000 relations, but that additional techniques, such as dynamic lexicon learning, are necessary to deal with sparsity. Extraction with lexicons While lexicons have been commonly used for IE (Cohen and Sarawagi, 2004; Agichtein and Ganti, 2004;</context>
</contexts>
<marker>Wang, Yu, Zhu, 2007</marker>
<rawString>Gang Wang, Yong Yu, and Haiping Zhu. 2007. Pore: Positive-only relation extraction from wikipedia text. In Proceedings of the 6th International Semantic Web Conference and 2nd Asian Semantic Web Conference (ISWC/ASWC-2007), pages 580–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Raphael Hoffmann</author>
<author>Xiao Li</author>
<author>Alex Acero</author>
</authors>
<title>Semi-supervised acquisition of semantic classes – from the web and for the web.</title>
<date>2009</date>
<booktitle>In International Conference on Information and Knowledge Management (CIKM2009),</booktitle>
<pages>37--46</pages>
<contexts>
<context position="31376" citStr="Wang et al., 2009" startWordPosition="4984" endWordPosition="4987">e only on a few infobox classes. In contrast, LUCHS shows that the idea scales to more than 5000 relations, but that additional techniques, such as dynamic lexicon learning, are necessary to deal with sparsity. Extraction with lexicons While lexicons have been commonly used for IE (Cohen and Sarawagi, 2004; Agichtein and Ganti, 2004; Bellare and McCallum, 2007), many approaches assume that lexicons are clean and are supplied by a user before training. Other approaches (Talukdar et al., 2006; Miller et al., 2004; Riloff, 1993) learn lexicons automatically from distributional patterns in text. (Wang et al., 2009) learns lexicons from Web lists for query tagging. LUCHS differs from these approaches in that it is not limited to a small set of well-defined relations. Rather than creating large lexicons of common entities, LUCHS attempts to efficiently instantiate a series of lexicons from a small set of seeds to bias extractors of sparse attributes. Crucual to LUCHS’s different setting is also the need to avoid overfitting. Set expansion A large amount of work has looked at automatically generating sets of related items. Starting with a set of seed terms, (Etzioni et al., 2004) extract lists by learning </context>
</contexts>
<marker>Wang, Hoffmann, Li, Acero, 2009</marker>
<rawString>Ye-Yi Wang, Raphael Hoffmann, Xiao Li, and Alex Acero. 2009. Semi-supervised acquisition of semantic classes – from the web and for the web. In International Conference on Information and Knowledge Management (CIKM2009), pages 37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Autonomously semantifying wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Information and Knowledge Management (CIKM-2007),</booktitle>
<pages>41--50</pages>
<contexts>
<context position="2132" citStr="Wu and Weld, 2007" startWordPosition="300" endWordPosition="303">nually-labeled training data for each relation and hence can’t scale to handle the thousands of relations encoded in Web text. Open extraction is more scalable, but has lower precision and recall. Furthermore, open extraction doesn’t canonicalize relations, so any application using the output must deal with homonymy and synonymy. A third approach, sometimes refered to as weak supervision, is to heuristically match values from a database to text, thus generating a set of training data for self-supervised learning of relationspecific extractors (Craven and Kumlien, 1999). With the Kylin system (Wu and Weld, 2007) applied this idea to Wikipedia by matching values of an article’s infobox1 attributes to corresponding sentences in the article, and suggested that their approach could extract thousands of relations (Wu et al., 2008). Unfortunately, however, they never tested the idea on more than a dozen relations. Indeed, no one has demonstrated a practical way to extract more than about one hundred relations. We note that Wikipedia’s infobox ‘ontology’ is a particularly interesting target for extraction. As a by-product of thousands of contributors, it is broad in coverage and growing quickly. Unfortunate</context>
<context position="30681" citStr="Wu and Weld, 2007" startWordPosition="4871" endWordPosition="4874"> values – itself is noisy, contains many more relations, and has few instances per relation. Whereas the existing approaches focus on syntactic extraction patterns, LUCHS focuses on lexical information enhanced by dynamic lexicon learning. Extraction from Wikipedia Wikipedia has become an interesting target for extraction. (Suchanek et al., 2008) build a knowledgebase from Wikipedia’s semi-structured data. (Wang et al., 2007) propose a semisupervised positive-only learning technique. Although that extracts from text, its reliance on hyperlinks and other semistructured data limits extraction. (Wu and Weld, 2007; Wu et al., 2008)’s systems generate training data similar to LUCHS, but were only on a few infobox classes. In contrast, LUCHS shows that the idea scales to more than 5000 relations, but that additional techniques, such as dynamic lexicon learning, are necessary to deal with sparsity. Extraction with lexicons While lexicons have been commonly used for IE (Cohen and Sarawagi, 2004; Agichtein and Ganti, 2004; Bellare and McCallum, 2007), many approaches assume that lexicons are clean and are supplied by a user before training. Other approaches (Talukdar et al., 2006; Miller et al., 2004; Rilof</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel S. Weld. 2007. Autonomously semantifying wikipedia. In Proceedings of the International Conference on Information and Knowledge Management (CIKM-2007), pages 41–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Automatically refining the wikipedia infobox ontology.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th International Conference on World Wide Web (WWW-2008),</booktitle>
<pages>635--644</pages>
<contexts>
<context position="33645" citStr="Wu and Weld, 2008" startWordPosition="5356" endWordPosition="5359">ways. First, we note that a big weakness is that the system currently only works for Wikipedia pages. 293 For example, LUCHS assumes that each page corresponds to exactly one schema and that the subject of relations on a page are the same. Also, LUCHS makes predictions on a token basis, thus sometimes failing to recognize larger segments. To remove these limitations we plan to add a deeper linguistic analysis, making better use of parse and dependency information and including coreference resolution. We also plan to employ relation-independent Open extraction techniques, e.g. as suggested in (Wu and Weld, 2008) (retraining). Second, we note that LUCHS’s performance may benefit substantially from an attribute ontology. As we showed in Section 5.6, LUCHS’s current extractors can also greatly facilitate learning a full attribute ontology. We therefore plan to interleave extractor learning and ontology inference, hence jointly learning ontology and extractors. 8 Conclusion Many researchers are trying to use IE to create large-scale knowledge bases from natural language text on the Web, but existing relationspecific techniques do not scale to the thousands of relations encoded in Web text – while relatio</context>
</contexts>
<marker>Wu, Weld, 2008</marker>
<rawString>Fei Wu and Daniel S. Weld. 2008. Automatically refining the wikipedia infobox ontology. In Proceedings of the 17th International Conference on World Wide Web (WWW-2008), pages 635–644.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction using wikipedia.</title>
<date>2010</date>
<booktitle>In The Annual Meeting of the Association for Computational Linguistics (ACL-2010).</booktitle>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S. Weld. 2010. Open information extraction using wikipedia. In The Annual Meeting of the Association for Computational Linguistics (ACL-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Raphael Hoffmann</author>
<author>Daniel S Weld</author>
</authors>
<title>Information extraction from wikipedia: moving down the long tail.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2008),</booktitle>
<pages>731--739</pages>
<contexts>
<context position="2350" citStr="Wu et al., 2008" startWordPosition="334" endWordPosition="337">ction doesn’t canonicalize relations, so any application using the output must deal with homonymy and synonymy. A third approach, sometimes refered to as weak supervision, is to heuristically match values from a database to text, thus generating a set of training data for self-supervised learning of relationspecific extractors (Craven and Kumlien, 1999). With the Kylin system (Wu and Weld, 2007) applied this idea to Wikipedia by matching values of an article’s infobox1 attributes to corresponding sentences in the article, and suggested that their approach could extract thousands of relations (Wu et al., 2008). Unfortunately, however, they never tested the idea on more than a dozen relations. Indeed, no one has demonstrated a practical way to extract more than about one hundred relations. We note that Wikipedia’s infobox ‘ontology’ is a particularly interesting target for extraction. As a by-product of thousands of contributors, it is broad in coverage and growing quickly. Unfortunately, the schemata are surprisingly noisy and most are sparsely populated; challenging conditions for extraction. This paper presents LUCHS, an autonomous, self-supervised system, which learns 5025 relational extractors </context>
<context position="30699" citStr="Wu et al., 2008" startWordPosition="4875" endWordPosition="4878"> noisy, contains many more relations, and has few instances per relation. Whereas the existing approaches focus on syntactic extraction patterns, LUCHS focuses on lexical information enhanced by dynamic lexicon learning. Extraction from Wikipedia Wikipedia has become an interesting target for extraction. (Suchanek et al., 2008) build a knowledgebase from Wikipedia’s semi-structured data. (Wang et al., 2007) propose a semisupervised positive-only learning technique. Although that extracts from text, its reliance on hyperlinks and other semistructured data limits extraction. (Wu and Weld, 2007; Wu et al., 2008)’s systems generate training data similar to LUCHS, but were only on a few infobox classes. In contrast, LUCHS shows that the idea scales to more than 5000 relations, but that additional techniques, such as dynamic lexicon learning, are necessary to deal with sparsity. Extraction with lexicons While lexicons have been commonly used for IE (Cohen and Sarawagi, 2004; Agichtein and Ganti, 2004; Bellare and McCallum, 2007), many approaches assume that lexicons are clean and are supplied by a user before training. Other approaches (Talukdar et al., 2006; Miller et al., 2004; Riloff, 1993) learn lex</context>
</contexts>
<marker>Wu, Hoffmann, Weld, 2008</marker>
<rawString>Fei Wu, Raphael Hoffmann, and Daniel S. Weld. 2008. Information extraction from wikipedia: moving down the long tail. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2008), pages 731–739.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>