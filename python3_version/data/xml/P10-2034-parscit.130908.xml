<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000300">
<title confidence="0.910288">
Learning Common Grammar from Multilingual Corpus
</title>
<note confidence="0.553007">
Tomoharu Iwata Daichi Mochihashi Hiroshi Sawada
NTT Communication Science Laboratories
</note>
<address confidence="0.816394">
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
</address>
<email confidence="0.994007">
{iwata,daichi,sawada}@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.994685" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998485">
We propose a corpus-based probabilis-
tic framework to extract hidden common
syntax across languages from non-parallel
multilingual corpora in an unsupervised
fashion. For this purpose, we assume a
generative model for multilingual corpora,
where each sentence is generated from a
language dependent probabilistic context-
free grammar (PCFG), and these PCFGs
are generated from a prior grammar that
is common across languages. We also de-
velop a variational method for efficient in-
ference. Experiments on a non-parallel
multilingual corpus of eleven languages
demonstrate the feasibility of the proposed
method.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999790891891892">
Languages share certain common proper-
ties (Pinker, 1994). For example, the word order
in most European languages is subject-verb-object
(SVO), and some words with similar forms are
used with similar meanings in different languages.
The reasons for these common properties can be
attributed to: 1) a common ancestor language,
2) borrowing from nearby languages, and 3) the
innate abilities of humans (Chomsky, 1965).
We assume hidden commonalities in syntax
across languages, and try to extract a common
grammar from non-parallel multilingual corpora.
For this purpose, we propose a generative model
for multilingual grammars that is learned in an
unsupervised fashion. There are some computa-
tional models for capturing commonalities at the
phoneme and word level (Oakes, 2000; Bouchard-
Cˆot´e et al., 2008), but, as far as we know, no at-
tempt has been made to extract commonalities in
syntax level from non-parallel and non-annotated
multilingual corpora.
In our scenario, we use probabilistic context-
free grammars (PCFGs) as our monolingual gram-
mar model. We assume that a PCFG for each
language is generated from a general model that
are common across languages, and each sentence
in multilingual corpora is generated from the lan-
guage dependent PCFG. The inference of the gen-
eral model as well as the multilingual PCFGs can
be performed by using a variational method for
efficiency. Our approach is based on a Bayesian
multitask learning framework (Yu et al., 2005;
Daum´e III, 2009). Hierarchical Bayesian model-
ing provides a natural way of obtaining a joint reg-
ularization for individual models by assuming that
the model parameters are drawn from a common
prior distribution (Yu et al., 2005).
</bodyText>
<sectionHeader confidence="0.999672" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999976826086956">
The unsupervised grammar induction task has
been extensively studied (Carroll and Charniak,
1992; Stolcke and Omohundro, 1994; Klein and
Manning, 2002; Klein and Manning, 2004; Liang
et al., 2007). Recently, models have been pro-
posed that outperform PCFG in the grammar in-
duction task (Klein and Manning, 2002; Klein and
Manning, 2004). We used PCFG as a first step
for capturing commonalities in syntax across lan-
guages because of its simplicity. The proposed
framework can be used for probabilistic grammar
models other than PCFG.
Grammar induction using bilingual parallel cor-
pora has been studied mainly in machine transla-
tion research (Wu, 1997; Melamed, 2003; Eisner,
2003; Chiang, 2005; Blunsom et al., 2009; Sny-
der et al., 2009). These methods require sentence-
aligned parallel data, which can be costly to obtain
and difficult to scale to many languages. On the
other hand, our model does not require sentences
to be aligned. Moreover, since the complexity of
our model increases linearly with the number of
languages, our model is easily applicable to cor-
</bodyText>
<page confidence="0.982167">
184
</page>
<note confidence="0.507616">
Proceedings of the ACL 2010 Conference Short Papers, pages 184–188,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99988725">
pora of more than two languages, as we will show
in the experiments. To our knowledge, the only
grammar induction work on non-parallel corpora
is (Cohen and Smith, 2009), but their method does
not model a common grammar, and requires prior
information such as part-of-speech tags. In con-
trast, our method does not require any such prior
information.
</bodyText>
<sectionHeader confidence="0.979136" genericHeader="method">
3 Proposed Method
</sectionHeader>
<subsectionHeader confidence="0.998689">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.992177307692308">
Let X = {Xl}lEL be a non-parallel and non-
annotated multilingual corpus, where Xl is a set
of sentences in language l, and L is a set of lan-
guages. The task is to learn multilingual PCFGs
G = {Gl}lEL and a common grammar that gen-
erates these PCFGs. Here, Gl = (K, Wl, 4)l)
represents a PCFG of language l, where K is a
set of nonterminals, Wl is a set of terminals, and
4)l is a set of rule probabilities. Note that a set of
nonterminals K is shared among languages, but
a set of terminals Wl and rule probabilities 4)l
are specific to the language. For simplicity, we
consider Chomsky normal form grammars, which
have two types of rules: emissions rewrite a non-
terminal as a terminal A → w, and binary pro-
ductions rewrite a nonterminal as two nontermi-
nals A → BC, where A, B, C ∈ K and w ∈ Wl.
The rule probabilities for each nonterminal
A of PCFG Gl in language l consist of: 1)
0Al = {BlAt}tEJ0,11, where 0lA0 and 0lA1 repre-
sent probabilities of choosing the emission rule
and the binary production rule, respectively, 2)
0lA = {0lABC}B,CEK, where 0lABC repre-
sents the probability of nonterminal production
A → BC, and 3) IPlA = {OlAw}wEWl, where
OlAw represents the probability of terminal emis-
sion A → w. Note that 0lA0 + 0lA1 = 1, BlAt ≥ 0,
EB,C 0lABC = 1, 0lABC ≥ 0, Ew OlAw = 1,
and OlAw ≥ 0. In the proposed model, multino-
mial parameters 0lA and 0lA are generated from
Dirichlet distributions that are common across lan-
guages: 0lA ∼ Dir(αA) and 0lA ∼ Dir(αA),
since we assume that languages share a common
syntax structure. αA and αA represent the param-
eters of a common grammar. We use the Dirichlet
prior because it is the conjugate prior for the multi-
nomial distribution. In summary, the proposed
model assumes the following generative process
for a multilingual corpus,
</bodyText>
<listItem confidence="0.389521">
1. For each nonterminal A E K:
</listItem>
<figureCaption confidence="0.999292">
Figure 1: Graphical model.
</figureCaption>
<figure confidence="0.945296739130435">
(a) For each rule type t E {0, 11:
i. Draw common rule type parameters
aθAt — Gam(aθ, bθ)
(b) For each nonterminal pair (B, C):
i. Draw common production parameters
aφABC — Gam(aφ, bφ)
2. For each language l E L:
(a) For each nonterminal A E K:
i. Draw rule type parameters
0lA — Dir(αθA)
ii. Draw binary production parameters
0lA — Dir(αφA)
iii. Draw emission parameters
VIlA — Dir(aψ)
(b) For each node i in the parse tree:
i. Choose rule type
tli — Mult(0lzi)
ii. If tli = 0:
A. Emit terminal
xli — Mult(VIlzi)
iii. Otherwise:
A. Generate children nonterminals
(zlL(i), zlR(i)) — Mult(0lzi),
</figure>
<bodyText confidence="0.999939">
where L(i) and R(i) represent the left and right
children of node i. Figure 1 shows a graphi-
cal model representation of the proposed model,
where the shaded and unshaded nodes indicate ob-
served and latent variables, respectively.
</bodyText>
<subsectionHeader confidence="0.987497">
3.2 Inference
</subsectionHeader>
<bodyText confidence="0.988580416666667">
The inference of the proposed model can be ef-
ficiently computed using a variational Bayesian
method. We extend the variational method to
the monolingual PCFG learning of Kurihara and
Sato (2004) for multilingual corpora. The goal
is to estimate posterior p(Z, 4), α|X), where Z
is a set of parse trees, 4) = {4)l}lEL is a
set of language dependent parameters, 4)l =
{0lA, 0lA, IPlA}AEK, and α = {αA, αA}AEK
is a set of common parameters. In the variational
method, posterior p(Z, 4), α|X) is approximated
by a tractable variational distribution q(Z, 4), α).
</bodyText>
<figure confidence="0.997057705882353">
a,b
θ θ
a,b
φ φ
αψ
αAθ
αφA
ψ lA
θ lA
φ
lA
|L|
|K|
z 2 z 3
x2 x3
z 1
|L|
</figure>
<page confidence="0.987543">
185
</page>
<bodyText confidence="0.993695">
We use the following variational distribution,
</bodyText>
<equation confidence="0.95611275">
∏
q(αθ A)q(αφ A)
l,d
q(θlA)q(φlA)q(ψlA), (1)
</equation>
<bodyText confidence="0.999725125">
where we assume that hyperparameters q(αθ A) and
q(αφA) are degenerated, or q(α) = δα∗(α), and
infer them by point estimation instead of distribu-
tion estimation. We find an approximate posterior
distribution that minimizes the Kullback-Leibler
divergence from the true posterior. The variational
distribution of the parse tree of the dth sentence in
language l is obtained as follows,
</bodyText>
<equation confidence="0.9960372">
( )C(A→BC;zld,l,d)
πθ lA1πφ lABC
( )C(A→w;zld,l,d)
πθ lA0πψ , (2)
lAw
</equation>
<bodyText confidence="0.999682">
where C(r; z,l, d) is the count of rule r that oc-
curs in the dth sentence of language l with parse
tree z. The multinomial weights are calculated as
follows,
</bodyText>
<equation confidence="0.90853755">
πθlAt = exp(Eq(θlA) [log θlAt]), (3)
πlABC = exp(Eq(φlA) ]), (4)
φ [log φlABC
πlAw = exp(Eq(ψlA) ]). (5)
ψ [log ψlAw
The variational Dirichlet parameters for q(θlA) =
Dir(γθlA), q(φlA) = Dir(γφlA), and q(ψlA) =
Dir(γψlA), are obtained as follows,
∑γθlAt = αθAt + q(zld)C(A, t; zld,l, d), (6)
d,zld
γlABC = α
φ ABC+
φ ∑ q(zld)C(A→BC; zld, l, d),
d,zld
(7)
∑ q(zld)C(A → w; zld, l, d),
γlAw = αψ +
ψ
d,zld
(8)
</equation>
<bodyText confidence="0.9356833">
where C(A, t; z,l, d) is the count of rule type t
that is selected in nonterminal A in the dth sen-
tence of language l with parse tree z.
The common rule type parameter αθ that min-
At
imizes the KL divergence between the true pos-
terior and the approximate posterior can be ob-
tained by using the fixed-point iteration method
described in (Minka, 2000). The update rule is as
follows,
</bodyText>
<equation confidence="0.978425166666667">
aθ−1+αθ AtL(Ψ(∑t0 αθAt0)−Ψ(αθAt))
bθ + ∑ (Ψ(∑t0 γθ lAt0) − Ψ(γθ lAt)) ,
l
(9)
where L is the number of languages, and Ψ(x) =
∂ log �(x)
</equation>
<bodyText confidence="0.999447333333333">
∂x is the digamma function. Similarly, the
common production parameter αφABC can be up-
dated as follows,
</bodyText>
<equation confidence="0.9982145">
aφ − 1 + αφ ABCLJABC
αABC ←
φ(new) bφ + ∑ , (10)
l J0 lABC
</equation>
<bodyText confidence="0.9991922">
where JABC = Ψ(∑B0,C0 αφAB0C0) − Ψ(αφABC),
and J0lABC = Ψ(∑B0,C0 γφlAB0C0) − Ψ(γφlABC).
Since factored variational distributions depend
on each other, an optimal approximated posterior
can be obtained by updating parameters by (2) -
(10) alternatively until convergence. The updat-
ing of language dependent distributions by (2) -
(8) is also described in (Kurihara and Sato, 2004;
Liang et al., 2007) while the updating of common
grammar parameters by (9) and (10) is new. The
inference can be carried out efficiently using the
inside-outside algorithm based on dynamic pro-
gramming (Lari and Young, 1990).
After the inference, the probability of a com-
mon grammar rule A → BC is calculated by
</bodyText>
<equation confidence="0.977448666666667">
φA→BC = θ1 φABC, where ˆθ1 = αθ1/(αθ0 + αθ1)
ˆφABC = αφ φ
ABC/ ∑B0,C0 αAB0C0 represent
</equation>
<bodyText confidence="0.985278">
the mean values of θl0 and φlABC, respectively.
</bodyText>
<sectionHeader confidence="0.997227" genericHeader="evaluation">
4 Experimental results
</sectionHeader>
<bodyText confidence="0.999435588235294">
We evaluated our method by employing the Eu-
roParl corpus (Koehn, 2005). The corpus con-
sists of the proceedings of the European Parlia-
ment in eleven western European languages: Dan-
ish (da), German (de), Greek (el), English (en),
Spanish (es), Finnish (fi), French (fr), Italian (it),
Dutch (nl), Portuguese (pt), and Swedish (sv), and
it contains roughly 1,500,000 sentences in each
language. We set the number of nonterminals at
|K |= 20, and omitted sentences with more than
ten words for tractability. We randomly sampled
100,000 sentences for each language, and ana-
lyzed them using our method. It should be noted
that our random samples are not sentence-aligned.
Figure 2 shows the most probable terminals of
emission for each language and nonterminal with
a high probability of selecting the emission rule.
</bodyText>
<figure confidence="0.93403425">
q(Z, �, α) = ∏
A
∏×
l,A
∏q(zld) ∝
A→BC
∏×
A→w
q(zld)
θ(new)
α At ←
and
</figure>
<page confidence="0.906108">
186
</page>
<note confidence="0.7091">
2: verb and auxiliary verb (V)
</note>
<figureCaption confidence="0.989545">
Figure 2: Probable terminals of emission for each
language and nonterminal.
</figureCaption>
<figure confidence="0.999757">
0 → 16 11 (R → S . ) 0.11
16 → 7 6 (S → SBJ VP) 0.06
6 → 2 12 (VP → V NP) 0.04
12 → 13 5 (NP → DT N) 0.19
15 → 17 19 (NP → NP N) 0.07
17 → 5 9 (NP → N PR) 0.07
15 → 13 5 (NP → DT N) 0.06
</figure>
<figureCaption confidence="0.904514">
Figure 3: Examples of inferred common gram-
mar rules in eleven languages, and their proba-
bilities. Hand-provided annotations have the fol-
lowing meanings, R: root, S: sentence, NP: noun
phrase, VP: verb phrase, and others appear in Fig-
ure 2.
</figureCaption>
<bodyText confidence="0.999967125">
We named nonterminals by using grammatical cat-
egories after the inference. We can see that words
in the same grammatical category clustered across
languages as well as within a language. Fig-
ure 3 shows examples of inferred common gram-
mar rules with high probabilities. Grammar rules
that seem to be common to European languages
have been extracted.
</bodyText>
<sectionHeader confidence="0.997706" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9999476">
We have proposed a Bayesian hierarchical PCFG
model for capturing commonalities at the syntax
level for non-parallel multilingual corpora. Al-
though our results have been encouraging, a num-
ber of directions remain in which we must extend
our approach. First, we need to evaluate our model
quantitatively using corpora with a greater diver-
sity of languages. Measurement examples include
the perplexity, and machine translation score. Sec-
ond, we need to improve our model. For ex-
ample, we can infer the number of nonterminals
with a nonparametric Bayesian model (Liang et
al., 2007), infer the model more robustly based
on a Markov chain Monte Carlo inference (John-
son et al., 2007), and use probabilistic grammar
models other than PCFGs. In our model, all the
multilingual grammars are generated from a gen-
eral model. We can extend it hierarchically using
the coalescent (Kingman, 1982). That model may
help to infer an evolutionary tree of languages in
terms of grammatical structure without the etymo-
logical information that is generally used (Gray
and Atkinson, 2003). Finally, the proposed ap-
proach may help to indicate the presence of a uni-
versal grammar (Chomsky, 1965), or to find it.
</bodyText>
<figure confidence="0.9989982">
5: noun (N)
7: subject (SBJ)
9: preposition (PR)
11: punctuation (.)
13: determiner (DT)
</figure>
<page confidence="0.97436">
187
</page>
<sectionHeader confidence="0.980266" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999851097345133">
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009.
Bayesian synchronous grammar induction. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-
vances in Neural Information Processing Systems 21,
pages 161–168.
Alexandre Bouchard-Cˆot´e, Percy Liang, Thomas Griffiths,
and Dan Klein. 2008. A probabilistic approach to lan-
guage change. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information Pro-
cessing Systems 20, pages 169–176, Cambridge, MA.
MIT Press.
Glenn Carroll and Eugene Charniak. 1992. Two experiments
on learning probabilistic dependency grammars from cor-
pora. In Working Notes of the Workshop Statistically-
Based NLP Techniques, pages 1–13. AAAI.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In ACL ’05: Proceedings
of the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263–270, Morristown, NJ, USA.
Association for Computational Linguistics.
Norm Chomsky. 1965. Aspects of the Theory of Syntax. MIT
Press.
Shay B. Cohen and Noah A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsuper-
vised grammar induction. In NAACL ’09: Proceedings of
Human Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 74–82, Morristown,
NJ, USA. Association for Computational Linguistics.
Hal Daum´e III. 2009. Bayesian multitask learning with la-
tent hierarchies. In Proceedings of the Twenty-Fifth An-
nual Conference on Uncertainty in Artificial Intelligence
(UAI-09), pages 135–142, Corvallis, Oregon. AUAI Press.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In ACL ’03: Proceedings of the
41st Annual Meeting on Association for Computational
Linguistics, pages 205–208, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Russell D. Gray and Quentin D. Atkinson. 2003. Language-
tree divergence times support the Anatolian theory of
Indo-European origin. Nature, 426(6965):435–439,
November.
Mark Johnson, Thomas Griffiths, and Sharon Goldwater.
2007. Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Human Language Technologies 2007:
The Conference of the North American Chapter of the
Association for Computational Linguistics; Proceedings
of the Main Conference, pages 139–146, Rochester, New
York, April. Association for Computational Linguistics.
J. F. C. Kingman. 1982. The coalescent. Stochastic Pro-
cesses and their Applications, 13:235–248.
Dan Klein and Christopher D. Manning. 2002. A generative
constituent-context model for improved grammar induc-
tion. In ACL ’02: Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics, pages
128–135, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of depen-
dency and constituency. In ACL ’04: Proceedings of the
42nd Annual Meeting on Association for Computational
Linguistics, page 478, Morristown, NJ, USA. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of the 10th
Machine Translation Summit, pages 79–86.
Kenichi Kurihara and Taisuke Sato. 2004. An applica-
tion of the variational Bayesian approach to probabilistic
context-free grammars. In International Joint Conference
on Natural Language Processing Workshop Beyond Shal-
low Analysis.
K. Lari and S.J. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algorithm.
Computer Speech and Language, 4:35–56.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical dirichlet pro-
cesses. In EMNLP ’07: Proceedings of the Empirical
Methods on Natural Language Processing, pages 688–
697.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In NAACL ’03: Proceedings of the 2003
Conference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Language
Technology, pages 79–86, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Thomas Minka. 2000. Estimating a Dirichlet distribution.
Technical report, M.I.T.
Michael P. Oakes. 2000. Computer estimation of vocabu-
lary in a protolanguage from word lists in four daughter
languages. Journal of Quantitative Linguistics, 7(3):233–
243.
Steven Pinker. 1994. The Language Instinct: How the Mind
Creates Language. HarperCollins, New York.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction. In
Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint Con-
ference on Natural Language Processing of the AFNLP,
pages 73–81, Suntec, Singapore, August. Association for
Computational Linguistics.
Andreas Stolcke and Stephen M. Omohundro. 1994. In-
ducing probabilistic grammars by Bayesian model merg-
ing. In ICGI ’94: Proceedings of the Second International
Colloquium on Grammatical Inference and Applications,
pages 106–118, London, UK. Springer-Verlag.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Comput.
Linguist., 23(3):377–403.
Kai Yu, Volker Tresp, and Anton Schwaighofer. 2005.
Learning gaussian processes from multiple tasks. In
ICML ’05: Proceedings of the 22nd International Confer-
ence on Machine Learning, pages 1012–1019, New York,
NY, USA. ACM.
</reference>
<page confidence="0.997527">
188
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.935098">
<title confidence="0.999896">Learning Common Grammar from Multilingual Corpus</title>
<author confidence="0.993605">Tomoharu Iwata Daichi Mochihashi Hiroshi Sawada</author>
<affiliation confidence="0.999791">NTT Communication Science Laboratories</affiliation>
<address confidence="0.995074">2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan</address>
<abstract confidence="0.99681605882353">We propose a corpus-based probabilistic framework to extract hidden common syntax across languages from non-parallel multilingual corpora in an unsupervised fashion. For this purpose, we assume a generative model for multilingual corpora, where each sentence is generated from a language dependent probabilistic contextfree grammar (PCFG), and these PCFGs are generated from a prior grammar that is common across languages. We also develop a variational method for efficient inference. Experiments on a non-parallel multilingual corpus of eleven languages demonstrate the feasibility of the proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>Bayesian synchronous grammar induction.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems 21,</booktitle>
<pages>161--168</pages>
<editor>In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors,</editor>
<contexts>
<context position="3290" citStr="Blunsom et al., 2009" startWordPosition="494" endWordPosition="497">92; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Klein and Manning, 2004; Liang et al., 2007). Recently, models have been proposed that outperform PCFG in the grammar induction task (Klein and Manning, 2002; Klein and Manning, 2004). We used PCFG as a first step for capturing commonalities in syntax across languages because of its simplicity. The proposed framework can be used for probabilistic grammar models other than PCFG. Grammar induction using bilingual parallel corpora has been studied mainly in machine translation research (Wu, 1997; Melamed, 2003; Eisner, 2003; Chiang, 2005; Blunsom et al., 2009; Snyder et al., 2009). These methods require sentencealigned parallel data, which can be costly to obtain and difficult to scale to many languages. On the other hand, our model does not require sentences to be aligned. Moreover, since the complexity of our model increases linearly with the number of languages, our model is easily applicable to cor184 Proceedings of the ACL 2010 Conference Short Papers, pages 184–188, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics pora of more than two languages, as we will show in the experiments. To our knowledge, the only</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009. Bayesian synchronous grammar induction. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Percy Liang</author>
<author>Thomas Griffiths</author>
<author>Dan Klein</author>
</authors>
<title>A probabilistic approach to language change.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 20,</booktitle>
<pages>169--176</pages>
<editor>In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<marker>Bouchard-Cˆot´e, Liang, Griffiths, Klein, 2008</marker>
<rawString>Alexandre Bouchard-Cˆot´e, Percy Liang, Thomas Griffiths, and Dan Klein. 2008. A probabilistic approach to language change. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 169–176, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Eugene Charniak</author>
</authors>
<title>Two experiments on learning probabilistic dependency grammars from corpora.</title>
<date>1992</date>
<booktitle>In Working Notes of the Workshop StatisticallyBased NLP Techniques,</booktitle>
<pages>1--13</pages>
<publisher>AAAI.</publisher>
<contexts>
<context position="2672" citStr="Carroll and Charniak, 1992" startWordPosition="395" endWordPosition="398"> multilingual corpora is generated from the language dependent PCFG. The inference of the general model as well as the multilingual PCFGs can be performed by using a variational method for efficiency. Our approach is based on a Bayesian multitask learning framework (Yu et al., 2005; Daum´e III, 2009). Hierarchical Bayesian modeling provides a natural way of obtaining a joint regularization for individual models by assuming that the model parameters are drawn from a common prior distribution (Yu et al., 2005). 2 Related work The unsupervised grammar induction task has been extensively studied (Carroll and Charniak, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Klein and Manning, 2004; Liang et al., 2007). Recently, models have been proposed that outperform PCFG in the grammar induction task (Klein and Manning, 2002; Klein and Manning, 2004). We used PCFG as a first step for capturing commonalities in syntax across languages because of its simplicity. The proposed framework can be used for probabilistic grammar models other than PCFG. Grammar induction using bilingual parallel corpora has been studied mainly in machine translation research (Wu, 1997; Melamed, 2003; Eisner, 2003; Chiang, 2005; Bl</context>
</contexts>
<marker>Carroll, Charniak, 1992</marker>
<rawString>Glenn Carroll and Eugene Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. In Working Notes of the Workshop StatisticallyBased NLP Techniques, pages 1–13. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3268" citStr="Chiang, 2005" startWordPosition="492" endWordPosition="493">d Charniak, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Klein and Manning, 2004; Liang et al., 2007). Recently, models have been proposed that outperform PCFG in the grammar induction task (Klein and Manning, 2002; Klein and Manning, 2004). We used PCFG as a first step for capturing commonalities in syntax across languages because of its simplicity. The proposed framework can be used for probabilistic grammar models other than PCFG. Grammar induction using bilingual parallel corpora has been studied mainly in machine translation research (Wu, 1997; Melamed, 2003; Eisner, 2003; Chiang, 2005; Blunsom et al., 2009; Snyder et al., 2009). These methods require sentencealigned parallel data, which can be costly to obtain and difficult to scale to many languages. On the other hand, our model does not require sentences to be aligned. Moreover, since the complexity of our model increases linearly with the number of languages, our model is easily applicable to cor184 Proceedings of the ACL 2010 Conference Short Papers, pages 184–188, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics pora of more than two languages, as we will show in the experiments. To o</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norm Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1276" citStr="Chomsky, 1965" startWordPosition="175" endWordPosition="176"> We also develop a variational method for efficient inference. Experiments on a non-parallel multilingual corpus of eleven languages demonstrate the feasibility of the proposed method. 1 Introduction Languages share certain common properties (Pinker, 1994). For example, the word order in most European languages is subject-verb-object (SVO), and some words with similar forms are used with similar meanings in different languages. The reasons for these common properties can be attributed to: 1) a common ancestor language, 2) borrowing from nearby languages, and 3) the innate abilities of humans (Chomsky, 1965). We assume hidden commonalities in syntax across languages, and try to extract a common grammar from non-parallel multilingual corpora. For this purpose, we propose a generative model for multilingual grammars that is learned in an unsupervised fashion. There are some computational models for capturing commonalities at the phoneme and word level (Oakes, 2000; BouchardCˆot´e et al., 2008), but, as far as we know, no attempt has been made to extract commonalities in syntax level from non-parallel and non-annotated multilingual corpora. In our scenario, we use probabilistic contextfree grammars </context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Norm Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>74--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3964" citStr="Cohen and Smith, 2009" startWordPosition="603" endWordPosition="606">cealigned parallel data, which can be costly to obtain and difficult to scale to many languages. On the other hand, our model does not require sentences to be aligned. Moreover, since the complexity of our model increases linearly with the number of languages, our model is easily applicable to cor184 Proceedings of the ACL 2010 Conference Short Papers, pages 184–188, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics pora of more than two languages, as we will show in the experiments. To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags. In contrast, our method does not require any such prior information. 3 Proposed Method 3.1 Model Let X = {Xl}lEL be a non-parallel and nonannotated multilingual corpus, where Xl is a set of sentences in language l, and L is a set of languages. The task is to learn multilingual PCFGs G = {Gl}lEL and a common grammar that generates these PCFGs. Here, Gl = (K, Wl, 4)l) represents a PCFG of language l, where K is a set of nonterminals, Wl is a set of terminals, and 4)l is a set of rule p</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 74–82, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Bayesian multitask learning with latent hierarchies.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-Fifth Annual Conference on Uncertainty in Artificial Intelligence (UAI-09),</booktitle>
<pages>135--142</pages>
<publisher>AUAI Press.</publisher>
<location>Corvallis, Oregon.</location>
<marker>Daum´e, 2009</marker>
<rawString>Hal Daum´e III. 2009. Bayesian multitask learning with latent hierarchies. In Proceedings of the Twenty-Fifth Annual Conference on Uncertainty in Artificial Intelligence (UAI-09), pages 135–142, Corvallis, Oregon. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>205--208</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3254" citStr="Eisner, 2003" startWordPosition="490" endWordPosition="491">ed (Carroll and Charniak, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Klein and Manning, 2004; Liang et al., 2007). Recently, models have been proposed that outperform PCFG in the grammar induction task (Klein and Manning, 2002; Klein and Manning, 2004). We used PCFG as a first step for capturing commonalities in syntax across languages because of its simplicity. The proposed framework can be used for probabilistic grammar models other than PCFG. Grammar induction using bilingual parallel corpora has been studied mainly in machine translation research (Wu, 1997; Melamed, 2003; Eisner, 2003; Chiang, 2005; Blunsom et al., 2009; Snyder et al., 2009). These methods require sentencealigned parallel data, which can be costly to obtain and difficult to scale to many languages. On the other hand, our model does not require sentences to be aligned. Moreover, since the complexity of our model increases linearly with the number of languages, our model is easily applicable to cor184 Proceedings of the ACL 2010 Conference Short Papers, pages 184–188, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics pora of more than two languages, as we will show in the exp</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 205–208, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Russell D Gray</author>
<author>Quentin D Atkinson</author>
</authors>
<title>Languagetree divergence times support the Anatolian theory of Indo-European origin.</title>
<date>2003</date>
<journal>Nature,</journal>
<volume>426</volume>
<issue>6965</issue>
<marker>Gray, Atkinson, 2003</marker>
<rawString>Russell D. Gray and Quentin D. Atkinson. 2003. Languagetree divergence times support the Anatolian theory of Indo-European origin. Nature, 426(6965):435–439, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>139--146</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="12589" citStr="Johnson et al., 2007" startWordPosition="2158" endWordPosition="2162">pturing commonalities at the syntax level for non-parallel multilingual corpora. Although our results have been encouraging, a number of directions remain in which we must extend our approach. First, we need to evaluate our model quantitatively using corpora with a greater diversity of languages. Measurement examples include the perplexity, and machine translation score. Second, we need to improve our model. For example, we can infer the number of nonterminals with a nonparametric Bayesian model (Liang et al., 2007), infer the model more robustly based on a Markov chain Monte Carlo inference (Johnson et al., 2007), and use probabilistic grammar models other than PCFGs. In our model, all the multilingual grammars are generated from a general model. We can extend it hierarchically using the coalescent (Kingman, 1982). That model may help to infer an evolutionary tree of languages in terms of grammatical structure without the etymological information that is generally used (Gray and Atkinson, 2003). Finally, the proposed approach may help to indicate the presence of a universal grammar (Chomsky, 1965), or to find it. 5: noun (N) 7: subject (SBJ) 9: preposition (PR) 11: punctuation (.) 13: determiner (DT) </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139–146, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F C Kingman</author>
</authors>
<date>1982</date>
<booktitle>The coalescent. Stochastic Processes and their Applications,</booktitle>
<pages>13--235</pages>
<marker>Kingman, 1982</marker>
<rawString>J. F. C. Kingman. 1982. The coalescent. Stochastic Processes and their Applications, 13:235–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A generative constituent-context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2726" citStr="Klein and Manning, 2002" startWordPosition="403" endWordPosition="406">ndent PCFG. The inference of the general model as well as the multilingual PCFGs can be performed by using a variational method for efficiency. Our approach is based on a Bayesian multitask learning framework (Yu et al., 2005; Daum´e III, 2009). Hierarchical Bayesian modeling provides a natural way of obtaining a joint regularization for individual models by assuming that the model parameters are drawn from a common prior distribution (Yu et al., 2005). 2 Related work The unsupervised grammar induction task has been extensively studied (Carroll and Charniak, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Klein and Manning, 2004; Liang et al., 2007). Recently, models have been proposed that outperform PCFG in the grammar induction task (Klein and Manning, 2002; Klein and Manning, 2004). We used PCFG as a first step for capturing commonalities in syntax across languages because of its simplicity. The proposed framework can be used for probabilistic grammar models other than PCFG. Grammar induction using bilingual parallel corpora has been studied mainly in machine translation research (Wu, 1997; Melamed, 2003; Eisner, 2003; Chiang, 2005; Blunsom et al., 2009; Snyder et al., 2009). These method</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. A generative constituent-context model for improved grammar induction. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 128–135, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>478</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2751" citStr="Klein and Manning, 2004" startWordPosition="407" endWordPosition="410"> of the general model as well as the multilingual PCFGs can be performed by using a variational method for efficiency. Our approach is based on a Bayesian multitask learning framework (Yu et al., 2005; Daum´e III, 2009). Hierarchical Bayesian modeling provides a natural way of obtaining a joint regularization for individual models by assuming that the model parameters are drawn from a common prior distribution (Yu et al., 2005). 2 Related work The unsupervised grammar induction task has been extensively studied (Carroll and Charniak, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Klein and Manning, 2004; Liang et al., 2007). Recently, models have been proposed that outperform PCFG in the grammar induction task (Klein and Manning, 2002; Klein and Manning, 2004). We used PCFG as a first step for capturing commonalities in syntax across languages because of its simplicity. The proposed framework can be used for probabilistic grammar models other than PCFG. Grammar induction using bilingual parallel corpora has been studied mainly in machine translation research (Wu, 1997; Melamed, 2003; Eisner, 2003; Chiang, 2005; Blunsom et al., 2009; Snyder et al., 2009). These methods require sentencealigned</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: models of dependency and constituency. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 478, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="10198" citStr="Koehn, 2005" startWordPosition="1732" endWordPosition="1733">istributions by (2) - (8) is also described in (Kurihara and Sato, 2004; Liang et al., 2007) while the updating of common grammar parameters by (9) and (10) is new. The inference can be carried out efficiently using the inside-outside algorithm based on dynamic programming (Lari and Young, 1990). After the inference, the probability of a common grammar rule A → BC is calculated by φA→BC = θ1 φABC, where ˆθ1 = αθ1/(αθ0 + αθ1) ˆφABC = αφ φ ABC/ ∑B0,C0 αAB0C0 represent the mean values of θl0 and φlABC, respectively. 4 Experimental results We evaluated our method by employing the EuroParl corpus (Koehn, 2005). The corpus consists of the proceedings of the European Parliament in eleven western European languages: Danish (da), German (de), Greek (el), English (en), Spanish (es), Finnish (fi), French (fr), Italian (it), Dutch (nl), Portuguese (pt), and Swedish (sv), and it contains roughly 1,500,000 sentences in each language. We set the number of nonterminals at |K |= 20, and omitted sentences with more than ten words for tractability. We randomly sampled 100,000 sentences for each language, and analyzed them using our method. It should be noted that our random samples are not sentence-aligned. Figu</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the 10th Machine Translation Summit, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenichi Kurihara</author>
<author>Taisuke Sato</author>
</authors>
<title>An application of the variational Bayesian approach to probabilistic context-free grammars.</title>
<date>2004</date>
<booktitle>In International Joint Conference on Natural Language Processing Workshop Beyond Shallow Analysis.</booktitle>
<contexts>
<context position="7046" citStr="Kurihara and Sato (2004)" startWordPosition="1159" endWordPosition="1162"> the parse tree: i. Choose rule type tli — Mult(0lzi) ii. If tli = 0: A. Emit terminal xli — Mult(VIlzi) iii. Otherwise: A. Generate children nonterminals (zlL(i), zlR(i)) — Mult(0lzi), where L(i) and R(i) represent the left and right children of node i. Figure 1 shows a graphical model representation of the proposed model, where the shaded and unshaded nodes indicate observed and latent variables, respectively. 3.2 Inference The inference of the proposed model can be efficiently computed using a variational Bayesian method. We extend the variational method to the monolingual PCFG learning of Kurihara and Sato (2004) for multilingual corpora. The goal is to estimate posterior p(Z, 4), α|X), where Z is a set of parse trees, 4) = {4)l}lEL is a set of language dependent parameters, 4)l = {0lA, 0lA, IPlA}AEK, and α = {αA, αA}AEK is a set of common parameters. In the variational method, posterior p(Z, 4), α|X) is approximated by a tractable variational distribution q(Z, 4), α). a,b θ θ a,b φ φ αψ αAθ αφA ψ lA θ lA φ lA |L| |K| z 2 z 3 x2 x3 z 1 |L| 185 We use the following variational distribution, ∏ q(αθ A)q(αφ A) l,d q(θlA)q(φlA)q(ψlA), (1) where we assume that hyperparameters q(αθ A) and q(αφA) are degenera</context>
<context position="9657" citStr="Kurihara and Sato, 2004" startWordPosition="1635" endWordPosition="1638">0) − Ψ(γθ lAt)) , l (9) where L is the number of languages, and Ψ(x) = ∂ log �(x) ∂x is the digamma function. Similarly, the common production parameter αφABC can be updated as follows, aφ − 1 + αφ ABCLJABC αABC ← φ(new) bφ + ∑ , (10) l J0 lABC where JABC = Ψ(∑B0,C0 αφAB0C0) − Ψ(αφABC), and J0lABC = Ψ(∑B0,C0 γφlAB0C0) − Ψ(γφlABC). Since factored variational distributions depend on each other, an optimal approximated posterior can be obtained by updating parameters by (2) - (10) alternatively until convergence. The updating of language dependent distributions by (2) - (8) is also described in (Kurihara and Sato, 2004; Liang et al., 2007) while the updating of common grammar parameters by (9) and (10) is new. The inference can be carried out efficiently using the inside-outside algorithm based on dynamic programming (Lari and Young, 1990). After the inference, the probability of a common grammar rule A → BC is calculated by φA→BC = θ1 φABC, where ˆθ1 = αθ1/(αθ0 + αθ1) ˆφABC = αφ φ ABC/ ∑B0,C0 αAB0C0 represent the mean values of θl0 and φlABC, respectively. 4 Experimental results We evaluated our method by employing the EuroParl corpus (Koehn, 2005). The corpus consists of the proceedings of the European Pa</context>
</contexts>
<marker>Kurihara, Sato, 2004</marker>
<rawString>Kenichi Kurihara and Taisuke Sato. 2004. An application of the variational Bayesian approach to probabilistic context-free grammars. In International Joint Conference on Natural Language Processing Workshop Beyond Shallow Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="9882" citStr="Lari and Young, 1990" startWordPosition="1672" endWordPosition="1675"> , (10) l J0 lABC where JABC = Ψ(∑B0,C0 αφAB0C0) − Ψ(αφABC), and J0lABC = Ψ(∑B0,C0 γφlAB0C0) − Ψ(γφlABC). Since factored variational distributions depend on each other, an optimal approximated posterior can be obtained by updating parameters by (2) - (10) alternatively until convergence. The updating of language dependent distributions by (2) - (8) is also described in (Kurihara and Sato, 2004; Liang et al., 2007) while the updating of common grammar parameters by (9) and (10) is new. The inference can be carried out efficiently using the inside-outside algorithm based on dynamic programming (Lari and Young, 1990). After the inference, the probability of a common grammar rule A → BC is calculated by φA→BC = θ1 φABC, where ˆθ1 = αθ1/(αθ0 + αθ1) ˆφABC = αφ φ ABC/ ∑B0,C0 αAB0C0 represent the mean values of θl0 and φlABC, respectively. 4 Experimental results We evaluated our method by employing the EuroParl corpus (Koehn, 2005). The corpus consists of the proceedings of the European Parliament in eleven western European languages: Danish (da), German (de), Greek (el), English (en), Spanish (es), Finnish (fi), French (fr), Italian (it), Dutch (nl), Portuguese (pt), and Swedish (sv), and it contains roughly </context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S.J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite PCFG using hierarchical dirichlet processes.</title>
<date>2007</date>
<booktitle>In EMNLP ’07: Proceedings of the Empirical Methods on Natural Language Processing,</booktitle>
<pages>688--697</pages>
<contexts>
<context position="2772" citStr="Liang et al., 2007" startWordPosition="411" endWordPosition="414">well as the multilingual PCFGs can be performed by using a variational method for efficiency. Our approach is based on a Bayesian multitask learning framework (Yu et al., 2005; Daum´e III, 2009). Hierarchical Bayesian modeling provides a natural way of obtaining a joint regularization for individual models by assuming that the model parameters are drawn from a common prior distribution (Yu et al., 2005). 2 Related work The unsupervised grammar induction task has been extensively studied (Carroll and Charniak, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Klein and Manning, 2004; Liang et al., 2007). Recently, models have been proposed that outperform PCFG in the grammar induction task (Klein and Manning, 2002; Klein and Manning, 2004). We used PCFG as a first step for capturing commonalities in syntax across languages because of its simplicity. The proposed framework can be used for probabilistic grammar models other than PCFG. Grammar induction using bilingual parallel corpora has been studied mainly in machine translation research (Wu, 1997; Melamed, 2003; Eisner, 2003; Chiang, 2005; Blunsom et al., 2009; Snyder et al., 2009). These methods require sentencealigned parallel data, which</context>
<context position="9678" citStr="Liang et al., 2007" startWordPosition="1639" endWordPosition="1642">here L is the number of languages, and Ψ(x) = ∂ log �(x) ∂x is the digamma function. Similarly, the common production parameter αφABC can be updated as follows, aφ − 1 + αφ ABCLJABC αABC ← φ(new) bφ + ∑ , (10) l J0 lABC where JABC = Ψ(∑B0,C0 αφAB0C0) − Ψ(αφABC), and J0lABC = Ψ(∑B0,C0 γφlAB0C0) − Ψ(γφlABC). Since factored variational distributions depend on each other, an optimal approximated posterior can be obtained by updating parameters by (2) - (10) alternatively until convergence. The updating of language dependent distributions by (2) - (8) is also described in (Kurihara and Sato, 2004; Liang et al., 2007) while the updating of common grammar parameters by (9) and (10) is new. The inference can be carried out efficiently using the inside-outside algorithm based on dynamic programming (Lari and Young, 1990). After the inference, the probability of a common grammar rule A → BC is calculated by φA→BC = θ1 φABC, where ˆθ1 = αθ1/(αθ0 + αθ1) ˆφABC = αφ φ ABC/ ∑B0,C0 αAB0C0 represent the mean values of θl0 and φlABC, respectively. 4 Experimental results We evaluated our method by employing the EuroParl corpus (Koehn, 2005). The corpus consists of the proceedings of the European Parliament in eleven we</context>
<context position="12489" citStr="Liang et al., 2007" startWordPosition="2141" endWordPosition="2144">uages have been extracted. 5 Discussion We have proposed a Bayesian hierarchical PCFG model for capturing commonalities at the syntax level for non-parallel multilingual corpora. Although our results have been encouraging, a number of directions remain in which we must extend our approach. First, we need to evaluate our model quantitatively using corpora with a greater diversity of languages. Measurement examples include the perplexity, and machine translation score. Second, we need to improve our model. For example, we can infer the number of nonterminals with a nonparametric Bayesian model (Liang et al., 2007), infer the model more robustly based on a Markov chain Monte Carlo inference (Johnson et al., 2007), and use probabilistic grammar models other than PCFGs. In our model, all the multilingual grammars are generated from a general model. We can extend it hierarchically using the coalescent (Kingman, 1982). That model may help to infer an evolutionary tree of languages in terms of grammatical structure without the etymological information that is generally used (Gray and Atkinson, 2003). Finally, the proposed approach may help to indicate the presence of a universal grammar (Chomsky, 1965), or t</context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical dirichlet processes. In EMNLP ’07: Proceedings of the Empirical Methods on Natural Language Processing, pages 688– 697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Multitext grammars and synchronous parsers.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3240" citStr="Melamed, 2003" startWordPosition="488" endWordPosition="489">tensively studied (Carroll and Charniak, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Klein and Manning, 2004; Liang et al., 2007). Recently, models have been proposed that outperform PCFG in the grammar induction task (Klein and Manning, 2002; Klein and Manning, 2004). We used PCFG as a first step for capturing commonalities in syntax across languages because of its simplicity. The proposed framework can be used for probabilistic grammar models other than PCFG. Grammar induction using bilingual parallel corpora has been studied mainly in machine translation research (Wu, 1997; Melamed, 2003; Eisner, 2003; Chiang, 2005; Blunsom et al., 2009; Snyder et al., 2009). These methods require sentencealigned parallel data, which can be costly to obtain and difficult to scale to many languages. On the other hand, our model does not require sentences to be aligned. Moreover, since the complexity of our model increases linearly with the number of languages, our model is easily applicable to cor184 Proceedings of the ACL 2010 Conference Short Papers, pages 184–188, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics pora of more than two languages, as we will s</context>
</contexts>
<marker>Melamed, 2003</marker>
<rawString>I. Dan Melamed. 2003. Multitext grammars and synchronous parsers. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 79–86, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Minka</author>
</authors>
<title>Estimating a Dirichlet distribution.</title>
<date>2000</date>
<tech>Technical report, M.I.T.</tech>
<contexts>
<context position="8947" citStr="Minka, 2000" startWordPosition="1511" endWordPosition="1512">arameters for q(θlA) = Dir(γθlA), q(φlA) = Dir(γφlA), and q(ψlA) = Dir(γψlA), are obtained as follows, ∑γθlAt = αθAt + q(zld)C(A, t; zld,l, d), (6) d,zld γlABC = α φ ABC+ φ ∑ q(zld)C(A→BC; zld, l, d), d,zld (7) ∑ q(zld)C(A → w; zld, l, d), γlAw = αψ + ψ d,zld (8) where C(A, t; z,l, d) is the count of rule type t that is selected in nonterminal A in the dth sentence of language l with parse tree z. The common rule type parameter αθ that minAt imizes the KL divergence between the true posterior and the approximate posterior can be obtained by using the fixed-point iteration method described in (Minka, 2000). The update rule is as follows, aθ−1+αθ AtL(Ψ(∑t0 αθAt0)−Ψ(αθAt)) bθ + ∑ (Ψ(∑t0 γθ lAt0) − Ψ(γθ lAt)) , l (9) where L is the number of languages, and Ψ(x) = ∂ log �(x) ∂x is the digamma function. Similarly, the common production parameter αφABC can be updated as follows, aφ − 1 + αφ ABCLJABC αABC ← φ(new) bφ + ∑ , (10) l J0 lABC where JABC = Ψ(∑B0,C0 αφAB0C0) − Ψ(αφABC), and J0lABC = Ψ(∑B0,C0 γφlAB0C0) − Ψ(γφlABC). Since factored variational distributions depend on each other, an optimal approximated posterior can be obtained by updating parameters by (2) - (10) alternatively until convergenc</context>
</contexts>
<marker>Minka, 2000</marker>
<rawString>Thomas Minka. 2000. Estimating a Dirichlet distribution. Technical report, M.I.T.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael P Oakes</author>
</authors>
<title>Computer estimation of vocabulary in a protolanguage from word lists in four daughter languages.</title>
<date>2000</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>7</volume>
<issue>3</issue>
<pages>243</pages>
<contexts>
<context position="1637" citStr="Oakes, 2000" startWordPosition="229" endWordPosition="230">imilar forms are used with similar meanings in different languages. The reasons for these common properties can be attributed to: 1) a common ancestor language, 2) borrowing from nearby languages, and 3) the innate abilities of humans (Chomsky, 1965). We assume hidden commonalities in syntax across languages, and try to extract a common grammar from non-parallel multilingual corpora. For this purpose, we propose a generative model for multilingual grammars that is learned in an unsupervised fashion. There are some computational models for capturing commonalities at the phoneme and word level (Oakes, 2000; BouchardCˆot´e et al., 2008), but, as far as we know, no attempt has been made to extract commonalities in syntax level from non-parallel and non-annotated multilingual corpora. In our scenario, we use probabilistic contextfree grammars (PCFGs) as our monolingual grammar model. We assume that a PCFG for each language is generated from a general model that are common across languages, and each sentence in multilingual corpora is generated from the language dependent PCFG. The inference of the general model as well as the multilingual PCFGs can be performed by using a variational method for ef</context>
</contexts>
<marker>Oakes, 2000</marker>
<rawString>Michael P. Oakes. 2000. Computer estimation of vocabulary in a protolanguage from word lists in four daughter languages. Journal of Quantitative Linguistics, 7(3):233– 243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Pinker</author>
</authors>
<title>The Language Instinct: How the Mind Creates Language. HarperCollins,</title>
<date>1994</date>
<location>New York.</location>
<contexts>
<context position="918" citStr="Pinker, 1994" startWordPosition="120" endWordPosition="121">n common syntax across languages from non-parallel multilingual corpora in an unsupervised fashion. For this purpose, we assume a generative model for multilingual corpora, where each sentence is generated from a language dependent probabilistic contextfree grammar (PCFG), and these PCFGs are generated from a prior grammar that is common across languages. We also develop a variational method for efficient inference. Experiments on a non-parallel multilingual corpus of eleven languages demonstrate the feasibility of the proposed method. 1 Introduction Languages share certain common properties (Pinker, 1994). For example, the word order in most European languages is subject-verb-object (SVO), and some words with similar forms are used with similar meanings in different languages. The reasons for these common properties can be attributed to: 1) a common ancestor language, 2) borrowing from nearby languages, and 3) the innate abilities of humans (Chomsky, 1965). We assume hidden commonalities in syntax across languages, and try to extract a common grammar from non-parallel multilingual corpora. For this purpose, we propose a generative model for multilingual grammars that is learned in an unsupervi</context>
</contexts>
<marker>Pinker, 1994</marker>
<rawString>Steven Pinker. 1994. The Language Instinct: How the Mind Creates Language. HarperCollins, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>73--81</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="3312" citStr="Snyder et al., 2009" startWordPosition="498" endWordPosition="502">dro, 1994; Klein and Manning, 2002; Klein and Manning, 2004; Liang et al., 2007). Recently, models have been proposed that outperform PCFG in the grammar induction task (Klein and Manning, 2002; Klein and Manning, 2004). We used PCFG as a first step for capturing commonalities in syntax across languages because of its simplicity. The proposed framework can be used for probabilistic grammar models other than PCFG. Grammar induction using bilingual parallel corpora has been studied mainly in machine translation research (Wu, 1997; Melamed, 2003; Eisner, 2003; Chiang, 2005; Blunsom et al., 2009; Snyder et al., 2009). These methods require sentencealigned parallel data, which can be costly to obtain and difficult to scale to many languages. On the other hand, our model does not require sentences to be aligned. Moreover, since the complexity of our model increases linearly with the number of languages, our model is easily applicable to cor184 Proceedings of the ACL 2010 Conference Short Papers, pages 184–188, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics pora of more than two languages, as we will show in the experiments. To our knowledge, the only grammar induction wor</context>
</contexts>
<marker>Snyder, Naseem, Barzilay, 2009</marker>
<rawString>Benjamin Snyder, Tahira Naseem, and Regina Barzilay. 2009. Unsupervised multilingual grammar induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 73–81, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Stephen M Omohundro</author>
</authors>
<title>Inducing probabilistic grammars by Bayesian model merging. In</title>
<date>1994</date>
<booktitle>ICGI ’94: Proceedings of the Second International Colloquium on Grammatical Inference and Applications,</booktitle>
<pages>106--118</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="2701" citStr="Stolcke and Omohundro, 1994" startWordPosition="399" endWordPosition="402">erated from the language dependent PCFG. The inference of the general model as well as the multilingual PCFGs can be performed by using a variational method for efficiency. Our approach is based on a Bayesian multitask learning framework (Yu et al., 2005; Daum´e III, 2009). Hierarchical Bayesian modeling provides a natural way of obtaining a joint regularization for individual models by assuming that the model parameters are drawn from a common prior distribution (Yu et al., 2005). 2 Related work The unsupervised grammar induction task has been extensively studied (Carroll and Charniak, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Klein and Manning, 2004; Liang et al., 2007). Recently, models have been proposed that outperform PCFG in the grammar induction task (Klein and Manning, 2002; Klein and Manning, 2004). We used PCFG as a first step for capturing commonalities in syntax across languages because of its simplicity. The proposed framework can be used for probabilistic grammar models other than PCFG. Grammar induction using bilingual parallel corpora has been studied mainly in machine translation research (Wu, 1997; Melamed, 2003; Eisner, 2003; Chiang, 2005; Blunsom et al., 2009; Snyder et</context>
</contexts>
<marker>Stolcke, Omohundro, 1994</marker>
<rawString>Andreas Stolcke and Stephen M. Omohundro. 1994. Inducing probabilistic grammars by Bayesian model merging. In ICGI ’94: Proceedings of the Second International Colloquium on Grammatical Inference and Applications, pages 106–118, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Comput. Linguist.,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="3225" citStr="Wu, 1997" startWordPosition="486" endWordPosition="487">as been extensively studied (Carroll and Charniak, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Klein and Manning, 2004; Liang et al., 2007). Recently, models have been proposed that outperform PCFG in the grammar induction task (Klein and Manning, 2002; Klein and Manning, 2004). We used PCFG as a first step for capturing commonalities in syntax across languages because of its simplicity. The proposed framework can be used for probabilistic grammar models other than PCFG. Grammar induction using bilingual parallel corpora has been studied mainly in machine translation research (Wu, 1997; Melamed, 2003; Eisner, 2003; Chiang, 2005; Blunsom et al., 2009; Snyder et al., 2009). These methods require sentencealigned parallel data, which can be costly to obtain and difficult to scale to many languages. On the other hand, our model does not require sentences to be aligned. Moreover, since the complexity of our model increases linearly with the number of languages, our model is easily applicable to cor184 Proceedings of the ACL 2010 Conference Short Papers, pages 184–188, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics pora of more than two language</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Comput. Linguist., 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Yu</author>
<author>Volker Tresp</author>
<author>Anton Schwaighofer</author>
</authors>
<title>Learning gaussian processes from multiple tasks.</title>
<date>2005</date>
<booktitle>In ICML ’05: Proceedings of the 22nd International Conference on Machine Learning,</booktitle>
<pages>1012--1019</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2328" citStr="Yu et al., 2005" startWordPosition="342" endWordPosition="345">made to extract commonalities in syntax level from non-parallel and non-annotated multilingual corpora. In our scenario, we use probabilistic contextfree grammars (PCFGs) as our monolingual grammar model. We assume that a PCFG for each language is generated from a general model that are common across languages, and each sentence in multilingual corpora is generated from the language dependent PCFG. The inference of the general model as well as the multilingual PCFGs can be performed by using a variational method for efficiency. Our approach is based on a Bayesian multitask learning framework (Yu et al., 2005; Daum´e III, 2009). Hierarchical Bayesian modeling provides a natural way of obtaining a joint regularization for individual models by assuming that the model parameters are drawn from a common prior distribution (Yu et al., 2005). 2 Related work The unsupervised grammar induction task has been extensively studied (Carroll and Charniak, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Klein and Manning, 2004; Liang et al., 2007). Recently, models have been proposed that outperform PCFG in the grammar induction task (Klein and Manning, 2002; Klein and Manning, 2004). We used PCFG as</context>
</contexts>
<marker>Yu, Tresp, Schwaighofer, 2005</marker>
<rawString>Kai Yu, Volker Tresp, and Anton Schwaighofer. 2005. Learning gaussian processes from multiple tasks. In ICML ’05: Proceedings of the 22nd International Conference on Machine Learning, pages 1012–1019, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>