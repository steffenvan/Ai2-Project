<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003276">
<title confidence="0.999124">
Japanese Dependency Parsing Using Sequential Labeling
for Semi-spoken Language
</title>
<author confidence="0.718563">
Kenji Imamura and Genichiro Kikui
</author>
<affiliation confidence="0.586625">
NTT Cyber Space Laboratories, NTT Corporation
</affiliation>
<address confidence="0.915913">
1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan
</address>
<email confidence="0.996198">
{imamura.kenji, kikui.genichiro}@lab.ntt.co.jp
</email>
<author confidence="0.970473">
Norihito Yasuda
</author>
<affiliation confidence="0.933981">
NTT Communication Science Laboratories, NTT Corporation
</affiliation>
<address confidence="0.983584">
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
</address>
<email confidence="0.99961">
n-yasuda@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999591454545455">
The amount of documents directly published
by end users is increasing along with the
growth of Web 2.0. Such documents of-
ten contain spoken-style expressions, which
are difficult to analyze using conventional
parsers. This paper presents dependency
parsing whose goal is to analyze Japanese
semi-spoken expressions. One characteris-
tic of our method is that it can parse self-
dependent (independent) segments using se-
quential labeling.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957057142857">
Dependency parsing is a way of structurally ana-
lyzing a sentence from the viewpoint of modifica-
tion. In Japanese, relationships of modification be-
tween phrasal units called bunsetsu segments are an-
alyzed. A number of studies have focused on parsing
of Japanese as well as of other languages. Popular
parsers are CaboCha (Kudo and Matsumoto, 2002)
and KNP (Kurohashi and Nagao, 1994), which were
developed to analyze formal written language ex-
pressions such as that in newspaper articles.
Generally, the syntactic structure of a sentence
is represented as a tree, and parsing is carried out
by maximizing the likelihood of the tree (Charniak,
2000; Uchimoto et al., 1999). Units that do not
modify any other units, such as fillers, are difficult
to place in the tree structure. Conventional parsers
have forced such independent units to modify other
units.
Documents published by end users (e.g., blogs)
are increasing on the Internet along with the growth
of Web 2.0. Such documents do not use controlled
written language and contain fillers and emoticons.
This implies that analyzing such documents is diffi-
cult for conventional parsers.
This paper presents a new method of Japanese
dependency parsing that utilizes sequential labeling
based on conditional random fields (CRFs) in or-
der to analyze semi-spoken language. Concretely,
sequential labeling assigns each segment a depen-
dency label that indicates its relative position of de-
pendency. If the label set includes self-dependency,
the fillers and emoticons would be analyzed as seg-
ments depending on themselves. Therefore, since it
is not necessary for the parsing result to be a tree,
our method is suitable for semi-spoken language.
</bodyText>
<sectionHeader confidence="0.996159" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.99949625">
Japanese dependency parsing for written language
is based on the following principles. Our method re-
laxes the first principle to allow self-dependent seg-
ments (c.f. Section 2.3).
</bodyText>
<listItem confidence="0.9790245">
1. Dependency moves from left to right.
2. Dependencies do not cross each other.
3. Each segment, except for the top of the parsed
tree, modifies at most one other segment.
</listItem>
<subsectionHeader confidence="0.985026">
2.1 Dependency Parsing Using Cascaded
Chunking (CaboCha)
</subsectionHeader>
<bodyText confidence="0.8866474">
Our method is based on the cascaded chunking
method (Kudo and Matsumoto, 2002) proposed as
the CaboCha parser 1. CaboCha is a sort of shift-
reduce parser and determines whether or not a seg-
ment depends on the next segment by using an
</bodyText>
<footnote confidence="0.993429">
1http://www.chasen.org/˜taku/software/cabocha/
</footnote>
<page confidence="0.976651">
225
</page>
<bodyText confidence="0.918478125">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 225–228,
Prague, June 2007. c�2007 Association for Computational Linguistics
SVM-based classifier. To analyze long-distance de-
pendencies, CaboCha shortens the sentence by re-
moving segments for which dependencies are al-
ready determined and which no other segments de-
pend on. CaboCha constructs a tree structure by re-
peating the above process.
</bodyText>
<subsectionHeader confidence="0.999904">
2.2 Sequential Labeling
</subsectionHeader>
<bodyText confidence="0.9999424">
Sequential labeling is a process that assigns each
unit of an input sequence an appropriate label (or
tag). In natural language processing, it is applied
to, for example, English part-of-speech tagging and
named entity recognition. Hidden Markov models
or conditional random fields (Lafferty et al., 2001)
are used for labeling. In this paper, we use linear-
chain CRFs.
In sequential labeling, training data developers
can design labels with no restrictions.
</bodyText>
<subsectionHeader confidence="0.991496">
2.3 Cascaded Chunking Using Sequential
Labeling
</subsectionHeader>
<bodyText confidence="0.999989222222222">
The method proposed in this paper is a generaliza-
tion of CaboCha. Our method considers not only
the next segment, but also the following N segments
to determine dependencies. This area, including the
considered segment, is called the window, and N is
called the window size. The parser assigns each seg-
ment a dependency label that indicates where the
segment depends on the segments in the window.
The flow is summarized as follows:
</bodyText>
<listItem confidence="0.995172153846154">
1. Extract features from segments such as the
part-of-speech of the headword in a segment
(c.f. Section 3.1).
2. Carry out sequential labeling using the above
features.
3. Determine the actual dependency by interpret-
ing the labels.
4. Shorten the sentence by deleting segments for
which the dependency is already determined
and that other segments have never depended
on.
5. If only one segment remains, then finish the
process. If not, return to Step 1.
</listItem>
<bodyText confidence="0.99721025">
An example of dependency parsing for written
language is shown in Figure 1 (a).
In Steps 1 and 2, dependency labels are supplied
to each segment in a way similar to that used by
</bodyText>
<tableCaption confidence="0.9333525">
Table 1: Label List Used by Sequential Labeling
(Window Size: 2)
</tableCaption>
<bodyText confidence="0.9305063">
other sequential labeling methods. However, our
sequential labeling has the following characteristics
since this task is dependency parsing.
• The labels indicate relative positions of the de-
pendent segment from the current segment (Ta-
ble 1). Therefore, the number of labels changes
according to the window size. Long-distance de-
pendencies can be parsed by one labeling process
if we set a large window size. However, growth
of label variety causes data sparseness problems.
</bodyText>
<listItem confidence="0.932675333333333">
• One possible label is that of self-dependency
(noted as ‘0Q’ in this paper). This is assigned
to independent segments in a tree.
• Also possible are two special labels. Label ‘-1O’
denotes a segment that is the top of the parsed
tree. Label ‘—’ denotes a segment that depends
on a segment outside of the window. When the
window size is two, the segment depends on a
segment that is over two segments ahead.
</listItem>
<bodyText confidence="0.959242058823529">
• The label for the current segment is determined
based on all features in the window and on the
label of the previous segment.
In Step 4, segments, which no other segments de-
pend on, are removed in a way similar to that used
by CaboCha. The principle that dependencies do
not cross each other is applied in this step. For ex-
ample, if a segment depends on a segment after the
next, the next segment cannot be modified by other
segments. Therefore, it can be removed. Similarly,
since the ‘—’ label indicates that the segment de-
pends on a segment after N segments, all interme-
diate segments can be removed if they do not have
‘—’ labels.
The sentence is shortened by iteration of the
above steps. The parsing finishes when only one
segment remains in the sentence (this is the segment
</bodyText>
<figure confidence="0.941862055555556">
Label
Description
Segment depends on a segment outside of win-
dow.
Self-dependency
Segment depends on next segment.
Segment depends on segment after next.
Segment is top of parsed tree.
—
0Q
1D
2D
-1O
226
(He was moved by her warm heart.) (Uuuum, my condition .... was good today.)
Input kare wa kanojo no atatakai magokoro ni kando-shita. Input Uuuum, kyo wa choshi yokatta desu.
(he) (her) (warm) (heart) (be moved) (today) (condition) (be good)
Seg. No. 1 2 3 4 5 Seg. No. 1 2 3 4 5
</figure>
<figureCaption confidence="0.999745">
Figure 1: Examples of Dependency Parsing (Window Size: 2)
</figureCaption>
<figure confidence="0.99833040625">
(a) Written Language
(b) Semi-spoken Language
1st
Labeling
0Q --- 0Q 1D -1O
Label
Label
--- 2D 1D 1D -1O
Label
Label
2D 1D -1O
1D -1O
2nd
Labeling
kanojo no
(her)
atatakai
(warm)
magokoro ni
(heart)
kando-shita.
(be moved)
choshi
(condition)
yokatta desu.
(be good)
kare wa
(he)
Output
Output
Uuuum, kyo wa
(today)
</figure>
<table confidence="0.9496364">
Corpus Type # of Sentences # of Segments
Kyoto Training 24,283 234,685
Test 9,284 89,874
Blog Training 18,163 106,177
Test 8,950 53,228
</table>
<tableCaption confidence="0.992566">
Table 2: Corpus Size
</tableCaption>
<bodyText confidence="0.9996445">
at the top of the parsed tree). In the example in Fig-
ure 1 (a), the process finishes in two iterations.
In a sentence containing fillers, the self-
dependency labels are assigned by sequential label-
ing, as shown in Figure 1 (b), and are parsed as in-
dependent segments. Therefore, our method is suit-
able for parsing semi-spoken language that contains
independent segments.
</bodyText>
<sectionHeader confidence="0.999858" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999845">
3.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.823201545454545">
Corpora In our experiments, we used two cor-
pora. One is the Kyoto Text Corpus 4.0 2, which is
a collection of newspaper articles with segment and
dependency annotations. The other is a blog cor-
pus, which is a collection of blog articles taken as
semi-spoken language. The blog corpus is manually
annotated in a way similar to that used for the Kyoto
text corpus. The sizes of the corpora are shown in
Table 2.
Training We used CRF++ 3, a linear-chain CRF
training tool, with eleven features per segment. All
</bodyText>
<footnote confidence="0.999696">
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus.html
3http://www.chasen.org/˜taku/software/CRF++/
</footnote>
<bodyText confidence="0.999813125">
of these are static features (proper to each segment)
such as surface forms, parts-of-speech, inflections
of a content headword and a functional headword
in a segment. These are parts of a feature set that
many papers have referenced (Uchimoto et al., 1999;
Kudo and Matsumoto, 2002).
Evaluation Metrics Dependency accuracy and
sentence accuracy were used as evaluation metrics.
Sentence accuracy is the proportion of total sen-
tences in which all dependencies in the sentence
are accurately labeled. In Japanese, the last seg-
ment of most sentences is the top of the parsed trees,
and many papers exclude this last segment from the
accuracy calculation. We, in contrast, include the
last one because some of the last segments are self-
dependent.
</bodyText>
<subsectionHeader confidence="0.999971">
3.2 Accuracy of Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.997856714285714">
Dependency parsing was carried out by combining
training and test corpora. We used a window size
of three. We also used CaboCha as a reference for
the set of sentences trained only with the Kyoto cor-
pus because it is designed for written language. The
results are shown in Table 3.
CaboCha had better accuracies for the Kyoto test
corpus. One reason might be that our method man-
ually combined features and used parts of com-
binations, while CaboCha automatically finds the
best combinations by using second-order polyno-
mial kernels.
For the blog test corpus, the proposed method
using the Kyoto+Blog model had the best depen-
</bodyText>
<page confidence="0.990865">
227
</page>
<table confidence="0.9991293">
Test Corpus Method Training Corpus Dependency Accuracy Sentence Accuracy
(Model)
Kyoto Proposed Method Kyoto 89.87% (80766 / 89874) 48.12% (4467 / 9284)
(Written Language) (Window Size: 3)
Kyoto + Blog 89.76% (80670 / 89874) 47.63% (4422 / 9284)
CaboCha Kyoto 92.03% (82714 / 89874) 55.36% (5140 / 9284)
Blog Proposed Method Kyoto 77.19% (41083 / 53226) 41.41% (3706 / 8950)
(Semi-spoken Language) (Window Size: 3)
Kyoto + Blog 84.59% (45022 / 53226) 52.72% (4718 / 8950)
CaboCha Kyoto 77.44% (41220 / 53226) 43.45% (3889 / 8950)
</table>
<tableCaption confidence="0.999539">
Table 3: Dependency and Sentence Accuracies among Methods/Corpora
</tableCaption>
<figure confidence="0.991029333333333">
91
Dependency Accuracy (%)
90.5
90
89.5
89
88.5
Dependency Accuracy
# of Features
6e+06
4e+06
2e+06
88 0
1 2 3 4 5
Window Size
</figure>
<figureCaption confidence="0.888579666666667">
Figure 2: Dependency Accuracy and Number of
Features According to Window Size (The Kyoto
Text Corpus was used for training and testing.)
</figureCaption>
<bodyText confidence="0.999945636363636">
dency accuracy result at 84.59%. This result was
influenced not only by the training corpus that con-
tains the blog corpus but also by the effect of self-
dependent segments. The blog test corpus contains
3,089 self-dependent segments, and 2,326 of them
(75.30%) were accurately parsed. This represents
a dependency accuracy improvement of over 60%
compared with the Kyoto model.
Our method is effective in parsing blogs be-
cause fillers and emoticons can be parsed as self-
dependent segments.
</bodyText>
<subsectionHeader confidence="0.998134">
3.3 Accuracy According to Window Size
</subsectionHeader>
<bodyText confidence="0.999991333333333">
Another characteristic of our method is that all de-
pendencies, including long-distance ones, can be
parsed by one labeling process if the window cov-
ers the entire sentence. To analyze this characteris-
tic, we evaluated dependency accuracies in various
window sizes. The results are shown in Figure 2.
The number of features used for labeling in-
creases exponentially as window size increases.
However, dependency accuracy was saturated after a
</bodyText>
<sectionHeader confidence="0.999846" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999989571428571">
We presented a new dependency parsing method us-
ing sequential labeling for the semi-spoken language
that frequently appears in Web documents. Sequen-
tial labeling can supply segments with flexible la-
bels, so our method can parse independent words
as self-dependent segments. This characteristic af-
fects robust parsing when sentences contain fillers
and emoticons.
The other characteristics of our method are us-
ing CRFs and that long dependencies are parsed in
one labeling process. SVM-based parsers that have
the same characteristics can be constructed if we in-
troduce multi-class classifiers. Further comparisons
with SVM-based parsers are future work.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999805058823529">
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. ofNAACL-2000, pages 132–139.
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analyisis using cascaded chunking. In Proc. of
CoNLL-2002, Taipei.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4):507–534.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML-2001, pages 282–289.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
1999. Japanese dependency structure analysis based
on maximum entropy models. In Proc. of EACL’99,
pages 196–203, Bergen, Norway.
</reference>
<figure confidence="0.969334">
1e+07 window size of two, and the best accuracy was when
the window size was four. This phenomenon implies
8e+06 a data sparseness problem.
# of Features
</figure>
<page confidence="0.961122">
228
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.829693">
<title confidence="0.996313">Japanese Dependency Parsing Using Sequential Labeling for Semi-spoken Language</title>
<author confidence="0.999329">Imamura Kikui</author>
<affiliation confidence="0.999782">NTT Cyber Space Laboratories, NTT Corporation</affiliation>
<address confidence="0.976238">1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan</address>
<author confidence="0.98717">Norihito Yasuda</author>
<affiliation confidence="0.999846">NTT Communication Science Laboratories, NTT Corporation</affiliation>
<address confidence="0.992726">2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan</address>
<email confidence="0.974115">n-yasuda@cslab.kecl.ntt.co.jp</email>
<abstract confidence="0.991235083333333">The amount of documents directly published by end users is increasing along with the growth of Web 2.0. Such documents often contain spoken-style expressions, which are difficult to analyze using conventional parsers. This paper presents dependency parsing whose goal is to analyze Japanese semi-spoken expressions. One characteristic of our method is that it can parse selfdependent (independent) segments using sequential labeling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. ofNAACL-2000,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1530" citStr="Charniak, 2000" startWordPosition="214" endWordPosition="215">tructurally analyzing a sentence from the viewpoint of modification. In Japanese, relationships of modification between phrasal units called bunsetsu segments are analyzed. A number of studies have focused on parsing of Japanese as well as of other languages. Popular parsers are CaboCha (Kudo and Matsumoto, 2002) and KNP (Kurohashi and Nagao, 1994), which were developed to analyze formal written language expressions such as that in newspaper articles. Generally, the syntactic structure of a sentence is represented as a tree, and parsing is carried out by maximizing the likelihood of the tree (Charniak, 2000; Uchimoto et al., 1999). Units that do not modify any other units, such as fillers, are difficult to place in the tree structure. Conventional parsers have forced such independent units to modify other units. Documents published by end users (e.g., blogs) are increasing on the Internet along with the growth of Web 2.0. Such documents do not use controlled written language and contain fillers and emoticons. This implies that analyzing such documents is difficult for conventional parsers. This paper presents a new method of Japanese dependency parsing that utilizes sequential labeling based on </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proc. ofNAACL-2000, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analyisis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Proc. of CoNLL-2002,</booktitle>
<location>Taipei.</location>
<contexts>
<context position="1230" citStr="Kudo and Matsumoto, 2002" startWordPosition="164" endWordPosition="167">ifficult to analyze using conventional parsers. This paper presents dependency parsing whose goal is to analyze Japanese semi-spoken expressions. One characteristic of our method is that it can parse selfdependent (independent) segments using sequential labeling. 1 Introduction Dependency parsing is a way of structurally analyzing a sentence from the viewpoint of modification. In Japanese, relationships of modification between phrasal units called bunsetsu segments are analyzed. A number of studies have focused on parsing of Japanese as well as of other languages. Popular parsers are CaboCha (Kudo and Matsumoto, 2002) and KNP (Kurohashi and Nagao, 1994), which were developed to analyze formal written language expressions such as that in newspaper articles. Generally, the syntactic structure of a sentence is represented as a tree, and parsing is carried out by maximizing the likelihood of the tree (Charniak, 2000; Uchimoto et al., 1999). Units that do not modify any other units, such as fillers, are difficult to place in the tree structure. Conventional parsers have forced such independent units to modify other units. Documents published by end users (e.g., blogs) are increasing on the Internet along with t</context>
<context position="3073" citStr="Kudo and Matsumoto, 2002" startWordPosition="455" endWordPosition="458">ves. Therefore, since it is not necessary for the parsing result to be a tree, our method is suitable for semi-spoken language. 2 Methods Japanese dependency parsing for written language is based on the following principles. Our method relaxes the first principle to allow self-dependent segments (c.f. Section 2.3). 1. Dependency moves from left to right. 2. Dependencies do not cross each other. 3. Each segment, except for the top of the parsed tree, modifies at most one other segment. 2.1 Dependency Parsing Using Cascaded Chunking (CaboCha) Our method is based on the cascaded chunking method (Kudo and Matsumoto, 2002) proposed as the CaboCha parser 1. CaboCha is a sort of shiftreduce parser and determines whether or not a segment depends on the next segment by using an 1http://www.chasen.org/˜taku/software/cabocha/ 225 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 225–228, Prague, June 2007. c�2007 Association for Computational Linguistics SVM-based classifier. To analyze long-distance dependencies, CaboCha shortens the sentence by removing segments for which dependencies are already determined and which no other segments depend on. CaboCha constructs a tree structure by repeating the above p</context>
<context position="9326" citStr="Kudo and Matsumoto, 2002" startWordPosition="1496" endWordPosition="1499">. The blog corpus is manually annotated in a way similar to that used for the Kyoto text corpus. The sizes of the corpora are shown in Table 2. Training We used CRF++ 3, a linear-chain CRF training tool, with eleven features per segment. All 2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus.html 3http://www.chasen.org/˜taku/software/CRF++/ of these are static features (proper to each segment) such as surface forms, parts-of-speech, inflections of a content headword and a functional headword in a segment. These are parts of a feature set that many papers have referenced (Uchimoto et al., 1999; Kudo and Matsumoto, 2002). Evaluation Metrics Dependency accuracy and sentence accuracy were used as evaluation metrics. Sentence accuracy is the proportion of total sentences in which all dependencies in the sentence are accurately labeled. In Japanese, the last segment of most sentences is the top of the parsed trees, and many papers exclude this last segment from the accuracy calculation. We, in contrast, include the last one because some of the last segments are selfdependent. 3.2 Accuracy of Dependency Parsing Dependency parsing was carried out by combining training and test corpora. We used a window size of thre</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analyisis using cascaded chunking. In Proc. of CoNLL-2002, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="1266" citStr="Kurohashi and Nagao, 1994" startWordPosition="170" endWordPosition="173">nal parsers. This paper presents dependency parsing whose goal is to analyze Japanese semi-spoken expressions. One characteristic of our method is that it can parse selfdependent (independent) segments using sequential labeling. 1 Introduction Dependency parsing is a way of structurally analyzing a sentence from the viewpoint of modification. In Japanese, relationships of modification between phrasal units called bunsetsu segments are analyzed. A number of studies have focused on parsing of Japanese as well as of other languages. Popular parsers are CaboCha (Kudo and Matsumoto, 2002) and KNP (Kurohashi and Nagao, 1994), which were developed to analyze formal written language expressions such as that in newspaper articles. Generally, the syntactic structure of a sentence is represented as a tree, and parsing is carried out by maximizing the likelihood of the tree (Charniak, 2000; Uchimoto et al., 1999). Units that do not modify any other units, such as fillers, are difficult to place in the tree structure. Conventional parsers have forced such independent units to modify other units. Documents published by end users (e.g., blogs) are increasing on the Internet along with the growth of Web 2.0. Such documents</context>
</contexts>
<marker>Kurohashi, Nagao, 1994</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1994. A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures. Computational Linguistics, 20(4):507–534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML-2001,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="4010" citStr="Lafferty et al., 2001" startWordPosition="596" endWordPosition="599">r Computational Linguistics SVM-based classifier. To analyze long-distance dependencies, CaboCha shortens the sentence by removing segments for which dependencies are already determined and which no other segments depend on. CaboCha constructs a tree structure by repeating the above process. 2.2 Sequential Labeling Sequential labeling is a process that assigns each unit of an input sequence an appropriate label (or tag). In natural language processing, it is applied to, for example, English part-of-speech tagging and named entity recognition. Hidden Markov models or conditional random fields (Lafferty et al., 2001) are used for labeling. In this paper, we use linearchain CRFs. In sequential labeling, training data developers can design labels with no restrictions. 2.3 Cascaded Chunking Using Sequential Labeling The method proposed in this paper is a generalization of CaboCha. Our method considers not only the next segment, but also the following N segments to determine dependencies. This area, including the considered segment, is called the window, and N is called the window size. The parser assigns each segment a dependency label that indicates where the segment depends on the segments in the window. T</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML-2001, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Japanese dependency structure analysis based on maximum entropy models.</title>
<date>1999</date>
<booktitle>In Proc. of EACL’99,</booktitle>
<pages>196--203</pages>
<location>Bergen,</location>
<contexts>
<context position="1554" citStr="Uchimoto et al., 1999" startWordPosition="216" endWordPosition="219">yzing a sentence from the viewpoint of modification. In Japanese, relationships of modification between phrasal units called bunsetsu segments are analyzed. A number of studies have focused on parsing of Japanese as well as of other languages. Popular parsers are CaboCha (Kudo and Matsumoto, 2002) and KNP (Kurohashi and Nagao, 1994), which were developed to analyze formal written language expressions such as that in newspaper articles. Generally, the syntactic structure of a sentence is represented as a tree, and parsing is carried out by maximizing the likelihood of the tree (Charniak, 2000; Uchimoto et al., 1999). Units that do not modify any other units, such as fillers, are difficult to place in the tree structure. Conventional parsers have forced such independent units to modify other units. Documents published by end users (e.g., blogs) are increasing on the Internet along with the growth of Web 2.0. Such documents do not use controlled written language and contain fillers and emoticons. This implies that analyzing such documents is difficult for conventional parsers. This paper presents a new method of Japanese dependency parsing that utilizes sequential labeling based on conditional random field</context>
<context position="9299" citStr="Uchimoto et al., 1999" startWordPosition="1492" endWordPosition="1495">as semi-spoken language. The blog corpus is manually annotated in a way similar to that used for the Kyoto text corpus. The sizes of the corpora are shown in Table 2. Training We used CRF++ 3, a linear-chain CRF training tool, with eleven features per segment. All 2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus.html 3http://www.chasen.org/˜taku/software/CRF++/ of these are static features (proper to each segment) such as surface forms, parts-of-speech, inflections of a content headword and a functional headword in a segment. These are parts of a feature set that many papers have referenced (Uchimoto et al., 1999; Kudo and Matsumoto, 2002). Evaluation Metrics Dependency accuracy and sentence accuracy were used as evaluation metrics. Sentence accuracy is the proportion of total sentences in which all dependencies in the sentence are accurately labeled. In Japanese, the last segment of most sentences is the top of the parsed trees, and many papers exclude this last segment from the accuracy calculation. We, in contrast, include the last one because some of the last segments are selfdependent. 3.2 Accuracy of Dependency Parsing Dependency parsing was carried out by combining training and test corpora. We</context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 1999</marker>
<rawString>Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 1999. Japanese dependency structure analysis based on maximum entropy models. In Proc. of EACL’99, pages 196–203, Bergen, Norway.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>