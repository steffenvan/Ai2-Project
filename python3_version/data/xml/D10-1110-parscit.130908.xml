<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000121">
<title confidence="0.975208">
Learning Recurrent Event Queries for Web Search
</title>
<author confidence="0.911122">
Ruiqiang Zhang and Yuki Konda and Anlei Dong
Pranam Kolari and Yi Chang and ZhaohuiZheng
</author>
<affiliation confidence="0.849708">
Yahoo! Inc
</affiliation>
<address confidence="0.538058">
701 First Avenue, Sunnyvale, CA94089
</address>
<sectionHeader confidence="0.990157" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999783607142857">
Recurrent event queries (REQ) constitute a
special class of search queries occurring at
regular, predictable time intervals. The fresh-
ness of documents ranked for such queries is
generally of critical importance. REQ forms a
significant volume, as much as 6% of query
traffic received by search engines. In this
work, we develop an improved REQ classi-
fier that could provide significant improve-
ments in addressing this problem. We ana-
lyze REQ queries, and develop novel features
from multiple sources, and evaluate them us-
ing machine learning techniques. From histor-
ical query logs, we develop features utilizing
query frequency, click information, and user
intent dynamics within a search session. We
also develop temporal features by time series
analysis from query frequency. Other gener-
ated features include word matching with re-
current event seed words and time sensitiv-
ity of search result set. We use Naive Bayes,
SVM and decision tree based logistic regres-
sion model to train REQ classifier. The re-
sults on test data show that our models outper-
formed baseline approach significantly. Ex-
periments on a commercial Web search en-
gine also show significant gains in overall rel-
evance, and thus overall user experience.
</bodyText>
<sectionHeader confidence="0.999628" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.909374536585366">
REQ pertains to queries about events which oc-
cur at regular, predictable time intervals, most often
weekly, monthly, annually, bi-annually, etc. Natu-
rally, users issue REQ periodically. REQ usually re-
fer to:
Organized public events such as festivals, confer-
ences, expos, sports competitions, elections: winter
olympics, boston marathon, the International Ocean
Research Conference, oscar night.
Public holidays and other noteworthy dates: labor day,
date of Good Friday, Thanksgiving, black friday.
Products with annual model releases, such as car models:
ford explorer, prius.
Lottery drawings: California lotto results.
TV shows and programs which are currently running:
American idol, Inside Edition.
Cultural related activities: presidential election, tax re-
turn, 1040 form.
Our interest in studying REQ arises from the chal-
lenge imposed on Web search ranking. To illustrate
this, we show an example in Fig. 1 that snapshots
the real ranking results of the query, EMNLP, is-
sued in 2010 when the authors composed this pa-
per, on Google search engine. It is obvious the
ranking is not satisfactory because the page about
EMNLP2008 is on the first position in 2010. Ide-
ally, the page about EMNLP2010 on the 6th position
should be on the first position even if users don’t
explicitly issue the query, EMNLP 2010, because
EMNLP is a REQ. The query, “EMNLP”, implic-
itly, without a year qualifier, needs to be served the
most recent pages about “EMNLP”.
A better search ranking result cannot be achieved
if we do not categorize “EMNLP” as a REQ, and
provide special ranking treatment to such queries.
Existing search engines adopt a fairly involved rank-
ing algorithm to order Web search results by con-
sidering many factors. Time is an important fac-
tor but not the most critical. The page’s rank-
ing score mostly depends on other features such
as tf-idf (Salton and McGill, 1983), BM25 (Jones
</bodyText>
<page confidence="0.969356">
1129
</page>
<note confidence="0.920851">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1129–1139,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.978032333333333">
Figure 1: A real problematic ranking result by Google for
a REQ query, “EMNLP”. The EMNLP2010 page should
be on the 1st position.
</figureCaption>
<bodyText confidence="0.99996135483871">
et al., 2000), anchor text, historical clicks, pager-
ank (Brin and Page, 1998), and overall page qual-
ity. New pages about EMNLP2010 obtain less fa-
vorable feature values than the pages of 2009 earlier
in terms of anchor text, click or pagerank because
they have existed for a shorter time and haven’t ac-
cumulated sufficient popularity to make them stand
out. Without special treatment, the new pages about
“EMNLP2010” will typically not be ranked appro-
priately for the users.
Typically, a recurrent event is associated with a
root, and spawns a large set of queries. Oscar,
for instance, is a recurrent event about the annual
Academy Award. Based on this, queries like “oscar
best actress”, “oscar best dress”, “oscar best movie
award”, are all recurrent event queries. As such,
REQ is a highly frequent category of query in Web
search. By Web search query log analysis, we ob-
serve that there about 5-6% queries of total query
volume belongs to this category.
In this work, we learn if a query is in the REQ
class, by effectively combining multiple features.
Our features are developed through analysis of his-
torical query logs. We discuss our approaches in de-
tail in Section 3. We then develop a REQ classi-
fier where all the features are integrated by machine
learning models. We use Naive Bayes, SVM and de-
cision tree based logistic regression models. These
models are described in Section 4. Our experiments
for REQ classifier and Web search ranking are de-
tailed in Section 5 and 6.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99954097368421">
We found our work were related to two other prob-
lems: general query classification and time-sensitive
query classification. For general query classifica-
tion, the task is to assign a Web search query to
one or more predefined categories based on its top-
ics. In the query classification contest in KDD-
CUP 2005 (Li et al., 2005), seven categories and
67 sub-categories were defined. The winning so-
lution (Shen et al., 2005) used multiple classifiers
integrated by ensemble method. The difficulties for
query classification are from short queries, lack of
labeled data, and query sense ambiguity. Most pop-
ular studies use query log, web search results, unla-
beled data to enrich query classification (Shen et al.,
2006; Beitzel et al., 2005), or use document classifi-
cation to predict query classification (Broder et al., ).
General query classification is also studied for query
intent detection by (Li et al., 2008).
There are many prior works to study the time sen-
sitivity issue in web search. For example, Baeza-
Yates et al. (Baeza-Yates et al., 2002) studied the re-
lation between the web dynamics, structure and page
quality, and demonstrated that PageRank is biased
against new pages. In T-Rank Light and T-Rank al-
gorithms (Berberich et al., 2005), both activity (i.e.,
update rates) and freshness (i.e., timestamps of most
recent updates) of pages and links are taken into ac-
count for link analysis. Cho et al. (Cho et al., 2005)
proposed a page quality ranking function in order to
alleviate the problem of popularity-based ranking,
and they used the derivatives of PageRank to fore-
cast future PageRank values for new pages. Pandey
et al. (Pandey et al., 2005) studied the tradeoff be-
tween new page exploration and high-quality page
exploitation, which is based on a ranking method to
randomly promote some new pages so that they can
accumulate links quickly.
More recently, Dong et al. (Dong et al., 2010a)
</bodyText>
<page confidence="0.983721">
1130
</page>
<bodyText confidence="0.999856890909091">
proposed a machine-learned framework to improve
ranking result freshness, in which novel features,
modeling algorithms and editorial guideline are used
to deal with time sensitivities of queries and doc-
uments. In another work (Dong et al., 2010b), they
use micro-blogging data (e.g., Twitter data) to detect
fresh URLs. Novel and effective features are also
extracted for fresh URLs so that ranking recency in
web search is improved.
Perhaps the most related work to this paper is
the query classification approach used in (Zhang
et al., 2009) and (Metzler et al., 2009), in which
year qualified queries (YQQs) are detected based
on heuristic rules. For example, a query contain-
ing a year stamp is an explicit YQQ; if the year
stamp is removed from this YQQ, the remaining part
of this query is also a YQQ, which is called im-
plicit YQQ. Different ranking approaches were used
in (Zhang et al., 2009) and (Metzler et al., 2009)
where (Zhang et al., 2009) boosted pages of the most
latest year while (Metzler et al., 2009) promoted
pages of the most influential years. Similarly, Nunes
et al. (Nunes, 2007) applied information extraction
techniques to identify temporal expression in web
search queries, and found 1.5% of queries contain-
ing temporal expression.
Dong et al. (Dong et al., 2010a) proposed a
breaking-news query classifier with high accuracy
and reasonable coverage, which works not by mod-
eling each individual topic and tracking it over time,
but by modeling each discrete time slot, and compar-
ing the models representing different time slots. The
buzziness of a query is computed as the language
model likelihood difference between different time
slots. In this approach, both query log and news
contents are exploited to compute language model
likelihood.
Diaz (Diaz, 2009) determined the newsworthiness
of a query by predicting the probability of a user
clicks on the news display of a query. In this frame-
work, the data sources of both query log and news
corpus are leveraged to compute contextual features.
Furthermore, the online click feedback also plays a
critical role for future click prediction.
Konig et al. (Knig et al., 2009) estimated the
click-through rate for dedicated news search result
with a supervised model, which is to satisfy the
requirement of adapting quickly to emerging news
event. Some additional corpora such as blog crawl
and Wikipedia is used for buzziness inference. Com-
pared with (Diaz, 2009), different feature and learn-
ing algorithms are used.
Elsas et al. (Elsas and Dumais, 2010) studied
improving relevance ranking by detecting document
content change to leverage temporal information.
</bodyText>
<sectionHeader confidence="0.985796" genericHeader="method">
3 Feature Generation
</sectionHeader>
<bodyText confidence="0.999984394736842">
To better understand our work, we first introduce
three terms. We subdivide all raw queries in query
log into three categories: Explicit Timestamp, Im-
plicit Timestamp, and No Timestamp. An Explicit
Timestamp query contains at least one token being a
time indicator. For example, emnlp 2010, 2007 De-
cember holiday calendar, amsterdam weather sum-
mer 2009, Google Q1 reports 2010. These queries
are considered to conatin time indicators, because
we can regard {2010, 2007, 2009} as year indica-
tor, december as month indicator, {summer, Q1(first
quarter)} as seasonal indicator. To simplify our
work, we only consider the year indicators, 2010,
2007, 2009. Such year indicators are also the most
important and most popular indicators, as noted in
(Zhang et al., 2009). Any query containing at least
one year indicator is an Explicit Timestamp query.
Due to word sense ambiguity, some queries labeled
as Explicit Timestamp by this method may have no
connection with time such as Windows Office 2007,
2010 Sunset Boulevard, or call number 2008. In this
work, we tolerate this type of error because word
sense disambiguation is a peripheral problem for this
task.
Implicit Timestamp queries are resulted by re-
moving all year indicators from the corresponding
Explicit Timestamp queries. For example, the Im-
plicit Timestamp query of emnlp 2010 is emnlp.
All other queries are No Timestamp queries because
they have never been found together with a year in-
dicator.
Classifying queries into the above three cate-
gories depends on the used query log. A search
engine company partner provided us a query log
from 08/01/2009 to 02/29/2010 for this research.
We found the proportions of the three categories
in this query log are 13.8% (Explicit), 17.1% (Im-
plicit) and 69.1% (No Timestamp). These numbers
</bodyText>
<page confidence="0.957145">
1131
</page>
<bodyText confidence="0.999911055555555">
could be slightly different depending on the source
of query logs. Note that 17.1% of Implicit Times-
tamp queries in the query log is a significant num-
ber. However, not all Implicit Timestamp queries
are REQ. Many Implicit Timestamp queries have no
time sense. They belong to Implicit Timestamp just
because users issued the query with a year indica-
tor through varied intents. For example, “google” is
found to be an Implicit Timestamp query since there
were many “google 2008” or “google 2009” in the
query log.
The next few sections introduce our work in rec-
ognizing recurrent event time sense for Implicit
Timestamp queries. We first focus on features.
There are many features that were exploited in REQ
classifier. We extract these features from query log,
query session log, click log, search results, time se-
ries and NLP morphological analysis.
</bodyText>
<subsectionHeader confidence="0.998839">
3.1 Query log analysis
</subsectionHeader>
<bodyText confidence="0.962934387096774">
The following features are extracted from query log
analysis:
QueryDailyFrequency: the total counts of the
query divided by the number of the days in the pe-
riod.
ExplicitQueryRatio: Ratio of number of counts
query was issued with year and number of counts
query was issued with or without year. This feature
is the method used by (Zhang et al., 2009).
UniqExplicitQueryCount: Number of uniq Ex-
plicit Timestamp queries associated with query. For
example, if a query was issued with query+2009 and
query+2008, this feature’s value is two.
ChiSquareYearDist: this feature is the distance be-
tween two distributions: one is frequency distribu-
tion over years for all REQ queries. The other is
that for single REQ query. It is calculated through
following steps: (a) Aggregate the frequencies for
all queries for all years. Suppose we observe all
years from 2001 to 2010. So we can get vector,
E = ( af10
sum1, a f09
sum1, ..., a f01
sum1) where afi is the frequency
sum of year 20i for all REQ queries. sum1 =
af10 + af09 + ... + af01, the sum of all year fre-
quency. (b) Given a query, suppose we observe
this query’s yearly frequency distribution is , Oq =
(qf10, qf09, , ..., qf01). qfi is this query’s frequency
for the year 20i. Pad the slot with zeros if no fre-
quency found. The expected distribution for this
</bodyText>
<equation confidence="0.943314">
Eq sum2∗af10 sum2∗af09 sum2∗af01
query is, E = ( sum1 , sum1 , ..., sum1 ),
</equation>
<bodyText confidence="0.99306775">
where sum2 = qf10 + qf09 + ... + qf01 is sum of
all year frequency for the query. (d) Calculate CHI-
squared value to represent the different yearly fre-
quency distribution between Eq and Oq according to
</bodyText>
<equation confidence="0.962452333333333">
χ2 =N (Oq i −Eq i )2
i=1 Eq
i
</equation>
<bodyText confidence="0.9991525">
method is widely used for statistical hypothesis test.
We found it to be a useful feature for REQ classifier.
</bodyText>
<subsectionHeader confidence="0.999745">
3.2 Query reformulation
</subsectionHeader>
<bodyText confidence="0.932006307692308">
If users cannot find the newest page by issuing Im-
plicit Timestamp query, they may re-issue the query
using an Explicit Timestamp query. We can detect
this change in a search session (a 30 minutes period
for each query). By finding this kind of behavior
from users, we next extract three features.
UserSwitch: Number of unique users that switched
from Implicit Timestamp queries to Explicit Times-
tamp queries.
YearSwitch: Number of unique year-like tokens
switched by users in a query session.
NormalizedUserSwitch: Feature UserSwitch di-
vided by QueryDailyFrequency.
</bodyText>
<subsectionHeader confidence="0.99807">
3.3 Click log analysis
</subsectionHeader>
<bodyText confidence="0.989094733333333">
If a query is time sensitive, users may click a
page that displays the year indicator on title or
url. An example that shows year indicator on
url is www.lsi.upc.edu/events/emnlp2010/call.html.
Search engine click log saves all users’ click infor-
mation. We used click log to derive the following
features.
YearUrlTop5CTR: Aggregated click through rate
(CTR) of all top five URLs containing a year in-
dicator. CTR of an URL is defined as the number
of clicks of an URL divided by the number of page
views.
YearUrlFPCTR: Aggregated click through rate
(CTR) of all first page URLs containing a year in-
dicator.
</bodyText>
<subsectionHeader confidence="0.995221">
3.4 Search engine result set
</subsectionHeader>
<bodyText confidence="0.9998076">
For each Implicited Timestamp query, we can scrape
the search engine to get search results. We count the
number of titles and urls that contain year indicator.
We use this number as a feature, and generate 6 fea-
tures.
</bodyText>
<note confidence="0.611785">
.Using CHI square distance as a
</note>
<page confidence="0.953189">
1132
</page>
<bodyText confidence="0.9116842">
TitleYearTop5: the number of titles containing a
year indication on the top 5 results. This value is
4 in Fig. 1.
TitleYearTop10: the number of titles containing a
year indication on the top 10 results. This value is 6
in Fig. 1.
TitleYearTop30: the number of titles containing a
year indication on the top 30 results.
UrlYearTop5: the number of urls containing a year
indication on the top 5 results. This value is 1 in
Fig. 1.
UrlYearTop10: the number of urls containing a year
indication on the top 10 results.
UrlYearTop30: the number of titles containing a
year indication on the top 30 results.
</bodyText>
<subsectionHeader confidence="0.915228">
3.5 Time series analysis
</subsectionHeader>
<bodyText confidence="0.9999645">
Recurrent event query has periodic occurrence pat-
tern in time series. Top graph of Figure 2 shows the
frequency change of the query, “Oscar”. The annual
event usually starts from Oscar nomination as ear-
lier as last year December to award announcement
of February this year. So a small spike and a big
spike are observed in the graph to indicate nomina-
tion period and ceremony period. There are a period
of silence between the two periods. The frequency
pattern keeps unchanged each year. We show three
years (2007,8,9) in the graph. By making use of re-
current event queries’ periodic properties, we calcu-
lated the query period as a new feature.
We use autocorrelation to calculate the period.
</bodyText>
<equation confidence="0.9330405">
{�N−τ
t=1 (xt − p)2(xt+τ − p)211/2
</equation>
<bodyText confidence="0.999941">
where x(t) is query daily frequency. N is the num-
ber of days used for this query. We can get maxi-
mum of 3 years data for some queries but only a few
months for others. R(τ) is autocorrelation function.
Peaks (the local biggest R(τ) given a time window)
can be detected from R(τ) plot. The period T is cal-
culated as the duration between two neighbor peaks.
T = 365 for the query, “Oscar”. The bottom graph
of Fig. 2 shows the autocorrelation function plot for
the query Oscar.
</bodyText>
<subsectionHeader confidence="0.992541">
3.6 Recurrent event seed word list
</subsectionHeader>
<bodyText confidence="0.96712">
Many recurrent event queries share some common
words that have recurrent time sense. We list most
</bodyText>
<table confidence="0.999838375">
new results top schedule
football festival movie world
show day best tax
result calendar honda ford
download exam nfl miss
awards toyota tour sale
american fair list pictures
election game basketball cup
</table>
<tableCaption confidence="0.999932">
Table 1: Top recurrent event seed words
</tableCaption>
<bodyText confidence="0.999262916666667">
frequently used recurrent seeds in Table 1. Those
seeds are likely combined with other words to form
new recurrent event queries. For example, the seed,
“new”, can be used by queries “new bmw cars”,
“whitney houston new songs”, “apple new iphone”,
or “hairstyle new”.
To generate the seed list, we tokenized all the
queries from Implicit Timestamp queries and split
all the tokens. We then sort and unique all the to-
kens, and submit top tokens to professional editors
who are asked to pick 8,000 seeds from the top fre-
quent tokens. Some top tokens were removed if they
are not qualified to form recurrent event queries. The
editors took about four days to do the judgment ac-
cording to the token’s time sense and examples of
recurrent event queries. However, this is a one-time
effort. A token will be in the seed if there are many
recurrent event examples formed by this token, by
editors’ judgment.
Table 1 shows 32 top seeds. Some seeds connect
with time such as, “new, schedule, day, best, calen-
dar”; some relate to sports, “football, game, nfl, tour,
basketball, cup”; some about cars, “honda, ford, toy-
ota”. The reason why “miss” is in the seeds is that
there are many annual events about beauty contest
such as “miss america, miss california, miss korea”.
We use the seed list to generate the following
three features:
AveNumberTokensSeeds: number of tokens that is
in the seed list divided by number of tokens in the
query.
AveNumberTokensNotSeeds: number of tokens
that is not in the seed list divided by number of to-
kens in the query.
DiffNumberTokensSeeds: The difference of the
above two values.
</bodyText>
<equation confidence="0.9312375">
R(τ) = ENi τ
(xt − p)(xt+τ − p)
</equation>
<page confidence="0.935441">
1133
</page>
<figureCaption confidence="0.984624">
Figure 2: Frequency waveform(top) and corresponding autocorrelation curve (bottom) for query Oscar.
</figureCaption>
<figure confidence="0.998057454545454">
-0.2
0.8
0.6
0.4
0.2
1.6
1.4
1.2
0
1
1 15 29 43 57 71 85 99 113 127 141 155 169 183 197 211 225 239 253 267 281 295 309 323 337 351 365 379 393 407 421 435 449 463 477 491 505 519 533 547 561 575 589 603 617 631 645 659 673 687 701 715 729 743 757 771 785 799 813 827 841 855 869 883 897
</figure>
<sectionHeader confidence="0.781992" genericHeader="method">
4 Learning Approach for REQ
</sectionHeader>
<bodyText confidence="0.999378375">
The REQ classification is a typical machine learn-
ing task. Given M observed samples used for train-
ing data, {(x0,y0), (x1,y1), · · · , (xM,yM)} where xi is
a feature vector we developed in last section for a
given query. yi is the observation value, {+1, −1},
indicating the class of REQ and non-REQ. The task
is to find the class probability given an unknown fea-
ture vector, x′, that is,
</bodyText>
<equation confidence="0.839456">
p(y = c|x′), c = +1, −1. (1)
</equation>
<bodyText confidence="0.999427833333333">
There are a lot of machine learning methods ap-
plicable to implement Eq. 1. In this work, we
adopted three representative methods.
The first method is Naive Bayes method. This
method treats features independent. If x is enx-
tended into feature vector, x = {x0, x1, · · · , xN} then,
</bodyText>
<equation confidence="0.995659">
i=N
1
p(y = c|x) = Z p(c)
i=0
</equation>
<bodyText confidence="0.9999604">
The second method is SVM. In this work we used
the tool for our experiments, LIBSVM (Chang and
Lin, 2001). Because SVM is a well known approach
and widely used in many classification task, we skip
to describe how to use this tool. Readers can turn to
the reference for more details.
The third method is based on decision tree based
logistic regression model. The probability is given
by the formula below,
.
Gradient Boosted Decision Tree is an additive re-
gression algorithm consisting of an ensemble of
trees, fitted to current residuals, gradients of the loss
function, in a forward step-wise manner. It itera-
tively fits an additive model as
</bodyText>
<equation confidence="0.995554666666667">
T
ft(x) = Tt(x; O) + A Z fitTt(x; Ot)
t=1
</equation>
<bodyText confidence="0.998566571428571">
such that certain loss function L(yi, fT(x + i)) is
minimized, where Tt(x; Ot) is a tree at iteration t,
weighted by parameter fit, with a finite number of
parameters, Ot and A is the learning rate. At iteration
t, tree Tt(x; fi) is induced to fit the negative gradient
by least squares.
The optimal weights of trees fit are determined by
</bodyText>
<equation confidence="0.993979333333333">
N
L(yi, ft−1(xi) + fiT(xi, B))
i
</equation>
<bodyText confidence="0.99870725">
Each node in the trees represents a split on a fea-
ture. The tuneable parameters in such a machine-
learnt model include the number of leaf nodes in
each tree, the relative contribution of score from
each tree called the shrinkage, and total number of
shallow decision trees.
The relative importance of a feature Si, in such
forests of decision trees, is aggregated over all the
</bodyText>
<equation confidence="0.998657666666667">
1
p(y = c|x) = (2)
1 + e−f(x)
</equation>
<bodyText confidence="0.924747">
We employ Gradient Boosted Decision Tree algo-
rithm (Friedman, 2001) to learn the function f(X)
</bodyText>
<equation confidence="0.9878355">
p(xi|c)
fit = argminfi
</equation>
<page confidence="0.937929">
1134
</page>
<bodyText confidence="0.965163">
m shallow decision trees (Breiman et al., 1984) as
follows:
</bodyText>
<equation confidence="0.9806205">
wl ∗ wr(ylyr)2I(vt = i) (3)
wl + wr
</equation>
<bodyText confidence="0.999777333333333">
where vt is the feature on which a split occurs, yl
and yr are the mean regression responses from the
right, and left sub-tree, and wl and wr are the corre-
sponding weights to the means, as measured by the
number of training examples traversing the left and
right sub-trees.
</bodyText>
<sectionHeader confidence="0.992805" genericHeader="method">
5 REQ Learner Evaluation
</sectionHeader>
<bodyText confidence="0.999960294117647">
We collected 6,000 queries labeled as either Recur-
rent or Non-recurrent by professional human edi-
tors. The 6,000 queries were sampled from Implicit
Timestamp queries according to frequency distribu-
tion to be representative. We split the queries into
5,000 for training and 1,000 for test. For each query,
we calculated features’ values as described in Sec-
tion 3.
The Naive Bayes method used single Gaussian
function for each independent feature. Mean and
variance were calculated from the training data.
As for LIBSVM, we used C-SVC, linear function
as kernel and 1.0 of shrinkage.
The parameters used in the regression model were
20 of trees, 20 of nodes and 0.8 of learning rate
(shrinkage).
The test results are shown in Fig. 3, recall-
precision curve. We set a series of threshold to the
probability of c = +1 calculated by Eq. 1 so that
we can get the point values of recall and precision in
Fig. 3. For example, if we set a threshold of 0.6, a
query with a probability larger than 0.6 is classified
as REQ. Otherwise, it is non-REQ. The precision
is a measure of correctly classified REQ queries di-
vided by all classified REQ queries. The recall is a
measure of correctly classified REQ queries divided
by all REQ queries in test data.
In addition to the three plots, we also show the
results using only one feature, ExplicitQueryRatio,
for comparison with the classification method used
by (Zhang et al., 2009).All the three models us-
ing all features performed better than the existing
method using ExplicitQueryRatio. The highest im-
provement was achieved by GBDT regression tree
</bodyText>
<figure confidence="0.7733">
0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
</figure>
<figureCaption confidence="0.9994895">
Figure 3: Comparison of precision and recall rate be-
tween our method and the existing method.
</figureCaption>
<bodyText confidence="0.995149555555556">
model. The results of Naive Bayes were lower than
SVM and GBDTree. This model is weaker because
it treats features independently. Typically SVMs and
GBDT gives comparable results on a large class of
problems. Since for this task we use features from
different sources, the feature values are designed to
have larger dynamic range, which is better handled
by GBDT.
The features’ importance ranked by Equation 3
is shown in Table 2. We list the top 10 features.
The No.1 important feature is ExplicitQueryRatio.
The second and seventh features are from search ses-
sion analysis by counting users who changed queries
from Implicit Timestamp to Explicit Timestamp.
This is a strong source of features. The time se-
ries analysis feature is ranked No.3. Calculation of
this feature needs two years query log to be much
more effective, but we didn’t get so large data for
many queries. One of the features from recurrent
event seed list is ranked No.4. This is also an impor-
tant feature source. The ChiSquareYearDist feature
is ranked 5th, that proves the recurrent event query
frequency has a statistical distribution pattern over
years. TitleYearTop30 and TitleYearTop10 that are
derived from scraping results are ranked the 9th and
10th important.
Fig. 4 shows the distribution of feature values for
</bodyText>
<figure confidence="0.998466375">
ExplicitQueryRatio
GBDTree
Naive Bayes
SVM
S2 i = 1 M �M L−1�
m=1 n=1
Precision 0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
</figure>
<page confidence="0.889857">
1135
</page>
<table confidence="0.888115272727273">
Feature Rank Score
ExplicitQueryRatio 1 100
NormalizedUserSwitch 2 71.7
AutoCorrelation 3 54.0
AveNumberTokenSeeds 4 48.7
ChiSquareYearDist 5 36.3
YearUrlFPCTR 6 19.1
UserSwitch 7 11.7
QueryDailyFreq 8 10.7
TitleYearTop30 9 10.6
TitleYearTop10 10 5.8
</table>
<tableCaption confidence="0.736016">
Table 2: Top 10 most important features: rank and im-
portance score (100 is maximum)
</tableCaption>
<figure confidence="0.9937429">
1
2
3
4
5
6
7
8
9
10
</figure>
<figureCaption confidence="0.9896945">
Figure 4: Feature value distribution of all data
(blue=REQ, red=non-REQ)
</figureCaption>
<bodyText confidence="0.9975615">
each sample of the 6,000 data, where each point rep-
resents a query and each line represents a feature’s
value for all queries. One point is a query. The fea-
tures are ordered according to feature importance of
Table 2. The “blue” points indicate REQ queries and
the “red” points, non-REQ queries. Some features
are continuous like the 1st and 2nd. Some feature
values are discrete like the last two indicating Ti-
tleYearTop30 and TitleYearTop10. There are “red”
samples in the 4th feature but overlapped with and
covered by “blue” samples visually.
In the Table 3, we show F-Measure values as we
gradually added features from the feature, Explicit-
QueryRatio, according to feature importance in Ta-
ble 2. We listed the F-Measure values under three
threshold, 0.6, 0.7 and 0.8. Higher threshold will in-
crease classifier precision rate but reduce recall rate.
F-Measure is a metric combining precision rate and
recall rate. It is clearly observed that the classifier
performance is improved as more features are used.
</bodyText>
<table confidence="0.999156384615385">
Threshold
Feature 0.6 0.7 0.8
ExplicitQueryRatio 0.833 0.833 0.752
+NormalizedUserSwitch 0.840 0.837 0.791
+AutoCorrelation 0.850 0.839 0.823
+AveNumberTokenSeeds 0.857 0.854 0.834
+ChiSquareYearDist 0.857 0.864 0.839
+YearUrlFPCTR 0.869 0.867 0.837
+UserSwitch 0.862 0.862 0.846
+QueryDailyFreq 0.860 0.852 0.847
+TitleYearTop30 0.854 0.853 0.843
+TitleYearTop10 0.858 0.861 0.852
+All 0.876 0.867 0.862
</table>
<tableCaption confidence="0.9952755">
Table 3: F-Measures as varying thresholds by adding top
features.
</tableCaption>
<bodyText confidence="0.938012777777778">
Query Probability
ncaa men’s basketball tournament 0.999
bmw 328i sedan reviews 0.999
new apple iphone release 0.932
sigir 0.920
new york weather in april 0.717
academy awards reviews 0.404
google ipo 0.120
adidas jp 0.082
</bodyText>
<tableCaption confidence="0.8863605">
Table 4: Probabilities of example queries by GBDT tree
classifier
</tableCaption>
<bodyText confidence="0.999940363636364">
Some query examples, and their scores from our
model are listed in Table 4. The last two exam-
ples, google ipo and adidas jp, have very low values,
and are not REQs. The first four queries are typical
REQs. They have higher values of features Explicit-
QueryRatio,Normalized UserSwitch and YearUrlF-
PCTR. Although both new apple iphone release re-
views and academy awards reviews are about re-
views, academy awards reviews has lower value
of NormalizedUserSwitch and ChiSquareYearDist
could be the reason for a lower score.
</bodyText>
<sectionHeader confidence="0.998371" genericHeader="method">
6 Web Search Ranking
</sectionHeader>
<bodyText confidence="0.99979">
In this section, we use the approach proposed
by (Zhang et al., 2009) to test the REQ classifier
for Web search ranking. In their approach, search
ranking is altered by boosting pages with most re-
cent year if the query is a REQ. The year indicator
</bodyText>
<page confidence="0.970016">
1136
</page>
<table confidence="0.999188846153846">
DCG@5 DCG@1
bucket #(query) Organic Our’s % over Organic Organic Ours % over Organic
[0.0,0.1] 59 6.87 6.96 1.48(-2.3) 4.08 4.19 2.69(-1.07)
[0.1,0.2] 76 5.86 6.01 2.52(0.98) 2.88 2.91 1.14(1.69)
[0.2,0.3] 85 6.33 6.41 1.24(2.12) 3.7 3.7 0.0(0.8)
[0.3,0.4] 75 5.18 5.24 1.18(-0.7) 2.92 2.95 1.14(1.37)
[0.4,0.5] 78 4.96 4.82 -2.84(-1.35) 2.5 2.42 -3.06(0)
[0.5,0.6] 84 5.4 5.37 -0.45(-0.3) 2.82 2.85 1.05(-1.5)
[0.6,0.7] 78 4.78) 5.19) 8.42(3.64) 2.56 2.83 10.75(4.1)
[0.7,0.8] 80 4.45 4.60 3.41(3.19) 2.21 2.26 1.98(2.8)
[0.8,0.9] 78 4.81 4.96 3.15(4.79) 2.32 2.33 0.55(0.65)
[0.9,1.0] 107 5.08 5.50 8.41*(4.41) 2.64 3.09 16.78*(1.36)
[0.0,1.0] 800 5.33 5.47 2.74*(2.17) 2.83 2.93 3.6*(1.26)
</table>
<tableCaption confidence="0.907321333333333">
Table 5: REQ learner improves search engine organic results. The numbers in the brackets are by Zhang’s methods.
Direct comparison with Zhang’s method is valid only in the last line, using all queries. A sign “∗” indicates statistical
significance (p-value&lt;0.05)
</tableCaption>
<bodyText confidence="0.997753432432432">
can be detected either from title or URL of the re-
sult. For clarity, we re-write their ranking function
as below,
F(q, d) = R(q, d) + [e(do, dn) + k]eλα(q)
where the ranking function, F(q, d), consists of
two parts: the base function R(q, d) plus boosting.
If the query q is not a REQ, boosting is set to zero.
Otherwise, boosting is decided by e(do, dn), k, λ and
α(q). e(do, dn) is the difference of base ranking score
between the oldest page and the newest page. If the
newest page has a lower ranking score than the old-
est page, then the difference is added to the newest
page to promote the ranking of the newest page.
α(q) is the confidence score of a REQ query. It is
the value of Eq. 1. λ and k are two empirical param-
eters. (Zhang et al., 2009)’s work has experimented
the effects of using different value of λ and k (λ = 0
equals to no discounts for ranking adjustment). We
used λ = 0.4 and k = 0.3 which were the best con-
figuration in (Zhang et al., 2009).
For evaluating our methods, we randomly ex-
tracted 800 queries from the Implicit Timestamp
queries. We scraped a commercial search engine us-
ing the 800 queries. We extracted the top five search
results for each query under three configures: or-
ganic search engine results, (Zhang et al., 2009)’s
method and ours using REQ classifier. We asked
human editors to judge all the scraped (query, url)
pairs. Editors assign five grades according to rel-
evance between query and articles: Perfect, Excel-
lent, Good, Fair, and Bad. For example, a “Perfect”
grade means the content of the url match exactly the
query intent.
We use Discounted Cumulative Gain
(DCG) (Jarvelin and Kekalainen, 2002) at rank k as
our primary evaluation metrics to measure retrieval
performance. DCG is defined as,
</bodyText>
<equation confidence="0.586759">
DCG@k =
</equation>
<bodyText confidence="0.999978466666667">
where r(i) E {0 ... 4} is the relevance grade of the ith
ranked document.
The Web search ranking results are shown in Ta-
ble 5. We used GBDT tree learning methods be-
cause it achieved the best results. We divided 800
test queries into 10 buckets according to the classi-
fier probability. The bucket, [0.0,0.1], contains the
query with a classifier probability greater than 0 but
less than 0.1. Our results are compared with organic
search results, but we also show the improvements
over search organic by (Zhang et al., 2009) in the
brackets. Because Zhang’s approach output differ-
ent classifier values from Ours for the same query,
buckets of the same range in the Table contain dif-
ferent queries. Hence, it is inappropriate to compare
</bodyText>
<equation confidence="0.999893">
k 2r(i) − 1
i=1 log2(1 + i)
</equation>
<page confidence="0.976376">
1137
</page>
<bodyText confidence="0.99973075">
Zhang’s with Ours for the same buckets except the
last row where we used all the queries.
Our classifier’s overall performance is much bet-
ter than the organic search results. We achieved
2.74% DCG@5 gain and 3.6% DCG@1 gain over
organic search for all queries. The gains are higher
than (Zhang et al., 2009)’s results with regards to
improvement over organic results. By direct com-
parison, Ours was 2.7% better than Zhangs signif-
icantly in terms of DCG@1 by Wilcoxon signifi-
cant test. DCG@5 is 1.1% better, but not signifi-
cant. The table also show that the higher buckets
with higher probability achieved higher DCG gain
than the lower buckets overall. Our approach ob-
served 16.78% DCG@1 gain for bucket [0.9,1.0].
This shows that our methods are very effective.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999954375">
We found most of REQ are long tail queries that
pose a major challenge to Web search. We have
demonstrated learning REQ is important for Web
search. this type of queries can’t be solved in tra-
ditional ranking method. We found building a REQ
classifier was a good solution. Our work described
using machine learning method to build REQ clas-
sifier. Our proposed methods are novel compar-
ing with traditional query classification methods.
We identified and developed features from query
log, search session, click and time series analysis.
We applied several ML approaches including Naive
Bayes, SVM and GBDT tree to implement REQ
learner. Finally, we show through ranking experi-
ments that the methods we proposed are very effec-
tive and beneficial for search engine ranking.
</bodyText>
<sectionHeader confidence="0.998022" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998479">
We express our thanks to who have assisted us
to complete this work, especially, to Fumiaki Ya-
maoka, Toru Shimizu, Yoshinori Kobayashi, Mit-
suharu Makita, Garrett Kaminaga, Zhuoran Chen.
</bodyText>
<sectionHeader confidence="0.999612" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999891576271186">
R. Baeza-Yates, F. Saint-Jean, and C. Castillo. 2002.
Web dynamics, age and page qualit. String Process-
ing and Information Retrieval, pages 453–461.
Steven M. Beitzel, Eric C. Jensen, Ophir Frieder, David
Grossman, David D. Lewis, Abdur Chowdhury, and
Aleksandr Kolcz. 2005. Automatic web query classi-
fication using labeled and unlabeled training data. In
SIGIR ’05, pages 581–582.
K. Berberich, M. Vazirgiannis, and G. Weikum. 2005.
Time-aware authority rankings. Internet Math,
2(3):301–332.
L. Breiman, J. Friedman, R. Olshen, and C. Stone. 1984.
Classification and Regression Trees. Wadsworth and
Brooks, Monterey, CA.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Proceedings of
International Conference on World Wide Web.
Andrei Z. Broder, Marcus Fontoura, Evgeniy
Gabrilovich, Amruta Joshi, Vanja Josifovski, and
Tong Zhang. Robust classification of rare queries
using web knowledge. In SIGIR ’07, pages 231–238.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines. Software avail-
able at http://www.csie.ntu.edu.tw/ cjlin/libsvm.
J. Cho, S. Roy, and R. Adams. 2005. Page quality: In
search of an unbiased web ranking. Proc. ofACMSIG-
MOD Conference.
F. Diaz. 2009. Integration of news content into web re-
sults. Proceedings of the Second ACM International
Conference on Web Search and Data Mining (WSDM),
pages 182–191.
Anlei Dong, Yi Chang, Zhaohui Zheng, Gilad Mishne,
Jing Bai, Ruiqiang Zhang, Karolina Buchner, Ciya
Liao, and Fernando Diaz. 2010a. Towards recency
ranking in web search. Proceedings of the Third ACM
International Conference on Web Search and Data
Mining (WSDM), pages 11–20.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010b. Time is of the essence: im-
proving recency ranking using twitter data. 19th Inter-
national World Wide Web Conference (WWW), pages
331–340.
Jonathan L. Elsas and Susan T. Dumais. 2010. Lever-
aging temporal dynamics of document content in rele-
vance ranking. In WSDM, pages 1–10.
J. H. Friedman. 2001. Greedy function approximation:
A gradient boosting machine. Annals of Statistics,
29(5):1189–1232.
Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20:2002.
K. Sparck Jones, S. Walker, and S. E. Robertson. 2000.
A probabilistic model of information retrieval: devel-
opment and comparative experiments. Inf. Process.
Manage., 36(6):779–808.
A. C. Knig, M. Gamon, and Q. Wu. 2009. Click-through
prediction for news queries. Proc. of SIGIR, pages
347–354.
</reference>
<page confidence="0.865121">
1138
</page>
<reference confidence="0.99965303030303">
Ying Li, Zijian Zheng, and Honghua (Kathy) Dai.
2005. Kdd cup-2005 report: facing a great challenge.
SIGKDD Explor. Newsl., 7(2):91–99.
Xiao Li, Ye yi Wang, and Alex Acero. 2008. Learning
query intent from regularized click graphs. In In SI-
GIR 2008, pages 339–346. ACM.
Donald Metzler, Rosie Jones, Fuchun Peng, and Ruiqiang
Zhang. 2009. Improving search relevance for im-
plicitly temporal queries. In SIGIR ’09: Proceed-
ings of the 32nd international ACM SIGIR conference
on Research and development in information retrieval,
pages 700–701.
S. Nunes. 2007. Exploring temporal evidence in web
information retrieval. BCS IRSG Symposium: Future
Directions in Information Access.
S. Pandey, S. Roy, C. Olston, J. Cho, and S. Chakrabarti.
2005. Shuffling a stacked deck: The case for partially
randomized ranking of search engine results. VLDB.
G. Salton and M. J. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, NY.
Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey Junfeng
Pan, Kangheng Wu, Jie Yin, and Qiang Yang. 2005.
Q2c@ust: our winning solution to query classification
in kddcup 2005. SIGKDD Explor. Newsl., 7(2):100–
110.
Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey Junfeng Pan,
Kangheng Wu, Jie Yin, and Qiang Yang. 2006. Query
enrichment for web-query classification. ACM Trans.
Inf. Syst., 24(3):320–352.
Ruiqiang Zhang, Yi Chang, Zhaohui Zheng, Donald
Metzler, and Jian-yun Nie. 2009. Search result
re-ranking by feedback control adjustment for time-
sensitive query. In HLT-NAACL ’09, pages 165–168.
</reference>
<page confidence="0.9968">
1139
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.558057">
<title confidence="0.999335">Learning Recurrent Event Queries for Web Search</title>
<author confidence="0.8632955">Zhang Konda Kolari Chang</author>
<affiliation confidence="0.858145">Yahoo!</affiliation>
<address confidence="0.92631">701 First Avenue, Sunnyvale, CA94089</address>
<abstract confidence="0.996385896551724">Recurrent event queries (REQ) constitute a special class of search queries occurring at regular, predictable time intervals. The freshness of documents ranked for such queries is generally of critical importance. REQ forms a significant volume, as much as 6% of query traffic received by search engines. In this work, we develop an improved REQ classifier that could provide significant improvements in addressing this problem. We analyze REQ queries, and develop novel features from multiple sources, and evaluate them using machine learning techniques. From historical query logs, we develop features utilizing query frequency, click information, and user intent dynamics within a search session. We also develop temporal features by time series analysis from query frequency. Other generated features include word matching with recurrent event seed words and time sensitivity of search result set. We use Naive Bayes, SVM and decision tree based logistic regression model to train REQ classifier. The results on test data show that our models outperformed baseline approach significantly. Experiments on a commercial Web search engine also show significant gains in overall relevance, and thus overall user experience.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Baeza-Yates</author>
<author>F Saint-Jean</author>
<author>C Castillo</author>
</authors>
<title>Web dynamics, age and page qualit. String Processing and Information Retrieval,</title>
<date>2002</date>
<pages>453--461</pages>
<contexts>
<context position="6199" citStr="Baeza-Yates et al., 2002" startWordPosition="1007" endWordPosition="1010">d multiple classifiers integrated by ensemble method. The difficulties for query classification are from short queries, lack of labeled data, and query sense ambiguity. Most popular studies use query log, web search results, unlabeled data to enrich query classification (Shen et al., 2006; Beitzel et al., 2005), or use document classification to predict query classification (Broder et al., ). General query classification is also studied for query intent detection by (Li et al., 2008). There are many prior works to study the time sensitivity issue in web search. For example, BaezaYates et al. (Baeza-Yates et al., 2002) studied the relation between the web dynamics, structure and page quality, and demonstrated that PageRank is biased against new pages. In T-Rank Light and T-Rank algorithms (Berberich et al., 2005), both activity (i.e., update rates) and freshness (i.e., timestamps of most recent updates) of pages and links are taken into account for link analysis. Cho et al. (Cho et al., 2005) proposed a page quality ranking function in order to alleviate the problem of popularity-based ranking, and they used the derivatives of PageRank to forecast future PageRank values for new pages. Pandey et al. (Pandey </context>
</contexts>
<marker>Baeza-Yates, Saint-Jean, Castillo, 2002</marker>
<rawString>R. Baeza-Yates, F. Saint-Jean, and C. Castillo. 2002. Web dynamics, age and page qualit. String Processing and Information Retrieval, pages 453–461.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven M Beitzel</author>
<author>Eric C Jensen</author>
<author>Ophir Frieder</author>
<author>David Grossman</author>
<author>David D Lewis</author>
<author>Abdur Chowdhury</author>
<author>Aleksandr Kolcz</author>
</authors>
<title>Automatic web query classification using labeled and unlabeled training data.</title>
<date>2005</date>
<booktitle>In SIGIR ’05,</booktitle>
<pages>581--582</pages>
<contexts>
<context position="5886" citStr="Beitzel et al., 2005" startWordPosition="954" endWordPosition="957">ication. For general query classification, the task is to assign a Web search query to one or more predefined categories based on its topics. In the query classification contest in KDDCUP 2005 (Li et al., 2005), seven categories and 67 sub-categories were defined. The winning solution (Shen et al., 2005) used multiple classifiers integrated by ensemble method. The difficulties for query classification are from short queries, lack of labeled data, and query sense ambiguity. Most popular studies use query log, web search results, unlabeled data to enrich query classification (Shen et al., 2006; Beitzel et al., 2005), or use document classification to predict query classification (Broder et al., ). General query classification is also studied for query intent detection by (Li et al., 2008). There are many prior works to study the time sensitivity issue in web search. For example, BaezaYates et al. (Baeza-Yates et al., 2002) studied the relation between the web dynamics, structure and page quality, and demonstrated that PageRank is biased against new pages. In T-Rank Light and T-Rank algorithms (Berberich et al., 2005), both activity (i.e., update rates) and freshness (i.e., timestamps of most recent updat</context>
</contexts>
<marker>Beitzel, Jensen, Frieder, Grossman, Lewis, Chowdhury, Kolcz, 2005</marker>
<rawString>Steven M. Beitzel, Eric C. Jensen, Ophir Frieder, David Grossman, David D. Lewis, Abdur Chowdhury, and Aleksandr Kolcz. 2005. Automatic web query classification using labeled and unlabeled training data. In SIGIR ’05, pages 581–582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Berberich</author>
<author>M Vazirgiannis</author>
<author>G Weikum</author>
</authors>
<title>Time-aware authority rankings.</title>
<date>2005</date>
<journal>Internet Math,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="6397" citStr="Berberich et al., 2005" startWordPosition="1039" endWordPosition="1042">log, web search results, unlabeled data to enrich query classification (Shen et al., 2006; Beitzel et al., 2005), or use document classification to predict query classification (Broder et al., ). General query classification is also studied for query intent detection by (Li et al., 2008). There are many prior works to study the time sensitivity issue in web search. For example, BaezaYates et al. (Baeza-Yates et al., 2002) studied the relation between the web dynamics, structure and page quality, and demonstrated that PageRank is biased against new pages. In T-Rank Light and T-Rank algorithms (Berberich et al., 2005), both activity (i.e., update rates) and freshness (i.e., timestamps of most recent updates) of pages and links are taken into account for link analysis. Cho et al. (Cho et al., 2005) proposed a page quality ranking function in order to alleviate the problem of popularity-based ranking, and they used the derivatives of PageRank to forecast future PageRank values for new pages. Pandey et al. (Pandey et al., 2005) studied the tradeoff between new page exploration and high-quality page exploitation, which is based on a ranking method to randomly promote some new pages so that they can accumulate </context>
</contexts>
<marker>Berberich, Vazirgiannis, Weikum, 2005</marker>
<rawString>K. Berberich, M. Vazirgiannis, and G. Weikum. 2005. Time-aware authority rankings. Internet Math, 2(3):301–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
<author>J Friedman</author>
<author>R Olshen</author>
<author>C Stone</author>
</authors>
<title>Classification and Regression Trees. Wadsworth and Brooks,</title>
<date>1984</date>
<location>Monterey, CA.</location>
<contexts>
<context position="22253" citStr="Breiman et al., 1984" startWordPosition="3777" endWordPosition="3780">termined by N L(yi, ft−1(xi) + fiT(xi, B)) i Each node in the trees represents a split on a feature. The tuneable parameters in such a machinelearnt model include the number of leaf nodes in each tree, the relative contribution of score from each tree called the shrinkage, and total number of shallow decision trees. The relative importance of a feature Si, in such forests of decision trees, is aggregated over all the 1 p(y = c|x) = (2) 1 + e−f(x) We employ Gradient Boosted Decision Tree algorithm (Friedman, 2001) to learn the function f(X) p(xi|c) fit = argminfi 1134 m shallow decision trees (Breiman et al., 1984) as follows: wl ∗ wr(ylyr)2I(vt = i) (3) wl + wr where vt is the feature on which a split occurs, yl and yr are the mean regression responses from the right, and left sub-tree, and wl and wr are the corresponding weights to the means, as measured by the number of training examples traversing the left and right sub-trees. 5 REQ Learner Evaluation We collected 6,000 queries labeled as either Recurrent or Non-recurrent by professional human editors. The 6,000 queries were sampled from Implicit Timestamp queries according to frequency distribution to be representative. We split the queries into 5,</context>
</contexts>
<marker>Breiman, Friedman, Olshen, Stone, 1984</marker>
<rawString>L. Breiman, J. Friedman, R. Olshen, and C. Stone. 1984. Classification and Regression Trees. Wadsworth and Brooks, Monterey, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a largescale hypertextual web search engine.</title>
<date>1998</date>
<booktitle>Proceedings of International Conference on World Wide Web.</booktitle>
<contexts>
<context position="3728" citStr="Brin and Page, 1998" startWordPosition="589" endWordPosition="592">earch results by considering many factors. Time is an important factor but not the most critical. The page’s ranking score mostly depends on other features such as tf-idf (Salton and McGill, 1983), BM25 (Jones 1129 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1129–1139, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Figure 1: A real problematic ranking result by Google for a REQ query, “EMNLP”. The EMNLP2010 page should be on the 1st position. et al., 2000), anchor text, historical clicks, pagerank (Brin and Page, 1998), and overall page quality. New pages about EMNLP2010 obtain less favorable feature values than the pages of 2009 earlier in terms of anchor text, click or pagerank because they have existed for a shorter time and haven’t accumulated sufficient popularity to make them stand out. Without special treatment, the new pages about “EMNLP2010” will typically not be ranked appropriately for the users. Typically, a recurrent event is associated with a root, and spawns a large set of queries. Oscar, for instance, is a recurrent event about the annual Academy Award. Based on this, queries like “oscar bes</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>S. Brin and L. Page. 1998. The anatomy of a largescale hypertextual web search engine. Proceedings of International Conference on World Wide Web.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Andrei Z Broder</author>
<author>Marcus Fontoura</author>
</authors>
<title>Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, and Tong Zhang. Robust classification of rare queries using web knowledge.</title>
<booktitle>In SIGIR ’07,</booktitle>
<pages>231--238</pages>
<marker>Broder, Fontoura, </marker>
<rawString>Andrei Z. Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, and Tong Zhang. Robust classification of rare queries using web knowledge. In SIGIR ’07, pages 231–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="20724" citStr="Chang and Lin, 2001" startWordPosition="3503" endWordPosition="3506">query. yi is the observation value, {+1, −1}, indicating the class of REQ and non-REQ. The task is to find the class probability given an unknown feature vector, x′, that is, p(y = c|x′), c = +1, −1. (1) There are a lot of machine learning methods applicable to implement Eq. 1. In this work, we adopted three representative methods. The first method is Naive Bayes method. This method treats features independent. If x is enxtended into feature vector, x = {x0, x1, · · · , xN} then, i=N 1 p(y = c|x) = Z p(c) i=0 The second method is SVM. In this work we used the tool for our experiments, LIBSVM (Chang and Lin, 2001). Because SVM is a well known approach and widely used in many classification task, we skip to describe how to use this tool. Readers can turn to the reference for more details. The third method is based on decision tree based logistic regression model. The probability is given by the formula below, . Gradient Boosted Decision Tree is an additive regression algorithm consisting of an ensemble of trees, fitted to current residuals, gradients of the loss function, in a forward step-wise manner. It iteratively fits an additive model as T ft(x) = Tt(x; O) + A Z fitTt(x; Ot) t=1 such that certain l</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cho</author>
<author>S Roy</author>
<author>R Adams</author>
</authors>
<title>Page quality: In search of an unbiased web ranking.</title>
<date>2005</date>
<booktitle>Proc. ofACMSIGMOD Conference.</booktitle>
<contexts>
<context position="6580" citStr="Cho et al., 2005" startWordPosition="1071" endWordPosition="1074">l., ). General query classification is also studied for query intent detection by (Li et al., 2008). There are many prior works to study the time sensitivity issue in web search. For example, BaezaYates et al. (Baeza-Yates et al., 2002) studied the relation between the web dynamics, structure and page quality, and demonstrated that PageRank is biased against new pages. In T-Rank Light and T-Rank algorithms (Berberich et al., 2005), both activity (i.e., update rates) and freshness (i.e., timestamps of most recent updates) of pages and links are taken into account for link analysis. Cho et al. (Cho et al., 2005) proposed a page quality ranking function in order to alleviate the problem of popularity-based ranking, and they used the derivatives of PageRank to forecast future PageRank values for new pages. Pandey et al. (Pandey et al., 2005) studied the tradeoff between new page exploration and high-quality page exploitation, which is based on a ranking method to randomly promote some new pages so that they can accumulate links quickly. More recently, Dong et al. (Dong et al., 2010a) 1130 proposed a machine-learned framework to improve ranking result freshness, in which novel features, modeling algorit</context>
</contexts>
<marker>Cho, Roy, Adams, 2005</marker>
<rawString>J. Cho, S. Roy, and R. Adams. 2005. Page quality: In search of an unbiased web ranking. Proc. ofACMSIGMOD Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Diaz</author>
</authors>
<title>Integration of news content into web results.</title>
<date>2009</date>
<booktitle>Proceedings of the Second ACM International Conference on Web Search and Data Mining (WSDM),</booktitle>
<pages>182--191</pages>
<contexts>
<context position="8853" citStr="Diaz, 2009" startWordPosition="1444" endWordPosition="1445">n in web search queries, and found 1.5% of queries containing temporal expression. Dong et al. (Dong et al., 2010a) proposed a breaking-news query classifier with high accuracy and reasonable coverage, which works not by modeling each individual topic and tracking it over time, but by modeling each discrete time slot, and comparing the models representing different time slots. The buzziness of a query is computed as the language model likelihood difference between different time slots. In this approach, both query log and news contents are exploited to compute language model likelihood. Diaz (Diaz, 2009) determined the newsworthiness of a query by predicting the probability of a user clicks on the news display of a query. In this framework, the data sources of both query log and news corpus are leveraged to compute contextual features. Furthermore, the online click feedback also plays a critical role for future click prediction. Konig et al. (Knig et al., 2009) estimated the click-through rate for dedicated news search result with a supervised model, which is to satisfy the requirement of adapting quickly to emerging news event. Some additional corpora such as blog crawl and Wikipedia is used</context>
</contexts>
<marker>Diaz, 2009</marker>
<rawString>F. Diaz. 2009. Integration of news content into web results. Proceedings of the Second ACM International Conference on Web Search and Data Mining (WSDM), pages 182–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anlei Dong</author>
<author>Yi Chang</author>
<author>Zhaohui Zheng</author>
<author>Gilad Mishne</author>
<author>Jing Bai</author>
<author>Ruiqiang Zhang</author>
<author>Karolina Buchner</author>
<author>Ciya Liao</author>
<author>Fernando Diaz</author>
</authors>
<title>Towards recency ranking in web search.</title>
<date>2010</date>
<booktitle>Proceedings of the Third ACM International Conference on Web Search and Data Mining (WSDM),</booktitle>
<pages>11--20</pages>
<contexts>
<context position="7057" citStr="Dong et al., 2010" startWordPosition="1150" endWordPosition="1153">eshness (i.e., timestamps of most recent updates) of pages and links are taken into account for link analysis. Cho et al. (Cho et al., 2005) proposed a page quality ranking function in order to alleviate the problem of popularity-based ranking, and they used the derivatives of PageRank to forecast future PageRank values for new pages. Pandey et al. (Pandey et al., 2005) studied the tradeoff between new page exploration and high-quality page exploitation, which is based on a ranking method to randomly promote some new pages so that they can accumulate links quickly. More recently, Dong et al. (Dong et al., 2010a) 1130 proposed a machine-learned framework to improve ranking result freshness, in which novel features, modeling algorithms and editorial guideline are used to deal with time sensitivities of queries and documents. In another work (Dong et al., 2010b), they use micro-blogging data (e.g., Twitter data) to detect fresh URLs. Novel and effective features are also extracted for fresh URLs so that ranking recency in web search is improved. Perhaps the most related work to this paper is the query classification approach used in (Zhang et al., 2009) and (Metzler et al., 2009), in which year qualif</context>
<context position="8355" citStr="Dong et al., 2010" startWordPosition="1365" endWordPosition="1368">ntaining a year stamp is an explicit YQQ; if the year stamp is removed from this YQQ, the remaining part of this query is also a YQQ, which is called implicit YQQ. Different ranking approaches were used in (Zhang et al., 2009) and (Metzler et al., 2009) where (Zhang et al., 2009) boosted pages of the most latest year while (Metzler et al., 2009) promoted pages of the most influential years. Similarly, Nunes et al. (Nunes, 2007) applied information extraction techniques to identify temporal expression in web search queries, and found 1.5% of queries containing temporal expression. Dong et al. (Dong et al., 2010a) proposed a breaking-news query classifier with high accuracy and reasonable coverage, which works not by modeling each individual topic and tracking it over time, but by modeling each discrete time slot, and comparing the models representing different time slots. The buzziness of a query is computed as the language model likelihood difference between different time slots. In this approach, both query log and news contents are exploited to compute language model likelihood. Diaz (Diaz, 2009) determined the newsworthiness of a query by predicting the probability of a user clicks on the news d</context>
</contexts>
<marker>Dong, Chang, Zheng, Mishne, Bai, Zhang, Buchner, Liao, Diaz, 2010</marker>
<rawString>Anlei Dong, Yi Chang, Zhaohui Zheng, Gilad Mishne, Jing Bai, Ruiqiang Zhang, Karolina Buchner, Ciya Liao, and Fernando Diaz. 2010a. Towards recency ranking in web search. Proceedings of the Third ACM International Conference on Web Search and Data Mining (WSDM), pages 11–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anlei Dong</author>
<author>Ruiqiang Zhang</author>
<author>Pranam Kolari</author>
<author>Jing Bai</author>
<author>Fernando Diaz</author>
<author>Yi Chang</author>
<author>Zhaohui Zheng</author>
<author>Hongyuan Zha</author>
</authors>
<title>Time is of the essence: improving recency ranking using twitter data.</title>
<date>2010</date>
<booktitle>19th International World Wide Web Conference (WWW),</booktitle>
<pages>331--340</pages>
<contexts>
<context position="7057" citStr="Dong et al., 2010" startWordPosition="1150" endWordPosition="1153">eshness (i.e., timestamps of most recent updates) of pages and links are taken into account for link analysis. Cho et al. (Cho et al., 2005) proposed a page quality ranking function in order to alleviate the problem of popularity-based ranking, and they used the derivatives of PageRank to forecast future PageRank values for new pages. Pandey et al. (Pandey et al., 2005) studied the tradeoff between new page exploration and high-quality page exploitation, which is based on a ranking method to randomly promote some new pages so that they can accumulate links quickly. More recently, Dong et al. (Dong et al., 2010a) 1130 proposed a machine-learned framework to improve ranking result freshness, in which novel features, modeling algorithms and editorial guideline are used to deal with time sensitivities of queries and documents. In another work (Dong et al., 2010b), they use micro-blogging data (e.g., Twitter data) to detect fresh URLs. Novel and effective features are also extracted for fresh URLs so that ranking recency in web search is improved. Perhaps the most related work to this paper is the query classification approach used in (Zhang et al., 2009) and (Metzler et al., 2009), in which year qualif</context>
<context position="8355" citStr="Dong et al., 2010" startWordPosition="1365" endWordPosition="1368">ntaining a year stamp is an explicit YQQ; if the year stamp is removed from this YQQ, the remaining part of this query is also a YQQ, which is called implicit YQQ. Different ranking approaches were used in (Zhang et al., 2009) and (Metzler et al., 2009) where (Zhang et al., 2009) boosted pages of the most latest year while (Metzler et al., 2009) promoted pages of the most influential years. Similarly, Nunes et al. (Nunes, 2007) applied information extraction techniques to identify temporal expression in web search queries, and found 1.5% of queries containing temporal expression. Dong et al. (Dong et al., 2010a) proposed a breaking-news query classifier with high accuracy and reasonable coverage, which works not by modeling each individual topic and tracking it over time, but by modeling each discrete time slot, and comparing the models representing different time slots. The buzziness of a query is computed as the language model likelihood difference between different time slots. In this approach, both query log and news contents are exploited to compute language model likelihood. Diaz (Diaz, 2009) determined the newsworthiness of a query by predicting the probability of a user clicks on the news d</context>
</contexts>
<marker>Dong, Zhang, Kolari, Bai, Diaz, Chang, Zheng, Zha, 2010</marker>
<rawString>Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and Hongyuan Zha. 2010b. Time is of the essence: improving recency ranking using twitter data. 19th International World Wide Web Conference (WWW), pages 331–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan L Elsas</author>
<author>Susan T Dumais</author>
</authors>
<title>Leveraging temporal dynamics of document content in relevance ranking.</title>
<date>2010</date>
<booktitle>In WSDM,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="9596" citStr="Elsas and Dumais, 2010" startWordPosition="1564" endWordPosition="1567"> In this framework, the data sources of both query log and news corpus are leveraged to compute contextual features. Furthermore, the online click feedback also plays a critical role for future click prediction. Konig et al. (Knig et al., 2009) estimated the click-through rate for dedicated news search result with a supervised model, which is to satisfy the requirement of adapting quickly to emerging news event. Some additional corpora such as blog crawl and Wikipedia is used for buzziness inference. Compared with (Diaz, 2009), different feature and learning algorithms are used. Elsas et al. (Elsas and Dumais, 2010) studied improving relevance ranking by detecting document content change to leverage temporal information. 3 Feature Generation To better understand our work, we first introduce three terms. We subdivide all raw queries in query log into three categories: Explicit Timestamp, Implicit Timestamp, and No Timestamp. An Explicit Timestamp query contains at least one token being a time indicator. For example, emnlp 2010, 2007 December holiday calendar, amsterdam weather summer 2009, Google Q1 reports 2010. These queries are considered to conatin time indicators, because we can regard {2010, 2007, 2</context>
</contexts>
<marker>Elsas, Dumais, 2010</marker>
<rawString>Jonathan L. Elsas and Susan T. Dumais. 2010. Leveraging temporal dynamics of document content in relevance ranking. In WSDM, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Friedman</author>
</authors>
<title>Greedy function approximation: A gradient boosting machine.</title>
<date>2001</date>
<journal>Annals of Statistics,</journal>
<volume>29</volume>
<issue>5</issue>
<contexts>
<context position="22150" citStr="Friedman, 2001" startWordPosition="3761" endWordPosition="3762">is induced to fit the negative gradient by least squares. The optimal weights of trees fit are determined by N L(yi, ft−1(xi) + fiT(xi, B)) i Each node in the trees represents a split on a feature. The tuneable parameters in such a machinelearnt model include the number of leaf nodes in each tree, the relative contribution of score from each tree called the shrinkage, and total number of shallow decision trees. The relative importance of a feature Si, in such forests of decision trees, is aggregated over all the 1 p(y = c|x) = (2) 1 + e−f(x) We employ Gradient Boosted Decision Tree algorithm (Friedman, 2001) to learn the function f(X) p(xi|c) fit = argminfi 1134 m shallow decision trees (Breiman et al., 1984) as follows: wl ∗ wr(ylyr)2I(vt = i) (3) wl + wr where vt is the feature on which a split occurs, yl and yr are the mean regression responses from the right, and left sub-tree, and wl and wr are the corresponding weights to the means, as measured by the number of training examples traversing the left and right sub-trees. 5 REQ Learner Evaluation We collected 6,000 queries labeled as either Recurrent or Non-recurrent by professional human editors. The 6,000 queries were sampled from Implicit T</context>
</contexts>
<marker>Friedman, 2001</marker>
<rawString>J. H. Friedman. 2001. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5):1189–1232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalervo Jarvelin</author>
<author>Jaana Kekalainen</author>
</authors>
<title>Cumulated gain-based evaluation of IR techniques.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<pages>20--2002</pages>
<contexts>
<context position="31337" citStr="Jarvelin and Kekalainen, 2002" startWordPosition="5292" endWordPosition="5295">0 queries from the Implicit Timestamp queries. We scraped a commercial search engine using the 800 queries. We extracted the top five search results for each query under three configures: organic search engine results, (Zhang et al., 2009)’s method and ours using REQ classifier. We asked human editors to judge all the scraped (query, url) pairs. Editors assign five grades according to relevance between query and articles: Perfect, Excellent, Good, Fair, and Bad. For example, a “Perfect” grade means the content of the url match exactly the query intent. We use Discounted Cumulative Gain (DCG) (Jarvelin and Kekalainen, 2002) at rank k as our primary evaluation metrics to measure retrieval performance. DCG is defined as, DCG@k = where r(i) E {0 ... 4} is the relevance grade of the ith ranked document. The Web search ranking results are shown in Table 5. We used GBDT tree learning methods because it achieved the best results. We divided 800 test queries into 10 buckets according to the classifier probability. The bucket, [0.0,0.1], contains the query with a classifier probability greater than 0 but less than 0.1. Our results are compared with organic search results, but we also show the improvements over search org</context>
</contexts>
<marker>Jarvelin, Kekalainen, 2002</marker>
<rawString>Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems, 20:2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck Jones</author>
<author>S Walker</author>
<author>S E Robertson</author>
</authors>
<title>A probabilistic model of information retrieval: development and comparative experiments.</title>
<date>2000</date>
<journal>Inf. Process. Manage.,</journal>
<volume>36</volume>
<issue>6</issue>
<marker>Jones, Walker, Robertson, 2000</marker>
<rawString>K. Sparck Jones, S. Walker, and S. E. Robertson. 2000. A probabilistic model of information retrieval: development and comparative experiments. Inf. Process. Manage., 36(6):779–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Knig</author>
<author>M Gamon</author>
<author>Q Wu</author>
</authors>
<title>Click-through prediction for news queries.</title>
<date>2009</date>
<booktitle>Proc. of SIGIR,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="9217" citStr="Knig et al., 2009" startWordPosition="1504" endWordPosition="1507"> different time slots. The buzziness of a query is computed as the language model likelihood difference between different time slots. In this approach, both query log and news contents are exploited to compute language model likelihood. Diaz (Diaz, 2009) determined the newsworthiness of a query by predicting the probability of a user clicks on the news display of a query. In this framework, the data sources of both query log and news corpus are leveraged to compute contextual features. Furthermore, the online click feedback also plays a critical role for future click prediction. Konig et al. (Knig et al., 2009) estimated the click-through rate for dedicated news search result with a supervised model, which is to satisfy the requirement of adapting quickly to emerging news event. Some additional corpora such as blog crawl and Wikipedia is used for buzziness inference. Compared with (Diaz, 2009), different feature and learning algorithms are used. Elsas et al. (Elsas and Dumais, 2010) studied improving relevance ranking by detecting document content change to leverage temporal information. 3 Feature Generation To better understand our work, we first introduce three terms. We subdivide all raw queries </context>
</contexts>
<marker>Knig, Gamon, Wu, 2009</marker>
<rawString>A. C. Knig, M. Gamon, and Q. Wu. 2009. Click-through prediction for news queries. Proc. of SIGIR, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Li</author>
<author>Zijian Zheng</author>
<author>Honghua Dai</author>
</authors>
<title>Kdd cup-2005 report: facing a great challenge.</title>
<date>2005</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>7</volume>
<issue>2</issue>
<contexts>
<context position="5475" citStr="Li et al., 2005" startWordPosition="890" endWordPosition="893">r where all the features are integrated by machine learning models. We use Naive Bayes, SVM and decision tree based logistic regression models. These models are described in Section 4. Our experiments for REQ classifier and Web search ranking are detailed in Section 5 and 6. 2 Related Work We found our work were related to two other problems: general query classification and time-sensitive query classification. For general query classification, the task is to assign a Web search query to one or more predefined categories based on its topics. In the query classification contest in KDDCUP 2005 (Li et al., 2005), seven categories and 67 sub-categories were defined. The winning solution (Shen et al., 2005) used multiple classifiers integrated by ensemble method. The difficulties for query classification are from short queries, lack of labeled data, and query sense ambiguity. Most popular studies use query log, web search results, unlabeled data to enrich query classification (Shen et al., 2006; Beitzel et al., 2005), or use document classification to predict query classification (Broder et al., ). General query classification is also studied for query intent detection by (Li et al., 2008). There are m</context>
</contexts>
<marker>Li, Zheng, Dai, 2005</marker>
<rawString>Ying Li, Zijian Zheng, and Honghua (Kathy) Dai. 2005. Kdd cup-2005 report: facing a great challenge. SIGKDD Explor. Newsl., 7(2):91–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Li</author>
<author>Ye yi Wang</author>
<author>Alex Acero</author>
</authors>
<title>Learning query intent from regularized click graphs.</title>
<date>2008</date>
<booktitle>In In SIGIR 2008,</booktitle>
<pages>339--346</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6062" citStr="Li et al., 2008" startWordPosition="982" endWordPosition="985">KDDCUP 2005 (Li et al., 2005), seven categories and 67 sub-categories were defined. The winning solution (Shen et al., 2005) used multiple classifiers integrated by ensemble method. The difficulties for query classification are from short queries, lack of labeled data, and query sense ambiguity. Most popular studies use query log, web search results, unlabeled data to enrich query classification (Shen et al., 2006; Beitzel et al., 2005), or use document classification to predict query classification (Broder et al., ). General query classification is also studied for query intent detection by (Li et al., 2008). There are many prior works to study the time sensitivity issue in web search. For example, BaezaYates et al. (Baeza-Yates et al., 2002) studied the relation between the web dynamics, structure and page quality, and demonstrated that PageRank is biased against new pages. In T-Rank Light and T-Rank algorithms (Berberich et al., 2005), both activity (i.e., update rates) and freshness (i.e., timestamps of most recent updates) of pages and links are taken into account for link analysis. Cho et al. (Cho et al., 2005) proposed a page quality ranking function in order to alleviate the problem of pop</context>
</contexts>
<marker>Li, Wang, Acero, 2008</marker>
<rawString>Xiao Li, Ye yi Wang, and Alex Acero. 2008. Learning query intent from regularized click graphs. In In SIGIR 2008, pages 339–346. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>Rosie Jones</author>
<author>Fuchun Peng</author>
<author>Ruiqiang Zhang</author>
</authors>
<title>Improving search relevance for implicitly temporal queries.</title>
<date>2009</date>
<booktitle>In SIGIR ’09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>700--701</pages>
<contexts>
<context position="7635" citStr="Metzler et al., 2009" startWordPosition="1242" endWordPosition="1245">re recently, Dong et al. (Dong et al., 2010a) 1130 proposed a machine-learned framework to improve ranking result freshness, in which novel features, modeling algorithms and editorial guideline are used to deal with time sensitivities of queries and documents. In another work (Dong et al., 2010b), they use micro-blogging data (e.g., Twitter data) to detect fresh URLs. Novel and effective features are also extracted for fresh URLs so that ranking recency in web search is improved. Perhaps the most related work to this paper is the query classification approach used in (Zhang et al., 2009) and (Metzler et al., 2009), in which year qualified queries (YQQs) are detected based on heuristic rules. For example, a query containing a year stamp is an explicit YQQ; if the year stamp is removed from this YQQ, the remaining part of this query is also a YQQ, which is called implicit YQQ. Different ranking approaches were used in (Zhang et al., 2009) and (Metzler et al., 2009) where (Zhang et al., 2009) boosted pages of the most latest year while (Metzler et al., 2009) promoted pages of the most influential years. Similarly, Nunes et al. (Nunes, 2007) applied information extraction techniques to identify temporal ex</context>
</contexts>
<marker>Metzler, Jones, Peng, Zhang, 2009</marker>
<rawString>Donald Metzler, Rosie Jones, Fuchun Peng, and Ruiqiang Zhang. 2009. Improving search relevance for implicitly temporal queries. In SIGIR ’09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 700–701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nunes</author>
</authors>
<title>Exploring temporal evidence in web information retrieval.</title>
<date>2007</date>
<booktitle>BCS IRSG Symposium: Future Directions in Information Access.</booktitle>
<contexts>
<context position="8169" citStr="Nunes, 2007" startWordPosition="1339" endWordPosition="1340">lassification approach used in (Zhang et al., 2009) and (Metzler et al., 2009), in which year qualified queries (YQQs) are detected based on heuristic rules. For example, a query containing a year stamp is an explicit YQQ; if the year stamp is removed from this YQQ, the remaining part of this query is also a YQQ, which is called implicit YQQ. Different ranking approaches were used in (Zhang et al., 2009) and (Metzler et al., 2009) where (Zhang et al., 2009) boosted pages of the most latest year while (Metzler et al., 2009) promoted pages of the most influential years. Similarly, Nunes et al. (Nunes, 2007) applied information extraction techniques to identify temporal expression in web search queries, and found 1.5% of queries containing temporal expression. Dong et al. (Dong et al., 2010a) proposed a breaking-news query classifier with high accuracy and reasonable coverage, which works not by modeling each individual topic and tracking it over time, but by modeling each discrete time slot, and comparing the models representing different time slots. The buzziness of a query is computed as the language model likelihood difference between different time slots. In this approach, both query log and</context>
</contexts>
<marker>Nunes, 2007</marker>
<rawString>S. Nunes. 2007. Exploring temporal evidence in web information retrieval. BCS IRSG Symposium: Future Directions in Information Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pandey</author>
<author>S Roy</author>
<author>C Olston</author>
<author>J Cho</author>
<author>S Chakrabarti</author>
</authors>
<title>Shuffling a stacked deck: The case for partially randomized ranking of search engine results.</title>
<date>2005</date>
<publisher>VLDB.</publisher>
<contexts>
<context position="6812" citStr="Pandey et al., 2005" startWordPosition="1109" endWordPosition="1112">, 2002) studied the relation between the web dynamics, structure and page quality, and demonstrated that PageRank is biased against new pages. In T-Rank Light and T-Rank algorithms (Berberich et al., 2005), both activity (i.e., update rates) and freshness (i.e., timestamps of most recent updates) of pages and links are taken into account for link analysis. Cho et al. (Cho et al., 2005) proposed a page quality ranking function in order to alleviate the problem of popularity-based ranking, and they used the derivatives of PageRank to forecast future PageRank values for new pages. Pandey et al. (Pandey et al., 2005) studied the tradeoff between new page exploration and high-quality page exploitation, which is based on a ranking method to randomly promote some new pages so that they can accumulate links quickly. More recently, Dong et al. (Dong et al., 2010a) 1130 proposed a machine-learned framework to improve ranking result freshness, in which novel features, modeling algorithms and editorial guideline are used to deal with time sensitivities of queries and documents. In another work (Dong et al., 2010b), they use micro-blogging data (e.g., Twitter data) to detect fresh URLs. Novel and effective feature</context>
</contexts>
<marker>Pandey, Roy, Olston, Cho, Chakrabarti, 2005</marker>
<rawString>S. Pandey, S. Roy, C. Olston, J. Cho, and S. Chakrabarti. 2005. Shuffling a stacked deck: The case for partially randomized ranking of search engine results. VLDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to modern information retrieval.</title>
<date>1983</date>
<location>McGraw-Hill, NY.</location>
<contexts>
<context position="3304" citStr="Salton and McGill, 1983" startWordPosition="525" endWordPosition="528">on even if users don’t explicitly issue the query, EMNLP 2010, because EMNLP is a REQ. The query, “EMNLP”, implicitly, without a year qualifier, needs to be served the most recent pages about “EMNLP”. A better search ranking result cannot be achieved if we do not categorize “EMNLP” as a REQ, and provide special ranking treatment to such queries. Existing search engines adopt a fairly involved ranking algorithm to order Web search results by considering many factors. Time is an important factor but not the most critical. The page’s ranking score mostly depends on other features such as tf-idf (Salton and McGill, 1983), BM25 (Jones 1129 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1129–1139, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Figure 1: A real problematic ranking result by Google for a REQ query, “EMNLP”. The EMNLP2010 page should be on the 1st position. et al., 2000), anchor text, historical clicks, pagerank (Brin and Page, 1998), and overall page quality. New pages about EMNLP2010 obtain less favorable feature values than the pages of 2009 earlier in terms of anchor text, click or pagerank because the</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M. J. McGill. 1983. Introduction to modern information retrieval. McGraw-Hill, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Rong Pan</author>
<author>Jian-Tao Sun</author>
<author>Jeffrey Junfeng Pan</author>
<author>Kangheng Wu</author>
<author>Jie Yin</author>
<author>Qiang Yang</author>
</authors>
<title>Q2c@ust: our winning solution to query classification in kddcup</title>
<date>2005</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>7</volume>
<issue>2</issue>
<pages>110</pages>
<contexts>
<context position="5570" citStr="Shen et al., 2005" startWordPosition="905" endWordPosition="908">nd decision tree based logistic regression models. These models are described in Section 4. Our experiments for REQ classifier and Web search ranking are detailed in Section 5 and 6. 2 Related Work We found our work were related to two other problems: general query classification and time-sensitive query classification. For general query classification, the task is to assign a Web search query to one or more predefined categories based on its topics. In the query classification contest in KDDCUP 2005 (Li et al., 2005), seven categories and 67 sub-categories were defined. The winning solution (Shen et al., 2005) used multiple classifiers integrated by ensemble method. The difficulties for query classification are from short queries, lack of labeled data, and query sense ambiguity. Most popular studies use query log, web search results, unlabeled data to enrich query classification (Shen et al., 2006; Beitzel et al., 2005), or use document classification to predict query classification (Broder et al., ). General query classification is also studied for query intent detection by (Li et al., 2008). There are many prior works to study the time sensitivity issue in web search. For example, BaezaYates et a</context>
</contexts>
<marker>Shen, Pan, Sun, Pan, Wu, Yin, Yang, 2005</marker>
<rawString>Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey Junfeng Pan, Kangheng Wu, Jie Yin, and Qiang Yang. 2005. Q2c@ust: our winning solution to query classification in kddcup 2005. SIGKDD Explor. Newsl., 7(2):100– 110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Rong Pan</author>
<author>Jian-Tao Sun</author>
<author>Jeffrey Junfeng Pan</author>
<author>Kangheng Wu</author>
<author>Jie Yin</author>
<author>Qiang Yang</author>
</authors>
<title>Query enrichment for web-query classification.</title>
<date>2006</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="5863" citStr="Shen et al., 2006" startWordPosition="950" endWordPosition="953">itive query classification. For general query classification, the task is to assign a Web search query to one or more predefined categories based on its topics. In the query classification contest in KDDCUP 2005 (Li et al., 2005), seven categories and 67 sub-categories were defined. The winning solution (Shen et al., 2005) used multiple classifiers integrated by ensemble method. The difficulties for query classification are from short queries, lack of labeled data, and query sense ambiguity. Most popular studies use query log, web search results, unlabeled data to enrich query classification (Shen et al., 2006; Beitzel et al., 2005), or use document classification to predict query classification (Broder et al., ). General query classification is also studied for query intent detection by (Li et al., 2008). There are many prior works to study the time sensitivity issue in web search. For example, BaezaYates et al. (Baeza-Yates et al., 2002) studied the relation between the web dynamics, structure and page quality, and demonstrated that PageRank is biased against new pages. In T-Rank Light and T-Rank algorithms (Berberich et al., 2005), both activity (i.e., update rates) and freshness (i.e., timestam</context>
</contexts>
<marker>Shen, Pan, Sun, Pan, Wu, Yin, Yang, 2006</marker>
<rawString>Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey Junfeng Pan, Kangheng Wu, Jie Yin, and Qiang Yang. 2006. Query enrichment for web-query classification. ACM Trans. Inf. Syst., 24(3):320–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Yi Chang</author>
<author>Zhaohui Zheng</author>
<author>Donald Metzler</author>
<author>Jian-yun Nie</author>
</authors>
<title>Search result re-ranking by feedback control adjustment for timesensitive query.</title>
<date>2009</date>
<booktitle>In HLT-NAACL ’09,</booktitle>
<pages>165--168</pages>
<contexts>
<context position="7608" citStr="Zhang et al., 2009" startWordPosition="1237" endWordPosition="1240">umulate links quickly. More recently, Dong et al. (Dong et al., 2010a) 1130 proposed a machine-learned framework to improve ranking result freshness, in which novel features, modeling algorithms and editorial guideline are used to deal with time sensitivities of queries and documents. In another work (Dong et al., 2010b), they use micro-blogging data (e.g., Twitter data) to detect fresh URLs. Novel and effective features are also extracted for fresh URLs so that ranking recency in web search is improved. Perhaps the most related work to this paper is the query classification approach used in (Zhang et al., 2009) and (Metzler et al., 2009), in which year qualified queries (YQQs) are detected based on heuristic rules. For example, a query containing a year stamp is an explicit YQQ; if the year stamp is removed from this YQQ, the remaining part of this query is also a YQQ, which is called implicit YQQ. Different ranking approaches were used in (Zhang et al., 2009) and (Metzler et al., 2009) where (Zhang et al., 2009) boosted pages of the most latest year while (Metzler et al., 2009) promoted pages of the most influential years. Similarly, Nunes et al. (Nunes, 2007) applied information extraction techniq</context>
<context position="10488" citStr="Zhang et al., 2009" startWordPosition="1701" endWordPosition="1704">mp, Implicit Timestamp, and No Timestamp. An Explicit Timestamp query contains at least one token being a time indicator. For example, emnlp 2010, 2007 December holiday calendar, amsterdam weather summer 2009, Google Q1 reports 2010. These queries are considered to conatin time indicators, because we can regard {2010, 2007, 2009} as year indicator, december as month indicator, {summer, Q1(first quarter)} as seasonal indicator. To simplify our work, we only consider the year indicators, 2010, 2007, 2009. Such year indicators are also the most important and most popular indicators, as noted in (Zhang et al., 2009). Any query containing at least one year indicator is an Explicit Timestamp query. Due to word sense ambiguity, some queries labeled as Explicit Timestamp by this method may have no connection with time such as Windows Office 2007, 2010 Sunset Boulevard, or call number 2008. In this work, we tolerate this type of error because word sense disambiguation is a peripheral problem for this task. Implicit Timestamp queries are resulted by removing all year indicators from the corresponding Explicit Timestamp queries. For example, the Implicit Timestamp query of emnlp 2010 is emnlp. All other queries</context>
<context position="12743" citStr="Zhang et al., 2009" startWordPosition="2075" endWordPosition="2078">cit Timestamp queries. We first focus on features. There are many features that were exploited in REQ classifier. We extract these features from query log, query session log, click log, search results, time series and NLP morphological analysis. 3.1 Query log analysis The following features are extracted from query log analysis: QueryDailyFrequency: the total counts of the query divided by the number of the days in the period. ExplicitQueryRatio: Ratio of number of counts query was issued with year and number of counts query was issued with or without year. This feature is the method used by (Zhang et al., 2009). UniqExplicitQueryCount: Number of uniq Explicit Timestamp queries associated with query. For example, if a query was issued with query+2009 and query+2008, this feature’s value is two. ChiSquareYearDist: this feature is the distance between two distributions: one is frequency distribution over years for all REQ queries. The other is that for single REQ query. It is calculated through following steps: (a) Aggregate the frequencies for all queries for all years. Suppose we observe all years from 2001 to 2010. So we can get vector, E = ( af10 sum1, a f09 sum1, ..., a f01 sum1) where afi is the </context>
<context position="24017" citStr="Zhang et al., 2009" startWordPosition="4086" endWordPosition="4089">ility of c = +1 calculated by Eq. 1 so that we can get the point values of recall and precision in Fig. 3. For example, if we set a threshold of 0.6, a query with a probability larger than 0.6 is classified as REQ. Otherwise, it is non-REQ. The precision is a measure of correctly classified REQ queries divided by all classified REQ queries. The recall is a measure of correctly classified REQ queries divided by all REQ queries in test data. In addition to the three plots, we also show the results using only one feature, ExplicitQueryRatio, for comparison with the classification method used by (Zhang et al., 2009).All the three models using all features performed better than the existing method using ExplicitQueryRatio. The highest improvement was achieved by GBDT regression tree 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Figure 3: Comparison of precision and recall rate between our method and the existing method. model. The results of Naive Bayes were lower than SVM and GBDTree. This model is weaker because it treats features independently. Typically SVMs and GBDT gives comparable results on a large class of problems. Since for this task we use features from different sources, the feature values are designed to</context>
<context position="28547" citStr="Zhang et al., 2009" startWordPosition="4814" endWordPosition="4817">classifier Some query examples, and their scores from our model are listed in Table 4. The last two examples, google ipo and adidas jp, have very low values, and are not REQs. The first four queries are typical REQs. They have higher values of features ExplicitQueryRatio,Normalized UserSwitch and YearUrlFPCTR. Although both new apple iphone release reviews and academy awards reviews are about reviews, academy awards reviews has lower value of NormalizedUserSwitch and ChiSquareYearDist could be the reason for a lower score. 6 Web Search Ranking In this section, we use the approach proposed by (Zhang et al., 2009) to test the REQ classifier for Web search ranking. In their approach, search ranking is altered by boosting pages with most recent year if the query is a REQ. The year indicator 1136 DCG@5 DCG@1 bucket #(query) Organic Our’s % over Organic Organic Ours % over Organic [0.0,0.1] 59 6.87 6.96 1.48(-2.3) 4.08 4.19 2.69(-1.07) [0.1,0.2] 76 5.86 6.01 2.52(0.98) 2.88 2.91 1.14(1.69) [0.2,0.3] 85 6.33 6.41 1.24(2.12) 3.7 3.7 0.0(0.8) [0.3,0.4] 75 5.18 5.24 1.18(-0.7) 2.92 2.95 1.14(1.37) [0.4,0.5] 78 4.96 4.82 -2.84(-1.35) 2.5 2.42 -3.06(0) [0.5,0.6] 84 5.4 5.37 -0.45(-0.3) 2.82 2.85 1.05(-1.5) [0.6,</context>
<context position="30440" citStr="Zhang et al., 2009" startWordPosition="5138" endWordPosition="5141">[e(do, dn) + k]eλα(q) where the ranking function, F(q, d), consists of two parts: the base function R(q, d) plus boosting. If the query q is not a REQ, boosting is set to zero. Otherwise, boosting is decided by e(do, dn), k, λ and α(q). e(do, dn) is the difference of base ranking score between the oldest page and the newest page. If the newest page has a lower ranking score than the oldest page, then the difference is added to the newest page to promote the ranking of the newest page. α(q) is the confidence score of a REQ query. It is the value of Eq. 1. λ and k are two empirical parameters. (Zhang et al., 2009)’s work has experimented the effects of using different value of λ and k (λ = 0 equals to no discounts for ranking adjustment). We used λ = 0.4 and k = 0.3 which were the best configuration in (Zhang et al., 2009). For evaluating our methods, we randomly extracted 800 queries from the Implicit Timestamp queries. We scraped a commercial search engine using the 800 queries. We extracted the top five search results for each query under three configures: organic search engine results, (Zhang et al., 2009)’s method and ours using REQ classifier. We asked human editors to judge all the scraped (quer</context>
<context position="31965" citStr="Zhang et al., 2009" startWordPosition="5402" endWordPosition="5405">k as our primary evaluation metrics to measure retrieval performance. DCG is defined as, DCG@k = where r(i) E {0 ... 4} is the relevance grade of the ith ranked document. The Web search ranking results are shown in Table 5. We used GBDT tree learning methods because it achieved the best results. We divided 800 test queries into 10 buckets according to the classifier probability. The bucket, [0.0,0.1], contains the query with a classifier probability greater than 0 but less than 0.1. Our results are compared with organic search results, but we also show the improvements over search organic by (Zhang et al., 2009) in the brackets. Because Zhang’s approach output different classifier values from Ours for the same query, buckets of the same range in the Table contain different queries. Hence, it is inappropriate to compare k 2r(i) − 1 i=1 log2(1 + i) 1137 Zhang’s with Ours for the same buckets except the last row where we used all the queries. Our classifier’s overall performance is much better than the organic search results. We achieved 2.74% DCG@5 gain and 3.6% DCG@1 gain over organic search for all queries. The gains are higher than (Zhang et al., 2009)’s results with regards to improvement over orga</context>
</contexts>
<marker>Zhang, Chang, Zheng, Metzler, Nie, 2009</marker>
<rawString>Ruiqiang Zhang, Yi Chang, Zhaohui Zheng, Donald Metzler, and Jian-yun Nie. 2009. Search result re-ranking by feedback control adjustment for timesensitive query. In HLT-NAACL ’09, pages 165–168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>