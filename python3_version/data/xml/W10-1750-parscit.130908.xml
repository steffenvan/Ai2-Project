<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002627">
<title confidence="0.9840915">
Document-level Automatic MT Evaluation
based on Discourse Representations
</title>
<note confidence="0.9567678">
Jes´us Gim´enez and Elisabet Comelles and Victoria Arranz
Lluis M`arquez Irene Castell´on ELDA/ELRA
TALP UPC Universitat de Barcelona Paris, France
Barcelona, Spain Barcelona, Spain arranz@elda.org
{jgimenez,lluism} {elicomelles,
</note>
<email confidence="0.97004">
@lsi.upc.edu icastellon}@ub.edu
</email>
<sectionHeader confidence="0.993391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999924">
This paper describes the joint submission of
Universitat Polit`ecnica de Catalunya and Uni-
versitat de Barcelona to the Metrics MaTr
2010 evaluation challenge, in collaboration with
ELDA/ELRA. Our work is aimed at widening the
scope of current automatic evaluation measures
from sentence to document level. Preliminary ex-
periments, based on an extension of the metrics by
Gim´enez and M`arquez (2009) operating over dis-
course representations, are presented.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994600375">
Current automatic similarity measures for Ma-
chine Translation (MT) evaluation operate all,
without exception, at the segment level. Trans-
lations are analyzed on a segment-by-segment1
fashion, ignoring the text structure. Document
and system scores are obtained using aggregate
statistics over individual segments. This strategy
presents the main disadvantage of ignoring cross-
sentential/discursive phenomena.
In this work we suggest widening the scope
of evaluation methods. We have defined genuine
document-level measures which are able to ex-
ploit the structure of text to provide more informed
evaluation scores. For that purpose we take advan-
tage of two coincidental facts. First, test beds em-
ployed in recent MT evaluation campaigns include
a document structure grouping sentences related
to the same event, story or topic (Przybocki et al.,
2008; Przybocki et al., 2009; Callison-Burch et al.,
2009). Second, we count on automatic linguistic
processors which provide very detailed discourse-
level representations of text (Curran et al., 2007).
Discourse representations allow us to focus on
relevant pieces of information, such as the agent
</bodyText>
<footnote confidence="0.638036">
1A segment typically consists of one or two sentences.
</footnote>
<bodyText confidence="0.999884804878048">
(who), location (where), time (when), and theme
(what), which may be spread all over the text.
Counting on a means of discerning the events, the
individuals taking part in each of them, and their
role, is crucial to determine the semantic equiva-
lence between a reference document and a candi-
date translation.
Moreover, the discourse analysis of a document
is not a mere concatenation of the analyses of its
individual sentences. There are some phenom-
ena which may go beyond the scope of a sen-
tence and can only be explained within the con-
text of the whole document. For instance, in a
newspaper article, facts and entities are progres-
sively added to the discourse and then referred
to anaphorically later on. The following extract
from the development set illustrates the impor-
tance of such a phenomenon in the discourse anal-
ysis: ‘Among the current or underlying crises in
the Middle East, Rod Larsen mentioned the Arab-
Israeli conflict and the Iranian nuclear portfolio,
as well as the crisis between Lebanon and Syria.
He stated: “All this leads us back to crucial val-
ues and opinions, which render the situation prone
at any moment to getting out of control, more so
than it was in past days.”’. The subject pronoun
“he” works as an anaphoric pronoun whose an-
tecedent is the proper noun “Rod Larson”. The
anaphoric relation established between these two
elements can only be identified by analyzing the
text as a whole, thus considering the gender agree-
ment between the third person singular masculine
subject pronoun “he” and the masculine proper
noun “Rod Larson”. However, if the two sen-
tences were analyzed separately, the identification
of this anaphoric relation would not be feasible
due to the lack of connection between the two ele-
ments. Discourse representations allow us to trace
links across sentences between the different facts
and entities appearing in them. Therefore, provid-
ing an approach to the text more similar to that of
</bodyText>
<page confidence="0.991142">
333
</page>
<note confidence="0.4533685">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 333–338,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999715538461538">
a human, which implies taking into account the
whole text structure instead of considering each
sentence separately.
The rest of the paper is organized as follows.
Section 2 describes our evaluation methods and
the linguistic theory upon which they are based.
Experimental results are reported and discussed in
Section 3. Section 4 presents the metric submitted
to the evaluation challenge. Future work is out-
lined in Section 5.
As an additional result, document-level metrics
generated in this study have been incorporated to
the IQMT package for automatic MT evaluation2.
</bodyText>
<sectionHeader confidence="0.990783" genericHeader="method">
2 Metric Description
</sectionHeader>
<bodyText confidence="0.9964494">
This section provides a brief description of our ap-
proach. First, in Section 2.1, we describe the un-
derlying theory and give examples on its capabili-
ties. Then, in Section 2.2, we describe the associ-
ated similarity measures.
</bodyText>
<subsectionHeader confidence="0.989455">
2.1 Discourse Representations
</subsectionHeader>
<bodyText confidence="0.983890444444444">
As previously mentioned in Section 1, a document
has some features which need to be analyzed con-
sidering it as a whole instead of dividing it up
into sentences. The anaphoric relation between
a subject pronoun and a proper noun has already
been exemplified. However, this is not the only
anaphoric relation which can be found inside a
text, there are some others which are worth men-
tioning:
</bodyText>
<listItem confidence="0.990494470588235">
• the connection between a possessive adjec-
tive and a proper noun or a subject pro-
noun, as exemplified in the sentences “Maria
bought a new sweater. Her new sweater is
blue.”, where the possessive feminine adjec-
tive “her” refers to the proper noun “Maria”.
• the link between a demonstrative pronoun
and its referent, which is exemplified in the
sentences “He developed a new theory on
grammar. However, this is not the only the-
ory he developed”. In the second sentence,
the demonstrative pronoun ”this” refers back
to the noun phrase “new theory on grammar”
which occurs in the previous sentence.
• the relation between a main verb and an aux-
iliary verb in certain contexts, as illustrated in
the following pair of sentences “Would you
</listItem>
<footnote confidence="0.698895">
2http://www.lsi.upc.edu/˜nlp/IQMT
</footnote>
<bodyText confidence="0.9872926">
like more sugar? Yes, I would”. In this ex-
ample, the auxiliary verb “would” used in
the short answer substitutes the verb phrase
“would like”.
In addition to anaphoric relations, other features
need to be highlighted, such as the use of discourse
markers which help to give cohesion to the text,
link parts of a discourse and show the relations es-
tablished between them. Below, some examples
are given:
</bodyText>
<listItem confidence="0.988661444444445">
• “Moreover”, “Furthermore”, “In addition”
indicate that the upcoming sentence adds
more information.
• “However”, “Nonetheless”, “Nevertheless”
show contrast with previous ideas.
• “Therefore”, “As a result”, “Consequently”
show a cause and effect relation.
• “For instance”, “For example” clarify or il-
lustrate the previous idea.
</listItem>
<bodyText confidence="0.99978825">
It is worth noticing that anaphora, as well as dis-
course markers, are key features in the interface
between syntax, semantics and pragmatics. Thus,
when dealing with these phenomena at a text level
we are not just looking separately at the different
language levels, but we are trying to give a com-
plete representation of both the surface and the
deep structures of a text.
</bodyText>
<subsectionHeader confidence="0.999613">
2.2 Definition of Similarity Measures
</subsectionHeader>
<bodyText confidence="0.999986578947368">
In this work, as a first proposal, instead of elabo-
rating on novel similarity measures, we have bor-
rowed and extended the Discourse Representation
(DR) metrics defined by Gim´enez and M`arquez
(2009). These metrics analyze similarities be-
tween automatic and reference translations by
comparing their respective discourse representa-
tions over individual sentences.
For the discursive analysis of texts, DR met-
rics rely on the C&amp;C Tools (Curran et al., 2007),
specifically on the Boxer component (Bos, 2008).
This software is based on the Discourse Represen-
tation Theory (DRT) by Kamp and Reyle (1993).
DRT is a theoretical framework offering a rep-
resentation language for the examination of con-
textually dependent meaning in discourse. A dis-
course is represented in a discourse representation
structure (DRS), which is essentially a variation of
first-order predicate calculus —its forms are pairs
</bodyText>
<page confidence="0.9946">
334
</page>
<bodyText confidence="0.963421046511628">
of first-order formulae and the free variables that
occur in them.
DRSs are viewed as semantic trees, built
through the application of two types of DRS con-
ditions:
basic conditions: one-place properties (pred-
icates), two-place properties (relations),
named entities, time-expressions, cardinal
expressions and equalities.
complex conditions: disjunction, implication,
negation, question, and propositional attitude
operations.
For instance, the DRS representation for the
sentence “Every man loves Mary.” is as follows:
]y named(y, mary, per) n (bx man(x) —*
]z love(z) n event(z) n agent(z, x) n
patient(z, y)). DR integrates three different
kinds of metrics:
DR-STM These metrics are similar to the Syntac-
tic Tree Matching metric defined by Liu and
Gildea (2005), in this case applied to DRSs
instead of constituent trees. All semantic sub-
paths in the candidate and reference trees are
retrieved. The fraction of matching subpaths
of a given length (l=4 in our experiments) is
computed.
DR-Or(*) Average lexical overlap between dis-
course representation structures of the same
type. Overlap is measured according to the
formulae and definitions by Gim´enez and
M`arquez (2007).
DR-Orp(*) Average morphosyntactic overlap,
i.e., between grammatical categories –parts-
of-speech– associated to lexical items, be-
tween discourse representation structures of
the same type.
We have extended these metrics to operate at
document level. For that purpose, instead of run-
ning the C&amp;C Tools in a sentence-by-sentence
fashion, we run them document by document.
This is as simple as introducing a “&lt;META&gt;” tag
at the beginning of each document to denote doc-
ument boundaries3.
</bodyText>
<footnote confidence="0.980191">
3Details on the advanced use of Boxer are avail-
able at http://svn.ask.it.usyd.edu.au/trac/
candc/wiki/BoxerComplex.
</footnote>
<sectionHeader confidence="0.998003" genericHeader="method">
3 Experimental Work
</sectionHeader>
<bodyText confidence="0.997949666666667">
In this section, we analyze the behavior of the new
DR metrics operating at document level with re-
spect to their sentence-level counterparts.
</bodyText>
<subsectionHeader confidence="0.993972">
3.1 Settings
</subsectionHeader>
<bodyText confidence="0.99993703030303">
We have used the ‘mt06’ part of the development
set provided by the Metrics MaTr 2010 organiza-
tion, which corresponds to a subset of 25 docu-
ments from the NIST 2006 Open MT Evaluation
Campaign Arabic-to-English translation. The to-
tal number of segments is 249. The average num-
ber of segments per document is, thus, 9.96. The
number of segments per document varies between
2 and 30. For the purpose of automatic evaluation,
4 human reference translations and automatic out-
puts by 8 different MT systems are available. In
addition, we count on the results of a process of
manual evaluation. Each translation segment was
assessed by two judges. After independently and
completely assessing the entire set, the judges re-
viewed their individual assessments together and
settled on a single final score. Average system ad-
equacy is 5.38.
In our experiments, metrics are evaluated in
terms of their correlation with human assess-
ments. We have computed Pearson, Spearman
and Kendall correlation coefficients between met-
ric scores and adequacy assessments. Document-
level and system-level assessments have been ob-
tained by averaging over segment-level assess-
ments. We have computed correlation coefficients
and confidence intervals applying bootstrap re-
sampling at a 99% statistical significance (Efron
and Tibshirani, 1986; Koehn, 2004). Since the
cost of exhaustive resampling was prohibitive, we
have limited to 1,000 resamplings. Confidence in-
tervals, not shown in the tables, are in all cases
lower than 10−3.
</bodyText>
<subsectionHeader confidence="0.999217">
3.2 Metric Performance
</subsectionHeader>
<bodyText confidence="0.999843857142857">
Table 1 shows correlation coefficients at the docu-
ment level for several DR metric representatives,
and their document-level counterparts (DRdoc).
For the sake of comparison, the performance of
the METEOR metric is also reported4.
Contrary to our expectations, DRdoc variants
obtain lower levels of correlation than their DR
</bodyText>
<footnote confidence="0.9877055">
4We have used METEOR version 1.0 with default param-
eters optimized by its developers over adequacy and fluency
assessments. The METEOR metric is publicly available at
http://www.cs.cmu.edu/˜alavie/METEOR/
</footnote>
<page confidence="0.995264">
335
</page>
<table confidence="0.99982125">
Metric Pearsonρ Spearmanρ Kendallτ
METEOR 0.9182 0.8478 0.6728
DR-Or(⋆) 0.8567 0.8061 0.6193
DR-Orp(⋆) 0.8286 0.7790 0.5875
DR-STM 0.7880 0.7468 0.5554
DRdoc-Or(⋆) 0.7936 0.7784 0.5875
DRdoc-Orp(⋆) 0.7219 0.6737 0.4929
DRdoc-STM 0.7553 0.7421 0.5458
</table>
<tableCaption confidence="0.996051">
Table 1: Meta-evaluation results at document level
</tableCaption>
<table confidence="0.999956363636364">
Metric Pearsonρ Spearmanρ Kendallτ
METEOR 0.9669 0.9151 0.8533
DR-Or(⋆) 0.9100 0.6549 0.5764
DR-Orp(⋆) 0.9471 0.7918 0.7261
DR-STM 0.9295 0.7676 0.7165
DRdoc-Or(⋆) 0.9534 0.8434 0.7828
DRdoc-Orp(⋆) 0.9595 0.9101 0.8518
DRdoc-STM 0.9676 0.9655 0.9272
DR-Or(⋆)′ 0.9836 0.9594 0.9296
DR-Orp(⋆)′ 0.9959 1.0000 1.0000
DR-STM′ 0.9933 0.9634 0.9307
</table>
<tableCaption confidence="0.999284">
Table 2: Meta-evaluation results at system level
</tableCaption>
<bodyText confidence="0.999970722222222">
counterparts. There are three different factors
which could provide a possible explanation for
this negative result. First, the C&amp;C Tools, like any
other automatic linguistic processor are not per-
fect. Parsing errors could be causing the metric
to confer less informed scores. This is especially
relevant taking into account that candidate transla-
tions are not always well-formed. Secondly, we
argue that the way in which we have obtained
document-level quality assessments, as an average
of segment-level assessments, may be biasing the
correlation. Thirdly, perhaps the similarity mea-
sures employed are not able to take advantage of
the document-level features provided by the dis-
course analysis. In the following subsection we
show some error analysis we have conducted by
inspecting particular cases.
Table 2 shows correlation coefficients at system
level. In the case of DR and DRdo, metrics, sys-
tem scores are computed by simple average over
individual documents. Interestingly, in this case
DRdo, variants seem to obtain higher correlation
than their DR counterparts. The improvement is
especially substantial in terms of Spearman and
Kendall coefficients, which do not consider ab-
solute values but ranking positions. However, it
could be the case that it was just an average ef-
fect. While DR metrics compute system scores as
an average of segment scores, DRdo, metrics av-
erage directly document scores. In order to clarify
this result, we have modified DR metrics so as to
compute system scores as an average of document
scores (DR′ variants, the last three rows in the ta-
ble). It can be observed that DR’ variants out-
perform their DRdo, counterparts, thus confirming
our suspicion about the averaging effect.
</bodyText>
<subsectionHeader confidence="0.998061">
3.3 Analysis
</subsectionHeader>
<bodyText confidence="0.999927125">
It is worth noting that DRdo, metrics are able to
detect and deal with several linguistic phenomena
related to both syntax and semantics at sentence
and document level. Below, several examples il-
lustrating the potential of this metric are presented.
Control structures. Control structures (either
subject or object control) are always a
difficult issue as they mix both syntactic and
semantic knowledge. In Example 1 a couple
of control structures must be identified
and DRdo, metrics deal correctly with the
argument structure of all the verbs involved.
Thus, in the first part of the sentence, a
subject control verb can be identified being
“the minister” the agent of both verb forms
“go” and “say”. On the other hand, in the
</bodyText>
<page confidence="0.99783">
336
</page>
<bodyText confidence="0.862239288888889">
quoted question, the verb “invite” works as
an object control verb because its patient
“Chechen representatives” is also the agent
of the verb visit.
Example 1: The minister went on to say,
“What would Moscow say if we were to invite
Chechen representatives to visit Jerusalem?”
Anaphora and pronoun resolution. Whenever
there is a pronoun whose antecedent is a
named entity (NE), the metric identifies
correctly its antecedent. This feature is
highly valuable because a relationship be-
tween syntax and semantics is established.
Moreover, when dealing with Semantic
Roles the roles of Agent or Patient are given
to the antecedents instead of the pronouns.
Thus, in Example 2 the antecedent of the
relative pronoun “who” is the NE “Putin”
and the patient of the verb “classified” is
also the NE “Putin” instead of the relative
pronoun “who”.
Example 2: Putin, who was not classified
as his country Hamas as “terrorist organiza-
tions”, recently said that the European Union
is “a big mistake” if it decided to suspend fi-
nancial aid to the Palestinians.
Nevertheless, although Boxer was expected
to deal with long-distance anaphoric relations
beyond the sentence, after analyzing several
cases, results show that it did not succeed in
capturing this type of relations as shown in
Example 3. In this example, the antecedent
of the pronoun “he” in the second sentence
is the NE “Roberto Calderoli” which ap-
pears in the first sentence. DRdo, metrics
should be capable of showing this connec-
tion. However, although the proper noun
“Roberto Calderoli” is identified as a NE, it
does not share the same reference as the third
person singular pronoun “he”.
Example 3: Roberto Calderoli does not in-
tend to apologize. The newspaper Corriere
Della Sera reported today, Saturday, that
he said “I don’t feel responsible for those
deaths.”
</bodyText>
<sectionHeader confidence="0.992029" genericHeader="method">
4 Our Submission
</sectionHeader>
<bodyText confidence="0.994593606060606">
Instead of participating with individual metrics,
we have combined them by averaging their scores
as described in (Gim´enez and M`arquez, 2008).
This strategy has proven as an effective means of
combining the scores conferred by different met-
rics (Callison-Burch et al., 2008; Callison-Burch
et al., 2009). Metrics submitted are:
DRdo, an arithmetic mean over a heuristically-
defined set of DRdo, metric variants, respec-
tively computing lexical overlap, morphosyn-
tactic overlap, and semantic tree match-
ing (M = {‘DRdoc-Or(*)’, ‘DRdoc-Orp(*)’, ‘DRdoc-
STM4’}). Since DRdo, metrics do not operate
over individual segments, we have assigned
each segment the score of the document in
which it is contained.
DR a measure analog to DRdo, but using the de-
fault version of DR metrics operating at the
segment level (M = {‘DR-Or(*)’, ‘DR-Orp(*)’,
‘DR-STM4’}).
ULCh an arithmetic mean over a heuristically-
defined set of metrics operating at differ-
ent linguistic levels, including lexical met-
rics, and measures of overlap between con-
stituent parses, dependency parses, seman-
tic roles, and discourse representations (M =
{‘ROUGEW’, ‘METEOR’, ‘DP-HWCr’, ‘DP-Oc(*)’,
‘DP-Ol(*)’, ‘DP-Or(*)’, ‘CP-STM4’, ‘SR-Or(*)’,
‘SR-Orv’, ‘DR-Orp(*)’}). This metric corre-
sponds exactly to the metric submitted in our
previous participation.
The performance of these metrics at the docu-
ment and system levels is shown in Table 3.
</bodyText>
<sectionHeader confidence="0.998177" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999986125">
We have presented a modified version of the DR
metrics by Gim´enez and M`arquez (2009) which,
instead of limiting their scope to the segment level,
are able to capture and exploit document-level fea-
tures. However, results in terms of correlation
with human assessments have not reported any im-
provement of these metrics over their sentence-
level counterparts as document and system quality
predictors. It must be clarified whether the prob-
lem is on the side of the linguistic tools, in the
similarity measure, or in the way in which we have
built document-level human assessments.
For future work, we plan to continue the er-
ror analysis to clarify why DRdo, metrics do not
outperform their DR counterparts at the document
level, and how to improve their behavior. This
</bodyText>
<page confidence="0.992768">
337
</page>
<table confidence="0.9997228">
Document level System level
Metric Pearsonp Spearmanp Kendall, Pearsonp Spearmanp Kendall,
ULCDR 0.8418 0.8066 0.6135 0.9349 0.7936 0.7145
ULCDRdoc 0.7739 0.7358 0.5474 0.9655 0.9062 0.8435
ULCh 0.8963 0.8614 0.6848 0.9842 0.9088 0.8638
</table>
<tableCaption confidence="0.999791">
Table 3: Meta-evaluation results at document and system level for submitted metrics
</tableCaption>
<bodyText confidence="0.99979325">
may imply defining new metrics possibly using
alternative linguistic processors. In addition, we
plan to work on the identification and analysis
of discourse markers. Finally, we plan to repeat
this experiment over other test beds with docu-
ment structure, such as those from the 2009 Work-
shop on Statistical Machine Translation shared
task (Callison-Burch et al., 2009) and the 2009
NIST MT Evaluation Campaign (Przybocki et al.,
2009). In the case that document-level assess-
ments are not provided, we will also explore the
possibility of producing them ourselves.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999514">
This work has been partially funded by the
Spanish Government (projects OpenMT-2,
TIN2009-14675-C03, and KNOW, TIN-2009-
14715-C0403) and the European Community’s
Seventh Framework Programme (FP7/2007-2013)
under grant agreement numbers 247762 (FAUST
project, FP7-ICT-2009-4-247762) and 247914
(MOLTO project, FP7-ICT-2009-4-247914). We
are also thankful to anonymous reviewers for their
comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999258" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999843403225807">
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277–286. College Publi-
cations.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation, pages 70–106.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1–28.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale nlp with c&amp;c
and boxer. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 33–36.
Bradley Efron and Robert Tibshirani. 1986. Bootstrap
Methods for Standard Errors, Confidence Intervals,
and Other Measures of Statistical Accuracy. Statis-
tical Science, 1(1):54–77.
Jes´us Gim´enez and Lluis M`arquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
256–264.
Jes´us Gim´enez and Lluis M`arquez. 2008. A Smor-
gasbord of Features for Automatic MT Evaluation.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 195–198.
Jes´us Gim´enez and Lluis M`arquez. 2009. On the Ro-
bustness of Syntactic and Semantic Features for Au-
tomatic MT Evaluation. In Proceedings of the 4th
Workshop on Statistical Machine Translation (EACL
2009).
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic: An Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Dordrecht: Kluwer.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 388–395.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization,
pages 25–32.
Mark Przybocki, Kay Peterson, and S´ebastien Bron-
sart. 2008. NIST Metrics for Machine Translation
2008 Evaluation (MetricsMATR08). Technical re-
port, National Institute of Standards and Technol-
ogy.
Mark Przybocki, Kay Peterson, and S´ebastien Bron-
sart. 2009. NIST Open Machine Translation 2009
Evaluation (MT09). Technical report, National In-
stitute of Standards and Technology.
</reference>
<page confidence="0.998379">
338
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.417984">
<title confidence="0.89242">Document-level Automatic MT based on Discourse Representations Gim´enez Comelles Victoria</title>
<author confidence="0.886481">Lluis Irene Paris</author>
<affiliation confidence="0.925181">TALP Universitat de arranz@elda.org</affiliation>
<address confidence="0.894476">Barcelona, Barcelona,</address>
<email confidence="0.999946">@lsi.upc.edu</email>
<abstract confidence="0.946182">This paper describes the joint submission of Universitat Polit`ecnica de Catalunya and Universitat de Barcelona to the Metrics MaTr 2010 evaluation challenge, in collaboration with ELDA/ELRA. Our work is aimed at widening the scope of current automatic evaluation measures from sentence to document level. Preliminary experiments, based on an extension of the metrics by Gim´enez and M`arquez (2009) operating over discourse representations, are presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with boxer.</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics,</booktitle>
<pages>277--286</pages>
<editor>In Johan Bos and Rodolfo Delmonte, editors,</editor>
<publisher>College Publications.</publisher>
<contexts>
<context position="7819" citStr="Bos, 2008" startWordPosition="1225" endWordPosition="1226">resentation of both the surface and the deep structures of a text. 2.2 Definition of Similarity Measures In this work, as a first proposal, instead of elaborating on novel similarity measures, we have borrowed and extended the Discourse Representation (DR) metrics defined by Gim´enez and M`arquez (2009). These metrics analyze similarities between automatic and reference translations by comparing their respective discourse representations over individual sentences. For the discursive analysis of texts, DR metrics rely on the C&amp;C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008). This software is based on the Discourse Representation Theory (DRT) by Kamp and Reyle (1993). DRT is a theoretical framework offering a representation language for the examination of contextually dependent meaning in discourse. A discourse is represented in a discourse representation structure (DRS), which is essentially a variation of first-order predicate calculus —its forms are pairs 334 of first-order formulae and the free variables that occur in them. DRSs are viewed as semantic trees, built through the application of two types of DRS conditions: basic conditions: one-place properties (</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-coverage semantic analysis with boxer. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics, pages 277–286. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>70--106</pages>
<contexts>
<context position="17500" citStr="Callison-Burch et al., 2008" startWordPosition="2716" endWordPosition="2719"> However, although the proper noun “Roberto Calderoli” is identified as a NE, it does not share the same reference as the third person singular pronoun “he”. Example 3: Roberto Calderoli does not intend to apologize. The newspaper Corriere Della Sera reported today, Saturday, that he said “I don’t feel responsible for those deaths.” 4 Our Submission Instead of participating with individual metrics, we have combined them by averaging their scores as described in (Gim´enez and M`arquez, 2008). This strategy has proven as an effective means of combining the scores conferred by different metrics (Callison-Burch et al., 2008; Callison-Burch et al., 2009). Metrics submitted are: DRdo, an arithmetic mean over a heuristicallydefined set of DRdo, metric variants, respectively computing lexical overlap, morphosyntactic overlap, and semantic tree matching (M = {‘DRdoc-Or(*)’, ‘DRdoc-Orp(*)’, ‘DRdocSTM4’}). Since DRdo, metrics do not operate over individual segments, we have assigned each segment the score of the document in which it is contained. DR a measure analog to DRdo, but using the default version of DR metrics operating at the segment level (M = {‘DR-Or(*)’, ‘DR-Orp(*)’, ‘DR-STM4’}). ULCh an arithmetic mean ove</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 70–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--28</pages>
<contexts>
<context position="1721" citStr="Callison-Burch et al., 2009" startWordPosition="236" endWordPosition="239"> statistics over individual segments. This strategy presents the main disadvantage of ignoring crosssentential/discursive phenomena. In this work we suggest widening the scope of evaluation methods. We have defined genuine document-level measures which are able to exploit the structure of text to provide more informed evaluation scores. For that purpose we take advantage of two coincidental facts. First, test beds employed in recent MT evaluation campaigns include a document structure grouping sentences related to the same event, story or topic (Przybocki et al., 2008; Przybocki et al., 2009; Callison-Burch et al., 2009). Second, we count on automatic linguistic processors which provide very detailed discourselevel representations of text (Curran et al., 2007). Discourse representations allow us to focus on relevant pieces of information, such as the agent 1A segment typically consists of one or two sentences. (who), location (where), time (when), and theme (what), which may be spread all over the text. Counting on a means of discerning the events, the individuals taking part in each of them, and their role, is crucial to determine the semantic equivalence between a reference document and a candidate translat</context>
<context position="17530" citStr="Callison-Burch et al., 2009" startWordPosition="2720" endWordPosition="2723"> noun “Roberto Calderoli” is identified as a NE, it does not share the same reference as the third person singular pronoun “he”. Example 3: Roberto Calderoli does not intend to apologize. The newspaper Corriere Della Sera reported today, Saturday, that he said “I don’t feel responsible for those deaths.” 4 Our Submission Instead of participating with individual metrics, we have combined them by averaging their scores as described in (Gim´enez and M`arquez, 2008). This strategy has proven as an effective means of combining the scores conferred by different metrics (Callison-Burch et al., 2008; Callison-Burch et al., 2009). Metrics submitted are: DRdo, an arithmetic mean over a heuristicallydefined set of DRdo, metric variants, respectively computing lexical overlap, morphosyntactic overlap, and semantic tree matching (M = {‘DRdoc-Or(*)’, ‘DRdoc-Orp(*)’, ‘DRdocSTM4’}). Since DRdo, metrics do not operate over individual segments, we have assigned each segment the score of the document in which it is contained. DR a measure analog to DRdo, but using the default version of DR metrics operating at the segment level (M = {‘DR-Or(*)’, ‘DR-Orp(*)’, ‘DR-STM4’}). ULCh an arithmetic mean over a heuristicallydefined set o</context>
<context position="20114" citStr="Callison-Burch et al., 2009" startWordPosition="3118" endWordPosition="3121"> Pearsonp Spearmanp Kendall, ULCDR 0.8418 0.8066 0.6135 0.9349 0.7936 0.7145 ULCDRdoc 0.7739 0.7358 0.5474 0.9655 0.9062 0.8435 ULCh 0.8963 0.8614 0.6848 0.9842 0.9088 0.8638 Table 3: Meta-evaluation results at document and system level for submitted metrics may imply defining new metrics possibly using alternative linguistic processors. In addition, we plan to work on the identification and analysis of discourse markers. Finally, we plan to repeat this experiment over other test beds with document structure, such as those from the 2009 Workshop on Statistical Machine Translation shared task (Callison-Burch et al., 2009) and the 2009 NIST MT Evaluation Campaign (Przybocki et al., 2009). In the case that document-level assessments are not provided, we will also explore the possibility of producing them ourselves. Acknowledgments This work has been partially funded by the Spanish Government (projects OpenMT-2, TIN2009-14675-C03, and KNOW, TIN-2009- 14715-C0403) and the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement numbers 247762 (FAUST project, FP7-ICT-2009-4-247762) and 247914 (MOLTO project, FP7-ICT-2009-4-247914). We are also thankful to anonymous reviewers for their </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Stephen Clark</author>
<author>Johan Bos</author>
</authors>
<title>Linguistically motivated large-scale nlp with c&amp;c and boxer.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>33--36</pages>
<contexts>
<context position="1863" citStr="Curran et al., 2007" startWordPosition="256" endWordPosition="259">uggest widening the scope of evaluation methods. We have defined genuine document-level measures which are able to exploit the structure of text to provide more informed evaluation scores. For that purpose we take advantage of two coincidental facts. First, test beds employed in recent MT evaluation campaigns include a document structure grouping sentences related to the same event, story or topic (Przybocki et al., 2008; Przybocki et al., 2009; Callison-Burch et al., 2009). Second, we count on automatic linguistic processors which provide very detailed discourselevel representations of text (Curran et al., 2007). Discourse representations allow us to focus on relevant pieces of information, such as the agent 1A segment typically consists of one or two sentences. (who), location (where), time (when), and theme (what), which may be spread all over the text. Counting on a means of discerning the events, the individuals taking part in each of them, and their role, is crucial to determine the semantic equivalence between a reference document and a candidate translation. Moreover, the discourse analysis of a document is not a mere concatenation of the analyses of its individual sentences. There are some ph</context>
<context position="7770" citStr="Curran et al., 2007" startWordPosition="1216" endWordPosition="1219">t language levels, but we are trying to give a complete representation of both the surface and the deep structures of a text. 2.2 Definition of Similarity Measures In this work, as a first proposal, instead of elaborating on novel similarity measures, we have borrowed and extended the Discourse Representation (DR) metrics defined by Gim´enez and M`arquez (2009). These metrics analyze similarities between automatic and reference translations by comparing their respective discourse representations over individual sentences. For the discursive analysis of texts, DR metrics rely on the C&amp;C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008). This software is based on the Discourse Representation Theory (DRT) by Kamp and Reyle (1993). DRT is a theoretical framework offering a representation language for the examination of contextually dependent meaning in discourse. A discourse is represented in a discourse representation structure (DRS), which is essentially a variation of first-order predicate calculus —its forms are pairs 334 of first-order formulae and the free variables that occur in them. DRSs are viewed as semantic trees, built through the application of two types of DRS con</context>
</contexts>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>James Curran, Stephen Clark, and Johan Bos. 2007. Linguistically motivated large-scale nlp with c&amp;c and boxer. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 33–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert Tibshirani</author>
</authors>
<title>Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy.</title>
<date>1986</date>
<journal>Statistical Science,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="11480" citStr="Efron and Tibshirani, 1986" startWordPosition="1779" endWordPosition="1782"> the entire set, the judges reviewed their individual assessments together and settled on a single final score. Average system adequacy is 5.38. In our experiments, metrics are evaluated in terms of their correlation with human assessments. We have computed Pearson, Spearman and Kendall correlation coefficients between metric scores and adequacy assessments. Documentlevel and system-level assessments have been obtained by averaging over segment-level assessments. We have computed correlation coefficients and confidence intervals applying bootstrap resampling at a 99% statistical significance (Efron and Tibshirani, 1986; Koehn, 2004). Since the cost of exhaustive resampling was prohibitive, we have limited to 1,000 resamplings. Confidence intervals, not shown in the tables, are in all cases lower than 10−3. 3.2 Metric Performance Table 1 shows correlation coefficients at the document level for several DR metric representatives, and their document-level counterparts (DRdoc). For the sake of comparison, the performance of the METEOR metric is also reported4. Contrary to our expectations, DRdoc variants obtain lower levels of correlation than their DR 4We have used METEOR version 1.0 with default parameters opt</context>
</contexts>
<marker>Efron, Tibshirani, 1986</marker>
<rawString>Bradley Efron and Robert Tibshirani. 1986. Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy. Statistical Science, 1(1):54–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on Statistical Machine Translation,</booktitle>
<pages>256--264</pages>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2007. Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems. In Proceedings of the ACL Workshop on Statistical Machine Translation, pages 256–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>A Smorgasbord of Features for Automatic MT Evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>195--198</pages>
<marker>Gim´enez, M`arquez, 2008</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2008. A Smorgasbord of Features for Automatic MT Evaluation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 195–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>On the Robustness of Syntactic and Semantic Features for Automatic MT Evaluation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th Workshop on Statistical Machine Translation (EACL</booktitle>
<marker>Gim´enez, M`arquez, 2009</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2009. On the Robustness of Syntactic and Semantic Features for Automatic MT Evaluation. In Proceedings of the 4th Workshop on Statistical Machine Translation (EACL 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<title>From Discourse to Logic: An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory.</title>
<date>1993</date>
<publisher>Kluwer.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="7913" citStr="Kamp and Reyle (1993)" startWordPosition="1239" endWordPosition="1242">f Similarity Measures In this work, as a first proposal, instead of elaborating on novel similarity measures, we have borrowed and extended the Discourse Representation (DR) metrics defined by Gim´enez and M`arquez (2009). These metrics analyze similarities between automatic and reference translations by comparing their respective discourse representations over individual sentences. For the discursive analysis of texts, DR metrics rely on the C&amp;C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008). This software is based on the Discourse Representation Theory (DRT) by Kamp and Reyle (1993). DRT is a theoretical framework offering a representation language for the examination of contextually dependent meaning in discourse. A discourse is represented in a discourse representation structure (DRS), which is essentially a variation of first-order predicate calculus —its forms are pairs 334 of first-order formulae and the free variables that occur in them. DRSs are viewed as semantic trees, built through the application of two types of DRS conditions: basic conditions: one-place properties (predicates), two-place properties (relations), named entities, time-expressions, cardinal expr</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Hans Kamp and Uwe Reyle. 1993. From Discourse to Logic: An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory. Dordrecht: Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>388--395</pages>
<contexts>
<context position="11494" citStr="Koehn, 2004" startWordPosition="1783" endWordPosition="1784">reviewed their individual assessments together and settled on a single final score. Average system adequacy is 5.38. In our experiments, metrics are evaluated in terms of their correlation with human assessments. We have computed Pearson, Spearman and Kendall correlation coefficients between metric scores and adequacy assessments. Documentlevel and system-level assessments have been obtained by averaging over segment-level assessments. We have computed correlation coefficients and confidence intervals applying bootstrap resampling at a 99% statistical significance (Efron and Tibshirani, 1986; Koehn, 2004). Since the cost of exhaustive resampling was prohibitive, we have limited to 1,000 resamplings. Confidence intervals, not shown in the tables, are in all cases lower than 10−3. 3.2 Metric Performance Table 1 shows correlation coefficients at the document level for several DR metric representatives, and their document-level counterparts (DRdoc). For the sake of comparison, the performance of the METEOR metric is also reported4. Contrary to our expectations, DRdoc variants obtain lower levels of correlation than their DR 4We have used METEOR version 1.0 with default parameters optimized by its </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic Features for Evaluation of Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="8979" citStr="Liu and Gildea (2005)" startWordPosition="1396" endWordPosition="1399">s of DRS conditions: basic conditions: one-place properties (predicates), two-place properties (relations), named entities, time-expressions, cardinal expressions and equalities. complex conditions: disjunction, implication, negation, question, and propositional attitude operations. For instance, the DRS representation for the sentence “Every man loves Mary.” is as follows: ]y named(y, mary, per) n (bx man(x) —* ]z love(z) n event(z) n agent(z, x) n patient(z, y)). DR integrates three different kinds of metrics: DR-STM These metrics are similar to the Syntactic Tree Matching metric defined by Liu and Gildea (2005), in this case applied to DRSs instead of constituent trees. All semantic subpaths in the candidate and reference trees are retrieved. The fraction of matching subpaths of a given length (l=4 in our experiments) is computed. DR-Or(*) Average lexical overlap between discourse representation structures of the same type. Overlap is measured according to the formulae and definitions by Gim´enez and M`arquez (2007). DR-Orp(*) Average morphosyntactic overlap, i.e., between grammatical categories –partsof-speech– associated to lexical items, between discourse representation structures of the same typ</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic Features for Evaluation of Machine Translation. In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Przybocki</author>
<author>Kay Peterson</author>
<author>S´ebastien Bronsart</author>
</authors>
<title>NIST Metrics for Machine Translation</title>
<date>2008</date>
<booktitle>Evaluation (MetricsMATR08). Technical report, National Institute of Standards and Technology.</booktitle>
<contexts>
<context position="1667" citStr="Przybocki et al., 2008" startWordPosition="228" endWordPosition="231">t and system scores are obtained using aggregate statistics over individual segments. This strategy presents the main disadvantage of ignoring crosssentential/discursive phenomena. In this work we suggest widening the scope of evaluation methods. We have defined genuine document-level measures which are able to exploit the structure of text to provide more informed evaluation scores. For that purpose we take advantage of two coincidental facts. First, test beds employed in recent MT evaluation campaigns include a document structure grouping sentences related to the same event, story or topic (Przybocki et al., 2008; Przybocki et al., 2009; Callison-Burch et al., 2009). Second, we count on automatic linguistic processors which provide very detailed discourselevel representations of text (Curran et al., 2007). Discourse representations allow us to focus on relevant pieces of information, such as the agent 1A segment typically consists of one or two sentences. (who), location (where), time (when), and theme (what), which may be spread all over the text. Counting on a means of discerning the events, the individuals taking part in each of them, and their role, is crucial to determine the semantic equivalence</context>
</contexts>
<marker>Przybocki, Peterson, Bronsart, 2008</marker>
<rawString>Mark Przybocki, Kay Peterson, and S´ebastien Bronsart. 2008. NIST Metrics for Machine Translation 2008 Evaluation (MetricsMATR08). Technical report, National Institute of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Przybocki</author>
<author>Kay Peterson</author>
<author>S´ebastien Bronsart</author>
</authors>
<date>2009</date>
<booktitle>NIST Open Machine Translation 2009 Evaluation (MT09). Technical report, National Institute of Standards and Technology.</booktitle>
<contexts>
<context position="1691" citStr="Przybocki et al., 2009" startWordPosition="232" endWordPosition="235">obtained using aggregate statistics over individual segments. This strategy presents the main disadvantage of ignoring crosssentential/discursive phenomena. In this work we suggest widening the scope of evaluation methods. We have defined genuine document-level measures which are able to exploit the structure of text to provide more informed evaluation scores. For that purpose we take advantage of two coincidental facts. First, test beds employed in recent MT evaluation campaigns include a document structure grouping sentences related to the same event, story or topic (Przybocki et al., 2008; Przybocki et al., 2009; Callison-Burch et al., 2009). Second, we count on automatic linguistic processors which provide very detailed discourselevel representations of text (Curran et al., 2007). Discourse representations allow us to focus on relevant pieces of information, such as the agent 1A segment typically consists of one or two sentences. (who), location (where), time (when), and theme (what), which may be spread all over the text. Counting on a means of discerning the events, the individuals taking part in each of them, and their role, is crucial to determine the semantic equivalence between a reference doc</context>
</contexts>
<marker>Przybocki, Peterson, Bronsart, 2009</marker>
<rawString>Mark Przybocki, Kay Peterson, and S´ebastien Bronsart. 2009. NIST Open Machine Translation 2009 Evaluation (MT09). Technical report, National Institute of Standards and Technology.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>