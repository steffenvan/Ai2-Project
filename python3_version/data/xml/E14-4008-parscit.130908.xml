<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001740">
<title confidence="0.995854">
Chasing Hypernyms in Vector Spaces with Entropy
</title>
<author confidence="0.984086">
Enrico Santus
</author>
<affiliation confidence="0.843931">
Dept. of Chinese and Bilingual Studies
The Hong Kong Polytechnic
University, Hong Kong
</affiliation>
<email confidence="0.997418">
e.santus@connect.polyu.hk
</email>
<author confidence="0.998259">
Qin Lu
</author>
<affiliation confidence="0.890121">
Dept. of Computing
The Hong Kong Polytechnic
University, Hong Kong
</affiliation>
<email confidence="0.988453">
csluqin@comp.polyu.edu.hk
</email>
<author confidence="0.6712585">
Alessandro Lenci
CoLing Lab – Dept. of Philology,
</author>
<affiliation confidence="0.8842895">
Literature, and Linguistics
University of Pisa, Italy
</affiliation>
<email confidence="0.996892">
alessandro.lenci@ling.unipi.it
</email>
<author confidence="0.998742">
Sabine Schulte im Walde
</author>
<affiliation confidence="0.922993333333333">
Inst. for Natural Language Processing
University of Stuttgart
Germany
</affiliation>
<email confidence="0.990254">
schulte@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.995095" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.952405">
In this paper, we introduce SLQS, a new
entropy-based measure for the unsupervised
identification of hypernymy and its
directionality in Distributional Semantic
Models (DSMs). SLQS is assessed through
two tasks: (i.) identifying the hypernym in
hyponym-hypernym pairs, and (ii.)
discriminating hypernymy among various
semantic relations. In both tasks, SLQS
outperforms other state-of-the-art measures.
</bodyText>
<sectionHeader confidence="0.99889" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996855272727272">
In recent years, Distributional Semantic Models
(DSMs) have gained much attention in
computational linguistics as unsupervised
methods to build lexical semantic representations
from corpus-derived co-occurrences encoded as
distributional vectors (Sahlgren, 2006; Turney
and Pantel, 2010). DSMs rely on the
Distributional Hypothesis (Harris, 1954) and
model lexical semantic similarity as a function of
distributional similarity, which is most
commonly measured with the vector cosine
(Turney and Pantel, 2010). DSMs have achieved
impressive results in tasks such as synonym
detection, semantic categorization, etc. (Padó and
Lapata, 2007; Baroni and Lenci, 2010).
One major shortcoming of current DSMs is
that they are not able to discriminate among
different types of semantic relations linking
distributionally similar lexemes. For instance, the
nearest neighbors of dog in vector spaces
typically include hypernyms like animal, co-
hyponyms like cat, meronyms like tail, together
with other words semantically related to dog.
DSMs tell us how similar these words are to dog,
but they do not give us a principled way to single
out the items linked by a specific relation (e.g.,
hypernyms).
Another related issue is to what extent
distributional similarity, as currently measured
by DSMs, is appropriate to model the semantic
properties of a relation like hypernymy, which is
crucial for Natural Language Processing.
Similarity is by definition a symmetric notion (a
is similar to b if and only if b is similar to a) and
it can therefore naturally model symmetric
semantic relations, such as synonymy and co-
hyponymy (Murphy, 2003). It is not clear,
however, how this notion can also model
hypernymy, which is asymmetric. In fact, it is
not enough to say that animal is distributionally
similar to dog. We must also account for the fact
that animal is semantically broader than dog:
every dog is an animal, but not every animal is a
dog.
</bodyText>
<page confidence="0.991969">
38
</page>
<note confidence="0.688066">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38–42,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999941764705882">
In this paper, we introduce SLQS, a new
entropy-based distributional measure that aims to
identify hypernyms by providing a distributional
characterization of their semantic generality. We
assess it with two tasks: (i.) the identification of
the broader term in hyponym-hypernym pairs
(directionality task); (ii.) the discrimination of
hypernymy among other semantic relations
(detection task). Given the centrality of
hypernymy, the relevance of the themes we
address hardly needs any further motivation.
Improving the ability of DSMs to identify
hypernyms is in fact extremely important in tasks
such as Recognizing Textual Entailment (RTE)
and ontology learning, as well as to enhance the
cognitive plausibility of DSMs as general models
of the semantic lexicon.
</bodyText>
<sectionHeader confidence="0.999593" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999968">
The problem of identifying asymmetric relations
like hypernymy has so far been addressed in
distributional semantics only in a limited way
(Kotlerman et al., 2010) or treated through semi-
supervised approaches, such as pattern-based
approaches (Hearst, 1992). The few works that
have attempted a completely unsupervised
approach to the identification of hypernymy in
corpora have mostly relied on some versions of
the Distributional Inclusion Hypothesis (DIH;
Weeds and Weir, 2003; Weeds et al., 2004),
according to which the contexts of a narrow term
are also shared by the broad term.
One of the first proposed measures
formalizing the DIH is WeedsPrec (Weeds and
Weir, 2003; Weeds et al., 2004), which
quantifies the weights of the features f of a
narrow term u that are included into the set of
features of a broad term v:
where Fx is the set of features of a term x, and
wx(f) is the weight of the feature f of the term x.
Variations of this measure have been introduced
by Clarke (2009), Kotlerman et al. (2010) and
Lenci and Benotto (2012).
In this paper, we adopt a different approach,
which is not based on DIH, but on the hypothesis
that hypernyms are semantically more general
than hyponyms, and therefore tend to occur in
less informative contexts than hypernyms.
</bodyText>
<sectionHeader confidence="0.891499" genericHeader="method">
3 SLQS: A new entropy-based measure
</sectionHeader>
<bodyText confidence="0.999839264705882">
DIH is grounded on an “extensional” definition
of the asymmetric character of hypernymy: since
the class (i.e., extension) denoted by a hyponym
is included in the class denoted by the hypernym,
hyponyms are expected to occur in a subset of
the contexts of their hypernyms. However, it is
also possible to provide an “intensional”
definition of the same asymmetry. In fact, the
typical characteristics making up the “intension”
(i.e., concept) expressed by a hypernym (e.g.,
move or eat for animal) are semantically more
general than the characteristics forming the
“intension” of its hyponyms (e.g., bark or has fur
for dog). This corresponds to the idea that
superordinate terms like animal are less
informative than their hyponyms (Murphy, 2002).
From a distributional point of view, we can
therefore expect that the most typical linguistic
contexts of a hypernym are less informative than
the most typical linguistic contexts of its
hyponyms. In fact, contexts such as bark and has
fur are likely to co-occur with a smaller number
of words than move and eat. Starting from this
hypothesis and using entropy as an estimate of
context informativeness (Shannon, 1948), we
propose SLQS, which measures the semantic
generality of a word by the entropy of its
statistically most prominent contexts.
For every term wi we identify the N most
associated contexts c (where N is a parameter
empirically set to 50)1. The association strength
has been calculated with Local Mutual
Information (LMI; Evert, 2005). For each
selected context c, we define its entropy H(c) as:
</bodyText>
<footnote confidence="0.752431">
1 N=50 is the result of an optimization of the model
against the dataset after trying the following
suboptimal values: 5, 10, 25, 75 and 100.
</footnote>
<equation confidence="0.79108">
Z�EF�nF�W1( )
TAFeedsPre�(�,�) _
ZfEFuW1( )
39
n
H(C) = � I P(MC) • 1 092(P(MC�)
i=1
</equation>
<bodyText confidence="0.999850125">
where p(fi|c) is the probability of the feature fi
given the context c, obtained through the ratio
between the frequency of &lt;c, fi&gt; and the total
frequency of c. The resulting values H(c) are
then normalized in the range 0-1 by using the
Min-Max-Scaling (Priddy and Keller, 2005):
Hn(c). Finally, for each term wi we calculate the
median entropy Ewi of its I contexts:
</bodyText>
<equation confidence="0.8166175">
Ewi = M�J��
� (Hn(CJ))
</equation>
<bodyText confidence="0.961151230769231">
Ewi can be considered as a semantic generality
index for the term wi: the higher Ewi, the more
semantically general wi is. SLQS is then defined
as the reciprocal difference between the semantic
generality Ew/ and Ew� of two terms w1 and w2:
SLQS(W,, W2) = 1
According to this formula, SLQS&lt;0, if Ew/&gt;Ew2;
SLQS≃0, if Ew/≃Ew2; and SLQS&gt;0, if Ew/&lt;Ew2.
SLQS is an asymmetric measure because, by
definition, SLQS(w1,w2)≠SLQS(w2,w1) (except
when w1 and w2 have exactly the same
generality). Therefore, if SLQS(w1,w2)&gt;0, w1 is
semantically less general than w2.
</bodyText>
<sectionHeader confidence="0.995938" genericHeader="evaluation">
4 Experiments and evaluation
</sectionHeader>
<subsectionHeader confidence="0.996384">
4.1 The DSM and the dataset
</subsectionHeader>
<bodyText confidence="0.999948217391305">
For the experiments, we used a standard
window-based DSM recording co-occurrences
with the nearest 2 content words to the left and
right of each target word. Co-occurrences were
extracted from a combination of the freely
available ukWaC and WaCkypedia corpora (with
1.915 billion and 820 million words, respectively)
and weighted with LMI.
To assess SLQS we relied on a subset of
BLESS (Baroni and Lenci, 2011), a freely-
available dataset that includes 200 distinct
English concrete nouns as target concepts,
equally divided between living and non-living
entities (e.g. BIRD, FRUIT, etc.). For each target
concept, BLESS contains several relata,
connected to it through one relation, such as co-
hyponymy (COORD), hypernymy (HYPER),
meronymy (MERO) or no-relation (RANDOM-N).2
Since BLESS contains different numbers of
pairs for every relation, we randomly extracted a
subset of 1,277 pairs for each relation, where
1,277 is the maximum number of HYPER-related
pairs for which vectors existed in our DSM.
</bodyText>
<subsectionHeader confidence="0.990494">
4.2 Task 1: Directionality
</subsectionHeader>
<bodyText confidence="0.999768363636364">
In this experiment we aimed at identifying the
hypernym in the 1,277 hypernymy-related pairs
of our dataset. Since the HYPER-related pairs in
BLESS are in the order hyponym-hypernym (e.g.
eagle-bird, eagle-animal, etc.), the hypernym in
a pair (w1,w2) is correctly identified by SLQS, if
SLQS (w1,w2) &gt; 0. Following Weeds et al. (2004),
we used word frequency as a baseline model.
This baseline is grounded on the hypothesis that
hypernyms are more frequent than hyponyms in
corpora. Table 1 gives the evaluation results:
</bodyText>
<table confidence="0.9903394">
SLQS WeedsPrec BASELINE
POSITIVE 1111 805 844
NEGATIVE 166 472 433
TOTAL 1277 1277 1277
PRECISION 87.00% 63.04% 66.09%
</table>
<tableCaption confidence="0.997522">
Table 1. Accuracy for Task 1.
</tableCaption>
<bodyText confidence="0.999820636363636">
As it can be seen in Table 1, SLQS scores a
precision of 87% in identifying the second term
of the test pairs as the hypernym. This result is
particularly significant when compared to the
one obtained by applying WeedsPrec (+23.96%).
As it was also noticed by Geffet and Dagan
(2005) with reference to a previous similar
experiment performed on a different corpus
(Weeds et al., 2004), the WeedsPrec precision in
this task is comparable to the naïve baseline.
SLQS scores instead a +20.91%.
</bodyText>
<footnote confidence="0.6618735">
2 In these experiments, we only consider the BLESS
pairs containing a noun relatum.
</footnote>
<figure confidence="0.4286235">
Ew/
Ew2
</figure>
<page confidence="0.978674">
40
</page>
<subsectionHeader confidence="0.988775">
4.3 Task 2: Detection
</subsectionHeader>
<bodyText confidence="0.998148777777778">
The second experiment aimed at discriminating
HYPER test pairs from those linked by other
types of relations in BLESS (i. e., MERO, COORD
and RANDOM-N). To this purpose, we assumed
that hypernymy is cha
racterized by two main
properties: (
i.) the hypernym and the hyponym
are distributionally similar (in the sense of the
Distributional Hypothesis)
, and (ii.) the
hyponym is semantically less general than the
hypernym. We measured the first property with
the vector cosine and the second one with SLQS.
After calculating SLQS for all the pairs in our
datasets, we set to zero all the negative values,
that is to say those in which – according to
SLQS – the first term is semantically more
general than the second one. Then, we combined
SLQS and vector cosine by their product. The
greater the resulting value, the greater the
likelihood that we are considering a hypernymy-
related pair, in which the first word is a hyponym
and the second word is a hypernym.
To evaluate the performance of SLQS, we
used Average Precision (AP; Kotlerman et al.,
2010), a method derived from Information
Retrieval that combines precision, relevance
ranking and overall recall, returning a value that
ranges from 0 to 1. AP=1 means that all the
instances of a relation are in the top of the rank,
whereas AP=0 means they are in the bottom. AP
is calculated for the four relations we extracted
from BLESS. SLQS was also compared with
WeedsPrec and vector cosine, again using
frequency as baseline. Table 2 shows the results:
</bodyText>
<table confidence="0.992201166666667">
HYPER COORD MERO RANDOM
Baseline 0.40 0.51 0.38 0.17
Cosine 0.48 0.46 0.31 0.21
WeedsPrec 0.50 0.35 0.39 0.21
SLQS * 0.59 0.27 0.35 0.24
Cosine
</table>
<tableCaption confidence="0.999577">
Table 2. AP values for Task 2.
</tableCaption>
<bodyText confidence="0.999335045454545">
The AP values show the performances of the
tested measures on the four relations. The
optimal result would be obtained scoring 1 for
HYPER and 0 for the other relations.
The product between SLQS and vector cosine
gets the best performance in identifying HYPER
(+0.09 in comparison to WeedsPrec) and in
discriminating it from COORD (-0.08 than
WeedsPrec). It also achieves better results in
discriminating MERO (-0. 04 than WeedsPrec).
On the other hand, it seems to get a slightly
lower precision in discriminating RANDOM-N
(+0.03 in comparison to WeedsPrec). The likely
reason is th
at unrelated pairs might also have a
fairly high semantic generality difference,
slightly affecting the measure’s performance.
Figure 1 gives a graphic depiction of the
performances. SLQS corresponds to the black
line in comparison to the WeedsPrec (black
borders, grey fill), the vector cosine (grey
borders) and the baseline (grey fill).
</bodyText>
<figureCaption confidence="0.995294">
Figure 1. AP values for Task 2.
</figureCaption>
<sectionHeader confidence="0.961816" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999034736842105">
In this paper, we have proposed SLQS, a new
asymmetric distributional measure of semantic
generality which is able to identify the broader
term in a hypernym-hyponym pair and, when
combined with vector cosine, to discriminate
hypernymy from other types of semantic
relations. The successful performance of SLQS in
the reported experiments confirms that
hyponyms and hypernyms are distributionally
similar, but hyponyms t
end to occur in more
informative contexts than hypernyms. SLQS
shows that an “intensional” characterization of
hypernymy can be pursued in distributional
terms. This opens u
p new possibilities for the
study of semantic relations in DSMs. In further
research, SLQS will also be tested on other
datasets and languages.
</bodyText>
<page confidence="0.999288">
41
</page>
<sectionHeader confidence="0.988812" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999703718309859">
Baroni, Marco and Lenci, Alessandro. 2010.
“Distributional Memory: A general framework for
corpus-based semantics”. Computational
Linguistics, Vol. 36 (4). 673-721.
Baroni, Marco and Lenci, Alessandro. 2011. “How
we BLESSed distributional semantic
evaluation”. Proceedings of the EMNLP 2011
Geometrical Models for Natural Language
Semantics (GEMS 2011) Workshop. Edinburg, UK.
1-10.
Clarke, Daoud. 2009. “Context-theoretic semantics
for natural language: An overview”. Proceedings
of the Workshop on Geometrical Models of Natural
Language Semantics. Athens, Greece. 112-119.
Evert, Stefan. 2005. The Statistics of Word
Cooccurrences. Dissertation, Stuttgart University.
Geffet, Maayan and Dagan, Idan. 2005. “The
Distributional Inclusion Hypotheses and Lexical
Entailment”. Proceedings of 43rd Annual Meeting
of the ACL. Michigan, USA. 107-114.
Harris, Zellig. 1954. “Distributional structure”. Word,
Vol. 10 (23). 146-162.
Hearst, Marti A. 1992. “Automatic Acquisition of
Hyponyms from Large Text Corpora”.
Proceedings of the 14th International Conference
on Computational Linguistics. Nantes, France.
539-545.
Kotlerman, Lili, Dagan, Ido, Szpektor, Idan, and
Zhitomirsky-Geffet, Maayan. 2010. “Directional
Distributional Similarity for Lexical Inference”.
Natural Language Engineering, Vol. 16 (4). 359-
389.
Lenci, Alessandro and Benotto, Giulia. 2012.
“Identifying hypernyms in distributional semantic
spaces”. SEM 2012 – The First Joint Conference
on Lexical and Computational Semantics. Montréal,
Canada. Vol. 2. 75-79.
Murphy, Gregory L.. 2002. The Big Book of Concepts.
The MIT Press, Cambridge, MA.
Murphy, M. Lynne. 2003. Lexical meaning.
Cambridge University Press, Cambridge.
Padó, Sebastian and Lapata, Mirella. 2007.
“Dependency-based Construction of Semantic
Space Models”. Computational Linguistics, Vol.
33 (2). 161-199.
Priddy, Kevin L. and Keller, Paul E. 2005. Artificial
Neural Networks: An Introduction. SPIE Press -
International Society for Optical Engineering,
October 2005.
Sahlgren, Magnus. 2006. The Word-Space Model:
Using distributional analysis to represent
syntagmatic and paradigmatic relations between
words in high-dimensional vector spaces. Ph.D.
dissertation, Department of Linguistics, Stockholm
University.
Shannon, Claude E. 1948. “A mathematical theory of
communication”. Bell System Technical Journal,
Vol. 27. 379-423 and 623-656.
Turney, Peter D. and Pantel, Patrick. 2010. “From
Frequency to Meaning: Vector Space Models of
Semantics”. Journal of Articial Intelligence
Research, Vol. 37. 141-188.
Weeds, Julie and Weir, David. 2003. “A general
framework for distributional similarity”.
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing. Sapporo,
Japan. 81-88.
Weeds, Julie, Weir, David and McCarthy, Diana.
2004. “Characterising measures of lexical
distributional similarity”. Proceedings of COLING
2004. Geneva, Switzerland.1015-1021.
</reference>
<page confidence="0.999298">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.137356">
<title confidence="0.999504">Chasing Hypernyms in Vector Spaces with Entropy</title>
<author confidence="0.993837">Enrico</author>
<affiliation confidence="0.99685">Dept. of Chinese and Bilingual</affiliation>
<address confidence="0.773619">The Hong Kong University, Hong Kong</address>
<email confidence="0.991499">e.santus@connect.polyu.hk</email>
<author confidence="0.82897">Qin</author>
<affiliation confidence="0.7958135">Dept. of The Hong Kong</affiliation>
<address confidence="0.976752">University, Hong Kong</address>
<email confidence="0.988296">csluqin@comp.polyu.edu.hk</email>
<author confidence="0.901789">Alessandro</author>
<affiliation confidence="0.779678666666667">CoLing Lab – Dept. of Literature, and University of Pisa, Italy</affiliation>
<email confidence="0.980586">alessandro.lenci@ling.unipi.it</email>
<author confidence="0.971114">Sabine Schulte im</author>
<affiliation confidence="0.997249">Inst. for Natural Language University of</affiliation>
<address confidence="0.979522">Germany</address>
<email confidence="0.99858">schulte@ims.uni-stuttgart.de</email>
<abstract confidence="0.998133818181818">this paper, we introduce a new entropy-based measure for the unsupervised identification of hypernymy and its directionality in Distributional Semantic (DSMs). assessed through two tasks: (i.) identifying the hypernym in hyponym-hypernym pairs, and (ii.) discriminating hypernymy among various relations. In both tasks, outperforms other state-of-the-art measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional Memory: A general framework for corpus-based semantics”.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<pages>673--721</pages>
<contexts>
<context position="1622" citStr="Baroni and Lenci, 2010" startWordPosition="207" endWordPosition="210">c Models (DSMs) have gained much attention in computational linguistics as unsupervised methods to build lexical semantic representations from corpus-derived co-occurrences encoded as distributional vectors (Sahlgren, 2006; Turney and Pantel, 2010). DSMs rely on the Distributional Hypothesis (Harris, 1954) and model lexical semantic similarity as a function of distributional similarity, which is most commonly measured with the vector cosine (Turney and Pantel, 2010). DSMs have achieved impressive results in tasks such as synonym detection, semantic categorization, etc. (Padó and Lapata, 2007; Baroni and Lenci, 2010). One major shortcoming of current DSMs is that they are not able to discriminate among different types of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of dog in vector spaces typically include hypernyms like animal, cohyponyms like cat, meronyms like tail, together with other words semantically related to dog. DSMs tell us how similar these words are to dog, but they do not give us a principled way to single out the items linked by a specific relation (e.g., hypernyms). Another related issue is to what extent distributional similarity, as cu</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Baroni, Marco and Lenci, Alessandro. 2010. “Distributional Memory: A general framework for corpus-based semantics”. Computational Linguistics, Vol. 36 (4). 673-721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>How we BLESSed distributional semantic evaluation”.</title>
<date>2011</date>
<booktitle>Proceedings of the EMNLP 2011 Geometrical Models for Natural Language Semantics (GEMS 2011) Workshop.</booktitle>
<pages>1--10</pages>
<location>Edinburg, UK.</location>
<contexts>
<context position="8404" citStr="Baroni and Lenci, 2011" startWordPosition="1313" endWordPosition="1316">on, SLQS(w1,w2)≠SLQS(w2,w1) (except when w1 and w2 have exactly the same generality). Therefore, if SLQS(w1,w2)&gt;0, w1 is semantically less general than w2. 4 Experiments and evaluation 4.1 The DSM and the dataset For the experiments, we used a standard window-based DSM recording co-occurrences with the nearest 2 content words to the left and right of each target word. Co-occurrences were extracted from a combination of the freely available ukWaC and WaCkypedia corpora (with 1.915 billion and 820 million words, respectively) and weighted with LMI. To assess SLQS we relied on a subset of BLESS (Baroni and Lenci, 2011), a freelyavailable dataset that includes 200 distinct English concrete nouns as target concepts, equally divided between living and non-living entities (e.g. BIRD, FRUIT, etc.). For each target concept, BLESS contains several relata, connected to it through one relation, such as cohyponymy (COORD), hypernymy (HYPER), meronymy (MERO) or no-relation (RANDOM-N).2 Since BLESS contains different numbers of pairs for every relation, we randomly extracted a subset of 1,277 pairs for each relation, where 1,277 is the maximum number of HYPER-related pairs for which vectors existed in our DSM. 4.2 Task</context>
</contexts>
<marker>Baroni, Lenci, 2011</marker>
<rawString>Baroni, Marco and Lenci, Alessandro. 2011. “How we BLESSed distributional semantic evaluation”. Proceedings of the EMNLP 2011 Geometrical Models for Natural Language Semantics (GEMS 2011) Workshop. Edinburg, UK. 1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Context-theoretic semantics for natural language: An overview”.</title>
<date>2009</date>
<booktitle>Proceedings of the Workshop on Geometrical Models of Natural Language Semantics.</booktitle>
<pages>112--119</pages>
<location>Athens,</location>
<contexts>
<context position="4880" citStr="Clarke (2009)" startWordPosition="733" endWordPosition="734"> mostly relied on some versions of the Distributional Inclusion Hypothesis (DIH; Weeds and Weir, 2003; Weeds et al., 2004), according to which the contexts of a narrow term are also shared by the broad term. One of the first proposed measures formalizing the DIH is WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), which quantifies the weights of the features f of a narrow term u that are included into the set of features of a broad term v: where Fx is the set of features of a term x, and wx(f) is the weight of the feature f of the term x. Variations of this measure have been introduced by Clarke (2009), Kotlerman et al. (2010) and Lenci and Benotto (2012). In this paper, we adopt a different approach, which is not based on DIH, but on the hypothesis that hypernyms are semantically more general than hyponyms, and therefore tend to occur in less informative contexts than hypernyms. 3 SLQS: A new entropy-based measure DIH is grounded on an “extensional” definition of the asymmetric character of hypernymy: since the class (i.e., extension) denoted by a hyponym is included in the class denoted by the hypernym, hyponyms are expected to occur in a subset of the contexts of their hypernyms. However</context>
</contexts>
<marker>Clarke, 2009</marker>
<rawString>Clarke, Daoud. 2009. “Context-theoretic semantics for natural language: An overview”. Proceedings of the Workshop on Geometrical Models of Natural Language Semantics. Athens, Greece. 112-119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>The Statistics of Word Cooccurrences. Dissertation,</title>
<date>2005</date>
<institution>Stuttgart University.</institution>
<contexts>
<context position="6701" citStr="Evert, 2005" startWordPosition="1026" endWordPosition="1027">an the most typical linguistic contexts of its hyponyms. In fact, contexts such as bark and has fur are likely to co-occur with a smaller number of words than move and eat. Starting from this hypothesis and using entropy as an estimate of context informativeness (Shannon, 1948), we propose SLQS, which measures the semantic generality of a word by the entropy of its statistically most prominent contexts. For every term wi we identify the N most associated contexts c (where N is a parameter empirically set to 50)1. The association strength has been calculated with Local Mutual Information (LMI; Evert, 2005). For each selected context c, we define its entropy H(c) as: 1 N=50 is the result of an optimization of the model against the dataset after trying the following suboptimal values: 5, 10, 25, 75 and 100. Z�EF�nF�W1( ) TAFeedsPre�(�,�) _ ZfEFuW1( ) 39 n H(C) = � I P(MC) • 1 092(P(MC�) i=1 where p(fi|c) is the probability of the feature fi given the context c, obtained through the ratio between the frequency of &lt;c, fi&gt; and the total frequency of c. The resulting values H(c) are then normalized in the range 0-1 by using the Min-Max-Scaling (Priddy and Keller, 2005): Hn(c). Finally, for each term </context>
</contexts>
<marker>Evert, 2005</marker>
<rawString>Evert, Stefan. 2005. The Statistics of Word Cooccurrences. Dissertation, Stuttgart University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Geffet</author>
<author>Idan Dagan</author>
</authors>
<title>The Distributional Inclusion Hypotheses and Lexical Entailment”.</title>
<date>2005</date>
<booktitle>Proceedings of 43rd Annual Meeting of the ACL.</booktitle>
<pages>107--114</pages>
<location>Michigan, USA.</location>
<contexts>
<context position="9977" citStr="Geffet and Dagan (2005)" startWordPosition="1564" endWordPosition="1567">004), we used word frequency as a baseline model. This baseline is grounded on the hypothesis that hypernyms are more frequent than hyponyms in corpora. Table 1 gives the evaluation results: SLQS WeedsPrec BASELINE POSITIVE 1111 805 844 NEGATIVE 166 472 433 TOTAL 1277 1277 1277 PRECISION 87.00% 63.04% 66.09% Table 1. Accuracy for Task 1. As it can be seen in Table 1, SLQS scores a precision of 87% in identifying the second term of the test pairs as the hypernym. This result is particularly significant when compared to the one obtained by applying WeedsPrec (+23.96%). As it was also noticed by Geffet and Dagan (2005) with reference to a previous similar experiment performed on a different corpus (Weeds et al., 2004), the WeedsPrec precision in this task is comparable to the naïve baseline. SLQS scores instead a +20.91%. 2 In these experiments, we only consider the BLESS pairs containing a noun relatum. Ew/ Ew2 40 4.3 Task 2: Detection The second experiment aimed at discriminating HYPER test pairs from those linked by other types of relations in BLESS (i. e., MERO, COORD and RANDOM-N). To this purpose, we assumed that hypernymy is cha racterized by two main properties: ( i.) the hypernym and the hyponym ar</context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>Geffet, Maayan and Dagan, Idan. 2005. “The Distributional Inclusion Hypotheses and Lexical Entailment”. Proceedings of 43rd Annual Meeting of the ACL. Michigan, USA. 107-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure”. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<pages>146--162</pages>
<contexts>
<context position="1306" citStr="Harris, 1954" startWordPosition="163" endWordPosition="164"> Semantic Models (DSMs). SLQS is assessed through two tasks: (i.) identifying the hypernym in hyponym-hypernym pairs, and (ii.) discriminating hypernymy among various semantic relations. In both tasks, SLQS outperforms other state-of-the-art measures. 1 Introduction In recent years, Distributional Semantic Models (DSMs) have gained much attention in computational linguistics as unsupervised methods to build lexical semantic representations from corpus-derived co-occurrences encoded as distributional vectors (Sahlgren, 2006; Turney and Pantel, 2010). DSMs rely on the Distributional Hypothesis (Harris, 1954) and model lexical semantic similarity as a function of distributional similarity, which is most commonly measured with the vector cosine (Turney and Pantel, 2010). DSMs have achieved impressive results in tasks such as synonym detection, semantic categorization, etc. (Padó and Lapata, 2007; Baroni and Lenci, 2010). One major shortcoming of current DSMs is that they are not able to discriminate among different types of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of dog in vector spaces typically include hypernyms like animal, cohyponyms like</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Harris, Zellig. 1954. “Distributional structure”. Word, Vol. 10 (23). 146-162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora”.</title>
<date>1992</date>
<booktitle>Proceedings of the 14th International Conference on Computational Linguistics.</booktitle>
<pages>539--545</pages>
<location>Nantes,</location>
<contexts>
<context position="4146" citStr="Hearst, 1992" startWordPosition="601" endWordPosition="602">ymy, the relevance of the themes we address hardly needs any further motivation. Improving the ability of DSMs to identify hypernyms is in fact extremely important in tasks such as Recognizing Textual Entailment (RTE) and ontology learning, as well as to enhance the cognitive plausibility of DSMs as general models of the semantic lexicon. 2 Related work The problem of identifying asymmetric relations like hypernymy has so far been addressed in distributional semantics only in a limited way (Kotlerman et al., 2010) or treated through semisupervised approaches, such as pattern-based approaches (Hearst, 1992). The few works that have attempted a completely unsupervised approach to the identification of hypernymy in corpora have mostly relied on some versions of the Distributional Inclusion Hypothesis (DIH; Weeds and Weir, 2003; Weeds et al., 2004), according to which the contexts of a narrow term are also shared by the broad term. One of the first proposed measures formalizing the DIH is WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), which quantifies the weights of the features f of a narrow term u that are included into the set of features of a broad term v: where Fx is the set of features</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Hearst, Marti A. 1992. “Automatic Acquisition of Hyponyms from Large Text Corpora”. Proceedings of the 14th International Conference on Computational Linguistics. Nantes, France. 539-545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional Distributional Similarity for Lexical Inference”.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<pages>359--389</pages>
<contexts>
<context position="4052" citStr="Kotlerman et al., 2010" startWordPosition="587" endWordPosition="590">crimination of hypernymy among other semantic relations (detection task). Given the centrality of hypernymy, the relevance of the themes we address hardly needs any further motivation. Improving the ability of DSMs to identify hypernyms is in fact extremely important in tasks such as Recognizing Textual Entailment (RTE) and ontology learning, as well as to enhance the cognitive plausibility of DSMs as general models of the semantic lexicon. 2 Related work The problem of identifying asymmetric relations like hypernymy has so far been addressed in distributional semantics only in a limited way (Kotlerman et al., 2010) or treated through semisupervised approaches, such as pattern-based approaches (Hearst, 1992). The few works that have attempted a completely unsupervised approach to the identification of hypernymy in corpora have mostly relied on some versions of the Distributional Inclusion Hypothesis (DIH; Weeds and Weir, 2003; Weeds et al., 2004), according to which the contexts of a narrow term are also shared by the broad term. One of the first proposed measures formalizing the DIH is WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), which quantifies the weights of the features f of a narrow term u</context>
<context position="11354" citStr="Kotlerman et al., 2010" startWordPosition="1799" endWordPosition="1802">ured the first property with the vector cosine and the second one with SLQS. After calculating SLQS for all the pairs in our datasets, we set to zero all the negative values, that is to say those in which – according to SLQS – the first term is semantically more general than the second one. Then, we combined SLQS and vector cosine by their product. The greater the resulting value, the greater the likelihood that we are considering a hypernymyrelated pair, in which the first word is a hyponym and the second word is a hypernym. To evaluate the performance of SLQS, we used Average Precision (AP; Kotlerman et al., 2010), a method derived from Information Retrieval that combines precision, relevance ranking and overall recall, returning a value that ranges from 0 to 1. AP=1 means that all the instances of a relation are in the top of the rank, whereas AP=0 means they are in the bottom. AP is calculated for the four relations we extracted from BLESS. SLQS was also compared with WeedsPrec and vector cosine, again using frequency as baseline. Table 2 shows the results: HYPER COORD MERO RANDOM Baseline 0.40 0.51 0.38 0.17 Cosine 0.48 0.46 0.31 0.21 WeedsPrec 0.50 0.35 0.39 0.21 SLQS * 0.59 0.27 0.35 0.24 Cosine T</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Kotlerman, Lili, Dagan, Ido, Szpektor, Idan, and Zhitomirsky-Geffet, Maayan. 2010. “Directional Distributional Similarity for Lexical Inference”. Natural Language Engineering, Vol. 16 (4). 359-389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
<author>Giulia Benotto</author>
</authors>
<title>Identifying hypernyms in distributional semantic spaces”.</title>
<date>2012</date>
<booktitle>SEM 2012 – The First Joint Conference on Lexical and Computational Semantics.</booktitle>
<volume>2</volume>
<pages>75--79</pages>
<location>Montréal,</location>
<contexts>
<context position="4934" citStr="Lenci and Benotto (2012)" startWordPosition="740" endWordPosition="743">ributional Inclusion Hypothesis (DIH; Weeds and Weir, 2003; Weeds et al., 2004), according to which the contexts of a narrow term are also shared by the broad term. One of the first proposed measures formalizing the DIH is WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), which quantifies the weights of the features f of a narrow term u that are included into the set of features of a broad term v: where Fx is the set of features of a term x, and wx(f) is the weight of the feature f of the term x. Variations of this measure have been introduced by Clarke (2009), Kotlerman et al. (2010) and Lenci and Benotto (2012). In this paper, we adopt a different approach, which is not based on DIH, but on the hypothesis that hypernyms are semantically more general than hyponyms, and therefore tend to occur in less informative contexts than hypernyms. 3 SLQS: A new entropy-based measure DIH is grounded on an “extensional” definition of the asymmetric character of hypernymy: since the class (i.e., extension) denoted by a hyponym is included in the class denoted by the hypernym, hyponyms are expected to occur in a subset of the contexts of their hypernyms. However, it is also possible to provide an “intensional” defi</context>
</contexts>
<marker>Lenci, Benotto, 2012</marker>
<rawString>Lenci, Alessandro and Benotto, Giulia. 2012. “Identifying hypernyms in distributional semantic spaces”. SEM 2012 – The First Joint Conference on Lexical and Computational Semantics. Montréal, Canada. Vol. 2. 75-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory L Murphy</author>
</authors>
<title>The Big Book of Concepts.</title>
<date>2002</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5947" citStr="Murphy, 2002" startWordPosition="903" endWordPosition="904">a hyponym is included in the class denoted by the hypernym, hyponyms are expected to occur in a subset of the contexts of their hypernyms. However, it is also possible to provide an “intensional” definition of the same asymmetry. In fact, the typical characteristics making up the “intension” (i.e., concept) expressed by a hypernym (e.g., move or eat for animal) are semantically more general than the characteristics forming the “intension” of its hyponyms (e.g., bark or has fur for dog). This corresponds to the idea that superordinate terms like animal are less informative than their hyponyms (Murphy, 2002). From a distributional point of view, we can therefore expect that the most typical linguistic contexts of a hypernym are less informative than the most typical linguistic contexts of its hyponyms. In fact, contexts such as bark and has fur are likely to co-occur with a smaller number of words than move and eat. Starting from this hypothesis and using entropy as an estimate of context informativeness (Shannon, 1948), we propose SLQS, which measures the semantic generality of a word by the entropy of its statistically most prominent contexts. For every term wi we identify the N most associated</context>
</contexts>
<marker>Murphy, 2002</marker>
<rawString>Murphy, Gregory L.. 2002. The Big Book of Concepts. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lynne Murphy</author>
</authors>
<title>Lexical meaning.</title>
<date>2003</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="2589" citStr="Murphy, 2003" startWordPosition="365" endWordPosition="366">ed to dog. DSMs tell us how similar these words are to dog, but they do not give us a principled way to single out the items linked by a specific relation (e.g., hypernyms). Another related issue is to what extent distributional similarity, as currently measured by DSMs, is appropriate to model the semantic properties of a relation like hypernymy, which is crucial for Natural Language Processing. Similarity is by definition a symmetric notion (a is similar to b if and only if b is similar to a) and it can therefore naturally model symmetric semantic relations, such as synonymy and cohyponymy (Murphy, 2003). It is not clear, however, how this notion can also model hypernymy, which is asymmetric. In fact, it is not enough to say that animal is distributionally similar to dog. We must also account for the fact that animal is semantically broader than dog: every dog is an animal, but not every animal is a dog. 38 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38–42, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics In this paper, we introduce SLQS, a new entropy-based distributional measure that</context>
</contexts>
<marker>Murphy, 2003</marker>
<rawString>Murphy, M. Lynne. 2003. Lexical meaning. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Padó</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based Construction of Semantic Space Models”.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<pages>161--199</pages>
<contexts>
<context position="1597" citStr="Padó and Lapata, 2007" startWordPosition="203" endWordPosition="206"> Distributional Semantic Models (DSMs) have gained much attention in computational linguistics as unsupervised methods to build lexical semantic representations from corpus-derived co-occurrences encoded as distributional vectors (Sahlgren, 2006; Turney and Pantel, 2010). DSMs rely on the Distributional Hypothesis (Harris, 1954) and model lexical semantic similarity as a function of distributional similarity, which is most commonly measured with the vector cosine (Turney and Pantel, 2010). DSMs have achieved impressive results in tasks such as synonym detection, semantic categorization, etc. (Padó and Lapata, 2007; Baroni and Lenci, 2010). One major shortcoming of current DSMs is that they are not able to discriminate among different types of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of dog in vector spaces typically include hypernyms like animal, cohyponyms like cat, meronyms like tail, together with other words semantically related to dog. DSMs tell us how similar these words are to dog, but they do not give us a principled way to single out the items linked by a specific relation (e.g., hypernyms). Another related issue is to what extent distrib</context>
</contexts>
<marker>Padó, Lapata, 2007</marker>
<rawString>Padó, Sebastian and Lapata, Mirella. 2007. “Dependency-based Construction of Semantic Space Models”. Computational Linguistics, Vol. 33 (2). 161-199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin L Priddy</author>
<author>Paul E Keller</author>
</authors>
<title>Artificial Neural Networks: An Introduction.</title>
<date>2005</date>
<publisher>SPIE Press -International Society</publisher>
<institution>for Optical Engineering,</institution>
<contexts>
<context position="7269" citStr="Priddy and Keller, 2005" startWordPosition="1126" endWordPosition="1129">culated with Local Mutual Information (LMI; Evert, 2005). For each selected context c, we define its entropy H(c) as: 1 N=50 is the result of an optimization of the model against the dataset after trying the following suboptimal values: 5, 10, 25, 75 and 100. Z�EF�nF�W1( ) TAFeedsPre�(�,�) _ ZfEFuW1( ) 39 n H(C) = � I P(MC) • 1 092(P(MC�) i=1 where p(fi|c) is the probability of the feature fi given the context c, obtained through the ratio between the frequency of &lt;c, fi&gt; and the total frequency of c. The resulting values H(c) are then normalized in the range 0-1 by using the Min-Max-Scaling (Priddy and Keller, 2005): Hn(c). Finally, for each term wi we calculate the median entropy Ewi of its I contexts: Ewi = M�J�� � (Hn(CJ)) Ewi can be considered as a semantic generality index for the term wi: the higher Ewi, the more semantically general wi is. SLQS is then defined as the reciprocal difference between the semantic generality Ew/ and Ew� of two terms w1 and w2: SLQS(W,, W2) = 1 According to this formula, SLQS&lt;0, if Ew/&gt;Ew2; SLQS≃0, if Ew/≃Ew2; and SLQS&gt;0, if Ew/&lt;Ew2. SLQS is an asymmetric measure because, by definition, SLQS(w1,w2)≠SLQS(w2,w1) (except when w1 and w2 have exactly the same generality). Th</context>
</contexts>
<marker>Priddy, Keller, 2005</marker>
<rawString>Priddy, Kevin L. and Keller, Paul E. 2005. Artificial Neural Networks: An Introduction. SPIE Press -International Society for Optical Engineering, October 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces.</title>
<date>2006</date>
<institution>Department of Linguistics, Stockholm University.</institution>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="1221" citStr="Sahlgren, 2006" startWordPosition="151" endWordPosition="152"> the unsupervised identification of hypernymy and its directionality in Distributional Semantic Models (DSMs). SLQS is assessed through two tasks: (i.) identifying the hypernym in hyponym-hypernym pairs, and (ii.) discriminating hypernymy among various semantic relations. In both tasks, SLQS outperforms other state-of-the-art measures. 1 Introduction In recent years, Distributional Semantic Models (DSMs) have gained much attention in computational linguistics as unsupervised methods to build lexical semantic representations from corpus-derived co-occurrences encoded as distributional vectors (Sahlgren, 2006; Turney and Pantel, 2010). DSMs rely on the Distributional Hypothesis (Harris, 1954) and model lexical semantic similarity as a function of distributional similarity, which is most commonly measured with the vector cosine (Turney and Pantel, 2010). DSMs have achieved impressive results in tasks such as synonym detection, semantic categorization, etc. (Padó and Lapata, 2007; Baroni and Lenci, 2010). One major shortcoming of current DSMs is that they are not able to discriminate among different types of semantic relations linking distributionally similar lexemes. For instance, the nearest neigh</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Sahlgren, Magnus. 2006. The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces. Ph.D. dissertation, Department of Linguistics, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude E Shannon</author>
</authors>
<title>A mathematical theory of communication”.</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<volume>27</volume>
<pages>379--423</pages>
<contexts>
<context position="6367" citStr="Shannon, 1948" startWordPosition="972" endWordPosition="973">forming the “intension” of its hyponyms (e.g., bark or has fur for dog). This corresponds to the idea that superordinate terms like animal are less informative than their hyponyms (Murphy, 2002). From a distributional point of view, we can therefore expect that the most typical linguistic contexts of a hypernym are less informative than the most typical linguistic contexts of its hyponyms. In fact, contexts such as bark and has fur are likely to co-occur with a smaller number of words than move and eat. Starting from this hypothesis and using entropy as an estimate of context informativeness (Shannon, 1948), we propose SLQS, which measures the semantic generality of a word by the entropy of its statistically most prominent contexts. For every term wi we identify the N most associated contexts c (where N is a parameter empirically set to 50)1. The association strength has been calculated with Local Mutual Information (LMI; Evert, 2005). For each selected context c, we define its entropy H(c) as: 1 N=50 is the result of an optimization of the model against the dataset after trying the following suboptimal values: 5, 10, 25, 75 and 100. Z�EF�nF�W1( ) TAFeedsPre�(�,�) _ ZfEFuW1( ) 39 n H(C) = � I P(</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Shannon, Claude E. 1948. “A mathematical theory of communication”. Bell System Technical Journal, Vol. 27. 379-423 and 623-656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: Vector Space Models of Semantics”.</title>
<date>2010</date>
<journal>Journal of Articial Intelligence Research,</journal>
<volume>37</volume>
<pages>141--188</pages>
<contexts>
<context position="1247" citStr="Turney and Pantel, 2010" startWordPosition="153" endWordPosition="156">d identification of hypernymy and its directionality in Distributional Semantic Models (DSMs). SLQS is assessed through two tasks: (i.) identifying the hypernym in hyponym-hypernym pairs, and (ii.) discriminating hypernymy among various semantic relations. In both tasks, SLQS outperforms other state-of-the-art measures. 1 Introduction In recent years, Distributional Semantic Models (DSMs) have gained much attention in computational linguistics as unsupervised methods to build lexical semantic representations from corpus-derived co-occurrences encoded as distributional vectors (Sahlgren, 2006; Turney and Pantel, 2010). DSMs rely on the Distributional Hypothesis (Harris, 1954) and model lexical semantic similarity as a function of distributional similarity, which is most commonly measured with the vector cosine (Turney and Pantel, 2010). DSMs have achieved impressive results in tasks such as synonym detection, semantic categorization, etc. (Padó and Lapata, 2007; Baroni and Lenci, 2010). One major shortcoming of current DSMs is that they are not able to discriminate among different types of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of dog in vector spac</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Turney, Peter D. and Pantel, Patrick. 2010. “From Frequency to Meaning: Vector Space Models of Semantics”. Journal of Articial Intelligence Research, Vol. 37. 141-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>A general framework for distributional similarity”.</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>81--88</pages>
<location>Sapporo,</location>
<contexts>
<context position="4368" citStr="Weeds and Weir, 2003" startWordPosition="632" endWordPosition="635">E) and ontology learning, as well as to enhance the cognitive plausibility of DSMs as general models of the semantic lexicon. 2 Related work The problem of identifying asymmetric relations like hypernymy has so far been addressed in distributional semantics only in a limited way (Kotlerman et al., 2010) or treated through semisupervised approaches, such as pattern-based approaches (Hearst, 1992). The few works that have attempted a completely unsupervised approach to the identification of hypernymy in corpora have mostly relied on some versions of the Distributional Inclusion Hypothesis (DIH; Weeds and Weir, 2003; Weeds et al., 2004), according to which the contexts of a narrow term are also shared by the broad term. One of the first proposed measures formalizing the DIH is WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), which quantifies the weights of the features f of a narrow term u that are included into the set of features of a broad term v: where Fx is the set of features of a term x, and wx(f) is the weight of the feature f of the term x. Variations of this measure have been introduced by Clarke (2009), Kotlerman et al. (2010) and Lenci and Benotto (2012). In this paper, we adopt a differ</context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>Weeds, Julie and Weir, David. 2003. “A general framework for distributional similarity”. Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing. Sapporo, Japan. 81-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity”.</title>
<date>2004</date>
<booktitle>Proceedings of COLING 2004. Geneva,</booktitle>
<pages>1015--1021</pages>
<contexts>
<context position="4389" citStr="Weeds et al., 2004" startWordPosition="636" endWordPosition="639">ng, as well as to enhance the cognitive plausibility of DSMs as general models of the semantic lexicon. 2 Related work The problem of identifying asymmetric relations like hypernymy has so far been addressed in distributional semantics only in a limited way (Kotlerman et al., 2010) or treated through semisupervised approaches, such as pattern-based approaches (Hearst, 1992). The few works that have attempted a completely unsupervised approach to the identification of hypernymy in corpora have mostly relied on some versions of the Distributional Inclusion Hypothesis (DIH; Weeds and Weir, 2003; Weeds et al., 2004), according to which the contexts of a narrow term are also shared by the broad term. One of the first proposed measures formalizing the DIH is WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), which quantifies the weights of the features f of a narrow term u that are included into the set of features of a broad term v: where Fx is the set of features of a term x, and wx(f) is the weight of the feature f of the term x. Variations of this measure have been introduced by Clarke (2009), Kotlerman et al. (2010) and Lenci and Benotto (2012). In this paper, we adopt a different approach, which i</context>
<context position="9358" citStr="Weeds et al. (2004)" startWordPosition="1458" endWordPosition="1461"> or no-relation (RANDOM-N).2 Since BLESS contains different numbers of pairs for every relation, we randomly extracted a subset of 1,277 pairs for each relation, where 1,277 is the maximum number of HYPER-related pairs for which vectors existed in our DSM. 4.2 Task 1: Directionality In this experiment we aimed at identifying the hypernym in the 1,277 hypernymy-related pairs of our dataset. Since the HYPER-related pairs in BLESS are in the order hyponym-hypernym (e.g. eagle-bird, eagle-animal, etc.), the hypernym in a pair (w1,w2) is correctly identified by SLQS, if SLQS (w1,w2) &gt; 0. Following Weeds et al. (2004), we used word frequency as a baseline model. This baseline is grounded on the hypothesis that hypernyms are more frequent than hyponyms in corpora. Table 1 gives the evaluation results: SLQS WeedsPrec BASELINE POSITIVE 1111 805 844 NEGATIVE 166 472 433 TOTAL 1277 1277 1277 PRECISION 87.00% 63.04% 66.09% Table 1. Accuracy for Task 1. As it can be seen in Table 1, SLQS scores a precision of 87% in identifying the second term of the test pairs as the hypernym. This result is particularly significant when compared to the one obtained by applying WeedsPrec (+23.96%). As it was also noticed by Geff</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Weeds, Julie, Weir, David and McCarthy, Diana. 2004. “Characterising measures of lexical distributional similarity”. Proceedings of COLING 2004. Geneva, Switzerland.1015-1021.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>