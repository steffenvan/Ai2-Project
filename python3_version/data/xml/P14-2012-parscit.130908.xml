<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001426">
<title confidence="0.99809">
Employing Word Representations and Regularization for
Domain Adaptation of Relation Extraction
</title>
<author confidence="0.996952">
Thien Huu Nguyen
</author>
<affiliation confidence="0.996349">
Computer Science Department
</affiliation>
<address confidence="0.7837255">
New York University
New York, NY 10003 USA
</address>
<email confidence="0.999637">
thien@cs.nyu.edu
</email>
<sectionHeader confidence="0.993911" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999815285714286">
Relation extraction suffers from a perfor-
mance loss when a model is applied to
out-of-domain data. This has fostered the
development of domain adaptation tech-
niques for relation extraction. This paper
evaluates word embeddings and clustering
on adapting feature-based relation extrac-
tion systems. We systematically explore
various ways to apply word embeddings
and show the best adaptation improvement
by combining word cluster and word em-
bedding information. Finally, we demon-
strate the effectiveness of regularization
for the adaptability of relation extractors.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999596434782609">
The goal of Relation Extraction (RE) is to detect
and classify relation mentions between entity pairs
into predefined relation types such as Employ-
ment or Citizenship relationships. Recent research
in this area, whether feature-based (Kambhatla,
2004; Boschee et al., 2005; Zhou et al., 2005;
Grishman et al., 2005; Jiang and Zhai, 2007a;
Chan and Roth, 2010; Sun et al., 2011) or kernel-
based (Zelenko et al., 2003; Bunescu and Mooney,
2005a; Bunescu and Mooney, 2005b; Zhang et al.,
2006; Qian et al., 2008; Nguyen et al., 2009), at-
tempts to improve the RE performance by enrich-
ing the feature sets from multiple sentence anal-
yses and knowledge resources. The fundamental
assumption of these supervised systems is that the
training data and the data to which the systems are
applied are sampled independently and identically
from the same distribution. When there is a mis-
match between data distributions, the RE perfor-
mance of these systems tends to degrade dramat-
ically (Plank and Moschitti, 2013). This is where
we need to resort to domain adaptation techniques
(DA) to adapt a model trained on one domain (the
</bodyText>
<author confidence="0.708429">
Ralph Grishman
</author>
<affiliation confidence="0.6221075">
Computer Science Department
New York University
</affiliation>
<address confidence="0.875411">
New York, NY 10003 USA
</address>
<email confidence="0.995025">
grishman@cs.nyu.edu
</email>
<bodyText confidence="0.999953853658537">
source domain) into a new model which can per-
form well on new domains (the target domains).
The consequences of linguistic variation be-
tween training and testing data on NLP tools have
been studied extensively in the last couple of years
for various NLP tasks such as Part-of-Speech tag-
ging (Blitzer et al., 2006; Huang and Yates, 2010;
Schnabel and Sch¨utze, 2014), named entity recog-
nition (Daum´e III, 2007) and sentiment analysis
(Blitzer et al., 2007; Daum´e III, 2007; Daum´e
III et al., 2010; Blitzer et al., 2011), etc. Un-
fortunately, there is very little work on domain
adaptation for RE. The only study explicitly tar-
geting this problem so far is by Plank and Mos-
chitti (2013) who find that the out-of-domain per-
formance of kernel-based relation extractors can
be improved by embedding semantic similarity in-
formation generated from word clustering and la-
tent semantic analysis (LSA) into syntactic tree
kernels. Although this idea is interesting, it suf-
fers from two major limitations:
+ It does not incorporate word cluster informa-
tion at different levels of granularity. In fact, Plank
and Moschitti (2013) only use the 10-bit cluster
prefix in their study. We will demonstrate later
that the adaptability of relation extractors can ben-
efit significantly from the addition of word cluster
features at various granularities.
+ It is unclear if this approach can encode real-
valued features of words (such as word embed-
dings (Mnih and Hinton, 2007; Collobert and We-
ston, 2008)) effectively. As the real-valued fea-
tures are able to capture latent yet useful proper-
ties of words, the augmentation of lexical terms
with these features is desirable to provide a more
general representation, potentially helping relation
extractors perform more robustly across domains.
In this work, we propose to avoid these limita-
tions by applying a feature-based approach for RE
which allows us to integrate various word features
of generalization into a single system more natu-
</bodyText>
<page confidence="0.992117">
68
</page>
<bodyText confidence="0.95972596969697">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 68–74,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
rally and effectively.
The application of word representations such
as word clusters in domain adaptation of RE
(Plank and Moschitti, 2013) is motivated by its
successes in semi-supervised methods (Chan and
Roth, 2010; Sun et al., 2011) where word repre-
sentations help to reduce data-sparseness of lexi-
cal information in the training data. In DA terms,
since the vocabularies of the source and target do-
mains are usually different, word representations
would mitigate the lexical sparsity by providing
general features of words that are shared across
domains, hence bridge the gap between domains.
The underlying hypothesis here is that the absence
of lexical target-domain features in the source do-
main can be compensated by these general fea-
tures to improve RE performance on the target do-
mains.
We extend this motivation by further evaluat-
ing word embeddings (Bengio et al., 2001; Ben-
gio et al., 2003; Mnih and Hinton, 2007; Col-
lobert and Weston, 2008; Turian et al., 2010) on
feature-based methods to adapt RE systems to new
domains. We explore the embedding-based fea-
tures in a principled way and demonstrate that
word embedding itself is also an effective repre-
sentation for domain adaptation of RE. More im-
portantly, we show empirically that word embed-
dings and word clusters capture different informa-
tion and their combination would further improve
the adaptability of relation extractors.
</bodyText>
<sectionHeader confidence="0.991026" genericHeader="introduction">
2 Regularization
</sectionHeader>
<bodyText confidence="0.999978268292683">
Given the more general representations provided
by word representations above, how can we learn a
relation extractor from the labeled source domain
data that generalizes well to new domains? In tra-
ditional machine learning where the challenge is
to utilize the training data to make predictions on
unseen data points (generated from the same dis-
tribution as the training data), the classifier with
a good generalization performance is the one that
not only fits the training data, but also avoids ov-
efitting over it. This is often obtained via regular-
ization methods to penalize complexity of classi-
fiers. Exploiting the shared interest in generaliza-
tion performance with traditional machine learn-
ing, in domain adaptation for RE, we would prefer
the relation extractor that fits the source domain
data, but also circumvents the overfitting problem
over this source domain1 so that it could general-
ize well on new domains. Eventually, regulariza-
tion methods can be considered naturally as a sim-
ple yet general technique to cope with DA prob-
lems.
Following Plank and Moschitti (2013), we as-
sume that we only have labeled data in a single
source domain but no labeled as well as unlabeled
target data. Moreover, we consider the single-
system DA setting where we construct a single
system able to work robustly with different but
related domains (multiple target domains). This
setting differs from most previous studies (Blitzer
et al., 2006) on DA which have attempted to de-
sign a specialized system for every specific tar-
get domain. In our view, although this setting is
more challenging, it is more practical for RE. In
fact, this setting can benefit considerably from our
general approach of applying word representations
and regularization. Finally, due to this setting, the
best way to set up the regularization parameter is
to impose the same regularization parameter on
every feature rather than a skewed regularization
(Jiang and Zhai, 2007b).
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999962958333333">
Although word embeddings have been success-
fully employed in many NLP tasks (Collobert and
Weston, 2008; Turian et al., 2010; Maas and
Ng, 2010), the application of word embeddings
in RE is very recent. Kuksa et al. (2010) pro-
pose an abstraction-augmented string kernel for
bio-relation extraction via word embeddings. In
the surge of deep learning, Socher et al. (2012)
and Khashabi (2013) use pre-trained word embed-
dings as input for Matrix-Vector Recursive Neu-
ral Networks (MV-RNN) to learn compositional
structures for RE. However, none of these works
evaluate word embeddings for domain adaptation
of RE which is our main focus in this paper.
Regarding domain adaptation, in representation
learning, Blitzer et al. (2006) propose structural
correspondence learning (SCL) while Huang and
Yates (2010) attempt to learn a multi-dimensional
feature representation. Unfortunately, these meth-
ods require unlabeled target domain data which
are unavailable in our single-system setting of DA.
Daum´e III (2007) proposes an easy adaptation
framework (EA) which is later extended to a semi-
supervised version (EA++) to incorporate unla-
</bodyText>
<footnote confidence="0.58817">
1domain overfitting (Jiang and Zhai, 2007b)
</footnote>
<page confidence="0.995021">
69
</page>
<bodyText confidence="0.995738636363636">
beled data (Daum´e III et al., 2010). In terms of
word embeddings for DA, recently, Xiao and Guo
(2013) present a log-bilinear language adaptation
framework for sequential labeling tasks. However,
these methods assume some labeled data in target
domains and are thus not applicable in our setting
of unsupervised DA. Above all, we move one step
further by evaluating the effectiveness of word em-
beddings on domain adaptation for RE which is
very different from the principal topic of sequence
labeling in the previous research.
</bodyText>
<sectionHeader confidence="0.972" genericHeader="method">
4 Word Representations
</sectionHeader>
<bodyText confidence="0.964768277777778">
We consider two types of word representations and
use them as additional features in our DA sys-
tem, namely Brown word clustering (Brown et
al., 1992) and word embeddings (Bengio et al.,
2001). While word clusters can be recognized
as an one-hot vector representation over a small
vocabulary, word embeddings are dense, low-
dimensional, and real-valued vectors (distributed
representations). Each dimension of the word em-
beddings expresses a latent feature of the words,
hopefully reflecting useful semantic and syntactic
regularities (Turian et al., 2010). We investigate
word embeddings induced by two typical language
models: Collobert and Weston (2008) embeddings
(C&amp;W) (Collobert and Weston, 2008; Turian et
al., 2010) and Hierarchical log-bilinear embed-
dings (HLBL) (Mnih and Hinton, 2007; Mnih and
Hinton, 2009; Turian et al., 2010).
</bodyText>
<sectionHeader confidence="0.996621" genericHeader="method">
5 Feature Set
</sectionHeader>
<subsectionHeader confidence="0.999154">
5.1 Baseline Feature Set
</subsectionHeader>
<bodyText confidence="0.999695333333334">
Sun et al. (2011) utilize the full feature set from
(Zhou et al., 2005) plus some additional features
and achieve the state-of-the-art feature-based RE
system. Unfortunately, this feature set includes
the human-annotated (gold-standard) information
on entity and mention types which is often miss-
ing or noisy in reality (Plank and Moschitti, 2013).
This issue becomes more serious in our setting of
single-system DA where we have a single source
domain with multiple dissimilar target domains
and an automatic system able to recognize entity
and mention types very well in different domains
may not be available. Therefore, following the set-
tings of Plank and Moschitti (2013), we will only
assume entity boundaries and not rely on the gold
standard information in the experiments. We ap-
ply the same feature set as Sun et al. (2011) but
remove the entity and mention type information2.
</bodyText>
<subsectionHeader confidence="0.997935">
5.2 Lexical Feature Augmentation
</subsectionHeader>
<bodyText confidence="0.999262086956522">
While Sun et al. (2011) show that adding word
clusters to the heads of the two mentions is the
most effective way to improve the generaliza-
tion accuracy, the right lexical features into which
word embeddings should be introduced to obtain
the best adaptability improvement are unexplored.
Also, which dimensionality of which word embed-
ding should we use with which lexical features?
In order to answer these questions, following Sun
et al. (2011), we first group lexical features into 4
groups and rank their importance based on linguis-
tic intuition and illustrations of the contributions
of different lexical features from various feature-
based RE systems. After that, we evaluate the ef-
fectiveness of these lexical feature groups for word
embedding augmentation individually and incre-
mentally according to the rank of importance. For
each of these group combinations, we assess the
system performance with different numbers of di-
mensions for both C&amp;W and HLBL word embed-
dings. Let M1 and M2 be the first and second men-
tions in the relation. Table 1 describes the lexical
feature groups.
</bodyText>
<table confidence="0.999671">
Rank Group Lexical Features
1 HM HM1 (head of M1)
HM2 (head of M2)
2 BagWM WM1 (words in M1)
WM2 (words in M2)
3 HC heads of chunks in context
4 BagWC words of context
</table>
<tableCaption confidence="0.999504">
Table 1: Lexical feature groups ordered by importance.
</tableCaption>
<sectionHeader confidence="0.998647" genericHeader="conclusions">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99692">
6.1 Tools and Data
</subsectionHeader>
<bodyText confidence="0.998300928571428">
Our relation extraction system is hierarchical
(Bunescu and Mooney, 2005b; Sun et al., 2011)
and apply maximum entropy (MaxEnt) in the
MALLET3 toolkit as the machine learning tool.
For Brown word clusters, we directly apply the
clustering trained by Plank and Moschitti (2013)
2We have the same observation as Plank and Moschitti
(2013) that when the gold-standard labels are used, the
impact of word representations is limited since the gold-
standard information seems to dominate. However, whenever
the gold labels are not available or inaccurate, the word rep-
resentations would be useful for improving adaptability per-
formance. Moreover, in all the cases, regularization methods
are still effective for domain adaptation of RE.
</bodyText>
<footnote confidence="0.965747">
3http://mallet.cs.umass.edu/
</footnote>
<page confidence="0.978602">
70
</page>
<table confidence="0.9998312">
System In-domain (bn+nw) Out-of-domain (bc development set)
C&amp;W,25 C&amp;W,50 C&amp;W,100 HLBL,50 HLBL,100 C&amp;W,25 C&amp;W,50 C&amp;W,100 HLBL,50 HLBL,100
1 Baseline 51.4 51.4 51.4 51.4 51.4 49.0 49.0 49.0 49.0 49.0
2 1+HM ED 54.0( +2.6) 54.1( +2.7) 55.7(+4.3) 53.7(+2.3) 55.2( +3.8) 51.5( +2.5) 52.7( +3.7) 52.5( +3.5) 50.2( +1.2) 50.6( +1.6)
3 1+BagWM ED 52.3( +0.9) 50.9( -0.5) 51.5(+0.1) 51.8(+0.4) 52.5( +1.1) 48.5( -0.5) 48.9( -0.1) 48.6( -0.4) 48.7( -0.3) 49.0( +0.0)
4 1+HC ED 51.3(-0.1) 50.9( -0.5) 48.3(-3.1) 50.8(-0.6) 49.8( -1.6) 44.9( -4.1) 45.8( -3.2) 45.8( -3.2) 48.7( -0.3) 47.3( -1.7)
5 1+BagWC ED 51.5( +0.1) 50.8( -0.6) 49.5(-1.9) 51.4(+0.0) 50.3( -1.1) 48.3( -0.7) 46.3( -2.7) 44.0( -5.0) 46.6( -2.4) 44.8( -4.2)
6 2+BagWM ED 54.3( +2.9) 53.2( +1.8) 53.2(+1.8) 54.0(+2.6) 53.8( +2.4) 52.5( +3.5) 51.4( +2.4) 50.6( +1.6) 50.0( +1.0) 48.6( -0.4)
7 6+HC ED 53.4( +2.0) 52.3( +0.9) 52.7(+1.3) 54.2(+2.8) 53.1( +1.7) 50.5( +1.5) 50.9( +1.9) 48.4( -0.6) 50.0( +1.0) 48.9( -0.1)
8 7+BagWC ED 53.4( +2.0) 52.2( +0.8) 50.8(-0.6) 53.5(+2.1) 53.6( +2.2) 49.2( +0.2) 50.7( +1.7) 49.2( +0.2) 47.9( -1.1) 49.5( +0.5)
</table>
<tableCaption confidence="0.999333">
Table 2: In-domain and Out-of-domain performance for different embedding features. The cells in bold are the best results.
</tableCaption>
<bodyText confidence="0.999948538461538">
to facilitate system comparison later. We evalu-
ate C&amp;W word embeddings with 25, 50 and 100
dimensions as well as HLBL word embeddings
with 50 and 100 dimensions that are introduced
in Turian et al. (2010) and can be downloaded
here4. The fact that we utilize the large, general
and unbiased resources generated from the previ-
ous works for evaluation not only helps to verify
the effectiveness of the resources across different
tasks and settings but also supports our setting of
single-system DA.
We use the ACE 2005 corpus for DA experi-
ments (as in Plank and Moschitti (2013)). It in-
volves 6 relation types and 6 domains: broadcast
news (bn), newswire (nw), broadcast conversation
(bc), telephone conversation (cts), weblogs (wl)
and usenet (un). We follow the standard prac-
tices on ACE (Plank and Moschitti, 2013) and use
news (the union of bn and nw) as the source do-
main and bc, cts and wl as our target domains. We
take half of bc as the only target development set,
and use the remaining data and domains for testing
purposes (as they are small already). As noted in
Plank and Moschitti (2013), the distributions of re-
lations as well as the vocabularies of the domains
are quite different.
</bodyText>
<subsectionHeader confidence="0.999845">
6.2 Evaluation of Word Embedding Features
</subsectionHeader>
<bodyText confidence="0.999871666666667">
We investigate the effectiveness of word embed-
dings on lexical features by following the proce-
dure described in Section 5.2. We test our system
on two scenarios: In-domain: the system is trained
and evaluated on the source domain (bn+nw, 5-
fold cross validation); Out-of-domain: the system
is trained on the source domain and evaluated on
the target development set of bc (bc dev). Table
2 presents the F measures of this experiment5 (the
</bodyText>
<footnote confidence="0.99296475">
4http://metaoptimize.com/projects/
wordreprs/
5All the in-domain improvement in rows 2, 6, 7 of Table
2 are significant at confidence levels ≥ 95%.
</footnote>
<bodyText confidence="0.996521">
suffix ED in lexical group names is to indicate the
embedding features).
From the tables, we find that for C&amp;W and
HLBL embeddings of 50 and 100 dimensions, the
most effective way to introduce word embeddings
is to add embeddings to the heads of the two men-
tions (row 2; both in-domain and out-of-domain)
although it is less pronounced for HLBL embed-
ding with 50 dimensions. Interestingly, for C&amp;W
embedding with 25 dimensions, adding the em-
bedding to both heads and words of the two men-
tions (row 6) performs the best for both in-domain
and out-of-domain scenarios. This is new com-
pared to the word cluster features where the heads
of the two mentions are always the best places for
augmentation (Sun et al., 2011). It suggests that
a suitable amount of embeddings for words in the
mentions might be useful for the augmentation of
the heads and inspires further exploration. Intro-
ducing embeddings to words of mentions alone
has mild impact while it is generally a bad idea to
augment chunk heads and words in the contexts.
Comparing C&amp;W and HLBL embeddings is
somehow more complicated. For both in-domain
and out-of-domain settings with different num-
bers of dimensions, C&amp;W embedding outperforms
HLBL embedding when only the heads of the
mentions are augmented while the degree of neg-
ative impact of HLBL embedding on chunk heads
as well as context words seems less serious than
C&amp;W’s. Regarding the incremental addition of
features (rows 6, 7, 8), C&amp;W is better for the out-
of-domain performance when 50 dimensions are
used, whereas HLBL (with both 50 and 100 di-
mensions) is more effective for the in-domain set-
ting. For the next experiments, we will apply the
C&amp;W embedding of 50 dimensions to the heads
of the mentions for its best out-of-domain perfor-
mance.
</bodyText>
<page confidence="0.99689">
71
</page>
<subsectionHeader confidence="0.721483">
6.3 Domain Adaptation with Word
Embeddings
</subsectionHeader>
<bodyText confidence="0.999456">
This section examines the effectiveness of word
representations for RE across domains. We evalu-
ate word cluster and embedding (denoted by ED)
features by adding them individually as well as
simultaneously into the baseline feature set. For
word clusters, we experiment with two possibil-
ities: (i) only using a single prefix length of 10
(as Plank and Moschitti (2013) did) (denoted by
WC10) and (ii) applying multiple prefix lengths of
4, 6, 8, 10 together with the full string (denoted by
WC). Table 3 presents the system performance (F
measures) for both in-domain and out-of-domain
settings.
</bodyText>
<table confidence="0.9994955">
System In-domain bc cts wl
Baseline(B) 51.4 49.7 41.5 36.6
B+WC10 52.3( +0.9) 50.8(+1.1) 45.7( +4.2) 39.6( +3)
B+WC 53.7( +2.3) 52.8(+3.1) 46.8( +5.3) 41.7( +5.1)
B+ED 54.1( +2.7) 52.4(+2.7) 46.2( +4.7) 42.5( +5.9)
B+WC+ED 55.5( +4.1) 53.8(+4.1) 47.4( +5.9) 44.7( +8.1)
</table>
<tableCaption confidence="0.939893666666667">
Table 3: Domain Adaptation Results with Word Represen-
tations. All the improvements over the baseline in Table 3 are
significant at confidence level &gt; 95%.
</tableCaption>
<bodyText confidence="0.9871336875">
The key observations from the table are:
(i): The baseline system achieves a performance
of 51.4% within its own domain while the per-
formance on target domains bc, cts, wl drops to
49.7%, 41.5% and 36.6% respectively. Our base-
line performance is worse than that of Plank and
Moschitti (2013) only on the target domain cts and
better in the other cases. This might be explained
by the difference between our baseline feature set
and the feature set underlying their kernel-based
system. However, the performance order across
domains of the two baselines are the same. Be-
sides, the baseline performance is improved over
all target domains when the system is enriched
with word cluster features of the 10 prefix length
only (row 2).
</bodyText>
<listItem confidence="0.918966827586207">
(ii): Over all the target domains, the perfor-
mance of the system augmented with word cluster
features of various granularities (row 3) is supe-
rior to that when only cluster features for the pre-
fix length 10 are added (row 2). This is significant
(at confidence level &gt; 95%) for domains bc and
wl and verifies our assumption that various granu-
larities for word cluster features are more effective
than a single granularity for domain adaptation of
RE.
(iii): Row 4 shows that word embedding itself is
also very useful for domain adaptation in RE since
it improves the baseline system for all the target
domains.
(iv): In row 5, we see that the addition of both
word cluster and word embedding features im-
proves the system further and results in the best
performance over all target domains (this is sig-
nificant with confidence level &gt; 95% in domains
bc and wl). The result suggests that word embed-
dings seem to capture different information from
word clusters and their combination would be ef-
fective to generalize relation extractors across do-
mains. However, in domain cts, the improvement
that word embeddings provide for word clusters is
modest. This is because the RCV1 corpus used to
induce the word embeddings (Turian et al., 2010)
does not cover spoken language words in cts very
well.
</listItem>
<bodyText confidence="0.62521075">
(v): Finally, the in-domain performance is also
improved consistently demonstrating the robust-
ness of word representations (Plank and Moschitti,
2013).
</bodyText>
<subsectionHeader confidence="0.910128">
6.4 Domain Adaptation with Regularization
</subsectionHeader>
<bodyText confidence="0.999783375">
All the experiments we have conducted so far do
not apply regularization for training. In this sec-
tion, in order to evaluate the effect of regulariza-
tion on the generalization capacity of relation ex-
tractors across domains, we replicate all the ex-
periments in Section 6.3 but apply regularization
when relation extractors are trained6. Table 4
presents the results.
</bodyText>
<table confidence="0.999476666666667">
System In-domain bc cts wl
Baseline(B) 56.2 55.5 48.7 42.2
B+WC10 57.5( +1.3) 57.3(+1.8) 52.3(+3.6) 45.0( +2.8)
B+WC 58.9( +2.7) 58.4(+2.9) 52.8(+4.1) 47.3( +5.1)
B+ED 58.9( +2.7) 59.5(+4.0) 52.6(+3.9) 48.6( +6.4)
B+WC+ED 59.4( +3.2) 59.8(+4.3) 52.9(+4.2) 49.7( +7.5)
</table>
<tableCaption confidence="0.954754666666667">
Table 4: Domain Adaptation Results with Regularization.
All the improvements over the baseline in Table 4 are signif-
icant at confidence level &gt; 95%.
</tableCaption>
<bodyText confidence="0.909795727272727">
For this experiment, every statement in (ii), (iii),
(iv) and (v) of Section 6.3 also holds. More impor-
tantly, the performance in every cell of Table 4 is
significantly better than the corresponding cell in
Table 3 (5% or better gain in F measure, a sig-
nificant improvement at confidence level &gt; 95%).
This demonstrates the effectiveness of regulariza-
tion for RE in general and for domain adaptation
of RE specifically.
6We use a L2 regularizer with the regularization parame-
ter of 0.5 for its best experimental results.
</bodyText>
<page confidence="0.997474">
72
</page>
<sectionHeader confidence="0.990174" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999747924528302">
Yoshua Bengio, R´ejean Ducharme, and Pascal Vincent.
2001. A Neural Probabilistic Language Model. In
Advances in Neural Information Processing Systems
(NIPS’13), pages 932-938, MIT Press, 2001.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. In Journal of Machine Learning Re-
search (JMLR), 3, pages 1137-1155, 2003.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain Adaptation with Structural Corre-
spondence Learning. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, Sydney, Australia.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, Boom-boxes, and
Blenders: Domain Adaptation for Sentiment Classi-
fication. In Proceedings of the ACL, pages 440-447,
Prague, Czech Republic, June 2007.
John Blitzer, Dean Foster, and Sham Kakade. 2011.
Domain Adaptation with Coupled Subspaces. In
Proceedings of the 14th International Conference on
Artificial Intelligence and Statistics, pages 173-181,
Fort Lauderdale, FL, USA.
Elizabeth Boschee, Ralph Weischedel, and Alex Zama-
nian. 2005. Automatic Information Extraction. In
Proceedings of the International Conference on In-
telligence Analysis.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-Based n-gram Models of Natural Language.
In Journal of Computational Linguistics, Volume 18,
Issue 4, pages 467-479, December 1992.
Razvan C. Bunescu and Raymond J. Mooney. 2005a.
A Shortest Path Dependency Kenrel for Relation Ex-
traction. In Proceedings of HLT/EMNLP.
Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence Kernels for Relation Extraction. In
Proceedings of NIPS.
Yee S. Chan and Dan Roth. 2010. Exploiting Back-
ground Knowledge for Relation Extraction. In
Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
152-160, Beijing, China, August.
Ronan Collobert and Jason Weston. 2008. A Unied Ar-
chitecture for Natural Language Processing: Deep
Neural Networks with Multitask Learning. In Inter-
national Conference on Machine Learning, ICML,
2008.
Hal Daum´e III. 2007. Frustratingly Easy Domain
Adaptation. In Proceedings of the ACL, pages 256-
263, Prague, Czech Republic, June 2007.
Hal Daum´e III, Abhishek Kumar and Avishek Saha.
2010. Co-regularization Based Semi-supervised
Domain Adaptation. In Advances in Neural Infor-
mation Processing Systems 23 (2010).
Ralph Grishman, David Westbrook and Adam Meyers.
2005. NYU’s English ACE 2005 System Descrip-
tion. ACE 2005 Evaluation Workshop.
Fei Huang and Alexander Yates. 2010. Explor-
ing Representation-Learning Approaches to Domain
Adaptation. In Proceedings of the 2010 Workshop
on Domain Adaptation for Natural Language Pro-
cessing, pages 23-30, Uppsala, Sweden, July 2010.
Jing Jiang and ChengXiang Zhai. 2007a. A Sys-
tematic Exploration of the Feature Space for Re-
lation Extraction. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (NAACL-HLT’07), pages 113-
120, 2007.
Jing Jiang and ChengXiang Zhai. 2007b. A Two-stage
Approach to Domain Adaptation for Statistical Clas-
sifiers. In Proceedings of the ACM 16th Confer-
ence on Information and Knowledge Management
(CIKM’07), pages 401-410, 2007.
Nanda Kambhatla. 2004. Combining Lexical, Syntac-
tic, and Semantic Features with Maximum Entropy
Models for Information Extraction. In Proceedings
of ACL-04.
Daniel Khashabi. 2013. On the Recursive Neural Net-
works for Relation Extraction and Entity Recogni-
tion. Technical Report (May, 2013), UIUC.
Pavel Kuksa, Yanjun Qi, Bing Bai, Ronan Collobert,
Jason Weston, Vladimir Pavlovic, and Xia Ning.
2010. Semi-Supervised Abstraction-Augmented
String Kernel for Multi-Level Bio-Relation Extrac-
tion. In Proceedings of the 2010 European Confer-
ence on Machine Learning and Knowledge Discov-
ery in Databases, Part II (ECML PKDD’10), pages
128-144, 2010.
Andrew L. Maas and Andrew Y. Ng. 2010. A Proba-
bilistic Model for Semantic Word Vectors. In NIPS
Workshop on Deep Learning and Unsupervised Fea-
ture Learning, 2010.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
Graphical Models for Statistical Language Mod-
elling. In Proceedings of ICML’07, pages 641-648,
Corvallis, OR, 2007.
Andriy Mnih and Geoffrey Hinton. 2009. A Scalable
Hierarchical Distributed Language Model. In NIPS,
page 1081-1088.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution Kernels on
Constituent, Dependency and Sequential Structures
for Relation Extraction. In Proceedings of EMNLP
09, pages 1378-1387, Stroudsburg, PA, USA.
</reference>
<page confidence="0.983496">
73
</page>
<reference confidence="0.999714708333333">
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding Semantic Similarity in Tree Kernels for Do-
main Adaptation of Relation Extraction. In Proceed-
ings of the ACL 2013, pages 1498-1507, Sofia, Bul-
garia.
Longhua Qian, Guodong Zhou, Qiaoming Zhu and
Peide Qian. 2008. Exploiting Constituent Dde-
pendencies for Tree Kernel-based Semantic Relation
Extraction. In Proceedings of COLING, pages 697-
704, Manchester.
Tobias Schnabel and Hinrich Sch¨utze. 2014. FLORS:
Fast and Simple Domain Adaptation for Part-of-
Speech Tagging. In Transactions of the Associa-
tion for Computational Linguistics, 2 (2014), pages
1526.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Compo-
sitionality through Recursive Matrix-Vector Spaces.
In Proceedings EMNLP-CoNLL’12, pages 1201-
1211, Jeju Island, Korea, July 2012.
Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised Relation Extraction with Large-
scale Word Clustering. In Proceedings of ACL-
HLT, pages 521-529, Portland, Oregon, USA.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL’10), pages 384-394, Upp-
sala, Sweden, July, 2010.
Min Xiao and Yuhong Guo. 2013. Domain Adaptation
for Sequence Labeling Tasks with a Probabilistic
Language Adaptation Model. In Proceedings of the
30th International Conference on Machine Learning
(ICML-13), pages 293-301, 2013.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel Methods for Relation
Extraction. Journal of Machine Learning Research,
3:10831106.
Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. In Proceedings of COLING-ACL-06, pages
825-832, Sydney.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various Knowledge in Relation Ex-
traction. In Proceedings of ACL’05, pages 427-434,
Ann Arbor, USA, 2005.
</reference>
<page confidence="0.999135">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.965651">
<title confidence="0.9993355">Employing Word Representations and Regularization Domain Adaptation of Relation Extraction</title>
<author confidence="0.998746">Thien Huu</author>
<affiliation confidence="0.998433">Computer Science</affiliation>
<address confidence="0.993285">New York New York, NY 10003</address>
<email confidence="0.999433">thien@cs.nyu.edu</email>
<abstract confidence="0.998746266666667">Relation extraction suffers from a performance loss when a model is applied to out-of-domain data. This has fostered the development of domain adaptation techniques for relation extraction. This paper evaluates word embeddings and clustering on adapting feature-based relation extraction systems. We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS’13),</booktitle>
<pages>932--938</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="5074" citStr="Bengio et al., 2001" startWordPosition="793" endWordPosition="796"> help to reduce data-sparseness of lexical information in the training data. In DA terms, since the vocabularies of the source and target domains are usually different, word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains, hence bridge the gap between domains. The underlying hypothesis here is that the absence of lexical target-domain features in the source domain can be compensated by these general features to improve RE performance on the target domains. We extend this motivation by further evaluating word embeddings (Bengio et al., 2001; Bengio et al., 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008; Turian et al., 2010) on feature-based methods to adapt RE systems to new domains. We explore the embedding-based features in a principled way and demonstrate that word embedding itself is also an effective representation for domain adaptation of RE. More importantly, we show empirically that word embeddings and word clusters capture different information and their combination would further improve the adaptability of relation extractors. 2 Regularization Given the more general representations provided by word representat</context>
<context position="9489" citStr="Bengio et al., 2001" startWordPosition="1496" endWordPosition="1499">near language adaptation framework for sequential labeling tasks. However, these methods assume some labeled data in target domains and are thus not applicable in our setting of unsupervised DA. Above all, we move one step further by evaluating the effectiveness of word embeddings on domain adaptation for RE which is very different from the principal topic of sequence labeling in the previous research. 4 Word Representations We consider two types of word representations and use them as additional features in our DA system, namely Brown word clustering (Brown et al., 1992) and word embeddings (Bengio et al., 2001). While word clusters can be recognized as an one-hot vector representation over a small vocabulary, word embeddings are dense, lowdimensional, and real-valued vectors (distributed representations). Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al., 2010). We investigate word embeddings induced by two typical language models: Collobert and Weston (2008) embeddings (C&amp;W) (Collobert and Weston, 2008; Turian et al., 2010) and Hierarchical log-bilinear embeddings (HLBL) (Mnih and Hinton, 200</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2001</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, and Pascal Vincent. 2001. A Neural Probabilistic Language Model. In Advances in Neural Information Processing Systems (NIPS’13), pages 932-938, MIT Press, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2003</date>
<journal>In Journal of Machine Learning Research (JMLR),</journal>
<volume>3</volume>
<pages>1137--1155</pages>
<contexts>
<context position="5095" citStr="Bengio et al., 2003" startWordPosition="797" endWordPosition="801">sparseness of lexical information in the training data. In DA terms, since the vocabularies of the source and target domains are usually different, word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains, hence bridge the gap between domains. The underlying hypothesis here is that the absence of lexical target-domain features in the source domain can be compensated by these general features to improve RE performance on the target domains. We extend this motivation by further evaluating word embeddings (Bengio et al., 2001; Bengio et al., 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008; Turian et al., 2010) on feature-based methods to adapt RE systems to new domains. We explore the embedding-based features in a principled way and demonstrate that word embedding itself is also an effective representation for domain adaptation of RE. More importantly, we show empirically that word embeddings and word clusters capture different information and their combination would further improve the adaptability of relation extractors. 2 Regularization Given the more general representations provided by word representations above, how can w</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A Neural Probabilistic Language Model. In Journal of Machine Learning Research (JMLR), 3, pages 1137-1155, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain Adaptation with Structural Correspondence Learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2322" citStr="Blitzer et al., 2006" startWordPosition="358" endWordPosition="361"> performance of these systems tends to degrade dramatically (Plank and Moschitti, 2013). This is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one domain (the Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu source domain) into a new model which can perform well on new domains (the target domains). The consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging (Blitzer et al., 2006; Huang and Yates, 2010; Schnabel and Sch¨utze, 2014), named entity recognition (Daum´e III, 2007) and sentiment analysis (Blitzer et al., 2007; Daum´e III, 2007; Daum´e III et al., 2010; Blitzer et al., 2011), etc. Unfortunately, there is very little work on domain adaptation for RE. The only study explicitly targeting this problem so far is by Plank and Moschitti (2013) who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tre</context>
<context position="7046" citStr="Blitzer et al., 2006" startWordPosition="1112" endWordPosition="1115">mvents the overfitting problem over this source domain1 so that it could generalize well on new domains. Eventually, regularization methods can be considered naturally as a simple yet general technique to cope with DA problems. Following Plank and Moschitti (2013), we assume that we only have labeled data in a single source domain but no labeled as well as unlabeled target data. Moreover, we consider the singlesystem DA setting where we construct a single system able to work robustly with different but related domains (multiple target domains). This setting differs from most previous studies (Blitzer et al., 2006) on DA which have attempted to design a specialized system for every specific target domain. In our view, although this setting is more challenging, it is more practical for RE. In fact, this setting can benefit considerably from our general approach of applying word representations and regularization. Finally, due to this setting, the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization (Jiang and Zhai, 2007b). 3 Related Work Although word embeddings have been successfully employed in many NLP tasks</context>
<context position="8297" citStr="Blitzer et al. (2006)" startWordPosition="1311" endWordPosition="1314">Turian et al., 2010; Maas and Ng, 2010), the application of word embeddings in RE is very recent. Kuksa et al. (2010) propose an abstraction-augmented string kernel for bio-relation extraction via word embeddings. In the surge of deep learning, Socher et al. (2012) and Khashabi (2013) use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks (MV-RNN) to learn compositional structures for RE. However, none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper. Regarding domain adaptation, in representation learning, Blitzer et al. (2006) propose structural correspondence learning (SCL) while Huang and Yates (2010) attempt to learn a multi-dimensional feature representation. Unfortunately, these methods require unlabeled target domain data which are unavailable in our single-system setting of DA. Daum´e III (2007) proposes an easy adaptation framework (EA) which is later extended to a semisupervised version (EA++) to incorporate unla1domain overfitting (Jiang and Zhai, 2007b) 69 beled data (Daum´e III et al., 2010). In terms of word embeddings for DA, recently, Xiao and Guo (2013) present a log-bilinear language adaptation fra</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain Adaptation with Structural Correspondence Learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes, and Blenders: Domain Adaptation for Sentiment Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>440--447</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2465" citStr="Blitzer et al., 2007" startWordPosition="380" endWordPosition="383">echniques (DA) to adapt a model trained on one domain (the Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu source domain) into a new model which can perform well on new domains (the target domains). The consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging (Blitzer et al., 2006; Huang and Yates, 2010; Schnabel and Sch¨utze, 2014), named entity recognition (Daum´e III, 2007) and sentiment analysis (Blitzer et al., 2007; Daum´e III, 2007; Daum´e III et al., 2010; Blitzer et al., 2011), etc. Unfortunately, there is very little work on domain adaptation for RE. The only study explicitly targeting this problem so far is by Plank and Moschitti (2013) who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suffers from two major limitations: + It does not incorporate word cluster information at diff</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, Boom-boxes, and Blenders: Domain Adaptation for Sentiment Classification. In Proceedings of the ACL, pages 440-447, Prague, Czech Republic, June 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Dean Foster</author>
<author>Sham Kakade</author>
</authors>
<title>Domain Adaptation with Coupled Subspaces.</title>
<date>2011</date>
<booktitle>In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>173--181</pages>
<location>Fort Lauderdale, FL, USA.</location>
<contexts>
<context position="2531" citStr="Blitzer et al., 2011" startWordPosition="392" endWordPosition="395">Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu source domain) into a new model which can perform well on new domains (the target domains). The consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging (Blitzer et al., 2006; Huang and Yates, 2010; Schnabel and Sch¨utze, 2014), named entity recognition (Daum´e III, 2007) and sentiment analysis (Blitzer et al., 2007; Daum´e III, 2007; Daum´e III et al., 2010; Blitzer et al., 2011), etc. Unfortunately, there is very little work on domain adaptation for RE. The only study explicitly targeting this problem so far is by Plank and Moschitti (2013) who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suffers from two major limitations: + It does not incorporate word cluster information at different levels of granularity. In fact, Plank and Moschitti (2013) o</context>
</contexts>
<marker>Blitzer, Foster, Kakade, 2011</marker>
<rawString>John Blitzer, Dean Foster, and Sham Kakade. 2011. Domain Adaptation with Coupled Subspaces. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, pages 173-181, Fort Lauderdale, FL, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Boschee</author>
<author>Ralph Weischedel</author>
<author>Alex Zamanian</author>
</authors>
<title>Automatic Information Extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Intelligence Analysis.</booktitle>
<contexts>
<context position="1061" citStr="Boschee et al., 2005" startWordPosition="146" endWordPosition="149">lustering on adapting feature-based relation extraction systems. We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mi</context>
</contexts>
<marker>Boschee, Weischedel, Zamanian, 2005</marker>
<rawString>Elizabeth Boschee, Ralph Weischedel, and Alex Zamanian. 2005. Automatic Information Extraction. In Proceedings of the International Conference on Intelligence Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-Based n-gram Models of Natural Language.</title>
<date>1992</date>
<journal>In Journal of Computational Linguistics, Volume</journal>
<volume>18</volume>
<pages>467--479</pages>
<contexts>
<context position="9447" citStr="Brown et al., 1992" startWordPosition="1489" endWordPosition="1492">y, Xiao and Guo (2013) present a log-bilinear language adaptation framework for sequential labeling tasks. However, these methods assume some labeled data in target domains and are thus not applicable in our setting of unsupervised DA. Above all, we move one step further by evaluating the effectiveness of word embeddings on domain adaptation for RE which is very different from the principal topic of sequence labeling in the previous research. 4 Word Representations We consider two types of word representations and use them as additional features in our DA system, namely Brown word clustering (Brown et al., 1992) and word embeddings (Bengio et al., 2001). While word clusters can be recognized as an one-hot vector representation over a small vocabulary, word embeddings are dense, lowdimensional, and real-valued vectors (distributed representations). Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al., 2010). We investigate word embeddings induced by two typical language models: Collobert and Weston (2008) embeddings (C&amp;W) (Collobert and Weston, 2008; Turian et al., 2010) and Hierarchical log-biline</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-Based n-gram Models of Natural Language. In Journal of Computational Linguistics, Volume 18, Issue 4, pages 467-479, December 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A Shortest Path Dependency Kenrel for Relation Extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="1229" citStr="Bunescu and Mooney, 2005" startWordPosition="177" endWordPosition="180">ement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically (Plank and Moschitti, 2013). This is where we need to resort to dom</context>
<context position="12511" citStr="Bunescu and Mooney, 2005" startWordPosition="1979" endWordPosition="1982">y according to the rank of importance. For each of these group combinations, we assess the system performance with different numbers of dimensions for both C&amp;W and HLBL word embeddings. Let M1 and M2 be the first and second mentions in the relation. Table 1 describes the lexical feature groups. Rank Group Lexical Features 1 HM HM1 (head of M1) HM2 (head of M2) 2 BagWM WM1 (words in M1) WM2 (words in M2) 3 HC heads of chunks in context 4 BagWC words of context Table 1: Lexical feature groups ordered by importance. 6 Experiments 6.1 Tools and Data Our relation extraction system is hierarchical (Bunescu and Mooney, 2005b; Sun et al., 2011) and apply maximum entropy (MaxEnt) in the MALLET3 toolkit as the machine learning tool. For Brown word clusters, we directly apply the clustering trained by Plank and Moschitti (2013) 2We have the same observation as Plank and Moschitti (2013) that when the gold-standard labels are used, the impact of word representations is limited since the goldstandard information seems to dominate. However, whenever the gold labels are not available or inaccurate, the word representations would be useful for improving adaptability performance. Moreover, in all the cases, regularization</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005a. A Shortest Path Dependency Kenrel for Relation Extraction. In Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Subsequence Kernels for Relation Extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="1229" citStr="Bunescu and Mooney, 2005" startWordPosition="177" endWordPosition="180">ement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically (Plank and Moschitti, 2013). This is where we need to resort to dom</context>
<context position="12511" citStr="Bunescu and Mooney, 2005" startWordPosition="1979" endWordPosition="1982">y according to the rank of importance. For each of these group combinations, we assess the system performance with different numbers of dimensions for both C&amp;W and HLBL word embeddings. Let M1 and M2 be the first and second mentions in the relation. Table 1 describes the lexical feature groups. Rank Group Lexical Features 1 HM HM1 (head of M1) HM2 (head of M2) 2 BagWM WM1 (words in M1) WM2 (words in M2) 3 HC heads of chunks in context 4 BagWC words of context Table 1: Lexical feature groups ordered by importance. 6 Experiments 6.1 Tools and Data Our relation extraction system is hierarchical (Bunescu and Mooney, 2005b; Sun et al., 2011) and apply maximum entropy (MaxEnt) in the MALLET3 toolkit as the machine learning tool. For Brown word clusters, we directly apply the clustering trained by Plank and Moschitti (2013) 2We have the same observation as Plank and Moschitti (2013) that when the gold-standard labels are used, the impact of word representations is limited since the goldstandard information seems to dominate. However, whenever the gold labels are not available or inaccurate, the word representations would be useful for improving adaptability performance. Moreover, in all the cases, regularization</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005b. Subsequence Kernels for Relation Extraction. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee S Chan</author>
<author>Dan Roth</author>
</authors>
<title>Exploiting Background Knowledge for Relation Extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>152--160</pages>
<location>Beijing, China,</location>
<contexts>
<context position="1147" citStr="Chan and Roth, 2010" startWordPosition="162" endWordPosition="165">ore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrad</context>
<context position="4409" citStr="Chan and Roth, 2010" startWordPosition="685" endWordPosition="688">is work, we propose to avoid these limitations by applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more natu68 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 68–74, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics rally and effectively. The application of word representations such as word clusters in domain adaptation of RE (Plank and Moschitti, 2013) is motivated by its successes in semi-supervised methods (Chan and Roth, 2010; Sun et al., 2011) where word representations help to reduce data-sparseness of lexical information in the training data. In DA terms, since the vocabularies of the source and target domains are usually different, word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains, hence bridge the gap between domains. The underlying hypothesis here is that the absence of lexical target-domain features in the source domain can be compensated by these general features to improve RE performance on the target domains. We extend this moti</context>
</contexts>
<marker>Chan, Roth, 2010</marker>
<rawString>Yee S. Chan and Dan Roth. 2010. Exploiting Background Knowledge for Relation Extraction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 152-160, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A Unied Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning, ICML,</booktitle>
<contexts>
<context position="3496" citStr="Collobert and Weston, 2008" startWordPosition="548" endWordPosition="552">and latent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suffers from two major limitations: + It does not incorporate word cluster information at different levels of granularity. In fact, Plank and Moschitti (2013) only use the 10-bit cluster prefix in their study. We will demonstrate later that the adaptability of relation extractors can benefit significantly from the addition of word cluster features at various granularities. + It is unclear if this approach can encode realvalued features of words (such as word embeddings (Mnih and Hinton, 2007; Collobert and Weston, 2008)) effectively. As the real-valued features are able to capture latent yet useful properties of words, the augmentation of lexical terms with these features is desirable to provide a more general representation, potentially helping relation extractors perform more robustly across domains. In this work, we propose to avoid these limitations by applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more natu68 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 68–</context>
<context position="5146" citStr="Collobert and Weston, 2008" startWordPosition="806" endWordPosition="810">ining data. In DA terms, since the vocabularies of the source and target domains are usually different, word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains, hence bridge the gap between domains. The underlying hypothesis here is that the absence of lexical target-domain features in the source domain can be compensated by these general features to improve RE performance on the target domains. We extend this motivation by further evaluating word embeddings (Bengio et al., 2001; Bengio et al., 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008; Turian et al., 2010) on feature-based methods to adapt RE systems to new domains. We explore the embedding-based features in a principled way and demonstrate that word embedding itself is also an effective representation for domain adaptation of RE. More importantly, we show empirically that word embeddings and word clusters capture different information and their combination would further improve the adaptability of relation extractors. 2 Regularization Given the more general representations provided by word representations above, how can we learn a relation extractor from the labeled sourc</context>
<context position="7674" citStr="Collobert and Weston, 2008" startWordPosition="1213" endWordPosition="1216">n DA which have attempted to design a specialized system for every specific target domain. In our view, although this setting is more challenging, it is more practical for RE. In fact, this setting can benefit considerably from our general approach of applying word representations and regularization. Finally, due to this setting, the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization (Jiang and Zhai, 2007b). 3 Related Work Although word embeddings have been successfully employed in many NLP tasks (Collobert and Weston, 2008; Turian et al., 2010; Maas and Ng, 2010), the application of word embeddings in RE is very recent. Kuksa et al. (2010) propose an abstraction-augmented string kernel for bio-relation extraction via word embeddings. In the surge of deep learning, Socher et al. (2012) and Khashabi (2013) use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks (MV-RNN) to learn compositional structures for RE. However, none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper. Regarding domain adaptation, in representation learning</context>
<context position="9952" citStr="Collobert and Weston (2008)" startWordPosition="1561" endWordPosition="1564">rd representations and use them as additional features in our DA system, namely Brown word clustering (Brown et al., 1992) and word embeddings (Bengio et al., 2001). While word clusters can be recognized as an one-hot vector representation over a small vocabulary, word embeddings are dense, lowdimensional, and real-valued vectors (distributed representations). Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al., 2010). We investigate word embeddings induced by two typical language models: Collobert and Weston (2008) embeddings (C&amp;W) (Collobert and Weston, 2008; Turian et al., 2010) and Hierarchical log-bilinear embeddings (HLBL) (Mnih and Hinton, 2007; Mnih and Hinton, 2009; Turian et al., 2010). 5 Feature Set 5.1 Baseline Feature Set Sun et al. (2011) utilize the full feature set from (Zhou et al., 2005) plus some additional features and achieve the state-of-the-art feature-based RE system. Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often missing or noisy in reality (Plank and Moschitti, 2013). This issue becomes more ser</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A Unied Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In International Conference on Machine Learning, ICML, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly Easy Domain Adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>256--263</pages>
<location>Prague, Czech Republic,</location>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly Easy Domain Adaptation. In Proceedings of the ACL, pages 256-263, Prague, Czech Republic, June 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e Abhishek Kumar</author>
<author>Avishek Saha</author>
</authors>
<title>Co-regularization Based Semi-supervised Domain Adaptation.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems</booktitle>
<volume>23</volume>
<marker>Kumar, Saha, 2010</marker>
<rawString>Hal Daum´e III, Abhishek Kumar and Avishek Saha. 2010. Co-regularization Based Semi-supervised Domain Adaptation. In Advances in Neural Information Processing Systems 23 (2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>David Westbrook</author>
<author>Adam Meyers</author>
</authors>
<date>2005</date>
<booktitle>NYU’s English ACE 2005 System Description. ACE 2005 Evaluation Workshop.</booktitle>
<contexts>
<context position="1103" citStr="Grishman et al., 2005" startWordPosition="154" endWordPosition="157">ion extraction systems. We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE </context>
</contexts>
<marker>Grishman, Westbrook, Meyers, 2005</marker>
<rawString>Ralph Grishman, David Westbrook and Adam Meyers. 2005. NYU’s English ACE 2005 System Description. ACE 2005 Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Exploring Representation-Learning Approaches to Domain Adaptation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing,</booktitle>
<pages>23--30</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2345" citStr="Huang and Yates, 2010" startWordPosition="362" endWordPosition="365">systems tends to degrade dramatically (Plank and Moschitti, 2013). This is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one domain (the Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu source domain) into a new model which can perform well on new domains (the target domains). The consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging (Blitzer et al., 2006; Huang and Yates, 2010; Schnabel and Sch¨utze, 2014), named entity recognition (Daum´e III, 2007) and sentiment analysis (Blitzer et al., 2007; Daum´e III, 2007; Daum´e III et al., 2010; Blitzer et al., 2011), etc. Unfortunately, there is very little work on domain adaptation for RE. The only study explicitly targeting this problem so far is by Plank and Moschitti (2013) who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although thi</context>
<context position="8375" citStr="Huang and Yates (2010)" startWordPosition="1321" endWordPosition="1324">n RE is very recent. Kuksa et al. (2010) propose an abstraction-augmented string kernel for bio-relation extraction via word embeddings. In the surge of deep learning, Socher et al. (2012) and Khashabi (2013) use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks (MV-RNN) to learn compositional structures for RE. However, none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper. Regarding domain adaptation, in representation learning, Blitzer et al. (2006) propose structural correspondence learning (SCL) while Huang and Yates (2010) attempt to learn a multi-dimensional feature representation. Unfortunately, these methods require unlabeled target domain data which are unavailable in our single-system setting of DA. Daum´e III (2007) proposes an easy adaptation framework (EA) which is later extended to a semisupervised version (EA++) to incorporate unla1domain overfitting (Jiang and Zhai, 2007b) 69 beled data (Daum´e III et al., 2010). In terms of word embeddings for DA, recently, Xiao and Guo (2013) present a log-bilinear language adaptation framework for sequential labeling tasks. However, these methods assume some label</context>
</contexts>
<marker>Huang, Yates, 2010</marker>
<rawString>Fei Huang and Alexander Yates. 2010. Exploring Representation-Learning Approaches to Domain Adaptation. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, pages 23-30, Uppsala, Sweden, July 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>A Systematic Exploration of the Feature Space for Relation Extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT’07),</booktitle>
<pages>113--120</pages>
<contexts>
<context position="1125" citStr="Jiang and Zhai, 2007" startWordPosition="158" endWordPosition="161"> We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these s</context>
<context position="7553" citStr="Jiang and Zhai, 2007" startWordPosition="1194" endWordPosition="1197">related domains (multiple target domains). This setting differs from most previous studies (Blitzer et al., 2006) on DA which have attempted to design a specialized system for every specific target domain. In our view, although this setting is more challenging, it is more practical for RE. In fact, this setting can benefit considerably from our general approach of applying word representations and regularization. Finally, due to this setting, the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization (Jiang and Zhai, 2007b). 3 Related Work Although word embeddings have been successfully employed in many NLP tasks (Collobert and Weston, 2008; Turian et al., 2010; Maas and Ng, 2010), the application of word embeddings in RE is very recent. Kuksa et al. (2010) propose an abstraction-augmented string kernel for bio-relation extraction via word embeddings. In the surge of deep learning, Socher et al. (2012) and Khashabi (2013) use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks (MV-RNN) to learn compositional structures for RE. However, none of these works evaluate word embeddings f</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007a. A Systematic Exploration of the Feature Space for Relation Extraction. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT’07), pages 113-120, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>A Two-stage Approach to Domain Adaptation for Statistical Classifiers.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACM 16th Conference on Information and Knowledge Management (CIKM’07),</booktitle>
<pages>401--410</pages>
<contexts>
<context position="1125" citStr="Jiang and Zhai, 2007" startWordPosition="158" endWordPosition="161"> We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these s</context>
<context position="7553" citStr="Jiang and Zhai, 2007" startWordPosition="1194" endWordPosition="1197">related domains (multiple target domains). This setting differs from most previous studies (Blitzer et al., 2006) on DA which have attempted to design a specialized system for every specific target domain. In our view, although this setting is more challenging, it is more practical for RE. In fact, this setting can benefit considerably from our general approach of applying word representations and regularization. Finally, due to this setting, the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization (Jiang and Zhai, 2007b). 3 Related Work Although word embeddings have been successfully employed in many NLP tasks (Collobert and Weston, 2008; Turian et al., 2010; Maas and Ng, 2010), the application of word embeddings in RE is very recent. Kuksa et al. (2010) propose an abstraction-augmented string kernel for bio-relation extraction via word embeddings. In the surge of deep learning, Socher et al. (2012) and Khashabi (2013) use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks (MV-RNN) to learn compositional structures for RE. However, none of these works evaluate word embeddings f</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007b. A Two-stage Approach to Domain Adaptation for Statistical Classifiers. In Proceedings of the ACM 16th Conference on Information and Knowledge Management (CIKM’07), pages 401-410, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Information Extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-04.</booktitle>
<contexts>
<context position="1039" citStr="Kambhatla, 2004" startWordPosition="144" endWordPosition="145"> embeddings and clustering on adapting feature-based relation extraction systems. We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distributi</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Information Extraction. In Proceedings of ACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Khashabi</author>
</authors>
<title>On the Recursive Neural Networks for Relation Extraction and Entity Recognition.</title>
<date>2013</date>
<tech>Technical Report</tech>
<publisher>UIUC.</publisher>
<contexts>
<context position="7961" citStr="Khashabi (2013)" startWordPosition="1262" endWordPosition="1263">on. Finally, due to this setting, the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization (Jiang and Zhai, 2007b). 3 Related Work Although word embeddings have been successfully employed in many NLP tasks (Collobert and Weston, 2008; Turian et al., 2010; Maas and Ng, 2010), the application of word embeddings in RE is very recent. Kuksa et al. (2010) propose an abstraction-augmented string kernel for bio-relation extraction via word embeddings. In the surge of deep learning, Socher et al. (2012) and Khashabi (2013) use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks (MV-RNN) to learn compositional structures for RE. However, none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper. Regarding domain adaptation, in representation learning, Blitzer et al. (2006) propose structural correspondence learning (SCL) while Huang and Yates (2010) attempt to learn a multi-dimensional feature representation. Unfortunately, these methods require unlabeled target domain data which are unavailable in our single-system setting of DA. </context>
</contexts>
<marker>Khashabi, 2013</marker>
<rawString>Daniel Khashabi. 2013. On the Recursive Neural Networks for Relation Extraction and Entity Recognition. Technical Report (May, 2013), UIUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Kuksa</author>
<author>Yanjun Qi</author>
<author>Bing Bai</author>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>Vladimir Pavlovic</author>
<author>Xia Ning</author>
</authors>
<title>Semi-Supervised Abstraction-Augmented String Kernel for Multi-Level Bio-Relation Extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases, Part II (ECML PKDD’10),</booktitle>
<pages>128--144</pages>
<contexts>
<context position="7793" citStr="Kuksa et al. (2010)" startWordPosition="1235" endWordPosition="1238">is more challenging, it is more practical for RE. In fact, this setting can benefit considerably from our general approach of applying word representations and regularization. Finally, due to this setting, the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization (Jiang and Zhai, 2007b). 3 Related Work Although word embeddings have been successfully employed in many NLP tasks (Collobert and Weston, 2008; Turian et al., 2010; Maas and Ng, 2010), the application of word embeddings in RE is very recent. Kuksa et al. (2010) propose an abstraction-augmented string kernel for bio-relation extraction via word embeddings. In the surge of deep learning, Socher et al. (2012) and Khashabi (2013) use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks (MV-RNN) to learn compositional structures for RE. However, none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper. Regarding domain adaptation, in representation learning, Blitzer et al. (2006) propose structural correspondence learning (SCL) while Huang and Yates (2010) attempt to learn </context>
</contexts>
<marker>Kuksa, Qi, Bai, Collobert, Weston, Pavlovic, Ning, 2010</marker>
<rawString>Pavel Kuksa, Yanjun Qi, Bing Bai, Ronan Collobert, Jason Weston, Vladimir Pavlovic, and Xia Ning. 2010. Semi-Supervised Abstraction-Augmented String Kernel for Multi-Level Bio-Relation Extraction. In Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases, Part II (ECML PKDD’10), pages 128-144, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Andrew Y Ng</author>
</authors>
<title>A Probabilistic Model for Semantic Word Vectors.</title>
<date>2010</date>
<booktitle>In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,</booktitle>
<contexts>
<context position="7715" citStr="Maas and Ng, 2010" startWordPosition="1221" endWordPosition="1224"> system for every specific target domain. In our view, although this setting is more challenging, it is more practical for RE. In fact, this setting can benefit considerably from our general approach of applying word representations and regularization. Finally, due to this setting, the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization (Jiang and Zhai, 2007b). 3 Related Work Although word embeddings have been successfully employed in many NLP tasks (Collobert and Weston, 2008; Turian et al., 2010; Maas and Ng, 2010), the application of word embeddings in RE is very recent. Kuksa et al. (2010) propose an abstraction-augmented string kernel for bio-relation extraction via word embeddings. In the surge of deep learning, Socher et al. (2012) and Khashabi (2013) use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks (MV-RNN) to learn compositional structures for RE. However, none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper. Regarding domain adaptation, in representation learning, Blitzer et al. (2006) propose structura</context>
</contexts>
<marker>Maas, Ng, 2010</marker>
<rawString>Andrew L. Maas and Andrew Y. Ng. 2010. A Probabilistic Model for Semantic Word Vectors. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new Graphical Models for Statistical Language Modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML’07,</booktitle>
<pages>641--648</pages>
<location>Corvallis, OR,</location>
<contexts>
<context position="3467" citStr="Mnih and Hinton, 2007" startWordPosition="544" endWordPosition="547">d from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suffers from two major limitations: + It does not incorporate word cluster information at different levels of granularity. In fact, Plank and Moschitti (2013) only use the 10-bit cluster prefix in their study. We will demonstrate later that the adaptability of relation extractors can benefit significantly from the addition of word cluster features at various granularities. + It is unclear if this approach can encode realvalued features of words (such as word embeddings (Mnih and Hinton, 2007; Collobert and Weston, 2008)) effectively. As the real-valued features are able to capture latent yet useful properties of words, the augmentation of lexical terms with these features is desirable to provide a more general representation, potentially helping relation extractors perform more robustly across domains. In this work, we propose to avoid these limitations by applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more natu68 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguist</context>
<context position="5118" citStr="Mnih and Hinton, 2007" startWordPosition="802" endWordPosition="805"> information in the training data. In DA terms, since the vocabularies of the source and target domains are usually different, word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains, hence bridge the gap between domains. The underlying hypothesis here is that the absence of lexical target-domain features in the source domain can be compensated by these general features to improve RE performance on the target domains. We extend this motivation by further evaluating word embeddings (Bengio et al., 2001; Bengio et al., 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008; Turian et al., 2010) on feature-based methods to adapt RE systems to new domains. We explore the embedding-based features in a principled way and demonstrate that word embedding itself is also an effective representation for domain adaptation of RE. More importantly, we show empirically that word embeddings and word clusters capture different information and their combination would further improve the adaptability of relation extractors. 2 Regularization Given the more general representations provided by word representations above, how can we learn a relation extr</context>
<context position="10090" citStr="Mnih and Hinton, 2007" startWordPosition="1581" endWordPosition="1584">Bengio et al., 2001). While word clusters can be recognized as an one-hot vector representation over a small vocabulary, word embeddings are dense, lowdimensional, and real-valued vectors (distributed representations). Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al., 2010). We investigate word embeddings induced by two typical language models: Collobert and Weston (2008) embeddings (C&amp;W) (Collobert and Weston, 2008; Turian et al., 2010) and Hierarchical log-bilinear embeddings (HLBL) (Mnih and Hinton, 2007; Mnih and Hinton, 2009; Turian et al., 2010). 5 Feature Set 5.1 Baseline Feature Set Sun et al. (2011) utilize the full feature set from (Zhou et al., 2005) plus some additional features and achieve the state-of-the-art feature-based RE system. Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often missing or noisy in reality (Plank and Moschitti, 2013). This issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic syst</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new Graphical Models for Statistical Language Modelling. In Proceedings of ICML’07, pages 641-648, Corvallis, OR, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>A Scalable Hierarchical Distributed Language Model.</title>
<date>2009</date>
<booktitle>In NIPS,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="10113" citStr="Mnih and Hinton, 2009" startWordPosition="1585" endWordPosition="1588">hile word clusters can be recognized as an one-hot vector representation over a small vocabulary, word embeddings are dense, lowdimensional, and real-valued vectors (distributed representations). Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al., 2010). We investigate word embeddings induced by two typical language models: Collobert and Weston (2008) embeddings (C&amp;W) (Collobert and Weston, 2008; Turian et al., 2010) and Hierarchical log-bilinear embeddings (HLBL) (Mnih and Hinton, 2007; Mnih and Hinton, 2009; Turian et al., 2010). 5 Feature Set 5.1 Baseline Feature Set Sun et al. (2011) utilize the full feature set from (Zhou et al., 2005) plus some additional features and achieve the state-of-the-art feature-based RE system. Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often missing or noisy in reality (Plank and Moschitti, 2013). This issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize en</context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2009. A Scalable Hierarchical Distributed Language Model. In NIPS, page 1081-1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Convolution Kernels on Constituent, Dependency and Sequential Structures for Relation Extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP 09,</booktitle>
<pages>1378--1387</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1318" citStr="Nguyen et al., 2009" startWordPosition="193" endWordPosition="196">ffectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically (Plank and Moschitti, 2013). This is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one domain (the Ralph Grishman</context>
</contexts>
<marker>Nguyen, Moschitti, Riccardi, 2009</marker>
<rawString>Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. 2009. Convolution Kernels on Constituent, Dependency and Sequential Structures for Relation Extraction. In Proceedings of EMNLP 09, pages 1378-1387, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL 2013,</booktitle>
<pages>1498--1507</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1789" citStr="Plank and Moschitti, 2013" startWordPosition="269" endWordPosition="272">11) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically (Plank and Moschitti, 2013). This is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one domain (the Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu source domain) into a new model which can perform well on new domains (the target domains). The consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging (Blitzer et al., 2006; Huang and Yates, 2010; Schnabel and Sch¨utze, 2014), named entity</context>
<context position="3129" citStr="Plank and Moschitti (2013)" startWordPosition="488" endWordPosition="491"> 2010; Blitzer et al., 2011), etc. Unfortunately, there is very little work on domain adaptation for RE. The only study explicitly targeting this problem so far is by Plank and Moschitti (2013) who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suffers from two major limitations: + It does not incorporate word cluster information at different levels of granularity. In fact, Plank and Moschitti (2013) only use the 10-bit cluster prefix in their study. We will demonstrate later that the adaptability of relation extractors can benefit significantly from the addition of word cluster features at various granularities. + It is unclear if this approach can encode realvalued features of words (such as word embeddings (Mnih and Hinton, 2007; Collobert and Weston, 2008)) effectively. As the real-valued features are able to capture latent yet useful properties of words, the augmentation of lexical terms with these features is desirable to provide a more general representation, potentially helping re</context>
<context position="6689" citStr="Plank and Moschitti (2013)" startWordPosition="1052" endWordPosition="1055"> fits the training data, but also avoids ovefitting over it. This is often obtained via regularization methods to penalize complexity of classifiers. Exploiting the shared interest in generalization performance with traditional machine learning, in domain adaptation for RE, we would prefer the relation extractor that fits the source domain data, but also circumvents the overfitting problem over this source domain1 so that it could generalize well on new domains. Eventually, regularization methods can be considered naturally as a simple yet general technique to cope with DA problems. Following Plank and Moschitti (2013), we assume that we only have labeled data in a single source domain but no labeled as well as unlabeled target data. Moreover, we consider the singlesystem DA setting where we construct a single system able to work robustly with different but related domains (multiple target domains). This setting differs from most previous studies (Blitzer et al., 2006) on DA which have attempted to design a specialized system for every specific target domain. In our view, although this setting is more challenging, it is more practical for RE. In fact, this setting can benefit considerably from our general a</context>
<context position="10523" citStr="Plank and Moschitti, 2013" startWordPosition="1648" endWordPosition="1651">wo typical language models: Collobert and Weston (2008) embeddings (C&amp;W) (Collobert and Weston, 2008; Turian et al., 2010) and Hierarchical log-bilinear embeddings (HLBL) (Mnih and Hinton, 2007; Mnih and Hinton, 2009; Turian et al., 2010). 5 Feature Set 5.1 Baseline Feature Set Sun et al. (2011) utilize the full feature set from (Zhou et al., 2005) plus some additional features and achieve the state-of-the-art feature-based RE system. Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often missing or noisy in reality (Plank and Moschitti, 2013). This issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available. Therefore, following the settings of Plank and Moschitti (2013), we will only assume entity boundaries and not rely on the gold standard information in the experiments. We apply the same feature set as Sun et al. (2011) but remove the entity and mention type information2. 5.2 Lexical Feature Augmentation While Sun et al. (2011) show</context>
<context position="12715" citStr="Plank and Moschitti (2013)" startWordPosition="2012" endWordPosition="2015"> the first and second mentions in the relation. Table 1 describes the lexical feature groups. Rank Group Lexical Features 1 HM HM1 (head of M1) HM2 (head of M2) 2 BagWM WM1 (words in M1) WM2 (words in M2) 3 HC heads of chunks in context 4 BagWC words of context Table 1: Lexical feature groups ordered by importance. 6 Experiments 6.1 Tools and Data Our relation extraction system is hierarchical (Bunescu and Mooney, 2005b; Sun et al., 2011) and apply maximum entropy (MaxEnt) in the MALLET3 toolkit as the machine learning tool. For Brown word clusters, we directly apply the clustering trained by Plank and Moschitti (2013) 2We have the same observation as Plank and Moschitti (2013) that when the gold-standard labels are used, the impact of word representations is limited since the goldstandard information seems to dominate. However, whenever the gold labels are not available or inaccurate, the word representations would be useful for improving adaptability performance. Moreover, in all the cases, regularization methods are still effective for domain adaptation of RE. 3http://mallet.cs.umass.edu/ 70 System In-domain (bn+nw) Out-of-domain (bc development set) C&amp;W,25 C&amp;W,50 C&amp;W,100 HLBL,50 HLBL,100 C&amp;W,25 C&amp;W,50 C</context>
<context position="15006" citStr="Plank and Moschitti (2013)" startWordPosition="2375" endWordPosition="2378">features. The cells in bold are the best results. to facilitate system comparison later. We evaluate C&amp;W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al. (2010) and can be downloaded here4. The fact that we utilize the large, general and unbiased resources generated from the previous works for evaluation not only helps to verify the effectiveness of the resources across different tasks and settings but also supports our setting of single-system DA. We use the ACE 2005 corpus for DA experiments (as in Plank and Moschitti (2013)). It involves 6 relation types and 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un). We follow the standard practices on ACE (Plank and Moschitti, 2013) and use news (the union of bn and nw) as the source domain and bc, cts and wl as our target domains. We take half of bc as the only target development set, and use the remaining data and domains for testing purposes (as they are small already). As noted in Plank and Moschitti (2013), the distributions of relations as well as the vocabularies of the domains a</context>
<context position="18432" citStr="Plank and Moschitti (2013)" startWordPosition="2952" endWordPosition="2955"> 50 and 100 dimensions) is more effective for the in-domain setting. For the next experiments, we will apply the C&amp;W embedding of 50 dimensions to the heads of the mentions for its best out-of-domain performance. 71 6.3 Domain Adaptation with Word Embeddings This section examines the effectiveness of word representations for RE across domains. We evaluate word cluster and embedding (denoted by ED) features by adding them individually as well as simultaneously into the baseline feature set. For word clusters, we experiment with two possibilities: (i) only using a single prefix length of 10 (as Plank and Moschitti (2013) did) (denoted by WC10) and (ii) applying multiple prefix lengths of 4, 6, 8, 10 together with the full string (denoted by WC). Table 3 presents the system performance (F measures) for both in-domain and out-of-domain settings. System In-domain bc cts wl Baseline(B) 51.4 49.7 41.5 36.6 B+WC10 52.3( +0.9) 50.8(+1.1) 45.7( +4.2) 39.6( +3) B+WC 53.7( +2.3) 52.8(+3.1) 46.8( +5.3) 41.7( +5.1) B+ED 54.1( +2.7) 52.4(+2.7) 46.2( +4.7) 42.5( +5.9) B+WC+ED 55.5( +4.1) 53.8(+4.1) 47.4( +5.9) 44.7( +8.1) Table 3: Domain Adaptation Results with Word Representations. All the improvements over the baseline i</context>
<context position="21257" citStr="Plank and Moschitti, 2013" startWordPosition="3421" endWordPosition="3424">onfidence level &gt; 95% in domains bc and wl). The result suggests that word embeddings seem to capture different information from word clusters and their combination would be effective to generalize relation extractors across domains. However, in domain cts, the improvement that word embeddings provide for word clusters is modest. This is because the RCV1 corpus used to induce the word embeddings (Turian et al., 2010) does not cover spoken language words in cts very well. (v): Finally, the in-domain performance is also improved consistently demonstrating the robustness of word representations (Plank and Moschitti, 2013). 6.4 Domain Adaptation with Regularization All the experiments we have conducted so far do not apply regularization for training. In this section, in order to evaluate the effect of regularization on the generalization capacity of relation extractors across domains, we replicate all the experiments in Section 6.3 but apply regularization when relation extractors are trained6. Table 4 presents the results. System In-domain bc cts wl Baseline(B) 56.2 55.5 48.7 42.2 B+WC10 57.5( +1.3) 57.3(+1.8) 52.3(+3.6) 45.0( +2.8) B+WC 58.9( +2.7) 58.4(+2.9) 52.8(+4.1) 47.3( +5.1) B+ED 58.9( +2.7) 59.5(+4.0)</context>
</contexts>
<marker>Plank, Moschitti, 2013</marker>
<rawString>Barbara Plank and Alessandro Moschitti. 2013. Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction. In Proceedings of the ACL 2013, pages 1498-1507, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longhua Qian</author>
<author>Guodong Zhou</author>
<author>Qiaoming Zhu</author>
<author>Peide Qian</author>
</authors>
<title>Exploiting Constituent Ddependencies for Tree Kernel-based Semantic Relation Extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>697--704</pages>
<location>Manchester.</location>
<contexts>
<context position="1296" citStr="Qian et al., 2008" startWordPosition="189" endWordPosition="192">e demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically (Plank and Moschitti, 2013). This is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one doma</context>
</contexts>
<marker>Qian, Zhou, Zhu, Qian, 2008</marker>
<rawString>Longhua Qian, Guodong Zhou, Qiaoming Zhu and Peide Qian. 2008. Exploiting Constituent Ddependencies for Tree Kernel-based Semantic Relation Extraction. In Proceedings of COLING, pages 697-704, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Schnabel</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>FLORS: Fast and Simple Domain Adaptation for Part-ofSpeech Tagging.</title>
<date>2014</date>
<journal>In Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<pages>1526</pages>
<marker>Schnabel, Sch¨utze, 2014</marker>
<rawString>Tobias Schnabel and Hinrich Sch¨utze. 2014. FLORS: Fast and Simple Domain Adaptation for Part-ofSpeech Tagging. In Transactions of the Association for Computational Linguistics, 2 (2014), pages 1526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic Compositionality through Recursive Matrix-Vector Spaces.</title>
<date>2012</date>
<booktitle>In Proceedings EMNLP-CoNLL’12,</booktitle>
<pages>1201--1211</pages>
<location>Jeju Island,</location>
<contexts>
<context position="7941" citStr="Socher et al. (2012)" startWordPosition="1257" endWordPosition="1260">ntations and regularization. Finally, due to this setting, the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization (Jiang and Zhai, 2007b). 3 Related Work Although word embeddings have been successfully employed in many NLP tasks (Collobert and Weston, 2008; Turian et al., 2010; Maas and Ng, 2010), the application of word embeddings in RE is very recent. Kuksa et al. (2010) propose an abstraction-augmented string kernel for bio-relation extraction via word embeddings. In the surge of deep learning, Socher et al. (2012) and Khashabi (2013) use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks (MV-RNN) to learn compositional structures for RE. However, none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper. Regarding domain adaptation, in representation learning, Blitzer et al. (2006) propose structural correspondence learning (SCL) while Huang and Yates (2010) attempt to learn a multi-dimensional feature representation. Unfortunately, these methods require unlabeled target domain data which are unavailable in our single-sy</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic Compositionality through Recursive Matrix-Vector Spaces. In Proceedings EMNLP-CoNLL’12, pages 1201-1211, Jeju Island, Korea, July 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ang Sun</author>
<author>Ralph Grishman</author>
<author>Satoshi Sekine</author>
</authors>
<title>Semi-supervised Relation Extraction with Largescale Word Clustering.</title>
<date>2011</date>
<booktitle>In Proceedings of ACLHLT,</booktitle>
<pages>521--529</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="1166" citStr="Sun et al., 2011" startWordPosition="166" endWordPosition="169">pply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically (Pla</context>
<context position="4428" citStr="Sun et al., 2011" startWordPosition="689" endWordPosition="692">o avoid these limitations by applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more natu68 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 68–74, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics rally and effectively. The application of word representations such as word clusters in domain adaptation of RE (Plank and Moschitti, 2013) is motivated by its successes in semi-supervised methods (Chan and Roth, 2010; Sun et al., 2011) where word representations help to reduce data-sparseness of lexical information in the training data. In DA terms, since the vocabularies of the source and target domains are usually different, word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains, hence bridge the gap between domains. The underlying hypothesis here is that the absence of lexical target-domain features in the source domain can be compensated by these general features to improve RE performance on the target domains. We extend this motivation by further e</context>
<context position="10193" citStr="Sun et al. (2011)" startWordPosition="1600" endWordPosition="1603">l vocabulary, word embeddings are dense, lowdimensional, and real-valued vectors (distributed representations). Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al., 2010). We investigate word embeddings induced by two typical language models: Collobert and Weston (2008) embeddings (C&amp;W) (Collobert and Weston, 2008; Turian et al., 2010) and Hierarchical log-bilinear embeddings (HLBL) (Mnih and Hinton, 2007; Mnih and Hinton, 2009; Turian et al., 2010). 5 Feature Set 5.1 Baseline Feature Set Sun et al. (2011) utilize the full feature set from (Zhou et al., 2005) plus some additional features and achieve the state-of-the-art feature-based RE system. Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often missing or noisy in reality (Plank and Moschitti, 2013). This issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available. Ther</context>
<context position="11541" citStr="Sun et al. (2011)" startWordPosition="1815" endWordPosition="1818">d information in the experiments. We apply the same feature set as Sun et al. (2011) but remove the entity and mention type information2. 5.2 Lexical Feature Augmentation While Sun et al. (2011) show that adding word clusters to the heads of the two mentions is the most effective way to improve the generalization accuracy, the right lexical features into which word embeddings should be introduced to obtain the best adaptability improvement are unexplored. Also, which dimensionality of which word embedding should we use with which lexical features? In order to answer these questions, following Sun et al. (2011), we first group lexical features into 4 groups and rank their importance based on linguistic intuition and illustrations of the contributions of different lexical features from various featurebased RE systems. After that, we evaluate the effectiveness of these lexical feature groups for word embedding augmentation individually and incrementally according to the rank of importance. For each of these group combinations, we assess the system performance with different numbers of dimensions for both C&amp;W and HLBL word embeddings. Let M1 and M2 be the first and second mentions in the relation. Tabl</context>
<context position="16971" citStr="Sun et al., 2011" startWordPosition="2710" endWordPosition="2713">that for C&amp;W and HLBL embeddings of 50 and 100 dimensions, the most effective way to introduce word embeddings is to add embeddings to the heads of the two mentions (row 2; both in-domain and out-of-domain) although it is less pronounced for HLBL embedding with 50 dimensions. Interestingly, for C&amp;W embedding with 25 dimensions, adding the embedding to both heads and words of the two mentions (row 6) performs the best for both in-domain and out-of-domain scenarios. This is new compared to the word cluster features where the heads of the two mentions are always the best places for augmentation (Sun et al., 2011). It suggests that a suitable amount of embeddings for words in the mentions might be useful for the augmentation of the heads and inspires further exploration. Introducing embeddings to words of mentions alone has mild impact while it is generally a bad idea to augment chunk heads and words in the contexts. Comparing C&amp;W and HLBL embeddings is somehow more complicated. For both in-domain and out-of-domain settings with different numbers of dimensions, C&amp;W embedding outperforms HLBL embedding when only the heads of the mentions are augmented while the degree of negative impact of HLBL embeddin</context>
</contexts>
<marker>Sun, Grishman, Sekine, 2011</marker>
<rawString>Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011. Semi-supervised Relation Extraction with Largescale Word Clustering. In Proceedings of ACLHLT, pages 521-529, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL’10),</booktitle>
<pages>384--394</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5168" citStr="Turian et al., 2010" startWordPosition="811" endWordPosition="814">ce the vocabularies of the source and target domains are usually different, word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains, hence bridge the gap between domains. The underlying hypothesis here is that the absence of lexical target-domain features in the source domain can be compensated by these general features to improve RE performance on the target domains. We extend this motivation by further evaluating word embeddings (Bengio et al., 2001; Bengio et al., 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008; Turian et al., 2010) on feature-based methods to adapt RE systems to new domains. We explore the embedding-based features in a principled way and demonstrate that word embedding itself is also an effective representation for domain adaptation of RE. More importantly, we show empirically that word embeddings and word clusters capture different information and their combination would further improve the adaptability of relation extractors. 2 Regularization Given the more general representations provided by word representations above, how can we learn a relation extractor from the labeled source domain data that gen</context>
<context position="7695" citStr="Turian et al., 2010" startWordPosition="1217" endWordPosition="1220"> design a specialized system for every specific target domain. In our view, although this setting is more challenging, it is more practical for RE. In fact, this setting can benefit considerably from our general approach of applying word representations and regularization. Finally, due to this setting, the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization (Jiang and Zhai, 2007b). 3 Related Work Although word embeddings have been successfully employed in many NLP tasks (Collobert and Weston, 2008; Turian et al., 2010; Maas and Ng, 2010), the application of word embeddings in RE is very recent. Kuksa et al. (2010) propose an abstraction-augmented string kernel for bio-relation extraction via word embeddings. In the surge of deep learning, Socher et al. (2012) and Khashabi (2013) use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks (MV-RNN) to learn compositional structures for RE. However, none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper. Regarding domain adaptation, in representation learning, Blitzer et al. (200</context>
<context position="9852" citStr="Turian et al., 2010" startWordPosition="1547" endWordPosition="1550">equence labeling in the previous research. 4 Word Representations We consider two types of word representations and use them as additional features in our DA system, namely Brown word clustering (Brown et al., 1992) and word embeddings (Bengio et al., 2001). While word clusters can be recognized as an one-hot vector representation over a small vocabulary, word embeddings are dense, lowdimensional, and real-valued vectors (distributed representations). Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al., 2010). We investigate word embeddings induced by two typical language models: Collobert and Weston (2008) embeddings (C&amp;W) (Collobert and Weston, 2008; Turian et al., 2010) and Hierarchical log-bilinear embeddings (HLBL) (Mnih and Hinton, 2007; Mnih and Hinton, 2009; Turian et al., 2010). 5 Feature Set 5.1 Baseline Feature Set Sun et al. (2011) utilize the full feature set from (Zhou et al., 2005) plus some additional features and achieve the state-of-the-art feature-based RE system. Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types</context>
<context position="14634" citStr="Turian et al. (2010)" startWordPosition="2312" endWordPosition="2315">.6( +1.6) 50.0( +1.0) 48.6( -0.4) 7 6+HC ED 53.4( +2.0) 52.3( +0.9) 52.7(+1.3) 54.2(+2.8) 53.1( +1.7) 50.5( +1.5) 50.9( +1.9) 48.4( -0.6) 50.0( +1.0) 48.9( -0.1) 8 7+BagWC ED 53.4( +2.0) 52.2( +0.8) 50.8(-0.6) 53.5(+2.1) 53.6( +2.2) 49.2( +0.2) 50.7( +1.7) 49.2( +0.2) 47.9( -1.1) 49.5( +0.5) Table 2: In-domain and Out-of-domain performance for different embedding features. The cells in bold are the best results. to facilitate system comparison later. We evaluate C&amp;W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al. (2010) and can be downloaded here4. The fact that we utilize the large, general and unbiased resources generated from the previous works for evaluation not only helps to verify the effectiveness of the resources across different tasks and settings but also supports our setting of single-system DA. We use the ACE 2005 corpus for DA experiments (as in Plank and Moschitti (2013)). It involves 6 relation types and 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un). We follow the standard practices on ACE (Plank and Mosch</context>
<context position="21051" citStr="Turian et al., 2010" startWordPosition="3391" endWordPosition="3394"> In row 5, we see that the addition of both word cluster and word embedding features improves the system further and results in the best performance over all target domains (this is significant with confidence level &gt; 95% in domains bc and wl). The result suggests that word embeddings seem to capture different information from word clusters and their combination would be effective to generalize relation extractors across domains. However, in domain cts, the improvement that word embeddings provide for word clusters is modest. This is because the RCV1 corpus used to induce the word embeddings (Turian et al., 2010) does not cover spoken language words in cts very well. (v): Finally, the in-domain performance is also improved consistently demonstrating the robustness of word representations (Plank and Moschitti, 2013). 6.4 Domain Adaptation with Regularization All the experiments we have conducted so far do not apply regularization for training. In this section, in order to evaluate the effect of regularization on the generalization capacity of relation extractors across domains, we replicate all the experiments in Section 6.3 but apply regularization when relation extractors are trained6. Table 4 presen</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL’10), pages 384-394, Uppsala, Sweden, July, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Xiao</author>
<author>Yuhong Guo</author>
</authors>
<title>Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model.</title>
<date>2013</date>
<booktitle>In Proceedings of the 30th International Conference on Machine Learning (ICML-13),</booktitle>
<pages>293--301</pages>
<contexts>
<context position="8850" citStr="Xiao and Guo (2013)" startWordPosition="1394" endWordPosition="1397">main adaptation, in representation learning, Blitzer et al. (2006) propose structural correspondence learning (SCL) while Huang and Yates (2010) attempt to learn a multi-dimensional feature representation. Unfortunately, these methods require unlabeled target domain data which are unavailable in our single-system setting of DA. Daum´e III (2007) proposes an easy adaptation framework (EA) which is later extended to a semisupervised version (EA++) to incorporate unla1domain overfitting (Jiang and Zhai, 2007b) 69 beled data (Daum´e III et al., 2010). In terms of word embeddings for DA, recently, Xiao and Guo (2013) present a log-bilinear language adaptation framework for sequential labeling tasks. However, these methods assume some labeled data in target domains and are thus not applicable in our setting of unsupervised DA. Above all, we move one step further by evaluating the effectiveness of word embeddings on domain adaptation for RE which is very different from the principal topic of sequence labeling in the previous research. 4 Word Representations We consider two types of word representations and use them as additional features in our DA system, namely Brown word clustering (Brown et al., 1992) an</context>
</contexts>
<marker>Xiao, Guo, 2013</marker>
<rawString>Min Xiao and Yuhong Guo. 2013. Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 293-301, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel Methods for Relation Extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--10831106</pages>
<contexts>
<context position="1203" citStr="Zelenko et al., 2003" startWordPosition="173" endWordPosition="176">best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically (Plank and Moschitti, 2013). This is wher</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel Methods for Relation Extraction. Journal of Machine Learning Research, 3:10831106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>GuoDong Zhou</author>
</authors>
<title>A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL-06,</booktitle>
<pages>825--832</pages>
<location>Sydney.</location>
<contexts>
<context position="1277" citStr="Zhang et al., 2006" startWordPosition="185" endWordPosition="188">ormation. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically (Plank and Moschitti, 2013). This is where we need to resort to domain adaptation techniques (DA) to adapt a model </context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou. 2006. A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features. In Proceedings of COLING-ACL-06, pages 825-832, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Jie Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Exploring various Knowledge in Relation Extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05,</booktitle>
<pages>427--434</pages>
<location>Ann Arbor, USA,</location>
<contexts>
<context position="1080" citStr="Zhou et al., 2005" startWordPosition="150" endWordPosition="153">feature-based relation extraction systems. We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data</context>
<context position="10247" citStr="Zhou et al., 2005" startWordPosition="1610" endWordPosition="1613">al, and real-valued vectors (distributed representations). Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al., 2010). We investigate word embeddings induced by two typical language models: Collobert and Weston (2008) embeddings (C&amp;W) (Collobert and Weston, 2008; Turian et al., 2010) and Hierarchical log-bilinear embeddings (HLBL) (Mnih and Hinton, 2007; Mnih and Hinton, 2009; Turian et al., 2010). 5 Feature Set 5.1 Baseline Feature Set Sun et al. (2011) utilize the full feature set from (Zhou et al., 2005) plus some additional features and achieve the state-of-the-art feature-based RE system. Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often missing or noisy in reality (Plank and Moschitti, 2013). This issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available. Therefore, following the settings of Plank and Moschitti (</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various Knowledge in Relation Extraction. In Proceedings of ACL’05, pages 427-434, Ann Arbor, USA, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>