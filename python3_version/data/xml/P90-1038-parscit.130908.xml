<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<sectionHeader confidence="0.958529" genericHeader="abstract">
BOTTOM-UP PARSING EXTENDING CONTEXT-FREENESS
IN A PROCESS GRAMMAR PROCESSOR
</sectionHeader>
<author confidence="0.263473">
Massimo Marino
</author>
<affiliation confidence="0.419953">
Department of Linguistics - University of Pisa
</affiliation>
<address confidence="0.423333">
Via S. Maria 36 1-56100 Pisa - ITALY
</address>
<email confidence="0.974599">
Bitnet: massimom@icnucevrri.cnuce.cnr.it
</email>
<sectionHeader confidence="0.994716" genericHeader="introduction">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999675230769231">
A new approach to bottom-up parsing that extends
Augmented Context-Free Grammar to a Process Grammar
is formally presented. A Process Grammar (PG) defines a
set of rules suited for bottom-up parsing and conceived as
processes that are applied by a PG Processor. The matching
phase is a crucial step for process application, and a
parsing structure for efficient matching is also presented.
The PG Processor is composed of a process scheduler that
allows immediate constituent analysis of structures, and
behaves in a non-deterministic fashion. On the other side,
the PG offers means for implementing specOc parsing
strategies improving the lack of determinism innate in the
processor.
</bodyText>
<sectionHeader confidence="0.997126" genericHeader="method">
I. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.998740916666667">
Bottom-up parsing methods are usually preferred
because of their property of being driven from both the
input&apos;s syntactic/semantic structures and reduced
constituents structures. Different strategies have been
realized for handling the structures construction, e.g.,
parallel parsers, backtracking parsers, augmented context-
free parsers (Aho et al., 1972; Grishman, 1976; Winograd,
1983). The aim of this paper is to introduce a new approach
to bottom-up parsing starting from a well known and based
framework - parallel bottom-up parsing in immediate
constituent analysis, where all possible parses are
considered - making use of an Augmented Phrase-Structure
Grammar (APSG). In such environment we must perform
efficient searches in the graph the parser builds, and limit as
much as possible the building of structures that will not be
in the final parse tree. For the efficiency of the search we
introduce a Parse Graph Structure, based on the definition of
adjacency of the subtre-es, that provides an easy method of
evaluation for deciding at any step whether a matching
process can be accomplished or not. The control of the
parsing process is in the hands of an APSG called Process
Grammar (PG), where grammar rules are conceived as
processes that are applied whenever proper conditions,
detected by a process scheduler, exist. This is why the
parser, called PG Processor, works following a non-
deterministic parallel strategy, and only the Process
Grammar has the power of altering and constraining this
behaviour by means of some Kernel Functions that can
modify the control structures of the PG Processor, thus
improving determinism of the parsing process, or avoiding
construction of useless mixtures. Some of the concepts
introduced in this paper, such as some definitions in Section
2., are a development from Grishman (1976) that can be also
an introductory reading regarding the description of a
parallel bottom-up parser which is, even if under a different
aspect, the core of the PG Processor.
</bodyText>
<sectionHeader confidence="0.998214" genericHeader="method">
2. PARSE GRAPH STRUCTURE
</sectionHeader>
<bodyText confidence="0.8040286">
The Parse Graph Structure (PGS) is built by the parser
while applying grammar rules. Ifs = a, a, ... ; is an input
string the initial PGS is composed by a set of terminal nodes
&lt;0,5&gt;, , &lt;n+1,$&gt;, where nodes
0,n+ I represent border markers for the sentence. All the next
non-terminal nodes are numbered starting from n+2.
Definition 2.1. A PGS is a triple (NT,NN,T) where NT is the
set of the terminal nodes numbers [0, 1, n, n+1 ) ; NH is
the set of the non-terminal nodes numbers n+2,... ),and T
is the set of the subtrees.
The elements of N, and NT are numbers identifying nodes
of the PGS whose structure is defined below, and
throughout the paper we refer to nodes of the PGS by means
of such nodes number.
Definition 2.2. If ke NN the node ie NT labeling a, at the
beginning of the clause covered by k is said to be the left
corner leaf of k lcI(k). If ke NT then 1c1(k)=k.
Definition 23. if he NI, the node je NT labeling a, at the end
of the clause covered by his said to be the right corner leaf
of k rcl(k). If ke NT then rcl(k) = k.
</bodyText>
<construct confidence="0.823514">
Definition 2.4. If ke NH the node he NT that follows the right
corner leaf of k rcl(k) is said to be the anchor leaf of k al(k),
and al(k) = h = rcl(k)+1. If ke NT-(n+I) then al(k) = k+1.
Definition 2.5. If ke NT the set of the anchored nodes of
k an(k) is an(k) = je NTUNN I al(j) = k).
From this definition it follows that for every ke NT-(0),
an(k) contains at the initial time the node number (k-1).
Definition 2.6. a. If ke NT the subtree rooted in k T(k) is
represented by TOO = rlicl(k),rcl(k),an(k),cat(k)&gt;, where
his the root node; Icl(k) = rcI(k)= k; an(k) = (k - 1)) initially;
cat(k) -= c, the terminal category of the node.
</construct>
<bodyText confidence="0.8978836">
b. If he NH the subtree rooted in k T(k) is represented by
T(k).rkjei(k),rcl(k),sons00,cat(k)&gt;, where k is the root
node; sons(k) = ,sr) , SIE NTLINN, I = 1,... ,p, is the set
of the direct descendants of k; cat(k) = A, a non-terminal
category assigned to the node.
</bodyText>
<page confidence="0.996523">
299
</page>
<bodyText confidence="0.996463142857143">
From the above definitions the initial PGS for a
sentence s=a1a2...a. is: NT= [0,1,...,n,n+1) , NN=(),
T= (T(0),T(1),...,T(n),T(n+1)); and: T(0)=&lt;0,0,0, ,$&gt;,
T(i)=&lt;i,i,i,(i -1 ) ,a,› for i=1,... in , and T(n+1)=&lt;n+1,
n+1,n+1,(n) ,$&gt;. With this PGS the parser starts its work
reducing new nodes from the already existing ones. If for
some k NN, T(k)=&lt;k,1c1(k),rcl(k), s ,sp ,A&gt;, and
T(s,)=&lt;si,1c1(s),rcl(si),(s11,...,s,0),zi&gt;e T, for i = 1,...,p, are
the direct descendants of k, then k has been reduced from
sy• • ..sT, by some grammar rule whose reduction rule, as we
shall see later, has the form (A4—z1...zp), and the following
holds: kl(k) = tel(s1) =1c14)-1, rcl(s2) =1c1(s3)-1,
rcl(sp.1)=1c1(s)-1, rcl(s)= rcl(k).From that we can give the
following definition:
of the match process the matcher must start from the last
scanned or built node zy finding afterwards z2 and z1,
respectively, sailing in the PGS right-to-left and passing
through adjacent subtrees. Steps through adjacent subtrees
are easily accomplished by using the sets of the anchored
nodes in the terminal nodes. It follows from the above
definitions that if ke NI, then the subtrees adjacent to T(k)
are given by an(Icl(k)), whereas if he NT then the adjacent
subtrees are given by an(k). The lists of the anchored nodes
provide an efficient way to represent the relation of
adjacency between nodes. These sets stored only in the
terminal nodes provide an efficient data structure useful for
the matcher to accomplish its purpose. Figure 1 shows a
parse tree at a certain time of a parse, where under each
</bodyText>
<equation confidence="0.9945656">
&lt;12,a12&gt;
&lt;14,a14&gt; &lt;13,a13&gt;
T(9) =&lt;9,1,2,(1,2),a9&gt;
T(10) = &lt;10,2,2,(2),a10&gt;
T(11).&lt;11,2,3,(10,31,all&gt;
T(12) .&lt;12,1,3,(9,31,a12&gt;
T(13) = &lt;13,4,5,(4,5),a13&gt;
T(14) = &lt;14,3,5,(3,4,5),a14&gt;
&lt;0,$&gt; &lt;Ltd&gt; &lt;2,a2&gt; &lt;3,a3&gt; &lt;4,a4&gt; &lt;5,a5&gt; &lt;6,a6&gt; &lt;7,a7&gt; &lt;8,5&gt;
() (0) (1) (2,9,101 (3,11,12) (4) (5,13,141 (6) (7)
</equation>
<figureCaption confidence="0.999209">
Figure 1. A parse tree with the sets of the anchored nodes
</figureCaption>
<figure confidence="0.995820884615385">
8
a7
7
a6 I
6
a5 al4
14
a9
2 9 I0
a3
5
a4
4
112 a2
3
9
2
12
10
al0
2
I al
1
al
I al al 1
1
</figure>
<figureCaption confidence="0.999779">
Figure 2. Adjacency Tree
</figureCaption>
<equation confidence="0.8376025">
Definition 2.7. If ( is a set of nodes in the PGS, then
their subtrees T(s,),...,T(sp) are said to be adjacent when
rel(si) = Icl(s61)-1 or, alternatively, al(;) = lc.1(si.1), for i =
1....&apos;p-1.
</equation>
<bodyText confidence="0.926879416666667">
During a parsing process a great effort is made in finding a
set of adjacent subtrees that match a right-hand side of a
reduction rule. Let (M--z, z1 z3)be a reduction rule, then the
parser should start a match process to find all possible sets
of adjacent subtrees such that their categories match z, z2 z.
The parser scans the input string left-to-right, so reductions
grow on the left of the scanner pointer, and for the efficiency
terminal node there is the corresponding list of the anchored
nodes. A useful structure that can be derived from these sets
is an adjacency tree, recursively defined as follows:
Definition 2.8. If (NT,NN,T) is a PGS for an input sentence
s, and isi = n, then the adjacency tree for the PGS is so built:
</bodyText>
<listItem confidence="0.764784">
- n+1 is the root of the adjacency tree;
- for every kE NT- (0,1)uNN, the sons of k are the nodes in
an(1c1(k)) unless an(1c1(1c))= (01.
</listItem>
<bodyText confidence="0.872914666666667">
Figure 2 shows the adjacency tree obtained from the partial
parse tree in Figure 1. Any passage from a node k to one of
its sons h in the adjacency tree represents a passage from a
</bodyText>
<page confidence="0.994927">
300
</page>
<bodyText confidence="0.968357891891892">
subtree T(k) to one of its adjacent subtrees T(h) in the PGS.
Moreover, during a match process this means that a
constituent of the right-hand side has been consumed, and
matching the first symbol that match process is finished.
The adjacency tree also provides further useful information
for optimizing the search during a match. For every node k,
if we consider the longest path from k to a leaf, its length is
an upper bound for the length of the right hand side still to
consume, and since the sons of k are the nodes in an(lcl(k)),
the longest path is always given by the sequence of the
terminal nodes from the node 1 to the node kl(k)-1. Thus its
length is just 1c1(k)-1.
Property 2.1. If (NrNN,T) is a PGS, (Ac—; _xi)) is a
reduction rule whose right-hand side has to be matched, and
T(k)e T such that cat(k) = zp, then:
a. the string; zp is matchable if p kl(k);
b. for i = p,...,1, zi is partially matchable to a node
Definition 2.10. If (NT,NN,T) is a PGS, an adjacency
digraph can be represented as follows:
a. for any ke Nr, k has outgoing arcs directed to the nodes in
aria*
b. for any ke NN, k has one outgoing arc directed to 1c1(k).
In the classic literature the lists of the anchored nodes are
called adjacency lists, and are used for representing graphs
(Aho et al., 1974). A graph G=(V,E) can be usually
represented by IVI adjacency lists. In our representation we
can obtain an optimization representing an adjacency
digraph by n adjacency lists, if n is the length of the sentence,
and by EN) simple pointers for accessing the adjacency lists
from the non-terminal nodes, with respect to n+INNI
adjacency lists for a full representation of an adjacency
digraph composed of arcs as in Defmition 2.10.a.
Figure 3 shows how a new non-terminal node is connected
in an adjacency digraph, and Figure 4 shows the adjacency
•-&apos;&apos; ak 1c1(k) 411- - - - k access from k to Ici(k)
A&amp;quot; r ax k T(k) is adjacent to T(r)
400-1c1(k-1) ■ati— kl(k)44— reI(k) al(k) = rcl(k)+ hog-
</bodyText>
<figureCaption confidence="0.924209">
Figure 3. Adding a non-terminal node k to an adjacency digraph
</figureCaption>
<figure confidence="0.990931">
••• - 12
.P 2
1-01 a1 --- a 4 a4 5 -4‘ 15 a? 8
</figure>
<figureCaption confidence="0.862932">
.. 113
. ,..
&apos; li , .. .14 ■ .
Figure 4. Adjacency Digraph
</figureCaption>
<bodyText confidence="0.999462666666667">
he NNuNr iff cat(h) = z. and i kl(h).
Property 2.1. along wittil the adjacency relation provides a
method for an efficient navigation within the PGS among
the subtrees. This navigation is performed by the matcher in
the PGS as visiting the adjacency tree in a pre-order fashion.
It is easy to see that a pre-order visit of the adjacency tree
scans all possible sequences of the adjacent subtrees in the
PGS, but Property 2.1 provides a shortcut for avoiding
useless passages when matchable conditions do not hold.
When a match ends the matcher returns one or more sets of
nodes satisfying the following conditions:
Deftnition2.9. A set RS et = (ni,...,np) is a match for a string
iff cat(ni) ;, for i 1,...,p, and T(n) is adjacent to
T(ni.), for i = l,... ,p-1. The set RSet is called a reduction
set.
The adjacency tree shows the hypothetical search space for
searching the reduction sets in a PGS, thus it is not a
representation of what memory is actually required to store
the useful data for such a search. A more suitable
representation is an adjacency directed graph defined by
means of the lists of the anchored nodes in the terminal
nodes, and by the pointers to the left comer leaf in the non-
terminal nodes.
digraph for the parse tree of Figure 1.
</bodyText>
<sectionHeader confidence="0.997619" genericHeader="method">
3. PROCESS GRAMMAR
</sectionHeader>
<bodyText confidence="0.99937325">
The Process Grammar is an extension of the Augmented
Context-Free Grammar such as APSG, oriented to bottom-
up parsing. Some relevant features make a Process
Grammar quite different from classical APSG.
</bodyText>
<listItem confidence="0.5526185625">
1. The parser is a PG processor that tries to apply the rules
in a bottom-up fashion. It does not have any knowledge
about the running grammar but for the necessary structures
to access its niles. Furthermore, it sees only its internal state,
the Parse Graph Structure, and works with a non-
deterministic strategy.
2. The rules are conceived as processes that the PG
processor schedules somehow. Any rule defines a reduction
rule that does not represent a rewriting rule, but rather a
statement for search and construction of new nodes in a
bottom-up way within the Parse Graph Structure.
3. The rules are augmented with some sequences of
operations to be performed as in the classical APSG. In
general, augmentations such as tests and actions concern
manipulation of linguistic data at syntactic and/or semantic
level. In this paper we are not concerned with this aspect (an
</listItem>
<page confidence="0.994471">
301
</page>
<bodyText confidence="0.99989505882353">
informal description about this is in Marino (1989)), rather
we examine some aspects concerning parsing strategies by
means of the augmentations.
In a Process Grammar the rules can have knowledge of
the existence of other rules and the purpose for which they
are defined. They can call some functions that act as filters
on the control structures of the parser for the scheduling of
the processes, thus altering the state of the processor and
forcing alternative applications. This means that any rule
has the power of changing the state of the processor
requiring different scheduling, and the processor is a blind
operator that works following a loose strategy such as the
non-deterministic one, whereas the grammar can drive the
processor altering its state. In such a way the lack of
determinism of the processor can be put in the Process
Grammar, implementing parsing strategies which are
transparent to the processor.
</bodyText>
<listItem confidence="0.840821857142857">
Definition 3.1. A Process Grammar PG is a 6-tuple
(VDV„,S,R,Vs,F) where:
VT is the set of terminal symbols;
- VN is the set of non-terminal symbols;
- Se V, is the Root Symbol of PG;
- R = tri,...,ri) is the set of the niles. Any rule r. in R is of
the form I., = &lt;red(r),st(ri),t(r.),a(r)&gt;, where red(r1) is a
</listItem>
<bodyText confidence="0.997027073170731">
reduction rule (Ak-a), Ae VN, etc st(ri) is the
state of the rule that can be active or inactive; t(r) and a(r1)
are the tests and the actions, respectively;
- Vs is a set of special symbols that can occur in a reduction
rule and have a special meaning. A special symbol is el, a
null category that can occur only in the left-hand side of a
reduction rule. Therefore, a reduction rule can also have the
form (y-a), and in the following we refer to it as e-
red uction ;
F = J..) is a set of functions the rules can call within
their au g men tations .
Such a definition extends classical APSG in some specific
ways: first, a Process Grammar is suited for bottom-up
parsing; second, rules have a state concerning the
applicability of a rule at a certain time; third, we extend the
CF structure of the reduction rule allowing null left-hand
sides by means of e-reductions; fourth, the set F is the
strategic side that should provide the necessary functions to
perform operations on the processor structures. As a matter
of fact, the set F can be further structured giving the PG a
wider complexity and power. In this paper we cannot treat
a formal extended definition for F due to space restrictions,
but a brief outline can be given. The set F can be defined as
F=Fx...L.,F,. In FK., are all those functions devoted to
operations on the processor structures (Kernel Functions),
and, in the case of a feature-based system, in F6 are all the
functions devoted to the management of feature structures
(Marino, 1989). In what follows we are also concerned with
the combined use of e-reductions and the function RA,
standing for Rule Activation, devoted to the immediate
scheduling of a rule. RAeFK., and a call to it means that the
specified rule must be applied, involving the scheduling
process we describe in Section 4. Before we introduce the
PG processor we must give a useful definition:
Definition 32. Let re R be a rule with t(r)=Ife;--.;fuil,
a(r)=[f1: be sequences of operations in its
augmentations, f1,...,f4,fai,...,firE F. Let (ni,...,np) be a
reduction set for red(r)= (M-;...zp), and he N, be the new
node for A such that T(h) is the new subtre-e created in the
PGS, then we define the Process Environment for t(r) and
a(r), denoted briefly by ProcEnv(r), as:
</bodyText>
<equation confidence="0.934334">
ProcEnv(r) = (h,ni,...,n )
If red(r) is an e-reduction then ProcErt4) =
</equation>
<bodyText confidence="0.999929">
This definition states the operative range for the
augmentations of any rule is limited to the nodes involved
by the match of the reduction rule.
</bodyText>
<sectionHeader confidence="0.997418" genericHeader="method">
4. PG PROCESSOR
</sectionHeader>
<bodyText confidence="0.981691083333333">
Process Scheduler. The process scheduler makes
possible the scheduling of the proper rules to run whenever
a terminal node is consumed in input or a new non-terminal
node is added to the PGS by a process. By proper rules we
mean all the rules satisfying Property 2.1.a. with respect to
the node being scanned or built. These rules are given by the
sets defined in the following definition:
Definition 4.1. Vce Vt4t..NT such that 3 re R where red(r) =
(M-ac), VNu (c) , being c the right corner of the
reduction rule, and lad l .5 L, being L the size of the longest
right-hand side having c as the right corner, the sets P(c,i),
P(c,i) for i = 1,...,L, can be built as follows:
</bodyText>
<equation confidence="0.9963055">
P(c,i) = (re R I red(r)=(Ak-ac), 1 5 lacl i, st(r)=active)
R I red(r)=(y-ac), 1 5 lacl I, st(r)=active)
</equation>
<bodyText confidence="0.999960227272727">
Whenever a node he NruN, has been scanned or built and
k=lcl(h), then the process scheduler has to schedule the rules
in P(cat(h),k)uPd(cat(h),k). In the following this union is
also denoted by 1-1(cat(h),k). Such a rule scheduling allows
an efficient realization of the immediate constituent
analysis approach within a bottom-up parser by means of a
partitioning of the rules in a Process Grammar.
The process scheduler sets up a process descriptor for each
rule in Il(cat(h),k) where the necessary data for applying a
process in the proper environment are supplied. In a Process
Grammar we can have three main kinds of rules: rules that
are activated by others by means of the function RA; e-
reduction rules; and standard rules that do not fall in the
previous cases. This categorization implies that processes
have assigned a priority depending on their kind. Thus
activated rules have the highest priority, e-reduction rules
have an intermediate priority and standard rules the lowest
priority. Rules become scheduled processes whenever a
process descriptor for them is created and inserted in a
priority queue by the process scheduler. The priority queue
is divided into three stacks, one for each kind of rule, and
they form one of the structures of the processor state.
</bodyText>
<page confidence="0.994182">
302
</page>
<bodyText confidence="0.995191044444445">
Definition 4.2. A process descriptor is a triple PD=fr,h,CI
where: re R is the rule involved; he NTk..)Nxt.)[NEL) is either
the right corner node from which the matcher starts or NIL;
C is a set of adjacent nodes or the empty set. A process
descriptor of the form fr,NlL,(ni,...,nc] is built for an
activated rule r and pushed in the stack si. A process
descriptor of the form [r,h,( ]] is built for all the other rules
and is pushed either in the stack s, if r is an e-reduction rule
or in the stack s, if a standard rule. Process descriptors of
these latter forms are handled by the process scheduler,
whereas process descriptors for activated rules are only
created and queued by the function RA.
State of Computation. The PG processor operates by
means of an operation Op on some internal structures that
define the processor state ProeState, and on the parsing
structures accessible by the process environment ProcEnv.
The whole state of computation is therefore given by:
[0p,ProcS tate ,ProcEnv] = [Op ,pt,[si,s2,s3],PD,pn,RS et]
where ptE NT is the input pointer to the last terminal node
scanned; pee NN is the pointer to the last non-terminal node
added to the PGS. For a sentence s=a,...a. the computation
starts from the initial state Ibegin,0,[NIL,NIL.NILL
NIL,n+1,(]], and terminates when the state becomes
[end,n,[NIL,N1L,NIL],NIL,pn,f )1 The aim of this section
is not to give a complete description of the processor cycle
in a parsing process, but an analysis of the activation
mechanism of the processes by means of two main cases of
rule scheduling and processing.
Scheduling and Processing of Standard Rules.
Whenever the state of computation becomes as [scan, pt,
[N1L,NIL,NIL],N]L,pn,( )1 the processor scans the next
terminal node, performing the following operations:
scan: scl if pt = n then Op &lt;— end
sc2 else pt 4— pt + 1;
sc3 schedule gi(cat(p0,1c1(pt)));
sc4 Op &lt;— activate.
Step sc4 allows the processor to enter in the state where it
determines the first non-empty higher priority stack where
the process descriptor for the next process to be activated
must be popped off. Let suppose that cat(pt)=z , and
fl(z ,k1(pt)).(r) where r is a standard rule suet&apos; that
red(r)=(A4--z1...zp). At this point the state is [activate,
pt,[NIL,N1L,[r,pt,[ DLNIL,pn,f)] and the processor has to
try reduction for the process in the stack ss, thus
ON—reduce performing the following statements:
</bodyText>
<table confidence="0.839856928571429">
reduce: rl PIN—pop (53);
[reduce,ptANIL,N11.„NIL],[r,pt,( ) ] ,pn, ]
C r2 C4—match (red(r), pt); 303
r3 pt, C];
[reduce,pt,fNIG,N1L,N1L] ,[r,pt,C] ,pn, (1]
r4 V rsete C:
r5 RS et &lt;—rset;
[reduce,pt,[NI1LNlL,N1L] ,[r,pt, [1] ,pn,RS et]
16 if t(r) then pn4—pn + 1;
r7 add_subtree(pn,red(r),RS et);
r8 a(r);
r9 schedule (II(cat(pn),1c1(pn));
[reduce,pt,[N1L,s21s3],[ript,( )],pn,RSet]
r10 Opt—activate.
</table>
<bodyText confidence="0.999432583333333">
Step r9, where the process scheduler produces process
descriptors for all the rules in 1-1(A,Icl(pn)), implies
immediate analysis of the new constituent added to the PGS.
Scheduling and Processing of Rules Activated bye-
Reduction Rules. Let consider the case when an e-
reduction rule r activates an inactive rule r&apos; such that:
red(r)-=(e14—zi...z„), a(r)=[RA (r&apos;)], red(e(M—z.k...;),
1KILS1Kp, and st(?)=inactive. When the operation activate
has checked that an e-reduction rule has to be activated then
0134—e-reduce, thus the state of computation becomes:
[e- re d u ce ,pt [NM , fr ,m , ( ) ] ,N1L] ( ) ] , and the
following statements are performed:
</bodyText>
<equation confidence="0.474043">
E-reduce: erl PIN—pop (s2);
[E-reduce,pt,[NIL„N1L,N1L],[r,m,( ) ] ,pn, [ )]
Er2 C‹—match (red(r), m);
Er3 fD4—rr,rn,C];
</equation>
<bodyText confidence="0.980566457627119">
[e -red uce,pt, [N1L,NTL ,N1L] ,[rim ,C],pn, ]
er4 V rsete C:
Er5 RSeu—rset;
[e-reduce,pt,[NIL,NIL,NIL],[r,m,[ )],pn,RSet]
er6 if t(r) then a(r)==[RA (r&apos;)];
[e-reduce,pt,ffr&apos;,NIL,(nk,...,n,)],NIL,N1L1,
[r,m,( )],pn,RSet]
Er7 Op—activate.
In this case, unlike that which the process scheduler does,
the function RA performs at step er6 the scheduling of a
process descriptor in the stack si where a subset of
ProcEnv(r) is passed as the ProcEnv(e). Therefore, when an
e-reduction ruler activates another rule r&apos; the step er2 does
the work also for r&apos;, and RA just has to identify the ProcEnv
of the activated rule inserting it in the process descriptor.
Afterwards, the operation activate checks the highest
priority stack s, is not empty, therefore it pops the process
descriptor re,NIL,Ink,...,nh)] and Opt--h-reduce that skips
the match process applying immediately the rule r&apos;:
h-reduce: hrl RSetc—C;
[h-reduce,pt,[1g1L,N1L,NlL],[r&apos;,N1L,( ] ,pn,RS et]
hr2 through hr6 as r6 through r10.
From the above descriptions it turns out that the
operation activate plays a central role for deciding what
operation must run next depending on the state of the three
stacks. The operation activate just has to check whether
some process descriptor is in the first non-empty higher
priority stack, and afterwards to set the proper operation.
The following statements describe such a work and Figure
5 depicts graphically the connections among the operations
defined in this Section.
S. EXAMPLE
It is well known that bottom-up parsers have problems
in managing rules with common right-hand sides like X --+
ABCD, X --) BCD, X —&gt; CD, X —*1), since some or all of
these rules can be fired and build unwanted nodes. A strategy
called top-down filtering in order to circumvent such a
problem has been stated, and it is adopted within bottom-up
parsers (Kay, 1982; Pratt, 1975; Slocum, 1981; Wiren,
1987) where it simulates a top-down parser together with the
bottom-up parser. The PG Processor must face this problem
as well, and the example we give is a Process Grammar
subset of rules that tries to resolve it. The kind of solution
proposed can be put in the family of top-down filters as well,
taking advantage firstly of using e-reduction rules.
Unfortunately, the means described so far are still
insufficient to solve our problem, thus the following
definitions introduce some functions that extend the Process
Grammar and the control over the PCS and the PG
Processor,
Definition 5.1. Let r be a rule of R with red(r)=(y—z1...zp),
and RSet= up) be a reduction set for red(r). Taken two
nodes rvie RSet where ri,E NN such that we have cat(n)=z,
cat(n)=z, and T(n.), T(n1) are adjacent, i.e., either j=i+1 or
j=i-1, then the function Add_Son_Rel of F.. when called in
a(r) as Add_Son_Rel (zi,z) has the effect of creating a new
parent-son relation between ii,, the parent, and nj, the son,
altering the sets sons(n,), and either 1c1(n) or rcl(n.) as
follows:
</bodyText>
<equation confidence="0.966874">
a) sons(n) sons(n.) u (ni)
h) 1c1(n) 1c109 if j=i-1
c) rcl(n,) 1t1(n) if j=i+1
</equation>
<bodyText confidence="0.999914212765958">
Such a function has the power of making an alteration in the
structure of a subtree in the PGS extending its coverage to
one of its adjacent subnees.
Definition 5.2. The function RE of F, standing for Rule
Enable, when called in the augmentations of some rule r as
RE (r&apos;), where r, r&apos; are in R, sets the state of r&apos; as active,
masking the original state set in the definition of r&apos;.
Without entering into greater detail, the function RE can
have the side effect of scheduling the just enabled rule r&apos;
whenever the call to RE follows the call Add_Son_Rel
(X,Y) for some category XE VN,YE V.uVT, and the right
corner of red(?) is X.
Definition 53. The function RD of F,, standing for Rule
Disable, when called in the augmentations of some ruler as
RD (r&apos;), where r, r&apos; are in R, sets the state of r&apos; as inactive,
masking the original state set in the definition of r&apos;.
We are now ready to put the problem as follows: given,
for instance, the following set P1 of productions:
P1=
we want to define a set of PG rules having the same coverage
of the productions in P1 with the feature of building in any
case just one node X in the PGS.
Such a set of rules is shown in Figure 6 and its aim is to create
links among the node X and the other constituents just when
the case occurs and is detected. All the possible cases are
depicted in Figure 7 in chronological order of building.
The only active rule is r0 that is fired whenever a D is inserted
in the PGS, thus a new node X is created by r0 (case (a)).
Since the next possible case is to have a node C adjacent to
the node X, the only action of r0 enables the rule rl whose
work is to find such an adjacency in the PGS by means of the
e-reduction rule red(r1)=(E— C X). If such a C exists rI is
scheduled and applied, thus the actions of r1 create a new
link between X and C (ease (b)), and the rule r2 is enabled in
preparation of the third possible case where a node B is
adjacent to the node X. The actions of rl disable rl itself
before ending their work. Because of the side effect of RE
cited above the rule r2 is always scheduled, and whenever a
node B exists then it is applied. At this point it is clear how
the mechanism works and cases (e) and (d) are handled in the
same way by the rules r2 and r3, respectively.
As the example shows, whenever the rules r1,r2,r3 are
scheduled their task is realized in two phases. The first phase
is the match process of the E-reduction rules. At this stage it
is like when a top-down parser searches lower-level
constituents for expanding the higher level constituent. If
this search succeeds the second phase is when the
</bodyText>
<figure confidence="0.98452775">
Op 4-- h-reduce.
a3 then if s3=NIL
then Op 4.— scan
a5 else Op reduce
a6 else Op 4-- e-reduce
a7 else PD 4— pop (;);
PD = [r,NIL,C]
a8
</figure>
<figureCaption confidence="0.954266">
Figure 5. Operations Transition Diagram
</figureCaption>
<figure confidence="0.695027">
activate: al if&apos; s,=NIL
a2 then ir s2=1•11L
</figure>
<page confidence="0.976579">
304
</page>
<equation confidence="0.999011666666667">
red(r0) = (X *--D)
st(r0) = active
a(r0) = [RE (r1)]
recl(r2) = (e14— B X)
st(r2) = inactive
a(r2) [Add_Son_Rel (X,B); RE (r3); RD (12)I
red(r1) = C X)
st(rI) = inactive
a(r1) = [Add_Son_Rel (X,C); RE (r2); RD (r1)]
red(r3) = (etc— A X)
st(r3) = inactive
a(r3) = [Add_Son_Rel (X,A); RD (r3)]
</equation>
<figureCaption confidence="0.999375">
Figure 6. The Process Grammar of the example
</figureCaption>
<figure confidence="0.9978704">
X
I r° riA10 rXix3/4.0
ri
A LI AAAAAA,A
(a) (b) (c) (d)
</figure>
<figureCaption confidence="0.999398">
Figure 7. All the possible cases of the example
</figureCaption>
<page confidence="0.775067">
3
</page>
<bodyText confidence="0.9949107">
appropriate links are created by means of the actions, and the
advantage of this solution is that the search process
terminates in a natural way without searching and proposing
useless relations between constituents.
We terminate this Section pointing out that this same
approach can be used in the dual case of this example, with
a set P2 of productions like:
P2 (X —&gt; A, X —&gt; An, X —› ABC, X ABCD)
The exercise of finding a corresponding set of PG rules is left
to the reader.
</bodyText>
<sectionHeader confidence="0.999867" genericHeader="method">
6. RELATED WORKS
</sectionHeader>
<bodyText confidence="0.999952678571429">
Some comparisons can be made with related works on
three main levels: the data structure PGS; the Process
Grammar; the PG Processor.
The POS can be compared with the chart (Kaplan, 1973;
Kay, 1982). The PGS embodies much of the information the
chart has. As a matter of fact, our PGS can be seen as a
denotational variant of the chart, and it is managed in a
different way by the PG Processor since in the PGS we
mainly use classical relations between the nodes of the
parse-trees: the dominance relation between a parent and a
son node, encoded in the non-terminal nodes; the left-
adjacency relation between subtrees, encoded in the
terminal nodes. Note that if we add the right-adjacency
relation to the PGS we obtain a structure fully comparable to
the chart.
The Process Grammar can embody many kinds of
information. Its structure comes from the general structure
stated for the APS G, being very close to the ATN Grammars
structure. On the other hand, our approach proposes that
grammar rules contain directives relative to the control of
the parsing process. This is a feature not in line with the
current trend of keeping separate control and linguistic
restrictions expressed in a declarative way, and it can be
found in parsing systems making use of grammars based on
situation-action rules (Winograd, 1983); furthermore, our
way of managing grammar rules, i.e., operations on the
states, activation and scheduling mechanisms, is very
similar to that realized in Marcus (1980).
</bodyText>
<sectionHeader confidence="0.996574" genericHeader="discussions">
7. DISCUSSION AND CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.993335642857143">
The PG Processor is bottom-up based, and it has to try
to take advantage from all the available sources of
information which are just the input sentence and the
grammar structure. A strong improvement in the parsing
process is determined by how the rules of a Process
Grammar are organized. Take, for instance, a grammar
where the only active rules are e-reduction rules. Within the
activation model they merely have to activate inactive rules
to be needed next, after having determined a proper context
for them. This can be extended to chains of activations at
different levels of context in a sentence, thus limiting both
calls to the matcher and nodes proliferation in the PGS. This
case can be represented writing (e,4—crel) (A4--y),
reading it as if the e-reduction in the lhs applies then activate
the rule with the reduction in the rhs, thus realizing a
mechanism that works as a contex t- sensitive reduction of the
form (ci.A13(—ct43), easily extendable also to the general case
(ci(--aiTicc...apYpc&apos;po) (A]&lt;-71).. .(A4—yd.
This is not the only reason for the presence of the s-reduction
rules in the Process Grammar. It also becomes apparent from
the example that the e-reduction rules are a powerful tool
that, extending the context-freeness of the reduction rules,
allow the realization of a wide alternative of techniques,
especially when its use is combined together with Kernel
Functions such as RA getting a powerful mean for the
control of the parsing process. From that, a parser driven by
the input - for the main scheduling - and both by the PG S and
the rules - for more complex phenomena - can be a valid
</bodyText>
<page confidence="0.997271">
305
</page>
<bodyText confidence="0.999884034482759">
framework for solving, as much as possible, classical
problems of efficiency such as minimal activation of rules,
and minimal node generation. Our description is
implementation-independent, it is responsive to
improvements and extensions, and a first advantage is that it
can be a valid approach for realizing efficient
implementations of the PG Processor.
Extending the Process Grammar. In this paper we
have described a Process Grammar where rules are
augmented with simple tests and actions. An extension of
this structure that we have not described here and that can
offer further performance to the parsing process is if we
introduce in the PG some recovery actions that are applied
whenever the detection of one of the two passible cases of
process failure happens in either the match process or the
tests. Consider, for instance, the reduction rule. Its final aim
is to find a process environment for the rule when scheduled.
This leads to say that whenever some failure conditions
happen and a process environment cannot be provided, the
recovery actions would have to manage just the control of
what to do next to undertake some recovery task. It is easy
to add such an extension to the PG, consequently modifying
properly the reduction operations of the PG processor.
Other extensions concern the set F,, by adding further
control and process management functions. Functions such
as RE and RD can be defined for changing the state of the
rules during a parsing process, thus a Process Grammar can
be partitioned in clusters of rules that can be enabled or
disabled under proper circumstances detected by &apos;low-
lever(c-reduction) rules. Finally, there can be also some
cutting functions that stop local partial parses, or even halt
the PG processor accepting or rejecting the input, e.g., when
a fatal condition has been detected making the input
unparsable, the PG processor might be halted, thus avoiding
the complete parse of the sentence and even starting a
recovery process. The reader can refer to Marino (1988) and
Marino (1989) for an informal description regarding the
implementation of such extensions.
Conclusions. We have presented a complete
framework for efficient bottom-up parsing. Efficiency is
gained by means of: a structured representation of the
parsing structure, the Parse Graph Structure, that allows
efficient matching of the reduction rules; the Process
Grammar that extends APS G by means of the process-based
conception of the grammar rules and by the presence of
Kernel Functions; the PG Processor that implements a non-
deterministic parser whose behaviour can be altered by the
Process Grammar increasing the determinism of the whole
system. The mechanism of rule activation that can be
realized in a Process Grammar is context-sensitive-based,
but this does not increase computational effort since
processes involved in the activations receive their process
environments - which are computed only once - from the
activating rules. At present we cannot tell which degree of
determinism can be got, but we infer that the partition of a
Process Grammar in clusters of rules, and the driving role the
c-reductions can have are two basic aspects whose
importance should be highlighted in the future.
</bodyText>
<sectionHeader confidence="0.999734" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.99983">
The author is thankful to Giorgio Satta who made
helpful comments and corrections on the preliminary draft
of this paper.
</bodyText>
<sectionHeader confidence="0.999907" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999880511627907">
Aho, Alfred, V. and Ullman, Jeffrey, D. (1972). The
Theory of Parsing, Translation, and Compiling. Volume 1:
Parsing. Prentice Hall, Englewood Cliffs, NI.
Aho, Alfred, V., Hoperoft, John, E. and Ullman, Jeffrey,
D. (1974). The Design and Analysis of Computer
Algorithms. Addison-Wesley.
Grishman, Ralph (1976). A Survey of Syntactic
Analysis Procedures for Natural Language. American
Journal of Computational Linguistics. Microfiche 47, pp. 2-
96.
Kaplan, Ronald, M. (1973). A General Syntactic
Processor. In Randall Rustin, ed., Natural Language
Processing, Algorithmics Press, New York, pp. 193-241.
Kay, Martin (1982). Algorithm Schemata and Data
Structures in Syntactic Processing. In Barbara J. Grosz,
Karen Sparck Jones and Bonnie Lynn Webber, eds.,
Readings in Natural Language Processing, Morgan
Kaufmann, Los Altos, pp. 35-70. Also CSL-80-12, Xerox
PARC, Palo Alto, California.
Marcus, Mitchell, P. (1980). A Theory of Syntactic
Recognition for Natural Language. MIT Press, Cambridge,
MA.
Marino, Massimo (1988). A Process-Activation Based
Parsing Algorithm for the Development of Natural
Language Grammars. Proceedings of 12th International
Conference on Computational Linguistics. Budapest,
Hungary, pp. 390-395.
Marino, Massimo (1989). A Framework for the
Development of Natural Language Gram mars. Proceedings
ofInternational Workshop on Parsing Technologies. CMU,
Pittsburgh, PA, August 28-31 1989, pp. 350-360.
Pratt, Vaughan, R. (1975). L1NGOL • A Progress
Report. Proceedings of 4th /./CA/, Tbilisi, Georgia, USSR,
pp. 422-428.
Slocum, Johnathan (1981). A Practical Comparison of
Parsing Strategies. Proceedings of 19th ACL, Stanford,
California, pp. 1-6.
Winograd, Terry (1983). Language as a Cognitive
Process. Vol. 1: Syntax, Addison-Wesley, Reading, MA.
Wirdn, Mats (1987). A Comparison of Rule-Invocation
Strategies in Context-Free Chart Parsing. Proceedings of
3rd Conference of the European Chapter of the ACL,
Copenhagen, Denmark, pp. 226-233.
</reference>
<page confidence="0.999219">
306
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.864850">
<title confidence="0.996255">BOTTOM-UP PARSING EXTENDING CONTEXT-FREENESS IN A PROCESS GRAMMAR PROCESSOR</title>
<author confidence="0.999952">Massimo Marino</author>
<affiliation confidence="0.999969">Department of Linguistics - University of Pisa</affiliation>
<address confidence="0.972173">Via S. Maria 36 1-56100 Pisa - ITALY</address>
<email confidence="0.950994">Bitnet:massimom@icnucevrri.cnuce.cnr.it</email>
<abstract confidence="0.995698142857143">approach to bottom-up parsing that extends Augmented Context-Free Grammar to a Process Grammar is formally presented. A Process Grammar (PG) defines a set of rules suited for bottom-up parsing and conceived as processes that are applied by a PG Processor. The matching phase is a crucial step for process application, and a parsing structure for efficient matching is also presented. The PG Processor is composed of a process scheduler that allows immediate constituent analysis of structures, and behaves in a non-deterministic fashion. On the other side, the PG offers means for implementing specOc parsing strategies improving the lack of determinism innate in the processor.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred Aho</author>
<author>V</author>
<author>Jeffrey Ullman</author>
<author>D</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling. Volume 1: Parsing.</booktitle>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, NI.</location>
<contexts>
<context position="1280" citStr="Aho et al., 1972" startWordPosition="179" endWordPosition="182">er that allows immediate constituent analysis of structures, and behaves in a non-deterministic fashion. On the other side, the PG offers means for implementing specOc parsing strategies improving the lack of determinism innate in the processor. I. INTRODUCTION Bottom-up parsing methods are usually preferred because of their property of being driven from both the input&apos;s syntactic/semantic structures and reduced constituents structures. Different strategies have been realized for handling the structures construction, e.g., parallel parsers, backtracking parsers, augmented contextfree parsers (Aho et al., 1972; Grishman, 1976; Winograd, 1983). The aim of this paper is to introduce a new approach to bottom-up parsing starting from a well known and based framework - parallel bottom-up parsing in immediate constituent analysis, where all possible parses are considered - making use of an Augmented Phrase-Structure Grammar (APSG). In such environment we must perform efficient searches in the graph the parser builds, and limit as much as possible the building of structures that will not be in the final parse tree. For the efficiency of the search we introduce a Parse Graph Structure, based on the definit</context>
</contexts>
<marker>Aho, V, Ullman, D, 1972</marker>
<rawString>Aho, Alfred, V. and Ullman, Jeffrey, D. (1972). The Theory of Parsing, Translation, and Compiling. Volume 1: Parsing. Prentice Hall, Englewood Cliffs, NI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred Aho</author>
<author>V Hoperoft</author>
<author>E John</author>
<author>Jeffrey Ullman</author>
<author>D</author>
</authors>
<title>The Design and Analysis of Computer Algorithms.</title>
<date>1974</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="9506" citStr="Aho et al., 1974" startWordPosition="1646" endWordPosition="1649">)-1. Property 2.1. If (NrNN,T) is a PGS, (Ac—; _xi)) is a reduction rule whose right-hand side has to be matched, and T(k)e T such that cat(k) = zp, then: a. the string; zp is matchable if p kl(k); b. for i = p,...,1, zi is partially matchable to a node Definition 2.10. If (NT,NN,T) is a PGS, an adjacency digraph can be represented as follows: a. for any ke Nr, k has outgoing arcs directed to the nodes in aria* b. for any ke NN, k has one outgoing arc directed to 1c1(k). In the classic literature the lists of the anchored nodes are called adjacency lists, and are used for representing graphs (Aho et al., 1974). A graph G=(V,E) can be usually represented by IVI adjacency lists. In our representation we can obtain an optimization representing an adjacency digraph by n adjacency lists, if n is the length of the sentence, and by EN) simple pointers for accessing the adjacency lists from the non-terminal nodes, with respect to n+INNI adjacency lists for a full representation of an adjacency digraph composed of arcs as in Defmition 2.10.a. Figure 3 shows how a new non-terminal node is connected in an adjacency digraph, and Figure 4 shows the adjacency •-&apos;&apos; ak 1c1(k) 411- - - - k access from k to Ici(k) A</context>
</contexts>
<marker>Aho, Hoperoft, John, Ullman, D, 1974</marker>
<rawString>Aho, Alfred, V., Hoperoft, John, E. and Ullman, Jeffrey, D. (1974). The Design and Analysis of Computer Algorithms. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>A Survey of Syntactic Analysis Procedures for Natural Language.</title>
<date>1976</date>
<journal>American Journal of Computational Linguistics. Microfiche</journal>
<volume>47</volume>
<pages>2--96</pages>
<contexts>
<context position="1296" citStr="Grishman, 1976" startWordPosition="183" endWordPosition="184">ediate constituent analysis of structures, and behaves in a non-deterministic fashion. On the other side, the PG offers means for implementing specOc parsing strategies improving the lack of determinism innate in the processor. I. INTRODUCTION Bottom-up parsing methods are usually preferred because of their property of being driven from both the input&apos;s syntactic/semantic structures and reduced constituents structures. Different strategies have been realized for handling the structures construction, e.g., parallel parsers, backtracking parsers, augmented contextfree parsers (Aho et al., 1972; Grishman, 1976; Winograd, 1983). The aim of this paper is to introduce a new approach to bottom-up parsing starting from a well known and based framework - parallel bottom-up parsing in immediate constituent analysis, where all possible parses are considered - making use of an Augmented Phrase-Structure Grammar (APSG). In such environment we must perform efficient searches in the graph the parser builds, and limit as much as possible the building of structures that will not be in the final parse tree. For the efficiency of the search we introduce a Parse Graph Structure, based on the definition of adjacency</context>
<context position="2763" citStr="Grishman (1976)" startWordPosition="420" endWordPosition="421"> are conceived as processes that are applied whenever proper conditions, detected by a process scheduler, exist. This is why the parser, called PG Processor, works following a nondeterministic parallel strategy, and only the Process Grammar has the power of altering and constraining this behaviour by means of some Kernel Functions that can modify the control structures of the PG Processor, thus improving determinism of the parsing process, or avoiding construction of useless mixtures. Some of the concepts introduced in this paper, such as some definitions in Section 2., are a development from Grishman (1976) that can be also an introductory reading regarding the description of a parallel bottom-up parser which is, even if under a different aspect, the core of the PG Processor. 2. PARSE GRAPH STRUCTURE The Parse Graph Structure (PGS) is built by the parser while applying grammar rules. Ifs = a, a, ... ; is an input string the initial PGS is composed by a set of terminal nodes &lt;0,5&gt;, , &lt;n+1,$&gt;, where nodes 0,n+ I represent border markers for the sentence. All the next non-terminal nodes are numbered starting from n+2. Definition 2.1. A PGS is a triple (NT,NN,T) where NT is the set of the terminal n</context>
</contexts>
<marker>Grishman, 1976</marker>
<rawString>Grishman, Ralph (1976). A Survey of Syntactic Analysis Procedures for Natural Language. American Journal of Computational Linguistics. Microfiche 47, pp. 2-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Kaplan</author>
<author>M</author>
</authors>
<title>A General Syntactic Processor.</title>
<date>1973</date>
<booktitle>Natural Language Processing,</booktitle>
<pages>193--241</pages>
<editor>In Randall Rustin, ed.,</editor>
<publisher>Algorithmics Press,</publisher>
<location>New York,</location>
<marker>Kaplan, M, 1973</marker>
<rawString>Kaplan, Ronald, M. (1973). A General Syntactic Processor. In Randall Rustin, ed., Natural Language Processing, Algorithmics Press, New York, pp. 193-241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Algorithm Schemata and Data Structures in Syntactic Processing.</title>
<date>1982</date>
<booktitle>Readings in Natural Language Processing,</booktitle>
<pages>35--70</pages>
<editor>In Barbara J. Grosz, Karen Sparck Jones and Bonnie Lynn Webber, eds.,</editor>
<publisher>Morgan Kaufmann,</publisher>
<location>Los Altos,</location>
<contexts>
<context position="23948" citStr="Kay, 1982" startWordPosition="4087" endWordPosition="4088"> is in the first non-empty higher priority stack, and afterwards to set the proper operation. The following statements describe such a work and Figure 5 depicts graphically the connections among the operations defined in this Section. S. EXAMPLE It is well known that bottom-up parsers have problems in managing rules with common right-hand sides like X --+ ABCD, X --) BCD, X —&gt; CD, X —*1), since some or all of these rules can be fired and build unwanted nodes. A strategy called top-down filtering in order to circumvent such a problem has been stated, and it is adopted within bottom-up parsers (Kay, 1982; Pratt, 1975; Slocum, 1981; Wiren, 1987) where it simulates a top-down parser together with the bottom-up parser. The PG Processor must face this problem as well, and the example we give is a Process Grammar subset of rules that tries to resolve it. The kind of solution proposed can be put in the family of top-down filters as well, taking advantage firstly of using e-reduction rules. Unfortunately, the means described so far are still insufficient to solve our problem, thus the following definitions introduce some functions that extend the Process Grammar and the control over the PCS and the </context>
<context position="29105" citStr="Kay, 1982" startWordPosition="5047" endWordPosition="5048">lution is that the search process terminates in a natural way without searching and proposing useless relations between constituents. We terminate this Section pointing out that this same approach can be used in the dual case of this example, with a set P2 of productions like: P2 (X —&gt; A, X —&gt; An, X —› ABC, X ABCD) The exercise of finding a corresponding set of PG rules is left to the reader. 6. RELATED WORKS Some comparisons can be made with related works on three main levels: the data structure PGS; the Process Grammar; the PG Processor. The POS can be compared with the chart (Kaplan, 1973; Kay, 1982). The PGS embodies much of the information the chart has. As a matter of fact, our PGS can be seen as a denotational variant of the chart, and it is managed in a different way by the PG Processor since in the PGS we mainly use classical relations between the nodes of the parse-trees: the dominance relation between a parent and a son node, encoded in the non-terminal nodes; the leftadjacency relation between subtrees, encoded in the terminal nodes. Note that if we add the right-adjacency relation to the PGS we obtain a structure fully comparable to the chart. The Process Grammar can embody many</context>
</contexts>
<marker>Kay, 1982</marker>
<rawString>Kay, Martin (1982). Algorithm Schemata and Data Structures in Syntactic Processing. In Barbara J. Grosz, Karen Sparck Jones and Bonnie Lynn Webber, eds., Readings in Natural Language Processing, Morgan Kaufmann, Los Altos, pp. 35-70. Also CSL-80-12, Xerox PARC, Palo Alto, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>P</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Marcus, P, 1980</marker>
<rawString>Marcus, Mitchell, P. (1980). A Theory of Syntactic Recognition for Natural Language. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Marino</author>
</authors>
<title>A Process-Activation Based Parsing Algorithm for the Development of Natural Language Grammars.</title>
<date>1988</date>
<booktitle>Proceedings of 12th International Conference on Computational Linguistics.</booktitle>
<pages>390--395</pages>
<location>Budapest, Hungary,</location>
<contexts>
<context position="34080" citStr="Marino (1988)" startWordPosition="5872" endWordPosition="5873">efined for changing the state of the rules during a parsing process, thus a Process Grammar can be partitioned in clusters of rules that can be enabled or disabled under proper circumstances detected by &apos;lowlever(c-reduction) rules. Finally, there can be also some cutting functions that stop local partial parses, or even halt the PG processor accepting or rejecting the input, e.g., when a fatal condition has been detected making the input unparsable, the PG processor might be halted, thus avoiding the complete parse of the sentence and even starting a recovery process. The reader can refer to Marino (1988) and Marino (1989) for an informal description regarding the implementation of such extensions. Conclusions. We have presented a complete framework for efficient bottom-up parsing. Efficiency is gained by means of: a structured representation of the parsing structure, the Parse Graph Structure, that allows efficient matching of the reduction rules; the Process Grammar that extends APS G by means of the process-based conception of the grammar rules and by the presence of Kernel Functions; the PG Processor that implements a nondeterministic parser whose behaviour can be altered by the Process Gr</context>
</contexts>
<marker>Marino, 1988</marker>
<rawString>Marino, Massimo (1988). A Process-Activation Based Parsing Algorithm for the Development of Natural Language Grammars. Proceedings of 12th International Conference on Computational Linguistics. Budapest, Hungary, pp. 390-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Marino</author>
</authors>
<title>A Framework for the Development of Natural Language Gram mars.</title>
<date>1989</date>
<booktitle>Proceedings ofInternational Workshop on Parsing Technologies. CMU,</booktitle>
<pages>350--360</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="12771" citStr="Marino (1989)" startWordPosition="2225" endWordPosition="2226">The rules are conceived as processes that the PG processor schedules somehow. Any rule defines a reduction rule that does not represent a rewriting rule, but rather a statement for search and construction of new nodes in a bottom-up way within the Parse Graph Structure. 3. The rules are augmented with some sequences of operations to be performed as in the classical APSG. In general, augmentations such as tests and actions concern manipulation of linguistic data at syntactic and/or semantic level. In this paper we are not concerned with this aspect (an 301 informal description about this is in Marino (1989)), rather we examine some aspects concerning parsing strategies by means of the augmentations. In a Process Grammar the rules can have knowledge of the existence of other rules and the purpose for which they are defined. They can call some functions that act as filters on the control structures of the parser for the scheduling of the processes, thus altering the state of the processor and forcing alternative applications. This means that any rule has the power of changing the state of the processor requiring different scheduling, and the processor is a blind operator that works following a loo</context>
<context position="15464" citStr="Marino, 1989" startWordPosition="2701" endWordPosition="2702">rategic side that should provide the necessary functions to perform operations on the processor structures. As a matter of fact, the set F can be further structured giving the PG a wider complexity and power. In this paper we cannot treat a formal extended definition for F due to space restrictions, but a brief outline can be given. The set F can be defined as F=Fx...L.,F,. In FK., are all those functions devoted to operations on the processor structures (Kernel Functions), and, in the case of a feature-based system, in F6 are all the functions devoted to the management of feature structures (Marino, 1989). In what follows we are also concerned with the combined use of e-reductions and the function RA, standing for Rule Activation, devoted to the immediate scheduling of a rule. RAeFK., and a call to it means that the specified rule must be applied, involving the scheduling process we describe in Section 4. Before we introduce the PG processor we must give a useful definition: Definition 32. Let re R be a rule with t(r)=Ife;--.;fuil, a(r)=[f1: be sequences of operations in its augmentations, f1,...,f4,fai,...,firE F. Let (ni,...,np) be a reduction set for red(r)= (M-;...zp), and he N, be the new</context>
<context position="34098" citStr="Marino (1989)" startWordPosition="5875" endWordPosition="5876">g the state of the rules during a parsing process, thus a Process Grammar can be partitioned in clusters of rules that can be enabled or disabled under proper circumstances detected by &apos;lowlever(c-reduction) rules. Finally, there can be also some cutting functions that stop local partial parses, or even halt the PG processor accepting or rejecting the input, e.g., when a fatal condition has been detected making the input unparsable, the PG processor might be halted, thus avoiding the complete parse of the sentence and even starting a recovery process. The reader can refer to Marino (1988) and Marino (1989) for an informal description regarding the implementation of such extensions. Conclusions. We have presented a complete framework for efficient bottom-up parsing. Efficiency is gained by means of: a structured representation of the parsing structure, the Parse Graph Structure, that allows efficient matching of the reduction rules; the Process Grammar that extends APS G by means of the process-based conception of the grammar rules and by the presence of Kernel Functions; the PG Processor that implements a nondeterministic parser whose behaviour can be altered by the Process Grammar increasing t</context>
</contexts>
<marker>Marino, 1989</marker>
<rawString>Marino, Massimo (1989). A Framework for the Development of Natural Language Gram mars. Proceedings ofInternational Workshop on Parsing Technologies. CMU, Pittsburgh, PA, August 28-31 1989, pp. 350-360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vaughan Pratt</author>
<author>R</author>
</authors>
<title>L1NGOL • A Progress Report.</title>
<date>1975</date>
<booktitle>Proceedings of 4th /./CA/,</booktitle>
<pages>422--428</pages>
<location>Tbilisi, Georgia, USSR,</location>
<marker>Pratt, R, 1975</marker>
<rawString>Pratt, Vaughan, R. (1975). L1NGOL • A Progress Report. Proceedings of 4th /./CA/, Tbilisi, Georgia, USSR, pp. 422-428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johnathan Slocum</author>
</authors>
<title>A Practical Comparison of Parsing Strategies.</title>
<date>1981</date>
<booktitle>Proceedings of 19th ACL,</booktitle>
<pages>1--6</pages>
<location>Stanford, California,</location>
<contexts>
<context position="23975" citStr="Slocum, 1981" startWordPosition="4091" endWordPosition="4092">ty higher priority stack, and afterwards to set the proper operation. The following statements describe such a work and Figure 5 depicts graphically the connections among the operations defined in this Section. S. EXAMPLE It is well known that bottom-up parsers have problems in managing rules with common right-hand sides like X --+ ABCD, X --) BCD, X —&gt; CD, X —*1), since some or all of these rules can be fired and build unwanted nodes. A strategy called top-down filtering in order to circumvent such a problem has been stated, and it is adopted within bottom-up parsers (Kay, 1982; Pratt, 1975; Slocum, 1981; Wiren, 1987) where it simulates a top-down parser together with the bottom-up parser. The PG Processor must face this problem as well, and the example we give is a Process Grammar subset of rules that tries to resolve it. The kind of solution proposed can be put in the family of top-down filters as well, taking advantage firstly of using e-reduction rules. Unfortunately, the means described so far are still insufficient to solve our problem, thus the following definitions introduce some functions that extend the Process Grammar and the control over the PCS and the PG Processor, Definition 5.</context>
</contexts>
<marker>Slocum, 1981</marker>
<rawString>Slocum, Johnathan (1981). A Practical Comparison of Parsing Strategies. Proceedings of 19th ACL, Stanford, California, pp. 1-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Language as a Cognitive Process.</title>
<date>1983</date>
<booktitle>Proceedings of 3rd Conference of the European Chapter of the ACL,</booktitle>
<volume>1</volume>
<pages>226--233</pages>
<publisher>Syntax, Addison-Wesley,</publisher>
<location>Reading, MA. Wirdn, Mats</location>
<contexts>
<context position="1313" citStr="Winograd, 1983" startWordPosition="185" endWordPosition="186">nt analysis of structures, and behaves in a non-deterministic fashion. On the other side, the PG offers means for implementing specOc parsing strategies improving the lack of determinism innate in the processor. I. INTRODUCTION Bottom-up parsing methods are usually preferred because of their property of being driven from both the input&apos;s syntactic/semantic structures and reduced constituents structures. Different strategies have been realized for handling the structures construction, e.g., parallel parsers, backtracking parsers, augmented contextfree parsers (Aho et al., 1972; Grishman, 1976; Winograd, 1983). The aim of this paper is to introduce a new approach to bottom-up parsing starting from a well known and based framework - parallel bottom-up parsing in immediate constituent analysis, where all possible parses are considered - making use of an Augmented Phrase-Structure Grammar (APSG). In such environment we must perform efficient searches in the graph the parser builds, and limit as much as possible the building of structures that will not be in the final parse tree. For the efficiency of the search we introduce a Parse Graph Structure, based on the definition of adjacency of the subtre-es</context>
<context position="30223" citStr="Winograd, 1983" startWordPosition="5237" endWordPosition="5238"> the PGS we obtain a structure fully comparable to the chart. The Process Grammar can embody many kinds of information. Its structure comes from the general structure stated for the APS G, being very close to the ATN Grammars structure. On the other hand, our approach proposes that grammar rules contain directives relative to the control of the parsing process. This is a feature not in line with the current trend of keeping separate control and linguistic restrictions expressed in a declarative way, and it can be found in parsing systems making use of grammars based on situation-action rules (Winograd, 1983); furthermore, our way of managing grammar rules, i.e., operations on the states, activation and scheduling mechanisms, is very similar to that realized in Marcus (1980). 7. DISCUSSION AND CONCLUSIONS The PG Processor is bottom-up based, and it has to try to take advantage from all the available sources of information which are just the input sentence and the grammar structure. A strong improvement in the parsing process is determined by how the rules of a Process Grammar are organized. Take, for instance, a grammar where the only active rules are e-reduction rules. Within the activation model</context>
</contexts>
<marker>Winograd, 1983</marker>
<rawString>Winograd, Terry (1983). Language as a Cognitive Process. Vol. 1: Syntax, Addison-Wesley, Reading, MA. Wirdn, Mats (1987). A Comparison of Rule-Invocation Strategies in Context-Free Chart Parsing. Proceedings of 3rd Conference of the European Chapter of the ACL, Copenhagen, Denmark, pp. 226-233.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>