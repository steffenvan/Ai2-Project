<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.991055">
Statistical Bistratal Dependency Parsing
</title>
<author confidence="0.998131">
Richard Johansson
</author>
<affiliation confidence="0.849387666666667">
Department of Information Engineering and Computer Science
University of Trento
Trento, Italy
</affiliation>
<email confidence="0.992931">
johansson@disi.unitn.it
</email>
<sectionHeader confidence="0.996597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999809764705882">
We present an inexact search algorithm for
the problem of predicting a two-layered
dependency graph. The algorithm is based
on a k-best version of the standard cubic-
time search algorithm for projective de-
pendency parsing, which is used as the
backbone of a beam search procedure.
This allows us to handle the complex non-
local feature dependencies occurring in
bistratal parsing if we model the interde-
pendency between the two layers.
We apply the algorithm to the syntactic–
semantic dependency parsing task of the
CoNLL-2008 Shared Task, and we obtain
a competitive result equal to the highest
published for a system that jointly learns
syntactic and semantic structure.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999934941176471">
Numerous linguistic theories assume a multistratal
model of linguistic structure, such as a layer of
surface syntax, deep syntax, and shallow seman-
tics. Examples include Meaning–Text Theory
(Mel’ˇcuk, 1988), Discontinuous Grammar (Buch-
Kromann, 2006), Extensible Dependency Gram-
mar (Debusmann et al., 2004), and the Functional
Generative Description (Sgall et al., 1986) which
forms the theoretical foundation of the Prague De-
pendency Treebank (Hajiˇc, 1998).
In the statistical NLP community, the most
widely used grammatical resource is the Penn
Treebank (Marcus et al., 1993). This is a purely
syntactic resource, but we can also include this
treebank in the category of multistratal resources
since the PropBank (Palmer et al., 2005) and
NomBank (Meyers et al., 2004) projects have an-
notated shallow semantic structures on top of it.
Dependency-converted versions of the Penn Tree-
bank, PropBank and NomBank were used in the
CoNLL-2008 Shared Task (Surdeanu et al., 2008),
in which the task of the participants was to pro-
duce a bistratal dependency structure consisting of
surface syntax and shallow semantics.
Producing a consistent multistratal structure is
a conceptually and computationally complex task,
and most previous methods have employed a
purely pipeline-based decomposition of the task.
This includes the majority of work on shallow se-
mantic analysis (Gildea and Jurafsky, 2002, in-
ter alia). Nevertheless, since it is obvious that
syntax and semantics are highly interdependent, it
has repeatedly been suggested that the problems of
syntactic and semantic analysis should be carried
out simultaneously rather than in a pipeline, and
that modeling the interdependency between syn-
tax and semantics would improve the quality of all
the substructures.
The purpose of the CoNLL-2008 Shared Task
was to study the feasibility of a joint analysis
of syntax and semantics, and while most partici-
pating systems used a pipeline-based approach to
the problem, there were a number of contribu-
tions that attempted to take the interdependence
between syntax and semantics into account. The
top-performing system in the task (Johansson and
Nugues, 2008) applied a very simple reranking
scheme by means of a k-best syntactic output,
similar to previous attempts (Gildea and Juraf-
sky, 2002; Toutanova et al., 2005) to improve se-
mantic role labeling performance by using mul-
</bodyText>
<page confidence="0.968335">
561
</page>
<note confidence="0.996618">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 561–569,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999315285714286">
tiple parses. The system by Henderson et al.
(2008) extended previous stack-based algorithms
for dependency parsing by using two separate
stacks to build the syntactic and semantic graphs.
Lluis and M`arquez (2008) proposed a model that
simultaneously predicts syntactic and semantic
links, but since its search algorithm could not take
the syntactic–semantic interdependencies into ac-
count, a pre-parsing step was still needed. In ad-
dition, before the CoNLL-2008 shared task there
have been a few attempts to jointly learn syntac-
tic and semantic structure; for instance, Merlo and
Musillo (2008) appended semantic role labels to
the phrase tags in a constituent treebank and ap-
plied a conventional constituent parser to predict
constituent structure and semantic roles.
In this paper, we propose a new approximate
search method for bistratal dependency analysis.
The search method is based on a beam search pro-
cedure that extends a k-best version of the stan-
dard cubic-time search algorithm for projective
dependency parsing. This is similar to the search
method for constituent parsing used by Huang
(2008), who referred to it as cube pruning, in-
spired by an idea from machine translation decod-
ing (Chiang, 2007). The cube pruning approach,
which is normally used to solve the arg max prob-
lem, was also recently extended to summing prob-
lems, which is needed in some learning algorithms
(Gimpel and Smith, 2009).
We apply the algorithm on the CoNLL-2008
Shared Task data, and obtain the same evalua-
tion score as the best previously published system
that simultaneously learns syntactic and semantic
structure (Titov et al., 2009).
</bodyText>
<sectionHeader confidence="0.885462" genericHeader="method">
2 Bistratal Dependency Parsing
</sectionHeader>
<bodyText confidence="0.9999583125">
In the tradition of dependency representation of
sentence structure, starting from Tesni`ere (1959),
the linguistic structure of the sentence is repre-
sented as a directed graph of relations between
words. In most theories, certain constraints are im-
posed on this graph; the most common constraint
on dependency graphs in syntax, for instance, is
that the graph should form a tree (i.e. it should be
connected, acyclic, and every node should have at
most one incoming edge). This assumption un-
derlies almost all dependency parsing, although
there are also a few parsers based on slightly more
general problem formulations (Sagae and Tsuji,
2008).
In this paper, we assume a different type of con-
straint: that the graph can be partitioned into two
subgraphs that we will refer to as strata or layers,
where the first of the layers forms a tree. For the
second layer, the only assumption we make is that
there is at most one link between any two words.
However, we believe that for any interesting lin-
guistic structure, the second layer will be highly
dependent on the structure of the first layer.
Figure 1 shows an example of a bistratal depen-
dency graph such as in the CoNLL-2008 Shared
Task on syntactic and semantic dependency pars-
ing. The figure shows the representation of the
sentence We were expecting prices to fall. The pri-
mary layer represents surface-syntactic relations,
shown above the sentence, and the secondary layer
consists of predicate–argument links (here, we
have two predicates expecting and fall).
</bodyText>
<equation confidence="0.872999666666667">
A1 A1
A0
C−A1
</equation>
<figureCaption confidence="0.980095">
Figure 1: Example of a bistratal dependency
graph.
</figureCaption>
<bodyText confidence="0.999926111111111">
We now give a formal model of the statistical
parsing problem of prediction of a bistratal depen-
dency graph. For a given input sentence x, the task
of our algorithm is to predict a structure yˆ consist-
ing of a primary layer ˆyp and a secondary layer
ˆys. In a discriminative modeling framework, we
model this prediction problem as the search for the
highest-scoring output from the candidate space Y
under a scoring function F:
</bodyText>
<equation confidence="0.994114">
(ˆyp, ˆys) = arg max F(x, yp, ys)
(yp,ys)EY
</equation>
<bodyText confidence="0.9943595">
The learning problem consists of searching in the
model space for a scoring function F that mini-
mizes the cost of predictions on unseen examples
according to a given cost function ρ. In this work,
we consider linear scoring functions of the follow-
ing form:
</bodyText>
<equation confidence="0.598562">
F(x, yp, ys) = w · Φ(x, yp, ys)
</equation>
<bodyText confidence="0.999356333333333">
where Φ(x, y) is a numeric feature representation
of the tuple (x, yp, ys) and w a high-dimensional
vector of feature weights.
</bodyText>
<equation confidence="0.9526505">
ROOT
OPRD
VC OBJ IM
SBJ
</equation>
<bodyText confidence="0.97997">
We were expecting prices to fall
</bodyText>
<page confidence="0.995169">
562
</page>
<bodyText confidence="0.99983">
Based on the structural assumptions made
above, we now decompose the feature represen-
tation into three parts:
</bodyText>
<equation confidence="0.998394">
Φ = Φp + Φi + Φs
</equation>
<bodyText confidence="0.998694">
Here, Φp represents the primary layer, assumed to
be a tree, Φs the secondary layer, and finally Φi
is the representation of the interdependency be-
tween the layers. For the feature representations
of the primary and secondary layers, we employ
edge factorization, a decomposition widely used
in statistical dependency parsing, and assume that
all edges can be scored independently:
</bodyText>
<equation confidence="0.9517705">
Φp(x, yp) = � Op(x, f)
f∈yp
</equation>
<bodyText confidence="0.9993915">
The representation of the interdependency be-
tween the layers assumes that each secondary link
is dependent on the primary layer, but independent
of other secondary links.
</bodyText>
<equation confidence="0.934449">
�Φi(x, yp,ys) = Oi(x, f, yp)
f∈ys
</equation>
<bodyText confidence="0.999935">
The interdependency between layers is the bottle-
neck for the search algorithm that we will present
in Section 3. For semantic role analysis, this in-
volves all features that rely on a syntactic repre-
sentation, most importantly the PATH feature that
represents the grammatical relation between pred-
icate and argument words. For instance, in Fig-
ure 1, we can represent the surface-syntactic re-
lation between the tokens fall and prices as the
string IMTOPRDTOBJI. In this work, all interde-
pendency features will be based on paths in the
primary layer.
</bodyText>
<sectionHeader confidence="0.950831" genericHeader="method">
3 A Bistratal Search Algorithm
</sectionHeader>
<bodyText confidence="0.995695928571429">
This section presents an algorithm to approxi-
mately solve the arg max problem for prediction
of bistratal dependency structures. We present the
algorithm in two steps: first, we review a k-best
version of the standard search algorithm for pro-
jective monostratal dependency parsing, based on
the work by Huang and Chiang (2005).1 In the
second step, starting from the k-best monostratal
search, we devise a search method for the bistratal
problem.
1Huang and Chiang (2005) described an even more effi-
cient k-best algorithm based on lazy evaluation, which we
will not use here since it is not obviously adaptable to the
situation where the search is inexact.
</bodyText>
<subsectionHeader confidence="0.999748">
3.1 Review of k-Best Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.99960592">
The search method commonly used in dependency
parsers is a chart-based dynamic programming al-
gorithm that finds the highest-scoring projective
dependency tree under an edge-factored scoring
function. It runs in cubic time with respect to the
sentence length. In a slightly more general for-
mulation, it was first published by Eisner (1996).
Starting from McDonald et al. (2005), it has been
widely used in recent statistical dependency pars-
ing frameworks.
The algorithm works by creating open struc-
tures, which consist of a dependency link and the
set of links that it spans, and closed structures,
consisting of the left or right half of a complete
subtree. An open structure is created by a proce-
dure LINK that adds a dependency link to connect
a right-pointing and a left-pointing closed struc-
ture, and a closed structure by a procedure JOIN
that joins an open structure with a closed structure.
Figure 2 shows schematic illustrations: a LINK
operation connects the right-pointing closed struc-
ture between s and j with the left-pointing closed
structure between j + 1 and e, and a JOIN oper-
ation connects an open structure between s and j
with a closed structure between j and e.
</bodyText>
<figure confidence="0.627969">
s j j+1 e s j e
</figure>
<figureCaption confidence="0.981649">
Figure 2: Illustrations of the LINK and JOIN oper-
ations.
</figureCaption>
<bodyText confidence="0.9999599375">
The search algorithm can easily be extended to
find the k best parses, not only the best one. In
k-best parsing, we maintain a k-best list in every
cell in the dynamic programming table. To create
the k-best list of derivations for an open structure
between the positions s and e, for instance, there
are up to ILI (e − s) k2 possible combinations
to consider if the set of allowed labels is L. The
key observation by Huang and Chiang (2005) is to
make use of the fact that the lists are sorted. For
every position between s and e, we add the best
combination to a priority queue, from which we
then repeatedly remove the front item. For every
item we remove, we add three successors: an item
with a next-best left part, an item with a next-best
right part, and finally an item with a next-best edge
</bodyText>
<page confidence="0.991682">
563
</page>
<bodyText confidence="0.99961685">
label.
The pseudocode of the search algorithm for
k-best dependency parsing is given in Algo-
rithms 1 and 2. For brevity, we omitted the
code for ADVANCE-LEFT and ADVANCE-RIGHT,
which are similar to ADVANCE-EDGE, as well as
ADVANCE-LOWER, which resembles ADVANCE-
UPPER. The FST function used in the pseudocode
returns the first element of a tuple.
The algorithm uses a priority queue with stan-
dard operations ENQUEUE, which enqueues an
element, and DEQUEUE, which removes the
highest-scoring item from the queue. With a stan-
dard binary heap implementation of the priority
queue, these two operations execute in logarithmic
time. To build the queue, we use a constant-time
TOSS operation, which appends an item to the
queue without enforcing the priority queue con-
straint, and a HEAPIFY operation that constructs a
consistent priority queue in linear time.
</bodyText>
<subsectionHeader confidence="0.9947435">
3.2 Extension to Bistratal Dependency
Parsing
</subsectionHeader>
<bodyText confidence="0.997380147058823">
The k-best algorithm forms the core of the inexact
bistratal search algorithm. Our method is similar
to the forest reranking method by Huang (2008),
although there is no forest pruning or reranking in-
volved here. Crucially, we divide the features into
local features, which can be computed “offline”,
and nonlocal features, which must be computed
during search. In our case, the local features are
Φp and Φ3, while the nonlocal features are the in-
terdependent features Φi.
Algorithm 3 shows pseudocode for the main
part of the bistratal search algorithm, and Algo-
rithm 4 for its support functions. The algorithm
works as follows: for every span (s, e), the algo-
rithm first uses the LINK procedure from the k-
best monostratal search to construct a k-best list of
open structures without semantic links. In the next
step, secondary links are added in the procedure
LINK-SECONDARY. For brevity, we show only
the procedures that create open structures; they are
very similar to their closed-structure counterparts.
The LINK-SECONDARY procedure starts by
creating an initial candidate (FIRST-SEC-OPEN)
based on the best open structure for the primary
layer. FIRST-SEC-OPEN creates the candidate
space for secondary links for a single primary
open structure. To reduce search complexity, it
makes use of a problem-specific function SCOPE
Algorithm 1 k-best search algorithm for depen-
dency parsing.
function k-BEST-SEARCH(k)
n ← length of the sentence
initialize the table O of open structures
initialize the table C of closed structures
</bodyText>
<equation confidence="0.954578477272728">
form ∈ [1,...,n]
for s ∈ [0,...,n − m]
LINK(s, s + m, →, k)
LINK(s, s + m, ←, k)
JOIN(s, s + m, →, k)
JOIN(s, s + m, ←, k)
return C[0, n, →]
procedure LINK(s, e, dir, k)
E ← CREATE-EDGES(s,e, dir, k)
q ← empty priority queue
for j ∈ [s, ... , e − 1]
l ← C[s, j, →]
r ← C[j + 1, e, ←]
o ← CREATE-OPEN(E,l, r, 1, 1, 1)
TOSS(q, o)
HEAPIFY(q)
while |O[s, e, dir] |&lt; k and |q |&gt; 0
o ← DEQUEUE(q)
if o ∈/ O[s, e, dir]
APPEND(O[s, e, dir], o)
ENQUEUE(q, ADVANCE-EDGE(o))
ENQUEUE(q, ADVANCE-LEFT(o))
ENQUEUE(q, ADVANCE-RIGHT(o))
procedure JOIN(s, e, dir, k)
q ← empty priority queue
if dir =→
for j ∈ [s + 1, ... ,e]
u ← O[s, j, →]
l ← C[j, e, →]
c ← CREATE-CLOSED(u, l, 1, 1)
TOSS(q, c)
else
for j ∈ [s, ... , e − 1]
u ← O[j, e, ←]
l ← C[s, j, ←]
c ← CREATE-CLOSED(u, l, 1, 1)
TOSS(q, c)
HEAPIFY(q)
while |C[s, e, dir] |&lt; k and |q |&gt; 0
c ← DEQUEUE(q)
if c ∈/ C[s, e, dir]
APPEND(C[s, e, dir], c)
ENQUEUE(q, ADVANCE-UPPER(c))
ENQUEUE(q, ADVANCE-LOWER(c))
</equation>
<bodyText confidence="0.9994704">
that defines which secondary links are possible
from a given token, given a primary-layer context.
An important insight by Huang (2008) is that
nonlocal features should be computed as early as
possible during search. In our case, we assume
that the interdependency features are based on tree
paths in the primary layer. This means that sec-
ondary links between two tokens can be added
when there is a complete path in the primary layer
between the tokens. When we create an open
</bodyText>
<page confidence="0.96477">
564
</page>
<bodyText confidence="0.537164">
Algorithm 2 Support operations for the k-best
search.
</bodyText>
<equation confidence="0.977796142857143">
function CREATE-EDGES(s,e, dir, k)
E ← ∅
for l ∈ ALLOWED-LABELS(s,e, dir)
scoreL ← W · φp(s, e, dir, l)
edge ← hscoreL, s, e, dir, li
APPEND(E, edge)
return the top k edges in E
function CREATE-OPEN(E,l, r, ie, il, ir)
scoreL ← FST(E[ie]) + FST(l[il]) + FST(r[ir])
return hscoreL + scoreN, E, l, r, ie, il, iri
function CREATE-CLOSED(u,l, iu, ir)
scoreL ← FST(u[iu]) + FST(l[il])
return hscoreL + scoreN, u,l, iu, ili
function ADVANCE-EDGE(o)
</equation>
<bodyText confidence="0.727464">
where o = (score, E,l, r, ie, il, ir)
</bodyText>
<equation confidence="0.9010371">
if ie = LENGTH(E)
return ∅
else
return CREATE-OPEN(E,l, r, ie + 1, il, ir)
function ADVANCE-UPPER(c)
where c = (u, l, iu, il)
if iu = LENGTH(u)
return ∅
else
return CREATE-CLOSED(u,l, iu + 1, il)
</equation>
<bodyText confidence="0.979185323529412">
structure by adding a link between two substruc-
tures, a complete path is created between the to-
kens in the substructures. We thus search for pos-
sible secondary links only between the two sub-
structures that are joined.
Figure 3 illustrates this process. A primary open
structure between s and e has been created by
adding a link from the right-pointing closed struc-
ture between s and j to the left-pointing closed
structure between j + 1 and e. We now try to
add secondary links between the two substruc-
tures. For instance, in the semantic role parsing
task described in subsection 3.3, if we know that
there is a predicate between s and j, then we look
for arguments between j + 1 and e, i.e. we apply
the SCOPE function to the right substructure.
When computing the scores for secondary links,
note that for efficiency only the interdependent
part Φi should be computed in CREATE-SEC-
EDGES; the part of the score that does not depend
on the primary layer can be computed before en-
tering the search procedure.
Figure 3: Illustration of the secondary linking pro-
cess: When two substructures are connected, we
can compute the path between a predicate in the
left substructure and an argument in the right sub-
structure.
Algorithm 3 Search algorithm for bistratal depen-
dency parsing.
function BISTRATAL-SEARCH(k)
n ← length of the sentence
initialize the table O of open structures
initialize the table C of closed structures
using φs, compute a table scoress for all
</bodyText>
<equation confidence="0.693945875">
possible secondary edges hh, d, li
form ∈ [1, ... ,n]
for s ∈ [0,...,n − m]
LINK(s, s + m, →, k)
LINK-SECONDARY(s,s + m, →, k)
LINK(s, s + m, ←, k)
LINK-SECONDARY(s,s + m, ←, k)
JOIN(s, s + m, →, k)
JOIN-SECONDARY(s,s + m, →, k)
JOIN(s, s + m, ←, k)
JOIN-SECONDARY(s,s + m, ←, k)
return FIRST(C[0, n, →])
procedure LINK-SECONDARY(s,e, dir, k)
q ← empty priority queue
o ← FIRST-SEC-OPEN(O[s,e, dir], 1, k)
ENQUEUE(q, o)
buf ← empty list
while |buf |&lt; k and |q |&gt; 0
o ← DEQUEUE(q)
if o ∈/buf
APPEND(buf, o)
for o′ ∈ ADVANCE-SEC-OPEN(o, k)
ENQUEUE(q,o′)
SORT(buf) to O[s, e, dir]
</equation>
<subsectionHeader confidence="0.993543">
3.3 Application on the CoNLL-2008 Shared
Task Treebank
</subsectionHeader>
<bodyText confidence="0.999941">
We applied the bistratal search method in Algo-
rithm 3 on the data from the CoNLL-2008 Shared
Task (Surdeanu et al., 2008). Here, the primary
layer is the tree of surface-syntactic relations such
as subject and object, and the secondary layer con-
tains the links between the predicate words in the
sentence and their respective logical arguments,
such as agent and patient. The training corpus con-
sists of sections 02 – 21 of the Penn Treebank, and
contains roughly 1 million words.
</bodyText>
<figure confidence="0.983599333333333">
s j j+1
p a
e
</figure>
<page confidence="0.883277">
565
</page>
<bodyText confidence="0.6298005">
Algorithm 4 Support operations in bistratal
search.
</bodyText>
<equation confidence="0.94900775">
function FIRST-SEC-OPEN(L,iL,k)
if i = LENGTH(L)
return 0
l +—GET-LEFT(L[iL]), r +—GET-RIGHT(L[iL])
for h E [START(l), ... , END(l)]
for d E SCOPE(r, h)]
E[h][d] +— CREATE-SEC-EDGES(h, d, L[iL], k)]
IE[h][d] +— 1
for h E [START(r), ... , END(r)]
for d E SCOPE(l, h)]
E[h][d] +— CREATE-SEC-EDGES(h, d, L[iL], k)]
IE[h][d] +— 1
return CREATE-SEC-OPEN(L, iL, E, I)
function CREATE-SEC-EDGES(h,d, o, k)
E +— 0
for l E ALLOWED-SEC-LABELS(h,d)
score +— w · Oi(h, d,l, o) + scoress[h, d, l]
edge +— (score, h, d, l)
APPEND(E, edge)
return the top k edges in E
function CREATE-SEC-OPEN(L,iL, E, I)
score +— FST(L[iL]) + Eh,d FST(E[h, d, IE[h, d]])
return (score, L, iL, E, IE)
function ADVANCE-SEC-OPEN(o,k)
</equation>
<bodyText confidence="0.577488">
where o = (score, L, iL, E, IE)
</bodyText>
<equation confidence="0.888113777777778">
buf +— 0
if iL &lt; LENGTH(L) and IE = [1, . . . , 1]
APPEND(buf, FIRST-SEC-OPEN(L, iL + 1, k))
for h, d
if IE[h, d] &lt; LENGTH(E[h, d])
I′E +— COPY(IE)
I′E[h, d] +— I′E[h, d] + 1
APPEND(buf, CREATE-SEC-OPEN(L, iL, E, I′E))
return buf
</equation>
<bodyText confidence="0.994551857142857">
To apply the bistratal search algorithm to
the problem of syntactic–semantic parsing, a
problem-specific implementation of the SCOPE
function is needed. In this case, we made two as-
sumptions. First, we assumed that the identities
of the predicate words are known a priori2. Sec-
ondly, we assumed that every argument of a given
predicate word is either a direct dependent of the
predicate, one of its ancestors, or a direct depen-
dent of one of its ancestors. This assumption is a
simple adaptation of the pruning algorithm by Xue
and Palmer (2004), and it holds for the vast major-
ity of arguments in the CoNLL-2008 data; in the
training set, we measured that this covers 99.04%
of the arguments of verbs and 97.55% of the argu-
2Since our algorithm needs to know the positions of the
predicates, we trained a separate classifier using the LIBLIN-
EAR toolkit (Fan et al., 2008) to identify the predicate words.
As features for the classifier, we used the words and part-of-
speech tags in a f3 window around the word under consid-
eration.
ments of nouns.
Figure 4 shows an example of how the SCOPE
function works in our case. If a predicate is con-
tained in the right substructure, we find two po-
tential arguments: one at the start of the left sub-
structure, and one more by recursively searching
the left structure.
</bodyText>
<figure confidence="0.645288">
a1 ap p
</figure>
<figureCaption confidence="0.826201">
Figure 4: Illustration of the SCOPE function for
predicate–argument links. If the right substructure
contains a predicate, we can find potential argu-
ments in the left substructure.
</figureCaption>
<bodyText confidence="0.962109142857143">
While the primary layer is assumed to be pro-
jective in Algorithm 3, the syntactic trees in the
CoNLL-2008 data have a small number of nonpro-
jective links. We used a pseudo-projective edge la-
bel encoding to handle nonprojectivity (Nivre and
Nilsson, 2005).
To implement the model, we constructed fea-
ture representations 4)p, 4)3, and 4)Z. The surface-
syntactic representation 4)p was a standard first-
order edge factorization using the same features
as McDonald et al. (2005). The features in 4)3 and
4)Z are shown in Table 1 and are standard features
in statistical semantic role classification.
4&apos;s 4&apos;i
</bodyText>
<table confidence="0.9953352">
Predicate word Path
Predicate POS Path + arg. POS
Argument word Path + pred. POS
Argument POS Path + arg. word
Pred. + arg. words Path + pred. word
Predicate word + label Path + label
Predicate POS + label Path + arg. POS + label
Argument word + label Path + pred. POS + label
Argument POS + label Path + arg. word + label
Pred. + arg. words + label Path + pred. word + label
</table>
<tableCaption confidence="0.910983">
Table 1: Feature representation for secondary
links.
</tableCaption>
<bodyText confidence="0.999479">
We trained the discriminative model using
the Online Passive–aggressive algorithm (Cram-
mer et al., 2006), which is an efficient online
learning method that can be used to train mod-
els for learning problems with structured out-
put spaces. A cost function ρ is needed in the
learning algorithm; we decomposed it into a pri-
</bodyText>
<page confidence="0.995888">
566
</page>
<bodyText confidence="0.902156">
mary part ρp and a secondary part ρs. We com-
puted the primary part as the sum of link errors:
ρp(yp, ˆyp) = El∈ˆy, cp(l, yp), where
</bodyText>
<equation confidence="0.997525666666667">
0 if l E yp and its label is correct
cp(l,yp) = 0.5 if l E yp but its label is incorrect
1 if l E/ yp
</equation>
<bodyText confidence="0.999915777777778">
In a similar vein, we computed the secondary part
ρs of the cost function as #fp + #fn + 0.5·#fl,
where #fp is the number of false positive sec-
ondary links, #fn the number of false negative
links, and #fl the number of links with correct
endpoints but incorrect label.
The training procedure took roughly 24 hours
on an 2.3 GHz AMD Athlon processor. The mem-
ory consumption was about 1 GB during training.
</bodyText>
<sectionHeader confidence="0.999231" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9992045">
We evaluated the performance of our system on
the test set from the CoNLL-2008 shared task,
which consists of section 23 of the WSJ part of
the Penn Treebank, as well as a small part of the
Brown corpus. A beam width k of 4 was used
in this experiment. Table 2 shows the results of
the evaluation. The table shows the three most
important scores computed by the official evalua-
tion script: labeled syntactic dependency accuracy
(LAS), labeled semantic dependency F1-measure
(Sem. F1), and the macro-averaged F1-measure, a
weighted combination of the syntactic and seman-
tic scores (M. F1). Our result is competitive; we
obtain the same macro F1 as the newly published
result by Titov et al. (2009), which is the high-
est published figure for a joint syntactic–semantic
parser so far. Importantly, our system clearly out-
performs the system by Llu´ıs and M`arquez (2008),
which is the most similar system in problem mod-
eling, but which uses a different search strategy.
</bodyText>
<table confidence="0.999608">
System LAS Sem. F1 M. F1
This paper 86.6 77.1 81.8
Titov et al. (2009) 87.5 76.1 81.8
H. et al (2008) 87.6 73.1 80.5
L. &amp; M. (2008) 85.8 70.3 78.1
</table>
<tableCaption confidence="0.8955735">
Table 2: Results of published joint syntactic–
semantic parsers on the CoNLL-2008 test set.
</tableCaption>
<bodyText confidence="0.999758111111111">
Since the search procedure is inexact, it is im-
portant to quantify roughly how much of a detri-
mental impact the approximation has on the pars-
ing quality. We studied the influence of the beam
width parameter k on the performance of the
parser. The results on the development set can be
seen in Table 3. As can be seen, a modest increase
in performance can be obtained by increasing the
beam width, at the cost of increased parsing time.
</bodyText>
<table confidence="0.999238">
k LAS Sem. F1 M. F1 Time
1 85.14 77.05 81.10 242
2 85.43 77.17 81.30 369
4 85.49 77.20 81.35 625
8 85.58 77.20 81.40 1178
</table>
<tableCaption confidence="0.926457">
Table 3: Influence of beam width on parsing accu-
racy.
</tableCaption>
<bodyText confidence="0.999616777777778">
In addition, to have a rough indication of the im-
pact of search errors on the quality of the parses,
we computed the fraction of sentences where the
gold-standard parse had a higher score accord-
ing to the model than the parse returned by the
search3. Table 4 shows the results of this exper-
iment. This suggests that the search errors, al-
though they clearly have an impact, are not the ma-
jor source of errors, even with small beam widths.
</bodyText>
<table confidence="0.8181968">
k Fraction
1 0.121
2 0.104
4 0.096
8 0.090
</table>
<tableCaption confidence="0.964928">
Table 4: Fraction of sentences in the development
</tableCaption>
<bodyText confidence="0.999277428571429">
set where the gold-standard parse has a higher
score than the parse returned by the search pro-
cedure.
To investigate where future optimization efforts
should be spent, we used the built-in hprof pro-
filing tool of Java to locate the bottlenecks. Once
again, we ran the program on the development
set with a beam width of 4, and Table 5 shows
the three types of operations where the algorithm
spent most of its time. It turns out that 74% of the
time was spent on the computation and scoring of
interdependency features. To make our algorithm
truly useful in practice, we thus need to devise a
way to speed up or cache these computations.
</bodyText>
<footnote confidence="0.992598">
3To be able to compare the scores of the gold-standard
and predicted parses, we disabled the automatic classifier for
predicate identification and provided the parser with gold-
standard predicates in this experiment.
</footnote>
<page confidence="0.981821">
567
</page>
<table confidence="0.86065225">
Operation Fraction
W · Φi 0.64
Queue operations 0.15
Computation of Φi 0.10
</table>
<tableCaption confidence="0.9296905">
Table 5: The three most significant bottlenecks
and their fraction of the total runtime.
</tableCaption>
<sectionHeader confidence="0.99908" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999987125000001">
In this paper, we have presented a new approxi-
mate search method to solve the problem of jointly
predicting the two layers in a bistratal dependency
graph. The algorithm shows competitive perfor-
mance on the treebank used in the CoNLL-2008
Shared Task, a bistratal treebank consisting of a
surface-syntactic and a shallow semantic layer. In
addition to the syntactic–semantic task that we
have described in this paper, we believe that our
method can be used in other types of multistratal
syntactic frameworks, such as a representation of
surface and deep syntax as in Meaning–Text The-
ory (Mel’ˇcuk, 1988).
The optimization problem that we set out to
solve is intractable, but we have shown that rea-
sonable performance can be achieved with an in-
exact, beam search-based search method. This is
not obvious: it has previously been shown that us-
ing an inexact search procedure when the learn-
ing algorithm assumes that the search is exact
may lead to slow convergence or even divergence
(Kulesza and Pereira, 2008), but this does not
seem to be a problem in our case.
While we used a beam search method as the
method of approximation, other methods are cer-
tainly possible. An interesting example is the re-
cent system by Smith and Eisner (2008), which
used loopy belief propagation in a dependency
parser using highly complex features, while still
maintaining cubic-time search complexity.
An obvious drawback of our approach com-
pared to traditional pipeline-based semantic role
labeling methods is that the speed of the algo-
rithm is highly dependent on the size of the in-
terdependency feature representation Φi. Also,
extracting these features is fairly complex, and it
is of critical importance to implement the feature
extraction procedure efficiently since it is one of
the bottlenecks of the algorithm. It is plausible
that our performance suffers from the absence of
other frequently used syntax-based features such
as dependent-of-dependent and voice.
It is thus highly dubious that a joint modeling
of syntactic and semantic structure is worth the
additional implementational effort. So far, no sys-
tem using tightly integrated syntactic and semantic
processing has been competitive with the best sys-
tems, which have been either completely pipeline-
based (Che et al., 2008; Ciaramita et al., 2008)
or employed only a loose syntactic–semantic cou-
pling (Johansson and Nugues, 2008). It has been
conjectured that modeling the semantics of the
sentence would also help in syntactic disambigua-
tion; however, it is likely that this is already im-
plicitly taken into account by the lexical features
present in virtually all modern parsers.
In addition, a problem that our beam search
method has in common with the constituent pars-
ing method by Huang (2008) is that highly non-
local features must be computed late. In our case,
this means that if there is a long distance between a
predicate and an argument, the secondary link be-
tween them will be unlikely to influence the final
search result.
</bodyText>
<sectionHeader confidence="0.994589" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999955">
The author is grateful for the helpful comments by
the reviewers. This work has been funded by the
LivingKnowledge project under the seventh EU
framework program.
</bodyText>
<sectionHeader confidence="0.997801" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999163238095238">
Matthias Buch-Kromann. 2006. Discontinuous Gram-
mar. A dependency-based model of human parsing
and language learning. Ph.D. thesis, Copenhagen
Business School.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang
Li, Bing Qin, Ting Liu, and Sheng Li. 2008. A
cascaded syntactic and semantic dependency pars-
ing system. In CoNLL 2008: Proceedings of the
Twelfth Conference on Natural Language Learning.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Massimiliano Ciaramita, Giuseppe Attardi, Felice
Dell’Orletta, and Mihai Surdeanu. 2008. DeSRL:
A linear-time semantic role labeling system. In Pro-
ceedings of the Shared Task Session of CoNLL-2008.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Schwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 2006(7):551–585.
Ralph Debusmann, Denys Duchier, Alexander Koller,
Marco Kuhlmann, Gert Smolka, and Stefan Thater.
</reference>
<page confidence="0.976386">
568
</page>
<reference confidence="0.999784601851852">
2004. A relational syntax-semantics interface based
on dependency grammar. In Proceedings of the
20th International Conference on Computational
Linguistics (COLING 2004).
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In Pro-
ceedings of the 16th International Conference on
Computational Linguistics, pages 340–345.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Kevin Gimpel and Noah A. Smith. 2009. Cube
summing, approximate inference with non-local fea-
tures, and dynamic programming without semirings.
In Proceedings of the Twelfth Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics (EACL).
Jan Hajiˇc. 1998. Building a syntactically annotated
corpus: The Prague Dependency Treebank. In Is-
sues of Valency and Meaning, pages 106–132.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of
synchronous parsing for syntactic and semantic de-
pendencies. In CoNLL 2008: Proceedings of the
Twelfth Conference on Natural Language Learning.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT 2005).
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586–594.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic–semantic analysis with
PropBank and NomBank. In Proceedings of the
Shared Task Session of CoNLL-2008.
Alex Kulesza and Fernando Pereira. 2008. Structured
learning with approximate inference. In Advances
in Neural Information Processing Systems 20.
Xavier Llu´ıs and Llu´ıs M`arquez. 2008. A joint model
for parsing syntactic and semantic dependencies. In
CoNLL 2008: Proceedings of the Twelfth Confer-
ence on Natural Language Learning.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL’05), pages 91–98.
Igor A. Mel’ˇcuk. 1988. Dependency Syntax: Theory
and Practice. State University Press of New York.
Paola Merlo and Gabriele Musillo. 2008. Semantic
parsing for high-precision semantic role labelling.
In Proceedings of the 12th Conference on Computa-
tional Natural Language Learning (CoNLL–2008).
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank project:
An interim report. In HLT-NAACL 2004 Workshop:
Frontiers in Corpus Annotation.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL’05).
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Kenji Sagae and Jun’ichi Tsuji. 2008. Shift–reduce
dependency DAG parsing. In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008).
Petr Sgall, Eva Hajiˇcov´a, and Jarmila Panevov´a. 1986.
The Meaning of the Sentence in Its Semantic and
Pragmatic Aspects. Dordrecht:Reidel Publishing
Company and Prague:Academia.
David Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Honolulu, United
States.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The
CoNLL–2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In Proceedings
of CoNLL–2008.
Lucien Tesni`ere. 1959. ´El´ements de syntaxe struc-
turale. Klincksieck, Paris.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online graph planarisation
for synchronous parsing of semantic and syntactic
dependencies. In Proceedings of the International
Joint Conferences on Artificial Intelligence.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL’05), pages 589–596.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of the 2004 Conference on Empirical Methods in
Natural Language Processing, pages 88–94.
</reference>
<page confidence="0.998584">
569
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.738685">
<title confidence="0.99978">Statistical Bistratal Dependency Parsing</title>
<author confidence="0.998249">Richard</author>
<affiliation confidence="0.999867">Department of Information Engineering and Computer University of</affiliation>
<address confidence="0.752251">Trento,</address>
<email confidence="0.999339">johansson@disi.unitn.it</email>
<abstract confidence="0.999088666666667">We present an inexact search algorithm for the problem of predicting a two-layered dependency graph. The algorithm is based a version of the standard cubictime search algorithm for projective dependency parsing, which is used as the backbone of a beam search procedure. This allows us to handle the complex nonlocal feature dependencies occurring in bistratal parsing if we model the interdependency between the two layers. We apply the algorithm to the syntactic– semantic dependency parsing task of the CoNLL-2008 Shared Task, and we obtain a competitive result equal to the highest published for a system that jointly learns syntactic and semantic structure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthias Buch-Kromann</author>
</authors>
<title>Discontinuous Grammar. A dependency-based model of human parsing and language learning.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Copenhagen Business School.</institution>
<marker>Buch-Kromann, 2006</marker>
<rawString>Matthias Buch-Kromann. 2006. Discontinuous Grammar. A dependency-based model of human parsing and language learning. Ph.D. thesis, Copenhagen Business School.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Yuxuan Hu</author>
<author>Yongqiang Li</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>A cascaded syntactic and semantic dependency parsing system.</title>
<date>2008</date>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="29029" citStr="Che et al., 2008" startWordPosition="4952" endWordPosition="4955">, and it is of critical importance to implement the feature extraction procedure efficiently since it is one of the bottlenecks of the algorithm. It is plausible that our performance suffers from the absence of other frequently used syntax-based features such as dependent-of-dependent and voice. It is thus highly dubious that a joint modeling of syntactic and semantic structure is worth the additional implementational effort. So far, no system using tightly integrated syntactic and semantic processing has been competitive with the best systems, which have been either completely pipelinebased (Che et al., 2008; Ciaramita et al., 2008) or employed only a loose syntactic–semantic coupling (Johansson and Nugues, 2008). It has been conjectured that modeling the semantics of the sentence would also help in syntactic disambiguation; however, it is likely that this is already implicitly taken into account by the lexical features present in virtually all modern parsers. In addition, a problem that our beam search method has in common with the constituent parsing method by Huang (2008) is that highly nonlocal features must be computed late. In our case, this means that if there is a long distance between a </context>
</contexts>
<marker>Che, Li, Hu, Li, Qin, Liu, Li, 2008</marker>
<rawString>Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, and Sheng Li. 2008. A cascaded syntactic and semantic dependency parsing system. In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="4613" citStr="Chiang, 2007" startWordPosition="704" endWordPosition="705">nded semantic role labels to the phrase tags in a constituent treebank and applied a conventional constituent parser to predict constituent structure and semantic roles. In this paper, we propose a new approximate search method for bistratal dependency analysis. The search method is based on a beam search procedure that extends a k-best version of the standard cubic-time search algorithm for projective dependency parsing. This is similar to the search method for constituent parsing used by Huang (2008), who referred to it as cube pruning, inspired by an idea from machine translation decoding (Chiang, 2007). The cube pruning approach, which is normally used to solve the arg max problem, was also recently extended to summing problems, which is needed in some learning algorithms (Gimpel and Smith, 2009). We apply the algorithm on the CoNLL-2008 Shared Task data, and obtain the same evaluation score as the best previously published system that simultaneously learns syntactic and semantic structure (Titov et al., 2009). 2 Bistratal Dependency Parsing In the tradition of dependency representation of sentence structure, starting from Tesni`ere (1959), the linguistic structure of the sentence is repres</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Giuseppe Attardi</author>
<author>Felice Dell’Orletta</author>
<author>Mihai Surdeanu</author>
</authors>
<title>DeSRL: A linear-time semantic role labeling system.</title>
<date>2008</date>
<booktitle>In Proceedings of the Shared Task Session of CoNLL-2008.</booktitle>
<marker>Ciaramita, Attardi, Dell’Orletta, Surdeanu, 2008</marker>
<rawString>Massimiliano Ciaramita, Giuseppe Attardi, Felice Dell’Orletta, and Mihai Surdeanu. 2008. DeSRL: A linear-time semantic role labeling system. In Proceedings of the Shared Task Session of CoNLL-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Schwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>2006</volume>
<issue>7</issue>
<contexts>
<context position="22502" citStr="Crammer et al., 2006" startWordPosition="3812" endWordPosition="3816"> Table 1 and are standard features in statistical semantic role classification. 4&apos;s 4&apos;i Predicate word Path Predicate POS Path + arg. POS Argument word Path + pred. POS Argument POS Path + arg. word Pred. + arg. words Path + pred. word Predicate word + label Path + label Predicate POS + label Path + arg. POS + label Argument word + label Path + pred. POS + label Argument POS + label Path + arg. word + label Pred. + arg. words + label Path + pred. word + label Table 1: Feature representation for secondary links. We trained the discriminative model using the Online Passive–aggressive algorithm (Crammer et al., 2006), which is an efficient online learning method that can be used to train models for learning problems with structured output spaces. A cost function ρ is needed in the learning algorithm; we decomposed it into a pri566 mary part ρp and a secondary part ρs. We computed the primary part as the sum of link errors: ρp(yp, ˆyp) = El∈ˆy, cp(l, yp), where 0 if l E yp and its label is correct cp(l,yp) = 0.5 if l E yp but its label is incorrect 1 if l E/ yp In a similar vein, we computed the secondary part ρs of the cost function as #fp + #fn + 0.5·#fl, where #fp is the number of false positive seconda</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Schwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Schwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 2006(7):551–585.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ralph Debusmann</author>
<author>Denys Duchier</author>
<author>Alexander Koller</author>
<author>Marco Kuhlmann</author>
<author>Gert Smolka</author>
<author>Stefan Thater</author>
</authors>
<marker>Debusmann, Duchier, Koller, Kuhlmann, Smolka, Thater, </marker>
<rawString>Ralph Debusmann, Denys Duchier, Alexander Koller, Marco Kuhlmann, Gert Smolka, and Stefan Thater.</rawString>
</citation>
<citation valid="true">
<title>A relational syntax-semantics interface based on dependency grammar.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="20417" citStr="(2004)" startWordPosition="3449" endWordPosition="3449">] + 1 APPEND(buf, CREATE-SEC-OPEN(L, iL, E, I′E)) return buf To apply the bistratal search algorithm to the problem of syntactic–semantic parsing, a problem-specific implementation of the SCOPE function is needed. In this case, we made two assumptions. First, we assumed that the identities of the predicate words are known a priori2. Secondly, we assumed that every argument of a given predicate word is either a direct dependent of the predicate, one of its ancestors, or a direct dependent of one of its ancestors. This assumption is a simple adaptation of the pruning algorithm by Xue and Palmer (2004), and it holds for the vast majority of arguments in the CoNLL-2008 data; in the training set, we measured that this covers 99.04% of the arguments of verbs and 97.55% of the argu2Since our algorithm needs to know the positions of the predicates, we trained a separate classifier using the LIBLINEAR toolkit (Fan et al., 2008) to identify the predicate words. As features for the classifier, we used the words and part-ofspeech tags in a f3 window around the word under consideration. ments of nouns. Figure 4 shows an example of how the SCOPE function works in our case. If a predicate is contained </context>
</contexts>
<marker>2004</marker>
<rawString>2004. A relational syntax-semantics interface based on dependency grammar. In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>340--345</pages>
<contexts>
<context position="9952" citStr="Eisner (1996)" startWordPosition="1595" endWordPosition="1596">r the bistratal problem. 1Huang and Chiang (2005) described an even more efficient k-best algorithm based on lazy evaluation, which we will not use here since it is not obviously adaptable to the situation where the search is inexact. 3.1 Review of k-Best Dependency Parsing The search method commonly used in dependency parsers is a chart-based dynamic programming algorithm that finds the highest-scoring projective dependency tree under an edge-factored scoring function. It runs in cubic time with respect to the sentence length. In a slightly more general formulation, it was first published by Eisner (1996). Starting from McDonald et al. (2005), it has been widely used in recent statistical dependency parsing frameworks. The algorithm works by creating open structures, which consist of a dependency link and the set of links that it spans, and closed structures, consisting of the left or right half of a complete subtree. An open structure is created by a procedure LINK that adds a dependency link to connect a right-pointing and a left-pointing closed structure, and a closed structure by a procedure JOIN that joins an open structure with a closed structure. Figure 2 shows schematic illustrations: </context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th International Conference on Computational Linguistics, pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="20743" citStr="Fan et al., 2008" startWordPosition="3505" endWordPosition="3508">re known a priori2. Secondly, we assumed that every argument of a given predicate word is either a direct dependent of the predicate, one of its ancestors, or a direct dependent of one of its ancestors. This assumption is a simple adaptation of the pruning algorithm by Xue and Palmer (2004), and it holds for the vast majority of arguments in the CoNLL-2008 data; in the training set, we measured that this covers 99.04% of the arguments of verbs and 97.55% of the argu2Since our algorithm needs to know the positions of the predicates, we trained a separate classifier using the LIBLINEAR toolkit (Fan et al., 2008) to identify the predicate words. As features for the classifier, we used the words and part-ofspeech tags in a f3 window around the word under consideration. ments of nouns. Figure 4 shows an example of how the SCOPE function works in our case. If a predicate is contained in the right substructure, we find two potential arguments: one at the start of the left substructure, and one more by recursively searching the left structure. a1 ap p Figure 4: Illustration of the SCOPE function for predicate–argument links. If the right substructure contains a predicate, we can find potential arguments in</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="2265" citStr="Gildea and Jurafsky, 2002" startWordPosition="335" endWordPosition="338">04) projects have annotated shallow semantic structures on top of it. Dependency-converted versions of the Penn Treebank, PropBank and NomBank were used in the CoNLL-2008 Shared Task (Surdeanu et al., 2008), in which the task of the participants was to produce a bistratal dependency structure consisting of surface syntax and shallow semantics. Producing a consistent multistratal structure is a conceptually and computationally complex task, and most previous methods have employed a purely pipeline-based decomposition of the task. This includes the majority of work on shallow semantic analysis (Gildea and Jurafsky, 2002, inter alia). Nevertheless, since it is obvious that syntax and semantics are highly interdependent, it has repeatedly been suggested that the problems of syntactic and semantic analysis should be carried out simultaneously rather than in a pipeline, and that modeling the interdependency between syntax and semantics would improve the quality of all the substructures. The purpose of the CoNLL-2008 Shared Task was to study the feasibility of a joint analysis of syntax and semantics, and while most participating systems used a pipeline-based approach to the problem, there were a number of contri</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Cube summing, approximate inference with non-local features, and dynamic programming without semirings.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twelfth Conference of the European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="4811" citStr="Gimpel and Smith, 2009" startWordPosition="736" endWordPosition="739">propose a new approximate search method for bistratal dependency analysis. The search method is based on a beam search procedure that extends a k-best version of the standard cubic-time search algorithm for projective dependency parsing. This is similar to the search method for constituent parsing used by Huang (2008), who referred to it as cube pruning, inspired by an idea from machine translation decoding (Chiang, 2007). The cube pruning approach, which is normally used to solve the arg max problem, was also recently extended to summing problems, which is needed in some learning algorithms (Gimpel and Smith, 2009). We apply the algorithm on the CoNLL-2008 Shared Task data, and obtain the same evaluation score as the best previously published system that simultaneously learns syntactic and semantic structure (Titov et al., 2009). 2 Bistratal Dependency Parsing In the tradition of dependency representation of sentence structure, starting from Tesni`ere (1959), the linguistic structure of the sentence is represented as a directed graph of relations between words. In most theories, certain constraints are imposed on this graph; the most common constraint on dependency graphs in syntax, for instance, is tha</context>
</contexts>
<marker>Gimpel, Smith, 2009</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2009. Cube summing, approximate inference with non-local features, and dynamic programming without semirings. In Proceedings of the Twelfth Conference of the European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Building a syntactically annotated corpus: The Prague Dependency Treebank.</title>
<date>1998</date>
<booktitle>In Issues of Valency and Meaning,</booktitle>
<pages>106--132</pages>
<marker>Hajiˇc, 1998</marker>
<rawString>Jan Hajiˇc. 1998. Building a syntactically annotated corpus: The Prague Dependency Treebank. In Issues of Valency and Meaning, pages 106–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
<author>Ivan Titov</author>
</authors>
<title>A latent variable model of synchronous parsing for syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="3450" citStr="Henderson et al. (2008)" startWordPosition="521" endWordPosition="524">problem, there were a number of contributions that attempted to take the interdependence between syntax and semantics into account. The top-performing system in the task (Johansson and Nugues, 2008) applied a very simple reranking scheme by means of a k-best syntactic output, similar to previous attempts (Gildea and Jurafsky, 2002; Toutanova et al., 2005) to improve semantic role labeling performance by using mul561 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 561–569, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tiple parses. The system by Henderson et al. (2008) extended previous stack-based algorithms for dependency parsing by using two separate stacks to build the syntactic and semantic graphs. Lluis and M`arquez (2008) proposed a model that simultaneously predicts syntactic and semantic links, but since its search algorithm could not take the syntactic–semantic interdependencies into account, a pre-parsing step was still needed. In addition, before the CoNLL-2008 shared task there have been a few attempts to jointly learn syntactic and semantic structure; for instance, Merlo and Musillo (2008) appended semantic role labels to the phrase tags in a </context>
</contexts>
<marker>Henderson, Merlo, Musillo, Titov, 2008</marker>
<rawString>James Henderson, Paola Merlo, Gabriele Musillo, and Ivan Titov. 2008. A latent variable model of synchronous parsing for syntactic and semantic dependencies. In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technologies (IWPT</booktitle>
<contexts>
<context position="9243" citStr="Huang and Chiang (2005)" startWordPosition="1480" endWordPosition="1483">n between predicate and argument words. For instance, in Figure 1, we can represent the surface-syntactic relation between the tokens fall and prices as the string IMTOPRDTOBJI. In this work, all interdependency features will be based on paths in the primary layer. 3 A Bistratal Search Algorithm This section presents an algorithm to approximately solve the arg max problem for prediction of bistratal dependency structures. We present the algorithm in two steps: first, we review a k-best version of the standard search algorithm for projective monostratal dependency parsing, based on the work by Huang and Chiang (2005).1 In the second step, starting from the k-best monostratal search, we devise a search method for the bistratal problem. 1Huang and Chiang (2005) described an even more efficient k-best algorithm based on lazy evaluation, which we will not use here since it is not obviously adaptable to the situation where the search is inexact. 3.1 Review of k-Best Dependency Parsing The search method commonly used in dependency parsers is a chart-based dynamic programming algorithm that finds the highest-scoring projective dependency tree under an edge-factored scoring function. It runs in cubic time with re</context>
<context position="11310" citStr="Huang and Chiang (2005)" startWordPosition="1838" endWordPosition="1841"> e, and a JOIN operation connects an open structure between s and j with a closed structure between j and e. s j j+1 e s j e Figure 2: Illustrations of the LINK and JOIN operations. The search algorithm can easily be extended to find the k best parses, not only the best one. In k-best parsing, we maintain a k-best list in every cell in the dynamic programming table. To create the k-best list of derivations for an open structure between the positions s and e, for instance, there are up to ILI (e − s) k2 possible combinations to consider if the set of allowed labels is L. The key observation by Huang and Chiang (2005) is to make use of the fact that the lists are sorted. For every position between s and e, we add the best combination to a priority queue, from which we then repeatedly remove the front item. For every item we remove, we add three successors: an item with a next-best left part, an item with a next-best right part, and finally an item with a next-best edge 563 label. The pseudocode of the search algorithm for k-best dependency parsing is given in Algorithms 1 and 2. For brevity, we omitted the code for ADVANCE-LEFT and ADVANCE-RIGHT, which are similar to ADVANCE-EDGE, as well as ADVANCE-LOWER,</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the 9th International Workshop on Parsing Technologies (IWPT 2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>586--594</pages>
<contexts>
<context position="4507" citStr="Huang (2008)" startWordPosition="685" endWordPosition="686">w attempts to jointly learn syntactic and semantic structure; for instance, Merlo and Musillo (2008) appended semantic role labels to the phrase tags in a constituent treebank and applied a conventional constituent parser to predict constituent structure and semantic roles. In this paper, we propose a new approximate search method for bistratal dependency analysis. The search method is based on a beam search procedure that extends a k-best version of the standard cubic-time search algorithm for projective dependency parsing. This is similar to the search method for constituent parsing used by Huang (2008), who referred to it as cube pruning, inspired by an idea from machine translation decoding (Chiang, 2007). The cube pruning approach, which is normally used to solve the arg max problem, was also recently extended to summing problems, which is needed in some learning algorithms (Gimpel and Smith, 2009). We apply the algorithm on the CoNLL-2008 Shared Task data, and obtain the same evaluation score as the best previously published system that simultaneously learns syntactic and semantic structure (Titov et al., 2009). 2 Bistratal Dependency Parsing In the tradition of dependency representation</context>
<context position="12720" citStr="Huang (2008)" startWordPosition="2075" endWordPosition="2076">lement, and DEQUEUE, which removes the highest-scoring item from the queue. With a standard binary heap implementation of the priority queue, these two operations execute in logarithmic time. To build the queue, we use a constant-time TOSS operation, which appends an item to the queue without enforcing the priority queue constraint, and a HEAPIFY operation that constructs a consistent priority queue in linear time. 3.2 Extension to Bistratal Dependency Parsing The k-best algorithm forms the core of the inexact bistratal search algorithm. Our method is similar to the forest reranking method by Huang (2008), although there is no forest pruning or reranking involved here. Crucially, we divide the features into local features, which can be computed “offline”, and nonlocal features, which must be computed during search. In our case, the local features are Φp and Φ3, while the nonlocal features are the interdependent features Φi. Algorithm 3 shows pseudocode for the main part of the bistratal search algorithm, and Algorithm 4 for its support functions. The algorithm works as follows: for every span (s, e), the algorithm first uses the LINK procedure from the kbest monostratal search to construct a k</context>
<context position="15185" citStr="Huang (2008)" startWordPosition="2520" endWordPosition="2521">E(q, ADVANCE-LEFT(o)) ENQUEUE(q, ADVANCE-RIGHT(o)) procedure JOIN(s, e, dir, k) q ← empty priority queue if dir =→ for j ∈ [s + 1, ... ,e] u ← O[s, j, →] l ← C[j, e, →] c ← CREATE-CLOSED(u, l, 1, 1) TOSS(q, c) else for j ∈ [s, ... , e − 1] u ← O[j, e, ←] l ← C[s, j, ←] c ← CREATE-CLOSED(u, l, 1, 1) TOSS(q, c) HEAPIFY(q) while |C[s, e, dir] |&lt; k and |q |&gt; 0 c ← DEQUEUE(q) if c ∈/ C[s, e, dir] APPEND(C[s, e, dir], c) ENQUEUE(q, ADVANCE-UPPER(c)) ENQUEUE(q, ADVANCE-LOWER(c)) that defines which secondary links are possible from a given token, given a primary-layer context. An important insight by Huang (2008) is that nonlocal features should be computed as early as possible during search. In our case, we assume that the interdependency features are based on tree paths in the primary layer. This means that secondary links between two tokens can be added when there is a complete path in the primary layer between the tokens. When we create an open 564 Algorithm 2 Support operations for the k-best search. function CREATE-EDGES(s,e, dir, k) E ← ∅ for l ∈ ALLOWED-LABELS(s,e, dir) scoreL ← W · φp(s, e, dir, l) edge ← hscoreL, s, e, dir, li APPEND(E, edge) return the top k edges in E function CREATE-OPEN(</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL-08: HLT, pages 586–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based syntactic–semantic analysis with PropBank and NomBank.</title>
<date>2008</date>
<booktitle>In Proceedings of the Shared Task Session of CoNLL-2008.</booktitle>
<contexts>
<context position="3025" citStr="Johansson and Nugues, 2008" startWordPosition="453" endWordPosition="456">d that the problems of syntactic and semantic analysis should be carried out simultaneously rather than in a pipeline, and that modeling the interdependency between syntax and semantics would improve the quality of all the substructures. The purpose of the CoNLL-2008 Shared Task was to study the feasibility of a joint analysis of syntax and semantics, and while most participating systems used a pipeline-based approach to the problem, there were a number of contributions that attempted to take the interdependence between syntax and semantics into account. The top-performing system in the task (Johansson and Nugues, 2008) applied a very simple reranking scheme by means of a k-best syntactic output, similar to previous attempts (Gildea and Jurafsky, 2002; Toutanova et al., 2005) to improve semantic role labeling performance by using mul561 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 561–569, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tiple parses. The system by Henderson et al. (2008) extended previous stack-based algorithms for dependency parsing by using two separate stacks to build the syntactic and semantic graphs. Lluis and M`arquez (2008) proposed a </context>
<context position="29136" citStr="Johansson and Nugues, 2008" startWordPosition="4968" endWordPosition="4971">ce it is one of the bottlenecks of the algorithm. It is plausible that our performance suffers from the absence of other frequently used syntax-based features such as dependent-of-dependent and voice. It is thus highly dubious that a joint modeling of syntactic and semantic structure is worth the additional implementational effort. So far, no system using tightly integrated syntactic and semantic processing has been competitive with the best systems, which have been either completely pipelinebased (Che et al., 2008; Ciaramita et al., 2008) or employed only a loose syntactic–semantic coupling (Johansson and Nugues, 2008). It has been conjectured that modeling the semantics of the sentence would also help in syntactic disambiguation; however, it is likely that this is already implicitly taken into account by the lexical features present in virtually all modern parsers. In addition, a problem that our beam search method has in common with the constituent parsing method by Huang (2008) is that highly nonlocal features must be computed late. In our case, this means that if there is a long distance between a predicate and an argument, the secondary link between them will be unlikely to influence the final search r</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic–semantic analysis with PropBank and NomBank. In Proceedings of the Shared Task Session of CoNLL-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Kulesza</author>
<author>Fernando Pereira</author>
</authors>
<title>Structured learning with approximate inference.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 20.</booktitle>
<contexts>
<context position="27766" citStr="Kulesza and Pereira, 2008" startWordPosition="4752" endWordPosition="4755">t we have described in this paper, we believe that our method can be used in other types of multistratal syntactic frameworks, such as a representation of surface and deep syntax as in Meaning–Text Theory (Mel’ˇcuk, 1988). The optimization problem that we set out to solve is intractable, but we have shown that reasonable performance can be achieved with an inexact, beam search-based search method. This is not obvious: it has previously been shown that using an inexact search procedure when the learning algorithm assumes that the search is exact may lead to slow convergence or even divergence (Kulesza and Pereira, 2008), but this does not seem to be a problem in our case. While we used a beam search method as the method of approximation, other methods are certainly possible. An interesting example is the recent system by Smith and Eisner (2008), which used loopy belief propagation in a dependency parser using highly complex features, while still maintaining cubic-time search complexity. An obvious drawback of our approach compared to traditional pipeline-based semantic role labeling methods is that the speed of the algorithm is highly dependent on the size of the interdependency feature representation Φi. Al</context>
</contexts>
<marker>Kulesza, Pereira, 2008</marker>
<rawString>Alex Kulesza and Fernando Pereira. 2008. Structured learning with approximate inference. In Advances in Neural Information Processing Systems 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Llu´ıs</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>A joint model for parsing syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning.</booktitle>
<marker>Llu´ıs, M`arquez, 2008</marker>
<rawString>Xavier Llu´ıs and Llu´ıs M`arquez. 2008. A joint model for parsing syntactic and semantic dependencies. In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1450" citStr="Marcus et al., 1993" startWordPosition="209" endWordPosition="212">tic and semantic structure. 1 Introduction Numerous linguistic theories assume a multistratal model of linguistic structure, such as a layer of surface syntax, deep syntax, and shallow semantics. Examples include Meaning–Text Theory (Mel’ˇcuk, 1988), Discontinuous Grammar (BuchKromann, 2006), Extensible Dependency Grammar (Debusmann et al., 2004), and the Functional Generative Description (Sgall et al., 1986) which forms the theoretical foundation of the Prague Dependency Treebank (Hajiˇc, 1998). In the statistical NLP community, the most widely used grammatical resource is the Penn Treebank (Marcus et al., 1993). This is a purely syntactic resource, but we can also include this treebank in the category of multistratal resources since the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) projects have annotated shallow semantic structures on top of it. Dependency-converted versions of the Penn Treebank, PropBank and NomBank were used in the CoNLL-2008 Shared Task (Surdeanu et al., 2008), in which the task of the participants was to produce a bistratal dependency structure consisting of surface syntax and shallow semantics. Producing a consistent multistratal structure is a conceptually </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>91--98</pages>
<contexts>
<context position="9990" citStr="McDonald et al. (2005)" startWordPosition="1599" endWordPosition="1602">ng and Chiang (2005) described an even more efficient k-best algorithm based on lazy evaluation, which we will not use here since it is not obviously adaptable to the situation where the search is inexact. 3.1 Review of k-Best Dependency Parsing The search method commonly used in dependency parsers is a chart-based dynamic programming algorithm that finds the highest-scoring projective dependency tree under an edge-factored scoring function. It runs in cubic time with respect to the sentence length. In a slightly more general formulation, it was first published by Eisner (1996). Starting from McDonald et al. (2005), it has been widely used in recent statistical dependency parsing frameworks. The algorithm works by creating open structures, which consist of a dependency link and the set of links that it spans, and closed structures, consisting of the left or right half of a complete subtree. An open structure is created by a procedure LINK that adds a dependency link to connect a right-pointing and a left-pointing closed structure, and a closed structure by a procedure JOIN that joins an open structure with a closed structure. Figure 2 shows schematic illustrations: a LINK operation connects the right-po</context>
<context position="21839" citStr="McDonald et al. (2005)" startWordPosition="3691" endWordPosition="3694">COPE function for predicate–argument links. If the right substructure contains a predicate, we can find potential arguments in the left substructure. While the primary layer is assumed to be projective in Algorithm 3, the syntactic trees in the CoNLL-2008 data have a small number of nonprojective links. We used a pseudo-projective edge label encoding to handle nonprojectivity (Nivre and Nilsson, 2005). To implement the model, we constructed feature representations 4)p, 4)3, and 4)Z. The surfacesyntactic representation 4)p was a standard firstorder edge factorization using the same features as McDonald et al. (2005). The features in 4)3 and 4)Z are shown in Table 1 and are standard features in statistical semantic role classification. 4&apos;s 4&apos;i Predicate word Path Predicate POS Path + arg. POS Argument word Path + pred. POS Argument POS Path + arg. word Pred. + arg. words Path + pred. word Predicate word + label Path + label Predicate POS + label Path + arg. POS + label Argument word + label Path + pred. POS + label Argument POS + label Path + arg. word + label Pred. + arg. words + label Path + pred. word + label Table 1: Feature representation for secondary links. We trained the discriminative model using</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University Press of</publisher>
<location>New York.</location>
<marker>Mel’ˇcuk, 1988</marker>
<rawString>Igor A. Mel’ˇcuk. 1988. Dependency Syntax: Theory and Practice. State University Press of New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
</authors>
<title>Semantic parsing for high-precision semantic role labelling.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL–2008).</booktitle>
<contexts>
<context position="3995" citStr="Merlo and Musillo (2008)" startWordPosition="602" endWordPosition="605"> 2009. c�2009 ACL and AFNLP tiple parses. The system by Henderson et al. (2008) extended previous stack-based algorithms for dependency parsing by using two separate stacks to build the syntactic and semantic graphs. Lluis and M`arquez (2008) proposed a model that simultaneously predicts syntactic and semantic links, but since its search algorithm could not take the syntactic–semantic interdependencies into account, a pre-parsing step was still needed. In addition, before the CoNLL-2008 shared task there have been a few attempts to jointly learn syntactic and semantic structure; for instance, Merlo and Musillo (2008) appended semantic role labels to the phrase tags in a constituent treebank and applied a conventional constituent parser to predict constituent structure and semantic roles. In this paper, we propose a new approximate search method for bistratal dependency analysis. The search method is based on a beam search procedure that extends a k-best version of the standard cubic-time search algorithm for projective dependency parsing. This is similar to the search method for constituent parsing used by Huang (2008), who referred to it as cube pruning, inspired by an idea from machine translation decod</context>
</contexts>
<marker>Merlo, Musillo, 2008</marker>
<rawString>Paola Merlo and Gabriele Musillo. 2008. Semantic parsing for high-precision semantic role labelling. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL–2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>The NomBank project: An interim report.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation.</booktitle>
<contexts>
<context position="1643" citStr="Meyers et al., 2004" startWordPosition="241" endWordPosition="244">s. Examples include Meaning–Text Theory (Mel’ˇcuk, 1988), Discontinuous Grammar (BuchKromann, 2006), Extensible Dependency Grammar (Debusmann et al., 2004), and the Functional Generative Description (Sgall et al., 1986) which forms the theoretical foundation of the Prague Dependency Treebank (Hajiˇc, 1998). In the statistical NLP community, the most widely used grammatical resource is the Penn Treebank (Marcus et al., 1993). This is a purely syntactic resource, but we can also include this treebank in the category of multistratal resources since the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) projects have annotated shallow semantic structures on top of it. Dependency-converted versions of the Penn Treebank, PropBank and NomBank were used in the CoNLL-2008 Shared Task (Surdeanu et al., 2008), in which the task of the participants was to produce a bistratal dependency structure consisting of surface syntax and shallow semantics. Producing a consistent multistratal structure is a conceptually and computationally complex task, and most previous methods have employed a purely pipeline-based decomposition of the task. This includes the majority of work on shallow semantic analysis (Gil</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The NomBank project: An interim report. In HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudoprojective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05).</booktitle>
<contexts>
<context position="21621" citStr="Nivre and Nilsson, 2005" startWordPosition="3657" endWordPosition="3660">dicate is contained in the right substructure, we find two potential arguments: one at the start of the left substructure, and one more by recursively searching the left structure. a1 ap p Figure 4: Illustration of the SCOPE function for predicate–argument links. If the right substructure contains a predicate, we can find potential arguments in the left substructure. While the primary layer is assumed to be projective in Algorithm 3, the syntactic trees in the CoNLL-2008 data have a small number of nonprojective links. We used a pseudo-projective edge label encoding to handle nonprojectivity (Nivre and Nilsson, 2005). To implement the model, we constructed feature representations 4)p, 4)3, and 4)Z. The surfacesyntactic representation 4)p was a standard firstorder edge factorization using the same features as McDonald et al. (2005). The features in 4)3 and 4)Z are shown in Table 1 and are standard features in statistical semantic role classification. 4&apos;s 4&apos;i Predicate word Path Predicate POS Path + arg. POS Argument word Path + pred. POS Argument POS Path + arg. word Pred. + arg. words Path + pred. word Predicate word + label Path + label Predicate POS + label Path + arg. POS + label Argument word + label </context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudoprojective dependency parsing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1609" citStr="Palmer et al., 2005" startWordPosition="235" endWordPosition="238"> deep syntax, and shallow semantics. Examples include Meaning–Text Theory (Mel’ˇcuk, 1988), Discontinuous Grammar (BuchKromann, 2006), Extensible Dependency Grammar (Debusmann et al., 2004), and the Functional Generative Description (Sgall et al., 1986) which forms the theoretical foundation of the Prague Dependency Treebank (Hajiˇc, 1998). In the statistical NLP community, the most widely used grammatical resource is the Penn Treebank (Marcus et al., 1993). This is a purely syntactic resource, but we can also include this treebank in the category of multistratal resources since the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) projects have annotated shallow semantic structures on top of it. Dependency-converted versions of the Penn Treebank, PropBank and NomBank were used in the CoNLL-2008 Shared Task (Surdeanu et al., 2008), in which the task of the participants was to produce a bistratal dependency structure consisting of surface syntax and shallow semantics. Producing a consistent multistratal structure is a conceptually and computationally complex task, and most previous methods have employed a purely pipeline-based decomposition of the task. This includes the majority of work</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsuji</author>
</authors>
<title>Shift–reduce dependency DAG parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<contexts>
<context position="5706" citStr="Sagae and Tsuji, 2008" startWordPosition="876" endWordPosition="879">pendency representation of sentence structure, starting from Tesni`ere (1959), the linguistic structure of the sentence is represented as a directed graph of relations between words. In most theories, certain constraints are imposed on this graph; the most common constraint on dependency graphs in syntax, for instance, is that the graph should form a tree (i.e. it should be connected, acyclic, and every node should have at most one incoming edge). This assumption underlies almost all dependency parsing, although there are also a few parsers based on slightly more general problem formulations (Sagae and Tsuji, 2008). In this paper, we assume a different type of constraint: that the graph can be partitioned into two subgraphs that we will refer to as strata or layers, where the first of the layers forms a tree. For the second layer, the only assumption we make is that there is at most one link between any two words. However, we believe that for any interesting linguistic structure, the second layer will be highly dependent on the structure of the first layer. Figure 1 shows an example of a bistratal dependency graph such as in the CoNLL-2008 Shared Task on syntactic and semantic dependency parsing. The fi</context>
</contexts>
<marker>Sagae, Tsuji, 2008</marker>
<rawString>Kenji Sagae and Jun’ichi Tsuji. 2008. Shift–reduce dependency DAG parsing. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajiˇcov´a</author>
<author>Jarmila Panevov´a</author>
</authors>
<title>The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. Dordrecht:Reidel Publishing Company and Prague:Academia.</title>
<date>1986</date>
<marker>Sgall, Hajiˇcov´a, Panevov´a, 1986</marker>
<rawString>Petr Sgall, Eva Hajiˇcov´a, and Jarmila Panevov´a. 1986. The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. Dordrecht:Reidel Publishing Company and Prague:Academia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Honolulu, United States.</location>
<contexts>
<context position="27995" citStr="Smith and Eisner (2008)" startWordPosition="4795" endWordPosition="4798">ization problem that we set out to solve is intractable, but we have shown that reasonable performance can be achieved with an inexact, beam search-based search method. This is not obvious: it has previously been shown that using an inexact search procedure when the learning algorithm assumes that the search is exact may lead to slow convergence or even divergence (Kulesza and Pereira, 2008), but this does not seem to be a problem in our case. While we used a beam search method as the method of approximation, other methods are certainly possible. An interesting example is the recent system by Smith and Eisner (2008), which used loopy belief propagation in a dependency parser using highly complex features, while still maintaining cubic-time search complexity. An obvious drawback of our approach compared to traditional pipeline-based semantic role labeling methods is that the speed of the algorithm is highly dependent on the size of the interdependency feature representation Φi. Also, extracting these features is fairly complex, and it is of critical importance to implement the feature extraction procedure efficiently since it is one of the bottlenecks of the algorithm. It is plausible that our performance</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>David Smith and Jason Eisner. 2008. Dependency parsing by belief propagation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Honolulu, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL–2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL–2008.</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL–2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of CoNLL–2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesni`ere</author>
</authors>
<title>El´ements de syntaxe structurale.</title>
<date>1959</date>
<location>Klincksieck, Paris.</location>
<marker>Tesni`ere, 1959</marker>
<rawString>Lucien Tesni`ere. 1959. ´El´ements de syntaxe structurale. Klincksieck, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
</authors>
<title>Online graph planarisation for synchronous parsing of semantic and syntactic dependencies.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Joint Conferences on Artificial Intelligence.</booktitle>
<contexts>
<context position="5029" citStr="Titov et al., 2009" startWordPosition="770" endWordPosition="773">ndency parsing. This is similar to the search method for constituent parsing used by Huang (2008), who referred to it as cube pruning, inspired by an idea from machine translation decoding (Chiang, 2007). The cube pruning approach, which is normally used to solve the arg max problem, was also recently extended to summing problems, which is needed in some learning algorithms (Gimpel and Smith, 2009). We apply the algorithm on the CoNLL-2008 Shared Task data, and obtain the same evaluation score as the best previously published system that simultaneously learns syntactic and semantic structure (Titov et al., 2009). 2 Bistratal Dependency Parsing In the tradition of dependency representation of sentence structure, starting from Tesni`ere (1959), the linguistic structure of the sentence is represented as a directed graph of relations between words. In most theories, certain constraints are imposed on this graph; the most common constraint on dependency graphs in syntax, for instance, is that the graph should form a tree (i.e. it should be connected, acyclic, and every node should have at most one incoming edge). This assumption underlies almost all dependency parsing, although there are also a few parser</context>
<context position="24070" citStr="Titov et al. (2009)" startWordPosition="4103" endWordPosition="4106">8 shared task, which consists of section 23 of the WSJ part of the Penn Treebank, as well as a small part of the Brown corpus. A beam width k of 4 was used in this experiment. Table 2 shows the results of the evaluation. The table shows the three most important scores computed by the official evaluation script: labeled syntactic dependency accuracy (LAS), labeled semantic dependency F1-measure (Sem. F1), and the macro-averaged F1-measure, a weighted combination of the syntactic and semantic scores (M. F1). Our result is competitive; we obtain the same macro F1 as the newly published result by Titov et al. (2009), which is the highest published figure for a joint syntactic–semantic parser so far. Importantly, our system clearly outperforms the system by Llu´ıs and M`arquez (2008), which is the most similar system in problem modeling, but which uses a different search strategy. System LAS Sem. F1 M. F1 This paper 86.6 77.1 81.8 Titov et al. (2009) 87.5 76.1 81.8 H. et al (2008) 87.6 73.1 80.5 L. &amp; M. (2008) 85.8 70.3 78.1 Table 2: Results of published joint syntactic– semantic parsers on the CoNLL-2008 test set. Since the search procedure is inexact, it is important to quantify roughly how much of a de</context>
</contexts>
<marker>Titov, Henderson, Merlo, Musillo, 2009</marker>
<rawString>Ivan Titov, James Henderson, Paola Merlo, and Gabriele Musillo. 2009. Online graph planarisation for synchronous parsing of semantic and syntactic dependencies. In Proceedings of the International Joint Conferences on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>589--596</pages>
<contexts>
<context position="3184" citStr="Toutanova et al., 2005" startWordPosition="479" endWordPosition="482">n syntax and semantics would improve the quality of all the substructures. The purpose of the CoNLL-2008 Shared Task was to study the feasibility of a joint analysis of syntax and semantics, and while most participating systems used a pipeline-based approach to the problem, there were a number of contributions that attempted to take the interdependence between syntax and semantics into account. The top-performing system in the task (Johansson and Nugues, 2008) applied a very simple reranking scheme by means of a k-best syntactic output, similar to previous attempts (Gildea and Jurafsky, 2002; Toutanova et al., 2005) to improve semantic role labeling performance by using mul561 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 561–569, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tiple parses. The system by Henderson et al. (2008) extended previous stack-based algorithms for dependency parsing by using two separate stacks to build the syntactic and semantic graphs. Lluis and M`arquez (2008) proposed a model that simultaneously predicts syntactic and semantic links, but since its search algorithm could not take the syntactic–semantic interdependencies into ac</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2005. Joint learning improves semantic role labeling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 589–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>88--94</pages>
<contexts>
<context position="20417" citStr="Xue and Palmer (2004)" startWordPosition="3446" endWordPosition="3449"> d] +— I′E[h, d] + 1 APPEND(buf, CREATE-SEC-OPEN(L, iL, E, I′E)) return buf To apply the bistratal search algorithm to the problem of syntactic–semantic parsing, a problem-specific implementation of the SCOPE function is needed. In this case, we made two assumptions. First, we assumed that the identities of the predicate words are known a priori2. Secondly, we assumed that every argument of a given predicate word is either a direct dependent of the predicate, one of its ancestors, or a direct dependent of one of its ancestors. This assumption is a simple adaptation of the pruning algorithm by Xue and Palmer (2004), and it holds for the vast majority of arguments in the CoNLL-2008 data; in the training set, we measured that this covers 99.04% of the arguments of verbs and 97.55% of the argu2Since our algorithm needs to know the positions of the predicates, we trained a separate classifier using the LIBLINEAR toolkit (Fan et al., 2008) to identify the predicate words. As features for the classifier, we used the words and part-ofspeech tags in a f3 window around the word under consideration. ments of nouns. Figure 4 shows an example of how the SCOPE function works in our case. If a predicate is contained </context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 88–94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>