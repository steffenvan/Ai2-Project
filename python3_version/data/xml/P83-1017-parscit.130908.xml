<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.6654705">
Sentence Disambiguation
by a Shift-Reduce Parsing Technique*
</title>
<author confidence="0.790455">
Stuart M. Shieber
</author>
<affiliation confidence="0.448578">
Artificial Intelligence Center
SRI International
</affiliation>
<address confidence="0.637716">
333 Ravenswood Avenue
Menlo Park, CA 94025
</address>
<sectionHeader confidence="0.83933" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999766235294118">
Native speakers of English show definite and consistent
preferences for certain readings of syntactically ambiguous sen-
tences. A user of a natural-language-processing system would
naturally expect it to reflect the same preferences. Thus, such
systems must model in some way the linguistic performance as
well as the linguistic competence of the native speaker. We
have developed a parsing algorithm—a variant of the LALR(1)
shift-reduce algorithm—that models the preference behavior of
native speakers for a range of syntactic preference phenomena
reported in the psycholinguistic literature, including the recent
data on lexical preferences. The algorithm yields the preferred
parse deterministically, without building multiple parse trees
and choosing among them. As a side effect, it displays ap-
propriate behavior in processing the much discussed garden-path
sentences. The parsing algorithm has been implemented and has
confirmed the feasibility of our approach to the modeling of these
phenomena.
</bodyText>
<sectionHeader confidence="0.989468" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999965375">
For natural language processing systems to be useful, they
must assign the same interpretation to a given sentence that a
native speaker would, since that is precisely the behavior users
will expect.. Consider, for example, the case of ambiguous sen-
tences. Native speakers of English show definite and consistent
preferences for certain readings of syntactically ambiguous sen-
tences [Kimball, 1973, Frazier and Fodor, 1978, Ford et al., 1982].
A user of a natural-language-processing system would naturally
expect, it to reflect the same preferences. Thus, such systems
must model in some way the linguistic performance as well as
the /inguiatic competence of the native speaker.
This idea is certainly not new in the artificial-intelligence
literature. The pioneering work of Marcus [Marcus, 1980] is per-
haps the best known example of linguistic-performance modeling
in Al. Starting from the hypothesis that &apos;deterministic&amp;quot; parsing
of English is possible, he demonstrated that certain performance
</bodyText>
<footnote confidence="0.779191833333333">
*This research wa.s supported by the Defense Advanced Research Projects
Agency under Contract N00039-80-C-0575 with the Naval Electronic
Systems Command. The views and conclusions contained in this document
are those or the author and should not be interpreted aa representative of
the official policies, either expressed or implied, of the Defense Advanced
Research Projects Agency or the United States government.
</footnote>
<bodyText confidence="0.999302076923077">
constraints, e.g., the difficulty of parsing garden-path sentences,
could be modeled. His claim about deterministic parsing was
quite strong. Not only was the behavior of the parser required
to be deterministic, but, as Marcus claimed,
The interpreter cannot use some general rule to take
a nondeterministic grammar specification and im-
pose arbitrary constraints to convert it to a deter-
ministic specification (unless, of course, there is a
general rule which will always lead to the correct
decision in such a case). [Marcus, 1980, p.14]
We have developed and implemented a parsing system
that, given a nondeterministic grammar, forces disambiguation
in just the manner Marcus rejected (i.e. through general rules);
it thereby exhibits the same preference behavior that psycholin-
guists have attributed to native speakers of English for a cer-
tain range of ambiguities. These include structural ambiguities
[Frazier and Fodor, 1978, Frazier and Fodor, 1980, Wanner, 1980]
and lexical preferences [Ford et at., 1982], as well as the garden-
path sentences as a side effect. The parsing system is based on
the shift-reduce scheduling technique of Pereira [forthcoming].
Our parsing algorithm is a slight variant of LALR(1) pars-
ing, and, as such, exhibits the three conditions postulated by
Marcus for a deterministic mechanism: it is data-driven, reflects
expectations, and has look-ahead. Like Marcus&apos;s parser, our
parsing system is deterministic. Unlike Marcus&apos;s parser, the
grammars used by our parser can be ambiguous.
</bodyText>
<sectionHeader confidence="0.547955" genericHeader="method">
2. The Phenomena to be Modeled
</sectionHeader>
<bodyText confidence="0.999946571428572">
The parsing system was designed to manifest preferences
among structurally distinct parses of ambiguous sentences. It
does this by building just one parse tree—rather than build-
ing multiple parse trees and choosing among them. Like the
Marcus parsing system, ours does not do disambiguation requir-
ing &amp;quot;extensive semantic processing,&amp;quot; but, in contrast to Marcus,
it does handle such phenomena as PP-attachment insofar as
there exist a priori preferences for one attachment over another.
By a priori we mean preferences that are exhibited in contexts
where pragmatic or plausibility considerations do not tend to
favor one reading over the other. Rather than make such value
judgments ourselves, we defer to the psycholinguistic literature
(specifically [Frazier and Fodor, 1978], [Frazier and Fodor, 1980]
and [Ford et at., 1982]) for our examples.
</bodyText>
<page confidence="0.998616">
113
</page>
<bodyText confidence="0.667129">
The parsing system models the following phenomena:
</bodyText>
<subsectionHeader confidence="0.937662">
Right Association
</subsectionHeader>
<bodyText confidence="0.915831375">
Native speakers of English tend to prefer readings in which
constituents are &apos;attached low.&apos; For instance, in the sen-
tence
Joe bought the book that I had been trying to obtain for
Susan.
the preferred reading is one in which the prepositional
phrase &amp;quot;for Susan&amp;quot; is associated with &amp;quot;to obtain&amp;quot; rather
than &amp;quot;bought.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.98921">
Minimal Attachment
</subsectionHeader>
<bodyText confidence="0.978709">
On the other hand, higher attachment is preferred in cer-
tain cases such as
Joe bought the book for Susan.
in which &amp;quot;for S113311&amp;quot; modifies &apos;the book&amp;quot; rather than
&amp;quot;bought.&amp;quot; Frazier and Fodor [1978] note that these are
cases in which the higher attachment includes fewer nodes
in the parse tree. Our analysis is somewhat different.
</bodyText>
<subsectionHeader confidence="0.958589">
Lexical Preference
</subsectionHeader>
<bodyText confidence="0.994490619047619">
Ford et at. 119821 present evidence that attachment
preferences depend on lexical choice. Thus, the preferred
reading for
The woman wanted the dress on that rack.
has low attachment of the PP, whereas
The woman positioned the dress on that rack.
has high attachment.
Garden-Path Sentences
Grammatical sentences such as
The horse raced past the barn fell.
seem actually to receive no parse by the native speaker
until some sort of &amp;quot;conscious parsing&amp;quot; is done. Following
Marcus (Marcus, 19801, we take this to be a hard failure
of the human sentence-processing mechanism.
It will be seen that all these phenomena are handled in our
parser by the same general rules. The simple context-free gram-
mar used&apos; (see Appendix I) allows both parses of the ambiguous
sentences as well as one for the garden-path sentences. The par-
ser disambiguates the grammar and yields only the preferred
structure. The actual output of the parsing system can be found
in Appendix II.
</bodyText>
<sectionHeader confidence="0.929999" genericHeader="method">
3. The Parsing System
</sectionHeader>
<bodyText confidence="0.993333851851852">
The parsing system we use is a shift-reduce parser. Shift-
reduce parsers [Aho and Johnson, 1974] are a very general class
of bottom-up parsers characterized by the following architecture.
They incorporate a stack for holding constituents built up during
1We make no claims as to the accuracy of the sample grammar. It is
obviously a gross simplification of English syntax. Its role is merely to
show that the parsing system is able to disambiguate the sentences under
consideration correctly.
the parse and a shift-reduce table for guiding the parse. At each
step in the parse, the table is used for deciding between two basic
types of operations: the shift operation, which adds the next
word in the sentence (with its preterminal category) to the top
of the stack, and the reduce operation, which removes several
elements from the top of the stack and replaces them with a
new element—for instance, removing an NP and a VP from the
top of the stack and replacing them with an S. The state of the
parser is also updated in accordance with the shift-reduce table
at each stage. The combination of the stack, input, and state of
the parser will be called a configuration and will be notated as,
for example,
NP V Mary 10
where the stack contains the nonterminals NP and V, the input
contains the lexical item Mary and the parser is in state 10.
By way of example, we demonstrate the operation of the
parser (using the grammar of Appendix I) on the oft-cited sen-
tence &apos;John loves Mary.&amp;quot; Initially the stack is empty and no
input has been consumed. The parser begins in state 0.
</bodyText>
<subsubsectionHeader confidence="0.843194">
John loves Mary
</subsubsectionHeader>
<bodyText confidence="0.974662333333333">
As elements are shifted to the stack, they are replaced by their
preterminal category.2 The shift-reduce table for the grammar
of Appendix I states that in state 0, with a proper noun as the
next word in the input, the appropriate action is a shift. The
new configuration, therefore, is
PNOUN j loves Mary 1 4 1
The next operation specified is a reduction of the proper noun
to a noun phrase yielding
NP loves ,Vfary 2
The verb and second proper noun are now shifted, in accordance
with the shift-reduce table, exhausting the input, and the proper
noun is then reduced to an NP.
</bodyText>
<equation confidence="0.954085333333333">
NP V Mary
NP V PNOUN
NP V NP
</equation>
<bodyText confidence="0.9991465">
Finally, the verb and noun phrase on the top of the stack are
reduced to a VP
</bodyText>
<equation confidence="0.954933666666667">
NP VP
which is in turn reduced, together with the subject NP, to an S.
1
</equation>
<bodyText confidence="0.831319">
This final configuration is an accepting configuration, since all
But see Section 3.2 for an exception.
</bodyText>
<equation confidence="0.7533275">
0 I
; 14 I
</equation>
<page confidence="0.979225">
114
</page>
<bodyText confidence="0.9990375">
the input has been consumed and an S derived. Thus the sen-
tence is grammatical in the grammar of Appendix I, as expected.
</bodyText>
<subsectionHeader confidence="0.964817">
3.1 Differences from the Standard LR Techniques
</subsectionHeader>
<bodyText confidence="0.999989133333334">
The shift-reduce table mentioned above is generated
automatically from a context-free grammar by the standard al-
gorithm (Aho and Johnson, 1974.1 The parsing alogrithat differs,
however, front the standard LALR(I) parsing algorithm in two
ways. First, instead of assigning preterminal symbols to words
as they are shifted, the algorithm allows the assignment to be
delayed if the word is ambiguous among preterminals. When
the word is used in a reduction, the appropriate preterminal is
assigned.
Second, and most importantly, since true LR parsers exist
only for unambiguous grammars, the normal algorithm for deriv-
ing LALR(I) shift-reduce tables yields a table that may specify
conflicting actions under certain configurations. It is through the
choice made from the options in a conflict that the preference
behavior we desire is engendered.
</bodyText>
<subsectionHeader confidence="0.999054">
3.2 Preterminal Delaying
</subsectionHeader>
<bodyText confidence="0.988439630136986">
One key advantage of shift-reduce parsing that is critical
in our system is the fact that decisions about the structure to
be assigned to a phrase are postponed as long as possible. In
keeping with this general principle, we extend the algorithm
to allow the assignment of a preterminal category to a lexical
item to be deferred until a decision is forced upon it, so to
speak, by an encompassing reduction. For instance, we would not
want to decide on the preterminal category of the word &amp;quot;that,&amp;quot;
which can serve as either a determiner (DET) or complementizer
(THAT), until some further information is available. Consider
the sentences
That problem is important.
That problems are difficult to solve is important.
Instead of assigning a preterminal to &apos;that,&amp;quot; we leave open the
possibility of assigning either DET or THAT until the first reduc-
tion that involves the word. In the first case, this reduction
will be by the rule NP -*DET NOM, thus forcing, once and for
all, the assignment of DET as preterminal. In the second case,
the DET NOM analysis is disallowed on the basis of number
agreement, so that the first applicable reduction is the COMP S
reduction to 3, forcing the assignment of THAT as preterminal.
Of course, the question arises as to what state the par-
ser goes into after shifting the lexical item &amp;quot;that.&amp;quot; The answer
is quite straightforward, though its interpretation via a VI.3 the
determinism hypothesis is subtle. The simple answer is that
the parser enters into a state corresponding to the union of the
states entered upon shifting a DET and upon shifting a THAT
respectively, in much the same way as the deterministic simula-
tion of a nondeterministic finite automaton enters a &amp;quot;union&amp;quot;
state when faced with a nondeterministic choice. Are we then
merely simulating a nondeterministic machine here! The answer
is equivocal. Although the imp!ementation acts as a simulator
for a nondeterministic machine, the nondeterminism is a priori
bounded, given a particular grammar and lexicon.3 Thus, the
nondeterminism could be traded in for a larger, albeit still finite,
set of states, unlike the nondeterminism found in other pars-
ing algorithms. Another way of looking at the situation is to
note that there is no observable property of the algorithm that
would distinguish the operation of the parser from a determinis-
tic one. In some sense, there is no interesting difference between
the limited nondeterminism of this parser, and Marcus&apos;s notion
of strict determinism. In fact, the implementation of Marcus&apos;s
parser also embodies a bounded nondeterminism in much the
same way this parser does.
The differentiating property between this parser and that
of Marcus is a slightly different one, namely, the property of
quasi-real-time operation.4 By quasi-real-time operation, Marcus
means that there exists a maximum interval of parser operation
for which no output can be generated. If the parser operates for
longer than this, it must generate some output. For instance,
the parser might be guaranteed to produce output (i.e., struc-
ture) at least every three words. However, because preterminal
assignment can be delayed indefinitely in pathological grammars,
there may exist sentences in such grammars for which arbitrary
numbers of words need to be read before output can be produced.
It is not clear whether this is a real disadvantage or not, and.
if so, whether there are simple adjustments to the algorithm
that would result in quasi-real-time behavior. In fact, it is a
property of bottom-up parsing in general that quasi-real-time
behavior is not guaranteed. Our parser has a less restrictive but
similar property, fairness, that is, our parser generates output
linear in the input, though there is no constant over which out-
put is guaranteed. For a fuller discussion of these properties, see
Pereira and Shieber (forthcomingl.
To summarize, preterminal delaying, as an intrinsic part
of the algorithm, does not actually change the basic properties
of the algorithm in any observable way. Note, however, that
preterminal assignments, like reductions, are irrevocable once
they are made (as a byproduct of the determinism of the algo-
rithm). Such decisions can therefore lead to garden paths, as
they do for the sentences presented in Section 3.8.
We now discuss the central feature of the algorithm,
namely, the resolution of shift-reduce conflicts.
</bodyText>
<subsectionHeader confidence="0.99343">
3.3 The Disambiguation Rules
</subsectionHeader>
<bodyText confidence="0.98065175">
Conflicts arise in two ways: shift-reduce conflicts, in which
the parser has the option of either shifting a word onto the stack
or reducing a set of elements on the stack to a new element;
reduce-reduce conflicts, in which reductions by several grammar
</bodyText>
<footnote confidence="0.9993912">
3The boundedness comes about because only a finite amount of informa-
tion is kept per state (an integer) and the nondeterminism stops at the
preterminal level, so that the splitting of states does not propogate.
41 am indebted to Mitch Marcus for this observation and the previous
comparison with his parser.
</footnote>
<page confidence="0.9816995">
115
11.6
</page>
<figure confidence="0.996510533333333">
(I)
Resolve shift-reduce conflicts by shifting.
20
NP V NP PP
3.5 Lexical Preference
23
for Susan
NP V NP that NP V
Susan
12
NP V NP that NP V P
19
NP V NP that NP V P NP
24
NP V NP that NP V PP
18
NP V NP that S/NP
1
20
NP wanted NP PP
isJoetyptookiNpiNpthe boolchthat I bought for Susan1111
14
NP wanted NP
1 6 I
NP VP
The sentence
rules are possible. The parser uses two rules to resolve these
conflicts:5
(2) Resolve reduce-reduce conflicts by performing
the longer reduction.
</figure>
<bodyText confidence="0.9514055">
These two rules suffice to engender the appropriate be-
havior in the parser for cases of right association and minimal
attachment. Though we demonstrate our system primarily with
PP-attachment examples, we claim that the rules are generally
valid for the phenomena being modeled (Pereira and Shieber,
forthcoming).
</bodyText>
<subsectionHeader confidence="0.973078">
3.4 Some Examples
</subsectionHeader>
<bodyText confidence="0.991946090909091">
Some examples demonstrate these principles. Consider the
sentence
Joe took the book that I bought for Susan.
After a certain amount of parsing has been completed deter-
ministically, the parser will be in the following configuration:
Joe bought the book for Susan.
demonstrates resolution of a reduce-reduce conflict. At some
point, in the parse, the parser is in the following configuration:
with a reduce-reduce conflict. Either a more complex NP or a
VP can be built. The conflict is resolved in favor of the longer
reduction, i.e., the VP reduction. The derivation continues:
</bodyText>
<figure confidence="0.8603516">
NP VP
1
ending in an accepting state with the following generated struc-
ture:
Is Joe(vp bought(Npthe bookl[ppfor Su.sanIII
</figure>
<bodyText confidence="0.89571475">
with a shift-reduce conflict, since the V can be reduced to a
VP/NP8 or the P can be shifted. The principles presented would
solve the conflict in favor of the shift, thereby leading to the
following derivation:
</bodyText>
<page confidence="0.9366345">
7
14
</page>
<bodyText confidence="0.963928863636364">
To handle the lexical-preference examples, we extend the
second rule slightly. Preterminal-word pairs can be stipulated as
either weak or strong. The second rule becomes
(2) Resolve reduce-reduce conflicts by performing
the longest reduction with the strongest leftmost
stack element.7
Therefore, if it is assumed that the lexicon encodes the
information that the triadic form of &apos;want&amp;quot; (V2 in the sample
grammar) and the dyadic form of &apos;position&amp;quot; (V1) are both weak,
we can see the operation of the shift-reduce parser on the &amp;quot;dress
on that rack&amp;quot; sentences of Section 2. Both sentences are similar
in form and will thus have a similar configuration when the
reduce-reduce conflict arises. For example, the first sentence will
be in the following configuration:
In this case, the longer reduction would require assignment of the
preterminal category V2 to &amp;quot;want,&amp;quot; which is the weak form: thus,
the shorter reduction will be preferred, leading to the derivation:
6The original notion of using a shift-reduce parser and general scheduling
principles to handle right association and minimal attachment, together
with the following two rules, are due to Fernando Pereira (Pereira, 19821.
The formalization of preterminal delaying and the extensions to the lexical-
preference cases and garden-path behavior are due to the author.
</bodyText>
<footnote confidence="0.991662714285714">
6The &amp;quot;slash-category&amp;quot; analysis of long-distance dependencies used here is
loosely based on the work of Gazdar 119811. The Appendix I grammar
does not incorporate the full range of slashed rules, however, but merely a
representative selection for illustrative purposes.
and the underlying structure:
(stbe woman(vpwanted(tvp(NPthe dress)fepon that rack))))
7Note that strength takes precedence over length.
</footnote>
<figure confidence="0.579536833333333">
NP V NP
NP V NP
NP VP
which yields the structure:
NP V NP that NP VP/NP
122
</figure>
<bodyText confidence="0.982493545454545">
In the case in which the verb is &amp;quot;positioned,&amp;quot; however, the longer
reduction does not yield the weak form of the verb; it will there-
fore be invoked, reslting in the structure:
[sthe woman[vppositioned[Npthe dressllppon that rack)))
Ford, M., J. Bresnan, and R. Kaplan, 1982: &amp;quot;A Competence-
Based Theory of Syntactic Closure,&amp;quot; in The Mental
Representation of Grammatical Relations, J. Bresnan, ed.
(Cambridge, Massachusetts: MIT Press).
Frazier, L., and J.D. Fodor, 1978: &amp;quot;The Sausage Machine: A
New Two-Stage Parsing Model,&amp;quot; Cognition, Volume 8, pp.
291-325.
</bodyText>
<subsectionHeader confidence="0.981004">
3.6 Garden-Path Sentences
</subsectionHeader>
<bodyText confidence="0.974927111111111">
As a side effect of these conflict resolution rules, certain
sentences in the language of the grammar will receive no parse
by the parsing system just discussed. These sentences are ap-
parently the ones classified as &amp;quot;garden-path&amp;quot; sentences, a class
that humans also have great difficulty parsing. Marcus&apos;s conjec-
ture that such difficulty stems from a hard failure of the normal
sentence-processing mechanism is directly modeled by the pars-
ing system presented here.
For instance, the sentence
The horse raced past the barn fell.
exhibits a reduce-reduce conflict before the last word. If the
participial form of &amp;quot;raced&amp;quot; is weak, the finite verb form will be
chosen; consequently, &amp;quot;raced past the barn&amp;quot; will be reduced to a
VP rather than a participial phrase. The parser will fail shortly,
since the correct choice of reduction was not made.
Similarly, the sentence
That scaly, deep-sea fish should be underwater is impor-
tant.
will fail, though grammatical. Before the word &amp;quot;should&amp;quot; is
shifted, a reduce-reduce conflict arises in forming an NP from
either &amp;quot;That scaly, deep-sea fish&amp;quot; or &amp;quot;scaly, deep-sea fish.&amp;quot; The
longer (incorrect) reduction will be performed and the parser will
fail.
Other examples, e.g., &amp;quot;the boy got fat melted,&amp;quot; or &apos;the
prime number few&amp;quot; would be handled similarly by the parser,
though the sample grammar of Appendix I does not parse them
[Pereira and Shieber, forthcoming).
</bodyText>
<sectionHeader confidence="0.994183" genericHeader="method">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.999994">
To be useful, natural-language systems must model the
behavior, if not the method, of the native speaker. We have
demonstrated that a parser using simple general rules for disam-
biguating sentences can yield appropriate behavior for a large
class of performance phenomena—right association, minimal at-
tachment, lexical preference, and garden-path sentences—and
that, morever, it can do so deterministically without generating
all the parses and choosing among them. The parsing system
has been implemented and has confirmed the feasibility of our
approach to the modeling of these phenomena.
</bodyText>
<sectionHeader confidence="0.995871" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.97240745">
Aho, A.V.. and S.C. Johnson, 1974: &amp;quot;LR Parsing,&amp;quot; Computing
Surveys. Volume 6, Number 2, pp. 99-i23 (Spring).
Frazier, L., and J.D. Fodor, 1980: &apos;Is the Human Sentence
Parsing Mechanism an ATN?&amp;quot; Cognition, Volume 8, pp.
411-459.
Gazdar, G., 1981: &amp;quot;Unbounded dependencies and coordinate
structure,&amp;quot; Linguistic Inquiry, Volume 12, pp. 185-179.
Kimball, J., 1973: &amp;quot;Seven Principles of Surface Structure Parsing
in Natural Language,&amp;quot; Cognition, Volume 2, Number 1,
pp. 15-47.
Marcus, M., 1980: A Theory of Syntactic Recognition for Natural
Language, (Cambridge, Massachusetts: MIT Press).
Pereira, F.C.N., forthcoming: &amp;quot;A New Characterization of
Attachment Preferences,&amp;quot; to appear in D. Dowty,
L. Karttunen, and A. Zwicky (eds.) Natural
Language Processing. Psycholinguiatic, Computational,
and Theoretical Perspectives, Cambridge, England:
Cambridge University Press.
Pereira. F.C;.N., and S.M. Shieber, forthcoming: &apos;Shift-Reduce
Scheduling and Syntactic Closure,&amp;quot; to appear.
</reference>
<bodyText confidence="0.6550175">
Wanner, E., 1980: &amp;quot;The ATN and the Sausage Machine: Which
One is Baloney?&amp;quot; Cognition, Volume 8, pp. 209-225.
</bodyText>
<sectionHeader confidence="0.835054" genericHeader="method">
Appendix I. The Test Grammar
</sectionHeader>
<bodyText confidence="0.999281">
The following is the grammar used to test the parsing
system descibed in the paper. Not a robust grammar of English
by any means, it, is presented only for the purpose of establishing
that the preference rules yield the correct results.
</bodyText>
<table confidence="0.828020733333333">
S •-■ NP VP VP V3 INF
S 3 VP VP --• V4 ADJ
NP DET NOM vp V5 PP
NP NOM g--• that S
NP PNOUN INF to VP
NP NP &apos;/NP PP — P NP
NP NP PARTP PARTP VPART PP
NP NP PP g/NP that S/NP
DET NP &apos;s S/NP VP
NOM N S/NP NP VP/NP
NOM — ADJ NOM VP/NP VI
VP — AUX VP VP/NP V2 PP
VP — VO VP/NP V3 INF/NP
VP VI NP VP/NP AUX VP/NP
VP V2 NP PP INF/NP to VP/NP
</table>
<sectionHeader confidence="0.819724" genericHeader="method">
Appendix H. Sample Runs
</sectionHeader>
<bodyText confidence="0.5224255">
&gt;&gt; Joe bought the boot that I bad been trying to obtain
for Susan
</bodyText>
<page confidence="0.983487">
117
</page>
<figure confidence="0.9491923">
Accepted: Co state: (I)
(up (pnoun Joe))
(vp stack: &lt;(0), Es lap (det. That)
(v1 bought) (nom (adj scaly)
(up (nom (MI deep-sea)
(up (det the) (nos Cu fish]
(nos (a book))) (9) (aux should)
(sbar/np (vp (v4 be)
(that that) (adj underwater)
(ship
Cap (paean I))
(vp/np input: (v4 is)
(aux had) (adj important)
(vp/ap (end)
(aux been)
(vp/np (v3 trying)
(iat/ap
(to to)
(vp/ap
(v2 obtain)
(pp (p for)
Cap (paean Snow&apos;)
&gt;, Joe bought the book for Susan
Accepted: Cs (up (pnoun Joe))
(vp (v2 bought)
(up (dei the)
(nom (a book)))
(pp (p for)
(up (pnoun Summit)
» The woman wanted the dress on that rack
Accepted: (a (up (det The)
(nom Cu woman)))
(vp (vi wanted)
(up (dot the)
(nos (n dress)))
(PP (P on)
Cap (det that)
(nom (a rack)
» The woman positioned the dress on that rack
Accepted: (s (up (det The)
</figure>
<reference confidence="0.96046505">
(nos Cu moans)))
(TP (c2 positioned)
(up (dei the)
(nos (a dress)))
(pp (p on)
(up (dei that)
(nom Cu rack)
&gt;7. The horse raced past the barn fell
Parse failed. Current configuration:
state: (1)
stack: &lt;(0)&gt; (s (up (det The)
(nom (n horse)))
(vp (v5 raced)
(pp (p past)
(up (dot the)
(nom (n barn)
input. (w0 fell)
(end)
&gt;&gt; That scaly deep-sea fish should be underwater is important
Parse failed. Current configuration:
</reference>
<page confidence="0.996142">
118
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.918839">
<title confidence="0.987981">Sentence Disambiguation by a Shift-Reduce Parsing Technique*</title>
<author confidence="0.999984">Stuart M Shieber</author>
<affiliation confidence="0.999759">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.998419">333 Ravenswood Avenue Menlo Park, CA 94025</address>
<abstract confidence="0.996906166666667">Native speakers of English show definite and consistent preferences for certain readings of syntactically ambiguous sentences. A user of a natural-language-processing system would naturally expect it to reflect the same preferences. Thus, such must model in some way the performance as the competence the native speaker. We have developed a parsing algorithm—a variant of the LALR(1) shift-reduce algorithm—that models the preference behavior of native speakers for a range of syntactic preference phenomena reported in the psycholinguistic literature, including the recent data on lexical preferences. The algorithm yields the preferred parse deterministically, without building multiple parse trees and choosing among them. As a side effect, it displays appropriate behavior in processing the much discussed garden-path sentences. The parsing algorithm has been implemented and has confirmed the feasibility of our approach to the modeling of these phenomena.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>S C Johnson</author>
</authors>
<title>LR Parsing,&amp;quot; Computing Surveys.</title>
<date>1974</date>
<volume>6</volume>
<pages>99--23</pages>
<location>(Spring).</location>
<contexts>
<context position="6839" citStr="Aho and Johnson, 1974" startWordPosition="1041" endWordPosition="1044">done. Following Marcus (Marcus, 19801, we take this to be a hard failure of the human sentence-processing mechanism. It will be seen that all these phenomena are handled in our parser by the same general rules. The simple context-free grammar used&apos; (see Appendix I) allows both parses of the ambiguous sentences as well as one for the garden-path sentences. The parser disambiguates the grammar and yields only the preferred structure. The actual output of the parsing system can be found in Appendix II. 3. The Parsing System The parsing system we use is a shift-reduce parser. Shiftreduce parsers [Aho and Johnson, 1974] are a very general class of bottom-up parsers characterized by the following architecture. They incorporate a stack for holding constituents built up during 1We make no claims as to the accuracy of the sample grammar. It is obviously a gross simplification of English syntax. Its role is merely to show that the parsing system is able to disambiguate the sentences under consideration correctly. the parse and a shift-reduce table for guiding the parse. At each step in the parse, the table is used for deciding between two basic types of operations: the shift operation, which adds the next word i</context>
<context position="9526" citStr="Aho and Johnson, 1974" startWordPosition="1525" endWordPosition="1528">to an NP. NP V Mary NP V PNOUN NP V NP Finally, the verb and noun phrase on the top of the stack are reduced to a VP NP VP which is in turn reduced, together with the subject NP, to an S. 1 This final configuration is an accepting configuration, since all But see Section 3.2 for an exception. 0 I ; 14 I 114 the input has been consumed and an S derived. Thus the sentence is grammatical in the grammar of Appendix I, as expected. 3.1 Differences from the Standard LR Techniques The shift-reduce table mentioned above is generated automatically from a context-free grammar by the standard algorithm (Aho and Johnson, 1974.1 The parsing alogrithat differs, however, front the standard LALR(I) parsing algorithm in two ways. First, instead of assigning preterminal symbols to words as they are shifted, the algorithm allows the assignment to be delayed if the word is ambiguous among preterminals. When the word is used in a reduction, the appropriate preterminal is assigned. Second, and most importantly, since true LR parsers exist only for unambiguous grammars, the normal algorithm for deriving LALR(I) shift-reduce tables yields a table that may specify conflicting actions under certain configurations. It is through</context>
</contexts>
<marker>Aho, Johnson, 1974</marker>
<rawString>Aho, A.V.. and S.C. Johnson, 1974: &amp;quot;LR Parsing,&amp;quot; Computing Surveys. Volume 6, Number 2, pp. 99-i23 (Spring).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
<author>J D Fodor</author>
</authors>
<title>Is the Human Sentence Parsing Mechanism an ATN?&amp;quot;</title>
<date>1980</date>
<journal>Cognition,</journal>
<volume>8</volume>
<pages>411--459</pages>
<contexts>
<context position="3568" citStr="Frazier and Fodor, 1980" startWordPosition="518" endWordPosition="521">d impose arbitrary constraints to convert it to a deterministic specification (unless, of course, there is a general rule which will always lead to the correct decision in such a case). [Marcus, 1980, p.14] We have developed and implemented a parsing system that, given a nondeterministic grammar, forces disambiguation in just the manner Marcus rejected (i.e. through general rules); it thereby exhibits the same preference behavior that psycholinguists have attributed to native speakers of English for a certain range of ambiguities. These include structural ambiguities [Frazier and Fodor, 1978, Frazier and Fodor, 1980, Wanner, 1980] and lexical preferences [Ford et at., 1982], as well as the gardenpath sentences as a side effect. The parsing system is based on the shift-reduce scheduling technique of Pereira [forthcoming]. Our parsing algorithm is a slight variant of LALR(1) parsing, and, as such, exhibits the three conditions postulated by Marcus for a deterministic mechanism: it is data-driven, reflects expectations, and has look-ahead. Like Marcus&apos;s parser, our parsing system is deterministic. Unlike Marcus&apos;s parser, the grammars used by our parser can be ambiguous. 2. The Phenomena to be Modeled The pa</context>
<context position="4965" citStr="Frazier and Fodor, 1980" startWordPosition="731" endWordPosition="734"> multiple parse trees and choosing among them. Like the Marcus parsing system, ours does not do disambiguation requiring &amp;quot;extensive semantic processing,&amp;quot; but, in contrast to Marcus, it does handle such phenomena as PP-attachment insofar as there exist a priori preferences for one attachment over another. By a priori we mean preferences that are exhibited in contexts where pragmatic or plausibility considerations do not tend to favor one reading over the other. Rather than make such value judgments ourselves, we defer to the psycholinguistic literature (specifically [Frazier and Fodor, 1978], [Frazier and Fodor, 1980] and [Ford et at., 1982]) for our examples. 113 The parsing system models the following phenomena: Right Association Native speakers of English tend to prefer readings in which constituents are &apos;attached low.&apos; For instance, in the sentence Joe bought the book that I had been trying to obtain for Susan. the preferred reading is one in which the prepositional phrase &amp;quot;for Susan&amp;quot; is associated with &amp;quot;to obtain&amp;quot; rather than &amp;quot;bought.&amp;quot; Minimal Attachment On the other hand, higher attachment is preferred in certain cases such as Joe bought the book for Susan. in which &amp;quot;for S113311&amp;quot; modifies &apos;the book&amp;quot;</context>
</contexts>
<marker>Frazier, Fodor, 1980</marker>
<rawString>Frazier, L., and J.D. Fodor, 1980: &apos;Is the Human Sentence Parsing Mechanism an ATN?&amp;quot; Cognition, Volume 8, pp. 411-459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Unbounded dependencies and coordinate structure,&amp;quot;</title>
<date>1981</date>
<journal>Linguistic Inquiry,</journal>
<volume>12</volume>
<pages>185--179</pages>
<marker>Gazdar, 1981</marker>
<rawString>Gazdar, G., 1981: &amp;quot;Unbounded dependencies and coordinate structure,&amp;quot; Linguistic Inquiry, Volume 12, pp. 185-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kimball</author>
</authors>
<title>Seven Principles of Surface Structure Parsing in Natural Language,&amp;quot;</title>
<date>1973</date>
<journal>Cognition,</journal>
<volume>2</volume>
<pages>15--47</pages>
<contexts>
<context position="1596" citStr="Kimball, 1973" startWordPosition="226" endWordPosition="227">riate behavior in processing the much discussed garden-path sentences. The parsing algorithm has been implemented and has confirmed the feasibility of our approach to the modeling of these phenomena. 1. Introduction For natural language processing systems to be useful, they must assign the same interpretation to a given sentence that a native speaker would, since that is precisely the behavior users will expect.. Consider, for example, the case of ambiguous sentences. Native speakers of English show definite and consistent preferences for certain readings of syntactically ambiguous sentences [Kimball, 1973, Frazier and Fodor, 1978, Ford et al., 1982]. A user of a natural-language-processing system would naturally expect, it to reflect the same preferences. Thus, such systems must model in some way the linguistic performance as well as the /inguiatic competence of the native speaker. This idea is certainly not new in the artificial-intelligence literature. The pioneering work of Marcus [Marcus, 1980] is perhaps the best known example of linguistic-performance modeling in Al. Starting from the hypothesis that &apos;deterministic&amp;quot; parsing of English is possible, he demonstrated that certain performance</context>
</contexts>
<marker>Kimball, 1973</marker>
<rawString>Kimball, J., 1973: &amp;quot;Seven Principles of Surface Structure Parsing in Natural Language,&amp;quot; Cognition, Volume 2, Number 1, pp. 15-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language,</title>
<date>1980</date>
<publisher>MIT Press).</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="1996" citStr="Marcus, 1980" startWordPosition="287" endWordPosition="288">s will expect.. Consider, for example, the case of ambiguous sentences. Native speakers of English show definite and consistent preferences for certain readings of syntactically ambiguous sentences [Kimball, 1973, Frazier and Fodor, 1978, Ford et al., 1982]. A user of a natural-language-processing system would naturally expect, it to reflect the same preferences. Thus, such systems must model in some way the linguistic performance as well as the /inguiatic competence of the native speaker. This idea is certainly not new in the artificial-intelligence literature. The pioneering work of Marcus [Marcus, 1980] is perhaps the best known example of linguistic-performance modeling in Al. Starting from the hypothesis that &apos;deterministic&amp;quot; parsing of English is possible, he demonstrated that certain performance *This research wa.s supported by the Defense Advanced Research Projects Agency under Contract N00039-80-C-0575 with the Naval Electronic Systems Command. The views and conclusions contained in this document are those or the author and should not be interpreted aa representative of the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the United St</context>
<context position="6254" citStr="Marcus, 1980" startWordPosition="943" endWordPosition="944">which the higher attachment includes fewer nodes in the parse tree. Our analysis is somewhat different. Lexical Preference Ford et at. 119821 present evidence that attachment preferences depend on lexical choice. Thus, the preferred reading for The woman wanted the dress on that rack. has low attachment of the PP, whereas The woman positioned the dress on that rack. has high attachment. Garden-Path Sentences Grammatical sentences such as The horse raced past the barn fell. seem actually to receive no parse by the native speaker until some sort of &amp;quot;conscious parsing&amp;quot; is done. Following Marcus (Marcus, 19801, we take this to be a hard failure of the human sentence-processing mechanism. It will be seen that all these phenomena are handled in our parser by the same general rules. The simple context-free grammar used&apos; (see Appendix I) allows both parses of the ambiguous sentences as well as one for the garden-path sentences. The parser disambiguates the grammar and yields only the preferred structure. The actual output of the parsing system can be found in Appendix II. 3. The Parsing System The parsing system we use is a shift-reduce parser. Shiftreduce parsers [Aho and Johnson, 1974] are a very ge</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Marcus, M., 1980: A Theory of Syntactic Recognition for Natural Language, (Cambridge, Massachusetts: MIT Press).</rawString>
</citation>
<citation valid="false">
<authors>
<author>F C N Pereira</author>
</authors>
<title>forthcoming: &amp;quot;A New Characterization of Attachment Preferences,&amp;quot; to appear</title>
<booktitle>Natural Language Processing. Psycholinguiatic, Computational, and Theoretical Perspectives,</booktitle>
<editor>in D. Dowty, L. Karttunen, and A. Zwicky (eds.)</editor>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge, England:</location>
<marker>Pereira, </marker>
<rawString>Pereira, F.C.N., forthcoming: &amp;quot;A New Characterization of Attachment Preferences,&amp;quot; to appear in D. Dowty, L. Karttunen, and A. Zwicky (eds.) Natural Language Processing. Psycholinguiatic, Computational, and Theoretical Perspectives, Cambridge, England: Cambridge University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>N F C</author>
<author>S M Shieber</author>
</authors>
<title>forthcoming: &apos;Shift-Reduce Scheduling and Syntactic Closure,&amp;quot; to appear. (nos Cu</title>
<note>moans))) (TP (c2 positioned) (up (dei the) (nos (a dress))) (pp (p on)</note>
<marker>C, Shieber, </marker>
<rawString>Pereira. F.C;.N., and S.M. Shieber, forthcoming: &apos;Shift-Reduce Scheduling and Syntactic Closure,&amp;quot; to appear. (nos Cu moans))) (TP (c2 positioned) (up (dei the) (nos (a dress))) (pp (p on)</rawString>
</citation>
<citation valid="false">
<title>up (dei that) (nom Cu rack) &gt;7. The horse raced past the barn fell Parse failed. Current configuration: state: (1)</title>
<marker></marker>
<rawString>(up (dei that) (nom Cu rack) &gt;7. The horse raced past the barn fell Parse failed. Current configuration: state: (1)</rawString>
</citation>
<citation valid="false">
<title>stack: &lt;(0)&gt; (s (up (det The) (nom (n horse))) (vp (v5 raced) (pp (p past) (up (dot the) (nom (n barn)</title>
<marker></marker>
<rawString>stack: &lt;(0)&gt; (s (up (det The) (nom (n horse))) (vp (v5 raced) (pp (p past) (up (dot the) (nom (n barn)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>