<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016294">
<title confidence="0.992009">
Coreference Resolution Evaluation for Higher Level Applications
</title>
<author confidence="0.998032">
Don Tuggener
</author>
<affiliation confidence="0.9975415">
Unversity of Zurich
Institute of Computational Linguistics
</affiliation>
<email confidence="0.982711">
tuggener@cl.uzh.ch
</email>
<sectionHeader confidence="0.993557" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999589777777778">
This paper presents an evaluation frame-
work for coreference resolution geared to-
wards interpretability for higher-level ap-
plications. Three application scenarios
for coreference resolution are outlined and
metrics for them are devised. The metrics
provide detailed system analysis and aim
at measuring the potential benefit of using
coreference systems in preprocessing.
</bodyText>
<sectionHeader confidence="0.998771" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999695318181818">
Coreference Resolution is often described as an
important preprocessing step for higher-level ap-
plications. However, the commonly used coref-
erence evaluation metrics (MUC, BCUB, CEAF,
BLANC) treat coreference as a generic clustering
problem and perform cluster similarity measures
to evaluate coreference system outputs. Mentions
are seen as unsorted generic items rather than lin-
early ordered linguistic objects (Chen and Ng,
2013). This makes it arguably hard to interpret
the scores and assess the potential benefit of using
a coreference system as a preprocessing step.
Therefore, this paper proposes an evaluation
framework for coreference systems which aims at
bridging the gap between coreference system de-
velopment, evaluation, and higher level applica-
tions. For this purpose, we outline three types
of application scenarios which coreference resolu-
tion can benefit and devise metrics for them which
are easy to interpret and provide detailed system
output analysis based on any available mention
feature.
</bodyText>
<sectionHeader confidence="0.993456" genericHeader="method">
2 Basic Concepts
</sectionHeader>
<bodyText confidence="0.979686523809524">
Like other coreference metrics, we adapt the con-
cepts of Recall and Precision from evaluation in
Information Retrieval (IR) to compare mentions
in a system output (the response) to the anno-
tated mentions in a gold standard (the key). To
stay close to the originally clear definitions of Re-
call and Precision in IR, Recall is aimed at iden-
tifying how many of the annotated key mentions
are correctly resolved by a system, and Precision
will measure the correctness of the returned sys-
tem mentions.
However, if we define Recall as tp, the de-
tp+f n
nominator will not include key mentions that have
been put in the wrong coreference chain, and will
not denote all mentions in the key. Therefore,
borrowing nomenclature from (Durrett and Klein,
2013), we introduce an additional error class,
wrong linkage (wl), which signifies key mentions
that have been linked to incorrect antecedents. Re-
call can then be defined as tp
</bodyText>
<sectionHeader confidence="0.382908" genericHeader="method">
tp+wl+fn and Precision
</sectionHeader>
<bodyText confidence="0.99534884">
as tp
tp+wl+fp. Recall then extends over all key men-
tions, and Precision calculation includes all sys-
tem mentions.
Furthermore, including wrong linkage in the
Recall equation prevents it from inflating com-
pared to Precision when a large number of key
mentions are incorrectly resolved. Evaluation
is also sensitive to the anaphoricity detection
problem. For example, an incorrectly resolved
anaphoric “it” pronoun is counted as wrong link-
age and thus also affects Recall, while a resolved
pleonastic “it” pronoun is considered a false posi-
tive which is only penalized by Precision. Beside
the “it” pronoun, this is of particular relevance for
noun markables, as determining their referential
status is a non-trivial subtask in coreference res-
olution.
As we evaluate each mention individually, we
are able to measure performance regarding any
feature type of a mention, e.g. PoS, number, gen-
der, semantic class etc. We will focus on men-
tion types based on PoS tags (i.e. pronouns, nouns
etc.), as they are often the building blocks of coref-
erence systems. Furthermore, mention type based
</bodyText>
<page confidence="0.968732">
231
</page>
<note confidence="0.686624">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 231–235,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999837333333333">
performance analysis is informative for higher-
level applications, as they might be specifically in-
terested in certain mention types.
</bodyText>
<sectionHeader confidence="0.937392" genericHeader="method">
3 Application Scenarios
</sectionHeader>
<bodyText confidence="0.999796">
Next, we will outline three higher-level applica-
tion types which consume coreference and devise
relevant metrics for them.
</bodyText>
<subsectionHeader confidence="0.999952">
3.1 Models of entity distributions
</subsectionHeader>
<bodyText confidence="0.999975146341463">
The first application scenario subsumes models
that investigate distributions and patterns of en-
tity occurrences in discourse. For example, Cen-
tering theory (Grosz et al., 1995) and the thereof
derived entity grid model (Barzilay and Lapata,
2008; Elsner and Charniak, 2011) record transi-
tions of grammatical functions that entities occur
with in coherent discourse. These models can
benefit from coreference resolution if entities are
pronominalized or occur as a non-string matching
nominal mentions.
Another application which tracks sequences of
entity occurrences is event sequence modeling.
Such models investigate prototypical sequences of
events to derive event schemes or templates of suc-
cessive events (Lee et al., 2012; Irwin et al., 2011;
Kuo and Chen, 2007). Here, coreference res-
olution can help link pronominalized arguments
of events to their previous mention and, thereby,
maintain the event argument sequence.
The outlined applications in this scenario pri-
marily rely on the identification of correct and
gapless sequences of entity occurrences. We can
approximate this requirement in a metric by re-
quiring the immediate antecedent of a mention in
a response chain to be the immediate antecedent
of that mention in the key chain.
Note that this restriction deems mentions as in-
correct, if they skip an antecedent but are resolved
to another antecedent in the correct chain. For ex-
ample, given a key [A-B-C-D], mention D in a re-
sponse [A-B-D] would not be considered correct,
as the immediate antecedent is not the same as in
the key. The original sequence of the entity’s oc-
currence is broken between mention B and D in
the response, as mention C is missing.
We use the following algorithm (table 1) to cal-
culate Recall and Precision for evaluating imme-
diate antecedents. Let K be the key and S be the
system response. Let e be an entity denoted by mn
mentions.
</bodyText>
<equation confidence="0.99470225">
01 for ek ∈ K:
02 for mi ∈ ek ∧ i &gt; 0:
03 if ¬∃e3, mj : (e3 ∈ S ∧ mj ∈ e3 ∧ mj = mi∧
∃predecessor(mj, e3)) → fn++
04 elif ∃e3, mj : (e3 ∈ S ∧ mj ∈ e3 ∧ mj = mi∧
predecessor(mi,ek) = predecessor(mj, e3))
→ tp++
05 else wl++
06 for e3 ∈ S:
07 for mi ∈ e3 ∧ i &gt; 0:
08 if ¬∃ek, mj : (ek ∈ K ∧ mj ∈ ek ∧ mj = mi∧
∃predecessor(mj, ek)) → fp++
</equation>
<tableCaption confidence="0.6235775">
Table 1: Algorithm for calculating Recall and Pre-
cision.
</tableCaption>
<bodyText confidence="0.999988307692308">
We traverse the key K and each entity ek in
it1. We evaluate each mention mi in ek, except for
the first one (line 2), as we investigate coreference
links. If no response chain exists that contains
mi and its predecessor, we count mi as a false
negative (line 3). This condition subsumes the
case where mi is not in the response, and the case
where mi is the first mention of a response chain.
In the latter case, the system has deemed mi to be
non-anaphoric (i.e. the starter of a chain), while it
is anaphoric in the key2. We check whether the
immediate predecessor of mi in the key chain ek
is also the immediate predecessor of mj in the re-
sponse chain es (line 4). If true, we count mi as a
true positive, or as wrong linkage otherwise.
We traverse the response chains to detect spu-
rious system mentions, i.e. mentions not in the
key, and count them as false positives, i.e. non-
anaphoric markables that have been resolved by
the system (lines 6-8). Here, we also count men-
tions in the response, which have no predecessor
in a key chain, as false positives. If a mention
in the response chain is the chain starter in a key
chain, it means that the system has falsely deemed
it to be anaphoric and we regard it as a false posi-
tive3.
</bodyText>
<subsectionHeader confidence="0.99495">
3.2 Inferred local entities
</subsectionHeader>
<bodyText confidence="0.999935833333333">
The second application scenario relies on corefer-
ence resolution to infer local nominal antecedents.
For example, in Summarization, a target sentence
may contain a pronoun which should be replaced
by a nominal antecedent to avoid ambiguities and
ensure coherence in the summary. Machine Trans-
</bodyText>
<footnote confidence="0.977481">
1We disregard singleton entities, as it is not clear what
benefit a higher level application could gain from them.
2(Durrett and Klein, 2013) call this error false new (FN).
3This error is called false anaphoric (FA) by (Durrett and
Klein, 2013).
</footnote>
<page confidence="0.986317">
232
</page>
<bodyText confidence="0.997776894736842">
lation can benefit from pronoun resolution in lan-
guage pairs where nouns have grammatical gen-
der. In such language pairs, the gender of a pro-
noun antecedent has to be retrieved in the source
language in order to insert the pronoun with the
correct gender in the target language.
In these applications, it is not sufficient to link
pronouns to other pronouns of the same corefer-
ence chain because they do not help infer the un-
derlying entity. Therefore, in our metric, we re-
quire the closest preceding nominal antecedent of
a mention in a response chain to be an antecedent
in the key chain.
The algorithm for calculation of Recall and Pre-
cision is similar to the one in table 1. We modify
lines 3 and 4 to require the closest nominal an-
tecedent of mi in the response chain es to be an
antecedent of mj in the corresponding key chain
ek, where mj = mi, i.e.:
</bodyText>
<equation confidence="0.765747">
∃mh ∈ es : is closest noun(mh, mi) ∧
∃ek, mj, ml : (ek ∈ K ∧ mj ∈ ek ∧ mj =
mi ∧ ml ∈ ek ∧ l &lt; j ∧ ml = mh) → tp++
</equation>
<bodyText confidence="0.962906857142857">
Note that we cannot process chains without a
nominal mention in this scenario4. Therefore, we
skip evaluation for such ek ∈ K. We still want
to find incorrectly inferred nominal antecedents of
anaphoric mentions, i.e. mentions in es ∈ S that
have been assigned a nominal antecedent in the re-
sponse but have none in the key and count them as
wrong linkage, as they infer an incorrect nominal
antecedent. Therefore, we traverse all es ∈ S and
add to the algorithm:
∀mi ∈ es : ¬is noun(mi) ∧ ∃mh ∈ es :
is noun(mh) ∧ ∃ek, mj : (ek ∈ K ∧ mj ∈
ek ∧ mj = mi ∧ ¬∃ml ∈ ek : is noun(ml)) →
wl++
</bodyText>
<subsectionHeader confidence="0.998934">
3.3 Finding contexts for a specific entity
</subsectionHeader>
<bodyText confidence="0.99830503125">
The last scenario we consider covers applications
that are primarily query driven. Such applications
search for references to a given entity and analyze
or extract its occurrence contexts. For example,
Sentiment Analysis searches large text collections
for occurrences of a target entity and then derives
polarity information from its contexts. Biomedical
relation mining looks for interaction contexts of
specific genes or proteins etc.
4We found that 476 of 4532 key chains (10.05%) do not
contain a nominal mention. Furthermore, we do not treat
cataphora (i.e. pronouns at chain start) in this scenario. We
found that 241 (5.31%) of the key chains start with cataphoric
pronouns.
For these applications, references to relevant en-
tities have to be accessible by queries. For ex-
ample, if a sentiment system investigates polarity
contexts of the entity “Barack Obama”, given a
key chain [Obama - the president - he], a response
chain [the president - he] is not sufficient, because
the higher level application is not looking for in-
stances of the generic “president” entity.
Therefore, we determine an anchor mention for
each coreference chain which represents the most
likely unique surface form an entity occurs with.
As a simple approximation, we choose the first
nominal mention of a coreference chain to be the
anchor of the entity, because first mentions of enti-
ties introduce them to discourse and are, therefore,
generally informative, unambiguous, semantically
extensive and are likely to contain surface forms a
higher level application will query.
</bodyText>
<table confidence="0.986534058823529">
Entity Detection
01 for ek ∈ K:
02 if ∃mn ∈ ek : is noun(mn)
→ manchor = determine anchor(ek)
03 if ∃manchor ∧ ∃es ∈ S : manchor ∈ es → tp++
04 else → fn++
05 for es ∈ S:
06 if ∃mn ∈ es : is noun(mn)
→ manchor = determine anchor(es)
07 if ¬∃ek ∈ K : manchor ∈ ek → fp++
Entity Mentions
01 for ek ∈ K : ∃manchor ∧ ∃es ∈ S : manchor ∈ es :
02 for mi ∈ ek :
03 if mi ∈ es → tp++
04 else→ fn++
05 for mi ∈ es :
06 if mi¬ ∈ ek → fp++
</table>
<tableCaption confidence="0.915844">
Table 2: Algorithm for calculating Recall and Pre-
cision using anchor mentions.
</tableCaption>
<bodyText confidence="0.9996244375">
To calculate Recall and Precision, we align
coreference chains in the responses to those in the
key via their anchors and then measure how many
(in)correct references to that anchor the corefer-
ence systems find (table 2). We divide evaluation
into entity detection (ED), which measures how
many of the anchor mentions a system identifies.
We then measure the quality of the entity men-
tions (EM) for only those entities which have been
aligned through their anchors.
The quality of the references to the anchor men-
tions are not directly comparable between sys-
tems, as their basis is not the same if the num-
ber of aligned anchors differs. Therefore, we cal-
culate the harmonic mean of entity detection and
entity mentions to enable direct system compari-
</bodyText>
<page confidence="0.996047">
233
</page>
<bodyText confidence="0.998404333333333">
son. Where applicable, we obtain the named en-
tity class of the entity and measure performance
for each such class.
</bodyText>
<sectionHeader confidence="0.994599" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.998052">
We apply our metrics to three available corefer-
ence systems, namely the Berkley system (Dur-
rett and Klein, 2013), the IMS system (Bj¨orkelund
and Farkas, 2012), and the Stanford system (Lee
et al., 2013) and their responses for the CoNLL
2012 shared task test set for English (Pradhan et
al., 2012). Tables 3 and 4 report the results.
</bodyText>
<table confidence="0.999857588235294">
Immediate antecedent Inferred antecedent
R P F R P F
BERK (Durrett and Klein, 2013)
NOUN 45.06 47.06 46.04 55.54 60.37 57.85
PRP 67.66 64.87 66.24 48.92 53.62 51.16
PRP$ 74.49 74.32 74.41 61.95 66.80 64.28
TOTAL 56.60 56.91 56.76 52.94 58.04 55.37
IMS (Bj¨orkelund and Farkas, 2012)
NOUN 38.01 43.09 40.39 46.90 54.96 50.61
PRP 69.06 68.64 68.85 43.04 57.42 49.20
PRP$ 72.57 72.11 72.34 51.51 63.54 56.90
TOTAL 53.55 57.55 55.48 45.27 56.47 50.25
STAN (Lee et al., 2013)
NOUN 38.51 42.92 40.60 50.03 57.62 53.56
PRP 65.55 61.09 63.25 36.67 45.97 40.80
PRP$ 66.12 65.70 65.91 40.64 52.38 45.77
TOTAL 51.70 52.69 52.19 43.01 51.73 46.97
</table>
<tableCaption confidence="0.999914">
Table 3: Antecedent based evaluation
</tableCaption>
<bodyText confidence="0.999933761904762">
We note that the system ranking based on the
MELA score5 is retained by our metrics. MELA
rates the Berkley system best (61.62), followed by
the IMS system (57.42), and then the Stanford sys-
tem (55.69).
Beside detailed analysis based on PoS tags, our
metrics reveal interesting nuances. Somewhat ex-
pectedly, noun resolution is worse when the imme-
diate antecedent is evaluated, than if the next nom-
inal antecedent is analyzed. Symmetrically in-
verse, pronouns achieve higher scores when their
direct antecedent is measured, as compared to
when the next nominal antecedent has to be cor-
rect.
Our evaluation shows that the IMS system
achieves a higher score for pronouns than the
Berkley system when immediate antecedents are
measured and has a higher Precision for pronouns
regarding the inferred antecedents. The Berkley
system performs best mainly due to Recall. For
e.g. personal pronouns (PRP), Berkley has the
</bodyText>
<footnote confidence="0.589603">
5 MUC+BCUB+CEAFE
</footnote>
<page confidence="0.989732">
3
</page>
<bodyText confidence="0.999876272727273">
following counts for the inferred antecedents:
tp=2687, wl=1935, fn=871, fp=389, while IMS
shows tp=2243, wl=1376, fn=1592, fp=287. This
indicates that the IMS Recall is lower because of
the high false negative count, rather than being due
to too many wrong linkages.
Finally, table 4 suggests that the IMS systems
performs significantly worse in the PERSON class
than the other systems and is outperformed by the
Stanford system in the ORG class, but performs
best in the GPE class.
</bodyText>
<table confidence="0.999964379310345">
R P F Fφ
PERSON (18.69%)
BERK ED 64.02 75.88 69.45 67.11
EM 63.60 66.29 64.92
IMS ED 45.66 51.69 48.48 52.74
EM 47.67 73.45 57.82
STAN ED 56.33 59.74 57.98 61.61
EM 53.84 84.37 65.73
GPE (13.28%)
BERK ED 73.21 77.36 75.23 75.71
EM 69.89 83.73 76.19
IMS ED 73.51 74.17 73.84 76.21
EM 69.94 90.04 78.73
STAN ED 70.24 76.62 73.29 75.24
EM 68.44 88.81 77.30
ORG (9.63%)
BERK ED 62.78 67.13 64.88 67.62
EM 66.87 74.78 70.60
IMS ED 44.98 54.30 49.20 56.85
EM 57.26 81.66 67.32
STAN ED 49.68 58.56 53.75 59.41
EM 57.25 79.05 66.41
TOTAL (100%)
BERK ED 58.65 53.19 55.79 63.41
EM 72.65 74.28 73.45
IMS ED 47.16 42.66 44.80 55.24
EM 65.88 79.40 72.01
STAN ED 48.62 41.40 44.72 55.27
EM 65.66 80.48 72.32
</table>
<tableCaption confidence="0.999644">
Table 4: Anchor mention based evaluation
</tableCaption>
<sectionHeader confidence="0.994149" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999361">
We have presented a simple evaluation framework
for coreference evaluation with higher level ap-
plications in mind. The metrics allow specific
performance measurement regarding different an-
tecedent requirements and any mention feature,
such as PoS type, lemma, or named entity class,
which can aid system development and compari-
son. Furthermore, the metrics do not alter system
rankings compared to the commonly used evalua-
tion approach6.
</bodyText>
<footnote confidence="0.990467">
6The scorers are freely available on our website:
http://www.cl.uzh.ch/research/coreferenceresolution.html
</footnote>
<page confidence="0.996559">
234
</page>
<sectionHeader confidence="0.989947" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999735933333333">
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Com-
put. Linguist., 34(1):1–34, March.
Anders Bj¨orkelund and Rich´ard Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Joint Conference on EMNLP and
CoNLL - Shared Task, CoNLL ’12, pages 49–55,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Chen Chen and Vincent Ng. 2013. Linguistically
aware coreference evaluation metrics. In Proceed-
ings of the 6th International Joint Conference on
Natural Language Processing, pages 1366–1374.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, Seattle, Washington, Oc-
tober. Association for Computational Linguistics.
Micha Elsner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
’11, pages 125–129, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Barbara J. Grosz, Scott Weinstein, and Aravind K.
Joshi. 1995. Centering: a framework for modeling
the local coherence of discourse. Comput. Linguist.,
21(2):203–225, June.
Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.
2011. Narrative schema as world knowledge for
coreference resolution. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, CONLL Shared Task
’11, pages 86–92, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
June-Jei Kuo and Hsin-Hsi Chen. 2007. Cross-
document event clustering using knowledge mining
from co-reference chains. Inf. Process. Manage.,
43(2):327–343, March.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ’12, pages 489–500, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4).
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of the Sixteenth Conference on Computational Natu-
ral Language Learning (CoNLL 2012), Jeju, Korea.
</reference>
<page confidence="0.998518">
235
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867644">
<title confidence="0.999935">Coreference Resolution Evaluation for Higher Level Applications</title>
<author confidence="0.993164">Don</author>
<affiliation confidence="0.998526">Unversity of Institute of Computational</affiliation>
<email confidence="0.877426">tuggener@cl.uzh.ch</email>
<abstract confidence="0.999682">This paper presents an evaluation framework for coreference resolution geared towards interpretability for higher-level applications. Three application scenarios for coreference resolution are outlined and metrics for them are devised. The metrics provide detailed system analysis and aim at measuring the potential benefit of using coreference systems in preprocessing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="4385" citStr="Barzilay and Lapata, 2008" startWordPosition="661" endWordPosition="664">il 26-30 2014. c�2014 Association for Computational Linguistics performance analysis is informative for higherlevel applications, as they might be specifically interested in certain mention types. 3 Application Scenarios Next, we will outline three higher-level application types which consume coreference and devise relevant metrics for them. 3.1 Models of entity distributions The first application scenario subsumes models that investigate distributions and patterns of entity occurrences in discourse. For example, Centering theory (Grosz et al., 1995) and the thereof derived entity grid model (Barzilay and Lapata, 2008; Elsner and Charniak, 2011) record transitions of grammatical functions that entities occur with in coherent discourse. These models can benefit from coreference resolution if entities are pronominalized or occur as a non-string matching nominal mentions. Another application which tracks sequences of entity occurrences is event sequence modeling. Such models investigate prototypical sequences of events to derive event schemes or templates of successive events (Lee et al., 2012; Irwin et al., 2011; Kuo and Chen, 2007). Here, coreference resolution can help link pronominalized arguments of even</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Comput. Linguist., 34(1):1–34, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Rich´ard Farkas</author>
</authors>
<title>Datadriven multilingual coreference resolution using resolver stacking.</title>
<date>2012</date>
<booktitle>In Joint Conference on EMNLP and CoNLL - Shared Task, CoNLL ’12,</booktitle>
<pages>49--55</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Bj¨orkelund, Farkas, 2012</marker>
<rawString>Anders Bj¨orkelund and Rich´ard Farkas. 2012. Datadriven multilingual coreference resolution using resolver stacking. In Joint Conference on EMNLP and CoNLL - Shared Task, CoNLL ’12, pages 49–55, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Chen</author>
<author>Vincent Ng</author>
</authors>
<title>Linguistically aware coreference evaluation metrics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1366--1374</pages>
<contexts>
<context position="982" citStr="Chen and Ng, 2013" startWordPosition="128" endWordPosition="131">d and metrics for them are devised. The metrics provide detailed system analysis and aim at measuring the potential benefit of using coreference systems in preprocessing. 1 Introduction Coreference Resolution is often described as an important preprocessing step for higher-level applications. However, the commonly used coreference evaluation metrics (MUC, BCUB, CEAF, BLANC) treat coreference as a generic clustering problem and perform cluster similarity measures to evaluate coreference system outputs. Mentions are seen as unsorted generic items rather than linearly ordered linguistic objects (Chen and Ng, 2013). This makes it arguably hard to interpret the scores and assess the potential benefit of using a coreference system as a preprocessing step. Therefore, this paper proposes an evaluation framework for coreference systems which aims at bridging the gap between coreference system development, evaluation, and higher level applications. For this purpose, we outline three types of application scenarios which coreference resolution can benefit and devise metrics for them which are easy to interpret and provide detailed system output analysis based on any available mention feature. 2 Basic Concepts L</context>
</contexts>
<marker>Chen, Ng, 2013</marker>
<rawString>Chen Chen and Vincent Ng. 2013. Linguistically aware coreference evaluation metrics. In Proceedings of the 6th International Joint Conference on Natural Language Processing, pages 1366–1374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Easy victories and uphill battles in coreference resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Seattle, Washington,</location>
<contexts>
<context position="2327" citStr="Durrett and Klein, 2013" startWordPosition="347" endWordPosition="350">) to compare mentions in a system output (the response) to the annotated mentions in a gold standard (the key). To stay close to the originally clear definitions of Recall and Precision in IR, Recall is aimed at identifying how many of the annotated key mentions are correctly resolved by a system, and Precision will measure the correctness of the returned system mentions. However, if we define Recall as tp, the detp+f n nominator will not include key mentions that have been put in the wrong coreference chain, and will not denote all mentions in the key. Therefore, borrowing nomenclature from (Durrett and Klein, 2013), we introduce an additional error class, wrong linkage (wl), which signifies key mentions that have been linked to incorrect antecedents. Recall can then be defined as tp tp+wl+fn and Precision as tp tp+wl+fp. Recall then extends over all key mentions, and Precision calculation includes all system mentions. Furthermore, including wrong linkage in the Recall equation prevents it from inflating compared to Precision when a large number of key mentions are incorrectly resolved. Evaluation is also sensitive to the anaphoricity detection problem. For example, an incorrectly resolved anaphoric “it”</context>
<context position="8108" citStr="Durrett and Klein, 2013" startWordPosition="1336" endWordPosition="1339">n in the response chain is the chain starter in a key chain, it means that the system has falsely deemed it to be anaphoric and we regard it as a false positive3. 3.2 Inferred local entities The second application scenario relies on coreference resolution to infer local nominal antecedents. For example, in Summarization, a target sentence may contain a pronoun which should be replaced by a nominal antecedent to avoid ambiguities and ensure coherence in the summary. Machine Trans1We disregard singleton entities, as it is not clear what benefit a higher level application could gain from them. 2(Durrett and Klein, 2013) call this error false new (FN). 3This error is called false anaphoric (FA) by (Durrett and Klein, 2013). 232 lation can benefit from pronoun resolution in language pairs where nouns have grammatical gender. In such language pairs, the gender of a pronoun antecedent has to be retrieved in the source language in order to insert the pronoun with the correct gender in the target language. In these applications, it is not sufficient to link pronouns to other pronouns of the same coreference chain because they do not help infer the underlying entity. Therefore, in our metric, we require the closest</context>
<context position="12881" citStr="Durrett and Klein, 2013" startWordPosition="2215" endWordPosition="2219">the entity mentions (EM) for only those entities which have been aligned through their anchors. The quality of the references to the anchor mentions are not directly comparable between systems, as their basis is not the same if the number of aligned anchors differs. Therefore, we calculate the harmonic mean of entity detection and entity mentions to enable direct system compari233 son. Where applicable, we obtain the named entity class of the entity and measure performance for each such class. 4 Evaluation We apply our metrics to three available coreference systems, namely the Berkley system (Durrett and Klein, 2013), the IMS system (Bj¨orkelund and Farkas, 2012), and the Stanford system (Lee et al., 2013) and their responses for the CoNLL 2012 shared task test set for English (Pradhan et al., 2012). Tables 3 and 4 report the results. Immediate antecedent Inferred antecedent R P F R P F BERK (Durrett and Klein, 2013) NOUN 45.06 47.06 46.04 55.54 60.37 57.85 PRP 67.66 64.87 66.24 48.92 53.62 51.16 PRP$ 74.49 74.32 74.41 61.95 66.80 64.28 TOTAL 56.60 56.91 56.76 52.94 58.04 55.37 IMS (Bj¨orkelund and Farkas, 2012) NOUN 38.01 43.09 40.39 46.90 54.96 50.61 PRP 69.06 68.64 68.85 43.04 57.42 49.20 PRP$ 72.57 72</context>
</contexts>
<marker>Durrett, Klein, 2013</marker>
<rawString>Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Seattle, Washington, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>Extending the entity grid with entity-specific features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>125--129</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4413" citStr="Elsner and Charniak, 2011" startWordPosition="665" endWordPosition="668">iation for Computational Linguistics performance analysis is informative for higherlevel applications, as they might be specifically interested in certain mention types. 3 Application Scenarios Next, we will outline three higher-level application types which consume coreference and devise relevant metrics for them. 3.1 Models of entity distributions The first application scenario subsumes models that investigate distributions and patterns of entity occurrences in discourse. For example, Centering theory (Grosz et al., 1995) and the thereof derived entity grid model (Barzilay and Lapata, 2008; Elsner and Charniak, 2011) record transitions of grammatical functions that entities occur with in coherent discourse. These models can benefit from coreference resolution if entities are pronominalized or occur as a non-string matching nominal mentions. Another application which tracks sequences of entity occurrences is event sequence modeling. Such models investigate prototypical sequences of events to derive event schemes or templates of successive events (Lee et al., 2012; Irwin et al., 2011; Kuo and Chen, 2007). Here, coreference resolution can help link pronominalized arguments of events to their previous mention</context>
</contexts>
<marker>Elsner, Charniak, 2011</marker>
<rawString>Micha Elsner and Eugene Charniak. 2011. Extending the entity grid with entity-specific features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 125–129, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Scott Weinstein</author>
<author>Aravind K Joshi</author>
</authors>
<title>Centering: a framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Comput. Linguist.,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="4316" citStr="Grosz et al., 1995" startWordPosition="650" endWordPosition="653">mputational Linguistics, pages 231–235, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics performance analysis is informative for higherlevel applications, as they might be specifically interested in certain mention types. 3 Application Scenarios Next, we will outline three higher-level application types which consume coreference and devise relevant metrics for them. 3.1 Models of entity distributions The first application scenario subsumes models that investigate distributions and patterns of entity occurrences in discourse. For example, Centering theory (Grosz et al., 1995) and the thereof derived entity grid model (Barzilay and Lapata, 2008; Elsner and Charniak, 2011) record transitions of grammatical functions that entities occur with in coherent discourse. These models can benefit from coreference resolution if entities are pronominalized or occur as a non-string matching nominal mentions. Another application which tracks sequences of entity occurrences is event sequence modeling. Such models investigate prototypical sequences of events to derive event schemes or templates of successive events (Lee et al., 2012; Irwin et al., 2011; Kuo and Chen, 2007). Here, </context>
</contexts>
<marker>Grosz, Weinstein, Joshi, 1995</marker>
<rawString>Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi. 1995. Centering: a framework for modeling the local coherence of discourse. Comput. Linguist., 21(2):203–225, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Irwin</author>
<author>Mamoru Komachi</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Narrative schema as world knowledge for coreference resolution.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, CONLL Shared Task ’11,</booktitle>
<pages>86--92</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4887" citStr="Irwin et al., 2011" startWordPosition="734" endWordPosition="737"> example, Centering theory (Grosz et al., 1995) and the thereof derived entity grid model (Barzilay and Lapata, 2008; Elsner and Charniak, 2011) record transitions of grammatical functions that entities occur with in coherent discourse. These models can benefit from coreference resolution if entities are pronominalized or occur as a non-string matching nominal mentions. Another application which tracks sequences of entity occurrences is event sequence modeling. Such models investigate prototypical sequences of events to derive event schemes or templates of successive events (Lee et al., 2012; Irwin et al., 2011; Kuo and Chen, 2007). Here, coreference resolution can help link pronominalized arguments of events to their previous mention and, thereby, maintain the event argument sequence. The outlined applications in this scenario primarily rely on the identification of correct and gapless sequences of entity occurrences. We can approximate this requirement in a metric by requiring the immediate antecedent of a mention in a response chain to be the immediate antecedent of that mention in the key chain. Note that this restriction deems mentions as incorrect, if they skip an antecedent but are resolved t</context>
</contexts>
<marker>Irwin, Komachi, Matsumoto, 2011</marker>
<rawString>Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto. 2011. Narrative schema as world knowledge for coreference resolution. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, CONLL Shared Task ’11, pages 86–92, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>June-Jei Kuo</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>Crossdocument event clustering using knowledge mining from co-reference chains.</title>
<date>2007</date>
<journal>Inf. Process. Manage.,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="4908" citStr="Kuo and Chen, 2007" startWordPosition="738" endWordPosition="741">theory (Grosz et al., 1995) and the thereof derived entity grid model (Barzilay and Lapata, 2008; Elsner and Charniak, 2011) record transitions of grammatical functions that entities occur with in coherent discourse. These models can benefit from coreference resolution if entities are pronominalized or occur as a non-string matching nominal mentions. Another application which tracks sequences of entity occurrences is event sequence modeling. Such models investigate prototypical sequences of events to derive event schemes or templates of successive events (Lee et al., 2012; Irwin et al., 2011; Kuo and Chen, 2007). Here, coreference resolution can help link pronominalized arguments of events to their previous mention and, thereby, maintain the event argument sequence. The outlined applications in this scenario primarily rely on the identification of correct and gapless sequences of entity occurrences. We can approximate this requirement in a metric by requiring the immediate antecedent of a mention in a response chain to be the immediate antecedent of that mention in the key chain. Note that this restriction deems mentions as incorrect, if they skip an antecedent but are resolved to another antecedent </context>
</contexts>
<marker>Kuo, Chen, 2007</marker>
<rawString>June-Jei Kuo and Hsin-Hsi Chen. 2007. Crossdocument event clustering using knowledge mining from co-reference chains. Inf. Process. Manage., 43(2):327–343, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Marta Recasens</author>
<author>Angel Chang</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Joint entity and event coreference resolution across documents.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>489--500</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4867" citStr="Lee et al., 2012" startWordPosition="730" endWordPosition="733"> in discourse. For example, Centering theory (Grosz et al., 1995) and the thereof derived entity grid model (Barzilay and Lapata, 2008; Elsner and Charniak, 2011) record transitions of grammatical functions that entities occur with in coherent discourse. These models can benefit from coreference resolution if entities are pronominalized or occur as a non-string matching nominal mentions. Another application which tracks sequences of entity occurrences is event sequence modeling. Such models investigate prototypical sequences of events to derive event schemes or templates of successive events (Lee et al., 2012; Irwin et al., 2011; Kuo and Chen, 2007). Here, coreference resolution can help link pronominalized arguments of events to their previous mention and, thereby, maintain the event argument sequence. The outlined applications in this scenario primarily rely on the identification of correct and gapless sequences of entity occurrences. We can approximate this requirement in a metric by requiring the immediate antecedent of a mention in a response chain to be the immediate antecedent of that mention in the key chain. Note that this restriction deems mentions as incorrect, if they skip an anteceden</context>
</contexts>
<marker>Lee, Recasens, Chang, Surdeanu, Jurafsky, 2012</marker>
<rawString>Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, and Dan Jurafsky. 2012. Joint entity and event coreference resolution across documents. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 489–500, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="12972" citStr="Lee et al., 2013" startWordPosition="2231" endWordPosition="2234">e quality of the references to the anchor mentions are not directly comparable between systems, as their basis is not the same if the number of aligned anchors differs. Therefore, we calculate the harmonic mean of entity detection and entity mentions to enable direct system compari233 son. Where applicable, we obtain the named entity class of the entity and measure performance for each such class. 4 Evaluation We apply our metrics to three available coreference systems, namely the Berkley system (Durrett and Klein, 2013), the IMS system (Bj¨orkelund and Farkas, 2012), and the Stanford system (Lee et al., 2013) and their responses for the CoNLL 2012 shared task test set for English (Pradhan et al., 2012). Tables 3 and 4 report the results. Immediate antecedent Inferred antecedent R P F R P F BERK (Durrett and Klein, 2013) NOUN 45.06 47.06 46.04 55.54 60.37 57.85 PRP 67.66 64.87 66.24 48.92 53.62 51.16 PRP$ 74.49 74.32 74.41 61.95 66.80 64.28 TOTAL 56.60 56.91 56.76 52.94 58.04 55.37 IMS (Bj¨orkelund and Farkas, 2012) NOUN 38.01 43.09 40.39 46.90 54.96 50.61 PRP 69.06 68.64 68.85 43.04 57.42 49.20 PRP$ 72.57 72.11 72.34 51.51 63.54 56.90 TOTAL 53.55 57.55 55.48 45.27 56.47 50.25 STAN (Lee et al., 201</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics, 39(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixteenth Conference on Computational Natural Language Learning (CoNLL</booktitle>
<location>Jeju,</location>
<contexts>
<context position="13067" citStr="Pradhan et al., 2012" startWordPosition="2248" endWordPosition="2251">ems, as their basis is not the same if the number of aligned anchors differs. Therefore, we calculate the harmonic mean of entity detection and entity mentions to enable direct system compari233 son. Where applicable, we obtain the named entity class of the entity and measure performance for each such class. 4 Evaluation We apply our metrics to three available coreference systems, namely the Berkley system (Durrett and Klein, 2013), the IMS system (Bj¨orkelund and Farkas, 2012), and the Stanford system (Lee et al., 2013) and their responses for the CoNLL 2012 shared task test set for English (Pradhan et al., 2012). Tables 3 and 4 report the results. Immediate antecedent Inferred antecedent R P F R P F BERK (Durrett and Klein, 2013) NOUN 45.06 47.06 46.04 55.54 60.37 57.85 PRP 67.66 64.87 66.24 48.92 53.62 51.16 PRP$ 74.49 74.32 74.41 61.95 66.80 64.28 TOTAL 56.60 56.91 56.76 52.94 58.04 55.37 IMS (Bj¨orkelund and Farkas, 2012) NOUN 38.01 43.09 40.39 46.90 54.96 50.61 PRP 69.06 68.64 68.85 43.04 57.42 49.20 PRP$ 72.57 72.11 72.34 51.51 63.54 56.90 TOTAL 53.55 57.55 55.48 45.27 56.47 50.25 STAN (Lee et al., 2013) NOUN 38.51 42.92 40.60 50.03 57.62 53.56 PRP 65.55 61.09 63.25 36.67 45.97 40.80 PRP$ 66.12 </context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. In Proceedings of the Sixteenth Conference on Computational Natural Language Learning (CoNLL 2012), Jeju, Korea.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>