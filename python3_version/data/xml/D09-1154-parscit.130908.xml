<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000425">
<title confidence="0.985177">
Discovery of Term Variation in Japanese Web Search Queries
</title>
<author confidence="0.99716">
Hisami Suzuki, Xiao Li, and Jianfeng Gao
</author>
<affiliation confidence="0.9495805">
Microsoft Research, Redmond
One Microsoft Way, Redmond, WA 98052 USA
</affiliation>
<email confidence="0.997361">
{hisamis,xiaol,jfgao}@microsoft.com
</email>
<sectionHeader confidence="0.994185" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99728424">
In this paper we address the problem of identi-
fying a broad range of term variations in Japa-
nese web search queries, where these varia-
tions pose a particularly thorny problem due to
the multiple character types employed in its
writing system. Our method extends the tech-
niques proposed for English spelling correc-
tion of web queries to handle a wider range of
term variants including spelling mistakes, va-
lid alternative spellings using multiple charac-
ter types, transliterations and abbreviations.
The core of our method is a statistical model
built on the MART algorithm (Friedman,
2001). We show that both string and semantic
similarity features contribute to identifying
term variation in web search queries; specifi-
cally, the semantic similarity features used in
our system are learned by mining user session
and click-through logs, and are useful not only
as model features but also in generating term
variation candidates efficiently. The proposed
method achieves 70% precision on the term
variation identification task with the recall
slightly higher than 60%, reducing the error
rate of a naïve baseline by 38%.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925678571429">
Identification of term variations is fundamental
to many NLP applications: words (or more gen-
erally, terms) are the building blocks of NLP ap-
plications, and any robust application must be
able to handle variations in the surface represen-
tation of terms, be it a spelling mistake, valid
spelling variation, or abbreviation. In search ap-
plications, term variations can be used for query
expansion, which generates additional query
terms for better matching with the terms in the
document set. Identifying term variations is also
useful in other scenarios where semantic equiva-
lence of terms is sought, as it represents a very
special case of paraphrase.
This paper addresses the problem of identify-
ing term variations in Japanese, specifically for
the purpose of query expansion in web search,
which appends additional terms to the original
query string for better retrieval quality. Query
expansion has been shown to be effective in im-
proving web search results in English, where dif-
ferent methods of generating the expansion terms
have been attempted, including relevance feed-
back (e.g., Salton and Buckley, 1990), correction
of spelling errors (e.g., Cucerzan and Brill, 2004),
stemming or lemmatization (e.g., Frakes, 1992),
use of manually- (e.g., Aitchison and Gilchrist,
1987) or automatically- (e.g., Rasmussen 1992)
constructed thesauri, and Latent Semantic Index-
ing (e.g., Deerwester et al, 1990). Though many
of these methods can be applied to Japanese
query expansion, there are unique problems
posed by Japanese search queries, the most chal-
lenging of which is that valid alternative spel-
lings of a word are extremely common due to the
multiple script types employed in the language.
For example, the word for &apos;protein&apos; can be spelled
as ftiv6!くLam, �r &apos;,W, X0W, ftiv0W
and so on, all pronounced tanpakushitsu but us-
ing combinations of different script types. We
give a detailed description of the problem posed
by the Japanese writing system in Section 2.
Though there has been previous work on ad-
dressing specific subsets of spelling alterations
within and across character types in Japanese,
there has not been any comprehensive solution
for the purpose of query expansion.
Our approach to Japanese query expansion is
unique in that we address the problem compre-
hensively: our method works independently of
the character types used, and targets a wide range
of term variations that are both orthographically
and semantically similar, including spelling er-
rors, valid alternative spellings, transliterations
and abbreviations. As described in Section 4, we
define the problem of term variation identifica-
</bodyText>
<page confidence="0.942939">
1484
</page>
<note confidence="0.996588">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1484–1492,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.99968435">
tion as a binary classification task, and build two
types of classifiers according to the maximum
entropy model (Berger et al., 1996) and the
MART algorithm (Friedman, 2001), where all
term similarity metrics are incorporated as fea-
tures and are jointly optimized. Another impor-
tant contribution of our approach is that we de-
rive our semantic similarity models by mining
user query logs, which has been explored for the
purposes of collecting related words (e.g., Jones
et al., 2006a), improving search results ranking
(e.g., Craswell and Szummer, 2007) and learning
query intention (e.g., Li et al., 2008), but not for
the task of collecting term variations. We show
that our semantic similarity models are not only
effective in the term variation identification task,
but also for generating candidates of term varia-
tions much more efficiently than the standard
method whose candidate generation is based on
edit distance metrics.
</bodyText>
<sectionHeader confidence="0.991581" genericHeader="introduction">
2 Term Variations in Japanese
</sectionHeader>
<bodyText confidence="0.9999665">
In this section we give a summary of the Japa-
nese writing system and the problem it poses for
identifying term variations, and define the prob-
lem we want to solve in this paper.
</bodyText>
<subsectionHeader confidence="0.993129">
2.1 The Japanese Writing System
</subsectionHeader>
<bodyText confidence="0.99998328125">
There are four different character types that are
used in Japanese text: hiragana, katakana, kanji
and Roman alphabet. Hiragana and katakana are
the two subtypes of kana characters, which are
syllabic character sets, each with about 50 basic
characters. There is a one-to-one correspondence
between hiragana and katakana characters, and,
as they are phonetic, they can be unambiguously
converted into a sequence of Roman characters.
For example, the word for &apos;mackerel&apos; is spelled in
hiragana as さば or in katakana as サバ, both of
which can be transcribed in Roman characters as
saba, which is how the word is pronounced.
Kanji characters, on the other hand, are ideo-
graphic and therefore numerous – more than
5,000 are in common usage. One difficulty in
handling Japanese kanji is that each character has
multiple pronunciations, and the correct pronun-
ciation is determined by the context in which the
character is used. For instance, the character 行 is
read as kou in the word 銀行 ginkou &apos;bank&apos;, gyou
in 行 &apos;column&apos;, and i or okona in 行った itta
&apos;went&apos; or okonatta &apos;done&apos; depending on the con-
text in which the word is used.1 Proper name
readings are particularly difficult to disambiguate,
as their pronunciation cannot be inferred from
the context (they tend to have the same grammat-
ical function) or from the dictionary (they tend to
be out-of-vocabulary). Therefore, in Japanese,
computing a pronunciation-based edit distance
metric is not straightforward, as it requires esti-
mating the readings of kanji characters.
</bodyText>
<subsectionHeader confidence="0.996959">
2.2 Term Variation by Character Type
</subsectionHeader>
<bodyText confidence="0.999966966666667">
Spelling variations are commonly observed both
within and across character types in Japanese.
Within a character type, the most prevalent is the
variation observed in katakana words. Katakana
is used to transliterate words from English and
other foreign languages, and therefore reflects
the variations in the sound adaptation from the
source language. For example, the word
&apos;spaghetti&apos; is transliterated into six different
forms (スパゲッティ supagetti, スパゲッティー
supagettii, スパゲッテイ supagettei, スパゲティ
supageti, ス パゲ テ ィーsupagetii, ス パゲ テ イ
supagetei) within a newspaper corpus (Masuya-
ma et al., 2004).
Spelling variants are also prevalent across
character types: in theory, a word can be spelled
using any of the character types, as we have seen
in the example for the word &apos;protein&apos; in Section 1.
Though there are certainly preferred character
types for spelling each word, variations are still
very common in Japanese text and search queries.
Alterations are particularly common among hira-
gana, katakana and kanji (e.g. さば~サバ~ 鯖 sa-
ba &apos;mackerel&apos;), and between katakana and Roman
alphabet (e.g. フェデックス fedekkusu fedex).
This latter case constitutes the problem of transli-
teration, which has been extensively studied in
the context of machine translation (e.g. Knight
and Graehl, 1998; Bilac and Tanaka, 2004; Brill
et al., 2001).
</bodyText>
<subsectionHeader confidence="0.995758">
2.3 Term Variation by Re-write Categories
</subsectionHeader>
<bodyText confidence="0.99994375">
Table 1 shows the re-write categories of related
terms observed in web query logs, drawing on
our own data analysis as well as on previous
work such as Jones et al. (2006a) and Okazaki et
al. (2008b). Categories 1 though 9 represent
strictly synonymous relations; in addition, terms
in Categories 1 through 5 are also similar ortho-
graphically or in pronunciation. Categories 10
</bodyText>
<footnote confidence="0.987654">
1 In a dictionary of 200K entries, we find that on average
each kanji character has 2.5 readings, with three characters
(直,生,空) with as many as 11 readings.
</footnote>
<page confidence="0.982448">
1485
</page>
<listItem confidence="0.941276705882353">
Categories Example in English Example in Japanese
1. Spelling mistake aple — apple グウグル guuguru — グーグル gu-guru &apos;google&apos;
2. Spelling variant color — colour さば—サバ—鯖; スパゲティ—スパゲッティー (Cf. Sec.2.2)
3. Inflection matrix — matrices 作る tsukuru &apos;make&apos; — 作った tsukutta &apos;made&apos;
4. Transliteration フェデックス — fedex &apos;Fedex&apos;
5. Abbreviation/ macintosh — mac 世界銀行 sekaiginkou — 世銀 segin &apos;World Bank&apos;; マクド
Acronym ナルド makudonarudo — マック makku &apos;McDonald&apos;s&apos;
6. Alias republican party — gop フランス furansu — 仏 futsu &apos;France&apos;
7. Translation パキスタン大使館 pakisutantaishikan — Pakistan embassy
8. Synonym carcinoma — cancer 暦 koyomi — カレンダー karendaa &apos;calendar&apos;
9. Abbreviation mini — mini cooper クロネコヤマト kuronekoyamato — クロネコ kuroneko
(user specific) (name of a delivery service company)
10. Generalization nike shoes — shoes シビック 部品 shibikku buhin &apos;Civic parts&apos; — 車 部品 ku-
ruma buhin &apos;car parts&apos;
11. Specification ipod — ipod nano 東京駅 toukyoueki &apos;Tokyo station&apos; — 東京駅時刻表 tou-
kyouekijikokuhyou &apos;Tokyo station timetable&apos;
12. Related windows — microsoft トヨタ toyota &apos;Toyota&apos; — ホンダ honda &apos;Honda&apos;
</listItem>
<tableCaption confidence="0.998421">
Table 1: Categories of Related Words Found in Web Search Logs
</tableCaption>
<bodyText confidence="0.998535853658537">
through 12, on the other hand, specify non-
synonymous relations.
Different sets out of these categories can be
useful for different purposes. For example, Jones
et al (2006a; 2006b) target all of these categories,
as their goal is to collect related terms as broadly
as possible for the application of sponsored
search, i.e., mapping search queries to a small
corpus of advertiser listings. Okazaki et al.
(2008b) define their task narrowly, to focusing
on spelling variants and inflection, as they aim at
building lexical resources for the specific domain
of medical text.
For web search, a conservative definition of
the task as dealing only with spelling errors has
been successful for English; a more general defi-
nition using related words for query expansion
has been a mixed blessing as it compromises re-
trieval precision. A comprehensive review on
this topic is provided by Baeza-Yates and Ribei-
ro-Neto (1999). In this paper, therefore, we adopt
a working definition of the term variation identi-
fication task as including Categories 1 through 5,
i.e., those that are synonymous and also similar
in spelling or in pronunciation.2 This definition is
reasonably narrow so as to make automatic dis-
covery of term variation pairs realistic, while
covering all common cases of term variation in
Japanese, including spelling variants and transli-
terations. It is also appropriate for the purpose of
query expansion: because term variation defined
in this manner is based on spelling or pronuncia-
tion similarity, their meaning and function tend
2 In reality, Category 3 (Inflection) is extremely rare in Jap-
anese web queries, because nouns do not inflect in Japanese,
and most queries are nominals.
to be completely equivalent, as opposed to Cate-
gories 6 through 9, where synonymy is more
context- or user-dependent. This will ensure that
the search results by query expansion will avoid
the problem of compromised precision.
</bodyText>
<sectionHeader confidence="0.999924" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999987413793104">
In information retrieval, the problem of vocabu-
lary mismatch between the query and the terms
in the document has been addressed in many
ways, as mentioned in Section 1, achieving vary-
ing degrees of success in the retrieval task. In
particular, our work is closely related to research
in spelling correction for English web queries
(e.g., Cucerzan and Brill, 2004; Ahmad and
Kondrak, 2005; Li et al., 2006; Chen et al., 2007).
Among these, Li et al. (2006) and Chen et al.
(2007) incorporate both string and semantic simi-
larity in their discriminative models of spelling
correction, similarly to our approach. In Li et al.
(2006), semantic similarity was computed as dis-
tributional similarity of the terms using query
strings in the log as context. Chen et al. (2007)
point out that this method suffers from the data
sparseness problem in that the statistics for rarer
terms are unreliable, and propose using web
search results as extended contextual information.
Their method, however, is expensive as it re-
quires web search results for each query-
candidate pair, and also because their candidate
set, generated using an edit distance function and
phonetic similarity from query log data, is im-
practically large and must be pruned by using a
language model. Our approach differs from these
methods in that we exploit user query logs to
derive semantic knowledge of terms, which is
</bodyText>
<page confidence="0.956353">
1486
</page>
<bodyText confidence="0.999969032258065">
used both for the purpose of generating a candi-
date set efficiently and as features in the term
variation identification model.
Acquiring semantic knowledge from a large
quantity of web query logs has become popular
in recent years. Some use only query strings and
their counts for learning word similarity (e.g.,
Sekine and Suzuki, 2007; Komachi and Suzuki,
2008), while others use additional information,
such as the user session information (i.e., a set of
queries issued by the same user within a time
frame, e.g., Jones et al., 2006a) or the URLs
clicked as a result of the query (e.g., Craswell
and Szummer, 2007; Li et al., 2008). This addi-
tional data serves as an approximation to the
meaning of the query; we use both user session
and click-through data for discovering term vari-
ations.
Our work also draws on some previous work
on string transformation, including spelling nor-
malization and transliteration. In addition to the
simple Levenshtein distance, we also use genera-
lized string-to-string edit distance (Brill and
Moore, 2000), which we trained on aligned kata-
kana-English word pairs in the same manner as
Brill et al. (2001). As mentioned in Section 2.2,
our work also tries to address the individual
problems targeted by such component technolo-
gies as Japanese katakana variation, English-to-
katakana transliteration and katakana-to-English
back-transliteration in a unified framework.
</bodyText>
<sectionHeader confidence="0.999063" genericHeader="method">
4 Discriminative Model of Identifying
</sectionHeader>
<subsectionHeader confidence="0.569886">
Term Variation
</subsectionHeader>
<bodyText confidence="0.999785">
Recent work in spelling correction (Ahmed and
Kondrak, 2005; Li et al., 2006; Chen et al., 2007)
and normalization (Okazaki et al., 2008b) formu-
lates the task in a discriminative framework:
</bodyText>
<equation confidence="0.71371">
𝑐∗ = argmax𝑐∈gen 𝑞 𝑃(𝑐|𝑞)
</equation>
<bodyText confidence="0.973418214285715">
This model consists of two components: gen(q)
generates a list of candidates C(q) for an input
query q, which are then ranked by the ranking
function P(c|q). In previous work, gen(q) is typi-
cally generated by using an edit distance function
or using a discriminative model trained for its
own purpose (Okazaki et al., 2008b), often in
combination with a pre-complied lexicon. In the
current work, we generate the list of candidates
by learning pairs of queries and their re-write
candidates automatically from query session and
click logs, which is far more robust and efficient
than using edit distance functions. We describe
our candidate generation method in detail in Sec-
tion 5.1.
Unlike the spelling correction and normaliza-
tion tasks, our goal is to identify term variations,
i.e., to determine whether each query-candidate
pair (q,c) constitutes a term variation or not. We
formulate this problem as a binary classification
task. There are various choices of classifiers for
such a task: we chose to build two types of clas-
sifiers that make a binary decision based on the
probability distribution P(c|q) over a set of fea-
ture functions fi(q,c). In maximum entropy
framework, this is defined as:
exp 𝑖= 1 𝜆𝑖𝑓𝑖 𝑐, 𝑞
𝑐 exp 𝐾
𝑖= 1 𝜆𝑖𝑓𝑖 𝑐, 𝑞
where λ1,..., λk are the feature weights. The op-
timal set of feature weights λ* is computed by
maximizing the log-likelihood of the training
data. We used stochastic gradient descent for
training the model with a Gaussian prior.
The second classifier is built on MART
(Friedman, 2001), which is a boosting algorithm.
At each boosting iteration, MART builds a re-
gression tree to model the functional gradient of
the cost function (which is cross entropy in our
case), evaluated on all training samples. MART
has three main parameters: M, the total number
of boosting iterations, L, the number of leaf
nodes for each regression tree, and v, the learning
rate. The optimal values of these parameters can
be chosen based on performance on a validation
set. In our experiments, we found that the per-
formance of the algorithm is relatively insensi-
tive to these parameters as long as they are in a
reasonable range: given the training set of a few
thousand samples or more, as in our experiments,
M=100, L=15, and v=0.1 usually give good per-
formance. Smaller trees and shrinkage may be
used if the training data set is smaller.
The classifiers output a binary decision ac-
cording to P(c|q): positive when P(c|q) &gt; 0.5 and
negative otherwise.
</bodyText>
<sectionHeader confidence="0.999906" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998954">
5.1 Candidate Generation
</subsectionHeader>
<bodyText confidence="0.9991303">
We used a set of Japanese query logs collected
over one year period in 2007 and 2008. More
specifically, we used two different extracts of log
data for generating term variation candidate
pairs:
Query session data. From raw query logs, we
extracted pairs of queries q1 and q2 such that they
are (i) issued by the same user; (ii) q2 follows
within 3 minutes of issuing q1; and (iii) q2 gener-
ated at least one click of a URL on the result
</bodyText>
<equation confidence="0.928239">
𝑃 𝑐 𝑞 =
</equation>
<page confidence="0.963858">
1487
</page>
<figureCaption confidence="0.999498">
Figure 1. Random Walk Algorithm
</figureCaption>
<bodyText confidence="0.987263810126582">
page while q1 did not result in any click. We then
scored each query pair (q1,q2) in this subset using
the log-likelihood ratio (LLR, Dunning, 1993)
between q1 and q2, which measures the mutual
dependence within the context of web search
queries (Jones et al., 2006a). After applying an
LLR threshold (LLR &gt; 15) and a count cutoff
(we used only the top 15 candidate q2 according
to the LLR value for each q1), we obtained a list
of 47,139,976 pairs for the 14,929,497 distinct q1,
on average generating 3.2 candidates per q13. We
took this set as comprising query-candidate pairs
for our model, along with the set extracted by
click-through data mining explained below.
Click-through data. This data extract is based
on the idea that if two queries led to the same
URLs being repeatedly clicked, we can reasona-
bly infer that the two queries are semantically
related. This is similar to computing the distribu-
tional similarity of terms given the context in
which they appear, where context is most often
defined as the words co-occurring with the terms.
Here, the clicked URLs serve as their context.
One challenge in using the URLs as contex-
tual information is that the contextual representa-
tion in this format is very sparse, as user clicks
are rare events. To learn query similarities from
incomplete click-through data, we used the ran-
dom walk algorithm similar to the one described
in Craswell and Szummer (2007). Figure 1 illu-
strates the basic idea: initially, document d3 has
a click-through link consisting of query q2 only;
the random walk algorithm adds the link from d3
to q1, which has a similar click pattern as q2.
Formally, we construct a click graph which is a
bipartite-graph representation of click-through
data. We use qi i=1
m to represent a set of query
nodes and {dj }n a set of document nodes. We
j=1
further define an m x n matrix W in which ele-
ment Wij represents the click count associated
with (qi, dj ). This matrix can be normalized to
be a query-to-document transition matrix, de-
3 We consider each query as an unbreakable term in this
paper, so term variation is equivalent to query variation.
noted by A, where Aij = P(1)(dj |qi) is the prob-
ability that qi transits to dj in one step. Similarly,
we can normalize the transpose of W to be a
document-to-query transition matrix, denoted by
B, where Bj,i = P(1)(qi |dj ). It is easy to see that
using A and B we can compute the probability of
transiting from any node to any other node in k
steps. In this work, we use a simple measure
which is the probability that one query transits to
another in two steps, and the corresponding
probability matrix is given by AB.
We used this probability and ranked all pairs
of queries in the same raw query logs as in the
query session data described above to generate
additional candidates for term variation pairs.
20,308,693 pairs were extracted after applying
the count cutoff of 5, generating on average 6.8
candidates for 2,973,036 unique queries.
It is interesting to note that these two data ex-
tracts are quite complementary: of all the data
generated, only 4.2% of the pairs were found in
both the session and click-through data. We be-
lieve that this diversity is attributable to the na-
ture of the extracts: the session data tends to col-
lect the term pairs that are issued by the same
user as a result of conscious re-writing effort,
such as typing error corrections and query speci-
fications (Categories 1 and 11 in Table 1), while
the click-though data collects the terms issued by
different users, possibly with different intentions,
and tends to include many spelling variants, syn-
onyms and queries with different specificity
(Categories 2, 8, 10 and 11).
</bodyText>
<subsectionHeader confidence="0.929889">
5.2 Features
</subsectionHeader>
<bodyText confidence="0.9999933125">
We used the same set of features for the maxi-
mum entropy and MART models, which are giv-
en in Table 2. They are divided into three main
types: string similarity features (1-16), semantic
similarity features (17, 18), and character type
features (19-39). Among the string similarity
features, half of them are based on Levenshtein
distance applied to surface forms (1-8), while the
other half is based on Levenshtein and string-to-
string edit distance metrics computed over the
Romanized form of the query, reflecting its pro-
nunciation. The conversion into Roman charac-
ters was done deterministically for kana charac-
ters using a simple mapping table. For Romaniz-
ing kanji characters, we used the function availa-
ble from Windows IFELanguage API (version
</bodyText>
<page confidence="0.985459">
1488
</page>
<bodyText confidence="0.83569">
String similarity features (16 real-valued features)
</bodyText>
<listItem confidence="0.958989333333333">
1. Lev distance on surface form
2. Lev distance on surface form normalized by q1 length
3. Lev distance on surface form using character equivalence table
4. Lev distance on surface form normalized by q1 length using character equivalence table
5. Lev distance on surface form w/o space
6. Lev distance on surface form normalized q1 length w/o space
7. Lev distance on surface form using character equivalence table w/o space
8. Lev distance on surface form normalized by q1 using character equivalence table w/o space
9. Lev distance on Roman
</listItem>
<figure confidence="0.907078363636364">
10. Lev distance on Roman normalized by q1 length
11. Alpha-beta edit distance on Roman
12. Alpha-beta edit distance on Roman normalized by q1 length
13. Lev distance on Roman w/o space
14. Lev distance on Roman normalized by q1 length w/o space
15. Alpha-beta edit distance on Roman w/o space
16. Alpha-beta edit distance on Roman normalized by q1 length w/o space
Features for semantic similarity (2 real-valued features)
17. LLR score
18. Click-though data probability
Character type features (21 binary features)
</figure>
<reference confidence="0.830611">
19. BothHira, 20. BothKata, 21. BothRoman, 22. BothKanji, 23. BothMixedNoKanji, 24. BothMixed,
25. HiraKata, 26. HiraKanji, 27. HiraRoman, 28. HiraMixedNoKanji, 29. HiraMixed, 30. KataKanji,
31.KataRoman, 32. KataMixedNoKanji, 33. KataMixed, 34. KanjiRoman, 35. KanjiMixedNoKanji,
36. KanjiMixed, 37. RomanMixedNoKanji, 38. RomanMixed, 39. MixedNoKanjiMixed
</reference>
<tableCaption confidence="0.979299">
Table 2: Classifier Features
</tableCaption>
<bodyText confidence="0.965601543478261">
2).4 The character equivalence table mentioned in
the features 3,4,7,8 is a table of 643 pairs of cha-
racters that are known to be equivalent, including
kanji allography (same kanji in different graphi-
cal styles). The alpha-beta edit distance (11, 12,
15, 16) is the string-to-string edit distance pro-
posed in Brill and Moore (2001), which we
trained over about 60K parallel English-to-
katakana Wikipedia title pairs, specifically to
capture the edit operations between English and
katakana words, which are different from the edit
operations between two Japanese words. Seman-
tic similarity features (17, 18) use the LLR score
from the session data, and the click-though pair
probability described in the subsection above.
Finally, features 19-39 capture the script types of
the query-candidate pair. We first defined six
basic character types for each query or candidate:
Hira (hiragana only), Kata (katakana only), Kanji
(kanji only), Roman (Roman alphabet only),
MixedNoKanji (includes more than one charac-
ter sets but not kanji) and Mixed (includes more
than one character sets with kanji). We then de-
rived 21 binary features by concatenating these
basic character type features for the combination
4 http://msdn.microsoft.com/en-us/library/ms970129.aspx.
We took the one-best conversion result from the API. The
conversion accuracy on a randomly sampled 100 kanji que-
ries was 89.6%.
of query and candidate strings. For example, if
both the query and candidate are in hiragana,
BothHira will be on; if the query is Mixed and
the candidate is Roman, then RomanMixed will
be on. Punctuation characters and Arabic numer-
als were treated as being transparent to character
type assignment. The addition of these features is
motivated by the assumption that appropriate
types of edit distance operations might depend
on different character types for the query-
candidate pair.
Since the dynamic ranges of different features
can be drastically different, we normalized each
feature dimension to a normal variable with zero-
mean and unit-variance. We then used the same
normalized features for both the maximum en-
tropy and the MART classifiers.
</bodyText>
<subsectionHeader confidence="0.996756">
5.3 Training and Evaluation Data
</subsectionHeader>
<bodyText confidence="0.999967363636364">
In order to generate the training data for the bi-
nary classification task, we randomly sampled
the query session (5,712 samples) and click-
through data (6,228 samples), and manually la-
beled each pair as positive or negative: the posi-
tive label was assigned when the term pair fell
into Categories 1 through 5 in Table 1; otherwise
it was assigned a negative label. Only 364 (6.4%)
and 244 (3.9%) of the samples were positive ex-
amples for the query session and click-through
data respectively, which makes the baseline per-
</bodyText>
<page confidence="0.98996">
1489
</page>
<bodyText confidence="0.999991130434783">
formance of the classifier quite high (always
predict the negative label – the accuracy will be
95%). Note, however, that these data sets include
term variation candidates much more efficiently
than a candidate set generated by the standard
method that uses an edit distance function with a
threshold. For example, there is a query-
candidate pair q=家風情報 kafuujouhou &apos;house-
style information&apos; c= 花 粉 情 報 kafunjouhou
&apos;pollen information&apos;) in the session data extract,
the first one of which is likely to be a mis-
spelling of the second.5 If we try to find candi-
dates for the query 家風情報 using an edit dis-
tance function naively with a threshold of 2 from
the queries in the log, we end up collecting a
large amount of completely irrelevant set of can-
didates such as 台風情報 taifuujouhou &apos;typhoon
information&apos;, 株情報 kabu jouhou &apos;stock informa-
tion&apos;, 降雨情報 kouu jouhou &apos;rainfall information&apos;
and so on – as many as 372 candidates were
found in the top one million most frequent que-
ries in the query log from the same period; for
rarer queries these numbers will only be worse.
Computing the edit distance based on the pro-
nunciation will not help here: the examples
above are within the edit distance of 2 even in
terms of Romanized strings.
Another advantage of generating the annotated
data using the result of query log data mining is
that the annotation process is less prone to sub-
jectivity than creating the annotation from
scratch. As Cucerzan and Brill (2004) point out,
the process of manually creating a spelling cor-
rection candidate is seriously flawed as the inten-
tion of the original query is completely lost: for
the query gogle, it is not clear out of context if
the user meant goggle, google, or gogle. Using
data mined from query logs solves this problem:
an annotator can safely assume that if gogle-
goggle appears in the candidate set, it is very
likely to be a valid term variation intended by the
user. This makes the annotation more robust and
efficient: the inter-annotator agreement rate for
2,000 query pairs by two annotators was 95.7%
on our data set, each annotator spending only
about two hours to annotate 2,000 pairs.
</bodyText>
<subsectionHeader confidence="0.845746">
5.4 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.998188333333333">
In order to compare the performance of two clas-
sifiers, we first built maximum entropy and
MART classifiers as described in Section 4 using
</bodyText>
<footnote confidence="0.928394333333333">
5 家風情報 does not make any sense in Japanese; on the
other hand, information about cedar pollen is commonly
sought after in spring due to widespread pollen allergy.
</footnote>
<table confidence="0.998467166666667">
Features Error rate (%)
A. All features (1-39 in Table 2) 3.07
B. String features only (1-16) 3.49
C. Surface string features only (1-8) 4.9
D. No semantic feats (1-16,19-39) 3.28
E. No character type feats (1-18) 3.5
</table>
<tableCaption confidence="0.900429">
Table 3: Results of Features Ablation Experiments
Using MART Model
</tableCaption>
<bodyText confidence="0.99990097826087">
all the features in Section 5.2. We run five expe-
riments using different random split of training
and test data: in each run, we used 10,000 sam-
ples for training and the remaining 1,940 samples
for testing, and measured the performance of the
two classifiers on the task of term variation iden-
tification in terms of the error rate i.e., 1–
accuracy. The results, average over five runs,
were 4.18 for the maximum entropy model, and
3.07 for the MART model. In all five runs, the
MART model outperformed the maximum en-
tropy classifier. This is not surprising given the
superior performance of tree-boosting algorithms
previously reported on similar classification
tasks (e.g., Hastie et al., 2001). In our task where
different types of features are likely to perform
better when they are combined (such as semantic
features and character types features), MART
would be a better fit than linear classifiers be-
cause the decision trees generated by MART op-
timally combines features in the local sense. In
what follows, we only discuss the results pro-
duced by MART for further experiments. Note
that the baseline classifier, which always predicts
the label to be negative, achieves 95.04% in ac-
curacy (or 4.96% error rate), which sounds ex-
tremely high, but in fact this baseline classifier is
useless for the purpose of collecting term varia-
tions, as it learns none of them by classifying all
samples as negative.
For evaluating the contribution of different
types of features in Section 5.2, we performed
feature ablation experiments using MART. Table
3 shows the results in error rate by various
MART classifiers using different combination of
features. The results in this table are also aver-
aged over five run with random training/test data
split. From Table 3, we can see that the best per-
formance was achieved by the model using all
features (line A of the table), which reduces the
baseline error rate (4.96%) by 38%. The im-
provement is statistically significant according to
the McNemar test (P &lt; 0.05). Models that use
string edit distance features only (lines B and C)
did not perform well: in particular, the model
that uses surface edit distance features only
</bodyText>
<page confidence="0.988887">
1490
</page>
<figureCaption confidence="0.999529">
Figure 2: Precision/Recall Curve of MART
</figureCaption>
<bodyText confidence="0.999976940476191">
without considering the term pronunciation per-
formed horribly (line C), which confirms the re-
sults reported by Jones et al. (2006b). However,
unlike Jones et al. (2006b), we see a positive
contribution of semantic features: the use of se-
mantic features reduced the error rate from 3.28
(line D) to 3.07 (line A), which is statistically
significant. This may be attributable to the nature
of semantic information used in our experiments:
we used the user session and click-though data to
extract semantic knowledge, which may be se-
mantically more specific than the probability of
word substitution in a query collection as a
whole, which is used by Jones et al. (2006b).
Finally, the character type features also contri-
buted to reducing the error rate (lines A and E).
In particular, the observation that the addition of
semantic features without the character type fea-
tures (comparing lines B and E) did not improve
the error rate indicates that the character type
features are also important in bringing about the
contribution of semantic features.
Figure 2 displays the test data precision/recall
curve of one of the runs of MART that uses all
features. The x-axis of the graph is the confi-
dence score of classification P(c|q), which was
set to 0.5 for the results in Table 3. At this confi-
dence, the model achieves 70% precision with
the recall slightly higher than 60%. In the graph,
we observe a familiar trade-off between preci-
sion and recall, which is useful for practical ap-
plications that may favor one over the other.
In order to find out where the weaknesses of
our classifiers lie, we performed a manual error
analysis on the same MART run whose results
are shown in Figure 2. Most of the classification
errors are false negatives, i.e., the model failed to
predict a case of term variation as such. The most
conspicuous error is the failure to capture ab-
breviations, such as failing to capture the altera-
tion between + * r&apos; * bZ juujoochuugakkou
&apos;Juujoo middle school&apos; and +*r&apos; juujoochuu,
which our edit distance-based features fail as the
length difference between a term and its abbrevi-
ation is significant. Addition of more targeted
features for this subclass of term variation (e.g.,
Okazaki et al., 2008a) is called for, and will be
considered in future work. Mistakes in the Ro-
manization of kanji characters were not always
punished as the query and the candidate string
may contain the same mistake, but when they
occurred in either in the query or the candidate
string (but not in both), the result was destruc-
tive: for example, we assigned a wrong Romani-
zation on *VM as suiginnakari ‘mercury lamp’,
as opposed to the correct suiginntou, which caus-
es the failure to capture the alteration with *V
rA suiginntou, (a misspelling of *VM). Using
N-best (N&gt;1) candidate pronunciations for kanji
terms or using all possible pronunciations for
kanji characters might reduce this type of error.
Finally, the features of our models are the edit
distance functions themselves, rather than the
individual edit rules or operations. Using these
individual operations as features in the classifica-
tion task directly has been shown to perform well
on spelling correction and normalization tasks
(e.g., Brill and Moore, 2000; Okazaki et al.,
2008b). Okazaki et al.’s (2008b) method of gene-
rating edit operations may not be viable for our
purposes, as they assume that the original and
candidate strings are very similar in their surface
representation – they target only spelling variants
and inflection in English. One interesting future
avenue to consider is to use the edit distance
functions in our current model to select a subset
of query-candidate pairs that are similar in terms
of these functions, separately for the surface and
Romanized forms, and use this subset to align
the character strings in these query-candidate
pairs as described in Brill and Moore (2000), and
add the edit operations derived in this manner to
the term variation identification classifier as fea-
tures.
</bodyText>
<sectionHeader confidence="0.99949" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998770333333333">
In this paper we have addressed the problem of
acquiring term variations in Japanese query logs
for the purpose of query expansion. We generate
term variation candidates efficiently by mining
query log data, and our best classifier, based on
the MART algorithm, can make use of both edit-
distance-based and semantic features, and can
identify term variation with the precision of 70%
at the recall slightly higher than 60%. Our next
</bodyText>
<figure confidence="0.998165">
100
40
90
80
70
60
50
30
20
10
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
precision
recall
confidence
</figure>
<page confidence="0.985347">
1491
</page>
<bodyText confidence="0.999924833333333">
goal is to use and evaluate the term variation col-
lected by the proposed method in an actual
search scenario, as well as improving the per-
formance of our classifier by using individual,
character-dependent edit operations as features in
classification.
</bodyText>
<sectionHeader confidence="0.999186" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999909350515464">
Ahmad, Farooq, and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs.
In Proceedings of EMNLP, pp.955-962.
Aitchison, J. and A. Gilchrist. 1987. Thesaurus Con-
struction: A Practical Manual. Second edition.
ASLIB, London.
Aramaki, Eiji, Takeshi Imai, Kengo Miyo, and Kazu-
hiko Ohe. 2008. Orthographic disambiguation in-
corporating transliterated probability. In Proceed-
ings of IJCNLP, pp.48-55.
Baeza-Yates, Ricardo, and Berthier Ribeiro-Neto.
1999. Modern Information Retrieval. Addison
Wesley.
Berger, A.L., S. A. D. Pietra, and V. J. D. Pietra. 1996.
A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1): 39-
72.
Bilac, Slaven, and Hozumi Tanaka. 2004. A hybrid
back-transliteration system for Japanese. In Pro-
ceedings of COLING, pp.597-603.
Brill, Eric, Gary Kacmarcik and Chris Brockett. 2001.
Automatically harvesting katakana-English term
pairs from search engine query logs. In Proceed-
ings of the Sixth Natural Language Processing Pa-
cific Rim Symposium (NLPRS-2001), pp.393-399.
Brill, Eric, and Robert C. Moore. 2000. An improved
error model for noisy channel spelling. In Proceed-
ings of ACL, pp.286-293.
Chen, Qing, Mu Li and Ming Zhou. 2007. Improving
query spelling correction using web search results.
In Proceedings of EMNLP-CoNLL, pp.181-189.
Craswell, Nick, and Martin Szummer. 2007. Random
walk on the click graph. In Proceedings of SIGIR.
Cucerzan, Silviu, and Eric Brill. 2004. Spelling cor-
rection as an iterative process that exploits the col-
lective knowledge of web users. In Proceedings of
EMNLP, pp.293-300.
Deerwester, S., S.T. Dumais, T. Landauer and
Harshman. 1990. Indexing by latent semantic anal-
ysis. In Journal of the American Society for Infor-
mation Science, 41(6): 391-407.
Dunning, Ted. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1): 61-74.
Frakes, W.B. 1992. Stemming algorithm. In
W.B.Frakes and R.Baeza-Yates (eds.), Information
Retrieval: Data Structure and Algorithms, Chapter
8. Prentice Hall.
Friedman, J. 2001. Greedy function approximation: a
gradient boosting machine. Annals of Statistics,
29(5).
Jones, Rosie, Benjamin Rey, Omid Madani and Wiley
Greiner. 2006a. Generating query substitutions. In
Proceedings of WWW, pp.387–396.
Jones, Rosie, Kevin Bartz, Pero Subasic and Benja-
min Rey. 2006b. Automatically generating related
aueries in Japanese. Language Resources and
Evaluation 40: 219-232.
Hastie, Trevor, Robert Tibshirani and Jerome Fried-
man. 2001. The Elements of Statistical Learning.
Springer.
Knight, Kevin, and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):
599-612.
Komachi, Mamoru and Hisami Suzuki. 2008. Mini-
mally supervised learning of semantic knowledge
from query logs. In Proceedings of IJCNLP,
pp.358–365.
Li, Mu, Muhua Zhu, Yang Zhang and Ming Zhou.
2006. Exploring distributional similarity based
models for query spelling correction. In Proceed-
ings of COLING/ACL, pp.1025-1032.
Li, Xiao, Ye-Yi Wang and Alex Acero. 2008. Learn-
ing query intent from regularized click graphs. In
Proceedings of SIGIR.
Masuyama, Takeshi, Satoshi Sekine, and Hiroshi Na-
kagawa. 2004. Automatic construction of Japanese
katakana variant list from large corpus. In Proceed-
ings COLING, pp.1214-1219.
Okazaki, Naoaki, Mitsuru Ishizuka and Jun’ichi Tsujii.
2008a. A discriminative approach to Japanese ab-
breviation extraction. In Proceedings of IJCNLP.
Okazaki, Naoaki, Yoshimasa Tsuruoka, Sophia Ana-
niadou and Jun’ichi Tsujii. 2008b. A discriminative
candidate generator for string transformations. In
Proceedings of EMNLP.
Rasmussen, E. 1992. Clustering algorithm. In
W.B.Frakes and R.Baeza-Yates (eds.), Information
Retrieval: Data Structure and Algorithms, Chapter
16. Prentice Hall.
Salton, G., and C. Buckley. 1990. Improving retrieval
performance by relevance feedback. Journal of the
American Society for Information Science, 41(4):
288-297.
Sekine, Satoshi, and Hisami Suzuki. 2007. Acquiring
ontological knowledge from query logs. In Pro-
ceedings of WWW, pp.1223-1224
</reference>
<page confidence="0.994536">
1492
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.851436">
<title confidence="0.996373">Discovery of Term Variation in Japanese Web Search Queries</title>
<author confidence="0.873717">Hisami Suzuki</author>
<author confidence="0.873717">Xiao Li</author>
<author confidence="0.873717">Jianfeng</author>
<affiliation confidence="0.995272">Microsoft Research,</affiliation>
<address confidence="0.999885">One Microsoft Way, Redmond, WA 98052</address>
<email confidence="0.999656">hisamis@microsoft.com</email>
<email confidence="0.999656">xiaol@microsoft.com</email>
<email confidence="0.999656">jfgao@microsoft.com</email>
<abstract confidence="0.999094423076923">In this paper we address the problem of identifying a broad range of term variations in Japanese web search queries, where these variations pose a particularly thorny problem due to the multiple character types employed in its writing system. Our method extends the techniques proposed for English spelling correction of web queries to handle a wider range of term variants including spelling mistakes, valid alternative spellings using multiple character types, transliterations and abbreviations. The core of our method is a statistical model built on the MART algorithm (Friedman, 2001). We show that both string and semantic similarity features contribute to identifying term variation in web search queries; specifically, the semantic similarity features used in our system are learned by mining user session and click-through logs, and are useful not only as model features but also in generating term variation candidates efficiently. The proposed method achieves 70% precision on the term variation identification task with the recall slightly higher than 60%, reducing the error rate of a naïve baseline by 38%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>BothKata</author>
</authors>
<date></date>
<journal>BothKanji, 23. BothMixedNoKanji,</journal>
<volume>24</volume>
<note>RomanMixed, 39. MixedNoKanjiMixed</note>
<marker>BothKata, </marker>
<rawString>19. BothHira, 20. BothKata, 21. BothRoman, 22. BothKanji, 23. BothMixedNoKanji, 24. BothMixed, 25. HiraKata, 26. HiraKanji, 27. HiraRoman, 28. HiraMixedNoKanji, 29. HiraMixed, 30. KataKanji, 31.KataRoman, 32. KataMixedNoKanji, 33. KataMixed, 34. KanjiRoman, 35. KanjiMixedNoKanji, 36. KanjiMixed, 37. RomanMixedNoKanji, 38. RomanMixed, 39. MixedNoKanjiMixed</rawString>
</citation>
<citation valid="true">
<authors>
<author>Farooq Ahmad</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Learning a spelling error model from search query logs.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>955--962</pages>
<contexts>
<context position="12199" citStr="Ahmad and Kondrak, 2005" startWordPosition="1934" endWordPosition="1937">completely equivalent, as opposed to Categories 6 through 9, where synonymy is more context- or user-dependent. This will ensure that the search results by query expansion will avoid the problem of compromised precision. 3 Related Work In information retrieval, the problem of vocabulary mismatch between the query and the terms in the document has been addressed in many ways, as mentioned in Section 1, achieving varying degrees of success in the retrieval task. In particular, our work is closely related to research in spelling correction for English web queries (e.g., Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007). Among these, Li et al. (2006) and Chen et al. (2007) incorporate both string and semantic similarity in their discriminative models of spelling correction, similarly to our approach. In Li et al. (2006), semantic similarity was computed as distributional similarity of the terms using query strings in the log as context. Chen et al. (2007) point out that this method suffers from the data sparseness problem in that the statistics for rarer terms are unreliable, and propose using web search results as extended contextual information. Their method, however, i</context>
</contexts>
<marker>Ahmad, Kondrak, 2005</marker>
<rawString>Ahmad, Farooq, and Grzegorz Kondrak. 2005. Learning a spelling error model from search query logs. In Proceedings of EMNLP, pp.955-962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Aitchison</author>
<author>A Gilchrist</author>
</authors>
<title>Thesaurus Construction: A Practical Manual. Second edition.</title>
<date>1987</date>
<publisher>ASLIB,</publisher>
<location>London.</location>
<contexts>
<context position="2631" citStr="Aitchison and Gilchrist, 1987" startWordPosition="403" endWordPosition="406"> This paper addresses the problem of identifying term variations in Japanese, specifically for the purpose of query expansion in web search, which appends additional terms to the original query string for better retrieval quality. Query expansion has been shown to be effective in improving web search results in English, where different methods of generating the expansion terms have been attempted, including relevance feedback (e.g., Salton and Buckley, 1990), correction of spelling errors (e.g., Cucerzan and Brill, 2004), stemming or lemmatization (e.g., Frakes, 1992), use of manually- (e.g., Aitchison and Gilchrist, 1987) or automatically- (e.g., Rasmussen 1992) constructed thesauri, and Latent Semantic Indexing (e.g., Deerwester et al, 1990). Though many of these methods can be applied to Japanese query expansion, there are unique problems posed by Japanese search queries, the most challenging of which is that valid alternative spellings of a word are extremely common due to the multiple script types employed in the language. For example, the word for &apos;protein&apos; can be spelled as ftiv6!くLam, �r &apos;,W, X0W, ftiv0W and so on, all pronounced tanpakushitsu but using combinations of different script types. We give a </context>
</contexts>
<marker>Aitchison, Gilchrist, 1987</marker>
<rawString>Aitchison, J. and A. Gilchrist. 1987. Thesaurus Construction: A Practical Manual. Second edition. ASLIB, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eiji Aramaki</author>
</authors>
<title>Takeshi Imai, Kengo Miyo, and Kazuhiko Ohe.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>48--55</pages>
<marker>Aramaki, 2008</marker>
<rawString>Aramaki, Eiji, Takeshi Imai, Kengo Miyo, and Kazuhiko Ohe. 2008. Orthographic disambiguation incorporating transliterated probability. In Proceedings of IJCNLP, pp.48-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Berthier Ribeiro-Neto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>1999</date>
<publisher>Addison Wesley.</publisher>
<contexts>
<context position="10790" citStr="Baeza-Yates and Ribeiro-Neto (1999)" startWordPosition="1706" endWordPosition="1710">r the application of sponsored search, i.e., mapping search queries to a small corpus of advertiser listings. Okazaki et al. (2008b) define their task narrowly, to focusing on spelling variants and inflection, as they aim at building lexical resources for the specific domain of medical text. For web search, a conservative definition of the task as dealing only with spelling errors has been successful for English; a more general definition using related words for query expansion has been a mixed blessing as it compromises retrieval precision. A comprehensive review on this topic is provided by Baeza-Yates and Ribeiro-Neto (1999). In this paper, therefore, we adopt a working definition of the term variation identification task as including Categories 1 through 5, i.e., those that are synonymous and also similar in spelling or in pronunciation.2 This definition is reasonably narrow so as to make automatic discovery of term variation pairs realistic, while covering all common cases of term variation in Japanese, including spelling variants and transliterations. It is also appropriate for the purpose of query expansion: because term variation defined in this manner is based on spelling or pronunciation similarity, their </context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Baeza-Yates, Ricardo, and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval. Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>39--72</pages>
<contexts>
<context position="4259" citStr="Berger et al., 1996" startWordPosition="657" endWordPosition="660">r method works independently of the character types used, and targets a wide range of term variations that are both orthographically and semantically similar, including spelling errors, valid alternative spellings, transliterations and abbreviations. As described in Section 4, we define the problem of term variation identifica1484 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1484–1492, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tion as a binary classification task, and build two types of classifiers according to the maximum entropy model (Berger et al., 1996) and the MART algorithm (Friedman, 2001), where all term similarity metrics are incorporated as features and are jointly optimized. Another important contribution of our approach is that we derive our semantic similarity models by mining user query logs, which has been explored for the purposes of collecting related words (e.g., Jones et al., 2006a), improving search results ranking (e.g., Craswell and Szummer, 2007) and learning query intention (e.g., Li et al., 2008), but not for the task of collecting term variations. We show that our semantic similarity models are not only effective in the</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, A.L., S. A. D. Pietra, and V. J. D. Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1): 39-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slaven Bilac</author>
<author>Hozumi Tanaka</author>
</authors>
<title>A hybrid back-transliteration system for Japanese.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>597--603</pages>
<contexts>
<context position="8152" citStr="Bilac and Tanaka, 2004" startWordPosition="1281" endWordPosition="1284">be spelled using any of the character types, as we have seen in the example for the word &apos;protein&apos; in Section 1. Though there are certainly preferred character types for spelling each word, variations are still very common in Japanese text and search queries. Alterations are particularly common among hiragana, katakana and kanji (e.g. さば~サバ~ 鯖 saba &apos;mackerel&apos;), and between katakana and Roman alphabet (e.g. フェデックス fedekkusu fedex). This latter case constitutes the problem of transliteration, which has been extensively studied in the context of machine translation (e.g. Knight and Graehl, 1998; Bilac and Tanaka, 2004; Brill et al., 2001). 2.3 Term Variation by Re-write Categories Table 1 shows the re-write categories of related terms observed in web query logs, drawing on our own data analysis as well as on previous work such as Jones et al. (2006a) and Okazaki et al. (2008b). Categories 1 though 9 represent strictly synonymous relations; in addition, terms in Categories 1 through 5 are also similar orthographically or in pronunciation. Categories 10 1 In a dictionary of 200K entries, we find that on average each kanji character has 2.5 readings, with three characters (直,生,空) with as many as 11 readings. </context>
</contexts>
<marker>Bilac, Tanaka, 2004</marker>
<rawString>Bilac, Slaven, and Hozumi Tanaka. 2004. A hybrid back-transliteration system for Japanese. In Proceedings of COLING, pp.597-603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Gary Kacmarcik</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically harvesting katakana-English term pairs from search engine query logs.</title>
<date>2001</date>
<booktitle>In Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium (NLPRS-2001),</booktitle>
<pages>393--399</pages>
<contexts>
<context position="8173" citStr="Brill et al., 2001" startWordPosition="1285" endWordPosition="1288">the character types, as we have seen in the example for the word &apos;protein&apos; in Section 1. Though there are certainly preferred character types for spelling each word, variations are still very common in Japanese text and search queries. Alterations are particularly common among hiragana, katakana and kanji (e.g. さば~サバ~ 鯖 saba &apos;mackerel&apos;), and between katakana and Roman alphabet (e.g. フェデックス fedekkusu fedex). This latter case constitutes the problem of transliteration, which has been extensively studied in the context of machine translation (e.g. Knight and Graehl, 1998; Bilac and Tanaka, 2004; Brill et al., 2001). 2.3 Term Variation by Re-write Categories Table 1 shows the re-write categories of related terms observed in web query logs, drawing on our own data analysis as well as on previous work such as Jones et al. (2006a) and Okazaki et al. (2008b). Categories 1 though 9 represent strictly synonymous relations; in addition, terms in Categories 1 through 5 are also similar orthographically or in pronunciation. Categories 10 1 In a dictionary of 200K entries, we find that on average each kanji character has 2.5 readings, with three characters (直,生,空) with as many as 11 readings. 1485 Categories Examp</context>
<context position="14340" citStr="Brill et al. (2001)" startWordPosition="2289" endWordPosition="2292"> et al., 2006a) or the URLs clicked as a result of the query (e.g., Craswell and Szummer, 2007; Li et al., 2008). This additional data serves as an approximation to the meaning of the query; we use both user session and click-through data for discovering term variations. Our work also draws on some previous work on string transformation, including spelling normalization and transliteration. In addition to the simple Levenshtein distance, we also use generalized string-to-string edit distance (Brill and Moore, 2000), which we trained on aligned katakana-English word pairs in the same manner as Brill et al. (2001). As mentioned in Section 2.2, our work also tries to address the individual problems targeted by such component technologies as Japanese katakana variation, English-tokatakana transliteration and katakana-to-English back-transliteration in a unified framework. 4 Discriminative Model of Identifying Term Variation Recent work in spelling correction (Ahmed and Kondrak, 2005; Li et al., 2006; Chen et al., 2007) and normalization (Okazaki et al., 2008b) formulates the task in a discriminative framework: 𝑐∗ = argmax𝑐∈gen 𝑞 𝑃(𝑐|𝑞) This model consists of two components: gen(q) generates a list of can</context>
</contexts>
<marker>Brill, Kacmarcik, Brockett, 2001</marker>
<rawString>Brill, Eric, Gary Kacmarcik and Chris Brockett. 2001. Automatically harvesting katakana-English term pairs from search engine query logs. In Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium (NLPRS-2001), pp.393-399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>286--293</pages>
<contexts>
<context position="14241" citStr="Brill and Moore, 2000" startWordPosition="2271" endWordPosition="2274">r session information (i.e., a set of queries issued by the same user within a time frame, e.g., Jones et al., 2006a) or the URLs clicked as a result of the query (e.g., Craswell and Szummer, 2007; Li et al., 2008). This additional data serves as an approximation to the meaning of the query; we use both user session and click-through data for discovering term variations. Our work also draws on some previous work on string transformation, including spelling normalization and transliteration. In addition to the simple Levenshtein distance, we also use generalized string-to-string edit distance (Brill and Moore, 2000), which we trained on aligned katakana-English word pairs in the same manner as Brill et al. (2001). As mentioned in Section 2.2, our work also tries to address the individual problems targeted by such component technologies as Japanese katakana variation, English-tokatakana transliteration and katakana-to-English back-transliteration in a unified framework. 4 Discriminative Model of Identifying Term Variation Recent work in spelling correction (Ahmed and Kondrak, 2005; Li et al., 2006; Chen et al., 2007) and normalization (Okazaki et al., 2008b) formulates the task in a discriminative framewo</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Brill, Eric, and Robert C. Moore. 2000. An improved error model for noisy channel spelling. In Proceedings of ACL, pp.286-293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Chen</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>Improving query spelling correction using web search results.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>181--189</pages>
<contexts>
<context position="12236" citStr="Chen et al., 2007" startWordPosition="1942" endWordPosition="1945">ories 6 through 9, where synonymy is more context- or user-dependent. This will ensure that the search results by query expansion will avoid the problem of compromised precision. 3 Related Work In information retrieval, the problem of vocabulary mismatch between the query and the terms in the document has been addressed in many ways, as mentioned in Section 1, achieving varying degrees of success in the retrieval task. In particular, our work is closely related to research in spelling correction for English web queries (e.g., Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007). Among these, Li et al. (2006) and Chen et al. (2007) incorporate both string and semantic similarity in their discriminative models of spelling correction, similarly to our approach. In Li et al. (2006), semantic similarity was computed as distributional similarity of the terms using query strings in the log as context. Chen et al. (2007) point out that this method suffers from the data sparseness problem in that the statistics for rarer terms are unreliable, and propose using web search results as extended contextual information. Their method, however, is expensive as it requires web search</context>
<context position="14751" citStr="Chen et al., 2007" startWordPosition="2347" endWordPosition="2350"> simple Levenshtein distance, we also use generalized string-to-string edit distance (Brill and Moore, 2000), which we trained on aligned katakana-English word pairs in the same manner as Brill et al. (2001). As mentioned in Section 2.2, our work also tries to address the individual problems targeted by such component technologies as Japanese katakana variation, English-tokatakana transliteration and katakana-to-English back-transliteration in a unified framework. 4 Discriminative Model of Identifying Term Variation Recent work in spelling correction (Ahmed and Kondrak, 2005; Li et al., 2006; Chen et al., 2007) and normalization (Okazaki et al., 2008b) formulates the task in a discriminative framework: 𝑐∗ = argmax𝑐∈gen 𝑞 𝑃(𝑐|𝑞) This model consists of two components: gen(q) generates a list of candidates C(q) for an input query q, which are then ranked by the ranking function P(c|q). In previous work, gen(q) is typically generated by using an edit distance function or using a discriminative model trained for its own purpose (Okazaki et al., 2008b), often in combination with a pre-complied lexicon. In the current work, we generate the list of candidates by learning pairs of queries and their re-write </context>
</contexts>
<marker>Chen, Li, Zhou, 2007</marker>
<rawString>Chen, Qing, Mu Li and Ming Zhou. 2007. Improving query spelling correction using web search results. In Proceedings of EMNLP-CoNLL, pp.181-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Craswell</author>
<author>Martin Szummer</author>
</authors>
<title>Random walk on the click graph.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="4679" citStr="Craswell and Szummer, 2007" startWordPosition="723" endWordPosition="726">essing, pages 1484–1492, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tion as a binary classification task, and build two types of classifiers according to the maximum entropy model (Berger et al., 1996) and the MART algorithm (Friedman, 2001), where all term similarity metrics are incorporated as features and are jointly optimized. Another important contribution of our approach is that we derive our semantic similarity models by mining user query logs, which has been explored for the purposes of collecting related words (e.g., Jones et al., 2006a), improving search results ranking (e.g., Craswell and Szummer, 2007) and learning query intention (e.g., Li et al., 2008), but not for the task of collecting term variations. We show that our semantic similarity models are not only effective in the term variation identification task, but also for generating candidates of term variations much more efficiently than the standard method whose candidate generation is based on edit distance metrics. 2 Term Variations in Japanese In this section we give a summary of the Japanese writing system and the problem it poses for identifying term variations, and define the problem we want to solve in this paper. 2.1 The Japa</context>
<context position="13815" citStr="Craswell and Szummer, 2007" startWordPosition="2204" endWordPosition="2207">is 1486 used both for the purpose of generating a candidate set efficiently and as features in the term variation identification model. Acquiring semantic knowledge from a large quantity of web query logs has become popular in recent years. Some use only query strings and their counts for learning word similarity (e.g., Sekine and Suzuki, 2007; Komachi and Suzuki, 2008), while others use additional information, such as the user session information (i.e., a set of queries issued by the same user within a time frame, e.g., Jones et al., 2006a) or the URLs clicked as a result of the query (e.g., Craswell and Szummer, 2007; Li et al., 2008). This additional data serves as an approximation to the meaning of the query; we use both user session and click-through data for discovering term variations. Our work also draws on some previous work on string transformation, including spelling normalization and transliteration. In addition to the simple Levenshtein distance, we also use generalized string-to-string edit distance (Brill and Moore, 2000), which we trained on aligned katakana-English word pairs in the same manner as Brill et al. (2001). As mentioned in Section 2.2, our work also tries to address the individua</context>
<context position="19286" citStr="Craswell and Szummer (2007)" startWordPosition="3124" endWordPosition="3127">icked, we can reasonably infer that the two queries are semantically related. This is similar to computing the distributional similarity of terms given the context in which they appear, where context is most often defined as the words co-occurring with the terms. Here, the clicked URLs serve as their context. One challenge in using the URLs as contextual information is that the contextual representation in this format is very sparse, as user clicks are rare events. To learn query similarities from incomplete click-through data, we used the random walk algorithm similar to the one described in Craswell and Szummer (2007). Figure 1 illustrates the basic idea: initially, document d3 has a click-through link consisting of query q2 only; the random walk algorithm adds the link from d3 to q1, which has a similar click pattern as q2. Formally, we construct a click graph which is a bipartite-graph representation of click-through data. We use qi i=1 m to represent a set of query nodes and {dj }n a set of document nodes. We j=1 further define an m x n matrix W in which element Wij represents the click count associated with (qi, dj ). This matrix can be normalized to be a query-to-document transition matrix, de3 We con</context>
</contexts>
<marker>Craswell, Szummer, 2007</marker>
<rawString>Craswell, Nick, and Martin Szummer. 2007. Random walk on the click graph. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>Eric Brill</author>
</authors>
<title>Spelling correction as an iterative process that exploits the collective knowledge of web users.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>293--300</pages>
<contexts>
<context position="2527" citStr="Cucerzan and Brill, 2004" startWordPosition="389" endWordPosition="392"> where semantic equivalence of terms is sought, as it represents a very special case of paraphrase. This paper addresses the problem of identifying term variations in Japanese, specifically for the purpose of query expansion in web search, which appends additional terms to the original query string for better retrieval quality. Query expansion has been shown to be effective in improving web search results in English, where different methods of generating the expansion terms have been attempted, including relevance feedback (e.g., Salton and Buckley, 1990), correction of spelling errors (e.g., Cucerzan and Brill, 2004), stemming or lemmatization (e.g., Frakes, 1992), use of manually- (e.g., Aitchison and Gilchrist, 1987) or automatically- (e.g., Rasmussen 1992) constructed thesauri, and Latent Semantic Indexing (e.g., Deerwester et al, 1990). Though many of these methods can be applied to Japanese query expansion, there are unique problems posed by Japanese search queries, the most challenging of which is that valid alternative spellings of a word are extremely common due to the multiple script types employed in the language. For example, the word for &apos;protein&apos; can be spelled as ftiv6!くLam, �r &apos;,W, X0W, fti</context>
<context position="12174" citStr="Cucerzan and Brill, 2004" startWordPosition="1930" endWordPosition="1933">eries are nominals. to be completely equivalent, as opposed to Categories 6 through 9, where synonymy is more context- or user-dependent. This will ensure that the search results by query expansion will avoid the problem of compromised precision. 3 Related Work In information retrieval, the problem of vocabulary mismatch between the query and the terms in the document has been addressed in many ways, as mentioned in Section 1, achieving varying degrees of success in the retrieval task. In particular, our work is closely related to research in spelling correction for English web queries (e.g., Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007). Among these, Li et al. (2006) and Chen et al. (2007) incorporate both string and semantic similarity in their discriminative models of spelling correction, similarly to our approach. In Li et al. (2006), semantic similarity was computed as distributional similarity of the terms using query strings in the log as context. Chen et al. (2007) point out that this method suffers from the data sparseness problem in that the statistics for rarer terms are unreliable, and propose using web search results as extended contextual information.</context>
</contexts>
<marker>Cucerzan, Brill, 2004</marker>
<rawString>Cucerzan, Silviu, and Eric Brill. 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users. In Proceedings of EMNLP, pp.293-300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>T Landauer</author>
<author>Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>In Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<pages>391--407</pages>
<contexts>
<context position="2754" citStr="Deerwester et al, 1990" startWordPosition="420" endWordPosition="423">eb search, which appends additional terms to the original query string for better retrieval quality. Query expansion has been shown to be effective in improving web search results in English, where different methods of generating the expansion terms have been attempted, including relevance feedback (e.g., Salton and Buckley, 1990), correction of spelling errors (e.g., Cucerzan and Brill, 2004), stemming or lemmatization (e.g., Frakes, 1992), use of manually- (e.g., Aitchison and Gilchrist, 1987) or automatically- (e.g., Rasmussen 1992) constructed thesauri, and Latent Semantic Indexing (e.g., Deerwester et al, 1990). Though many of these methods can be applied to Japanese query expansion, there are unique problems posed by Japanese search queries, the most challenging of which is that valid alternative spellings of a word are extremely common due to the multiple script types employed in the language. For example, the word for &apos;protein&apos; can be spelled as ftiv6!くLam, �r &apos;,W, X0W, ftiv0W and so on, all pronounced tanpakushitsu but using combinations of different script types. We give a detailed description of the problem posed by the Japanese writing system in Section 2. Though there has been previous work </context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Harshman, 1990</marker>
<rawString>Deerwester, S., S.T. Dumais, T. Landauer and Harshman. 1990. Indexing by latent semantic analysis. In Journal of the American Society for Information Science, 41(6): 391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>61--74</pages>
<contexts>
<context position="18018" citStr="Dunning, 1993" startWordPosition="2911" endWordPosition="2912"> Japanese query logs collected over one year period in 2007 and 2008. More specifically, we used two different extracts of log data for generating term variation candidate pairs: Query session data. From raw query logs, we extracted pairs of queries q1 and q2 such that they are (i) issued by the same user; (ii) q2 follows within 3 minutes of issuing q1; and (iii) q2 generated at least one click of a URL on the result 𝑃 𝑐 𝑞 = 1487 Figure 1. Random Walk Algorithm page while q1 did not result in any click. We then scored each query pair (q1,q2) in this subset using the log-likelihood ratio (LLR, Dunning, 1993) between q1 and q2, which measures the mutual dependence within the context of web search queries (Jones et al., 2006a). After applying an LLR threshold (LLR &gt; 15) and a count cutoff (we used only the top 15 candidate q2 according to the LLR value for each q1), we obtained a list of 47,139,976 pairs for the 14,929,497 distinct q1, on average generating 3.2 candidates per q13. We took this set as comprising query-candidate pairs for our model, along with the set extracted by click-through data mining explained below. Click-through data. This data extract is based on the idea that if two queries</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Dunning, Ted. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1): 61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Frakes</author>
</authors>
<title>Stemming algorithm.</title>
<date>1992</date>
<booktitle>In W.B.Frakes and R.Baeza-Yates (eds.), Information Retrieval: Data Structure and Algorithms, Chapter 8.</booktitle>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="2575" citStr="Frakes, 1992" startWordPosition="397" endWordPosition="398">ents a very special case of paraphrase. This paper addresses the problem of identifying term variations in Japanese, specifically for the purpose of query expansion in web search, which appends additional terms to the original query string for better retrieval quality. Query expansion has been shown to be effective in improving web search results in English, where different methods of generating the expansion terms have been attempted, including relevance feedback (e.g., Salton and Buckley, 1990), correction of spelling errors (e.g., Cucerzan and Brill, 2004), stemming or lemmatization (e.g., Frakes, 1992), use of manually- (e.g., Aitchison and Gilchrist, 1987) or automatically- (e.g., Rasmussen 1992) constructed thesauri, and Latent Semantic Indexing (e.g., Deerwester et al, 1990). Though many of these methods can be applied to Japanese query expansion, there are unique problems posed by Japanese search queries, the most challenging of which is that valid alternative spellings of a word are extremely common due to the multiple script types employed in the language. For example, the word for &apos;protein&apos; can be spelled as ftiv6!くLam, �r &apos;,W, X0W, ftiv0W and so on, all pronounced tanpakushitsu but </context>
</contexts>
<marker>Frakes, 1992</marker>
<rawString>Frakes, W.B. 1992. Stemming algorithm. In W.B.Frakes and R.Baeza-Yates (eds.), Information Retrieval: Data Structure and Algorithms, Chapter 8. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Friedman</author>
</authors>
<title>Greedy function approximation: a gradient boosting machine.</title>
<date>2001</date>
<journal>Annals of Statistics,</journal>
<volume>29</volume>
<issue>5</issue>
<contexts>
<context position="803" citStr="Friedman, 2001" startWordPosition="122" endWordPosition="123">o}@microsoft.com Abstract In this paper we address the problem of identifying a broad range of term variations in Japanese web search queries, where these variations pose a particularly thorny problem due to the multiple character types employed in its writing system. Our method extends the techniques proposed for English spelling correction of web queries to handle a wider range of term variants including spelling mistakes, valid alternative spellings using multiple character types, transliterations and abbreviations. The core of our method is a statistical model built on the MART algorithm (Friedman, 2001). We show that both string and semantic similarity features contribute to identifying term variation in web search queries; specifically, the semantic similarity features used in our system are learned by mining user session and click-through logs, and are useful not only as model features but also in generating term variation candidates efficiently. The proposed method achieves 70% precision on the term variation identification task with the recall slightly higher than 60%, reducing the error rate of a naïve baseline by 38%. 1 Introduction Identification of term variations is fundamental to m</context>
<context position="4299" citStr="Friedman, 2001" startWordPosition="665" endWordPosition="666"> types used, and targets a wide range of term variations that are both orthographically and semantically similar, including spelling errors, valid alternative spellings, transliterations and abbreviations. As described in Section 4, we define the problem of term variation identifica1484 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1484–1492, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tion as a binary classification task, and build two types of classifiers according to the maximum entropy model (Berger et al., 1996) and the MART algorithm (Friedman, 2001), where all term similarity metrics are incorporated as features and are jointly optimized. Another important contribution of our approach is that we derive our semantic similarity models by mining user query logs, which has been explored for the purposes of collecting related words (e.g., Jones et al., 2006a), improving search results ranking (e.g., Craswell and Szummer, 2007) and learning query intention (e.g., Li et al., 2008), but not for the task of collecting term variations. We show that our semantic similarity models are not only effective in the term variation identification task, but</context>
<context position="16399" citStr="Friedman, 2001" startWordPosition="2625" endWordPosition="2626">ry classification task. There are various choices of classifiers for such a task: we chose to build two types of classifiers that make a binary decision based on the probability distribution P(c|q) over a set of feature functions fi(q,c). In maximum entropy framework, this is defined as: exp 𝑖= 1 𝜆𝑖𝑓𝑖 𝑐, 𝑞 𝑐 exp 𝐾 𝑖= 1 𝜆𝑖𝑓𝑖 𝑐, 𝑞 where λ1,..., λk are the feature weights. The optimal set of feature weights λ* is computed by maximizing the log-likelihood of the training data. We used stochastic gradient descent for training the model with a Gaussian prior. The second classifier is built on MART (Friedman, 2001), which is a boosting algorithm. At each boosting iteration, MART builds a regression tree to model the functional gradient of the cost function (which is cross entropy in our case), evaluated on all training samples. MART has three main parameters: M, the total number of boosting iterations, L, the number of leaf nodes for each regression tree, and v, the learning rate. The optimal values of these parameters can be chosen based on performance on a validation set. In our experiments, we found that the performance of the algorithm is relatively insensitive to these parameters as long as they ar</context>
</contexts>
<marker>Friedman, 2001</marker>
<rawString>Friedman, J. 2001. Greedy function approximation: a gradient boosting machine. Annals of Statistics, 29(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Benjamin Rey</author>
<author>Omid Madani</author>
<author>Wiley Greiner</author>
</authors>
<title>Generating query substitutions.</title>
<date>2006</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>387--396</pages>
<contexts>
<context position="4608" citStr="Jones et al., 2006" startWordPosition="714" endWordPosition="717"> 2009 Conference on Empirical Methods in Natural Language Processing, pages 1484–1492, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tion as a binary classification task, and build two types of classifiers according to the maximum entropy model (Berger et al., 1996) and the MART algorithm (Friedman, 2001), where all term similarity metrics are incorporated as features and are jointly optimized. Another important contribution of our approach is that we derive our semantic similarity models by mining user query logs, which has been explored for the purposes of collecting related words (e.g., Jones et al., 2006a), improving search results ranking (e.g., Craswell and Szummer, 2007) and learning query intention (e.g., Li et al., 2008), but not for the task of collecting term variations. We show that our semantic similarity models are not only effective in the term variation identification task, but also for generating candidates of term variations much more efficiently than the standard method whose candidate generation is based on edit distance metrics. 2 Term Variations in Japanese In this section we give a summary of the Japanese writing system and the problem it poses for identifying term variatio</context>
<context position="8387" citStr="Jones et al. (2006" startWordPosition="1323" endWordPosition="1326"> and search queries. Alterations are particularly common among hiragana, katakana and kanji (e.g. さば~サバ~ 鯖 saba &apos;mackerel&apos;), and between katakana and Roman alphabet (e.g. フェデックス fedekkusu fedex). This latter case constitutes the problem of transliteration, which has been extensively studied in the context of machine translation (e.g. Knight and Graehl, 1998; Bilac and Tanaka, 2004; Brill et al., 2001). 2.3 Term Variation by Re-write Categories Table 1 shows the re-write categories of related terms observed in web query logs, drawing on our own data analysis as well as on previous work such as Jones et al. (2006a) and Okazaki et al. (2008b). Categories 1 though 9 represent strictly synonymous relations; in addition, terms in Categories 1 through 5 are also similar orthographically or in pronunciation. Categories 10 1 In a dictionary of 200K entries, we find that on average each kanji character has 2.5 readings, with three characters (直,生,空) with as many as 11 readings. 1485 Categories Example in English Example in Japanese 1. Spelling mistake aple — apple グウグル guuguru — グーグル gu-guru &apos;google&apos; 2. Spelling variant color — colour さば—サバ—鯖; スパゲティ—スパゲッティー (Cf. Sec.2.2) 3. Inflection matrix — matrices 作る tsu</context>
<context position="10046" citStr="Jones et al (2006" startWordPosition="1587" endWordPosition="1590">uronekoyamato — クロネコ kuroneko (user specific) (name of a delivery service company) 10. Generalization nike shoes — shoes シビック 部品 shibikku buhin &apos;Civic parts&apos; — 車 部品 kuruma buhin &apos;car parts&apos; 11. Specification ipod — ipod nano 東京駅 toukyoueki &apos;Tokyo station&apos; — 東京駅時刻表 toukyouekijikokuhyou &apos;Tokyo station timetable&apos; 12. Related windows — microsoft トヨタ toyota &apos;Toyota&apos; — ホンダ honda &apos;Honda&apos; Table 1: Categories of Related Words Found in Web Search Logs through 12, on the other hand, specify nonsynonymous relations. Different sets out of these categories can be useful for different purposes. For example, Jones et al (2006a; 2006b) target all of these categories, as their goal is to collect related terms as broadly as possible for the application of sponsored search, i.e., mapping search queries to a small corpus of advertiser listings. Okazaki et al. (2008b) define their task narrowly, to focusing on spelling variants and inflection, as they aim at building lexical resources for the specific domain of medical text. For web search, a conservative definition of the task as dealing only with spelling errors has been successful for English; a more general definition using related words for query expansion has been</context>
<context position="13734" citStr="Jones et al., 2006" startWordPosition="2189" endWordPosition="2192"> we exploit user query logs to derive semantic knowledge of terms, which is 1486 used both for the purpose of generating a candidate set efficiently and as features in the term variation identification model. Acquiring semantic knowledge from a large quantity of web query logs has become popular in recent years. Some use only query strings and their counts for learning word similarity (e.g., Sekine and Suzuki, 2007; Komachi and Suzuki, 2008), while others use additional information, such as the user session information (i.e., a set of queries issued by the same user within a time frame, e.g., Jones et al., 2006a) or the URLs clicked as a result of the query (e.g., Craswell and Szummer, 2007; Li et al., 2008). This additional data serves as an approximation to the meaning of the query; we use both user session and click-through data for discovering term variations. Our work also draws on some previous work on string transformation, including spelling normalization and transliteration. In addition to the simple Levenshtein distance, we also use generalized string-to-string edit distance (Brill and Moore, 2000), which we trained on aligned katakana-English word pairs in the same manner as Brill et al. </context>
<context position="18135" citStr="Jones et al., 2006" startWordPosition="2929" endWordPosition="2932">tracts of log data for generating term variation candidate pairs: Query session data. From raw query logs, we extracted pairs of queries q1 and q2 such that they are (i) issued by the same user; (ii) q2 follows within 3 minutes of issuing q1; and (iii) q2 generated at least one click of a URL on the result 𝑃 𝑐 𝑞 = 1487 Figure 1. Random Walk Algorithm page while q1 did not result in any click. We then scored each query pair (q1,q2) in this subset using the log-likelihood ratio (LLR, Dunning, 1993) between q1 and q2, which measures the mutual dependence within the context of web search queries (Jones et al., 2006a). After applying an LLR threshold (LLR &gt; 15) and a count cutoff (we used only the top 15 candidate q2 according to the LLR value for each q1), we obtained a list of 47,139,976 pairs for the 14,929,497 distinct q1, on average generating 3.2 candidates per q13. We took this set as comprising query-candidate pairs for our model, along with the set extracted by click-through data mining explained below. Click-through data. This data extract is based on the idea that if two queries led to the same URLs being repeatedly clicked, we can reasonably infer that the two queries are semantically related</context>
</contexts>
<marker>Jones, Rey, Madani, Greiner, 2006</marker>
<rawString>Jones, Rosie, Benjamin Rey, Omid Madani and Wiley Greiner. 2006a. Generating query substitutions. In Proceedings of WWW, pp.387–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Kevin Bartz</author>
<author>Pero Subasic</author>
<author>Benjamin Rey</author>
</authors>
<title>Automatically generating related aueries in Japanese.</title>
<date>2006</date>
<journal>Language Resources and Evaluation</journal>
<volume>40</volume>
<pages>219--232</pages>
<contexts>
<context position="4608" citStr="Jones et al., 2006" startWordPosition="714" endWordPosition="717"> 2009 Conference on Empirical Methods in Natural Language Processing, pages 1484–1492, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tion as a binary classification task, and build two types of classifiers according to the maximum entropy model (Berger et al., 1996) and the MART algorithm (Friedman, 2001), where all term similarity metrics are incorporated as features and are jointly optimized. Another important contribution of our approach is that we derive our semantic similarity models by mining user query logs, which has been explored for the purposes of collecting related words (e.g., Jones et al., 2006a), improving search results ranking (e.g., Craswell and Szummer, 2007) and learning query intention (e.g., Li et al., 2008), but not for the task of collecting term variations. We show that our semantic similarity models are not only effective in the term variation identification task, but also for generating candidates of term variations much more efficiently than the standard method whose candidate generation is based on edit distance metrics. 2 Term Variations in Japanese In this section we give a summary of the Japanese writing system and the problem it poses for identifying term variatio</context>
<context position="8387" citStr="Jones et al. (2006" startWordPosition="1323" endWordPosition="1326"> and search queries. Alterations are particularly common among hiragana, katakana and kanji (e.g. さば~サバ~ 鯖 saba &apos;mackerel&apos;), and between katakana and Roman alphabet (e.g. フェデックス fedekkusu fedex). This latter case constitutes the problem of transliteration, which has been extensively studied in the context of machine translation (e.g. Knight and Graehl, 1998; Bilac and Tanaka, 2004; Brill et al., 2001). 2.3 Term Variation by Re-write Categories Table 1 shows the re-write categories of related terms observed in web query logs, drawing on our own data analysis as well as on previous work such as Jones et al. (2006a) and Okazaki et al. (2008b). Categories 1 though 9 represent strictly synonymous relations; in addition, terms in Categories 1 through 5 are also similar orthographically or in pronunciation. Categories 10 1 In a dictionary of 200K entries, we find that on average each kanji character has 2.5 readings, with three characters (直,生,空) with as many as 11 readings. 1485 Categories Example in English Example in Japanese 1. Spelling mistake aple — apple グウグル guuguru — グーグル gu-guru &apos;google&apos; 2. Spelling variant color — colour さば—サバ—鯖; スパゲティ—スパゲッティー (Cf. Sec.2.2) 3. Inflection matrix — matrices 作る tsu</context>
<context position="10046" citStr="Jones et al (2006" startWordPosition="1587" endWordPosition="1590">uronekoyamato — クロネコ kuroneko (user specific) (name of a delivery service company) 10. Generalization nike shoes — shoes シビック 部品 shibikku buhin &apos;Civic parts&apos; — 車 部品 kuruma buhin &apos;car parts&apos; 11. Specification ipod — ipod nano 東京駅 toukyoueki &apos;Tokyo station&apos; — 東京駅時刻表 toukyouekijikokuhyou &apos;Tokyo station timetable&apos; 12. Related windows — microsoft トヨタ toyota &apos;Toyota&apos; — ホンダ honda &apos;Honda&apos; Table 1: Categories of Related Words Found in Web Search Logs through 12, on the other hand, specify nonsynonymous relations. Different sets out of these categories can be useful for different purposes. For example, Jones et al (2006a; 2006b) target all of these categories, as their goal is to collect related terms as broadly as possible for the application of sponsored search, i.e., mapping search queries to a small corpus of advertiser listings. Okazaki et al. (2008b) define their task narrowly, to focusing on spelling variants and inflection, as they aim at building lexical resources for the specific domain of medical text. For web search, a conservative definition of the task as dealing only with spelling errors has been successful for English; a more general definition using related words for query expansion has been</context>
<context position="13734" citStr="Jones et al., 2006" startWordPosition="2189" endWordPosition="2192"> we exploit user query logs to derive semantic knowledge of terms, which is 1486 used both for the purpose of generating a candidate set efficiently and as features in the term variation identification model. Acquiring semantic knowledge from a large quantity of web query logs has become popular in recent years. Some use only query strings and their counts for learning word similarity (e.g., Sekine and Suzuki, 2007; Komachi and Suzuki, 2008), while others use additional information, such as the user session information (i.e., a set of queries issued by the same user within a time frame, e.g., Jones et al., 2006a) or the URLs clicked as a result of the query (e.g., Craswell and Szummer, 2007; Li et al., 2008). This additional data serves as an approximation to the meaning of the query; we use both user session and click-through data for discovering term variations. Our work also draws on some previous work on string transformation, including spelling normalization and transliteration. In addition to the simple Levenshtein distance, we also use generalized string-to-string edit distance (Brill and Moore, 2000), which we trained on aligned katakana-English word pairs in the same manner as Brill et al. </context>
<context position="18135" citStr="Jones et al., 2006" startWordPosition="2929" endWordPosition="2932">tracts of log data for generating term variation candidate pairs: Query session data. From raw query logs, we extracted pairs of queries q1 and q2 such that they are (i) issued by the same user; (ii) q2 follows within 3 minutes of issuing q1; and (iii) q2 generated at least one click of a URL on the result 𝑃 𝑐 𝑞 = 1487 Figure 1. Random Walk Algorithm page while q1 did not result in any click. We then scored each query pair (q1,q2) in this subset using the log-likelihood ratio (LLR, Dunning, 1993) between q1 and q2, which measures the mutual dependence within the context of web search queries (Jones et al., 2006a). After applying an LLR threshold (LLR &gt; 15) and a count cutoff (we used only the top 15 candidate q2 according to the LLR value for each q1), we obtained a list of 47,139,976 pairs for the 14,929,497 distinct q1, on average generating 3.2 candidates per q13. We took this set as comprising query-candidate pairs for our model, along with the set extracted by click-through data mining explained below. Click-through data. This data extract is based on the idea that if two queries led to the same URLs being repeatedly clicked, we can reasonably infer that the two queries are semantically related</context>
</contexts>
<marker>Jones, Bartz, Subasic, Rey, 2006</marker>
<rawString>Jones, Rosie, Kevin Bartz, Pero Subasic and Benjamin Rey. 2006b. Automatically generating related aueries in Japanese. Language Resources and Evaluation 40: 219-232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
<author>Jerome Friedman</author>
</authors>
<title>The Elements of Statistical Learning.</title>
<date>2001</date>
<publisher>Springer.</publisher>
<marker>Hastie, Tibshirani, Friedman, 2001</marker>
<rawString>Hastie, Trevor, Robert Tibshirani and Jerome Friedman. 2001. The Elements of Statistical Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<volume>24</volume>
<issue>4</issue>
<pages>599--612</pages>
<contexts>
<context position="8128" citStr="Knight and Graehl, 1998" startWordPosition="1277" endWordPosition="1280">s: in theory, a word can be spelled using any of the character types, as we have seen in the example for the word &apos;protein&apos; in Section 1. Though there are certainly preferred character types for spelling each word, variations are still very common in Japanese text and search queries. Alterations are particularly common among hiragana, katakana and kanji (e.g. さば~サバ~ 鯖 saba &apos;mackerel&apos;), and between katakana and Roman alphabet (e.g. フェデックス fedekkusu fedex). This latter case constitutes the problem of transliteration, which has been extensively studied in the context of machine translation (e.g. Knight and Graehl, 1998; Bilac and Tanaka, 2004; Brill et al., 2001). 2.3 Term Variation by Re-write Categories Table 1 shows the re-write categories of related terms observed in web query logs, drawing on our own data analysis as well as on previous work such as Jones et al. (2006a) and Okazaki et al. (2008b). Categories 1 though 9 represent strictly synonymous relations; in addition, terms in Categories 1 through 5 are also similar orthographically or in pronunciation. Categories 10 1 In a dictionary of 200K entries, we find that on average each kanji character has 2.5 readings, with three characters (直,生,空) with </context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Knight, Kevin, and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4): 599-612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mamoru Komachi</author>
<author>Hisami Suzuki</author>
</authors>
<title>Minimally supervised learning of semantic knowledge from query logs.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>358--365</pages>
<contexts>
<context position="13561" citStr="Komachi and Suzuki, 2008" startWordPosition="2159" endWordPosition="2162"> distance function and phonetic similarity from query log data, is impractically large and must be pruned by using a language model. Our approach differs from these methods in that we exploit user query logs to derive semantic knowledge of terms, which is 1486 used both for the purpose of generating a candidate set efficiently and as features in the term variation identification model. Acquiring semantic knowledge from a large quantity of web query logs has become popular in recent years. Some use only query strings and their counts for learning word similarity (e.g., Sekine and Suzuki, 2007; Komachi and Suzuki, 2008), while others use additional information, such as the user session information (i.e., a set of queries issued by the same user within a time frame, e.g., Jones et al., 2006a) or the URLs clicked as a result of the query (e.g., Craswell and Szummer, 2007; Li et al., 2008). This additional data serves as an approximation to the meaning of the query; we use both user session and click-through data for discovering term variations. Our work also draws on some previous work on string transformation, including spelling normalization and transliteration. In addition to the simple Levenshtein distance</context>
</contexts>
<marker>Komachi, Suzuki, 2008</marker>
<rawString>Komachi, Mamoru and Hisami Suzuki. 2008. Minimally supervised learning of semantic knowledge from query logs. In Proceedings of IJCNLP, pp.358–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mu Li</author>
<author>Muhua Zhu</author>
<author>Yang Zhang</author>
<author>Ming Zhou</author>
</authors>
<title>Exploring distributional similarity based models for query spelling correction.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>1025--1032</pages>
<contexts>
<context position="12216" citStr="Li et al., 2006" startWordPosition="1938" endWordPosition="1941"> opposed to Categories 6 through 9, where synonymy is more context- or user-dependent. This will ensure that the search results by query expansion will avoid the problem of compromised precision. 3 Related Work In information retrieval, the problem of vocabulary mismatch between the query and the terms in the document has been addressed in many ways, as mentioned in Section 1, achieving varying degrees of success in the retrieval task. In particular, our work is closely related to research in spelling correction for English web queries (e.g., Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007). Among these, Li et al. (2006) and Chen et al. (2007) incorporate both string and semantic similarity in their discriminative models of spelling correction, similarly to our approach. In Li et al. (2006), semantic similarity was computed as distributional similarity of the terms using query strings in the log as context. Chen et al. (2007) point out that this method suffers from the data sparseness problem in that the statistics for rarer terms are unreliable, and propose using web search results as extended contextual information. Their method, however, is expensive as it</context>
<context position="14731" citStr="Li et al., 2006" startWordPosition="2343" endWordPosition="2346">n addition to the simple Levenshtein distance, we also use generalized string-to-string edit distance (Brill and Moore, 2000), which we trained on aligned katakana-English word pairs in the same manner as Brill et al. (2001). As mentioned in Section 2.2, our work also tries to address the individual problems targeted by such component technologies as Japanese katakana variation, English-tokatakana transliteration and katakana-to-English back-transliteration in a unified framework. 4 Discriminative Model of Identifying Term Variation Recent work in spelling correction (Ahmed and Kondrak, 2005; Li et al., 2006; Chen et al., 2007) and normalization (Okazaki et al., 2008b) formulates the task in a discriminative framework: 𝑐∗ = argmax𝑐∈gen 𝑞 𝑃(𝑐|𝑞) This model consists of two components: gen(q) generates a list of candidates C(q) for an input query q, which are then ranked by the ranking function P(c|q). In previous work, gen(q) is typically generated by using an edit distance function or using a discriminative model trained for its own purpose (Okazaki et al., 2008b), often in combination with a pre-complied lexicon. In the current work, we generate the list of candidates by learning pairs of queries</context>
</contexts>
<marker>Li, Zhu, Zhang, Zhou, 2006</marker>
<rawString>Li, Mu, Muhua Zhu, Yang Zhang and Ming Zhou. 2006. Exploring distributional similarity based models for query spelling correction. In Proceedings of COLING/ACL, pp.1025-1032.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Li</author>
<author>Ye-Yi Wang</author>
<author>Alex Acero</author>
</authors>
<title>Learning query intent from regularized click graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="4732" citStr="Li et al., 2008" startWordPosition="732" endWordPosition="735">and AFNLP tion as a binary classification task, and build two types of classifiers according to the maximum entropy model (Berger et al., 1996) and the MART algorithm (Friedman, 2001), where all term similarity metrics are incorporated as features and are jointly optimized. Another important contribution of our approach is that we derive our semantic similarity models by mining user query logs, which has been explored for the purposes of collecting related words (e.g., Jones et al., 2006a), improving search results ranking (e.g., Craswell and Szummer, 2007) and learning query intention (e.g., Li et al., 2008), but not for the task of collecting term variations. We show that our semantic similarity models are not only effective in the term variation identification task, but also for generating candidates of term variations much more efficiently than the standard method whose candidate generation is based on edit distance metrics. 2 Term Variations in Japanese In this section we give a summary of the Japanese writing system and the problem it poses for identifying term variations, and define the problem we want to solve in this paper. 2.1 The Japanese Writing System There are four different characte</context>
<context position="13833" citStr="Li et al., 2008" startWordPosition="2208" endWordPosition="2211">rpose of generating a candidate set efficiently and as features in the term variation identification model. Acquiring semantic knowledge from a large quantity of web query logs has become popular in recent years. Some use only query strings and their counts for learning word similarity (e.g., Sekine and Suzuki, 2007; Komachi and Suzuki, 2008), while others use additional information, such as the user session information (i.e., a set of queries issued by the same user within a time frame, e.g., Jones et al., 2006a) or the URLs clicked as a result of the query (e.g., Craswell and Szummer, 2007; Li et al., 2008). This additional data serves as an approximation to the meaning of the query; we use both user session and click-through data for discovering term variations. Our work also draws on some previous work on string transformation, including spelling normalization and transliteration. In addition to the simple Levenshtein distance, we also use generalized string-to-string edit distance (Brill and Moore, 2000), which we trained on aligned katakana-English word pairs in the same manner as Brill et al. (2001). As mentioned in Section 2.2, our work also tries to address the individual problems targete</context>
</contexts>
<marker>Li, Wang, Acero, 2008</marker>
<rawString>Li, Xiao, Ye-Yi Wang and Alex Acero. 2008. Learning query intent from regularized click graphs. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Masuyama</author>
<author>Satoshi Sekine</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Automatic construction of Japanese katakana variant list from large corpus.</title>
<date>2004</date>
<booktitle>In Proceedings COLING,</booktitle>
<pages>1214--1219</pages>
<contexts>
<context position="7445" citStr="Masuyama et al., 2004" startWordPosition="1168" endWordPosition="1172"> Variation by Character Type Spelling variations are commonly observed both within and across character types in Japanese. Within a character type, the most prevalent is the variation observed in katakana words. Katakana is used to transliterate words from English and other foreign languages, and therefore reflects the variations in the sound adaptation from the source language. For example, the word &apos;spaghetti&apos; is transliterated into six different forms (スパゲッティ supagetti, スパゲッティー supagettii, スパゲッテイ supagettei, スパゲティ supageti, ス パゲ テ ィーsupagetii, ス パゲ テ イ supagetei) within a newspaper corpus (Masuyama et al., 2004). Spelling variants are also prevalent across character types: in theory, a word can be spelled using any of the character types, as we have seen in the example for the word &apos;protein&apos; in Section 1. Though there are certainly preferred character types for spelling each word, variations are still very common in Japanese text and search queries. Alterations are particularly common among hiragana, katakana and kanji (e.g. さば~サバ~ 鯖 saba &apos;mackerel&apos;), and between katakana and Roman alphabet (e.g. フェデックス fedekkusu fedex). This latter case constitutes the problem of transliteration, which has been exte</context>
</contexts>
<marker>Masuyama, Sekine, Nakagawa, 2004</marker>
<rawString>Masuyama, Takeshi, Satoshi Sekine, and Hiroshi Nakagawa. 2004. Automatic construction of Japanese katakana variant list from large corpus. In Proceedings COLING, pp.1214-1219.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>Mitsuru Ishizuka and Jun’ichi Tsujii. 2008a. A discriminative approach to Japanese abbreviation extraction.</title>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<marker>Okazaki, </marker>
<rawString>Okazaki, Naoaki, Mitsuru Ishizuka and Jun’ichi Tsujii. 2008a. A discriminative approach to Japanese abbreviation extraction. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>Yoshimasa Tsuruoka, Sophia Ananiadou and Jun’ichi Tsujii. 2008b. A discriminative candidate generator for string transformations.</title>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Okazaki, </marker>
<rawString>Okazaki, Naoaki, Yoshimasa Tsuruoka, Sophia Ananiadou and Jun’ichi Tsujii. 2008b. A discriminative candidate generator for string transformations. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rasmussen</author>
</authors>
<title>Clustering algorithm.</title>
<date>1992</date>
<booktitle>In W.B.Frakes and R.Baeza-Yates (eds.), Information Retrieval: Data Structure and Algorithms, Chapter 16.</booktitle>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="2672" citStr="Rasmussen 1992" startWordPosition="410" endWordPosition="411">riations in Japanese, specifically for the purpose of query expansion in web search, which appends additional terms to the original query string for better retrieval quality. Query expansion has been shown to be effective in improving web search results in English, where different methods of generating the expansion terms have been attempted, including relevance feedback (e.g., Salton and Buckley, 1990), correction of spelling errors (e.g., Cucerzan and Brill, 2004), stemming or lemmatization (e.g., Frakes, 1992), use of manually- (e.g., Aitchison and Gilchrist, 1987) or automatically- (e.g., Rasmussen 1992) constructed thesauri, and Latent Semantic Indexing (e.g., Deerwester et al, 1990). Though many of these methods can be applied to Japanese query expansion, there are unique problems posed by Japanese search queries, the most challenging of which is that valid alternative spellings of a word are extremely common due to the multiple script types employed in the language. For example, the word for &apos;protein&apos; can be spelled as ftiv6!くLam, �r &apos;,W, X0W, ftiv0W and so on, all pronounced tanpakushitsu but using combinations of different script types. We give a detailed description of the problem posed</context>
</contexts>
<marker>Rasmussen, 1992</marker>
<rawString>Rasmussen, E. 1992. Clustering algorithm. In W.B.Frakes and R.Baeza-Yates (eds.), Information Retrieval: Data Structure and Algorithms, Chapter 16. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Improving retrieval performance by relevance feedback.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>4</issue>
<pages>288--297</pages>
<contexts>
<context position="2463" citStr="Salton and Buckley, 1990" startWordPosition="380" endWordPosition="383">t. Identifying term variations is also useful in other scenarios where semantic equivalence of terms is sought, as it represents a very special case of paraphrase. This paper addresses the problem of identifying term variations in Japanese, specifically for the purpose of query expansion in web search, which appends additional terms to the original query string for better retrieval quality. Query expansion has been shown to be effective in improving web search results in English, where different methods of generating the expansion terms have been attempted, including relevance feedback (e.g., Salton and Buckley, 1990), correction of spelling errors (e.g., Cucerzan and Brill, 2004), stemming or lemmatization (e.g., Frakes, 1992), use of manually- (e.g., Aitchison and Gilchrist, 1987) or automatically- (e.g., Rasmussen 1992) constructed thesauri, and Latent Semantic Indexing (e.g., Deerwester et al, 1990). Though many of these methods can be applied to Japanese query expansion, there are unique problems posed by Japanese search queries, the most challenging of which is that valid alternative spellings of a word are extremely common due to the multiple script types employed in the language. For example, the w</context>
</contexts>
<marker>Salton, Buckley, 1990</marker>
<rawString>Salton, G., and C. Buckley. 1990. Improving retrieval performance by relevance feedback. Journal of the American Society for Information Science, 41(4): 288-297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Hisami Suzuki</author>
</authors>
<title>Acquiring ontological knowledge from query logs.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>1223--1224</pages>
<contexts>
<context position="13534" citStr="Sekine and Suzuki, 2007" startWordPosition="2155" endWordPosition="2158">, generated using an edit distance function and phonetic similarity from query log data, is impractically large and must be pruned by using a language model. Our approach differs from these methods in that we exploit user query logs to derive semantic knowledge of terms, which is 1486 used both for the purpose of generating a candidate set efficiently and as features in the term variation identification model. Acquiring semantic knowledge from a large quantity of web query logs has become popular in recent years. Some use only query strings and their counts for learning word similarity (e.g., Sekine and Suzuki, 2007; Komachi and Suzuki, 2008), while others use additional information, such as the user session information (i.e., a set of queries issued by the same user within a time frame, e.g., Jones et al., 2006a) or the URLs clicked as a result of the query (e.g., Craswell and Szummer, 2007; Li et al., 2008). This additional data serves as an approximation to the meaning of the query; we use both user session and click-through data for discovering term variations. Our work also draws on some previous work on string transformation, including spelling normalization and transliteration. In addition to the </context>
</contexts>
<marker>Sekine, Suzuki, 2007</marker>
<rawString>Sekine, Satoshi, and Hisami Suzuki. 2007. Acquiring ontological knowledge from query logs. In Proceedings of WWW, pp.1223-1224</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>