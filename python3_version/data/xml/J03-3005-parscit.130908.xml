<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9977965">
Using the Web to Obtain Frequencies for
Unseen Bigrams
</title>
<author confidence="0.999841">
Frank Keller∗ Mirella Lapata†
</author>
<affiliation confidence="0.999284">
University of Edinburgh University of Sheffield
</affiliation>
<bodyText confidence="0.997593714285714">
This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen
in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun,
and verb-object bigrams from the Web by querying a search engine. We evaluate this method
by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a
reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correla-
tion between Web frequencies and frequencies recreated using class-based smoothing; (d) a good
performance of Web frequencies in a pseudodisambiguation task.
</bodyText>
<sectionHeader confidence="0.996971" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999546142857143">
In two recent papers, Banko and Brill (2001a, 2001b) criticize the fact that current NLP
algorithms are typically optimized, tested, and compared on fairly small data sets (cor-
pora with millions of words), even though data sets several orders of magnitude larger
are available, at least for some NLP tasks. Banko and Brill (2001a, 2001b) experiment
with context-sensitive spelling correction, a task for which large amounts of data can
be obtained straightforwardly, as no manual annotation is required. They demonstrate
that the learning algorithms typically used for spelling correction benefit significantly
from larger training sets, and that their performance shows no sign of reaching an
asymptote as the size of the training set increases.
Arguably, the largest data set that is available for NLP is the Web,1 which currently
consists of at least 3,033 million pages.2 Data retrieved from the Web therefore provide
enormous potential for training NLP algorithms, if Banko and Brill’s (2001a, 2001b)
findings for spelling corrections generalize; potential applications include tasks that
involve word n-grams and simple surface syntax. There is a small body of existing re-
search that tries to harness the potential of the Web for NLP. Grefenstette and Nioche
(2000) and Jones and Ghani (2000) use the Web to generate corpora for languages
for which electronic resources are scarce, and Resnik (1999) describes a method for
mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and
Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001)
proposes a method for resolving PP attachment ambiguities based on Web data, Mark-
ert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora,
</bodyText>
<affiliation confidence="0.7175515">
* School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk
† Department of Computer Science, 211 Portobello Street, Sheffield S1 4DP, UK.
</affiliation>
<email confidence="0.879">
E-mail: mlap@dcs.shef.ac.uk
</email>
<footnote confidence="0.984879714285714">
1 A reviewer points out that information providers such as Lexis Nexis (http://www.lexisnexis.com/)
might have databases that are even larger than the Web. Lexis Nexis provides full-text access to news
sources (including newspapers, wire services, and broadcast transcripts) and legal data (including case
law, codes, regulations, legal news, and law reviews).
2 This is the number of pages indexed by Google in December 2002, as estimated by Search Engine
Showdown (http://www.searchengineshowdown.com/).
© 2003 Association for Computational Linguistics
</footnote>
<note confidence="0.850518">
Computational Linguistics Volume 29, Number 3
</note>
<bodyText confidence="0.99905298">
and Zhu and Rosenfeld (2001) use Web-based n-gram counts to improve language
modeling.
A particularly interesting application is proposed by Grefenstette (1998), who uses
the Web for example-based machine translation. His task is to translate compounds
from French into English, with corpus evidence serving as a filter for candidate transla-
tions. An example is the French compound groupe de travail. There are five translations
of groupe and three translations for travail (in the dictionary that Grefenstette [1998]
is using), resulting in 15 possible candidate translations. Only one of them, namely,
work group, has a high corpus frequency, which makes it likely that this is the correct
translation into English. Grefenstette (1998) observes that this approach suffers from
an acute data sparseness problem if the counts are obtained from a conventional cor-
pus. However, as Grefenstette (1998) demonstrates, this problem can be overcome by
obtaining counts through Web searches, instead of relying on a corpus. Grefenstette
(1998) therefore effectively uses the Web as a way of obtaining counts for compounds
that are sparse in a given corpus.
Although this is an important initial result, it raises the question of the generality
of the proposed approach to overcoming data sparseness. It remains to be shown that
Web counts are generally useful for approximating data that are sparse or unseen
in a given corpus. It seems possible, for instance, that Grefenstette’s (1998) results
are limited to his particular task (filtering potential translations) or to his particular
linguistic phenomenon (noun-noun compounds). Another potential problem is the fact
that Web counts are far more noisy than counts obtained from a well-edited, carefully
balanced corpus. The effect of this noise on the usefulness of the Web counts is largely
unexplored.
Zhu and Rosenfeld (2001) use Web-based n-gram counts for language modeling.
They obtain a standard language model from a 103-million-word corpus and employ
Web-based counts to interpolate unreliable trigram estimates. They compare their in-
terpolated model against a baseline trigram language model (without interpolation)
and show that the interpolated model yields an absolute reduction in word error rate
of .93% over the baseline. Zhu and Rosenfeld’s (2001) results demonstrate that the
Web can be a source of data for language modeling. It is not clear, however, whether
their result carries over to tasks that employ linguistically meaningful word sequences
(e.g., head-modifier pairs or predicate-argument tuples) rather than simply adjacent
words. Furthermore, Zhu and Rosenfeld (2001) do not undertake any studies that eval-
uate Web frequencies directly (i.e., without a task such as language modeling). This
could be done, for instance, by comparing Web frequencies to corpus frequencies, or
to frequencies re-created by smoothing techniques.
The aim of the present article is to generalize Grefenstette’s (1998) and Zhu and
Rosenfeld’s (2001) findings by testing the hypothesis that the Web can be employed
to obtain frequencies for bigrams that are unseen in a given corpus. Instead of hav-
ing a particular task in mind (which would introduce a sampling bias), we rely on
sets of bigrams that are randomly selected from a corpus. We use a Web-based ap-
proach for bigrams that encode meaningful syntactic relations and obtain Web fre-
quencies not only for noun-noun bigrams, but also for adjective-noun and verb-object
bigrams. We thus explore whether this approach generalizes to different predicate-
argument combinations. We evaluate our Web counts in four ways: (a) comparison
with actual corpus frequencies from two different corpora, (b) comparison with human
plausibility judgments, (c) comparison with frequencies re-created using class-based
smoothing, and (d) performance in a pseudodisambiguation task on data sets from the
literature.
</bodyText>
<page confidence="0.99844">
460
</page>
<note confidence="0.976731">
Keller and Lapata Web Frequencies for Unseen Bigrams
</note>
<sectionHeader confidence="0.821498" genericHeader="keywords">
2. Obtaining Frequencies from the Web
</sectionHeader>
<subsectionHeader confidence="0.999173">
2.1 Sampling Bigrams from the BNC
</subsectionHeader>
<bodyText confidence="0.999982022727273">
The data sets used in the present experiment were obtained from the British National
Corpus (BNC) (see Burnard [1995]). The BNC is a large, synchronic corpus, consisting
of 90 million words of text and 10 million words of speech. The BNC is a balanced
corpus (i.e., it was compiled so as to represent a wide range of present day British
English). The written part includes samples from newspapers, magazines, books (both
academic and fiction), letters, and school and university essays, among other kinds of
text. The spoken part consists of spontaneous conversations, recorded from volunteers
balanced by age, region, and social class. Other samples of spoken language are also
included, ranging from business or government meetings to radio shows and phone-
ins. The corpus represents many different styles and varieties and is not limited to
any particular subject field, genre, or register.
For the present study, the BNC was used to extract data for three types of predicate-
argument relations. The first type is adjective-noun bigrams, in which we assume
that the noun is the predicate that takes the adjective as its argument.3 The second
predicate-argument type we investigated is noun-noun compounds. For these, we
assume that the rightmost noun is the predicate that selects the leftmost noun as
its argument (as compound nouns are generally right-headed in English). Third, we
included verb-object bigrams, in which the verb is the predicate that selects the object
as its argument. We considered only direct NP objects; the bigram consists of the verb
and the head noun of the object. For each of the three predicate-argument relations,
we gathered two data sets, one containing seen bigrams (i.e., bigrams that occur in
the BNC) and one with unseen bigrams (i.e., bigrams that do not occur in the BNC).
For the seen adjective-noun bigrams, we used the data of Lapata, McDonald, and
Keller (1999), who compiled a set of 90 bigrams as follows. First, 30 adjectives were
randomly chosen from a part-of-speech-tagged and lemmatized version of the BNC
so that each adjective had exactly two senses according to WordNet (Miller et al. 1990)
and was unambiguously tagged as “adjective” 98.6% of the time. Lapata, McDonald,
and Keller used the part-of-speech-tagged version that is made available with the BNC
and was tagged using CLAWS4 (Leech, Garside, and Bryant 1994), a probabilistic part-
of-speech tagger, with error rate ranging from 3% to 4%. The lemmatized version of
the corpus was obtained using Karp et al.’s (1992) morphological analyzer.
The 30 adjectives ranged in BNC frequency from 1.9 to 49.1 per million words;
that is, they covered the whole range from fairly infrequent to highly frequent items.
Gsearch (Corley et al. 2001), a chart parser that detects syntactic patterns in a tagged
corpus by exploiting a user-specified context-free grammar and a syntactic query, was
used to extract all nouns occurring in a head-modifier relationship with one of the
30 adjectives. Examples of the syntactic patterns the parser identified are given in Ta-
ble 1. In the case of adjectives modifying compound nouns, only sequences of two
nouns were included, and the rightmost-occurring noun was considered the head.
Bigrams involving proper nouns or low-frequency nouns (less than 10 per million
words) were discarded. This was necessary because the bigrams were used in exper-
iments involving native speakers (see Section 3.2), and we wanted to reduce the risk
of including words unfamiliar to the experimental subjects. For each adjective, the set
of bigrams was divided into three frequency bands based on an equal division of the
</bodyText>
<footnote confidence="0.9069255">
3 This assumption is disputed in the theoretical linguistics literature. For instance, Pollard and Sag (1994)
present an analysis in which there is mutual selection between the noun and the adjective.
</footnote>
<page confidence="0.996573">
461
</page>
<note confidence="0.803297">
Computational Linguistics Volume 29, Number 3
</note>
<tableCaption confidence="0.925796">
Table 1
</tableCaption>
<figure confidence="0.9554984">
Example of patterns used for the extraction of adjective-noun bigrams.
Pattern Example
A N educational material
A Adv N usual weekly classes
A N N environmental health officers
</figure>
<bodyText confidence="0.997710444444445">
range of log-transformed co-occurrence frequencies. Then one bigram was chosen at
random from each band. This procedure ensures that the whole range of frequencies
is represented in our sample.
Lapata, Keller, and McDonald (2001) compiled a set of 90 unseen adjective-noun
bigrams using the same 30 adjectives. For each adjective, Gsearch was used to com-
pile a list of all nouns that did not co-occur in a head-modifier relationship with the
adjective. Again, proper nouns and low-frequency nouns were discarded from this
list. Then each adjective was paired with three randomly chosen nouns from its list
of non-co-occurring nouns. Examples of seen and unseen adjective-noun bigrams are
shown in Table 2.
For the present study, we applied the procedure used by Lapata, McDonald, and
Keller (1999) and Lapata, Keller, and McDonald (2001) to noun-noun bigrams and to
verb-object bigrams, creating a set of 90 seen and 90 unseen bigrams for each type
of predicate-argument relationship. More specifically, 30 nouns and 30 verbs were
chosen according to the same criteria proposed for the adjective study (i.e., minimal
sense ambiguity and unambiguous part of speech). All nouns modifying one of the
30 nouns were extracted from the BNC using a heuristic from Lauer (1995) that looks
for consecutive pairs of nouns that are neither preceded nor succeeded by another
</bodyText>
<tableCaption confidence="0.894335333333333">
Table 2
Example stimuli for seen and unseen adjective-noun, noun-noun, and verb-object bigrams
(with log-transformed BNC counts).
</tableCaption>
<table confidence="0.9792642">
High Adjective-Noun Bigrams
Medium Low
animal 1.79 pleasure 1.38 application 0
verdict 3.91 secret 2.56 cat 0
girl 2.94 dog 1.6 lunch .69
</table>
<figure confidence="0.820714666666667">
Noun-Noun Bigrams
Adjective
hungry
guilty
naughty
Unseen
</figure>
<figureCaption confidence="0.501851333333333">
tradition, innovation, prey
system, wisdom, wartime
regime, rival, protocol
</figureCaption>
<figure confidence="0.298751083333333">
High
process
television
plasma
Medium Low
1.14 user .95 gala 0
1.53 satellite .95 edition 0
1.78 nylon 1.20 unit .60
Unseen Head Noun
collection, clause, coat directory
chain, care, vote broadcast
fund, theology, minute membrane
</figure>
<footnote confidence="0.702708230769231">
High Verb-Object Bigrams
Medium Low
obligation 3.87 goal 2.20 scripture .69
problem 1.79 effect 1.10 alarm 0
name 3.74 law 1.61 series 1.10
Verb
fulfill
intensify
choose
Unseen
participant, muscle, grade
score, quota, chest
lift, bride, listener
</footnote>
<page confidence="0.991389">
462
</page>
<note confidence="0.687195">
Keller and Lapata Web Frequencies for Unseen Bigrams
</note>
<bodyText confidence="0.9779725">
noun. Lauer’s heuristic (see (1)) effectively avoids identifying as two-word compounds
noun sequences that are part of a larger compound.
</bodyText>
<equation confidence="0.949911">
(1) C = {(w2,w3)  |w1w2w3w4; w1, w4 E� N; w2, w3 E N}
</equation>
<bodyText confidence="0.998580934782609">
Here, w1 w2 w3 w4 denotes the occurrence of a sequence of four words and N is the
set of words tagged as nouns in the corpus. C is the set of compounds identified by
Lauer’s (1995) heuristic.
Verb-object bigrams for the 30 preselected verbs were obtained from the BNC
using Cass (Abney 1996), a robust chunk parser designed for the shallow analysis
of noisy text. The parser recognizes chunks and simplex clauses (i.e., sequences of
nonrecursive clauses) using a regular expression grammar and a part-of-speech-tagged
corpus, without attempting to resolve attachment ambiguities. It comes with a large-
scale grammar for English and a built-in tool that extracts predicate-argument tuples
out of the parse trees that Cass produces.
The parser’s output was postprocessed to remove bracketing errors and errors in
identifying chunk categories that could potentially result in bigrams whose members
do not stand in a verb-argument relationship. Tuples containing verbs or nouns at-
tested in a verb-argument relationship only once were eliminated. Particle verbs were
retained only if the particle was adjacent to the verb (e.g., come off heroin). Verbs fol-
lowed by the preposition by and a head noun were considered instances of verb-subject
relations. It was assumed that PPs adjacent to the verb headed by any of the preposi-
tions in, to, for, with, on, at, from, of, into, through, and upon were prepositional objects (see
Lapata [2001] for details on the filtering process). Only nominal heads were retained
from the objects returned by the parser. As in the adjective study, noun-noun bigrams
and verb-object bigrams with proper nouns or low-frequency nouns (less than 10 per
million words) were discarded. The sets of noun-noun and verb-object bigrams were
divided into three frequency bands, and one bigram was chosen at random from each
band.
The procedure described by Lapata, Keller, and McDonald (2001) was followed for
creating sets of unseen noun-noun and verb-object bigrams: for each noun or verb, we
compiled a list of all nouns with which it did not co-occur within a noun-noun or verb-
object bigram in the BNC. Again, Lauer’s (1995) heuristic and Abney’s (1996) partial
parser were used to identify bigrams, and proper nouns and low-frequency nouns
were excluded. For each noun and verb, three bigrams were formed by pairing it with
a noun randomly selected from the set of the non-co-occurring nouns for that noun
or verb. Table 2 lists examples for the seen and unseen noun-noun and verb-object
bigrams generated by this procedure.
The extracted bigrams are in several respects an imperfect source of information
about adjective-noun or noun-noun modification and verb-object relations. First notice
that both Gsearch and Cass detect syntactic patterns on part-of-speech-tagged corpora.
This means that parsing errors are likely to result because of tagging mistakes. Second,
even if one assumes perfect tagging, the heuristic nature of our extraction procedures
may introduce additional noise or miss bigrams for which detailed structural infor-
mation would be needed.
For instance, our method for extracting adjective-noun pairs ignores cases in which
the adjective modifies noun sequences of length greater than two. The heuristic in (1)
considers only two-word noun sequences. Abney’s (1996) chunker recognizes basic
syntactic units without resolving attachment ambiguities or recovering missing infor-
mation (such as traces resulting from the movement of constituents). Although pars-
ing is robust and fast (since unlike in traditional parsers, no global optimization takes
</bodyText>
<page confidence="0.998963">
463
</page>
<note confidence="0.438539">
Computational Linguistics Volume 29, Number 3
</note>
<bodyText confidence="0.9974304">
place), the identified verb-argument relations are undoubtedly somewhat noisy, given
the errors inherent in the part-of-speech tagging and chunk recognition procedure.
When evaluated against manually annotated data, Abney’s (1996) parser identified
chunks with 87.9% precision and 87.1% recall. The parser further achieved a per-word
accuracy of 92.1% (where per-word accuracy includes the chunk category and chunk
length identified correctly).
Despite their imperfect output, heuristic methods for the extraction of syntactic
relations are relatively common in statistical NLP. Several statistical models employ
frequencies obtained from the output of partial parsers and other heuristic methods;
these include models for disambiguating the attachment site of prepositional phrases
(Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns
(Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the
induction of selectional preferences (Abney and Light 1999), methods for automati-
cally clustering words according to their distribution in particular syntactic contexts
(Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994;
Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee
1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways
for obtaining bigram frequencies that are potentially useful for such models despite
the fact that some of these bigrams are identified in a heuristic manner and may be
noisy.
</bodyText>
<subsectionHeader confidence="0.99997">
2.2 Sampling Bigrams from the NANTC
</subsectionHeader>
<bodyText confidence="0.999433206896552">
We also obtained corpus counts from a second corpus, the North American News
Text Corpus (NANTC). This corpus differs in several important respects from the
BNC. It is substantially larger, as it contains 350 million words of text. Also, it is not
a balanced corpus, as it contains material from only one genre, namely, news text.
However, the text originates from a variety of sources (Los Angeles Times, Washington
Post, New York Times News Syndicate, Reuters News Service, and Wall Street Journal).
Whereas the BNC covers British English, the NANTC covers American English. All
these differences mean that the NANTC provides a second, independent standard
against which to compare Web counts. At the same time the correlation found between
the counts obtained from the two corpora can serve as an upper limit for the correlation
that we can expect between corpus counts and Web counts.
The NANTC corpus was parsed using MINIPAR (Lin 1994, 2001), a broad-coverage
parser for English. MINIPAR employs a manually constructed grammar and a lexicon
derived from WordNet with the addition of proper names (130,000 entries in total).
Lexicon entries contain part-of-speech and subcategorization information. The gram-
mar is represented as a network of 35 nodes (i.e., grammatical categories) and 59 edges
(i.e., types of syntactic [dependency] relationships). MINIPAR employs a distributed-
chart parsing algorithm. Instead of a single chart, each node in the grammar network
maintains a chart containing partially built structures belonging to the grammatical
category represented by the node. Grammar rules are implemented as constraints as-
sociated with the nodes and edges.
The output of MINIPAR is a dependency tree that represents the dependency
relations between words in a sentence. Table 3 shows a subset of the dependencies
MINIPAR outputs for the sentence The fat cat ate the door mat. In contrast to Gsearch
and Cass, MINIPAR produces all possible parses for a given sentence. The parses are
ranked according to the product of the probabilities of their edges, and the most likely
parse is returned. Lin (1998) evaluated the parser on the SUSANNE corpus (Sampson
1995), a domain-independent corpus of British English, and achieved a recall of 79%
and precision of 89% on the dependency relations.
</bodyText>
<page confidence="0.999637">
464
</page>
<note confidence="0.921092">
Keller and Lapata Web Frequencies for Unseen Bigrams
</note>
<tableCaption confidence="0.995521">
Table 3
</tableCaption>
<table confidence="0.419659">
Examples of dependencies generated by MINIPAR for The fat cat ate the door mat.
Head Relation Modifier Description
cat N:det:Det the determiner of noun
cat N:mod:A fat adjective modifier of noun
eat V:subj:N cat subject of verb
eat V:obj:N mat object of verb
</table>
<bodyText confidence="0.98779505882353">
mat N:det:Det the determiner of noun
mat N:nn:N door prenominal modifier of noun
For our experiments, we concentrated solely on adjective-noun, noun-noun, and
verb object relations (denoted as N:mod:A, N:nn:N, and V:obj:N in Table 3). From the
syntactic analysis provided by the parser, we extracted all occurrences of bigrams that
were attested both in the BNC and the NANTC corpus. In this way, we obtained
NANTC frequency counts for the bigrams that we had randomly selected from the
BNC. Table 4 shows the NANTC counts for the set of seen bigrams from Table 2.
Because of the differences in the extraction methodology (chunking versus full
parsing) and the text genre (balanced corpus versus news text), we expected that
some BNC bigrams would not be attested in the NANTC corpus. More precisely, zero
frequencies were returned for 23 adjective-noun, 16 verb-noun, and 37 noun-noun
bigrams. The fact that more zero frequencies were observed for noun-noun bigrams
than for the other two types is perhaps not surprising considering the ease with which
novel compounds are created (Levi 1978). We adjusted the zero counts by setting
them to .5. This was necessary because all further analyses were carried out on log-
transformed frequencies (see Table 4).
</bodyText>
<tableCaption confidence="0.996378">
Table 4
</tableCaption>
<table confidence="0.797899">
Log-transformed NANTC counts for seen adjective-noun, noun-noun, and verb-object bigrams.
Adjective-Noun Bigrams
Adjective High Medium Low
hungry animal .90 pleasure -.30 application .60
guilty verdict 2.82 secret .95 cat -.30
naughty girl .69 dog -.30 lunch -.30
Noun-Noun Bigrams
High Medium Low Head Noun
process -.30 user -.30 gala -.30 directory
television 2.70 satellite -.30 edition -.30 broadcast
plasma -.30 nylon 0 unit 0 membrane
Verb-Object Bigrams
Verb High Medium Low
fulfill obligation 2.38 goal 2.04 scripture -.30
intensify problem 1.20 effect .60 alarm -.30
choose name 2.25 law .90 series .48
</table>
<page confidence="0.991777">
465
</page>
<note confidence="0.446294">
Computational Linguistics Volume 29, Number 3
</note>
<subsectionHeader confidence="0.999346">
2.3 Obtaining Web Counts
</subsectionHeader>
<bodyText confidence="0.999957714285714">
Web counts for bigrams were obtained using a simple heuristic based on queries to the
search engines AltaVista and Google. All search terms took into account the inflectional
morphology of nouns and verbs.
The search terms for verb-object bigrams matched not only cases in which the
object was directly adjacent to the verb (e.g., fulfill obligation), but also cases in which
there was an intervening determiner (e.g., fulfill the/an obligation). The following search
terms were used for adjective-noun, noun-noun, and verb-object bigrams, respectively:
</bodyText>
<listItem confidence="0.98380125">
(2) &amp;quot;A N&amp;quot;, where A is the adjective and N is the singular or plural form of the
noun.
(3) &amp;quot;N1 N2&amp;quot;, where N1 is the singular form of the first noun and N2 is the
singular or plural form of the second noun.
(4) &amp;quot;V Det N&amp;quot;, where V is the infinitive, singular present, plural present,
past, perfect, or gerund form of the verb, Det is the determiner the, the
determiner a, or the empty string, and N is the singular or plural form of
the noun.
</listItem>
<bodyText confidence="0.999310870967742">
Note that all searches were for exact matches, which means that the words in the search
terms had to be directly adjacent to score a match. This is encoded by enclosing the
search term in quotation marks. All our search terms were in lower case. We searched
the whole Web (as indexed by AltaVista and Google); that is, the queries were not
restricted to pages in English.
Based on the Web searches, we obtained bigram frequencies by adding up the
number of pages that matched the morphologically expanded forms of the search
terms (see the patterns in (2)–(4)). This process can be automated straightforwardly
using a script that generates all the search terms for a given bigram, issues an AltaVista
or Google query for each of the search terms, and then adds up the resulting number
of matches for each bigram. We applied this process to all the bigrams in our data set,
covering seen and unseen adjective-noun, noun-noun, and verb-object bigrams (i.e., a
set of 540 bigrams in total). The queries were carried out in January 2003 (and thus
the counts are higher than those reported in Keller, Lapata, and Ourioupina [2002],
which were generated about a year earlier).
For some bigrams that were unseen in the BNC, our Web-based procedure returned
zero counts; that is, there were no matches for those bigrams in the Web searches. It
is interesting to compare the Web and NANTC with respect to zero counts: Both
data sources are larger than the BNC and hence should be able to mitigate the data
sparseness problem to a certain extent. Table 5 provides the number of zero counts for
both Web search engines and compares them to the number of bigrams that yielded no
matches in the NANTC. We observe that the Web counts are substantially less sparse
than the NANTC counts: In the worst case, there are nine bigrams for which our Web
queries returned no matches (10% of the data), whereas up to 82 bigrams were unseen
in the NANTC (91% of the data). Recall that the NANTC is 3.5 times larger than the
BNC, which does not seem to be enough to substantially mitigate data sparseness. All
further analyses were carried out on log-transformed frequencies; hence we adjusted
zero counts by setting them to .5.
Table 6 shows descriptive statistics for the bigram counts we obtained using
AltaVista and Google. For comparison, this table also provides descriptive statis-
tics for the BNC and NANTC counts (for seen bigrams only) and for the counts
</bodyText>
<page confidence="0.999173">
466
</page>
<note confidence="0.927882">
Keller and Lapata Web Frequencies for Unseen Bigrams
</note>
<tableCaption confidence="0.997687">
Table 5
</tableCaption>
<table confidence="0.9549925">
Number of zero counts returned by queries to search engines and in the NANTC (for bigrams
unseen in BNC).
Adjective-Noun Noun-Noun Verb-Object
AltaVista 2 9 1
Google 2 5 0
NANTC 76 82 78
</table>
<tableCaption confidence="0.884113">
Table 6
Descriptive statistics for Web counts, corpus counts, and counts re-created using class-based
smoothing (log-transformed).
</tableCaption>
<table confidence="0.999730833333333">
Min Adjective-Noun SD Noun-Noun SD Min Verb-Object SD
Max Mean Min Max Mean Max Mean
Seen Bigrams
AltaVista 1.15 5.84 3.72 1.02 .60 6.16 3.52 1.22 .48 5.86 3.42 1.13
Google 1.54 6.11 4.01 1.01 .90 6.30 3.80 1.23 .60 5.96 3.70 1.11
BNC 0 2.19 .89 .69 0 2.14 .74 .64 0 2.55 .68 .58
NANTC -.30 2.84 .84 .96 -.30 3.02 .56 .94 -.30 3.73 1.90 .98
Smoothing -.06 2.32 1.28 .51 -.70 1.71 .30 .61 -.51 2.07 .53 .57
Unseen Bigrams
AltaVista -.30 5.00 1.50 .99 - .30 3.97 1.20 1.14 -.30 3.88 1.55 1.06
Google -.30 4.11 1.79 .95 - .30 4.15 1.60 1.12 0 4.19 1.90 1.04
Smoothing -.03 2.10 1.25 .46 -1.01 1.93 .28 .66 -.70 1.95 .53 .58
</table>
<tableCaption confidence="0.99753">
Table 7
</tableCaption>
<table confidence="0.50523">
Average factor by which Web counts are larger than BNC counts (seen bigrams).
Adjective-Noun Noun-Noun Verb-Object
AltaVista 665 691 550
Google 1,306 1,151 1,064
</table>
<bodyText confidence="0.9986085">
re-created using class-based smoothing (see Section 3.3 for details on the re-created
frequencies).
From these data, we computed the average factor by which the Web counts are
larger than the BNC counts. The results are given in Table 7 and indicate that the
AltaVista counts are between 550 and 691 times larger than the BNC counts, and that
the Google counts are between 1,064 and 1,306 times larger than the BNC counts.
As we know the size of the BNC (100 million words), we can use these figures to
estimate the number of words available on the Web: between 55.0 and 69.1 billion
words for AltaVista, and between 106.4 and 139.6 billion words for Google. These
estimates are in the same order of magnitude as Grefenstette and Nioche’s (2000)
estimate that 48.1 billion words of English are available on the Web (based on AltaVista
counts compiled in February 2000). They also agree with Zhu and Rosenfeld’s (2001)
estimate that the effective size of the Web is between 79 and 108 billion words (based
on AltaVista, Lycos, and FAST counts; no date given).
</bodyText>
<page confidence="0.994998">
467
</page>
<note confidence="0.435774">
Computational Linguistics Volume 29, Number 3
</note>
<subsectionHeader confidence="0.989202">
2.4 Potential Sources of Noise in Web Counts
</subsectionHeader>
<bodyText confidence="0.999868363636364">
The method we used to retrieve Web counts is based on very simple heuristics; it is
thus inevitable that the counts generated will contain a certain amount of noise. In
this section we discuss a number of potential sources of such noise.
An obvious limitation of our method is that it relies on the page counts returned by
the search engines; we do not download the pages themselves for further processing.
Note that many of the bigrams in our sample are very frequent (up to 106 matches;
see the “Max” columns in Table 6), hence the effort involved in downloading all pages
would be immense (though methods for downloading a representative sample could
probably be devised).
Our approach estimates Web frequencies based not on bigram counts directly, but
on page counts. In other words, it ignores the fact that a bigram can occur more than
once on a given Web page. This approximation is justified, as Zhu and Rosenfeld (2001)
demonstrated for unigrams, bigrams, and trigrams: Page counts and n-gram counts
are highly correlated on a log-log scale. This result is based on Zhu and Rosenfeld’s
queries to AltaVista, a search engine that at the time of their research returned both
the number of pages and the overall number of matches for a given query.4
Another important limitation of our approach arises from the fact that both Google
and AltaVista disregard punctuation and capitalization, even if the search term is
placed within quotation marks. This can lead to false positives, for instance, if the
match crosses a phrase boundary, such as in (5), which matches hungry prey. Other
false positives can be generated by page titles and links, such as the examples (6)
and (7) which match edition broadcast.5
</bodyText>
<listItem confidence="0.9923535">
(5) The lion will kill only when it’s hungry. Prey can usually sense when
lions are hunting.
(6) 10th Edition Broadcast Products Catalog (as a page title)
(7) Issue/Edition/Broadcast (as a link)
</listItem>
<bodyText confidence="0.997863625">
The fact that our method does not download Web pages means that no tagging, chunk-
ing, or parsing can be carried out to ensure that the matches are correct. Instead we
rely on the simple adjacency of the search terms, which is enforced by using queries
enclosed within quotation marks (see Section 2.3 for details). This means that we miss
any nonadjacent matches, even though a chunker or parser (such as the one used for
extracting BNC or NANTC bigrams) would find them. An example is an adjective-
noun bigram in which an adverbial intervenes between the adjective and the noun
(see Table 1).
Furthermore, the absence of tagging, chunking, and parsing can also generate false
positives, in particular for queries containing words with part-of-speech ambiguity.
(Recall that our bigram selection procedure ensures that the predicate word, but not
the argument word, is unambiguous in terms of its POS tagging in the BNC.) As an
example, consider process directory, which in our data set is a noun-noun bigram (see
Table 2). One of the matches returned by Google is (8), in which process is a verb.
Another example is fund membrane, which is a noun-noun bigram in our data set but
matches (9) in Google.
</bodyText>
<footnote confidence="0.8193226">
4 Note that this feature of AltaVista has since been discontinued; hence in the present article we had no
option but to use page counts. However, Keller, Lapata, and Ourioupina (2002) used AltaVista match
counts (instead of page counts) on the same data sets; their results agree with the ones reported in the
present article very closely.
5 Some of the examples in (5)–(9) were kindly provided by a reviewer.
</footnote>
<page confidence="0.995362">
468
</page>
<bodyText confidence="0.587198">
Keller and Lapata Web Frequencies for Unseen Bigrams
</bodyText>
<listItem confidence="0.923720333333333">
(8) The global catalog server’s function is to process directory searches for
the entire forest.
(9) Green grants fund membrane technology.
</listItem>
<bodyText confidence="0.9995939375">
Another source of noise is the fact that Google (but not AltaVista) will sometimes
return pages that do not include the search term at all. This can happen if the search
term is contained in a link to the page (but not on the page itself).
As we did not limit our Web searches to English (even though many search engines
now allow the target language for a search to be set), there is also a risk that false posi-
tives are generated by cross-linguistic homonyms, that is, by words of other languages
that are spelled in the same way as the English words in our data sets. However, this
problem is mitigated by the fact that English is by far the most common language on
the Web, as shown by Grefenstette and Nioche (2000). Also, the chance of two such
homonyms forming a valid bigram in another language is probably fairly small.
To summarize, Web counts are certainly less sparse than the counts in a corpus of a
fixed size (see Section 2.3). However, Web counts are also likely to be significantly more
noisy than counts obtained from a carefully tagged and chunked or parsed corpus, as
the examples in this section show. It is therefore essential to carry out a comprehensive
evaluation of the Web counts generated by our method. This is the topic of the next
section.
</bodyText>
<sectionHeader confidence="0.994886" genericHeader="introduction">
3. Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999928">
3.1 Evaluation against Corpus Frequencies
</subsectionHeader>
<bodyText confidence="0.999983055555556">
Since Web counts can be relatively noisy, as discussed in the previous section, it is
crucial to determine whether there is a reliable relationship between Web counts and
corpus counts. Once this is assured, we can explore the usefulness of Web counts
for overcoming data sparseness. We carried out a correlation analysis to determine
whether there is a linear relationship between BNC and NANTC counts and AltaVista
and Google counts. All correlation coefficients reported in this article refer to Pear-
son’s r.6 All results were obtained on log-transformed counts.7
Table 8 shows the results of correlating Web counts with corpus counts from the
BNC, the corpus from which our bigrams were sampled (see Section 2.1). A high
correlation coefficient was obtained across the board, ranging from .720 to .847 for
AltaVista counts and from .720 to .850 for Google counts. This indicates that Web
counts approximate BNC counts for the three types of bigrams under investigation.
Note that there is almost no difference between the correlations achieved using Google
and AltaVista counts.
It is important to check that these results are also valid for counts obtained from
other corpora. We therefore correlated our Web counts with the counts obtained from
NANTC, a corpus that is larger than the BNC but is drawn from a single genre,
namely, news text (see Section 2.2). The results are shown in Table 9. We find that
</bodyText>
<footnote confidence="0.767798875">
6 Correlation analysis is a way of measuring the degree of linear association between two variables.
Effectively, we are fitting a linear equation y = ax + b to the data; this means that the two variables x
and y (which in our case represent frequencies or judgments) can still differ by a multiplicative
constant a and an additive constant b, even if they are highly correlated.
7 It is well-known that corpus frequencies have a Zipfian distribution. Log-transforming them is a way
of normalizing the counts before applying statistical tests. We apply correlation analysis on the
log-transformed data, which is equivalent to computing a log-linear regression coefficient on the
untransformed data.
</footnote>
<page confidence="0.998004">
469
</page>
<note confidence="0.409767">
Computational Linguistics Volume 29, Number 3
</note>
<tableCaption confidence="0.995539">
Table 8
</tableCaption>
<table confidence="0.984848833333333">
Correlation of BNC counts with Web counts (seen bigrams).
Adjective-Noun Noun-Noun Verb-Object
AltaVista .847** .720** .762**
Google .850** .720** .766**
Smoothing .248* .277** .342**
*p &lt; .05 (one-tailed). **p &lt; .01 (one-tailed).
</table>
<tableCaption confidence="0.995562">
Table 9
</tableCaption>
<table confidence="0.964913428571428">
Correlation of NANTC counts with Web counts (seen bigrams).
Adjective-Noun Noun-Noun Verb-Object
AltaVista .712** .667** .788**
Google .712** .662** .787**
BNC .710** .672** .814**
Smoothing .338** .317** .263*
*p &lt; .05 (one-tailed). **p &lt; .01 (one-tailed).
</table>
<bodyText confidence="0.998927586206897">
Google and AltaVista counts also correlate significantly with NANTC counts. The
correlation coefficients range from .667 to .788 for AltaVista and from .662 to .787 for
Google. Again, there is virtually no difference between the correlations for the two
search engines. We also observe that the correlation between Web counts and BNC is
generally slightly higher than the correlation between Web counts and NANTC counts.
We carried out one-tailed t-tests to determine whether the differences in the correlation
coefficients were significant. We found that both AltaVista counts (t(87) = 3.11, p &lt; .01)
and Google counts (t(87) = 3.21, p &lt; .01) were significantly better correlated with
BNC counts than with NANTC counts for adjective-noun bigrams. The difference in
correlation coefficients was not significant for noun-noun and verb-object bigrams, for
either search engine.
Table 9 also shows the correlations between BNC counts and NANTC counts.
The intercorpus correlation can be regarded as an upper limit for the correlations we
can expect between counts from two corpora that differ in size and genre and that
have been obtained using different extraction methods. The correlation between Al-
taVista and Google counts and NANTC counts reached the upper limit for all three
bigram types (one-tailed t-tests found no significant differences between the correla-
tion coefficients). The correlation between BNC counts and Web counts reached the
upper limit for noun-noun and verb-object bigrams (no significant differences for either
search engine) and significantly exceeded it for adjective-noun bigrams for AltaVista
(t(87) = 3.16, p &lt; .01) and Google (t(87) = 3.26, p &lt; .01).
We conclude that simple heuristics (see Section 2.3) are sufficient to obtain useful
frequencies from the Web; it seems that the large amount of data available for Web
counts outweighs the associated problems (noisy, unbalanced, etc.). We found that
Web counts were highly correlated with frequencies from two different corpora. Fur-
thermore, Web counts and corpus counts are as highly correlated as counts from two
different corpora (which can be regarded as an upper bound).
Note that Tables 8 and 9 also provide the correlation coefficients obtained when
corpus frequencies are compared with frequencies that were re-created through class-
</bodyText>
<page confidence="0.98289">
470
</page>
<note confidence="0.820474">
Keller and Lapata Web Frequencies for Unseen Bigrams
</note>
<bodyText confidence="0.998371">
based smoothing, using the BNC as a training corpus (after removing the seen bi-
grams). This will be discussed in more detail in Section 3.3.
</bodyText>
<subsectionHeader confidence="0.998429">
3.2 Evaluation against Plausibility Judgments
</subsectionHeader>
<bodyText confidence="0.995071543478261">
Previous work has demonstrated that corpus counts correlate with human plausibility
judgments for adjective-noun bigrams. This result holds both for seen bigrams (La-
pata, McDonald, and Keller 1999) and for unseen bigrams whose counts have been
re-created using smoothing techniques (Lapata, Keller, and McDonald 2001). Based on
these findings, we decided to evaluate our Web counts on the task of predicting plausi-
bility ratings. If the Web counts for bigrams correlate with plausibility judgments, then
this indicates that the counts are valid, in the sense of being useful for predicting the
intuitive plausibility of predicate-argument pairs. The degree of correlation between
Web counts and plausibility judgments is an indicator of the quality of the Web counts
(compared to corpus counts or counts re-created using smoothing techniques).
3.2.1 Method. For seen and unseen adjective-noun bigrams, we used the two sets of
plausibility judgments collected by Lapata, McDonald, and Keller (1999) and Lapata,
Keller, and McDonald (2001), respectively. We conducted four additional experiments
to collect judgments for noun-noun and verb-object bigrams, both seen and unseen.
The experimental method was the same for all six experiments.
Materials. The experimental stimuli were based on the six sets of seen or unseen
bigrams extracted from the BNC as described in Section 2.1 (adjective-noun, noun-
noun, and verb-object bigrams). In the adjective-noun and noun-noun cases, the stimuli
consisted simply of the bigrams. In the verb-object case, the bigrams were embedded
in a short sentence to make them more natural: A proper-noun subject was added.
Procedure. The experimental paradigm was magnitude estimation (ME), a tech-
nique standardly used in psychophysics to measure judgments of sensory stimuli
(Stevens 1975), which Bard, Robertson, and Sorace (1996) and Cowart (1997) have ap-
plied to the elicitation of linguistic judgments. The ME procedure requires subjects to
estimate the magnitude of physical stimuli by assigning numerical values proportional
to the stimulus magnitude they perceive. In contrast to the five- or seven-point scale
conventionally used to measure human intuitions, ME employs an interval scale and
therefore produces data for which parametric inferential statistics are valid.
ME requires subjects to assign numbers to a series of linguistic stimuli in a propor-
tional fashion. Subjects are first exposed to a modulus item, to which they assign an
arbitrary number. All other stimuli are rated proportional to the modulus. In this way,
each subject can establish his or her own rating scale, thus yielding maximally fine-
graded data and avoiding the known problems with the conventional ordinal scales
for linguistic data (Bard, Robertson, and Sorace 1996; Cowart 1997; Sch¨utze 1996).
The experiments reported in this article were carried out using the WebExp soft-
ware package (Keller et al. 1998). A series of previous studies has shown that data
obtained using WebExp closely replicate results obtained in a controlled laboratory set-
ting; this has been demonstrated for acceptability judgments (Keller and Alexopoulou
2001), coreference judgments (Keller and Asudeh 2001), and sentence completions
(Corley and Scheepers 2002).
In the present experiments, subjects were presented with bigram pairs and were
asked to rate the degree of plausibility proportional to a modulus item. They first saw a
set of instructions that explained the ME technique and the judgment task. The concept
of plausibility was not defined, but examples of plausible and implausible bigrams
were given (different examples for each stimulus set). Then subjects were asked to
fill in a questionnaire with basic demographic information. The experiment proper
</bodyText>
<page confidence="0.992637">
471
</page>
<table confidence="0.468548">
Computational Linguistics Volume 29, Number 3
</table>
<tableCaption confidence="0.993303">
Table 10
</tableCaption>
<table confidence="0.848840333333333">
Descriptive statistics for plausibility judgments (log-transformed). N is the number of subjects
used in each experiment.
Adjective-Noun Bigrams Noun-Noun Bigrams Verb-Object Bigrams
N Min Max Mean SD N Min Max Mean SD N Min Max Mean SD
Seen 30 -.85 .11 -.13 .22 25 -.15 .69 .40 .21 27 -.52 .45 .12 .24
Unseen 41 -.56 .37 -.07 .20 25 -.49 .52 -.01 .23 21 -.51 .28 -.16 .22
</table>
<bodyText confidence="0.988588175">
consisted of three phases: (1) a calibration phase, designed to familiarize subjects with
the task, in which they had to estimate the length of five horizontal lines; (2) a practice
phase, in which subjects judged the plausibility of eight bigrams (similar to the ones
in the stimulus set); (3) the main experiment, in which each subject judged one of the
six stimulus sets (90 bigrams). The stimuli were presented in random order, with a
new randomization being generated for each subject.
Subjects. A separate experiment was conducted for each set of stimuli. The number
of subjects per experiment is shown in Table 10 (in the column labeled N). All sub-
jects were self-reported native speakers of English; they were recruited by postings to
newsgroups and mailing lists. Participation was voluntary and unpaid.
WebExp collects by-item response time data; subjects whose response times were
very short or very long were excluded from the sample, as they are unlikely to have
completed the experiment adequately. We also excluded the data of subjects who had
participated more than once in the same experiment, based on their demographic data
and on their Internet connection data, which is logged by WebExp.
3.2.2 Results and Discussion. The experimental data were normalized by dividing
each numerical judgment by the modulus value that the subject had assigned to the
reference sentence. This operation creates a common scale for all subjects. Then the
data were transformed by taking the decadic logarithm. This transformation ensures
that the judgments are normally distributed and is standard practice for magnitude
estimation data (Bard, Robertson, and Sorace 1996; Cowart 1997; Stevens 1975). All
further analyses were conducted on the normalized, log-transformed judgments.
Table 10 shows the descriptive statistics for all six judgment experiments: the
original experiments by Lapata, McDonald, and Keller (1999) and Lapata, Keller, and
McDonald (2001) for adjective-noun bigrams, and our new ones for noun-noun and
verb-object bigrams.
We used correlation analysis to compare corpus counts and Web counts with plau-
sibility judgments. Table 11 (top half) lists the correlation coefficients that were ob-
tained when correlating log-transformed Web counts (AltaVista and Google) and cor-
pus counts (BNC and NANTC) with mean plausibility judgments for seen adjective-
noun, noun-noun, and verb-object bigrams. The results show that both AltaVista and
Google counts correlate well with plausibility judgments for seen bigrams. The corre-
lation coefficient for AltaVista ranges from .641 to .700; for Google, it ranges from .624
to .692. The correlations for the two search engines are very similar, which is also what
we found in Section 3.1 for the correlations between Web counts and corpus counts.
Note that the Web counts consistently achieve a higher correlation with the judg-
ments than the BNC counts, which range from .488 to .569. We carried out a series
of one-tailed t-tests to determine whether the differences between the correlation co-
efficients for the Web counts and the correlation coefficients for the BNC counts were
significant. For the adjective-noun bigrams, the AltaVista coefficient was significantly
</bodyText>
<page confidence="0.988654">
472
</page>
<note confidence="0.827227">
Keller and Lapata Web Frequencies for Unseen Bigrams
</note>
<bodyText confidence="0.999832766666667">
higher than the BNC coefficient (t(87) = 1.76, p &lt; .05), whereas the difference between
the Google coefficient and the BNC coefficient failed to reach significance. For the
noun-noun bigrams, both the AltaVista and the Google coefficients were significantly
higher than the BNC coefficient (t(87) = 3.11, p &lt; .01 and t(87) = 2.95, p &lt; .01).
Also, for the verb-object bigrams, both the AltaVista coefficient and the Google coef-
ficient were significantly higher than the BNC coefficient (t(87) = 2.64, p &lt; .01 and
t(87) = 2.32, p &lt; .05).
A similar picture was observed for the NANTC counts. Again, the Web counts
outperformed the corpus counts in predicting plausibility. For the adjective-noun bi-
grams, both the AltaVista and the Google coefficient were significantly higher than the
NANTC coefficient (t(87) = 1.97, p &lt; .05; t(87) = 1.81, p &lt; .05). For the noun-noun bi-
grams, the AltaVista coefficient was higher than the NANTC coefficient (t(87) = 1.64,
p &lt; .05), but the Google coefficient was not significantly different from the NANTC co-
efficient. For verb-object bigrams, the difference was significant for both search engines
(t(87) = 2.74, p &lt; .01; t(87) = 2.38, p &lt; .01).
In sum, for all three types of bigrams, the correlation coefficients achieved with
AltaVista were significantly higher than the ones achieved by either the BNC or the
NANTC. Google counts outperformed corpus counts for all bigrams with the exception
of adjective-noun counts from the BNC and noun-noun counts from the NANTC.
The bottom panel of Table 11 shows the correlation coefficients obtained by com-
paring log-transformed judgments with log-transformed Web counts for unseen adjec-
tive-noun, noun-noun, and verb-object bigrams. We observe that the Web counts con-
sistently show a significant correlation with the judgments, with the coefficient rang-
ing from .480 to .578 for AltaVista counts and from .473 to .595 for the Google counts.
Table 11 also provides the correlations between plausibility judgments and counts
re-created using class-based smoothing, which we will discuss in Section 3.3.
An important question is how well humans agree when judging the plausibility of
adjective-noun, noun-noun, and verb-noun bigrams. Intersubject agreement gives an
upper bound for the task and allows us to interpret how well our Web-based method
performs in relation to humans. To calculate intersubject agreement we used leave-
</bodyText>
<tableCaption confidence="0.996531">
Table 11
</tableCaption>
<table confidence="0.978112647058824">
Correlation of plausibility judgments with Web counts, corpus counts, and counts re-created
using class-based smoothing. “Agreement” refers to the intersubject agreement on the
judgment task.
Adjective-Noun Noun-Noun Verb-Object
Seen Bigrams
AltaVista .650** .700 ** .641**
Google .641** .692 ** .624**
BNC .569** .517 ** .488**
NANTC .526** .597 ** .491**
Smoothing .329** .318 ** .223*
Agreement .630** .641 ** .604**
Unseen Bigrams
AltaVista .480** .578 ** .551**
Google .473** .595 ** .520**
Smoothing .342** .372 ** .298**
Agreement .550** .570 ** .640**
*p &lt; .05 (one-tailed). **p &lt; .01 (one-tailed).
</table>
<page confidence="0.912966">
473
</page>
<note confidence="0.578091">
Computational Linguistics Volume 29, Number 3
</note>
<bodyText confidence="0.998912454545455">
one-out resampling. This technique is a special case of n-fold cross-validation (Weiss
and Kulikowski 1991) and has been previously used for measuring how well humans
agree in judging semantic similarity (Resnik 1999, 2000).
For each subject group, we divided the set of the subjects’ responses with size n
into a set of size n − 1 (i.e., the response data of all but one subject) and a set of size 1
(i.e., the response data of a single subject). We then correlated the mean ratings of the
former set with the ratings of the latter. This was repeated n times (see the number
of participants in Table 6); the mean of the correlation coefficients for the seen and
unseen bigrams is shown in Table 11 in the rows labeled “Agreement.”
For both seen and unseen bigrams, we found no significant difference between the
upper bound (intersubject agreement) and the correlation coefficients obtained using
either AltaVista or Google counts. This finding holds for all three types of bigrams. The
same picture emerged for the BNC and NANTC counts: These correlation coefficients
were not significantly different from the upper limit, for all three types of bigrams,
both for seen and for unseen bigrams.
To conclude, our evaluation demonstrated that Web counts reliably predict human
plausibility judgments, both for seen and for unseen predicate-argument bigrams.
AltaVista counts for seen bigrams are a better predictor of human judgments than
BNC and NANTC counts. These results show that our heuristic method yields valid
frequencies; the simplifications we made in obtaining the Web counts (see Section 2.3),
as well as the fact that Web data are noisy (see Section 2.4), seem to be outweighed
by the fact that the Web is up to a thousand times larger than the BNC.
</bodyText>
<subsectionHeader confidence="0.998629">
3.3 Evaluation against Class-Based Smoothing
</subsectionHeader>
<bodyText confidence="0.999980074074074">
The evaluation in the last two sections established that Web counts are useful for
approximating corpus counts and for predicting plausibility judgments. As a further
step in our evaluation, we correlated Web counts with counts re-created by applying
a class-based smoothing method to the BNC.
We re-created co-occurrence frequencies for predicate-argument bigrams using a
simplified version of Resnik’s (1993) selectional association measure proposed by La-
pata, Keller, and McDonald (2001). In a nutshell, this measure replaces Resnik’s (1993)
information-theoretic approach with a simpler measure that makes no assumptions
with respect to the contribution of a semantic class to the total quantity of information
provided by the predicate about the semantic classes of its argument. It simply substi-
tutes the argument occurring in the predicate-argument bigram with the concept by
which it is represented in the WordNet taxonomy. Predicate-argument co-occurrence
frequency is estimated by counting the number of times the concept corresponding
to the argument is observed to co-occur with the predicate in the corpus. Because
a given word is not always represented by a single class in the taxonomy (i.e., the
argument co-occurring with a predicate can generally be the realization of one of
several conceptual classes), Lapata, Keller, and McDonald (2001) constructed the fre-
quency counts for a predicate-argument bigram for each conceptual class by dividing
the contribution from the argument by the number of classes to which it belongs.
They demonstrate that the counts re-created using this smoothing technique correlate
significantly with plausibility judgments for adjective-noun bigrams. They also show
that this class-based approach outperforms distance-weighted averaging (Dagan, Lee,
and Pereira 1999), a smoothing method that re-creates unseen word co-occurrences on
the basis of distributional similarity (without relying on a predefined taxonomy), in
predicting plausibility.
In the current study, we used the smoothing technique of Lapata, Keller, and
McDonald (2001) to re-create not only adjective-noun bigrams, but also noun-noun
</bodyText>
<page confidence="0.999001">
474
</page>
<note confidence="0.959427">
Keller and Lapata Web Frequencies for Unseen Bigrams
</note>
<tableCaption confidence="0.998035">
Table 12
</tableCaption>
<table confidence="0.957240222222222">
Correlation of counts re-created using class-based smoothing with Web counts.
Adjective-Noun Noun-Noun Verb-Object
Seen Bigrams
AltaVista .344** .362** .361**
Google .330** .343** .349**
Unseen Bigrams
AltaVista .439** .386** .412**
Google .444** .421** .397**
*p &lt; .05 (one-tailed). **p &lt; .01 (one-tailed).
</table>
<bodyText confidence="0.999643277777778">
and verb-object bigrams. As already mentioned in Section 2.1, it was assumed that the
noun is the predicate in adjective-noun bigrams; for noun-noun bigrams, we treated
the right noun as the predicate, and for verb-object bigrams, we treated the verb as the
predicate. We applied Lapata, Keller, and McDonald’s (2001) technique to the unseen
bigrams for all three bigram types. We also used it on the seen bigrams, which we
were able to treat as unseen by removing all instances of the bigrams from the training
corpus.
To test the claim that Web frequencies can be used to overcome data sparseness,
we correlated the frequencies re-created using class-based smoothing on the BNC with
the frequencies obtained from the Web. The correlation coefficients for both seen and
unseen bigrams are shown in Table 12. In all cases, a significant correlation between
Web counts and re-created counts is obtained. For seen bigrams, the correlation coef-
ficient ranged from .344 to .362 for AltaVista counts and from .330 to .349 for Google
counts. For unseen bigrams, the correlations were somewhat higher, ranging from .386
to .439 for AltaVista counts and from .397 to .444 for Google counts. For both seen
and unseen bigrams, there was only a very small difference between the correlation
coefficients obtained with the two search engines.
It is also interesting to compare the performance of class-based smoothing and Web
counts on the task of predicting plausibility judgments. The correlation coefficients are
listed in Table 11. The re-created frequencies are correlated significantly with all three
types of bigrams, both for seen and unseen bigrams. For the seen bigrams, we found
that the correlation coefficients obtained using smoothed counts were significantly
lower than the upper bound for all three types of bigrams (t(87) = 3.01, p &lt; .01;
t(87) = 3.23, p &lt; .01; t(87) = 3.43, p &lt; .01). This result also held for the unseen
bigrams: The correlations obtained using smoothing were significantly lower than the
upper bound for all three types of bigrams (t(87) = 1.86, p &lt; .05; t(87) = 1.97, p &lt; .05;
t(87) = 3.36, p &lt; .01).
Recall that the correlation coefficients obtained using the Web counts were not
found to be significantly different from the upper bound, which indicates that Web
counts are better predictors of plausibility than smoothed counts. This fact was con-
firmed by further significance testing: For seen bigrams, we found that the AltaVista
correlation coefficients were significantly higher than correlation coefficients obtained
using smoothing, for all three types of bigrams (t(87) = 3.31, p &lt; .01; t(87) = 4.11,
p &lt; .01; t(87) = 4.32, p &lt; .01). This also held for Google counts (t(87) = 3.16, p &lt; .01;
t(87) = 4.02, p &lt; .01; t(87) = 4.03, p &lt; .01). For unseen bigrams, the AltaVista coef-
ficients and the coefficients obtained using smoothing were not significantly different
</bodyText>
<page confidence="0.99752">
475
</page>
<note confidence="0.640836">
Computational Linguistics Volume 29, Number 3
</note>
<bodyText confidence="0.999513">
for adjective-noun bigrams, but the difference reached significance for noun-noun and
verb-object bigrams (t(87) = 2.08, p &lt; .05; t(87) = 2.53, p &lt; .01). For Google counts,
the difference was again not significant for adjective-noun bigrams, but it reached sig-
nificance for noun-noun and verb-object bigrams (t(87) = 2.34, p &lt; .05; t(87) = 2.15,
p &lt; .05).
Finally, we conducted a small study to investigate the validity of the counts that
were re-created using class-based smoothing. We correlated the re-created counts for
the seen bigrams with their actual BNC and NANTC frequencies. The correlation
coefficients are reported in Tables 8 and 9. We found that the correlation between re-
created counts and corpus counts was significant for all three types of bigrams, for
both corpora. This demonstrates that the smoothing technique we employed generates
realistic corpus counts, in the sense that the re-created counts are correlated with
the actual counts. However, the correlation coefficients obtained using Web counts
were always substantially higher than those obtained using smoothed counts. These
differences were significant for the BNC counts for AltaVista (t(87) = 8.38, p &lt; .01;
t(87) = 5.00, p &lt; .01; t(87) = 5.03, p &lt; .01) and Google (t(87) = 8.35, p &lt; .01;
t(87) = 5.00, p &lt; .01; t(87) = 5.03, p &lt; .01). They were also significant for the NANTC
counts for AltaVista (t(87) = 4.12, p &lt; .01; t(87) = 3.72, p &lt; .01; t(87) = 6.58, p &lt; .01)
and Google (t(87) = 4.08, p &lt; .01; t(87) = 3.06, p &lt; .01; t(87) = 6.47, p &lt; .01).
To summarize, the results presented in this section indicate that Web counts are
indeed a valid way of obtaining counts for bigrams that are unseen in a given corpus:
They correlate reliably with counts re-created using class-based smoothing. For seen
bigrams, we found that Web counts correlate with counts that were re-created using
smoothing techniques (after removing the seen bigrams from the training corpus). For
the task of predicting plausibility judgments, we were able to show that Web counts
outperform re-created counts, both for seen and for unseen bigrams. Finally, we found
that Web counts for seen bigrams correlate better than re-created counts with the real
corpus counts.
It is beyond the scope of the present study to undertake a full comparison between
Web counts and frequencies re-created using all available smoothing techniques (and
all available taxonomies that might be used for class-based smoothing). The smooth-
ing method discussed above is simply one type of class-based smoothing. Other, more
sophisticated class-based methods do away with the simplifying assumption that the
argument co-occurring with a given predicate (adjective, noun, verb) is distributed
evenly across its conceptual classes and attempt to find the right level of generalization
in a concept hierarchy, by discounting, for example, the contribution of very general
classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing ap-
proaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman
and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word com-
binations by exploiting only corpus-internal evidence, without relying on taxonomic
information. Our goal was to demonstrate that frequencies retrieved from the Web
are a viable alternative to conventional smoothing methods when data are sparse; we
do not claim that our Web-based method is necessarily superior to smoothing or that
it should be generally preferred over smoothing methods. However, the next section
will present a small-scale study that compares the performance of several smoothing
techniques with the performance of Web counts on a standard task from the literature.
</bodyText>
<subsectionHeader confidence="0.937753">
3.4 Pseudodisambiguation
</subsectionHeader>
<bodyText confidence="0.996594">
In the smoothing literature, re-created frequencies are typically evaluated using pseu-
dodisambiguation (Clark and Weir 2001; Dagan, Lee, and Pereira 1999; Lee 1999;
Pereira, Tishby, and Lee 1993; Prescher, Riezler, and Rooth 2000; Rooth et al. 1999).
</bodyText>
<page confidence="0.996941">
476
</page>
<note confidence="0.813108">
Keller and Lapata Web Frequencies for Unseen Bigrams
</note>
<bodyText confidence="0.996420538461539">
The aim of the pseudodisambiguation task is to decide whether a given algorithm
re-creates frequencies that make it possible to distinguish between seen and unseen
bigrams in a given corpus. A set of pseudobigrams is constructed according to a set
of criteria (detailed below) that ensure that they are unattested in the training corpus.
Then the seen bigrams are removed from the training data, and the smoothing method
is used to re-create the frequencies of both the seen bigrams and the pseudobigrams.
The smoothing method is then evaluated by comparing the frequencies it re-creates
for both types of bigrams.
We evaluated our Web counts by applying the pseudodisambiguation procedure
that Rooth et al. (1999), Prescher, Riezler, and Rooth (2000), and Clark and Weir (2001)
employed for evaluating re-created verb-object bigram counts. In this procedure, the
noun n from a verb-object bigram (v, n) that is seen in a given corpus is paired with a
randomly chosen verb v&apos; that does not take n as its object within the corpus. This results
in an unseen verb-object bigram (v&apos;, n). The seen bigram is now treated as unseen (i.e.,
all of its occurrences are removed from the training corpus), and the frequencies of
both the seen and the unseen bigram are re-created (using smoothing, or Web counts,
in our case). The task is then to decide which of the two verbs v and v&apos; takes the
noun n as its object. For this, the re-created bigram frequency is used: The bigram
with the higher re-created frequency (or probability) is taken to be the seen bigram.
If this bigram is really the seen one, then the disambiguation is correct. The overall
percentage of correct disambiguations is a measure of the quality of the re-created
frequencies (or probabilities). In the following, we will first describe in some detail
the experiments that Rooth et al. (1999) and Clark and Weir (2001) conducted. We
will then discuss how we replicated their experiments using the Web as an alternative
smoothing method.
Rooth et al. (1999) used pseudodisambiguation to evaluate a class-based model
that is derived from unlabeled data using the expectation maximization (EM) algo-
rithm. From a data set of 1,280,712 (v, n) pairs (obtained from the BNC using Carroll
and Rooth’s [1998] parser), they randomly selected 3,000 pairs, with each pair con-
taining a fairly frequent verb and noun (only verbs and nouns that occurred between
30 and 3,000 times in the data were considered). For each pair (v, n) a fairly frequent
verb v&apos; was randomly chosen such that the pair (v&apos;, n) did not occur in the data set.
Given the set of (v, n, v&apos;) triples (a total of 1,337), the task was to decide whether (v, n)
or (v&apos;,n) was the correct (i.e., seen) pair by comparing the probabilities P(nlv) and
P(nlv&apos;). The probabilities were re-created using Rooth et al.’s (1999) EM-based clus-
tering model on a training set from which all seen pairs (v, n) had been removed. An
accuracy of 80% on the pseudodisambiguation task was achieved (see Table 13).
Prescher, Riezler, and Rooth (2000) evaluated Rooth et al.’s (1999) EM-based clus-
tering model again using pseudodisambiguation, but on a separate data set using a
</bodyText>
<tableCaption confidence="0.994302">
Table 13
</tableCaption>
<table confidence="0.940512571428571">
Percentage of correct disambiguations on the pseudodisambiguation task using Web counts
and counts re-created using EM-based clustering (Rooth et al. 1999).
Data Set N AltaVista AltaVista Rooth et al.
Conditional Probability Joint Probability
Subject 717 71.2 68.5 —
Objects 620 85.2 77.5 —
Subjects and objects 1,337 77.7 72.7 80.0
</table>
<page confidence="0.730424">
477
</page>
<note confidence="0.438644">
Computational Linguistics Volume 29, Number 3
</note>
<tableCaption confidence="0.990852">
Table 14
</tableCaption>
<table confidence="0.929975">
Percentage of correct disambiguations on the pseudodisambiguation task using Web counts
and counts re-created using EM-based clustering (Prescher, Riezler, and Rooth 2000).
Data Set N AltaVista AltaVista VA Model VO Model
Conditional Probability Joint Probability
Subjects 159 66.7 59.1 — —
Objects 139 70.5 66.2 — —
Subjects and objects 298 68.5 62.4 79.0 88.3
</table>
<bodyText confidence="0.998758606060606">
slightly different method for constructing the pseudobigrams. They used a set of 298
(v, n, n&apos;) BNC triples in which (v, n) was chosen as in Rooth et al. (1999) but paired with
a randomly chosen noun n&apos;. Given the set of (v, n, n&apos;) triples, the task was to decide
whether (v, n) or (v, n&apos;) was the correct pair in each triple. Prescher, Riezler, and Rooth
(2000) reported pseudodisambiguation results with two clustering models: (1) Rooth
et al.’s (1999) clustering approach, which models the semantic fit between a verb and
its argument (VA model), and (2) a refined version of this approach that models only
the fit between a verb and its object (VO model), disregarding other arguments of the
verb. The results of the two models on the pseudodisambiguation task are shown in
Table 14.
At this point, it is important to note that neither Rooth et al. (1999) nor Prescher,
Riezler, and Rooth (2000) used pseudodisambiguation for the final evaluation of their
models. Rather, the performance on the pseudodisambiguation task was used to op-
timize the model parameters. The results in Tables 13 and 14 show the pseudodisam-
biguation performance achieved for the best parameter settings. In other words, these
results were obtained on the development set (i.e., on the same data set that was used
to optimize the parameters), not on a completely unseen test set. This procedure is
well-justified in the context of Rooth et al.’s (1999) and Prescher, Riezler, and Rooth’s
(2000) work, which aimed at building models of lexical semantics, not of pseudodis-
ambiguation. Therefore, they carried out their final evaluations on unseen test sets for
the tasks of lexicon induction (Rooth et al. 1999) and target language disambiguation
(Prescher, Riezler, and Rooth 2000), once the model parameters had been fixed using
the pseudodisambiguation development set.8
Clark and Weir (2002) use a setting similar to that of Rooth et al. (1999) and
Prescher, Riezler, and Rooth (2000); here pseudodisambiguation is employed to evalu-
ate the performance of a class-based probability estimation method. In order to address
the problem of estimating conditional probabilities in the face of sparse data, Clark
and Weir (2002) define probabilities in terms of classes in a semantic hierarchy and
propose hypothesis testing as a means of determining a suitable level of generalization
in the hierarchy. Clark and Weir (2002) report pseudodisambiguation results on two
data sets, with an experimental setup similar to that of Rooth et al. (1999). For the first
data set, 3,000 pairs were randomly chosen from 1.3 million (v, n) tuples extracted
from the BNC (using the parser of Briscoe and Carroll [1997]). The selected pairs con-
</bodyText>
<footnote confidence="0.948249857142857">
8 Stefan Riezler (personal communication, 2003) points out that the main variance in Rooth et al.’s (1999)
pseudodisambiguation results comes from the class cardinality parameter (start values account for only
2% of the performance, and iterations do not seem to make a difference at all). Figure 3 of Rooth et al.
(1999) shows that a performance of more than 75% is obtained for every reasonable choice of classes.
This indicates that a “proper” pseudodisambiguation setting with separate development and test data
would have resulted in a similar choice of class cardinality and thus achieved the same 80%
performance that is cited in Table 13.
</footnote>
<page confidence="0.993644">
478
</page>
<note confidence="0.970576">
Keller and Lapata Web Frequencies for Unseen Bigrams
</note>
<tableCaption confidence="0.996695">
Table 15
</tableCaption>
<table confidence="0.984951142857143">
Percentage of correct disambiguations on the pseudodisambiguation task using Web counts
and counts re-created using class-based smoothing (Clark and Weir 2002).
Data Set N AltaVista AltaVista Clark and Li and Resnik
Conditional Joint Probability Weir Abe
Probability
Objects (low frequency) 3000 83.9 81.1 72.4 62.9 62.6
Objects (high frequency) 3000 87.7 85.3 73.9 68.3 63.9
</table>
<bodyText confidence="0.99980296969697">
tained relatively frequent verbs (occurring between 500 and 5,000 times in the data).
The data sets were constructed as proposed by Rooth et al. (1999). The procedure for
creating the second data set was identical, but this time only verbs that occurred be-
tween 100 and 1,000 times were considered. Clark and Weir (2002) further compared
their approach with Resnik’s (1993) selectional association model and Li and Abe’s
(1998) tree cut model on the same data sets. These methods are directly comparable,
as they can be used for class-based probability estimation and address the question
of how to find a suitable level of generalization in a hierarchy (i.e., WordNet). The
results of the three methods used on the two data sets are shown in Table 15.
We employed the same pseudodisambiguation method to test whether Web-based
frequencies can be used for distinguishing between seen and artificially constructed
unseen bigrams. We obtained the data sets of Rooth et al. (1999), Prescher, Riezler,
and Rooth (2000), and Clark and Weir (2002) described above. Given a set of (v, n, v&apos;)
triples, the task was to decide whether (v, n) or (v&apos;, n) was the correct pair. We obtained
AltaVista counts for f (v, n), f (v&apos;, n), f (v), and f (v&apos;) as described in Section 2.3.9 Then
we used two models for pseudodisambiguation: the joint probability model compared
the joint probability estimates f (v, n) and f (v&apos;, n) and predicted that the bigram with
the highest estimate is the seen one. The conditional probability model compared the
conditional probability estimates f (v, n)/f (v) and f (v&apos;, n)/f (v&apos;) and again selected as
the seen bigram the one with the highest estimate (in both cases, ties were resolved
by choosing at random).10 The same two models were used to perform pseudodisam-
biguation for the (v, n, n&apos;) triples, where we have to choose between (v, n) and (v, n&apos;).
Here, the probability estimates f (v, n) and f (v, n&apos;) were used for the joint probability
model, and f (v, n)/f (n) and f (v, n&apos;)/f (n&apos;) for the conditional probability model.
The results for Rooth et al.’s (1999) data set are given in Table 13. The conditional
probability model achieves a performance of 71.2% correct for subjects and 85.2% cor-
rect for objects. The performance on the whole data set is 77.7%, which is below the
performance of 80.0% reported by Rooth et al. (1999). However, the difference is not
found to be significant using a chi-square test comparing the number of correct and
incorrect classifications (x2(1) = 2.02, p = .16). The joint probability model performs
consistently worse than the conditional probability model: It achieves an overall accu-
racy of 72.7%, which is significantly lower than the accuracy of the Rooth et al. (1999)
model (x2(1) = 19.50, p &lt; .01).
</bodyText>
<footnote confidence="0.934329666666667">
9 We used only AltaVista counts, as there was virtually no difference between AltaVista and Google
counts in our previous evaluations (see Sections 3.1–3.3). Google allows only 1,000 queries per day (for
registered users), which makes it time-consuming to obtain large numbers of Google counts. AltaVista
has no such restriction.
10 The probability estimates are P(a, b) = f (a, b)/N and P(alb) = f (a, b)/f (a) for the joint probability and
the conditional probability, respectively. However, the corpus size N can be ignored, as it is constant.
</footnote>
<page confidence="0.993331">
479
</page>
<note confidence="0.727164">
Computational Linguistics Volume 29, Number 3
</note>
<bodyText confidence="0.9999459">
A similar picture emerges with regard to Prescher, Riezler, and Rooth’s (2000) data
set (see Table 14). The conditional probability model achieves an accuracy of 66.7%
for subjects and 70.5% for objects. The combined performance of 68.5% is significantly
lower than the performance of both the VA model (x2(1) = 7.78, p &lt; .01) and the
VO model (x2(1) = 33.28, p &lt; .01) reported by Prescher, Riezler, and Rooth (2000).
Again, the joint probability model performs worse than the conditional probability
model, achieving an overall accuracy of 62.4%.
We also applied our Web-based method to the pseudodisambiguation data set of
Clark and Weir (2002). Here, the conditional probability model reached a performance
of 83.9% correct on the low-frequency data set. This is significantly higher than the
highest performance of 72.4% reported by Clark and Weir (2002) on the same data
set (x2(1) = 115.50, p &lt; .01). The joint probability model performs worse than the
conditional model, at 81.1%. However, this is still significantly better than the best
result of Clark and Weir (2002) (x2(1) = 63.14, p &lt; .01). The same pattern is observed
for the high-frequency data set, on which the conditional probability model achieves
87.7% correct and thus significantly outperforms Clark and Weir (2002), who obtained
73.9% (x2(1) = 283.73, p &lt; .01). The joint probability model achieved 85.3% on this data
set, also significantly outperforming Clark and Weir (2002) (x2(1) = 119.35, p &lt; .01).
To summarize, we demonstrated that the simple Web-based approach proposed
in this article yields results for pseudodisambiguation that outperform class-based
smoothing techniques, such as the ones proposed by Resnik (1993), Li and Abe (1998),
and Clark and Weir (2002). We were also able to show that a Web-based approach is
able to achieve the same performance as an EM-based smoothing model proposed by
Rooth et al. (1999). However, the Web-based approach was not able to outperform the
more sophisticated EM-based model of Prescher, Riezler, and Rooth (2000). Another
result we obtained is that Web-based models that use conditional probabilities (where
unigram frequencies are used to normalize the bigram frequencies) generally outper-
form a more simple-minded approach that relies directly on bigram frequencies for
pseudodisambiguation.
There are a number of reasons why our results regarding pseudodisambiguation
have to be treated with some caution. First of all, the two smoothing methods (i.e.,
EM-based clustering and class-based probability estimation using WordNet) were not
evaluated on the same data set, and therefore the two results are not directly compa-
rable. For instance, Clark and Weir’s (2002) data set is substantially less noisy than
Rooth et al.’s (1999) and Prescher, Riezler, and Rooth’s (2000), as it contains only words
and nouns that occur in WordNet. Furthermore, Stephen Clark (personal communica-
tion, 2003) points out that WordNet-based approaches are at a disadvantage when it
comes to pseudodisambiguation. Pseudodisambiguation assumes that the correct pair
is unseen in the training data; this makes the task deliberately hard, because some of
the pairs might be frequent enough that reliable corpus counts can be obtained with-
out having to use WordNet (using WordNet is likely to be more noisy than using the
actual counts). Another problem with WordNet-based approaches is that they offer
no systematic treatment of word sense ambiguity, which puts them at a disadvan-
tage with respect to approaches that do not rely on a predefined inventory of word
senses.
Finally, recall that the results for the EM-based approaches in Tables 13 and 14 were
obtained on the development set (as pseudodisambiguation was used as a means of
parameter tuning by Rooth et al. [1999] and Prescher, Riezler, and Rooth [2000]). It is
possible that this fact inflates the performance values for the EM-based approaches
(but see note 8).
</bodyText>
<page confidence="0.997001">
480
</page>
<note confidence="0.953637">
Keller and Lapata Web Frequencies for Unseen Bigrams
</note>
<sectionHeader confidence="0.981394" genericHeader="conclusions">
4. Conclusions
</sectionHeader>
<bodyText confidence="0.999977083333333">
This article explored a novel approach to overcoming data sparseness. If a bigram
is unseen in a given corpus, conventional approaches re-create its frequency using
techniques such as back-off, linear interpolation, class-based smoothing or distance-
weighted averaging (see Dagan, Lee, and Pereira [1999] and Lee [1999] for overviews).
The approach proposed here does not re-create the missing counts but instead retrieves
them from a corpus that is much larger (but also much more noisy) than any existing
corpus: it launches queries to a search engine in order to determine how often the
bigram occurs on the Web.
We systematically investigated the validity of this approach by using it to obtain
frequencies for predicate-argument bigrams (adjective-noun, noun-noun, and verb-
object bigrams). We first applied the approach to seen bigrams randomly sampled from
the BNC. We found that the counts obtained from the Web are highly correlated with
the counts obtained from the BNC. We then obtained bigram counts from NANTC,
a corpus that is substantially larger than the BNC. Again, we found that Web counts
are highly correlated with corpus counts. This indicates that Web queries can generate
frequencies that are comparable to the ones obtained from a balanced, carefully edited
corpus such as the BNC, but also from a large news text corpus such as NANTC.
Secondly, we performed an evaluation that used the Web frequencies to predict
human plausibility judgments for predicate-argument bigrams. The results show that
Web counts correlate reliably with judgments, for all three types of predicate-argument
bigrams tested, both seen and unseen. For the seen bigrams, we showed that the Web
frequencies correlate better with judged plausibility than corpus frequencies.
To substantiate the claim that the Web counts can be used to overcome data sparse-
ness, we compared bigram counts obtained from the Web with bigram counts re-
created using a class-based smoothing technique (a variant of the one proposed by
Resnik [1993]). We found that Web frequencies and re-created frequencies are reliably
correlated, and that Web frequencies are better at predicting plausibility judgments
than smoothed frequencies. This holds both for unseen bigrams and for seen bigrams
that are treated as unseen by omitting them from the training corpus.
Finally, we tested the performance of our frequencies in a standard pseudodis-
ambiguation task. We applied our method to three data sets from the literature. The
results show that Web counts outperform counts re-created using a number of class-
based smoothing techniques. However, counts re-created using an EM-based smooth-
ing approach yielded better pseudodisambiguation performance than Web counts.
To summarize, we have proposed a simple heuristic method for obtaining bigram
counts from the Web. Using four different types of evaluation, we demonstrated that
this simple heuristic method is sufficient to obtain valid frequency estimates. It seems
that the large amount of data available outweighs the problems associated with using
the Web as a corpus (such as the fact that it is noisy and unbalanced).
A number of questions arise for future research: (1) Are Web frequencies suitable
for probabilistic modeling, in particular since Web counts are not perfectly normalized,
as Zhu and Rosenfeld (2001) have shown? (2) How can existing smoothing methods
benefit from Web counts? (3) How do the results reported in this article carry over
to languages other than English (for which a much smaller amount of Web data are
available)? (4) What is the effect of the noise introduced by our heuristic approach?
The last question could be assessed by reproducing our results using a snapshot of
the Web, from which argument relations can be extracted more accurately using POS
tagging and chunking techniques.
</bodyText>
<page confidence="0.994915">
481
</page>
<note confidence="0.703418">
Computational Linguistics Volume 29, Number 3
</note>
<bodyText confidence="0.9998724">
Finally, it will be crucial to test the usefulness of Web-based frequencies for realistic
NLP tasks. Preliminary results are reported by Lapata and Keller (2003), who use Web
counts successfully for a range of NLP tasks, including candidate selection for ma-
chine translation, context-sensitive spelling correction, bracketing and interpretation
of compounds, adjective ordering, and PP attachment.
</bodyText>
<sectionHeader confidence="0.985071" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.615027444444444">
This work was conducted while both
authors were at the Department of
Computational Linguistics, Saarland
University, Saarbr¨ucken. The work was
inspired by a talk that Gregory Grefenstette
gave in Saarbr¨ucken in 2001 about his
research on using the Web as a corpus. The
present article is an extended and revised
version of Keller, Lapata, and Ourioupina
(2002). Stephen Clark and Stefan Riezler
provided valuable comments on this
research. We are also grateful to four
anonymous reviewers for Computational
Linguistics; their feedback helped to
substantially improve the present article.
Special thanks are due to Stephen Clark and
Detlef Prescher for making their
pseudodisambiguation data sets available.
</bodyText>
<sectionHeader confidence="0.985382" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998135690476191">
Abney, Steve. 1996. Partial parsing via
finite-state cascades. In John Carroll,
editor, Workshop on Robust Parsing Eighth
European Summer School in Logic, Language
and Information, pages 8–15, Prague, Czech
Republic.
Abney, Steve and Marc Light. 1999. Hiding
a semantic class hierarchy in a Markov
model. In Andrew Kehler and Andreas
Stolcke, editors, Proceedings of the ACL
Workshop on Unsupervised Learning in
Natural Language Processing, pages 1–8,
College Park, MD.
Agirre, Eneko and David Martinez. 2000.
Exploring automatic word sense
disambiguation with decision lists and the
Web. In Proceedings of the 18th International
Conference on Computational Linguistics,
pages 11–19, Saarbr¨ucken, Germany.
Banko, Michele and Eric Brill. 2001a.
Mitigating the paucity-of-data problem:
Exploring the effect of training corpus
size on classifier performance for natural
language processing. In James Allan,
editor, Proceedings of the First International
Conference on Human Language Technology
Research. Morgan Kaufmann, San
Francisco.
Banko, Michele and Eric Brill. 2001b. Scaling
to very very large corpora for natural
language disambiguation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics and the
10th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 26–33, Toulouse, France.
Bard, Ellen Gurman, Dan Robertson, and
Antonella Sorace. 1996. Magnitude
estimation of linguistic acceptability.
Language, 72(1):32–68.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the Fifth
Conference on Applied Natural Language
Processing, pages 356–363, Washington,
DC.
Burnard, Lou, editor, 1995. Users Reference
Guide, British National Corpus. British
National Corpus Consortium, Oxford
University Computing Services, Oxford,
England.
Carroll, Glenn and Mats Rooth. 1998.
Valence induction with a head-lexicalized
PCFG. In Nancy Ide and Atro Voutilainen,
editors, Proceedings of the Third Conference
on Empirical Methods in Natural Language
Processing, pages 36–45, Granada, Spain.
Clark, Stephen and David Weir. 2001.
Class-based probability estimation using a
semantic hierarchy. In Proceedings of the
Second Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 95–102, Pittsburgh, PA.
Clark, Stephen and David Weir. 2002.
Class-based probability estimation using a
semantic hierarchy. Computational
Linguistics, 28(2):187–206.
Corley, Martin and Christoph Scheepers.
2002. Syntactic priming in English
sentence production: Categorical and
latency evidence from an Internet-based
study. Psychonomic Bulletin and Review,
9(1):126–131.
Corley, Steffan, Martin Corley, Frank Keller,
Matthew W. Crocker, and Shari Trewin.
2001. Finding syntactic structure in
unparsed corpora: The Gsearch corpus
query system. Computers and the
Humanities, 35(2):81–94.
Cowart, Wayne. 1997. Experimental Syntax:
Applying Objective Methods to Sentence
Judgments. Sage, Thousand Oaks, CA.
Curran, James. 2002. Scaling context space.
In Proceedings of the 40th Annual Meeting of
</reference>
<page confidence="0.997142">
482
</page>
<note confidence="0.954618">
Keller and Lapata Web Frequencies for Unseen Bigrams
</note>
<reference confidence="0.995735680327869">
the Association for Computational Linguistics,
pages 231–238, Philadelphia.
Dagan, Ido, Lillian Lee, and Fernando
Pereira. 1999. Similarity-based models of
word cooccurrence probabilities. Machine
Learning, 34(1):43–69.
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic, Boston.
Grefenstette, Gregory. 1998. The World
Wide Web as a resource for
example-based machine translation tasks.
In Proceedings of the ASLIB Conference on
Translating and the Computer, London.
Grefenstette, Gregory and Jean Nioche.
2000. Estimation of English and
non-English language use on the WWW.
In Proceedings of the RIAO Conference on
Content-Based Multimedia Information
Access, pages 237–246, Paris.
Grishman, Ralph and John Sterling. 1994.
Generalizing automatically generated
selectional patterns. In Proceedings of the
15th International Conference on
Computational Linguistics, pages 742–747,
Kyoto, Japan.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103–120.
Jones, Rosie and Rayid Ghani. 2000.
Automatically building a corpus for a
minority language from the Web. In
Proceedings of the Student Research Workshop
at the 38th Annual Meeting of the Association
for Computational Linguistics, pages 29–36,
Hong Kong.
Karp, Daniel, Yves Schabes, Martin Zaidel,
and Dania Egedi. 1992. A freely available
wide coverage morphological analyzer for
English. In Proceedings of the 14th
International Conference on Computational
Linguistics, pages 950–954, Nantes, France.
Katz, Slava M. 1987. Estimation of
probabilities from sparse data for the
language model component of a speech
recognizer. IEEE Transactions on Acoustics
Speech and Signal Processing, 33(3):400–401.
Keller, Frank and Theodora Alexopoulou.
2001. Phonology competes with syntax:
Experimental evidence for the interaction
of word order and accent placement in
the realization of information structure.
Cognition, 79(3):301–372.
Keller, Frank and Ash Asudeh. 2001.
Constraints on linguistic coreference:
Structural vs. pragmatic factors. In
Johanna D. Moore and Keith Stenning,
editors, Proceedings of the 23rd Annual
Conference of the Cognitive Science Society,
pages 483–488. Erlbaum, Mahwah, NJ.
Keller, Frank, Martin Corley, Steffan Corley,
Lars Konieczny, and Amalia Todirascu.
1998. WebExp: A Java toolbox for
Web-based psychological experiments.
Technical Report HCRC/TR-99, Human
Communication Research Centre,
University of Edinburgh.
Keller, Frank, Maria Lapata, and Olga
Ourioupina. 2002. Using the Web to
overcome data sparseness. In Jan Hajiˇc
and Yuji Matsumoto, editors, Proceedings
of the Conference on Empirical Methods in
Natural Language Processing, pages
230–237, Philadelphia.
Lapata, Maria. 2001. A corpus-based
account of regular polysemy: The case of
context-sensitive adjectives. In Proceedings
of the Second Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 63–70,
Pittsburgh, PA.
Lapata, Maria. 2002. The disambiguation of
nominalizations. Computational Linguistics,
28(3):357–388.
Lapata, Maria and Frank Keller. 2003.
Evaluating the performance of
unsupervised Web-based models for a
range of NLP tasks. Unpublished
manuscript, University of Sheffield,
Sheffield, England, and University of
Edinburgh, Edinburgh, Scotland.
Lapata, Maria, Frank Keller, and Scott
McDonald. 2001. Evaluating smoothing
algorithms against plausibility judgments.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics
and the 10th Conference of the European
Chapter of the Association for Computational
Linguistics, pages 346–353, Toulouse,
France.
Lapata, Maria, Scott McDonald, and Frank
Keller. 1999. Determinants of
adjective-noun plausibility. In Proceedings
of the Ninth Conference of the European
Chapter of the Association for Computational
Linguistics, pages 30–36, Bergen, Norway.
Lauer, Mark. 1995. Designing Statistical
Language Learners: Experiments on
Compound Nouns. Ph.D. thesis, Macquarie
University, Sydney, Australia.
Lee, Lilian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics, pages 25–32, College Park,
MD.
Leech, Geoffrey, Roger Garside, and Michael
Bryant. 1994. The tagging of the British
national corpus. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 622–628, Kyoto, Japan.
Levi, Judith N. 1978. The Syntax and
Semantics of Complex Nominals. Academic
</reference>
<page confidence="0.981201">
483
</page>
<note confidence="0.3624">
Computational Linguistics Volume 29, Number 3
</note>
<reference confidence="0.999695841666667">
Press, New York.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217–244.
Lin, Dekang. 1994. PRINCIPAR—An
efficient broad-coverage, principle-based
parser. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 482–488, Kyoto, Japan.
Lin, Dekang. 1998. Dependency-based
evaluation of MINIPAR. In Proceedings of
the LREC Workshop on the Evaluation of
Parsing Systems, pages 48–56, Granada,
Spain.
Lin, Dekang. 2001. LaTaT: Language and
text analysis tools. In Proceedings of the
First International Conference on Human
Language Technology Research. Morgan
Kaufmann, San Francisco.
Markert, Katja, Malvina Nissim, and
Natalia N. Modjeska. 2003. Using the Web
for nominal anaphora resolution. In
Proceedings of the EACL Workshop on the
Computational Treatment of Anaphora,
Budapest, pages 39–46.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal
participation in role switching
alternations. In Proceedings of the First
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 256–263, Seattle, WA.
Mihalcea, Rada and Dan Moldovan. 1999. A
method for word sense disambiguation of
unrestricted text. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 152–158,
College Park, MD.
Miller, George A., Richard Beckwith,
Christiane Fellbaum, Derek Gross, and
Katherine J. Miller. 1990. Introduction to
WordNet: An on-line lexical database.
International Journal of Lexicography,
3(4):235–244.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics, pages 183–190,
Columbus, OH.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press, Chicago.
Prescher, Detlef, Stefan Riezler, and Mats
Rooth. 2000. Using a probabilistic
class-based lexicon for lexical ambiguity
resolution. In Proceedings of the 18th
International Conference on Computational
Linguistics, pages 649–655, Saarbr¨ucken,
Germany.
Ratnaparkhi, Adwait. 1998. Unsupervised
statistical models for prepositional phrase
attachment. In Proceedings of the
17th International Conference on
Computational Linguistics and 36th Annual
Meeting of the Association for Computational
Linguistics, pages 1079–1085, Montr ´eal.
Resnik, Philip. 1993. Selection and
Information: A Class-Based Approach to
Lexical Relationships. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Resnik, Philip. 1999. Mining the Web for
bilingual text. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 527–534,
College Park, MD.
Resnik, Philip. 2000. Measuring verb
similarity. In Lila R. Gleitman and
Aravid K. Joshi, editors, Proceedings of the
22nd Annual Conference of the Cognitive
Science Society, pages 399–404. Erlbaum,
Mahwah, NJ.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated lexicon
via EM-based clustering. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 104–111,
College Park, MD.
Sampson, Geoffrey. 1995. English for the
Computer: The SUSANNE Corpus and
Analytic Scheme. Oxford University Press,
Oxford.
Sch¨utze, Carson T. 1996. The Empirical Base of
Linguistics: Grammaticality Judgments and
Linguistic Methodology. University of
Chicago Press, Chicago.
Stevens, S. S. 1975. Psychophysics:
Introduction to its Perceptual, Neural, and
Social Prospects. John Wiley, New York.
Volk, Martin. 2001. Exploiting the WWW as
a corpus to resolve PP attachment
ambiguities. In Paul Rayson, Andrew
Wilson, Tony McEnery, Andrew Hardie,
and Shereen Khoja, editors, Proceedings of
the Corpus Linguistics Conference, pages
601–606, Lancaster, England.
Weiss, Sholom M. and Casimir A.
Kulikowski. 1991. Computer Systems That
Learn: Classification and Prediction Methods
from Statistics, Neural Nets, Machine
Learning, and Expert Systems. Morgan
Kaufmann, San Mateo, CA.
Zhu, Xiaojin and Ronald Rosenfeld. 2001.
Improving trigram language modeling
with the World Wide Web. In Proceedings
of the International Conference on Acoustics
Speech and Signal Processing, Salt Lake City,
UT.
</reference>
<page confidence="0.998953">
484
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.843607">
<title confidence="0.9967005">Using the Web to Obtain Frequencies for Unseen Bigrams</title>
<author confidence="0.992362">Mirella</author>
<affiliation confidence="0.925743">University of Edinburgh University of Sheffield</affiliation>
<abstract confidence="0.989226428571429">This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steve Abney</author>
</authors>
<title>Partial parsing via finite-state cascades. In</title>
<date>1996</date>
<booktitle>Workshop on Robust Parsing Eighth European Summer School in Logic, Language and Information,</booktitle>
<pages>8--15</pages>
<editor>John Carroll, editor,</editor>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="14226" citStr="Abney 1996" startWordPosition="2207" endWordPosition="2208">en participant, muscle, grade score, quota, chest lift, bride, listener 462 Keller and Lapata Web Frequencies for Unseen Bigrams noun. Lauer’s heuristic (see (1)) effectively avoids identifying as two-word compounds noun sequences that are part of a larger compound. (1) C = {(w2,w3) |w1w2w3w4; w1, w4 E� N; w2, w3 E N} Here, w1 w2 w3 w4 denotes the occurrence of a sequence of four words and N is the set of words tagged as nouns in the corpus. C is the set of compounds identified by Lauer’s (1995) heuristic. Verb-object bigrams for the 30 preselected verbs were obtained from the BNC using Cass (Abney 1996), a robust chunk parser designed for the shallow analysis of noisy text. The parser recognizes chunks and simplex clauses (i.e., sequences of nonrecursive clauses) using a regular expression grammar and a part-of-speech-tagged corpus, without attempting to resolve attachment ambiguities. It comes with a largescale grammar for English and a built-in tool that extracts predicate-argument tuples out of the parse trees that Cass produces. The parser’s output was postprocessed to remove bracketing errors and errors in identifying chunk categories that could potentially result in bigrams whose membe</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Abney, Steve. 1996. Partial parsing via finite-state cascades. In John Carroll, editor, Workshop on Robust Parsing Eighth European Summer School in Logic, Language and Information, pages 8–15, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Abney</author>
<author>Marc Light</author>
</authors>
<title>Hiding a semantic class hierarchy in a Markov model.</title>
<date>1999</date>
<booktitle>In Andrew Kehler and Andreas Stolcke, editors, Proceedings of the ACL Workshop on Unsupervised Learning in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<location>College Park, MD.</location>
<contexts>
<context position="18579" citStr="Abney and Light 1999" startWordPosition="2853" endWordPosition="2856"> the chunk category and chunk length identified correctly). Despite their imperfect output, heuristic methods for the extraction of syntactic relations are relatively common in statistical NLP. Several statistical models employ frequencies obtained from the output of partial parsers and other heuristic methods; these include models for disambiguating the attachment site of prepositional phrases (Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns (Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the induction of selectional preferences (Abney and Light 1999), methods for automatically clustering words according to their distribution in particular syntactic contexts (Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994; Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee 1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways for obtaining bigram frequencies that are potentially useful for such models despite the fact that some of these bigrams are identified in a heuristic manner and may be noisy. 2.2 Sampling Bigrams from the NANTC We also obtained corpus co</context>
</contexts>
<marker>Abney, Light, 1999</marker>
<rawString>Abney, Steve and Marc Light. 1999. Hiding a semantic class hierarchy in a Markov model. In Andrew Kehler and Andreas Stolcke, editors, Proceedings of the ACL Workshop on Unsupervised Learning in Natural Language Processing, pages 1–8, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
</authors>
<title>Exploring automatic word sense disambiguation with decision lists and the Web.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>11--19</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="2317" citStr="Agirre and Martinez (2000)" startWordPosition="350" endWordPosition="353"> provide enormous potential for training NLP algorithms, if Banko and Brill’s (2001a, 2001b) findings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax. There is a small body of existing research that tries to harness the potential of the Web for NLP. Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, * School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk † Department of Computer Science, 211 Portobello Street, Sheffield S1 4DP, UK. E-mail: mlap@dcs.shef.ac.uk 1 A reviewer points out that information providers such as Lexis Nexis (http://www.lexisnexis.com/) might have databases that are even larger than the Web. Lexis Nexis provides</context>
</contexts>
<marker>Agirre, Martinez, 2000</marker>
<rawString>Agirre, Eneko and David Martinez. 2000. Exploring automatic word sense disambiguation with decision lists and the Web. In Proceedings of the 18th International Conference on Computational Linguistics, pages 11–19, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Mitigating the paucity-of-data problem: Exploring the effect of training corpus size on classifier performance for natural language processing.</title>
<date>2001</date>
<booktitle>Proceedings of the First International Conference on Human Language Technology Research.</booktitle>
<editor>In James Allan, editor,</editor>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="821" citStr="Banko and Brill (2001" startWordPosition="118" endWordPosition="121">es for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task. 1. Introduction In two recent papers, Banko and Brill (2001a, 2001b) criticize the fact that current NLP algorithms are typically optimized, tested, and compared on fairly small data sets (corpora with millions of words), even though data sets several orders of magnitude larger are available, at least for some NLP tasks. Banko and Brill (2001a, 2001b) experiment with context-sensitive spelling correction, a task for which large amounts of data can be obtained straightforwardly, as no manual annotation is required. They demonstrate that the learning algorithms typically used for spelling correction benefit significantly from larger training sets, and t</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Banko, Michele and Eric Brill. 2001a. Mitigating the paucity-of-data problem: Exploring the effect of training corpus size on classifier performance for natural language processing. In James Allan, editor, Proceedings of the First International Conference on Human Language Technology Research. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics and the 10th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="821" citStr="Banko and Brill (2001" startWordPosition="118" endWordPosition="121">es for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task. 1. Introduction In two recent papers, Banko and Brill (2001a, 2001b) criticize the fact that current NLP algorithms are typically optimized, tested, and compared on fairly small data sets (corpora with millions of words), even though data sets several orders of magnitude larger are available, at least for some NLP tasks. Banko and Brill (2001a, 2001b) experiment with context-sensitive spelling correction, a task for which large amounts of data can be obtained straightforwardly, as no manual annotation is required. They demonstrate that the learning algorithms typically used for spelling correction benefit significantly from larger training sets, and t</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Banko, Michele and Eric Brill. 2001b. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics and the 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 26–33, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Gurman Bard</author>
<author>Dan Robertson</author>
<author>Antonella Sorace</author>
</authors>
<title>Magnitude estimation of linguistic acceptability.</title>
<date>1996</date>
<journal>Language,</journal>
<volume>72</volume>
<issue>1</issue>
<marker>Bard, Robertson, Sorace, 1996</marker>
<rawString>Bard, Ellen Gurman, Dan Robertson, and Antonella Sorace. 1996. Magnitude estimation of linguistic acceptability. Language, 72(1):32–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>356--363</pages>
<location>Washington, DC.</location>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Briscoe, Ted and John Carroll. 1997. Automatic extraction of subcategorization from corpora. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 356–363, Washington, DC.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Users Reference Guide, British National Corpus. British National Corpus Consortium, Oxford University Computing Services,</booktitle>
<editor>Burnard, Lou, editor,</editor>
<location>Oxford, England.</location>
<contexts>
<context position="7488" citStr="[1995]" startWordPosition="1131" endWordPosition="1131">erent predicateargument combinations. We evaluate our Web counts in four ways: (a) comparison with actual corpus frequencies from two different corpora, (b) comparison with human plausibility judgments, (c) comparison with frequencies re-created using class-based smoothing, and (d) performance in a pseudodisambiguation task on data sets from the literature. 460 Keller and Lapata Web Frequencies for Unseen Bigrams 2. Obtaining Frequencies from the Web 2.1 Sampling Bigrams from the BNC The data sets used in the present experiment were obtained from the British National Corpus (BNC) (see Burnard [1995]). The BNC is a large, synchronic corpus, consisting of 90 million words of text and 10 million words of speech. The BNC is a balanced corpus (i.e., it was compiled so as to represent a wide range of present day British English). The written part includes samples from newspapers, magazines, books (both academic and fiction), letters, and school and university essays, among other kinds of text. The spoken part consists of spontaneous conversations, recorded from volunteers balanced by age, region, and social class. Other samples of spoken language are also included, ranging from business or gov</context>
</contexts>
<marker>1995</marker>
<rawString>Burnard, Lou, editor, 1995. Users Reference Guide, British National Corpus. British National Corpus Consortium, Oxford University Computing Services, Oxford, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Mats Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized PCFG.</title>
<date>1998</date>
<booktitle>In Nancy Ide and Atro Voutilainen, editors, Proceedings of the Third Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>36--45</pages>
<location>Granada, Spain.</location>
<marker>Carroll, Rooth, 1998</marker>
<rawString>Carroll, Glenn and Mats Rooth. 1998. Valence induction with a head-lexicalized PCFG. In Nancy Ide and Atro Voutilainen, editors, Proceedings of the Third Conference on Empirical Methods in Natural Language Processing, pages 36–45, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>95--102</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="60524" citStr="Clark and Weir 2001" startWordPosition="9609" endWordPosition="9612"> counts and frequencies re-created using all available smoothing techniques (and all available taxonomies that might be used for class-based smoothing). The smoothing method discussed above is simply one type of class-based smoothing. Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing</context>
<context position="62441" citStr="Clark and Weir (2001)" startWordPosition="9901" endWordPosition="9904">unseen bigrams in a given corpus. A set of pseudobigrams is constructed according to a set of criteria (detailed below) that ensure that they are unattested in the training corpus. Then the seen bigrams are removed from the training data, and the smoothing method is used to re-create the frequencies of both the seen bigrams and the pseudobigrams. The smoothing method is then evaluated by comparing the frequencies it re-creates for both types of bigrams. We evaluated our Web counts by applying the pseudodisambiguation procedure that Rooth et al. (1999), Prescher, Riezler, and Rooth (2000), and Clark and Weir (2001) employed for evaluating re-created verb-object bigram counts. In this procedure, the noun n from a verb-object bigram (v, n) that is seen in a given corpus is paired with a randomly chosen verb v&apos; that does not take n as its object within the corpus. This results in an unseen verb-object bigram (v&apos;, n). The seen bigram is now treated as unseen (i.e., all of its occurrences are removed from the training corpus), and the frequencies of both the seen and the unseen bigram are re-created (using smoothing, or Web counts, in our case). The task is then to decide which of the two verbs v and v&apos; take</context>
</contexts>
<marker>Clark, Weir, 2001</marker>
<rawString>Clark, Stephen and David Weir. 2001. Class-based probability estimation using a semantic hierarchy. In Proceedings of the Second Conference of the North American Chapter of the Association for Computational Linguistics, pages 95–102, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="67470" citStr="Clark and Weir (2002)" startWordPosition="10736" endWordPosition="10739">(i.e., on the same data set that was used to optimize the parameters), not on a completely unseen test set. This procedure is well-justified in the context of Rooth et al.’s (1999) and Prescher, Riezler, and Rooth’s (2000) work, which aimed at building models of lexical semantics, not of pseudodisambiguation. Therefore, they carried out their final evaluations on unseen test sets for the tasks of lexicon induction (Rooth et al. 1999) and target language disambiguation (Prescher, Riezler, and Rooth 2000), once the model parameters had been fixed using the pseudodisambiguation development set.8 Clark and Weir (2002) use a setting similar to that of Rooth et al. (1999) and Prescher, Riezler, and Rooth (2000); here pseudodisambiguation is employed to evaluate the performance of a class-based probability estimation method. In order to address the problem of estimating conditional probabilities in the face of sparse data, Clark and Weir (2002) define probabilities in terms of classes in a semantic hierarchy and propose hypothesis testing as a means of determining a suitable level of generalization in the hierarchy. Clark and Weir (2002) report pseudodisambiguation results on two data sets, with an experiment</context>
<context position="69174" citStr="Clark and Weir 2002" startWordPosition="11006" endWordPosition="11009">ot seem to make a difference at all). Figure 3 of Rooth et al. (1999) shows that a performance of more than 75% is obtained for every reasonable choice of classes. This indicates that a “proper” pseudodisambiguation setting with separate development and test data would have resulted in a similar choice of class cardinality and thus achieved the same 80% performance that is cited in Table 13. 478 Keller and Lapata Web Frequencies for Unseen Bigrams Table 15 Percentage of correct disambiguations on the pseudodisambiguation task using Web counts and counts re-created using class-based smoothing (Clark and Weir 2002). Data Set N AltaVista AltaVista Clark and Li and Resnik Conditional Joint Probability Weir Abe Probability Objects (low frequency) 3000 83.9 81.1 72.4 62.9 62.6 Objects (high frequency) 3000 87.7 85.3 73.9 68.3 63.9 tained relatively frequent verbs (occurring between 500 and 5,000 times in the data). The data sets were constructed as proposed by Rooth et al. (1999). The procedure for creating the second data set was identical, but this time only verbs that occurred between 100 and 1,000 times were considered. Clark and Weir (2002) further compared their approach with Resnik’s (1993) selection</context>
<context position="70432" citStr="Clark and Weir (2002)" startWordPosition="11209" endWordPosition="11212"> (1998) tree cut model on the same data sets. These methods are directly comparable, as they can be used for class-based probability estimation and address the question of how to find a suitable level of generalization in a hierarchy (i.e., WordNet). The results of the three methods used on the two data sets are shown in Table 15. We employed the same pseudodisambiguation method to test whether Web-based frequencies can be used for distinguishing between seen and artificially constructed unseen bigrams. We obtained the data sets of Rooth et al. (1999), Prescher, Riezler, and Rooth (2000), and Clark and Weir (2002) described above. Given a set of (v, n, v&apos;) triples, the task was to decide whether (v, n) or (v&apos;, n) was the correct pair. We obtained AltaVista counts for f (v, n), f (v&apos;, n), f (v), and f (v&apos;) as described in Section 2.3.9 Then we used two models for pseudodisambiguation: the joint probability model compared the joint probability estimates f (v, n) and f (v&apos;, n) and predicted that the bigram with the highest estimate is the seen one. The conditional probability model compared the conditional probability estimates f (v, n)/f (v) and f (v&apos;, n)/f (v&apos;) and again selected as the seen bigram the </context>
<context position="73404" citStr="Clark and Weir (2002)" startWordPosition="11714" endWordPosition="11717"> with regard to Prescher, Riezler, and Rooth’s (2000) data set (see Table 14). The conditional probability model achieves an accuracy of 66.7% for subjects and 70.5% for objects. The combined performance of 68.5% is significantly lower than the performance of both the VA model (x2(1) = 7.78, p &lt; .01) and the VO model (x2(1) = 33.28, p &lt; .01) reported by Prescher, Riezler, and Rooth (2000). Again, the joint probability model performs worse than the conditional probability model, achieving an overall accuracy of 62.4%. We also applied our Web-based method to the pseudodisambiguation data set of Clark and Weir (2002). Here, the conditional probability model reached a performance of 83.9% correct on the low-frequency data set. This is significantly higher than the highest performance of 72.4% reported by Clark and Weir (2002) on the same data set (x2(1) = 115.50, p &lt; .01). The joint probability model performs worse than the conditional model, at 81.1%. However, this is still significantly better than the best result of Clark and Weir (2002) (x2(1) = 63.14, p &lt; .01). The same pattern is observed for the high-frequency data set, on which the conditional probability model achieves 87.7% correct and thus signi</context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>Clark, Stephen and David Weir. 2002. Class-based probability estimation using a semantic hierarchy. Computational Linguistics, 28(2):187–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Corley</author>
<author>Christoph Scheepers</author>
</authors>
<title>Syntactic priming in English sentence production: Categorical and latency evidence from an Internet-based study.</title>
<date>2002</date>
<journal>Psychonomic Bulletin and Review,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="43023" citStr="Corley and Scheepers 2002" startWordPosition="6834" endWordPosition="6837">ding maximally finegraded data and avoiding the known problems with the conventional ordinal scales for linguistic data (Bard, Robertson, and Sorace 1996; Cowart 1997; Sch¨utze 1996). The experiments reported in this article were carried out using the WebExp software package (Keller et al. 1998). A series of previous studies has shown that data obtained using WebExp closely replicate results obtained in a controlled laboratory setting; this has been demonstrated for acceptability judgments (Keller and Alexopoulou 2001), coreference judgments (Keller and Asudeh 2001), and sentence completions (Corley and Scheepers 2002). In the present experiments, subjects were presented with bigram pairs and were asked to rate the degree of plausibility proportional to a modulus item. They first saw a set of instructions that explained the ME technique and the judgment task. The concept of plausibility was not defined, but examples of plausible and implausible bigrams were given (different examples for each stimulus set). Then subjects were asked to fill in a questionnaire with basic demographic information. The experiment proper 471 Computational Linguistics Volume 29, Number 3 Table 10 Descriptive statistics for plausibi</context>
</contexts>
<marker>Corley, Scheepers, 2002</marker>
<rawString>Corley, Martin and Christoph Scheepers. 2002. Syntactic priming in English sentence production: Categorical and latency evidence from an Internet-based study. Psychonomic Bulletin and Review, 9(1):126–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffan Corley</author>
<author>Martin Corley</author>
<author>Frank Keller</author>
<author>Matthew W Crocker</author>
<author>Shari Trewin</author>
</authors>
<title>Finding syntactic structure in unparsed corpora: The Gsearch corpus query system.</title>
<date>2001</date>
<booktitle>Computers and the Humanities,</booktitle>
<pages>35--2</pages>
<contexts>
<context position="10117" citStr="Corley et al. 2001" startWordPosition="1553" endWordPosition="1556">ller et al. 1990) and was unambiguously tagged as “adjective” 98.6% of the time. Lapata, McDonald, and Keller used the part-of-speech-tagged version that is made available with the BNC and was tagged using CLAWS4 (Leech, Garside, and Bryant 1994), a probabilistic partof-speech tagger, with error rate ranging from 3% to 4%. The lemmatized version of the corpus was obtained using Karp et al.’s (1992) morphological analyzer. The 30 adjectives ranged in BNC frequency from 1.9 to 49.1 per million words; that is, they covered the whole range from fairly infrequent to highly frequent items. Gsearch (Corley et al. 2001), a chart parser that detects syntactic patterns in a tagged corpus by exploiting a user-specified context-free grammar and a syntactic query, was used to extract all nouns occurring in a head-modifier relationship with one of the 30 adjectives. Examples of the syntactic patterns the parser identified are given in Table 1. In the case of adjectives modifying compound nouns, only sequences of two nouns were included, and the rightmost-occurring noun was considered the head. Bigrams involving proper nouns or low-frequency nouns (less than 10 per million words) were discarded. This was necessary </context>
</contexts>
<marker>Corley, Corley, Keller, Crocker, Trewin, 2001</marker>
<rawString>Corley, Steffan, Martin Corley, Frank Keller, Matthew W. Crocker, and Shari Trewin. 2001. Finding syntactic structure in unparsed corpora: The Gsearch corpus query system. Computers and the Humanities, 35(2):81–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Cowart</author>
</authors>
<title>Experimental Syntax: Applying Objective Methods to Sentence Judgments. Sage,</title>
<date>1997</date>
<location>Thousand Oaks, CA.</location>
<contexts>
<context position="41644" citStr="Cowart (1997)" startWordPosition="6627" endWordPosition="6628">l stimuli were based on the six sets of seen or unseen bigrams extracted from the BNC as described in Section 2.1 (adjective-noun, nounnoun, and verb-object bigrams). In the adjective-noun and noun-noun cases, the stimuli consisted simply of the bigrams. In the verb-object case, the bigrams were embedded in a short sentence to make them more natural: A proper-noun subject was added. Procedure. The experimental paradigm was magnitude estimation (ME), a technique standardly used in psychophysics to measure judgments of sensory stimuli (Stevens 1975), which Bard, Robertson, and Sorace (1996) and Cowart (1997) have applied to the elicitation of linguistic judgments. The ME procedure requires subjects to estimate the magnitude of physical stimuli by assigning numerical values proportional to the stimulus magnitude they perceive. In contrast to the five- or seven-point scale conventionally used to measure human intuitions, ME employs an interval scale and therefore produces data for which parametric inferential statistics are valid. ME requires subjects to assign numbers to a series of linguistic stimuli in a proportional fashion. Subjects are first exposed to a modulus item, to which they assign an </context>
<context position="45645" citStr="Cowart 1997" startWordPosition="7258" endWordPosition="7259">ore than once in the same experiment, based on their demographic data and on their Internet connection data, which is logged by WebExp. 3.2.2 Results and Discussion. The experimental data were normalized by dividing each numerical judgment by the modulus value that the subject had assigned to the reference sentence. This operation creates a common scale for all subjects. Then the data were transformed by taking the decadic logarithm. This transformation ensures that the judgments are normally distributed and is standard practice for magnitude estimation data (Bard, Robertson, and Sorace 1996; Cowart 1997; Stevens 1975). All further analyses were conducted on the normalized, log-transformed judgments. Table 10 shows the descriptive statistics for all six judgment experiments: the original experiments by Lapata, McDonald, and Keller (1999) and Lapata, Keller, and McDonald (2001) for adjective-noun bigrams, and our new ones for noun-noun and verb-object bigrams. We used correlation analysis to compare corpus counts and Web counts with plausibility judgments. Table 11 (top half) lists the correlation coefficients that were obtained when correlating log-transformed Web counts (AltaVista and Google</context>
</contexts>
<marker>Cowart, 1997</marker>
<rawString>Cowart, Wayne. 1997. Experimental Syntax: Applying Objective Methods to Sentence Judgments. Sage, Thousand Oaks, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
</authors>
<title>Scaling context space.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>231--238</pages>
<location>Philadelphia.</location>
<contexts>
<context position="18785" citStr="Curran 2002" startWordPosition="2881" endWordPosition="2882">odels employ frequencies obtained from the output of partial parsers and other heuristic methods; these include models for disambiguating the attachment site of prepositional phrases (Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns (Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the induction of selectional preferences (Abney and Light 1999), methods for automatically clustering words according to their distribution in particular syntactic contexts (Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994; Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee 1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways for obtaining bigram frequencies that are potentially useful for such models despite the fact that some of these bigrams are identified in a heuristic manner and may be noisy. 2.2 Sampling Bigrams from the NANTC We also obtained corpus counts from a second corpus, the North American News Text Corpus (NANTC). This corpus differs in several important respects from the BNC. It is substantially larger, as it contains 350 million words of text. </context>
</contexts>
<marker>Curran, 2002</marker>
<rawString>Curran, James. 2002. Scaling context space. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 231–238, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Similarity-based models of word cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<issue>1</issue>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Dagan, Ido, Lillian Lee, and Fernando Pereira. 1999. Similarity-based models of word cooccurrence probabilities. Machine Learning, 34(1):43–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic,</publisher>
<location>Boston.</location>
<contexts>
<context position="18771" citStr="Grefenstette 1994" startWordPosition="2879" endWordPosition="2880">veral statistical models employ frequencies obtained from the output of partial parsers and other heuristic methods; these include models for disambiguating the attachment site of prepositional phrases (Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns (Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the induction of selectional preferences (Abney and Light 1999), methods for automatically clustering words according to their distribution in particular syntactic contexts (Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994; Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee 1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways for obtaining bigram frequencies that are potentially useful for such models despite the fact that some of these bigrams are identified in a heuristic manner and may be noisy. 2.2 Sampling Bigrams from the NANTC We also obtained corpus counts from a second corpus, the North American News Text Corpus (NANTC). This corpus differs in several important respects from the BNC. It is substantially larger, as it contains 350 million w</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette, Gregory. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>The World Wide Web as a resource for example-based machine translation tasks.</title>
<date>1998</date>
<booktitle>In Proceedings of the ASLIB Conference on Translating and the Computer,</booktitle>
<location>London.</location>
<contexts>
<context position="3502" citStr="Grefenstette (1998)" startWordPosition="520" endWordPosition="521">than the Web. Lexis Nexis provides full-text access to news sources (including newspapers, wire services, and broadcast transcripts) and legal data (including case law, codes, regulations, legal news, and law reviews). 2 This is the number of pages indexed by Google in December 2002, as estimated by Search Engine Showdown (http://www.searchengineshowdown.com/). © 2003 Association for Computational Linguistics Computational Linguistics Volume 29, Number 3 and Zhu and Rosenfeld (2001) use Web-based n-gram counts to improve language modeling. A particularly interesting application is proposed by Grefenstette (1998), who uses the Web for example-based machine translation. His task is to translate compounds from French into English, with corpus evidence serving as a filter for candidate translations. An example is the French compound groupe de travail. There are five translations of groupe and three translations for travail (in the dictionary that Grefenstette [1998] is using), resulting in 15 possible candidate translations. Only one of them, namely, work group, has a high corpus frequency, which makes it likely that this is the correct translation into English. Grefenstette (1998) observes that this app</context>
</contexts>
<marker>Grefenstette, 1998</marker>
<rawString>Grefenstette, Gregory. 1998. The World Wide Web as a resource for example-based machine translation tasks. In Proceedings of the ASLIB Conference on Translating and the Computer, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
<author>Jean Nioche</author>
</authors>
<title>Estimation of English and non-English language use on the WWW.</title>
<date>2000</date>
<booktitle>In Proceedings of the RIAO Conference on Content-Based Multimedia Information Access,</booktitle>
<pages>237--246</pages>
<location>Paris.</location>
<contexts>
<context position="2049" citStr="Grefenstette and Nioche (2000)" startWordPosition="306" endWordPosition="309"> that their performance shows no sign of reaching an asymptote as the size of the training set increases. Arguably, the largest data set that is available for NLP is the Web,1 which currently consists of at least 3,033 million pages.2 Data retrieved from the Web therefore provide enormous potential for training NLP algorithms, if Banko and Brill’s (2001a, 2001b) findings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax. There is a small body of existing research that tries to harness the potential of the Web for NLP. Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, * School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk † Department of</context>
<context position="33928" citStr="Grefenstette and Nioche (2000)" startWordPosition="5423" endWordPosition="5426">pages that do not include the search term at all. This can happen if the search term is contained in a link to the page (but not on the page itself). As we did not limit our Web searches to English (even though many search engines now allow the target language for a search to be set), there is also a risk that false positives are generated by cross-linguistic homonyms, that is, by words of other languages that are spelled in the same way as the English words in our data sets. However, this problem is mitigated by the fact that English is by far the most common language on the Web, as shown by Grefenstette and Nioche (2000). Also, the chance of two such homonyms forming a valid bigram in another language is probably fairly small. To summarize, Web counts are certainly less sparse than the counts in a corpus of a fixed size (see Section 2.3). However, Web counts are also likely to be significantly more noisy than counts obtained from a carefully tagged and chunked or parsed corpus, as the examples in this section show. It is therefore essential to carry out a comprehensive evaluation of the Web counts generated by our method. This is the topic of the next section. 3. Evaluation 3.1 Evaluation against Corpus Frequ</context>
</contexts>
<marker>Grefenstette, Nioche, 2000</marker>
<rawString>Grefenstette, Gregory and Jean Nioche. 2000. Estimation of English and non-English language use on the WWW. In Proceedings of the RIAO Conference on Content-Based Multimedia Information Access, pages 237–246, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>John Sterling</author>
</authors>
<title>Generalizing automatically generated selectional patterns.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>742--747</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="60677" citStr="Grishman and Sterling 1994" startWordPosition="9631" endWordPosition="9634">hing). The smoothing method discussed above is simply one type of class-based smoothing. Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. However, the next section will present a small-scale study that compares the performance of several smoothing techniques with the performance o</context>
</contexts>
<marker>Grishman, Sterling, 1994</marker>
<rawString>Grishman, Ralph and John Sterling. 1994. Generalizing automatically generated selectional patterns. In Proceedings of the 15th International Conference on Computational Linguistics, pages 742–747, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="18378" citStr="Hindle and Rooth 1993" startWordPosition="2826" endWordPosition="2829">gainst manually annotated data, Abney’s (1996) parser identified chunks with 87.9% precision and 87.1% recall. The parser further achieved a per-word accuracy of 92.1% (where per-word accuracy includes the chunk category and chunk length identified correctly). Despite their imperfect output, heuristic methods for the extraction of syntactic relations are relatively common in statistical NLP. Several statistical models employ frequencies obtained from the output of partial parsers and other heuristic methods; these include models for disambiguating the attachment site of prepositional phrases (Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns (Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the induction of selectional preferences (Abney and Light 1999), methods for automatically clustering words according to their distribution in particular syntactic contexts (Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994; Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee 1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways for obtaining bigram frequencies that</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Hindle, Donald and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Rayid Ghani</author>
</authors>
<title>Automatically building a corpus for a minority language from the Web.</title>
<date>2000</date>
<booktitle>In Proceedings of the Student Research Workshop at the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>29--36</pages>
<location>Hong Kong.</location>
<contexts>
<context position="2076" citStr="Jones and Ghani (2000)" startWordPosition="311" endWordPosition="314">gn of reaching an asymptote as the size of the training set increases. Arguably, the largest data set that is available for NLP is the Web,1 which currently consists of at least 3,033 million pages.2 Data retrieved from the Web therefore provide enormous potential for training NLP algorithms, if Banko and Brill’s (2001a, 2001b) findings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax. There is a small body of existing research that tries to harness the potential of the Web for NLP. Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, * School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk † Department of Computer Science, 211 Port</context>
</contexts>
<marker>Jones, Ghani, 2000</marker>
<rawString>Jones, Rosie and Rayid Ghani. 2000. Automatically building a corpus for a minority language from the Web. In Proceedings of the Student Research Workshop at the 38th Annual Meeting of the Association for Computational Linguistics, pages 29–36, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Karp</author>
<author>Yves Schabes</author>
<author>Martin Zaidel</author>
<author>Dania Egedi</author>
</authors>
<title>A freely available wide coverage morphological analyzer for English.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics,</booktitle>
<pages>950--954</pages>
<location>Nantes, France.</location>
<marker>Karp, Schabes, Zaidel, Egedi, 1992</marker>
<rawString>Karp, Daniel, Yves Schabes, Martin Zaidel, and Dania Egedi. 1992. A freely available wide coverage morphological analyzer for English. In Proceedings of the 14th International Conference on Computational Linguistics, pages 950–954, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics Speech and Signal Processing,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="60617" citStr="Katz 1987" startWordPosition="9626" endWordPosition="9627">ies that might be used for class-based smoothing). The smoothing method discussed above is simply one type of class-based smoothing. Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. However, the next section will present a small-scale study that compares the perfor</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, Slava M. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics Speech and Signal Processing, 33(3):400–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Theodora Alexopoulou</author>
</authors>
<title>Phonology competes with syntax: Experimental evidence for the interaction of word order and accent placement in the realization of information structure.</title>
<date>2001</date>
<journal>Cognition,</journal>
<volume>79</volume>
<issue>3</issue>
<contexts>
<context position="42921" citStr="Keller and Alexopoulou 2001" startWordPosition="6821" endWordPosition="6824">portional to the modulus. In this way, each subject can establish his or her own rating scale, thus yielding maximally finegraded data and avoiding the known problems with the conventional ordinal scales for linguistic data (Bard, Robertson, and Sorace 1996; Cowart 1997; Sch¨utze 1996). The experiments reported in this article were carried out using the WebExp software package (Keller et al. 1998). A series of previous studies has shown that data obtained using WebExp closely replicate results obtained in a controlled laboratory setting; this has been demonstrated for acceptability judgments (Keller and Alexopoulou 2001), coreference judgments (Keller and Asudeh 2001), and sentence completions (Corley and Scheepers 2002). In the present experiments, subjects were presented with bigram pairs and were asked to rate the degree of plausibility proportional to a modulus item. They first saw a set of instructions that explained the ME technique and the judgment task. The concept of plausibility was not defined, but examples of plausible and implausible bigrams were given (different examples for each stimulus set). Then subjects were asked to fill in a questionnaire with basic demographic information. The experiment</context>
</contexts>
<marker>Keller, Alexopoulou, 2001</marker>
<rawString>Keller, Frank and Theodora Alexopoulou. 2001. Phonology competes with syntax: Experimental evidence for the interaction of word order and accent placement in the realization of information structure. Cognition, 79(3):301–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Ash Asudeh</author>
</authors>
<title>Constraints on linguistic coreference: Structural vs. pragmatic factors.</title>
<date>2001</date>
<booktitle>Proceedings of the 23rd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>483--488</pages>
<editor>In Johanna D. Moore and Keith Stenning, editors,</editor>
<publisher>Erlbaum,</publisher>
<location>Mahwah, NJ.</location>
<contexts>
<context position="42969" citStr="Keller and Asudeh 2001" startWordPosition="6827" endWordPosition="6830">an establish his or her own rating scale, thus yielding maximally finegraded data and avoiding the known problems with the conventional ordinal scales for linguistic data (Bard, Robertson, and Sorace 1996; Cowart 1997; Sch¨utze 1996). The experiments reported in this article were carried out using the WebExp software package (Keller et al. 1998). A series of previous studies has shown that data obtained using WebExp closely replicate results obtained in a controlled laboratory setting; this has been demonstrated for acceptability judgments (Keller and Alexopoulou 2001), coreference judgments (Keller and Asudeh 2001), and sentence completions (Corley and Scheepers 2002). In the present experiments, subjects were presented with bigram pairs and were asked to rate the degree of plausibility proportional to a modulus item. They first saw a set of instructions that explained the ME technique and the judgment task. The concept of plausibility was not defined, but examples of plausible and implausible bigrams were given (different examples for each stimulus set). Then subjects were asked to fill in a questionnaire with basic demographic information. The experiment proper 471 Computational Linguistics Volume 29,</context>
</contexts>
<marker>Keller, Asudeh, 2001</marker>
<rawString>Keller, Frank and Ash Asudeh. 2001. Constraints on linguistic coreference: Structural vs. pragmatic factors. In Johanna D. Moore and Keith Stenning, editors, Proceedings of the 23rd Annual Conference of the Cognitive Science Society, pages 483–488. Erlbaum, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Martin Corley</author>
<author>Steffan Corley</author>
<author>Lars Konieczny</author>
<author>Amalia Todirascu</author>
</authors>
<title>WebExp: A Java toolbox for Web-based psychological experiments.</title>
<date>1998</date>
<tech>Technical Report HCRC/TR-99,</tech>
<institution>Human Communication Research Centre, University of Edinburgh.</institution>
<contexts>
<context position="42693" citStr="Keller et al. 1998" startWordPosition="6788" endWordPosition="6791">ME requires subjects to assign numbers to a series of linguistic stimuli in a proportional fashion. Subjects are first exposed to a modulus item, to which they assign an arbitrary number. All other stimuli are rated proportional to the modulus. In this way, each subject can establish his or her own rating scale, thus yielding maximally finegraded data and avoiding the known problems with the conventional ordinal scales for linguistic data (Bard, Robertson, and Sorace 1996; Cowart 1997; Sch¨utze 1996). The experiments reported in this article were carried out using the WebExp software package (Keller et al. 1998). A series of previous studies has shown that data obtained using WebExp closely replicate results obtained in a controlled laboratory setting; this has been demonstrated for acceptability judgments (Keller and Alexopoulou 2001), coreference judgments (Keller and Asudeh 2001), and sentence completions (Corley and Scheepers 2002). In the present experiments, subjects were presented with bigram pairs and were asked to rate the degree of plausibility proportional to a modulus item. They first saw a set of instructions that explained the ME technique and the judgment task. The concept of plausibil</context>
</contexts>
<marker>Keller, Corley, Corley, Konieczny, Todirascu, 1998</marker>
<rawString>Keller, Frank, Martin Corley, Steffan Corley, Lars Konieczny, and Amalia Todirascu. 1998. WebExp: A Java toolbox for Web-based psychological experiments. Technical Report HCRC/TR-99, Human Communication Research Centre, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Maria Lapata</author>
<author>Olga Ourioupina</author>
</authors>
<title>Using the Web to overcome data sparseness.</title>
<date>2002</date>
<booktitle>In Jan Hajiˇc and Yuji Matsumoto, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>230--237</pages>
<location>Philadelphia.</location>
<marker>Keller, Lapata, Ourioupina, 2002</marker>
<rawString>Keller, Frank, Maria Lapata, and Olga Ourioupina. 2002. Using the Web to overcome data sparseness. In Jan Hajiˇc and Yuji Matsumoto, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 230–237, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
</authors>
<title>A corpus-based account of regular polysemy: The case of context-sensitive adjectives.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>63--70</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="18503" citStr="Lapata 2001" startWordPosition="2844" endWordPosition="2845">eved a per-word accuracy of 92.1% (where per-word accuracy includes the chunk category and chunk length identified correctly). Despite their imperfect output, heuristic methods for the extraction of syntactic relations are relatively common in statistical NLP. Several statistical models employ frequencies obtained from the output of partial parsers and other heuristic methods; these include models for disambiguating the attachment site of prepositional phrases (Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns (Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the induction of selectional preferences (Abney and Light 1999), methods for automatically clustering words according to their distribution in particular syntactic contexts (Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994; Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee 1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways for obtaining bigram frequencies that are potentially useful for such models despite the fact that some of these bigrams are identified in a heuristic manner and </context>
</contexts>
<marker>Lapata, 2001</marker>
<rawString>Lapata, Maria. 2001. A corpus-based account of regular polysemy: The case of context-sensitive adjectives. In Proceedings of the Second Conference of the North American Chapter of the Association for Computational Linguistics, pages 63–70, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
</authors>
<title>The disambiguation of nominalizations.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="18463" citStr="Lapata 2002" startWordPosition="2839" endWordPosition="2840">nd 87.1% recall. The parser further achieved a per-word accuracy of 92.1% (where per-word accuracy includes the chunk category and chunk length identified correctly). Despite their imperfect output, heuristic methods for the extraction of syntactic relations are relatively common in statistical NLP. Several statistical models employ frequencies obtained from the output of partial parsers and other heuristic methods; these include models for disambiguating the attachment site of prepositional phrases (Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns (Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the induction of selectional preferences (Abney and Light 1999), methods for automatically clustering words according to their distribution in particular syntactic contexts (Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994; Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee 1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways for obtaining bigram frequencies that are potentially useful for such models despite the fact that some of these bigrams a</context>
</contexts>
<marker>Lapata, 2002</marker>
<rawString>Lapata, Maria. 2002. The disambiguation of nominalizations. Computational Linguistics, 28(3):357–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
<author>Frank Keller</author>
</authors>
<title>Evaluating the performance of unsupervised Web-based models for a range of NLP tasks.</title>
<date>2003</date>
<institution>University of Sheffield, Sheffield, England, and University of Edinburgh,</institution>
<location>Edinburgh, Scotland.</location>
<note>Unpublished manuscript,</note>
<contexts>
<context position="80810" citStr="Lapata and Keller (2003)" startWordPosition="12873" endWordPosition="12876">the results reported in this article carry over to languages other than English (for which a much smaller amount of Web data are available)? (4) What is the effect of the noise introduced by our heuristic approach? The last question could be assessed by reproducing our results using a snapshot of the Web, from which argument relations can be extracted more accurately using POS tagging and chunking techniques. 481 Computational Linguistics Volume 29, Number 3 Finally, it will be crucial to test the usefulness of Web-based frequencies for realistic NLP tasks. Preliminary results are reported by Lapata and Keller (2003), who use Web counts successfully for a range of NLP tasks, including candidate selection for machine translation, context-sensitive spelling correction, bracketing and interpretation of compounds, adjective ordering, and PP attachment. Acknowledgments This work was conducted while both authors were at the Department of Computational Linguistics, Saarland University, Saarbr¨ucken. The work was inspired by a talk that Gregory Grefenstette gave in Saarbr¨ucken in 2001 about his research on using the Web as a corpus. The present article is an extended and revised version of Keller, Lapata, and Ou</context>
</contexts>
<marker>Lapata, Keller, 2003</marker>
<rawString>Lapata, Maria and Frank Keller. 2003. Evaluating the performance of unsupervised Web-based models for a range of NLP tasks. Unpublished manuscript, University of Sheffield, Sheffield, England, and University of Edinburgh, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
<author>Frank Keller</author>
<author>Scott McDonald</author>
</authors>
<title>Evaluating smoothing algorithms against plausibility judgments.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics and the 10th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>346--353</pages>
<location>Toulouse, France.</location>
<marker>Lapata, Keller, McDonald, 2001</marker>
<rawString>Lapata, Maria, Frank Keller, and Scott McDonald. 2001. Evaluating smoothing algorithms against plausibility judgments. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics and the 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 346–353, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
<author>Scott McDonald</author>
<author>Frank Keller</author>
</authors>
<title>Determinants of adjective-noun plausibility.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>30--36</pages>
<location>Bergen,</location>
<marker>Lapata, McDonald, Keller, 1999</marker>
<rawString>Lapata, Maria, Scott McDonald, and Frank Keller. 1999. Determinants of adjective-noun plausibility. In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics, pages 30–36, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Designing Statistical Language Learners: Experiments on Compound Nouns.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Macquarie University,</institution>
<location>Sydney, Australia.</location>
<contexts>
<context position="12717" citStr="Lauer (1995)" startWordPosition="1967" endWordPosition="1968">een adjective-noun bigrams are shown in Table 2. For the present study, we applied the procedure used by Lapata, McDonald, and Keller (1999) and Lapata, Keller, and McDonald (2001) to noun-noun bigrams and to verb-object bigrams, creating a set of 90 seen and 90 unseen bigrams for each type of predicate-argument relationship. More specifically, 30 nouns and 30 verbs were chosen according to the same criteria proposed for the adjective study (i.e., minimal sense ambiguity and unambiguous part of speech). All nouns modifying one of the 30 nouns were extracted from the BNC using a heuristic from Lauer (1995) that looks for consecutive pairs of nouns that are neither preceded nor succeeded by another Table 2 Example stimuli for seen and unseen adjective-noun, noun-noun, and verb-object bigrams (with log-transformed BNC counts). High Adjective-Noun Bigrams Medium Low animal 1.79 pleasure 1.38 application 0 verdict 3.91 secret 2.56 cat 0 girl 2.94 dog 1.6 lunch .69 Noun-Noun Bigrams Adjective hungry guilty naughty Unseen tradition, innovation, prey system, wisdom, wartime regime, rival, protocol High process television plasma Medium Low 1.14 user .95 gala 0 1.53 satellite .95 edition 0 1.78 nylon 1.</context>
<context position="18449" citStr="Lauer 1995" startWordPosition="2837" endWordPosition="2838"> precision and 87.1% recall. The parser further achieved a per-word accuracy of 92.1% (where per-word accuracy includes the chunk category and chunk length identified correctly). Despite their imperfect output, heuristic methods for the extraction of syntactic relations are relatively common in statistical NLP. Several statistical models employ frequencies obtained from the output of partial parsers and other heuristic methods; these include models for disambiguating the attachment site of prepositional phrases (Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns (Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the induction of selectional preferences (Abney and Light 1999), methods for automatically clustering words according to their distribution in particular syntactic contexts (Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994; Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee 1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways for obtaining bigram frequencies that are potentially useful for such models despite the fact that some of t</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Lauer, Mark. 1995. Designing Statistical Language Learners: Experiments on Compound Nouns. Ph.D. thesis, Macquarie University, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lilian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<location>College Park, MD.</location>
<contexts>
<context position="18860" citStr="Lee 1999" startWordPosition="2890" endWordPosition="2891">heuristic methods; these include models for disambiguating the attachment site of prepositional phrases (Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns (Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the induction of selectional preferences (Abney and Light 1999), methods for automatically clustering words according to their distribution in particular syntactic contexts (Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994; Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee 1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways for obtaining bigram frequencies that are potentially useful for such models despite the fact that some of these bigrams are identified in a heuristic manner and may be noisy. 2.2 Sampling Bigrams from the NANTC We also obtained corpus counts from a second corpus, the North American News Text Corpus (NANTC). This corpus differs in several important respects from the BNC. It is substantially larger, as it contains 350 million words of text. Also, it is not a balanced corpus, as it contains material from only one ge</context>
<context position="61518" citStr="Lee 1999" startWordPosition="9756" endWordPosition="9757">re a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. However, the next section will present a small-scale study that compares the performance of several smoothing techniques with the performance of Web counts on a standard task from the literature. 3.4 Pseudodisambiguation In the smoothing literature, re-created frequencies are typically evaluated using pseudodisambiguation (Clark and Weir 2001; Dagan, Lee, and Pereira 1999; Lee 1999; Pereira, Tishby, and Lee 1993; Prescher, Riezler, and Rooth 2000; Rooth et al. 1999). 476 Keller and Lapata Web Frequencies for Unseen Bigrams The aim of the pseudodisambiguation task is to decide whether a given algorithm re-creates frequencies that make it possible to distinguish between seen and unseen bigrams in a given corpus. A set of pseudobigrams is constructed according to a set of criteria (detailed below) that ensure that they are unattested in the training corpus. Then the seen bigrams are removed from the training data, and the smoothing method is used to re-create the frequenci</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lee, Lilian. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 25–32, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
<author>Michael Bryant</author>
</authors>
<title>The tagging of the British national corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>622--628</pages>
<location>Kyoto, Japan.</location>
<marker>Leech, Garside, Bryant, 1994</marker>
<rawString>Leech, Geoffrey, Roger Garside, and Michael Bryant. 1994. The tagging of the British national corpus. In Proceedings of the 15th International Conference on Computational Linguistics, pages 622–628, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith N Levi</author>
</authors>
<title>The Syntax and Semantics of Complex Nominals.</title>
<date>1978</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="22871" citStr="Levi 1978" startWordPosition="3531" endWordPosition="3532">the BNC. Table 4 shows the NANTC counts for the set of seen bigrams from Table 2. Because of the differences in the extraction methodology (chunking versus full parsing) and the text genre (balanced corpus versus news text), we expected that some BNC bigrams would not be attested in the NANTC corpus. More precisely, zero frequencies were returned for 23 adjective-noun, 16 verb-noun, and 37 noun-noun bigrams. The fact that more zero frequencies were observed for noun-noun bigrams than for the other two types is perhaps not surprising considering the ease with which novel compounds are created (Levi 1978). We adjusted the zero counts by setting them to .5. This was necessary because all further analyses were carried out on logtransformed frequencies (see Table 4). Table 4 Log-transformed NANTC counts for seen adjective-noun, noun-noun, and verb-object bigrams. Adjective-Noun Bigrams Adjective High Medium Low hungry animal .90 pleasure -.30 application .60 guilty verdict 2.82 secret .95 cat -.30 naughty girl .69 dog -.30 lunch -.30 Noun-Noun Bigrams High Medium Low Head Noun process -.30 user -.30 gala -.30 directory television 2.70 satellite -.30 edition -.30 broadcast plasma -.30 nylon 0 unit</context>
</contexts>
<marker>Levi, 1978</marker>
<rawString>Levi, Judith N. 1978. The Syntax and Semantics of Complex Nominals. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="60557" citStr="Li and Abe 1998" startWordPosition="9615" endWordPosition="9618">sing all available smoothing techniques (and all available taxonomies that might be used for class-based smoothing). The smoothing method discussed above is simply one type of class-based smoothing. Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. However, the next secti</context>
<context position="74478" citStr="Li and Abe (1998)" startWordPosition="11883" endWordPosition="11886"> The same pattern is observed for the high-frequency data set, on which the conditional probability model achieves 87.7% correct and thus significantly outperforms Clark and Weir (2002), who obtained 73.9% (x2(1) = 283.73, p &lt; .01). The joint probability model achieved 85.3% on this data set, also significantly outperforming Clark and Weir (2002) (x2(1) = 119.35, p &lt; .01). To summarize, we demonstrated that the simple Web-based approach proposed in this article yields results for pseudodisambiguation that outperform class-based smoothing techniques, such as the ones proposed by Resnik (1993), Li and Abe (1998), and Clark and Weir (2002). We were also able to show that a Web-based approach is able to achieve the same performance as an EM-based smoothing model proposed by Rooth et al. (1999). However, the Web-based approach was not able to outperform the more sophisticated EM-based model of Prescher, Riezler, and Rooth (2000). Another result we obtained is that Web-based models that use conditional probabilities (where unigram frequencies are used to normalize the bigram frequencies) generally outperform a more simple-minded approach that relies directly on bigram frequencies for pseudodisambiguation</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Li, Hang and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the MDL principle. Computational Linguistics, 24(2):217–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>PRINCIPAR—An efficient broad-coverage, principle-based parser.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>482--488</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="20093" citStr="Lin 1994" startWordPosition="3091" endWordPosition="3092">owever, the text originates from a variety of sources (Los Angeles Times, Washington Post, New York Times News Syndicate, Reuters News Service, and Wall Street Journal). Whereas the BNC covers British English, the NANTC covers American English. All these differences mean that the NANTC provides a second, independent standard against which to compare Web counts. At the same time the correlation found between the counts obtained from the two corpora can serve as an upper limit for the correlation that we can expect between corpus counts and Web counts. The NANTC corpus was parsed using MINIPAR (Lin 1994, 2001), a broad-coverage parser for English. MINIPAR employs a manually constructed grammar and a lexicon derived from WordNet with the addition of proper names (130,000 entries in total). Lexicon entries contain part-of-speech and subcategorization information. The grammar is represented as a network of 35 nodes (i.e., grammatical categories) and 59 edges (i.e., types of syntactic [dependency] relationships). MINIPAR employs a distributedchart parsing algorithm. Instead of a single chart, each node in the grammar network maintains a chart containing partially built structures belonging to th</context>
</contexts>
<marker>Lin, 1994</marker>
<rawString>Lin, Dekang. 1994. PRINCIPAR—An efficient broad-coverage, principle-based parser. In Proceedings of the 15th International Conference on Computational Linguistics, pages 482–488, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Proceedings of the LREC Workshop on the Evaluation of Parsing Systems,</booktitle>
<pages>48--56</pages>
<location>Granada,</location>
<contexts>
<context position="21267" citStr="Lin (1998)" startWordPosition="3272" endWordPosition="3273">lly built structures belonging to the grammatical category represented by the node. Grammar rules are implemented as constraints associated with the nodes and edges. The output of MINIPAR is a dependency tree that represents the dependency relations between words in a sentence. Table 3 shows a subset of the dependencies MINIPAR outputs for the sentence The fat cat ate the door mat. In contrast to Gsearch and Cass, MINIPAR produces all possible parses for a given sentence. The parses are ranked according to the product of the probabilities of their edges, and the most likely parse is returned. Lin (1998) evaluated the parser on the SUSANNE corpus (Sampson 1995), a domain-independent corpus of British English, and achieved a recall of 79% and precision of 89% on the dependency relations. 464 Keller and Lapata Web Frequencies for Unseen Bigrams Table 3 Examples of dependencies generated by MINIPAR for The fat cat ate the door mat. Head Relation Modifier Description cat N:det:Det the determiner of noun cat N:mod:A fat adjective modifier of noun eat V:subj:N cat subject of verb eat V:obj:N mat object of verb mat N:det:Det the determiner of noun mat N:nn:N door prenominal modifier of noun For our </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, Dekang. 1998. Dependency-based evaluation of MINIPAR. In Proceedings of the LREC Workshop on the Evaluation of Parsing Systems, pages 48–56, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>LaTaT: Language and text analysis tools.</title>
<date>2001</date>
<booktitle>In Proceedings of the First International Conference on Human Language Technology Research.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<marker>Lin, 2001</marker>
<rawString>Lin, Dekang. 2001. LaTaT: Language and text analysis tools. In Proceedings of the First International Conference on Human Language Technology Research. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
<author>Natalia N Modjeska</author>
</authors>
<title>Using the Web for nominal anaphora resolution.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL Workshop on the Computational Treatment of Anaphora,</booktitle>
<pages>39--46</pages>
<location>Budapest,</location>
<marker>Markert, Nissim, Modjeska, 2003</marker>
<rawString>Markert, Katja, Malvina Nissim, and Natalia N. Modjeska. 2003. Using the Web for nominal anaphora resolution. In Proceedings of the EACL Workshop on the Computational Treatment of Anaphora, Budapest, pages 39–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Using semantic preferences to identify verbal participation in role switching alternations.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>256--263</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="60539" citStr="McCarthy 2000" startWordPosition="9613" endWordPosition="9614">es re-created using all available smoothing techniques (and all available taxonomies that might be used for class-based smoothing). The smoothing method discussed above is simply one type of class-based smoothing. Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. Howev</context>
</contexts>
<marker>McCarthy, 2000</marker>
<rawString>McCarthy, Diana. 2000. Using semantic preferences to identify verbal participation in role switching alternations. In Proceedings of the First Conference of the North American Chapter of the Association for Computational Linguistics, pages 256–263, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>A method for word sense disambiguation of unrestricted text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>152--158</pages>
<location>College Park, MD.</location>
<contexts>
<context position="2286" citStr="Mihalcea and Moldovan (1999)" startWordPosition="345" endWordPosition="348"> retrieved from the Web therefore provide enormous potential for training NLP algorithms, if Banko and Brill’s (2001a, 2001b) findings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax. There is a small body of existing research that tries to harness the potential of the Web for NLP. Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, * School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk † Department of Computer Science, 211 Portobello Street, Sheffield S1 4DP, UK. E-mail: mlap@dcs.shef.ac.uk 1 A reviewer points out that information providers such as Lexis Nexis (http://www.lexisnexis.com/) might have databases that are even larger tha</context>
</contexts>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>Mihalcea, Rada and Dan Moldovan. 1999. A method for word sense disambiguation of unrestricted text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 152–158, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="9515" citStr="Miller et al. 1990" startWordPosition="1457" endWordPosition="1460">ts; the bigram consists of the verb and the head noun of the object. For each of the three predicate-argument relations, we gathered two data sets, one containing seen bigrams (i.e., bigrams that occur in the BNC) and one with unseen bigrams (i.e., bigrams that do not occur in the BNC). For the seen adjective-noun bigrams, we used the data of Lapata, McDonald, and Keller (1999), who compiled a set of 90 bigrams as follows. First, 30 adjectives were randomly chosen from a part-of-speech-tagged and lemmatized version of the BNC so that each adjective had exactly two senses according to WordNet (Miller et al. 1990) and was unambiguously tagged as “adjective” 98.6% of the time. Lapata, McDonald, and Keller used the part-of-speech-tagged version that is made available with the BNC and was tagged using CLAWS4 (Leech, Garside, and Bryant 1994), a probabilistic partof-speech tagger, with error rate ranging from 3% to 4%. The lemmatized version of the corpus was obtained using Karp et al.’s (1992) morphological analyzer. The 30 adjectives ranged in BNC frequency from 1.9 to 49.1 per million words; that is, they covered the whole range from fairly infrequent to highly frequent items. Gsearch (Corley et al. 200</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, George A., Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990. Introduction to WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<location>Columbus, OH.</location>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Pereira, Fernando, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 183–190, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="11121" citStr="Pollard and Sag (1994)" startWordPosition="1711" endWordPosition="1714">sequences of two nouns were included, and the rightmost-occurring noun was considered the head. Bigrams involving proper nouns or low-frequency nouns (less than 10 per million words) were discarded. This was necessary because the bigrams were used in experiments involving native speakers (see Section 3.2), and we wanted to reduce the risk of including words unfamiliar to the experimental subjects. For each adjective, the set of bigrams was divided into three frequency bands based on an equal division of the 3 This assumption is disputed in the theoretical linguistics literature. For instance, Pollard and Sag (1994) present an analysis in which there is mutual selection between the noun and the adjective. 461 Computational Linguistics Volume 29, Number 3 Table 1 Example of patterns used for the extraction of adjective-noun bigrams. Pattern Example A N educational material A Adv N usual weekly classes A N N environmental health officers range of log-transformed co-occurrence frequencies. Then one bigram was chosen at random from each band. This procedure ensures that the whole range of frequencies is represented in our sample. Lapata, Keller, and McDonald (2001) compiled a set of 90 unseen adjective-noun </context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, Carl and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detlef Prescher</author>
<author>Stefan Riezler</author>
<author>Mats Rooth</author>
</authors>
<title>Using a probabilistic class-based lexicon for lexical ambiguity resolution.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>649--655</pages>
<location>Saarbr¨ucken, Germany.</location>
<marker>Prescher, Riezler, Rooth, 2000</marker>
<rawString>Prescher, Detlef, Stefan Riezler, and Mats Rooth. 2000. Using a probabilistic class-based lexicon for lexical ambiguity resolution. In Proceedings of the 18th International Conference on Computational Linguistics, pages 649–655, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Unsupervised statistical models for prepositional phrase attachment.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1079--1085</pages>
<note>Montr ´eal.</note>
<contexts>
<context position="18397" citStr="Ratnaparkhi 1998" startWordPosition="2830" endWordPosition="2831">ed data, Abney’s (1996) parser identified chunks with 87.9% precision and 87.1% recall. The parser further achieved a per-word accuracy of 92.1% (where per-word accuracy includes the chunk category and chunk length identified correctly). Despite their imperfect output, heuristic methods for the extraction of syntactic relations are relatively common in statistical NLP. Several statistical models employ frequencies obtained from the output of partial parsers and other heuristic methods; these include models for disambiguating the attachment site of prepositional phrases (Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns (Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the induction of selectional preferences (Abney and Light 1999), methods for automatically clustering words according to their distribution in particular syntactic contexts (Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994; Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee 1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways for obtaining bigram frequencies that are potentially us</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Ratnaparkhi, Adwait. 1998. Unsupervised statistical models for prepositional phrase attachment. In Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics, pages 1079–1085, Montr ´eal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="74459" citStr="Resnik (1993)" startWordPosition="11881" endWordPosition="11882">3.14, p &lt; .01). The same pattern is observed for the high-frequency data set, on which the conditional probability model achieves 87.7% correct and thus significantly outperforms Clark and Weir (2002), who obtained 73.9% (x2(1) = 283.73, p &lt; .01). The joint probability model achieved 85.3% on this data set, also significantly outperforming Clark and Weir (2002) (x2(1) = 119.35, p &lt; .01). To summarize, we demonstrated that the simple Web-based approach proposed in this article yields results for pseudodisambiguation that outperform class-based smoothing techniques, such as the ones proposed by Resnik (1993), Li and Abe (1998), and Clark and Weir (2002). We were also able to show that a Web-based approach is able to achieve the same performance as an EM-based smoothing model proposed by Rooth et al. (1999). However, the Web-based approach was not able to outperform the more sophisticated EM-based model of Prescher, Riezler, and Rooth (2000). Another result we obtained is that Web-based models that use conditional probabilities (where unigram frequencies are used to normalize the bigram frequencies) generally outperform a more simple-minded approach that relies directly on bigram frequencies for p</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Resnik, Philip. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Mining the Web for bilingual text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>527--534</pages>
<location>College Park, MD.</location>
<contexts>
<context position="2183" citStr="Resnik (1999)" startWordPosition="330" endWordPosition="331">ble for NLP is the Web,1 which currently consists of at least 3,033 million pages.2 Data retrieved from the Web therefore provide enormous potential for training NLP algorithms, if Banko and Brill’s (2001a, 2001b) findings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax. There is a small body of existing research that tries to harness the potential of the Web for NLP. Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, * School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk † Department of Computer Science, 211 Portobello Street, Sheffield S1 4DP, UK. E-mail: mlap@dcs.shef.ac.uk 1 A reviewer points out that information p</context>
<context position="50530" citStr="Resnik 1999" startWordPosition="8015" endWordPosition="8016">0 ** .641** Google .641** .692 ** .624** BNC .569** .517 ** .488** NANTC .526** .597 ** .491** Smoothing .329** .318 ** .223* Agreement .630** .641 ** .604** Unseen Bigrams AltaVista .480** .578 ** .551** Google .473** .595 ** .520** Smoothing .342** .372 ** .298** Agreement .550** .570 ** .640** *p &lt; .05 (one-tailed). **p &lt; .01 (one-tailed). 473 Computational Linguistics Volume 29, Number 3 one-out resampling. This technique is a special case of n-fold cross-validation (Weiss and Kulikowski 1991) and has been previously used for measuring how well humans agree in judging semantic similarity (Resnik 1999, 2000). For each subject group, we divided the set of the subjects’ responses with size n into a set of size n − 1 (i.e., the response data of all but one subject) and a set of size 1 (i.e., the response data of a single subject). We then correlated the mean ratings of the former set with the ratings of the latter. This was repeated n times (see the number of participants in Table 6); the mean of the correlation coefficients for the seen and unseen bigrams is shown in Table 11 in the rows labeled “Agreement.” For both seen and unseen bigrams, we found no significant difference between the upp</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Resnik, Philip. 1999. Mining the Web for bilingual text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 527–534, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Measuring verb similarity.</title>
<date>2000</date>
<booktitle>Proceedings of the 22nd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>399--404</pages>
<editor>In Lila R. Gleitman and Aravid K. Joshi, editors,</editor>
<publisher>Erlbaum,</publisher>
<location>Mahwah, NJ.</location>
<marker>Resnik, 2000</marker>
<rawString>Resnik, Philip. 2000. Measuring verb similarity. In Lila R. Gleitman and Aravid K. Joshi, editors, Proceedings of the 22nd Annual Conference of the Cognitive Science Society, pages 399–404. Erlbaum, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Glenn Carroll</author>
<author>Franz Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via EM-based clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<location>College Park, MD.</location>
<contexts>
<context position="61604" citStr="Rooth et al. 1999" startWordPosition="9768" endWordPosition="9771">se; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. However, the next section will present a small-scale study that compares the performance of several smoothing techniques with the performance of Web counts on a standard task from the literature. 3.4 Pseudodisambiguation In the smoothing literature, re-created frequencies are typically evaluated using pseudodisambiguation (Clark and Weir 2001; Dagan, Lee, and Pereira 1999; Lee 1999; Pereira, Tishby, and Lee 1993; Prescher, Riezler, and Rooth 2000; Rooth et al. 1999). 476 Keller and Lapata Web Frequencies for Unseen Bigrams The aim of the pseudodisambiguation task is to decide whether a given algorithm re-creates frequencies that make it possible to distinguish between seen and unseen bigrams in a given corpus. A set of pseudobigrams is constructed according to a set of criteria (detailed below) that ensure that they are unattested in the training corpus. Then the seen bigrams are removed from the training data, and the smoothing method is used to re-create the frequencies of both the seen bigrams and the pseudobigrams. The smoothing method is then evalua</context>
<context position="63516" citStr="Rooth et al. (1999)" startWordPosition="10088" endWordPosition="10091"> the unseen bigram are re-created (using smoothing, or Web counts, in our case). The task is then to decide which of the two verbs v and v&apos; takes the noun n as its object. For this, the re-created bigram frequency is used: The bigram with the higher re-created frequency (or probability) is taken to be the seen bigram. If this bigram is really the seen one, then the disambiguation is correct. The overall percentage of correct disambiguations is a measure of the quality of the re-created frequencies (or probabilities). In the following, we will first describe in some detail the experiments that Rooth et al. (1999) and Clark and Weir (2001) conducted. We will then discuss how we replicated their experiments using the Web as an alternative smoothing method. Rooth et al. (1999) used pseudodisambiguation to evaluate a class-based model that is derived from unlabeled data using the expectation maximization (EM) algorithm. From a data set of 1,280,712 (v, n) pairs (obtained from the BNC using Carroll and Rooth’s [1998] parser), they randomly selected 3,000 pairs, with each pair containing a fairly frequent verb and noun (only verbs and nouns that occurred between 30 and 3,000 times in the data were considere</context>
<context position="64999" citStr="Rooth et al. 1999" startWordPosition="10335" endWordPosition="10338">r by comparing the probabilities P(nlv) and P(nlv&apos;). The probabilities were re-created using Rooth et al.’s (1999) EM-based clustering model on a training set from which all seen pairs (v, n) had been removed. An accuracy of 80% on the pseudodisambiguation task was achieved (see Table 13). Prescher, Riezler, and Rooth (2000) evaluated Rooth et al.’s (1999) EM-based clustering model again using pseudodisambiguation, but on a separate data set using a Table 13 Percentage of correct disambiguations on the pseudodisambiguation task using Web counts and counts re-created using EM-based clustering (Rooth et al. 1999). Data Set N AltaVista AltaVista Rooth et al. Conditional Probability Joint Probability Subject 717 71.2 68.5 — Objects 620 85.2 77.5 — Subjects and objects 1,337 77.7 72.7 80.0 477 Computational Linguistics Volume 29, Number 3 Table 14 Percentage of correct disambiguations on the pseudodisambiguation task using Web counts and counts re-created using EM-based clustering (Prescher, Riezler, and Rooth 2000). Data Set N AltaVista AltaVista VA Model VO Model Conditional Probability Joint Probability Subjects 159 66.7 59.1 — — Objects 139 70.5 66.2 — — Subjects and objects 298 68.5 62.4 79.0 88.3 s</context>
<context position="66457" citStr="Rooth et al. (1999)" startWordPosition="10580" endWordPosition="10583"> the task was to decide whether (v, n) or (v, n&apos;) was the correct pair in each triple. Prescher, Riezler, and Rooth (2000) reported pseudodisambiguation results with two clustering models: (1) Rooth et al.’s (1999) clustering approach, which models the semantic fit between a verb and its argument (VA model), and (2) a refined version of this approach that models only the fit between a verb and its object (VO model), disregarding other arguments of the verb. The results of the two models on the pseudodisambiguation task are shown in Table 14. At this point, it is important to note that neither Rooth et al. (1999) nor Prescher, Riezler, and Rooth (2000) used pseudodisambiguation for the final evaluation of their models. Rather, the performance on the pseudodisambiguation task was used to optimize the model parameters. The results in Tables 13 and 14 show the pseudodisambiguation performance achieved for the best parameter settings. In other words, these results were obtained on the development set (i.e., on the same data set that was used to optimize the parameters), not on a completely unseen test set. This procedure is well-justified in the context of Rooth et al.’s (1999) and Prescher, Riezler, and </context>
<context position="68117" citStr="Rooth et al. (1999)" startWordPosition="10838" endWordPosition="10841">at of Rooth et al. (1999) and Prescher, Riezler, and Rooth (2000); here pseudodisambiguation is employed to evaluate the performance of a class-based probability estimation method. In order to address the problem of estimating conditional probabilities in the face of sparse data, Clark and Weir (2002) define probabilities in terms of classes in a semantic hierarchy and propose hypothesis testing as a means of determining a suitable level of generalization in the hierarchy. Clark and Weir (2002) report pseudodisambiguation results on two data sets, with an experimental setup similar to that of Rooth et al. (1999). For the first data set, 3,000 pairs were randomly chosen from 1.3 million (v, n) tuples extracted from the BNC (using the parser of Briscoe and Carroll [1997]). The selected pairs con8 Stefan Riezler (personal communication, 2003) points out that the main variance in Rooth et al.’s (1999) pseudodisambiguation results comes from the class cardinality parameter (start values account for only 2% of the performance, and iterations do not seem to make a difference at all). Figure 3 of Rooth et al. (1999) shows that a performance of more than 75% is obtained for every reasonable choice of classes.</context>
<context position="69542" citStr="Rooth et al. (1999)" startWordPosition="11065" endWordPosition="11068">hat is cited in Table 13. 478 Keller and Lapata Web Frequencies for Unseen Bigrams Table 15 Percentage of correct disambiguations on the pseudodisambiguation task using Web counts and counts re-created using class-based smoothing (Clark and Weir 2002). Data Set N AltaVista AltaVista Clark and Li and Resnik Conditional Joint Probability Weir Abe Probability Objects (low frequency) 3000 83.9 81.1 72.4 62.9 62.6 Objects (high frequency) 3000 87.7 85.3 73.9 68.3 63.9 tained relatively frequent verbs (occurring between 500 and 5,000 times in the data). The data sets were constructed as proposed by Rooth et al. (1999). The procedure for creating the second data set was identical, but this time only verbs that occurred between 100 and 1,000 times were considered. Clark and Weir (2002) further compared their approach with Resnik’s (1993) selectional association model and Li and Abe’s (1998) tree cut model on the same data sets. These methods are directly comparable, as they can be used for class-based probability estimation and address the question of how to find a suitable level of generalization in a hierarchy (i.e., WordNet). The results of the three methods used on the two data sets are shown in Table 15</context>
<context position="71749" citStr="Rooth et al. (1999)" startWordPosition="11446" endWordPosition="11449">e two models were used to perform pseudodisambiguation for the (v, n, n&apos;) triples, where we have to choose between (v, n) and (v, n&apos;). Here, the probability estimates f (v, n) and f (v, n&apos;) were used for the joint probability model, and f (v, n)/f (n) and f (v, n&apos;)/f (n&apos;) for the conditional probability model. The results for Rooth et al.’s (1999) data set are given in Table 13. The conditional probability model achieves a performance of 71.2% correct for subjects and 85.2% correct for objects. The performance on the whole data set is 77.7%, which is below the performance of 80.0% reported by Rooth et al. (1999). However, the difference is not found to be significant using a chi-square test comparing the number of correct and incorrect classifications (x2(1) = 2.02, p = .16). The joint probability model performs consistently worse than the conditional probability model: It achieves an overall accuracy of 72.7%, which is significantly lower than the accuracy of the Rooth et al. (1999) model (x2(1) = 19.50, p &lt; .01). 9 We used only AltaVista counts, as there was virtually no difference between AltaVista and Google counts in our previous evaluations (see Sections 3.1–3.3). Google allows only 1,000 queri</context>
<context position="74661" citStr="Rooth et al. (1999)" startWordPosition="11916" endWordPosition="11919">002), who obtained 73.9% (x2(1) = 283.73, p &lt; .01). The joint probability model achieved 85.3% on this data set, also significantly outperforming Clark and Weir (2002) (x2(1) = 119.35, p &lt; .01). To summarize, we demonstrated that the simple Web-based approach proposed in this article yields results for pseudodisambiguation that outperform class-based smoothing techniques, such as the ones proposed by Resnik (1993), Li and Abe (1998), and Clark and Weir (2002). We were also able to show that a Web-based approach is able to achieve the same performance as an EM-based smoothing model proposed by Rooth et al. (1999). However, the Web-based approach was not able to outperform the more sophisticated EM-based model of Prescher, Riezler, and Rooth (2000). Another result we obtained is that Web-based models that use conditional probabilities (where unigram frequencies are used to normalize the bigram frequencies) generally outperform a more simple-minded approach that relies directly on bigram frequencies for pseudodisambiguation. There are a number of reasons why our results regarding pseudodisambiguation have to be treated with some caution. First of all, the two smoothing methods (i.e., EM-based clustering</context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>Rooth, Mats, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1999. Inducing a semantically annotated lexicon via EM-based clustering. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 104–111, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
</authors>
<title>English for the Computer: The SUSANNE Corpus and Analytic Scheme.</title>
<date>1995</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="21325" citStr="Sampson 1995" startWordPosition="3281" endWordPosition="3282">ory represented by the node. Grammar rules are implemented as constraints associated with the nodes and edges. The output of MINIPAR is a dependency tree that represents the dependency relations between words in a sentence. Table 3 shows a subset of the dependencies MINIPAR outputs for the sentence The fat cat ate the door mat. In contrast to Gsearch and Cass, MINIPAR produces all possible parses for a given sentence. The parses are ranked according to the product of the probabilities of their edges, and the most likely parse is returned. Lin (1998) evaluated the parser on the SUSANNE corpus (Sampson 1995), a domain-independent corpus of British English, and achieved a recall of 79% and precision of 89% on the dependency relations. 464 Keller and Lapata Web Frequencies for Unseen Bigrams Table 3 Examples of dependencies generated by MINIPAR for The fat cat ate the door mat. Head Relation Modifier Description cat N:det:Det the determiner of noun cat N:mod:A fat adjective modifier of noun eat V:subj:N cat subject of verb eat V:obj:N mat object of verb mat N:det:Det the determiner of noun mat N:nn:N door prenominal modifier of noun For our experiments, we concentrated solely on adjective-noun, nou</context>
</contexts>
<marker>Sampson, 1995</marker>
<rawString>Sampson, Geoffrey. 1995. English for the Computer: The SUSANNE Corpus and Analytic Scheme. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carson T Sch¨utze</author>
</authors>
<title>The Empirical Base of Linguistics: Grammaticality Judgments and Linguistic Methodology.</title>
<date>1996</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<marker>Sch¨utze, 1996</marker>
<rawString>Sch¨utze, Carson T. 1996. The Empirical Base of Linguistics: Grammaticality Judgments and Linguistic Methodology. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Stevens</author>
</authors>
<title>Psychophysics: Introduction to its Perceptual, Neural, and Social Prospects.</title>
<date>1975</date>
<publisher>John Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="41584" citStr="Stevens 1975" startWordPosition="6618" endWordPosition="6619">the same for all six experiments. Materials. The experimental stimuli were based on the six sets of seen or unseen bigrams extracted from the BNC as described in Section 2.1 (adjective-noun, nounnoun, and verb-object bigrams). In the adjective-noun and noun-noun cases, the stimuli consisted simply of the bigrams. In the verb-object case, the bigrams were embedded in a short sentence to make them more natural: A proper-noun subject was added. Procedure. The experimental paradigm was magnitude estimation (ME), a technique standardly used in psychophysics to measure judgments of sensory stimuli (Stevens 1975), which Bard, Robertson, and Sorace (1996) and Cowart (1997) have applied to the elicitation of linguistic judgments. The ME procedure requires subjects to estimate the magnitude of physical stimuli by assigning numerical values proportional to the stimulus magnitude they perceive. In contrast to the five- or seven-point scale conventionally used to measure human intuitions, ME employs an interval scale and therefore produces data for which parametric inferential statistics are valid. ME requires subjects to assign numbers to a series of linguistic stimuli in a proportional fashion. Subjects a</context>
<context position="45660" citStr="Stevens 1975" startWordPosition="7260" endWordPosition="7261"> in the same experiment, based on their demographic data and on their Internet connection data, which is logged by WebExp. 3.2.2 Results and Discussion. The experimental data were normalized by dividing each numerical judgment by the modulus value that the subject had assigned to the reference sentence. This operation creates a common scale for all subjects. Then the data were transformed by taking the decadic logarithm. This transformation ensures that the judgments are normally distributed and is standard practice for magnitude estimation data (Bard, Robertson, and Sorace 1996; Cowart 1997; Stevens 1975). All further analyses were conducted on the normalized, log-transformed judgments. Table 10 shows the descriptive statistics for all six judgment experiments: the original experiments by Lapata, McDonald, and Keller (1999) and Lapata, Keller, and McDonald (2001) for adjective-noun bigrams, and our new ones for noun-noun and verb-object bigrams. We used correlation analysis to compare corpus counts and Web counts with plausibility judgments. Table 11 (top half) lists the correlation coefficients that were obtained when correlating log-transformed Web counts (AltaVista and Google) and corpus co</context>
</contexts>
<marker>Stevens, 1975</marker>
<rawString>Stevens, S. S. 1975. Psychophysics: Introduction to its Perceptual, Neural, and Social Prospects. John Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Volk</author>
</authors>
<title>Exploiting the WWW as a corpus to resolve PP attachment ambiguities.</title>
<date>2001</date>
<booktitle>Proceedings of the Corpus Linguistics Conference,</booktitle>
<pages>601--606</pages>
<editor>In Paul Rayson, Andrew Wilson, Tony McEnery, Andrew Hardie, and Shereen Khoja, editors,</editor>
<location>Lancaster, England.</location>
<contexts>
<context position="2372" citStr="Volk (2001)" startWordPosition="361" endWordPosition="362">Brill’s (2001a, 2001b) findings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax. There is a small body of existing research that tries to harness the potential of the Web for NLP. Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, * School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk † Department of Computer Science, 211 Portobello Street, Sheffield S1 4DP, UK. E-mail: mlap@dcs.shef.ac.uk 1 A reviewer points out that information providers such as Lexis Nexis (http://www.lexisnexis.com/) might have databases that are even larger than the Web. Lexis Nexis provides full-text access to news sources (including newspapers</context>
</contexts>
<marker>Volk, 2001</marker>
<rawString>Volk, Martin. 2001. Exploiting the WWW as a corpus to resolve PP attachment ambiguities. In Paul Rayson, Andrew Wilson, Tony McEnery, Andrew Hardie, and Shereen Khoja, editors, Proceedings of the Corpus Linguistics Conference, pages 601–606, Lancaster, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sholom M Weiss</author>
<author>Casimir A Kulikowski</author>
</authors>
<date>1991</date>
<booktitle>Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="50421" citStr="Weiss and Kulikowski 1991" startWordPosition="7996" endWordPosition="7999">s to the intersubject agreement on the judgment task. Adjective-Noun Noun-Noun Verb-Object Seen Bigrams AltaVista .650** .700 ** .641** Google .641** .692 ** .624** BNC .569** .517 ** .488** NANTC .526** .597 ** .491** Smoothing .329** .318 ** .223* Agreement .630** .641 ** .604** Unseen Bigrams AltaVista .480** .578 ** .551** Google .473** .595 ** .520** Smoothing .342** .372 ** .298** Agreement .550** .570 ** .640** *p &lt; .05 (one-tailed). **p &lt; .01 (one-tailed). 473 Computational Linguistics Volume 29, Number 3 one-out resampling. This technique is a special case of n-fold cross-validation (Weiss and Kulikowski 1991) and has been previously used for measuring how well humans agree in judging semantic similarity (Resnik 1999, 2000). For each subject group, we divided the set of the subjects’ responses with size n into a set of size n − 1 (i.e., the response data of all but one subject) and a set of size 1 (i.e., the response data of a single subject). We then correlated the mean ratings of the former set with the ratings of the latter. This was repeated n times (see the number of participants in Table 6); the mean of the correlation coefficients for the seen and unseen bigrams is shown in Table 11 in the r</context>
</contexts>
<marker>Weiss, Kulikowski, 1991</marker>
<rawString>Weiss, Sholom M. and Casimir A. Kulikowski. 1991. Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Improving trigram language modeling with the World Wide Web.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Acoustics Speech and Signal Processing,</booktitle>
<location>Salt Lake City, UT.</location>
<contexts>
<context position="3370" citStr="Zhu and Rosenfeld (2001)" startWordPosition="501" endWordPosition="504">eviewer points out that information providers such as Lexis Nexis (http://www.lexisnexis.com/) might have databases that are even larger than the Web. Lexis Nexis provides full-text access to news sources (including newspapers, wire services, and broadcast transcripts) and legal data (including case law, codes, regulations, legal news, and law reviews). 2 This is the number of pages indexed by Google in December 2002, as estimated by Search Engine Showdown (http://www.searchengineshowdown.com/). © 2003 Association for Computational Linguistics Computational Linguistics Volume 29, Number 3 and Zhu and Rosenfeld (2001) use Web-based n-gram counts to improve language modeling. A particularly interesting application is proposed by Grefenstette (1998), who uses the Web for example-based machine translation. His task is to translate compounds from French into English, with corpus evidence serving as a filter for candidate translations. An example is the French compound groupe de travail. There are five translations of groupe and three translations for travail (in the dictionary that Grefenstette [1998] is using), resulting in 15 possible candidate translations. Only one of them, namely, work group, has a high c</context>
<context position="5215" citStr="Zhu and Rosenfeld (2001)" startWordPosition="784" endWordPosition="787">ed approach to overcoming data sparseness. It remains to be shown that Web counts are generally useful for approximating data that are sparse or unseen in a given corpus. It seems possible, for instance, that Grefenstette’s (1998) results are limited to his particular task (filtering potential translations) or to his particular linguistic phenomenon (noun-noun compounds). Another potential problem is the fact that Web counts are far more noisy than counts obtained from a well-edited, carefully balanced corpus. The effect of this noise on the usefulness of the Web counts is largely unexplored. Zhu and Rosenfeld (2001) use Web-based n-gram counts for language modeling. They obtain a standard language model from a 103-million-word corpus and employ Web-based counts to interpolate unreliable trigram estimates. They compare their interpolated model against a baseline trigram language model (without interpolation) and show that the interpolated model yields an absolute reduction in word error rate of .93% over the baseline. Zhu and Rosenfeld’s (2001) results demonstrate that the Web can be a source of data for language modeling. It is not clear, however, whether their result carries over to tasks that employ li</context>
<context position="30418" citStr="Zhu and Rosenfeld (2001)" startWordPosition="4822" endWordPosition="4825">counts returned by the search engines; we do not download the pages themselves for further processing. Note that many of the bigrams in our sample are very frequent (up to 106 matches; see the “Max” columns in Table 6), hence the effort involved in downloading all pages would be immense (though methods for downloading a representative sample could probably be devised). Our approach estimates Web frequencies based not on bigram counts directly, but on page counts. In other words, it ignores the fact that a bigram can occur more than once on a given Web page. This approximation is justified, as Zhu and Rosenfeld (2001) demonstrated for unigrams, bigrams, and trigrams: Page counts and n-gram counts are highly correlated on a log-log scale. This result is based on Zhu and Rosenfeld’s queries to AltaVista, a search engine that at the time of their research returned both the number of pages and the overall number of matches for a given query.4 Another important limitation of our approach arises from the fact that both Google and AltaVista disregard punctuation and capitalization, even if the search term is placed within quotation marks. This can lead to false positives, for instance, if the match crosses a phra</context>
<context position="80098" citStr="Zhu and Rosenfeld (2001)" startWordPosition="12758" endWordPosition="12761"> than Web counts. To summarize, we have proposed a simple heuristic method for obtaining bigram counts from the Web. Using four different types of evaluation, we demonstrated that this simple heuristic method is sufficient to obtain valid frequency estimates. It seems that the large amount of data available outweighs the problems associated with using the Web as a corpus (such as the fact that it is noisy and unbalanced). A number of questions arise for future research: (1) Are Web frequencies suitable for probabilistic modeling, in particular since Web counts are not perfectly normalized, as Zhu and Rosenfeld (2001) have shown? (2) How can existing smoothing methods benefit from Web counts? (3) How do the results reported in this article carry over to languages other than English (for which a much smaller amount of Web data are available)? (4) What is the effect of the noise introduced by our heuristic approach? The last question could be assessed by reproducing our results using a snapshot of the Web, from which argument relations can be extracted more accurately using POS tagging and chunking techniques. 481 Computational Linguistics Volume 29, Number 3 Finally, it will be crucial to test the usefulnes</context>
</contexts>
<marker>Zhu, Rosenfeld, 2001</marker>
<rawString>Zhu, Xiaojin and Ronald Rosenfeld. 2001. Improving trigram language modeling with the World Wide Web. In Proceedings of the International Conference on Acoustics Speech and Signal Processing, Salt Lake City, UT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>