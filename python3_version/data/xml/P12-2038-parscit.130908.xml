<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.054277">
<title confidence="0.94858">
Native Language Detection with Tree Substitution Grammars
</title>
<author confidence="0.998683">
Ben Swanson Eugene Charniak
</author>
<affiliation confidence="0.999175">
Brown University Brown University
</affiliation>
<email confidence="0.998934">
chonger@cs.brown.edu ec@cs.brown.edu
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999713461538462">
We investigate the potential of Tree Substitu-
tion Grammars as a source of features for na-
tive language detection, the task of inferring
an author’s native language from text in a dif-
ferent language. We compare two state of the
art methods for Tree Substitution Grammar
induction and show that features from both
methods outperform previous state of the art
results at native language detection. Further-
more, we contrast these two induction algo-
rithms and show that the Bayesian approach
produces superior classification results with a
smaller feature set.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946">
The correlation between a person’s native language
(L1) and aspects of their writing in a second lan-
guage (L2) can be exploited to predict L1 label given
L2 text. The International Corpus of Learner En-
glish (Granger et al, 2002), or ICLE, is a large set
of English student essays annotated with L1 labels
that allows us to bring the power of supervised ma-
chine learning techniques to bear on this task. In
this work we explore the possibility of automatically
induced Tree Substitution Grammar (TSG) rules as
features for a logistic regression model1 trained to
predict these L1 labels.
Automatic TSG induction is made difficult by the
exponential number of possible TSG rules given a
corpus. This is an active area of research with two
distinct effective solutions. The first uses a nonpara-
metric Bayesian model to handle the large number
</bodyText>
<sectionHeader confidence="0.397539" genericHeader="introduction">
1a.k.a. Maximum Entropy Model
</sectionHeader>
<bodyText confidence="0.999723285714286">
of rules (Cohn and Blunsom, 2010), while the sec-
ond is inspired by tree kernel methods and extracts
common subtrees from pairs of parse trees (Sangati
and Zuidema, 2011). While both are effective, we
show that the Bayesian method of TSG induction
produces superior features and achieves a new best
result at the task of native language detection.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.988542">
2.1 Native Language Detection
</subsectionHeader>
<bodyText confidence="0.999977956521739">
Work in automatic native language detection has
been mainly associated with the ICLE, published in
2002. Koppel et al (2005) first constructed such
a system with a feature set consisting of function
words, POS bi-grams, and character n-grams. These
features provide a strong baseline but cannot capture
many linguistic phenomena.
More recently, Wong and Dras (2011a) consid-
ered syntactic features for this task, using logis-
tic regression with features extracted from parse
trees produced by a state of the art statistical parser.
They investigated two classes of features: rerank-
ing features from the Charniak parser and CFG fea-
tures. They showed that while reranking features
capture long range dependencies in parse trees that
CFG rules cannot, they do not produce classification
performance superior to simple CFG rules. Their
CFG feature approach represents the best perform-
ing model to date for the task of native language de-
tection. Wong and Dras (2011b) also investigated
the use of LDA topic modeling to produce a latent
feature set of reduced dimensionality, but failed to
outperform baseline systems with this approach.
</bodyText>
<page confidence="0.989821">
193
</page>
<note confidence="0.783925">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 193–197,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.97381">
2.2 TSG induction
</subsectionHeader>
<bodyText confidence="0.99997734375">
One inherent difficulty in the use of TSGs is con-
trolling the size of grammars automatically in-
duced from data, which with any reasonable corpus
quickly becomes too large for modern workstations
to handle. When automatically induced TSGs were
first proposed by Bod (1991), the problem of gram-
mar induction was tackled with random selection of
fragments or weak constraints that led to massive
grammars.
A more principled technique is to use a sparse
nonparametric prior, as was recently presented by
Cohn et al (2009) and Post and Gildea (2009). They
provide a local Gibbs sampling algorithm, and Cohn
and Blunsom (2010) later developed a block sam-
pling algorithm with better convergence behavior.
While this Bayesian method has yet to produce
state of the art parsing results, it has achieved state
of the art results for unsupervised grammar induc-
tion (Blunsom and Cohn, 2010) and has been ex-
tended to synchronous grammars for use in sentence
compression (Yamangil and Shieber, 2010).
More recently, (Sangati and Zuidema, 2011) pre-
sented an elegantly simple heuristic inspired by tree
kernels that they call DoubleDOP. They showed that
manageable grammar sizes can be obtained from a
corpus the size of the Penn Treebank by recording
all fragments that occur at least twice, subject to a
pairwise constraint of maximality. Using an addi-
tional heuristic to provide a distribution over frag-
ments, DoubleDOP achieved the current state of the
art for TSG parsing, competing closely with the ab-
solute best results set by refinement based parsers.
</bodyText>
<subsectionHeader confidence="0.999728">
2.3 Fragment Based Classification
</subsectionHeader>
<bodyText confidence="0.999977894736842">
The use of parse tree fragments for classification
began with Collins and Duffy (2001). They used
the number of common subtrees between two parse
trees as a convolution kernel in a voted perceptron
and applied it as a parse reranker. Since then, such
tree kernels have been used to perform a variety of
text classification tasks, such as semantic role la-
beling (Moschitti et al, 2008), authorship attribu-
tion (Kim et al, 2010), or the work of Suzuki and
Isozaki (2006) that performs question classification,
subjectivity detection, and polarity identification.
Syntactic features have also been used in non-
kernelized classifiers, such as in the work of Wong
and Dras (2011a) mentioned in Section 2.1. Ad-
ditional examples include Raghavan et al (2010),
which uses a CFG language model to perform au-
thorship attribution, and Post (2011), which uses
TSG features in a logistic regression model to per-
form grammaticality detection.
</bodyText>
<sectionHeader confidence="0.993569" genericHeader="method">
3 Tree Substitution Grammars
</sectionHeader>
<bodyText confidence="0.998185285714286">
Tree Substitution Grammars are similar to Context
Free Grammars, differing in that they allow rewrite
rules of arbitrary parse tree structure with any num-
ber of nonterminal or terminal leaves. We adopt the
common term fragment2 to refer to these rules, as
they are easily visualised as fragments of a complete
parse tree.
</bodyText>
<figureCaption confidence="0.965545333333333">
Figure 1: Fragments from a Tree Substitution Grammar
capable of deriving the sentences “George hates broccoli”
and “George hates shoes”.
</figureCaption>
<subsectionHeader confidence="0.998229">
3.1 Bayesian Induction
</subsectionHeader>
<bodyText confidence="0.992264875">
Nonparametric Bayesian models can represent dis-
tributions of unbounded size with a dynamic param-
eter set that grows with the size of the training data.
One method of TSG induction is to represent a prob-
abilistic TSG with Dirichlet Process priors and sam-
ple derivations of a corpus using MCMC.
Under this model the posterior probability of a
fragment e is given as
</bodyText>
<equation confidence="0.992427">
P(e|a−, α, Po) _ #e + αP0 (1)
#• + α
</equation>
<bodyText confidence="0.99988725">
where a− is the multiset of fragments in the current
derivations excluding e, #e is the count of the frag-
ment e in a−, and #• is the total number of frag-
ments in a− with the same root node as e. Po is
</bodyText>
<footnote confidence="0.394937">
2As opposed to elementary tree, often used in related work
</footnote>
<figure confidence="0.998240642857143">
shoes
broccoli
S
NP VP
VBZ
hates
NP
George
NNP
NP
NP
NN
NP
NNS
</figure>
<page confidence="0.989091">
194
</page>
<bodyText confidence="0.9984515">
a PCFG distribution over fragments with a bias to-
wards small fragments. a is the concentration pa-
rameter of the DP, and can be used to roughly tune
the number of fragments that appear in the sampled
derivations.
With this posterior distribution the derivations of
a corpus can be sampled tree by tree using the block
sampling algorithm of Cohn and Blunsom (2010),
converging eventually on a sample from the true
posterior of all derivations.
</bodyText>
<subsectionHeader confidence="0.989064">
3.2 DoubleDOP Induction
</subsectionHeader>
<bodyText confidence="0.999969117647059">
DoubleDOP uses a heuristic inspired by tree kernels,
which are commonly used to measure similarity be-
tween two parse trees by counting the number of
fragments they share. DoubleDOP uses the same un-
derlying technique, but caches the shared fragments
instead of simply counting them. This yields a set
of fragments where each member is guaranteed to
appear at least twice in the training set.
In order to avoid unmanageably large grammars
only maximal fragments are retained in each pair-
wise extraction, which is to say that any shared frag-
ment that occurs inside another shared fragment is
discarded. The main disadvantage of this method
is that the complexity scales quadratically with the
training set size, as all pairs of sentences must be
considered. It is fully parallelizable, however, which
mediates this disadvantage to some extent.
</bodyText>
<sectionHeader confidence="0.999795" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.850828">
4.1 Methodology
</subsectionHeader>
<bodyText confidence="0.999961230769231">
Our data is drawn from the International Corpus
of Learner English (Version 2), which consists of
raw unsegmented English text tagged with L1 la-
bels. Our experimental setup follows Wong and
Dras (2011a) in analyzing Chinese, Russian, Bul-
garian, Japanese, French, Czech, and Spanish L1 es-
says. As in their work we randomly sample 70 train-
ing and 25 test documents for each language. All re-
ported results are averaged over 5 subsamplings of
the full data set.
Our data preproccesing pipeline is as fol-
lows: First we perform sentence segmentation with
OpenNLP and then parse each sentence with a 6
split grammar for the Berkeley Parser (Petrov et al,
2006). We then replace all terminal symbols which
do not occur in a list of 598 function words3 with
a single UNK terminal. This aggressive removal of
lexical items is standard in this task and mitigates the
effect of other unwanted information sources such
as topic and geographic location that are correlated
with native language in the data.
We contrast three different TSG feature sets in our
experiments. First, to provide a baseline, we sim-
ply read off the CFG rules from the data set (note
that a CFG can be taken as a TSG with all frag-
ments having depth one). Second, in the method
we call BTSG, we use the Bayesian induction model
with the Dirichlet process’ concentration parameters
tuned to 100 and run for 1000 iterations of sampling.
We take as our resulting finite grammar the frag-
ments that appear in the sampled derivations. Third,
we run the parameterless DoubleDOP (2DOP) in-
duction method.
Using the full 2DOP feature set produces over
400k features, which heavily taxes the resources of
a single modern workstation. To balance the feature
set sizes between 2DOP and BTSG we pass back
over the training data and count the actual number
of times each fragment recovered by 2DOP appears.
We then limit the list to the n most common frag-
ments, where n is the average number of fragments
recovered by the BTSG method (around 7k). We re-
fer to results using this trimmed feature set with the
label 2DOP, using 2DOP(F) to refer to DoubleDOP
with the full set of features.
Given each TSG, we create a binary feature func-
tion for each fragment e in the grammar such that the
feature fe(d) is active for a document d if there ex-
ists a derivation of some tree t E d that uses e. Clas-
sification is performed with the Mallet package for
logistic regression using the default initialized Max-
EntTrainer.
</bodyText>
<sectionHeader confidence="0.999991" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.998461">
5.1 Predictive Power
</subsectionHeader>
<bodyText confidence="0.9414095">
The resulting classification accuracies are shown in
Table 1. The BTSG feature set gives the highest per-
formance, and both true TSG induction techniques
outperform the CFG baseline.
</bodyText>
<footnote confidence="0.901666">
3We use the stop word list distributed with the ROUGE sum-
marization evaluation package.
</footnote>
<page confidence="0.993392">
195
</page>
<table confidence="0.9930326">
Model Accuracy (%)
CFG 72.6
2DOP 73.5
2DOP(F) 76.8
BTSG 78.4
</table>
<tableCaption confidence="0.999828">
Table 1: Classification accuracy
</tableCaption>
<bodyText confidence="0.999965">
The CFG result represents the work of Wong and
Dras (2011a), the previous best result for this task.
While in their work they report 80% accuracy with
the CFG model, this is for a single sampling of the
full data set. We observed a large variance in clas-
sification accuracy over such samplings, which in-
cludes some values in their reported range but with
a much lower mean. The numbers we report are
from our own implementation of their CFG tech-
nique, and all results are averaged over 5 random
samplings from the full corpus.
For 2DOP we limit the 2DOP(F) fragments by
choosing the 7k with maximum frequency, but there
may exist superior methods. Indeed, Wong and
Dras (2011a) claims that Information Gain is a better
criteria. However, this metric requires a probabilis-
tic formulation of the grammar, which 2DOP does
not supply. Instead of experimenting with different
limiting metrics, we note that when all 400k rules
are used, the averaged accuracy is only 76.8 percent,
which still lags behind BTSG.
</bodyText>
<subsectionHeader confidence="0.997554">
5.2 Robustness
</subsectionHeader>
<bodyText confidence="0.9995685">
We also investigated different classification strate-
gies, as binary indicators of fragment occurrence
over an entire document may lead to noisy results.
Consider a single outlier sentence in a document
with a single fragment that is indicative of the in-
correct L1 label. Note that it is just as important in
the eyes of the classifier as a fragment indicative of
the correct label that appears many times. To inves-
tigate this phenomena we classified individual sen-
tences, and used these results to vote for each docu-
ment level label in the test set.
We employed two voting schemes. In the first,
VoteOne, each sentence contributes one vote to its
maximum probability label. In the second, VoteAll,
the probability of each L1 label is contributed as a
partial vote. Neither method increases performance
</bodyText>
<table confidence="0.9979335">
Model VoteOne (%) VoteAll (%)
CFG 69.6 74.7
2DOP 69.1 73.5
BTSG 72.5 76.5
</table>
<tableCaption confidence="0.999798">
Table 2: Sentence based classification accuracy
</tableCaption>
<bodyText confidence="0.999851857142857">
for BTSG or 2DOP, but what is more interesting
is that in both cases the CFG model outperforms
2DOP (with less than half of the features). The
robust behavior of the BTSG method shows that it
finds correctly discriminative features across several
sentences in each document to a greater extent than
other methods.
</bodyText>
<subsectionHeader confidence="0.932061">
5.3 Concision
</subsectionHeader>
<bodyText confidence="0.999994">
One possible explanation for the superior perfor-
mance of BTSG is that DDOP is prone to yielding
multiple fragments that represent the same linguistic
phenomena, leading to sets of highly correlated fea-
tures. While correlated features are not crippling to
a logistic regression model, they add computational
complexity without contributing to higher classifica-
tion accuracy.
To address this hypothesis empirically, we con-
sidered pairs of fragments eA and eB and calcu-
lated the pointwise mutual information (PMI) be-
tween events signifying their occurrence in a sen-
tence. For BTSG, the average pointwise mutual in-
formation over all pairs (eA, eB) is −.14, while for
2DOP it is −.01. As increasingly negative values
of PMI indicate exclusivity, this supports the claim
that DDOP’s comparative weakness is to some ex-
tent due to feature redundancy.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999962555555556">
In this work we investigate automatically induced
TSG fragments as classification features for native
language detection. We compare Bayesian and Dou-
bleDOP induced features and find that the former
represents the data with less redundancy, is more ro-
bust to classification strategy, and gives higher clas-
sification accuracy. Additionally, the Bayesian TSG
features give a new best result for the task of native
language detection.
</bodyText>
<page confidence="0.998513">
196
</page>
<sectionHeader confidence="0.996109" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999208279411765">
Mohit Bansal and Dan Klein 2010. Simple, accurate
parsing with an all-fragments grammar. Association
for Computational Linguistics.
Phil Blunsom and Trevor Cohn 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. Empirical Methods in Natural Lan-
guage Processing.
Rens Bod 1991. A Computational Model of Language
Performance: Data Oriented Parsing. Computational
Linguistics in the Netherlands.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing Compact but Accurate Tree-
Substitution Grammars. In Proceedings NAACL.
Trevor Cohn, and Phil Blunsom 2010. Blocked inference
in Bayesian tree substitution grammars. Association
for Computational Linguistics.
Michael Collins, Nigel Duffy 2001. Convolution Ker-
nels for Natural Language. Advances in Neural Infor-
mation Processing Systems.
Joshua Goodman 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod et al. chapter 8..
S. Granger, E. Dagneaux and F. Meunier. 2002. Interna-
tional Corpus of Learner English, (ICLE).
Sangkyum Kim, Hyungsul Kim, Tim Weninger, and Ji-
awei Han 2010. Authorship classification: a syn-
tactic tree mining approach. Proceedings of the ACM
SIGKDD Workshop on Useful Patterns.
Koppel, Moshe and Schler, Jonathan and Zigdon, Kfir.
2005. Determining an author’s native language by
mining a text for errors. Proceedings of the eleventh
ACM SIGKDD international conference on Knowl-
edge discovery in data mining.
Alessandro Moschitti, Daniele Pighin and Roberto Basili
2008. Tree Kernels for Semantic Role Labeling. Com-
putational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. Association for Compu-
tational Linguistics.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. Association for Com-
putational Linguistics.
Matt Post. 2011. Judging Grammaticality with Tree Sub-
stitution Grammar Derivations. Association for Com-
putational Linguistics.
Sindhu Raghavan, Adriana Kovashka and Raymond
Mooney 2010. Authorship attribution using proba-
bilistic context-free grammars. Association for Com-
putational Linguistics.
Sangati, Federico and Zuidema, Willem 2011. Accurate
Parsing with Compact Tree-Substitution Grammars:
Double-DOP. Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing.
Jun Suzuki and Hideki Isozaki 2006. Sequence and tree
kernels with statistical feature mining. Advances in
Neural Information Processing Systems.
Sze-Meng Jojo Wong and Mark Dras 2011. Exploit-
ing Parse Structures for Native Language Identifica-
tion. Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing.
Sze-Meng Jojo Wong and Mark Dras 2011. Topic Mod-
eling for Native Language Identification. Proceedings
of the Australasian Language Technology Association
Workshop.
Elif Yamangil, Stuart M. Shieber 2010. Bayesian Syn-
chronous Tree-Substitution Grammar Induction and
Its Application to Sentence Compression.. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.99819">
197
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.992998">
<title confidence="0.999981">Native Language Detection with Tree Substitution Grammars</title>
<author confidence="0.999708">Ben Swanson Eugene Charniak</author>
<affiliation confidence="0.999957">Brown University Brown University</affiliation>
<email confidence="0.998601">chonger@cs.brown.eduec@cs.brown.edu</email>
<abstract confidence="0.999613357142857">We investigate the potential of Tree Substitution Grammars as a source of features for native language detection, the task of inferring an author’s native language from text in a different language. We compare two state of the art methods for Tree Substitution Grammar induction and show that features from both methods outperform previous state of the art results at native language detection. Furthermore, we contrast these two induction algorithms and show that the Bayesian approach produces superior classification results with a smaller feature set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Simple, accurate parsing with an all-fragments grammar. Association for Computational Linguistics.</title>
<date>2010</date>
<marker>Bansal, Klein, 2010</marker>
<rawString>Mohit Bansal and Dan Klein 2010. Simple, accurate parsing with an all-fragments grammar. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing. Empirical Methods in Natural Language Processing.</title>
<date>2010</date>
<contexts>
<context position="4223" citStr="Blunsom and Cohn, 2010" startWordPosition="667" endWordPosition="670">ed by Bod (1991), the problem of grammar induction was tackled with random selection of fragments or weak constraints that led to massive grammars. A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). They provide a local Gibbs sampling algorithm, and Cohn and Blunsom (2010) later developed a block sampling algorithm with better convergence behavior. While this Bayesian method has yet to produce state of the art parsing results, it has achieved state of the art results for unsupervised grammar induction (Blunsom and Cohn, 2010) and has been extended to synchronous grammars for use in sentence compression (Yamangil and Shieber, 2010). More recently, (Sangati and Zuidema, 2011) presented an elegantly simple heuristic inspired by tree kernels that they call DoubleDOP. They showed that manageable grammar sizes can be obtained from a corpus the size of the Penn Treebank by recording all fragments that occur at least twice, subject to a pairwise constraint of maximality. Using an additional heuristic to provide a distribution over fragments, DoubleDOP achieved the current state of the art for TSG parsing, competing closel</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn 2010. Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing. Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>A Computational Model of Language Performance: Data Oriented Parsing. Computational Linguistics in the Netherlands.</title>
<date>1991</date>
<contexts>
<context position="3616" citStr="Bod (1991)" startWordPosition="569" endWordPosition="570">g to produce a latent feature set of reduced dimensionality, but failed to outperform baseline systems with this approach. 193 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 193–197, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 2.2 TSG induction One inherent difficulty in the use of TSGs is controlling the size of grammars automatically induced from data, which with any reasonable corpus quickly becomes too large for modern workstations to handle. When automatically induced TSGs were first proposed by Bod (1991), the problem of grammar induction was tackled with random selection of fragments or weak constraints that led to massive grammars. A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). They provide a local Gibbs sampling algorithm, and Cohn and Blunsom (2010) later developed a block sampling algorithm with better convergence behavior. While this Bayesian method has yet to produce state of the art parsing results, it has achieved state of the art results for unsupervised grammar induction (Blunsom and Cohn</context>
</contexts>
<marker>Bod, 1991</marker>
<rawString>Rens Bod 1991. A Computational Model of Language Performance: Data Oriented Parsing. Computational Linguistics in the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Phil Blunsom</author>
</authors>
<title>Inducing Compact but Accurate TreeSubstitution Grammars.</title>
<date>2009</date>
<booktitle>In Proceedings NAACL.</booktitle>
<contexts>
<context position="3862" citStr="Cohn et al (2009)" startWordPosition="608" endWordPosition="611">public of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 2.2 TSG induction One inherent difficulty in the use of TSGs is controlling the size of grammars automatically induced from data, which with any reasonable corpus quickly becomes too large for modern workstations to handle. When automatically induced TSGs were first proposed by Bod (1991), the problem of grammar induction was tackled with random selection of fragments or weak constraints that led to massive grammars. A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). They provide a local Gibbs sampling algorithm, and Cohn and Blunsom (2010) later developed a block sampling algorithm with better convergence behavior. While this Bayesian method has yet to produce state of the art parsing results, it has achieved state of the art results for unsupervised grammar induction (Blunsom and Cohn, 2010) and has been extended to synchronous grammars for use in sentence compression (Yamangil and Shieber, 2010). More recently, (Sangati and Zuidema, 2011) presented an elegantly simple heuristic inspired by tree kernels that they call DoubleD</context>
</contexts>
<marker>Cohn, Goldwater, Blunsom, 2009</marker>
<rawString>Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing Compact but Accurate TreeSubstitution Grammars. In Proceedings NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>Blocked inference in Bayesian tree substitution grammars. Association for Computational Linguistics.</title>
<date>2010</date>
<contexts>
<context position="1640" citStr="Cohn and Blunsom, 2010" startWordPosition="257" endWordPosition="260">annotated with L1 labels that allows us to bring the power of supervised machine learning techniques to bear on this task. In this work we explore the possibility of automatically induced Tree Substitution Grammar (TSG) rules as features for a logistic regression model1 trained to predict these L1 labels. Automatic TSG induction is made difficult by the exponential number of possible TSG rules given a corpus. This is an active area of research with two distinct effective solutions. The first uses a nonparametric Bayesian model to handle the large number 1a.k.a. Maximum Entropy Model of rules (Cohn and Blunsom, 2010), while the second is inspired by tree kernel methods and extracts common subtrees from pairs of parse trees (Sangati and Zuidema, 2011). While both are effective, we show that the Bayesian method of TSG induction produces superior features and achieves a new best result at the task of native language detection. 2 Related Work 2.1 Native Language Detection Work in automatic native language detection has been mainly associated with the ICLE, published in 2002. Koppel et al (2005) first constructed such a system with a feature set consisting of function words, POS bi-grams, and character n-grams</context>
<context position="3965" citStr="Cohn and Blunsom (2010)" startWordPosition="625" endWordPosition="628">on One inherent difficulty in the use of TSGs is controlling the size of grammars automatically induced from data, which with any reasonable corpus quickly becomes too large for modern workstations to handle. When automatically induced TSGs were first proposed by Bod (1991), the problem of grammar induction was tackled with random selection of fragments or weak constraints that led to massive grammars. A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). They provide a local Gibbs sampling algorithm, and Cohn and Blunsom (2010) later developed a block sampling algorithm with better convergence behavior. While this Bayesian method has yet to produce state of the art parsing results, it has achieved state of the art results for unsupervised grammar induction (Blunsom and Cohn, 2010) and has been extended to synchronous grammars for use in sentence compression (Yamangil and Shieber, 2010). More recently, (Sangati and Zuidema, 2011) presented an elegantly simple heuristic inspired by tree kernels that they call DoubleDOP. They showed that manageable grammar sizes can be obtained from a corpus the size of the Penn Treeba</context>
<context position="7452" citStr="Cohn and Blunsom (2010)" startWordPosition="1215" endWordPosition="1218">xcluding e, #e is the count of the fragment e in a−, and #• is the total number of fragments in a− with the same root node as e. Po is 2As opposed to elementary tree, often used in related work shoes broccoli S NP VP VBZ hates NP George NNP NP NP NN NP NNS 194 a PCFG distribution over fragments with a bias towards small fragments. a is the concentration parameter of the DP, and can be used to roughly tune the number of fragments that appear in the sampled derivations. With this posterior distribution the derivations of a corpus can be sampled tree by tree using the block sampling algorithm of Cohn and Blunsom (2010), converging eventually on a sample from the true posterior of all derivations. 3.2 DoubleDOP Induction DoubleDOP uses a heuristic inspired by tree kernels, which are commonly used to measure similarity between two parse trees by counting the number of fragments they share. DoubleDOP uses the same underlying technique, but caches the shared fragments instead of simply counting them. This yields a set of fragments where each member is guaranteed to appear at least twice in the training set. In order to avoid unmanageably large grammars only maximal fragments are retained in each pairwise extrac</context>
</contexts>
<marker>Cohn, Blunsom, 2010</marker>
<rawString>Trevor Cohn, and Phil Blunsom 2010. Blocked inference in Bayesian tree substitution grammars. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="5009" citStr="Collins and Duffy (2001)" startWordPosition="792" endWordPosition="795">an elegantly simple heuristic inspired by tree kernels that they call DoubleDOP. They showed that manageable grammar sizes can be obtained from a corpus the size of the Penn Treebank by recording all fragments that occur at least twice, subject to a pairwise constraint of maximality. Using an additional heuristic to provide a distribution over fragments, DoubleDOP achieved the current state of the art for TSG parsing, competing closely with the absolute best results set by refinement based parsers. 2.3 Fragment Based Classification The use of parse tree fragments for classification began with Collins and Duffy (2001). They used the number of common subtrees between two parse trees as a convolution kernel in a voted perceptron and applied it as a parse reranker. Since then, such tree kernels have been used to perform a variety of text classification tasks, such as semantic role labeling (Moschitti et al, 2008), authorship attribution (Kim et al, 2010), or the work of Suzuki and Isozaki (2006) that performs question classification, subjectivity detection, and polarity identification. Syntactic features have also been used in nonkernelized classifiers, such as in the work of Wong and Dras (2011a) mentioned i</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins, Nigel Duffy 2001. Convolution Kernels for Natural Language. Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Efficient parsing of DOP with PCFG-reductions.</title>
<date>2003</date>
<note>In Bod et al. chapter 8..</note>
<marker>Goodman, 2003</marker>
<rawString>Joshua Goodman 2003. Efficient parsing of DOP with PCFG-reductions. In Bod et al. chapter 8..</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Granger</author>
<author>E Dagneaux</author>
<author>F Meunier</author>
</authors>
<date>2002</date>
<booktitle>International Corpus of Learner English,</booktitle>
<location>(ICLE).</location>
<contexts>
<context position="965" citStr="Granger et al, 2002" startWordPosition="145" endWordPosition="148">different language. We compare two state of the art methods for Tree Substitution Grammar induction and show that features from both methods outperform previous state of the art results at native language detection. Furthermore, we contrast these two induction algorithms and show that the Bayesian approach produces superior classification results with a smaller feature set. 1 Introduction The correlation between a person’s native language (L1) and aspects of their writing in a second language (L2) can be exploited to predict L1 label given L2 text. The International Corpus of Learner English (Granger et al, 2002), or ICLE, is a large set of English student essays annotated with L1 labels that allows us to bring the power of supervised machine learning techniques to bear on this task. In this work we explore the possibility of automatically induced Tree Substitution Grammar (TSG) rules as features for a logistic regression model1 trained to predict these L1 labels. Automatic TSG induction is made difficult by the exponential number of possible TSG rules given a corpus. This is an active area of research with two distinct effective solutions. The first uses a nonparametric Bayesian model to handle the l</context>
</contexts>
<marker>Granger, Dagneaux, Meunier, 2002</marker>
<rawString>S. Granger, E. Dagneaux and F. Meunier. 2002. International Corpus of Learner English, (ICLE).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sangkyum Kim</author>
<author>Hyungsul Kim</author>
<author>Tim Weninger</author>
<author>Jiawei Han</author>
</authors>
<title>Authorship classification: a syntactic tree mining approach.</title>
<date>2010</date>
<booktitle>Proceedings of the ACM SIGKDD Workshop on Useful Patterns.</booktitle>
<contexts>
<context position="5349" citStr="Kim et al, 2010" startWordPosition="851" endWordPosition="854">agments, DoubleDOP achieved the current state of the art for TSG parsing, competing closely with the absolute best results set by refinement based parsers. 2.3 Fragment Based Classification The use of parse tree fragments for classification began with Collins and Duffy (2001). They used the number of common subtrees between two parse trees as a convolution kernel in a voted perceptron and applied it as a parse reranker. Since then, such tree kernels have been used to perform a variety of text classification tasks, such as semantic role labeling (Moschitti et al, 2008), authorship attribution (Kim et al, 2010), or the work of Suzuki and Isozaki (2006) that performs question classification, subjectivity detection, and polarity identification. Syntactic features have also been used in nonkernelized classifiers, such as in the work of Wong and Dras (2011a) mentioned in Section 2.1. Additional examples include Raghavan et al (2010), which uses a CFG language model to perform authorship attribution, and Post (2011), which uses TSG features in a logistic regression model to perform grammaticality detection. 3 Tree Substitution Grammars Tree Substitution Grammars are similar to Context Free Grammars, diff</context>
</contexts>
<marker>Kim, Kim, Weninger, Han, 2010</marker>
<rawString>Sangkyum Kim, Hyungsul Kim, Tim Weninger, and Jiawei Han 2010. Authorship classification: a syntactic tree mining approach. Proceedings of the ACM SIGKDD Workshop on Useful Patterns.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Kfir Zigdon</author>
</authors>
<title>Determining an author’s native language by mining a text for errors.</title>
<date>2005</date>
<booktitle>Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining.</booktitle>
<contexts>
<context position="2123" citStr="Koppel et al (2005)" startWordPosition="336" endWordPosition="339">The first uses a nonparametric Bayesian model to handle the large number 1a.k.a. Maximum Entropy Model of rules (Cohn and Blunsom, 2010), while the second is inspired by tree kernel methods and extracts common subtrees from pairs of parse trees (Sangati and Zuidema, 2011). While both are effective, we show that the Bayesian method of TSG induction produces superior features and achieves a new best result at the task of native language detection. 2 Related Work 2.1 Native Language Detection Work in automatic native language detection has been mainly associated with the ICLE, published in 2002. Koppel et al (2005) first constructed such a system with a feature set consisting of function words, POS bi-grams, and character n-grams. These features provide a strong baseline but cannot capture many linguistic phenomena. More recently, Wong and Dras (2011a) considered syntactic features for this task, using logistic regression with features extracted from parse trees produced by a state of the art statistical parser. They investigated two classes of features: reranking features from the Charniak parser and CFG features. They showed that while reranking features capture long range dependencies in parse trees </context>
</contexts>
<marker>Koppel, Schler, Zigdon, 2005</marker>
<rawString>Koppel, Moshe and Schler, Jonathan and Zigdon, Kfir. 2005. Determining an author’s native language by mining a text for errors. Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Daniele Pighin and Roberto Basili</title>
<date>2008</date>
<marker>Moschitti, 2008</marker>
<rawString>Alessandro Moschitti, Daniele Pighin and Roberto Basili 2008. Tree Kernels for Semantic Role Labeling. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation. Association for Computational Linguistics.</title>
<date>2006</date>
<contexts>
<context position="9080" citStr="Petrov et al, 2006" startWordPosition="1481" endWordPosition="1484">tional Corpus of Learner English (Version 2), which consists of raw unsegmented English text tagged with L1 labels. Our experimental setup follows Wong and Dras (2011a) in analyzing Chinese, Russian, Bulgarian, Japanese, French, Czech, and Spanish L1 essays. As in their work we randomly sample 70 training and 25 test documents for each language. All reported results are averaged over 5 subsamplings of the full data set. Our data preproccesing pipeline is as follows: First we perform sentence segmentation with OpenNLP and then parse each sentence with a 6 split grammar for the Berkeley Parser (Petrov et al, 2006). We then replace all terminal symbols which do not occur in a list of 598 function words3 with a single UNK terminal. This aggressive removal of lexical items is standard in this task and mitigates the effect of other unwanted information sources such as topic and geographic location that are correlated with native language in the data. We contrast three different TSG feature sets in our experiments. First, to provide a baseline, we simply read off the CFG rules from the data set (note that a CFG can be taken as a TSG with all fragments having depth one). Second, in the method we call BTSG, w</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian Learning of a Tree Substitution Grammar. Association for Computational Linguistics.</title>
<date>2009</date>
<contexts>
<context position="3889" citStr="Post and Gildea (2009)" startWordPosition="613" endWordPosition="616">July 2012. c�2012 Association for Computational Linguistics 2.2 TSG induction One inherent difficulty in the use of TSGs is controlling the size of grammars automatically induced from data, which with any reasonable corpus quickly becomes too large for modern workstations to handle. When automatically induced TSGs were first proposed by Bod (1991), the problem of grammar induction was tackled with random selection of fragments or weak constraints that led to massive grammars. A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). They provide a local Gibbs sampling algorithm, and Cohn and Blunsom (2010) later developed a block sampling algorithm with better convergence behavior. While this Bayesian method has yet to produce state of the art parsing results, it has achieved state of the art results for unsupervised grammar induction (Blunsom and Cohn, 2010) and has been extended to synchronous grammars for use in sentence compression (Yamangil and Shieber, 2010). More recently, (Sangati and Zuidema, 2011) presented an elegantly simple heuristic inspired by tree kernels that they call DoubleDOP. They showed that manage</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Bayesian Learning of a Tree Substitution Grammar. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
</authors>
<title>Judging Grammaticality with Tree Substitution Grammar Derivations. Association for Computational Linguistics.</title>
<date>2011</date>
<contexts>
<context position="5757" citStr="Post (2011)" startWordPosition="916" endWordPosition="917"> reranker. Since then, such tree kernels have been used to perform a variety of text classification tasks, such as semantic role labeling (Moschitti et al, 2008), authorship attribution (Kim et al, 2010), or the work of Suzuki and Isozaki (2006) that performs question classification, subjectivity detection, and polarity identification. Syntactic features have also been used in nonkernelized classifiers, such as in the work of Wong and Dras (2011a) mentioned in Section 2.1. Additional examples include Raghavan et al (2010), which uses a CFG language model to perform authorship attribution, and Post (2011), which uses TSG features in a logistic regression model to perform grammaticality detection. 3 Tree Substitution Grammars Tree Substitution Grammars are similar to Context Free Grammars, differing in that they allow rewrite rules of arbitrary parse tree structure with any number of nonterminal or terminal leaves. We adopt the common term fragment2 to refer to these rules, as they are easily visualised as fragments of a complete parse tree. Figure 1: Fragments from a Tree Substitution Grammar capable of deriving the sentences “George hates broccoli” and “George hates shoes”. 3.1 Bayesian Induc</context>
</contexts>
<marker>Post, 2011</marker>
<rawString>Matt Post. 2011. Judging Grammaticality with Tree Substitution Grammar Derivations. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sindhu Raghavan</author>
</authors>
<title>Adriana Kovashka and Raymond Mooney 2010. Authorship attribution using probabilistic context-free grammars. Association for Computational Linguistics.</title>
<marker>Raghavan, </marker>
<rawString>Sindhu Raghavan, Adriana Kovashka and Raymond Mooney 2010. Authorship attribution using probabilistic context-free grammars. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Federico Sangati</author>
<author>Willem Zuidema</author>
</authors>
<title>Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP.</title>
<date>2011</date>
<booktitle>Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1776" citStr="Sangati and Zuidema, 2011" startWordPosition="280" endWordPosition="283">rk we explore the possibility of automatically induced Tree Substitution Grammar (TSG) rules as features for a logistic regression model1 trained to predict these L1 labels. Automatic TSG induction is made difficult by the exponential number of possible TSG rules given a corpus. This is an active area of research with two distinct effective solutions. The first uses a nonparametric Bayesian model to handle the large number 1a.k.a. Maximum Entropy Model of rules (Cohn and Blunsom, 2010), while the second is inspired by tree kernel methods and extracts common subtrees from pairs of parse trees (Sangati and Zuidema, 2011). While both are effective, we show that the Bayesian method of TSG induction produces superior features and achieves a new best result at the task of native language detection. 2 Related Work 2.1 Native Language Detection Work in automatic native language detection has been mainly associated with the ICLE, published in 2002. Koppel et al (2005) first constructed such a system with a feature set consisting of function words, POS bi-grams, and character n-grams. These features provide a strong baseline but cannot capture many linguistic phenomena. More recently, Wong and Dras (2011a) considered</context>
<context position="4374" citStr="Sangati and Zuidema, 2011" startWordPosition="690" endWordPosition="693">A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). They provide a local Gibbs sampling algorithm, and Cohn and Blunsom (2010) later developed a block sampling algorithm with better convergence behavior. While this Bayesian method has yet to produce state of the art parsing results, it has achieved state of the art results for unsupervised grammar induction (Blunsom and Cohn, 2010) and has been extended to synchronous grammars for use in sentence compression (Yamangil and Shieber, 2010). More recently, (Sangati and Zuidema, 2011) presented an elegantly simple heuristic inspired by tree kernels that they call DoubleDOP. They showed that manageable grammar sizes can be obtained from a corpus the size of the Penn Treebank by recording all fragments that occur at least twice, subject to a pairwise constraint of maximality. Using an additional heuristic to provide a distribution over fragments, DoubleDOP achieved the current state of the art for TSG parsing, competing closely with the absolute best results set by refinement based parsers. 2.3 Fragment Based Classification The use of parse tree fragments for classification </context>
</contexts>
<marker>Sangati, Zuidema, 2011</marker>
<rawString>Sangati, Federico and Zuidema, Willem 2011. Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Sequence and tree kernels with statistical feature mining.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="5391" citStr="Suzuki and Isozaki (2006)" startWordPosition="859" endWordPosition="862">urrent state of the art for TSG parsing, competing closely with the absolute best results set by refinement based parsers. 2.3 Fragment Based Classification The use of parse tree fragments for classification began with Collins and Duffy (2001). They used the number of common subtrees between two parse trees as a convolution kernel in a voted perceptron and applied it as a parse reranker. Since then, such tree kernels have been used to perform a variety of text classification tasks, such as semantic role labeling (Moschitti et al, 2008), authorship attribution (Kim et al, 2010), or the work of Suzuki and Isozaki (2006) that performs question classification, subjectivity detection, and polarity identification. Syntactic features have also been used in nonkernelized classifiers, such as in the work of Wong and Dras (2011a) mentioned in Section 2.1. Additional examples include Raghavan et al (2010), which uses a CFG language model to perform authorship attribution, and Post (2011), which uses TSG features in a logistic regression model to perform grammaticality detection. 3 Tree Substitution Grammars Tree Substitution Grammars are similar to Context Free Grammars, differing in that they allow rewrite rules of </context>
</contexts>
<marker>Suzuki, Isozaki, 2006</marker>
<rawString>Jun Suzuki and Hideki Isozaki 2006. Sequence and tree kernels with statistical feature mining. Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Exploiting Parse Structures for Native Language Identification.</title>
<date>2011</date>
<booktitle>Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2363" citStr="Wong and Dras (2011" startWordPosition="372" endWordPosition="375">rees (Sangati and Zuidema, 2011). While both are effective, we show that the Bayesian method of TSG induction produces superior features and achieves a new best result at the task of native language detection. 2 Related Work 2.1 Native Language Detection Work in automatic native language detection has been mainly associated with the ICLE, published in 2002. Koppel et al (2005) first constructed such a system with a feature set consisting of function words, POS bi-grams, and character n-grams. These features provide a strong baseline but cannot capture many linguistic phenomena. More recently, Wong and Dras (2011a) considered syntactic features for this task, using logistic regression with features extracted from parse trees produced by a state of the art statistical parser. They investigated two classes of features: reranking features from the Charniak parser and CFG features. They showed that while reranking features capture long range dependencies in parse trees that CFG rules cannot, they do not produce classification performance superior to simple CFG rules. Their CFG feature approach represents the best performing model to date for the task of native language detection. Wong and Dras (2011b) als</context>
<context position="5595" citStr="Wong and Dras (2011" startWordPosition="888" endWordPosition="891">gan with Collins and Duffy (2001). They used the number of common subtrees between two parse trees as a convolution kernel in a voted perceptron and applied it as a parse reranker. Since then, such tree kernels have been used to perform a variety of text classification tasks, such as semantic role labeling (Moschitti et al, 2008), authorship attribution (Kim et al, 2010), or the work of Suzuki and Isozaki (2006) that performs question classification, subjectivity detection, and polarity identification. Syntactic features have also been used in nonkernelized classifiers, such as in the work of Wong and Dras (2011a) mentioned in Section 2.1. Additional examples include Raghavan et al (2010), which uses a CFG language model to perform authorship attribution, and Post (2011), which uses TSG features in a logistic regression model to perform grammaticality detection. 3 Tree Substitution Grammars Tree Substitution Grammars are similar to Context Free Grammars, differing in that they allow rewrite rules of arbitrary parse tree structure with any number of nonterminal or terminal leaves. We adopt the common term fragment2 to refer to these rules, as they are easily visualised as fragments of a complete parse</context>
<context position="8627" citStr="Wong and Dras (2011" startWordPosition="1403" endWordPosition="1406">agments are retained in each pairwise extraction, which is to say that any shared fragment that occurs inside another shared fragment is discarded. The main disadvantage of this method is that the complexity scales quadratically with the training set size, as all pairs of sentences must be considered. It is fully parallelizable, however, which mediates this disadvantage to some extent. 4 Experiments 4.1 Methodology Our data is drawn from the International Corpus of Learner English (Version 2), which consists of raw unsegmented English text tagged with L1 labels. Our experimental setup follows Wong and Dras (2011a) in analyzing Chinese, Russian, Bulgarian, Japanese, French, Czech, and Spanish L1 essays. As in their work we randomly sample 70 training and 25 test documents for each language. All reported results are averaged over 5 subsamplings of the full data set. Our data preproccesing pipeline is as follows: First we perform sentence segmentation with OpenNLP and then parse each sentence with a 6 split grammar for the Berkeley Parser (Petrov et al, 2006). We then replace all terminal symbols which do not occur in a list of 598 function words3 with a single UNK terminal. This aggressive removal of l</context>
<context position="11338" citStr="Wong and Dras (2011" startWordPosition="1874" endWordPosition="1877">erivation of some tree t E d that uses e. Classification is performed with the Mallet package for logistic regression using the default initialized MaxEntTrainer. 5 Results 5.1 Predictive Power The resulting classification accuracies are shown in Table 1. The BTSG feature set gives the highest performance, and both true TSG induction techniques outperform the CFG baseline. 3We use the stop word list distributed with the ROUGE summarization evaluation package. 195 Model Accuracy (%) CFG 72.6 2DOP 73.5 2DOP(F) 76.8 BTSG 78.4 Table 1: Classification accuracy The CFG result represents the work of Wong and Dras (2011a), the previous best result for this task. While in their work they report 80% accuracy with the CFG model, this is for a single sampling of the full data set. We observed a large variance in classification accuracy over such samplings, which includes some values in their reported range but with a much lower mean. The numbers we report are from our own implementation of their CFG technique, and all results are averaged over 5 random samplings from the full corpus. For 2DOP we limit the 2DOP(F) fragments by choosing the 7k with maximum frequency, but there may exist superior methods. Indeed, W</context>
</contexts>
<marker>Wong, Dras, 2011</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras 2011. Exploiting Parse Structures for Native Language Identification. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Topic Modeling for Native Language Identification.</title>
<date>2011</date>
<booktitle>Proceedings of the Australasian Language Technology Association Workshop.</booktitle>
<contexts>
<context position="2363" citStr="Wong and Dras (2011" startWordPosition="372" endWordPosition="375">rees (Sangati and Zuidema, 2011). While both are effective, we show that the Bayesian method of TSG induction produces superior features and achieves a new best result at the task of native language detection. 2 Related Work 2.1 Native Language Detection Work in automatic native language detection has been mainly associated with the ICLE, published in 2002. Koppel et al (2005) first constructed such a system with a feature set consisting of function words, POS bi-grams, and character n-grams. These features provide a strong baseline but cannot capture many linguistic phenomena. More recently, Wong and Dras (2011a) considered syntactic features for this task, using logistic regression with features extracted from parse trees produced by a state of the art statistical parser. They investigated two classes of features: reranking features from the Charniak parser and CFG features. They showed that while reranking features capture long range dependencies in parse trees that CFG rules cannot, they do not produce classification performance superior to simple CFG rules. Their CFG feature approach represents the best performing model to date for the task of native language detection. Wong and Dras (2011b) als</context>
<context position="5595" citStr="Wong and Dras (2011" startWordPosition="888" endWordPosition="891">gan with Collins and Duffy (2001). They used the number of common subtrees between two parse trees as a convolution kernel in a voted perceptron and applied it as a parse reranker. Since then, such tree kernels have been used to perform a variety of text classification tasks, such as semantic role labeling (Moschitti et al, 2008), authorship attribution (Kim et al, 2010), or the work of Suzuki and Isozaki (2006) that performs question classification, subjectivity detection, and polarity identification. Syntactic features have also been used in nonkernelized classifiers, such as in the work of Wong and Dras (2011a) mentioned in Section 2.1. Additional examples include Raghavan et al (2010), which uses a CFG language model to perform authorship attribution, and Post (2011), which uses TSG features in a logistic regression model to perform grammaticality detection. 3 Tree Substitution Grammars Tree Substitution Grammars are similar to Context Free Grammars, differing in that they allow rewrite rules of arbitrary parse tree structure with any number of nonterminal or terminal leaves. We adopt the common term fragment2 to refer to these rules, as they are easily visualised as fragments of a complete parse</context>
<context position="8627" citStr="Wong and Dras (2011" startWordPosition="1403" endWordPosition="1406">agments are retained in each pairwise extraction, which is to say that any shared fragment that occurs inside another shared fragment is discarded. The main disadvantage of this method is that the complexity scales quadratically with the training set size, as all pairs of sentences must be considered. It is fully parallelizable, however, which mediates this disadvantage to some extent. 4 Experiments 4.1 Methodology Our data is drawn from the International Corpus of Learner English (Version 2), which consists of raw unsegmented English text tagged with L1 labels. Our experimental setup follows Wong and Dras (2011a) in analyzing Chinese, Russian, Bulgarian, Japanese, French, Czech, and Spanish L1 essays. As in their work we randomly sample 70 training and 25 test documents for each language. All reported results are averaged over 5 subsamplings of the full data set. Our data preproccesing pipeline is as follows: First we perform sentence segmentation with OpenNLP and then parse each sentence with a 6 split grammar for the Berkeley Parser (Petrov et al, 2006). We then replace all terminal symbols which do not occur in a list of 598 function words3 with a single UNK terminal. This aggressive removal of l</context>
<context position="11338" citStr="Wong and Dras (2011" startWordPosition="1874" endWordPosition="1877">erivation of some tree t E d that uses e. Classification is performed with the Mallet package for logistic regression using the default initialized MaxEntTrainer. 5 Results 5.1 Predictive Power The resulting classification accuracies are shown in Table 1. The BTSG feature set gives the highest performance, and both true TSG induction techniques outperform the CFG baseline. 3We use the stop word list distributed with the ROUGE summarization evaluation package. 195 Model Accuracy (%) CFG 72.6 2DOP 73.5 2DOP(F) 76.8 BTSG 78.4 Table 1: Classification accuracy The CFG result represents the work of Wong and Dras (2011a), the previous best result for this task. While in their work they report 80% accuracy with the CFG model, this is for a single sampling of the full data set. We observed a large variance in classification accuracy over such samplings, which includes some values in their reported range but with a much lower mean. The numbers we report are from our own implementation of their CFG technique, and all results are averaged over 5 random samplings from the full corpus. For 2DOP we limit the 2DOP(F) fragments by choosing the 7k with maximum frequency, but there may exist superior methods. Indeed, W</context>
</contexts>
<marker>Wong, Dras, 2011</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras 2011. Topic Modeling for Native Language Identification. Proceedings of the Australasian Language Technology Association Workshop.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Elif Yamangil</author>
<author>M Stuart</author>
</authors>
<title>Shieber 2010. Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression.. Association for Computational Linguistics.</title>
<marker>Yamangil, Stuart, </marker>
<rawString>Elif Yamangil, Stuart M. Shieber 2010. Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression.. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>