<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002338">
<title confidence="0.989579">
The OpenGrm open-source finite-state grammar software libraries
</title>
<author confidence="0.891548">
Brian Roark† Richard Sproat†◦ Cyril Allauzen◦ Michael Riley◦ Jeffrey Sorensen◦ &amp; Terry Tai◦
†Oregon Health &amp; Science University, Portland, Oregon ◦Google, Inc., New York
</author>
<sectionHeader confidence="0.955194" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9986652">
In this paper, we present a new collection
of open-source software libraries that pro-
vides command line binary utilities and library
classes and functions for compiling regular
expression and context-sensitive rewrite rules
into finite-state transducers, and for n-gram
language modeling. The OpenGrm libraries
use the OpenFst library to provide an efficient
encoding of grammars and general algorithms
for building, modifying and applying models.
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999776782608696">
The OpenGrm libraries1 are a (growing) collec-
tion of open-source software libraries for build-
ing and applying various kinds of formal gram-
mars. The C++ libraries use the OpenFst library2
for the underlying finite-state representation, which
allows for easy inspection of the resulting grammars
and models, as well as straightforward combination
with other finite-state transducers. Like OpenFst,
there are easy-to-use command line binaries for fre-
quently used operations, as well as a C++ library
interface, allowing library users to create their own
algorithms from the basic classes and functions pro-
vided.
The libraries can be used for a range of com-
mon string processing tasks, such as text normal-
ization, as well as for building and using large sta-
tistical models for applications like speech recogni-
tion. In the rest of the paper, we will present each of
the two libraries, starting with the Thrax grammar
compiler and then the NGram library. First, though,
we will briefly present some preliminary (infor-
mal) background on weighted finite-state transduc-
ers (WFST), just as needed for this paper.
</bodyText>
<footnote confidence="0.999931">
1http://www.opengrm.org/
2http://www.openfst.org/
</footnote>
<page confidence="0.995963">
61
</page>
<sectionHeader confidence="0.998163" genericHeader="method">
2 Informal WFST preliminaries
</sectionHeader>
<bodyText confidence="0.999993103448276">
A weighted finite-state transducer consists of a set
of states and transitions between states. There is an
initial state and a subset of states are final. Each tran-
sition is labeled with an input symbol from an input
alphabet; an output symbol from an output alpha-
bet; an origin state; a destination state; and a weight.
Each final state has an associated final weight. A
path in the WFST is a sequence of transitions where
each transition’s destination state is the next transi-
tion’s origin state. A valid path through the WFST is
a path where the origin state of the first transition is
an initial state, and the the last transition is to a final
state. Weights combine along the path according to
the semiring of the WFST.
If every transition in the transducer has the same
input and output symbol, then the WFST represents
a weighted finite-state automaton. In the OpenFst
library, there are a small number of special sym-
bols that can be used. The c symbol represents the
empty string, which allows the transition to be tra-
versed without consuming any symbol. The 0 (or
failure) symbol on a transition also allows it to be
traversed without consuming any symbol, but it dif-
fers from c in only allowing traversal if the symbol
being matched does not label any other transition
leaving the same state, i.e., it encodes the semantics
of otherwise, which is useful for language models.
For a more detailed presentation of WFSTs, see Al-
lauzen et al. (2007).
</bodyText>
<sectionHeader confidence="0.990339" genericHeader="method">
3 The Thrax Grammar Compiler
</sectionHeader>
<bodyText confidence="0.99771875">
The Thrax grammar compiler3 compiles grammars
that consist of regular expressions, and context-
dependent rewrite rules, into FST archives (fars) of
weighted finite state transducers. Grammars may
</bodyText>
<footnote confidence="0.9994965">
3The compiler is named after Dionysius Thrax (170–
90BCE), the reputed first Greek grammarian.
</footnote>
<note confidence="0.879164">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 61–66,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999715">
be split over multiple files and imported into other
grammars. Strings in the rules may be parsed
in one of three different ways: as a sequence of
bytes (the default), as utf8 encodings, or accord-
ing to a user-provided symbol table. With the
--save symbols flag, the transducers can be
saved out into fars with appropriate symbol tables.
The Thrax libraries provide full support for dif-
ferent weight (semiring) classes. The command-line
flag --semiring allows one to set the semiring,
currently to one of: tropical (default), log or log64
semirings.
</bodyText>
<subsectionHeader confidence="0.997927">
3.1 General Description
</subsectionHeader>
<bodyText confidence="0.994913">
Thrax revolves around rules which, typically, con-
struct an FST based on a given input. In the simplest
case, we can just provide a string that represents a
(trivial) transducer and name it using the assignment
operator:
</bodyText>
<equation confidence="0.835255">
pear = &amp;quot;pear&amp;quot;;
</equation>
<bodyText confidence="0.998916666666667">
In this example, we have an FST consisting of the
characters “p”, “e”, “a”, and “r” in a chain, assigned
to the identifier pear:
This identifier can be used later in order to build
further FSTs, using built-in operators or using cus-
tom functions:
</bodyText>
<equation confidence="0.979109">
kiwi = &amp;quot;kiwi&amp;quot;;
fruits = pear  |kiwi; # union
</equation>
<bodyText confidence="0.999816736842105">
In Thrax, string FSTs are enclosed by double-quotes
(&amp;quot;) whereas simple strings (often used as pathnames
for functions) are enclosed in single-quotes (’).
Thrax provides a set of built-in functions that
aid in the construction of more complex expres-
sions. We have already seen the disjunction “|” in
the previous example. Other standard regular op-
erations are expr*, expr+, expr? and expr{m,n},
the latter repeating expr between m and n times,
inclusive. Composition is notated with “@” so
that expr1 @ expr2 denotes the composition of
expr1 and expr2. Rewriting is denoted with “:”
where expr1 : expr2 rewrites strings that match
expr1 into expr2. Weights can be added to expres-
sions using the notation “&lt;&gt;”: thus, expr&lt;2.4&gt;
adds weight 2.4 to expr. Various operations on
FSTs are also provided by built-in functions, includ-
ing Determinize, Minimize, Optimize and
Invert, among many others.
</bodyText>
<subsectionHeader confidence="0.998998">
3.2 Detailed Description
</subsectionHeader>
<bodyText confidence="0.999130222222222">
A Thrax grammar consists of a set of one or more
source files, each of which must have the extension
.grm. The compiler compiles each source file to a
single FST archive with the extension .far. Each
grammar file has sections: Imports and Body, each
of which is optional. The body section can include
statements interleaved with functions, as specified
below. Comments begin with a single pound sign
(#) and last until the next newline.
</bodyText>
<subsectionHeader confidence="0.803415">
3.2.1 Imports
</subsectionHeader>
<bodyText confidence="0.999911363636364">
The Thrax compiler compiles source files (with
the extension .grm) into FST archive files (with
the extension .far). FST archives are an Open-
Fst storage format for a series of one or more FSTs.
The FST archive and the original source file then
form a pair which can be imported into other source
files, allowing a Python-esque include system that is
hopefully familiar to many. Instead of working with
a monolithic file, Thrax allows for a modular con-
struction of the final rule set as well as sharing of
common elements across projects.
</bodyText>
<subsectionHeader confidence="0.894002">
3.2.2 Functions
</subsectionHeader>
<bodyText confidence="0.991832130434783">
Thrax has extensive support for functions that can
greatly augment the capabilities of the language.
Functions in Thrax can be specified in two ways.
The first is inline via the func keyword within grm
files. These functions can take any number of input
arguments and must return a single result (usually an
FST) to the caller via the return keyword:
func DumbPluralize[fst] {
# Concatenate with &amp;quot;s&amp;quot;...
result = fst &amp;quot;s&amp;quot;;
# ...and then return to caller.
return result;
}
Alternatively, functions can be written C++ and
added to the language. Regardless of the func-
tion implementation method (inline in Thrax or
subclassed in C++), functions are integrated into
the Thrax environment and can be called directly
by using the function name and providing the
necessary arguments. Thus, assuming someone has
written a function called NetworkPluralize
that retrieves the plural of a word from some web-
site, one could write a grammar fragment as follows:
</bodyText>
<page confidence="0.847204">
62
</page>
<equation confidence="0.9642818">
apple = &amp;quot;apple&amp;quot;;
plural_apple = DumbPluralize[apple];
plural_tomato = NetworkPluralize[
&amp;quot;tomato&amp;quot;,
’http://server:port/...’];
</equation>
<subsectionHeader confidence="0.574055">
3.2.3 Statements
</subsectionHeader>
<bodyText confidence="0.99491675">
Functions can be interleaved with grammar state-
ments that generate the FSTs that are exported to the
FST archive as output. Each statement consists of an
assignment terminating with a semicolon:
foo = &amp;quot;abc&amp;quot;;
export bar = foo  |&amp;quot;xyz&amp;quot;;
Statements preceded with the export keyword will
be written to the final output archive. Statements
lacking this keyword define temporaries that be used
later, but are themselves not output.
The basic elements of any grammar are string
FSTs, which, as mentioned earlier, are defined by
text enclosed by double quotes (&amp;quot;), in contrast to
raw strings, which are enclosed by single quotes (’).
String FSTs can be parsed in one of three ways,
which are denoted using a dot (.) followed by ei-
ther byte, utf8, or an identifier holding a symbol ta-
ble. Note that within strings, the backslash character
(\) is used to escape the next character. Of partic-
ular note, ‘\n’ translates into a newline, ‘\r’ into
a line feed, and ‘\t’ into the tab character. Literal
left and right square brackets also need escaping, as
they are used to generate symbols (see below). All
other characters following the backslash are unin-
terpreted, so that we can use \” and \’ to insert an
actual quote (double) quote symbol instead of termi-
nating the string.
Strings, by default, are interpreted as sequences
of bytes, each transition of the resulting FST
corresponding to a single 1-byte character of the
input. This can be specified either by leaving off the
parse mode (&amp;quot;abc&amp;quot;) or by explicitly using the byte
mode (&amp;quot;abc&amp;quot;.byte). The second way is to use
UTF8 parsing by using the special keyword, e.g.:
Finally, we can load a symbol table and split
thestringusingthefst field separatorflag
(found in fst/src/lib/symbol-table.cc)
and then perform symbol table lookups. Symbol ta-
bles can be loaded using the SymbolTable built-in
function:
</bodyText>
<equation confidence="0.967118333333333">
arctic_symbol_table =
SymbolTable[’/path/to/bears.symtab’];
pb = &amp;quot;polar bear&amp;quot;.arctic_symbol_table;
</equation>
<bodyText confidence="0.9997854">
One can also create temporary symbols on the
fly by enclosing a symbol name inside brackets
within an FST string. All of the text inside the
brackets will be taken to be part of the symbol
name, and future encounters of the same symbol
name will map to the same label. By default, la-
bels use “Private Use Area B” of the unicode ta-
ble (0x100000 - 0x10FFFD), except that the last two
code points 0x10FFFC and 0x10FFFD are reserved
for the “[BOS]” and “[EOS]” tags discussed below.
</bodyText>
<equation confidence="0.923384">
cross_pos = &amp;quot;cross&amp;quot; (&amp;quot;&amp;quot; : &amp;quot;_[s_noun]&amp;quot;);
pluralize_nouns = &amp;quot;_[s_noun]&amp;quot; : &amp;quot;es&amp;quot;;
</equation>
<subsectionHeader confidence="0.795257">
3.3 Standard Library Functions and
Operations
</subsectionHeader>
<bodyText confidence="0.999959266666667">
Built-in functions are provided that operate on FSTs
and perform most of the operations that are available
in the OpenFst library. These include: closure, con-
catenation, difference, composition and union. In
most cases the notation of these functions follows
standard conventions. Thus, for example, for clo-
sure, the following syntax applies: fst* (accepts fst
0 or more times); fst+ (accepts fst 1 or more times);
fst? (accepts fst 0 or 1 times) fst{x,y} (accepts fst at
least x but no more than y times).
The operator “@” is used for composition: a @
b denotes a composed with b. A “:” is used to de-
note rewrite, where a : b denotes a transducer
that deletes a and inserts b. Most functions can also
be expressed using functional notation:
</bodyText>
<equation confidence="0.705565">
b = Rewrite[&amp;quot;abc&amp;quot;, &amp;quot;def&amp;quot;];
</equation>
<bodyText confidence="0.999250625">
The delimiters &lt; and &gt; add a weight to an expres-
sion in the chosen semiring: a&lt;3&gt; adds the weight
3 (in the tropical semiring by default) to a.
Functions lacking operators (hence only called
by function name) include: ArcSort, Connect,
Determinize, RmEpsilon, Minimize,
Optimize, Invert, Project and Reverse.
Most of these call the obvious underlying OpenFst
function.
One function in particular, CDRewrite is worth
further discussion. This function takes a transducer
and two context acceptors (and the alphabet ma-
chine), and generates a new FST that performs a
context dependent rewrite everywhere in the pro-
vided contexts. The context-dependent rewrite algo-
rithm used is that of Mohri and Sproat (1996), and
</bodyText>
<page confidence="0.997453">
63
</page>
<bodyText confidence="0.972281171428571">
see also Kaplan and Kay (1994). The fourth argu-
ment (sigma star) needs to be a minimized ma-
chine. The fifth argument selects the direction of
rewrite; we can either rewrite left-to-right or right-
to-left or simultaneously. The sixth argument selects
whether the rewrite is optional.
CDRewrite[tau, lambda, rho,
sigma_star,
’ltr’|’rtl’|’sim’,
’obl’|’opt’]
For context-dependent rewrite rules, two built-in
symbols “[BOS]” and “[EOS]” have a special mean-
ing in the context specifications: they refer to the
beginning and end of string, respectively.
There are also built-in functions that perform
other tasks. In the interest of space we concentrate
here on the StringFile function, which loads a
file consisting of a list of strings, or tab-separated
pairs of strings, and compiles them to an acceptor
that represents the union of the strings.
StringFile[’strings_file’]
While it is equivalent to the union of the individual
string (pairs), StringFile uses an efficient algo-
rithm for constructing a prefix tree (trie) from the
list and can be significantly more efficient than com-
puting a union for large lists. If a line consists of a
tab-separated pair of strings a, b, a transducer equiv-
alent to Rewrite[a, b] is compiled.
The optional keywords byte (default), utf8 or
the name of a symbol table can be used to specify
the parsing mode for the strings. Thus
StringFile[’strings_file’, utf8, my_symtab]
would parse a sequence of tab-separated pairs, using
utf8 parsing for the left-hand string, and the symbol
table my symtab for the right-hand string.
</bodyText>
<sectionHeader confidence="0.987241" genericHeader="method">
4 NGram Library
</sectionHeader>
<bodyText confidence="0.999044">
The OpenGrm NGram library contains tools for
building, manipulating and using n-gram language
models represented as weighted finite-state trans-
ducers. The same finite-state topology is used to en-
code raw counts as well as smoothed models. Here
we briefly present this structure, followed by details
on the operations manipulating it.
An n-gram is a sequence of n symbols: w1 ... w,,,.
Each state in the model represents a prefix history
of the n-gram (w1 ... w,,,_1), and transitions in the
model represent either n-grams or backoff transi-
tions following that history. Figure 1 lists conven-
tions for states and transitions used to encode the
n-grams as a WFST.
This representation is similar to that used in other
WFST-based n-gram software libraries, such as the
AT&amp;T GRM library (Allauzen et al., 2005). One
key difference is the implicit representation of &lt;s&gt;
and &lt;/s&gt;, as opposed to encoding them as symbols
in the grammar. This has the benefit of including all
start and stop symbol functionality while avoiding
common pitfalls that arise with explicit symbols.
Another difference from the GRM library repre-
sentation is explicit inclusion of failure links from
states to their backoff states even in the raw count
files. The OpenGrm n-gram FST format is consis-
tent through all stages of building the models, mean-
ing that model manipulation (e.g., merging of two
</bodyText>
<figureCaption confidence="0.999684">
Figure 1: List of state and transition conventions used to encode collection of n-grams in WFST.
</figureCaption>
<bodyText confidence="0.935018">
An n-gram is a sequence of n symbols: w1 ... wn. Its proper prefixes include all sequences w1 ... wk for k &lt; n.
</bodyText>
<listItem confidence="0.995825090909091">
• There is a unigram state in every model, representing the empty string.
• Every proper prefix of every n-gram in the model has an associated state in the model.
• The state associated with an n-gram w1...wn has a backoff transition (labeled with c) to the state associated
with its suffix w2...wn.
• An n-gram w1...wn is represented as a transition, labeled with wn, from the state associated with its prefix
w1...wn−1 to a destination state defined as follows:
– If w1...wn is a proper prefix of an n-gram in the model, then the destination of the transition is the state
associated with w1...wn
– Otherwise, the destination of the transition is the state associated with the suffix w2...wn.
• Start and end of the sequence are not represented via transitions in the automaton or symbols in the symbol
table. Rather
</listItem>
<bodyText confidence="0.6941125">
– The start state of the automaton encodes the “start of sequence” n-gram prefix (commonly denoted &lt;s&gt;).
– The end of the sequence (often denoted &lt;/s&gt;) is included in the model through state final weights, i.e.,
for a state associated with an n-gram prefix w1...wn, the final weight of that state represents the weight
of the n-gram w1...wn&lt;/s&gt;.
</bodyText>
<page confidence="0.993611">
64
</page>
<figure confidence="0.999916">
(a)
(b)
</figure>
<figureCaption confidence="0.985122333333333">
Figure 2: FST representations of (a) bigram and unigram
counts; and (b) smoothed bigram model, when trained on the
single string “a b a b b a”
</figureCaption>
<bodyText confidence="0.999969333333333">
models or count files, or pruning them) can be pro-
cessed by the same operations. By convention, all
counts and probabilities are stored as negative logs,
and the FSTs are in the Tropical semiring. The sym-
bol table provided during counting is kept with the
model FSTs.
</bodyText>
<subsectionHeader confidence="0.986765">
4.1 N-gram Counting
</subsectionHeader>
<bodyText confidence="0.995857617647059">
The command line binary ngramcount takes as
input an FST archive (far) consisting of a collection
of acyclic WFSTs and outputs an n-gram WFST of
the specified order. The acyclic WFSTs can be linear
automata representing strings from a corpus – easily
compiled using the farcompilestrings com-
mand of OpenFst – or weighted word lattices output
from, say, a speech recognition or machine transla-
tion system. In such a way, expected frequencies of
n-grams can be counted. To count all trigrams, bi-
grams and unigrams in the compiled (far) corpus:
ngramcount -order=3 in.far &gt;3g.cnt.fst
For example, counting with the -order=2 flag
(bigrams) from a corpus consisting of a single string
“a b a b b a” yields the FST in Figure 2(a).
Each state represents a prefix history: the leftmost
state is the initial state, representing the &lt;s&gt; his-
tory; the central state is the unigram state, represent-
ing the e history; the topmost state represents the his-
tory ‘a’; and the bottom state represents the history
‘b’. Since this is a bigram model, histories consist of
at most one prior symbol from the vocabulary. Dou-
ble circles represent final states, which come with a
final weight encoding the negative log count of end-
ing the string at that state. Only the ‘a’ history state
and the unigram state are final states, since our ex-
ample string ends with the symbol ‘a’. (The unigram
state is always final.) The e transitions are backoff
transitions, and the weights on each n-gram transi-
tion are negative log counts of that symbol occurring
following the history that the state represents. Hence
the bigram “b b” occurs once, yielding a negative
log of zero for the transition labeled with ‘b’ leaving
the state representing the history ‘b’.
</bodyText>
<subsectionHeader confidence="0.856817">
4.2 N-gram Model Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.995340272727273">
Given counts, one can build a smoothed n-gram
model by normalizing and smoothing, which is ac-
complished with the ngrammake command line
binary. The library has several available smooth-
ing methods, including Katz (1987), absolute dis-
counting (Ney et al., 1994), Kneser-Ney (1995) and
(the default) Witten-Bell (1991). See Chen and
Goodman (1998) for a detailed presentation of these
smoothing methods. Each of these smoothing meth-
ods is implemented as a relatively simple derived
subclass, thus allowing for straightforward exten-
sion to new and different smoothing methods. To
make a smoothed n-gram model from counts:
ngrammake 3g.cnt.fst &gt;3g.mod.fst
Figure 2(b) shows the model built using the de-
fault Witten-Bell smoothing from the count FST in
2(a). The topology remains identical, but now the
n-gram transition weights and final weights are neg-
ative log probabilities. The backoff transitions (la-
beled with e) have the negative log backoff weights,
which ensure that the model is correctly normalized.
Models, by default, are smoothed by interpolat-
ing higher- and lower-order probabilities. This is
even true for methods more typically associated with
backoff (rather than mixture) smoothing styles, such
as Katz. While the smoothing values are estimated
using interpolation, the model is encoded as a back-
off model by pre-summing the interpolated proba-
bilities, so that the backoff transitions are to be tra-
versed only for symbols without transitions out of
the current state. While these backoff transitions are
labeled with e, see Section 4.4 for discussion of ap-
plying them as failure transitions.
</bodyText>
<figure confidence="0.99824705">
a/0
0
ε
a/-1.1
ε
0 a/-0.69
b/-0.69
b/-1.1
ε
b/0
a/0.336
1.36
ε/0.916
a/0.847
ε/0.693
0.946 a/0.56
b/0.56
b/0.847
ε/0.916
b/0.99
</figure>
<page confidence="0.991395">
65
</page>
<subsectionHeader confidence="0.805003">
4.3 N-gram Model Merging and Pruning
</subsectionHeader>
<bodyText confidence="0.978072842105263">
Two n-gram count FSTs or two model FSTs can be
merged into a single FST using ngrammerge, with
command line flags to allow for scaling of each of
the two, and to indicate whether to carry out full nor-
malization. This approach allows for various sorts of
MAP adaptation approaches for the n-gram models
(Bacchiani et al., 2006). To merge two input FST
models with no scaling:
ngrammerge in.mod1 in.mod2 &gt;mrg.mod
N-gram model pruning is provided with three dif-
ferent methods: count pruning based on a threshold;
the method from Seymore and Rosenfeld (1996);
and relative entropy pruning of Stolcke (1998). Like
smoothing, each of these pruning methods is imple-
mented as a relatively simple derived subclass, thus
allowing for straightforward extension to new and
different pruning methods. To prune a smoothed n-
gram model:
ngramshrink -theta=4 in.mod &gt;prn.mod
</bodyText>
<subsectionHeader confidence="0.883265">
4.4 N-gram Utilities
</subsectionHeader>
<bodyText confidence="0.9999956">
In addition to the above detailed core operations on
language models, the OpenGrm NGram library has
a number of utilities that make building and using
the models very easy. There are utilities related
to input and output, including ngramsymbols,
which produces a symbol table from a corpus;
ngramread, which reads in textual count files
and models in ARPA format and encodes them
as an FST; ngramprint which prints n-gram
counts or ARPA format text files; and ngraminfo
which displays information about the model, such
as number of n-grams of various orders. There
are also utilities related to the use of the models,
including ngramapply, which applies the model
to an input FST archive (far); ngramrandgen
which randomly generates strings from the model;
and ngramperplexity which calculates the per-
plexity of a corpus given the model. Note that
ngramapply includes options for interpreting the
backoff transitions as failure transitions.
</bodyText>
<sectionHeader confidence="0.998845" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999336">
This work was supported in part by a Google Fac-
ulty Research Award, NSF grant #IIS-0811745 and
DARPA #HR0011-09-1-0041. Any opinions, find-
ings, conclusions or recommendations expressed in
this publication are those of the authors and do not
necessarily reflect the views of the NSF or DARPA.
</bodyText>
<sectionHeader confidence="0.996423" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999852">
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2005.
The design principles and algorithms of a weighted
grammar library. International Journal of Founda-
tions of Computer Science, 16(3):403–421.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Twelfth International
Conference on Implementation and Application ofAu-
tomata (CIAA 2007), Lecture Notes in Computer Sci-
ence, volume 4793, pages 11–23.
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41–68.
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical report, TR-10-98, Harvard University.
Ronald M. Kaplan and Martin Kay. 1994. Regular mod-
els of phonological rule systems. Computational Lin-
guistics, 20:331–378.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recogniser. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3):400–401.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP),
pages 181–184.
Mehryar Mohri and Richard Sproat. 1996. An efficient
compiler for weighted rewrite rules. In Proceedings of
the 34th Annual Meeting of the Association for Com-
putational Linguistics, pages 231–238.
Hermann Ney, Ute Essen, and Reinhard Kneser. 1994.
On structuring probabilistic dependences in stochastic
language modeling. Computer Speech and Language,
8:1–38.
Kristie Seymore and Ronald Rosenfeld. 1996. Scalable
backoff language models. In Proceedings of the Inter-
national Conference on Spoken Language Processing
(ICSLP).
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In Proc. DARPA Broadcast News
Transcription and Understanding Workshop, pages
270–274.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085–
1094.
</reference>
<page confidence="0.988983">
66
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.902575">
<title confidence="0.999901">The OpenGrm open-source finite-state grammar software libraries</title>
<author confidence="0.9619135">Cyril Michael Jeffrey</author>
<author confidence="0.9619135">Terry Health</author>
<author confidence="0.9619135">Science University</author>
<author confidence="0.9619135">Oregon Inc Portland</author>
<author confidence="0.9619135">New York</author>
<abstract confidence="0.997713454545455">In this paper, we present a new collection of open-source software libraries that provides command line binary utilities and library classes and functions for compiling regular expression and context-sensitive rewrite rules finite-state transducers, and for language modeling. The OpenGrm libraries use the OpenFst library to provide an efficient encoding of grammars and general algorithms for building, modifying and applying models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>The design principles and algorithms of a weighted grammar library.</title>
<date>2005</date>
<journal>International Journal of Foundations of Computer Science,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="14374" citStr="Allauzen et al., 2005" startWordPosition="2319" endWordPosition="2322">is used to encode raw counts as well as smoothed models. Here we briefly present this structure, followed by details on the operations manipulating it. An n-gram is a sequence of n symbols: w1 ... w,,,. Each state in the model represents a prefix history of the n-gram (w1 ... w,,,_1), and transitions in the model represent either n-grams or backoff transitions following that history. Figure 1 lists conventions for states and transitions used to encode the n-grams as a WFST. This representation is similar to that used in other WFST-based n-gram software libraries, such as the AT&amp;T GRM library (Allauzen et al., 2005). One key difference is the implicit representation of &lt;s&gt; and &lt;/s&gt;, as opposed to encoding them as symbols in the grammar. This has the benefit of including all start and stop symbol functionality while avoiding common pitfalls that arise with explicit symbols. Another difference from the GRM library representation is explicit inclusion of failure links from states to their backoff states even in the raw count files. The OpenGrm n-gram FST format is consistent through all stages of building the models, meaning that model manipulation (e.g., merging of two Figure 1: List of state and transitio</context>
</contexts>
<marker>Allauzen, Mohri, Roark, 2005</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2005. The design principles and algorithms of a weighted grammar library. International Journal of Foundations of Computer Science, 16(3):403–421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twelfth International Conference on Implementation and Application ofAutomata (CIAA 2007), Lecture Notes in Computer Science,</booktitle>
<volume>4793</volume>
<pages>11--23</pages>
<contexts>
<context position="3347" citStr="Allauzen et al. (2007)" startWordPosition="529" endWordPosition="533">hted finite-state automaton. In the OpenFst library, there are a small number of special symbols that can be used. The c symbol represents the empty string, which allows the transition to be traversed without consuming any symbol. The 0 (or failure) symbol on a transition also allows it to be traversed without consuming any symbol, but it differs from c in only allowing traversal if the symbol being matched does not label any other transition leaving the same state, i.e., it encodes the semantics of otherwise, which is useful for language models. For a more detailed presentation of WFSTs, see Allauzen et al. (2007). 3 The Thrax Grammar Compiler The Thrax grammar compiler3 compiles grammars that consist of regular expressions, and contextdependent rewrite rules, into FST archives (fars) of weighted finite state transducers. Grammars may 3The compiler is named after Dionysius Thrax (170– 90BCE), the reputed first Greek grammarian. Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 61–66, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics be split over multiple files and imported into other grammars. Strings in the rules may</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer library. In Proceedings of the Twelfth International Conference on Implementation and Application ofAutomata (CIAA 2007), Lecture Notes in Computer Science, volume 4793, pages 11–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Michael Riley</author>
<author>Brian Roark</author>
<author>Richard Sproat</author>
</authors>
<title>MAP adaptation of stochastic grammars.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="20615" citStr="Bacchiani et al., 2006" startWordPosition="3365" endWordPosition="3368">ate. While these backoff transitions are labeled with e, see Section 4.4 for discussion of applying them as failure transitions. a/0 0 ε a/-1.1 ε 0 a/-0.69 b/-0.69 b/-1.1 ε b/0 a/0.336 1.36 ε/0.916 a/0.847 ε/0.693 0.946 a/0.56 b/0.56 b/0.847 ε/0.916 b/0.99 65 4.3 N-gram Model Merging and Pruning Two n-gram count FSTs or two model FSTs can be merged into a single FST using ngrammerge, with command line flags to allow for scaling of each of the two, and to indicate whether to carry out full normalization. This approach allows for various sorts of MAP adaptation approaches for the n-gram models (Bacchiani et al., 2006). To merge two input FST models with no scaling: ngrammerge in.mod1 in.mod2 &gt;mrg.mod N-gram model pruning is provided with three different methods: count pruning based on a threshold; the method from Seymore and Rosenfeld (1996); and relative entropy pruning of Stolcke (1998). Like smoothing, each of these pruning methods is implemented as a relatively simple derived subclass, thus allowing for straightforward extension to new and different pruning methods. To prune a smoothed ngram model: ngramshrink -theta=4 in.mod &gt;prn.mod 4.4 N-gram Utilities In addition to the above detailed core operatio</context>
</contexts>
<marker>Bacchiani, Riley, Roark, Sproat, 2006</marker>
<rawString>Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat. 2006. MAP adaptation of stochastic grammars. Computer Speech and Language, 20(1):41–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report, TR-10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="18859" citStr="Chen and Goodman (1998)" startWordPosition="3082" endWordPosition="3085">ive log counts of that symbol occurring following the history that the state represents. Hence the bigram “b b” occurs once, yielding a negative log of zero for the transition labeled with ‘b’ leaving the state representing the history ‘b’. 4.2 N-gram Model Parameter Estimation Given counts, one can build a smoothed n-gram model by normalizing and smoothing, which is accomplished with the ngrammake command line binary. The library has several available smoothing methods, including Katz (1987), absolute discounting (Ney et al., 1994), Kneser-Ney (1995) and (the default) Witten-Bell (1991). See Chen and Goodman (1998) for a detailed presentation of these smoothing methods. Each of these smoothing methods is implemented as a relatively simple derived subclass, thus allowing for straightforward extension to new and different smoothing methods. To make a smoothed n-gram model from counts: ngrammake 3g.cnt.fst &gt;3g.mod.fst Figure 2(b) shows the model built using the default Witten-Bell smoothing from the count FST in 2(a). The topology remains identical, but now the n-gram transition weights and final weights are negative log probabilities. The backoff transitions (labeled with e) have the negative log backoff </context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report, TR-10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Martin Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--331</pages>
<contexts>
<context position="12031" citStr="Kaplan and Kay (1994)" startWordPosition="1946" endWordPosition="1949">ng by default) to a. Functions lacking operators (hence only called by function name) include: ArcSort, Connect, Determinize, RmEpsilon, Minimize, Optimize, Invert, Project and Reverse. Most of these call the obvious underlying OpenFst function. One function in particular, CDRewrite is worth further discussion. This function takes a transducer and two context acceptors (and the alphabet machine), and generates a new FST that performs a context dependent rewrite everywhere in the provided contexts. The context-dependent rewrite algorithm used is that of Mohri and Sproat (1996), and 63 see also Kaplan and Kay (1994). The fourth argument (sigma star) needs to be a minimized machine. The fifth argument selects the direction of rewrite; we can either rewrite left-to-right or rightto-left or simultaneously. The sixth argument selects whether the rewrite is optional. CDRewrite[tau, lambda, rho, sigma_star, ’ltr’|’rtl’|’sim’, ’obl’|’opt’] For context-dependent rewrite rules, two built-in symbols “[BOS]” and “[EOS]” have a special meaning in the context specifications: they refer to the beginning and end of string, respectively. There are also built-in functions that perform other tasks. In the interest of spac</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Ronald M. Kaplan and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20:331–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recogniser.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="18733" citStr="Katz (1987)" startWordPosition="3065" endWordPosition="3066">e is always final.) The e transitions are backoff transitions, and the weights on each n-gram transition are negative log counts of that symbol occurring following the history that the state represents. Hence the bigram “b b” occurs once, yielding a negative log of zero for the transition labeled with ‘b’ leaving the state representing the history ‘b’. 4.2 N-gram Model Parameter Estimation Given counts, one can build a smoothed n-gram model by normalizing and smoothing, which is accomplished with the ngrammake command line binary. The library has several available smoothing methods, including Katz (1987), absolute discounting (Ney et al., 1994), Kneser-Ney (1995) and (the default) Witten-Bell (1991). See Chen and Goodman (1998) for a detailed presentation of these smoothing methods. Each of these smoothing methods is implemented as a relatively simple derived subclass, thus allowing for straightforward extension to new and different smoothing methods. To make a smoothed n-gram model from counts: ngrammake 3g.cnt.fst &gt;3g.mod.fst Figure 2(b) shows the model built using the default Witten-Bell smoothing from the count FST in 2(a). The topology remains identical, but now the n-gram transition wei</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recogniser. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<pages>181--184</pages>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Richard Sproat</author>
</authors>
<title>An efficient compiler for weighted rewrite rules.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>231--238</pages>
<contexts>
<context position="11992" citStr="Mohri and Sproat (1996)" startWordPosition="1938" endWordPosition="1941">adds the weight 3 (in the tropical semiring by default) to a. Functions lacking operators (hence only called by function name) include: ArcSort, Connect, Determinize, RmEpsilon, Minimize, Optimize, Invert, Project and Reverse. Most of these call the obvious underlying OpenFst function. One function in particular, CDRewrite is worth further discussion. This function takes a transducer and two context acceptors (and the alphabet machine), and generates a new FST that performs a context dependent rewrite everywhere in the provided contexts. The context-dependent rewrite algorithm used is that of Mohri and Sproat (1996), and 63 see also Kaplan and Kay (1994). The fourth argument (sigma star) needs to be a minimized machine. The fifth argument selects the direction of rewrite; we can either rewrite left-to-right or rightto-left or simultaneously. The sixth argument selects whether the rewrite is optional. CDRewrite[tau, lambda, rho, sigma_star, ’ltr’|’rtl’|’sim’, ’obl’|’opt’] For context-dependent rewrite rules, two built-in symbols “[BOS]” and “[EOS]” have a special meaning in the context specifications: they refer to the beginning and end of string, respectively. There are also built-in functions that perfo</context>
</contexts>
<marker>Mohri, Sproat, 1996</marker>
<rawString>Mehryar Mohri and Richard Sproat. 1996. An efficient compiler for weighted rewrite rules. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 231–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>Ute Essen</author>
<author>Reinhard Kneser</author>
</authors>
<title>On structuring probabilistic dependences in stochastic language modeling. Computer Speech and Language,</title>
<date>1994</date>
<pages>8--1</pages>
<contexts>
<context position="18774" citStr="Ney et al., 1994" startWordPosition="3070" endWordPosition="3073">ns are backoff transitions, and the weights on each n-gram transition are negative log counts of that symbol occurring following the history that the state represents. Hence the bigram “b b” occurs once, yielding a negative log of zero for the transition labeled with ‘b’ leaving the state representing the history ‘b’. 4.2 N-gram Model Parameter Estimation Given counts, one can build a smoothed n-gram model by normalizing and smoothing, which is accomplished with the ngrammake command line binary. The library has several available smoothing methods, including Katz (1987), absolute discounting (Ney et al., 1994), Kneser-Ney (1995) and (the default) Witten-Bell (1991). See Chen and Goodman (1998) for a detailed presentation of these smoothing methods. Each of these smoothing methods is implemented as a relatively simple derived subclass, thus allowing for straightforward extension to new and different smoothing methods. To make a smoothed n-gram model from counts: ngrammake 3g.cnt.fst &gt;3g.mod.fst Figure 2(b) shows the model built using the default Witten-Bell smoothing from the count FST in 2(a). The topology remains identical, but now the n-gram transition weights and final weights are negative log p</context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>Hermann Ney, Ute Essen, and Reinhard Kneser. 1994. On structuring probabilistic dependences in stochastic language modeling. Computer Speech and Language, 8:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristie Seymore</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Scalable backoff language models.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP).</booktitle>
<contexts>
<context position="20843" citStr="Seymore and Rosenfeld (1996)" startWordPosition="3401" endWordPosition="3404">.56 b/0.56 b/0.847 ε/0.916 b/0.99 65 4.3 N-gram Model Merging and Pruning Two n-gram count FSTs or two model FSTs can be merged into a single FST using ngrammerge, with command line flags to allow for scaling of each of the two, and to indicate whether to carry out full normalization. This approach allows for various sorts of MAP adaptation approaches for the n-gram models (Bacchiani et al., 2006). To merge two input FST models with no scaling: ngrammerge in.mod1 in.mod2 &gt;mrg.mod N-gram model pruning is provided with three different methods: count pruning based on a threshold; the method from Seymore and Rosenfeld (1996); and relative entropy pruning of Stolcke (1998). Like smoothing, each of these pruning methods is implemented as a relatively simple derived subclass, thus allowing for straightforward extension to new and different pruning methods. To prune a smoothed ngram model: ngramshrink -theta=4 in.mod &gt;prn.mod 4.4 N-gram Utilities In addition to the above detailed core operations on language models, the OpenGrm NGram library has a number of utilities that make building and using the models very easy. There are utilities related to input and output, including ngramsymbols, which produces a symbol table</context>
</contexts>
<marker>Seymore, Rosenfeld, 1996</marker>
<rawString>Kristie Seymore and Ronald Rosenfeld. 1996. Scalable backoff language models. In Proceedings of the International Conference on Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proc. DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<contexts>
<context position="20891" citStr="Stolcke (1998)" startWordPosition="3410" endWordPosition="3411">and Pruning Two n-gram count FSTs or two model FSTs can be merged into a single FST using ngrammerge, with command line flags to allow for scaling of each of the two, and to indicate whether to carry out full normalization. This approach allows for various sorts of MAP adaptation approaches for the n-gram models (Bacchiani et al., 2006). To merge two input FST models with no scaling: ngrammerge in.mod1 in.mod2 &gt;mrg.mod N-gram model pruning is provided with three different methods: count pruning based on a threshold; the method from Seymore and Rosenfeld (1996); and relative entropy pruning of Stolcke (1998). Like smoothing, each of these pruning methods is implemented as a relatively simple derived subclass, thus allowing for straightforward extension to new and different pruning methods. To prune a smoothed ngram model: ngramshrink -theta=4 in.mod &gt;prn.mod 4.4 N-gram Utilities In addition to the above detailed core operations on language models, the OpenGrm NGram library has a number of utilities that make building and using the models very easy. There are utilities related to input and output, including ngramsymbols, which produces a symbol table from a corpus; ngramread, which reads in textua</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In Proc. DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Timothy C Bell</author>
</authors>
<title>The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>4</issue>
<pages>1094</pages>
<marker>Witten, Bell, 1991</marker>
<rawString>Ian H. Witten and Timothy C. Bell. 1991. The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information Theory, 37(4):1085– 1094.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>