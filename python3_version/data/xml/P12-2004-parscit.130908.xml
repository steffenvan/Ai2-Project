<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000540">
<title confidence="0.998737">
A Feature-Rich Constituent Context Model for Grammar Induction
</title>
<author confidence="0.995192">
Dave Golland John DeNero Jakob Uszkoreit
</author>
<affiliation confidence="0.999724">
University of California, Berkeley Google Google
</affiliation>
<email confidence="0.992593">
dsg@cs.berkeley.edu denero@google.com uszkoreit@google.com
</email>
<sectionHeader confidence="0.997323" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99982525">
We present LLCCM, a log-linear variant of the
constituent context model (CCM) of grammar
induction. LLCCM retains the simplicity of
the original CCM but extends robustly to long
sentences. On sentences of up to length 40,
LLCCM outperforms CCM by 13.9% brack-
eting F1 and outperforms a right-branching
baseline in regimes where CCM does not.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988720967742">
Unsupervised grammar induction is a fundamental
challenge of statistical natural language processing
(Lari and Young, 1990; Pereira and Schabes, 1992;
Carroll and Charniak, 1992). The constituent con-
text model (CCM) for inducing constituency parses
(Klein and Manning, 2002) was the first unsuper-
vised approach to surpass a right-branching base-
line. However, the CCM only effectively models
short sentences. This paper shows that a simple re-
parameterization of the model, which ties together
the probabilities of related events, allows the CCM
to extend robustly to long sentences.
Much recent research has explored dependency
grammar induction. For instance, the dependency
model with valence (DMV) of Klein and Manning
(2004) has been extended to utilize multilingual in-
formation (Berg-Kirkpatrick and Klein, 2010; Co-
hen et al., 2011), lexical information (Headden III et
al., 2009), and linguistic universals (Naseem et al.,
2010). Nevertheless, simplistic dependency models
like the DMV do not contain information present in
a constituency parse, such as the attachment order of
object and subject to a verb.
Unsupervised constituency parsing is also an ac-
tive research area. Several studies (Seginer, 2007;
Reichart and Rappoport, 2010; Ponvert et al., 2011)
have considered the problem of inducing parses
over raw lexical items rather than part-of-speech
(POS) tags. Additional advances have come from
more complex models, such as combining CCM
and DMV (Klein and Manning, 2004) and model-
ing large tree fragments (Bod, 2006).
The CCM scores each parse as a product of prob-
abilities of span and context subsequences. It was
originally evaluated only on unpunctuated sentences
up to length 10 (Klein and Manning, 2002), which
account for only 15% of the WSJ corpus; our exper-
iments confirm the observation in (Klein, 2005) that
performance degrades dramatically on longer sen-
tences. This problem is unsurprising: CCM scores
each constituent type by a single, isolated multino-
mial parameter.
Our work leverages the idea that sharing infor-
mation between local probabilities in a structured
unsupervised model can lead to substantial accu-
racy gains, previously demonstrated for dependency
grammar induction (Cohen and Smith, 2009; Berg-
Kirkpatrick et al., 2010). Our model, Log-Linear
CCM (LLCCM), shares information between the
probabilities of related constituents by expressing
them as a log-linear combination of features trained
using the gradient-based learning procedure of Berg-
Kirkpatrick et al. (2010). In this way, the probabil-
ity of generating a constituent is informed by related
constituents.
Our model improves unsupervised constituency
parsing of sentences longer than 10 words. On sen-
tences of up to length 40 (96% of all sentences in
the Penn Treebank), LLCCM outperforms CCM by
13.9% (unlabeled) bracketing F1 and, unlike CCM,
outperforms a right-branching baseline on sentences
longer than 15 words.
</bodyText>
<page confidence="0.992368">
17
</page>
<note confidence="0.828316">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 17–22,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.984888" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.99584815">
The CCM is a generative model for the unsuper-
vised induction of binary constituency parses over
sequences of part-of-speech (POS) tags (Klein and
Manning, 2002). Conditioned on the constituency or
distituency of each span in the parse, CCM generates
both the complete sequence of terminals it contains
and the terminals in the surrounding context.
Formally, the CCM is a probabilistic model that
jointly generates a sentence, s, and a bracketing,
B, specifying whether each contiguous subsequence
is a constituent or not, in which case the span is
called a distituent. Each subsequence of POS tags,
or SPAN, a, occurs in a CONTEXT, 0, which is an
ordered pair of preceding and following tags. A
bracketing is a boolean matrix B, indicating which
spans (i, j) are constituents (Bij = true) and which
are distituents (Bij = false). A bracketing is con-
sidered legal if its constituents are nested and form a
binary tree T(B).
The joint distribution is given by:
</bodyText>
<equation confidence="0.9949292">
P(s, B) = PT (B) ·
ri PS (a(i, j, s)|true) PC (0(i, j, s)|true) ·
i,jET(B)
ri PS (a(i, j, s)|false) PC (0(i, j, s)|false)
i,jj�T(B)
</equation>
<bodyText confidence="0.999567428571429">
The prior over unobserved bracketings PT (B) is
fixed to be the uniform distribution over all legal
bracketings. The other distributions, PS (·) and
PC (·), are multinomials whose isolated parameters
are estimated to maximize the likelihood of a set of
observed sentences {sn} using EM (Dempster et al.,
1977).1
</bodyText>
<subsectionHeader confidence="0.998355">
2.1 The Log-Linear CCM
</subsectionHeader>
<bodyText confidence="0.9999114">
A fundamental limitation of the CCM is that it con-
tains a single isolated parameter for every span. The
number of different possible span types increases ex-
ponentially in span length, leading to data sparsity as
the sentence length increases.
</bodyText>
<footnote confidence="0.9270714">
1As mentioned in (Klein and Manning, 2002), the CCM
model is deficient because it assigns probability mass to yields
and spans that cannot consistently combine to form a valid sen-
tence. Our model does not address this issue, and hence it is
similarly deficient.
</footnote>
<bodyText confidence="0.999302411764706">
The Log-Linear CCM (LLCCM) reparameterizes
the distributions in the CCM using intuitive features
to address the limitations of CCM while retaining
its predictive power. The set of proposed features
includes a BASIC feature for each parameter of the
original CCM, enabling the LLCCM to retain the
full expressive power of the CCM. In addition, LL-
CCM contains a set of coarse features that activate
across distinct spans.
To introduce features into the CCM, we express
each of its local conditional distributions as a multi-
class logistic regression model. Each local distri-
bution, Pt(y|x) fort ∈ {SPAN, CONTEXT}, condi-
tions on label x ∈ {true, false} and generates an
event (span or context) y. We can define each lo-
cal distribution in terms of a weight vector, w, and
feature vector, fxyt, using a log-linear model:
</bodyText>
<equation confidence="0.8278285">
exp hw, fxyti
Pt(y|x) = Ey, exp * fxy,t) (1)
</equation>
<bodyText confidence="0.999984">
This technique for parameter transformation was
shown to be effective in unsupervised models for
part-of-speech induction, dependency grammar in-
duction, word alignment, and word segmentation
(Berg-Kirkpatrick et al., 2010). In our case, replac-
ing multinomials via featurized models not only im-
proves model accuracy, but also lets the model apply
effectively to a new regime of long sentences.
</bodyText>
<subsectionHeader confidence="0.998306">
2.2 Feature Templates
</subsectionHeader>
<bodyText confidence="0.999731411764706">
In the SPAN model, for each span y = [a1, ... , an]
and label x, we use the following feature templates:
BASIC: ff [y = · ∧ x = ·]
BOUNDARY: ff [a1 = · ∧ an = · ∧ x = ·]
PREFIX: ff [a1 = · ∧ x = ·]
SUFFIX: ff [an = · ∧ x = ·]
Just as the external CONTEXT is a signal of con-
stituency, so too is the internal “context.” For exam-
ple, there are many distinct noun phrases with differ-
ent spans that all begin with DT and end with NN; a
fact expressed by the BOUNDARY feature (Table 1).
In the CONTEXT model, for each context y =
[01, 02] and constituent/distituent decision x, we use
the following feature templates:
BASIC: ff [y = · ∧ x = ·]
L-CONTEXT: ff [01 = · ∧ x = ·]
R-CONTEXT: ff [02 = · ∧ x = ·]
</bodyText>
<page confidence="0.994898">
18
</page>
<bodyText confidence="0.9662255">
Consider the following example extracted from
the WSJ:
</bodyText>
<equation confidence="0.952092666666667">
S
NP-SBJ VP
DT JJ NN VBD NP-TMP
</equation>
<bodyText confidence="0.89352725">
this 5 year 6
Both spans (0, 3) and (4, 6) are constituents corre-
sponding to noun phrases whose features are shown
in Table 1:
</bodyText>
<figureCaption confidence="0.458006285714286">
Feature Name (0,3) (4, 6)
BASIC-DT-JJ-NN: 1 0
BASIC-DT-NN: 0 1
BOUNDARY-DT-NN: 1 1
PREFIX-DT: 1 1
SUFFIX-NN: 1 1
BASIC-O-VBD: 1 0
BASIC-VBD-O: 0 1
L-CONTEXT-O: 1 0
L-CONTEXT-VBD: 0 1
R-CONTEXT-VBD: 1 0
R-CONTEXT-O: 0 1
Table 1: Span and context features for constituent spans (0, 3)
and (4, 6). The symbol o indicates a sentence boundary.
</figureCaption>
<bodyText confidence="0.99994775">
Notice that although the BASIC span features are
active for at most one span, the remaining features
fire for both spans, effectively sharing information
between the local probabilities of these events.
The coarser CONTEXT features factor the context
pair into its components, which allow the LLCCM
to more easily learn, for example, that a constituent
is unlikely to immediately follow a determiner.
</bodyText>
<sectionHeader confidence="0.99767" genericHeader="method">
3 Training
</sectionHeader>
<bodyText confidence="0.999923625">
In the EM algorithm for estimating CCM parame-
ters, the E-Step computes posteriors over bracket-
ings using the Inside-Outside algorithm. The M-
Step chooses parameters that maximize the expected
complete log likelihood of the data.
The weights, w, of LLCCM are estimated to max-
imize the data log likelihood of the training sen-
tences {sn}, summing out all possible bracketings
</bodyText>
<equation confidence="0.86715125">
B for each sentence:
X
L(w) =
sn
</equation>
<bodyText confidence="0.9999025">
We optimize this objective via L-BFGS (Liu and
Nocedal, 1989), which requires us to compute the
objective gradient. Berg-Kirkpatrick et al. (2010)
showed that the data log likelihood gradient is equiv-
alent to the gradient of the expected complete log
likelihood (the objective maximized in the M-step of
EM) at the point from which expectations are com-
puted. This gradient can be computed in three steps.
First, we compute the local probabilities of the
CCM, Pt(y|x), from the current w using Equa-
tion (1). We approximate the normalization over an
exponential number of terms by only summing over
spans that appeared in the training corpus.
Second, we compute posteriors over bracketings,
P(i, j|sn), just as in the E-step of CCM training,2 in
order to determine the expected counts:
</bodyText>
<equation confidence="0.98588675">
Xexy,SPAN = X � [α(i, j, sn) = y] δ(x)
sn ij
Xexy,CONTEXT = X � [β(i, j,sn) = y] δ(x)
sn ij
</equation>
<bodyText confidence="0.99744">
where δ(true) = P(i,j|sn), and δ(false) = 1 −
δ(true).
We summarize these expected count quantities as:
</bodyText>
<equation confidence="0.990259333333333">
(
exy,SPAN if t = SPAN
exyt = exy,CONTEXT if t = CONTEXT
</equation>
<bodyText confidence="0.995949">
Finally, we compute the gradient with respect to
w, expressed in terms of these expected counts and
conditional probabilities:
</bodyText>
<equation confidence="0.9749076">
VL(w) = X exytfxyt − G(w)
xyt
! X
exyt Pt(y|x)fxy&apos;t
y&apos;
</equation>
<bodyText confidence="0.9999012">
Following (Klein and Manning, 2002), we initialize
the model weights by optimizing against posterior
probabilities fixed to the split-uniform distribution,
which generates binary trees by randomly choosing
a split point and recursing on each side of the split.3
</bodyText>
<footnote confidence="0.933003">
2We follow the dynamic program presented in Appendix A.1
of (Klein, 2005).
3In Appendix B.2, Klein (2005) shows this posterior can be
expressed in closed form. As in previous work, we start the ini-
tialization optimization with the zero vector, and terminate after
10 iterations to regularize against achieving a local maximum.
</footnote>
<figure confidence="0.995839454545455">
DT
NN
0 The 1
Venezuelan 2
plummeted 4
currency 3
span
context
X Pw(sn, B)
log
B
</figure>
<equation confidence="0.54345">
G(w) = X X
xt y
</equation>
<page confidence="0.982956">
19
</page>
<subsectionHeader confidence="0.907572">
3.1 Efficiently Computing the Gradient
</subsectionHeader>
<bodyText confidence="0.978865">
The following quantity appears in G(w):
</bodyText>
<equation confidence="0.9919478">
X�t(x) = exyt
y
Which expands as follows depending on t:
X�CONTEXT(x) = X X ff [O(i, j, sn) = y] S(x)
y sn ij
</equation>
<bodyText confidence="0.9982455">
In each of these expressions, the S(x) term can
be factored outside the sum over y. Each fixed
(i, j) and sn pair has exactly one span and con-
text, hence the quantities Py ff [a(i, j, sn) = y] and
</bodyText>
<equation confidence="0.9716785">
P
y ff [O(i, j, sn) = y] are both equal to 1.
X�t(x) =
sn
</equation>
<bodyText confidence="0.998043">
This expression further simplifies to a constant.
The sum of the posterior probabilities, S(true), over
all positions is equal to the total number of con-
stituents in the tree. Any binary tree over N ter-
minals contains exactly 2N − 1 constituents and
</bodyText>
<equation confidence="0.9983588">
�(N − 2)(N − 1) distituents.
1
(P
sn (2|sn |− 1) if x = true
7t(x) = 2 Psn(|sn |− 2)(|sn |− 1) if x = false
</equation>
<bodyText confidence="0.99938925">
where |sn |denotes the length of sentence sn.
Thus, G(w) can be precomputed once for the en-
tire dataset at each minimization step. Moreover,
-yt(x) can be precomputed once before all iterations.
</bodyText>
<subsectionHeader confidence="0.993055">
3.2 Relationship to Smoothing
</subsectionHeader>
<bodyText confidence="0.999836846153846">
The original CCM uses additive smoothing in its M-
step to capture the fact that distituents outnumber
constituents. For each span or context, CCM adds
10 counts: 2 as a constituent and 8 as a distituent.4
We note that these smoothing parameters are tai-
lored to short sentences: in a binary tree, the number
of constituents grows linearly with sentence length,
whereas the number of distituents grows quadrati-
cally. Therefore, the ratio of constituents to dis-
tituents is not constant across sentence lengths. In
contrast, by virtue of the log-linear model, LLCCM
assigns positive probability to all spans or contexts
without explicit smoothing.
</bodyText>
<footnote confidence="0.637021">
4These counts are specified in (Klein, 2005); Klein and
Manning (2002) added 10 constituent and 50 distituent counts.
</footnote>
<figure confidence="0.6593155">
100
Maximum sentence length
</figure>
<figureCaption confidence="0.9390135">
Figure 1: CCM and LLCCM trained and tested on sentences of
a fixed length. LLCCM performs well on longer sentences. The
binary branching upper bound correponds to UBOUND from
(Klein and Manning, 2002).
</figureCaption>
<sectionHeader confidence="0.998909" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999963769230769">
We train our models on gold POS sequences from
all sections (0-24) of the WSJ (Marcus et al., 1993)
with punctuation removed. We report bracketing
F1 scores between the binary trees predicted by the
models on these sequences and the treebank parses.
We train and evaluate both a CCM implementa-
tion (Luque, 2011) and our LLCCM on sentences up
to a fixed length n, for n E 110,15, ... , 40}. Fig-
ure 1 shows that LLCCM substantially outperforms
the CCM on longer sentences. After length 15,
CCM accuracy falls below the right branching base-
line, whereas LLCCM remains significantly better
than right-branching through length 40.
</bodyText>
<sectionHeader confidence="0.995189" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999639333333333">
Our log-linear variant of the CCM extends robustly
to long sentences, enabling constituent grammar in-
duction to be used in settings that typically include
long sentences, such as machine translation reorder-
ing (Chiang, 2005; DeNero and Uszkoreit, 2011;
Dyer et al., 2011).
</bodyText>
<sectionHeader confidence="0.991724" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9655868">
We thank Taylor Berg-Kirkpatrick and Dan Klein
for helpful discussions regarding the work on which
this paper is based. This work was partially sup-
ported by the National Science Foundation through
a Graduate Research Fellowship to the first author.
</bodyText>
<equation confidence="0.598104">
X�SPAN(x) = X
y sn
X ff [a(i, j, sn) = y] S(x)
</equation>
<figure confidence="0.98341447826087">
ij
Binary branching upper bound
64.6
60.0 56.2
50.3 49.2 47.6
53.0
46.6 42.7
39.9 37.5
33.7
Log-linear CCM
Standard CCM
Right branching
10 15 20 25 30 35 40
0
72.0
71.9
75
50
Bracketing F1
25
X
ij
S(x)
</figure>
<page confidence="0.889184">
20
</page>
<sectionHeader confidence="0.964298" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999330875">
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1288–1297, Uppsala, Sweden, July.
Association for Computational Linguistics.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero, and Dan Klein. 2010. Painless unsu-
pervised learning with features. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 582–590, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Rens Bod. 2006. Unsupervised parsing with U-DOP.
In Proceedings of the Conference on Computational
Natural Language Learning.
Glenn Carroll and Eugene Charniak. 1992. Two experi-
ments on learning probabilistic dependency grammars
from corpora. In Workshop Notes for Statistically-
Based NLP Techniques, AAAI, pages 1–13.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263–270, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
Shay B. Cohen and Noah A. Smith. 2009. Shared logis-
tic normal distributions for soft parameter tying in un-
supervised grammar induction. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 74–82,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 50–61, Edinburgh, Scotland,
UK., July. Association for Computational Linguistics.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1–38.
John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 193–
203, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English translation system. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
337–343, Edinburgh, Scotland, July. Association for
Computational Linguistics.
William P. Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
101–109, Boulder, Colorado, June. Association for
Computational Linguistics.
Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar
induction. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
128–135, Philadelphia, Pennsylvania, USA, July. As-
sociation for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics, Main Volume, pages 478–485, Barcelona,
Spain, July.
Dan Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis.
Karim Lari and Steve J. Young. 1990. The estimation
of stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4:35–56.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory method for large scale optimization. Mathe-
matical Programming B, 45(3):503–528.
Franco Luque. 2011. Una implementaci´on del mod-
elo DMV+CCM para parsing no supervisado. In 2do
Workshop Argentino en Procesamiento de Lenguaje
Natural.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Tahira Naseem and Regina Barzilay. 2011. Using se-
mantic cues to learn syntax. In AAAI.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1234–1244, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
</reference>
<page confidence="0.983548">
21
</page>
<reference confidence="0.998833045454545">
In Proceedings of the 30th Annual Meeting of the As-
sociation for Computational Linguistics, pages 128–
135, Newark, Delaware, USA, June. Association for
Computational Linguistics.
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011.
Simple unsupervised grammar induction from raw text
with cascaded finite state models. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1077–1086, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Roi Reichart and Ari Rappoport. 2010. Improved fully
unsupervised parsing with zoomed learning. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 684–693,
Cambridge, MA, October. Association for Computa-
tional Linguistics.
Yoav Seginer. 2007. Fast unsupervised incremental pars-
ing. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 384–
391, Prague, Czech Republic, June. Association for
Computational Linguistics.
</reference>
<page confidence="0.999029">
22
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968863">
<title confidence="0.99995">A Feature-Rich Constituent Context Model for Grammar Induction</title>
<author confidence="0.999865">Dave Golland John DeNero Jakob Uszkoreit</author>
<affiliation confidence="0.989581">University of California, Berkeley Google</affiliation>
<email confidence="0.994463">dsg@cs.berkeley.edudenero@google.comuszkoreit@google.com</email>
<abstract confidence="0.998260444444445">We present LLCCM, a log-linear variant of the constituent context model (CCM) of grammar induction. LLCCM retains the simplicity of the original CCM but extends robustly to long sentences. On sentences of up to length 40, LLCCM outperforms CCM by 13.9% bracketing F1 and outperforms a right-branching baseline in regimes where CCM does not.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Phylogenetic grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1288--1297</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1392" citStr="Berg-Kirkpatrick and Klein, 2010" startWordPosition="196" endWordPosition="199"> constituent context model (CCM) for inducing constituency parses (Klein and Manning, 2002) was the first unsupervised approach to surpass a right-branching baseline. However, the CCM only effectively models short sentences. This paper shows that a simple reparameterization of the model, which ties together the probabilities of related events, allows the CCM to extend robustly to long sentences. Much recent research has explored dependency grammar induction. For instance, the dependency model with valence (DMV) of Klein and Manning (2004) has been extended to utilize multilingual information (Berg-Kirkpatrick and Klein, 2010; Cohen et al., 2011), lexical information (Headden III et al., 2009), and linguistic universals (Naseem et al., 2010). Nevertheless, simplistic dependency models like the DMV do not contain information present in a constituency parse, such as the attachment order of object and subject to a verb. Unsupervised constituency parsing is also an active research area. Several studies (Seginer, 2007; Reichart and Rappoport, 2010; Ponvert et al., 2011) have considered the problem of inducing parses over raw lexical items rather than part-of-speech (POS) tags. Additional advances have come from more co</context>
</contexts>
<marker>Berg-Kirkpatrick, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylogenetic grammar induction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1288–1297, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>582--590</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 582–590, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Unsupervised parsing with U-DOP.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="2107" citStr="Bod, 2006" startWordPosition="309" endWordPosition="310">et al., 2010). Nevertheless, simplistic dependency models like the DMV do not contain information present in a constituency parse, such as the attachment order of object and subject to a verb. Unsupervised constituency parsing is also an active research area. Several studies (Seginer, 2007; Reichart and Rappoport, 2010; Ponvert et al., 2011) have considered the problem of inducing parses over raw lexical items rather than part-of-speech (POS) tags. Additional advances have come from more complex models, such as combining CCM and DMV (Klein and Manning, 2004) and modeling large tree fragments (Bod, 2006). The CCM scores each parse as a product of probabilities of span and context subsequences. It was originally evaluated only on unpunctuated sentences up to length 10 (Klein and Manning, 2002), which account for only 15% of the WSJ corpus; our experiments confirm the observation in (Klein, 2005) that performance degrades dramatically on longer sentences. This problem is unsurprising: CCM scores each constituent type by a single, isolated multinomial parameter. Our work leverages the idea that sharing information between local probabilities in a structured unsupervised model can lead to substan</context>
</contexts>
<marker>Bod, 2006</marker>
<rawString>Rens Bod. 2006. Unsupervised parsing with U-DOP. In Proceedings of the Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Eugene Charniak</author>
</authors>
<title>Two experiments on learning probabilistic dependency grammars from corpora.</title>
<date>1992</date>
<booktitle>In Workshop Notes for StatisticallyBased NLP Techniques, AAAI,</booktitle>
<pages>1--13</pages>
<contexts>
<context position="755" citStr="Carroll and Charniak, 1992" startWordPosition="101" endWordPosition="104">, Berkeley Google Google dsg@cs.berkeley.edu denero@google.com uszkoreit@google.com Abstract We present LLCCM, a log-linear variant of the constituent context model (CCM) of grammar induction. LLCCM retains the simplicity of the original CCM but extends robustly to long sentences. On sentences of up to length 40, LLCCM outperforms CCM by 13.9% bracketing F1 and outperforms a right-branching baseline in regimes where CCM does not. 1 Introduction Unsupervised grammar induction is a fundamental challenge of statistical natural language processing (Lari and Young, 1990; Pereira and Schabes, 1992; Carroll and Charniak, 1992). The constituent context model (CCM) for inducing constituency parses (Klein and Manning, 2002) was the first unsupervised approach to surpass a right-branching baseline. However, the CCM only effectively models short sentences. This paper shows that a simple reparameterization of the model, which ties together the probabilities of related events, allows the CCM to extend robustly to long sentences. Much recent research has explored dependency grammar induction. For instance, the dependency model with valence (DMV) of Klein and Manning (2004) has been extended to utilize multilingual informat</context>
</contexts>
<marker>Carroll, Charniak, 1992</marker>
<rawString>Glenn Carroll and Eugene Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. In Workshop Notes for StatisticallyBased NLP Techniques, AAAI, pages 1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 263–270, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>74--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="2807" citStr="Cohen and Smith, 2009" startWordPosition="415" endWordPosition="418">ubsequences. It was originally evaluated only on unpunctuated sentences up to length 10 (Klein and Manning, 2002), which account for only 15% of the WSJ corpus; our experiments confirm the observation in (Klein, 2005) that performance degrades dramatically on longer sentences. This problem is unsurprising: CCM scores each constituent type by a single, isolated multinomial parameter. Our work leverages the idea that sharing information between local probabilities in a structured unsupervised model can lead to substantial accuracy gains, previously demonstrated for dependency grammar induction (Cohen and Smith, 2009; BergKirkpatrick et al., 2010). Our model, Log-Linear CCM (LLCCM), shares information between the probabilities of related constituents by expressing them as a log-linear combination of features trained using the gradient-based learning procedure of BergKirkpatrick et al. (2010). In this way, the probability of generating a constituent is informed by related constituents. Our model improves unsupervised constituency parsing of sentences longer than 10 words. On sentences of up to length 40 (96% of all sentences in the Penn Treebank), LLCCM outperforms CCM by 13.9% (unlabeled) bracketing F1 an</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 74–82, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Unsupervised structure prediction with non-parallel multilingual guidance.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>50--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="1413" citStr="Cohen et al., 2011" startWordPosition="200" endWordPosition="204">or inducing constituency parses (Klein and Manning, 2002) was the first unsupervised approach to surpass a right-branching baseline. However, the CCM only effectively models short sentences. This paper shows that a simple reparameterization of the model, which ties together the probabilities of related events, allows the CCM to extend robustly to long sentences. Much recent research has explored dependency grammar induction. For instance, the dependency model with valence (DMV) of Klein and Manning (2004) has been extended to utilize multilingual information (Berg-Kirkpatrick and Klein, 2010; Cohen et al., 2011), lexical information (Headden III et al., 2009), and linguistic universals (Naseem et al., 2010). Nevertheless, simplistic dependency models like the DMV do not contain information present in a constituency parse, such as the attachment order of object and subject to a verb. Unsupervised constituency parsing is also an active research area. Several studies (Seginer, 2007; Reichart and Rappoport, 2010; Ponvert et al., 2011) have considered the problem of inducing parses over raw lexical items rather than part-of-speech (POS) tags. Additional advances have come from more complex models, such as</context>
</contexts>
<marker>Cohen, Das, Smith, 2011</marker>
<rawString>Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011. Unsupervised structure prediction with non-parallel multilingual guidance. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Dempster</author>
<author>Nan Laird</author>
<author>Donald Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="5099" citStr="Dempster et al., 1977" startWordPosition="784" endWordPosition="787">ij = true) and which are distituents (Bij = false). A bracketing is considered legal if its constituents are nested and form a binary tree T(B). The joint distribution is given by: P(s, B) = PT (B) · ri PS (a(i, j, s)|true) PC (0(i, j, s)|true) · i,jET(B) ri PS (a(i, j, s)|false) PC (0(i, j, s)|false) i,jj�T(B) The prior over unobserved bracketings PT (B) is fixed to be the uniform distribution over all legal bracketings. The other distributions, PS (·) and PC (·), are multinomials whose isolated parameters are estimated to maximize the likelihood of a set of observed sentences {sn} using EM (Dempster et al., 1977).1 2.1 The Log-Linear CCM A fundamental limitation of the CCM is that it contains a single isolated parameter for every span. The number of different possible span types increases exponentially in span length, leading to data sparsity as the sentence length increases. 1As mentioned in (Klein and Manning, 2002), the CCM model is deficient because it assigns probability mass to yields and spans that cannot consistently combine to form a valid sentence. Our model does not address this issue, and hence it is similarly deficient. The Log-Linear CCM (LLCCM) reparameterizes the distributions in the C</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur Dempster, Nan Laird, and Donald Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Inducing sentence structure from parallel corpora for reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>193--203</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<marker>DeNero, Uszkoreit, 2011</marker>
<rawString>John DeNero and Jakob Uszkoreit. 2011. Inducing sentence structure from parallel corpora for reordering. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 193– 203, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Jonathan H Clark</author>
<author>Noah A Smith</author>
</authors>
<title>The CMU-ARK GermanEnglish translation system.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>337--343</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Dyer, Gimpel, Clark, Smith, 2011</marker>
<rawString>Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and Noah A. Smith. 2011. The CMU-ARK GermanEnglish translation system. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 337–343, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William P Headden Mark Johnson</author>
<author>David McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>101--109</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<marker>Johnson, McClosky, 2009</marker>
<rawString>William P. Headden III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 101–109, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A generative constituent-context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="851" citStr="Klein and Manning, 2002" startWordPosition="115" endWordPosition="118">sent LLCCM, a log-linear variant of the constituent context model (CCM) of grammar induction. LLCCM retains the simplicity of the original CCM but extends robustly to long sentences. On sentences of up to length 40, LLCCM outperforms CCM by 13.9% bracketing F1 and outperforms a right-branching baseline in regimes where CCM does not. 1 Introduction Unsupervised grammar induction is a fundamental challenge of statistical natural language processing (Lari and Young, 1990; Pereira and Schabes, 1992; Carroll and Charniak, 1992). The constituent context model (CCM) for inducing constituency parses (Klein and Manning, 2002) was the first unsupervised approach to surpass a right-branching baseline. However, the CCM only effectively models short sentences. This paper shows that a simple reparameterization of the model, which ties together the probabilities of related events, allows the CCM to extend robustly to long sentences. Much recent research has explored dependency grammar induction. For instance, the dependency model with valence (DMV) of Klein and Manning (2004) has been extended to utilize multilingual information (Berg-Kirkpatrick and Klein, 2010; Cohen et al., 2011), lexical information (Headden III et </context>
<context position="2299" citStr="Klein and Manning, 2002" startWordPosition="339" endWordPosition="342">ect to a verb. Unsupervised constituency parsing is also an active research area. Several studies (Seginer, 2007; Reichart and Rappoport, 2010; Ponvert et al., 2011) have considered the problem of inducing parses over raw lexical items rather than part-of-speech (POS) tags. Additional advances have come from more complex models, such as combining CCM and DMV (Klein and Manning, 2004) and modeling large tree fragments (Bod, 2006). The CCM scores each parse as a product of probabilities of span and context subsequences. It was originally evaluated only on unpunctuated sentences up to length 10 (Klein and Manning, 2002), which account for only 15% of the WSJ corpus; our experiments confirm the observation in (Klein, 2005) that performance degrades dramatically on longer sentences. This problem is unsurprising: CCM scores each constituent type by a single, isolated multinomial parameter. Our work leverages the idea that sharing information between local probabilities in a structured unsupervised model can lead to substantial accuracy gains, previously demonstrated for dependency grammar induction (Cohen and Smith, 2009; BergKirkpatrick et al., 2010). Our model, Log-Linear CCM (LLCCM), shares information betwe</context>
<context position="3859" citStr="Klein and Manning, 2002" startWordPosition="572" endWordPosition="575"> of sentences longer than 10 words. On sentences of up to length 40 (96% of all sentences in the Penn Treebank), LLCCM outperforms CCM by 13.9% (unlabeled) bracketing F1 and, unlike CCM, outperforms a right-branching baseline on sentences longer than 15 words. 17 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 17–22, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 2 Model The CCM is a generative model for the unsupervised induction of binary constituency parses over sequences of part-of-speech (POS) tags (Klein and Manning, 2002). Conditioned on the constituency or distituency of each span in the parse, CCM generates both the complete sequence of terminals it contains and the terminals in the surrounding context. Formally, the CCM is a probabilistic model that jointly generates a sentence, s, and a bracketing, B, specifying whether each contiguous subsequence is a constituent or not, in which case the span is called a distituent. Each subsequence of POS tags, or SPAN, a, occurs in a CONTEXT, 0, which is an ordered pair of preceding and following tags. A bracketing is a boolean matrix B, indicating which spans (i, j) a</context>
<context position="5410" citStr="Klein and Manning, 2002" startWordPosition="835" endWordPosition="838">(B) The prior over unobserved bracketings PT (B) is fixed to be the uniform distribution over all legal bracketings. The other distributions, PS (·) and PC (·), are multinomials whose isolated parameters are estimated to maximize the likelihood of a set of observed sentences {sn} using EM (Dempster et al., 1977).1 2.1 The Log-Linear CCM A fundamental limitation of the CCM is that it contains a single isolated parameter for every span. The number of different possible span types increases exponentially in span length, leading to data sparsity as the sentence length increases. 1As mentioned in (Klein and Manning, 2002), the CCM model is deficient because it assigns probability mass to yields and spans that cannot consistently combine to form a valid sentence. Our model does not address this issue, and hence it is similarly deficient. The Log-Linear CCM (LLCCM) reparameterizes the distributions in the CCM using intuitive features to address the limitations of CCM while retaining its predictive power. The set of proposed features includes a BASIC feature for each parameter of the original CCM, enabling the LLCCM to retain the full expressive power of the CCM. In addition, LLCCM contains a set of coarse featur</context>
<context position="10232" citStr="Klein and Manning, 2002" startWordPosition="1686" endWordPosition="1689">ng corpus. Second, we compute posteriors over bracketings, P(i, j|sn), just as in the E-step of CCM training,2 in order to determine the expected counts: Xexy,SPAN = X � [α(i, j, sn) = y] δ(x) sn ij Xexy,CONTEXT = X � [β(i, j,sn) = y] δ(x) sn ij where δ(true) = P(i,j|sn), and δ(false) = 1 − δ(true). We summarize these expected count quantities as: ( exy,SPAN if t = SPAN exyt = exy,CONTEXT if t = CONTEXT Finally, we compute the gradient with respect to w, expressed in terms of these expected counts and conditional probabilities: VL(w) = X exytfxyt − G(w) xyt ! X exyt Pt(y|x)fxy&apos;t y&apos; Following (Klein and Manning, 2002), we initialize the model weights by optimizing against posterior probabilities fixed to the split-uniform distribution, which generates binary trees by randomly choosing a split point and recursing on each side of the split.3 2We follow the dynamic program presented in Appendix A.1 of (Klein, 2005). 3In Appendix B.2, Klein (2005) shows this posterior can be expressed in closed form. As in previous work, we start the initialization optimization with the zero vector, and terminate after 10 iterations to regularize against achieving a local maximum. DT NN 0 The 1 Venezuelan 2 plummeted 4 currenc</context>
<context position="12626" citStr="Klein and Manning (2002)" startWordPosition="2110" endWordPosition="2113">s outnumber constituents. For each span or context, CCM adds 10 counts: 2 as a constituent and 8 as a distituent.4 We note that these smoothing parameters are tailored to short sentences: in a binary tree, the number of constituents grows linearly with sentence length, whereas the number of distituents grows quadratically. Therefore, the ratio of constituents to distituents is not constant across sentence lengths. In contrast, by virtue of the log-linear model, LLCCM assigns positive probability to all spans or contexts without explicit smoothing. 4These counts are specified in (Klein, 2005); Klein and Manning (2002) added 10 constituent and 50 distituent counts. 100 Maximum sentence length Figure 1: CCM and LLCCM trained and tested on sentences of a fixed length. LLCCM performs well on longer sentences. The binary branching upper bound correponds to UBOUND from (Klein and Manning, 2002). 4 Experiments We train our models on gold POS sequences from all sections (0-24) of the WSJ (Marcus et al., 1993) with punctuation removed. We report bracketing F1 scores between the binary trees predicted by the models on these sequences and the treebank parses. We train and evaluate both a CCM implementation (Luque, 20</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. A generative constituent-context model for improved grammar induction. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 128–135, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Main Volume,</booktitle>
<pages>478--485</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1304" citStr="Klein and Manning (2004)" startWordPosition="184" endWordPosition="187">ari and Young, 1990; Pereira and Schabes, 1992; Carroll and Charniak, 1992). The constituent context model (CCM) for inducing constituency parses (Klein and Manning, 2002) was the first unsupervised approach to surpass a right-branching baseline. However, the CCM only effectively models short sentences. This paper shows that a simple reparameterization of the model, which ties together the probabilities of related events, allows the CCM to extend robustly to long sentences. Much recent research has explored dependency grammar induction. For instance, the dependency model with valence (DMV) of Klein and Manning (2004) has been extended to utilize multilingual information (Berg-Kirkpatrick and Klein, 2010; Cohen et al., 2011), lexical information (Headden III et al., 2009), and linguistic universals (Naseem et al., 2010). Nevertheless, simplistic dependency models like the DMV do not contain information present in a constituency parse, such as the attachment order of object and subject to a verb. Unsupervised constituency parsing is also an active research area. Several studies (Seginer, 2007; Reichart and Rappoport, 2010; Ponvert et al., 2011) have considered the problem of inducing parses over raw lexical</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Main Volume, pages 478–485, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
</authors>
<title>The Unsupervised Learning of Natural Language Structure.</title>
<date>2005</date>
<tech>Ph.D. thesis.</tech>
<contexts>
<context position="2403" citStr="Klein, 2005" startWordPosition="359" endWordPosition="360">ichart and Rappoport, 2010; Ponvert et al., 2011) have considered the problem of inducing parses over raw lexical items rather than part-of-speech (POS) tags. Additional advances have come from more complex models, such as combining CCM and DMV (Klein and Manning, 2004) and modeling large tree fragments (Bod, 2006). The CCM scores each parse as a product of probabilities of span and context subsequences. It was originally evaluated only on unpunctuated sentences up to length 10 (Klein and Manning, 2002), which account for only 15% of the WSJ corpus; our experiments confirm the observation in (Klein, 2005) that performance degrades dramatically on longer sentences. This problem is unsurprising: CCM scores each constituent type by a single, isolated multinomial parameter. Our work leverages the idea that sharing information between local probabilities in a structured unsupervised model can lead to substantial accuracy gains, previously demonstrated for dependency grammar induction (Cohen and Smith, 2009; BergKirkpatrick et al., 2010). Our model, Log-Linear CCM (LLCCM), shares information between the probabilities of related constituents by expressing them as a log-linear combination of features </context>
<context position="10532" citStr="Klein, 2005" startWordPosition="1733" endWordPosition="1734">e these expected count quantities as: ( exy,SPAN if t = SPAN exyt = exy,CONTEXT if t = CONTEXT Finally, we compute the gradient with respect to w, expressed in terms of these expected counts and conditional probabilities: VL(w) = X exytfxyt − G(w) xyt ! X exyt Pt(y|x)fxy&apos;t y&apos; Following (Klein and Manning, 2002), we initialize the model weights by optimizing against posterior probabilities fixed to the split-uniform distribution, which generates binary trees by randomly choosing a split point and recursing on each side of the split.3 2We follow the dynamic program presented in Appendix A.1 of (Klein, 2005). 3In Appendix B.2, Klein (2005) shows this posterior can be expressed in closed form. As in previous work, we start the initialization optimization with the zero vector, and terminate after 10 iterations to regularize against achieving a local maximum. DT NN 0 The 1 Venezuelan 2 plummeted 4 currency 3 span context X Pw(sn, B) log B G(w) = X X xt y 19 3.1 Efficiently Computing the Gradient The following quantity appears in G(w): X�t(x) = exyt y Which expands as follows depending on t: X�CONTEXT(x) = X X ff [O(i, j, sn) = y] S(x) y sn ij In each of these expressions, the S(x) term can be factor</context>
<context position="12600" citStr="Klein, 2005" startWordPosition="2108" endWordPosition="2109">hat distituents outnumber constituents. For each span or context, CCM adds 10 counts: 2 as a constituent and 8 as a distituent.4 We note that these smoothing parameters are tailored to short sentences: in a binary tree, the number of constituents grows linearly with sentence length, whereas the number of distituents grows quadratically. Therefore, the ratio of constituents to distituents is not constant across sentence lengths. In contrast, by virtue of the log-linear model, LLCCM assigns positive probability to all spans or contexts without explicit smoothing. 4These counts are specified in (Klein, 2005); Klein and Manning (2002) added 10 constituent and 50 distituent counts. 100 Maximum sentence length Figure 1: CCM and LLCCM trained and tested on sentences of a fixed length. LLCCM performs well on longer sentences. The binary branching upper bound correponds to UBOUND from (Klein and Manning, 2002). 4 Experiments We train our models on gold POS sequences from all sections (0-24) of the WSJ (Marcus et al., 1993) with punctuation removed. We report bracketing F1 scores between the binary trees predicted by the models on these sequences and the treebank parses. We train and evaluate both a CCM</context>
</contexts>
<marker>Klein, 2005</marker>
<rawString>Dan Klein. 2005. The Unsupervised Learning of Natural Language Structure. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karim Lari</author>
<author>Steve J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the insideoutside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="699" citStr="Lari and Young, 1990" startWordPosition="93" endWordPosition="96">n DeNero Jakob Uszkoreit University of California, Berkeley Google Google dsg@cs.berkeley.edu denero@google.com uszkoreit@google.com Abstract We present LLCCM, a log-linear variant of the constituent context model (CCM) of grammar induction. LLCCM retains the simplicity of the original CCM but extends robustly to long sentences. On sentences of up to length 40, LLCCM outperforms CCM by 13.9% bracketing F1 and outperforms a right-branching baseline in regimes where CCM does not. 1 Introduction Unsupervised grammar induction is a fundamental challenge of statistical natural language processing (Lari and Young, 1990; Pereira and Schabes, 1992; Carroll and Charniak, 1992). The constituent context model (CCM) for inducing constituency parses (Klein and Manning, 2002) was the first unsupervised approach to surpass a right-branching baseline. However, the CCM only effectively models short sentences. This paper shows that a simple reparameterization of the model, which ties together the probabilities of related events, allows the CCM to extend robustly to long sentences. Much recent research has explored dependency grammar induction. For instance, the dependency model with valence (DMV) of Klein and Manning (</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>Karim Lari and Steve J. Young. 1990. The estimation of stochastic context-free grammars using the insideoutside algorithm. Computer Speech and Language, 4:35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical Programming B,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="9039" citStr="Liu and Nocedal, 1989" startWordPosition="1479" endWordPosition="1482">into its components, which allow the LLCCM to more easily learn, for example, that a constituent is unlikely to immediately follow a determiner. 3 Training In the EM algorithm for estimating CCM parameters, the E-Step computes posteriors over bracketings using the Inside-Outside algorithm. The MStep chooses parameters that maximize the expected complete log likelihood of the data. The weights, w, of LLCCM are estimated to maximize the data log likelihood of the training sentences {sn}, summing out all possible bracketings B for each sentence: X L(w) = sn We optimize this objective via L-BFGS (Liu and Nocedal, 1989), which requires us to compute the objective gradient. Berg-Kirkpatrick et al. (2010) showed that the data log likelihood gradient is equivalent to the gradient of the expected complete log likelihood (the objective maximized in the M-step of EM) at the point from which expectations are computed. This gradient can be computed in three steps. First, we compute the local probabilities of the CCM, Pt(y|x), from the current w using Equation (1). We approximate the normalization over an exponential number of terms by only summing over spans that appeared in the training corpus. Second, we compute p</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory method for large scale optimization. Mathematical Programming B, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franco Luque</author>
</authors>
<title>Una implementaci´on del modelo DMV+CCM para parsing no supervisado.</title>
<date>2011</date>
<booktitle>In 2do Workshop Argentino en Procesamiento de Lenguaje Natural.</booktitle>
<contexts>
<context position="13229" citStr="Luque, 2011" startWordPosition="2211" endWordPosition="2212">ng (2002) added 10 constituent and 50 distituent counts. 100 Maximum sentence length Figure 1: CCM and LLCCM trained and tested on sentences of a fixed length. LLCCM performs well on longer sentences. The binary branching upper bound correponds to UBOUND from (Klein and Manning, 2002). 4 Experiments We train our models on gold POS sequences from all sections (0-24) of the WSJ (Marcus et al., 1993) with punctuation removed. We report bracketing F1 scores between the binary trees predicted by the models on these sequences and the treebank parses. We train and evaluate both a CCM implementation (Luque, 2011) and our LLCCM on sentences up to a fixed length n, for n E 110,15, ... , 40}. Figure 1 shows that LLCCM substantially outperforms the CCM on longer sentences. After length 15, CCM accuracy falls below the right branching baseline, whereas LLCCM remains significantly better than right-branching through length 40. 5 Conclusion Our log-linear variant of the CCM extends robustly to long sentences, enabling constituent grammar induction to be used in settings that typically include long sentences, such as machine translation reordering (Chiang, 2005; DeNero and Uszkoreit, 2011; Dyer et al., 2011).</context>
</contexts>
<marker>Luque, 2011</marker>
<rawString>Franco Luque. 2011. Una implementaci´on del modelo DMV+CCM para parsing no supervisado. In 2do Workshop Argentino en Procesamiento de Lenguaje Natural.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary A Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="13017" citStr="Marcus et al., 1993" startWordPosition="2175" endWordPosition="2178">ross sentence lengths. In contrast, by virtue of the log-linear model, LLCCM assigns positive probability to all spans or contexts without explicit smoothing. 4These counts are specified in (Klein, 2005); Klein and Manning (2002) added 10 constituent and 50 distituent counts. 100 Maximum sentence length Figure 1: CCM and LLCCM trained and tested on sentences of a fixed length. LLCCM performs well on longer sentences. The binary branching upper bound correponds to UBOUND from (Klein and Manning, 2002). 4 Experiments We train our models on gold POS sequences from all sections (0-24) of the WSJ (Marcus et al., 1993) with punctuation removed. We report bracketing F1 scores between the binary trees predicted by the models on these sequences and the treebank parses. We train and evaluate both a CCM implementation (Luque, 2011) and our LLCCM on sentences up to a fixed length n, for n E 110,15, ... , 40}. Figure 1 shows that LLCCM substantially outperforms the CCM on longer sentences. After length 15, CCM accuracy falls below the right branching baseline, whereas LLCCM remains significantly better than right-branching through length 40. 5 Conclusion Our log-linear variant of the CCM extends robustly to long s</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>Using semantic cues to learn syntax.</title>
<date>2011</date>
<booktitle>In AAAI.</booktitle>
<marker>Naseem, Barzilay, 2011</marker>
<rawString>Tahira Naseem and Regina Barzilay. 2011. Using semantic cues to learn syntax. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Harr Chen</author>
<author>Regina Barzilay</author>
<author>Mark Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1234--1244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="1510" citStr="Naseem et al., 2010" startWordPosition="215" endWordPosition="218"> surpass a right-branching baseline. However, the CCM only effectively models short sentences. This paper shows that a simple reparameterization of the model, which ties together the probabilities of related events, allows the CCM to extend robustly to long sentences. Much recent research has explored dependency grammar induction. For instance, the dependency model with valence (DMV) of Klein and Manning (2004) has been extended to utilize multilingual information (Berg-Kirkpatrick and Klein, 2010; Cohen et al., 2011), lexical information (Headden III et al., 2009), and linguistic universals (Naseem et al., 2010). Nevertheless, simplistic dependency models like the DMV do not contain information present in a constituency parse, such as the attachment order of object and subject to a verb. Unsupervised constituency parsing is also an active research area. Several studies (Seginer, 2007; Reichart and Rappoport, 2010; Ponvert et al., 2011) have considered the problem of inducing parses over raw lexical items rather than part-of-speech (POS) tags. Additional advances have come from more complex models, such as combining CCM and DMV (Klein and Manning, 2004) and modeling large tree fragments (Bod, 2006). T</context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1234–1244, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Insideoutside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Newark, Delaware, USA,</location>
<contexts>
<context position="726" citStr="Pereira and Schabes, 1992" startWordPosition="97" endWordPosition="100">it University of California, Berkeley Google Google dsg@cs.berkeley.edu denero@google.com uszkoreit@google.com Abstract We present LLCCM, a log-linear variant of the constituent context model (CCM) of grammar induction. LLCCM retains the simplicity of the original CCM but extends robustly to long sentences. On sentences of up to length 40, LLCCM outperforms CCM by 13.9% bracketing F1 and outperforms a right-branching baseline in regimes where CCM does not. 1 Introduction Unsupervised grammar induction is a fundamental challenge of statistical natural language processing (Lari and Young, 1990; Pereira and Schabes, 1992; Carroll and Charniak, 1992). The constituent context model (CCM) for inducing constituency parses (Klein and Manning, 2002) was the first unsupervised approach to surpass a right-branching baseline. However, the CCM only effectively models short sentences. This paper shows that a simple reparameterization of the model, which ties together the probabilities of related events, allows the CCM to extend robustly to long sentences. Much recent research has explored dependency grammar induction. For instance, the dependency model with valence (DMV) of Klein and Manning (2004) has been extended to </context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. Insideoutside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, pages 128– 135, Newark, Delaware, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elias Ponvert</author>
<author>Jason Baldridge</author>
<author>Katrin Erk</author>
</authors>
<title>Simple unsupervised grammar induction from raw text with cascaded finite state models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1077--1086</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1840" citStr="Ponvert et al., 2011" startWordPosition="265" endWordPosition="268">on. For instance, the dependency model with valence (DMV) of Klein and Manning (2004) has been extended to utilize multilingual information (Berg-Kirkpatrick and Klein, 2010; Cohen et al., 2011), lexical information (Headden III et al., 2009), and linguistic universals (Naseem et al., 2010). Nevertheless, simplistic dependency models like the DMV do not contain information present in a constituency parse, such as the attachment order of object and subject to a verb. Unsupervised constituency parsing is also an active research area. Several studies (Seginer, 2007; Reichart and Rappoport, 2010; Ponvert et al., 2011) have considered the problem of inducing parses over raw lexical items rather than part-of-speech (POS) tags. Additional advances have come from more complex models, such as combining CCM and DMV (Klein and Manning, 2004) and modeling large tree fragments (Bod, 2006). The CCM scores each parse as a product of probabilities of span and context subsequences. It was originally evaluated only on unpunctuated sentences up to length 10 (Klein and Manning, 2002), which account for only 15% of the WSJ corpus; our experiments confirm the observation in (Klein, 2005) that performance degrades dramatical</context>
</contexts>
<marker>Ponvert, Baldridge, Erk, 2011</marker>
<rawString>Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011. Simple unsupervised grammar induction from raw text with cascaded finite state models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1077–1086, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Improved fully unsupervised parsing with zoomed learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>684--693</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="1817" citStr="Reichart and Rappoport, 2010" startWordPosition="261" endWordPosition="264">red dependency grammar induction. For instance, the dependency model with valence (DMV) of Klein and Manning (2004) has been extended to utilize multilingual information (Berg-Kirkpatrick and Klein, 2010; Cohen et al., 2011), lexical information (Headden III et al., 2009), and linguistic universals (Naseem et al., 2010). Nevertheless, simplistic dependency models like the DMV do not contain information present in a constituency parse, such as the attachment order of object and subject to a verb. Unsupervised constituency parsing is also an active research area. Several studies (Seginer, 2007; Reichart and Rappoport, 2010; Ponvert et al., 2011) have considered the problem of inducing parses over raw lexical items rather than part-of-speech (POS) tags. Additional advances have come from more complex models, such as combining CCM and DMV (Klein and Manning, 2004) and modeling large tree fragments (Bod, 2006). The CCM scores each parse as a product of probabilities of span and context subsequences. It was originally evaluated only on unpunctuated sentences up to length 10 (Klein and Manning, 2002), which account for only 15% of the WSJ corpus; our experiments confirm the observation in (Klein, 2005) that performa</context>
</contexts>
<marker>Reichart, Rappoport, 2010</marker>
<rawString>Roi Reichart and Ari Rappoport. 2010. Improved fully unsupervised parsing with zoomed learning. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 684–693, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Seginer</author>
</authors>
<title>Fast unsupervised incremental parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>384--391</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1787" citStr="Seginer, 2007" startWordPosition="259" endWordPosition="260">earch has explored dependency grammar induction. For instance, the dependency model with valence (DMV) of Klein and Manning (2004) has been extended to utilize multilingual information (Berg-Kirkpatrick and Klein, 2010; Cohen et al., 2011), lexical information (Headden III et al., 2009), and linguistic universals (Naseem et al., 2010). Nevertheless, simplistic dependency models like the DMV do not contain information present in a constituency parse, such as the attachment order of object and subject to a verb. Unsupervised constituency parsing is also an active research area. Several studies (Seginer, 2007; Reichart and Rappoport, 2010; Ponvert et al., 2011) have considered the problem of inducing parses over raw lexical items rather than part-of-speech (POS) tags. Additional advances have come from more complex models, such as combining CCM and DMV (Klein and Manning, 2004) and modeling large tree fragments (Bod, 2006). The CCM scores each parse as a product of probabilities of span and context subsequences. It was originally evaluated only on unpunctuated sentences up to length 10 (Klein and Manning, 2002), which account for only 15% of the WSJ corpus; our experiments confirm the observation </context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Yoav Seginer. 2007. Fast unsupervised incremental parsing. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 384– 391, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>