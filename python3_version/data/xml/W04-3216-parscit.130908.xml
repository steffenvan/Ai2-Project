<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.998074">
A Phrase-Based HMM Approach to Document/Abstract Alignment
</title>
<author confidence="0.976028">
Hal Daum´e III and Daniel Marcu
</author>
<affiliation confidence="0.899447">
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.74184">
Marina del Rey, CA 90292
</address>
<email confidence="0.999694">
{hdaume,marcu}@isi.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999426">
We describe a model for creating word-to-word and
phrase-to-phrase alignments between documents
and their human written abstracts. Such alignments
are critical for the development of statistical sum-
marization systems that can be trained on large cor-
pora of document/abstract pairs. Our model, which
is based on a novel Phrase-Based HMM, outper-
forms both the Cut &amp; Paste alignment model (Jing,
2002) and models developed in the context of ma-
chine translation (Brown et al., 1993).
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954608695652">
There are a wealth of document/abstract pairs that
statistical summarization systems could leverage to
learn how to create novel abstracts. Detailed stud-
ies of such pairs (Jing, 2002) show that human ab-
stractors perform a range of very sophisticated op-
erations when summarizing texts, which include re-
ordering, fusion, and paraphrasing. Unfortunately,
existing document/abstract alignment models are
not powerful enough to capture these operations.
To get around directly tackling this problem, re-
searchers in text summarization have employed one
of several techniques.
Some researchers (Banko et al., 2000) have de-
veloped simple statistical models for aligning doc-
uments and headlines. These models, which imple-
ment IBM Model 1 (Brown et al., 1993), treat docu-
ments and headlines as simple bags of words and
learn probabilistic word-based mappings between
the words in the documents and the words in the
headlines. As our results show, these models are
too weak for capturing the operations that are em-
ployed by humans in summarizing texts beyond the
headline level.
Other researchers have developed models that
make unreasonable assumptions about the data,
which lead to the utilization of a very small per-
cent of available data. For instance, the docu-
ment and sentence compression models of Daum´e
III, Knight, and Marcu (Knight and Marcu, 2002;
Daum´e III and Marcu, 2002a) assume that sen-
tences/documents can be summarized only through
deletion of contiguous text segments. Knight and
Marcu found that from a corpus of 39, 060 abstract
sentences, only 1067 sentence extracts existed: a re-
call of only 2.7%.
An alternate techinque employed in a large vari-
ety of systems is to treat the summarization prob-
lem as a sentence extraction problem. Such sys-
tems can be trained either on human constructed ex-
tracts or extracts generated automatically from doc-
ument/abstract pairs (see (Marcu, 1999; Jing and
McKeown, 1999) for two such approaches).
None of these techniques is adequate. Even for a
relatively simple sentence from an abstract, we can
see that none of the assumptions listed above holds.
In Figure 1, we observe several phenomena:
</bodyText>
<listItem confidence="0.9999058">
• Alignments can occur at the granularity of
words and at the granularity of phrases.
• The ordering of phrases in an abstract can be
different from the ordering in the document.
• Some abstract words do not have direct cor-
</listItem>
<bodyText confidence="0.943943625">
respondents in the document, and some doc-
ument words are never used.
It is thus desirable to be able to automatically
construct alignments between documents and their
abstracts, so that the correspondences between the
pairs are obvious. One might be initially tempted
to use readily-available machine translation systems
like GIZA++ (Och and Ney, 2003) to perform such
</bodyText>
<note confidence="0.4682645">
Connecting Point has become the single largest Mac retailer after tripling it ’s Macintosh sales since January 1989 .
Connecting Point Systems tripled it ’s sales of Apple Macintosh systems since last January . It is now the single largest seller of Macintosh .
</note>
<figureCaption confidence="0.999847">
Figure 1: Example abstract/text alignment.
</figureCaption>
<bodyText confidence="0.9994945">
alignments. However, as we will show, the align-
ments produced by such a system are inadequate for
this task.
The solution that we propose to this problem is
an alignment model based on a novel mathematical
structure we call the Phrase-Based HMM.
</bodyText>
<sectionHeader confidence="0.992201" genericHeader="method">
2 Designing a Model
</sectionHeader>
<bodyText confidence="0.999677">
As observed in Figure 1, our model needs to be able
to account for phrase-to-phrase alignments. It also
needs to be able to align abstract phrases with arbi-
trary parts of the document, and not require a mono-
tonic, left-to-right alignment.1
</bodyText>
<subsectionHeader confidence="0.996055">
2.1 The Generative Story
</subsectionHeader>
<bodyText confidence="0.944382851851852">
The model we propose calculates the probability of
an alignment/abstract pair in a generative fashion,
generating the summary 5 = (s1 ... sm) from the
document D = (d1 ... dn).
In a document/abstract corpus that we have
aligned by hand (see Section 3), we have observed
that 16% of abstract words are left unaligned. Our
model assumes that these “null-generated” words
and phrases are produced by a unique document
word 0, called the “null word.” The parame-
ters of our model are stored in two tables: a
rewrite/paraphrase table and a jump table. The
rewrite table stores probabilities of producing sum-
mary words/phrases from document words/phrases
and from the null word (namely, probabilities of the
form rewrite (¯s ¯d� and rewrite (¯s 0)); the jump ta-
ble stores the probabilities of moving within a doc-
ument from one position to another, and from and
to 0.
The generation of a summary from a document is
assumed to proceed as follows:
1In the remainder of the paper, we will use the words `sum-
mary” and `abstract” interchangeably. This is because we wish
to use the letter s to refer to summaries. We could use the letter
a as an abbreviation for `abstract”; however, in the definition
of the Phrase-Based HMM, we reuse common notation which
ascribes a different interpretation to a.
</bodyText>
<listItem confidence="0.994542571428571">
1. Choose a starting index i and jump to po-
sition di in the document with probability
jump (i). (If the first summary phrase is null-
generated, jump to the null-word with proba-
bility jump (0).)
2. Choose a document phrase of length k &gt; 0 and
a summary phrase of length l &gt; 1. Generate
</listItem>
<bodyText confidence="0.647797333333333">
summary words sl1 from document words di+k
i
�with probability rewrite sl 1
</bodyText>
<listItem confidence="0.963531666666667">
3. Choose a new document index i&apos; and
jump to position di with probability
jump (i&apos; − (i + k)) (or, if the new document
position is the empty state, then jump (0)).
4. Choose k&apos; and l&apos; as in step 2, and gener-
ate the summary words s1+l+l
</listItem>
<bodyText confidence="0.776253">
1+l from the
document words di+k with probability
</bodyText>
<equation confidence="0.6745955">
i
rewrite (s1+l1+l+l
</equation>
<listItem confidence="0.996803">
5. Repeat from step 3 until the entire summary
has been generated.
6. Jump to position dn+1 in the document with
probability jump (n + 1 − (i&apos; + k&apos;)).
</listItem>
<bodyText confidence="0.999723285714286">
Note that such a formulation allows the same
document word/phrase to generate many summary
words: unlike machine translation, where such be-
havior is typically avoided, in summarization, we
observe that such phenomena do occur. However,
if one were to build a decoder based on this model,
one would need to account for this issue to avoid
degenerate summaries from being produced.
The formal mathematical model behind the align-
ments is as follows: An alignment R defines both
a segmentation of the summary 5 and a mapping
from the segments of 5 to the segments of the doc-
ument D. We write si to refer to the ith segment of
5, and M to refer to the total number of segments
</bodyText>
<footnote confidence="0.825515">
2We write xba for the subsequence (xa ... xb).
</footnote>
<equation confidence="0.994887666666667">
di+k / 2
i J
dil+k)
</equation>
<bodyText confidence="0.98138275">
in S. We write dR(i) to refer to the words in the
document which correspond to segment si. Then,
the probability of a summary/alignment pair given a
document (Pr (S, R D)), becomes:
</bodyText>
<equation confidence="0.996703">
nor+1
(jump (R(i) R(i − 1)) rewrite (si dR(i)))
i=1
</equation>
<bodyText confidence="0.9997602">
Here, we implicitly define sm+1 to be the end-of-
document token () and dR(m+1) to generate this
with probability 1. We also define the initial posi-
tion in the document, R(0) to be 0, and assume a
uniform prior on segmentations.
</bodyText>
<subsectionHeader confidence="0.903086">
2.2.2 Backward algorithm
</subsectionHeader>
<bodyText confidence="0.9999404">
Just as we can compute the probability of an obser-
vation sequence by moving forward, so can we cal-
culate it by going backward. We define i(t) as the
probability of emitting the sequence oTt given that
we are starting out in state i.
</bodyText>
<subsectionHeader confidence="0.912261">
2.2.3 Best path
</subsectionHeader>
<bodyText confidence="0.974113142857143">
We define a path as a sequence P = (p1 ... pL) such
that pi is a tuple (t, x) where t corresponds to the
last of the (possibly multiple) observations made,
and x refers to the state we were coming from when
we output this observation (phrase). Thus, we want
to find:
argmax Pr (P oT1 , µ) = argmax Pr (P, oT1 µ)
</bodyText>
<subsectionHeader confidence="0.957798">
2.2 The Mathematical Model P P
</subsectionHeader>
<bodyText confidence="0.999982142857143">
Having decided to use this model, we must now
find a way to efficiently train it. The model is very
much like a Hidden Markov Model in which the
summary is the observed sequence. However, us-
ing a standard HMM would not allow us to account
for phrases in the summary. We therefore extend
a standard HMM to allow multiple observations to
be emitted on one transition. We call this model a
Phrase-Based HMM (PBHMM).
For this model, we have developed equiva-
lents of the forward and backward algorithms,
Viterbi search and forward-backward parameter re-
estimation. Our notation is shown in Table 1.
Here, S is the state space, and the observation se-
quences come from the alphabet K. j is the prob-
ability of beginning in state j. The transition prob-
ability ai,j is the probability of transitioning from
state i to state j. bi,j,¯k is the probability of emitting
(the non-empty) observation sequence k¯ while tran-
sitioning from state i to state j. Finally, xt denotes
the state after emitting t symbols.
The full derivation of the model is too lengthy to
include; the interested reader is directed to (Daum´e
III and Marcu, 2002b) for the derivations and proofs
of the formulae. To assist the reader in understand-
ing the mathematics, we follow the same notation as
(Manning and Schutze, 2000). The formulae for the
calculations are summarized in Table 2.
</bodyText>
<subsectionHeader confidence="0.521559">
2.2.1 Forward algorithm
</subsectionHeader>
<bodyText confidence="0.999778222222222">
The forward algorithm calculates the probability of
an observation sequence. We define j(t) as the
probability of being in state j after emitting the first
t − 1 symbols (in whatever grouping we want).
To do this, as in a traditional HMM, we estimate
the  table. When we calculate j(t), we essentially
need to choose an appropriate i and t&apos;, which we
store in another table, so we can calculate the actual
path at the end.
</bodyText>
<subsubsectionHeader confidence="0.804772">
2.2.4 Parameter re-estimation
</subsubsectionHeader>
<bodyText confidence="0.999953777777778">
We want to find the model µ which best explains
observations. There is no known analytic solution
for standard HMMs, so we are fairly safe in assum-
ing that we will not find an analytic solution for this
more complex problem. Thus, we also revert to an
iterative hill-climbing solution analogous to Baum-
Welch re-estimation (i.e., the Forward Backward al-
gorithm). The equations for the re-estimated values
aˆ and bˆ are shown in Table 2.
</bodyText>
<subsectionHeader confidence="0.567822">
2.2.5 Dirichlet Priors
</subsectionHeader>
<bodyText confidence="0.99998875">
Using simple maximum likelihood estimation is in-
adequate for this model: the maximum likelihood
solution is simply to make phrases as long as pos-
sible; unfortunately, doing so will first cut down on
the number of probabilities that need to be multi-
plied and second make nearly all observed summary
phrase/document phrase alignments unique, thus re-
sulting in rewrite probabilities of 1 after normaliza-
tion. In order to account for this, instead of finding
the maximum likelihood solution, we instead seek
the maximum a posteriori solution.
The distributions we deal with in HMMs, and,
in particular, PBHMMs, are all multinomial. The
Dirichlet distribution is in the conjugate family to
the multinomial distribution3. This makes Dirich-
let priors very appealing to work with, so long as
</bodyText>
<footnote confidence="0.938079">
3This effectively means that the product of a Dirichlet and
multinomial yields a multinomial.
</footnote>
<note confidence="0.370597">
S set of states
</note>
<equation confidence="0.997074">
K output alphabet
H = {j : j  S} initial state probabilities
A = {ai,j : i, j  S} transition probabilities
B = {bi,j,¯k : i,j  S, k¯  K+} emission probabilities
</equation>
<tableCaption confidence="0.703365">
Table 1: Notation used for the PBHMM
</tableCaption>
<equation confidence="0.94262125">
j(t) = Pr ot−1 t− 1   
1 , xt−1 = j µ = t=0 iS i(t + 1) · ai,j · bi,j,ot t+1
i(t) = Pr oTt µ, xt−1 = i = T jS 
t=t ai,j · bi,j,ot
 t ·j(t + 1)
j(t) =max
l,pl−1
1
 
Pr pl−1
1 ,ot−1
1 ,pl.t = t − 1,pl.x = j µ = i(t)ai,jbi,j,ot−1
t
i,j(t, t) = E # of transitions i --* j emitting ot  = i(t)ai,jbi,j,ottj(t + 1)
t Pr oT1 µ
ˆai,j = E [# of transitions i --* j] T T t=t i,j(t, t)
t=1
E [# of transitions i --*?] T T jS i,j(t, t)
t=1 t=t
E # of transitions i --* j with k¯ observed
E [# of transitions i --* j]
T+1−|k|
t=1 
T T t=t i,j(t, t)
t=1
ˆbi,j,¯k =
t+ |k|−1 )i,j(t,t +  |k |− 1)
k, ot
</equation>
<tableCaption confidence="0.962894">
Table 2: Summary of equations for a PBHMM
</tableCaption>
<bodyText confidence="0.9868375">
we can adequately express our prior beliefs in their
form. (See (Gauvain and Lee, 1994) for the appli-
cation to standard HMMs.)
Applying a Dirichlet prior effectively allows us to
add “fake counts” during parameter re-estimation,
according to the prior. The prior we choose has a
form such that fake counts are added as follows:
word-to-word rewrites get an additional count of 2;
identity rewrites get an additional count of 4; stem-
identity rewrites get an additional count of 3.
</bodyText>
<subsectionHeader confidence="0.998882">
2.3 Constructing the PBHMM
</subsectionHeader>
<bodyText confidence="0.999652">
Given our generative story, we construct a PBHMM
to calculate these probabilities efficiently. The
structure of the PBHMM for a given document is
conceptually simple. We provide values for each of
the following: the set of possible states S; the out-
put alphabet K; the initial state probabilities H; the
transition probabilities A; and the emission proba-
bilities B.
</bodyText>
<subsectionHeader confidence="0.63139">
2.3.1 State Space
</subsectionHeader>
<bodyText confidence="0.999879666666667">
The state set is large, but structured. There is a
unique initial state p, a unique final state q, and a
state for each possible document phrase. That is, for
all 1  i  i  n, there is a state that corresponds
to the document phrase beginning at position i and
ending at position i, di
i , which we will refer to as
ri,i. There is also a null state for each document po-
sition r ,i, so that when jumping out of a null state,
we can remember what our previous position in the
document was. Thus, S = {p, q}  {ri,i : 1  i 
i  n}  {r ,i : 1  i  n}. Figure 2 shows the
schematic drawing of the PBHMM constructed for
the document “a b”. K, the output alphabet, con-
sists of each word found in S, plus the token .
</bodyText>
<subsectionHeader confidence="0.799823">
2.3.2 Initial State Probabilities
</subsectionHeader>
<bodyText confidence="0.993379">
For initial state probabilities: since p is our initial
state, we say that p = 1 and that r = 0 for all
r =p.
</bodyText>
<subsubsectionHeader confidence="0.882075">
2.3.3 Transition Probabilities
</subsubsectionHeader>
<bodyText confidence="0.999985">
The transition probabilities A are governed by the
jump table. Each possible jump type and it’s as-
sociated probability is shown in Table 3. By these
calculations, regardless of document phrase lengths,
transitioning forward between two consecutive seg-
ments will result in jump (1). When transitioning
</bodyText>
<equation confidence="0.608904833333333">
jump(0) jump(0)
jump( )
jump(1)
jump(1)
p
jump(2)
</equation>
<figure confidence="0.990508">
ab
a
a
jump(2)
b
jump(1)
b
jump(1)
jump(2)
jump(1)
q
</figure>
<figureCaption confidence="0.998547">
Figure 2: Schematic drawing of the PBHMM (with some transition probabilities) for the document “a b”
</figureCaption>
<table confidence="0.997180777777778">
source target probability
p ri,i jump (i)
ri,i rj,j jump (j − i&apos;)
ri,j q jump (m + 1 − i&apos;)
p r ,i jump (0)jump (i)
r ,i rj,j jump (j − i)
r ,i r ,j jump (0)jump (j − i)
r ,i q jump (m + 1 − i)
ri,i r ,j jump (0)jump (j − i&apos;)
</table>
<tableCaption confidence="0.999522">
Table 3: Jump probability decomposition
</tableCaption>
<bodyText confidence="0.999706">
from p to ri,i, the value ap,ri,i = jump (i). Thus,
if we begin at the first word in the document, we
incur a transition probability ofjump (1). There are
no transitions into p.
</bodyText>
<subsubsectionHeader confidence="0.440389">
2.3.4 Rewrite Probabilities
</subsubsectionHeader>
<bodyText confidence="0.999972243243243">
Just as the transition probabilities are governed by
the jump table, the emission probabilities B are
governed by the rewrite table. In general, we write
bx,y,¯k to mean the probability of generating k while
transitioning from state x to state y. However, in
our case we do not need the x parameter, so we
will refer to these as bj,¯k, the probability of generat-
ing k¯ when jumping into state j. When j = ri,i,
this is rewrite (k df) . When j = r i, this is
rewrite (¯k 0). Finally, any state transitioning into
q generates the phrase (w) with probability 1 and
any other phrase with probability 0.
Consider again the document “a b” (the PBHMM
for which is shown in Figure 2) in the case when
the corresponding summary is “c d”. Suppose the
correct alignment is that “c d” is aligned to “a” and
“b” is left unaligned. Then, the path taken through
the PBHMM is p —* a —* q. During the transi-
tion p —* a, “c d” is emitted. During the transition
a —* q, w is emitted. Thus, the probability for the
alignment is: jump (1) rewrite (“cd” “a”) jump (2).
The rewrite probabilities themselves are gov-
erned by a mixture model with unknown mixing pa-
rameters. There are three mixture component, each
of which is represented by a multinomial. The first
is the standard word-for-word and phrase-for-phrase
table seen commonly in machine translation, where
rewrite (¯s ¯d� is simply a normalized count of how
many times we have seen s¯ aligned to
ond is a stem-based table, in which suffixes (using
Porter’s stemmer) of the words in s¯ and d¯are thrown
out before a comparison is made. The third is a
simple identity function, which has a constant zero
value when s¯ and d¯ are different (up to stem) and
a constant non-zero value when they have the same
stem. The mixing parameters are estimated simul-
taneously during EM.
</bodyText>
<subsubsectionHeader confidence="0.710298">
2.3.5 Parameter Initialization
</subsubsectionHeader>
<bodyText confidence="0.9999035">
Instead of initializing the jump and rewrite tables
randomly or uniformly, as it typically done with
HMMs, we initialize the tables according to the dis-
tribution specified by the prior. This is not atypi-
cal practice in problems in which a MAP solution is
sought.
</bodyText>
<sectionHeader confidence="0.995197" genericHeader="evaluation">
3 Evaluation and Results
</sectionHeader>
<bodyText confidence="0.999762923076923">
In this section, we describe an intrinsic evaluation of
the PBHMM document/abstract alignment model.
All experiments in this paper are done on the Ziff-
Davis corpus (statistics are in Table 4). In order
to judge the quality of the alignments produced by
a system, we first need to create a set of “gold
standard” alignments. Two human annotators man-
ually constructed such alignments between docu-
ments and their abstracts. Software for assisting this
process was developed and is made freely available.
An annotation guide, which explains in detail the
document/abstract alignment process was also pre-
pared and is freely available.4
</bodyText>
<footnote confidence="0.74873">
4Both the software and documentation are available on the
first author’s web page. The alignments are also available; con-
tact the authors for a copy.
</footnote>
<table confidence="0.992273272727273">
¯
d. The sec-
Abstracts Extracts
Documents 2033
Sentences 13k 41k
Words 261k 1m
Types 14k 26k
29k
Sentences/Doc 6.28 21.51
Words/Doc 128.52 510.99
Words/Sent 20.47 23.77
</table>
<tableCaption confidence="0.999306">
Table 4: Ziff-Davis extract corpus statistics
</tableCaption>
<subsectionHeader confidence="0.99354">
3.1 Human Annotation
</subsectionHeader>
<bodyText confidence="0.999947875">
From the Ziff-Davis corpus, we randomly selected
45 document/abstract pairs and had both annotators
align them. The first five were annotated separately
and then discussed; the last 40 were done indepen-
dently.
Annotators were asked to perform phrase-to-
phrase alignments between abstracts and documents
and to classify each alignment as either possible P
or sure S, where P C_ S. In order to calculate
scores for phrase alignments, we convert all phrase
alignments to word alignments. That is, if we have
an alignment between phrases A and B, then this
induces word alignments between a and b for all
words a E A and b E B. Given an alignment A,
we could calculate precision and recall as (see (Och
and Ney, 2003)):
</bodyText>
<equation confidence="0.97455">
Precision = |AP ||A |Recall =|AS|
|S|
</equation>
<bodyText confidence="0.999949111111111">
One problem with these definitions is that phrase-
based models are fond of making phrases. That is,
when given an abstract containing “the man” and a
document also containing “the man,” a human may
prefer to align “the” to “the” and “man” to “man.”
However, a phrase-based model will almost always
prefer to align the entire phrase “the man” to “the
man.” This is because it results in fewer probabili-
ties being multiplied together.
To compensate for this, we define soft precision
(SoftP in the tables) by counting alignments where
“a b” is aligned to “a b” the same as ones in which
“a” is aligned to “a” and “b” is aligned to “b.” Note,
however, that this is not the same as “a” aligned to
“a b” and “b” aligned to “b”. This latter alignment
will, of course, incur a precision error. The soft pre-
cision metric induces a new, soft F-Score, labeled
SoftF.
Often, even humans find it difficult to align func-
tion words and punctuation. A list of 58 function
words and punctuation marks which appeared in the
corpus (henceforth called the ignore-list) was as-
sembled. Agreement and precision/recall have been
calculated both on all words and on all words that
do not appear in the ignore-list.
Annotator agreement was strong for Sure align-
ments and fairly weak for Possible alignments (con-
sidering only the 40 independently annotated pairs).
When considering only Sure alignments, the kappa
statistic (over 7.2 million items, 2 annotators and 2
categories) for agreement was 0.63. When words
from the ignore-list were thrown out, this rose to
0.68. Carletta (1995) suggests that kappa values
over 0.80 reflect very strong agreement and that
kappa values between 0.60 and 0.80 reflect good
agreement.
</bodyText>
<subsectionHeader confidence="0.997908">
3.2 Machine Translation Experiments
</subsectionHeader>
<bodyText confidence="0.999492229166667">
In order to establish a baseline alignment model,
we used the IBM Model 4 (Brown et al., 1993)
and the HMM model (Stephan Vogel and Tillmann,
1996) as implemented in the GIZA++ package (Och
and Ney, 2003). We modified this slightly to allow
longer inputs and higher fertilities.
Such translation models require that input be in
sentence-aligned form. In the summarization task,
however, one abstract sentence often corresponds
to multiple document sentences. In order to over-
come this problem, each sentence in an abstract was
paired with three sentences from the corresponding
document, selected using the techniques described
by Marcu (1999). In an informal evaluation, 20 such
pairs were randomly extracted and evaluated by a
human. Each pair was ranked as 0 (document sen-
tences contain little-to-none of the information in
the abstract sentence), 1 (document sentences con-
tain some of the information in the abstract sen-
tence) or 2 (document sentences contain all of the
information). Of the twenty random examples, none
were labeled as 0; five were labeled as 1; and 15
were labeled as 2, giving a mean rating of 1.75.
We ran experiments using the document sen-
tences as both the source and the target language
in GIZA++. When document sentences were used
as the target language, each abstract word needed
to produce many document words, leading to very
high fertilities. However, since each target word is
generated independently, this led to very flat rewrite
tables and, hence, to poor results. Performance in-
creased dramatically by using the document as the
source language and the abstract as the target lan-
guage.
In all MT cases, the corpus was appended with
one-word sentence pairs for each word where that
word is translated as itself. In the two basic mod-
els, HMM and Model 4, the abstract sentence is the
source language and the document sentences are the
target language. To alleviate the fertility problem,
we also ran experiments with the translation going
in the opposite direction. These are called HMM-
flipped and Model 4-flipped, respectively. These
tend to out-perform the original translation direc-
tion. In all of these setups, 5 iterations of Model
1 were run, followed by 5 iterations of the HMM
model. In the Model 4 cases, 5 iterations of Model
4 were run, following the HMM.
</bodyText>
<subsectionHeader confidence="0.99587">
3.3 Cut and Paste Experiments
</subsectionHeader>
<bodyText confidence="0.999937642857143">
We also tested alignments using the Cut and
Paste summary decomposition method (Jing, 2002),
based on a non-trainable HMM. Briefly, the Cut and
Paste HMM searches for long contiguous blocks of
words in the document and abstract that are iden-
tical (up to stem). The longest such sequences are
aligned. By fixing a length cutoff of n and ignoring
sequences of length less than n, one can arbitrarily
increase the precision of this method. We found that
n = 2 yields the best balance between precision and
recall (and the highest F-measure). The results of
these experiments are shown under the header “Cut
&amp; Paste.” It clearly outperforms all of the MT-based
models.
</bodyText>
<subsectionHeader confidence="0.825591">
3.4 PBIEMM Experiments
</subsectionHeader>
<bodyText confidence="0.999884315789474">
While the PBHMM is based on a dynamic program-
ming algorithm, the effective search space in this
model is enormous, even for moderately sized doc-
ument/abstract pairs. We selected the 2000 shortest
document/abstract pairs from the Ziff-Davis corpus
for training; however, only 12 of the hand-annotated
documents were included in this set, so we addition-
ally added the other 33 hand-annotate documents to
this set, yielding 2033 document/abstract pairs. We
then performed sentence extraction on this corpus
exactly as in the MT case, using the technique of
(Marcu, 1999). The relevant data for this corpus is
in Table 4. We also restrict the state-space with a
beam, sized at 50% of the unrestricted state-space.
The PBHMM system was then trained on this ab-
stract/extract corpus. The precision/recall results
are shown in Table 5. Under the methodology for
combining the two human annotations by taking the
union, either of the human scores would achieve a
</bodyText>
<table confidence="0.9996588">
System SoftP Recall SoftF
Human, 0.727 0.746 0.736
Humane 0.680 0.695 0.687
HMM 0.120 0.260 0.164
Model 4 0.117 0.260 0.161
HMM-flipped 0.295 0.250 0.271
Model 4-flipped 0.280 0.247 0.262
Cut &amp; Paste 0.349 0.379 0.363
PBHMM 0.456 0.686 0.548
PBHMM O 0.523 0.686 0.594
</table>
<tableCaption confidence="0.999898">
Table 5: Results on the Ziff-Davis corpus
</tableCaption>
<bodyText confidence="0.999942916666667">
precision and recall of 1.0. To give a sense of how
well humans actually perform on this task (in addi-
tion to the kappa scores reported earlier), we com-
pare each human against the other.
One common precision mistake made by the
PBHMM system is to accidentally align words on
the summary side to words on the document side,
when the summary word should be null-aligned.
The PBHMMO system is an oracle system in which
system-produced alignments are removed for sum-
mary words that should be null-aligned (according
to the hand-annotated data). Doing this results in a
rather significant gain in SoftP score.
As we can see from Table 5, none of the ma-
chine translation models is well suited to this task,
achieving, at best, an F-score of 0.298. The Cut &amp;
Paste method performs significantly better, which is
to be expected, since it is designed specifically for
summarization. As one would expect, this method
achieves higher precision than recall, though not by
very much. Our method significantly outperforms
both the IBM models and the Cut &amp; Paste method,
achieving a precision of 0.456 and a recall nearing
0.7, yielding an overall F-score of 0.548.
</bodyText>
<sectionHeader confidence="0.99938" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999979078947368">
Despite the success of our model, it’s performance
still falls short of human performance (we achieve
an F-score of 0.548 while humans achieve 0.736).
Moreover, this number for human performance is
a lower-bound, since it is calculated with only one
reference, rather than two.
We have begun to perform a rigorous error anal-
ysis of the model to attempt to identify its deficien-
cies: currently, these appear to primarily be due to
the model having a zeal for aligning identical words.
This happens for one of two reasons: either a sum-
mary word should be null-aligned (but it is not),
or a summary word should be aligned to a differ-
ent, non-identical document word. We can see the
PBHMMO model as giving us an upper bound on
performance if we were to fix this first problem. The
second problem has to do either with synonyms that
do not appear frequently enough for the system to
learn reliable rewrite probabilities, or with corefer-
ence issues, in which the system chooses to align,
for instance, “Microsoft” to “Microsoft,” rather than
“Microsoft” to “the company,” as might be correct
in context. Clearly more work needs to be done
to fix these problems; we are investigating solving
the first problem by automatically building a list of
synonyms from larger corpora and using this in the
mixture model, and the second problem by inves-
tigating the possibility of including some (perhaps
weak) coreference knowledge into the model.
Finally, we are looking to incorporate the results
of this model into a real system. This can be done ei-
ther by using the word-for-word alignments to auto-
matically build sentence-to-sentence alignments for
training a sentence extraction system (in which case
the precision/recall numbers over full sentences are
likely to be much higher), or by building a system
that exploits the word-for-word alignments explic-
itly.
</bodyText>
<sectionHeader confidence="0.999714" genericHeader="acknowledgments">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999806">
This work was partially supported by DARPA-ITO
grant N66001-00-1-9814, NSF grant IIS-0097846,
and a USC Dean Fellowship to Hal Daum´e III.
Thanks to Franz Josef Och and Dave Blei for dis-
cussions related to the project.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99979525862069">
Michele Banko, Vibhu Mittal, and Michael Wit-
brock. 2000. Headline generation based on sta-
tistical translation. In Proceedings of the 38th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL–2000), pages 318–325,
Hong Kong, October 1–8.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Lin-
guistics, 19(2):263–311.
Jean Carletta. 1995. Assessing agreement on clas-
sification tasks: the kappa statistic. Computa-
tional Linguistics, 22(2):249–254.
Hal Daum´e III and Daniel Marcu. 2002a. A noisy-
channel model for document compression. In
Proceedings of the Conference of the Association
of Computational Linguistics (ACL 2002).
Hal Daum´e III and Daniel Marcu. 2002b.
A phrase-based HMM. Unpublished; avail-
able at http://www.isi.edu/˜hdaume/
docs/daume02pbhmm.ps, December.
J. Gauvain and C. Lee. 1994. Maximum a-
posteriori estimation for multivariate gaussian
mixture observations of markov chains. IEEE
Transactions SAP, 2:291–298.
Hongyan Jing and Kathleen R. McKeown. 1999.
The decomposition of human-written summary
sentences. In Proceedings of the 22nd Confer-
ence on Research and Development in Informa-
tion Retrieval (SIGIR–99), Berkeley, CA, August
15–19.
Hongyan Jing. 2002. Using hidden markov mod-
eling to decompose human-written summaries.
Computational Linguistics, 28(4):527 – 544, De-
cember.
Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: A proba-
bilistic approach to sentence compression. Arti-
ficialIntelligence, 139(1).
Christopher Manning and Hinrich Schutze. 2000.
Foundations ofStatistical Natural Language Pro-
cessing. The MIT Press.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research.
In Proceedings of the 22nd Conference on Re-
search and Development in Information Retrieval
(SIGIR–99), pages 137–144, Berkeley, CA, Au-
gust 15–19.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguistics,
29(1):19–51.
Hermann Ney Stephan Vogel and Christoph Till-
mann. 1996. HMM-based word alignment in sta-
tistical translation. In COLING ’96: The 16th Int.
Conf. on Computational Linguistics, pages 836–
841.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.126282">
<title confidence="0.996438">A Phrase-Based HMM Approach to Document/Abstract Alignment</title>
<author confidence="0.416511">Daum´e</author>
<affiliation confidence="0.933625">Information Sciences University of Southern</affiliation>
<address confidence="0.995282">4676 Admiralty Way, Suite</address>
<author confidence="0.723367">Marina del Rey</author>
<author confidence="0.723367">CA</author>
<abstract confidence="0.9821517">We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts. Such alignments are critical for the development of statistical summarization systems that can be trained on large corpora of document/abstract pairs. Our model, which is based on a novel Phrase-Based HMM, outperforms both the Cut &amp; Paste alignment model (Jing, 2002) and models developed in the context of ma-</abstract>
<note confidence="0.566613">chine translation (Brown et al., 1993).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Vibhu Mittal</author>
<author>Michael Witbrock</author>
</authors>
<title>Headline generation based on statistical translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL–2000),</booktitle>
<pages>318--325</pages>
<location>Hong Kong,</location>
<contexts>
<context position="1346" citStr="Banko et al., 2000" startWordPosition="193" endWordPosition="196">ntroduction There are a wealth of document/abstract pairs that statistical summarization systems could leverage to learn how to create novel abstracts. Detailed studies of such pairs (Jing, 2002) show that human abstractors perform a range of very sophisticated operations when summarizing texts, which include reordering, fusion, and paraphrasing. Unfortunately, existing document/abstract alignment models are not powerful enough to capture these operations. To get around directly tackling this problem, researchers in text summarization have employed one of several techniques. Some researchers (Banko et al., 2000) have developed simple statistical models for aligning documents and headlines. These models, which implement IBM Model 1 (Brown et al., 1993), treat documents and headlines as simple bags of words and learn probabilistic word-based mappings between the words in the documents and the words in the headlines. As our results show, these models are too weak for capturing the operations that are employed by humans in summarizing texts beyond the headline level. Other researchers have developed models that make unreasonable assumptions about the data, which lead to the utilization of a very small pe</context>
</contexts>
<marker>Banko, Mittal, Witbrock, 2000</marker>
<rawString>Michele Banko, Vibhu Mittal, and Michael Witbrock. 2000. Headline generation based on statistical translation. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL–2000), pages 318–325, Hong Kong, October 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="722" citStr="Brown et al., 1993" startWordPosition="103" endWordPosition="106">Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 {hdaume,marcu}@isi.edu Abstract We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts. Such alignments are critical for the development of statistical summarization systems that can be trained on large corpora of document/abstract pairs. Our model, which is based on a novel Phrase-Based HMM, outperforms both the Cut &amp; Paste alignment model (Jing, 2002) and models developed in the context of machine translation (Brown et al., 1993). 1 Introduction There are a wealth of document/abstract pairs that statistical summarization systems could leverage to learn how to create novel abstracts. Detailed studies of such pairs (Jing, 2002) show that human abstractors perform a range of very sophisticated operations when summarizing texts, which include reordering, fusion, and paraphrasing. Unfortunately, existing document/abstract alignment models are not powerful enough to capture these operations. To get around directly tackling this problem, researchers in text summarization have employed one of several techniques. Some research</context>
<context position="20820" citStr="Brown et al., 1993" startWordPosition="3643" endWordPosition="3646">nt was strong for Sure alignments and fairly weak for Possible alignments (considering only the 40 independently annotated pairs). When considering only Sure alignments, the kappa statistic (over 7.2 million items, 2 annotators and 2 categories) for agreement was 0.63. When words from the ignore-list were thrown out, this rose to 0.68. Carletta (1995) suggests that kappa values over 0.80 reflect very strong agreement and that kappa values between 0.60 and 0.80 reflect good agreement. 3.2 Machine Translation Experiments In order to establish a baseline alignment model, we used the IBM Model 4 (Brown et al., 1993) and the HMM model (Stephan Vogel and Tillmann, 1996) as implemented in the GIZA++ package (Och and Ney, 2003). We modified this slightly to allow longer inputs and higher fertilities. Such translation models require that input be in sentence-aligned form. In the summarization task, however, one abstract sentence often corresponds to multiple document sentences. In order to overcome this problem, each sentence in an abstract was paired with three sentences from the corresponding document, selected using the techniques described by Marcu (1999). In an informal evaluation, 20 such pairs were ran</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="20554" citStr="Carletta (1995)" startWordPosition="3602" endWordPosition="3603">8 function words and punctuation marks which appeared in the corpus (henceforth called the ignore-list) was assembled. Agreement and precision/recall have been calculated both on all words and on all words that do not appear in the ignore-list. Annotator agreement was strong for Sure alignments and fairly weak for Possible alignments (considering only the 40 independently annotated pairs). When considering only Sure alignments, the kappa statistic (over 7.2 million items, 2 annotators and 2 categories) for agreement was 0.63. When words from the ignore-list were thrown out, this rose to 0.68. Carletta (1995) suggests that kappa values over 0.80 reflect very strong agreement and that kappa values between 0.60 and 0.80 reflect good agreement. 3.2 Machine Translation Experiments In order to establish a baseline alignment model, we used the IBM Model 4 (Brown et al., 1993) and the HMM model (Stephan Vogel and Tillmann, 1996) as implemented in the GIZA++ package (Och and Ney, 2003). We modified this slightly to allow longer inputs and higher fertilities. Such translation models require that input be in sentence-aligned form. In the summarization task, however, one abstract sentence often corresponds t</context>
</contexts>
<marker>Carletta, 1995</marker>
<rawString>Jean Carletta. 1995. Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisychannel model for document compression.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference of the Association of Computational Linguistics (ACL</booktitle>
<marker>Daum´e, Marcu, 2002</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2002a. A noisychannel model for document compression. In Proceedings of the Conference of the Association of Computational Linguistics (ACL 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>A phrase-based HMM. Unpublished; available at http://www.isi.edu/˜hdaume/</title>
<date>2002</date>
<booktitle>docs/daume02pbhmm.ps,</booktitle>
<marker>Daum´e, Marcu, 2002</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2002b. A phrase-based HMM. Unpublished; available at http://www.isi.edu/˜hdaume/ docs/daume02pbhmm.ps, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gauvain</author>
<author>C Lee</author>
</authors>
<title>Maximum aposteriori estimation for multivariate gaussian mixture observations of markov chains.</title>
<date>1994</date>
<journal>IEEE Transactions SAP,</journal>
<pages>2--291</pages>
<contexts>
<context position="12407" citStr="Gauvain and Lee, 1994" startWordPosition="2160" endWordPosition="2163"> + 1) j(t) =max l,pl−1 1   Pr pl−1 1 ,ot−1 1 ,pl.t = t − 1,pl.x = j µ = i(t)ai,jbi,j,ot−1 t i,j(t, t) = E # of transitions i --* j emitting ot  = i(t)ai,jbi,j,ottj(t + 1) t Pr oT1 µ ˆai,j = E [# of transitions i --* j] T T t=t i,j(t, t) t=1 E [# of transitions i --*?] T T jS i,j(t, t) t=1 t=t E # of transitions i --* j with k¯ observed E [# of transitions i --* j] T+1−|k| t=1  T T t=t i,j(t, t) t=1 ˆbi,j,¯k = t+ |k|−1 )i,j(t,t + |k |− 1) k, ot Table 2: Summary of equations for a PBHMM we can adequately express our prior beliefs in their form. (See (Gauvain and Lee, 1994) for the application to standard HMMs.) Applying a Dirichlet prior effectively allows us to add “fake counts” during parameter re-estimation, according to the prior. The prior we choose has a form such that fake counts are added as follows: word-to-word rewrites get an additional count of 2; identity rewrites get an additional count of 4; stemidentity rewrites get an additional count of 3. 2.3 Constructing the PBHMM Given our generative story, we construct a PBHMM to calculate these probabilities efficiently. The structure of the PBHMM for a given document is conceptually simple. We provide va</context>
</contexts>
<marker>Gauvain, Lee, 1994</marker>
<rawString>J. Gauvain and C. Lee. 1994. Maximum aposteriori estimation for multivariate gaussian mixture observations of markov chains. IEEE Transactions SAP, 2:291–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen R McKeown</author>
</authors>
<title>The decomposition of human-written summary sentences.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Conference on Research and Development in Information Retrieval (SIGIR–99),</booktitle>
<location>Berkeley, CA,</location>
<contexts>
<context position="2657" citStr="Jing and McKeown, 1999" startWordPosition="408" endWordPosition="411">aum´e III, Knight, and Marcu (Knight and Marcu, 2002; Daum´e III and Marcu, 2002a) assume that sentences/documents can be summarized only through deletion of contiguous text segments. Knight and Marcu found that from a corpus of 39, 060 abstract sentences, only 1067 sentence extracts existed: a recall of only 2.7%. An alternate techinque employed in a large variety of systems is to treat the summarization problem as a sentence extraction problem. Such systems can be trained either on human constructed extracts or extracts generated automatically from document/abstract pairs (see (Marcu, 1999; Jing and McKeown, 1999) for two such approaches). None of these techniques is adequate. Even for a relatively simple sentence from an abstract, we can see that none of the assumptions listed above holds. In Figure 1, we observe several phenomena: • Alignments can occur at the granularity of words and at the granularity of phrases. • The ordering of phrases in an abstract can be different from the ordering in the document. • Some abstract words do not have direct correspondents in the document, and some document words are never used. It is thus desirable to be able to automatically construct alignments between docume</context>
</contexts>
<marker>Jing, McKeown, 1999</marker>
<rawString>Hongyan Jing and Kathleen R. McKeown. 1999. The decomposition of human-written summary sentences. In Proceedings of the 22nd Conference on Research and Development in Information Retrieval (SIGIR–99), Berkeley, CA, August 15–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Using hidden markov modeling to decompose human-written summaries.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<pages>544</pages>
<contexts>
<context position="642" citStr="Jing, 2002" startWordPosition="91" endWordPosition="92">Document/Abstract Alignment Hal Daum´e III and Daniel Marcu Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 {hdaume,marcu}@isi.edu Abstract We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts. Such alignments are critical for the development of statistical summarization systems that can be trained on large corpora of document/abstract pairs. Our model, which is based on a novel Phrase-Based HMM, outperforms both the Cut &amp; Paste alignment model (Jing, 2002) and models developed in the context of machine translation (Brown et al., 1993). 1 Introduction There are a wealth of document/abstract pairs that statistical summarization systems could leverage to learn how to create novel abstracts. Detailed studies of such pairs (Jing, 2002) show that human abstractors perform a range of very sophisticated operations when summarizing texts, which include reordering, fusion, and paraphrasing. Unfortunately, existing document/abstract alignment models are not powerful enough to capture these operations. To get around directly tackling this problem, research</context>
<context position="23149" citStr="Jing, 2002" startWordPosition="4028" endWordPosition="4029">is the source language and the document sentences are the target language. To alleviate the fertility problem, we also ran experiments with the translation going in the opposite direction. These are called HMMflipped and Model 4-flipped, respectively. These tend to out-perform the original translation direction. In all of these setups, 5 iterations of Model 1 were run, followed by 5 iterations of the HMM model. In the Model 4 cases, 5 iterations of Model 4 were run, following the HMM. 3.3 Cut and Paste Experiments We also tested alignments using the Cut and Paste summary decomposition method (Jing, 2002), based on a non-trainable HMM. Briefly, the Cut and Paste HMM searches for long contiguous blocks of words in the document and abstract that are identical (up to stem). The longest such sequences are aligned. By fixing a length cutoff of n and ignoring sequences of length less than n, one can arbitrarily increase the precision of this method. We found that n = 2 yields the best balance between precision and recall (and the highest F-measure). The results of these experiments are shown under the header “Cut &amp; Paste.” It clearly outperforms all of the MT-based models. 3.4 PBIEMM Experiments Whi</context>
</contexts>
<marker>Jing, 2002</marker>
<rawString>Hongyan Jing. 2002. Using hidden markov modeling to decompose human-written summaries. Computational Linguistics, 28(4):527 – 544, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: A probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>ArtificialIntelligence,</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="2086" citStr="Knight and Marcu, 2002" startWordPosition="315" endWordPosition="318">l 1 (Brown et al., 1993), treat documents and headlines as simple bags of words and learn probabilistic word-based mappings between the words in the documents and the words in the headlines. As our results show, these models are too weak for capturing the operations that are employed by humans in summarizing texts beyond the headline level. Other researchers have developed models that make unreasonable assumptions about the data, which lead to the utilization of a very small percent of available data. For instance, the document and sentence compression models of Daum´e III, Knight, and Marcu (Knight and Marcu, 2002; Daum´e III and Marcu, 2002a) assume that sentences/documents can be summarized only through deletion of contiguous text segments. Knight and Marcu found that from a corpus of 39, 060 abstract sentences, only 1067 sentence extracts existed: a recall of only 2.7%. An alternate techinque employed in a large variety of systems is to treat the summarization problem as a sentence extraction problem. Such systems can be trained either on human constructed extracts or extracts generated automatically from document/abstract pairs (see (Marcu, 1999; Jing and McKeown, 1999) for two such approaches). No</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to sentence compression. ArtificialIntelligence, 139(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Hinrich Schutze</author>
</authors>
<title>Foundations ofStatistical Natural Language Processing.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="9526" citStr="Manning and Schutze, 2000" startWordPosition="1626" endWordPosition="1629">m the alphabet K. j is the probability of beginning in state j. The transition probability ai,j is the probability of transitioning from state i to state j. bi,j,¯k is the probability of emitting (the non-empty) observation sequence k¯ while transitioning from state i to state j. Finally, xt denotes the state after emitting t symbols. The full derivation of the model is too lengthy to include; the interested reader is directed to (Daum´e III and Marcu, 2002b) for the derivations and proofs of the formulae. To assist the reader in understanding the mathematics, we follow the same notation as (Manning and Schutze, 2000). The formulae for the calculations are summarized in Table 2. 2.2.1 Forward algorithm The forward algorithm calculates the probability of an observation sequence. We define j(t) as the probability of being in state j after emitting the first t − 1 symbols (in whatever grouping we want). To do this, as in a traditional HMM, we estimate the  table. When we calculate j(t), we essentially need to choose an appropriate i and t&apos;, which we store in another table, so we can calculate the actual path at the end. 2.2.4 Parameter re-estimation We want to find the model µ which best explains observati</context>
</contexts>
<marker>Manning, Schutze, 2000</marker>
<rawString>Christopher Manning and Hinrich Schutze. 2000. Foundations ofStatistical Natural Language Processing. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The automatic construction of large-scale corpora for summarization research.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Conference on Research and Development in Information Retrieval (SIGIR–99),</booktitle>
<pages>137--144</pages>
<location>Berkeley, CA,</location>
<contexts>
<context position="2632" citStr="Marcu, 1999" startWordPosition="406" endWordPosition="407">n models of Daum´e III, Knight, and Marcu (Knight and Marcu, 2002; Daum´e III and Marcu, 2002a) assume that sentences/documents can be summarized only through deletion of contiguous text segments. Knight and Marcu found that from a corpus of 39, 060 abstract sentences, only 1067 sentence extracts existed: a recall of only 2.7%. An alternate techinque employed in a large variety of systems is to treat the summarization problem as a sentence extraction problem. Such systems can be trained either on human constructed extracts or extracts generated automatically from document/abstract pairs (see (Marcu, 1999; Jing and McKeown, 1999) for two such approaches). None of these techniques is adequate. Even for a relatively simple sentence from an abstract, we can see that none of the assumptions listed above holds. In Figure 1, we observe several phenomena: • Alignments can occur at the granularity of words and at the granularity of phrases. • The ordering of phrases in an abstract can be different from the ordering in the document. • Some abstract words do not have direct correspondents in the document, and some document words are never used. It is thus desirable to be able to automatically construct </context>
<context position="21369" citStr="Marcu (1999)" startWordPosition="3728" endWordPosition="3729">e alignment model, we used the IBM Model 4 (Brown et al., 1993) and the HMM model (Stephan Vogel and Tillmann, 1996) as implemented in the GIZA++ package (Och and Ney, 2003). We modified this slightly to allow longer inputs and higher fertilities. Such translation models require that input be in sentence-aligned form. In the summarization task, however, one abstract sentence often corresponds to multiple document sentences. In order to overcome this problem, each sentence in an abstract was paired with three sentences from the corresponding document, selected using the techniques described by Marcu (1999). In an informal evaluation, 20 such pairs were randomly extracted and evaluated by a human. Each pair was ranked as 0 (document sentences contain little-to-none of the information in the abstract sentence), 1 (document sentences contain some of the information in the abstract sentence) or 2 (document sentences contain all of the information). Of the twenty random examples, none were labeled as 0; five were labeled as 1; and 15 were labeled as 2, giving a mean rating of 1.75. We ran experiments using the document sentences as both the source and the target language in GIZA++. When document sen</context>
<context position="24313" citStr="Marcu, 1999" startWordPosition="4219" endWordPosition="4220"> the MT-based models. 3.4 PBIEMM Experiments While the PBHMM is based on a dynamic programming algorithm, the effective search space in this model is enormous, even for moderately sized document/abstract pairs. We selected the 2000 shortest document/abstract pairs from the Ziff-Davis corpus for training; however, only 12 of the hand-annotated documents were included in this set, so we additionally added the other 33 hand-annotate documents to this set, yielding 2033 document/abstract pairs. We then performed sentence extraction on this corpus exactly as in the MT case, using the technique of (Marcu, 1999). The relevant data for this corpus is in Table 4. We also restrict the state-space with a beam, sized at 50% of the unrestricted state-space. The PBHMM system was then trained on this abstract/extract corpus. The precision/recall results are shown in Table 5. Under the methodology for combining the two human annotations by taking the union, either of the human scores would achieve a System SoftP Recall SoftF Human, 0.727 0.746 0.736 Humane 0.680 0.695 0.687 HMM 0.120 0.260 0.164 Model 4 0.117 0.260 0.161 HMM-flipped 0.295 0.250 0.271 Model 4-flipped 0.280 0.247 0.262 Cut &amp; Paste 0.349 0.379 0</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Daniel Marcu. 1999. The automatic construction of large-scale corpora for summarization research. In Proceedings of the 22nd Conference on Research and Development in Information Retrieval (SIGIR–99), pages 137–144, Berkeley, CA, August 15–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3456" citStr="Och and Ney, 2003" startWordPosition="540" endWordPosition="543">In Figure 1, we observe several phenomena: • Alignments can occur at the granularity of words and at the granularity of phrases. • The ordering of phrases in an abstract can be different from the ordering in the document. • Some abstract words do not have direct correspondents in the document, and some document words are never used. It is thus desirable to be able to automatically construct alignments between documents and their abstracts, so that the correspondences between the pairs are obvious. One might be initially tempted to use readily-available machine translation systems like GIZA++ (Och and Ney, 2003) to perform such Connecting Point has become the single largest Mac retailer after tripling it ’s Macintosh sales since January 1989 . Connecting Point Systems tripled it ’s sales of Apple Macintosh systems since last January . It is now the single largest seller of Macintosh . Figure 1: Example abstract/text alignment. alignments. However, as we will show, the alignments produced by such a system are inadequate for this task. The solution that we propose to this problem is an alignment model based on a novel mathematical structure we call the Phrase-Based HMM. 2 Designing a Model As observed </context>
<context position="18951" citStr="Och and Ney, 2003" startWordPosition="3326" endWordPosition="3329">ators align them. The first five were annotated separately and then discussed; the last 40 were done independently. Annotators were asked to perform phrase-tophrase alignments between abstracts and documents and to classify each alignment as either possible P or sure S, where P C_ S. In order to calculate scores for phrase alignments, we convert all phrase alignments to word alignments. That is, if we have an alignment between phrases A and B, then this induces word alignments between a and b for all words a E A and b E B. Given an alignment A, we could calculate precision and recall as (see (Och and Ney, 2003)): Precision = |AP ||A |Recall =|AS| |S| One problem with these definitions is that phrasebased models are fond of making phrases. That is, when given an abstract containing “the man” and a document also containing “the man,” a human may prefer to align “the” to “the” and “man” to “man.” However, a phrase-based model will almost always prefer to align the entire phrase “the man” to “the man.” This is because it results in fewer probabilities being multiplied together. To compensate for this, we define soft precision (SoftP in the tables) by counting alignments where “a b” is aligned to “a b”</context>
<context position="20930" citStr="Och and Ney, 2003" startWordPosition="3662" endWordPosition="3665">y annotated pairs). When considering only Sure alignments, the kappa statistic (over 7.2 million items, 2 annotators and 2 categories) for agreement was 0.63. When words from the ignore-list were thrown out, this rose to 0.68. Carletta (1995) suggests that kappa values over 0.80 reflect very strong agreement and that kappa values between 0.60 and 0.80 reflect good agreement. 3.2 Machine Translation Experiments In order to establish a baseline alignment model, we used the IBM Model 4 (Brown et al., 1993) and the HMM model (Stephan Vogel and Tillmann, 1996) as implemented in the GIZA++ package (Och and Ney, 2003). We modified this slightly to allow longer inputs and higher fertilities. Such translation models require that input be in sentence-aligned form. In the summarization task, however, one abstract sentence often corresponds to multiple document sentences. In order to overcome this problem, each sentence in an abstract was paired with three sentences from the corresponding document, selected using the techniques described by Marcu (1999). In an informal evaluation, 20 such pairs were randomly extracted and evaluated by a human. Each pair was ranked as 0 (document sentences contain little-to-none</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney Stephan Vogel</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING ’96: The 16th Int. Conf. on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="20873" citStr="Vogel and Tillmann, 1996" startWordPosition="3652" endWordPosition="3655">ak for Possible alignments (considering only the 40 independently annotated pairs). When considering only Sure alignments, the kappa statistic (over 7.2 million items, 2 annotators and 2 categories) for agreement was 0.63. When words from the ignore-list were thrown out, this rose to 0.68. Carletta (1995) suggests that kappa values over 0.80 reflect very strong agreement and that kappa values between 0.60 and 0.80 reflect good agreement. 3.2 Machine Translation Experiments In order to establish a baseline alignment model, we used the IBM Model 4 (Brown et al., 1993) and the HMM model (Stephan Vogel and Tillmann, 1996) as implemented in the GIZA++ package (Och and Ney, 2003). We modified this slightly to allow longer inputs and higher fertilities. Such translation models require that input be in sentence-aligned form. In the summarization task, however, one abstract sentence often corresponds to multiple document sentences. In order to overcome this problem, each sentence in an abstract was paired with three sentences from the corresponding document, selected using the techniques described by Marcu (1999). In an informal evaluation, 20 such pairs were randomly extracted and evaluated by a human. Each pair w</context>
</contexts>
<marker>Vogel, Tillmann, 1996</marker>
<rawString>Hermann Ney Stephan Vogel and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In COLING ’96: The 16th Int. Conf. on Computational Linguistics, pages 836– 841.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>