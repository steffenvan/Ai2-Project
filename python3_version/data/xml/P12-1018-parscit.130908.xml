<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000084">
<title confidence="0.99437">
Machine Translation without Words through Substring Alignment
</title>
<author confidence="0.999733">
Graham Neubig1,2, Taro Watanabe2, Shinsuke Mori1, Tatsuya Kawahara1
</author>
<affiliation confidence="0.961125333333333">
1Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
2National Institute of Information and Communication Technology
</affiliation>
<address confidence="0.770406">
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
</address>
<sectionHeader confidence="0.971554" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99949">
In this paper, we demonstrate that accu-
rate machine translation is possible without
the concept of “words,” treating MT as a
problem of transformation between character
strings. We achieve this result by applying
phrasal inversion transduction grammar align-
ment techniques to character strings to train
a character-based translation model, and us-
ing this in the phrase-based MT framework.
We also propose a look-ahead parsing algo-
rithm and substring-informed prior probabil-
ities to achieve more effective and efficient
alignment. In an evaluation, we demonstrate
that character-based translation can achieve
results that compare to word-based systems
while effectively translating unknown and un-
common words over several language pairs.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990982229166667">
Traditionally, the task of statistical machine trans-
lation (SMT) is defined as translating a source sen-
tence fJ1 = {f1, ... , fJ} to a target sentence eI1 =
{e1, ..., eI}, where each element of fJ1 and eI1 is
assumed to be a word in the source and target lan-
guages. However, the definition of a “word” is of-
ten problematic. The most obvious example of this
lies in languages that do not separate words with
white space such as Chinese, Japanese, or Thai, in
which the choice of a segmentation standard has
a large effect on translation accuracy (Chang et
al., 2008). Even for languages with explicit word
The first author is now affiliated with the Nara Institute of Sci-
ence and Technology.
boundaries, all machine translation systems perform
at least some precursory form of tokenization, split-
ting punctuation and words to prevent the sparsity
that would occur if punctuated and non-punctuated
words were treated as different entities. Sparsity
also manifests itself in other forms, including the
large vocabularies produced by morphological pro-
ductivity, word compounding, numbers, and proper
names. A myriad of methods have been proposed
to handle each of these phenomena individually,
including morphological analysis, stemming, com-
pound breaking, number regularization, optimizing
word segmentation, and transliteration, which we
outline in more detail in Section 2.
These difficulties occur because we are translat-
ing sequences of words as our basic unit. On the
other hand, Vilar et al. (2007) examine the possibil-
ity of instead treating each sentence as sequences of
characters to be translated. This method is attrac-
tive, as it is theoretically able to handle all sparsity
phenomena in a single unified framework, but has
only been shown feasible between similar language
pairs such as Spanish-Catalan (Vilar et al., 2007),
Swedish-Norwegian (Tiedemann, 2009), and Thai-
Lao (Sornlertlamvanich et al., 2008), which have
a strong co-occurrence between single characters.
As Vilar et al. (2007) state and we confirm, accu-
rate translations cannot be achieved when applying
traditional translation techniques to character-based
translation for less similar language pairs.
In this paper, we propose improvements to the
alignment process tailored to character-based ma-
chine translation, and demonstrate that it is, in fact,
possible to achieve translation accuracies that ap-
</bodyText>
<page confidence="0.982328">
165
</page>
<note confidence="0.9857695">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 165–174,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999939875">
proach those of traditional word-based systems us-
ing only character strings. We draw upon recent
advances in many-to-many alignment, which allows
for the automatic choice of the length of units to
be aligned. As these units may be at the charac-
ter, subword, word, or multi-word phrase level, we
conjecture that this will allow for better character
alignments than one-to-many alignment techniques,
and will allow for better translation of uncommon
words than traditional word-based models by break-
ing down words into their component parts.
We also propose two improvements to the many-
to-many alignment method of Neubig et al. (2011).
One barrier to applying many-to-many alignment
models to character strings is training cost. In the
inversion transduction grammar (ITG) framework
(Wu, 1997), which is widely used in many-to-many
alignment, search is cumbersome for longer sen-
tences, a problem that is further exacerbated when
using characters instead of words as the basic unit.
As a step towards overcoming this difficulty, we in-
crease the efficiency of the beam-search technique of
Saers et al. (2009) by augmenting it with look-ahead
probabilities in the spirit of A* search. Secondly,
we describe a method to seed the search process us-
ing counts of all substring pairs in the corpus to bias
the phrase alignment model. We do this by defining
prior probabilities based on these substring counts
within the Bayesian phrasal ITG framework.
An evaluation on four language pairs with differ-
ing morphological properties shows that for distant
language pairs, character-based SMT can achieve
translation accuracy comparable to word-based sys-
tems. In addition, we perform ablation studies,
showing that these results were not possible with-
out the proposed enhancements to the model. Fi-
nally, we perform a qualitative analysis, which finds
that character-based translation can handle unseg-
mented text, conjugation, and proper names in a uni-
fied framework with no additional processing.
</bodyText>
<sectionHeader confidence="0.99586" genericHeader="related work">
2 Related Work on Data Sparsity in SMT
</sectionHeader>
<bodyText confidence="0.999938377358491">
As traditional SMT systems treat all words as single
tokens without considering their internal structure,
major problems of data sparsity occur for less fre-
quent tokens. In fact, it has been shown that there
is a direct negative correlation between vocabulary
size (and thus sparsity) of a language and transla-
tion accuracy (Koehn, 2005). Sparsity causes trou-
ble for alignment models, both in the form of incor-
rectly aligned uncommon words, and in the form of
garbage collection, where uncommon words in one
language are incorrectly aligned to large segments
of the sentence in the other language (Och and Ney,
2003). Unknown words are also a problem during
the translation process, and the default approach is
to map them as-is into the target sentence.
This is a major problem in agglutinative lan-
guages such as Finnish or compounding languages
such as German. Previous works have attempted to
handle morphology, decompounding and regulariza-
tion through lemmatization, morphological analysis,
or unsupervised techniques (Nießen and Ney, 2000;
Brown, 2002; Lee, 2004; Goldwater and McClosky,
2005; Talbot and Osborne, 2006; Mermer and Akın,
2010; Macherey et al., 2011). It has also been noted
that it is more difficult to translate into morpho-
logically rich languages, and methods for modeling
target-side morphology have attracted interest in re-
cent years (Bojar, 2007; Subotin, 2011).
Another source of data sparsity that occurs in all
languages is proper names, which have been handled
by using cognates or transliteration to improve trans-
lation (Knight and Graehl, 1998; Kondrak et al.,
2003; Finch and Sumita, 2007), and more sophisti-
cated methods for named entity translation that com-
bine translation and transliteration have also been
proposed (Al-Onaizan and Knight, 2002).
Choosing word units is also essential for creat-
ing good translation results for languages that do
not explicitly mark word boundaries, such as Chi-
nese, Japanese, and Thai. A number of works have
dealt with this word segmentation problem in trans-
lation, mainly focusing on Chinese-to-English trans-
lation (Bai et al., 2008; Chang et al., 2008; Zhang et
al., 2008b; Chung and Gildea, 2009; Nguyen et al.,
2010), although these works generally assume that a
word segmentation exists in one language (English)
and attempt to optimize the word segmentation in
the other language (Chinese).
We have enumerated these related works to
demonstrate the myriad of data sparsity problems
and proposed solutions. Character-based transla-
tion has the potential to handle all of the phenom-
ena in the previously mentioned research in a single
</bodyText>
<page confidence="0.997995">
166
</page>
<bodyText confidence="0.9999419">
unified framework, requiring no language specific
tools such as morphological analyzers or word seg-
menters. However, while the approach is attractive
conceptually, previous research has only been shown
effective for closely related language pairs (Vilar et
al., 2007; Tiedemann, 2009; Sornlertlamvanich et
al., 2008). In this work, we propose effective align-
ment techniques that allow character-based transla-
tion to achieve accurate translation results for both
close and distant language pairs.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="method">
3 Alignment Methods
</sectionHeader>
<bodyText confidence="0.999494357142857">
SMT systems are generally constructed from a par-
allel corpus consisting of target language sentences
£ and source language sentences T. The first step
of training is to find alignments A for the words in
each sentence pair.
We represent our target and source sentences as
e�1 and fJ1. ei and fj represent single elements of
the target and source sentences respectively. These
may be words in word-based alignment models or
single characters in character-based alignment mod-
els.l We define our alignment as aK , where each
element is a span ak = (s, t, u, v) indicating that the
target string es, ... , et and source string fu, ... , f„
are aligned to each-other.
</bodyText>
<subsectionHeader confidence="0.989342">
3.1 One-to-Many Alignment
</subsectionHeader>
<bodyText confidence="0.99659996">
The most well-known and widely-used models for
bitext alignment are for one-to-many alignment, in-
cluding the IBM models (Brown et al., 1993) and
HMM alignment model (Vogel et al., 1996). These
models are by nature directional, attempting to find
the alignments that maximize the conditional prob-
ability of the target sentence P(ei|fJ1, aK ). For
computational reasons, the IBM models are re-
stricted to aligning each word on the target side to
a single word on the source side. In the formal-
ism presented above, this means that each ei must
be included in at most one span, and for each span
u = v. Traditionally, these models are run in both
directions and combined using heuristics to create
many-to-many alignments (Koehn et al., 2003).
However, in order for one-to-many alignment
methods to be effective, each fj must contain
&apos;Some previous work has also performed alignment using
morphological analyzers to normalize or split the sentence into
morpheme streams (Corston-Oliver and Gamon, 2004).
enough information to allow for effective alignment
with its corresponding elements in ei. While this is
often the case in word-based models, for character-
based models this assumption breaks down, as there
is often no clear correspondence between characters.
</bodyText>
<subsectionHeader confidence="0.997305">
3.2 Many-to-Many Alignment
</subsectionHeader>
<bodyText confidence="0.999867">
On the other hand, in recent years, there have been
advances in many-to-many alignment techniques
that are able to align multi-element chunks on both
sides of the translation (Marcu and Wong, 2002;
DeNero et al., 2008; Blunsom et al., 2009; Neu-
big et al., 2011). Many-to-many methods can be ex-
pected to achieve superior results on character-based
alignment, as the aligner can use information about
substrings, which may correspond to letters, mor-
phemes, words, or short phrases.
Here, we focus on the model presented by Neu-
big et al. (2011), which uses Bayesian inference in
the phrasal inversion transduction grammar (ITG,
Wu (1997)) framework. ITGs are a variety of syn-
chronous context free grammar (SCFG) that allows
for many-to-many alignment to be achieved in poly-
nomial time through the process of biparsing, which
we explain more in the following section. Phrasal
ITGs are ITGs that allow for non-terminals that can
emit phrase pairs with multiple elements on both
the source and target sides. It should be noted
that there are other many-to-many alignment meth-
ods that have been used for simultaneously discov-
ering morphological boundaries over multiple lan-
guages (Snyder and Barzilay, 2008; Naradowsky
and Toutanova, 2011), but these have generally been
applied to single words or short phrases, and it is not
immediately clear that they will scale to aligning full
sentences.
</bodyText>
<sectionHeader confidence="0.993769" genericHeader="method">
4 Look-Ahead Biparsing
</sectionHeader>
<bodyText confidence="0.999954888888889">
In this work, we experiment with the alignment
method of Neubig et al. (2011), which can achieve
competitive accuracy with a much smaller phrase ta-
ble than traditional methods. This is important in
the character-based translation context, as we would
like to use phrases that contain large numbers of
characters without creating a phrase table so large
that it cannot be used in actual decoding. In this
framework, training is performed using sentence-
</bodyText>
<page confidence="0.993783">
167
</page>
<figureCaption confidence="0.977582166666667">
Figure 1: (a) A chart with inside probabilities in boxes
and forward/backward probabilities marking the sur-
rounding arrows. (b) Spans with corresponding look-
aheads added, and the minimum probability underlined.
Lightly and darkly shaded spans will be trimmed when
the beam is log(P) &gt; −3 and log(P) &gt; −6 respectively.
</figureCaption>
<bodyText confidence="0.99977775">
wise block sampling, acquiring a sample for each
sentence by first performing bottom-up biparsing to
create a chart of probabilities, then performing top-
down sampling of a new tree based on the probabil-
ities in this chart.
An example of a chart used in this parsing can
be found in Figure 1 (a). Within each cell of the
chart spanning ets and fv u is an “inside” probabil-
ity I(as,t,u,v). This probability is the combination
of the generative probability of each phrase pair
Pt(ets, fvu) as well as the sum the probabilities over
all shorter spans in straight and inverted order2
</bodyText>
<equation confidence="0.9804218">
I(as,t,u,v) = Pt(ets, fuv)
+ E E Px(str)I(as,S,u,U)I(aS,t,U,v)
s&lt;S&lt;t u&lt;U&lt;v
+ E E Px(inv)I(as,S,U,v)I(aS,t,u,U)
s&lt;S&lt;t u&lt;U&lt;v
</equation>
<bodyText confidence="0.999046">
where Px(str) and Px(inv) are the probability of
straight and inverted ITG productions.
While the exact calculation of these probabilities
can be performed in O(n6) time, where n is the
</bodyText>
<footnote confidence="0.6711175">
2Pt can be specified according to Bayesian statistics as de-
scribed by Neubig et al. (2011).
</footnote>
<bodyText confidence="0.989149291666667">
length of the sentence, this is impractical for all but
the shortest sentences. Thus it is necessary to use
methods to reduce the search space such as beam-
search based chart parsing (Saers et al., 2009) or
slice sampling (Blunsom and Cohn, 2010).3
In this section we propose the use of a look-ahead
probability to increase the efficiency of this chart
parsing. Taking the example of Saers et al. (2009),
spans are pushed onto a different queue based on
their size, and queues are processed in ascending or-
der of size. Agendas can further be trimmed based
on a histogram beam (Saers et al., 2009) or probabil-
ity beam (Neubig et al., 2011) compared to the best
hypothesis a. In other words, we have a queue dis-
cipline based on the inside probability, and all spans
ak where I(ak) &lt; cI(&amp;) are pruned. c is a constant
describing the width of the beam, and a smaller con-
stant probability will indicate a wider beam.
This method is insensitive to the existence of
competing hypotheses when performing pruning.
Figure 1 (a) provides an example of why it is unwise
to ignore competing hypotheses during beam prun-
ing. Particularly, the alignment “les/1960s” com-
petes with the high-probability alignment “les/the,”
so intuitively should be a good candidate for prun-
ing. However its probability is only slightly higher
than “ann´ees/1960s,” which has no competing hy-
potheses and thus should not be trimmed.
In order to take into account competing hypothe-
ses, we can use for our queue discipline not only the
inside probability I(ak), but also the outside proba-
bility O(ak), the probability of generating all spans
other than ak, as in A* search for CFGs (Klein and
Manning, 2003), and tic-tac-toe pruning for word-
based ITGs (Zhang and Gildea, 2005). As the cal-
culation of the actual outside probability O(ak) is
just as expensive as parsing itself, it is necessary to
approximate this with heuristic function O* that can
be calculated efficiently.
Here we propose a heuristic function that is de-
signed specifically for phrasal ITGs and is com-
putable with worst-case complexity of n2, compared
with the n3 amortized time of the tic-tac-toe pruning
3Applying beam-search before sampling will sample from
an improper distribution, although Metropolis-in-Gibbs sam-
pling (Johnson et al., 2007) can be used to compensate. How-
ever, we found that this had no significant effect on results, so
we omit the Metropolis-in-Gibbs step for experiments.
</bodyText>
<page confidence="0.993082">
168
</page>
<bodyText confidence="0.9998165">
algorithm described by (Zhang et al., 2008a). Dur-
ing the calculation of the phrase generation proba-
bilities Pt, we save the best inside probability I∗ for
each monolingual span.
</bodyText>
<equation confidence="0.969028">
I∗ e (s,t) = max
{˜a=h˜s,˜t,˜u,˜vi;˜s=s,˜t=t}
I∗f (u, v) = max
{˜a=h˜s,˜t,˜u,˜vi;˜u=u,˜v=v}
</equation>
<bodyText confidence="0.976973466666667">
For each language independently, we calculate for-
ward probabilities α and backward probabilities β.
For example, αe(s) is the maximum probability of
the span (0, s) of e that can be created by concate-
nating together consecutive values of I∗e:
I∗e (0, S1)I∗e (S1, S2) ... I∗e (Sx, s).
Backwards probabilities and probabilities over f can
be defined similarly. These probabilities are calcu-
lated for e and f independently, and can be calcu-
lated in n2 time by processing each α in ascending
order, and each β in descending order in a fashion
similar to that of the forward-backward algorithm.
Finally, for any span, we define the outside heuristic
as the minimum of the two independent look-ahead
probabilities over each language
</bodyText>
<equation confidence="0.997468">
O∗(as,t,u,v) = min(αe(s) ∗ βe(t), αf(u) ∗ βf(v)).
</equation>
<bodyText confidence="0.999888222222222">
Looking again at Figure 1 (b), it can be seen
that the relative probability difference between the
highest probability span “les/the” and the spans
“ann´ees/1960s” and “60/1960s” decreases, allowing
for tighter beam pruning without losing these good
hypotheses. In contrast, the relative probability of
“les/1960s” remains low as it is in conflict with a
high-probability alignment, allowing it to be dis-
carded.
</bodyText>
<sectionHeader confidence="0.959243" genericHeader="method">
5 Substring Prior Probabilities
</sectionHeader>
<bodyText confidence="0.9999393">
While the Bayesian phrasal ITG framework uses
the previously mentioned phrase distribution Pt dur-
ing search, it also allows for definition of a phrase
pair prior probability Pprior(ets, fvu), which can ef-
ficiently seed the search process with a bias towards
phrase pairs that satisfy certain properties. In this
section, we overview an existing method used to cal-
culate these prior probabilities, and also propose a
new way to calculate priors based on substring co-
occurrence statistics.
</bodyText>
<subsectionHeader confidence="0.90698">
5.1 Word-based Priors
</subsectionHeader>
<bodyText confidence="0.9999405">
Previous research on many-to-many translation has
used IBM model 1 probabilities to bias phrasal
alignments so that phrases whose member words are
good translations are also aligned. As a representa-
tive of this existing method, we adopt a base mea-
sure similar to that used by DeNero et al. (2008):
</bodyText>
<equation confidence="0.999541">
Pm1(e,f) =M0(e,f)Ppois(|e|;λ)Ppois(|f|;λ)
M0(e,f) =(Pm1(f|e)Puni(e)Pm1(e|f)Puni(f))
</equation>
<bodyText confidence="0.9999051">
Ppois is the Poisson distribution with the average
length parameter λ, which we set to 0.01. Pm1 is the
word-based (or character-based) Model 1 probabil-
ity, which can be efficiently calculated using the dy-
namic programming algorithm described by Brown
et al. (1993). However, for reasons previously stated
in Section 3, these methods are less satisfactory
when performing character-based alignment, as the
amount of information contained in a character does
not allow for proper alignment.
</bodyText>
<subsectionHeader confidence="0.999781">
5.2 Substring Co-occurrence Priors
</subsectionHeader>
<bodyText confidence="0.999939136363637">
Instead, we propose a method for using raw sub-
string co-occurrence statistics to bias alignments to-
wards substrings that often co-occur in the entire
training corpus. This is similar to the method of
Cromieres (2006), but instead of using these co-
occurrence statistics as a heuristic alignment crite-
rion, we incorporate them as a prior probability in
a statistical model that can take into account mutual
exclusivity of overlapping substrings in a sentence.
We define this prior probability using three counts
over substrings c(e), c(f), and c(e, f). c(e) and
c(f) count the total number of sentences in which
the substrings e and f occur respectively. c(e, f) is
a count of the total number of sentences in which the
substring e occurs on the target side, and f occurs
on the source side. We perform the calculation of
these statistics using enhanced suffix arrays, a data
structure that can efficiently calculate all substrings
in a corpus (Abouelhoda et al., 2004).a
While suffix arrays allow for efficient calculation
of these statistics, storing all co-occurrence counts
c(e, f) is an unrealistic memory burden for larger
</bodyText>
<footnote confidence="0.892021">
4Using the open-source implementation esaxx http://
code.google.com/p/esaxx/
</footnote>
<figure confidence="0.7361425">
Pt(a)
Pt(a)
αe(s) = max
{S1,...,S.}
1
2.
</figure>
<page confidence="0.988436">
169
</page>
<bodyText confidence="0.999937222222222">
corpora. In order to reduce the amount of mem-
ory used, we discount every count by a constant d,
which we set to 5. This has a dual effect of reducing
the amount of memory needed to hold co-occurrence
counts by removing values for which c(e, f) &lt; d, as
well as preventing over-fitting of the training data. In
addition, we heuristically prune values for which the
conditional probabilities P(e|f) or P(f|e) are less
than some fixed value, which we set to 0.1 for the
reported experiments.
To determine how to combine c(e), c(f), and
c(e, f) into prior probabilities, we performed pre-
liminary experiments testing methods proposed by
previous research including plain co-occurrence
counts, the Dice coefficient, and χ-squared statistics
(Cromieres, 2006), as well as a new method of defin-
ing substring pair probabilities to be proportional to
bidirectional conditional probabilities
</bodyText>
<equation confidence="0.987843333333333">
Pcooc(e, f) = Pcooc(e|f)Pcooc(f|e)/Z
Cc(e, f) − dl �c(e, f) − dl /Z
c(f) − d c(e) − d
</equation>
<bodyText confidence="0.860828466666667">
for all substring pairs where c(e, f) &gt; d an
d where
Z is a normalization term equal to
bility according to
As the prior is only sup-
posed to bias the model towards good solutions and
not explicitly rule out any possibilities, we linearly
interpolate the co-occurrence probability with the
one-to-many Model 1 probability, which will give
at least some probability mass to all substring pairs
+
We put a Dirichlet prior
= 1) on the interpolation
coefficient
and learn it duri
</bodyText>
<equation confidence="0.9768914">
Pcooc.
PprZor(e,f)=λPcooc(e,f)
(1 − λ)Pm1(e,f).
(α
λ
</equation>
<bodyText confidence="0.631987">
ng training.
</bodyText>
<sectionHeader confidence="0.998858" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.864251">
mental settings.
de-en fi-en fr-en ja-en
</bodyText>
<table confidence="0.99978825">
TM (en) 3.10M 2.77M 2.13M
TM (other) 3.05M 2.34M
LM (en)
LM (other) 15.3M11.3M 15.6M 11.9M
Tune (en) 58.7k 58.7k 58.7k 30.8k
Tune (other) 55.1k 42.0k 67.3k 34.4k
Test (en) 58.0k 58.0k 58.0k 26.6k
Test (other) k
</table>
<tableCaption confidence="0.899087">
1:
d testing.
Table
The number of words in each corpus for TM and
LM training, tuning, an
</tableCaption>
<bodyText confidence="0.999364666666667">
shared task on machine
We also did
experiments with Japanese-English Wikipedia arti-
cles from the Kyoto Free Translation Task (Neu-
big, 2011) using the designated training and tuning
sets, and reporting results on the test set. These lan-
guages were chosen as they have a variety of inter-
esting characteristics. French has some inflection,
but among the test languages has the strongest one-
to-one correspondence with English, and is gener-
ally considered easy to translate. German has many
compound words, which must be broken apart to
translate properly into English. Finnish is an ag-
glutinative language with extremely rich morphol-
ogy, resulting in long words and the largest vocab-
ulary of the languages in EuroParl. Japanese does
not have any clear word boundaries, and uses logo-
graphic characters, which contain more information
than phonetic characters.
With regards to data preparation, the EuroParl
data was pre-tokenized, so we simply used the to-
kenized data as-is for the training and evaluation of
all models. For word-based translation in the Kyoto
task, training was performed using the provided tok-
enization scripts. For character-based translation, no
tokenization was performed, using the original text
for both training an
</bodyText>
<equation confidence="0.575971">
translation.5
</equation>
<bodyText confidence="0.993321083333333">
d decoding. For both tasks, we
selected as training data all sentences for which both
Pcooc(e|f)Pcooc(f|e).
The experiments showed that the bidirectional con-
ditional probability method gave significantly better
results than all other methods, so we adopt this for
the remainder of our experiments.
It should be noted that as we are using discount-
ing, many substring pairs will be given zero proba-
In order to test the effectiveness of character-based
translation, we performed experiments over a variety
of language pairs and experi
</bodyText>
<equation confidence="0.9201455">
2.80M
2.56M 2.23M
15.5M 13.8M 11.5
16.0M
M
54.3k 41.4k 66.2k 28.5
</equation>
<subsectionHeader confidence="0.958982">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9860804">
We use a combination of four languages with En-
glish, using freely available data. We selected
French-English, German-English, Finnish-English
data from EuroParl (Koehn, 2005), with develop-
ment and test sets designated for the 2005 ACL
</bodyText>
<footnote confidence="0.72362">
5http://statmt.org/wpt05/mt-shared-task
</footnote>
<equation confidence="0.965196333333333">
=
�Z =
�e�f�c�e�f����
</equation>
<page confidence="0.985766">
170
</page>
<table confidence="0.9996455">
de-en fi-en fr-en ja-en
GIZA-word 24.58 / 64.28 / 30.43 20.41 / 60.01 / 27.89 30.23 / 68.79 / 34.20 17.95 / 56.47 / 24.70
ITG-word 23.87 / 64.89 / 30.71 20.83 / 61.04 / 28.46 29.92 / 68.64 / 34.29 17.14 / 56.60 / 24.89
GIZA-char 08.05 / 45.01 / 15.35 06.91 / 41.62 / 14.39 11.05 / 48.23 / 17.80 09.46 / 49.02 / 18.34
ITG-char 21.79 / 64.47 / 30.12 18.38 / 62.44 / 28.94 26.70 / 66.76 / 32.47 15.84 / 58.41 / 24.58
en-de en-fi en-fr en-ja
GIZA-word 17.94 / 62.71 / 37.88 13.22 / 58.50 / 27.03 32.19 / 69.20 / 52.39 20.79 / 27.01 / 38.41
ITG-word 17.47 / 63.18 / 37.79 13.12 / 59.27 / 27.09 31.66 / 69.61 / 51.98 20.26 / 28.34 / 38.34
GIZA-char 06.17 / 41.04 / 19.90 04.58 / 35.09 / 11.76 10.31 / 42.84 / 25.06 01.48 / 00.72 / 06.67
ITG-char 15.35 / 61.95 / 35.45 12.14 / 59.02 / 25.31 27.74 / 67.44 / 48.56 17.90 / 28.46 / 35.71
</table>
<tableCaption confidence="0.99253">
Table 2: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal
ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant differ-
ence from the best system according to the bootstrap resampling method at p = 0.05 (Koehn, 2004).
</tableCaption>
<bodyText confidence="0.991645666666667">
source and target were 100 characters or less,6 the
total size of which is shown in Table 1. In character-
based translation, white spaces between words were
treated as any other character and not given any spe-
cial treatment. Evaluation was performed on tok-
enized and lower-cased data.
For alignment, we use the GIZA++ implementa-
tion of one-to-many alignment� and the pialign im-
plementation of the phrasal ITG models$ modified
with the proposed improvements. For GIZA++, we
used the default settings for word-based alignment,
but used the HMM model for character-based align-
ment to allow for alignment of longer sentences.
For pialign, default settings were used except for
character-based ITG alignment, which used a prob-
ability beam of 10−4 instead 10−10.9 For decoding,
we use the Moses decoder,10 using the default set-
tings except for the stack size, which we set to 1000
instead of 200. Minimum error rate training was per-
formed to maximize word-based BLEU score for all
systems.11 For language models, word-based trans-
lation uses a word 5-gram model, and character-
based translation uses a character 12-gram model,
both smoothed using interpolated Kneser-Ney.
</bodyText>
<footnote confidence="0.908054">
6100 characters is an average of 18.8 English words
7http://code.google.com/p/giza-pp/
8http://phontron.com/pialign/
9Improvement by using a beam larger than 10−4 was
marginal, especially with co-occurrence prior probabilities.
10http://statmt.org/moses/
11We chose this set-up to minimize the effect of tuning crite-
rion on our experiments, although it does indicate that we must
have access to tokenized data for the development set.
</footnote>
<subsectionHeader confidence="0.999881">
6.2 Quantitative Evaluation
</subsectionHeader>
<bodyText confidence="0.9999373">
Table 2 presents a quantitative analysis of the trans-
lation results for each of the proposed methods. As
previous research has shown that it is more diffi-
cult to translate into morphologically rich languages
than into English (Koehn, 2005), we perform exper-
iments translating in both directions for all language
pairs. We evaluate translation quality using BLEU
score (Papineni et al., 2002), both on the word and
character level (with n = 4), as well as METEOR
(Denkowski and Lavie, 2011) on the word level.
It can be seen that character-based translation
with all of the proposed alignment improvements
greatly exceeds character-based translation using
one-to-many alignment, confirming that substring-
based information is necessary for accurate align-
ments. When compared with word-based trans-
lation, character-based translation achieves better,
comparable, or inferior results on character-based
BLEU, comparable or inferior results on METEOR,
and inferior results on word-based BLEU. The dif-
ferences between the evaluation metrics are due to
the fact that character-based translation often gets
words mostly correct other than one or two letters.
These are given partial credit by character-based
BLEU (and to a lesser extent METEOR), but marked
entirely wrong by word-based BLEU.
Interestingly, for translation into English,
character-based translation achieves higher ac-
curacy compared to word-based translation on
Japanese and Finnish input, followed by German,
</bodyText>
<page confidence="0.992596">
171
</page>
<table confidence="0.935779">
fi-en en-fi ja-en en-ja
ITG +cooc +look 28.94 25.31 24.58 35.71
ITG +cooc -look 28.51 24.24 24.32 35.74
ITG -cooc +look 28.65 24.49 24.36 35.05
ITG -cooc -look 27.45 23.30 23.57 34.50
fi-en ja-en
ITG-word
ITG-char
2.851 2.085
2.826 2.154
</table>
<tableCaption confidence="0.991686">
Table 3: Human evaluation scores (0-5 scale).
</tableCaption>
<bodyText confidence="0.990564111111111">
directive on equality
tasa-arvodirektiivi
equality directive
yoshiwara-juku station
yoshiwara no eki
yoshiwara-juku station
world health organisation
world health
world health organisation
</bodyText>
<tableCaption confidence="0.9574145">
Table 4: The major gains of character-based translation,
unknown, hyphenated, and uncommon words.
</tableCaption>
<bodyText confidence="0.9999224">
and finally French. This confirms that character-
based translation is performing well on languages
that have long words or ambiguous boundaries, and
less well on language pairs with relatively strong
one-to-one correspondence between words.
</bodyText>
<subsectionHeader confidence="0.995091">
6.3 Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.9999911875">
In addition, we performed a subjective evaluation of
Japanese-English and Finnish-English translations.
Two raters evaluated 100 sentences each, assigning
a score of 0-5 based on how well the translation con-
veys the information contained in the reference. We
focus on shorter sentences of 8-16 English words to
ease rating and interpretation. Table 3 shows that
the results are comparable, with no significant dif-
ference in average scores for either language pair.
Table 4 shows a breakdown of the sentences for
which character-based translation received a score
of at 2+ points more than word-based. It can be seen
that character-based translation is properly handling
sparsity phenomena. On the other hand, word-based
translation was generally stronger with reordering
and lexical choice of more common words.
</bodyText>
<subsectionHeader confidence="0.995686">
6.4 Effect of Alignment Method
</subsectionHeader>
<bodyText confidence="0.9957614">
In this section, we compare the translation accura-
cies for character-based translation using the phrasal
ITG model with and without the proposed improve-
ments of substring co-occurrence priors and look-
ahead parsing as described in Sections 4 and 5.2.
</bodyText>
<tableCaption confidence="0.992139">
Table 5: METEOR scores for alignment with and without
look-ahead and co-occurrence priors.
</tableCaption>
<bodyText confidence="0.999553">
Figure 5 shows METEOR scores12 for experi-
ments translating Japanese and Finnish. It can be
seen that the co-occurrence prior gives gains in all
cases, indicating that substring statistics are effec-
tively seeding the ITG aligner. The introduced look-
ahead probabilities improve accuracy significantly
when substring co-occurrence counts are not used,
and slightly when co-occurrence counts are used.
More importantly, they allow for more aggressive
beam pruning, increasing sampling speed from 1.3
sent/s to 2.5 sent/s for Finnish, and 6.8 sent/s to 11.6
sent/s for Japanese.
</bodyText>
<sectionHeader confidence="0.987909" genericHeader="conclusions">
7 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.999949238095238">
This paper demonstrated that character-based trans-
lation can act as a unified framework for handling
difficult problems in translation: morphology, com-
pound words, transliteration, and segmentation.
One future challenge includes scaling training up
to longer sentences, which can likely be achieved
through methods such as the heuristic span prun-
ing of Haghighi et al. (2009) or sentence splitting
of Vilar et al. (2007). Monolingual data could also
be used to improve estimates of our substring-based
prior. In addition, error analysis showed that word-
based translation performed better than character-
based translation on reordering and lexical choice,
indicating that improved decoding (or pre-ordering)
and language modeling tailored to character-based
translation will likely greatly improve accuracy. Fi-
nally, we plan to explore the middle ground between
word-based and character based translation, allow-
ing for the flexibility of character-based translation,
while using word boundary information to increase
efficiency and accuracy.
</bodyText>
<footnote confidence="0.471144">
12Similar results were found for character and word-based
BLEU, but are omitted for lack of space.
</footnote>
<table confidence="0.981400666666667">
Source Unk. Ref:
(13/26) Word:
Target Unk. Char:
(5/26) Ref:
Uncommon Word:
(5/26) Char:
Ref:
Word:
Char:
</table>
<page confidence="0.992343">
172
</page>
<sectionHeader confidence="0.985728" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998782653846154">
Mohamed I. Abouelhoda, Stefan Kurtz, and Enno Ohle-
busch. 2004. Replacing suffix trees with enhanced
suffix arrays. Journal of Discrete Algorithms, 2(1).
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual re-
sources. In Proc. ACL.
Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang.
2008. Improving word alignment by adjusting Chi-
nese word segmentation. In Proc. IJCNLP.
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Proc.
HLT-NAACL, pages 238–241.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proc. ACL.
Ond˘rej Bojar. 2007. English-to-Czech factored machine
translation. In Proc. WMT.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19.
Ralf D. Brown. 2002. Corpus-driven splitting of com-
pound words. In Proc. TMI.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Proc.
WMT.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Proc.
EMNLP.
Simon Corston-Oliver and Michael Gamon. 2004. Nor-
malizing German and English inflectional morphology
to improve statistical word alignment. Machine Trans-
lation: From Real Users to Research.
Fabien Cromieres. 2006. Sub-sentential alignment us-
ing substring co-occurrence counts. In Proc. COL-
ING/ACL 2006 Student Research Workshop.
John DeNero, Alex Bouchard-Cˆot´e, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proc. EMNLP.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization and
Evaluation of Machine Translation Systems. In Proc.
WMT.
Andrew Finch and Eiichiro Sumita. 2007. Phrase-based
machine transliteration. In Proc. TCAST.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
Proc. EMNLP.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proc. ACL.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Proc. NAACL.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: fast exact Viterbi parse selection. In Proc. HLT.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. HLT,
pages 48–54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In Proc. HLT.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proc. HLT.
Klaus Macherey, Andrew Dai, David Talbot, Ashok
Popat, and Franz Och. 2011. Language-independent
compound splitting with morphological operations. In
Proc. ACL.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. EMNLP.
Cos¸kun Mermer and Ahmet Afs¸ın Akın. 2010. Unsu-
pervised search for the optimal segmentation for sta-
tistical machine translation. In Proc. ACL Student Re-
search Workshop.
Jason Naradowsky and Kristina Toutanova. 2011. Unsu-
pervised bilingual morpheme segmentation and align-
ment with context-rich hidden semi-Markov models.
In Proc. ACL.
Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shin-
suke Mori, and Tatsuya Kawahara. 2011. An unsuper-
vised model for joint phrase alignment and extraction.
In Proc. ACL, pages 632–641, Portland, USA, June.
Graham Neubig. 2011. The Kyoto free translation task.
http://www.phontron.com/kftt.
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric word segmentation for machine
translation. In Proc. COLING.
Sonja Nießen and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In Proc. COL-
ING.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. COLING.
</reference>
<page confidence="0.988774">
173
</page>
<reference confidence="0.999801842105263">
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning stochastic bracketing inversion transduction
grammars with a cubic time biparsing algorithm. In
Proc. IWPT, pages 29–32.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. Proc. ACL.
Virach Sornlertlamvanich, Chumpol Mokarat, and Hi-
toshi Isahara. 2008. Thai-lao machine translation
based on phoneme transfer. In Proc. 14th Annual
Meeting of the Association for Natural Language Pro-
cessing.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Proc. ACL.
David Talbot and Miles Osborne. 2006. Modelling lexi-
cal redundancy for machine translation. In Proc. ACL.
J¨org Tiedemann. 2009. Character-based PSMT for
closely related languages. In Proc. 13th Annual
Conference of the European Association for Machine
Translation.
David Vilar, Jan-T. Peter, and Hermann Ney. 2007. Can
we translate letters. In Proc. WMT.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. COLING.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
Proc. ACL.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008a. Bayesian learning of
non-compositional phrases with synchronous parsing.
Proc. ACL.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008b. Improved statistical machine translation by
multiple Chinese word segmentation. In Proc. WMT.
</reference>
<page confidence="0.998417">
174
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.721979">
<title confidence="0.999833">Machine Translation without Words through Substring Alignment</title>
<author confidence="0.97375">Taro Shinsuke Tatsuya</author>
<affiliation confidence="0.923433333333333">School of Informatics, Kyoto Yoshida Honmachi, Sakyo-ku, Kyoto, Japan Institute of Information and Communication</affiliation>
<address confidence="0.940925">3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan</address>
<abstract confidence="0.998434111111111">In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohamed I Abouelhoda</author>
<author>Stefan Kurtz</author>
<author>Enno Ohlebusch</author>
</authors>
<title>Replacing suffix trees with enhanced suffix arrays.</title>
<date>2004</date>
<journal>Journal of Discrete Algorithms,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="20267" citStr="Abouelhoda et al., 2004" startWordPosition="3196" endWordPosition="3199">lity in a statistical model that can take into account mutual exclusivity of overlapping substrings in a sentence. We define this prior probability using three counts over substrings c(e), c(f), and c(e, f). c(e) and c(f) count the total number of sentences in which the substrings e and f occur respectively. c(e, f) is a count of the total number of sentences in which the substring e occurs on the target side, and f occurs on the source side. We perform the calculation of these statistics using enhanced suffix arrays, a data structure that can efficiently calculate all substrings in a corpus (Abouelhoda et al., 2004).a While suffix arrays allow for efficient calculation of these statistics, storing all co-occurrence counts c(e, f) is an unrealistic memory burden for larger 4Using the open-source implementation esaxx http:// code.google.com/p/esaxx/ Pt(a) Pt(a) αe(s) = max {S1,...,S.} 1 2. 169 corpora. In order to reduce the amount of memory used, we discount every count by a constant d, which we set to 5. This has a dual effect of reducing the amount of memory needed to hold co-occurrence counts by removing values for which c(e, f) &lt; d, as well as preventing over-fitting of the training data. In addition,</context>
</contexts>
<marker>Abouelhoda, Kurtz, Ohlebusch, 2004</marker>
<rawString>Mohamed I. Abouelhoda, Stefan Kurtz, and Enno Ohlebusch. 2004. Replacing suffix trees with enhanced suffix arrays. Journal of Discrete Algorithms, 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kevin Knight</author>
</authors>
<title>Translating named entities using monolingual and bilingual resources.</title>
<date>2002</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="7476" citStr="Al-Onaizan and Knight, 2002" startWordPosition="1137" endWordPosition="1140">ey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumer</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Yaser Al-Onaizan and Kevin Knight. 2002. Translating named entities using monolingual and bilingual resources. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Hong Bai</author>
<author>Keh-Jiann Chen</author>
<author>Jason S Chang</author>
</authors>
<title>Improving word alignment by adjusting Chinese word segmentation.</title>
<date>2008</date>
<booktitle>In Proc. IJCNLP.</booktitle>
<contexts>
<context position="7798" citStr="Bai et al., 2008" startWordPosition="1189" endWordPosition="1192">ed by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single 166 unified framework, requiring no language specific tools such as morphological analyzers or </context>
</contexts>
<marker>Bai, Chen, Chang, 2008</marker>
<rawString>Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang. 2008. Improving word alignment by adjusting Chinese word segmentation. In Proc. IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Inducing synchronous grammars with slice sampling.</title>
<date>2010</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>238--241</pages>
<contexts>
<context position="14205" citStr="Blunsom and Cohn, 2010" startWordPosition="2223" endWordPosition="2226">uv) + E E Px(str)I(as,S,u,U)I(aS,t,U,v) s&lt;S&lt;t u&lt;U&lt;v + E E Px(inv)I(as,S,U,v)I(aS,t,u,U) s&lt;S&lt;t u&lt;U&lt;v where Px(str) and Px(inv) are the probability of straight and inverted ITG productions. While the exact calculation of these probabilities can be performed in O(n6) time, where n is the 2Pt can be specified according to Bayesian statistics as described by Neubig et al. (2011). length of the sentence, this is impractical for all but the shortest sentences. Thus it is necessary to use methods to reduce the search space such as beamsearch based chart parsing (Saers et al., 2009) or slice sampling (Blunsom and Cohn, 2010).3 In this section we propose the use of a look-ahead probability to increase the efficiency of this chart parsing. Taking the example of Saers et al. (2009), spans are pushed onto a different queue based on their size, and queues are processed in ascending order of size. Agendas can further be trimmed based on a histogram beam (Saers et al., 2009) or probability beam (Neubig et al., 2011) compared to the best hypothesis a. In other words, we have a queue discipline based on the inside probability, and all spans ak where I(ak) &lt; cI(&amp;) are pruned. c is a constant describing the width of the bea</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2010. Inducing synchronous grammars with slice sampling. In Proc. HLT-NAACL, pages 238–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A Gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="11036" citStr="Blunsom et al., 2009" startWordPosition="1707" endWordPosition="1710">ers to normalize or split the sentence into morpheme streams (Corston-Oliver and Gamon, 2004). enough information to allow for effective alignment with its corresponding elements in ei. While this is often the case in word-based models, for characterbased models this assumption breaks down, as there is often no clear correspondence between characters. 3.2 Many-to-Many Alignment On the other hand, in recent years, there have been advances in many-to-many alignment techniques that are able to align multi-element chunks on both sides of the translation (Marcu and Wong, 2002; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). Many-to-many methods can be expected to achieve superior results on character-based alignment, as the aligner can use information about substrings, which may correspond to letters, morphemes, words, or short phrases. Here, we focus on the model presented by Neubig et al. (2011), which uses Bayesian inference in the phrasal inversion transduction grammar (ITG, Wu (1997)) framework. ITGs are a variety of synchronous context free grammar (SCFG) that allows for many-to-many alignment to be achieved in polynomial time through the process of biparsing, which we explain more i</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A Gibbs sampler for phrasal synchronous grammar induction. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ond˘rej Bojar</author>
</authors>
<title>English-to-Czech factored machine translation.</title>
<date>2007</date>
<booktitle>In Proc. WMT.</booktitle>
<contexts>
<context position="7065" citStr="Bojar, 2007" startWordPosition="1076" endWordPosition="1077">is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of wor</context>
</contexts>
<marker>Bojar, 2007</marker>
<rawString>Ond˘rej Bojar. 2007. English-to-Czech factored machine translation. In Proc. WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="9652" citStr="Brown et al., 1993" startWordPosition="1485" endWordPosition="1488">entence pair. We represent our target and source sentences as e�1 and fJ1. ei and fj represent single elements of the target and source sentences respectively. These may be words in word-based alignment models or single characters in character-based alignment models.l We define our alignment as aK , where each element is a span ak = (s, t, u, v) indicating that the target string es, ... , et and source string fu, ... , f„ are aligned to each-other. 3.1 One-to-Many Alignment The most well-known and widely-used models for bitext alignment are for one-to-many alignment, including the IBM models (Brown et al., 1993) and HMM alignment model (Vogel et al., 1996). These models are by nature directional, attempting to find the alignments that maximize the conditional probability of the target sentence P(ei|fJ1, aK ). For computational reasons, the IBM models are restricted to aligning each word on the target side to a single word on the source side. In the formalism presented above, this means that each ei must be included in at most one span, and for each span u = v. Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al., 2003). H</context>
<context position="19040" citStr="Brown et al. (1993)" startWordPosition="2999" endWordPosition="3002">-many translation has used IBM model 1 probabilities to bias phrasal alignments so that phrases whose member words are good translations are also aligned. As a representative of this existing method, we adopt a base measure similar to that used by DeNero et al. (2008): Pm1(e,f) =M0(e,f)Ppois(|e|;λ)Ppois(|f|;λ) M0(e,f) =(Pm1(f|e)Puni(e)Pm1(e|f)Puni(f)) Ppois is the Poisson distribution with the average length parameter λ, which we set to 0.01. Pm1 is the word-based (or character-based) Model 1 probability, which can be efficiently calculated using the dynamic programming algorithm described by Brown et al. (1993). However, for reasons previously stated in Section 3, these methods are less satisfactory when performing character-based alignment, as the amount of information contained in a character does not allow for proper alignment. 5.2 Substring Co-occurrence Priors Instead, we propose a method for using raw substring co-occurrence statistics to bias alignments towards substrings that often co-occur in the entire training corpus. This is similar to the method of Cromieres (2006), but instead of using these cooccurrence statistics as a heuristic alignment criterion, we incorporate them as a prior prob</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf D Brown</author>
</authors>
<title>Corpus-driven splitting of compound words.</title>
<date>2002</date>
<booktitle>In Proc. TMI.</booktitle>
<contexts>
<context position="6750" citStr="Brown, 2002" startWordPosition="1026" endWordPosition="1027">in the form of garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods f</context>
</contexts>
<marker>Brown, 2002</marker>
<rawString>Ralf D. Brown. 2002. Corpus-driven splitting of compound words. In Proc. TMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proc. WMT.</booktitle>
<contexts>
<context position="1663" citStr="Chang et al., 2008" startWordPosition="248" endWordPosition="251">mon words over several language pairs. 1 Introduction Traditionally, the task of statistical machine translation (SMT) is defined as translating a source sentence fJ1 = {f1, ... , fJ} to a target sentence eI1 = {e1, ..., eI}, where each element of fJ1 and eI1 is assumed to be a word in the source and target languages. However, the definition of a “word” is often problematic. The most obvious example of this lies in languages that do not separate words with white space such as Chinese, Japanese, or Thai, in which the choice of a segmentation standard has a large effect on translation accuracy (Chang et al., 2008). Even for languages with explicit word The first author is now affiliated with the Nara Institute of Science and Technology. boundaries, all machine translation systems perform at least some precursory form of tokenization, splitting punctuation and words to prevent the sparsity that would occur if punctuated and non-punctuated words were treated as different entities. Sparsity also manifests itself in other forms, including the large vocabularies produced by morphological productivity, word compounding, numbers, and proper names. A myriad of methods have been proposed to handle each of these</context>
<context position="7818" citStr="Chang et al., 2008" startWordPosition="1193" endWordPosition="1196">es or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single 166 unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. How</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proc. WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Unsupervised tokenization for machine translation. In</title>
<date>2009</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="7863" citStr="Chung and Gildea, 2009" startWordPosition="1201" endWordPosition="1204">ion (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single 166 unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. However, while the approach is attractive concep</context>
</contexts>
<marker>Chung, Gildea, 2009</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2009. Unsupervised tokenization for machine translation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Michael Gamon</author>
</authors>
<title>Normalizing German and English inflectional morphology to improve statistical word alignment. Machine Translation: From Real Users to Research.</title>
<date>2004</date>
<contexts>
<context position="10509" citStr="Corston-Oliver and Gamon, 2004" startWordPosition="1625" endWordPosition="1628">s, the IBM models are restricted to aligning each word on the target side to a single word on the source side. In the formalism presented above, this means that each ei must be included in at most one span, and for each span u = v. Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al., 2003). However, in order for one-to-many alignment methods to be effective, each fj must contain &apos;Some previous work has also performed alignment using morphological analyzers to normalize or split the sentence into morpheme streams (Corston-Oliver and Gamon, 2004). enough information to allow for effective alignment with its corresponding elements in ei. While this is often the case in word-based models, for characterbased models this assumption breaks down, as there is often no clear correspondence between characters. 3.2 Many-to-Many Alignment On the other hand, in recent years, there have been advances in many-to-many alignment techniques that are able to align multi-element chunks on both sides of the translation (Marcu and Wong, 2002; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). Many-to-many methods can be expected to achieve s</context>
</contexts>
<marker>Corston-Oliver, Gamon, 2004</marker>
<rawString>Simon Corston-Oliver and Michael Gamon. 2004. Normalizing German and English inflectional morphology to improve statistical word alignment. Machine Translation: From Real Users to Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabien Cromieres</author>
</authors>
<title>Sub-sentential alignment using substring co-occurrence counts.</title>
<date>2006</date>
<booktitle>In Proc. COLING/ACL 2006 Student Research Workshop.</booktitle>
<contexts>
<context position="19516" citStr="Cromieres (2006)" startWordPosition="3072" endWordPosition="3073">ter-based) Model 1 probability, which can be efficiently calculated using the dynamic programming algorithm described by Brown et al. (1993). However, for reasons previously stated in Section 3, these methods are less satisfactory when performing character-based alignment, as the amount of information contained in a character does not allow for proper alignment. 5.2 Substring Co-occurrence Priors Instead, we propose a method for using raw substring co-occurrence statistics to bias alignments towards substrings that often co-occur in the entire training corpus. This is similar to the method of Cromieres (2006), but instead of using these cooccurrence statistics as a heuristic alignment criterion, we incorporate them as a prior probability in a statistical model that can take into account mutual exclusivity of overlapping substrings in a sentence. We define this prior probability using three counts over substrings c(e), c(f), and c(e, f). c(e) and c(f) count the total number of sentences in which the substrings e and f occur respectively. c(e, f) is a count of the total number of sentences in which the substring e occurs on the target side, and f occurs on the source side. We perform the calculation</context>
<context position="21300" citStr="Cromieres, 2006" startWordPosition="3360" endWordPosition="3361">ct of reducing the amount of memory needed to hold co-occurrence counts by removing values for which c(e, f) &lt; d, as well as preventing over-fitting of the training data. In addition, we heuristically prune values for which the conditional probabilities P(e|f) or P(f|e) are less than some fixed value, which we set to 0.1 for the reported experiments. To determine how to combine c(e), c(f), and c(e, f) into prior probabilities, we performed preliminary experiments testing methods proposed by previous research including plain co-occurrence counts, the Dice coefficient, and χ-squared statistics (Cromieres, 2006), as well as a new method of defining substring pair probabilities to be proportional to bidirectional conditional probabilities Pcooc(e, f) = Pcooc(e|f)Pcooc(f|e)/Z Cc(e, f) − dl �c(e, f) − dl /Z c(f) − d c(e) − d for all substring pairs where c(e, f) &gt; d an d where Z is a normalization term equal to bility according to As the prior is only supposed to bias the model towards good solutions and not explicitly rule out any possibilities, we linearly interpolate the co-occurrence probability with the one-to-many Model 1 probability, which will give at least some probability mass to all substring</context>
</contexts>
<marker>Cromieres, 2006</marker>
<rawString>Fabien Cromieres. 2006. Sub-sentential alignment using substring co-occurrence counts. In Proc. COLING/ACL 2006 Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alex Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP.</booktitle>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>John DeNero, Alex Bouchard-Cˆot´e, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proc. WMT.</booktitle>
<contexts>
<context position="27851" citStr="Denkowski and Lavie, 2011" startWordPosition="4436" endWordPosition="4439">r experiments, although it does indicate that we must have access to tokenized data for the development set. 6.2 Quantitative Evaluation Table 2 presents a quantitative analysis of the translation results for each of the proposed methods. As previous research has shown that it is more difficult to translate into morphologically rich languages than into English (Koehn, 2005), we perform experiments translating in both directions for all language pairs. We evaluate translation quality using BLEU score (Papineni et al., 2002), both on the word and character level (with n = 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level. It can be seen that character-based translation with all of the proposed alignment improvements greatly exceeds character-based translation using one-to-many alignment, confirming that substringbased information is necessary for accurate alignments. When compared with word-based translation, character-based translation achieves better, comparable, or inferior results on character-based BLEU, comparable or inferior results on METEOR, and inferior results on word-based BLEU. The differences between the evaluation metrics are due to the fact that character-based translation of</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proc. WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Phrase-based machine transliteration.</title>
<date>2007</date>
<booktitle>In Proc. TCAST.</booktitle>
<contexts>
<context position="7316" citStr="Finch and Sumita, 2007" startWordPosition="1114" endWordPosition="1117">nsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generall</context>
</contexts>
<marker>Finch, Sumita, 2007</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2007. Phrase-based machine transliteration. In Proc. TCAST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>David McClosky</author>
</authors>
<title>Improving statistical MT through morphological analysis.</title>
<date>2005</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="6791" citStr="Goldwater and McClosky, 2005" startWordPosition="1030" endWordPosition="1033">ollection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine </context>
</contexts>
<marker>Goldwater, McClosky, 2005</marker>
<rawString>Sharon Goldwater and David McClosky. 2005. Improving statistical MT through morphological analysis. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised ITG models.</title>
<date>2009</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="31840" citStr="Haghighi et al. (2009)" startWordPosition="5016" endWordPosition="5019">when co-occurrence counts are used. More importantly, they allow for more aggressive beam pruning, increasing sampling speed from 1.3 sent/s to 2.5 sent/s for Finnish, and 6.8 sent/s to 11.6 sent/s for Japanese. 7 Conclusion and Future Directions This paper demonstrated that character-based translation can act as a unified framework for handling difficult problems in translation: morphology, compound words, transliteration, and segmentation. One future challenge includes scaling training up to longer sentences, which can likely be achieved through methods such as the heuristic span pruning of Haghighi et al. (2009) or sentence splitting of Vilar et al. (2007). Monolingual data could also be used to improve estimates of our substring-based prior. In addition, error analysis showed that wordbased translation performed better than characterbased translation on reordering and lexical choice, indicating that improved decoding (or pre-ordering) and language modeling tailored to character-based translation will likely greatly improve accuracy. Finally, we plan to explore the middle ground between word-based and character based translation, allowing for the flexibility of character-based translation, while usin</context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised ITG models. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo. In</title>
<date>2007</date>
<booktitle>Proc. NAACL.</booktitle>
<contexts>
<context position="16237" citStr="Johnson et al., 2007" startWordPosition="2560" endWordPosition="2563">nning, 2003), and tic-tac-toe pruning for wordbased ITGs (Zhang and Gildea, 2005). As the calculation of the actual outside probability O(ak) is just as expensive as parsing itself, it is necessary to approximate this with heuristic function O* that can be calculated efficiently. Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2, compared with the n3 amortized time of the tic-tac-toe pruning 3Applying beam-search before sampling will sample from an improper distribution, although Metropolis-in-Gibbs sampling (Johnson et al., 2007) can be used to compensate. However, we found that this had no significant effect on results, so we omit the Metropolis-in-Gibbs step for experiments. 168 algorithm described by (Zhang et al., 2008a). During the calculation of the phrase generation probabilities Pt, we save the best inside probability I∗ for each monolingual span. I∗ e (s,t) = max {˜a=h˜s,˜t,˜u,˜vi;˜s=s,˜t=t} I∗f (u, v) = max {˜a=h˜s,˜t,˜u,˜vi;˜u=u,˜v=v} For each language independently, we calculate forward probabilities α and backward probabilities β. For example, αe(s) is the maximum probability of the span (0, s) of e that </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* parsing: fast exact Viterbi parse selection.</title>
<date>2003</date>
<booktitle>In Proc. HLT.</booktitle>
<contexts>
<context position="15628" citStr="Klein and Manning, 2003" startWordPosition="2466" endWordPosition="2469">it is unwise to ignore competing hypotheses during beam pruning. Particularly, the alignment “les/1960s” competes with the high-probability alignment “les/the,” so intuitively should be a good candidate for pruning. However its probability is only slightly higher than “ann´ees/1960s,” which has no competing hypotheses and thus should not be trimmed. In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I(ak), but also the outside probability O(ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for wordbased ITGs (Zhang and Gildea, 2005). As the calculation of the actual outside probability O(ak) is just as expensive as parsing itself, it is necessary to approximate this with heuristic function O* that can be calculated efficiently. Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2, compared with the n3 amortized time of the tic-tac-toe pruning 3Applying beam-search before sampling will sample from an improper distribution, although Metropolis-in-Gibbs sampling (Johnson et a</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. A* parsing: fast exact Viterbi parse selection. In Proc. HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="7269" citStr="Knight and Graehl, 1998" startWordPosition="1106" endWordPosition="1109">ugh lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguy</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. HLT,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="10249" citStr="Koehn et al., 2003" startWordPosition="1588" endWordPosition="1591">s (Brown et al., 1993) and HMM alignment model (Vogel et al., 1996). These models are by nature directional, attempting to find the alignments that maximize the conditional probability of the target sentence P(ei|fJ1, aK ). For computational reasons, the IBM models are restricted to aligning each word on the target side to a single word on the source side. In the formalism presented above, this means that each ei must be included in at most one span, and for each span u = v. Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al., 2003). However, in order for one-to-many alignment methods to be effective, each fj must contain &apos;Some previous work has also performed alignment using morphological analyzers to normalize or split the sentence into morpheme streams (Corston-Oliver and Gamon, 2004). enough information to allow for effective alignment with its corresponding elements in ei. While this is often the case in word-based models, for characterbased models this assumption breaks down, as there is often no clear correspondence between characters. 3.2 Many-to-Many Alignment On the other hand, in recent years, there have been </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. HLT, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="25734" citStr="Koehn, 2004" startWordPosition="4110" endWordPosition="4111">-word 17.47 / 63.18 / 37.79 13.12 / 59.27 / 27.09 31.66 / 69.61 / 51.98 20.26 / 28.34 / 38.34 GIZA-char 06.17 / 41.04 / 19.90 04.58 / 35.09 / 11.76 10.31 / 42.84 / 25.06 01.48 / 00.72 / 06.67 ITG-char 15.35 / 61.95 / 35.45 12.14 / 59.02 / 25.31 27.74 / 67.44 / 48.56 17.90 / 28.46 / 35.71 Table 2: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant difference from the best system according to the bootstrap resampling method at p = 0.05 (Koehn, 2004). source and target were 100 characters or less,6 the total size of which is shown in Table 1. In characterbased translation, white spaces between words were treated as any other character and not given any special treatment. Evaluation was performed on tokenized and lower-cased data. For alignment, we use the GIZA++ implementation of one-to-many alignment� and the pialign implementation of the phrasal ITG models$ modified with the proposed improvements. For GIZA++, we used the default settings for word-based alignment, but used the HMM model for character-based alignment to allow for alignmen</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="6031" citStr="Koehn, 2005" startWordPosition="914" endWordPosition="915">oposed enhancements to the model. Finally, we perform a qualitative analysis, which finds that character-based translation can handle unsegmented text, conjugation, and proper names in a unified framework with no additional processing. 2 Related Work on Data Sparsity in SMT As traditional SMT systems treat all words as single tokens without considering their internal structure, major problems of data sparsity occur for less frequent tokens. In fact, it has been shown that there is a direct negative correlation between vocabulary size (and thus sparsity) of a language and translation accuracy (Koehn, 2005). Sparsity causes trouble for alignment models, both in the form of incorrectly aligned uncommon words, and in the form of garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regu</context>
<context position="24456" citStr="Koehn, 2005" startWordPosition="3869" endWordPosition="3870">d gave significantly better results than all other methods, so we adopt this for the remainder of our experiments. It should be noted that as we are using discounting, many substring pairs will be given zero probaIn order to test the effectiveness of character-based translation, we performed experiments over a variety of language pairs and experi 2.80M 2.56M 2.23M 15.5M 13.8M 11.5 16.0M M 54.3k 41.4k 66.2k 28.5 6.1 Experimental Setup We use a combination of four languages with English, using freely available data. We selected French-English, German-English, Finnish-English data from EuroParl (Koehn, 2005), with development and test sets designated for the 2005 ACL 5http://statmt.org/wpt05/mt-shared-task = �Z = �e�f�c�e�f���� 170 de-en fi-en fr-en ja-en GIZA-word 24.58 / 64.28 / 30.43 20.41 / 60.01 / 27.89 30.23 / 68.79 / 34.20 17.95 / 56.47 / 24.70 ITG-word 23.87 / 64.89 / 30.71 20.83 / 61.04 / 28.46 29.92 / 68.64 / 34.29 17.14 / 56.60 / 24.89 GIZA-char 08.05 / 45.01 / 15.35 06.91 / 41.62 / 14.39 11.05 / 48.23 / 17.80 09.46 / 49.02 / 18.34 ITG-char 21.79 / 64.47 / 30.12 18.38 / 62.44 / 28.94 26.70 / 66.76 / 32.47 15.84 / 58.41 / 24.58 en-de en-fi en-fr en-ja GIZA-word 17.94 / 62.71 / 37.88 13.</context>
<context position="27601" citStr="Koehn, 2005" startWordPosition="4396" endWordPosition="4397">tp://phontron.com/pialign/ 9Improvement by using a beam larger than 10−4 was marginal, especially with co-occurrence prior probabilities. 10http://statmt.org/moses/ 11We chose this set-up to minimize the effect of tuning criterion on our experiments, although it does indicate that we must have access to tokenized data for the development set. 6.2 Quantitative Evaluation Table 2 presents a quantitative analysis of the translation results for each of the proposed methods. As previous research has shown that it is more difficult to translate into morphologically rich languages than into English (Koehn, 2005), we perform experiments translating in both directions for all language pairs. We evaluate translation quality using BLEU score (Papineni et al., 2002), both on the word and character level (with n = 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level. It can be seen that character-based translation with all of the proposed alignment improvements greatly exceeds character-based translation using one-to-many alignment, confirming that substringbased information is necessary for accurate alignments. When compared with word-based translation, character-based translation achieves </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
<author>Daniel Marcu</author>
<author>Kevin Knight</author>
</authors>
<title>Cognates can improve statistical translation models.</title>
<date>2003</date>
<booktitle>In Proc. HLT.</booktitle>
<contexts>
<context position="7291" citStr="Kondrak et al., 2003" startWordPosition="1110" endWordPosition="1113">logical analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), alth</context>
</contexts>
<marker>Kondrak, Marcu, Knight, 2003</marker>
<rawString>Grzegorz Kondrak, Daniel Marcu, and Kevin Knight. 2003. Cognates can improve statistical translation models. In Proc. HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
</authors>
<title>Morphological analysis for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. HLT.</booktitle>
<contexts>
<context position="6761" citStr="Lee, 2004" startWordPosition="1028" endWordPosition="1029">f garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named en</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Young-Suk Lee. 2004. Morphological analysis for statistical machine translation. In Proc. HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Macherey</author>
<author>Andrew Dai</author>
<author>David Talbot</author>
<author>Ashok Popat</author>
<author>Franz Och</author>
</authors>
<title>Language-independent compound splitting with morphological operations.</title>
<date>2011</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="6864" citStr="Macherey et al., 2011" startWordPosition="1042" endWordPosition="1045">e segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and K</context>
</contexts>
<marker>Macherey, Dai, Talbot, Popat, Och, 2011</marker>
<rawString>Klaus Macherey, Andrew Dai, David Talbot, Ashok Popat, and Franz Och. 2011. Language-independent compound splitting with morphological operations. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="10993" citStr="Marcu and Wong, 2002" startWordPosition="1699" endWordPosition="1702">formed alignment using morphological analyzers to normalize or split the sentence into morpheme streams (Corston-Oliver and Gamon, 2004). enough information to allow for effective alignment with its corresponding elements in ei. While this is often the case in word-based models, for characterbased models this assumption breaks down, as there is often no clear correspondence between characters. 3.2 Many-to-Many Alignment On the other hand, in recent years, there have been advances in many-to-many alignment techniques that are able to align multi-element chunks on both sides of the translation (Marcu and Wong, 2002; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). Many-to-many methods can be expected to achieve superior results on character-based alignment, as the aligner can use information about substrings, which may correspond to letters, morphemes, words, or short phrases. Here, we focus on the model presented by Neubig et al. (2011), which uses Bayesian inference in the phrasal inversion transduction grammar (ITG, Wu (1997)) framework. ITGs are a variety of synchronous context free grammar (SCFG) that allows for many-to-many alignment to be achieved in polynomial time through the pr</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<title>Cos¸kun Mermer and Ahmet Afs¸ın Akın.</title>
<date>2010</date>
<booktitle>In Proc. ACL Student Research Workshop.</booktitle>
<marker>2010</marker>
<rawString>Cos¸kun Mermer and Ahmet Afs¸ın Akın. 2010. Unsupervised search for the optimal segmentation for statistical machine translation. In Proc. ACL Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Naradowsky</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised bilingual morpheme segmentation and alignment with context-rich hidden semi-Markov models.</title>
<date>2011</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="12029" citStr="Naradowsky and Toutanova, 2011" startWordPosition="1864" endWordPosition="1867">nsduction grammar (ITG, Wu (1997)) framework. ITGs are a variety of synchronous context free grammar (SCFG) that allows for many-to-many alignment to be achieved in polynomial time through the process of biparsing, which we explain more in the following section. Phrasal ITGs are ITGs that allow for non-terminals that can emit phrase pairs with multiple elements on both the source and target sides. It should be noted that there are other many-to-many alignment methods that have been used for simultaneously discovering morphological boundaries over multiple languages (Snyder and Barzilay, 2008; Naradowsky and Toutanova, 2011), but these have generally been applied to single words or short phrases, and it is not immediately clear that they will scale to aligning full sentences. 4 Look-Ahead Biparsing In this work, we experiment with the alignment method of Neubig et al. (2011), which can achieve competitive accuracy with a much smaller phrase table than traditional methods. This is important in the character-based translation context, as we would like to use phrases that contain large numbers of characters without creating a phrase table so large that it cannot be used in actual decoding. In this framework, trainin</context>
</contexts>
<marker>Naradowsky, Toutanova, 2011</marker>
<rawString>Jason Naradowsky and Kristina Toutanova. 2011. Unsupervised bilingual morpheme segmentation and alignment with context-rich hidden semi-Markov models. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>An unsupervised model for joint phrase alignment and extraction.</title>
<date>2011</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>632--641</pages>
<location>Portland, USA,</location>
<contexts>
<context position="4305" citStr="Neubig et al. (2011)" startWordPosition="642" endWordPosition="645"> of traditional word-based systems using only character strings. We draw upon recent advances in many-to-many alignment, which allows for the automatic choice of the length of units to be aligned. As these units may be at the character, subword, word, or multi-word phrase level, we conjecture that this will allow for better character alignments than one-to-many alignment techniques, and will allow for better translation of uncommon words than traditional word-based models by breaking down words into their component parts. We also propose two improvements to the manyto-many alignment method of Neubig et al. (2011). One barrier to applying many-to-many alignment models to character strings is training cost. In the inversion transduction grammar (ITG) framework (Wu, 1997), which is widely used in many-to-many alignment, search is cumbersome for longer sentences, a problem that is further exacerbated when using characters instead of words as the basic unit. As a step towards overcoming this difficulty, we increase the efficiency of the beam-search technique of Saers et al. (2009) by augmenting it with look-ahead probabilities in the spirit of A* search. Secondly, we describe a method to seed the search pr</context>
<context position="11058" citStr="Neubig et al., 2011" startWordPosition="1711" endWordPosition="1715">lit the sentence into morpheme streams (Corston-Oliver and Gamon, 2004). enough information to allow for effective alignment with its corresponding elements in ei. While this is often the case in word-based models, for characterbased models this assumption breaks down, as there is often no clear correspondence between characters. 3.2 Many-to-Many Alignment On the other hand, in recent years, there have been advances in many-to-many alignment techniques that are able to align multi-element chunks on both sides of the translation (Marcu and Wong, 2002; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). Many-to-many methods can be expected to achieve superior results on character-based alignment, as the aligner can use information about substrings, which may correspond to letters, morphemes, words, or short phrases. Here, we focus on the model presented by Neubig et al. (2011), which uses Bayesian inference in the phrasal inversion transduction grammar (ITG, Wu (1997)) framework. ITGs are a variety of synchronous context free grammar (SCFG) that allows for many-to-many alignment to be achieved in polynomial time through the process of biparsing, which we explain more in the following sectio</context>
<context position="12284" citStr="Neubig et al. (2011)" startWordPosition="1907" endWordPosition="1910">asal ITGs are ITGs that allow for non-terminals that can emit phrase pairs with multiple elements on both the source and target sides. It should be noted that there are other many-to-many alignment methods that have been used for simultaneously discovering morphological boundaries over multiple languages (Snyder and Barzilay, 2008; Naradowsky and Toutanova, 2011), but these have generally been applied to single words or short phrases, and it is not immediately clear that they will scale to aligning full sentences. 4 Look-Ahead Biparsing In this work, we experiment with the alignment method of Neubig et al. (2011), which can achieve competitive accuracy with a much smaller phrase table than traditional methods. This is important in the character-based translation context, as we would like to use phrases that contain large numbers of characters without creating a phrase table so large that it cannot be used in actual decoding. In this framework, training is performed using sentence167 Figure 1: (a) A chart with inside probabilities in boxes and forward/backward probabilities marking the surrounding arrows. (b) Spans with corresponding lookaheads added, and the minimum probability underlined. Lightly and</context>
<context position="13958" citStr="Neubig et al. (2011)" startWordPosition="2180" endWordPosition="2183">ide” probability I(as,t,u,v). This probability is the combination of the generative probability of each phrase pair Pt(ets, fvu) as well as the sum the probabilities over all shorter spans in straight and inverted order2 I(as,t,u,v) = Pt(ets, fuv) + E E Px(str)I(as,S,u,U)I(aS,t,U,v) s&lt;S&lt;t u&lt;U&lt;v + E E Px(inv)I(as,S,U,v)I(aS,t,u,U) s&lt;S&lt;t u&lt;U&lt;v where Px(str) and Px(inv) are the probability of straight and inverted ITG productions. While the exact calculation of these probabilities can be performed in O(n6) time, where n is the 2Pt can be specified according to Bayesian statistics as described by Neubig et al. (2011). length of the sentence, this is impractical for all but the shortest sentences. Thus it is necessary to use methods to reduce the search space such as beamsearch based chart parsing (Saers et al., 2009) or slice sampling (Blunsom and Cohn, 2010).3 In this section we propose the use of a look-ahead probability to increase the efficiency of this chart parsing. Taking the example of Saers et al. (2009), spans are pushed onto a different queue based on their size, and queues are processed in ascending order of size. Agendas can further be trimmed based on a histogram beam (Saers et al., 2009) or</context>
</contexts>
<marker>Neubig, Watanabe, Sumita, Mori, Kawahara, 2011</marker>
<rawString>Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised model for joint phrase alignment and extraction. In Proc. ACL, pages 632–641, Portland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
</authors>
<title>The Kyoto free translation task.</title>
<date>2011</date>
<note>http://www.phontron.com/kftt.</note>
<contexts>
<context position="22551" citStr="Neubig, 2011" startWordPosition="3574" endWordPosition="3576"> on the interpolation coefficient and learn it duri Pcooc. PprZor(e,f)=λPcooc(e,f) (1 − λ)Pm1(e,f). (α λ ng training. 6 Experiments mental settings. de-en fi-en fr-en ja-en TM (en) 3.10M 2.77M 2.13M TM (other) 3.05M 2.34M LM (en) LM (other) 15.3M11.3M 15.6M 11.9M Tune (en) 58.7k 58.7k 58.7k 30.8k Tune (other) 55.1k 42.0k 67.3k 34.4k Test (en) 58.0k 58.0k 58.0k 26.6k Test (other) k 1: d testing. Table The number of words in each corpus for TM and LM training, tuning, an shared task on machine We also did experiments with Japanese-English Wikipedia articles from the Kyoto Free Translation Task (Neubig, 2011) using the designated training and tuning sets, and reporting results on the test set. These languages were chosen as they have a variety of interesting characteristics. French has some inflection, but among the test languages has the strongest oneto-one correspondence with English, and is generally considered easy to translate. German has many compound words, which must be broken apart to translate properly into English. Finnish is an agglutinative language with extremely rich morphology, resulting in long words and the largest vocabulary of the languages in EuroParl. Japanese does not have a</context>
</contexts>
<marker>Neubig, 2011</marker>
<rawString>Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ThuyLinh Nguyen</author>
<author>Stephan Vogel</author>
<author>Noah A Smith</author>
</authors>
<title>Nonparametric word segmentation for machine translation. In</title>
<date>2010</date>
<booktitle>Proc. COLING.</booktitle>
<contexts>
<context position="7885" citStr="Nguyen et al., 2010" startWordPosition="1205" endWordPosition="1208">1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single 166 unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. However, while the approach is attractive conceptually, previous resea</context>
</contexts>
<marker>Nguyen, Vogel, Smith, 2010</marker>
<rawString>ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith. 2010. Nonparametric word segmentation for machine translation. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Hermann Ney</author>
</authors>
<title>Improving SMT quality with morpho-syntactic analysis.</title>
<date>2000</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="6737" citStr="Nießen and Ney, 2000" startWordPosition="1022" endWordPosition="1025">d uncommon words, and in the form of garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophistica</context>
</contexts>
<marker>Nießen, Ney, 2000</marker>
<rawString>Sonja Nießen and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="6310" citStr="Och and Ney, 2003" startWordPosition="959" endWordPosition="962">T As traditional SMT systems treat all words as single tokens without considering their internal structure, major problems of data sparsity occur for less frequent tokens. In fact, it has been shown that there is a direct negative correlation between vocabulary size (and thus sparsity) of a language and translation accuracy (Koehn, 2005). Sparsity causes trouble for alignment models, both in the form of incorrectly aligned uncommon words, and in the form of garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more diffi</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="27753" citStr="Papineni et al., 2002" startWordPosition="4417" endWordPosition="4420">tp://statmt.org/moses/ 11We chose this set-up to minimize the effect of tuning criterion on our experiments, although it does indicate that we must have access to tokenized data for the development set. 6.2 Quantitative Evaluation Table 2 presents a quantitative analysis of the translation results for each of the proposed methods. As previous research has shown that it is more difficult to translate into morphologically rich languages than into English (Koehn, 2005), we perform experiments translating in both directions for all language pairs. We evaluate translation quality using BLEU score (Papineni et al., 2002), both on the word and character level (with n = 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level. It can be seen that character-based translation with all of the proposed alignment improvements greatly exceeds character-based translation using one-to-many alignment, confirming that substringbased information is necessary for accurate alignments. When compared with word-based translation, character-based translation achieves better, comparable, or inferior results on character-based BLEU, comparable or inferior results on METEOR, and inferior results on word-based BLEU. The </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Joakim Nivre</author>
<author>Dekai Wu</author>
</authors>
<title>Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm.</title>
<date>2009</date>
<booktitle>In Proc. IWPT,</booktitle>
<pages>29--32</pages>
<contexts>
<context position="4777" citStr="Saers et al. (2009)" startWordPosition="715" endWordPosition="718">els by breaking down words into their component parts. We also propose two improvements to the manyto-many alignment method of Neubig et al. (2011). One barrier to applying many-to-many alignment models to character strings is training cost. In the inversion transduction grammar (ITG) framework (Wu, 1997), which is widely used in many-to-many alignment, search is cumbersome for longer sentences, a problem that is further exacerbated when using characters instead of words as the basic unit. As a step towards overcoming this difficulty, we increase the efficiency of the beam-search technique of Saers et al. (2009) by augmenting it with look-ahead probabilities in the spirit of A* search. Secondly, we describe a method to seed the search process using counts of all substring pairs in the corpus to bias the phrase alignment model. We do this by defining prior probabilities based on these substring counts within the Bayesian phrasal ITG framework. An evaluation on four language pairs with differing morphological properties shows that for distant language pairs, character-based SMT can achieve translation accuracy comparable to word-based systems. In addition, we perform ablation studies, showing that thes</context>
<context position="14162" citStr="Saers et al., 2009" startWordPosition="2216" endWordPosition="2219">inverted order2 I(as,t,u,v) = Pt(ets, fuv) + E E Px(str)I(as,S,u,U)I(aS,t,U,v) s&lt;S&lt;t u&lt;U&lt;v + E E Px(inv)I(as,S,U,v)I(aS,t,u,U) s&lt;S&lt;t u&lt;U&lt;v where Px(str) and Px(inv) are the probability of straight and inverted ITG productions. While the exact calculation of these probabilities can be performed in O(n6) time, where n is the 2Pt can be specified according to Bayesian statistics as described by Neubig et al. (2011). length of the sentence, this is impractical for all but the shortest sentences. Thus it is necessary to use methods to reduce the search space such as beamsearch based chart parsing (Saers et al., 2009) or slice sampling (Blunsom and Cohn, 2010).3 In this section we propose the use of a look-ahead probability to increase the efficiency of this chart parsing. Taking the example of Saers et al. (2009), spans are pushed onto a different queue based on their size, and queues are processed in ascending order of size. Agendas can further be trimmed based on a histogram beam (Saers et al., 2009) or probability beam (Neubig et al., 2011) compared to the best hypothesis a. In other words, we have a queue discipline based on the inside probability, and all spans ak where I(ak) &lt; cI(&amp;) are pruned. c is</context>
</contexts>
<marker>Saers, Nivre, Wu, 2009</marker>
<rawString>Markus Saers, Joakim Nivre, and Dekai Wu. 2009. Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm. In Proc. IWPT, pages 29–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation.</title>
<date>2008</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="11996" citStr="Snyder and Barzilay, 2008" startWordPosition="1860" endWordPosition="1863">n the phrasal inversion transduction grammar (ITG, Wu (1997)) framework. ITGs are a variety of synchronous context free grammar (SCFG) that allows for many-to-many alignment to be achieved in polynomial time through the process of biparsing, which we explain more in the following section. Phrasal ITGs are ITGs that allow for non-terminals that can emit phrase pairs with multiple elements on both the source and target sides. It should be noted that there are other many-to-many alignment methods that have been used for simultaneously discovering morphological boundaries over multiple languages (Snyder and Barzilay, 2008; Naradowsky and Toutanova, 2011), but these have generally been applied to single words or short phrases, and it is not immediately clear that they will scale to aligning full sentences. 4 Look-Ahead Biparsing In this work, we experiment with the alignment method of Neubig et al. (2011), which can achieve competitive accuracy with a much smaller phrase table than traditional methods. This is important in the character-based translation context, as we would like to use phrases that contain large numbers of characters without creating a phrase table so large that it cannot be used in actual dec</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Virach Sornlertlamvanich</author>
<author>Chumpol Mokarat</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Thai-lao machine translation based on phoneme transfer.</title>
<date>2008</date>
<booktitle>In Proc. 14th Annual Meeting of the Association for Natural Language Processing.</booktitle>
<contexts>
<context position="3013" citStr="Sornlertlamvanich et al., 2008" startWordPosition="449" endWordPosition="452">ng word segmentation, and transliteration, which we outline in more detail in Section 2. These difficulties occur because we are translating sequences of words as our basic unit. On the other hand, Vilar et al. (2007) examine the possibility of instead treating each sentence as sequences of characters to be translated. This method is attractive, as it is theoretically able to handle all sparsity phenomena in a single unified framework, but has only been shown feasible between similar language pairs such as Spanish-Catalan (Vilar et al., 2007), Swedish-Norwegian (Tiedemann, 2009), and ThaiLao (Sornlertlamvanich et al., 2008), which have a strong co-occurrence between single characters. As Vilar et al. (2007) state and we confirm, accurate translations cannot be achieved when applying traditional translation techniques to character-based translation for less similar language pairs. In this paper, we propose improvements to the alignment process tailored to character-based machine translation, and demonstrate that it is, in fact, possible to achieve translation accuracies that ap165 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 165–174, Jeju, Republic of Korea, 8-14 </context>
<context position="8623" citStr="Sornlertlamvanich et al., 2008" startWordPosition="1314" endWordPosition="1317">pt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single 166 unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. However, while the approach is attractive conceptually, previous research has only been shown effective for closely related language pairs (Vilar et al., 2007; Tiedemann, 2009; Sornlertlamvanich et al., 2008). In this work, we propose effective alignment techniques that allow character-based translation to achieve accurate translation results for both close and distant language pairs. 3 Alignment Methods SMT systems are generally constructed from a parallel corpus consisting of target language sentences £ and source language sentences T. The first step of training is to find alignments A for the words in each sentence pair. We represent our target and source sentences as e�1 and fJ1. ei and fj represent single elements of the target and source sentences respectively. These may be words in word-bas</context>
</contexts>
<marker>Sornlertlamvanich, Mokarat, Isahara, 2008</marker>
<rawString>Virach Sornlertlamvanich, Chumpol Mokarat, and Hitoshi Isahara. 2008. Thai-lao machine translation based on phoneme transfer. In Proc. 14th Annual Meeting of the Association for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Subotin</author>
</authors>
<title>An exponential translation model for target language morphology.</title>
<date>2011</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="7081" citStr="Subotin, 2011" startWordPosition="1078" endWordPosition="1079">oblem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt wi</context>
</contexts>
<marker>Subotin, 2011</marker>
<rawString>Michael Subotin. 2011. An exponential translation model for target language morphology. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Modelling lexical redundancy for machine translation. In</title>
<date>2006</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="6817" citStr="Talbot and Osborne, 2006" startWordPosition="1034" endWordPosition="1037">s in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliter</context>
</contexts>
<marker>Talbot, Osborne, 2006</marker>
<rawString>David Talbot and Miles Osborne. 2006. Modelling lexical redundancy for machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Character-based PSMT for closely related languages.</title>
<date>2009</date>
<booktitle>In Proc. 13th Annual Conference of the European Association for Machine Translation.</booktitle>
<contexts>
<context position="2967" citStr="Tiedemann, 2009" startWordPosition="444" endWordPosition="445">number regularization, optimizing word segmentation, and transliteration, which we outline in more detail in Section 2. These difficulties occur because we are translating sequences of words as our basic unit. On the other hand, Vilar et al. (2007) examine the possibility of instead treating each sentence as sequences of characters to be translated. This method is attractive, as it is theoretically able to handle all sparsity phenomena in a single unified framework, but has only been shown feasible between similar language pairs such as Spanish-Catalan (Vilar et al., 2007), Swedish-Norwegian (Tiedemann, 2009), and ThaiLao (Sornlertlamvanich et al., 2008), which have a strong co-occurrence between single characters. As Vilar et al. (2007) state and we confirm, accurate translations cannot be achieved when applying traditional translation techniques to character-based translation for less similar language pairs. In this paper, we propose improvements to the alignment process tailored to character-based machine translation, and demonstrate that it is, in fact, possible to achieve translation accuracies that ap165 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</context>
<context position="8590" citStr="Tiedemann, 2009" startWordPosition="1312" endWordPosition="1313">nglish) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single 166 unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. However, while the approach is attractive conceptually, previous research has only been shown effective for closely related language pairs (Vilar et al., 2007; Tiedemann, 2009; Sornlertlamvanich et al., 2008). In this work, we propose effective alignment techniques that allow character-based translation to achieve accurate translation results for both close and distant language pairs. 3 Alignment Methods SMT systems are generally constructed from a parallel corpus consisting of target language sentences £ and source language sentences T. The first step of training is to find alignments A for the words in each sentence pair. We represent our target and source sentences as e�1 and fJ1. ei and fj represent single elements of the target and source sentences respectivel</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>J¨org Tiedemann. 2009. Character-based PSMT for closely related languages. In Proc. 13th Annual Conference of the European Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Jan-T Peter</author>
<author>Hermann Ney</author>
</authors>
<title>Can we translate letters.</title>
<date>2007</date>
<booktitle>In Proc. WMT.</booktitle>
<contexts>
<context position="2599" citStr="Vilar et al. (2007)" startWordPosition="386" endWordPosition="389">on-punctuated words were treated as different entities. Sparsity also manifests itself in other forms, including the large vocabularies produced by morphological productivity, word compounding, numbers, and proper names. A myriad of methods have been proposed to handle each of these phenomena individually, including morphological analysis, stemming, compound breaking, number regularization, optimizing word segmentation, and transliteration, which we outline in more detail in Section 2. These difficulties occur because we are translating sequences of words as our basic unit. On the other hand, Vilar et al. (2007) examine the possibility of instead treating each sentence as sequences of characters to be translated. This method is attractive, as it is theoretically able to handle all sparsity phenomena in a single unified framework, but has only been shown feasible between similar language pairs such as Spanish-Catalan (Vilar et al., 2007), Swedish-Norwegian (Tiedemann, 2009), and ThaiLao (Sornlertlamvanich et al., 2008), which have a strong co-occurrence between single characters. As Vilar et al. (2007) state and we confirm, accurate translations cannot be achieved when applying traditional translation</context>
<context position="8573" citStr="Vilar et al., 2007" startWordPosition="1308" endWordPosition="1311">s in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single 166 unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. However, while the approach is attractive conceptually, previous research has only been shown effective for closely related language pairs (Vilar et al., 2007; Tiedemann, 2009; Sornlertlamvanich et al., 2008). In this work, we propose effective alignment techniques that allow character-based translation to achieve accurate translation results for both close and distant language pairs. 3 Alignment Methods SMT systems are generally constructed from a parallel corpus consisting of target language sentences £ and source language sentences T. The first step of training is to find alignments A for the words in each sentence pair. We represent our target and source sentences as e�1 and fJ1. ei and fj represent single elements of the target and source sent</context>
<context position="31885" citStr="Vilar et al. (2007)" startWordPosition="5024" endWordPosition="5027">ntly, they allow for more aggressive beam pruning, increasing sampling speed from 1.3 sent/s to 2.5 sent/s for Finnish, and 6.8 sent/s to 11.6 sent/s for Japanese. 7 Conclusion and Future Directions This paper demonstrated that character-based translation can act as a unified framework for handling difficult problems in translation: morphology, compound words, transliteration, and segmentation. One future challenge includes scaling training up to longer sentences, which can likely be achieved through methods such as the heuristic span pruning of Haghighi et al. (2009) or sentence splitting of Vilar et al. (2007). Monolingual data could also be used to improve estimates of our substring-based prior. In addition, error analysis showed that wordbased translation performed better than characterbased translation on reordering and lexical choice, indicating that improved decoding (or pre-ordering) and language modeling tailored to character-based translation will likely greatly improve accuracy. Finally, we plan to explore the middle ground between word-based and character based translation, allowing for the flexibility of character-based translation, while using word boundary information to increase effic</context>
</contexts>
<marker>Vilar, Peter, Ney, 2007</marker>
<rawString>David Vilar, Jan-T. Peter, and Hermann Ney. 2007. Can we translate letters. In Proc. WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="9697" citStr="Vogel et al., 1996" startWordPosition="1493" endWordPosition="1496">rce sentences as e�1 and fJ1. ei and fj represent single elements of the target and source sentences respectively. These may be words in word-based alignment models or single characters in character-based alignment models.l We define our alignment as aK , where each element is a span ak = (s, t, u, v) indicating that the target string es, ... , et and source string fu, ... , f„ are aligned to each-other. 3.1 One-to-Many Alignment The most well-known and widely-used models for bitext alignment are for one-to-many alignment, including the IBM models (Brown et al., 1993) and HMM alignment model (Vogel et al., 1996). These models are by nature directional, attempting to find the alignments that maximize the conditional probability of the target sentence P(ei|fJ1, aK ). For computational reasons, the IBM models are restricted to aligning each word on the target side to a single word on the source side. In the formalism presented above, this means that each ei must be included in at most one span, and for each span u = v. Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al., 2003). However, in order for one-to-many alignment me</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="4464" citStr="Wu, 1997" startWordPosition="666" endWordPosition="667">th of units to be aligned. As these units may be at the character, subword, word, or multi-word phrase level, we conjecture that this will allow for better character alignments than one-to-many alignment techniques, and will allow for better translation of uncommon words than traditional word-based models by breaking down words into their component parts. We also propose two improvements to the manyto-many alignment method of Neubig et al. (2011). One barrier to applying many-to-many alignment models to character strings is training cost. In the inversion transduction grammar (ITG) framework (Wu, 1997), which is widely used in many-to-many alignment, search is cumbersome for longer sentences, a problem that is further exacerbated when using characters instead of words as the basic unit. As a step towards overcoming this difficulty, we increase the efficiency of the beam-search technique of Saers et al. (2009) by augmenting it with look-ahead probabilities in the spirit of A* search. Secondly, we describe a method to seed the search process using counts of all substring pairs in the corpus to bias the phrase alignment model. We do this by defining prior probabilities based on these substring</context>
<context position="11431" citStr="Wu (1997)" startWordPosition="1772" endWordPosition="1773">ent years, there have been advances in many-to-many alignment techniques that are able to align multi-element chunks on both sides of the translation (Marcu and Wong, 2002; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). Many-to-many methods can be expected to achieve superior results on character-based alignment, as the aligner can use information about substrings, which may correspond to letters, morphemes, words, or short phrases. Here, we focus on the model presented by Neubig et al. (2011), which uses Bayesian inference in the phrasal inversion transduction grammar (ITG, Wu (1997)) framework. ITGs are a variety of synchronous context free grammar (SCFG) that allows for many-to-many alignment to be achieved in polynomial time through the process of biparsing, which we explain more in the following section. Phrasal ITGs are ITGs that allow for non-terminals that can emit phrase pairs with multiple elements on both the source and target sides. It should be noted that there are other many-to-many alignment methods that have been used for simultaneously discovering morphological boundaries over multiple languages (Snyder and Barzilay, 2008; Naradowsky and Toutanova, 2011), </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Stochastic lexicalized inversion transduction grammar for alignment.</title>
<date>2005</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="15697" citStr="Zhang and Gildea, 2005" startWordPosition="2477" endWordPosition="2480">cularly, the alignment “les/1960s” competes with the high-probability alignment “les/the,” so intuitively should be a good candidate for pruning. However its probability is only slightly higher than “ann´ees/1960s,” which has no competing hypotheses and thus should not be trimmed. In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I(ak), but also the outside probability O(ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for wordbased ITGs (Zhang and Gildea, 2005). As the calculation of the actual outside probability O(ak) is just as expensive as parsing itself, it is necessary to approximate this with heuristic function O* that can be calculated efficiently. Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2, compared with the n3 amortized time of the tic-tac-toe pruning 3Applying beam-search before sampling will sample from an improper distribution, although Metropolis-in-Gibbs sampling (Johnson et al., 2007) can be used to compensate. However, we found that this had </context>
</contexts>
<marker>Zhang, Gildea, 2005</marker>
<rawString>Hao Zhang and Daniel Gildea. 2005. Stochastic lexicalized inversion transduction grammar for alignment. In Proc. ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
</authors>
<location>and</location>
<marker>Zhang, Quirk, Moore, </marker>
<rawString>Hao Zhang, Chris Quirk, Robert C. Moore, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of non-compositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>Proc. ACL.</booktitle>
<marker>Gildea, 2008</marker>
<rawString>Daniel Gildea. 2008a. Bayesian learning of non-compositional phrases with synchronous parsing. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Keiji Yasuda</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Improved statistical machine translation by multiple Chinese word segmentation.</title>
<date>2008</date>
<booktitle>In Proc. WMT.</booktitle>
<contexts>
<context position="7838" citStr="Zhang et al., 2008" startWordPosition="1197" endWordPosition="1200">n to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single 166 unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. However, while the appr</context>
<context position="16434" citStr="Zhang et al., 2008" startWordPosition="2593" endWordPosition="2596"> approximate this with heuristic function O* that can be calculated efficiently. Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2, compared with the n3 amortized time of the tic-tac-toe pruning 3Applying beam-search before sampling will sample from an improper distribution, although Metropolis-in-Gibbs sampling (Johnson et al., 2007) can be used to compensate. However, we found that this had no significant effect on results, so we omit the Metropolis-in-Gibbs step for experiments. 168 algorithm described by (Zhang et al., 2008a). During the calculation of the phrase generation probabilities Pt, we save the best inside probability I∗ for each monolingual span. I∗ e (s,t) = max {˜a=h˜s,˜t,˜u,˜vi;˜s=s,˜t=t} I∗f (u, v) = max {˜a=h˜s,˜t,˜u,˜vi;˜u=u,˜v=v} For each language independently, we calculate forward probabilities α and backward probabilities β. For example, αe(s) is the maximum probability of the span (0, s) of e that can be created by concatenating together consecutive values of I∗e: I∗e (0, S1)I∗e (S1, S2) ... I∗e (Sx, s). Backwards probabilities and probabilities over f can be defined similarly. These probabi</context>
</contexts>
<marker>Zhang, Yasuda, Sumita, 2008</marker>
<rawString>Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita. 2008b. Improved statistical machine translation by multiple Chinese word segmentation. In Proc. WMT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>