<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.851338">
Template-Based Information Extraction without the Templates
</title>
<author confidence="0.986526">
Nathanael Chambers and Dan Jurafsky
</author>
<affiliation confidence="0.987038">
Department of Computer Science
Stanford University
</affiliation>
<email confidence="0.998835">
{natec,jurafsky}@stanford.edu
</email>
<sectionHeader confidence="0.994778" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999785086956522">
Standard algorithms for template-based in-
formation extraction (IE) require predefined
template schemas, and often labeled data,
to learn to extract their slot fillers (e.g., an
embassy is the Target of a Bombing tem-
plate). This paper describes an approach to
template-based IE that removes this require-
ment and performs extraction without know-
ing the template structure in advance. Our al-
gorithm instead learns the template structure
automatically from raw text, inducing tem-
plate schemas as sets of linked events (e.g.,
bombings include detonate, set off, and de-
stroy events) associated with semantic roles.
We also solve the standard IE task, using the
induced syntactic patterns to extract role fillers
from specific documents. We evaluate on the
MUC-4 terrorism dataset and show that we in-
duce template structure very similar to hand-
created gold structure, and we extract role
fillers with an F1 score of .40, approaching
the performance of algorithms that require full
knowledge of the templates.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954086956522">
A template defines a specific type of event (e.g.,
a bombing) with a set of semantic roles (or slots)
for the typical entities involved in such an event
(e.g., perpetrator, target, instrument). In contrast to
work in relation discovery that focuses on learning
atomic facts (Banko et al., 2007a; Carlson et al.,
2010), templates can extract a richer representation
of a particular domain. However, unlike relation dis-
covery, most template-based IE approaches assume
foreknowledge of the domain’s templates. Very little
work addresses how to learn the template structure
itself. Our goal in this paper is to perform the stan-
dard template filling task, but to first automatically
induce the templates from an unlabeled corpus.
There are many ways to represent events, rang-
ing from role-based representations such as frames
(Baker et al., 1998) to sequential events in scripts
(Schank and Abelson, 1977) and narrative schemas
(Chambers and Jurafsky, 2009; Kasch and Oates,
2010). Our approach learns narrative-like knowl-
edge in the form of IE templates; we learn sets of
related events and semantic roles, as shown in this
sample output from our system:
</bodyText>
<subsectionHeader confidence="0.898567">
Bombing Template
</subsectionHeader>
<bodyText confidence="0.880330772727273">
{detonate, blow up, plant, explode, defuse, destroy}
Perpetrator: Person who detonates, plants, blows up
Instrument: Object that is planted, detonated, defused
Target: Object that is destroyed, is blown up
A semantic role, such as target, is a cluster of syn-
tactic functions of the template’s event words (e.g.,
the objects of detonate and explode). Our goal is
to characterize a domain by learning this template
structure completely automatically. We learn tem-
plates by first clustering event words based on their
proximity in a training corpus. We then use a novel
approach to role induction that clusters the syntactic
functions of these events based on selectional prefer-
ences and coreferring arguments. The induced roles
are template-specific (e.g., perpetrator), not univer-
sal (e.g., agent or patient) or verb-specific.
After learning a domain’s template schemas, we
perform the standard IE task of role filling from in-
dividual documents, for example:
Perpetrator: guerrillas
Instrument: dynamite
Target: embassy
</bodyText>
<page confidence="0.969021">
976
</page>
<note confidence="0.9797555">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 976–986,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.987009785714286">
This extraction stage identifies entities using the ing the same exact event (e.g. Hurricane Ivan), and
learned syntactic functions of our roles. We evalu- observing repeated word patterns across documents
ate on the MUC-4 terrorism corpus with results ap- connecting the same proper nouns. Learned patterns
proaching those of supervised systems. represent binary relations, and they show how to
The core of this paper focuses on how to char- construct tables of extracted entities for these rela-
acterize a domain-specific corpus by learning rich tions. Our approach draws on this idea of using un-
template structure. We describe how to first expand labeled documents to discover relations in text, and
the small corpus’ size, how to cluster its events, and of defining semantic roles by sets of entities. How-
finally how to induce semantic roles. Section 5 then ever, the limitations to their approach are that (1)
describes the extraction algorithm, followed by eval- redundant documents about specific events are re-
uations against previous work in section 6 and 7. quired, (2) relations are binary, and (3) only slots
2 Previous Work with named entities are learned. We will extend
Many template extraction algorithms require full their work by showing how to learn without these
knowledge of the templates and labeled corpora, assumptions, obviating the need for redundant doc-
such as in rule-based systems (Chinchor et al., 1993; uments, and learning templates with any type and
Rau et al., 1992) and modern supervised classi- any number of slots.
fiers (Freitag, 1998; Chieu et al., 2003; Bunescu Large-scale learning of scripts and narrative
and Mooney, 2004; Patwardhan and Riloff, 2009). schemas also captures template-like knowledge
Classifiers rely on the labeled examples’ surround- from unlabeled text (Chambers and Jurafsky, 2008;
ing context for features such as nearby tokens, doc- Kasch and Oates, 2010). Scripts are sets of re-
ument position, syntax, named entities, semantic lated event words and semantic roles learned by
classes, and discourse relations (Maslennikov and linking syntactic functions with coreferring argu-
Chua, 2007). Ji and Grishman (2008) also supple- ments. While they learn interesting event structure,
mented labeled with unlabeled data. the structures are limited to frequent topics in a large
Weakly supervised approaches remove some of corpus. We borrow ideas from this work as well, but
the need for fully labeled data. Most still require the our goal is to instead characterize a specific domain
templates and their slots. One common approach is with limited data. Further, we are the first to apply
to begin with unlabeled, but clustered event-specific this knowledge to the IE task of filling in template
documents, and extract common word patterns as mentions in documents.
extractors (Riloff and Schmelzenbach, 1998; Sudo In summary, our work extends previous work on
et al., 2003; Riloff et al., 2005; Patwardhan and unsupervised IE in a number of ways. We are the
Riloff, 2007). Filatova et al. (2006) integrate named first to learn MUC-4 templates, and we are the first
entities into pattern learning (PERSON won) to ap- to extract entities without knowing how many tem-
proximate unknown semantic roles. Bootstrapping plates exist, without examples of slot fillers, and
with seed examples of known slot fillers has been without event-clustered documents.
shown to be effective (Surdeanu et al., 2006; Yan- 3 The Domain and its Templates
garber et al., 2000). In contrast, this paper removes Our goal is to learn the general event structure of
these data assumptions, learning instead from a cor- a domain, and then extract the instances of each
pus of unknown events and unclustered documents, learned event. In order to measure performance
without seed examples. in both tasks (learning structure and extracting in-
Shinyama and Sekine (2006) describe an ap- stances), we use the terrorism corpus of MUC-4
proach to template learning without labeled data. (Sundheim, 1991) as our target domain. This cor-
They present unrestricted relation discovery as a pus was chosen because it is annotated with tem-
means of discovering relations in unlabeled docu- plates that describe all of the entities involved in
ments, and extract their fillers. Central to the al- each event. An example snippet from a bombing
gorithm is collecting multiple documents describ- document is given here:
977
The terrorists used explosives against the
town hall. El Comercio reported that alleged
Shining Path members also attacked public fa-
cilities in huarpacha, Ambo, tomayquichua,
and kichki. Municipal official Sergio Horna
was seriously wounded in an explosion in
Ambo.
The entities from this document fill the following
slots in a MUC-4 bombing template.
</bodyText>
<subsectionHeader confidence="0.552917">
Perp: Shining Path members Victim: Sergio Horna
</subsectionHeader>
<bodyText confidence="0.987845176470588">
Target: public facilities Instrument: explosives
We focus on these four string-based slots1 from
the MUC-4 corpus, as is standard in this task. The
corpus consists of 1300 documents, 733 of which
are labeled with at least one template. There are six
types of templates, but only four are modestly fre-
quent: bombing (208 docs), kidnap (83 docs), attack
(479 docs), and arson (40 docs). 567 documents do
not have any templates. Our learning algorithm does
not know which documents contain (or do not con-
tain) which templates. After learning event words
that represent templates, we induce their slots, not
knowing a priori how many there are, and then fill
them in by extracting entities as in the standard task.
In our example above, the three bold verbs (use, at-
tack, wound) indicate the Bombing template, and
their syntactic arguments fill its slots.
</bodyText>
<sectionHeader confidence="0.813917" genericHeader="method">
4 Learning Templates from Raw Text
</sectionHeader>
<bodyText confidence="0.9999755">
Our goal is to learn templates that characterize a
domain as described in unclustered, unlabeled doc-
uments. This presents a two-fold problem to the
learner: it does not know how many events exist, and
it does not know which documents describe which
event (some may describe multiple events). We ap-
proach this problem with a three step process: (1)
cluster the domain’s event patterns to approximate
the template topics, (2) build a new corpus specific to
each cluster by retrieving documents from a larger
unrelated corpus, (3) induce each template’s slots
using its new (larger) corpus of documents.
</bodyText>
<subsectionHeader confidence="0.999531">
4.1 Clustering Events to Learn Templates
</subsectionHeader>
<bodyText confidence="0.9900845">
We cluster event patterns to create templates. An
event pattern is either (1) a verb, (2) a noun in Word-
</bodyText>
<footnote confidence="0.982503">
1There are two Perpetrator slots in MUC-4: Organization
and Individual. We consider their union as a single slot.
</footnote>
<bodyText confidence="0.999638583333333">
Net under the Event synset, or (3) a verb and the
head word of its syntactic object. Examples of each
include (1) ‘explode’, (2) ‘explosion’, and (3) ‘ex-
plode:bomb’. We also tag the corpus with an NER
system and allow patterns to include named entity
types, e.g., ‘kidnap:PERSON’. These patterns are
crucially needed later to learn a template’s slots.
However, we first need an algorithm to cluster these
patterns to learn the domain’s core events. We con-
sider two unsupervised algorithms: Latent Dirichlet
Allocation (LDA) (Blei et al., 2003), and agglomer-
ative clustering based on word distance.
</bodyText>
<sectionHeader confidence="0.554898" genericHeader="method">
4.1.1 LDA for Unknown Data
</sectionHeader>
<bodyText confidence="0.999943083333333">
LDA is a probabilistic model that treats documents
as mixtures of topics. It learns topics as discrete
distributions (multinomials) over the event patterns,
and thus meets our needs as it clusters patterns based
on co-occurrence in documents. The algorithm re-
quires the number of topics to be known ahead of
time, but in practice this number is set relatively high
and the resulting topics are still useful. Our best per-
forming LDA model used 200 topics. We had mixed
success with LDA though, and ultimately found our
next approach performed slightly better on the doc-
ument classification evaluation.
</bodyText>
<subsectionHeader confidence="0.962591">
4.1.2 Clustering on Event Distance
</subsectionHeader>
<bodyText confidence="0.993318263157895">
Agglomerative clustering does not require fore-
knowledge of the templates, but its success relies on
how event pattern similarity is determined.
Ideally, we want to learn that detonate and destroy
belong in the same cluster representing a bombing.
Vector-based approaches are often adopted to rep-
resent words as feature vectors and compute their
distance with cosine similarity. Unfortunately, these
approaches typically learn clusters of synonymous
words that can miss detonate and destroy. Our
goal is to instead capture world knowledge of co-
occuring events. We thus adopt an assumption that
closeness in the world is reflected by closeness in a
text’s discourse. We hypothesize that two patterns
are related if they occur near each other in a docu-
ment more often than chance.
Let g(wi, wj) be the distance between two events
(1 if in the same sentence, 2 in neighboring, etc). Let
%dist(wi, wj) be the distance-weighted frequency of
</bodyText>
<page confidence="0.996545">
978
</page>
<construct confidence="0.882389571428571">
kidnap: kidnap, kidnap:PER, abduct, release, kidnap-
ping, ransom, robbery, registration
bombing: explode, blow up, locate, place:bomb, det-
onate, damage, explosion, cause, damage, ...
attack: kill, shoot down, down, kill:civilian, kill:PER,
kill:soldier, kill:member, killing, shoot:PER, wave,...
arson: burn, search, burning, clip, collaborate, ...
</construct>
<figureCaption confidence="0.869625">
Figure 1: The 4 clusters mapped to MUC-4 templates.
two events occurring together:
</figureCaption>
<equation confidence="0.9838565">
�Cdist(wi,wj) =
dED
</equation>
<bodyText confidence="0.999982">
where d is a document in the set of all documents
D. The base 4 logarithm discounts neighboring sen-
tences by 0.5 and within the same sentence scores 1.
Using this definition of distance, pointwise mutual
information measures our similarity of two events:
</bodyText>
<equation confidence="0.990369333333333">
pmi(wi, wj) = Pdist(wi, wj)/(P(wi)P(wj)) (2)
C(wi)
P(wi) = (3)
Ej C(wj)
Cdist(wi, wj)
Ek El Cdist(wk, wl)
</equation>
<bodyText confidence="0.999991">
We run agglomerative clustering with pmi over
all event patterns. Merging decisions use the average
link score between all new links across two clusters.
As with all clustering algorithms, a stopping crite-
rion is needed. We continue merging clusters un-
til any single cluster grows beyond m patterns. We
briefly inspected the clustering process and chose
m = 40 to prevent learned scenarios from intuitively
growing too large and ambiguous. Post-evaluation
analysis shows that this value has wide flexibility.
For example, the Kidnap and Arson clusters are un-
changed in 30 &lt; m &lt; 80, and Bombing unchanged
in 30 &lt; m &lt; 50. Figure 1 shows 3 clusters (of 77
learned) that characterize the main template types.
</bodyText>
<subsectionHeader confidence="0.982787">
4.2 Information Retrieval for Templates
</subsectionHeader>
<bodyText confidence="0.99985945">
Learning a domain often suffers from a lack of train-
ing data. The previous section clustered events from
the MUC-4 corpus, but its 1300 documents do not
provide enough examples of verbs and argument
counts to further learn the semantic roles in each
cluster. Our solution is to assemble a larger IR-
corpus of documents for each cluster. For exam-
ple, MUC-4 labels 83 documents with Kidnap, but
our learned cluster (kidnap, abduct, release, ...) re-
trieved 3954 documents from a general corpus.
We use the Associated Press and New York Times
sections of the Gigaword Corpus (Graff, 2002) as
our general corpus. These sections include approxi-
mately 3.5 million news articles spanning 12 years.
Our retrieval algorithm retrieves documents that
score highly with a cluster’s tokens. The docu-
ment score is defined by two common metrics: word
match, and word coverage. A document’s match
score is defined as the average number of times the
words in cluster c appear in document d:
</bodyText>
<equation confidence="0.945575">
avgm(d, c) = EwEc EtEd 11w = t} (5)
jcj
</equation>
<bodyText confidence="0.975516894736842">
We define word coverage as the number of seen
cluster words. Coverage penalizes documents that
score highly by repeating a single cluster word a lot.
We only score a document if its coverage, cvg(d, c),
is at least 3 words (or less for tiny clusters):
ir(d, c) = r avgm(d, c) if cvg(d, c) &gt; min(3, jcj/4)
l 0 otherwise
A document d is retrieved for a cluster c if
ir(d, c) &gt; 0.4. Finally, we emphasize precision
by pruning away 50% of a cluster’s retrieved doc-
uments that are farthest in distance from the mean
document of the retrieved set. Distance is the co-
sine similarity between bag-of-words vector repre-
sentations. The confidence value of 0.4 was chosen
from a manual inspection among a single cluster’s
retrieved documents. Pruning 50% was arbitrarily
chosen to improve precision, and we did not exper-
iment with other quantities. A search for optimum
parameter values may lead to better results.
</bodyText>
<subsectionHeader confidence="0.999619">
4.3 Inducing Semantic Roles (Slots)
</subsectionHeader>
<bodyText confidence="0.999889">
Having successfully clustered event words and re-
trieved an IR-corpus for each cluster, we now ad-
dress the problem of inducing semantic roles. Our
learned roles will then extract entities in the next sec-
tion and we will evaluate their per-role accuracy.
Most work on unsupervised role induction fo-
cuses on learning verb-specific roles, starting with
seed examples (Swier and Stevenson, 2004; He and
</bodyText>
<equation confidence="0.999064">
E 1 − log4(g(wi, wj)) (1)
w;,wjEd
P (4)
dist(wi, wj) =
</equation>
<page confidence="0.987303">
979
</page>
<bodyText confidence="0.9999608">
Gildea, 2006) and/or knowing the number of roles
(Grenager and Manning, 2006; Lang and Lapata,
2010). Our previous work (Chambers and Juraf-
sky, 2009) learned situation-specific roles over nar-
rative schemas, similar to frame roles in FrameNet
(Baker et al., 1998). Schemas link the syntactic rela-
tions of verbs by clustering them based on observing
coreferring arguments in those positions. This paper
extends this intuition by introducing a new vector-
based approach to coreference similarity.
</bodyText>
<subsubsectionHeader confidence="0.629561">
4.3.1 Syntactic Relations as Roles
</subsubsectionHeader>
<bodyText confidence="0.994643">
We learn the roles of cluster C by clustering the syn-
tactic relations RC of its words. Consider the fol-
lowing example:
</bodyText>
<equation confidence="0.9945665">
C = {go off, explode, set off, damage, destroy}
RC = {go off:s, go off:p in, explode:s, set off:s}
</equation>
<bodyText confidence="0.99585">
where verb:s is the verb’s subject, :o the object, and
p in a preposition. We ideally want to cluster RC as:
</bodyText>
<equation confidence="0.994365">
bomb = {go off:s, explode:s, set off:o, destroy:s}
suspect = {set off:s}
target = {go off:p in, destroy:o}
</equation>
<bodyText confidence="0.987010648648648">
We want to cluster all subjects, objects, and
prepositions. Passive voice is normalized to active2.
We adopt two views of relation similarity:
coreferring arguments and selectional preferences.
Chambers and Jurafsky (2008) observed that core-
ferring arguments suggest a semantic relation be-
tween two predicates. In the sentence, he ran and
then he fell, the subjects of run and fall corefer, and
so they likely belong to the same scenario-specific
semantic role. We applied this idea to a new vec-
tor similarity framework. We represent a relation
as a vector of all relations with which their argu-
ments coreferred. For instance, arguments of the
relation go off:s were seen coreferring with men-
tions in plant:o, set off:o and injure:s. We represent
go off:s as a vector of these relation counts, calling
this its coref vector representation.
Selectional preferences (SPs) are also useful in
measuring similarity (Erk and Pado, 2008). A re-
lation can be represented as a vector of its observed
arguments during training. The SPs for go off:s in
our data include {bomb, device, charge, explosion}.
We measure similarity using cosine similarity be-
tween the vectors in both approaches. However,
2We use the Stanford Parser at nlp.stanford.edu/software
coreference and SPs measure different types of sim-
ilarity. Coreference is a looser narrative similarity
(bombings cause injuries), while SPs capture syn-
onymy (plant and place have similar arguments). We
observed that many narrative relations are not syn-
onymous, and vice versa. We thus take the max-
imum of either cosine score as our final similarity
metric between two relations. We then back off to
the average of the two cosine scores if the max is not
confident (less than 0.7); the average penalizes the
pair. We chose the value of 0.7 from a grid search to
optimize extraction results on the training set.
</bodyText>
<subsectionHeader confidence="0.917463">
4.3.2 Clustering Syntactic Functions
</subsectionHeader>
<bodyText confidence="0.99863">
We use agglomerative clustering with the above
pairwise similarity metric. Cluster similarity is the
average link score over all new links crossing two
clusters. We include the following sparsity penalty
r(ca, cb) if there are too few links between clusters
ca and cb.
</bodyText>
<equation confidence="0.962385">
score(ca, cb) =
r(ca,cb) = Ew;Eca EwjEcb 1{sim(wi,wj) &gt; 0} (7)
Ew;Eca EwjEcb 1
</equation>
<bodyText confidence="0.99996">
This penalizes clusters from merging when they
share only a few high scoring edges. Clustering
stops when the merged cluster scores drop below
a threshold optimized to extraction performance on
the training data.
We also begin with two assumptions about syntac-
tic functions and semantic roles. The first assumes
that the subject and object of a verb carry different
semantic roles. For instance, the subject of sell fills
a different role (Seller) than the object (Good). The
second assumption is that each semantic role has a
high-level entity type. For instance, the subject of
sell is a Person or Organization, and the object is a
Physical Object.
We implement the first assumption as a constraint
in the clustering algorithm, preventing two clusters
from merging if their union contains the same verb’s
subject and object.
We implement the second assumption by auto-
matically labeling each syntactic function with a role
type based on its observed arguments. The role types
are broad general classes: Person/Org, Physical Ob-
ject, or Other. A syntactic function is labeled as a
</bodyText>
<equation confidence="0.865593">
E E sim(wi, wj)∗r(ca, cb) (6)
w;Eca wjEcb
</equation>
<page confidence="0.945701">
980
</page>
<table confidence="0.993133">
Bombing Template (MUC-4)
Perpetrator Person/Org who detonates, blows up, plants,
hurls, stages, is detained, is suspected, is blamed on,
launches
Instrument A physical object that is exploded, explodes, is
hurled, causes, goes off, is planted, damages, is set off, is
defused
Target A physical object that is damaged, is destroyed, is
exploded at, is damaged, is thrown at, is hit, is struck
Police Person/Org who raids, questions, discovers, investi-
gates, defuses, arrests
N/A A physical object that is blown up, destroys
Attack/Shooting Template (MUC-4)
</table>
<tableCaption confidence="0.7346155">
Perpetrator Person/Org who assassinates, patrols, am-
bushes, raids, shoots, is linked to
Victim Person/Org who is assassinated, is toppled, is gunned
down, is executed, is evacuated
Target Person/Org who is hit, is struck, is downed, is set fire
to, is blown up, surrounded
</tableCaption>
<table confidence="0.820277913043478">
Instrument A physical object that is fired, injures, downs, is
set off, is exploded
Kidnap Template (MUC-4)
Perpetrator Person/Org who releases, abducts, kidnaps,
ambushes, holds, forces, captures, is imprisoned, frees
Target Person/Org who is kidnapped, is released, is freed,
escapes, disappears, travels, is harmed, is threatened
Police Person/Org who rules out, negotiates, condemns, is
pressured, finds, arrests, combs
Weapons Smuggling Template (NEW)
Perpetrator Person/Org who smuggles, is seized from, is
captured, is detained
Police Person/Org who raids, seizes, captures, confiscates,
detains, investigates
Instrument A physical object that is smuggled, is seized, is
confiscated, is transported
Election Template (NEW)
Voter Person/Org who chooses, is intimidated, favors, is ap-
pealed to, turns out
Government Person/Org who authorizes, is chosen, blames,
authorizes, denies
Candidate Person/Org who resigns, unites, advocates, ma-
nipulates, pledges, is blamed
</table>
<figureCaption confidence="0.993455">
Figure 2: Five learned example templates. All knowledge except the template/role names (e.g., ‘Victim’) is learned.
</figureCaption>
<bodyText confidence="0.999009736842105">
class if 20% of its arguments appear under the cor-
responding WordNet synset3, or if the NER system
labels them as such. Once labeled by type, we sep-
arately cluster the syntactic functions for each role
type. For instance, Person functions are clustered
separate from Physical Object functions. Figure 2
shows some of the resulting roles.
Finally, since agglomerative clustering makes
hard decisions, related events to a template may have
been excluded in the initial event clustering stage.
To address this problem, we identify the 200 nearby
events to each event cluster. These are simply the
top scoring event patterns with the cluster’s original
events. We add their syntactic functions to their best
matching roles. This expands the coverage of each
learned role. Varying the 200 amount does not lead
to wide variation in extraction performance. Once
induced, the roles are evaluated by their entity ex-
traction performance in Section 5.
</bodyText>
<subsectionHeader confidence="0.994791">
4.4 Template Evaluation
</subsectionHeader>
<bodyText confidence="0.999009333333333">
We now compare our learned templates to those
hand-created by human annotators for the MUC-4
terrorism corpus. The corpus contains 6 template
</bodyText>
<footnote confidence="0.441011">
3Physical objects are defined as non-person physical objects
</footnote>
<subsectionHeader confidence="0.601638">
Bombing Kidnap Attack Arson
</subsectionHeader>
<equation confidence="0.98220375">
Perpetrator x x x x
Victim x x x x
Target x x x
Instrument x x
</equation>
<figureCaption confidence="0.999195">
Figure 3: Slots in the hand-crafted MUC-4 templates.
</figureCaption>
<bodyText confidence="0.998612777777778">
types, but two of them occur in only 4 and 14 of the
1300 training documents. We thus only evaluate the
4 main templates (bombing, kidnapping, attack, and
arson). The gold slots are shown in figure 3.
We evaluate the four learned templates that score
highest in the document classification evaluation
(to be described in section 5.1), aligned with their
MUC-4 types. Figure 2 shows three of our four tem-
plates, and two brand new ones that our algorithm
learned. Of the four templates, we learned 12 of the
13 semantic roles as created for MUC. In addition,
we learned a new role not in MUC for bombings,
kidnappings, and arson: the Police or Authorities
role. The annotators chose not to include this in their
labeling, but this knowledge is clearly relevant when
understanding such events, so we consider it correct.
There is one additional Bombing and one Arson role
that does not align with MUC-4, marked incorrect.
</bodyText>
<page confidence="0.997061">
981
</page>
<bodyText confidence="0.9996135">
We thus report 92% slot recall, and precision as 14
of 16 (88%) learned slots.
We only measure agreement with the MUC tem-
plate schemas, but our system learns other events as
well. We show two such examples in figure 2: the
Weapons Smuggling and Election Templates.
</bodyText>
<sectionHeader confidence="0.996593" genericHeader="method">
5 Information Extraction: Slot Filling
</sectionHeader>
<bodyText confidence="0.9999758">
We now present how to apply our learned templates
to information extraction. This section will describe
how to extract slot fillers using our templates, but
without knowing which templates are correct.
We could simply use a standard IE approach, for
example, creating seed words for our new learned
templates. But instead, we propose a new method
that obviates the need for even a limited human la-
beling of seed sets. We consider each learned se-
mantic role as a potential slot, and we extract slot
fillers using the syntactic functions that were previ-
ously learned. Thus, the learned syntactic patterns
(e.g., the subject of release) serve the dual purpose
of both inducing the template slots, and extracting
appropriate slot fillers from text.
</bodyText>
<subsectionHeader confidence="0.991438">
5.1 Document Classification
</subsectionHeader>
<bodyText confidence="0.9999063">
A document is labeled for a template if two different
conditions are met: (1) it contains at least one trig-
ger phrase, and (2) its average per-token conditional
probability meets a strict threshold.
Both conditions require a definition of the condi-
tional probability of a template given a token. The
conditional is defined as the token’s importance rel-
ative to its uniqueness across all templates. This
is not the usual conditional probability definition as
IR-corpora are different sizes.
</bodyText>
<equation confidence="0.9988465">
P(t |w) =
EsET PIR. )(w) (8)
</equation>
<bodyText confidence="0.9683785">
where PIR,(w) is the probability of pattern w in the
IR-corpus of template t.
</bodyText>
<equation confidence="0.995940666666667">
Ct(w)
PIR,(w) = (9)
&amp; Ct(v)
</equation>
<bodyText confidence="0.982548">
where Ct(w) is the number of times word w appears
in the IR-corpus of template t. A template’s trigger
words are defined as words satisfying P(tIw) &gt; 0.2.
</bodyText>
<table confidence="0.9932315">
Kidnap Bomb Attack Arson
Precision .64 .83 .66 .30
Recall .54 .63 .35 1.0
F1 .58 .72 .46 .46
</table>
<figureCaption confidence="0.986173">
Figure 4: Document classification results on test.
</figureCaption>
<bodyText confidence="0.999543090909091">
Trigger phrases are thus template-specific patterns
that are highly indicative of that template.
After identifying triggers, we use the above defi-
nition to score a document with a template. A doc-
ument is labeled with a template if it contains at
least one trigger, and its average word probability
is greater than a parameter optimized on the training
set. A document can be (and often is) labeled with
multiple templates.
Finally, we label the sentences that contain trig-
gers and use them for extraction in section 5.2.
</bodyText>
<subsubsectionHeader confidence="0.956488">
5.1.1 Experiment: Document Classification
</subsubsectionHeader>
<bodyText confidence="0.999931166666667">
The MUC-4 corpus links templates to documents,
allowing us to evaluate our document labels. We
treat each link as a gold label (kidnap, bomb, or
attack) for that document, and documents can have
multiple labels. Our learned clusters naturally do not
have MUC labels, so we report results on the four
clusters that score highest with each label.
Figure 4 shows the document classification
scores. The bombing template performs best with
an F1 score of .72. Arson occurs very few times,
and Attack is lower because it is essentially an ag-
glomeration of diverse events (discussed later).
</bodyText>
<subsectionHeader confidence="0.999498">
5.2 Entity Extraction
</subsectionHeader>
<bodyText confidence="0.99996125">
Once documents are labeled with templates, we next
extract entities into the template slots. Extraction oc-
curs in the trigger sentences from the previous sec-
tion. The extraction process is two-fold:
</bodyText>
<listItem confidence="0.988646">
1. Extract all NPs that are arguments of patterns in the
template’s induced roles.
2. Extract NPs whose heads are observed frequently
with one of the roles (e.g., ‘bomb’ is seen with In-
strument relations in figure 2).
</listItem>
<bodyText confidence="0.949980666666667">
Take the following MUC-4 sentence as an example:
The two bombs were planted with the exclusive
purpose of intimidating the owners of...
</bodyText>
<page confidence="0.994041">
982
</page>
<bodyText confidence="0.999936555555556">
The verb plant is in our learned bombing cluster, so
step (1) will extract its passive subject bombs and
map it to the correct instrument role (see figure 2).
The human target, owners, is missed because intim-
idate was not learned. However, if owner is in the
selectional preferences of the learned ‘human target’
role, step (2) correctly extracts it into that role.
These are two different, but complementary,
views of semantic roles. The first is that a role is de-
fined by the set of syntactic relations that describe it.
Thus, we find all role relations and save their argu-
ments (pattern extraction). The second view is that
a role is defined by the arguments that fill it. Thus,
we extract all arguments that filled a role in training,
regardless of their current syntactic environment.
Finally, we filter extractions whose WordNet or
named entity label does not match the learned slot’s
type (e.g., a Location does not match a Person).
</bodyText>
<sectionHeader confidence="0.920606" genericHeader="method">
6 Standard Evaluation
</sectionHeader>
<bodyText confidence="0.997551592592593">
We trained on the 1300 documents in the MUC-4
corpus and tested on the 200 document TST3 and
TST4 test set. We evaluate the four string-based
slots: perpetrator, physical target, human target, and
instrument. We merge MUC’s two perpetrator slots
(individuals and orgs) into one gold Perpetrator slot.
As in Patwardhan and Riloff (2007; 2009), we ig-
nore missed optional slots in computing recall. We
induced clusters in training, performed IR, and in-
duced the slots. We then extracted entities from the
test documents as described in section 5.2.
The standard evaluation for this corpus is to report
the F1 score for slot type accuracy, ignoring the tem-
plate type. For instance, a perpetrator of a bombing
and a perpetrator of an attack are treated the same.
This allows supervised classifiers to train on all per-
petrators at once, rather than template-specific learn-
ers. Although not ideal for our learning goals, we
report it for comparison against previous work.
Several supervised approaches have presented re-
sults on MUC-4, but unfortunately we cannot com-
pare against them. Maslennikov and Chua (2006;
2007) evaluated a random subset of test (they report
.60 and .63 F1), and Xiao et al. (2004) did not eval-
uate all slot types (they report .57 F1).
Figure 5 thus shows our results with previous
work that is comparable: the fully supervised and
</bodyText>
<table confidence="0.903372">
P R F1
Patwardhan &amp; Riloff-09 : Supervised 48 59 53
Patwardhan &amp; Riloff-07 : Weak-Sup 42 48 44
Our Results (1 attack) 48 25 33
Our Results (5 attack) 44 36 40
Figure 5: MUC-4 extraction, ignoring template type.
F1 Score Kidnap Bomb Arson Attack
Results .53 .43 .42 .16 / .25
</table>
<figureCaption confidence="0.9984155">
Figure 6: Performance of individual templates. Attack
compares our 1 vs 5 best templates.
</figureCaption>
<bodyText confidence="0.999699">
weakly supervised approaches of Patwardhan and
Riloff (2009; 2007). We give two numbers for our
system: mapping one learned template to Attack,
and mapping five. Our learned templates for Attack
have a different granularity than MUC-4. Rather
than one broad Attack type, we learn several: Shoot-
ing, Murder, Coup, General Injury, and Pipeline At-
tack. We see these subtypes as strengths of our al-
gorithm, but it misses the MUC-4 granularity of At-
tack. We thus show results when we apply the best
five learned templates to Attack, rather than just one.
The final F1 with these Attack subtypes is .40.
Our precision is as good as (and our F1 score near)
two algorithms that require knowledge of the tem-
plates and/or labeled data. Our algorithm instead
learned this knowledge without such supervision.
</bodyText>
<sectionHeader confidence="0.939341" genericHeader="method">
7 Specific Evaluation
</sectionHeader>
<bodyText confidence="0.959476363636364">
In order to more precisely evaluate each learned
template, we also evaluated per-template perfor-
mance. Instead of merging all slots across all tem-
plate types, we score the slots within each template
type. This is a stricter evaluation than Section 6; for
example, bombing victims assigned to attacks were
previously deemed correct4.
Figure 6 gives our results. Three of the four tem-
plates score at or above .42 F1, showing that our
lower score from the previous section is mainly due
to the Attack template. Arson also unexpectedly
</bodyText>
<footnote confidence="0.985171666666667">
4We do not address the task of template instance identifica-
tion (e.g., splitting two bombings into separate instances). This
requires deeper discourse analysis not addressed by this paper.
</footnote>
<page confidence="0.993192">
983
</page>
<table confidence="0.9989144">
Precision Recall F1
Kidnap .82 .47 .60 (+.07)
Bomb .60 .36 .45 (+.02)
Arson 1.0 .29 .44 (+.02)
Attack .36 .09 .15 (0.0)
</table>
<figureCaption confidence="0.99374175">
Figure 7: Performance of each template type, but only
evaluated on documents labeled with each type. All oth-
ers are removed from test. The parentheses indicate F1
gain over evaluating on all test documents (figure 6).
</figureCaption>
<bodyText confidence="0.98929">
scored well. It only occurs in 40 documents overall,
suggesting our algorithm works with little evidence.
Per-template performace is good, and our .40
overall score from the previous section illustrates
that we perform quite well in comparison to the .44-
.53 range of weakly and fully supervised results.
These evaluations use the standard TST3 and
TST4 test sets, including the documents that are not
labeled with any templates. 74 of the 200 test doc-
uments are unlabeled. In order to determine where
the system’s false positives originate, we also mea-
sure performance only on the 126 test documents
that have at least one template. Figure 7 presents the
results on this subset. Kidnap improves most signifi-
cantly in F1 score (7 F1 points absolute), but the oth-
ers only change slightly. Most of the false positives
in the system thus do not originate from the unla-
beled documents (the 74 unlabeled), but rather from
extracting incorrect entities from correctly identified
documents (the 126 labeled).
</bodyText>
<sectionHeader confidence="0.999221" genericHeader="discussions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999984666666667">
Template-based IE systems typically assume knowl-
edge of the domain and its templates. We began
by showing that domain knowledge isn’t necessar-
ily required; we learned the MUC-4 template struc-
ture with surprising accuracy, learning new seman-
tic roles and several new template structures. We
are the first to our knowledge to automatically in-
duce MUC-4 templates. It is possible to take these
learned slots and use a previous approach to IE (such
as seed-based bootstrapping), but we presented an
algorithm that instead uses our learned syntactic pat-
terns. We achieved results with comparable preci-
sion, and an F1 score of .40 that approaches prior
algorithms that rely on hand-crafted knowledge.
The extraction results are encouraging, but the
template induction itself is a central contribution of
this work. Knowledge induction plays an important
role in moving to new domains and assisting users
who may not know what a corpus contains. Re-
cent work in Open IE learns atomic relations (Banko
et al., 2007b), but little work focuses on structured
scenarios. We learned more templates than just the
main MUC-4 templates. A user who seeks to know
what information is in a body of text would instantly
recognize these as key templates, and could then ex-
tract the central entities.
We hope to address in the future how the al-
gorithm’s unsupervised nature hurts recall. With-
out labeled or seed examples, it does not learn as
many patterns or robust classifiers as supervised ap-
proaches. We will investigate new text sources and
algorithms to try and capture more knowledge. The
final experiment in figure 7 shows that perhaps new
work should first focus on pattern learning and entity
extraction, rather than document identification.
Finally, while our pipelined approach (template
induction with an IR stage followed by entity ex-
traction) has the advantages of flexibility in devel-
opment and efficiency, it does involve a number
of parameters. We believe the IR parameters are
quite robust, and did not heavily focus on improving
this stage, but the two clustering steps during tem-
plate induction require parameters to control stop-
ping conditions and word filtering. While all learn-
ing algorithms require parameters, we think it is im-
portant for future work to focus on removing some
of these to help the algorithm be even more robust to
new domains and genres.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999007">
This work was supported by the National Science
Foundation IIS-0811974, and this material is also
based upon work supported by the Air Force Re-
search Laboratory (AFRL) under prime contract no.
FA8750-09-C-0181. Any opinions, findings, and
conclusion or recommendations expressed in this
material are those of the authors and do not necessar-
ily reflect the view of the Air Force Research Labo-
ratory (AFRL). Thanks to the Stanford NLP Group
and reviewers for helpful suggestions.
</bodyText>
<page confidence="0.996826">
984
</page>
<sectionHeader confidence="0.98341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999846423076923">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Christian
Boitet and Pete Whitelock, editors, ACL-98, pages 86–
90, San Francisco, California. Morgan Kaufmann Pub-
lishers.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007a. Learning
relations from the web. In Proceedings of the Interna-
tional Joint Conferences on Artificial Intelligence (IJ-
CAI).
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007b. Open in-
formation extraction from the web. In Proceedings of
the International Joint Conferences on Artificial Intel-
ligence (IJCAI).
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent dirichlet allocation. Journal of Machine Learning
Research.
Razvan Bunescu and Raymond Mooney. 2004. Collec-
tive information extraction with relational markov net-
works. In Proceedings of the Association of Computa-
tional Linguistics (ACL), pages 438–445.
Andrew Carlson, J. Betteridge, R.C. Wang, E.R. Hr-
uschka Jr., and T.M. Mitchell. 2010. Coupled semi-
supervised learning for information extraction. In Pro-
ceedings of the ACM International Conference on Web
Search and Data Mining (WSDM).
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceed-
ings of the Association of Computational Linguistics
(ACL), Hawaii, USA.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proceedings of the Association of Computa-
tional Linguistics (ACL), Columbus, Ohio.
Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee.
2003. Closing the gap: Learning-based information
extraction rivaling knowledge-engineering methods.
In Proceedings of the Association of Computational
Linguistics (ACL).
Nancy Chinchor, David Lewis, and Lynette Hirschman.
1993. Evaluating message understanding systems: an
analysis of the third message understanding confer-
ence. Computational Linguistics, 19:3:409–449.
Katrin Erk and Sebastian Pado. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods on Natural Language Processing (EMNLP).
Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen
McKeown. 2006. Automatic creation of domain tem-
plates. In Proceedings of the Association of Computa-
tional Linguistics (ACL).
Dayne Freitag. 1998. Toward general-purpose learning
for information extraction. In Proceedings of the As-
sociation of Computational Linguistics (ACL), pages
404–408.
David Graff. 2002. English gigaword. Linguistic Data
Consortium.
Trond Grenager and Christopher D. Manning. 2006. Un-
supervised discovery of a statistical verb lexicon. In
Proceedings of the the 2006 Conference on Empirical
Methods on Natural Language Processing (EMNLP).
Shan He and Daniel Gildea. 2006. Self-training and
co-training for semantic role labeling: Primary report.
Technical Report 891, University of Rochester.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through unsupervised cross-document infer-
ence. In Proceedings of the Association of Compu-
tational Linguistics (ACL).
Niels Kasch and Tim Oates. 2010. Mining script-like
structures from the web. In Proceedings of NAACL
HLT, pages 34–42.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In Proceedings of the North
American Association of Computational Linguistics.
Mstislav Maslennikov and Tat-Seng Chua. 2007. Auto-
matic acquisition of domain knowledge for informa-
tion extraction. In Proceedings of the Association of
Computational Linguistics (ACL).
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
ie with semantic affinity patterns and relevant regions.
In Proceedings of the 2007 Conference on Empirical
Methods on Natural Language Processing (EMNLP).
Siddharth Patwardhan and Ellen Riloff. 2009. A unified
model of phrasal and sentential evidence for informa-
tion extraction. In Proceedings of the 2009 Conference
on Empirical Methods on Natural Language Process-
ing (EMNLP).
Lisa Rau, George Krupka, Paul Jacobs, Ira Sider, and
Lois Childs. 1992. Ge nltoolset: Muc-4 test results
and analysis. In Proceedings of the Message Under-
standing Conference (MUC-4), pages 94–99.
Ellen Riloff and Mark Schmelzenbach. 1998. An em-
pirical approach to conceptual case frame acquisition.
In Proceedings of the Sixth Workshop on Very Large
Corpora.
Ellen Riloff, Janyce Wiebe, and William Phillips. 2005.
Exploiting subjectivity classification to improve infor-
mation extraction. In Proceedings of AAAI-05.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding. Lawrence Erlbaum.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
ie using unrestricted relation discovery. In Proceed-
ings of NAACL.
</reference>
<page confidence="0.985941">
985
</page>
<reference confidence="0.999695423076923">
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of the Association of Computational Linguis-
tics (ACL), pages 224–231.
Beth M. Sundheim. 1991. Third message understand-
ing evaluation and conference (muc-3): Phase 1 status
report. In Proceedings of the Message Understanding
Conference.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006.
A hybrid approach for the acquisition of information
extraction patterns. In Proceedings of the EACL Work-
shop on Adaptive Text Extraction and Mining.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In Proceedings of
the 2004 Conference on Empirical Methods on Nat-
ural Language Processing (EMNLP).
Jing Xiao, Tat-Seng Chua, and Hang Cui. 2004. Cas-
cading use of soft and hard matching pattern rules
for weakly supervised information extraction. In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING).
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
COLING, pages 940–946.
</reference>
<page confidence="0.998465">
986
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.448685">
<title confidence="0.996517">Template-Based Information Extraction without the Templates</title>
<author confidence="0.833071">Chambers</author>
<affiliation confidence="0.993165">Department of Computer</affiliation>
<address confidence="0.537504">Stanford</address>
<abstract confidence="0.999869458333333">Standard algorithms for template-based information extraction (IE) require predefined template schemas, and often labeled data, to learn to extract their slot fillers (e.g., an the a template). This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance. Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., include off, deassociated with semantic roles. We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents. We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>ACL-98,</booktitle>
<pages>86--90</pages>
<editor>In Christian Boitet and Pete Whitelock, editors,</editor>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>San Francisco, California.</location>
<contexts>
<context position="2044" citStr="Baker et al., 1998" startWordPosition="307" endWordPosition="310">ion discovery that focuses on learning atomic facts (Banko et al., 2007a; Carlson et al., 2010), templates can extract a richer representation of a particular domain. However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain’s templates. Very little work addresses how to learn the template structure itself. Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus. There are many ways to represent events, ranging from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson, 1977) and narrative schemas (Chambers and Jurafsky, 2009; Kasch and Oates, 2010). Our approach learns narrative-like knowledge in the form of IE templates; we learn sets of related events and semantic roles, as shown in this sample output from our system: Bombing Template {detonate, blow up, plant, explode, defuse, destroy} Perpetrator: Person who detonates, plants, blows up Instrument: Object that is planted, detonated, defused Target: Object that is destroyed, is blown up A semantic role, such as target, is a cluster of syntactic function</context>
<context position="16584" citStr="Baker et al., 1998" startWordPosition="2638" endWordPosition="2641">s the problem of inducing semantic roles. Our learned roles will then extract entities in the next section and we will evaluate their per-role accuracy. Most work on unsupervised role induction focuses on learning verb-specific roles, starting with seed examples (Swier and Stevenson, 2004; He and E 1 − log4(g(wi, wj)) (1) w;,wjEd P (4) dist(wi, wj) = 979 Gildea, 2006) and/or knowing the number of roles (Grenager and Manning, 2006; Lang and Lapata, 2010). Our previous work (Chambers and Jurafsky, 2009) learned situation-specific roles over narrative schemas, similar to frame roles in FrameNet (Baker et al., 1998). Schemas link the syntactic relations of verbs by clustering them based on observing coreferring arguments in those positions. This paper extends this intuition by introducing a new vectorbased approach to coreference similarity. 4.3.1 Syntactic Relations as Roles We learn the roles of cluster C by clustering the syntactic relations RC of its words. Consider the following example: C = {go off, explode, set off, damage, destroy} RC = {go off:s, go off:p in, explode:s, set off:s} where verb:s is the verb’s subject, :o the object, and p in a preposition. We ideally want to cluster RC as: bomb = </context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Christian Boitet and Pete Whitelock, editors, ACL-98, pages 86– 90, San Francisco, California. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Learning relations from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="1496" citStr="Banko et al., 2007" startWordPosition="223" endWordPosition="226"> role fillers from specific documents. We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates. 1 Introduction A template defines a specific type of event (e.g., a bombing) with a set of semantic roles (or slots) for the typical entities involved in such an event (e.g., perpetrator, target, instrument). In contrast to work in relation discovery that focuses on learning atomic facts (Banko et al., 2007a; Carlson et al., 2010), templates can extract a richer representation of a particular domain. However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain’s templates. Very little work addresses how to learn the template structure itself. Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus. There are many ways to represent events, ranging from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson</context>
<context position="35109" citStr="Banko et al., 2007" startWordPosition="5665" endWordPosition="5668">ese learned slots and use a previous approach to IE (such as seed-based bootstrapping), but we presented an algorithm that instead uses our learned syntactic patterns. We achieved results with comparable precision, and an F1 score of .40 that approaches prior algorithms that rely on hand-crafted knowledge. The extraction results are encouraging, but the template induction itself is a central contribution of this work. Knowledge induction plays an important role in moving to new domains and assisting users who may not know what a corpus contains. Recent work in Open IE learns atomic relations (Banko et al., 2007b), but little work focuses on structured scenarios. We learned more templates than just the main MUC-4 templates. A user who seeks to know what information is in a body of text would instantly recognize these as key templates, and could then extract the central entities. We hope to address in the future how the algorithm’s unsupervised nature hurts recall. Without labeled or seed examples, it does not learn as many patterns or robust classifiers as supervised approaches. We will investigate new text sources and algorithms to try and capture more knowledge. The final experiment in figure 7 sho</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007a. Learning relations from the web. In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="1496" citStr="Banko et al., 2007" startWordPosition="223" endWordPosition="226"> role fillers from specific documents. We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates. 1 Introduction A template defines a specific type of event (e.g., a bombing) with a set of semantic roles (or slots) for the typical entities involved in such an event (e.g., perpetrator, target, instrument). In contrast to work in relation discovery that focuses on learning atomic facts (Banko et al., 2007a; Carlson et al., 2010), templates can extract a richer representation of a particular domain. However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain’s templates. Very little work addresses how to learn the template structure itself. Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus. There are many ways to represent events, ranging from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson</context>
<context position="35109" citStr="Banko et al., 2007" startWordPosition="5665" endWordPosition="5668">ese learned slots and use a previous approach to IE (such as seed-based bootstrapping), but we presented an algorithm that instead uses our learned syntactic patterns. We achieved results with comparable precision, and an F1 score of .40 that approaches prior algorithms that rely on hand-crafted knowledge. The extraction results are encouraging, but the template induction itself is a central contribution of this work. Knowledge induction plays an important role in moving to new domains and assisting users who may not know what a corpus contains. Recent work in Open IE learns atomic relations (Banko et al., 2007b), but little work focuses on structured scenarios. We learned more templates than just the main MUC-4 templates. A user who seeks to know what information is in a body of text would instantly recognize these as key templates, and could then extract the central entities. We hope to address in the future how the algorithm’s unsupervised nature hurts recall. Without labeled or seed examples, it does not learn as many patterns or robust classifiers as supervised approaches. We will investigate new text sources and algorithms to try and capture more knowledge. The final experiment in figure 7 sho</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007b. Open information extraction from the web. In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="10700" citStr="Blei et al., 2003" startWordPosition="1676" endWordPosition="1679">-4: Organization and Individual. We consider their union as a single slot. Net under the Event synset, or (3) a verb and the head word of its syntactic object. Examples of each include (1) ‘explode’, (2) ‘explosion’, and (3) ‘explode:bomb’. We also tag the corpus with an NER system and allow patterns to include named entity types, e.g., ‘kidnap:PERSON’. These patterns are crucially needed later to learn a template’s slots. However, we first need an algorithm to cluster these patterns to learn the domain’s core events. We consider two unsupervised algorithms: Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and agglomerative clustering based on word distance. 4.1.1 LDA for Unknown Data LDA is a probabilistic model that treats documents as mixtures of topics. It learns topics as discrete distributions (multinomials) over the event patterns, and thus meets our needs as it clusters patterns based on co-occurrence in documents. The algorithm requires the number of topics to be known ahead of time, but in practice this number is set relatively high and the resulting topics are still useful. Our best performing LDA model used 200 topics. We had mixed success with LDA though, and ultimately found our </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng, and Michael Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Collective information extraction with relational markov networks.</title>
<date>2004</date>
<booktitle>In Proceedings of the Association of Computational Linguistics (ACL),</booktitle>
<pages>438--445</pages>
<marker>Bunescu, Mooney, 2004</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2004. Collective information extraction with relational markov networks. In Proceedings of the Association of Computational Linguistics (ACL), pages 438–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>J Betteridge</author>
<author>R C Wang</author>
<author>E R Hruschka Jr</author>
<author>T M Mitchell</author>
</authors>
<title>Coupled semisupervised learning for information extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM).</booktitle>
<contexts>
<context position="1520" citStr="Carlson et al., 2010" startWordPosition="227" endWordPosition="230">ecific documents. We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates. 1 Introduction A template defines a specific type of event (e.g., a bombing) with a set of semantic roles (or slots) for the typical entities involved in such an event (e.g., perpetrator, target, instrument). In contrast to work in relation discovery that focuses on learning atomic facts (Banko et al., 2007a; Carlson et al., 2010), templates can extract a richer representation of a particular domain. However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain’s templates. Very little work addresses how to learn the template structure itself. Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus. There are many ways to represent events, ranging from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson, 1977) and narrative sc</context>
</contexts>
<marker>Carlson, Betteridge, Wang, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, J. Betteridge, R.C. Wang, E.R. Hruschka Jr., and T.M. Mitchell. 2010. Coupled semisupervised learning for information extraction. In Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association of Computational Linguistics (ACL),</booktitle>
<location>Hawaii, USA.</location>
<contexts>
<context position="5423" citStr="Chambers and Jurafsky, 2008" startWordPosition="825" endWordPosition="828">ithms require full their work by showing how to learn without these knowledge of the templates and labeled corpora, assumptions, obviating the need for redundant docsuch as in rule-based systems (Chinchor et al., 1993; uments, and learning templates with any type and Rau et al., 1992) and modern supervised classi- any number of slots. fiers (Freitag, 1998; Chieu et al., 2003; Bunescu Large-scale learning of scripts and narrative and Mooney, 2004; Patwardhan and Riloff, 2009). schemas also captures template-like knowledge Classifiers rely on the labeled examples’ surround- from unlabeled text (Chambers and Jurafsky, 2008; ing context for features such as nearby tokens, doc- Kasch and Oates, 2010). Scripts are sets of reument position, syntax, named entities, semantic lated event words and semantic roles learned by classes, and discourse relations (Maslennikov and linking syntactic functions with coreferring arguChua, 2007). Ji and Grishman (2008) also supple- ments. While they learn interesting event structure, mented labeled with unlabeled data. the structures are limited to frequent topics in a large Weakly supervised approaches remove some of corpus. We borrow ideas from this work as well, but the need for</context>
<context position="17506" citStr="Chambers and Jurafsky (2008)" startWordPosition="2788" endWordPosition="2791">r C by clustering the syntactic relations RC of its words. Consider the following example: C = {go off, explode, set off, damage, destroy} RC = {go off:s, go off:p in, explode:s, set off:s} where verb:s is the verb’s subject, :o the object, and p in a preposition. We ideally want to cluster RC as: bomb = {go off:s, explode:s, set off:o, destroy:s} suspect = {set off:s} target = {go off:p in, destroy:o} We want to cluster all subjects, objects, and prepositions. Passive voice is normalized to active2. We adopt two views of relation similarity: coreferring arguments and selectional preferences. Chambers and Jurafsky (2008) observed that coreferring arguments suggest a semantic relation between two predicates. In the sentence, he ran and then he fell, the subjects of run and fall corefer, and so they likely belong to the same scenario-specific semantic role. We applied this idea to a new vector similarity framework. We represent a relation as a vector of all relations with which their arguments coreferred. For instance, arguments of the relation go off:s were seen coreferring with mentions in plant:o, set off:o and injure:s. We represent go off:s as a vector of these relation counts, calling this its coref vecto</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of the Association of Computational Linguistics (ACL), Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association of Computational Linguistics (ACL),</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="2154" citStr="Chambers and Jurafsky, 2009" startWordPosition="323" endWordPosition="326">ates can extract a richer representation of a particular domain. However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain’s templates. Very little work addresses how to learn the template structure itself. Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus. There are many ways to represent events, ranging from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson, 1977) and narrative schemas (Chambers and Jurafsky, 2009; Kasch and Oates, 2010). Our approach learns narrative-like knowledge in the form of IE templates; we learn sets of related events and semantic roles, as shown in this sample output from our system: Bombing Template {detonate, blow up, plant, explode, defuse, destroy} Perpetrator: Person who detonates, plants, blows up Instrument: Object that is planted, detonated, defused Target: Object that is destroyed, is blown up A semantic role, such as target, is a cluster of syntactic functions of the template’s event words (e.g., the objects of detonate and explode). Our goal is to characterize a dom</context>
<context position="16471" citStr="Chambers and Jurafsky, 2009" startWordPosition="2620" endWordPosition="2624">emantic Roles (Slots) Having successfully clustered event words and retrieved an IR-corpus for each cluster, we now address the problem of inducing semantic roles. Our learned roles will then extract entities in the next section and we will evaluate their per-role accuracy. Most work on unsupervised role induction focuses on learning verb-specific roles, starting with seed examples (Swier and Stevenson, 2004; He and E 1 − log4(g(wi, wj)) (1) w;,wjEd P (4) dist(wi, wj) = 979 Gildea, 2006) and/or knowing the number of roles (Grenager and Manning, 2006; Lang and Lapata, 2010). Our previous work (Chambers and Jurafsky, 2009) learned situation-specific roles over narrative schemas, similar to frame roles in FrameNet (Baker et al., 1998). Schemas link the syntactic relations of verbs by clustering them based on observing coreferring arguments in those positions. This paper extends this intuition by introducing a new vectorbased approach to coreference similarity. 4.3.1 Syntactic Relations as Roles We learn the roles of cluster C by clustering the syntactic relations RC of its words. Consider the following example: C = {go off, explode, set off, damage, destroy} RC = {go off:s, go off:p in, explode:s, set off:s} whe</context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proceedings of the Association of Computational Linguistics (ACL), Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
<author>Yoong Keok Lee</author>
</authors>
<title>Closing the gap: Learning-based information extraction rivaling knowledge-engineering methods.</title>
<date>2003</date>
<booktitle>In Proceedings of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5173" citStr="Chieu et al., 2003" startWordPosition="792" endWordPosition="795">ndant documents about specific events are reuations against previous work in section 6 and 7. quired, (2) relations are binary, and (3) only slots 2 Previous Work with named entities are learned. We will extend Many template extraction algorithms require full their work by showing how to learn without these knowledge of the templates and labeled corpora, assumptions, obviating the need for redundant docsuch as in rule-based systems (Chinchor et al., 1993; uments, and learning templates with any type and Rau et al., 1992) and modern supervised classi- any number of slots. fiers (Freitag, 1998; Chieu et al., 2003; Bunescu Large-scale learning of scripts and narrative and Mooney, 2004; Patwardhan and Riloff, 2009). schemas also captures template-like knowledge Classifiers rely on the labeled examples’ surround- from unlabeled text (Chambers and Jurafsky, 2008; ing context for features such as nearby tokens, doc- Kasch and Oates, 2010). Scripts are sets of reument position, syntax, named entities, semantic lated event words and semantic roles learned by classes, and discourse relations (Maslennikov and linking syntactic functions with coreferring arguChua, 2007). Ji and Grishman (2008) also supple- ment</context>
</contexts>
<marker>Chieu, Ng, Lee, 2003</marker>
<rawString>Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee. 2003. Closing the gap: Learning-based information extraction rivaling knowledge-engineering methods. In Proceedings of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
<author>David Lewis</author>
<author>Lynette Hirschman</author>
</authors>
<title>Evaluating message understanding systems: an analysis of the third message understanding conference. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5013" citStr="Chinchor et al., 1993" startWordPosition="765" endWordPosition="768">inally how to induce semantic roles. Section 5 then ever, the limitations to their approach are that (1) describes the extraction algorithm, followed by eval- redundant documents about specific events are reuations against previous work in section 6 and 7. quired, (2) relations are binary, and (3) only slots 2 Previous Work with named entities are learned. We will extend Many template extraction algorithms require full their work by showing how to learn without these knowledge of the templates and labeled corpora, assumptions, obviating the need for redundant docsuch as in rule-based systems (Chinchor et al., 1993; uments, and learning templates with any type and Rau et al., 1992) and modern supervised classi- any number of slots. fiers (Freitag, 1998; Chieu et al., 2003; Bunescu Large-scale learning of scripts and narrative and Mooney, 2004; Patwardhan and Riloff, 2009). schemas also captures template-like knowledge Classifiers rely on the labeled examples’ surround- from unlabeled text (Chambers and Jurafsky, 2008; ing context for features such as nearby tokens, doc- Kasch and Oates, 2010). Scripts are sets of reument position, syntax, named entities, semantic lated event words and semantic roles lea</context>
</contexts>
<marker>Chinchor, Lewis, Hirschman, 1993</marker>
<rawString>Nancy Chinchor, David Lewis, and Lynette Hirschman. 1993. Evaluating message understanding systems: an analysis of the third message understanding conference. Computational Linguistics, 19:3:409–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pado</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods on Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="18214" citStr="Erk and Pado, 2008" startWordPosition="2905" endWordPosition="2908">In the sentence, he ran and then he fell, the subjects of run and fall corefer, and so they likely belong to the same scenario-specific semantic role. We applied this idea to a new vector similarity framework. We represent a relation as a vector of all relations with which their arguments coreferred. For instance, arguments of the relation go off:s were seen coreferring with mentions in plant:o, set off:o and injure:s. We represent go off:s as a vector of these relation counts, calling this its coref vector representation. Selectional preferences (SPs) are also useful in measuring similarity (Erk and Pado, 2008). A relation can be represented as a vector of its observed arguments during training. The SPs for go off:s in our data include {bomb, device, charge, explosion}. We measure similarity using cosine similarity between the vectors in both approaches. However, 2We use the Stanford Parser at nlp.stanford.edu/software coreference and SPs measure different types of similarity. Coreference is a looser narrative similarity (bombings cause injuries), while SPs capture synonymy (plant and place have similar arguments). We observed that many narrative relations are not synonymous, and vice versa. We thus</context>
</contexts>
<marker>Erk, Pado, 2008</marker>
<rawString>Katrin Erk and Sebastian Pado. 2008. A structured vector space model for word meaning in context. In Proceedings of the 2008 Conference on Empirical Methods on Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen McKeown</author>
</authors>
<title>Automatic creation of domain templates.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="6632" citStr="Filatova et al. (2006)" startWordPosition="1018" endWordPosition="1021"> the need for fully labeled data. Most still require the our goal is to instead characterize a specific domain templates and their slots. One common approach is with limited data. Further, we are the first to apply to begin with unlabeled, but clustered event-specific this knowledge to the IE task of filling in template documents, and extract common word patterns as mentions in documents. extractors (Riloff and Schmelzenbach, 1998; Sudo In summary, our work extends previous work on et al., 2003; Riloff et al., 2005; Patwardhan and unsupervised IE in a number of ways. We are the Riloff, 2007). Filatova et al. (2006) integrate named first to learn MUC-4 templates, and we are the first entities into pattern learning (PERSON won) to ap- to extract entities without knowing how many temproximate unknown semantic roles. Bootstrapping plates exist, without examples of slot fillers, and with seed examples of known slot fillers has been without event-clustered documents. shown to be effective (Surdeanu et al., 2006; Yan- 3 The Domain and its Templates garber et al., 2000). In contrast, this paper removes Our goal is to learn the general event structure of these data assumptions, learning instead from a cor- a dom</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, McKeown, 2006</marker>
<rawString>Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen McKeown. 2006. Automatic creation of domain templates. In Proceedings of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Toward general-purpose learning for information extraction.</title>
<date>1998</date>
<booktitle>In Proceedings of the Association of Computational Linguistics (ACL),</booktitle>
<pages>404--408</pages>
<contexts>
<context position="5153" citStr="Freitag, 1998" startWordPosition="790" endWordPosition="791">d by eval- redundant documents about specific events are reuations against previous work in section 6 and 7. quired, (2) relations are binary, and (3) only slots 2 Previous Work with named entities are learned. We will extend Many template extraction algorithms require full their work by showing how to learn without these knowledge of the templates and labeled corpora, assumptions, obviating the need for redundant docsuch as in rule-based systems (Chinchor et al., 1993; uments, and learning templates with any type and Rau et al., 1992) and modern supervised classi- any number of slots. fiers (Freitag, 1998; Chieu et al., 2003; Bunescu Large-scale learning of scripts and narrative and Mooney, 2004; Patwardhan and Riloff, 2009). schemas also captures template-like knowledge Classifiers rely on the labeled examples’ surround- from unlabeled text (Chambers and Jurafsky, 2008; ing context for features such as nearby tokens, doc- Kasch and Oates, 2010). Scripts are sets of reument position, syntax, named entities, semantic lated event words and semantic roles learned by classes, and discourse relations (Maslennikov and linking syntactic functions with coreferring arguChua, 2007). Ji and Grishman (200</context>
</contexts>
<marker>Freitag, 1998</marker>
<rawString>Dayne Freitag. 1998. Toward general-purpose learning for information extraction. In Proceedings of the Association of Computational Linguistics (ACL), pages 404–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>English gigaword. Linguistic Data Consortium.</title>
<date>2002</date>
<contexts>
<context position="14497" citStr="Graff, 2002" startWordPosition="2286" endWordPosition="2287">tion Retrieval for Templates Learning a domain often suffers from a lack of training data. The previous section clustered events from the MUC-4 corpus, but its 1300 documents do not provide enough examples of verbs and argument counts to further learn the semantic roles in each cluster. Our solution is to assemble a larger IRcorpus of documents for each cluster. For example, MUC-4 labels 83 documents with Kidnap, but our learned cluster (kidnap, abduct, release, ...) retrieved 3954 documents from a general corpus. We use the Associated Press and New York Times sections of the Gigaword Corpus (Graff, 2002) as our general corpus. These sections include approximately 3.5 million news articles spanning 12 years. Our retrieval algorithm retrieves documents that score highly with a cluster’s tokens. The document score is defined by two common metrics: word match, and word coverage. A document’s match score is defined as the average number of times the words in cluster c appear in document d: avgm(d, c) = EwEc EtEd 11w = t} (5) jcj We define word coverage as the number of seen cluster words. Coverage penalizes documents that score highly by repeating a single cluster word a lot. We only score a docum</context>
</contexts>
<marker>Graff, 2002</marker>
<rawString>David Graff. 2002. English gigaword. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>Unsupervised discovery of a statistical verb lexicon.</title>
<date>2006</date>
<booktitle>In Proceedings of the the 2006 Conference on Empirical Methods on Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="16398" citStr="Grenager and Manning, 2006" startWordPosition="2609" endWordPosition="2612">for optimum parameter values may lead to better results. 4.3 Inducing Semantic Roles (Slots) Having successfully clustered event words and retrieved an IR-corpus for each cluster, we now address the problem of inducing semantic roles. Our learned roles will then extract entities in the next section and we will evaluate their per-role accuracy. Most work on unsupervised role induction focuses on learning verb-specific roles, starting with seed examples (Swier and Stevenson, 2004; He and E 1 − log4(g(wi, wj)) (1) w;,wjEd P (4) dist(wi, wj) = 979 Gildea, 2006) and/or knowing the number of roles (Grenager and Manning, 2006; Lang and Lapata, 2010). Our previous work (Chambers and Jurafsky, 2009) learned situation-specific roles over narrative schemas, similar to frame roles in FrameNet (Baker et al., 1998). Schemas link the syntactic relations of verbs by clustering them based on observing coreferring arguments in those positions. This paper extends this intuition by introducing a new vectorbased approach to coreference similarity. 4.3.1 Syntactic Relations as Roles We learn the roles of cluster C by clustering the syntactic relations RC of its words. Consider the following example: C = {go off, explode, set off</context>
</contexts>
<marker>Grenager, Manning, 2006</marker>
<rawString>Trond Grenager and Christopher D. Manning. 2006. Unsupervised discovery of a statistical verb lexicon. In Proceedings of the the 2006 Conference on Empirical Methods on Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shan He</author>
<author>Daniel Gildea</author>
</authors>
<title>Self-training and co-training for semantic role labeling: Primary report.</title>
<date>2006</date>
<tech>Technical Report 891,</tech>
<institution>University of Rochester.</institution>
<marker>He, Gildea, 2006</marker>
<rawString>Shan He and Daniel Gildea. 2006. Self-training and co-training for semantic role labeling: Primary report. Technical Report 891, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Refining event extraction through unsupervised cross-document inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5755" citStr="Ji and Grishman (2008)" startWordPosition="875" endWordPosition="878">fiers (Freitag, 1998; Chieu et al., 2003; Bunescu Large-scale learning of scripts and narrative and Mooney, 2004; Patwardhan and Riloff, 2009). schemas also captures template-like knowledge Classifiers rely on the labeled examples’ surround- from unlabeled text (Chambers and Jurafsky, 2008; ing context for features such as nearby tokens, doc- Kasch and Oates, 2010). Scripts are sets of reument position, syntax, named entities, semantic lated event words and semantic roles learned by classes, and discourse relations (Maslennikov and linking syntactic functions with coreferring arguChua, 2007). Ji and Grishman (2008) also supple- ments. While they learn interesting event structure, mented labeled with unlabeled data. the structures are limited to frequent topics in a large Weakly supervised approaches remove some of corpus. We borrow ideas from this work as well, but the need for fully labeled data. Most still require the our goal is to instead characterize a specific domain templates and their slots. One common approach is with limited data. Further, we are the first to apply to begin with unlabeled, but clustered event-specific this knowledge to the IE task of filling in template documents, and extract </context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>Heng Ji and Ralph Grishman. 2008. Refining event extraction through unsupervised cross-document inference. In Proceedings of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niels Kasch</author>
<author>Tim Oates</author>
</authors>
<title>Mining script-like structures from the web.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL HLT,</booktitle>
<pages>34--42</pages>
<contexts>
<context position="2178" citStr="Kasch and Oates, 2010" startWordPosition="327" endWordPosition="330">resentation of a particular domain. However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain’s templates. Very little work addresses how to learn the template structure itself. Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus. There are many ways to represent events, ranging from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson, 1977) and narrative schemas (Chambers and Jurafsky, 2009; Kasch and Oates, 2010). Our approach learns narrative-like knowledge in the form of IE templates; we learn sets of related events and semantic roles, as shown in this sample output from our system: Bombing Template {detonate, blow up, plant, explode, defuse, destroy} Perpetrator: Person who detonates, plants, blows up Instrument: Object that is planted, detonated, defused Target: Object that is destroyed, is blown up A semantic role, such as target, is a cluster of syntactic functions of the template’s event words (e.g., the objects of detonate and explode). Our goal is to characterize a domain by learning this tem</context>
<context position="5500" citStr="Kasch and Oates, 2010" startWordPosition="838" endWordPosition="841">he templates and labeled corpora, assumptions, obviating the need for redundant docsuch as in rule-based systems (Chinchor et al., 1993; uments, and learning templates with any type and Rau et al., 1992) and modern supervised classi- any number of slots. fiers (Freitag, 1998; Chieu et al., 2003; Bunescu Large-scale learning of scripts and narrative and Mooney, 2004; Patwardhan and Riloff, 2009). schemas also captures template-like knowledge Classifiers rely on the labeled examples’ surround- from unlabeled text (Chambers and Jurafsky, 2008; ing context for features such as nearby tokens, doc- Kasch and Oates, 2010). Scripts are sets of reument position, syntax, named entities, semantic lated event words and semantic roles learned by classes, and discourse relations (Maslennikov and linking syntactic functions with coreferring arguChua, 2007). Ji and Grishman (2008) also supple- ments. While they learn interesting event structure, mented labeled with unlabeled data. the structures are limited to frequent topics in a large Weakly supervised approaches remove some of corpus. We borrow ideas from this work as well, but the need for fully labeled data. Most still require the our goal is to instead characteri</context>
</contexts>
<marker>Kasch, Oates, 2010</marker>
<rawString>Niels Kasch and Tim Oates. 2010. Mining script-like structures from the web. In Proceedings of NAACL HLT, pages 34–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised induction of semantic roles.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Association of Computational Linguistics.</booktitle>
<contexts>
<context position="16422" citStr="Lang and Lapata, 2010" startWordPosition="2613" endWordPosition="2616"> may lead to better results. 4.3 Inducing Semantic Roles (Slots) Having successfully clustered event words and retrieved an IR-corpus for each cluster, we now address the problem of inducing semantic roles. Our learned roles will then extract entities in the next section and we will evaluate their per-role accuracy. Most work on unsupervised role induction focuses on learning verb-specific roles, starting with seed examples (Swier and Stevenson, 2004; He and E 1 − log4(g(wi, wj)) (1) w;,wjEd P (4) dist(wi, wj) = 979 Gildea, 2006) and/or knowing the number of roles (Grenager and Manning, 2006; Lang and Lapata, 2010). Our previous work (Chambers and Jurafsky, 2009) learned situation-specific roles over narrative schemas, similar to frame roles in FrameNet (Baker et al., 1998). Schemas link the syntactic relations of verbs by clustering them based on observing coreferring arguments in those positions. This paper extends this intuition by introducing a new vectorbased approach to coreference similarity. 4.3.1 Syntactic Relations as Roles We learn the roles of cluster C by clustering the syntactic relations RC of its words. Consider the following example: C = {go off, explode, set off, damage, destroy} RC = </context>
</contexts>
<marker>Lang, Lapata, 2010</marker>
<rawString>Joel Lang and Mirella Lapata. 2010. Unsupervised induction of semantic roles. In Proceedings of the North American Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mstislav Maslennikov</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Automatic acquisition of domain knowledge for information extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association of Computational Linguistics (ACL).</booktitle>
<marker>Maslennikov, Chua, 2007</marker>
<rawString>Mstislav Maslennikov and Tat-Seng Chua. 2007. Automatic acquisition of domain knowledge for information extraction. In Proceedings of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>Effective ie with semantic affinity patterns and relevant regions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Conference on Empirical Methods on Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="29826" citStr="Patwardhan and Riloff (2007" startWordPosition="4786" endWordPosition="4789">ll it. Thus, we extract all arguments that filled a role in training, regardless of their current syntactic environment. Finally, we filter extractions whose WordNet or named entity label does not match the learned slot’s type (e.g., a Location does not match a Person). 6 Standard Evaluation We trained on the 1300 documents in the MUC-4 corpus and tested on the 200 document TST3 and TST4 test set. We evaluate the four string-based slots: perpetrator, physical target, human target, and instrument. We merge MUC’s two perpetrator slots (individuals and orgs) into one gold Perpetrator slot. As in Patwardhan and Riloff (2007; 2009), we ignore missed optional slots in computing recall. We induced clusters in training, performed IR, and induced the slots. We then extracted entities from the test documents as described in section 5.2. The standard evaluation for this corpus is to report the F1 score for slot type accuracy, ignoring the template type. For instance, a perpetrator of a bombing and a perpetrator of an attack are treated the same. This allows supervised classifiers to train on all perpetrators at once, rather than template-specific learners. Although not ideal for our learning goals, we report it for com</context>
</contexts>
<marker>Patwardhan, Riloff, 2007</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2007. Effective ie with semantic affinity patterns and relevant regions. In Proceedings of the 2007 Conference on Empirical Methods on Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>A unified model of phrasal and sentential evidence for information extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods on Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="5275" citStr="Patwardhan and Riloff, 2009" startWordPosition="806" endWordPosition="809">. quired, (2) relations are binary, and (3) only slots 2 Previous Work with named entities are learned. We will extend Many template extraction algorithms require full their work by showing how to learn without these knowledge of the templates and labeled corpora, assumptions, obviating the need for redundant docsuch as in rule-based systems (Chinchor et al., 1993; uments, and learning templates with any type and Rau et al., 1992) and modern supervised classi- any number of slots. fiers (Freitag, 1998; Chieu et al., 2003; Bunescu Large-scale learning of scripts and narrative and Mooney, 2004; Patwardhan and Riloff, 2009). schemas also captures template-like knowledge Classifiers rely on the labeled examples’ surround- from unlabeled text (Chambers and Jurafsky, 2008; ing context for features such as nearby tokens, doc- Kasch and Oates, 2010). Scripts are sets of reument position, syntax, named entities, semantic lated event words and semantic roles learned by classes, and discourse relations (Maslennikov and linking syntactic functions with coreferring arguChua, 2007). Ji and Grishman (2008) also supple- ments. While they learn interesting event structure, mented labeled with unlabeled data. the structures ar</context>
<context position="31265" citStr="Patwardhan and Riloff (2009" startWordPosition="5032" endWordPosition="5035">(they report .60 and .63 F1), and Xiao et al. (2004) did not evaluate all slot types (they report .57 F1). Figure 5 thus shows our results with previous work that is comparable: the fully supervised and P R F1 Patwardhan &amp; Riloff-09 : Supervised 48 59 53 Patwardhan &amp; Riloff-07 : Weak-Sup 42 48 44 Our Results (1 attack) 48 25 33 Our Results (5 attack) 44 36 40 Figure 5: MUC-4 extraction, ignoring template type. F1 Score Kidnap Bomb Arson Attack Results .53 .43 .42 .16 / .25 Figure 6: Performance of individual templates. Attack compares our 1 vs 5 best templates. weakly supervised approaches of Patwardhan and Riloff (2009; 2007). We give two numbers for our system: mapping one learned template to Attack, and mapping five. Our learned templates for Attack have a different granularity than MUC-4. Rather than one broad Attack type, we learn several: Shooting, Murder, Coup, General Injury, and Pipeline Attack. We see these subtypes as strengths of our algorithm, but it misses the MUC-4 granularity of Attack. We thus show results when we apply the best five learned templates to Attack, rather than just one. The final F1 with these Attack subtypes is .40. Our precision is as good as (and our F1 score near) two algor</context>
</contexts>
<marker>Patwardhan, Riloff, 2009</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2009. A unified model of phrasal and sentential evidence for information extraction. In Proceedings of the 2009 Conference on Empirical Methods on Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Rau</author>
<author>George Krupka</author>
<author>Paul Jacobs</author>
<author>Ira Sider</author>
<author>Lois Childs</author>
</authors>
<title>Ge nltoolset: Muc-4 test results and analysis.</title>
<date>1992</date>
<booktitle>In Proceedings of the Message Understanding Conference (MUC-4),</booktitle>
<pages>94--99</pages>
<contexts>
<context position="5081" citStr="Rau et al., 1992" startWordPosition="777" endWordPosition="780">s to their approach are that (1) describes the extraction algorithm, followed by eval- redundant documents about specific events are reuations against previous work in section 6 and 7. quired, (2) relations are binary, and (3) only slots 2 Previous Work with named entities are learned. We will extend Many template extraction algorithms require full their work by showing how to learn without these knowledge of the templates and labeled corpora, assumptions, obviating the need for redundant docsuch as in rule-based systems (Chinchor et al., 1993; uments, and learning templates with any type and Rau et al., 1992) and modern supervised classi- any number of slots. fiers (Freitag, 1998; Chieu et al., 2003; Bunescu Large-scale learning of scripts and narrative and Mooney, 2004; Patwardhan and Riloff, 2009). schemas also captures template-like knowledge Classifiers rely on the labeled examples’ surround- from unlabeled text (Chambers and Jurafsky, 2008; ing context for features such as nearby tokens, doc- Kasch and Oates, 2010). Scripts are sets of reument position, syntax, named entities, semantic lated event words and semantic roles learned by classes, and discourse relations (Maslennikov and linking sy</context>
</contexts>
<marker>Rau, Krupka, Jacobs, Sider, Childs, 1992</marker>
<rawString>Lisa Rau, George Krupka, Paul Jacobs, Ira Sider, and Lois Childs. 1992. Ge nltoolset: Muc-4 test results and analysis. In Proceedings of the Message Understanding Conference (MUC-4), pages 94–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Mark Schmelzenbach</author>
</authors>
<title>An empirical approach to conceptual case frame acquisition.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="6444" citStr="Riloff and Schmelzenbach, 1998" startWordPosition="984" endWordPosition="987">ucture, mented labeled with unlabeled data. the structures are limited to frequent topics in a large Weakly supervised approaches remove some of corpus. We borrow ideas from this work as well, but the need for fully labeled data. Most still require the our goal is to instead characterize a specific domain templates and their slots. One common approach is with limited data. Further, we are the first to apply to begin with unlabeled, but clustered event-specific this knowledge to the IE task of filling in template documents, and extract common word patterns as mentions in documents. extractors (Riloff and Schmelzenbach, 1998; Sudo In summary, our work extends previous work on et al., 2003; Riloff et al., 2005; Patwardhan and unsupervised IE in a number of ways. We are the Riloff, 2007). Filatova et al. (2006) integrate named first to learn MUC-4 templates, and we are the first entities into pattern learning (PERSON won) to ap- to extract entities without knowing how many temproximate unknown semantic roles. Bootstrapping plates exist, without examples of slot fillers, and with seed examples of known slot fillers has been without event-clustered documents. shown to be effective (Surdeanu et al., 2006; Yan- 3 The D</context>
</contexts>
<marker>Riloff, Schmelzenbach, 1998</marker>
<rawString>Ellen Riloff and Mark Schmelzenbach. 1998. An empirical approach to conceptual case frame acquisition. In Proceedings of the Sixth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>William Phillips</author>
</authors>
<title>Exploiting subjectivity classification to improve information extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI-05.</booktitle>
<contexts>
<context position="6530" citStr="Riloff et al., 2005" startWordPosition="1000" endWordPosition="1003">rge Weakly supervised approaches remove some of corpus. We borrow ideas from this work as well, but the need for fully labeled data. Most still require the our goal is to instead characterize a specific domain templates and their slots. One common approach is with limited data. Further, we are the first to apply to begin with unlabeled, but clustered event-specific this knowledge to the IE task of filling in template documents, and extract common word patterns as mentions in documents. extractors (Riloff and Schmelzenbach, 1998; Sudo In summary, our work extends previous work on et al., 2003; Riloff et al., 2005; Patwardhan and unsupervised IE in a number of ways. We are the Riloff, 2007). Filatova et al. (2006) integrate named first to learn MUC-4 templates, and we are the first entities into pattern learning (PERSON won) to ap- to extract entities without knowing how many temproximate unknown semantic roles. Bootstrapping plates exist, without examples of slot fillers, and with seed examples of known slot fillers has been without event-clustered documents. shown to be effective (Surdeanu et al., 2006; Yan- 3 The Domain and its Templates garber et al., 2000). In contrast, this paper removes Our goal</context>
</contexts>
<marker>Riloff, Wiebe, Phillips, 2005</marker>
<rawString>Ellen Riloff, Janyce Wiebe, and William Phillips. 2005. Exploiting subjectivity classification to improve information extraction. In Proceedings of AAAI-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
<author>Robert P Abelson</author>
</authors>
<title>Scripts, plans, goals and understanding. Lawrence Erlbaum.</title>
<date>1977</date>
<contexts>
<context position="2103" citStr="Schank and Abelson, 1977" startWordPosition="316" endWordPosition="319">Banko et al., 2007a; Carlson et al., 2010), templates can extract a richer representation of a particular domain. However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain’s templates. Very little work addresses how to learn the template structure itself. Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus. There are many ways to represent events, ranging from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson, 1977) and narrative schemas (Chambers and Jurafsky, 2009; Kasch and Oates, 2010). Our approach learns narrative-like knowledge in the form of IE templates; we learn sets of related events and semantic roles, as shown in this sample output from our system: Bombing Template {detonate, blow up, plant, explode, defuse, destroy} Perpetrator: Person who detonates, plants, blows up Instrument: Object that is planted, detonated, defused Target: Object that is destroyed, is blown up A semantic role, such as target, is a cluster of syntactic functions of the template’s event words (e.g., the objects of deton</context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Roger C. Schank and Robert P. Abelson. 1977. Scripts, plans, goals and understanding. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive ie using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="7472" citStr="Shinyama and Sekine (2006)" startWordPosition="1150" endWordPosition="1154">ng plates exist, without examples of slot fillers, and with seed examples of known slot fillers has been without event-clustered documents. shown to be effective (Surdeanu et al., 2006; Yan- 3 The Domain and its Templates garber et al., 2000). In contrast, this paper removes Our goal is to learn the general event structure of these data assumptions, learning instead from a cor- a domain, and then extract the instances of each pus of unknown events and unclustered documents, learned event. In order to measure performance without seed examples. in both tasks (learning structure and extracting inShinyama and Sekine (2006) describe an ap- stances), we use the terrorism corpus of MUC-4 proach to template learning without labeled data. (Sundheim, 1991) as our target domain. This corThey present unrestricted relation discovery as a pus was chosen because it is annotated with temmeans of discovering relations in unlabeled docu- plates that describe all of the entities involved in ments, and extract their fillers. Central to the al- each event. An example snippet from a bombing gorithm is collecting multiple documents describ- document is given here: 977 The terrorists used explosives against the town hall. El Comer</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive ie using unrestricted relation discovery. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoshi Sudo</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>An improved extraction pattern representation model for automatic ie pattern acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings of the Association of Computational Linguistics (ACL),</booktitle>
<pages>224--231</pages>
<marker>Sudo, Sekine, Grishman, 2003</marker>
<rawString>Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2003. An improved extraction pattern representation model for automatic ie pattern acquisition. In Proceedings of the Association of Computational Linguistics (ACL), pages 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth M Sundheim</author>
</authors>
<title>Third message understanding evaluation and conference (muc-3): Phase 1 status report.</title>
<date>1991</date>
<booktitle>In Proceedings of the Message Understanding Conference.</booktitle>
<contexts>
<context position="7602" citStr="Sundheim, 1991" startWordPosition="1173" endWordPosition="1174">hown to be effective (Surdeanu et al., 2006; Yan- 3 The Domain and its Templates garber et al., 2000). In contrast, this paper removes Our goal is to learn the general event structure of these data assumptions, learning instead from a cor- a domain, and then extract the instances of each pus of unknown events and unclustered documents, learned event. In order to measure performance without seed examples. in both tasks (learning structure and extracting inShinyama and Sekine (2006) describe an ap- stances), we use the terrorism corpus of MUC-4 proach to template learning without labeled data. (Sundheim, 1991) as our target domain. This corThey present unrestricted relation discovery as a pus was chosen because it is annotated with temmeans of discovering relations in unlabeled docu- plates that describe all of the entities involved in ments, and extract their fillers. Central to the al- each event. An example snippet from a bombing gorithm is collecting multiple documents describ- document is given here: 977 The terrorists used explosives against the town hall. El Comercio reported that alleged Shining Path members also attacked public facilities in huarpacha, Ambo, tomayquichua, and kichki. Munic</context>
</contexts>
<marker>Sundheim, 1991</marker>
<rawString>Beth M. Sundheim. 1991. Third message understanding evaluation and conference (muc-3): Phase 1 status report. In Proceedings of the Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
<author>Alicia Ageno</author>
</authors>
<title>A hybrid approach for the acquisition of information extraction patterns.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL Workshop on Adaptive Text Extraction and Mining.</booktitle>
<contexts>
<context position="7030" citStr="Surdeanu et al., 2006" startWordPosition="1079" endWordPosition="1082">ors (Riloff and Schmelzenbach, 1998; Sudo In summary, our work extends previous work on et al., 2003; Riloff et al., 2005; Patwardhan and unsupervised IE in a number of ways. We are the Riloff, 2007). Filatova et al. (2006) integrate named first to learn MUC-4 templates, and we are the first entities into pattern learning (PERSON won) to ap- to extract entities without knowing how many temproximate unknown semantic roles. Bootstrapping plates exist, without examples of slot fillers, and with seed examples of known slot fillers has been without event-clustered documents. shown to be effective (Surdeanu et al., 2006; Yan- 3 The Domain and its Templates garber et al., 2000). In contrast, this paper removes Our goal is to learn the general event structure of these data assumptions, learning instead from a cor- a domain, and then extract the instances of each pus of unknown events and unclustered documents, learned event. In order to measure performance without seed examples. in both tasks (learning structure and extracting inShinyama and Sekine (2006) describe an ap- stances), we use the terrorism corpus of MUC-4 proach to template learning without labeled data. (Sundheim, 1991) as our target domain. This </context>
</contexts>
<marker>Surdeanu, Turmo, Ageno, 2006</marker>
<rawString>Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006. A hybrid approach for the acquisition of information extraction patterns. In Proceedings of the EACL Workshop on Adaptive Text Extraction and Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert S Swier</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Unsupervised semantic role labelling.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods on Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="16254" citStr="Swier and Stevenson, 2004" startWordPosition="2582" endWordPosition="2585">ster’s retrieved documents. Pruning 50% was arbitrarily chosen to improve precision, and we did not experiment with other quantities. A search for optimum parameter values may lead to better results. 4.3 Inducing Semantic Roles (Slots) Having successfully clustered event words and retrieved an IR-corpus for each cluster, we now address the problem of inducing semantic roles. Our learned roles will then extract entities in the next section and we will evaluate their per-role accuracy. Most work on unsupervised role induction focuses on learning verb-specific roles, starting with seed examples (Swier and Stevenson, 2004; He and E 1 − log4(g(wi, wj)) (1) w;,wjEd P (4) dist(wi, wj) = 979 Gildea, 2006) and/or knowing the number of roles (Grenager and Manning, 2006; Lang and Lapata, 2010). Our previous work (Chambers and Jurafsky, 2009) learned situation-specific roles over narrative schemas, similar to frame roles in FrameNet (Baker et al., 1998). Schemas link the syntactic relations of verbs by clustering them based on observing coreferring arguments in those positions. This paper extends this intuition by introducing a new vectorbased approach to coreference similarity. 4.3.1 Syntactic Relations as Roles We l</context>
</contexts>
<marker>Swier, Stevenson, 2004</marker>
<rawString>Robert S. Swier and Suzanne Stevenson. 2004. Unsupervised semantic role labelling. In Proceedings of the 2004 Conference on Empirical Methods on Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Xiao</author>
<author>Tat-Seng Chua</author>
<author>Hang Cui</author>
</authors>
<title>Cascading use of soft and hard matching pattern rules for weakly supervised information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="30690" citStr="Xiao et al. (2004)" startWordPosition="4930" endWordPosition="4933">orpus is to report the F1 score for slot type accuracy, ignoring the template type. For instance, a perpetrator of a bombing and a perpetrator of an attack are treated the same. This allows supervised classifiers to train on all perpetrators at once, rather than template-specific learners. Although not ideal for our learning goals, we report it for comparison against previous work. Several supervised approaches have presented results on MUC-4, but unfortunately we cannot compare against them. Maslennikov and Chua (2006; 2007) evaluated a random subset of test (they report .60 and .63 F1), and Xiao et al. (2004) did not evaluate all slot types (they report .57 F1). Figure 5 thus shows our results with previous work that is comparable: the fully supervised and P R F1 Patwardhan &amp; Riloff-09 : Supervised 48 59 53 Patwardhan &amp; Riloff-07 : Weak-Sup 42 48 44 Our Results (1 attack) 48 25 33 Our Results (5 attack) 44 36 40 Figure 5: MUC-4 extraction, ignoring template type. F1 Score Kidnap Bomb Arson Attack Results .53 .43 .42 .16 / .25 Figure 6: Performance of individual templates. Attack compares our 1 vs 5 best templates. weakly supervised approaches of Patwardhan and Riloff (2009; 2007). We give two numb</context>
</contexts>
<marker>Xiao, Chua, Cui, 2004</marker>
<rawString>Jing Xiao, Tat-Seng Chua, and Hang Cui. 2004. Cascading use of soft and hard matching pattern rules for weakly supervised information extraction. In Proceedings of the 20th International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
<author>Pasi Tapanainen</author>
<author>Silja Huttunen</author>
</authors>
<title>Automatic acquisition of domain knowledge for information extraction.</title>
<date>2000</date>
<booktitle>In COLING,</booktitle>
<pages>940--946</pages>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>Roman Yangarber, Ralph Grishman, Pasi Tapanainen, and Silja Huttunen. 2000. Automatic acquisition of domain knowledge for information extraction. In COLING, pages 940–946.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>