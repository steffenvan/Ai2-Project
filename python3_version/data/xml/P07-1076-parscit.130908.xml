<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027554">
<title confidence="0.995779">
Using Corpus Statistics on Entities to Improve Semi-supervised
Relation Extraction from the Web
</title>
<author confidence="0.935151">
Benjamin Rosenfeld
</author>
<affiliation confidence="0.830433">
Information Systems
HU School of Business,
Hebrew University, Jerusalem, Israel
</affiliation>
<email confidence="0.99696">
grurgrur@gmail.com
</email>
<author confidence="0.83988">
Ronen Feldman
</author>
<affiliation confidence="0.768162">
Information Systems
HU School of Business,
Hebrew University, Jerusalem, Israel
</affiliation>
<email confidence="0.99782">
ronen.feldman@huji.ac.il
</email>
<sectionHeader confidence="0.995619" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999881217391305">
Many errors produced by unsupervised and
semi-supervised relation extraction (RE)
systems occur because of wrong recogni-
tion of entities that participate in the rela-
tions. This is especially true for systems
that do not use separate named-entity rec-
ognition components, instead relying on
general-purpose shallow parsing. Such sys-
tems have greater applicability, because
they are able to extract relations that
contain attributes of unknown types.
However, this generality comes with the
cost in accuracy. In this paper we show
how to use corpus statistics to validate and
correct the arguments of extracted relation
instances, improving the overall RE
performance. We test the methods on
SRES – a self-supervised Web relation
extraction system. We also compare the
performance of corpus-based methods to
the performance of validation and correc-
tion methods based on supervised NER
components.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926581395349">
Information Extraction (IE) is the task of extract-
ing factual assertions from text. Most IE systems
rely on knowledge engineering or on machine
learning to generate the “task model” that is subse-
quently used for extracting instances of entities and
relations from new text. In the knowledge engi-
neering approach the model (usually in the form of
extraction rules) is created manually, and in the
machine learning approach the model is learned
automatically from a manually labeled training set
of documents. Both approaches require substantial
human effort, particularly when applied to the
broad range of documents, entities, and relations
on the Web. In order to minimize the manual ef-
fort necessary to build Web IE systems, semi-
supervised and completely unsupervised systems
are being developed by many researchers.
The task of extracting facts from the Web has
significantly different aims than the regular infor-
mation extraction. The goal of regular IE is to
identify and label all mentions of all instances of
the given relation type inside a document or inside
a collection of documents. Whereas, in the Web
Extraction (WE) tasks we are only interested in
extracting relation instances and not interested in
particular mentions.
This difference in goals leads to a difference in
the methods of performance evaluation. The usual
measures of performance of regular IE systems are
precision, recall, and their combinations – the
breakeven point and F-measure. Unfortunately, the
true recall usually cannot be known for WE tasks.
Consequently, for evaluating the performance of
WE systems, the recall is substituted by the num-
ber of extracted instances.
WE systems usually order the extracted in-
stances by the system’s confidence in their cor-
rectness. The precision of top-confidence extrac-
tions is usually very high, but it gets progressively
lower when lower-confidence candidates are con-
sidered. The curve that plots the number of extrac-
tions against precision level is the best indicator of
system’s quality. Naturally, for a comparision be-
</bodyText>
<page confidence="0.958199">
600
</page>
<note confidence="0.925485">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 600–607,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999517311688312">
tween different systems to be meaningful, the
evaluations must be performed on the same corpus.
In this paper we are concerned with Web RE
systems that extract binary relations between
named entities. Most of such systems utilize sepa-
rate named entity recognition (NER) components,
which are usualy trained in a supervised way on a
separate set of manually labeled documents. The
NER components recognize and extract the values
of relation attributes (also called arguments, or
slots), while the RE systems are concerned with
patterns of contexts in which the slots appear.
However, good NER components only exist for
common and very general entity types, such as
Person, Organization, and Location. For some re-
lations, the types of attributes are less common,
and no ready NER components (or ready labeled
training sets) exist for them. Also, some Web RE
systems (e.g., KnowItAll (Etzioni, Cafarella et al.
2005)) do not use separate NER components even
for known entity types, because such components
are usually domain-specific and may perform
poorly on cross-domain text collections extracted
from the Web.
In such cases, the values for relation attributes
must be extracted by generic methods – shallow
parsing (extracting noun phrases), or even simple
substring extraction. Such methods are naturally
much less precise and produce many entity-
recognition errors (Feldman and Rosenfeld 2006).
In this paper we propose several methods of us-
ing corpus statistics to improve Web RE precision
by validating and correcting the entities extracted
by generic methods. The task of Web Extraction is
particularly suited for the corpus statistics-based
methods because of very large size of the corpora
involved, and because the system is not required to
identify individual mentions of the relations.
Our methods of entity validation and correction
are based on the following two observations:
First, the entities that appear in target relations
will often also appear in many other contexts,
some of which may strongly discriminate in favor
of entities of specific type. For example, assume
the system encounters a sentence “Oracle bought
PeopleSoft.” If the system works without a NER
component, it only knows that “Oracle” and “Peo-
pleSoft” are proper noun phrases, and its confi-
dence in correctness of a candidate relation in-
stance Acquisition(Oracle, PeopleSoft) cannot be
very high. However, both entities occur many
times elsewhere in the corpus, sometimes in
strongly discriminating contexts, such as “Oracle
is a company that...” or “PeopleSoft Inc.” If the
system somehow learned that such contexts indi-
cate entities of the correct type for the Acquisition
relation (i.e., companies), then the system would
be able to boost its confidence in both entities
(“Oracle” and “PeopleSoft”) being of correct types
and, consequently, in (Oracle, PeopleSoft) being a
correct instance of the Acquisition relation.
Another observation that we can use is the fact
that the entities, in which we are interested, usually
have sufficient frequency in the corpus for statisti-
cal term extraction methods to perform reasonably
well. These methods may often correct a wrongly
placed entity boundary, which is a common mis-
take of general-purpose shallow parsers.
In this paper we show how to use these observa-
tions to supplement a Web RE system with an en-
tity validation and correction component, which is
able to significantly improve the system’s accu-
racy. We evaluate the methods using SRES
(Feldman and Rosenfeld 2006) – a Web RE sys-
tem, designed to extend and improve KnowItAll
(Etzioni, Cafarella et al. 2005). The contributions
of this paper are as follows:
</bodyText>
<listItem confidence="0.990499333333333">
• We show how to automatically generate
the validating patterns for the target relation
arguments, and how to integrate the results
produced by the validating patterns into the
whole relation extraction system.
• We show how to use corpus statistics and
term extraction methods to correct the
boundaries of relation arguments.
• We experimentally compare the improve-
</listItem>
<bodyText confidence="0.865077615384616">
ment produced by the corpus-based entity
validation and correction methods with the
improvements produced by two alternative
validators – a CRF-based NER system
trained on a separate labeled corpus, and a
small manually-built rule-based NER com-
ponent.
The rest of the paper is organized as follows:
Section 2 describes previous work. Section 3 out-
lines the general design principles of SRES and
briefly describes its components. Section 4 de-
scribes in detail the different entity validation and
correction methods, and Section 5 presents their
</bodyText>
<page confidence="0.944691">
601
</page>
<bodyText confidence="0.99934587755102">
experimental evaluation. Section 6 contains con- relations. A specification for a given relation con-
clusions and directions for future work. sists of the relation schema and a small set of seeds
2 Related Work – known true instances of the relation. In the full-
We are not aware of any work that deals specifi- scale SRES, the seeds are also generated automati-
cally with validation and/or correction of entity cally, by using a set of generic patterns instantiated
recognition for the purposes of improving relation with the relation schema. However, the seed gen-
extraction accuracy. However, the background eration is not relevant to this paper.
techniques of our methods are relatively simple A relation schema specifies the name of the rela-
and known. The validation is based on the same tion, the names and types of its arguments, and the
ideas that underlie semi-supervised entity extrac- arguments ordering. For example, the schema of
tion (Etzioni, Cafarella et al. 2005), and uses a the Acquisition relation
simplified SRES code. The boundary correction Acquisition(Buyer=ProperNP,
process utilizes well-known term extraction meth- Acquired=ProperNP) ordered
ods, e.g., (Su, Wu et al. 1994). specifies that Acquisition has two slots, named
We also recently became aware of the work by Buyer and Acquired, which must be filled with en-
Downey, Broadhead and Etzioni (2007) that deals tities of type ProperNP. The order of the slots is
with locating entities of arbitrary types in large important (as signified by the word “ordered”, and
corpora using corpus statistics. as opposed to relations like Merger, which are
The IE systems most similar to SRES are based “unordered” or, in binary case, “symmetric”).
on bootstrap learning: Mutual Bootstrapping The baseline SRES does not utilize a named en-
(Riloff and Jones 1999), the DIPRE system (Brin tity recognizer, instead using a shallow parser for
1998), and the Snowball system (Agichtein and exracting the relation slots. Thus, the only allowed
Gravano 2000). Ravichandran and Hovy entity types are ProperNP, CommonNP, and
(Ravichandran and Hovy 2002) also use bootstrap- AnyNP, which mean the heads of, respectively,
ping, and learn simple surface patterns for extract- proper, common, and arbitrary noun phrases. In the
ing binary relations from the Web. experimental section we compare the baseline
Unlike these systems, SRES surface patterns al- SRES to its extensions containing additional NER
low gaps that can be matched by any sequences of components. When using those components we
tokens. This makes SRES patterns more general, allow further subtypes of ProperNP, and the rela-
and allows to recognize instances in sentences in- tion schema above becomes
accessible to the simple surface patterns of systems ... (Buyer=Company, Acquired=Company) ...
such as (Brin 1998; Riloff and Jones 1999; Ravi- The main components of SRES are the Pattern
chandran and Hovy 2002). Learner, the Instance Extractor, and the Classifier.
Another direction for unsupervised relation The Pattern Learner uses the seeds to learn likely
learning was taken in (Hasegawa, Sekine et al. patterns of relation occurrences. Then, the Instance
2004; Chen, Ji et al. 2005). These systems use a Extractor uses the patterns to extract the candidate
NER system to identify frequent pairs of entities instances from the sentences. Finally, the Classifier
and then cluster the pairs based on the types of the assigns the confidence score to each extraction. We
entities and the words appearing between the enti- shall now briefly describe these components.
ties. The main benefit of this approach is that all 3.1 Pattern Learner
relations between two entity types can be discov- The Pattern Learner receives a relation schema
ered simultaneously and there is no need for the and a set of seeds. Then it finds the occurences of
user to supply the relations definitions. seeds inside a large (unlabeled) text corpus, ana-
3 Description of SRES lyzes their contexts, and extracts common patterns
The goal of SRES is extracting instances of speci- among these contexts. The details of the patterns
fied relations from the Web without human super- language and the process of pattern learning are
vision. Accordingly, the supervised input to the not significant for this paper, and are described
system is limited to the specifications of the target fully in (Feldman and Rosenfeld 2006).
602
</bodyText>
<subsectionHeader confidence="0.999442">
3.2 Instance Extractor
</subsectionHeader>
<bodyText confidence="0.9999843">
The Instance Extractor applies the patterns gener-
ated by the Pattern Learner to the text corpus. In
order to be able to match the slots of the patterns,
the Instance Extractor utilizes an external shallow
parser from the OpenNLP package
(http://opennlp.sourceforge.net/), which is able to
find all proper and common noun phrases in a sen-
tence. These phrases are matched to the slots of the
patterns. In other respects, the pattern matching
and extraction process is straightforward.
</bodyText>
<subsectionHeader confidence="0.993322">
3.3 Classifier
</subsectionHeader>
<bodyText confidence="0.99986752173913">
The goal of the final classification stage is to filter
the list of all extracted instances, keeping the cor-
rect extractions, and removing mistakes that would
always occur regardless of the quality of the pat-
terns. It is of course impossible to know which ex-
tractions are correct, but there exist properties of
patterns and pattern matches that increase or de-
crease the confidence in the extractions that they
produce.
These properties are turned into a set of binary
features, which are processed by a linear feature-
rich classifier. The classifier receives a feature vec-
tor for a candidate, and produces a confidence
score between 0 and 1.
The set of features is small and is not specific to
any particular relation. This allows to train a model
using a small amount of labeled data for one rela-
tion, and then use the model for scoring the candi-
dates of all other relations. Since the supervised
training stage needs to be run only once, it is a part
of the system development, and the complete sys-
tem remains unsupervised, as demonstrated in
(Feldman and Rosenfeld 2006).
</bodyText>
<sectionHeader confidence="0.980657" genericHeader="method">
4 Entity Validation and Correction
</sectionHeader>
<bodyText confidence="0.999352761904762">
In this paper we describe three different methods
of validation and correction of relation arguments
in the extracted instances. Two of them are “classi-
cal” and are based, respectively, on the knowledge-
engineering, and on the statistical supervised ap-
proaches to the named entity recognition problems.
The third is our novel approach, based on redun-
dancy and corpus statistics.
The methods are implemented as components
for SRES, called Entity Validators, inserted be-
tween the Instance Extractor and the Classifier.
The result of applying Entity Validator to a candi-
date instance is an (optionally) fixed instance, with
validity values attached to all slots. There are three
validity values: valid, invalid, and uncertain.
The Classifier uses the validity values by con-
verting them into two additional binary features,
which are then able to influence the confidence of
extractions.
We shall now describe the three different valida-
tors in details.
</bodyText>
<subsectionHeader confidence="0.996922">
4.1 Small Rule-based NER validator
</subsectionHeader>
<bodyText confidence="0.9972585">
This validator is a small Perl script that checks
whether a character string conforms to a set of
simple regular expression patterns, and whether it
appears inside lists of known named entities. There
are two sets of regular expression patterns – for
Person and for Company entity types, and three
large lists – for known personal names, known
companies, and “other known named entities”, cur-
rently including locations, universities, and gov-
ernment agencies.
The manually written regular expression repre-
sent simple regularities in the internal structure of
the entity types. For example, the patterns for Per-
son include:
</bodyText>
<figure confidence="0.983537904761905">
Person = KnownFirstName [Initial] LastName
Person = Honorific [FirstName] [Initial] LastName
Honorific = (“Mr”  |“Ms”  |“Dr” |...) [“.”]
Initial = CapitalLetter [“.”]
KnownFirstName = member of
KnownPersonalNamesList
FirstName = CapitalizedWord
LastName = CapitalizedWord
LastName = CapitalizedWord [“–”CapitalizedWord]
LastName = (“o”  |“de”  |...) “`”CapitalizedWord
...
while the patterns for Company include:
Company = KnownCompanyName
Company = CompanyName CompanyDesignator
Company = CompanyName FrequentCompanySfx
KnownCompanyName = member of
KnownCompaniesList
CompanyName = CapitalizedWord +
CompanyDesignator = “inc”  |“corp”  |“co”  |...
FrequentCompanySfx = “systems”  |“software”  |...
...
</figure>
<bodyText confidence="0.9919046">
The validator works in the following way: it re-
ceives a sentence with a labeled candidate entity of
a specified entity type (which can be either Person
or Company). It then applies all of the regular ex-
pression patterns to the labeled text and to its en-
</bodyText>
<page confidence="0.99677">
603
</page>
<bodyText confidence="0.999957086956522">
closing context. It also checks for membership in
the lists of known entities. If a boundary is incor-
rectly placed according to the patterns or to the
lists, it is fixed. Then, the following result is re-
turned:
Valid, if some pattern/list of the right entity type
matched the candidate entity, while there
were no matches for patterns/lists of other
entity types.
Invalid, if no pattern/list of the right entity type
matched the candidate entity, while there
were matches for patterns/lists of other entity
types.
Uncertain, otherwise, that is either if there were
no matches at all, or if both correct and in-
correct entity types matched.
The number of patterns is relatively small, and
the whole component consists of about 300 lines in
Perl and costs several person-days of knowledge
engineering work. Despite its simplicity, we will
show in the experimental section that it is quite
effective, and even often outperforms the CRF-
based NER component, described below.
</bodyText>
<subsectionHeader confidence="0.975193">
4.2 CRF-based NER validator
</subsectionHeader>
<bodyText confidence="0.999326285714286">
This validator is built using a feature-rich CRF-
based sequence classifier, trained upon an English
dataset of the CoNLL 2003 shared task (Rosenfeld,
Fresko et al. 2005). For the gazetteer lists it uses
the same large lists as the rule-based component
described above.
The validator receives a sentence with a labeled
candidate entity of a specified entity type (which
can be either Person or Company). It then sends
the sentence to the CRF-based classifier, which
labels all named entities it knows – Dates, Times,
Percents, Persons, Organizations, and Locations.
If the CRF classifier places the entity boundaries
differently, they are fixed. Then, the following re-
sult is returned:
Valid, if CRF classification of the entity accords
with the expected argument type.
Invalid, if CRF classification of the entity is dif-
ferent from the expected argument type.
Uncertain, otherwise, that is if the CRF classi-
fier didn’t recognize the entity at all.
</bodyText>
<subsectionHeader confidence="0.99521">
4.3 Corpus-based NER validator
</subsectionHeader>
<bodyText confidence="0.9987555">
The goal of building the corpus-based NER valida-
tor is to provide the same level of performance as
the supervised NER components, while requiring
neither additional human supervision nor addi-
tional labeled corpora or other resources. There are
several important facts that help achieve this goal.
First, the relation instances that are used as seeds
for the pattern learning are known to contain cor-
rect instances of the right entity type. These in-
stances can be used as seeds in their own right, for
learning the patterns of occurrence of the corre-
sponding entity types. Second, the entities in which
we are interested usually appear in the corpus with
a sufficient frequency. The validation is based on
the first observation, while the boundary fixing on
the second.
</bodyText>
<subsectionHeader confidence="0.507457">
Corpus-based entity validation
</subsectionHeader>
<bodyText confidence="0.999950896551724">
There is a preparation stage, during which the
information required for validation is extracted
from the corpus. This information is the lists of all
entities of every type that appears in the target rela-
tions. In order to extract these lists we use a simpli-
fied SRES. The entities are considered to be unary
relations, and the seeds for them are taken from the
slots of the target binary relations seeds. We don’t
use the Classifier on the extracted entity instances.
Instead, for every extracted instance we record the
number of different sentences the entity was ex-
tracted from.
During the validation process, the validator’s
task is to evaluate a given candidate entity in-
stance. The validator compares the number of
times the instance was extracted (during the prepa-
ration stage) by the patterns for the correct entity
type, and by the patterns for all other entity types.
The validator then returns
Valid, if the number of times the entity was ex-
tracted for the specified entity type is at least
5, and at least two times bigger than the
number of times it was extracted for all other
entity types.
Invalid, if the number of times the instance was
extracted for the specified entity type is less
than 5, and at least 2 times smaller than the
number of times it was extracted for all other
entity types.
</bodyText>
<page confidence="0.997729">
604
</page>
<bodyText confidence="0.99759338">
Uncertain, otherwise, that is if it was never ex-
tracted at all, or extracted with similar fre-
quency for both correct and wrong entity
types.
Corpus-based correction of entity boundaries
Our entity boundaries correction mechanism is
similar to the known statistical term extraction
techniques (Su, Wu et al. 1994). It is based on the
assumption that the component words of a term (an
entity in our case) are more tightly bound to each
other than to the context. In the statistical sense,
this fact is expressed by a high mutual information
between the adjacent words belonging to the same
term.
There are two possible boundary fixes: remov-
ing words from the candidate entity, or adding
words from the context to the entity. There is a
significant practical difference between the two
cases.
Assume that an entity boundary was placed too
broadly, and included extra words. If this was a
chance occurrence (and only such cases can be
found by statistical methods), then the resulting
sequence of tokens will be very infrequent, while
its parts will have relatively high frequency. For
example, consider a sequence “Formerly Microsoft
Corp.”, which is produced by mistakenly labeling
“Formerly” as a proper noun by the PoS tagger.
While it is easy to know from the frequencies that
a boundary mistake was made, it is unclear (to the
system) which part is the correct entity. But since
the entity (one of the parts of the candidate) has a
high frequency, there is a chance that the relation
instance, in which the entity appears, will be re-
peated elsewhere in the corpus and will be ex-
tracted correctly there. Therefore, in such case, the
simplest recourse is to simply label the entity as
Invalid, and not to try fixing the boundaries.
On the other hand, if a word was missed from an
entity (e.g., “Beverly O”, instead of “Beverly O &apos;
Neill”), the resulting sequence will be frequent.
Moreover, it is quite probable that the same
boundary mistake is made in many places, because
the same sequence of tokens is being analyzed in
all those places. Therefore, it makes sense to try to
fix the bounary in this case, especially since it can
be done simply and reliably: a word (or several
words) is attached to the entity string if both their
frequencies and their mutual information are above
a threshold.
</bodyText>
<sectionHeader confidence="0.98965" genericHeader="method">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999874142857143">
The experiments described in this paper aim to
confirm the effectiveness of the proposed corpus-
based relation argument validation and correction
method, and to compare its performance with the
classical knowledge-engineering-based and super-
vised-training-based methods. The experiments
were performed with five relations:
</bodyText>
<construct confidence="0.6386274">
Acquisition(BuyerCompany, AcquiredCompany),
Merger(Company1, Company2),
CEO_Of(Company, Person),
MayorOf(City, Person),
InventorOf(Person, Invention).
</construct>
<bodyText confidence="0.999982540540541">
The data for the experiments were collected by the
KnowItAll crawler. The data for the Acquisition
and Merger consist of about 900,000 sentences for
each of the two relations. The data for the bound
relations consist of sentences, such that each con-
tains one of a hundred values of the first (bound)
attribute. Half of the hundred are frequent entities
(&gt;100,000 search engine hits), and another half are
rare (&lt;10,000 hits).
For evaluating the validators we randomly se-
lected a set of 10000 sentences from the corpora
for each of the relations, and manually evaluated
the SRES results generated from these sentences.
Four sets of results were evaluated: the baseline
results produced without any NER validator, and
three sets of results produced using three different
NER validators. For the InventorOf relation, only
the corpus-based validator results can be produced,
since the other two NER components cannot be
adapted to validate/correct entities of type Inven-
tion.
The results for the five relations are shown in
the Figure 1. Several conclusions can be drawn
from the graphs. First, all of the NER validators
improve over the baseline SRES, sometimes as
much as doubling the recall at the same level of
precision. In most cases the three validators show
roughly similar levels of performance. A notable
difference is the CEO_Of relation, where the sim-
ple rule-based component performs much better
than CRF, which performs yet better than the cor-
pus-based component. The CEO_Of relation is
tested as bound, which means that only the second
relation argument, of type Person, is validated. The
Person entities have much more rigid internal
structure than the other entities – Companies and
Inventions. Consequently, the best performing of
</bodyText>
<page confidence="0.991213">
605
</page>
<figure confidence="0.999473454545454">
Precision
Precision
0.90
0.80
0.70
0.60
0.50
1.00
0.90
0.80
0.70
0.60
0.50
1.00
Baseline RB-NER CRF Corpus
Baseline RB-NER CRF Corpus
0 20 40 60 80 100 120
Correct Extractions
0 50 100 150
Correct Extractions
Acquisition
CeoOf
Precision
Precision
0.90
0.80
0.70
0.60
0.50
1.00
0.90
0.80
0.70
0.60
0.50
1.00
Baseline RB-NER CRF Corpus
0 50 100 150
Correct Extractions
0 20 40 60 80 100 120
Correct Extractions
InventorOf
Merger
Baseline Corpus
</figure>
<figureCaption confidence="0.890947">
Figure 1. Comparison between Baseline-SRES and its extensions with three different NER validators: a
simple Rule-Based one, a CRF-based statistical one, and a Corpus-based one.
</figureCaption>
<bodyText confidence="0.9999414">
the three validators is the rule-based, which di-
rectly tests this internal structure. The CRF-based
validator is also able to take advantage of the struc-
ture, although in a weaker manner. The Corpus-
based validator, however, works purely on the ba-
sis of context, entirely disregarding the internal
structure of entities, and thus performs worst of all
in this case. On the other hand, the Corpus-based
validator is able to improve the results for the In-
ventor relation, which the other two validators are
completely unable to do.
It is also of interest to compare the performance
of CRF-based and the rule-based NER components
in other cases. As can be seen, in most cases the
rule-based component, despite its simplicity, out-
performs the CRF-based one. The possible reason
for this is that relation extraction setting is signifi-
cantly different from the classical named entity
recognition setting. A classical NER system is set
to maximize the F1 measure of all mentions of all
entities in the corpus. A relation argument extrac-
tor, on the other hand, should maximize its per-
formance on relation arguments, and apparently
their statistical properties are often significantly
different.
</bodyText>
<sectionHeader confidence="0.999721" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999969166666667">
We have presented a novel method for validation
and correction of relation arguments for the state-
of-the-art unsupervised Web relation extraction
system SRES. The method is based on corpus sta-
tistics and requires no human supervision and no
additional corpus resources beyond the corpus that
is used for relation extraction.
We showed experimentally the effectiveness of
our method, which performed comparably to both
simple rule-based NER and a statistical CRF-based
NER in the task of validating Companies, and
somewhat worse in the task of validating Persons,
</bodyText>
<page confidence="0.996658">
606
</page>
<bodyText confidence="0.999924111111111">
due to its complete disregard of internal structure
of entities. The ways to learn and use this structure
in an unsupervised way are left for future research.
Our method also successfully validated the
Invention entities, which are inaccessible to the
other methods due to the lack of training data.
In our experiments we made use of a unique fea-
ture of SRES system – a feature-rich classifier that
assigns confidence score to the candidate in-
stances, basing its decisions on various features of
the patterns and of the contexts from which the
candidates were extracted. This architecture allows
easy integration of the entity validation compo-
nents as additional feature generators. We believe,
however, that our results have greater applicability,
and that the corpus statistics-based components can
be added to RE systems with other architectures as
well.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999782487804878">
Agichtein, E. and L. Gravano (2000). Snowball: Ex-
tracting Relations from Large Plain-Text Collections.
Proceedings of the 5th ACM International Confer-
ence on Digital Libraries (DL).
Brin, S. (1998). Extracting Patterns and Relations from
the World Wide Web. WebDB Workshop at 6th In-
ternational Conference on Extending Database Tech-
nology, EDBT’98, Valencia, Spain.
Chen, J., D. Ji, C. L. Tan and Z. Niu (2005). Unsuper-
vised Feature Selection for Relation Extraction.
IJCNLP-05, Jeju Island, Korea.
Downey, D., M. Broadhead and O. Etzioni (2007). Lo-
cating Complex Named Entities in Web Text. IJCAI-
07.
Etzioni, O., M. Cafarella, D. Downey, A. Popescu, T.
Shaked, S. Soderland, D. Weld and A. Yates (2005).
Unsupervised named-entity extraction from the Web:
An experimental study. Artificial Intelligence 165(1):
91-134.
Feldman, R. and B. Rosenfeld (2006). Boosting Unsu-
pervised Relation Extraction by Using NER.
EMNLP-06, Sydney, Australia.
Feldman, R. and B. Rosenfeld (2006). Self-Supervised
Relation Extraction from the Web. ISMIS-2006, Bari,
Italy.
Hasegawa, T., S. Sekine and R. Grishman (2004). Dis-
covering Relations among Named Entities from
Large Corpora. ACL 2004.
Ravichandran, D. and E. Hovy (2002). Learning Sur-
face Text Patterns for a Question Answering System.
40th ACL Conference.
Riloff, E. and R. Jones (1999). Learning Dictionaries
for Information Extraction by Multi-level Boot-
strapping. AAAI-99.
Rosenfeld, B., M. Fresko and R. Feldman (2005). A
Systematic Comparison of Feature-Rich Probabilis-
tic Classifiers for NER Tasks. PKDD.
Su, K.-Y., M.-W. Wu and J.-S. Chang (1994). A Cor-
pus-based Approach to Automatic Compound Ex-
traction. Meeting of the Association for Computa-
tional Linguistics: 242-247.
</reference>
<page confidence="0.997771">
607
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.376225">
<title confidence="0.9996665">Using Corpus Statistics on Entities to Improve Semi-supervised Relation Extraction from the Web</title>
<author confidence="0.999736">Benjamin Rosenfeld</author>
<affiliation confidence="0.875746333333333">Information Systems HU School of Business, Hebrew University, Jerusalem, Israel</affiliation>
<email confidence="0.999881">grurgrur@gmail.com</email>
<author confidence="0.99911">Ronen Feldman</author>
<affiliation confidence="0.992692">Information Systems HU School of Business,</affiliation>
<address confidence="0.676495">Hebrew University, Jerusalem, Israel</address>
<email confidence="0.99892">ronen.feldman@huji.ac.il</email>
<abstract confidence="0.99540975">Many errors produced by unsupervised and semi-supervised relation extraction (RE) systems occur because of wrong recognition of entities that participate in the relations. This is especially true for systems that do not use separate named-entity recognition components, instead relying on general-purpose shallow parsing. Such systems have greater applicability, because they are able to extract relations that contain attributes of unknown types. However, this generality comes with the cost in accuracy. In this paper we show how to use corpus statistics to validate and correct the arguments of extracted relation instances, improving the overall RE performance. We test the methods on SRES – a self-supervised Web relation extraction system. We also compare the performance of corpus-based methods to the performance of validation and correction methods based on supervised NER components.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>L Gravano</author>
</authors>
<title>Snowball: Extracting Relations from Large Plain-Text Collections.</title>
<date>2000</date>
<booktitle>Proceedings of the 5th ACM International Conference on Digital Libraries (DL).</booktitle>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Agichtein, E. and L. Gravano (2000). Snowball: Extracting Relations from Large Plain-Text Collections. Proceedings of the 5th ACM International Conference on Digital Libraries (DL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
</authors>
<title>Extracting Patterns and Relations from the World Wide Web. WebDB</title>
<date>1998</date>
<booktitle>Workshop at 6th International Conference on Extending Database Technology, EDBT’98,</booktitle>
<location>Valencia,</location>
<contexts>
<context position="10883" citStr="Brin 1998" startWordPosition="1681" endWordPosition="1682">ct- proper, common, and arbitrary noun phrases. In the ing binary relations from the Web. experimental section we compare the baseline Unlike these systems, SRES surface patterns al- SRES to its extensions containing additional NER low gaps that can be matched by any sequences of components. When using those components we tokens. This makes SRES patterns more general, allow further subtypes of ProperNP, and the relaand allows to recognize instances in sentences in- tion schema above becomes accessible to the simple surface patterns of systems ... (Buyer=Company, Acquired=Company) ... such as (Brin 1998; Riloff and Jones 1999; Ravi- The main components of SRES are the Pattern chandran and Hovy 2002). Learner, the Instance Extractor, and the Classifier. Another direction for unsupervised relation The Pattern Learner uses the seeds to learn likely learning was taken in (Hasegawa, Sekine et al. patterns of relation occurrences. Then, the Instance 2004; Chen, Ji et al. 2005). These systems use a Extractor uses the patterns to extract the candidate NER system to identify frequent pairs of entities instances from the sentences. Finally, the Classifier and then cluster the pairs based on the types </context>
</contexts>
<marker>Brin, 1998</marker>
<rawString>Brin, S. (1998). Extracting Patterns and Relations from the World Wide Web. WebDB Workshop at 6th International Conference on Extending Database Technology, EDBT’98, Valencia, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen</author>
<author>D Ji</author>
<author>C L Tan</author>
<author>Z Niu</author>
</authors>
<title>Unsupervised Feature Selection for Relation Extraction. IJCNLP-05,</title>
<date>2005</date>
<location>Jeju Island,</location>
<marker>Chen, Ji, Tan, Niu, 2005</marker>
<rawString>Chen, J., D. Ji, C. L. Tan and Z. Niu (2005). Unsupervised Feature Selection for Relation Extraction. IJCNLP-05, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Downey</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<date>2007</date>
<booktitle>Locating Complex Named Entities in Web Text. IJCAI07.</booktitle>
<marker>Downey, Broadhead, Etzioni, 2007</marker>
<rawString>Downey, D., M. Broadhead and O. Etzioni (2007). Locating Complex Named Entities in Web Text. IJCAI07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the Web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence</journal>
<volume>165</volume>
<issue>1</issue>
<pages>91--134</pages>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Etzioni, O., M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld and A. Yates (2005). Unsupervised named-entity extraction from the Web: An experimental study. Artificial Intelligence 165(1): 91-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Feldman</author>
<author>B Rosenfeld</author>
</authors>
<title>Boosting Unsupervised Relation Extraction by Using</title>
<date>2006</date>
<booktitle>NER. EMNLP-06,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="4884" citStr="Feldman and Rosenfeld 2006" startWordPosition="736" endWordPosition="739">ready NER components (or ready labeled training sets) exist for them. Also, some Web RE systems (e.g., KnowItAll (Etzioni, Cafarella et al. 2005)) do not use separate NER components even for known entity types, because such components are usually domain-specific and may perform poorly on cross-domain text collections extracted from the Web. In such cases, the values for relation attributes must be extracted by generic methods – shallow parsing (extracting noun phrases), or even simple substring extraction. Such methods are naturally much less precise and produce many entityrecognition errors (Feldman and Rosenfeld 2006). In this paper we propose several methods of using corpus statistics to improve Web RE precision by validating and correcting the entities extracted by generic methods. The task of Web Extraction is particularly suited for the corpus statistics-based methods because of very large size of the corpora involved, and because the system is not required to identify individual mentions of the relations. Our methods of entity validation and correction are based on the following two observations: First, the entities that appear in target relations will often also appear in many other contexts, some of</context>
<context position="6995" citStr="Feldman and Rosenfeld 2006" startWordPosition="1069" endWordPosition="1072">of the Acquisition relation. Another observation that we can use is the fact that the entities, in which we are interested, usually have sufficient frequency in the corpus for statistical term extraction methods to perform reasonably well. These methods may often correct a wrongly placed entity boundary, which is a common mistake of general-purpose shallow parsers. In this paper we show how to use these observations to supplement a Web RE system with an entity validation and correction component, which is able to significantly improve the system’s accuracy. We evaluate the methods using SRES (Feldman and Rosenfeld 2006) – a Web RE system, designed to extend and improve KnowItAll (Etzioni, Cafarella et al. 2005). The contributions of this paper are as follows: • We show how to automatically generate the validating patterns for the target relation arguments, and how to integrate the results produced by the validating patterns into the whole relation extraction system. • We show how to use corpus statistics and term extraction methods to correct the boundaries of relation arguments. • We experimentally compare the improvement produced by the corpus-based entity validation and correction methods with the improve</context>
<context position="12461" citStr="Feldman and Rosenfeld 2006" startWordPosition="1934" endWordPosition="1937">re is no need for the and a set of seeds. Then it finds the occurences of user to supply the relations definitions. seeds inside a large (unlabeled) text corpus, ana3 Description of SRES lyzes their contexts, and extracts common patterns The goal of SRES is extracting instances of speci- among these contexts. The details of the patterns fied relations from the Web without human super- language and the process of pattern learning are vision. Accordingly, the supervised input to the not significant for this paper, and are described system is limited to the specifications of the target fully in (Feldman and Rosenfeld 2006). 602 3.2 Instance Extractor The Instance Extractor applies the patterns generated by the Pattern Learner to the text corpus. In order to be able to match the slots of the patterns, the Instance Extractor utilizes an external shallow parser from the OpenNLP package (http://opennlp.sourceforge.net/), which is able to find all proper and common noun phrases in a sentence. These phrases are matched to the slots of the patterns. In other respects, the pattern matching and extraction process is straightforward. 3.3 Classifier The goal of the final classification stage is to filter the list of all e</context>
<context position="14060" citStr="Feldman and Rosenfeld 2006" startWordPosition="2203" endWordPosition="2206">d into a set of binary features, which are processed by a linear featurerich classifier. The classifier receives a feature vector for a candidate, and produces a confidence score between 0 and 1. The set of features is small and is not specific to any particular relation. This allows to train a model using a small amount of labeled data for one relation, and then use the model for scoring the candidates of all other relations. Since the supervised training stage needs to be run only once, it is a part of the system development, and the complete system remains unsupervised, as demonstrated in (Feldman and Rosenfeld 2006). 4 Entity Validation and Correction In this paper we describe three different methods of validation and correction of relation arguments in the extracted instances. Two of them are “classical” and are based, respectively, on the knowledgeengineering, and on the statistical supervised approaches to the named entity recognition problems. The third is our novel approach, based on redundancy and corpus statistics. The methods are implemented as components for SRES, called Entity Validators, inserted between the Instance Extractor and the Classifier. The result of applying Entity Validator to a ca</context>
</contexts>
<marker>Feldman, Rosenfeld, 2006</marker>
<rawString>Feldman, R. and B. Rosenfeld (2006). Boosting Unsupervised Relation Extraction by Using NER. EMNLP-06, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Feldman</author>
<author>B Rosenfeld</author>
</authors>
<title>Self-Supervised Relation Extraction from the Web. ISMIS-2006,</title>
<date>2006</date>
<location>Bari, Italy.</location>
<contexts>
<context position="4884" citStr="Feldman and Rosenfeld 2006" startWordPosition="736" endWordPosition="739">ready NER components (or ready labeled training sets) exist for them. Also, some Web RE systems (e.g., KnowItAll (Etzioni, Cafarella et al. 2005)) do not use separate NER components even for known entity types, because such components are usually domain-specific and may perform poorly on cross-domain text collections extracted from the Web. In such cases, the values for relation attributes must be extracted by generic methods – shallow parsing (extracting noun phrases), or even simple substring extraction. Such methods are naturally much less precise and produce many entityrecognition errors (Feldman and Rosenfeld 2006). In this paper we propose several methods of using corpus statistics to improve Web RE precision by validating and correcting the entities extracted by generic methods. The task of Web Extraction is particularly suited for the corpus statistics-based methods because of very large size of the corpora involved, and because the system is not required to identify individual mentions of the relations. Our methods of entity validation and correction are based on the following two observations: First, the entities that appear in target relations will often also appear in many other contexts, some of</context>
<context position="6995" citStr="Feldman and Rosenfeld 2006" startWordPosition="1069" endWordPosition="1072">of the Acquisition relation. Another observation that we can use is the fact that the entities, in which we are interested, usually have sufficient frequency in the corpus for statistical term extraction methods to perform reasonably well. These methods may often correct a wrongly placed entity boundary, which is a common mistake of general-purpose shallow parsers. In this paper we show how to use these observations to supplement a Web RE system with an entity validation and correction component, which is able to significantly improve the system’s accuracy. We evaluate the methods using SRES (Feldman and Rosenfeld 2006) – a Web RE system, designed to extend and improve KnowItAll (Etzioni, Cafarella et al. 2005). The contributions of this paper are as follows: • We show how to automatically generate the validating patterns for the target relation arguments, and how to integrate the results produced by the validating patterns into the whole relation extraction system. • We show how to use corpus statistics and term extraction methods to correct the boundaries of relation arguments. • We experimentally compare the improvement produced by the corpus-based entity validation and correction methods with the improve</context>
<context position="12461" citStr="Feldman and Rosenfeld 2006" startWordPosition="1934" endWordPosition="1937">re is no need for the and a set of seeds. Then it finds the occurences of user to supply the relations definitions. seeds inside a large (unlabeled) text corpus, ana3 Description of SRES lyzes their contexts, and extracts common patterns The goal of SRES is extracting instances of speci- among these contexts. The details of the patterns fied relations from the Web without human super- language and the process of pattern learning are vision. Accordingly, the supervised input to the not significant for this paper, and are described system is limited to the specifications of the target fully in (Feldman and Rosenfeld 2006). 602 3.2 Instance Extractor The Instance Extractor applies the patterns generated by the Pattern Learner to the text corpus. In order to be able to match the slots of the patterns, the Instance Extractor utilizes an external shallow parser from the OpenNLP package (http://opennlp.sourceforge.net/), which is able to find all proper and common noun phrases in a sentence. These phrases are matched to the slots of the patterns. In other respects, the pattern matching and extraction process is straightforward. 3.3 Classifier The goal of the final classification stage is to filter the list of all e</context>
<context position="14060" citStr="Feldman and Rosenfeld 2006" startWordPosition="2203" endWordPosition="2206">d into a set of binary features, which are processed by a linear featurerich classifier. The classifier receives a feature vector for a candidate, and produces a confidence score between 0 and 1. The set of features is small and is not specific to any particular relation. This allows to train a model using a small amount of labeled data for one relation, and then use the model for scoring the candidates of all other relations. Since the supervised training stage needs to be run only once, it is a part of the system development, and the complete system remains unsupervised, as demonstrated in (Feldman and Rosenfeld 2006). 4 Entity Validation and Correction In this paper we describe three different methods of validation and correction of relation arguments in the extracted instances. Two of them are “classical” and are based, respectively, on the knowledgeengineering, and on the statistical supervised approaches to the named entity recognition problems. The third is our novel approach, based on redundancy and corpus statistics. The methods are implemented as components for SRES, called Entity Validators, inserted between the Instance Extractor and the Classifier. The result of applying Entity Validator to a ca</context>
</contexts>
<marker>Feldman, Rosenfeld, 2006</marker>
<rawString>Feldman, R. and B. Rosenfeld (2006). Self-Supervised Relation Extraction from the Web. ISMIS-2006, Bari, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hasegawa</author>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<title>Discovering Relations among Named Entities from Large Corpora.</title>
<date>2004</date>
<publisher>ACL</publisher>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>Hasegawa, T., S. Sekine and R. Grishman (2004). Discovering Relations among Named Entities from Large Corpora. ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Learning Surface Text Patterns for a Question Answering System.</title>
<date>2002</date>
<booktitle>40th ACL Conference.</booktitle>
<contexts>
<context position="10158" citStr="Ravichandran and Hovy 2002" startWordPosition="1568" endWordPosition="1571"> in large important (as signified by the word “ordered”, and corpora using corpus statistics. as opposed to relations like Merger, which are The IE systems most similar to SRES are based “unordered” or, in binary case, “symmetric”). on bootstrap learning: Mutual Bootstrapping The baseline SRES does not utilize a named en(Riloff and Jones 1999), the DIPRE system (Brin tity recognizer, instead using a shallow parser for 1998), and the Snowball system (Agichtein and exracting the relation slots. Thus, the only allowed Gravano 2000). Ravichandran and Hovy entity types are ProperNP, CommonNP, and (Ravichandran and Hovy 2002) also use bootstrap- AnyNP, which mean the heads of, respectively, ping, and learn simple surface patterns for extract- proper, common, and arbitrary noun phrases. In the ing binary relations from the Web. experimental section we compare the baseline Unlike these systems, SRES surface patterns al- SRES to its extensions containing additional NER low gaps that can be matched by any sequences of components. When using those components we tokens. This makes SRES patterns more general, allow further subtypes of ProperNP, and the relaand allows to recognize instances in sentences in- tion schema ab</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Ravichandran, D. and E. Hovy (2002). Learning Surface Text Patterns for a Question Answering System. 40th ACL Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-level Bootstrapping.</title>
<date>1999</date>
<contexts>
<context position="9876" citStr="Riloff and Jones 1999" startWordPosition="1525" endWordPosition="1529">ifies that Acquisition has two slots, named We also recently became aware of the work by Buyer and Acquired, which must be filled with enDowney, Broadhead and Etzioni (2007) that deals tities of type ProperNP. The order of the slots is with locating entities of arbitrary types in large important (as signified by the word “ordered”, and corpora using corpus statistics. as opposed to relations like Merger, which are The IE systems most similar to SRES are based “unordered” or, in binary case, “symmetric”). on bootstrap learning: Mutual Bootstrapping The baseline SRES does not utilize a named en(Riloff and Jones 1999), the DIPRE system (Brin tity recognizer, instead using a shallow parser for 1998), and the Snowball system (Agichtein and exracting the relation slots. Thus, the only allowed Gravano 2000). Ravichandran and Hovy entity types are ProperNP, CommonNP, and (Ravichandran and Hovy 2002) also use bootstrap- AnyNP, which mean the heads of, respectively, ping, and learn simple surface patterns for extract- proper, common, and arbitrary noun phrases. In the ing binary relations from the Web. experimental section we compare the baseline Unlike these systems, SRES surface patterns al- SRES to its extensi</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Riloff, E. and R. Jones (1999). Learning Dictionaries for Information Extraction by Multi-level Bootstrapping. AAAI-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Rosenfeld</author>
<author>M Fresko</author>
<author>R Feldman</author>
</authors>
<title>A Systematic Comparison of Feature-Rich Probabilistic Classifiers for NER Tasks.</title>
<date>2005</date>
<publisher>PKDD.</publisher>
<marker>Rosenfeld, Fresko, Feldman, 2005</marker>
<rawString>Rosenfeld, B., M. Fresko and R. Feldman (2005). A Systematic Comparison of Feature-Rich Probabilistic Classifiers for NER Tasks. PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-Y Su</author>
<author>M-W Wu</author>
<author>J-S Chang</author>
</authors>
<title>A Corpus-based Approach to Automatic Compound Extraction. Meeting of the Association for Computational Linguistics:</title>
<date>1994</date>
<pages>242--247</pages>
<marker>Su, Wu, Chang, 1994</marker>
<rawString>Su, K.-Y., M.-W. Wu and J.-S. Chang (1994). A Corpus-based Approach to Automatic Compound Extraction. Meeting of the Association for Computational Linguistics: 242-247.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>