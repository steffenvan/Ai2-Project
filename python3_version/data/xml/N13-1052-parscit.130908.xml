<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.917583">
Approximate PCFG Parsing Using Tensor Decomposition
</title>
<author confidence="0.990339">
Shay B. Cohen Giorgio Satta
</author>
<affiliation confidence="0.995528">
Department of Computer Science Department of Information Engineering
Columbia University, USA University of Padua, Italy
</affiliation>
<email confidence="0.990348">
scohen@cs.columbia.edu satta@dei.unipd.it
</email>
<author confidence="0.98345">
Michael Collins
</author>
<affiliation confidence="0.9715165">
Department of Computer Science
Columbia University, USA
</affiliation>
<email confidence="0.997431">
mcollins@cs.columbia.edu
</email>
<sectionHeader confidence="0.994781" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999722857142857">
We provide an approximation algorithm for
PCFG parsing, which asymptotically im-
proves time complexity with respect to the in-
put grammar size, and prove upper bounds on
the approximation quality. We test our algo-
rithm on two treebanks, and get significant im-
provements in parsing speed.
</bodyText>
<sectionHeader confidence="0.998425" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957026315789">
The problem of speeding-up parsing algorithms
based on probabilistic context-free grammars
(PCFGs) has received considerable attention in
recent years. Several strategies have been proposed,
including beam-search, best-first and A*. In this
paper we focus on the standard approach of approx-
imating the source PCFG in such a way that parsing
accuracy is traded for efficiency.
Nederhof (2000) gives a thorough presentation
of old and novel ideas for approximating non-
probabilistic CFGs by means of finite automata,
on the basis of specialized preprocessing of self-
embedding structures. In the probabilistic domain,
approximation by means of regular grammars is also
exploited by Eisner and Smith (2005), who filter
long-distance dependencies on-the-fly.
Beyond finite automata approximation, Charniak
et al. (2006) propose a coarse-to-fine approach in
which an approximated (not necessarily regular)
PCFG is used to construct a parse forest for the in-
put sentence. Some statistical parameters are then
computed on such a structure, and exploited to filter
parsing with the non-approximated grammar. The
approach can also be iterated at several levels. In
the non-probabilistic setting, a similar filtering ap-
proach was also proposed by Boullier (2003), called
“guided parsing.”
In this paper we rely on an algebraic formulation
of the inside-outside algorithm for PCFGs, based on
a tensor formulation developed for latent-variable
PCFGs in Cohen et al. (2012). We combine the
method with known techniques for tensor decompo-
sition to approximate the source PCFG, and develop
a novel algorithm for approximate PCFG parsing.
We obtain improved time upper bounds with respect
to the input grammar size for PCFG parsing, and
provide error upper bounds on the PCFG approxi-
mation, in contrast with existing heuristic methods.
</bodyText>
<sectionHeader confidence="0.991792" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.9985946">
This section introduces the special representation for
probabilistic context-free grammars that we adopt in
this paper, along with the decoding algorithm that
we investigate. For an integer i &gt; 1, we let [i] =
{1,2,...,i}.
</bodyText>
<subsectionHeader confidence="0.984137">
2.1 Probabilistic Context-Free Grammars
</subsectionHeader>
<bodyText confidence="0.967515333333333">
We consider context-free grammars (CFGs) in
Chomsky normal form, and denote them as
(N, G, R) where:
</bodyText>
<listItem confidence="0.9975065">
• N is the finite set of nonterminal symbols, with
m = |N|, and G is the finite set of words (lexi-
cal tokens), with G n N = 0 and with n = |G|.
• R is a set of rules having the form a -4 b c,
a, b, c C N, or the form a -4 x, a C N and
x C G.
</listItem>
<bodyText confidence="0.810769">
A probabilistic CFG (PCFG) is a CFG associated
with a set of parameters defined as follows:
</bodyText>
<listItem confidence="0.9112265">
• For each (a -4 b c) C R, we have a parameter
p(a -4 b c  |a).
</listItem>
<page confidence="0.970883">
487
</page>
<note confidence="0.679481">
Proceedings of NAACL-HLT 2013, pages 487–496,
</note>
<address confidence="0.343826">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</address>
<listItem confidence="0.9205532">
• For each (a —4 x) E R, we have a parameter
p(a —4 x  |a).
• For each a E N, we have a parameter πa,
which is the probability of a being the root
symbol of a derivation.
</listItem>
<bodyText confidence="0.954342">
The parameters above satisfy the following nor-
malization conditions:
</bodyText>
<equation confidence="0.980999666666667">
E E
p(a —4 b c  |a) +
(a—b c)ER (a—x)ER
</equation>
<bodyText confidence="0.966355166666667">
for each a E N, and EaEN πa = 1.
The probability of a tree τ deriving a sentence in
the language, written p(τ), is calculated as the prod-
uct of the probabilities of all rule occurrences in τ,
times the parameter πa where a is the symbol at the
root of τ.
</bodyText>
<subsectionHeader confidence="0.991071">
2.2 Tensor Form of PCFGs
</subsectionHeader>
<bodyText confidence="0.946265">
A three-dimensional tensor C E R(mxmxm) is a
set of m3 parameters Ci,j,k for i, j, k E [m]. In what
follows, we associate with each tensor three func-
tions, each mapping a pair of vectors in Rm into a
vector in Rm.
Definition 1 Let C E R(mxmxm) be a tensor.
Given two vectors y1, y2 E Rm, we let C(y1, y2)
be the m-dimensional row vector with components:
tensor D E Rmxmxm where Di,j,k = xiyjzk (this
is analogous to the outer product: [xyT]i,j = xiyj).
We extend the parameter set of our PCFG such
that p(a —4 b c  |a) = 0 for all a —4 b c not in R,
and p(a —4 x  |a) = 0 for all a —4 x not in R. We
also represent each a E N by a unique index in [m],
and we represent each x E L by a unique index in
[n]: it will always be clear from the context whether
these indices refer to a nonterminal in N or else to a
word in L.
In this paper we assume a tensor representation
for the parameters p(a —4 b c  |a), and we denote by
T E Rmxmxm a tensor such that:
Ta,b,c g p(a —4 b c  |a).
Similarly, we denote by Q E Rmxn a matrix such
that:
Qa,x Ap(a —4 x  |a).
The root probabilities are denoted using a vector π E
Rmx1 such that πa is defined as before.
</bodyText>
<subsectionHeader confidence="0.999621">
2.3 Minimum Bayes-Risk Decoding
</subsectionHeader>
<bodyText confidence="0.9999042">
Let z = x1 • • • xN be some input sentence; we write
T (z) to denote the set of all possible trees for z. It
is often the case that parsing aims to find the high-
est scoring tree τ* for z according to the underlying
PCFG, also called the “Viterbi parse:”
</bodyText>
<equation confidence="0.966003928571429">
p(a —4 x  |a) = 1,
E 1 2 τ* = argmax p(τ)
[C(y1, y2)]i = Ci,j,kyj yk . τET (z)
jE[m],kE[m]
We also let C(1,2)(y1, y2) be the m-dimensional col-
umn vector with components:
[C(1,2)(y1, y2)]k = E Ci,j,kyi yj .
1 2
iE[m],jE[m]
Finally, we let C(1,3)(y1, y2) be the m-dimensional
column vector with components:
[C(1,3)(y1,y2)]j = E Ci,j,kyi yk .
1 2
iE[m],kE[m]
</equation>
<bodyText confidence="0.999638307692308">
For two vectors x, y E Rm we denote by x O y E
Rm the Hadamard product of x and y, i.e., [xOy]i =
xiyi. Finally, for vectors x, y, z E Rm, xyTzT is the
Goodman (1996) noted that Viterbi parsers do not
optimize the same metric that is usually used for
parsing evaluation (Black et al., 1991). He sug-
gested an alternative algorithm, which he called the
“Labelled Recall Algorithm,” which aims to fix this
issue.
Goodman’s algorithm has two phases. In the first
phase it computes, for each a E N and for each sub-
string xi • • • xj of z, the marginal µ(a, i, j) defined
as:
</bodyText>
<equation confidence="0.9945115">
µ(a,i,j) = E p(τ).
τET (z): (a,i,j)Eτ
</equation>
<bodyText confidence="0.978204">
Here we write (a, i, j) E τ if nonterminal a spans
words xi • • • xj in the parse tree τ.
</bodyText>
<page confidence="0.985988">
488
</page>
<figure confidence="0.5144">
Inputs: Sentence x1 • • • xN, PCFG (N, G, R), pa-
rameters T E R(mxmxm), Q E R(mxn), πE
R(mx1).
Data structures:
</figure>
<listItem confidence="0.988957">
• Each µ(a, i, j) E R for a E N, i, j E [N],
i &lt; j, is a marginal probability.
• Each γi,j E R for i, j E [N], i &lt; j, is the high-
est score for a tree spanning substring xi • • • xj.
</listItem>
<figure confidence="0.870798111111111">
Algorithm:
(Marginals) Va E N, Vi, j E [N], i &lt; j, compute
the marginals µ(a, i, j) using the inside-outside
algorithm.
(Base case) Vi E [N],
γi,i = max µ(a, i, i)
(a-xz)ER
(Maximize Labelled Recall) Vi, j E [N], i &lt; j,
γi,j = a N µ(a, i,j) + i ax (γi,k + γk+1,j1
</figure>
<figureCaption confidence="0.996043833333333">
Figure 1: The labelled recall algorithm from Goodman
(1996). The algorithm in this figure finds the highest
score for a tree which maximizes labelled recall. The ac-
tual parsing algorithm would use backtrack pointers in
the score computation to return a tree. These are omitted
for simplicity.
</figureCaption>
<bodyText confidence="0.999920666666667">
The second phase includes a dynamic program-
ming algorithm which finds the tree τ* that maxi-
mizes the sum over marginals in that tree:
</bodyText>
<equation confidence="0.9993635">
Eτ* = argmax µ(a, i, j).
τET (z) (a,i,j)Eτ
</equation>
<bodyText confidence="0.987427909090909">
Goodman’s algorithm is described in Figure 1.
As Goodman notes, the complexity of the second
phase (“Maximize Labelled Recall,” which is also
referred to as “minimum Bayes risk decoding”) is
O(N3 + mN2). There are two nested outer loops,
each of order N, and inside these, there are two sep-
arate loops, one of order m and one of order N,
yielding this computational complexity. The reason
for the linear dependence on the number of nonter-
minals is the lack of dependence on the actual gram-
mar rules, once the marginals are computed.
In its original form, Goodman’s algorithm does
not enforce that the output parse trees are included in
the tree language of the PCFG, that is, certain com-
binations of children and parent nonterminals may
violate the rules in the grammar. In our experiments
we departed from this, and changed Goodman’s al-
gorithm by incorporating the grammar into the dy-
namic programming algorithm in Figure 1. The rea-
son this is important for our experiments is that we
binarize the grammar prior to parsing, and we need
to enforce the links between the split nonterminals
(in the binarized grammar) that refer to the same
syntactic category. See Matsuzaki et al. (2005) for
more details about the binarization scheme we used.
This step changes the dynamic programming equa-
tion of Goodman to be linear in the size of the gram-
mar (figure 1). However, empirically, it is the inside-
outside algorithm which takes most of the time to
compute with Goodman’s algorithm. In this paper
we aim to asymptotically reduce the time complex-
ity of the calculation of the inside-outside probabili-
ties using an approximation algorithm.
</bodyText>
<sectionHeader confidence="0.947284" genericHeader="method">
3 Tensor Formulation of the
Inside-Outside Algorithm
</sectionHeader>
<bodyText confidence="0.999564411764706">
At the core of our approach lies the observation that
there is a (multi)linear algebraic formulation of the
inside-outside algorithm. It can be represented as a
series of tensor, matrix and vector products. A sim-
ilar observation has been made for latent-variable
PCFGs (Cohen et al., 2012) and hidden Markov
models, where only matrix multiplication is required
(Jaeger, 2000). Cohen and Collins (2012) use this
observation together with tensor decomposition to
improve the speed of latent-variable PCFG parsing.
The representation of the inside-outside algorithm
in tensor form is given in Figure 2. For example,
if we consider the recursive equation for the inside
probabilities (where αi,j is a vector varying over the
nonterminals in the grammar, describing the inside
probability for each nonterminal spanning words i
to j):
</bodyText>
<equation confidence="0.931634">
j-1
αi,j = T(αi,k, αk+1,j)
k=i
</equation>
<page confidence="0.996138">
489
</page>
<figureCaption confidence="0.9924385">
Figure 2: The tensor form of the inside-outside algorithm,
for calculation of marginal terms µ(a, i, j).
</figureCaption>
<bodyText confidence="0.914878333333333">
and then apply the tensor product from Definition 1
to this equation, we get that coordinate a in αi,j is
defined recursively as follows:
</bodyText>
<figure confidence="0.657042">
Ta,b,c × αi,k
b × αk+1,j c
p(a → b c  |a) × αi,k
b × αk+1,j
</figure>
<bodyText confidence="0.919981545454545">
c ,
which is exactly the recursive definition of the inside
algorithm. The correctness of the outside recursive
equations follows very similarly.
The time complexity of the algorithm in this case
is O(m3N3). To see this, observe that each tensor
application takes time O(m3). Furthermore, the ten-
sor T is applied O(N) times in the computation of
each vector αi,j and βi,j. Finally, we need to com-
pute a total of O(N2) inside and outside vectors, one
for each substring of the input sentence.
</bodyText>
<sectionHeader confidence="0.994295" genericHeader="method">
4 Tensor Decomposition for the
</sectionHeader>
<subsectionHeader confidence="0.688496">
Inside-Outside Algorithm
</subsectionHeader>
<bodyText confidence="0.9998755">
In this section, we detail our approach to approxi-
mate parsing using tensor decomposition.
</bodyText>
<subsectionHeader confidence="0.993437">
4.1 Tensor Decomposition
</subsectionHeader>
<bodyText confidence="0.972830833333334">
In the formulation of the inside-outside algorithm
based on tensor T, each vector αi,j and βi,j consists
of m elements, where computation of each element
requires time O(m2). Therefore, the algorithm has a
O(m3) multiplicative factor in its time complexity,
which we aim to reduce by means of an approximate
algorithm.
Our approximate method relies on a simple ob-
servation. Given an integer r ≥ 1, assume that
the tensor T has the following special form, called
“Kruskal form:”
λiuivi wi . (1)
In words, T is the sum of r tensors, where each
tensor is obtained as the product of three vectors
ui, vi and wi, together with a scalar λi. Exact
Kruskal decomposition of a tensor is not necessarily
unique. See Kolda and Bader (2009) for discussion
of uniqueness of tensor decomposition.
</bodyText>
<figure confidence="0.983572222222222">
Inputs: Sentence x1 · · · xN, PCFG (N, L, R), pa-
rameters T ∈ R(mxmxm), Q ∈ R(mxn), π∈
R(mx1).
Data structures:
• Each αi,j ∈ R1xm, i, j ∈ [N], i ≤ j, is a row
vector of inside terms ranging over a ∈ N.
• Each βi,j ∈ Rmx1, i, j ∈ [N], i ≤ j, is a
column vector of outside terms ranging over
a ∈ N.
• Each µ(a, i, j) ∈ R for a ∈ N, i, j ∈ [N],
i ≤ j, is a marginal probability.
Algorithm:
(Inside base case) ∀i ∈ [N],∀(a → xi) ∈ R,
[αi,i]a = Qa,x
(Inside recursion) ∀i, j ∈ [N], i &lt; j,
αi,j = j−1 X T(αi,k, αk+1,j)
k=i
(Outside base case) ∀a ∈ N,
</figure>
<equation confidence="0.97159119047619">
[β1,N]a = πa
(Outside recursion) ∀i, j ∈ [N], i ≤ j,
βi,j = Xi− 1 T(1,2)(βk,j, αk,i−1)+
k=1
N
X T(1,3)(βi,k, αj+1,k)
k=j+1
(Marginals) ∀a ∈ N, ∀i, j ∈ [N], i ≤ j,
µ(a, i, j) = [αi,j]a · [βi,j]a
X
b,c
[αi,j]a =
j−1 X
k=i
X
b,c
j−1 X
k=i
Xr
i=1
T=
</equation>
<page confidence="0.935376">
490
</page>
<bodyText confidence="0.961726888888889">
Consider now two vectors y1, y2 ∈ Rm, associ-
ated with the inside probabilities for the left (y1) and
right child (y2) of a given node in a parse tree. Let
us introduce auxiliary arrays U, V, W ∈ Rrxm, with
the i-th row being ui, vi and wi, respectively. Let
also λ = (λ1, ... , λr). Using the decomposition in
Eq. (1) within Definition 1 we can express the array
T (y1, y2) as:
Kruskal form Cr be:
</bodyText>
<equation confidence="0.993554555555555">
Cr ={C ∈ Rmxmxm  |C =
s.t. λi ∈ R, ui, vi, wi ∈ Rm,
||ui||2 = ||vi||2 = ||wi||2 = 1}.
The least squares CPD of C is a tensor C� such
r
i=1
λiuiv wT
Z Ti
that C ∈ argmin ˆCEC, ||C − �C||F. Here, we treat
� �r �
T (y1, y2) = λiuivT i wT (y1, y2) =
i
i=1
r
� λiui(vTi y1)(wTi y2) =
i=1
� �
UT(λ � V y1 � Wy2) . (2)
</equation>
<bodyText confidence="0.999740571428571">
The total complexity of the computation in Eq. (2)
is now O(rm). It is well-known that an exact tensor
decomposition for T can be achieved with r = m2
(Kruskal, 1989). In this case, there is no computa-
tional gain in using Eq. (2) for the inside calculation.
The minimal r required for an exact tensor decom-
position can be smaller than m2. However, identify-
ing that minimal r is NP-hard (Høastad, 1990).
In this section we focused on the computa-
tion of the inside probabilities through vectors
T(αi,k, αk+1,j). Nonetheless, the steps above can
be easily adapted for the computation of the outside
probabilities through vectors T(1,2)(βk,j,αk,i−1)
and T(1,3)(βi,k, αj+1,k).
</bodyText>
<subsectionHeader confidence="0.996596">
4.2 Approximate Tensor Decomposition
</subsectionHeader>
<bodyText confidence="0.981779629629629">
The PCFG tensor T will not necessarily have the ex-
act decomposed form in Eq. (1). We suggest to ap-
proximate the tensor T by finding the closest tensor
according to some norm over Rmxmxm.
An example of such an approximate decom-
position is the canonical polyadic decomposition
(CPD), also known as CANDECOMP/PARAFAC
decomposition (Carroll and Chang, 1970; Harsh-
man, 1970; Kolda and Bader, 2009). Given an in-
teger r, least squares CPD aims to find the nearest
tensor in Kruskal form, minimizing squared error.
More formally, for a given tensor D ∈ Rmxmxm,
��
let ||D||F = i,j,k D2i. Let the set of tensors in
,j,
the argmin as a set because there could be multiple
solutions which achieve the same accuracy.
There are various algorithms to perform CPD,
such as alternating least squares, direct linear de-
composition, alternating trilinear decomposition and
pseudo alternating least squares (Faber et al., 2003)
and even algorithms designed for sparse tensors (Chi
and Kolda, 2011). Most of these algorithms treat
the problem of identifying the approximate tensor as
an optimization problem. Generally speaking, these
optimization problems are hard to solve, but they
work quite well in practice.
</bodyText>
<subsectionHeader confidence="0.999955">
4.3 Parsing with Decomposed Tensors
</subsectionHeader>
<bodyText confidence="0.99998644">
Equipped with the notion of tensor decomposition,
we can now proceed with approximate tensor pars-
ing in two steps. The first is approximating the ten-
sor using a CPD algorithm, and the second is apply-
ing the algorithms in Figure 1 and Figure 2 to do
parsing, while substituting all tensor product com-
putations with the approximate O(rm) operation of
tensor product.
This is not sufficient to get a significant speed-up
in parsing time. Re-visiting Eq. (2) shows that there
are additional ways to speed-up the tensor applica-
tion T in the context of the inside-outside algorithm.
The first thing to note is that the projections V y1
and Wy2 in Eq. (2) can be cached, and do not have
to be re-calculated every time the tensor is applied.
Here, y1 and y2 will always refer to an outside or
an inside probability vector over the nonterminals in
the grammar. Caching these projections means that
after each computation of an inside or outside proba-
bility, we can immediately project it to the necessary
r-dimensional space, and then re-use this computa-
tion in subsequent application of the tensor.
The second thing to note is that the U projection
in T can be delayed, because of rule of distributiv-
ity. For example, the step in Figure 2 that computes
</bodyText>
<page confidence="0.99769">
491
</page>
<bodyText confidence="0.93997425">
the inside probability αi,j can be re-formulated as
follows (assuming an exact decomposition of T):
T(αi,k, αk+1,j)
and define F(a, N) = D(a, N)/Z(a, N). De-
fine Δ =  ||Tˆ − T ||F. Last, define ν =
min(a,bc)ERp(a → b c  |a). Then, the following
lemma holds:
Lemma 1 For any a and any N, it holds:
</bodyText>
<equation confidence="0.992488666666667">
j−1 �
k=i
αi,j =
j−1 � UT(λ O V αi,k O Wαk+1,j) D(a, N) ≤ Z(a, N) ((1 + Δ/ν)N − 1)
k=1
�(λ O V αi,k O Wαk+1,j) . (3)
</equation>
<bodyText confidence="0.9998697">
This means that projection through U can be done
outside of the loop over splitting points in the sen-
tence. Similar reliance on distributivity can be used
to speed-up the outside calculations as well.
The caching speed-up and the delayed projection
speed-up make the approximate inside-outside com-
putation asymptotically faster. While naive applica-
tion of the tensor yields an inside algorithm which
runs in time O(rmN3), the improved algorithm
runs in time O(rN3 + rmN2).
</bodyText>
<sectionHeader confidence="0.706483" genericHeader="method">
5 Quality of Approximate Tensor Parsing
</sectionHeader>
<bodyText confidence="0.968520566666667">
In this section, we give the main approximation re-
sult, that shows that the probability distribution in-
duced by the approximate tensor is close to the orig-
inal probability distribution, if the distance between
the approximate tensor and the rule probabilities is
not too large.
Denote by T (N) the set of trees in the tree lan-
guage of the PCFG with N words (any nontermi-
nal can be the root of the tree). Let T (N) be the
set of pairs of trees τ = (τ1, τ2) such that the to-
tal number of binary rules combined in τ1 and τ2 is
N − 2 (this means that the total number of words
combined is N). Let Tˆ be the approximate ten-
sor for T. Denote the probability distribution in-
duced by Tˆ by ˆp.1 Define the vector ξ(τ) such that
[ξ(τ)]a = Ta,b,c · p(τ1  |b) · p(τ2  |c) where the root
τ1 is nonterminal b and the root of τ2 is c. Similarly,
define [ˆξ(τ)]a = ˆTa,b,c · ˆp(τ1  |b) · ˆp(τ2  |c).
define D(a, N) = EτET (N) |[ˆξ(τ)]a − [ξ(τ)]a|
Define Z(a, N) = EτET (N)[ˆξ(τ)]a. In addition,
1Here, p does not have to be a distribution, because T could
have negative values, in principle, and its slices do not have to
normalize to 1. However, we just treat p as a function that maps
trees to products of values according to T.
Proof sketch: The proof is by induction on N.
Assuming that 1 + F(b, k) ≤ (1 + Δ/ν)k and
1 + F(c, N − k − 1) ≤ (1 + Δ/ν)N−k−1 for F
defined as above (this is the induction hypothesis), it
can be shown that the lemma holds. ■
Lemma 2 The following holds for any N:
</bodyText>
<equation confidence="0.871780166666667">
E |ˆp(τ) − p(τ) |≤ m ((1 + Δ/ν)N − 1)
τET (N)
Proof sketch: Using H¨older’s inequality and
Lemma 1 and the fact that Z(a, N) ≤ 1, it follows
that:
E |ˆp(τ) − p(τ) |≤ � |[ξ(τ)]a − [ˆξ(τ)]a|
τET (N) τET (N),a
≤ (I: �
a ((1 + Δ/ν)N − 1)
Z(a, N)
≤ m ((1 + Δ/ν)N − 1)
■
</equation>
<bodyText confidence="0.9659788">
Then, the following is a result that explains how
accuracy changes as a function of the quality of the
tensor approximation:
Theorem 1 For any N, and c &lt; 1/4, it holds that if
Eν
</bodyText>
<equation confidence="0.842907375">
Δ ≤ 2Nm, then:
L: |ˆp(τ) − p(τ) |≤ c
τET (N)
Proof sketch: This is the result of applying Lemma 2
together with the inequality (1 + y/t)t − 1 ≤ 2y for
any t &gt; 0 and y ≤ 1/2. ■
= UT j−1 �
k=1
</equation>
<page confidence="0.988683">
492
</page>
<bodyText confidence="0.999923714285714">
We note that Theorem 1 also implicitly bounds
the difference between a marginal µ(a, i, j) and its
approximate version. A marginal corresponds to a
sum over a subset of summands in Eq. (1).
A question that remains at this point is to decide
whether for a given grammar, the optimal ν that can
be achieved is large or small. We define:
</bodyText>
<equation confidence="0.974816">
||T − T�||F (4)
</equation>
<bodyText confidence="0.999660166666667">
The following theorem gives an upper bound on
the value of A∗ r based on intrinsic property of the
grammar, or more specifically T. It relies on the
fact that for three-dimensional tensors, where each
dimension is of length m, there exists an exact de-
composition of T using m2 components.
</bodyText>
<equation confidence="0.98360975">
Theorem 2 Let:
λ∗i u∗i (v∗i )&gt;(w∗i )&gt;
be an exact Kruskal decomposition of T such that
||u∗i||2 = ||v∗i ||2 = ||w∗i  ||= 1 and λ∗ i&gt; λ∗i+1 for
i E [m2 − 1]. Then, for a given r, it holds:
m2
A∗r &lt; X |λ∗i |
i=r+1
</equation>
<bodyText confidence="0.992486142857143">
Proof: Let T� be a tensor that achieves the minimum
in Eq. (4). Define:
λ∗i u∗i (v∗i )&gt;(w∗i )&gt;
Then, noting that A∗r is a minimizer of the norm
difference between T and T� and then applying the
triangle inequality and then Cauchy-Schwartz in-
equality leads to the following chain of inequalities:
</bodyText>
<equation confidence="0.998546857142857">
A∗ r = ||T − T�||F &lt; ||T −T0r||F
m2
=  ||X λ∗i u∗i (v∗i )&gt;(w∗i )&gt;||F
i=r+1
m2
&lt; X |λ∗i  |- ||u∗i (v∗i )&gt;(w∗i )&gt;||F =
i=r+1
</equation>
<bodyText confidence="0.919259">
as required. ■
</bodyText>
<sectionHeader confidence="0.996469" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999963731707317">
In this section, we describe experiments that demon-
strate the trade-off between the accuracy of the ten-
sor approximation (and as a consequence, the accu-
racy of the approximate parsing algorithm) and pars-
ing time.
Experimental Setting We compare the tensor ap-
proximation parsing algorithm versus the vanilla
Goodman algorithm. Both algorithms were imple-
mented in Java, and the code for both is almost iden-
tical, except for the set of instructions which com-
putes the dynamic programming equation for prop-
agating the beliefs up in the tree. This makes the
clocktime comparison reliable for drawing conclu-
sions about the speed of the algorithms. Our im-
plementation of the vanilla parsing algorithm is lin-
ear in the size of the grammar (and not cubic in the
number of nonterminals, which would give a worse
running time).
In our experiments, we use the method described
in Chi and Kolda (2011) for tensor decomposition.2
This method is fast, even for large tensors, as long
as they are sparse. Such is the case with the tensors
for our grammars.
We use two treebanks for our comparison: the
Penn treebank (Marcus et al., 1993) and the Arabic
treebank (Maamouri et al., 2004). With the Penn
treebank, we use sections 2–21 for training a max-
imum likelihood model and section 22 for parsing,
while for the Arabic treebank we divide the data into
two sets, of size 80% and 20%, one is used for train-
ing a maximum likelihood model and the other is
used for parsing.
The number of binary rules in the treebank gram-
mar is 7,240. The number of nonterminals is 112
and the number of preterminals is 2593Unary rules
are removed by collapsing non-terminal chains. This
increased the number of preterminals. The number
of binary rules in the Arabic treebank is significantly
smaller and consists of 232 rules. We run all parsing
experiments on sentences of length &lt; 40. The num-
ber of nonterminals is 48 and the number of preter-
</bodyText>
<footnote confidence="0.9355094">
2We use the implementation given in Sandia’s Mat-
lab Tensor Toolbox, which can be downloaded at http:
//www.sandia.gov/˜tgkolda/TensorToolbox/
index-2.5.html.
3.
</footnote>
<equation confidence="0.854425583333333">
A∗r = min
T�∈Cr
m2
X
i=1
T=
T0
r
Xr
i=1
X m2 |λ∗i |
i=r+1
</equation>
<page confidence="0.997823">
493
</page>
<table confidence="0.9993582">
rank (r) baseline 20 60 100 140 180 220 260 300 340
speed 0.57 0.04 0.06 0.1 0.12 0.16 0.19 0.22 0.26 0.28
F1 63.78 51.80 58.39 63.63 63.77 63.88 63.82 63.84 63.80 63.88
speed 3.89 0.15 0.21 0.30 0.37 0.44 0.52 0.60 0.70 0.79
F1 71.07 57.83 61.67 68.28 69.63 70.30 70.82 71.42 71.28 71.13
</table>
<tableCaption confidence="0.987282">
Table 1: Results for the Arabic and English treebank of parsing using a vanilla PCFG with and without tensor decom-
position. Speed is given in seconds per sentence.
</tableCaption>
<bodyText confidence="0.986118424242424">
minals is 81.
Results Table 1 describes the results of compar-
ing the tensor decomposition algorithm to the vanilla
PCFG parsing algorithm.
The first thing to note is that the running time of
the parsing algorithm is linear in r. This indeed
validates the asymptotic complexity of the inside-
outside component in Goodman’s algorithm with the
approximate tensors. It also shows that most of the
time during parsing is spent on the inside-outside al-
gorithm, and not on the dynamic programming algo-
rithm which follows it.
In addition, compared to the baseline which uses
a vanilla CKY algorithm (linear in the number of
rules), we get a speed up of a factor of 4.75 for
Arabic (r = 140) and 6.5 for English (r = 260)
while retaining similar performance. Perhaps more
surprising is that using the tensor approximation ac-
tually improves performance in several cases. We
hypothesize that the cause of this is that the tensor
decomposition requires less parameters to express
the rule probabilities in the grammar, and therefore
leads to better generalization than a vanilla maxi-
mum likelihood estimate.
We include results for a more complex model for
Arabic, which uses horizontal Markovization of or-
der 1 and vertical Markovization of order 2 (Klein
and Manning, 2003). This grammar includes 2,188
binary rules. Parsing exhaustively using this gram-
mar takes 1.30 seconds per sentence (on average)
with an F1 measure of 64.43. Parsing with tensor
decomposition for r = 280 takes 0.62 seconds per
sentence (on average) with an F1 measure of 64.05.
</bodyText>
<sectionHeader confidence="0.99961" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.997887">
In this section, we briefly touch on several other top-
ics related to tensor approximation.
</bodyText>
<subsectionHeader confidence="0.990808">
7.1 Approximating the Probability of a String
</subsectionHeader>
<bodyText confidence="0.879039272727273">
The probability of a sentence z under a PCFG is de-
fined as p(z) = Eτ∈T (z) p(-r), and can be approx-
imated using the algorithm in Section 4.3, running
in time O(rN3 + rmN2). Of theoretical interest,
we discuss here a time O(rN3 + r2N2) algorithm,
which is more convenient when r &lt; m.
Observe that in Eq. (3) vector αi,j always appears
within one of the two terms V αi,j and W αi,j in
Rr×1, whose dimensions are independent of m.
We can therefore use Eq. (3) to compute Vαi,j as
1
V αi,j = V U&gt; (Ej−1
k�1(� � V αi,k � W αk+1,j)
,
where V U&gt; is a Rr×r matrix that can be
computed off-line, i.e., independently of
z. A symmetrical relation can be used
to compute W αi,j. Finally, we can write
p(z) = 7r&gt;U (EN− 11(A O V α1,k O Wαk+1,N)1 ,
where 7r&gt;U is a R1×r vector that can again be
computed off-line. This algorithm then runs in time
O(rN3 + r2N2).
</bodyText>
<subsectionHeader confidence="0.998105">
7.2 Applications to Dynamic Programming
</subsectionHeader>
<bodyText confidence="0.9986682">
The approximation method presented in this paper is
not limited to PCFG parsing. A similar approxima-
tion method has been used for latent-variable PCFGs
(Cohen and Collins, 2012), and in general, ten-
sor approximation can be used to speed-up inside-
outside algorithms for general dynamic program-
ming algorithms or weighted logic programs (Eisner
et al., 2004; Cohen et al., 2011). In the general case,
the dimension of the tensors will not be necessarily
just three (corresponding to binary rules), but can be
of a higher dimension, and therefore the speed gain
can be even greater. In addition, tensor approxima-
tion can be used for computing marginals of latent
variables in graphical models.
For example, the complexity of the forward-
</bodyText>
<page confidence="0.997045">
494
</page>
<bodyText confidence="0.999848833333333">
backward algorithm for HMMs can be reduced to
be linear in the number of states (as opposed to
quadratic) and linear in the rank used in an approxi-
mate singular-value decomposition (instead of ten-
sor decomposition) of the transition and emission
matrices.
</bodyText>
<subsectionHeader confidence="0.991598">
7.3 Tighter (but Slower) Approximation Using
Singular Value Decomposition
</subsectionHeader>
<bodyText confidence="0.999993615384615">
The accuracy of the algorithm depends on the ability
of the tensor decomposition algorithm to decompose
the tensor with a small reconstruction error. The de-
composition algorithm is performed on the tensor T
which includes all rules in the grammar.
Instead, one can approach the approximation by
doing a decomposition for each slice of T separately
using singular value decomposition. This will lead
to a more accurate approximation, but will also lead
to an extra factor of m during parsing. This factor
is added because now there is not a single U, V and
W, but instead there are such matrices for each non-
terminal in the grammar.
</bodyText>
<sectionHeader confidence="0.998219" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999957285714286">
We described an approximation algorithm for prob-
abilistic context-free parsing. The approximation al-
gorithm is based on tensor decomposition performed
on the underlying rule table of the CFG grammar.
The approximation algorithm leads to significant
speed-up in PCFG parsing, with minimal loss in per-
formance.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999932417910448">
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of DARPA Work-
shop on Speech and Natural Language.
P. Boullier. 2003. Guided earley parsing. In 8th In-
ternational Workshop on Parsing Technologies, pages
43–54.
J. D. Carroll and J. J. Chang. 1970. Analysis of indi-
vidual differences in multidimensional scaling via an
N-way generalization of Eckart-Young decomposition.
Psychometrika, 35:283–319.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil,
D. Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore,
M. Pozar, and T. Vu. 2006. Multilevel coarse-to-fine
pcfg parsing. In Proceedings of HLT-NAACL.
E. C. Chi and T. G. Kolda. 2011. On tensors, spar-
sity, and nonnegative factorizations. arXiv:1112.2414
[math.NA], December.
S. B. Cohen and M. Collins. 2012. Tensor decomposi-
tion for fast latent-variable PCFG parsing. In Proceed-
ings of NIPS.
S. B. Cohen, R. J. Simmons, and N. A. Smith. 2011.
Products of weighted logic programs. Theory and
Practice of Logic Programming, 11(2–3):263–296.
S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft and
hard constraints on dependency length. In Proceed-
ings of IWPT, Parsing ’05.
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A
declarative language for implementing dynamic pro-
grams. In Proc. of ACL (companion volume).
N. M. Faber, R. Bro, and P. Hopke. 2003. Recent devel-
opments in CANDECOMP/PARAFAC algorithms: a
critical review. Chemometrics and Intelligent Labora-
tory Systems, 65(1):119–137.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings ofACL.
R. A. Harshman. 1970. Foundations of the PARAFAC
procedure: Models and conditions for an “explana-
tory” multi-modal factor analysis. UCLA working pa-
pers in phoentics, 16:1–84.
J. Høastad. 1990. Tensor rank is NP-complete. Algo-
rithms, 11:644–654.
H. Jaeger. 2000. Observable operator models for dis-
crete stochastic time series. Neural Computation,
12(6):1371–1398.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings ofACL.
T. G. Kolda and B. W. Bader. 2009. Tensor decomposi-
tions and applications. SIAM Rev., 51:455–500.
J. B. Kruskal. 1989. Rank, decomposition, and unique-
ness for 3-way and N-way arrays. In R. Coppi and
S. Bolasco, editors, Multiway Data Analysis, pages 7–
18.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a large-
scale annotated Arabic corpus. In Proceedings NEM-
LAR.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19(2):313–330.
</reference>
<page confidence="0.987997">
495
</page>
<reference confidence="0.9968755">
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of ACL.
M.-J. Nederhof. 2000. Practical experiments with regu-
lar approximation of context-free languages. Compu-
tational Linguistics, 26(1):17–44.
</reference>
<page confidence="0.999126">
496
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.550046">
<title confidence="0.999832">Approximate PCFG Parsing Using Tensor Decomposition</title>
<author confidence="0.99999">Shay B Cohen Giorgio Satta</author>
<affiliation confidence="0.805897">Department of Computer Science Department of Information Engineering Columbia University, USA University of Padua, Italy</affiliation>
<email confidence="0.975462">scohen@cs.columbia.edusatta@dei.unipd.it</email>
<author confidence="0.983077">Michael</author>
<affiliation confidence="0.9999875">Department of Computer Columbia University,</affiliation>
<email confidence="0.999824">mcollins@cs.columbia.edu</email>
<abstract confidence="0.993313">We provide an approximation algorithm for PCFG parsing, which asymptotically improves time complexity with respect to the input grammar size, and prove upper bounds on the approximation quality. We test our algorithm on two treebanks, and get significant improvements in parsing speed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickenger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of DARPA Workshop on Speech and Natural Language.</booktitle>
<contexts>
<context position="6009" citStr="Black et al., 1991" startWordPosition="1081" endWordPosition="1084"> = argmax p(τ) [C(y1, y2)]i = Ci,j,kyj yk . τET (z) jE[m],kE[m] We also let C(1,2)(y1, y2) be the m-dimensional column vector with components: [C(1,2)(y1, y2)]k = E Ci,j,kyi yj . 1 2 iE[m],jE[m] Finally, we let C(1,3)(y1, y2) be the m-dimensional column vector with components: [C(1,3)(y1,y2)]j = E Ci,j,kyi yk . 1 2 iE[m],kE[m] For two vectors x, y E Rm we denote by x O y E Rm the Hadamard product of x and y, i.e., [xOy]i = xiyi. Finally, for vectors x, y, z E Rm, xyTzT is the Goodman (1996) noted that Viterbi parsers do not optimize the same metric that is usually used for parsing evaluation (Black et al., 1991). He suggested an alternative algorithm, which he called the “Labelled Recall Algorithm,” which aims to fix this issue. Goodman’s algorithm has two phases. In the first phase it computes, for each a E N and for each substring xi • • • xj of z, the marginal µ(a, i, j) defined as: µ(a,i,j) = E p(τ). τET (z): (a,i,j)Eτ Here we write (a, i, j) E τ if nonterminal a spans words xi • • • xj in the parse tree τ. 488 Inputs: Sentence x1 • • • xN, PCFG (N, G, R), parameters T E R(mxmxm), Q E R(mxn), πE R(mx1). Data structures: • Each µ(a, i, j) E R for a E N, i, j E [N], i &lt; j, is a marginal probability</context>
</contexts>
<marker>Black, Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In Proceedings of DARPA Workshop on Speech and Natural Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Boullier</author>
</authors>
<title>Guided earley parsing.</title>
<date>2003</date>
<booktitle>In 8th International Workshop on Parsing Technologies,</booktitle>
<pages>43--54</pages>
<contexts>
<context position="1900" citStr="Boullier (2003)" startWordPosition="269" endWordPosition="270">s of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on an algebraic formulation of the inside-outside algorithm for PCFGs, based on a tensor formulation developed for latent-variable PCFGs in Cohen et al. (2012). We combine the method with known techniques for tensor decomposition to approximate the source PCFG, and develop a novel algorithm for approximate PCFG parsing. We obtain improved time upper bounds with respect to the input grammar size for PCFG parsing, and provide error upper bounds on the PCFG approximation, in contrast with existing heuristic methods. 2 Preliminaries This section int</context>
</contexts>
<marker>Boullier, 2003</marker>
<rawString>P. Boullier. 2003. Guided earley parsing. In 8th International Workshop on Parsing Technologies, pages 43–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Carroll</author>
<author>J J Chang</author>
</authors>
<title>Analysis of individual differences in multidimensional scaling via an N-way generalization of Eckart-Young decomposition.</title>
<date>1970</date>
<tech>Psychometrika,</tech>
<pages>35--283</pages>
<contexts>
<context position="14313" citStr="Carroll and Chang, 1970" startWordPosition="2616" endWordPosition="2619">omputation of the inside probabilities through vectors T(αi,k, αk+1,j). Nonetheless, the steps above can be easily adapted for the computation of the outside probabilities through vectors T(1,2)(βk,j,αk,i−1) and T(1,3)(βi,k, αj+1,k). 4.2 Approximate Tensor Decomposition The PCFG tensor T will not necessarily have the exact decomposed form in Eq. (1). We suggest to approximate the tensor T by finding the closest tensor according to some norm over Rmxmxm. An example of such an approximate decomposition is the canonical polyadic decomposition (CPD), also known as CANDECOMP/PARAFAC decomposition (Carroll and Chang, 1970; Harshman, 1970; Kolda and Bader, 2009). Given an integer r, least squares CPD aims to find the nearest tensor in Kruskal form, minimizing squared error. More formally, for a given tensor D ∈ Rmxmxm, �� let ||D||F = i,j,k D2i. Let the set of tensors in ,j, the argmin as a set because there could be multiple solutions which achieve the same accuracy. There are various algorithms to perform CPD, such as alternating least squares, direct linear decomposition, alternating trilinear decomposition and pseudo alternating least squares (Faber et al., 2003) and even algorithms designed for sparse tens</context>
</contexts>
<marker>Carroll, Chang, 1970</marker>
<rawString>J. D. Carroll and J. J. Chang. 1970. Analysis of individual differences in multidimensional scaling via an N-way generalization of Eckart-Young decomposition. Psychometrika, 35:283–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
<author>M Elsner</author>
<author>J Austerweil</author>
<author>D Ellis</author>
<author>I Haxton</author>
<author>C Hill</author>
<author>R Shrivaths</author>
<author>J Moore</author>
<author>M Pozar</author>
<author>T Vu</author>
</authors>
<title>Multilevel coarse-to-fine pcfg parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="1463" citStr="Charniak et al. (2006)" startWordPosition="200" endWordPosition="203">including beam-search, best-first and A*. In this paper we focus on the standard approach of approximating the source PCFG in such a way that parsing accuracy is traded for efficiency. Nederhof (2000) gives a thorough presentation of old and novel ideas for approximating nonprobabilistic CFGs by means of finite automata, on the basis of specialized preprocessing of selfembedding structures. In the probabilistic domain, approximation by means of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on an algebraic formulation of the inside-outside algorithm for PCFGs, based on a tensor formulation developed for</context>
</contexts>
<marker>Charniak, Johnson, Elsner, Austerweil, Ellis, Haxton, Hill, Shrivaths, Moore, Pozar, Vu, 2006</marker>
<rawString>E. Charniak, M. Johnson, M. Elsner, J. Austerweil, D. Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore, M. Pozar, and T. Vu. 2006. Multilevel coarse-to-fine pcfg parsing. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E C Chi</author>
<author>T G Kolda</author>
</authors>
<title>On tensors, sparsity, and nonnegative factorizations. arXiv:1112.2414 [math.NA],</title>
<date>2011</date>
<contexts>
<context position="14938" citStr="Chi and Kolda, 2011" startWordPosition="2720" endWordPosition="2723">shman, 1970; Kolda and Bader, 2009). Given an integer r, least squares CPD aims to find the nearest tensor in Kruskal form, minimizing squared error. More formally, for a given tensor D ∈ Rmxmxm, �� let ||D||F = i,j,k D2i. Let the set of tensors in ,j, the argmin as a set because there could be multiple solutions which achieve the same accuracy. There are various algorithms to perform CPD, such as alternating least squares, direct linear decomposition, alternating trilinear decomposition and pseudo alternating least squares (Faber et al., 2003) and even algorithms designed for sparse tensors (Chi and Kolda, 2011). Most of these algorithms treat the problem of identifying the approximate tensor as an optimization problem. Generally speaking, these optimization problems are hard to solve, but they work quite well in practice. 4.3 Parsing with Decomposed Tensors Equipped with the notion of tensor decomposition, we can now proceed with approximate tensor parsing in two steps. The first is approximating the tensor using a CPD algorithm, and the second is applying the algorithms in Figure 1 and Figure 2 to do parsing, while substituting all tensor product computations with the approximate O(rm) operation of</context>
<context position="21663" citStr="Chi and Kolda (2011)" startWordPosition="3991" endWordPosition="3994">n parsing algorithm versus the vanilla Goodman algorithm. Both algorithms were implemented in Java, and the code for both is almost identical, except for the set of instructions which computes the dynamic programming equation for propagating the beliefs up in the tree. This makes the clocktime comparison reliable for drawing conclusions about the speed of the algorithms. Our implementation of the vanilla parsing algorithm is linear in the size of the grammar (and not cubic in the number of nonterminals, which would give a worse running time). In our experiments, we use the method described in Chi and Kolda (2011) for tensor decomposition.2 This method is fast, even for large tensors, as long as they are sparse. Such is the case with the tensors for our grammars. We use two treebanks for our comparison: the Penn treebank (Marcus et al., 1993) and the Arabic treebank (Maamouri et al., 2004). With the Penn treebank, we use sections 2–21 for training a maximum likelihood model and section 22 for parsing, while for the Arabic treebank we divide the data into two sets, of size 80% and 20%, one is used for training a maximum likelihood model and the other is used for parsing. The number of binary rules in th</context>
</contexts>
<marker>Chi, Kolda, 2011</marker>
<rawString>E. C. Chi and T. G. Kolda. 2011. On tensors, sparsity, and nonnegative factorizations. arXiv:1112.2414 [math.NA], December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>M Collins</author>
</authors>
<title>Tensor decomposition for fast latent-variable PCFG parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="9536" citStr="Cohen and Collins (2012)" startWordPosition="1722" endWordPosition="1725">n’s algorithm. In this paper we aim to asymptotically reduce the time complexity of the calculation of the inside-outside probabilities using an approximation algorithm. 3 Tensor Formulation of the Inside-Outside Algorithm At the core of our approach lies the observation that there is a (multi)linear algebraic formulation of the inside-outside algorithm. It can be represented as a series of tensor, matrix and vector products. A similar observation has been made for latent-variable PCFGs (Cohen et al., 2012) and hidden Markov models, where only matrix multiplication is required (Jaeger, 2000). Cohen and Collins (2012) use this observation together with tensor decomposition to improve the speed of latent-variable PCFG parsing. The representation of the inside-outside algorithm in tensor form is given in Figure 2. For example, if we consider the recursive equation for the inside probabilities (where αi,j is a vector varying over the nonterminals in the grammar, describing the inside probability for each nonterminal spanning words i to j): j-1 αi,j = T(αi,k, αk+1,j) k=i 489 Figure 2: The tensor form of the inside-outside algorithm, for calculation of marginal terms µ(a, i, j). and then apply the tensor produc</context>
<context position="26114" citStr="Cohen and Collins, 2012" startWordPosition="4777" endWordPosition="4780">fore use Eq. (3) to compute Vαi,j as 1 V αi,j = V U&gt; (Ej−1 k�1(� � V αi,k � W αk+1,j) , where V U&gt; is a Rr×r matrix that can be computed off-line, i.e., independently of z. A symmetrical relation can be used to compute W αi,j. Finally, we can write p(z) = 7r&gt;U (EN− 11(A O V α1,k O Wαk+1,N)1 , where 7r&gt;U is a R1×r vector that can again be computed off-line. This algorithm then runs in time O(rN3 + r2N2). 7.2 Applications to Dynamic Programming The approximation method presented in this paper is not limited to PCFG parsing. A similar approximation method has been used for latent-variable PCFGs (Cohen and Collins, 2012), and in general, tensor approximation can be used to speed-up insideoutside algorithms for general dynamic programming algorithms or weighted logic programs (Eisner et al., 2004; Cohen et al., 2011). In the general case, the dimension of the tensors will not be necessarily just three (corresponding to binary rules), but can be of a higher dimension, and therefore the speed gain can be even greater. In addition, tensor approximation can be used for computing marginals of latent variables in graphical models. For example, the complexity of the forward494 backward algorithm for HMMs can be reduc</context>
</contexts>
<marker>Cohen, Collins, 2012</marker>
<rawString>S. B. Cohen and M. Collins. 2012. Tensor decomposition for fast latent-variable PCFG parsing. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>R J Simmons</author>
<author>N A Smith</author>
</authors>
<date>2011</date>
<booktitle>Products of weighted logic programs. Theory and Practice of Logic Programming,</booktitle>
<pages>11--2</pages>
<contexts>
<context position="26313" citStr="Cohen et al., 2011" startWordPosition="4809" endWordPosition="4812">sed to compute W αi,j. Finally, we can write p(z) = 7r&gt;U (EN− 11(A O V α1,k O Wαk+1,N)1 , where 7r&gt;U is a R1×r vector that can again be computed off-line. This algorithm then runs in time O(rN3 + r2N2). 7.2 Applications to Dynamic Programming The approximation method presented in this paper is not limited to PCFG parsing. A similar approximation method has been used for latent-variable PCFGs (Cohen and Collins, 2012), and in general, tensor approximation can be used to speed-up insideoutside algorithms for general dynamic programming algorithms or weighted logic programs (Eisner et al., 2004; Cohen et al., 2011). In the general case, the dimension of the tensors will not be necessarily just three (corresponding to binary rules), but can be of a higher dimension, and therefore the speed gain can be even greater. In addition, tensor approximation can be used for computing marginals of latent variables in graphical models. For example, the complexity of the forward494 backward algorithm for HMMs can be reduced to be linear in the number of states (as opposed to quadratic) and linear in the rank used in an approximate singular-value decomposition (instead of tensor decomposition) of the transition and em</context>
</contexts>
<marker>Cohen, Simmons, Smith, 2011</marker>
<rawString>S. B. Cohen, R. J. Simmons, and N. A. Smith. 2011. Products of weighted logic programs. Theory and Practice of Logic Programming, 11(2–3):263–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>K Stratos</author>
<author>M Collins</author>
<author>D F Foster</author>
<author>L Ungar</author>
</authors>
<title>Spectral learning of latent-variable PCFGs.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2108" citStr="Cohen et al. (2012)" startWordPosition="299" endWordPosition="302">approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on an algebraic formulation of the inside-outside algorithm for PCFGs, based on a tensor formulation developed for latent-variable PCFGs in Cohen et al. (2012). We combine the method with known techniques for tensor decomposition to approximate the source PCFG, and develop a novel algorithm for approximate PCFG parsing. We obtain improved time upper bounds with respect to the input grammar size for PCFG parsing, and provide error upper bounds on the PCFG approximation, in contrast with existing heuristic methods. 2 Preliminaries This section introduces the special representation for probabilistic context-free grammars that we adopt in this paper, along with the decoding algorithm that we investigate. For an integer i &gt; 1, we let [i] = {1,2,...,i}. 2</context>
<context position="9424" citStr="Cohen et al., 2012" startWordPosition="1706" endWordPosition="1709">However, empirically, it is the insideoutside algorithm which takes most of the time to compute with Goodman’s algorithm. In this paper we aim to asymptotically reduce the time complexity of the calculation of the inside-outside probabilities using an approximation algorithm. 3 Tensor Formulation of the Inside-Outside Algorithm At the core of our approach lies the observation that there is a (multi)linear algebraic formulation of the inside-outside algorithm. It can be represented as a series of tensor, matrix and vector products. A similar observation has been made for latent-variable PCFGs (Cohen et al., 2012) and hidden Markov models, where only matrix multiplication is required (Jaeger, 2000). Cohen and Collins (2012) use this observation together with tensor decomposition to improve the speed of latent-variable PCFG parsing. The representation of the inside-outside algorithm in tensor form is given in Figure 2. For example, if we consider the recursive equation for the inside probabilities (where αi,j is a vector varying over the nonterminals in the grammar, describing the inside probability for each nonterminal spanning words i to j): j-1 αi,j = T(αi,k, αk+1,j) k=i 489 Figure 2: The tensor form</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2012</marker>
<rawString>S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and L. Ungar. 2012. Spectral learning of latent-variable PCFGs. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>N A Smith</author>
</authors>
<title>Parsing with soft and hard constraints on dependency length.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT, Parsing ’05.</booktitle>
<contexts>
<context position="1351" citStr="Eisner and Smith (2005)" startWordPosition="187" endWordPosition="190">ree grammars (PCFGs) has received considerable attention in recent years. Several strategies have been proposed, including beam-search, best-first and A*. In this paper we focus on the standard approach of approximating the source PCFG in such a way that parsing accuracy is traded for efficiency. Nederhof (2000) gives a thorough presentation of old and novel ideas for approximating nonprobabilistic CFGs by means of finite automata, on the basis of specialized preprocessing of selfembedding structures. In the probabilistic domain, approximation by means of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on</context>
</contexts>
<marker>Eisner, Smith, 2005</marker>
<rawString>J. Eisner and N. A. Smith. 2005. Parsing with soft and hard constraints on dependency length. In Proceedings of IWPT, Parsing ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>E Goldlust</author>
<author>N A Smith</author>
</authors>
<title>Dyna: A declarative language for implementing dynamic programs.</title>
<date>2004</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="26292" citStr="Eisner et al., 2004" startWordPosition="4805" endWordPosition="4808">cal relation can be used to compute W αi,j. Finally, we can write p(z) = 7r&gt;U (EN− 11(A O V α1,k O Wαk+1,N)1 , where 7r&gt;U is a R1×r vector that can again be computed off-line. This algorithm then runs in time O(rN3 + r2N2). 7.2 Applications to Dynamic Programming The approximation method presented in this paper is not limited to PCFG parsing. A similar approximation method has been used for latent-variable PCFGs (Cohen and Collins, 2012), and in general, tensor approximation can be used to speed-up insideoutside algorithms for general dynamic programming algorithms or weighted logic programs (Eisner et al., 2004; Cohen et al., 2011). In the general case, the dimension of the tensors will not be necessarily just three (corresponding to binary rules), but can be of a higher dimension, and therefore the speed gain can be even greater. In addition, tensor approximation can be used for computing marginals of latent variables in graphical models. For example, the complexity of the forward494 backward algorithm for HMMs can be reduced to be linear in the number of states (as opposed to quadratic) and linear in the rank used in an approximate singular-value decomposition (instead of tensor decomposition) of </context>
</contexts>
<marker>Eisner, Goldlust, Smith, 2004</marker>
<rawString>J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A declarative language for implementing dynamic programs. In Proc. of ACL (companion volume).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N M Faber</author>
<author>R Bro</author>
<author>P Hopke</author>
</authors>
<title>Recent developments in CANDECOMP/PARAFAC algorithms: a critical review.</title>
<date>2003</date>
<journal>Chemometrics and Intelligent Laboratory Systems,</journal>
<volume>65</volume>
<issue>1</issue>
<contexts>
<context position="14868" citStr="Faber et al., 2003" startWordPosition="2709" endWordPosition="2712">nown as CANDECOMP/PARAFAC decomposition (Carroll and Chang, 1970; Harshman, 1970; Kolda and Bader, 2009). Given an integer r, least squares CPD aims to find the nearest tensor in Kruskal form, minimizing squared error. More formally, for a given tensor D ∈ Rmxmxm, �� let ||D||F = i,j,k D2i. Let the set of tensors in ,j, the argmin as a set because there could be multiple solutions which achieve the same accuracy. There are various algorithms to perform CPD, such as alternating least squares, direct linear decomposition, alternating trilinear decomposition and pseudo alternating least squares (Faber et al., 2003) and even algorithms designed for sparse tensors (Chi and Kolda, 2011). Most of these algorithms treat the problem of identifying the approximate tensor as an optimization problem. Generally speaking, these optimization problems are hard to solve, but they work quite well in practice. 4.3 Parsing with Decomposed Tensors Equipped with the notion of tensor decomposition, we can now proceed with approximate tensor parsing in two steps. The first is approximating the tensor using a CPD algorithm, and the second is applying the algorithms in Figure 1 and Figure 2 to do parsing, while substituting a</context>
</contexts>
<marker>Faber, Bro, Hopke, 2003</marker>
<rawString>N. M. Faber, R. Bro, and P. Hopke. 2003. Recent developments in CANDECOMP/PARAFAC algorithms: a critical review. Chemometrics and Intelligent Laboratory Systems, 65(1):119–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="5885" citStr="Goodman (1996)" startWordPosition="1062" endWordPosition="1063">est scoring tree τ* for z according to the underlying PCFG, also called the “Viterbi parse:” p(a —4 x |a) = 1, E 1 2 τ* = argmax p(τ) [C(y1, y2)]i = Ci,j,kyj yk . τET (z) jE[m],kE[m] We also let C(1,2)(y1, y2) be the m-dimensional column vector with components: [C(1,2)(y1, y2)]k = E Ci,j,kyi yj . 1 2 iE[m],jE[m] Finally, we let C(1,3)(y1, y2) be the m-dimensional column vector with components: [C(1,3)(y1,y2)]j = E Ci,j,kyi yk . 1 2 iE[m],kE[m] For two vectors x, y E Rm we denote by x O y E Rm the Hadamard product of x and y, i.e., [xOy]i = xiyi. Finally, for vectors x, y, z E Rm, xyTzT is the Goodman (1996) noted that Viterbi parsers do not optimize the same metric that is usually used for parsing evaluation (Black et al., 1991). He suggested an alternative algorithm, which he called the “Labelled Recall Algorithm,” which aims to fix this issue. Goodman’s algorithm has two phases. In the first phase it computes, for each a E N and for each substring xi • • • xj of z, the marginal µ(a, i, j) defined as: µ(a,i,j) = E p(τ). τET (z): (a,i,j)Eτ Here we write (a, i, j) E τ if nonterminal a spans words xi • • • xj in the parse tree τ. 488 Inputs: Sentence x1 • • • xN, PCFG (N, G, R), parameters T E R(m</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>J. Goodman. 1996. Parsing algorithms and metrics. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Harshman</author>
</authors>
<title>Foundations of the PARAFAC procedure: Models and conditions for an “explanatory” multi-modal factor analysis. UCLA working papers in phoentics,</title>
<date>1970</date>
<pages>16--1</pages>
<contexts>
<context position="14329" citStr="Harshman, 1970" startWordPosition="2620" endWordPosition="2622">probabilities through vectors T(αi,k, αk+1,j). Nonetheless, the steps above can be easily adapted for the computation of the outside probabilities through vectors T(1,2)(βk,j,αk,i−1) and T(1,3)(βi,k, αj+1,k). 4.2 Approximate Tensor Decomposition The PCFG tensor T will not necessarily have the exact decomposed form in Eq. (1). We suggest to approximate the tensor T by finding the closest tensor according to some norm over Rmxmxm. An example of such an approximate decomposition is the canonical polyadic decomposition (CPD), also known as CANDECOMP/PARAFAC decomposition (Carroll and Chang, 1970; Harshman, 1970; Kolda and Bader, 2009). Given an integer r, least squares CPD aims to find the nearest tensor in Kruskal form, minimizing squared error. More formally, for a given tensor D ∈ Rmxmxm, �� let ||D||F = i,j,k D2i. Let the set of tensors in ,j, the argmin as a set because there could be multiple solutions which achieve the same accuracy. There are various algorithms to perform CPD, such as alternating least squares, direct linear decomposition, alternating trilinear decomposition and pseudo alternating least squares (Faber et al., 2003) and even algorithms designed for sparse tensors (Chi and Kol</context>
</contexts>
<marker>Harshman, 1970</marker>
<rawString>R. A. Harshman. 1970. Foundations of the PARAFAC procedure: Models and conditions for an “explanatory” multi-modal factor analysis. UCLA working papers in phoentics, 16:1–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Høastad</author>
</authors>
<title>Tensor rank is NP-complete.</title>
<date>1990</date>
<journal>Algorithms,</journal>
<pages>11--644</pages>
<contexts>
<context position="13653" citStr="Høastad, 1990" startWordPosition="2518" endWordPosition="2519">ares CPD of C is a tensor C� such r i=1 λiuiv wT Z Ti that C ∈ argmin ˆCEC, ||C − �C||F. Here, we treat � �r � T (y1, y2) = λiuivT i wT (y1, y2) = i i=1 r � λiui(vTi y1)(wTi y2) = i=1 � � UT(λ � V y1 � Wy2) . (2) The total complexity of the computation in Eq. (2) is now O(rm). It is well-known that an exact tensor decomposition for T can be achieved with r = m2 (Kruskal, 1989). In this case, there is no computational gain in using Eq. (2) for the inside calculation. The minimal r required for an exact tensor decomposition can be smaller than m2. However, identifying that minimal r is NP-hard (Høastad, 1990). In this section we focused on the computation of the inside probabilities through vectors T(αi,k, αk+1,j). Nonetheless, the steps above can be easily adapted for the computation of the outside probabilities through vectors T(1,2)(βk,j,αk,i−1) and T(1,3)(βi,k, αj+1,k). 4.2 Approximate Tensor Decomposition The PCFG tensor T will not necessarily have the exact decomposed form in Eq. (1). We suggest to approximate the tensor T by finding the closest tensor according to some norm over Rmxmxm. An example of such an approximate decomposition is the canonical polyadic decomposition (CPD), also known</context>
</contexts>
<marker>Høastad, 1990</marker>
<rawString>J. Høastad. 1990. Tensor rank is NP-complete. Algorithms, 11:644–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jaeger</author>
</authors>
<title>Observable operator models for discrete stochastic time series.</title>
<date>2000</date>
<journal>Neural Computation,</journal>
<volume>12</volume>
<issue>6</issue>
<contexts>
<context position="9510" citStr="Jaeger, 2000" startWordPosition="1720" endWordPosition="1721">ute with Goodman’s algorithm. In this paper we aim to asymptotically reduce the time complexity of the calculation of the inside-outside probabilities using an approximation algorithm. 3 Tensor Formulation of the Inside-Outside Algorithm At the core of our approach lies the observation that there is a (multi)linear algebraic formulation of the inside-outside algorithm. It can be represented as a series of tensor, matrix and vector products. A similar observation has been made for latent-variable PCFGs (Cohen et al., 2012) and hidden Markov models, where only matrix multiplication is required (Jaeger, 2000). Cohen and Collins (2012) use this observation together with tensor decomposition to improve the speed of latent-variable PCFG parsing. The representation of the inside-outside algorithm in tensor form is given in Figure 2. For example, if we consider the recursive equation for the inside probabilities (where αi,j is a vector varying over the nonterminals in the grammar, describing the inside probability for each nonterminal spanning words i to j): j-1 αi,j = T(αi,k, αk+1,j) k=i 489 Figure 2: The tensor form of the inside-outside algorithm, for calculation of marginal terms µ(a, i, j). and th</context>
</contexts>
<marker>Jaeger, 2000</marker>
<rawString>H. Jaeger. 2000. Observable operator models for discrete stochastic time series. Neural Computation, 12(6):1371–1398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="24622" citStr="Klein and Manning, 2003" startWordPosition="4504" endWordPosition="4507"> up of a factor of 4.75 for Arabic (r = 140) and 6.5 for English (r = 260) while retaining similar performance. Perhaps more surprising is that using the tensor approximation actually improves performance in several cases. We hypothesize that the cause of this is that the tensor decomposition requires less parameters to express the rule probabilities in the grammar, and therefore leads to better generalization than a vanilla maximum likelihood estimate. We include results for a more complex model for Arabic, which uses horizontal Markovization of order 1 and vertical Markovization of order 2 (Klein and Manning, 2003). This grammar includes 2,188 binary rules. Parsing exhaustively using this grammar takes 1.30 seconds per sentence (on average) with an F1 measure of 64.43. Parsing with tensor decomposition for r = 280 takes 0.62 seconds per sentence (on average) with an F1 measure of 64.05. 7 Discussion In this section, we briefly touch on several other topics related to tensor approximation. 7.1 Approximating the Probability of a String The probability of a sentence z under a PCFG is defined as p(z) = Eτ∈T (z) p(-r), and can be approximated using the algorithm in Section 4.3, running in time O(rN3 + rmN2).</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Kolda</author>
<author>B W Bader</author>
</authors>
<title>Tensor decompositions and applications.</title>
<date>2009</date>
<journal>SIAM Rev.,</journal>
<pages>51--455</pages>
<contexts>
<context position="11693" citStr="Kolda and Bader (2009)" startWordPosition="2087" endWordPosition="2090">f m elements, where computation of each element requires time O(m2). Therefore, the algorithm has a O(m3) multiplicative factor in its time complexity, which we aim to reduce by means of an approximate algorithm. Our approximate method relies on a simple observation. Given an integer r ≥ 1, assume that the tensor T has the following special form, called “Kruskal form:” λiuivi wi . (1) In words, T is the sum of r tensors, where each tensor is obtained as the product of three vectors ui, vi and wi, together with a scalar λi. Exact Kruskal decomposition of a tensor is not necessarily unique. See Kolda and Bader (2009) for discussion of uniqueness of tensor decomposition. Inputs: Sentence x1 · · · xN, PCFG (N, L, R), parameters T ∈ R(mxmxm), Q ∈ R(mxn), π∈ R(mx1). Data structures: • Each αi,j ∈ R1xm, i, j ∈ [N], i ≤ j, is a row vector of inside terms ranging over a ∈ N. • Each βi,j ∈ Rmx1, i, j ∈ [N], i ≤ j, is a column vector of outside terms ranging over a ∈ N. • Each µ(a, i, j) ∈ R for a ∈ N, i, j ∈ [N], i ≤ j, is a marginal probability. Algorithm: (Inside base case) ∀i ∈ [N],∀(a → xi) ∈ R, [αi,i]a = Qa,x (Inside recursion) ∀i, j ∈ [N], i &lt; j, αi,j = j−1 X T(αi,k, αk+1,j) k=i (Outside base case) ∀a ∈ N, </context>
<context position="14353" citStr="Kolda and Bader, 2009" startWordPosition="2623" endWordPosition="2626">rough vectors T(αi,k, αk+1,j). Nonetheless, the steps above can be easily adapted for the computation of the outside probabilities through vectors T(1,2)(βk,j,αk,i−1) and T(1,3)(βi,k, αj+1,k). 4.2 Approximate Tensor Decomposition The PCFG tensor T will not necessarily have the exact decomposed form in Eq. (1). We suggest to approximate the tensor T by finding the closest tensor according to some norm over Rmxmxm. An example of such an approximate decomposition is the canonical polyadic decomposition (CPD), also known as CANDECOMP/PARAFAC decomposition (Carroll and Chang, 1970; Harshman, 1970; Kolda and Bader, 2009). Given an integer r, least squares CPD aims to find the nearest tensor in Kruskal form, minimizing squared error. More formally, for a given tensor D ∈ Rmxmxm, �� let ||D||F = i,j,k D2i. Let the set of tensors in ,j, the argmin as a set because there could be multiple solutions which achieve the same accuracy. There are various algorithms to perform CPD, such as alternating least squares, direct linear decomposition, alternating trilinear decomposition and pseudo alternating least squares (Faber et al., 2003) and even algorithms designed for sparse tensors (Chi and Kolda, 2011). Most of these</context>
</contexts>
<marker>Kolda, Bader, 2009</marker>
<rawString>T. G. Kolda and B. W. Bader. 2009. Tensor decompositions and applications. SIAM Rev., 51:455–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Kruskal</author>
</authors>
<title>Rank, decomposition, and uniqueness for 3-way and N-way arrays. In</title>
<date>1989</date>
<booktitle>Multiway Data Analysis,</booktitle>
<pages>7--18</pages>
<editor>R. Coppi and S. Bolasco, editors,</editor>
<contexts>
<context position="13418" citStr="Kruskal, 1989" startWordPosition="2476" endWordPosition="2477">o λ = (λ1, ... , λr). Using the decomposition in Eq. (1) within Definition 1 we can express the array T (y1, y2) as: Kruskal form Cr be: Cr ={C ∈ Rmxmxm |C = s.t. λi ∈ R, ui, vi, wi ∈ Rm, ||ui||2 = ||vi||2 = ||wi||2 = 1}. The least squares CPD of C is a tensor C� such r i=1 λiuiv wT Z Ti that C ∈ argmin ˆCEC, ||C − �C||F. Here, we treat � �r � T (y1, y2) = λiuivT i wT (y1, y2) = i i=1 r � λiui(vTi y1)(wTi y2) = i=1 � � UT(λ � V y1 � Wy2) . (2) The total complexity of the computation in Eq. (2) is now O(rm). It is well-known that an exact tensor decomposition for T can be achieved with r = m2 (Kruskal, 1989). In this case, there is no computational gain in using Eq. (2) for the inside calculation. The minimal r required for an exact tensor decomposition can be smaller than m2. However, identifying that minimal r is NP-hard (Høastad, 1990). In this section we focused on the computation of the inside probabilities through vectors T(αi,k, αk+1,j). Nonetheless, the steps above can be easily adapted for the computation of the outside probabilities through vectors T(1,2)(βk,j,αk,i−1) and T(1,3)(βi,k, αj+1,k). 4.2 Approximate Tensor Decomposition The PCFG tensor T will not necessarily have the exact dec</context>
</contexts>
<marker>Kruskal, 1989</marker>
<rawString>J. B. Kruskal. 1989. Rank, decomposition, and uniqueness for 3-way and N-way arrays. In R. Coppi and S. Bolasco, editors, Multiway Data Analysis, pages 7– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
<author>A Bies</author>
<author>T Buckwalter</author>
<author>W Mekki</author>
</authors>
<title>The Penn Arabic Treebank: Building a largescale annotated Arabic corpus.</title>
<date>2004</date>
<booktitle>In Proceedings NEMLAR.</booktitle>
<contexts>
<context position="21944" citStr="Maamouri et al., 2004" startWordPosition="4040" endWordPosition="4043">s the clocktime comparison reliable for drawing conclusions about the speed of the algorithms. Our implementation of the vanilla parsing algorithm is linear in the size of the grammar (and not cubic in the number of nonterminals, which would give a worse running time). In our experiments, we use the method described in Chi and Kolda (2011) for tensor decomposition.2 This method is fast, even for large tensors, as long as they are sparse. Such is the case with the tensors for our grammars. We use two treebanks for our comparison: the Penn treebank (Marcus et al., 1993) and the Arabic treebank (Maamouri et al., 2004). With the Penn treebank, we use sections 2–21 for training a maximum likelihood model and section 22 for parsing, while for the Arabic treebank we divide the data into two sets, of size 80% and 20%, one is used for training a maximum likelihood model and the other is used for parsing. The number of binary rules in the treebank grammar is 7,240. The number of nonterminals is 112 and the number of preterminals is 2593Unary rules are removed by collapsing non-terminal chains. This increased the number of preterminals. The number of binary rules in the Arabic treebank is significantly smaller and</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, Mekki, 2004</marker>
<rawString>M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki. 2004. The Penn Arabic Treebank: Building a largescale annotated Arabic corpus. In Proceedings NEMLAR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="21896" citStr="Marcus et al., 1993" startWordPosition="4032" endWordPosition="4035">pagating the beliefs up in the tree. This makes the clocktime comparison reliable for drawing conclusions about the speed of the algorithms. Our implementation of the vanilla parsing algorithm is linear in the size of the grammar (and not cubic in the number of nonterminals, which would give a worse running time). In our experiments, we use the method described in Chi and Kolda (2011) for tensor decomposition.2 This method is fast, even for large tensors, as long as they are sparse. Such is the case with the tensors for our grammars. We use two treebanks for our comparison: the Penn treebank (Marcus et al., 1993) and the Arabic treebank (Maamouri et al., 2004). With the Penn treebank, we use sections 2–21 for training a maximum likelihood model and section 22 for parsing, while for the Arabic treebank we divide the data into two sets, of size 80% and 20%, one is used for training a maximum likelihood model and the other is used for parsing. The number of binary rules in the treebank grammar is 7,240. The number of nonterminals is 112 and the number of preterminals is 2593Unary rules are removed by collapsing non-terminal chains. This increased the number of preterminals. The number of binary rules in </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsuzaki</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="8634" citStr="Matsuzaki et al. (2005)" startWordPosition="1578" endWordPosition="1581">s algorithm does not enforce that the output parse trees are included in the tree language of the PCFG, that is, certain combinations of children and parent nonterminals may violate the rules in the grammar. In our experiments we departed from this, and changed Goodman’s algorithm by incorporating the grammar into the dynamic programming algorithm in Figure 1. The reason this is important for our experiments is that we binarize the grammar prior to parsing, and we need to enforce the links between the split nonterminals (in the binarized grammar) that refer to the same syntactic category. See Matsuzaki et al. (2005) for more details about the binarization scheme we used. This step changes the dynamic programming equation of Goodman to be linear in the size of the grammar (figure 1). However, empirically, it is the insideoutside algorithm which takes most of the time to compute with Goodman’s algorithm. In this paper we aim to asymptotically reduce the time complexity of the calculation of the inside-outside probabilities using an approximation algorithm. 3 Tensor Formulation of the Inside-Outside Algorithm At the core of our approach lies the observation that there is a (multi)linear algebraic formulatio</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
</authors>
<title>Practical experiments with regular approximation of context-free languages.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="1041" citStr="Nederhof (2000)" startWordPosition="142" endWordPosition="143">improves time complexity with respect to the input grammar size, and prove upper bounds on the approximation quality. We test our algorithm on two treebanks, and get significant improvements in parsing speed. 1 Introduction The problem of speeding-up parsing algorithms based on probabilistic context-free grammars (PCFGs) has received considerable attention in recent years. Several strategies have been proposed, including beam-search, best-first and A*. In this paper we focus on the standard approach of approximating the source PCFG in such a way that parsing accuracy is traded for efficiency. Nederhof (2000) gives a thorough presentation of old and novel ideas for approximating nonprobabilistic CFGs by means of finite automata, on the basis of specialized preprocessing of selfembedding structures. In the probabilistic domain, approximation by means of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters</context>
</contexts>
<marker>Nederhof, 2000</marker>
<rawString>M.-J. Nederhof. 2000. Practical experiments with regular approximation of context-free languages. Computational Linguistics, 26(1):17–44.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>