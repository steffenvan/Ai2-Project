<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025418">
<title confidence="0.98697">
METEOR-NEXT and the METEOR Paraphrase Tables: Improved
Evaluation Support for Five Target Languages
</title>
<author confidence="0.986354">
Michael Denkowski and Alon Lavie
</author>
<affiliation confidence="0.9025005">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA
</affiliation>
<email confidence="0.999544">
{mdenkows,alavie}@cs.cmu.edu
</email>
<sectionHeader confidence="0.994818" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999898909090909">
This paper describes our submission to
the WMT10 Shared Evaluation Task and
MetricsMATR10. We present a version
of the METEOR-NEXT metric with para-
phrase tables for five target languages. We
describe the creation of these paraphrase
tables and conduct a tuning experiment
that demonstrates consistent improvement
across all languages over baseline ver-
sions of the metric without paraphrase re-
sources.
</bodyText>
<sectionHeader confidence="0.998417" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99982775">
Workshops such as WMT (Callison-Burch et al.,
2009) and MetricsMATR (Przybocki et al., 2008)
focus on the need for accurate automatic met-
rics for evaluating the quality of machine transla-
tion (MT) output. While these workshops evalu-
ate metric performance on many target languages,
most metrics are limited to English due to the rel-
ative lack of lexical resources for other languages.
This paper describes a language-independent
method for adding paraphrase support to the
METEOR-NEXT metric for all WMT10 target lan-
guages. Taking advantage of the large parallel cor-
pora released for the translation tasks often accom-
panying evaluation tasks, we automatically con-
struct paraphrase tables using the pivot method
(Bannard and Callison-Burch, 2005). We use the
WMT09 human evaluation data to tune versions
of METEOR-NEXT with and without paraphrases
and report significantly better performance for ver-
sions with paraphrase support.
</bodyText>
<sectionHeader confidence="0.987969" genericHeader="method">
2 The METEOR-NEXT Metric
</sectionHeader>
<bodyText confidence="0.999461216216216">
The METEOR-NEXT metric (Denkowski and
Lavie, 2010) evaluates a machine translation hy-
pothesis against a reference translation by calcu-
lating a similarity score based on an alignment be-
tween the two strings. When multiple references
are provided, the hypothesis is scored against each
and the reference producing the highest score is
used. Alignments are formed in two stages: search
space construction and alignment selection.
For a single hypothesis-reference pair, the space
of possible alignments is constructed by identify-
ing all possible word and phrase matches between
the strings according to the following matchers:
Exact: Words are matched if and only if their sur-
face forms are identical.
Stem: Words are stemmed using a language-
appropriate Snowball Stemmer (Porter, 2001) and
matched if the stems are identical.
Synonym: Words are matched if they are both
members of a synonym set according to the Word-
Net (Miller and Fellbaum, 2007) database.
Paraphrase: Phrases are matched if they are
listed as paraphrases in a paraphrase table. The
tables used are described in Section 3.
Previously, full support has been limited to En-
glish, with French, German, and Spanish having
exact and stem match support only, and Czech
having exact match support only.
Although the exact, stem, and synonym match-
ers identify word matches while the paraphrase
matcher identifies phrase matches, all matches can
be generalized to phrase matches with a start po-
sition and phrase length in each string. A word
occurring less than length positions after a match
start is considered covered by the match. Ex-
act, stem, and synonym matches always cover one
word in each string.
</bodyText>
<listItem confidence="0.705281428571429">
Once the search space is constructed, the final
alignment is identified as the largest possible sub-
set of all matches meeting the following criteria in
order of importance:
1. Each word in each sentence is covered by
zero or one matches
2. Largest number of covered words across both
</listItem>
<page confidence="0.957387">
339
</page>
<note confidence="0.464958">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 339–342,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.445365">
sentences
</bodyText>
<listItem confidence="0.996798125">
3. Smallest number of chunks, where a chunk
is defined as a series of matched phrases that
is contiguous and identically ordered in both
sentences
4. Smallest sum of absolute distances between
match start positions in the two sentences
(prefer to align words and phrases that occur
at similar positions in both sentences)
</listItem>
<bodyText confidence="0.995824">
Once an alignment is selected, the METEOR-
NEXT score is calculated as follows. The num-
ber of words in the translation hypothesis (t) and
reference (r) are counted. For each of the match-
ers (mi), count the number of words covered by
matches of this type in the hypothesis (mi(t)) and
reference (mi(r)) and apply matcher weight (wi).
The weighted Precision and Recall are then calcu-
lated:
</bodyText>
<equation confidence="0.9993035">
P = Ei wi · mi (t) R = Ei wi · mi (r)
|t ||r|
</equation>
<bodyText confidence="0.999183">
The parameterized harmonic mean of P and R
(van Rijsbergen, 1979) is then calculated:
</bodyText>
<subsectionHeader confidence="0.999952">
3.1 Paraphrasing with Parallel Corpora
</subsectionHeader>
<bodyText confidence="0.999983615384615">
Following Bannard and Callison-Burch (2005),
we extract paraphrases automatically from bilin-
gual corpora using a pivot phrase method. For a
given language pair, word alignment, phrase ex-
traction, and phrase scoring are conducted on par-
allel corpora to build a single bilingual phrase ta-
ble for the language pair. For each native phrase
(n1) in the table, we identify each foreign phrase
(f) that translates n1. Each alternate native phrase
(n2 =6 n1) that translates f is considered a para-
phrase of n1 with probability P(f|n1) · P(n2|f).
The total probability of n2 paraphrasing n1 is
given as the sum over all f:
</bodyText>
<equation confidence="0.975501">
�P(n2|n1) = P(f|n1) · P(n2|f)
f
</equation>
<bodyText confidence="0.999854">
The same method can be used to identify foreign
paraphrases (f1, f2) given native pivot phrases
n. To merge same-language paraphrases ex-
tracted from different parallel corpora, we take the
mean of the corpus-specific paraphrase probabili-
ties (PC) weighted by the size of the corpora (C)
used for paraphrase extraction:
</bodyText>
<equation confidence="0.9995014">
P · R
α · P + (1 − α) · R
P(n2|n1) = EC |C |· PC(n2 |n1)
Fmean =
EC |C|
</equation>
<bodyText confidence="0.996213">
To account for gaps and differences in word or-
der, a fragmentation penalty (Lavie and Agar-
wal, 2007) is calculated using the total number of
matched words (m) and number of chunks (ch):
To improve paraphrase accuracy, we apply mul-
tiple filtering techniques during paraphrase extrac-
tion. The following are applied to each paraphrase
instance (n1, f, n2):
</bodyText>
<equation confidence="0.89101425">
Pen = -y · (ch )� 1. Discard paraphrases with very low probabil-
m ity (P(f|n1) · P(n2|f) &lt; 0.001)
The final METEOR-NEXT score is then calculated:
Score = (1 − Pen) · Fmean
</equation>
<bodyText confidence="0.999716333333333">
The parameters α, Q, &apos;y, and wi...wn can be
tuned to maximize correlation with various types
of human judgments.
</bodyText>
<sectionHeader confidence="0.998219" genericHeader="method">
3 The METEOR Paraphrase Tables
</sectionHeader>
<bodyText confidence="0.998420166666667">
To extend support for WMT10 target languages,
we use released parallel corpora to construct para-
phrase tables for English, Czech, German, Span-
ish, and French. These tables are used by the
METEOR-NEXT paraphrase matcher to identify
additional phrase matches in each language.
</bodyText>
<listItem confidence="0.982266857142857">
2. Discard paraphrases for which n1, f, or n2
contain any punctuation characters.
3. Discard paraphrases for which n1, f, or
n2 contain only common words. Common
words are defined as having relative fre-
quency of 0.001 or greater in the parallel cor-
pus.
</listItem>
<bodyText confidence="0.998142">
Remaining phrase instances are summed to con-
struct corpus-specific paraphrase tables. Same-
language paraphrase tables are selectively merged
as part of the tuning process described in Sec-
tion 4.2. Final paraphrase tables are further fil-
tered to include only paraphrases with probabili-
ties above a final threshold (0.01).
</bodyText>
<page confidence="0.988638">
340
</page>
<table confidence="0.999889571428571">
Language Pair Corpus Phrase Table
Target Source Sentences Phrase Pairs
English Czech 7,321,950 128,326,269
English German 1,630,132 84,035,599
English Spanish 7,965,250 363,714,779
English French 8,993,161 404,883,736
German Spanish 1,305,650 70,992,157
</table>
<tableCaption confidence="0.999556">
Table 1: Sizes of training corpora and phrase ta-
bles used for paraphrase extraction
</tableCaption>
<table confidence="0.999913571428571">
Language Pivot Languages Phrase Pairs
English German, Spanish, 6,236,236
French
Czech English 756,113
German English, Spanish 3,521,052
Spanish English, German 6,352,690
French English 3,382,847
</table>
<tableCaption confidence="0.9984">
Table 2: Sizes of final paraphrase tables
</tableCaption>
<subsectionHeader confidence="0.998909">
3.2 Available Data
</subsectionHeader>
<bodyText confidence="0.993624666666667">
We conduct paraphrase extraction using parallel
corpora released for the WMT10 Shared Trans-
lation Task. This includes Europarl corpora
(French-English, Spanish-English, and German-
English), news commentary (French-English,
Spanish-English, German-English, and Czech-
English), United Nations corpora (French-English
and Spanish-English), and the CzEng (Bojar and
ˇZabokrtsk´y, 2009) corpus sections 0-8 (Czech-
English). In addition, we use the German-Spanish
Europarl corpus released for WMT08 (Callison-
Burch et al., 2008).
</bodyText>
<subsectionHeader confidence="0.999447">
3.3 Paraphrase Table Construction
</subsectionHeader>
<bodyText confidence="0.9987495625">
Using all available data for each language pair,
we create bilingual phrase tables for the follow-
ing: French-English, Spanish-English, German-
English, Czech-English, and German-Spanish.
The full training corpora and resulting phrase ta-
bles are described in Table 1. For each phrase ta-
ble, both foreign and native paraphrases are ex-
tracted. Same-language paraphrases are selec-
tively merged as described in Section 4.2 to pro-
duce the final paraphrase tables described in Ta-
ble 2. To keep table size reasonable, we only ex-
tract paraphrases for phrases occurring in target
corpora consisting of the pooled development data
from the WMT08, WMT09, and WMT10 trans-
lation tasks (10,158 sentences for Czech, 20,258
sentences for all other languages).
</bodyText>
<table confidence="0.996432833333333">
Target Systems Usable Judgments
English 45 20,357
Czech 5 11,242
German 11 6,563
Spanish 9 3,249
French 12 2,967
</table>
<tableCaption confidence="0.9696945">
Table 3: Human ranking judgment data from
WMT09
</tableCaption>
<sectionHeader confidence="0.985344" genericHeader="method">
4 Tuning METEOR-NEXT
</sectionHeader>
<subsectionHeader confidence="0.998076">
4.1 Development Data
</subsectionHeader>
<bodyText confidence="0.999984888888889">
As part of the WMT10 Shared Evaluation Task,
data from WMT09 (Callison-Burch et al., 2009),
including system output, reference translations,
and human judgments, is available for metric de-
velopment. As metrics are evaluated primarily
on their ability to rank system output on the seg-
ment level, we select the human ranking judg-
ments from WMT09 as our development set (de-
scribed in Table 3).
</bodyText>
<subsectionHeader confidence="0.99863">
4.2 Tuning Procedure
</subsectionHeader>
<bodyText confidence="0.999957">
Tuning a version of METEOR-NEXT consists of
selecting parameters (α, Q, -y, wi...w,,,) that opti-
mize an objective function for a given language.
If multiple paraphrase tables exist for a language,
tuning also requires selecting the optimal set of ta-
bles to merge.
For WMT10, we tune to rank consistency on the
WMT09 data. Following Callison-Burch et. al
(2009), we discard judgments where system out-
puts are deemed equivalent and calculate the pro-
portion of remaining judgments preserved when
system outputs are ranked by automatic metric
scores. For each target language, tuning is con-
ducted as an exhaustive grid search over metric pa-
rameters and possible paraphrase tables, resulting
in global optima for both.
</bodyText>
<sectionHeader confidence="0.998176" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999853">
To evaluate the impact of our paraphrase ta-
bles on metric performance, we tune versions of
METEOR-NEXT with and without the paraphrase
matchers for each language. For further compar-
ison, we tune a version of METEOR-NEXT using
the TERp English paraphrase table (Snover et al.,
2009) used by previous versions of the metric.
As shown in Table 4, the addition of paraphrases
leads to a better tuning point for every target lan-
guage. The best scoring subset of paraphrase ta-
</bodyText>
<page confidence="0.995566">
341
</page>
<table confidence="0.999851416666667">
Language Paraphrases Rank Consistency α Q &apos;y wexact wstem wsyn wpar
English none 0.619 0.85 2.35 0.45 1.00 0.80 0.60 –
TERp 0.625 0.70 1.40 0.25 1.00 0.80 0.80 0.60
de+es+fr 0.629 0.75 0.60 0.35 1.00 0.80 0.80 0.60
Czech none 0.564 0.95 0.20 0.70 1.00 – – –
en 0.574 0.95 2.15 0.35 1.00 – – 0.40
German none 0.550 0.20 0.75 0.25 1.00 0.80 – –
en+es 0.576 0.75 0.80 0.90 1.00 0.20 – 0.80
Spanish none 0.586 0.95 0.55 0.90 1.00 0.80 – –
en+de 0.608 0.15 0.25 0.75 1.00 0.80 – 0.40
French none 0.696 0.95 0.80 0.35 1.00 0.60 – –
en 0.707 0.90 0.85 0.45 1.00 0.00 – 0.60
</table>
<tableCaption confidence="0.999703">
Table 4: Optimal METEOR-NEXT parameters with and without paraphrases for WMT10 target languages
</tableCaption>
<bodyText confidence="0.99927095">
bles for English also outperforms the TERp para-
phrase table.
Analysis of the phrase matches contributed by
the paraphrase matchers reveals an interesting
point about the task of paraphrasing for MT eval-
uation. Despite filtering techniques, the final para-
phrase tables include some unusual, inaccurate,
or highly context-dependent paraphrases. How-
ever, the vast majority of matches identified be-
tween actual system output and reference trans-
lations correspond to valid paraphrases. In many
cases, the evaluation task itself acts as a final filter;
to produce a phrase that can match a spurious para-
phrase, not only must a MT system produce incor-
rect output, but it must produce output that over-
laps exactly with an obscure paraphrase of some
phrase in the reference translation. As systems
are far more likely to produce phrases with similar
words to those in reference translations, far more
valid paraphrases exist in typical system output.
</bodyText>
<sectionHeader confidence="0.999305" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999857428571428">
We have presented versions of METEOR-NEXT
and paraphrase tables for five target languages.
Tuning experiments indicate consistent improve-
ments across all languages over baseline versions
of the metric. Created for MT evaluation, the ME-
TEOR paraphrase tables can also be used for other
tasks in MT and natural language processing. Fur-
ther, the techniques used to build the paraphrase
tables are language-independent and can be used
to improve evaluation support for other target lan-
guages. METEOR-NEXT, the METEOR paraphrase
tables, and the software used to generate para-
phrases are released under an open source license
and made available via the METEOR website.
</bodyText>
<sectionHeader confidence="0.999207" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999768685714286">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. of
ACL05.
Ondˇrej Bojar and Zdenˇek ˇZabokrtsk´y. 2009. CzEng
0.9: Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proc. of WMT08.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of WMT09. In
Proc. of WMT09.
Michael Denkowski and Alon Lavie. 2010. Extend-
ing the METEOR Machine Translation Metric to
the Phrase Level for Improved Correlation with Hu-
man Post-Editing Judgments. In Proc. NAACL/HLT
2010.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
Proc. of WMT07.
George Miller and Christiane Fellbaum. 2007. Word-
Net. http://wordnet.princeton.edu/.
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/texts/.
M. Przybocki, K. Peterson, and S Bronsart. 2008. Offi-
cial results of the NIST 2008 ”Metrics for MAchine
TRanslation” Challenge (MetricsMATR08).
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments with
a Tunable MT Metric. In Proc. of WMT09.
C. van Rijsbergen, 1979. Information Retrieval, chap-
ter 7. 2nd edition.
</reference>
<page confidence="0.998244">
342
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.455945">
<title confidence="0.7716015">the Tables: Evaluation Support for Five Target Languages Denkowski Language Technologies</title>
<affiliation confidence="0.9972165">School of Computer Carnegie Mellon</affiliation>
<address confidence="0.992423">Pittsburgh, PA 15232,</address>
<abstract confidence="0.992377">This paper describes our submission to the WMT10 Shared Evaluation Task and MetricsMATR10. We present a version the with paraphrase tables for five target languages. We describe the creation of these paraphrase tables and conduct a tuning experiment that demonstrates consistent improvement across all languages over baseline versions of the metric without paraphrase resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proc. of ACL05.</booktitle>
<contexts>
<context position="1443" citStr="Bannard and Callison-Burch, 2005" startWordPosition="206" endWordPosition="209">for accurate automatic metrics for evaluating the quality of machine translation (MT) output. While these workshops evaluate metric performance on many target languages, most metrics are limited to English due to the relative lack of lexical resources for other languages. This paper describes a language-independent method for adding paraphrase support to the METEOR-NEXT metric for all WMT10 target languages. Taking advantage of the large parallel corpora released for the translation tasks often accompanying evaluation tasks, we automatically construct paraphrase tables using the pivot method (Bannard and Callison-Burch, 2005). We use the WMT09 human evaluation data to tune versions of METEOR-NEXT with and without paraphrases and report significantly better performance for versions with paraphrase support. 2 The METEOR-NEXT Metric The METEOR-NEXT metric (Denkowski and Lavie, 2010) evaluates a machine translation hypothesis against a reference translation by calculating a similarity score based on an alignment between the two strings. When multiple references are provided, the hypothesis is scored against each and the reference producing the highest score is used. Alignments are formed in two stages: search space co</context>
<context position="4721" citStr="Bannard and Callison-Burch (2005)" startWordPosition="738" endWordPosition="741">lar positions in both sentences) Once an alignment is selected, the METEORNEXT score is calculated as follows. The number of words in the translation hypothesis (t) and reference (r) are counted. For each of the matchers (mi), count the number of words covered by matches of this type in the hypothesis (mi(t)) and reference (mi(r)) and apply matcher weight (wi). The weighted Precision and Recall are then calculated: P = Ei wi · mi (t) R = Ei wi · mi (r) |t ||r| The parameterized harmonic mean of P and R (van Rijsbergen, 1979) is then calculated: 3.1 Paraphrasing with Parallel Corpora Following Bannard and Callison-Burch (2005), we extract paraphrases automatically from bilingual corpora using a pivot phrase method. For a given language pair, word alignment, phrase extraction, and phrase scoring are conducted on parallel corpora to build a single bilingual phrase table for the language pair. For each native phrase (n1) in the table, we identify each foreign phrase (f) that translates n1. Each alternate native phrase (n2 =6 n1) that translates f is considered a paraphrase of n1 with probability P(f|n1) · P(n2|f). The total probability of n2 paraphrasing n1 is given as the sum over all f: �P(n2|n1) = P(f|n1) · P(n2|f)</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proc. of ACL05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<title>CzEng 0.9: Large Parallel Treebank with Rich Annotation. Prague Bulletin of Mathematical Linguistics.</title>
<date>2009</date>
<marker>Bojar, ˇZabokrtsk´y, 2009</marker>
<rawString>Ondˇrej Bojar and Zdenˇek ˇZabokrtsk´y. 2009. CzEng 0.9: Large Parallel Treebank with Rich Annotation. Prague Bulletin of Mathematical Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of WMT08.</booktitle>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In Proc. of WMT08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of WMT09. In Proc. of WMT09.</booktitle>
<contexts>
<context position="750" citStr="Callison-Burch et al., 2009" startWordPosition="100" endWordPosition="103">Alon Lavie Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15232, USA {mdenkows,alavie}@cs.cmu.edu Abstract This paper describes our submission to the WMT10 Shared Evaluation Task and MetricsMATR10. We present a version of the METEOR-NEXT metric with paraphrase tables for five target languages. We describe the creation of these paraphrase tables and conduct a tuning experiment that demonstrates consistent improvement across all languages over baseline versions of the metric without paraphrase resources. 1 Introduction Workshops such as WMT (Callison-Burch et al., 2009) and MetricsMATR (Przybocki et al., 2008) focus on the need for accurate automatic metrics for evaluating the quality of machine translation (MT) output. While these workshops evaluate metric performance on many target languages, most metrics are limited to English due to the relative lack of lexical resources for other languages. This paper describes a language-independent method for adding paraphrase support to the METEOR-NEXT metric for all WMT10 target languages. Taking advantage of the large parallel corpora released for the translation tasks often accompanying evaluation tasks, we automa</context>
<context position="9418" citStr="Callison-Burch et al., 2009" startWordPosition="1477" endWordPosition="1480">ction 4.2 to produce the final paraphrase tables described in Table 2. To keep table size reasonable, we only extract paraphrases for phrases occurring in target corpora consisting of the pooled development data from the WMT08, WMT09, and WMT10 translation tasks (10,158 sentences for Czech, 20,258 sentences for all other languages). Target Systems Usable Judgments English 45 20,357 Czech 5 11,242 German 11 6,563 Spanish 9 3,249 French 12 2,967 Table 3: Human ranking judgment data from WMT09 4 Tuning METEOR-NEXT 4.1 Development Data As part of the WMT10 Shared Evaluation Task, data from WMT09 (Callison-Burch et al., 2009), including system output, reference translations, and human judgments, is available for metric development. As metrics are evaluated primarily on their ability to rank system output on the segment level, we select the human ranking judgments from WMT09 as our development set (described in Table 3). 4.2 Tuning Procedure Tuning a version of METEOR-NEXT consists of selecting parameters (α, Q, -y, wi...w,,,) that optimize an objective function for a given language. If multiple paraphrase tables exist for a language, tuning also requires selecting the optimal set of tables to merge. For WMT10, we </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of WMT09. In Proc. of WMT09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Extending the METEOR Machine Translation Metric to the Phrase Level for Improved Correlation with Human Post-Editing Judgments.</title>
<date>2010</date>
<booktitle>In Proc. NAACL/HLT</booktitle>
<contexts>
<context position="1702" citStr="Denkowski and Lavie, 2010" startWordPosition="244" endWordPosition="247">uages. This paper describes a language-independent method for adding paraphrase support to the METEOR-NEXT metric for all WMT10 target languages. Taking advantage of the large parallel corpora released for the translation tasks often accompanying evaluation tasks, we automatically construct paraphrase tables using the pivot method (Bannard and Callison-Burch, 2005). We use the WMT09 human evaluation data to tune versions of METEOR-NEXT with and without paraphrases and report significantly better performance for versions with paraphrase support. 2 The METEOR-NEXT Metric The METEOR-NEXT metric (Denkowski and Lavie, 2010) evaluates a machine translation hypothesis against a reference translation by calculating a similarity score based on an alignment between the two strings. When multiple references are provided, the hypothesis is scored against each and the reference producing the highest score is used. Alignments are formed in two stages: search space construction and alignment selection. For a single hypothesis-reference pair, the space of possible alignments is constructed by identifying all possible word and phrase matches between the strings according to the following matchers: Exact: Words are matched i</context>
</contexts>
<marker>Denkowski, Lavie, 2010</marker>
<rawString>Michael Denkowski and Alon Lavie. 2010. Extending the METEOR Machine Translation Metric to the Phrase Level for Improved Correlation with Human Post-Editing Judgments. In Proc. NAACL/HLT 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments.</title>
<date>2007</date>
<booktitle>In Proc. of WMT07.</booktitle>
<contexts>
<context position="5815" citStr="Lavie and Agarwal, 2007" startWordPosition="932" endWordPosition="936">ility P(f|n1) · P(n2|f). The total probability of n2 paraphrasing n1 is given as the sum over all f: �P(n2|n1) = P(f|n1) · P(n2|f) f The same method can be used to identify foreign paraphrases (f1, f2) given native pivot phrases n. To merge same-language paraphrases extracted from different parallel corpora, we take the mean of the corpus-specific paraphrase probabilities (PC) weighted by the size of the corpora (C) used for paraphrase extraction: P · R α · P + (1 − α) · R P(n2|n1) = EC |C |· PC(n2 |n1) Fmean = EC |C| To account for gaps and differences in word order, a fragmentation penalty (Lavie and Agarwal, 2007) is calculated using the total number of matched words (m) and number of chunks (ch): To improve paraphrase accuracy, we apply multiple filtering techniques during paraphrase extraction. The following are applied to each paraphrase instance (n1, f, n2): Pen = -y · (ch )� 1. Discard paraphrases with very low probabilm ity (P(f|n1) · P(n2|f) &lt; 0.001) The final METEOR-NEXT score is then calculated: Score = (1 − Pen) · Fmean The parameters α, Q, &apos;y, and wi...wn can be tuned to maximize correlation with various types of human judgments. 3 The METEOR Paraphrase Tables To extend support for WMT10 tar</context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2007. METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments. In Proc. of WMT07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Christiane Fellbaum</author>
</authors>
<date>2007</date>
<note>WordNet. http://wordnet.princeton.edu/.</note>
<contexts>
<context position="2596" citStr="Miller and Fellbaum, 2007" startWordPosition="385" endWordPosition="388">ghest score is used. Alignments are formed in two stages: search space construction and alignment selection. For a single hypothesis-reference pair, the space of possible alignments is constructed by identifying all possible word and phrase matches between the strings according to the following matchers: Exact: Words are matched if and only if their surface forms are identical. Stem: Words are stemmed using a languageappropriate Snowball Stemmer (Porter, 2001) and matched if the stems are identical. Synonym: Words are matched if they are both members of a synonym set according to the WordNet (Miller and Fellbaum, 2007) database. Paraphrase: Phrases are matched if they are listed as paraphrases in a paraphrase table. The tables used are described in Section 3. Previously, full support has been limited to English, with French, German, and Spanish having exact and stem match support only, and Czech having exact match support only. Although the exact, stem, and synonym matchers identify word matches while the paraphrase matcher identifies phrase matches, all matches can be generalized to phrase matches with a start position and phrase length in each string. A word occurring less than length positions after a ma</context>
</contexts>
<marker>Miller, Fellbaum, 2007</marker>
<rawString>George Miller and Christiane Fellbaum. 2007. WordNet. http://wordnet.princeton.edu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>Snowball: A language for stemming algorithms.</title>
<date>2001</date>
<note>http://snowball.tartarus.org/texts/.</note>
<contexts>
<context position="2434" citStr="Porter, 2001" startWordPosition="358" endWordPosition="359">an alignment between the two strings. When multiple references are provided, the hypothesis is scored against each and the reference producing the highest score is used. Alignments are formed in two stages: search space construction and alignment selection. For a single hypothesis-reference pair, the space of possible alignments is constructed by identifying all possible word and phrase matches between the strings according to the following matchers: Exact: Words are matched if and only if their surface forms are identical. Stem: Words are stemmed using a languageappropriate Snowball Stemmer (Porter, 2001) and matched if the stems are identical. Synonym: Words are matched if they are both members of a synonym set according to the WordNet (Miller and Fellbaum, 2007) database. Paraphrase: Phrases are matched if they are listed as paraphrases in a paraphrase table. The tables used are described in Section 3. Previously, full support has been limited to English, with French, German, and Spanish having exact and stem match support only, and Czech having exact match support only. Although the exact, stem, and synonym matchers identify word matches while the paraphrase matcher identifies phrase matche</context>
</contexts>
<marker>Porter, 2001</marker>
<rawString>Martin Porter. 2001. Snowball: A language for stemming algorithms. http://snowball.tartarus.org/texts/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Przybocki</author>
<author>K Peterson</author>
<author>S Bronsart</author>
</authors>
<title>Metrics for MAchine TRanslation” Challenge (MetricsMATR08).</title>
<date>2008</date>
<booktitle>Official results of the NIST</booktitle>
<contexts>
<context position="791" citStr="Przybocki et al., 2008" startWordPosition="106" endWordPosition="109">ool of Computer Science Carnegie Mellon University Pittsburgh, PA 15232, USA {mdenkows,alavie}@cs.cmu.edu Abstract This paper describes our submission to the WMT10 Shared Evaluation Task and MetricsMATR10. We present a version of the METEOR-NEXT metric with paraphrase tables for five target languages. We describe the creation of these paraphrase tables and conduct a tuning experiment that demonstrates consistent improvement across all languages over baseline versions of the metric without paraphrase resources. 1 Introduction Workshops such as WMT (Callison-Burch et al., 2009) and MetricsMATR (Przybocki et al., 2008) focus on the need for accurate automatic metrics for evaluating the quality of machine translation (MT) output. While these workshops evaluate metric performance on many target languages, most metrics are limited to English due to the relative lack of lexical resources for other languages. This paper describes a language-independent method for adding paraphrase support to the METEOR-NEXT metric for all WMT10 target languages. Taking advantage of the large parallel corpora released for the translation tasks often accompanying evaluation tasks, we automatically construct paraphrase tables using</context>
</contexts>
<marker>Przybocki, Peterson, Bronsart, 2008</marker>
<rawString>M. Przybocki, K. Peterson, and S Bronsart. 2008. Official results of the NIST 2008 ”Metrics for MAchine TRanslation” Challenge (MetricsMATR08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric.</title>
<date>2009</date>
<booktitle>In Proc. of WMT09.</booktitle>
<contexts>
<context position="10749" citStr="Snover et al., 2009" startWordPosition="1691" endWordPosition="1694">system outputs are deemed equivalent and calculate the proportion of remaining judgments preserved when system outputs are ranked by automatic metric scores. For each target language, tuning is conducted as an exhaustive grid search over metric parameters and possible paraphrase tables, resulting in global optima for both. 5 Experiments To evaluate the impact of our paraphrase tables on metric performance, we tune versions of METEOR-NEXT with and without the paraphrase matchers for each language. For further comparison, we tune a version of METEOR-NEXT using the TERp English paraphrase table (Snover et al., 2009) used by previous versions of the metric. As shown in Table 4, the addition of paraphrases leads to a better tuning point for every target language. The best scoring subset of paraphrase ta341 Language Paraphrases Rank Consistency α Q &apos;y wexact wstem wsyn wpar English none 0.619 0.85 2.35 0.45 1.00 0.80 0.60 – TERp 0.625 0.70 1.40 0.25 1.00 0.80 0.80 0.60 de+es+fr 0.629 0.75 0.60 0.35 1.00 0.80 0.80 0.60 Czech none 0.564 0.95 0.20 0.70 1.00 – – – en 0.574 0.95 2.15 0.35 1.00 – – 0.40 German none 0.550 0.20 0.75 0.25 1.00 0.80 – – en+es 0.576 0.75 0.80 0.90 1.00 0.20 – 0.80 Spanish none 0.586 0</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric. In Proc. of WMT09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C van Rijsbergen</author>
</authors>
<date>1979</date>
<note>Information Retrieval, chapter 7. 2nd edition.</note>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. van Rijsbergen, 1979. Information Retrieval, chapter 7. 2nd edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>