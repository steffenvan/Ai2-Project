<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.862164">
Semiring Parsing
</title>
<author confidence="0.972585">
Joshua Goodman*
</author>
<affiliation confidence="0.714169">
Microsoft Research
</affiliation>
<bodyText confidence="0.997616375">
We synthesize work on parsing algorithms, deductive parsing, and the theory of algebra applied
to formal languages into a general system for describing parsers. Each parser performs abstract
computations using the operations of a semiring. The system allows a single, simple representation
to be used for describing parsers that compute recognition, derivation forests, Viterbi, n-best,
inside values, and other values, simply by substituting the operations of different semirings. We
also show how to use the same representation, interpreted differently, to compute outside values.
The system can be used to describe a wide variety of parsers, including Earley&apos;s algorithm, tree
adjoining grammar parsing, Graham Harrison Ruzzo parsing, and prefix value computation.
</bodyText>
<sectionHeader confidence="0.990266" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999903814814815">
For a given grammar and string, there are many interesting quantities we can compute.
We can determine whether the string is generated by the grammar; we can enumerate
all of the derivations of the string; if the grammar is probabilistic, we can compute the
inside and outside probabilities of components of the string. Traditionally, a different
parser description has been needed to compute each of these values. For some parsers,
such as CKY parsers, all of these algorithms (except for the outside parser) strongly
resemble each other. For other parsers, such as Earley parsers, the algorithms for
computing each value are somewhat different, and a fair amount of work can be
required to construct each one. We present a formalism for describing parsers such
that a single simple description can be used to generate parsers that compute all of
these quantities and others. This will be especially useful for finding parsers for outside
values, and for parsers that can handle general grammars, like Earley-style parsers.
Although our description format is not limited to context-free grammars (CFGs),
we will begin by considering parsers for this common formalism. The input string will
be denoted w1 w2.. wn. We will refer to the complete string as the sentence. A CFG G
is a 4-tuple (N, E, R, S) where N is the set of nonterminals including the start symbol
S. E is the set of terminal symbols, and R is the set of rules, each of the form A —&gt; a
for A c N and a E (N u E)*. We will use the symbol = for immediate derivation and
for its reflexive, transitive closure.
We will illustrate the similarity of parsers for computing different values using
the CKY algorithm as an example. We can write this algorithm in its iterative form
as shown in Figure 1. Here, we explicitly construct a Boolean chart, chart[1..n,L.INI,
1..n + 1]. Element char*, A, j] contains TRUE if and only if A we . . . w1_1. The algo-
rithm consists of a first set of loops to handle the singleton productions, a second set of
loops to handle the binary productions, and a return of the start symbol&apos;s chart entry.
Next, we consider probabilistic grammars, in which we associate a probability
with every rule, P(A —&gt; a). These probabilities can be used to associate a probability
</bodyText>
<note confidence="0.650731666666667">
* One Microsoft Way, Redmond, WA 98052. E-mail: joshuago@microsoft.com
© 1999 Association for Computational Linguistics
Computational Linguistics Volume 25, Number 4
</note>
<equation confidence="0.545658">
boolean chart[1..n, 1 ..IN1,1..n +1] := FALSE;
for s := 1 to n /* start position */
for each rule A w, E R
chart[s, A, s+1] := TRUE;
</equation>
<bodyText confidence="0.9958815">
for 1 := 2 to n /* length, shortest to longest */
for s 1 to n— / +1 /* start position */
</bodyText>
<equation confidence="0.793587315789474">
fort := 1 to 1-1 /* split length */
for each rule A —&gt; BC E R
/* extra TRUE for expository purposes */
chart[s, A, s+1] := chart[s, A, s+1} v
(chart[s, B, s+t] A chart[s+t,C, s+1] A TRUE);
return char*, S, n+1];
Figure 1
CKY recognition algorithm.
float chart[1..n, := 0;
for s := 1 to n /* start position */
for each rule A —&gt; ws E R
chart[s, A, s+1] :=- P(A w5);
for 1 := 2 to n /* length, shortest to longest */
for s := 1 to n-1 +1 /* start position */
fort := 1 to 1-1 /* split length */
for each rule A -4 BC E R
chart[s, A, s+1] := chart[s, A, s+1] +
(chart[s, B, s+t] x chart[s+t,C, s+1] x P(A BC));
return chart[1, S, n+1];
</equation>
<figureCaption confidence="0.917952">
Figure 2
</figureCaption>
<bodyText confidence="0.990710736842105">
CKY inside algorithm.
with a particular derivation, equal to the product of the rule probabilities used in the
derivation, or to associate a probability with a set of derivations, A w,. equal
to the sum of the probabilities of the individual derivations. We call this latter prob-
ability the inside probability of i,A,j. We can rewrite the CKY algorithm to compute
the inside probabilities, as shown in Figure 2 (Baker 1979; Lan i and Young 1990).
Notice how similar the inside algorithm is to the recognition algorithm: essentially,
all that has been done is to substitute + for V, x for A, and P(A w5) and P(A —&gt; BC)
for TRUE. For many parsing algorithms, this, or a similarly simple modification, is all
that is needed to create a probabilistic version of the algorithm. On the other hand, a
simple substitution is not always sufficient. To give a trivial example, if in the CKY
recognition algorithm we had written
chart[s, A, s+1] := chart[s, A, s+1] v chart[s, B, s+t] A chart[s+t, C, s+1];
instead of the less natural
chart[s, A, s +1] := chart[s, A, s +1] V chart[s,B, s+t] A chart[s+t,C, s +1] A TRUE;
larger changes would be necessary to create the inside algorithm.
Besides recognition, four other quantities are commonly computed by parsing
algorithms: derivation forests, Viterbi scores, number of parses, and outside proba-
bilities. The first quantity, a derivation forest, is a data structure that allows one to
</bodyText>
<page confidence="0.994261">
574
</page>
<note confidence="0.616419">
Goodman Semiring Parsing
</note>
<bodyText confidence="0.998462586206897">
efficiently compute the set of legal derivations of the input string. The derivation for-
est is typically found by modifying the recognition algorithm to keep track of &amp;quot;back
pointers&amp;quot; for each cell of how it was produced. The second quantity often computed
is the Viterbi score, the probability of the most probable derivation of the sentence.
This can typically be computed by substituting x for A and max for V. Less commonly
computed is the total number of parses of the sentence, which, like the inside values,
can be computed using multiplication and addition; unlike for the inside values, the
probabilities of the rules are not multiplied into the scores. There is one last commonly
computed quantity, the outside probabilities, which we will describe later, in Section 4.
One of the key points of this paper is that all five of these commonly com-
puted quantities can be described as elements of complete semirings (Kuich 1997).
The relationship between grammars and semirings was discovered by Chomsky and
Schtitzenberger (1963), and for parsing with the CKY algorithm, dates back to Teit-
elbaum (1973). A complete semiring is a set of values over which a multiplicative
operator and a commutative additive operator have been defined, and for which infi-
nite summations are defined. For parsing algorithms satisfying certain conditions, the
multiplicative and additive operations of any complete semiring can be used in place
of A and V. and correct values will be returned. We will give a simple normal form
for describing parsers, then precisely define complete semirings, and the conditions
for correctness.
We now describe our normal form for parsers, which is very similar to that used
by Shieber, Schabes, and Pereira (1995) and by Sikkel (1993). This work can be thought
of as a generalization from their work in the Boolean semiring to semirings in general.
In most parsers, there is at least one chart of some form. In our normal form, we
will use a corresponding, equivalent concept, items. Rather than, for instance, a chart
element chart [i, A, j], we will use an item [i, A,]]. Furthermore, rather than use explicit,
procedural descriptions, such as
chart[s, A, s +1] := chart[s, A, s +1] v chart[s,B, s+ t] A chart[s+t, C, 9+1] A TRUE
we will use inference rules such as
</bodyText>
<equation confidence="0.892212">
R(A BC) [1,B, k] [k, C, j]
[i, A,]]
</equation>
<bodyText confidence="0.99460725">
The meaning of an inference rule is that if the top line is all true, then we can conclude
the bottom line. For instance, this example inference rule can be read as saying that if
A BC and B w, wk_i and C wk w/_i, then A . •
The general form for an inference rule will be
</bodyText>
<subsubsectionHeader confidence="0.531799">
Al • ••Ak
</subsubsectionHeader>
<bodyText confidence="0.999888777777778">
where if the conditions A1 Ak are all true, then we infer that B is also true. The A,
can be either items, or (in an extension of the usual convention for inference rules)
rules, such as R(A BC). We write R(A BC) rather than A BC to indicate that
we could be interested in a value associated with the rule, such as the probability of
the rule if we were computing inside probabilities. If an A, is in the form R(. . .), we
call it a rule. All of the A, must be rules or items; when we wish to refer to both rules
and items, we use the word terms.
We now give an example of an item-based description, and its semantics. Figure 3
gives a description of a CKY-style parser. For this example, we will use the inside
</bodyText>
<page confidence="0.997277">
575
</page>
<table confidence="0.9554397">
Computational Linguistics Volume 25, Number 4
Item form: Unary
[i, A, j] Binary
Goal:
[1, S, 72+1]
Rules:
R(A
[i,A,i+1]
R(A BC) [i, B , , C, j]
[i, A, j]
</table>
<figureCaption confidence="0.710435">
Figure 3
Item-based description of a CKY parser.
</figureCaption>
<bodyText confidence="0.9566275">
semiring, whose additive operator is addition and whose multiplicative operator is
multiplication. We use the input string xxx to the following grammar:
</bodyText>
<equation confidence="0.925458166666667">
—&gt; XX 1.0
X XX 0.2 (1)
X —+ x 0.8
Our first step is to use the unary rule,
R(A wi)
[i, A, i+ 1]
</equation>
<bodyText confidence="0.9997195">
The effect of the unary rule will exactly parallel the first set of loops in the CKY inside
algorithm. We will instantiate the free variables of the unary rule in every possible
way. For instance, we instantiate the free variable i with the value 1, and the free
variable A with the nontermirtal X. Since w1 = x, the instantiated rule is then
</bodyText>
<equation confidence="0.7958195">
R(X x)
[1, X, 2]
</equation>
<bodyText confidence="0.999925666666667">
Because the value of the top line of the instantiated unary rule, R(X x), has value
0.8, we deduce that the bottom line, [1, X, 2], has value 0.8. We instantiate the rule in
two other ways, and compute the following chart values:
</bodyText>
<equation confidence="0.891058833333333">
[1, X, 2] = 0.8
[2, X, 3] =- 0.8
[3, X, 4] = 0.8
Next, we will use the binary rule,
R(A —4 BC) [i, B,k] [k, C,
[i, A, j]
</equation>
<bodyText confidence="0.9745645">
The effect of the binary rule will parallel the second set of loops for the CKY inside
algorithm. Consider the instantiation i -= 1, k = 2, j = 3, A = X, B = X, C = X,
</bodyText>
<equation confidence="0.7247255">
R(X XX) [1, X, 2] [2, X, 3]
[1, X, 3]
</equation>
<page confidence="0.994682">
576
</page>
<note confidence="0.615992">
Goodman Semiring Parsing
</note>
<bodyText confidence="0.998799">
We use the multiplicative operator of the semiring of interest to multiply together the
values of the top line, deducing that [I, X, 3] = 0.2 x 0.8 x 0.8 = 0.128. Similarly,
</bodyText>
<equation confidence="0.99971775">
[1, X, 3] -= 0.128
[2, X, 4] = 0.128
[1, S, 3] = 0.64
[2, S, 4] = 0.64
</equation>
<bodyText confidence="0.714124">
There are two more ways to instantiate the conditions of the binary rule:
</bodyText>
<equation confidence="0.980329">
R(S XX) [1, X, 2] [2, X, 4]
[1, S, 4]
R(S XX) [I, X, 3] [3, X, 4]
[1, S, 4]
</equation>
<bodyText confidence="0.9999678">
The first has the value 1 x 0.8 x 0.128 = 0.1024, and the second also has the value
0.1024. When there is more than one way to derive a value for an item, we use the
additive operator of the semiring to sum them up. Thus, [1, S. 4] = 0.2048. Since [1, S, 4]
is the goal item for the CKY parser, we know that the inside value for xxx is 0.2048.
The goal item exactly parallels the return statement of the CKY inside algorithm.
</bodyText>
<subsectionHeader confidence="0.99928">
1.1 Earley Parsing
</subsectionHeader>
<bodyText confidence="0.999992">
Many parsers are much more complicated than the CKY parser, and we will need to
expand our notation a bit to describe them. Earley&apos;s algorithm (Earley 1970) exhibits
most of the complexities we wish to discuss. Earley&apos;s algorithm is often described as
a bottom-up parser with top-down filtering. In a probabilistic framework, the bottom-
up sections compute probabilities, while the top-down filtering nonprobabilistically
removes items that cannot be derived. To capture these differences, we expand our
notation for deduction rules, to the following:
</bodyText>
<equation confidence="0.9984305">
Ai • • • Ak
Ci • • • Ci
</equation>
<bodyText confidence="0.969035777777778">
Ci • • CI are side conditions, interpreted nonprobabilistically, while A1 • Ak are main
conditions with values in whichever semiring we are using.&apos; While the values of all
main conditions are multiplied together to yield the value for the item under the line,
the side conditions are interpreted in a Boolean manner: if all of them are nonzero,
the rule can be used, but if any of them are zero, it cannot be. Other than for checking
whether they are zero or nonzero, their values are ignored.
Figure 4 gives an item-based description of Earley&apos;s parser. We assume the addition
of a distinguished nonterminal S&apos; with a single rule S&apos; S. An item of the form
[i, A —&gt; a /3,j] asserts that A =&gt; a/3 4 w, . . .
</bodyText>
<footnote confidence="0.997795333333333">
1 The side conditions may depend on any purely local information—the values of A1 . ,B, or
C1 ... CI, as well as constant global functions, such as R(X) sin(Y) (assuming here X and Y are
variables in the A, B, C). The side conditions usually cannot depend on any contextual information,
such as the grandfather of A1, which would not be well defined, since there might be many derivations
of Al. Of course, one could encode the grandfather of A1 as a variable in the item A1, and then have a
dependency on that variable. This would guarantee that the context was unique and well defined.
</footnote>
<page confidence="0.977195">
577
</page>
<table confidence="0.647953882352941">
Computational Linguistics Volume 25, Number 4
Item form:
[i, A --+ a • /3,31
Goal:
[1,S&apos; —&gt; S • ,n+1]
Rules:
Initialization
Scanning
Prediction
Completion
[1,S&apos;-*• S, 1.]
[i, A a • w1/3, j]
[i, A awi • 0, j+1J
R(B [i, A -&gt; a •
1j, B •701
[i, A a • BO, ki [k, B • , ji
Ei, A aB • 0, j.1
</table>
<figureCaption confidence="0.991904">
Figure 4
Item-based description of Earley parser.
</figureCaption>
<bodyText confidence="0.980217">
The prediction rule includes a side condition, making it a good example. The
rule is:
</bodyText>
<equation confidence="0.993011">
R(B -y) [i ,A a • Bo,i1
B •
</equation>
<bodyText confidence="0.9902626875">
Through the prediction rule, Earley&apos;s algorithm guarantees that an item of the form
[j,B • -y, can only be produced if S w1 wi_iBS for some b; this top-down
filtering leads to significantly more efficient parsing for some grammars than the CKY
algorithm. The prediction rule combines side and main conditions. The side condi-
tion, [i, A —&gt; a • BO, j], provides the top-down filtering, ensuring that only items that
might be used later by the completion rule can be predicted, while the main con-
dition, R(B —&gt; 7), provides the probability of the relevant rule. The side condition
is interpreted in a Boolean fashion, while the main condition&apos;s actual probability is
used.
Unlike the CKY algorithm, Barley&apos;s algorithm can handle grammars with ep-
silon (e), unary, and n-ary branching rules. In some cases, this can significantly com-
plicate parsing. For instance, given unary rules A —+ B and B A, a cycle ex-
ists. This kind of cycle may allow an infinite number of different derivations, re-
quiring an infinite summation to compute the inside probabilities. The ability of
item-based parsers to handle these infinite loops with relative ease is a major
attraction.
</bodyText>
<subsectionHeader confidence="0.653355">
1.2 Overview
</subsectionHeader>
<bodyText confidence="0.9938645">
This paper will simplify the development of new parsers in three important ways.
First, it will simplify specification of parsers: the item-based description is simpler
than a procedural description. Second, it will make it easier to generalize parsers
across tasks: a single item-based description can be used to compute values for a
variety of applications, simply by changing semirings. This will be especially ad-
vantageous for parsers that can handle loops resulting from rules like A A and
computations resulting from € productions, both of which typically lead to infinite
sums. In these cases, the procedure for computing an infinite sum differs from semi-
</bodyText>
<page confidence="0.994139">
578
</page>
<subsectionHeader confidence="0.581228">
Goodman Semiring Parsing
</subsectionHeader>
<bodyText confidence="0.9999116">
ring to semiring, and the fact that we can specify that a parser computes an in-
finite sum separately from its method of computing that sum will be very help-
ful. The third use of these techniques is for computing outside probabilities, val-
ues related to the inside probabilities that we will define later. Unlike the other
quantities we wish to compute, outside probabilities cannot be computed by sim-
ply substituting a different semiring into either an iterative or item-based descrip-
tion. Instead, we will show how to compute the outside probabilities using a mod-
ified interpreter of the same item-based description used for computing the other
values.
In the next section, we describe the basics of semiring parsing. In Section 3, we
derive formulas for computing most of the values in semiring parsers, except out-
side values, and then in Section 4, show how to compute outside values as well. hi
Section 5, we give an algorithm for interpreting an item-based description, followed
in Section 6 by examples of using semiring parsers to solve a variety of problems.
Section 7 discusses previous work, and Section 8 concludes the paper.
</bodyText>
<sectionHeader confidence="0.50542" genericHeader="method">
2. Semiring Parsing
</sectionHeader>
<bodyText confidence="0.99922275">
In this section we first describe the inputs to a semiring parser: a semiring, an item-
based description, and a grammar. Next, we give the conditions under which a semi-
ring parser gives correct results. At the end of this section we discuss three especially
complicated and interesting semirings.
</bodyText>
<subsectionHeader confidence="0.999552">
2.1 Semiring
</subsectionHeader>
<bodyText confidence="0.993529954545455">
In this subsection, we define and discuss semirings (see Kuich [19971 for an intro-
duction). A semiring has two operations, ED and 0, that intuitively have most (but
not necessarily all) of the properties of the conventional + and x operations on the
positive integers. In particular, we require the following properties: 0 is associative
and commutative; 0 is associative and distributes over ED. If 0 is commutative, we
will say that the semiring is commutative. We assume an additive identity element,
which we write as 0, and a multiplicative identity element, which we write as 1. Both
addition and multiplication can be defined over finite sets of elements; if the set is
empty, then the value is the respective identity element, 0 or 1. We also assume that
x = x = 0 for all x. In other words, a semiring is just like a ring, except that the
additive operator need not have an inverse. We will write (A, ED, 0, 0, 1) to indicate a
semiring over the set A with additive operator 0, multiplicative operator 0, additive
identity 0, and multiplicative identity 1.
For parsers with loops, i.e., those in which an item can be used to derive itself,
we will also require that sums of an infinite number of elements be well defined. In
particular, we will require that the semirings be complete (Kuich 1997, 611). This means
that sums of an infinite number of elements should be associative and commutative,
just like finite sums, and that multiplication should distribute over infinite sums, just
as it does over finite ones. All of the semirings we will deal with in this paper are
complete.2
All of the semirings we discuss here are also co-continuous. Intuitively, this means
that if any partial sum of an infinite sequence is less than or equal to some value,
</bodyText>
<footnote confidence="0.998488">
2 Completeness is a somewhat stronger condition than we really need; we could, instead, require that
limits be appropriately defined for those infinite sums that occur while parsing, but this weaker
condition is more complicated to describe precisely.
</footnote>
<page confidence="0.996205">
579
</page>
<figure confidence="0.74459825">
Computational Linguistics Volume 25, Number 4
boolean
inside
Viterbi
counting
derivation forest
Viterbi-derivation
Viterbi-n-best
</figure>
<equation confidence="0.9162085">
({TRUE, FALSE}, V, A, FALSE, TRUE)
(Rg°, +, x , 0, 1)
(RI), max, x , 0, 1)
(N°,+, x , 0, 1)
(2E, U, •0, {O})
(R ;:i x 2E, max, x , (0,0), (1, {OM
vit Vit
({topn(X)IX E
</equation>
<bodyText confidence="0.928004285714286">
recognition
string probability
prob. of best derivation
number of derivations
set of derivations
best derivation
best n derivations
</bodyText>
<equation confidence="0.8406402">
211q) x El , max, x , 0,
vit-n Vit-n
{ (1, { }) })
Figure 5
Semirings used: (A, q, 0,0, 1).
</equation>
<bodyText confidence="0.998362555555556">
then the infinite sum is also less than or equal to that value.3 This important property
makes it easy to compute, or at least approximate, infinite sums.
There will be several especially useful semirings in this paper, which are defined
in Figure 5. We will write rb, to indicate the set of real numbers from a to b inclusive,
with similar notation for the natural numbers, N. We will write E to indicate the
set of all derivations in some canonical form, and 2E to indicate the set of all sets
of derivations in canonical form. There are three derivation semirings: the derivation
forest semiring, the Viterbi-derivation semiring, and the Viterbi-n-best semiring. The
operators used in the derivation semirings (., max, x ,max, and x) will be described
</bodyText>
<subsectionHeader confidence="0.715818">
Vit Vit Vit-n Vit-n
</subsectionHeader>
<bodyText confidence="0.999598928571429">
later, in Section 2.5.
The inside semiring includes all nonnegative real numbers, to be closed under
addition, and includes infinity to be closed under infinite sums, while the Viterbi
semiring contains only numbers up to 1, since under max this still leads to closure.
The three derivation forest semirings can be used to find especially important val-
ues: the derivation forest semiring computes all derivations of a sentence; the Viterbi-
derivation semiring computes the most probable derivation; and the Viterbi-n-best
semiring computes the n most probable derivations. A derivation is simply a list
of rules from the grammar. From a derivation, a parse tree can be derived, so the
derivation forest semiring is analogous to conventional parse forests. Unlike the other
semirings, all three of these semirings are noncommutative. The additive operation
of these semirings is essentially union or maximum, while the multiplicative oper-
ation is essentially concatenation. These semirings are described in more detail in
Section 2.5.
</bodyText>
<subsectionHeader confidence="0.998026">
2.2 Item-based Description
</subsectionHeader>
<bodyText confidence="0.999793428571428">
A semiring parser requires an item-based description of the parsing algorithm, in the
form given earlier. So far, we have skipped one important detail of semiring parsing. In
a simple recognition system, as used in deduction systems, all that matters is whether
an item can be deduced or not. Thus, in these simple systems, the order of processing
items is relatively unimportant, as long as some simple constraints are met. On the
other hand, for a semiring such as the inside semiring, there are important ordering
constraints: we cannot compute the inside value of an item until the inside values of
</bodyText>
<footnote confidence="0.9889862">
3 To be more precise, all semirings we discuss here are naturally ordered, meaning that we can define a
partial ordering, C, such that x L y if and only if there exists z such that x z = y. We call a naturally
ordered complete semiring co-continuous (Kuich 1997, 612) if for any sequence x2,... and for any
o&lt;i&lt;n
constant y, if for all n, pin E y, then ER , y.
</footnote>
<page confidence="0.990973">
580
</page>
<subsectionHeader confidence="0.575651">
Goodman Semiring Parsing
</subsectionHeader>
<bodyText confidence="0.986826">
all of its children have been computed.
Thus, we need to impose an ordering on the items, in such a way that no item
precedes any item on which it depends. We will assign each item x to a &amp;quot;bucket&amp;quot;
B, writing bucket(x) = B and saying that item x is associated with B. We order the
buckets in such a way that if item y depends on item x, then bucket(x) &lt; bucket(y). For
some pairs of items, it may be that both depend, directly or indirectly, on each other;
we associate these items with special &amp;quot;looping&amp;quot; buckets, whose values may require
infinite sums to compute. We will also call a bucket looping if an item associated with
it depends on itself.
One way to achieve a bucketing with the required ordering constraints (suggested
by Fernando Pereira) is to create a graph of the dependencies, with a node for each
item, and an edge from each item x to each item b that depends on it. We then
separate the graph into its strongly connected components (maximal sets of nodes all
reachable from each other), and perform a topological sort. Items forming singleton
strongly connected components are associated with their own buckets; items forming
nonsingleton strongly connected components are associated with the same looping
bucket. See also Section 5.
Later, when we discuss algorithms for interpreting an item-based description, we
will need another concept. Of all the items associated with a bucket B, we will be
able to find derivations for only a subset. If we can derive an item x associated with
bucket B, we write x E B, and say that item x is in bucket B. For example, the goal
item of a parser will almost always be associated with the last bucket; if the sentence
is grammatical, the goal item will be in the last bucket, and if it is not grammatical, it
will not be.
It will be useful to assume that there is a single, variable-free goal item, and that
this goal item does not occur as a condition for any rules. We could always add a
[old-goal]
new goal item [goal] and a rule where [old-goal] is[goal] the goal in the original
description.
</bodyText>
<subsectionHeader confidence="0.999837">
2.3 The Grammar
</subsectionHeader>
<bodyText confidence="0.999766285714286">
A semiring parser also requires a grammar as input. We will need a list of rules in the
grammar, and a function, R(rule), that gives the value for each rule in the grammar.
This latter function will be semiring-specific. For instance, for computing the inside
and Viterbi probabilities, the value of a grammar rule is just the conditional probability
of that rule, or 0 if it is not in the grammar. For the Boolean semiring, the value is
TRUE if the rule is in the grammar, FALSE otherwise. R(rule) replaces the set of rules
R of a conventional grammar description; a rule is in the grammar if R(rule) 0.
</bodyText>
<subsectionHeader confidence="0.998042">
2.4 Conditions for Correct Processing
</subsectionHeader>
<bodyText confidence="0.975031">
We will say that a semiring parser works correctly if, for any grammar, input, and
semiring, the value of the input according to the grammar equals the value of the input
using the parser. In this subsection, we will define the value of an input according
to the grammar, define the value of an input using the parser, and give a sufficient
condition for a semiring parser to work correctly. From this point onwards, unless we
specifically mention otherwise, we will assume that some fixed semiring, item-based
description, and grammar have been given, without specifically mentioning which
ones.
2.4.1 Value According to Grammar. Consider a derivation E, consisting of grammar
rules el, e2, . . ,em. We define the value of the derivation according to the grammar to
</bodyText>
<page confidence="0.978137">
581
</page>
<note confidence="0.380851">
Computational Linguistics Volume 25, Number 4
</note>
<bodyText confidence="0.87851">
be simply the product (in the semiring) of the values of the rules used in E:
</bodyText>
<equation confidence="0.903646">
VG(E) = R(e,)
1=1
</equation>
<bodyText confidence="0.7638795">
Then we can define the value of a sentence that can be derived using grammar deriva-
tions El, E2, , Ek to be:
</bodyText>
<equation confidence="0.996235">
VG = (DV G(Ei)
j=1
</equation>
<bodyText confidence="0.998253857142857">
where k is potentially infinite. In other words, the value of the sentence according to
the grammar is the sum of the values of all derivations. We will assume that in each
grammar formalism there is some way to define derivations uniquely; for instance, in
CFGs, one way would be using left-most derivations. For simplicity, we will simply
refer to derivations, rather than, for example, left-most derivations, since we are never
interested in nonunique derivations.
A short example will help clarify. We consider the following grammar:
</bodyText>
<figure confidence="0.2800947">
S —&gt; AA R(S —+ AA)
A --÷ AA R(A —&gt; AA) (2)
A —&gt; a R(A a)
and the input string aaa. There are two grammar derivations, the first of which
S—.AA A—qt
is S = AA =- AAA aAA = aaA = aaa, which has value R(S —&gt; AA) 0 R(A
AA) 0 R(A —&gt; a) 0 R(A —&gt; a) 0 R(A —&gt; a). Notice that the rules in the value are
the same rules in the same order as in the derivation. The other grammar deriva-
S-4 AA aA
A Aa A 11A A&gt;a A&apos;a
</figure>
<bodyText confidence="0.83466425">
tion is S • aAA AGIA
aaa, which has value R(S AA) 0 R(A
a) 0 R(A AA) 0 R(A a) 0 R(A ---+ a). The value of the sentence is the sum of the
values of the two derivations,
</bodyText>
<equation confidence="0.982974">
[R(S —› AA) 0 R(A —&gt; AA) 0 R(A a) R(A a) 0 R(A a)] ED
[R(s AA) 0 R(A —&gt; a) 0 R(A —› AA) R(A a) 0 R(A —&gt; a)]
</equation>
<bodyText confidence="0.926959416666667">
2.4.2 Item Derivations. Next, we define item derivations, i.e., derivations using the
item-based description of the parser. We define item derivation in such a way that
for a correct parser description, there is exactly one item derivation for each grammar
derivation. The value of a sentence using the parser is the sum of the value of all
item derivations of the goal item. Just as with grammar derivations, individual item
derivations are finite, but there may be infinitely many item or grammar derivations
of a sentence.
We say that al &apos; b&apos; • ak c1 ci is an instantiation of deduction rule A1 &apos; • &apos;
—1 . .
whenever the first expression is a variable-free instance of the second; that is, the first
expression is the result of consistently substituting constant terms for each variable in
the second. Now, we can define an item derivation tree. Intuitively, an item derivation
</bodyText>
<page confidence="0.973585">
582
</page>
<figure confidence="0.996998428571428">
Goodman Semiring Parsing
S--+AA A-4AA A-*a A-+a A.-■a
S AA AAA aAA aaA aaa
Grammar Derivation
R(S —&gt; AA)
R(A -4 AA) R(A-4 a)
R(A —&gt; a) R(A —&gt; a)
Grammar Derivation Tree
, S, 41
R(A 4 AA) [1, A, 2} [2, A, 3]
R_(A —&gt; a.) R(A -3 a)
Item Derivation Tree
R(S -4 AA) 0 R(A -4 AA) R(A —&gt; a) R(A a) 0 R(A -4 a)
Derivation Value
</figure>
<figureCaption confidence="0.995529">
Figure 6
</figureCaption>
<bodyText confidence="0.975200470588235">
Grammar derivation, grammar derivation tree, item derivation tree, and derivation value.
tree for x just gives a way of deducing x from the grammar rules. We define an
item derivation tree recursively. The base case is rules of the grammar: (r) is an item
derivation tree, where r is a rule of the grammar. Also, if D a„ . . . , Da„ , Da, are
derivation trees headed by al ak, ci respectively, and if al &apos; b&amp;quot; ak c1 cj is the
instantiation of a deduction rule, then (b: Da„ . , Dak) is also a derivation tree. Notice
that the D„ . . . Dc, do not occur in this tree: they are side conditions, and although their
existence is required to prove that c1 ci could be derived, they do not contribute to
the value of the tree. We will write al • • • ak to indicate that there is an item deri-
vation tree of the form (b: Da„ . . . ,Dak). As mentioned in Section 2.2, we will write
x E B if bucket(x) --= B and there is an item derivation tree for x.
We can continue the example of parsing aaa, now using the item-based CKY parser
of Figure 3. There are two item derivation trees for the goal item; in Figure 6, we give
the first as an example, displaying it as a tree, rather than with angle bracket notation,
for simplicity.
Notice that an item derivation is a tree, not a directed graph. Thus, an item sub-
derivation could occur multiple times in a given item derivation. This means that
</bodyText>
<page confidence="0.986456">
583
</page>
<note confidence="0.421787">
Computational Linguistics Volume 25, Number 4
</note>
<bodyText confidence="0.990992">
we can have a one-to-one correspondence between item derivations and grammar
derivations; loops in the grammar lead to an infinite number of grammar derivations,
and an infinite number of corresponding item derivations.
</bodyText>
<figure confidence="0.920883666666667">
A grammar including rules such as
S —AAA
A —› B
A —&gt; a
B ---&gt; A
B --&gt; E
</figure>
<bodyText confidence="0.780903">
would allow derivations such as S AAA = BAA AA = BA A = B €.
We would include the exact same item derivation showing A z B E three times.
Similarly, for a derivation such asA-B-i4-13-A-a, we would have a
corresponding item derivation tree that included multiple uses of the A —&gt; B and
B —&gt; A rules.
2.4.3 Value of Item Derivation. The value of an item derivation D, V (D), is the product
of the value of its rules, R(r), in the same order that they appear in the item derivation
tree. Since rules occur only in the leaves of item derivation trees, the order is precisely
determined. For an item derivation tree D with rule values d1, d2,. , d1 as its leaves,
</bodyText>
<equation confidence="0.997231">
V (D) = R(di)
</equation>
<bodyText confidence="0.6503702">
Alternatively, we can write this equation recursively as
R(D) if D is a rule
V(D)= {oki 1 V(D) if
v _ (b: D1, Dk)
Continuing our example, the value of the item derivation tree of Figure 6 is
</bodyText>
<equation confidence="0.748005">
R(S —&gt; AA) R(A —&gt; a) R(A AA) R(A —&gt; a) R(A —&gt; a)
</equation>
<bodyText confidence="0.99061525">
the same as the value of the first grammar derivation.
Let inner(x) represent the set of all item derivation trees headed by an item x. Then
the value of x is the sum of all the values of all item derivation trees headed by x.
Formally,
</bodyText>
<equation confidence="0.9809065">
V(x) = ISB V (D)
D Einner (x)
</equation>
<bodyText confidence="0.959208857142857">
The value of a sentence is just the value of the goal item, V(goal).
2.4.4 Iso-valued Derivations. In certain cases, a particular grammar derivation and a
particular item derivation will have the same value for any semiring and any rule value
function R. In this case, we say that the two derivations are iso-valued. In particular, if
and only if the same rules occur in the same order in both derivations, then their values
will always be the same, and they are iso-valued. In Figure 6, the grammar derivation
and item derivation meet this condition. In some cases, a grammar derivation and an
</bodyText>
<page confidence="0.984127">
584
</page>
<subsectionHeader confidence="0.388574">
Goodman Semiring Parsing
</subsectionHeader>
<bodyText confidence="0.999962">
item derivation will have the same value for any commutative semiring and any rule
value function. In this case, we say that the derivations are commutatively iso-valued.
Finishing our example, the value of the goal item given our example sentence is
just the sum of the values of the two item-based derivations,
</bodyText>
<equation confidence="0.877221">
[R(S AA) R(A —&gt; AA) 0 R(A —&gt; a) ® R(A —&gt; a) ® R(A a)] ED
[R(S —&gt; AA) R(A —&gt; a) R(A AA) R(A —&gt; a) R(A a)]
This value is the same as the value of the sentence according to the grammar.
</equation>
<bodyText confidence="0.5195605">
2.4.5 Conditions for Correctness. We can now specify the conditions for an item-based
description to be correct.
</bodyText>
<subsectionHeader confidence="0.40749">
Theorem 1
</subsectionHeader>
<bodyText confidence="0.999965833333333">
Given an item-based description I, if for every grammar G, there exists a one-to-one
correspondence between the item derivations using I and the grammar derivations,
and the corresponding derivations are iso-valued, then for every complete semiring,
the value of a given input w1 wn is the same according to the grammar as the value
of the goal item. (If the semiring is commutative, then the corresponding derivations
need only be commutatively iso-valued.)
</bodyText>
<subsectionHeader confidence="0.899582">
Proof
</subsectionHeader>
<bodyText confidence="0.999973">
The proof is very simple; essentially, each term in each sum occurs in the other. By
hypothesis, for a given input, there are grammar derivations E1 Ek (for 0 &lt; k &lt; oo)
and corresponding item derivation trees D1 . . . Dk of the goal item. Since corresponding
items are iso-valued, for all i, V(E) = V(DO. (If the semiring is commutative, then
since the items are commutatively iso-valued, it is still the case that for all i, V(E1) =
V (D,).) Now, since the value of the string according to the grammar is just @, V(E) =
ED, V(D), and the value of the goal item is el, V (D,), the value of the string according
to the grammar equals the value of the goal item. 0
There is one additional condition for an item-based description to be usable in
practice, which is that there be only a finite number of derivable items for a given
input sentence; there may, however, be an infinite number of derivations of any item.
</bodyText>
<subsectionHeader confidence="0.998829">
2.5 The Derivation Semirings
</subsectionHeader>
<bodyText confidence="0.9987738">
All of the semirings we use should be familiar, except for the derivation semirings,
which we now describe. These semirings, unlike the other semirings described in
Figure 5, are not commutative under their multiplicative operator, concatenation.
In many parsers, it is conventional to compute parse forests: compact represen-
tations of the set of trees consistent with the input. We will use a related concept,
derivation forests, a compact representation of the set of derivations consistent with
the input, which corresponds to the parse forest for CFGs, but is easily extended to
other formalisms.
Often, we will not be interested in the set of all derivations, but only in the most
probable derivation. The Viterbi-derivation semiring computes this value. Alterna-
tively, we might want the n best derivations, which would be useful if the output of
the parser were passed to another stage, such as semantic disambiguation; this value
is computed by the Viterbi-n-best derivation semiring.
Notice that each of the derivation semirings can also be used to create trans-
ducers. That is, we simply associate strings rather than grammar rules with each
</bodyText>
<page confidence="0.990097">
585
</page>
<note confidence="0.632842">
Computational Linguistics Volume 25, Number 4
</note>
<bodyText confidence="0.99849">
rule value. Instead of grammar rule concatenation, we perform string concatena-
tion. The derivation semiring then corresponds to nondeterministic transductions;
the Viterbi semiring corresponds to a weighted or probabilistic transducer; and the
Viterbi-n-best semiring could be used to get n-best lists from probabilistic transduc-
ers.
</bodyText>
<subsubsectionHeader confidence="0.65322">
2.5.1 Derivation Forest. The derivation forest semiring consists of sets of derivations,
</subsubsectionHeader>
<bodyText confidence="0.999832083333334">
where a derivation is a list of rules of the grammar.&apos; Sets containing one rule, such as
{ (X —&gt; YZ)} for a CFG, constitute the primitive elements of the semiring. The additive
operator U produces a union of derivations, and the multiplicative operator produces
the concatenation, one derivation concatenated with the next. The concatenation op-
eration 0 is defined on both derivations and sets of derivations; when applied to a
set of derivations, it produces the set of pairwise concatenations. The additive identity
is simply the empty set, 0: union with the empty set is an identity operation. The
multiplicative identity is the set containing the empty derivation, { }: concatenation
with the empty derivation is an identity operation. Derivations need not be complete.
For instance, for CFGs, {(X —&gt; YZ, Y —&gt; y)} is a valid element, as is {(Y —&gt; y, X —&gt; x)}.
In fact, { (X —&gt; A, B —&gt; b)} is a valid element, although it could not occur in a valid
grammar derivation, or in a correctly functioning parser. An example of concatenation
</bodyText>
<construct confidence="0.649597">
of sets is {(A —&gt; a), (B —&gt; b)} {(C —&gt; c), (D —&gt; d)} = {(A —&gt; a, C c), (A —&gt; a, D —+
—&gt; b, C —&gt; c), (B —&gt; b, D —&gt; d)} .
</construct>
<bodyText confidence="0.999519">
Potentially, derivation forests are sets of infinitely many items. However, it is still
possible to store them using finite-sized representations. Elsewhere (Goodman 1998),
we show how to implement derivation forests efficiently, using pointers, in a manner
analogous to the typical implementation of parse forests, and also similar to the work
of Billot and Lang (1989). Using these techniques, both union and concatenation can
be implemented in constant time, and even infinite unions will be reasonably efficient.
</bodyText>
<subsubsectionHeader confidence="0.63135">
2.5.2 Viterbi-derivation Semiring. The Viterbi-derivation semiring computes the most
</subsubsectionHeader>
<bodyText confidence="0.999258666666667">
probable derivation of the sentence, given a probabilistic grammar. Elements of this
semiring are a pair, a real number v and a derivation forest E, i.e., the set of derivations
with score v. We define max, the additive operator, as
</bodyText>
<equation confidence="0.9427535">
Vit
(v , E) if v &gt; w
max( (v, E), (w, D)) = (w, D) if v &lt; w
Vit (v, E u D) if v = w
</equation>
<bodyText confidence="0.987953272727273">
In typical practical Viterbi parsers, when two derivations have the same value, one of
the derivations is arbitrarily chosen. In practice, this is usually a fine solution, and one
that could be used in a real-world implementation of the ideas in this paper, but from
a theoretical viewpoint, the arbitrary choice destroys the associative property of the
additive operator, max. To preserve associativity, we keep derivation forests of all ele-
ments that tie for best.
The definition for max is only defined for two elements. Since the operator is
associative, it is clear how to define max for any finite number of elements, but we also
v
need infinite summations to be definited. We use the supremum, sup: the supremum
of a set is the smallest value at least as large as all elements of the set; that is, it is a
</bodyText>
<footnote confidence="0.925763">
4 This semiring is equivalent to one well known to mathematicians, the polynomials over
noncommuting variables.
</footnote>
<page confidence="0.986714">
586
</page>
<note confidence="0.480662">
Goodman Semiring Parsirtg
</note>
<bodyText confidence="0.5474635">
maximum that is defined in the infinite case. We can now define max for the case of
infinite sums. Let Vit
</bodyText>
<equation confidence="0.954637125">
w =- sup V
(v,E)EX
D = {El(w,E) E
Then max X = (w, D. D is potentially empty, but this causes us no problems in
vit
theory, and will not occur in practice. We define x as
(v, E) x (w,D) = (v x w,E • D)
Vit
</equation>
<bodyText confidence="0.995505">
where E • D represents the concatenation of the two derivation forests.
</bodyText>
<subsubsectionHeader confidence="0.88011">
2.5.3 Viterbi-n-best Semiring. The last kind of derivation semiring is the Viterbi-n-
</subsubsectionHeader>
<bodyText confidence="0.999829571428571">
best semiring, which is used for constructing n-best lists. Intuitively, the value of a
string using this semiring will be the n most likely derivations of that string (unless
there are fewer than n total derivations.) In practice, this is actually how a Viterbi-n-best
semiring would typically be implemented. From a theoretical viewpoint, however, this
implementation is inadequate, since we must also define infinite sums and be sure that
the distributive property holds. Elsewhere (Goodman 1998), we give a mathematically
precise definition of the semiring that handles these cases.
</bodyText>
<sectionHeader confidence="0.74971" genericHeader="method">
3. Efficient Computation of Item Values
</sectionHeader>
<bodyText confidence="0.9993635">
Recall that the value of an item x is just V(x) = @DEinner(x)11(D), the sum of the
values of all derivation trees headed by x. This definition may require summing over
exponentially many or even infinitely many terms. In this section, we give relatively
efficient formulas for computing the values of items. There are three cases that must
be handled. First is the base case, when x is a rule. In this case, inner(x) is trivially
{(x)}, the set containing the single derivation tree x. Thus, V(x) GDEmner(x) 17(D) =
</bodyText>
<equation confidence="0.966254">
EDDE{(x)} V(D) = V((x)) = R(x)
</equation>
<bodyText confidence="0.999949285714286">
The second and third cases occur when x is an item. Recall that each item is asso-
ciated with a bucket, and that the buckets are ordered. Each item x is either associated
with a nonlooping bucket, in which case its value depends only on the values of items
in earlier buckets; or with a looping bucket, in which case its value depends poten-
tially on the values of other items in the same bucket. In the case when the item is
associated with a nonlooping bucket, if we compute items in the same order as their
buckets, we can assume that the values of items al ... ak contributing to the value of
item b are known. We give a formula for computing the value of item b that depends
only on the values of items in earlier buckets.
For the final case, in which x is associated with a looping bucket, infinite loops
may occur, when the value of two items in the same bucket are mutually dependent,
or an item depends on its own value. These infinite loops may require computation
of infinite sums. Still, we can express these infinite sums in a relatively simple form,
allowing them to be efficiently computed or approximated.
</bodyText>
<page confidence="0.978077">
587
</page>
<note confidence="0.542377">
Computational Linguistics Volume 25, Number 4
</note>
<subsectionHeader confidence="0.8310415">
3.1 Item Value Formula
Theorem 2
</subsectionHeader>
<bodyText confidence="0.838906">
If an item x is not in a looping bucket, then
</bodyText>
<equation confidence="0.827403">
V(x) = ØV(a1) (5)
ak s t ak i=1
Proof
</equation>
<bodyText confidence="0.9314195">
Let us expand our notion of inner to include deduction rules: inner(a1&apos; al) is the set
of all derivation trees of the form (b: (ai .) (a2 . .) . . . (ak . .)). For any item derivation
tree that is not a simple rule, there is some al ...ak,b such that D E inner(al b&amp;quot; ak).
Thus, for any item x,
</bodyText>
<equation confidence="0.967886263157895">
V(x) = V(D)
DEinner(x)
IED V(D) (6)
ak DE inner(a).
Consider item derivation trees Da, . . Da, headed by items al ak such that al • x• • ak
Recall that (x: Dai,. .,Da„) is the item derivation tree formed by combining each of
these trees into a full tree, and notice that U (x: Dal, ... ,Da„) = inner(al • i• ak).
1;11 € Inner (al ),• • •,
Dk E inner (ak)
Therefore
V(D) = V((x: ,Dak))
DEinner(ai. ak) Dal Einner(ai), •
DakEinner(ak)
V(Dai)
Einner(ai) .....
DakEinner(ak)
=- ED V(Dai)
i=1 DaiEinner(ai)
V(ai)
</equation>
<bodyText confidence="0.656495">
Substituting this back into Equation 6, we get
</bodyText>
<equation confidence="0.931924">
V(x) -= (g)V(ai)
a ak s t ak i=1
</equation>
<bodyText confidence="0.683407">
completing the proof. 0
</bodyText>
<page confidence="0.980425">
588
</page>
<note confidence="0.369978">
Goodman Semiring Parsing
</note>
<bodyText confidence="0.999936714285714">
Now, we address the case in which x is an item in a looping bucket. This case
requires computation of an infinite sum. We will write out this infinite sum, and discuss
how to compute it exactly in all cases, except for one, where we approximate it.
Consider the derivable items x1 ... xn, in some looping bucket B. If we build up
derivation trees incrementally, when we begin processing bucket B, only those trees
with no items from bucket B will be available, what we will call zeroth generation
derivation trees. We can put these zeroth generation trees together to form first gener-
ation trees, headed by elements in B. We can combine these first generation trees with
each other and with zeroth generation trees to form second generation trees, and so
on. Formally, we define the generation of a derivation tree headed by x in bucket B
to be the largest number of items in B we can encounter on a path from the root to a
leaf.
Consider the set of all trees of generation at most g headed by x. Call this set
inner&lt;g(x,B). We can define the &lt;g generation value of an item x in bucket B, V&lt;g(x,B):
</bodyText>
<equation confidence="0.989662">
V&lt;g(x,B) ,ED V(D)
DEInner&lt;g(x,B)
</equation>
<bodyText confidence="0.9995436">
Intuitively, as g increases, for x E B, inner&lt;g(x,B) becomes closer and closer to
inner(x). That is, the finite sum of values in the former approaches the infinite sum of
values in the latter. For w-continuous semirings (which includes all of the semirings
considered in this paper), an infinite sum is equal to the supremum of the partial sums
(Kuich 1997, 613). Thus,
</bodyText>
<equation confidence="0.9981125">
V(x) = isp V(D) = sup V&lt;g(x,B)
DEInner(x,B)
</equation>
<bodyText confidence="0.946440833333333">
It will be easier to compute the supremum if we find a simple formula for V&lt;g(x,B).
Notice that for items x e B, there will be no generation 0 derivations, so V&lt;0(x, B) =
0. Thus, generation 0 makes a trivial base for a recursive formula. Now, we can consider
the general case:
Theorem 3
For x an item in a looping bucket B, and for g &gt; 1,
</bodyText>
<equation confidence="0.94486425">
V&lt;g(x,B) = ED ph V(a1) (7)
tV&lt;g_i(ai,B) if a B
if ai E B
ak s ak
</equation>
<bodyText confidence="0.894926">
The proof parallels that of Theorem 2 (Goodman 1998).
</bodyText>
<subsectionHeader confidence="0.997511">
3.2 Solving the Infinite Summation
</subsectionHeader>
<bodyText confidence="0.9994004">
A formula for V&lt;g(x,B) is useful, but what we really need is specific techniques for
computing the supremum, V(x) = supg V&lt;g(x,B). For all w-continuous semirings, the
supremum of iteratively approximating the value of a set of polynomial equations, as
we are essentially doing in Equation 7, is equal to the smallest solution to the equations
(Kuich 1997, 622). In particular, consider the equations:
</bodyText>
<equation confidence="0.8046225">
11(x,B) = V(ai) if ai B (8)
ak s t al ak 1V&lt;,„(ai,B) if ai E B
</equation>
<page confidence="0.974303">
1=1
589
</page>
<note confidence="0.635328">
Computational Linguistics Volume 25, Number 4
</note>
<bodyText confidence="0.999966647058824">
where V &lt;0„(x, B) can be thought of as indicating IB I different variables, one for each
item x in the looping bucket B. Equation 7 represents the iterative approximation of
Equation 8, and therefore the smallest solution to Equation 8 represents the supremum
of Equation 7.
One fact will be useful for several semirings: whenever the values of all items
x E B at generation g +1 are the same as the values of all items in the preceding
generation, g, they will be the same at all succeeding generations, as well. Thus, the
value at generation g will be the value of the supremum. Elsewhere (Goodman 1998),
we give a trivial proof of this fact.
Now, we can consider various semiring-specific algorithms for computing the
supremum. Most of these algorithms are well known, and we have simply extended
them from specific parsers (described in Section 7) to the general case, or from one
semiring to another.
Notice in this section the wide variety of different algorithms, one for each semi-
ring, and some of them fairly complicated. In a conventional system, these algorithms
are interweaved with the parsing algorithm, conflating computation of infinite sums
with parsing. The result is algorithms that are both harder to understand, and less
portable to other semirings.
We first examine the simplest case, the Boolean semiring. Notice that whenever
a particular item has value TRUE at generation g, it must also have value TRUE
at generation g+1, since if the item can be derived in at most g generations then
it can certainly be derived in at most g +1 generations. Thus, since the number
of TRUE valued items is nondecreasing, and is at most IBI, eventually the values
of all items must not change from one generation to the next. Therefore, for the
Boolean semiring, a simple algorithm suffices: keep computing successive genera-
tions, until no change is detected in some generation; the result is the supremum.
We can perform this computation efficiently if we keep track of items that change
value in generation g and only examine items that depend on them in generation
g+1. This algorithm is then similar to the algorithm of Shieber, Schabes, and Pereira
(1993).
For the counting semiring, the Viterbi semiring, and the derivation forest semi-
ring, we need the concept of a derivation subgraph. In Section 2.2 we considered
the strongly connected components of the dependency graph, consisting of items that
for some sentence could possibly depend on each other, and we put these possibly
interdependent items together in looping buckets. For a given sentence and gram-
mar, not all items will have derivations. We will find the subgraph of the dependency
graph of items with derivations, and compute the strongly connected components of
this subgraph. The strongly connected components of this subgraph correspond to
loops that actually occur given the sentence and the grammar, as opposed to loops
that might occur for some sentence and grammar, given the parser alone. We call this
subgraph the derivation subgraph, and we will say that items in a strongly connected
component of the derivation subgraph are part of a loop.
Now, we can discuss the counting semiring (integers under + and x). In the
counting semiring, for each item, there are three cases: the item can be in a loop;
the item can depend (directly or indirectly) on an item in a loop; or the item does
not depend on loops. If the item is in a loop or depends on a loop, its value is in-
finite. If the item does not depend on a loop in the current bucket, then its value
becomes fixed after some generation. We can now give the algorithm: first, com-
pute successive generations until the set of items in B does not change from one
generation to the next. Next, compute the derivation subgraph, and its strongly con-
nected components. Items in a strongly connected component (a loop) have an infi-
</bodyText>
<page confidence="0.98779">
590
</page>
<note confidence="0.360253">
Goodman Semiring Parsing
</note>
<bodyText confidence="0.962936693877551">
nite number of derivations, and thus an infinite value. Compute items that depend
directly or indirectly on items in loops: these items also have infinite value. Any
other items can only be derived in finitely many ways using items in the current
bucket, so compute successive generations until the values of these items do not
change.
The method for solving the infinite summation for the derivation forest semiring
depends on the implementation of derivation forests. Essentially, that representation
will use pointers to efficiently represent derivation forests. Pointers, in various forms,
allow one to efficiently represent infinite circular references, either directly (Goodman
1999), or indirectly (Goodman 1998). Roughly, the algorithm we will use is to compute
the derivation subgraph, and then create pointers analogous to the directed edges in
the derivation subgraph, including pointers in loops whenever there is a loop in the
derivation subgraph (corresponding to an infinite number of derivations). Details are
given elsewhere (Goodman 1998). As in the finite case, this representation is equivalent
to that of Billot and Lang (1989).
For the Viterbi semiring, the algorithm is analogous to the Boolean case. Deriva-
tions using loops in these semirings will always have values no greater than deriva-
tions not using loops, since the value with the loop will be the same as some value
without the loop, multiplied by some set of rule probabilities that are at most 1. Since
the additive operation is max, these lower (or at most equal) looping derivations do not
change the value of an item. Therefore, we can simply compute successive generations
until values fail to change from one iteration to the next.
Now, consider implementations of the Viterbi-derivation semiring in practice,
in which we keep only a representative derivation, rather than the whole deriva-
tion forest. In this case, loops do not change values, and we use the same algo-
rithm as for the Viterbi semiring. In an implementation of the Viterbi-n-best semi-
ring, in practice, loops can change values, but at most n times, so the same algo-
rithm used for the Viterbi semiring still works. Elsewhere (Goodman 1998), we de-
scribe theoretically correct implementations for both the Viterbi-derivation and Viterbi-
n-best semirings that keep all values in the event of ties, preserving addition&apos;s
associativity.
The last semiring we consider is the inside semiring. This semiring is the most
difficult. There are two cases of interest, one of which we can solve exactly, and the
other of which requires approximations. In many cases involving looping buckets, all
deduction rules will be of the form cf7x, where al and b are items in the looping bucket,
and x is either a rule, or an item in a previously computed bucket. This case corre-
sponds to the items used for deducing singleton productions, such as those Earley&apos;s
algorithm uses for rules of the form A —&gt; B and B A. In this case, Equation 8 forms
a set of linear equations that can be solved by matrix inversion. In the more general
case, as is likely to happen with epsilon rules, we get a set of nonlinear equations, and
must solve them by approximation techniques, such as simply computing successive
generations for many iterations.&apos; Stolcke (1993) provides an excellent discussion of
these cases, including a discussion of sparse matrix inversion, useful for speeding up
some computations.
5 Note that even in the case where we can only use approximation techniques, this algorithm is
relatively efficient. By assumption, in this case, there is at least one deduction rule with two items in
the current generation; thus, the number of deduction trees over which we are summing grows
exponentially with the number of generations: a linear amount of computation yields the sum of the
values of exponentially many trees.
</bodyText>
<page confidence="0.990211">
591
</page>
<figure confidence="0.959876333333333">
Volume 25, Number 4
goal
goal
</figure>
<figureCaption confidence="0.921803666666667">
Derivation of [goal Outer tree of [b]
Figure 7
Outside algorithm.
</figureCaption>
<sectionHeader confidence="0.936791" genericHeader="method">
4. Reverse Values
</sectionHeader>
<bodyText confidence="0.999981677419355">
The previous section showed how to compute several of the most commonly used
values for parsers, including Boolean, inside, Viterbi, counting, and derivation forest
values, among others. Noticeably absent from the list are the outside probabilities,
which we define below. In general, computing outside probabilities is significantly
more complicated than computing inside probabilities.
In this section, we show how to compute outside probabilities from the same
item-based descriptions used for computing inside values. Outside probabilities have
many uses, including for reestimating grammar probabilities (Baker 1979), for im-
proving parser performance on some criteria (Goodman 1996b), for speeding parsing
in some formalisms, such as data-oriented parsing (Goodman 1996a), and for good
thresholding algorithms (Goodman 1997).
We will show that by substituting other semirings, we can get values analogous
to the outside probabilities for any commutative semiring; elsewhere (Goodman 1998)
we have shown that we can get similar values for many noncommutative semirings
as well. We will refer to these analogous quantities as reverse values. For instance,
the quantity analogous to the outside value for the Viterbi semiring will be called
the reverse Viterbi value. Notice that the inside semiring values of a hidden Markov
model (1-IMM) correspond to the forward values of HMMs, and the reverse inside
values of an HMM correspond to the backwards values.
Compare the outside algorithm (Baker 1979; Lan i and Young 1990), given in Fig-
ure 7, to the inside algorithm of Figure 2. Notice that while the inside and recognition
algorithms are very similar, the outside algorithm is quite a bit different. In particular,
while the inside and recognition algorithms looped over items from shortest to longest,
the outside algorithm loops over items in the reverse order, from longest to shortest.
Also, compare the inside algorithm&apos;s main loop formula to the outside algorithm&apos;s
main loop formula. While there is clearly a relationship between the two equations,
the exact pattern of the relationship is not obvious. Notice that the outside formula is
about twice as complicated as the inside formula. This doubled complexity is typical
of outside formulas, and partially explains why the item-based description format is so
useful: descriptions for the simpler inside values can be developed with relative ease,
and then automatically used to compute the twice-as-complicated outside values.&apos;
</bodyText>
<footnote confidence="0.815526666666667">
6 Jumping ahead a bit, compare Equation 13 for reverse values to Equation 5 for forward values. Let k be
the number of terms above the line. Notice that the reverse values equation sums over k times as many
terms as the forward values equation. Parsers where all rules have k = 1 terms above the line can only
</footnote>
<figure confidence="0.7615186">
Computational Linguistics
592
Goodman Semiring Parsing
goal goal
Derivation of [goal] Outer tree of [bl
</figure>
<figureCaption confidence="0.995114">
Figure 8
</figureCaption>
<bodyText confidence="0.9666518">
Item derivation tree of [goal] and outer tree of [b].
For a context-free grammar, using the CKY parser of Figure 3, recall that the inside
probability for an item [i, A, j] is P(A --÷ w,. wi.A). The outside probability for the same
item is P(S Wt_lAwf wn). Thus, the outside probability has the property that
when multiplied by the inside probability, it gives the probability that the start symbol
generates the sentence using the given item, P(S wlAw wn w1 • • • wn)•
This probability equals the sum of the probabilities of all derivations using the given
item. Formally, letting P(D) represent the probability of a particular derivation, and
C(D, [i, X, j]) represent the number of occurrences of item [i, X, j] in derivation D (which
for some parsers could be more than one if X were part of a loop),
</bodyText>
<equation confidence="0.9523845">
inside(i, X, j) x outside(i, X, j) P(D) C(D, [i, X, j])
D a derivation
</equation>
<bodyText confidence="0.999881">
The reverse values in general have an analogous meaning. Let C(D, x) represent
the number of occurrences (the count) of item x in item derivation tree D. Then, for
an item x, the reverse value Z(x) should have the property
</bodyText>
<equation confidence="0.9132175">
V (x) Z(x) ED V (D)C(D, x) (9)
D a derivation
</equation>
<bodyText confidence="0.996691875">
Notice that we have multiplied an element of the semiring, V(D), by an integer, C(D, x).
This multiplication is meant to indicate repeated addition, using the additive operator
of the semiring. Thus, for instance, in the Viterbi semiring, multiplying by a count
other than 0 has no effect, since x x = max(x, x) = x, while in the inside semiring,
it corresponds to actual multiplication. This value represents the sum of the values of
all derivation trees that the item x occurs in; if an item x occurs more than once in a
derivation tree D, then the value of D is counted more than once.
To formally define the reverse value of an item x, we must first define the outer
trees outer(x). Consider an item derivation tree of the goal item, containing one or
more instances of item x. Remove one of these instances of x, and its children too,
leaving a gap in its place. This tree is an outer tree of x. Figure 8 shows an item
derivation tree of the goal item, including a subderivation of an item b, derived from
terms a1,. , ak. It also shows an outer tree of b, with b and its children removed; the
spot b was removed from is labeled (b).
parse regular grammars, and tend to be less useful. Thus, in most parsers of interest, k &gt; 1, and the
complexity of (at least some) outside equations, when the sum is written out, is at least doubled.
</bodyText>
<page confidence="0.996636">
593
</page>
<note confidence="0.607101">
Computational Linguistics Volume 25, Number 4
</note>
<bodyText confidence="0.889799">
For an outer tree D E outer(x), we define its value, Z(D), to be the product of the
values of all rules in D, OrED R(r)• Then, the reverse value of an item can be formally
defined as
</bodyText>
<equation confidence="0.932663416666667">
Z(x) = Z(D) (10)
DEouter(x)
That is, the reverse value of x is the sum of the values of each outer tree of x.
Now, we show that this definition of reverse values has the property described by
Equation 9.7
Theorem 4
V(x) z(x) , ED V (D)C(D, x)
D a derivation
Proof
First, observe that
V(x) 0 z(x) = v(1)) 0 z(o) = ED v(1)0 z(o) (11)
lEinner(x) 0Eouter(x) I E inner(x) 0Eouter(x)
</equation>
<bodyText confidence="0.9968636">
Next, we argue that this last expression equals the expression on the right-hand side
of Equation 9, EDD v(D)cp,x). For an item x, any outer part of an item derivation
tree for x can be combined with any inner part to form a complete item derivation
tree. That is, any 0 E outer(x) and any I E inner(x) can be combined to form an item
derivation tree D containing x, and any item derivation tree D containing x can be
decomposed into such outer and inner trees. Thus, the list of all combinations of outer
and inner trees corresponds exactly to the list of all item derivation trees containing
x. In fact, for an item derivation tree D containing C(D, x) instances of x, there are
C(D, x) ways to form D from combinations of outer and inner trees. Also, notice that
for D combined from 0 and I
</bodyText>
<equation confidence="0.929351625">
V(I) 0 Z(0) = R(r) 0 R(r) R(r) = V (D)
rEI rE0 rED
Thus,
V(I) 0 Z(0) = (1) V (D)C(D, x) (12)
lEinner(x) 0Eouter(x)
Combining Equation 11 with Equation 12, we see that
v(x) z(x) = ED V (D)C(D, x)
D a derivation
</equation>
<bodyText confidence="0.527232">
completing the proof. 0
</bodyText>
<footnote confidence="0.94537225">
7 We note that satisfying Equation 9 is a useful but not sufficient condition for using reverse inside
values for grammar reestimation. While this definition will typically provide the necessary values for
the E step of an E-M algorithm, additional work will typically be required to prove this fact; Equation
9 should be useful in such a proof.
</footnote>
<page confidence="0.991551">
594
</page>
<note confidence="0.386637">
Goodman Semiring Parsing
</note>
<bodyText confidence="0.9939195">
There is a simple, recursive formula for efficiently computing reverse values. Recall
that the basic equation for computing forward values not involved in loops was
</bodyText>
<equation confidence="0.954998">
V(x) -= ® V (az)
ak s t al.jak i=1
</equation>
<bodyText confidence="0.986668375">
At this point, for conciseness, we introduce a nonstandard notation. We will soon
be using many sequences of the form 1, 2, . ..,j-2,j-1,j+1,j+ 2, .. k— 1,k. We denote
such sequences by 1, k. By extension, we will also write f(1), (k) to indicate a
sequence of the form f (1),f (2), ,f(j — 2),f(j — 1) , f(j + 1), f(j + 2),. ,f (k — 1) , f (k).
Now, we can give a simple formula for computing reverse values Z(x) not involved
in loops:
Theorem 5
For items x E B where B is nonlooping,
</bodyText>
<equation confidence="0.9865526">
Z(x) = Z(b) V(i1) (13)
ak,b s.t. i•••a 5&amp;A x=ai
unless x is the goal item, in which case Z(x) = 1, the multiplicative identity of the
semiring.
Proof
</equation>
<bodyText confidence="0.99995775">
The simple case is when x is the goal item. Since an outer tree of the goal item is a
derivation of the goal item, with the goal item and its children removed, and since we
assumed in Section 2.2 that the goal item can only appear in the root of a derivation
tree, the outer trees of the goal item are all empty. Thus,
</bodyText>
<equation confidence="0.996353">
Z(goal) = Z(D) = Z({0}) = (g) R(r) = 1
DEouter(goal) rE{O}
</equation>
<bodyText confidence="0.978255">
As mentioned in Section 2.1, the value of the empty product is the multiplicative
identity.
Now, we consider the general case. We need to expand our concept of outer to
include deduction rules, where outer(j, al ak) is an item derivation tree of the goal
item with one subtree removed, a subtree headed by al whose parent is b and whose
siblings are headed by al, ak. Notice that for every outer tree D E outer(x), there is
exactly one], al,. , ak, and b such that x = aj and D E outer(j,a1 b* • ak): this corresponds
to the deduction rule used at the spot in the tree where the subtree headed by x was
deleted. Figure 9 illustrates the idea of putting together an outer tree of b with inner
trees for al, .7)., ak to form an outer tree of x = al. Using this observation,
</bodyText>
<equation confidence="0.998883666666667">
Z(x) = (1) Z(D)
DEouter(x)
Z(D) (14)
</equation>
<bodyText confidence="0.363178">
ak,b s.t. al ak A x=aj DEouter ak.)
</bodyText>
<page confidence="0.993684">
595
</page>
<figure confidence="0.946527">
Computational Linguistics Volume 25, Number 4
goal
</figure>
<figureCaption confidence="0.985093">
Figure 9
</figureCaption>
<bodyText confidence="0.931369666666667">
Combining an outer tree with inner trees to form an outer tree.
Now, consider all of the outer trees outer(j,a1 ak). For each item derivation tree
Dai E inner(ak) and for each outer tree Db E outer(b), there will be
one outer tree in the set outer(j,a1 .b.. ak..
) Similarly, each tree in outer(j,a1 ak) can be
decomposed into an outer tree in outer(b) and derivation trees for (21, TL, ak. Then,
</bodyText>
<equation confidence="0.965845666666667">
Z(D)
Deouter(01.i., ak)
z(Db) v(Dai)0 0V(Dak)
DbEouter(b),
Einner(ai) .... ,
DakEinnet(ak)
z(Db)) ( v(Dai))
Z(b) V(ai)0 OV(ak)
=-- Z(b) (8) V(ai)
i=i ,, , k
Substituting equation 15 into equation 14, we conclude that
Z(x) = Z(b) V(a)
</equation>
<bodyText confidence="0.7738855">
j,ai... ak,b s.t. i; al A x—a,
completing the general case.
Computing the reverse values for loops is somewhat more complicated, and as in
the forward case, requires an infinite sum, and the use of the concept of generation.
</bodyText>
<equation confidence="0.874537666666667">
(DakEitaker(4)
IED V(Dak))
(15)
</equation>
<page confidence="0.99054">
596
</page>
<note confidence="0.382511">
Goodman Semiring Parsing
</note>
<bodyText confidence="0.999767333333333">
We define the generation g of an outer tree D of item x in bucket B to be the number
of items in bucket B on the path between the root and the removal point, inclusive.
We can then let Z&lt;g(x,B) represent the sum of the values of all trees headed by x
of generation at most g. In the base case, Z&lt;0(x,B) = 0. For w-continuous semirings,
Z&lt;g(x,B) approaches Z(x) as g approaches oo. We can give a recursive equation for
Z&lt;g(x,B) as follows, using a proof similar to that of Theorem 5 (Goodman 1998):
</bodyText>
<subsectionHeader confidence="0.628809">
Theorem 6
</subsectionHeader>
<bodyText confidence="0.667807">
For items x E B and g &gt; 1,
</bodyText>
<equation confidence="0.818193666666667">
Z&lt;g(x,B) = (8) V(ai))Z&lt;g_i(b,B) if b E B
® 1Z(b) if b B (16)
ak,b s.t. ak. A x—ai
</equation>
<sectionHeader confidence="0.483332" genericHeader="method">
5. Semiring Parser Execution
</sectionHeader>
<bodyText confidence="0.999951117647059">
Executing a semiring parser is fairly simple. There is, however, one issue that must
be dealt with before we can actually begin parsing. A semiring parser computes the
values of items in the order of the buckets they fall into. Thus, before we can begin
parsing, we need to know which items fall into which buckets, and the ordering of
those buckets. There are three approaches to determining the buckets and ordering that
we will discuss in this section. The first approach is a simple, brute-force enumeration
of all items, derivable or not, followed by a topological sort. This approach will have
suboptimal time and space complexity for some item-based descriptions. The second
approach is to use an agenda parser in the Boolean semiring to determine the derivable
items and their dependencies, and to then perform a topological sort. This approach
has optimal time complexity, but typically suboptimal space complexity. The final
approach is to use bucketing code specific to the item-based interpreter. This achieves
optimal performance for additional programming effort.
The simplest way to determine the bucketing is to simply enumerate all possible
items for the given item-based description, grammar, and input sentence. Then, we
compute the strongly connected components and a partial ordering; both steps can be
done in time proportional to the number of items plus the number of dependencies
(Cormen, Leiserson, and Rivest 1990, Chap. 23). For some parsers, this technique has
optimal time complexity, although poor space complexity In particular, for the CKY
algorithm, the time complexity is optimal, but since it requires computing and storing
all possible 0(n3) dependencies between the items, it takes significantly more space
than the 0(n2) space required in the best implementation. In general, the brute-force
technique raises the space complexity to be the same as the time complexity. Further-
more, for some algorithms, such as Earley&apos;s algorithm, there could be a significant time
complexity added as well. In particular, Earley&apos;s algorithm may not need to examine
all possible items. For certain grammars, Earley&apos;s algorithm examines only a linear
number of items and a linear number of dependencies, even though there are 0(n2)
possible items, and 0(n3) possible dependencies. Thus the brute-force approach would
require 0(n3) time and space instead of 0(n) time and space, for these grammars.
The next approach to finding the bucketing solves the time complexity problem.
In this approach, we first parse in the Boolean semiring, using the agenda parser de-
scribed by Shieber, Schabes, and Pereira (1995), and then we perform a topological
sort. The techniques that Shieber, Schabes, and Pereira use work well for the Boolean
semiring, where items only have value TRUE or FALSE, but cannot be used directly for
</bodyText>
<page confidence="0.990131">
597
</page>
<note confidence="0.68478">
Computational Linguistics Volume 25, Number 4
</note>
<bodyText confidence="0.8740045">
for current := first bucket to last bucket
if current is a looping bucket
</bodyText>
<equation confidence="0.917432705882353">
/* replace with semiring-specific code * /
for x E current
V[x, 0] = 0;
for g :=-- 1 to oo
for each X E current, al ak s.t. al x&amp;quot; • a k
a, ct current
V[x, g] := V[x, g] eV[ai]
— a„
V[g — 1] a, E current
for each x E current
V[x] :=- V[x, oo];
else
for each x E current, al . . .ak s.t.
V[x] := V[x] Ok,_i V[a,];
return V[goal];
Figure 10
Forward semiring parser interpreter.
</equation>
<bodyText confidence="0.999963366666667">
other semirings. For other semirings, we need to make sure that the values of items are
not computed until after the values of all items they depend on are computed. How-
ever, we can use the algorithm of Shieber, Schabes, and Pereira to compute all of the
items that are derivable, and to store all of the dependencies between the items. Then
we perform a topological sort on the items. The time complexity of both the agenda
parser and the topological sort will be proportional to the number of dependencies,
which will be proportional to the optimal time complexity. Unfortunately, we still have
the space complexity problem, since again, the space used will be proportional to the
number of dependencies, rather than to the number of items.
The third approach to bucketing is to create algorithm-specific bucketing code;
this results in parsers with both optimal time and optimal space complexity. For in-
stance, in a CKY-style parser, we can simply create one bucket for each length, and
place each item into the bucket for its length. For some algorithms, such as Ear-
ley&apos;s algorithm, special-purpose code for bucketing might have to be combined with
code to make sure all and only derivable items are considered (using triggering tech-
niques described by Shieber, Schabes, and Pereira) in order to achieve optimal perfor-
mance.
Once we have the bucketing, the parsing step is fairly simple. The basic algorithm
appears in Figure 10. We simply loop over each item in each bucket. There are two
types of buckets: looping buckets, and nonlooping buckets. If the current bucket is
a looping bucket, we compute the infinite sum needed to determine the bucket&apos;s
values; in a working system, we substitute semiring-specific code for this section, as
described in Section 3.2. If the bucket is not a looping bucket, we simply compute
all of the possible instantiations that could contribute to the values of items in that
bucket. Finally, we return the value of the goal item.
The reverse semiring parser interpreter is very similar to the forward semiring
parser interpreter. The differences are that in the reverse semiring parser interpreter,
we traverse the buckets in reverse order, and we use the formulas for the reverse
values, rather than the forward values. Elsewhere (Goodman 1998), we give a simple
inductive proof to show that both interpreters compute the correct values.
</bodyText>
<page confidence="0.993996">
598
</page>
<note confidence="0.384461">
Goodman Semiring Parsing
</note>
<bodyText confidence="0.999879166666667">
There are two other implementation issues. First, for some parsers, it will be pos-
sible to discard some items. That is, some items serve the role of temporary variables,
and can be discarded after they are no longer needed, especially if only the forward
values are going to be computed. Also, some items do not depend on the input string,
but only on the rule value function of the grammar. The values of these items can be
precomputed.
</bodyText>
<sectionHeader confidence="0.907352" genericHeader="method">
6. Examples
</sectionHeader>
<bodyText confidence="0.999897333333333">
In this section, we survey other results that are described in more detail elsewhere
(Goodman 1998), including examples of formalisms that can be parsed using item-
based descriptions, and other uses for the technique of semiring parsing.
</bodyText>
<subsectionHeader confidence="0.999928">
6.1 Finite State Automata and Hidden Markov Models
</subsectionHeader>
<bodyText confidence="0.9997616">
Nondeterministic finite-state automata (NFAs) and HMMs turn out to be examples of
the same underlying formalism, whose values are simply computed in different semi-
rings. Other semirings lead to other interesting values. For HMMs, notice that the for-
ward values are simply the forward inside values; the backward values are the reverse
values of the inside semiring; and Viterbi values are the forward values of the Viterbi
semiring. For NFAs, we can use the Boolean semiring to determine whether a string is
in the language of an NFA; we can use the counting semiring to determine how many
state sequences there are in the NFA for a given string; and we can use the derivation
forest semiring to get a compact representation of all state sequences in an NFA for an
input string. A single item-based description can be used to find all of these values.
</bodyText>
<subsectionHeader confidence="0.999952">
6.2 Prefix Values
</subsectionHeader>
<bodyText confidence="0.951929666666667">
For language modeling, it may be useful to compute the prefix probability of a string.
That is, given a string wn, we may wish to know the total probability of all
sentences beginning with that string,
</bodyText>
<equation confidence="0.755549">
EP(S w1. • • wnvi • • • 0)
k&gt;o,vi,.••,vk
</equation>
<bodyText confidence="0.999805555555555">
where 01 vk represent words that could possibly follow w1 wn. Jelinek and Lafferty
(1991) and Stolcke (1993) both give algorithms for computing these prefix probabilities.
Elsewhere (Goodman 1998), we show how to produce an item-based description of a
prefix parser. There are two main advantages to using an item-based description: ease
of derivation, and reusability.
First, the conventional derivations are somewhat complex, requiring a fair amount
of inside-semiring-specific mathematics. In contrast, using item-based descriptions, we
only need to derive a parser that has the property that there is one item derivation for
each (complete) grammar derivation that would produce the prefix. The value of any
prefix given the parser will then automatically be the sum of all grammar derivations
that include that prefix.
The other advantage is that the same description can be used to compute many
values, not just the prefix probability. For instance, we can use this description with the
Viterbi-derivation semiring to find the most likely derivation that includes this prefix.
With this most likely derivation, we could begin interpretation of a sentence even be-
fore the sentence was finished being spoken to a speech recognition system. We could
even use the Viterbi-n-best semiring to find the n most likely derivations that include
this prefix, if we wanted to take into account ambiguities present in parses of the prefix.
</bodyText>
<page confidence="0.994341">
599
</page>
<note confidence="0.691202">
Computational Linguistics Volume 25, Number 4
</note>
<subsectionHeader confidence="0.995542">
6.3 Beyond Context-Free
</subsectionHeader>
<bodyText confidence="0.999990888888889">
There has been quite a bit of previous work on the intersection of formal language
theory and algebra, as described by Kuich (1997), among others. This previous work
has made heavy use of the fact that there is a strong correspondence between alge-
braic equations in certain noncommutative semirings, and CFGs. This correspondence
has made it possible to manipulate algebraic systems, rather than grammar systems,
simplifying many operations.
On the other hand, there is an inherent limit to such an approach, namely a limit
to context-free systems. It is then perhaps slightly surprising that we can avoid these
limitations, and create item-based descriptions of parsers for weakly context-sensitive
grammars, such as tree adjoining grammars (TAGs). We avoid the limitations of pre-
vious approaches using two techniques. One technique is to compute derivation trees,
rather than parse trees, for TAGs. Computing derivation trees for TAGs is significantly
easier than computing parse trees, since the derivation trees are context-free. The other
trick we use is to create a set of equations for each grammar and string length rather
than creating a set of equations for each grammar, as earlier formulations did. Because
the number of equations grows with the string length with our technique, we can rec-
ognize strings in weakly context-sensitive languages. Goodman (1998) gives a further
explication of this subject, including an item-based description for a simple TAG parser.
</bodyText>
<subsectionHeader confidence="0.998863">
6.4 Tomita Parsing
</subsectionHeader>
<bodyText confidence="0.99992275">
Our goal in this section has been to show that item-based descriptions can be used
to simply describe almost all parsers of interest. One parsing algorithm that would
seem particularly difficult to describe is Tomita&apos;s graph-structured-stack LR parsing
algorithm. This algorithm at first glance bears little resemblance to other parsing al-
gorithms. Despite this lack of similarity, Sikkel (1993) gives an item-based description
for a Tomita-style parser for the Boolean semiring, which is also more efficient than
Tomita&apos;s algorithm. Sikkel&apos;s parser can be easily converted to our format, where it can
be used for w-continuous semirings in general.
</bodyText>
<subsectionHeader confidence="0.997397">
6.5 Graham Harrison Ruzzo (GHR) Parsing
</subsectionHeader>
<bodyText confidence="0.999936076923077">
Graham, Harrison, and Ruzzo (1980) describe a parser similar to Earley&apos;s, but with
several speedups that lead to significant improvements. Essentially, there are three
improvements in the GHR parser. First, epsilon productions are precomputed; second,
unary productions are precomputed; and, finally, completion is separated into two
steps, allowing better dynamic programming.
Goodman (1998) gives a full item-based description of a GHR parser. The forward
values of many of the items in our parser related to unary and epsilon productions
can be computed off-line, once per grammar, which is an idea due to Stolcke (1993).
Since reverse values require entire strings, the reverse values of these items cannot
be computed until the input string is known. Because we use a single item-based
description for precomputed items and nonprecomputed items, and for forward and
reverse values, this combination of off-line and on-line computation is easily and
compactly specified.
</bodyText>
<subsectionHeader confidence="0.996065">
6.6 Grammar Transformations
</subsectionHeader>
<bodyText confidence="0.9994312">
We can apply the same techniques to grammar transformations that we have so far
applied to parsing. Consider a grammar transformation, such as the Chomsky normal
form (CNF) grammar transformation, which takes a grammar with epsilon, unary,
and n-ary branching productions, and converts it into one in which all productions
are of the form A —&gt; BC or A —&gt; a. For any sentence w1 ... wn its value under the
</bodyText>
<page confidence="0.985102">
600
</page>
<note confidence="0.408043">
Goodman Semiring Parsing
</note>
<bodyText confidence="0.9998004375">
original grammar in the Boolean semiring (TRUE if the sentence can be generated by
the grammar, FALSE otherwise) is the same as its value under a transformed gram-
mar. Therefore, we say that this grammar transformation is value preserving under
the Boolean semiring. We can generalize this concept of value preserving to other
semirings.
Elsewhere (Goodman 1998), we show that using essentially the same item-based
descriptions we have used for parsing, we can specify grammar transformations. The
concept of value preserving grammar transformation is already known in the inter-
section of formal language theory and algebra (Kuich 1997; Kuich and Salomaa 1986;
Teitelbaum 1973). Our contribution is to show that these value preserving transforma-
tions can be written as simple item-based descriptions, allowing the same computa-
tional machinery to be used for grammar transformations as is used for parsing, and to
some extent showing the relationship between certain grammar transformations and
certain parsers, such as that of Graham, Harrison, and Ruzzo (1980). This uniform
method of specifying grammar transformations is similar to, but clearer than, similar
techniques used with covering grammars (Nijholt 1980; Leermakers 1989).
</bodyText>
<sectionHeader confidence="0.991284" genericHeader="method">
7. Previous Work
</sectionHeader>
<subsectionHeader confidence="0.998334">
7.1 Historical Work
</subsectionHeader>
<bodyText confidence="0.999937580645161">
The previous work in this area is extensive, including work in deductive parsing,
work in statistical parsing, and work in the combination of formal language theory
and algebra. This paper can be thought of as synthetic, combining the work in all three
areas, although in the course of synthesis, several general formulas have been found,
most notably the general formula for reverse values. A comprehensive examination of
all three areas is beyond the scope of this paper, but we can touch on a few significant
areas of each.
First, there is the work in deductive parsing. This work in some sense dates back
to Earley (1970), in which the use of items in parsers is introduced. More recent work
(Pereira and Warren 1983; Pereira and Shieber 1987) demonstrates how to use deduc-
tion engines for parsing. Finally, both Shieber, Schabes, and Pereira (1995) and Sikkel
(1993) have shown how to specify parsers in a simple, interpretable, item-based format.
This format is roughly the format we have used here, although there are differences
due to the fact that their work was strictly in the Boolean semiring.
Work in statistical parsing has also greatly influenced this work. We can trace this
work back to research in HMMs by Baum and his colleagues (Baum and Eagon 1967;
Baum 1972). In particular, the work of Baum developed the concept of backward prob-
abilities (in the inside semiring), as well as many of the techniques for computing in
the inside semiring. Viterbi (1967) developed corresponding algorithms for computing
in the Viterbi semiring. Baker (1979) extended the work of Baum and his colleagues to
PCFGs, including to computation of the outside values (or reverse inside values in our
terminology). Baker&apos;s work is described by Lan i and Young (1990). Baker&apos;s work was
only for PCFGs in CNF, avoiding the need to compute infinite summations. Jelinek
and Lafferty (1991) showed how to compute some of the infinite summations in the
inside semiring, those needed to compute the prefix probabilities of PCFGs in CNF.
Stolcke (1993) showed how to use the same techniques to compute inside probabili-
ties for Earley parsing, dealing with the difficult problems of unary transitions, and
the more difficult problems of epsilon transitions. He thus solved all of the important
problems encountered in using an item-based parser to compute the inside and out-
side values (forward and reverse inside values); he also showed how to compute the
forward Viterbi values.
</bodyText>
<page confidence="0.992909">
601
</page>
<note confidence="0.705898">
Computational Linguistics Volume 25, Number 4
</note>
<bodyText confidence="0.999935">
The final area of work is in formal language theory and algebra. Although it is not
widely known, there has been quite a bit of work showing how to use formal power
series to elegantly derive results in formal language theory, dating back to Chomsky
and Schiitzenberger (1963). The major classic results can be derived in this frame-
work, but with the added benefit that they apply to all commutative w-continuous
semirings. The most accessible introduction to this literature we have found is by
Kuich (1997). There are also books by Salomaa and Soittola (1978) and Kuich and
Salomaa (1986).
One piece of work deserves special mention. Teitelbaum (1973) showed that any
semiring could be used in the CKY algorithm, laying the foundation for much of the
work that followed.
In summary, this paper synthesizes work from several different related fields, in-
cluding deductive parsing, statistical parsing, and formal language theory; we emulate
and expand on the earlier synthesis of Teitelbaum. The synthesis here is powerful: by
generalizing and integrating many results, we make the computation of a much wider
variety of values possible.
</bodyText>
<subsectionHeader confidence="0.999287">
7.2 Recent Similar Work
</subsectionHeader>
<bodyText confidence="0.999919291666667">
There has also been recent similar work by Tendeau (1997b, 1997a). Tendeau (1997b)
gives an Earley-like algorithm that can be adapted to work with complete semirings
satisfying certain conditions. Unlike our version of Earley&apos;s algorithm, Tendeau&apos;s ver-
sion requires time O(n1) where L is the length of the longest right-hand side, as
opposed to 0(n3) for the classic version, and for our description. While one could split
right-hand sides of rules to make them binary branching, speeding Tendeau&apos;s version
up, this would then change values in the derivation semirings. Tendeau (1997b, 1997a)
introduces a parse forest semiring, similar to our derivation forest semiring, in that
it encodes a parse forest succinctly. To implement this semiring, Tendeau&apos;s version of
rule value functions take as their input not only a nonterminal, but also the span that it
covers; this is somewhat less elegant than our version. Tendeau (1997a) gives a generic
description for dynamic programming algorithms. His description is very similar to
our item-based descriptions, except that it does not include side conditions. Thus, al-
gorithms such as Earley&apos;s algorithm cannot be described in Tendeau&apos;s formalism in a
way that captures their efficiency.
There are some similarities between our work and the work of Koller, McAllester,
and Pfeffer (1997), who create a general formalism for handling stochastic programs
that makes it easy to compute inside and outside probabilities. While their formalism
is more general than item-based descriptions, in that it is a good way to express any
stochastic program, it is also less compact than ours for expressing most dynamic pro-
gramming algorithms. Our formalism also has advantages for approximating infinite
sums, which we can do efficiently, and in some cases exactly. It would be interesting
to try to extend item-based descriptions to capture some of the formalisms covered
by Koller, McAllester, and Pfeffer, including Bayes&apos; nets.
</bodyText>
<sectionHeader confidence="0.86564" genericHeader="conclusions">
8. Conclusion
</sectionHeader>
<bodyText confidence="0.999748166666667">
In this paper, we have given a simple item-based description format that can be used
to describe a very wide variety of parsers. These parsers include the CKY algorithm,
Earley&apos;s algorithm, prefix probability computation, a TAG parsing algorithm, Graham,
Harrison, Ruzzo (GHR) parsing, and HMM computations. We have shown that this de-
scription format makes it easy to find parsers that compute values in any w-continuous
semiring. The same description can be used to find reverse values in commutative w-
</bodyText>
<page confidence="0.991714">
602
</page>
<note confidence="0.411582">
Goodman Semiring Parsing
</note>
<bodyText confidence="0.999871490196078">
continuous semirings, and in many noncommutative ones as well. This description
format can also be used to describe grammar transformations, including transfor-
mations to CNF and GNF, which preserve values in any commutative w-continuous
semiring.
While theoretical in nature, this paper is of some practical value. There are three
reasons the results of this paper would be used in practice: first, these techniques make
computation of the outside values simple and mechanical; second, these techniques
make it easy to show that a parser will work in any w-continuous semiring; and third,
these techniques isolate computation of infinite sums in a given semiring from the
parser specification process.
Perhaps the most useful application of these results is in finding formulas for
outside values. For parsers such as CKY parsers, finding outside formulas is not par-
ticularly burdensome, but for complicated parsers such as TAG parsers, GHR parsers,
and others, it can require a fair amount of thought to find these equations through
conventional reasoning. With these techniques, the formulas can be found in a simple
mechanical way.
The second advantage comes from clarifying the conditions under which a parser
can be converted from computing values in the Boolean semiring (a recognizer) to
computing values in any w-continuous semiring. We should note that because in the
Boolean semiring, infinite summations can be computed trivially and because repeat-
edly adding a term does not change results, it is not uncommon for parsers that work
in the Boolean semiring to require significant modification for other semirings. For
parsers like CKY parsers, verifying that the parser will work in any semiring is triv-
ial, but for other parsers the conditions are more complex. With the techniques in
this paper, all that is necessary is to show that there is a one-to-one correspondence
between item derivations and grammar derivations. Once that has been shown, any
w-continuous semiring can be used.
The third use of this paper is to separate the computation of infinite sums from
the main parsing process. Infinite sums can come from several different phenomena,
such as loops from productions of the form A A; productions involving c; and
left recursion. In traditional procedural specifications, the solution to these difficult
problems is intermixed with the parser specification, and makes the parser specific to
semirings using the same techniques for solving the summations.
It is important to notice that the algorithms for solving these infinite summations
vary fairly widely, depending on the semiring. On the one hand, Boolean infinite
summations are nearly trivial to compute. For other semirings, such as the counting
semiring, or derivation forest semiring, more complicated computations are required,
including the detection of loops. Finally, for the inside semiring, in most cases only
approximate techniques can be used, although in some cases, matrix inversion can be
used. Thus, the actual parsing algorithm, if specified procedurally, can vary quite a
bit depending on the semiring.
On the other hand, using our techniques makes infinite sums easier to deal with
in two ways. First, these difficult problems are separated out, relegated conceptu-
ally to the parser interpreter, where they can be ignored by the constructor of the
parsing algorithm. Second, because they are separated out, they can be solved once,
rather than again and again. Both of these advantages make it significantly easier to
construct parsers. Even in the case where, for efficiency, loops are precomputed off-
line, as in GHR parsing, the same item-based representation and interpreter can be
used.
In summary, the techniques of this paper will make it easier to compute outside
values, easier to construct parsers that work for any w-continuous semiring, and easier
</bodyText>
<page confidence="0.993575">
603
</page>
<note confidence="0.475868">
Computational Linguistics Volume 25, Number 4
</note>
<bodyText confidence="0.998450142857143">
to compute infinite sums in those semirings. In 1973, Teitelbaum wrote:
We have pointed out the relevance of the theory of algebraic power
series in noncommuting variables in order to minimize further piece-
meal rediscovery (page 199).
Many of the techniques needed to parse in specific semirings continue to be redis-
covered, and outside formulas are derived without observation of the basic formulas
given here. We hope this paper will bring about Teitelbaum&apos;s wish.
</bodyText>
<sectionHeader confidence="0.998234" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.896651888888889">
I would like to thank Stan Chen, Barbara
Grosz, Luke Hunsberger, Fernando Pereira,
and Stuart Shieber, for helpful comments
and discussions, as well as the anonymous
reviewers for their comments on earlier
drafts. This work was funded in part by the
National Science Foundation through Grant
IRI-9350192, Grant IRI-9712068, and an NSF
Graduate Student Fellowship.
</reference>
<sectionHeader confidence="0.690774" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999621142857143">
Baker, James K. 1979. Trainable grammars
for speech recognition. In Proceedings of the
Spring Conference of the Acoustical Society of
America, pages 547-550, Boston, MA, June.
Baum, Leonard E. 1972. An inequality and
associated maximization technique in
statistical estimation of probabilistic
functions of a Markov process.
Inequalities, 3:1-8.
Baum, Leonard E. and J. A. Eagon. 1967. An
inequality with application to statistical
estimation for probabilistic functions of
Markov processes and to a model for
ecology. Bulletin of the American
Mathematicians Society, 73:360-363.
Billot, Sylvie and Bernard Lang. 1989. The
structure of shared forests in ambiguous
parsing. In Proceedings of the 27th Annual
Meeting, pages 143-151, Vancouver.
Association for Computational
Linguistics.
Chomsky, Noam and Marcel-Paul
Schtitzenberger. 1963. The algebraic
theory of context-free languages. In
P. Braffort and D. Hirschberg, editors,
Computer Programming and Formal Systems.
North-Holland, pages 118-161.
Cormen, Thomas H., Charles E. Leiserson,
and Ronald L. Rivest. 1990. Introduction to
Algorithms. MIT Press, Cambridge, MA.
Earley, Jay. 1970. An efficient context-free
parsing algorithm. Communications of the
ACM, 13:94-102.
Goodman, Joshua. 1996a. Efficient
algorithms for parsing the DOP model. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 143-152, May. Available as
cmp-lg/9604008.
Goodman, Joshua. 1996b. Parsing
algorithms and metrics. In Proceedings of
the 34th Annual Meeting, pages 177-183,
Santa Cruz, CA, June. Association for
Computational Linguistics. Available as
cmp-lg/9605036.
Goodman, Joshua. 1997. Global
thresholding and multiple-pass parsing.
In Proceedings of the Second Conference on
Empirical Methods in Natural Language
Processing, pages 11-25.
Goodman, Joshua. 1998. Parsing Inside-Out.
Ph.D. thesis, Harvard University.
Available as cmp-lg/9805007 and from
http://www.eecs.harvard.edu/
—goodman/thesis.ps.
Goodman, Joshua. 1999. Semiring parsing.
Computational Linguistics, 25(4):573-605.
Graham, Susan L., Michael A. Harrison, and
Walter L. Ruzzo. 1980. An improved
context-free recognizer. ACM Transactions
on Programming Languages and Systems,
2(3):415-462, July.
Jelinek, Frederick and John D. Lafferty. 1991.
Computation of the probability of initial
substring generation by stochastic
context-free grammars. Computational
Linguistics, pages 315-323.
Koller, Daphne, David McAllester, and Avi
Pfeffer. 1997. Effective bayesian inference
for stochastic programs. In Proceedings of
the 14th National Conference on Artificial
Intelligence, pages 740-747, Providence, RI,
August.
Kuich, Werner. 1997. Semirings and formal
power series: Their relevance to formal
languages and automata. In Grzegorz
Rozenberg and Arto Salomaa, editors,
Handbook of Formal Languages.
Springer-Verlag, Berlin, pages 609-677.
Kuich, Werner and Arto Salomaa. 1986.
Semirings, Automata, Languages. Number 5
of EATCS Monographs on Theoretical
Computer Science. Springer-Verlag,
Berlin, Germany.
</reference>
<page confidence="0.968447">
604
</page>
<reference confidence="0.990677080645161">
Goodman Semiring Parsing
Lan, K. and S. J. Young. 1990. The
estimation of stochastic context-free
grammars using the inside-outside
algorithm. Computer Speech and Language,
4:35-56.
Leermakers, Rene. 1989. How to cover a
grammar. In Proceedings of the 27th Annual
Meeting, pages 135-142, Vancouver.
Association for Computational
Linguistics.
Nijholt, Anton. 1980. Context-Free Grammars:
Covers, Normal Forms, and Parsing.
Number 93 of Lecture Notes in Computer
Science. Springer-Verlag, Berlin, Germany.
Pereira, Fernando and Stuart Shieber. 1987.
Prolog and Natural Language Analysis.
Number 10 of CSU Lecture Notes. Center
for the Study of Language and
Information, Stanford, CA.
Pereira, Fernando and David Warren. 1983.
Parsing as deduction. In Proceedings of the
21st Annual Meeting, pages 137-44,
Cambridge, MA. Association for
Computational Linguistics.
Salomaa, Arto and Matti Soittola. 1978.
Automata-Theoretic Aspects of Formal Power
Series. Springer-Verlag, Berlin, Germany.
Shieber, Stuart, Yves Schabes, and Fernando
Pereira. 1995. Principles and
implementation of deductive parsing.
Journal of Logic Programming, 24(1-2):3-36.
Sikkel, Klaas. 1993. Parsing Schemata. Ph.D.
thesis, University of Twente, Enschede,
The Netherlands.
Stolcke, Andreas. 1993. An efficient
probabilistic context-free parsing
algorithm that computes prefix
probabilities. Technical Report TR-93-065,
International Computer Science Institute,
Berkeley, CA. Available as
cmp-lg/9411029.
Teitelbaum, Ray. 1973. Context-free error
analysis by evaluation of algebraic power
series. In Proceedings of the Fifth Annual
ACM Symposium on Theory of Computing,
pages 196-199, Austin, TX.
Tendeau, Frederic. 1997a. Computing
abstract decorations of parse forests using
dynamic programming and algebraic
power series. Theoretical Computer Science.
To appear.
Tendeau, Frederic. 1997b. An Earley
algorithm for generic attribute augmented
grammars and applications. In Proceedings
of the International Workshop on Parsing
Technologies 1997, pages 199-209.
Viterbi, Andrew J. 1967. Error bounds for
convolutional codes and an
asymptotically optimum decoding
algorithm. IEEE Transactions on Information
Theory, IT-13:260-267.
</reference>
<page confidence="0.998723">
605
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968822">
<title confidence="0.99996">Semiring Parsing</title>
<author confidence="0.999927">Joshua Goodman</author>
<affiliation confidence="0.993995">Microsoft Research</affiliation>
<abstract confidence="0.996280875">We synthesize work on parsing algorithms, deductive parsing, and the theory of algebra applied to formal languages into a general system for describing parsers. Each parser performs abstract computations using the operations of a semiring. The system allows a single, simple representation to be used for describing parsers that compute recognition, derivation forests, Viterbi, n-best, inside values, and other values, simply by substituting the operations of different semirings. We also show how to use the same representation, interpreted differently, to compute outside values. The system can be used to describe a wide variety of parsers, including Earley&apos;s algorithm, tree adjoining grammar parsing, Graham Harrison Ruzzo parsing, and prefix value computation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Stan Chen</author>
<author>Barbara Grosz</author>
<author>Luke Hunsberger</author>
<author>Fernando Pereira</author>
<author>Stuart Shieber</author>
</authors>
<title>I would like to thank</title>
<marker>Chen, Grosz, Hunsberger, Pereira, Shieber, </marker>
<rawString>I would like to thank Stan Chen, Barbara Grosz, Luke Hunsberger, Fernando Pereira, and Stuart Shieber, for helpful comments and discussions, as well as the anonymous reviewers for their comments on earlier drafts. This work was funded in part by the National Science Foundation through Grant IRI-9350192, Grant IRI-9712068, and an NSF Graduate Student Fellowship.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Proceedings of the Spring Conference of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<location>Boston, MA,</location>
<contexts>
<context position="4545" citStr="Baker 1979" startWordPosition="792" endWordPosition="793">*/ fort := 1 to 1-1 /* split length */ for each rule A -4 BC E R chart[s, A, s+1] := chart[s, A, s+1] + (chart[s, B, s+t] x chart[s+t,C, s+1] x P(A BC)); return chart[1, S, n+1]; Figure 2 CKY inside algorithm. with a particular derivation, equal to the product of the rule probabilities used in the derivation, or to associate a probability with a set of derivations, A w,. equal to the sum of the probabilities of the individual derivations. We call this latter probability the inside probability of i,A,j. We can rewrite the CKY algorithm to compute the inside probabilities, as shown in Figure 2 (Baker 1979; Lan i and Young 1990). Notice how similar the inside algorithm is to the recognition algorithm: essentially, all that has been done is to substitute + for V, x for A, and P(A w5) and P(A —&gt; BC) for TRUE. For many parsing algorithms, this, or a similarly simple modification, is all that is needed to create a probabilistic version of the algorithm. On the other hand, a simple substitution is not always sufficient. To give a trivial example, if in the CKY recognition algorithm we had written chart[s, A, s+1] := chart[s, A, s+1] v chart[s, B, s+t] A chart[s+t, C, s+1]; instead of the less natura</context>
<context position="53459" citStr="Baker 1979" startWordPosition="9466" endWordPosition="9467">us section showed how to compute several of the most commonly used values for parsers, including Boolean, inside, Viterbi, counting, and derivation forest values, among others. Noticeably absent from the list are the outside probabilities, which we define below. In general, computing outside probabilities is significantly more complicated than computing inside probabilities. In this section, we show how to compute outside probabilities from the same item-based descriptions used for computing inside values. Outside probabilities have many uses, including for reestimating grammar probabilities (Baker 1979), for improving parser performance on some criteria (Goodman 1996b), for speeding parsing in some formalisms, such as data-oriented parsing (Goodman 1996a), and for good thresholding algorithms (Goodman 1997). We will show that by substituting other semirings, we can get values analogous to the outside probabilities for any commutative semiring; elsewhere (Goodman 1998) we have shown that we can get similar values for many noncommutative semirings as well. We will refer to these analogous quantities as reverse values. For instance, the quantity analogous to the outside value for the Viterbi se</context>
<context position="79491" citStr="Baker (1979)" startWordPosition="13888" endWordPosition="13889">t. This format is roughly the format we have used here, although there are differences due to the fact that their work was strictly in the Boolean semiring. Work in statistical parsing has also greatly influenced this work. We can trace this work back to research in HMMs by Baum and his colleagues (Baum and Eagon 1967; Baum 1972). In particular, the work of Baum developed the concept of backward probabilities (in the inside semiring), as well as many of the techniques for computing in the inside semiring. Viterbi (1967) developed corresponding algorithms for computing in the Viterbi semiring. Baker (1979) extended the work of Baum and his colleagues to PCFGs, including to computation of the outside values (or reverse inside values in our terminology). Baker&apos;s work is described by Lan i and Young (1990). Baker&apos;s work was only for PCFGs in CNF, avoiding the need to compute infinite summations. Jelinek and Lafferty (1991) showed how to compute some of the infinite summations in the inside semiring, those needed to compute the prefix probabilities of PCFGs in CNF. Stolcke (1993) showed how to use the same techniques to compute inside probabilities for Earley parsing, dealing with the difficult pro</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>Baker, James K. 1979. Trainable grammars for speech recognition. In Proceedings of the Spring Conference of the Acoustical Society of America, pages 547-550, Boston, MA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process.</title>
<date>1972</date>
<journal>Inequalities,</journal>
<pages>3--1</pages>
<contexts>
<context position="79210" citStr="Baum 1972" startWordPosition="13845" endWordPosition="13846">oduced. More recent work (Pereira and Warren 1983; Pereira and Shieber 1987) demonstrates how to use deduction engines for parsing. Finally, both Shieber, Schabes, and Pereira (1995) and Sikkel (1993) have shown how to specify parsers in a simple, interpretable, item-based format. This format is roughly the format we have used here, although there are differences due to the fact that their work was strictly in the Boolean semiring. Work in statistical parsing has also greatly influenced this work. We can trace this work back to research in HMMs by Baum and his colleagues (Baum and Eagon 1967; Baum 1972). In particular, the work of Baum developed the concept of backward probabilities (in the inside semiring), as well as many of the techniques for computing in the inside semiring. Viterbi (1967) developed corresponding algorithms for computing in the Viterbi semiring. Baker (1979) extended the work of Baum and his colleagues to PCFGs, including to computation of the outside values (or reverse inside values in our terminology). Baker&apos;s work is described by Lan i and Young (1990). Baker&apos;s work was only for PCFGs in CNF, avoiding the need to compute infinite summations. Jelinek and Lafferty (1991</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>Baum, Leonard E. 1972. An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process. Inequalities, 3:1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
<author>J A Eagon</author>
</authors>
<title>An inequality with application to statistical estimation for probabilistic functions of Markov processes and to a model for ecology.</title>
<date>1967</date>
<journal>Bulletin of the American Mathematicians Society,</journal>
<pages>73--360</pages>
<contexts>
<context position="79198" citStr="Baum and Eagon 1967" startWordPosition="13841" endWordPosition="13844">ms in parsers is introduced. More recent work (Pereira and Warren 1983; Pereira and Shieber 1987) demonstrates how to use deduction engines for parsing. Finally, both Shieber, Schabes, and Pereira (1995) and Sikkel (1993) have shown how to specify parsers in a simple, interpretable, item-based format. This format is roughly the format we have used here, although there are differences due to the fact that their work was strictly in the Boolean semiring. Work in statistical parsing has also greatly influenced this work. We can trace this work back to research in HMMs by Baum and his colleagues (Baum and Eagon 1967; Baum 1972). In particular, the work of Baum developed the concept of backward probabilities (in the inside semiring), as well as many of the techniques for computing in the inside semiring. Viterbi (1967) developed corresponding algorithms for computing in the Viterbi semiring. Baker (1979) extended the work of Baum and his colleagues to PCFGs, including to computation of the outside values (or reverse inside values in our terminology). Baker&apos;s work is described by Lan i and Young (1990). Baker&apos;s work was only for PCFGs in CNF, avoiding the need to compute infinite summations. Jelinek and La</context>
</contexts>
<marker>Baum, Eagon, 1967</marker>
<rawString>Baum, Leonard E. and J. A. Eagon. 1967. An inequality with application to statistical estimation for probabilistic functions of Markov processes and to a model for ecology. Bulletin of the American Mathematicians Society, 73:360-363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvie Billot</author>
<author>Bernard Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting,</booktitle>
<pages>143--151</pages>
<institution>Vancouver. Association for Computational Linguistics.</institution>
<contexts>
<context position="37048" citStr="Billot and Lang (1989)" startWordPosition="6629" endWordPosition="6632">ement, although it could not occur in a valid grammar derivation, or in a correctly functioning parser. An example of concatenation of sets is {(A —&gt; a), (B —&gt; b)} {(C —&gt; c), (D —&gt; d)} = {(A —&gt; a, C c), (A —&gt; a, D —+ —&gt; b, C —&gt; c), (B —&gt; b, D —&gt; d)} . Potentially, derivation forests are sets of infinitely many items. However, it is still possible to store them using finite-sized representations. Elsewhere (Goodman 1998), we show how to implement derivation forests efficiently, using pointers, in a manner analogous to the typical implementation of parse forests, and also similar to the work of Billot and Lang (1989). Using these techniques, both union and concatenation can be implemented in constant time, and even infinite unions will be reasonably efficient. 2.5.2 Viterbi-derivation Semiring. The Viterbi-derivation semiring computes the most probable derivation of the sentence, given a probabilistic grammar. Elements of this semiring are a pair, a real number v and a derivation forest E, i.e., the set of derivations with score v. We define max, the additive operator, as Vit (v , E) if v &gt; w max( (v, E), (w, D)) = (w, D) if v &lt; w Vit (v, E u D) if v = w In typical practical Viterbi parsers, when two deri</context>
<context position="50015" citStr="Billot and Lang (1989)" startWordPosition="8905" endWordPosition="8908">ficiently represent derivation forests. Pointers, in various forms, allow one to efficiently represent infinite circular references, either directly (Goodman 1999), or indirectly (Goodman 1998). Roughly, the algorithm we will use is to compute the derivation subgraph, and then create pointers analogous to the directed edges in the derivation subgraph, including pointers in loops whenever there is a loop in the derivation subgraph (corresponding to an infinite number of derivations). Details are given elsewhere (Goodman 1998). As in the finite case, this representation is equivalent to that of Billot and Lang (1989). For the Viterbi semiring, the algorithm is analogous to the Boolean case. Derivations using loops in these semirings will always have values no greater than derivations not using loops, since the value with the loop will be the same as some value without the loop, multiplied by some set of rule probabilities that are at most 1. Since the additive operation is max, these lower (or at most equal) looping derivations do not change the value of an item. Therefore, we can simply compute successive generations until values fail to change from one iteration to the next. Now, consider implementation</context>
</contexts>
<marker>Billot, Lang, 1989</marker>
<rawString>Billot, Sylvie and Bernard Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings of the 27th Annual Meeting, pages 143-151, Vancouver. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>Marcel-Paul Schtitzenberger</author>
</authors>
<title>The algebraic theory of context-free languages.</title>
<date>1963</date>
<booktitle>Computer Programming and Formal Systems.</booktitle>
<pages>118--161</pages>
<editor>In P. Braffort and D. Hirschberg, editors,</editor>
<publisher>North-Holland,</publisher>
<contexts>
<context position="6612" citStr="Chomsky and Schtitzenberger (1963)" startWordPosition="1135" endWordPosition="1138"> max for V. Less commonly computed is the total number of parses of the sentence, which, like the inside values, can be computed using multiplication and addition; unlike for the inside values, the probabilities of the rules are not multiplied into the scores. There is one last commonly computed quantity, the outside probabilities, which we will describe later, in Section 4. One of the key points of this paper is that all five of these commonly computed quantities can be described as elements of complete semirings (Kuich 1997). The relationship between grammars and semirings was discovered by Chomsky and Schtitzenberger (1963), and for parsing with the CKY algorithm, dates back to Teitelbaum (1973). A complete semiring is a set of values over which a multiplicative operator and a commutative additive operator have been defined, and for which infinite summations are defined. For parsing algorithms satisfying certain conditions, the multiplicative and additive operations of any complete semiring can be used in place of A and V. and correct values will be returned. We will give a simple normal form for describing parsers, then precisely define complete semirings, and the conditions for correctness. We now describe our</context>
</contexts>
<marker>Chomsky, Schtitzenberger, 1963</marker>
<rawString>Chomsky, Noam and Marcel-Paul Schtitzenberger. 1963. The algebraic theory of context-free languages. In P. Braffort and D. Hirschberg, editors, Computer Programming and Formal Systems. North-Holland, pages 118-161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>Cormen, Thomas H., Charles E. Leiserson, and Ronald L. Rivest. 1990. Introduction to Algorithms. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<pages>13--94</pages>
<contexts>
<context position="11295" citStr="Earley 1970" startWordPosition="2053" endWordPosition="2054"> [1, S, 4] The first has the value 1 x 0.8 x 0.128 = 0.1024, and the second also has the value 0.1024. When there is more than one way to derive a value for an item, we use the additive operator of the semiring to sum them up. Thus, [1, S. 4] = 0.2048. Since [1, S, 4] is the goal item for the CKY parser, we know that the inside value for xxx is 0.2048. The goal item exactly parallels the return statement of the CKY inside algorithm. 1.1 Earley Parsing Many parsers are much more complicated than the CKY parser, and we will need to expand our notation a bit to describe them. Earley&apos;s algorithm (Earley 1970) exhibits most of the complexities we wish to discuss. Earley&apos;s algorithm is often described as a bottom-up parser with top-down filtering. In a probabilistic framework, the bottomup sections compute probabilities, while the top-down filtering nonprobabilistically removes items that cannot be derived. To capture these differences, we expand our notation for deduction rules, to the following: Ai • • • Ak Ci • • • Ci Ci • • CI are side conditions, interpreted nonprobabilistically, while A1 • Ak are main conditions with values in whichever semiring we are using.&apos; While the values of all main cond</context>
<context position="78554" citStr="Earley (1970)" startWordPosition="13735" endWordPosition="13736"> work in this area is extensive, including work in deductive parsing, work in statistical parsing, and work in the combination of formal language theory and algebra. This paper can be thought of as synthetic, combining the work in all three areas, although in the course of synthesis, several general formulas have been found, most notably the general formula for reverse values. A comprehensive examination of all three areas is beyond the scope of this paper, but we can touch on a few significant areas of each. First, there is the work in deductive parsing. This work in some sense dates back to Earley (1970), in which the use of items in parsers is introduced. More recent work (Pereira and Warren 1983; Pereira and Shieber 1987) demonstrates how to use deduction engines for parsing. Finally, both Shieber, Schabes, and Pereira (1995) and Sikkel (1993) have shown how to specify parsers in a simple, interpretable, item-based format. This format is roughly the format we have used here, although there are differences due to the fact that their work was strictly in the Boolean semiring. Work in statistical parsing has also greatly influenced this work. We can trace this work back to research in HMMs by </context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13:94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Efficient algorithms for parsing the DOP model.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>143--152</pages>
<note>Available as cmp-lg/9604008.</note>
<contexts>
<context position="53524" citStr="Goodman 1996" startWordPosition="9476" endWordPosition="9477">ed values for parsers, including Boolean, inside, Viterbi, counting, and derivation forest values, among others. Noticeably absent from the list are the outside probabilities, which we define below. In general, computing outside probabilities is significantly more complicated than computing inside probabilities. In this section, we show how to compute outside probabilities from the same item-based descriptions used for computing inside values. Outside probabilities have many uses, including for reestimating grammar probabilities (Baker 1979), for improving parser performance on some criteria (Goodman 1996b), for speeding parsing in some formalisms, such as data-oriented parsing (Goodman 1996a), and for good thresholding algorithms (Goodman 1997). We will show that by substituting other semirings, we can get values analogous to the outside probabilities for any commutative semiring; elsewhere (Goodman 1998) we have shown that we can get similar values for many noncommutative semirings as well. We will refer to these analogous quantities as reverse values. For instance, the quantity analogous to the outside value for the Viterbi semiring will be called the reverse Viterbi value. Notice that the </context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Goodman, Joshua. 1996a. Efficient algorithms for parsing the DOP model. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 143-152, May. Available as cmp-lg/9604008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting,</booktitle>
<pages>177--183</pages>
<location>Santa Cruz, CA,</location>
<contexts>
<context position="53524" citStr="Goodman 1996" startWordPosition="9476" endWordPosition="9477">ed values for parsers, including Boolean, inside, Viterbi, counting, and derivation forest values, among others. Noticeably absent from the list are the outside probabilities, which we define below. In general, computing outside probabilities is significantly more complicated than computing inside probabilities. In this section, we show how to compute outside probabilities from the same item-based descriptions used for computing inside values. Outside probabilities have many uses, including for reestimating grammar probabilities (Baker 1979), for improving parser performance on some criteria (Goodman 1996b), for speeding parsing in some formalisms, such as data-oriented parsing (Goodman 1996a), and for good thresholding algorithms (Goodman 1997). We will show that by substituting other semirings, we can get values analogous to the outside probabilities for any commutative semiring; elsewhere (Goodman 1998) we have shown that we can get similar values for many noncommutative semirings as well. We will refer to these analogous quantities as reverse values. For instance, the quantity analogous to the outside value for the Viterbi semiring will be called the reverse Viterbi value. Notice that the </context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Goodman, Joshua. 1996b. Parsing algorithms and metrics. In Proceedings of the 34th Annual Meeting, pages 177-183, Santa Cruz, CA, June. Association for Computational Linguistics. Available as cmp-lg/9605036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Global thresholding and multiple-pass parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>11--25</pages>
<contexts>
<context position="53667" citStr="Goodman 1997" startWordPosition="9495" endWordPosition="9496">st are the outside probabilities, which we define below. In general, computing outside probabilities is significantly more complicated than computing inside probabilities. In this section, we show how to compute outside probabilities from the same item-based descriptions used for computing inside values. Outside probabilities have many uses, including for reestimating grammar probabilities (Baker 1979), for improving parser performance on some criteria (Goodman 1996b), for speeding parsing in some formalisms, such as data-oriented parsing (Goodman 1996a), and for good thresholding algorithms (Goodman 1997). We will show that by substituting other semirings, we can get values analogous to the outside probabilities for any commutative semiring; elsewhere (Goodman 1998) we have shown that we can get similar values for many noncommutative semirings as well. We will refer to these analogous quantities as reverse values. For instance, the quantity analogous to the outside value for the Viterbi semiring will be called the reverse Viterbi value. Notice that the inside semiring values of a hidden Markov model (1-IMM) correspond to the forward values of HMMs, and the reverse inside values of an HMM corre</context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>Goodman, Joshua. 1997. Global thresholding and multiple-pass parsing. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 11-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing Inside-Out.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University.</institution>
<note>Available as cmp-lg/9805007 and from http://www.eecs.harvard.edu/ —goodman/thesis.ps.</note>
<contexts>
<context position="36849" citStr="Goodman 1998" startWordPosition="6599" endWordPosition="6600"> an identity operation. Derivations need not be complete. For instance, for CFGs, {(X —&gt; YZ, Y —&gt; y)} is a valid element, as is {(Y —&gt; y, X —&gt; x)}. In fact, { (X —&gt; A, B —&gt; b)} is a valid element, although it could not occur in a valid grammar derivation, or in a correctly functioning parser. An example of concatenation of sets is {(A —&gt; a), (B —&gt; b)} {(C —&gt; c), (D —&gt; d)} = {(A —&gt; a, C c), (A —&gt; a, D —+ —&gt; b, C —&gt; c), (B —&gt; b, D —&gt; d)} . Potentially, derivation forests are sets of infinitely many items. However, it is still possible to store them using finite-sized representations. Elsewhere (Goodman 1998), we show how to implement derivation forests efficiently, using pointers, in a manner analogous to the typical implementation of parse forests, and also similar to the work of Billot and Lang (1989). Using these techniques, both union and concatenation can be implemented in constant time, and even infinite unions will be reasonably efficient. 2.5.2 Viterbi-derivation Semiring. The Viterbi-derivation semiring computes the most probable derivation of the sentence, given a probabilistic grammar. Elements of this semiring are a pair, a real number v and a derivation forest E, i.e., the set of der</context>
<context position="39520" citStr="Goodman 1998" startWordPosition="7062" endWordPosition="7063">f the two derivation forests. 2.5.3 Viterbi-n-best Semiring. The last kind of derivation semiring is the Viterbi-nbest semiring, which is used for constructing n-best lists. Intuitively, the value of a string using this semiring will be the n most likely derivations of that string (unless there are fewer than n total derivations.) In practice, this is actually how a Viterbi-n-best semiring would typically be implemented. From a theoretical viewpoint, however, this implementation is inadequate, since we must also define infinite sums and be sure that the distributive property holds. Elsewhere (Goodman 1998), we give a mathematically precise definition of the semiring that handles these cases. 3. Efficient Computation of Item Values Recall that the value of an item x is just V(x) = @DEinner(x)11(D), the sum of the values of all derivation trees headed by x. This definition may require summing over exponentially many or even infinitely many terms. In this section, we give relatively efficient formulas for computing the values of items. There are three cases that must be handled. First is the base case, when x is a rule. In this case, inner(x) is trivially {(x)}, the set containing the single deriv</context>
<context position="44420" citStr="Goodman 1998" startWordPosition="7975" endWordPosition="7976">onsidered in this paper), an infinite sum is equal to the supremum of the partial sums (Kuich 1997, 613). Thus, V(x) = isp V(D) = sup V&lt;g(x,B) DEInner(x,B) It will be easier to compute the supremum if we find a simple formula for V&lt;g(x,B). Notice that for items x e B, there will be no generation 0 derivations, so V&lt;0(x, B) = 0. Thus, generation 0 makes a trivial base for a recursive formula. Now, we can consider the general case: Theorem 3 For x an item in a looping bucket B, and for g &gt; 1, V&lt;g(x,B) = ED ph V(a1) (7) tV&lt;g_i(ai,B) if a B if ai E B ak s ak The proof parallels that of Theorem 2 (Goodman 1998). 3.2 Solving the Infinite Summation A formula for V&lt;g(x,B) is useful, but what we really need is specific techniques for computing the supremum, V(x) = supg V&lt;g(x,B). For all w-continuous semirings, the supremum of iteratively approximating the value of a set of polynomial equations, as we are essentially doing in Equation 7, is equal to the smallest solution to the equations (Kuich 1997, 622). In particular, consider the equations: 11(x,B) = V(ai) if ai B (8) ak s t al ak 1V&lt;,„(ai,B) if ai E B 1=1 589 Computational Linguistics Volume 25, Number 4 where V &lt;0„(x, B) can be thought of as indica</context>
<context position="49586" citStr="Goodman 1998" startWordPosition="8840" endWordPosition="8841">n items in loops: these items also have infinite value. Any other items can only be derived in finitely many ways using items in the current bucket, so compute successive generations until the values of these items do not change. The method for solving the infinite summation for the derivation forest semiring depends on the implementation of derivation forests. Essentially, that representation will use pointers to efficiently represent derivation forests. Pointers, in various forms, allow one to efficiently represent infinite circular references, either directly (Goodman 1999), or indirectly (Goodman 1998). Roughly, the algorithm we will use is to compute the derivation subgraph, and then create pointers analogous to the directed edges in the derivation subgraph, including pointers in loops whenever there is a loop in the derivation subgraph (corresponding to an infinite number of derivations). Details are given elsewhere (Goodman 1998). As in the finite case, this representation is equivalent to that of Billot and Lang (1989). For the Viterbi semiring, the algorithm is analogous to the Boolean case. Derivations using loops in these semirings will always have values no greater than derivations </context>
<context position="51059" citStr="Goodman 1998" startWordPosition="9084" endWordPosition="9085">ge the value of an item. Therefore, we can simply compute successive generations until values fail to change from one iteration to the next. Now, consider implementations of the Viterbi-derivation semiring in practice, in which we keep only a representative derivation, rather than the whole derivation forest. In this case, loops do not change values, and we use the same algorithm as for the Viterbi semiring. In an implementation of the Viterbi-n-best semiring, in practice, loops can change values, but at most n times, so the same algorithm used for the Viterbi semiring still works. Elsewhere (Goodman 1998), we describe theoretically correct implementations for both the Viterbi-derivation and Viterbin-best semirings that keep all values in the event of ties, preserving addition&apos;s associativity. The last semiring we consider is the inside semiring. This semiring is the most difficult. There are two cases of interest, one of which we can solve exactly, and the other of which requires approximations. In many cases involving looping buckets, all deduction rules will be of the form cf7x, where al and b are items in the looping bucket, and x is either a rule, or an item in a previously computed bucket</context>
<context position="53831" citStr="Goodman 1998" startWordPosition="9519" endWordPosition="9520">lities. In this section, we show how to compute outside probabilities from the same item-based descriptions used for computing inside values. Outside probabilities have many uses, including for reestimating grammar probabilities (Baker 1979), for improving parser performance on some criteria (Goodman 1996b), for speeding parsing in some formalisms, such as data-oriented parsing (Goodman 1996a), and for good thresholding algorithms (Goodman 1997). We will show that by substituting other semirings, we can get values analogous to the outside probabilities for any commutative semiring; elsewhere (Goodman 1998) we have shown that we can get similar values for many noncommutative semirings as well. We will refer to these analogous quantities as reverse values. For instance, the quantity analogous to the outside value for the Viterbi semiring will be called the reverse Viterbi value. Notice that the inside semiring values of a hidden Markov model (1-IMM) correspond to the forward values of HMMs, and the reverse inside values of an HMM correspond to the backwards values. Compare the outside algorithm (Baker 1979; Lan i and Young 1990), given in Figure 7, to the inside algorithm of Figure 2. Notice that</context>
<context position="63786" citStr="Goodman 1998" startWordPosition="11341" endWordPosition="11342">infinite sum, and the use of the concept of generation. (DakEitaker(4) IED V(Dak)) (15) 596 Goodman Semiring Parsing We define the generation g of an outer tree D of item x in bucket B to be the number of items in bucket B on the path between the root and the removal point, inclusive. We can then let Z&lt;g(x,B) represent the sum of the values of all trees headed by x of generation at most g. In the base case, Z&lt;0(x,B) = 0. For w-continuous semirings, Z&lt;g(x,B) approaches Z(x) as g approaches oo. We can give a recursive equation for Z&lt;g(x,B) as follows, using a proof similar to that of Theorem 5 (Goodman 1998): Theorem 6 For items x E B and g &gt; 1, Z&lt;g(x,B) = (8) V(ai))Z&lt;g_i(b,B) if b E B ® 1Z(b) if b B (16) ak,b s.t. ak. A x—ai 5. Semiring Parser Execution Executing a semiring parser is fairly simple. There is, however, one issue that must be dealt with before we can actually begin parsing. A semiring parser computes the values of items in the order of the buckets they fall into. Thus, before we can begin parsing, we need to know which items fall into which buckets, and the ordering of those buckets. There are three approaches to determining the buckets and ordering that we will discuss in this sec</context>
<context position="69556" citStr="Goodman 1998" startWordPosition="12306" endWordPosition="12307">rking system, we substitute semiring-specific code for this section, as described in Section 3.2. If the bucket is not a looping bucket, we simply compute all of the possible instantiations that could contribute to the values of items in that bucket. Finally, we return the value of the goal item. The reverse semiring parser interpreter is very similar to the forward semiring parser interpreter. The differences are that in the reverse semiring parser interpreter, we traverse the buckets in reverse order, and we use the formulas for the reverse values, rather than the forward values. Elsewhere (Goodman 1998), we give a simple inductive proof to show that both interpreters compute the correct values. 598 Goodman Semiring Parsing There are two other implementation issues. First, for some parsers, it will be possible to discard some items. That is, some items serve the role of temporary variables, and can be discarded after they are no longer needed, especially if only the forward values are going to be computed. Also, some items do not depend on the input string, but only on the rule value function of the grammar. The values of these items can be precomputed. 6. Examples In this section, we survey </context>
<context position="71726" citStr="Goodman 1998" startWordPosition="12674" endWordPosition="12675">get a compact representation of all state sequences in an NFA for an input string. A single item-based description can be used to find all of these values. 6.2 Prefix Values For language modeling, it may be useful to compute the prefix probability of a string. That is, given a string wn, we may wish to know the total probability of all sentences beginning with that string, EP(S w1. • • wnvi • • • 0) k&gt;o,vi,.••,vk where 01 vk represent words that could possibly follow w1 wn. Jelinek and Lafferty (1991) and Stolcke (1993) both give algorithms for computing these prefix probabilities. Elsewhere (Goodman 1998), we show how to produce an item-based description of a prefix parser. There are two main advantages to using an item-based description: ease of derivation, and reusability. First, the conventional derivations are somewhat complex, requiring a fair amount of inside-semiring-specific mathematics. In contrast, using item-based descriptions, we only need to derive a parser that has the property that there is one item derivation for each (complete) grammar derivation that would produce the prefix. The value of any prefix given the parser will then automatically be the sum of all grammar derivation</context>
<context position="74407" citStr="Goodman (1998)" startWordPosition="13091" endWordPosition="13092">id the limitations of previous approaches using two techniques. One technique is to compute derivation trees, rather than parse trees, for TAGs. Computing derivation trees for TAGs is significantly easier than computing parse trees, since the derivation trees are context-free. The other trick we use is to create a set of equations for each grammar and string length rather than creating a set of equations for each grammar, as earlier formulations did. Because the number of equations grows with the string length with our technique, we can recognize strings in weakly context-sensitive languages. Goodman (1998) gives a further explication of this subject, including an item-based description for a simple TAG parser. 6.4 Tomita Parsing Our goal in this section has been to show that item-based descriptions can be used to simply describe almost all parsers of interest. One parsing algorithm that would seem particularly difficult to describe is Tomita&apos;s graph-structured-stack LR parsing algorithm. This algorithm at first glance bears little resemblance to other parsing algorithms. Despite this lack of similarity, Sikkel (1993) gives an item-based description for a Tomita-style parser for the Boolean semi</context>
</contexts>
<marker>Goodman, 1998</marker>
<rawString>Goodman, Joshua. 1998. Parsing Inside-Out. Ph.D. thesis, Harvard University. Available as cmp-lg/9805007 and from http://www.eecs.harvard.edu/ —goodman/thesis.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--4</pages>
<marker>Goodman, 1999</marker>
<rawString>Goodman, Joshua. 1999. Semiring parsing. Computational Linguistics, 25(4):573-605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan L Graham</author>
<author>Michael A Harrison</author>
<author>Walter L Ruzzo</author>
</authors>
<title>An improved context-free recognizer.</title>
<date>1980</date>
<journal>ACM Transactions on Programming Languages and Systems,</journal>
<pages>2--3</pages>
<marker>Graham, Harrison, Ruzzo, 1980</marker>
<rawString>Graham, Susan L., Michael A. Harrison, and Walter L. Ruzzo. 1980. An improved context-free recognizer. ACM Transactions on Programming Languages and Systems, 2(3):415-462, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
</authors>
<title>Computation of the probability of initial substring generation by stochastic context-free grammars. Computational Linguistics,</title>
<date>1991</date>
<pages>315--323</pages>
<contexts>
<context position="71619" citStr="Jelinek and Lafferty (1991)" startWordPosition="12658" endWordPosition="12661">rmine how many state sequences there are in the NFA for a given string; and we can use the derivation forest semiring to get a compact representation of all state sequences in an NFA for an input string. A single item-based description can be used to find all of these values. 6.2 Prefix Values For language modeling, it may be useful to compute the prefix probability of a string. That is, given a string wn, we may wish to know the total probability of all sentences beginning with that string, EP(S w1. • • wnvi • • • 0) k&gt;o,vi,.••,vk where 01 vk represent words that could possibly follow w1 wn. Jelinek and Lafferty (1991) and Stolcke (1993) both give algorithms for computing these prefix probabilities. Elsewhere (Goodman 1998), we show how to produce an item-based description of a prefix parser. There are two main advantages to using an item-based description: ease of derivation, and reusability. First, the conventional derivations are somewhat complex, requiring a fair amount of inside-semiring-specific mathematics. In contrast, using item-based descriptions, we only need to derive a parser that has the property that there is one item derivation for each (complete) grammar derivation that would produce the pr</context>
<context position="79811" citStr="Jelinek and Lafferty (1991)" startWordPosition="13939" endWordPosition="13942">and Eagon 1967; Baum 1972). In particular, the work of Baum developed the concept of backward probabilities (in the inside semiring), as well as many of the techniques for computing in the inside semiring. Viterbi (1967) developed corresponding algorithms for computing in the Viterbi semiring. Baker (1979) extended the work of Baum and his colleagues to PCFGs, including to computation of the outside values (or reverse inside values in our terminology). Baker&apos;s work is described by Lan i and Young (1990). Baker&apos;s work was only for PCFGs in CNF, avoiding the need to compute infinite summations. Jelinek and Lafferty (1991) showed how to compute some of the infinite summations in the inside semiring, those needed to compute the prefix probabilities of PCFGs in CNF. Stolcke (1993) showed how to use the same techniques to compute inside probabilities for Earley parsing, dealing with the difficult problems of unary transitions, and the more difficult problems of epsilon transitions. He thus solved all of the important problems encountered in using an item-based parser to compute the inside and outside values (forward and reverse inside values); he also showed how to compute the forward Viterbi values. 601 Computati</context>
</contexts>
<marker>Jelinek, Lafferty, 1991</marker>
<rawString>Jelinek, Frederick and John D. Lafferty. 1991. Computation of the probability of initial substring generation by stochastic context-free grammars. Computational Linguistics, pages 315-323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daphne Koller</author>
<author>David McAllester</author>
<author>Avi Pfeffer</author>
</authors>
<title>Effective bayesian inference for stochastic programs.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th National Conference on Artificial Intelligence,</booktitle>
<pages>740--747</pages>
<location>Providence, RI,</location>
<marker>Koller, McAllester, Pfeffer, 1997</marker>
<rawString>Koller, Daphne, David McAllester, and Avi Pfeffer. 1997. Effective bayesian inference for stochastic programs. In Proceedings of the 14th National Conference on Artificial Intelligence, pages 740-747, Providence, RI, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Werner Kuich</author>
</authors>
<title>Semirings and formal power series: Their relevance to formal languages and automata.</title>
<date>1997</date>
<booktitle>In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages.</booktitle>
<pages>609--677</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin,</location>
<contexts>
<context position="6510" citStr="Kuich 1997" startWordPosition="1124" endWordPosition="1125">ion of the sentence. This can typically be computed by substituting x for A and max for V. Less commonly computed is the total number of parses of the sentence, which, like the inside values, can be computed using multiplication and addition; unlike for the inside values, the probabilities of the rules are not multiplied into the scores. There is one last commonly computed quantity, the outside probabilities, which we will describe later, in Section 4. One of the key points of this paper is that all five of these commonly computed quantities can be described as elements of complete semirings (Kuich 1997). The relationship between grammars and semirings was discovered by Chomsky and Schtitzenberger (1963), and for parsing with the CKY algorithm, dates back to Teitelbaum (1973). A complete semiring is a set of values over which a multiplicative operator and a commutative additive operator have been defined, and for which infinite summations are defined. For parsing algorithms satisfying certain conditions, the multiplicative and additive operations of any complete semiring can be used in place of A and V. and correct values will be returned. We will give a simple normal form for describing pars</context>
<context position="18096" citStr="Kuich 1997" startWordPosition="3235" endWordPosition="3236">alue is the respective identity element, 0 or 1. We also assume that x = x = 0 for all x. In other words, a semiring is just like a ring, except that the additive operator need not have an inverse. We will write (A, ED, 0, 0, 1) to indicate a semiring over the set A with additive operator 0, multiplicative operator 0, additive identity 0, and multiplicative identity 1. For parsers with loops, i.e., those in which an item can be used to derive itself, we will also require that sums of an infinite number of elements be well defined. In particular, we will require that the semirings be complete (Kuich 1997, 611). This means that sums of an infinite number of elements should be associative and commutative, just like finite sums, and that multiplication should distribute over infinite sums, just as it does over finite ones. All of the semirings we will deal with in this paper are complete.2 All of the semirings we discuss here are also co-continuous. Intuitively, this means that if any partial sum of an infinite sequence is less than or equal to some value, 2 Completeness is a somewhat stronger condition than we really need; we could, instead, require that limits be appropriately defined for thos</context>
<context position="22032" citStr="Kuich 1997" startWordPosition="3892" endWordPosition="3893">whether an item can be deduced or not. Thus, in these simple systems, the order of processing items is relatively unimportant, as long as some simple constraints are met. On the other hand, for a semiring such as the inside semiring, there are important ordering constraints: we cannot compute the inside value of an item until the inside values of 3 To be more precise, all semirings we discuss here are naturally ordered, meaning that we can define a partial ordering, C, such that x L y if and only if there exists z such that x z = y. We call a naturally ordered complete semiring co-continuous (Kuich 1997, 612) if for any sequence x2,... and for any o&lt;i&lt;n constant y, if for all n, pin E y, then ER , y. 580 Goodman Semiring Parsing all of its children have been computed. Thus, we need to impose an ordering on the items, in such a way that no item precedes any item on which it depends. We will assign each item x to a &amp;quot;bucket&amp;quot; B, writing bucket(x) = B and saying that item x is associated with B. We order the buckets in such a way that if item y depends on item x, then bucket(x) &lt; bucket(y). For some pairs of items, it may be that both depend, directly or indirectly, on each other; we associate th</context>
<context position="43905" citStr="Kuich 1997" startWordPosition="7870" endWordPosition="7871">we can encounter on a path from the root to a leaf. Consider the set of all trees of generation at most g headed by x. Call this set inner&lt;g(x,B). We can define the &lt;g generation value of an item x in bucket B, V&lt;g(x,B): V&lt;g(x,B) ,ED V(D) DEInner&lt;g(x,B) Intuitively, as g increases, for x E B, inner&lt;g(x,B) becomes closer and closer to inner(x). That is, the finite sum of values in the former approaches the infinite sum of values in the latter. For w-continuous semirings (which includes all of the semirings considered in this paper), an infinite sum is equal to the supremum of the partial sums (Kuich 1997, 613). Thus, V(x) = isp V(D) = sup V&lt;g(x,B) DEInner(x,B) It will be easier to compute the supremum if we find a simple formula for V&lt;g(x,B). Notice that for items x e B, there will be no generation 0 derivations, so V&lt;0(x, B) = 0. Thus, generation 0 makes a trivial base for a recursive formula. Now, we can consider the general case: Theorem 3 For x an item in a looping bucket B, and for g &gt; 1, V&lt;g(x,B) = ED ph V(a1) (7) tV&lt;g_i(ai,B) if a B if ai E B ak s ak The proof parallels that of Theorem 2 (Goodman 1998). 3.2 Solving the Infinite Summation A formula for V&lt;g(x,B) is useful, but what we re</context>
<context position="73168" citStr="Kuich (1997)" startWordPosition="12901" endWordPosition="12902">nd the most likely derivation that includes this prefix. With this most likely derivation, we could begin interpretation of a sentence even before the sentence was finished being spoken to a speech recognition system. We could even use the Viterbi-n-best semiring to find the n most likely derivations that include this prefix, if we wanted to take into account ambiguities present in parses of the prefix. 599 Computational Linguistics Volume 25, Number 4 6.3 Beyond Context-Free There has been quite a bit of previous work on the intersection of formal language theory and algebra, as described by Kuich (1997), among others. This previous work has made heavy use of the fact that there is a strong correspondence between algebraic equations in certain noncommutative semirings, and CFGs. This correspondence has made it possible to manipulate algebraic systems, rather than grammar systems, simplifying many operations. On the other hand, there is an inherent limit to such an approach, namely a limit to context-free systems. It is then perhaps slightly surprising that we can avoid these limitations, and create item-based descriptions of parsers for weakly context-sensitive grammars, such as tree adjoinin</context>
<context position="77293" citStr="Kuich 1997" startWordPosition="13536" endWordPosition="13537">Boolean semiring (TRUE if the sentence can be generated by the grammar, FALSE otherwise) is the same as its value under a transformed grammar. Therefore, we say that this grammar transformation is value preserving under the Boolean semiring. We can generalize this concept of value preserving to other semirings. Elsewhere (Goodman 1998), we show that using essentially the same item-based descriptions we have used for parsing, we can specify grammar transformations. The concept of value preserving grammar transformation is already known in the intersection of formal language theory and algebra (Kuich 1997; Kuich and Salomaa 1986; Teitelbaum 1973). Our contribution is to show that these value preserving transformations can be written as simple item-based descriptions, allowing the same computational machinery to be used for grammar transformations as is used for parsing, and to some extent showing the relationship between certain grammar transformations and certain parsers, such as that of Graham, Harrison, and Ruzzo (1980). This uniform method of specifying grammar transformations is similar to, but clearer than, similar techniques used with covering grammars (Nijholt 1980; Leermakers 1989). 7</context>
<context position="80956" citStr="Kuich (1997)" startWordPosition="14128" endWordPosition="14129">lso showed how to compute the forward Viterbi values. 601 Computational Linguistics Volume 25, Number 4 The final area of work is in formal language theory and algebra. Although it is not widely known, there has been quite a bit of work showing how to use formal power series to elegantly derive results in formal language theory, dating back to Chomsky and Schiitzenberger (1963). The major classic results can be derived in this framework, but with the added benefit that they apply to all commutative w-continuous semirings. The most accessible introduction to this literature we have found is by Kuich (1997). There are also books by Salomaa and Soittola (1978) and Kuich and Salomaa (1986). One piece of work deserves special mention. Teitelbaum (1973) showed that any semiring could be used in the CKY algorithm, laying the foundation for much of the work that followed. In summary, this paper synthesizes work from several different related fields, including deductive parsing, statistical parsing, and formal language theory; we emulate and expand on the earlier synthesis of Teitelbaum. The synthesis here is powerful: by generalizing and integrating many results, we make the computation of a much wide</context>
</contexts>
<marker>Kuich, 1997</marker>
<rawString>Kuich, Werner. 1997. Semirings and formal power series: Their relevance to formal languages and automata. In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages. Springer-Verlag, Berlin, pages 609-677.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Werner Kuich</author>
<author>Arto Salomaa</author>
</authors>
<date>1986</date>
<journal>Semirings, Automata, Languages. Number</journal>
<booktitle>of EATCS Monographs on Theoretical Computer Science.</booktitle>
<volume>5</volume>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="77317" citStr="Kuich and Salomaa 1986" startWordPosition="13538" endWordPosition="13541">ring (TRUE if the sentence can be generated by the grammar, FALSE otherwise) is the same as its value under a transformed grammar. Therefore, we say that this grammar transformation is value preserving under the Boolean semiring. We can generalize this concept of value preserving to other semirings. Elsewhere (Goodman 1998), we show that using essentially the same item-based descriptions we have used for parsing, we can specify grammar transformations. The concept of value preserving grammar transformation is already known in the intersection of formal language theory and algebra (Kuich 1997; Kuich and Salomaa 1986; Teitelbaum 1973). Our contribution is to show that these value preserving transformations can be written as simple item-based descriptions, allowing the same computational machinery to be used for grammar transformations as is used for parsing, and to some extent showing the relationship between certain grammar transformations and certain parsers, such as that of Graham, Harrison, and Ruzzo (1980). This uniform method of specifying grammar transformations is similar to, but clearer than, similar techniques used with covering grammars (Nijholt 1980; Leermakers 1989). 7. Previous Work 7.1 Hist</context>
<context position="81038" citStr="Kuich and Salomaa (1986)" startWordPosition="14140" endWordPosition="14143">l Linguistics Volume 25, Number 4 The final area of work is in formal language theory and algebra. Although it is not widely known, there has been quite a bit of work showing how to use formal power series to elegantly derive results in formal language theory, dating back to Chomsky and Schiitzenberger (1963). The major classic results can be derived in this framework, but with the added benefit that they apply to all commutative w-continuous semirings. The most accessible introduction to this literature we have found is by Kuich (1997). There are also books by Salomaa and Soittola (1978) and Kuich and Salomaa (1986). One piece of work deserves special mention. Teitelbaum (1973) showed that any semiring could be used in the CKY algorithm, laying the foundation for much of the work that followed. In summary, this paper synthesizes work from several different related fields, including deductive parsing, statistical parsing, and formal language theory; we emulate and expand on the earlier synthesis of Teitelbaum. The synthesis here is powerful: by generalizing and integrating many results, we make the computation of a much wider variety of values possible. 7.2 Recent Similar Work There has also been recent s</context>
</contexts>
<marker>Kuich, Salomaa, 1986</marker>
<rawString>Kuich, Werner and Arto Salomaa. 1986. Semirings, Automata, Languages. Number 5 of EATCS Monographs on Theoretical Computer Science. Springer-Verlag, Berlin, Germany.</rawString>
</citation>
<citation valid="false">
<institution>Goodman Semiring Parsing</institution>
<marker></marker>
<rawString>Goodman Semiring Parsing</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lan</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm.</title>
<date>1990</date>
<journal>Computer Speech and Language,</journal>
<pages>4--35</pages>
<marker>Lan, Young, 1990</marker>
<rawString>Lan, K. and S. J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rene Leermakers</author>
</authors>
<title>How to cover a grammar.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting,</booktitle>
<pages>135--142</pages>
<institution>Vancouver. Association for Computational Linguistics.</institution>
<contexts>
<context position="77890" citStr="Leermakers 1989" startWordPosition="13623" endWordPosition="13624">algebra (Kuich 1997; Kuich and Salomaa 1986; Teitelbaum 1973). Our contribution is to show that these value preserving transformations can be written as simple item-based descriptions, allowing the same computational machinery to be used for grammar transformations as is used for parsing, and to some extent showing the relationship between certain grammar transformations and certain parsers, such as that of Graham, Harrison, and Ruzzo (1980). This uniform method of specifying grammar transformations is similar to, but clearer than, similar techniques used with covering grammars (Nijholt 1980; Leermakers 1989). 7. Previous Work 7.1 Historical Work The previous work in this area is extensive, including work in deductive parsing, work in statistical parsing, and work in the combination of formal language theory and algebra. This paper can be thought of as synthetic, combining the work in all three areas, although in the course of synthesis, several general formulas have been found, most notably the general formula for reverse values. A comprehensive examination of all three areas is beyond the scope of this paper, but we can touch on a few significant areas of each. First, there is the work in deduct</context>
</contexts>
<marker>Leermakers, 1989</marker>
<rawString>Leermakers, Rene. 1989. How to cover a grammar. In Proceedings of the 27th Annual Meeting, pages 135-142, Vancouver. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Nijholt</author>
</authors>
<title>Context-Free Grammars: Covers, Normal Forms, and Parsing.</title>
<date>1980</date>
<journal>Number</journal>
<booktitle>of Lecture Notes in Computer Science.</booktitle>
<volume>93</volume>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="77872" citStr="Nijholt 1980" startWordPosition="13621" endWordPosition="13622">ge theory and algebra (Kuich 1997; Kuich and Salomaa 1986; Teitelbaum 1973). Our contribution is to show that these value preserving transformations can be written as simple item-based descriptions, allowing the same computational machinery to be used for grammar transformations as is used for parsing, and to some extent showing the relationship between certain grammar transformations and certain parsers, such as that of Graham, Harrison, and Ruzzo (1980). This uniform method of specifying grammar transformations is similar to, but clearer than, similar techniques used with covering grammars (Nijholt 1980; Leermakers 1989). 7. Previous Work 7.1 Historical Work The previous work in this area is extensive, including work in deductive parsing, work in statistical parsing, and work in the combination of formal language theory and algebra. This paper can be thought of as synthetic, combining the work in all three areas, although in the course of synthesis, several general formulas have been found, most notably the general formula for reverse values. A comprehensive examination of all three areas is beyond the scope of this paper, but we can touch on a few significant areas of each. First, there is </context>
</contexts>
<marker>Nijholt, 1980</marker>
<rawString>Nijholt, Anton. 1980. Context-Free Grammars: Covers, Normal Forms, and Parsing. Number 93 of Lecture Notes in Computer Science. Springer-Verlag, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Stuart Shieber</author>
</authors>
<title>Prolog and Natural Language Analysis. Number 10 of CSU Lecture Notes. Center for the Study of Language and Information,</title>
<date>1987</date>
<location>Stanford, CA.</location>
<contexts>
<context position="78676" citStr="Pereira and Shieber 1987" startWordPosition="13754" endWordPosition="13757"> the combination of formal language theory and algebra. This paper can be thought of as synthetic, combining the work in all three areas, although in the course of synthesis, several general formulas have been found, most notably the general formula for reverse values. A comprehensive examination of all three areas is beyond the scope of this paper, but we can touch on a few significant areas of each. First, there is the work in deductive parsing. This work in some sense dates back to Earley (1970), in which the use of items in parsers is introduced. More recent work (Pereira and Warren 1983; Pereira and Shieber 1987) demonstrates how to use deduction engines for parsing. Finally, both Shieber, Schabes, and Pereira (1995) and Sikkel (1993) have shown how to specify parsers in a simple, interpretable, item-based format. This format is roughly the format we have used here, although there are differences due to the fact that their work was strictly in the Boolean semiring. Work in statistical parsing has also greatly influenced this work. We can trace this work back to research in HMMs by Baum and his colleagues (Baum and Eagon 1967; Baum 1972). In particular, the work of Baum developed the concept of backwar</context>
</contexts>
<marker>Pereira, Shieber, 1987</marker>
<rawString>Pereira, Fernando and Stuart Shieber. 1987. Prolog and Natural Language Analysis. Number 10 of CSU Lecture Notes. Center for the Study of Language and Information, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>David Warren</author>
</authors>
<title>Parsing as deduction.</title>
<date>1983</date>
<booktitle>In Proceedings of the 21st Annual Meeting,</booktitle>
<pages>137--44</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="78649" citStr="Pereira and Warren 1983" startWordPosition="13750" endWordPosition="13753">ical parsing, and work in the combination of formal language theory and algebra. This paper can be thought of as synthetic, combining the work in all three areas, although in the course of synthesis, several general formulas have been found, most notably the general formula for reverse values. A comprehensive examination of all three areas is beyond the scope of this paper, but we can touch on a few significant areas of each. First, there is the work in deductive parsing. This work in some sense dates back to Earley (1970), in which the use of items in parsers is introduced. More recent work (Pereira and Warren 1983; Pereira and Shieber 1987) demonstrates how to use deduction engines for parsing. Finally, both Shieber, Schabes, and Pereira (1995) and Sikkel (1993) have shown how to specify parsers in a simple, interpretable, item-based format. This format is roughly the format we have used here, although there are differences due to the fact that their work was strictly in the Boolean semiring. Work in statistical parsing has also greatly influenced this work. We can trace this work back to research in HMMs by Baum and his colleagues (Baum and Eagon 1967; Baum 1972). In particular, the work of Baum devel</context>
</contexts>
<marker>Pereira, Warren, 1983</marker>
<rawString>Pereira, Fernando and David Warren. 1983. Parsing as deduction. In Proceedings of the 21st Annual Meeting, pages 137-44, Cambridge, MA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arto Salomaa</author>
<author>Matti Soittola</author>
</authors>
<title>Automata-Theoretic Aspects of Formal Power Series.</title>
<date>1978</date>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="81009" citStr="Salomaa and Soittola (1978)" startWordPosition="14135" endWordPosition="14138">Viterbi values. 601 Computational Linguistics Volume 25, Number 4 The final area of work is in formal language theory and algebra. Although it is not widely known, there has been quite a bit of work showing how to use formal power series to elegantly derive results in formal language theory, dating back to Chomsky and Schiitzenberger (1963). The major classic results can be derived in this framework, but with the added benefit that they apply to all commutative w-continuous semirings. The most accessible introduction to this literature we have found is by Kuich (1997). There are also books by Salomaa and Soittola (1978) and Kuich and Salomaa (1986). One piece of work deserves special mention. Teitelbaum (1973) showed that any semiring could be used in the CKY algorithm, laying the foundation for much of the work that followed. In summary, this paper synthesizes work from several different related fields, including deductive parsing, statistical parsing, and formal language theory; we emulate and expand on the earlier synthesis of Teitelbaum. The synthesis here is powerful: by generalizing and integrating many results, we make the computation of a much wider variety of values possible. 7.2 Recent Similar Work</context>
</contexts>
<marker>Salomaa, Soittola, 1978</marker>
<rawString>Salomaa, Arto and Matti Soittola. 1978. Automata-Theoretic Aspects of Formal Power Series. Springer-Verlag, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Yves Schabes</author>
<author>Fernando Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--1</pages>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Shieber, Stuart, Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1-2):3-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaas Sikkel</author>
</authors>
<title>Parsing Schemata.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Twente,</institution>
<location>Enschede, The Netherlands.</location>
<contexts>
<context position="7333" citStr="Sikkel (1993)" startWordPosition="1254" endWordPosition="1255">alues over which a multiplicative operator and a commutative additive operator have been defined, and for which infinite summations are defined. For parsing algorithms satisfying certain conditions, the multiplicative and additive operations of any complete semiring can be used in place of A and V. and correct values will be returned. We will give a simple normal form for describing parsers, then precisely define complete semirings, and the conditions for correctness. We now describe our normal form for parsers, which is very similar to that used by Shieber, Schabes, and Pereira (1995) and by Sikkel (1993). This work can be thought of as a generalization from their work in the Boolean semiring to semirings in general. In most parsers, there is at least one chart of some form. In our normal form, we will use a corresponding, equivalent concept, items. Rather than, for instance, a chart element chart [i, A, j], we will use an item [i, A,]]. Furthermore, rather than use explicit, procedural descriptions, such as chart[s, A, s +1] := chart[s, A, s +1] v chart[s,B, s+ t] A chart[s+t, C, 9+1] A TRUE we will use inference rules such as R(A BC) [1,B, k] [k, C, j] [i, A,]] The meaning of an inference ru</context>
<context position="74928" citStr="Sikkel (1993)" startWordPosition="13169" endWordPosition="13170">ur technique, we can recognize strings in weakly context-sensitive languages. Goodman (1998) gives a further explication of this subject, including an item-based description for a simple TAG parser. 6.4 Tomita Parsing Our goal in this section has been to show that item-based descriptions can be used to simply describe almost all parsers of interest. One parsing algorithm that would seem particularly difficult to describe is Tomita&apos;s graph-structured-stack LR parsing algorithm. This algorithm at first glance bears little resemblance to other parsing algorithms. Despite this lack of similarity, Sikkel (1993) gives an item-based description for a Tomita-style parser for the Boolean semiring, which is also more efficient than Tomita&apos;s algorithm. Sikkel&apos;s parser can be easily converted to our format, where it can be used for w-continuous semirings in general. 6.5 Graham Harrison Ruzzo (GHR) Parsing Graham, Harrison, and Ruzzo (1980) describe a parser similar to Earley&apos;s, but with several speedups that lead to significant improvements. Essentially, there are three improvements in the GHR parser. First, epsilon productions are precomputed; second, unary productions are precomputed; and, finally, compl</context>
<context position="78800" citStr="Sikkel (1993)" startWordPosition="13775" endWordPosition="13776">, although in the course of synthesis, several general formulas have been found, most notably the general formula for reverse values. A comprehensive examination of all three areas is beyond the scope of this paper, but we can touch on a few significant areas of each. First, there is the work in deductive parsing. This work in some sense dates back to Earley (1970), in which the use of items in parsers is introduced. More recent work (Pereira and Warren 1983; Pereira and Shieber 1987) demonstrates how to use deduction engines for parsing. Finally, both Shieber, Schabes, and Pereira (1995) and Sikkel (1993) have shown how to specify parsers in a simple, interpretable, item-based format. This format is roughly the format we have used here, although there are differences due to the fact that their work was strictly in the Boolean semiring. Work in statistical parsing has also greatly influenced this work. We can trace this work back to research in HMMs by Baum and his colleagues (Baum and Eagon 1967; Baum 1972). In particular, the work of Baum developed the concept of backward probabilities (in the inside semiring), as well as many of the techniques for computing in the inside semiring. Viterbi (1</context>
</contexts>
<marker>Sikkel, 1993</marker>
<rawString>Sikkel, Klaas. 1993. Parsing Schemata. Ph.D. thesis, University of Twente, Enschede, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1993</date>
<tech>Technical Report TR-93-065,</tech>
<institution>International Computer Science Institute,</institution>
<location>Berkeley, CA.</location>
<note>Available as cmp-lg/9411029.</note>
<contexts>
<context position="52149" citStr="Stolcke (1993)" startWordPosition="9269" endWordPosition="9270">orm cf7x, where al and b are items in the looping bucket, and x is either a rule, or an item in a previously computed bucket. This case corresponds to the items used for deducing singleton productions, such as those Earley&apos;s algorithm uses for rules of the form A —&gt; B and B A. In this case, Equation 8 forms a set of linear equations that can be solved by matrix inversion. In the more general case, as is likely to happen with epsilon rules, we get a set of nonlinear equations, and must solve them by approximation techniques, such as simply computing successive generations for many iterations.&apos; Stolcke (1993) provides an excellent discussion of these cases, including a discussion of sparse matrix inversion, useful for speeding up some computations. 5 Note that even in the case where we can only use approximation techniques, this algorithm is relatively efficient. By assumption, in this case, there is at least one deduction rule with two items in the current generation; thus, the number of deduction trees over which we are summing grows exponentially with the number of generations: a linear amount of computation yields the sum of the values of exponentially many trees. 591 Volume 25, Number 4 goal </context>
<context position="71638" citStr="Stolcke (1993)" startWordPosition="12663" endWordPosition="12664">here are in the NFA for a given string; and we can use the derivation forest semiring to get a compact representation of all state sequences in an NFA for an input string. A single item-based description can be used to find all of these values. 6.2 Prefix Values For language modeling, it may be useful to compute the prefix probability of a string. That is, given a string wn, we may wish to know the total probability of all sentences beginning with that string, EP(S w1. • • wnvi • • • 0) k&gt;o,vi,.••,vk where 01 vk represent words that could possibly follow w1 wn. Jelinek and Lafferty (1991) and Stolcke (1993) both give algorithms for computing these prefix probabilities. Elsewhere (Goodman 1998), we show how to produce an item-based description of a prefix parser. There are two main advantages to using an item-based description: ease of derivation, and reusability. First, the conventional derivations are somewhat complex, requiring a fair amount of inside-semiring-specific mathematics. In contrast, using item-based descriptions, we only need to derive a parser that has the property that there is one item derivation for each (complete) grammar derivation that would produce the prefix. The value of </context>
<context position="75845" citStr="Stolcke (1993)" startWordPosition="13308" endWordPosition="13309">arrison, and Ruzzo (1980) describe a parser similar to Earley&apos;s, but with several speedups that lead to significant improvements. Essentially, there are three improvements in the GHR parser. First, epsilon productions are precomputed; second, unary productions are precomputed; and, finally, completion is separated into two steps, allowing better dynamic programming. Goodman (1998) gives a full item-based description of a GHR parser. The forward values of many of the items in our parser related to unary and epsilon productions can be computed off-line, once per grammar, which is an idea due to Stolcke (1993). Since reverse values require entire strings, the reverse values of these items cannot be computed until the input string is known. Because we use a single item-based description for precomputed items and nonprecomputed items, and for forward and reverse values, this combination of off-line and on-line computation is easily and compactly specified. 6.6 Grammar Transformations We can apply the same techniques to grammar transformations that we have so far applied to parsing. Consider a grammar transformation, such as the Chomsky normal form (CNF) grammar transformation, which takes a grammar w</context>
<context position="79970" citStr="Stolcke (1993)" startWordPosition="13967" endWordPosition="13968">computing in the inside semiring. Viterbi (1967) developed corresponding algorithms for computing in the Viterbi semiring. Baker (1979) extended the work of Baum and his colleagues to PCFGs, including to computation of the outside values (or reverse inside values in our terminology). Baker&apos;s work is described by Lan i and Young (1990). Baker&apos;s work was only for PCFGs in CNF, avoiding the need to compute infinite summations. Jelinek and Lafferty (1991) showed how to compute some of the infinite summations in the inside semiring, those needed to compute the prefix probabilities of PCFGs in CNF. Stolcke (1993) showed how to use the same techniques to compute inside probabilities for Earley parsing, dealing with the difficult problems of unary transitions, and the more difficult problems of epsilon transitions. He thus solved all of the important problems encountered in using an item-based parser to compute the inside and outside values (forward and reverse inside values); he also showed how to compute the forward Viterbi values. 601 Computational Linguistics Volume 25, Number 4 The final area of work is in formal language theory and algebra. Although it is not widely known, there has been quite a b</context>
</contexts>
<marker>Stolcke, 1993</marker>
<rawString>Stolcke, Andreas. 1993. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Technical Report TR-93-065, International Computer Science Institute, Berkeley, CA. Available as cmp-lg/9411029.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Teitelbaum</author>
</authors>
<title>Context-free error analysis by evaluation of algebraic power series.</title>
<date>1973</date>
<booktitle>In Proceedings of the Fifth Annual ACM Symposium on Theory of Computing,</booktitle>
<pages>196--199</pages>
<location>Austin, TX.</location>
<contexts>
<context position="6685" citStr="Teitelbaum (1973)" startWordPosition="1149" endWordPosition="1151">ike the inside values, can be computed using multiplication and addition; unlike for the inside values, the probabilities of the rules are not multiplied into the scores. There is one last commonly computed quantity, the outside probabilities, which we will describe later, in Section 4. One of the key points of this paper is that all five of these commonly computed quantities can be described as elements of complete semirings (Kuich 1997). The relationship between grammars and semirings was discovered by Chomsky and Schtitzenberger (1963), and for parsing with the CKY algorithm, dates back to Teitelbaum (1973). A complete semiring is a set of values over which a multiplicative operator and a commutative additive operator have been defined, and for which infinite summations are defined. For parsing algorithms satisfying certain conditions, the multiplicative and additive operations of any complete semiring can be used in place of A and V. and correct values will be returned. We will give a simple normal form for describing parsers, then precisely define complete semirings, and the conditions for correctness. We now describe our normal form for parsers, which is very similar to that used by Shieber, </context>
<context position="77335" citStr="Teitelbaum 1973" startWordPosition="13542" endWordPosition="13543">ce can be generated by the grammar, FALSE otherwise) is the same as its value under a transformed grammar. Therefore, we say that this grammar transformation is value preserving under the Boolean semiring. We can generalize this concept of value preserving to other semirings. Elsewhere (Goodman 1998), we show that using essentially the same item-based descriptions we have used for parsing, we can specify grammar transformations. The concept of value preserving grammar transformation is already known in the intersection of formal language theory and algebra (Kuich 1997; Kuich and Salomaa 1986; Teitelbaum 1973). Our contribution is to show that these value preserving transformations can be written as simple item-based descriptions, allowing the same computational machinery to be used for grammar transformations as is used for parsing, and to some extent showing the relationship between certain grammar transformations and certain parsers, such as that of Graham, Harrison, and Ruzzo (1980). This uniform method of specifying grammar transformations is similar to, but clearer than, similar techniques used with covering grammars (Nijholt 1980; Leermakers 1989). 7. Previous Work 7.1 Historical Work The pr</context>
<context position="81101" citStr="Teitelbaum (1973)" startWordPosition="14151" endWordPosition="14152">language theory and algebra. Although it is not widely known, there has been quite a bit of work showing how to use formal power series to elegantly derive results in formal language theory, dating back to Chomsky and Schiitzenberger (1963). The major classic results can be derived in this framework, but with the added benefit that they apply to all commutative w-continuous semirings. The most accessible introduction to this literature we have found is by Kuich (1997). There are also books by Salomaa and Soittola (1978) and Kuich and Salomaa (1986). One piece of work deserves special mention. Teitelbaum (1973) showed that any semiring could be used in the CKY algorithm, laying the foundation for much of the work that followed. In summary, this paper synthesizes work from several different related fields, including deductive parsing, statistical parsing, and formal language theory; we emulate and expand on the earlier synthesis of Teitelbaum. The synthesis here is powerful: by generalizing and integrating many results, we make the computation of a much wider variety of values possible. 7.2 Recent Similar Work There has also been recent similar work by Tendeau (1997b, 1997a). Tendeau (1997b) gives an</context>
</contexts>
<marker>Teitelbaum, 1973</marker>
<rawString>Teitelbaum, Ray. 1973. Context-free error analysis by evaluation of algebraic power series. In Proceedings of the Fifth Annual ACM Symposium on Theory of Computing, pages 196-199, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Tendeau</author>
</authors>
<title>Computing abstract decorations of parse forests using dynamic programming and algebraic power series. Theoretical Computer Science.</title>
<date>1997</date>
<note>To appear.</note>
<contexts>
<context position="81666" citStr="Tendeau (1997" startWordPosition="14240" endWordPosition="14241">work deserves special mention. Teitelbaum (1973) showed that any semiring could be used in the CKY algorithm, laying the foundation for much of the work that followed. In summary, this paper synthesizes work from several different related fields, including deductive parsing, statistical parsing, and formal language theory; we emulate and expand on the earlier synthesis of Teitelbaum. The synthesis here is powerful: by generalizing and integrating many results, we make the computation of a much wider variety of values possible. 7.2 Recent Similar Work There has also been recent similar work by Tendeau (1997b, 1997a). Tendeau (1997b) gives an Earley-like algorithm that can be adapted to work with complete semirings satisfying certain conditions. Unlike our version of Earley&apos;s algorithm, Tendeau&apos;s version requires time O(n1) where L is the length of the longest right-hand side, as opposed to 0(n3) for the classic version, and for our description. While one could split right-hand sides of rules to make them binary branching, speeding Tendeau&apos;s version up, this would then change values in the derivation semirings. Tendeau (1997b, 1997a) introduces a parse forest semiring, similar to our derivation f</context>
</contexts>
<marker>Tendeau, 1997</marker>
<rawString>Tendeau, Frederic. 1997a. Computing abstract decorations of parse forests using dynamic programming and algebraic power series. Theoretical Computer Science. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Tendeau</author>
</authors>
<title>An Earley algorithm for generic attribute augmented grammars and applications.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies</booktitle>
<pages>199--209</pages>
<contexts>
<context position="81666" citStr="Tendeau (1997" startWordPosition="14240" endWordPosition="14241">work deserves special mention. Teitelbaum (1973) showed that any semiring could be used in the CKY algorithm, laying the foundation for much of the work that followed. In summary, this paper synthesizes work from several different related fields, including deductive parsing, statistical parsing, and formal language theory; we emulate and expand on the earlier synthesis of Teitelbaum. The synthesis here is powerful: by generalizing and integrating many results, we make the computation of a much wider variety of values possible. 7.2 Recent Similar Work There has also been recent similar work by Tendeau (1997b, 1997a). Tendeau (1997b) gives an Earley-like algorithm that can be adapted to work with complete semirings satisfying certain conditions. Unlike our version of Earley&apos;s algorithm, Tendeau&apos;s version requires time O(n1) where L is the length of the longest right-hand side, as opposed to 0(n3) for the classic version, and for our description. While one could split right-hand sides of rules to make them binary branching, speeding Tendeau&apos;s version up, this would then change values in the derivation semirings. Tendeau (1997b, 1997a) introduces a parse forest semiring, similar to our derivation f</context>
</contexts>
<marker>Tendeau, 1997</marker>
<rawString>Tendeau, Frederic. 1997b. An Earley algorithm for generic attribute augmented grammars and applications. In Proceedings of the International Workshop on Parsing Technologies 1997, pages 199-209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>13--260</pages>
<contexts>
<context position="79404" citStr="Viterbi (1967)" startWordPosition="13877" endWordPosition="13878">kel (1993) have shown how to specify parsers in a simple, interpretable, item-based format. This format is roughly the format we have used here, although there are differences due to the fact that their work was strictly in the Boolean semiring. Work in statistical parsing has also greatly influenced this work. We can trace this work back to research in HMMs by Baum and his colleagues (Baum and Eagon 1967; Baum 1972). In particular, the work of Baum developed the concept of backward probabilities (in the inside semiring), as well as many of the techniques for computing in the inside semiring. Viterbi (1967) developed corresponding algorithms for computing in the Viterbi semiring. Baker (1979) extended the work of Baum and his colleagues to PCFGs, including to computation of the outside values (or reverse inside values in our terminology). Baker&apos;s work is described by Lan i and Young (1990). Baker&apos;s work was only for PCFGs in CNF, avoiding the need to compute infinite summations. Jelinek and Lafferty (1991) showed how to compute some of the infinite summations in the inside semiring, those needed to compute the prefix probabilities of PCFGs in CNF. Stolcke (1993) showed how to use the same techni</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Viterbi, Andrew J. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory, IT-13:260-267.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>