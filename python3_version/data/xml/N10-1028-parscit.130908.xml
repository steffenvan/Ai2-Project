<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030985">
<title confidence="0.995153">
Inducing Synchronous Grammars with Slice Sampling
</title>
<author confidence="0.96213">
Phil Blunsom
</author>
<affiliation confidence="0.9615165">
Computing Laboratory
Oxford University
</affiliation>
<email confidence="0.962384">
Phil.Blunsom@comlab.ox.ac.uk
</email>
<sectionHeader confidence="0.993177" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99984825">
This paper describes an efficient sampler for
synchronous grammar induction under a non-
parametric Bayesian prior. Inspired by ideas
from slice sampling, our sampler is able to
draw samples from the posterior distributions
of models for which the standard dynamic pro-
graming based sampler proves intractable on
non-trivial corpora. We compare our sampler
to a previously proposed Gibbs sampler and
demonstrate strong improvements in terms of
both training log-likelihood and performance
on an end-to-end translation evaluation.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960703703704">
Intractable optimisation algorithms abound in much
of the recent work in Natural Language Process-
ing. In fact, there is an increasing acceptance that
solutions to many of the great challenges of NLP
(e.g. machine translation, summarisation, question
answering) will rest on the quality of approximate
inference. In this work we tackle this problem in
the context of inducing synchronous grammars for
a machine translation system. We concern ourselves
with the lack of a principled, and scalable, algo-
rithm for learning a synchronous context free gram-
mar (SCFG) from sentence-aligned parallel corpora.
The predominant approach for learning phrase-
based translation models (both finite state or syn-
chronous grammar based) uses a cascade of heuris-
tics beginning with predicted word alignments
and producing a weighted set of translation rules
(Koehn et al., 2003). Alternative approaches avoid
such heuristics, instead learning structured align-
ment models directly from sentence aligned data
(e.g., (Marcu and Wong, 2002; Cherry and Lin,
2007; DeNero et al., 2008; Blunsom et al., 2009)).
Although these models are theoretically attractive,
inference is intractable (at least O(|f|3|e|3)). The
efficacy of direct estimation of structured alignment
models therefore rests on the approximations used
to make inference practicable – typically heuristic
</bodyText>
<page confidence="0.980815">
238
</page>
<author confidence="0.975104">
Trevor Cohn
</author>
<affiliation confidence="0.9984885">
Department of Computer Science
University of Sheffield
</affiliation>
<email confidence="0.832791">
T.Cohn@dcs.shef.ac.uk
</email>
<bodyText confidence="0.999679609756097">
constraints or Gibbs sampling. In this work we show
that naive Gibbs sampling (specifically, Blunsom et
al. (2009)) is ineffectual for inference and reliant on
a high quality initialisation, mixing very slowly and
being easily caught in modes. Instead, blocked sam-
pling over sentence pairs allows much faster mixing,
but done in the obvious way (following Johnson et al.
(2007)) would incur a O(|f|3|e|3) time complexity.
Here we draw inspiration from the work of
Van Gael et al. (2008) on inference in infinite hid-
den Markov models to develop a novel algorithm
for efficient sampling from a SCFG. We develop an
auxiliary variable ‘slice’ sampler which can dramati-
cally reduce inference complexity, and thereby make
blocked sampling practicable on real translation cor-
pora. Our evaluation demonstrates that our algorithm
mixes more quickly than the local Gibbs sampler, and
produces translation models which achieve state-of-
the-art BLEU scores without using GIZA++ or sym-
metrisation heuristics for initialisation.
We adopt the generative model of Blunsom et
al. (2009) which creates a parallel sentence pair
by a sequence (derivation) of SCFG productions
d = (r1, r2, ..., rn). The tokens in each language can
be read off the leaves of the derivation tree while
their order is defined hierarchically by the produc-
tions in use. The probability of a derivation is defined
as p(d|0) = H,Cd 01 where 0 are the model param-
eters which are drawn from a Bayesian prior. We
deviate from that models definition of the prior over
phrasal translations, instead adopting the hierarchical
Dirichlet process prior from DeNero et al. (2008),
which incorporates IBM Model 1. Blunsom et al.
(2009) describe a blocked sampler following John-
son et al. (2007) which uses the Metropolis-Hastings
algorithm to correct proposal samples drawn from
an approximating SCFG, however this is discounted
as impractical due to the O(|f|3|e|3) complexity.
Instead a Gibbs sampler is used which samples local
updates to the derivation structure of each training
instance. This avoids the dynamic program of the
</bodyText>
<subsubsectionHeader confidence="0.742347">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 238–241,
</subsubsectionHeader>
<subsectionHeader confidence="0.302917">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999166111111111">
blocked sampler but at the expense of considerably
slower mixing.
Recently Bouchard-Cˆot´e et al. (2009) proposed
an auxialliary variable sampler, possibly comple-
mentary to ours, which was also evaluated on syn-
chronous parsing. Rather than slice sampling deriva-
tions in a collapsed Bayesian model, this model
employed a secondary proposal model (IBM Mod-
els) and sampled expectations over rule parameters.
</bodyText>
<sectionHeader confidence="0.531395" genericHeader="method">
2 Slice Sampling a SCFG
</sectionHeader>
<bodyText confidence="0.999893461538461">
It would be advantageous to explore a middle ground
where the scope of the dynamic program is limited to
high probability regions, reducing the running time
to an acceptable level. By employing the technique
of slice sampling (Neal, 2003) we describe an algo-
rithm which stochastically samples from a reduced
space of possible derivations, while ensuring that
these samples are drawn from the correct distribu-
tion. We apply the slice sampler to the approximating
SCFG parameterised by O, which requires samples
from an inside chart p(d|O) (for brevity, we omit the
dependency on O in the following).
Slice sampling is an example of auxiliary variable
sampling in which we make use of the fact that if
we can draw samples from a joint distribution, then
we can trivially obtain samples from the marginal
distributions: p(d) = Pu p(d, u), where d is the
variable of interest and u is an auxiliary variable.
Using a Gibbs sampler we can draw samples from
this joint distribution by alternately sampling from
p(d|u) and p(u|d). The trick is to ensure that u is
defined such that drawing samples from p(d|u) is
more efficient than from p(d).
We define the variable u to contain a slice variable
us for every cell of a synchronous parse chart for
every training instance:1
</bodyText>
<equation confidence="0.808478">
S = {(i,j,x,y)  |0 &lt; i &lt; j &lt; |f|,0 &lt; x &lt; y &lt; |e|l
u = Jus E R  |0 &lt; us &lt; 1,s E S}
</equation>
<bodyText confidence="0.986965388888889">
These slice variables act as cutoffs on the probabili-
ties of the rules considered in each cell s: rule appli-
cations rs with Ors &lt; us will be pruned from the
dynamic program.2
1The dependence on training instances is omitted here and
subsequently for simplicity. Each instance is independent, and
therefore this formulation can be trivially applied to a set.
2Alternatively, we could naively sample from a pruned chart
using a fixed beam threshold. However, this would not produce
samples from p(d), but some other unknown distribution.
Sampling p(u|d) Unlike Van Gael et al. (2008),
there is not a one-to-one correspondence between the
spans of the rules in d and the set S, rather the deriva-
tion’s rule spans form a subset of S. This compli-
cates our definition of p(u|d); we must provide sepa-
rate accounts of how each us is generated depending
on whether there is a corresponding rule for s, i.e.,
rs E d. We define p(u|d) = Qs p(us|d), where:
</bodyText>
<equation confidence="0.92164">
( I(usθ&lt;θrs) , if rs E d
p(us|d) =Q(usrs; a, b) , else (1)
</equation>
<bodyText confidence="0.99989705">
which mixes a uniform distribution and a Beta dis-
tribution3 depending on the existence of a rule rs
in the derivation d.4 Eq. 1 is constructed such that
only rules with probability greater than the rele-
vant threshold, Jrs  |Ors &gt; usJ, could have feasibly
been part of a derivation resulting in auxiliary vari-
able u. This is critical in reasoning over the reverse
conditional p(d|u) which only has to consider the
reduced space of rules (formulation below in (4)).
Trivially, the conditioning derivation is recoverable,
Vrs E d, Ors &gt; us. We parameterise the Q distribu-
tion in (1) with a heavy skew towards zero in order
to limit the amount of pruning and thereby include
many competing derivations.5
Sampling p(d|u) Recall the probability of a
derivation, p(d) = QrsEd Ors. We draw samples
from the joint distribution, p(d, u), holding u fixed:
In step (2) we cancel the Ors terms while in step (3)
we introduce Q(us; a, b) terms to the numerator and
denominator for us : rs E d to simplify the range
</bodyText>
<footnote confidence="0.9903086">
3Any distribution defined over {x E R 10 &lt; x &lt; 11 may be
used in place of Q, however this may affect the efficiency of the
sampler.
4H(·) returns 1 if the condition is true and 0 otherwise.
5We experiment with a range of a &lt; 1 while fixing b = 1.
</footnote>
<equation confidence="0.962714631578947">
p(d|u) a p(d, u) = p(d) x p(u|d)
⎛ ⎞ Q I(us&lt;θrs)
⎝ Y
=
rsE
Y=
us:rsEd
Y
ff (us &lt; Ors)
us:rsod
Q(us; a, b) (2)
ff (us &lt; Ors) YQ(us; a, b) (3)
Q(us; a, b) us
ff (us &lt; Ors) (4)
Q(us; a, b)
Y=
us:rsEd
Ya
us:rsEd
</equation>
<page confidence="0.984428">
239
</page>
<table confidence="0.999686916666667">
System BLEU time(s) LLH
Moses (default settings) 47.3 – –
LB init. 36.5 – -257.1
M1 init. 48.8 – -153.4
M4 init. 49.1 – -151.4
Gibbs LB init. 45.3 44 -135.4
Gibbs M1 init. 48.2 40 -120.5
Gibbs M4 init. (Blunsom et al., 2009) 49.6 44 -110.3
Slice (a=0.15, b=1) LB init. 47.3 180 -98.9
Slice (a=0.10, b=1) M1 init. 50.4 908 -89.4
Slice (a=0.15, b=1) M1 init. 49.9 144 -90.2
Slice (a=0.25, b=1) M1 init. 49.2 80 -95.6
</table>
<tableCaption confidence="0.999849">
Table 1: IWSLT Chinese to English translation.
</tableCaption>
<bodyText confidence="0.9999361">
of the second product. The last step (4) discards the
term H.3 Q(us; a, b) which is constant wrt d. The
net result is a formulation which factors with the
derivation structure, thereby eliminating the need to
consider all O( e 2 f 2) spans in S. Critically p(d u)
is zero for all spans failing the ff (us &lt; 0,3) condition.
To exploit the decomposition of Equation 4 we
require a parsing algorithm that only explores chart
cells whose child cells have not already been pruned
by the slice variables. The standard approach of using
synchronous CYK (Wu, 1997) doesn’t posses this
property: all chart cells would be visited even if they
are to be pruned. Instead we use an agenda based
parsing algorithm, in particular we extend the algo-
rithm of Klein and Manning (2004) to synchronous
parsing.6 Finally, we need a Metropolis-Hastings
acceptance step to account for intra-instance depen-
dencies (the ‘rich-get-richer’ effect). We omit the
details, save to state that the calculation cancels to
the same test as presented in Johnson et al. (2007).7
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999905333333333">
In the following experiments we compare the slice
sampler and the Gibbs sampler (Blunsom et al.,
2009), in terms of mixing and translation quality. We
measure mixing in terms of training log-likelihood
(LLH) after a fixed number of sampling iterations.
Translations are produced using Moses (Koehn et al.,
2007), initialised with the word alignments from the
final sample, and are evaluated using BLEU(Papineni
et al., 2001). The slice sampled models are restricted
to learning binary branching one-to-one (or null)
alignments,8 while no restriction is placed on the
Gibbs sampler (both use the same model, so have
</bodyText>
<footnote confidence="0.9223464">
6Moreover, we only sample values for u3 as they are visited
by the parser, thus avoiding the quartic complexity.
7Acceptance rates averaged above 99%.
8This restriction is not strictly necessary, however it greatly
simplifies the implementation and increases efficiency.
</footnote>
<bodyText confidence="0.998922882352941">
comparable LLH). Of particular interest is how the
different samplers perform given initialisations of
varying quality. We evaluate three initialisers: M4:
the symmetrised output of GIZA++ factorised into
ITG form (as used in Blunsom et al. (2009)); M1:
the output of a heavily pruned ITG parser using the
IBM Model 1 prior for the rule probabilities;9 and
LB: left-branching monotone derivations.10
We experiment with the Chinese—*English trans-
lation task from IWSLT, as used in Blunsom et al.
(2009).11 Figure 1 shows LLH curves for the sam-
plers initialised with the M1 and LB derivations, plus
the curve for Gibbs sampler with the M4 initialiser.12
Table 1 gives BLEU scores on Test-05 for phrase-
based translation models built from the 1500th sam-
ple for the various models along with the average
time per sample and their final log-likelihood.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999938714285715">
The results are particularly encouraging. The slice
sampler uniformly finds much better solutions than
the Gibbs sampler regardless of initialisation. In
particular, the slice sampled model initialised with
the naive LB structure achieves a higher likelihood
than the M4 initialised model, although this is not
reflected in their relative BLEU scores. In contrast the
Gibbs sampler is more significantly affected by its
initialisation, only deviating slightly before becom-
ing trapped in a mode, as seen in Fig. 1. With suf-
ficient (infinite) time both sampling strategies will
converge on the true posterior regardless of initiali-
sation, however the slice sampler appears to be con-
verging much faster than the Gibbs sampler.
Interestingly, the initialisation heuristics (M1 and
M4) outperform the default heuristics (Koehn et al.,
2007) by a considerable margin. This is most likely
because the initialisation heuristics force the align-
ments to factorise with an ITG, resulting in more
aggressive pruning of spurious alignments which in
turn allows for more and larger phrase pairs.
</bodyText>
<footnote confidence="0.9912634">
9The following beam heuristics are employed: alignments to
null are only permitted on the longer sentence side; words are
only allowed to align to those whose relative sentence position
is within f3 words.
10Words of the longer sentence are randomly assigned to null.
11We limit the maximum training sentence length to 40, result-
ing in — 40k training sentences.
12The GIZA++ M4 alignments don’t readily factorise to
word-based ITG derivations, as such we haven’t produced results
for this initialiser using the slice sampler.
</footnote>
<page confidence="0.990429">
240
</page>
<figure confidence="0.998050947368421">
Samples
Samples
Slice (a=0.10 b=1) M1
Slice (a=0.15 b=1) M1
Slice (a=0.20 b=1) M1
Slice (a=0.25 b=1) M1
Gibbs M1
Gibbs M4
0 50 100 150 200 250
−140 −130 −120 −110 −100 −90
Log−likelihood
0 200 400 600 800 1000
Slice (a=0.15 b=1) M1
Slice (a=0.15 b=1) LB
Gibbs M1
Gibbs LB
Gibbs M4
−200 −180 −160 −140 −120 −100
Log−likelihood
</figure>
<figureCaption confidence="0.999979">
Figure 1: Training log-likelihood as a function of sampling iteration for Gibbs and slice sampling.
</figureCaption>
<bodyText confidence="0.999985416666667">
While the LLHs for the slice sampled models and
their BLEU scores appear correlated, this doesn’t
extend to comparisons with the Gibbs sampled mod-
els. We believe that this is because the GIZA++
initialisation alignments also explain the data well,
while not necessarily obtaining a high LLH under
the ITG model. Solutions which score highly in one
model score poorly in the other, despite both produc-
ing good translations.
The slice sampler is slower than the local Gibbs
sampler, its speed depending on the parameterisation
of the Beta distribution (affecting the width of the
beam). In the extreme, exhaustive search using the
full dynamic program is intractable on current hard-
ware,13 and therefore we have achieved our aim of
mediating between local and blocked inference.
This investigation has established the promise
of the SCFG slice sampling technique to provide
a scalable inference algorithm for non-parametric
Bayesian models. With further development, this
work could provide the basis for a family of prin-
cipled inference algorithms for parsing models, both
monolingual and synchronous, and other models that
prove intractable for exact dynamic programming.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998956195652174">
P. Blunsom, T. Cohn, C. Dyer, M. Osborne. 2009.
A Gibbs sampler for phrasal synchronous grammar
induction. In Proc. ACL/IJCNLP, 782–790, Suntec,
Singapore. Association for Computational Linguistics.
A. Bouchard-Cˆot´e, S. Petrov, D. Klein. 2009. Ran-
domized pruning: Efficiently calculating expectations
13Our implementation had not completed a single sample after
a week.
in large dynamic programs. In Advances in Neural
Information Processing Systems 22, 144–152.
C. Cherry, D. Lin. 2007. Inversion transduction grammar
for joint phrasal translation modeling. In Proc. SSST,
Rochester, USA.
J. DeNero, A. Bouchard-Cˆot´e, D. Klein. 2008. Sam-
pling alignment structure under a Bayesian translation
model. In Proc. EMNLP, 314–323, Honolulu, Hawaii.
M. Johnson, T. Griffiths, S. Goldwater. 2007. Bayesian
inference for PCFGs via Markov chain Monte Carlo.
In Proc. HLT-NAACL, 139–146, Rochester, New York.
D. Klein, C. D. Manning, 2004. Parsing and hypergraphs,
351–372. Kluwer Academic Publishers, Norwell, MA,
USA, 2004.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL, 81–88,
Edmonton, Canada.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, E. Herbst.
2007. Moses: Open source toolkit for statistical
machine translation. In Proc. ACL, Prague.
D. Marcu, W. Wong. 2002. A phrase-based, joint proba-
bility model for statistical machine translation. In Proc.
EMNLP, 133–139, Philadelphia.
R. Neal. 2003. Slice sampling. Annals of Statistics,
31:705–767.
K. Papineni, S. Roukos, T. Ward, W. Zhu. 2001. Bleu:
a method for automatic evaluation of machine trans-
lation. Technical Report RC22176 (W0109-022), IBM
Research Division, Thomas J. Watson Research Center,
2001.
J. Van Gael, Y. Saatci, Y. W. Teh, Z. Ghahramani. 2008.
Beam sampling for the infinite hidden markov model.
In ICML, 1088–1095, New York, NY, USA.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377–403.
</reference>
<page confidence="0.998106">
241
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.680786">
<title confidence="0.999814">Inducing Synchronous Grammars with Slice Sampling</title>
<author confidence="0.937154">Phil</author>
<affiliation confidence="0.842593">Computing Oxford</affiliation>
<email confidence="0.966135">Phil.Blunsom@comlab.ox.ac.uk</email>
<abstract confidence="0.999845692307692">This paper describes an efficient sampler for synchronous grammar induction under a nonparametric Bayesian prior. Inspired by ideas our sampler is able to draw samples from the posterior distributions of models for which the standard dynamic programing based sampler proves intractable on non-trivial corpora. We compare our sampler to a previously proposed Gibbs sampler and demonstrate strong improvements in terms of both training log-likelihood and performance on an end-to-end translation evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
<author>C Dyer</author>
<author>M Osborne</author>
</authors>
<title>A Gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. ACL/IJCNLP, 782–790, Suntec, Singapore. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1764" citStr="Blunsom et al., 2009" startWordPosition="252" endWordPosition="255">ack of a principled, and scalable, algorithm for learning a synchronous context free grammar (SCFG) from sentence-aligned parallel corpora. The predominant approach for learning phrasebased translation models (both finite state or synchronous grammar based) uses a cascade of heuristics beginning with predicted word alignments and producing a weighted set of translation rules (Koehn et al., 2003). Alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data (e.g., (Marcu and Wong, 2002; Cherry and Lin, 2007; DeNero et al., 2008; Blunsom et al., 2009)). Although these models are theoretically attractive, inference is intractable (at least O(|f|3|e|3)). The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic 238 Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2009)) is ineffectual for inference and reliant on a high quality initialisation, mixing very slowly and being easily caught in modes.</context>
<context position="3189" citStr="Blunsom et al. (2009)" startWordPosition="463" endWordPosition="466">from the work of Van Gael et al. (2008) on inference in infinite hidden Markov models to develop a novel algorithm for efficient sampling from a SCFG. We develop an auxiliary variable ‘slice’ sampler which can dramatically reduce inference complexity, and thereby make blocked sampling practicable on real translation corpora. Our evaluation demonstrates that our algorithm mixes more quickly than the local Gibbs sampler, and produces translation models which achieve state-ofthe-art BLEU scores without using GIZA++ or symmetrisation heuristics for initialisation. We adopt the generative model of Blunsom et al. (2009) which creates a parallel sentence pair by a sequence (derivation) of SCFG productions d = (r1, r2, ..., rn). The tokens in each language can be read off the leaves of the derivation tree while their order is defined hierarchically by the productions in use. The probability of a derivation is defined as p(d|0) = H,Cd 01 where 0 are the model parameters which are drawn from a Bayesian prior. We deviate from that models definition of the prior over phrasal translations, instead adopting the hierarchical Dirichlet process prior from DeNero et al. (2008), which incorporates IBM Model 1. Blunsom et</context>
<context position="8858" citStr="Blunsom et al., 2009" startWordPosition="1468" endWordPosition="1471">in place of Q, however this may affect the efficiency of the sampler. 4H(·) returns 1 if the condition is true and 0 otherwise. 5We experiment with a range of a &lt; 1 while fixing b = 1. p(d|u) a p(d, u) = p(d) x p(u|d) ⎛ ⎞ Q I(us&lt;θrs) ⎝ Y = rsE Y= us:rsEd Y ff (us &lt; Ors) us:rsod Q(us; a, b) (2) ff (us &lt; Ors) YQ(us; a, b) (3) Q(us; a, b) us ff (us &lt; Ors) (4) Q(us; a, b) Y= us:rsEd Ya us:rsEd 239 System BLEU time(s) LLH Moses (default settings) 47.3 – – LB init. 36.5 – -257.1 M1 init. 48.8 – -153.4 M4 init. 49.1 – -151.4 Gibbs LB init. 45.3 44 -135.4 Gibbs M1 init. 48.2 40 -120.5 Gibbs M4 init. (Blunsom et al., 2009) 49.6 44 -110.3 Slice (a=0.15, b=1) LB init. 47.3 180 -98.9 Slice (a=0.10, b=1) M1 init. 50.4 908 -89.4 Slice (a=0.15, b=1) M1 init. 49.9 144 -90.2 Slice (a=0.25, b=1) M1 init. 49.2 80 -95.6 Table 1: IWSLT Chinese to English translation. of the second product. The last step (4) discards the term H.3 Q(us; a, b) which is constant wrt d. The net result is a formulation which factors with the derivation structure, thereby eliminating the need to consider all O( e 2 f 2) spans in S. Critically p(d u) is zero for all spans failing the ff (us &lt; 0,3) condition. To exploit the decomposition of Equatio</context>
<context position="10254" citStr="Blunsom et al., 2009" startWordPosition="1707" endWordPosition="1710">nous CYK (Wu, 1997) doesn’t posses this property: all chart cells would be visited even if they are to be pruned. Instead we use an agenda based parsing algorithm, in particular we extend the algorithm of Klein and Manning (2004) to synchronous parsing.6 Finally, we need a Metropolis-Hastings acceptance step to account for intra-instance dependencies (the ‘rich-get-richer’ effect). We omit the details, save to state that the calculation cancels to the same test as presented in Johnson et al. (2007).7 3 Evaluation In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al., 2009), in terms of mixing and translation quality. We measure mixing in terms of training log-likelihood (LLH) after a fixed number of sampling iterations. Translations are produced using Moses (Koehn et al., 2007), initialised with the word alignments from the final sample, and are evaluated using BLEU(Papineni et al., 2001). The slice sampled models are restricted to learning binary branching one-to-one (or null) alignments,8 while no restriction is placed on the Gibbs sampler (both use the same model, so have 6Moreover, we only sample values for u3 as they are visited by the parser, thus avoidin</context>
<context position="11539" citStr="Blunsom et al. (2009)" startWordPosition="1903" endWordPosition="1906">8This restriction is not strictly necessary, however it greatly simplifies the implementation and increases efficiency. comparable LLH). Of particular interest is how the different samplers perform given initialisations of varying quality. We evaluate three initialisers: M4: the symmetrised output of GIZA++ factorised into ITG form (as used in Blunsom et al. (2009)); M1: the output of a heavily pruned ITG parser using the IBM Model 1 prior for the rule probabilities;9 and LB: left-branching monotone derivations.10 We experiment with the Chinese—*English translation task from IWSLT, as used in Blunsom et al. (2009).11 Figure 1 shows LLH curves for the samplers initialised with the M1 and LB derivations, plus the curve for Gibbs sampler with the M4 initialiser.12 Table 1 gives BLEU scores on Test-05 for phrasebased translation models built from the 1500th sample for the various models along with the average time per sample and their final log-likelihood. 4 Discussion The results are particularly encouraging. The slice sampler uniformly finds much better solutions than the Gibbs sampler regardless of initialisation. In particular, the slice sampled model initialised with the naive LB structure achieves a </context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>P. Blunsom, T. Cohn, C. Dyer, M. Osborne. 2009. A Gibbs sampler for phrasal synchronous grammar induction. In Proc. ACL/IJCNLP, 782–790, Suntec, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bouchard-Cˆot´e</author>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Randomized pruning: Efficiently calculating expectations 13Our implementation had not completed a single sample after a week.</title>
<date>2009</date>
<marker>Bouchard-Cˆot´e, Petrov, Klein, 2009</marker>
<rawString>A. Bouchard-Cˆot´e, S. Petrov, D. Klein. 2009. Randomized pruning: Efficiently calculating expectations 13Our implementation had not completed a single sample after a week.</rawString>
</citation>
<citation valid="false">
<title>in large dynamic programs.</title>
<booktitle>In Advances in Neural Information Processing Systems 22,</booktitle>
<pages>144--152</pages>
<marker></marker>
<rawString>in large dynamic programs. In Advances in Neural Information Processing Systems 22, 144–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>D Lin</author>
</authors>
<title>Inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In Proc. SSST,</booktitle>
<location>Rochester, USA.</location>
<contexts>
<context position="1720" citStr="Cherry and Lin, 2007" startWordPosition="244" endWordPosition="247">ion system. We concern ourselves with the lack of a principled, and scalable, algorithm for learning a synchronous context free grammar (SCFG) from sentence-aligned parallel corpora. The predominant approach for learning phrasebased translation models (both finite state or synchronous grammar based) uses a cascade of heuristics beginning with predicted word alignments and producing a weighted set of translation rules (Koehn et al., 2003). Alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data (e.g., (Marcu and Wong, 2002; Cherry and Lin, 2007; DeNero et al., 2008; Blunsom et al., 2009)). Although these models are theoretically attractive, inference is intractable (at least O(|f|3|e|3)). The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic 238 Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2009)) is ineffectual for inference and reliant on a high quality initialisation, mixing v</context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>C. Cherry, D. Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In Proc. SSST, Rochester, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>A Bouchard-Cˆot´e</author>
<author>D Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP, 314–323,</booktitle>
<location>Honolulu, Hawaii.</location>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>J. DeNero, A. Bouchard-Cˆot´e, D. Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proc. EMNLP, 314–323, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>T Griffiths</author>
<author>S Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Proc. HLT-NAACL, 139–146,</booktitle>
<location>Rochester, New York.</location>
<contexts>
<context position="2498" citStr="Johnson et al. (2007)" startWordPosition="357" endWordPosition="360">acy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic 238 Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2009)) is ineffectual for inference and reliant on a high quality initialisation, mixing very slowly and being easily caught in modes. Instead, blocked sampling over sentence pairs allows much faster mixing, but done in the obvious way (following Johnson et al. (2007)) would incur a O(|f|3|e|3) time complexity. Here we draw inspiration from the work of Van Gael et al. (2008) on inference in infinite hidden Markov models to develop a novel algorithm for efficient sampling from a SCFG. We develop an auxiliary variable ‘slice’ sampler which can dramatically reduce inference complexity, and thereby make blocked sampling practicable on real translation corpora. Our evaluation demonstrates that our algorithm mixes more quickly than the local Gibbs sampler, and produces translation models which achieve state-ofthe-art BLEU scores without using GIZA++ or symmetris</context>
<context position="3859" citStr="Johnson et al. (2007)" startWordPosition="577" endWordPosition="581">ence (derivation) of SCFG productions d = (r1, r2, ..., rn). The tokens in each language can be read off the leaves of the derivation tree while their order is defined hierarchically by the productions in use. The probability of a derivation is defined as p(d|0) = H,Cd 01 where 0 are the model parameters which are drawn from a Bayesian prior. We deviate from that models definition of the prior over phrasal translations, instead adopting the hierarchical Dirichlet process prior from DeNero et al. (2008), which incorporates IBM Model 1. Blunsom et al. (2009) describe a blocked sampler following Johnson et al. (2007) which uses the Metropolis-Hastings algorithm to correct proposal samples drawn from an approximating SCFG, however this is discounted as impractical due to the O(|f|3|e|3) complexity. Instead a Gibbs sampler is used which samples local updates to the derivation structure of each training instance. This avoids the dynamic program of the Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 238–241, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics blocked sampler but at the expense of considerably slower mixin</context>
<context position="10136" citStr="Johnson et al. (2007)" startWordPosition="1688" endWordPosition="1691">rt cells whose child cells have not already been pruned by the slice variables. The standard approach of using synchronous CYK (Wu, 1997) doesn’t posses this property: all chart cells would be visited even if they are to be pruned. Instead we use an agenda based parsing algorithm, in particular we extend the algorithm of Klein and Manning (2004) to synchronous parsing.6 Finally, we need a Metropolis-Hastings acceptance step to account for intra-instance dependencies (the ‘rich-get-richer’ effect). We omit the details, save to state that the calculation cancels to the same test as presented in Johnson et al. (2007).7 3 Evaluation In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al., 2009), in terms of mixing and translation quality. We measure mixing in terms of training log-likelihood (LLH) after a fixed number of sampling iterations. Translations are produced using Moses (Koehn et al., 2007), initialised with the word alignments from the final sample, and are evaluated using BLEU(Papineni et al., 2001). The slice sampled models are restricted to learning binary branching one-to-one (or null) alignments,8 while no restriction is placed on the Gibbs sampler (bo</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>M. Johnson, T. Griffiths, S. Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. HLT-NAACL, 139–146, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Parsing and hypergraphs,</title>
<date>2004</date>
<pages>351--372</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Norwell, MA, USA,</location>
<contexts>
<context position="9862" citStr="Klein and Manning (2004)" startWordPosition="1647" endWordPosition="1650">ors with the derivation structure, thereby eliminating the need to consider all O( e 2 f 2) spans in S. Critically p(d u) is zero for all spans failing the ff (us &lt; 0,3) condition. To exploit the decomposition of Equation 4 we require a parsing algorithm that only explores chart cells whose child cells have not already been pruned by the slice variables. The standard approach of using synchronous CYK (Wu, 1997) doesn’t posses this property: all chart cells would be visited even if they are to be pruned. Instead we use an agenda based parsing algorithm, in particular we extend the algorithm of Klein and Manning (2004) to synchronous parsing.6 Finally, we need a Metropolis-Hastings acceptance step to account for intra-instance dependencies (the ‘rich-get-richer’ effect). We omit the details, save to state that the calculation cancels to the same test as presented in Johnson et al. (2007).7 3 Evaluation In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al., 2009), in terms of mixing and translation quality. We measure mixing in terms of training log-likelihood (LLH) after a fixed number of sampling iterations. Translations are produced using Moses (Koehn et al., 2007</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein, C. D. Manning, 2004. Parsing and hypergraphs, 351–372. Kluwer Academic Publishers, Norwell, MA, USA, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<location>81–88, Edmonton, Canada.</location>
<contexts>
<context position="1541" citStr="Koehn et al., 2003" startWordPosition="219" endWordPosition="222"> question answering) will rest on the quality of approximate inference. In this work we tackle this problem in the context of inducing synchronous grammars for a machine translation system. We concern ourselves with the lack of a principled, and scalable, algorithm for learning a synchronous context free grammar (SCFG) from sentence-aligned parallel corpora. The predominant approach for learning phrasebased translation models (both finite state or synchronous grammar based) uses a cascade of heuristics beginning with predicted word alignments and producing a weighted set of translation rules (Koehn et al., 2003). Alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data (e.g., (Marcu and Wong, 2002; Cherry and Lin, 2007; DeNero et al., 2008; Blunsom et al., 2009)). Although these models are theoretically attractive, inference is intractable (at least O(|f|3|e|3)). The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic 238 Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk constraints or Gibbs</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrase-based translation. In Proc. HLT-NAACL, 81–88, Edmonton, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, </marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, E. Herbst.</rawString>
</citation>
<citation valid="true">
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<location>Prague.</location>
<contexts>
<context position="2498" citStr="(2007)" startWordPosition="360" endWordPosition="360">stimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic 238 Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2009)) is ineffectual for inference and reliant on a high quality initialisation, mixing very slowly and being easily caught in modes. Instead, blocked sampling over sentence pairs allows much faster mixing, but done in the obvious way (following Johnson et al. (2007)) would incur a O(|f|3|e|3) time complexity. Here we draw inspiration from the work of Van Gael et al. (2008) on inference in infinite hidden Markov models to develop a novel algorithm for efficient sampling from a SCFG. We develop an auxiliary variable ‘slice’ sampler which can dramatically reduce inference complexity, and thereby make blocked sampling practicable on real translation corpora. Our evaluation demonstrates that our algorithm mixes more quickly than the local Gibbs sampler, and produces translation models which achieve state-ofthe-art BLEU scores without using GIZA++ or symmetris</context>
<context position="3859" citStr="(2007)" startWordPosition="581" endWordPosition="581">n) of SCFG productions d = (r1, r2, ..., rn). The tokens in each language can be read off the leaves of the derivation tree while their order is defined hierarchically by the productions in use. The probability of a derivation is defined as p(d|0) = H,Cd 01 where 0 are the model parameters which are drawn from a Bayesian prior. We deviate from that models definition of the prior over phrasal translations, instead adopting the hierarchical Dirichlet process prior from DeNero et al. (2008), which incorporates IBM Model 1. Blunsom et al. (2009) describe a blocked sampler following Johnson et al. (2007) which uses the Metropolis-Hastings algorithm to correct proposal samples drawn from an approximating SCFG, however this is discounted as impractical due to the O(|f|3|e|3) complexity. Instead a Gibbs sampler is used which samples local updates to the derivation structure of each training instance. This avoids the dynamic program of the Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 238–241, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics blocked sampler but at the expense of considerably slower mixin</context>
<context position="10136" citStr="(2007)" startWordPosition="1691" endWordPosition="1691">child cells have not already been pruned by the slice variables. The standard approach of using synchronous CYK (Wu, 1997) doesn’t posses this property: all chart cells would be visited even if they are to be pruned. Instead we use an agenda based parsing algorithm, in particular we extend the algorithm of Klein and Manning (2004) to synchronous parsing.6 Finally, we need a Metropolis-Hastings acceptance step to account for intra-instance dependencies (the ‘rich-get-richer’ effect). We omit the details, save to state that the calculation cancels to the same test as presented in Johnson et al. (2007).7 3 Evaluation In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al., 2009), in terms of mixing and translation quality. We measure mixing in terms of training log-likelihood (LLH) after a fixed number of sampling iterations. Translations are produced using Moses (Koehn et al., 2007), initialised with the word alignments from the final sample, and are evaluated using BLEU(Papineni et al., 2001). The slice sampled models are restricted to learning binary branching one-to-one (or null) alignments,8 while no restriction is placed on the Gibbs sampler (bo</context>
</contexts>
<marker>2007</marker>
<rawString>2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP, 133–139,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="1698" citStr="Marcu and Wong, 2002" startWordPosition="240" endWordPosition="243">for a machine translation system. We concern ourselves with the lack of a principled, and scalable, algorithm for learning a synchronous context free grammar (SCFG) from sentence-aligned parallel corpora. The predominant approach for learning phrasebased translation models (both finite state or synchronous grammar based) uses a cascade of heuristics beginning with predicted word alignments and producing a weighted set of translation rules (Koehn et al., 2003). Alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data (e.g., (Marcu and Wong, 2002; Cherry and Lin, 2007; DeNero et al., 2008; Blunsom et al., 2009)). Although these models are theoretically attractive, inference is intractable (at least O(|f|3|e|3)). The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic 238 Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2009)) is ineffectual for inference and reliant on a high quality in</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>D. Marcu, W. Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proc. EMNLP, 133–139, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2003</date>
<journal>Annals of Statistics,</journal>
<pages>31--705</pages>
<contexts>
<context position="5063" citStr="Neal, 2003" startWordPosition="761" endWordPosition="762">r mixing. Recently Bouchard-Cˆot´e et al. (2009) proposed an auxialliary variable sampler, possibly complementary to ours, which was also evaluated on synchronous parsing. Rather than slice sampling derivations in a collapsed Bayesian model, this model employed a secondary proposal model (IBM Models) and sampled expectations over rule parameters. 2 Slice Sampling a SCFG It would be advantageous to explore a middle ground where the scope of the dynamic program is limited to high probability regions, reducing the running time to an acceptable level. By employing the technique of slice sampling (Neal, 2003) we describe an algorithm which stochastically samples from a reduced space of possible derivations, while ensuring that these samples are drawn from the correct distribution. We apply the slice sampler to the approximating SCFG parameterised by O, which requires samples from an inside chart p(d|O) (for brevity, we omit the dependency on O in the following). Slice sampling is an example of auxiliary variable sampling in which we make use of the fact that if we can draw samples from a joint distribution, then we can trivially obtain samples from the marginal distributions: p(d) = Pu p(d, u), wh</context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>R. Neal. 2003. Slice sampling. Annals of Statistics, 31:705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research Division, Thomas J. Watson Research Center,</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<contexts>
<context position="10576" citStr="Papineni et al., 2001" startWordPosition="1756" endWordPosition="1759">for intra-instance dependencies (the ‘rich-get-richer’ effect). We omit the details, save to state that the calculation cancels to the same test as presented in Johnson et al. (2007).7 3 Evaluation In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al., 2009), in terms of mixing and translation quality. We measure mixing in terms of training log-likelihood (LLH) after a fixed number of sampling iterations. Translations are produced using Moses (Koehn et al., 2007), initialised with the word alignments from the final sample, and are evaluated using BLEU(Papineni et al., 2001). The slice sampled models are restricted to learning binary branching one-to-one (or null) alignments,8 while no restriction is placed on the Gibbs sampler (both use the same model, so have 6Moreover, we only sample values for u3 as they are visited by the parser, thus avoiding the quartic complexity. 7Acceptance rates averaged above 99%. 8This restriction is not strictly necessary, however it greatly simplifies the implementation and increases efficiency. comparable LLH). Of particular interest is how the different samplers perform given initialisations of varying quality. We evaluate three </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, W. Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Van Gael</author>
<author>Y Saatci</author>
<author>Y W Teh</author>
<author>Z Ghahramani</author>
</authors>
<title>Beam sampling for the infinite hidden markov model.</title>
<date>2008</date>
<booktitle>In ICML,</booktitle>
<pages>1088--1095</pages>
<location>New York, NY, USA.</location>
<marker>Van Gael, Saatci, Teh, Ghahramani, 2008</marker>
<rawString>J. Van Gael, Y. Saatci, Y. W. Teh, Z. Ghahramani. 2008. Beam sampling for the infinite hidden markov model. In ICML, 1088–1095, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="9652" citStr="Wu, 1997" startWordPosition="1612" endWordPosition="1613">-95.6 Table 1: IWSLT Chinese to English translation. of the second product. The last step (4) discards the term H.3 Q(us; a, b) which is constant wrt d. The net result is a formulation which factors with the derivation structure, thereby eliminating the need to consider all O( e 2 f 2) spans in S. Critically p(d u) is zero for all spans failing the ff (us &lt; 0,3) condition. To exploit the decomposition of Equation 4 we require a parsing algorithm that only explores chart cells whose child cells have not already been pruned by the slice variables. The standard approach of using synchronous CYK (Wu, 1997) doesn’t posses this property: all chart cells would be visited even if they are to be pruned. Instead we use an agenda based parsing algorithm, in particular we extend the algorithm of Klein and Manning (2004) to synchronous parsing.6 Finally, we need a Metropolis-Hastings acceptance step to account for intra-instance dependencies (the ‘rich-get-richer’ effect). We omit the details, save to state that the calculation cancels to the same test as presented in Johnson et al. (2007).7 3 Evaluation In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al., 200</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>