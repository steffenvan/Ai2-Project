<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.989235">
Fast Unsupervised Incremental Parsing
</title>
<author confidence="0.987101">
Yoav Seginer
</author>
<affiliation confidence="0.987719">
Institute for Logic, Language and Computation
</affiliation>
<address confidence="0.73783225">
Universiteit van Amsterdam
Plantage Muidergracht 24
1018TV Amsterdam
The Netherlands
</address>
<email confidence="0.995575">
yseginer@science.uva.nl
</email>
<sectionHeader confidence="0.998572" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954933333333">
This paper describes an incremental parser
and an unsupervised learning algorithm for
inducing this parser from plain text. The
parser uses a representation for syntactic
structure similar to dependency links which
is well-suited for incremental parsing. In
contrast to previous unsupervised parsers,
the parser does not use part-of-speech tags
and both learning and parsing are local
and fast, requiring no explicit clustering or
global optimization. The parser is evalu-
ated by converting its output into equivalent
bracketing and improves on previously pub-
lished results for unsupervised parsing from
plain text.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943775510204">
Grammar induction, the learning of the grammar
of a language from unannotated example sentences,
has long been of interest to linguists because of its
relevance to language acquisition by children. In
recent years, interest in unsupervised learning of
grammar has also increased among computational
linguists, as the difficulty and cost of constructing
annotated corpora led researchers to look for ways
to train parsers on unannotated text. This can ei-
ther be semi-supervised parsing, using both anno-
tated and unannotated data (McClosky et al., 2006)
or unsupervised parsing, training entirely on unan-
notated text.
The past few years have seen considerable im-
provement in the performance of unsupervised
parsers (Klein and Manning, 2002; Klein and Man-
ning, 2004; Bod, 2006a; Bod, 2006b) and, for the
first time, unsupervised parsers have been able to
improve on the right-branching heuristic for pars-
ing English. All these parsers learn and parse
from sequences of part-of-speech tags and select,
for each sentence, the binary parse tree which maxi-
mizes some objective function. Learning is based on
global maximization of this objective function over
the whole corpus.
In this paper I present an unsupervised parser
from plain text which does not use parts-of-speech.
Learning is local and parsing is (locally) greedy. As
a result, both learning and parsing are fast. The
parser is incremental, using a new link representa-
tion for syntactic structure. Incremental parsing was
chosen because it considerably restricts the search
space for both learning and parsing. The represen-
tation the parser uses is designed for incremental
parsing and allows a prefix of an utterance to be
parsed before the full utterance has been read (see
section 3). The representation the parser outputs can
be converted into bracketing, thus allowing evalua-
tion of the parser on standard treebanks.
To achieve completely unsupervised parsing,
standard unsupervised parsers, working from part-
of-speech sequences, need first to induce the parts-
of-speech for the plain text they need to parse. There
are several algorithms for doing so (Sch¨utze, 1995;
Clark, 2000), which cluster words into classes based
on the most frequent neighbors of each word. This
step becomes superfluous in the algorithm I present
here: the algorithm collects lists of labels for each
word, based on neighboring words, and then directly
</bodyText>
<page confidence="0.98164">
384
</page>
<note confidence="0.925494">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 384–391,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999866">
uses these labels to parse. No clustering is per-
formed, but due to the Zipfian distribution of words,
high frequency words dominate these lists and pars-
ing decisions for words of similar distribution are
guided by the same labels.
Section 2 describes the syntactic representation
used, section 3 describes the general parser algo-
rithm and sections 4 and 5 complete the details by
describing the learning algorithm, the lexicon it con-
structs and the way the parser uses this lexicon. Sec-
tion 6 gives experimental results.
</bodyText>
<sectionHeader confidence="0.968779" genericHeader="method">
2 Common Cover Links
</sectionHeader>
<bodyText confidence="0.999835">
The representation of syntactic structure which I in-
troduce in this paper is based on links between pairs
of words. Given an utterance and a bracketing of
that utterance, shortest common cover link sets for
the bracketing are defined. The original bracketing
can be reconstructed from any of these link sets.
</bodyText>
<subsectionHeader confidence="0.990237">
2.1 Basic Definitions
</subsectionHeader>
<bodyText confidence="0.999078769230769">
An utterance is a sequence of words (x1, . . . , x,,,)
and a bracket is any sub-sequence (xi,. . . , xj) of
consecutive words in the utterance. A set 13 of brack-
ets over an utterance U is a bracketing of U if every
word in U is in some bracket and for any X, Y E 13
either X n Y = 0, X C_ Y or Y C_ X (non-
crossing brackets). The depth of a word x E U
under a bracket B E 13 (x E B) is the maxi-
mal number of brackets X1, ... , X,,, E 13 such that
x E X1 C ... C X,,, C B. A word x is agenerator
of depth d of B in 13 if x is of minimal depth under
B (among all words in B) and that depth is d. A
bracket may have more than one generator.
</bodyText>
<subsectionHeader confidence="0.999861">
2.2 Common Cover Link Sets
</subsectionHeader>
<bodyText confidence="0.857972571428572">
A common cover link over an utterance U is a triple
x d* y where x, y E U, x 74 y and d is a non-
negative integer. The word x is the base of the link,
the word y is its head and d is the depth of the link.
The common cover link set RB associated with a
bracketing 13 is the set of common cover links over
U such that x d* y E RB iff the word x is a gener-
ator of depth d of the smallest bracket B E 13 such
that x, y E B (see figure 1(a)).
Given RB, a simple algorithm reconstructs the
bracketing 13: for each word x and depth 0 &lt; d,
some
�
Some of the links in the common cover link set
are redundant. The first redundancy is the result
of brackets having more than one generator. The
bracketing reconstruction algorithm outlined above
can construct a bracket from the links based at any
of its generators. The bracketing 13 can therefore be
reconstructed from a subset R
if, for every
bracket B E 13, R contains the links based at least at
one generator) of B. Such a set R is a representative
subset of
(see figure 1(b)).
A second redundancy in the set
follows from
the linear transitivity of
</bodyText>
<equation confidence="0.895468230769231">
d0&lt;d,x
yERB.
RB
C_RB
RB
RB
RB:
Lemma 1 If y is between x and z, x � y E R
and
�
z E R
then x �
y z E R
</equation>
<bodyText confidence="0.697816">
where if there is a
</bodyText>
<equation confidence="0.985701">
x E
then d =
d2) and d =
otherwise.
</equation>
<bodyText confidence="0.9489808">
This property implies that longer links can be de-
duced from shorter links. It is, therefore, sufficient
to leave only the shortest necessary links in the set.
Given a representative subset R of
a shortest
common cover link set of
is constructed by re-
moving any link which can be deduced from shorter
links by linear transitivity. For each representative
subset R C_
this defines a unique shortest com-
mon cover link set (see figure 1(c)).
Given a shortest common cover link set
the
bracketing which it represents can
</bodyText>
<equation confidence="0.961093916666667">
B
B
B
��
link y �
RB
max(d1,
d1
RB,
RB
RB,
S,
</equation>
<bodyText confidence="0.71475">
be calculated by
</bodyText>
<figureCaption confidence="0.999515">
Figure 1: (a) The common cover link set RB of a
</figureCaption>
<figure confidence="0.96569445">
bracketing 13, (b) a representative subset R of RB,
(c) the shortest common cover link set based on R.
create a bracket covering x and all y such that for
&apos;From the bracket reconstruction algorithm it can
be seen
that links of depth 0 may never be dropped.
��
(a) [ [ w ]
[ x
[ y,0 &gt; z ] ] ]
�� ��
-r
0
1
0
1
1
0
1
1
0
��
(b) [ [ w ] [ x
A
[ y= 0&apos;z ] ] ]
1
0
��
(c) [ [ w ] [ x
[ y _ 0 =z ] ] ]
385
�� ����
[ [ the �� boy ] [ sleeps ] ] ] ]
(a) dependency structure
[ [ I ] [ know
0
0 1
�� 0 �� ��
[ [ the �� �� boy ] [ sleeps ] ] ] ]
(b) shortest common cover link set
</figure>
<figureCaption confidence="0.9889155">
Figure 2: A dependency structure and shortest com-
mon cover link set of the same sentence.
</figureCaption>
<bodyText confidence="0.987815333333333">
first using linear transitivity to deduce missing links
and then applying the bracket reconstruction algo-
rithm outlined above for R.U.
</bodyText>
<subsectionHeader confidence="0.999236">
2.3 Comparison with Dependency Structures
</subsectionHeader>
<bodyText confidence="0.9999945">
Having defined a link-based representation of syn-
tactic structure, it is natural to wonder what the rela-
tion is between this representation and standard de-
pendency structures. The main differences between
the two representations can all be seen in figure 2.
The first difference is in the linking of the NP the
boy. While the shortest common cover link set has
an exocentric construction for this NP (that is, links
going back and forth between the two words), the
dependency structure forces us to decide which of
the two words in the NP is its head. Considering
that linguists have not been able to agree whether it
is the determiner or the noun that is the head of an
NP, it may be easier for a learning algorithm if it did
not have to make such a choice.
The second difference between the structures can
be seen in the link from know to sleeps. In the short-
est common cover link set, there is a path of links
connecting know to each of the words separating it
from sleeps, while in the dependency structure no
such links exist. This property, which I will refer to
as adjacency plays an important role in incremental
parsing, as explained in the next section.
The last main difference between the represen-
tations is the assignment of depth to the common
cover links. In the present example, this allows us to
distinguish between the attachment of the external
(subject) and the internal (object) arguments of the
verb. Dependencies cannot capture this difference
without additional labeling of the links. In what fol-
lows, I will restrict common cover links to having
depth 0 or 1. This restriction means that any tree
represented by a shortest common cover link set will
be skewed - every subtree must have a short branch.
It seems that this is indeed a property of the syntax
of natural languages. Building this restriction into
the syntactic representation considerably reduces the
search space for both parsing and learning.
</bodyText>
<sectionHeader confidence="0.997046" genericHeader="method">
3 Incremental Parsing
</sectionHeader>
<bodyText confidence="0.999980631578947">
To calculate a shortest common cover link for an
utterance, I will use an incremental parser. Incre-
mentality means that the parser reads the words of
the utterance one by one and, as each word is read,
the parser is only allowed to add links which have
one of their ends at that word. Words which have
not yet been read are not available to the parser at
this stage. This restriction is inspired by psycholin-
guistic research which suggests that humans process
language incrementally (Crocker et al., 2000). If the
incrementality of the parser roughly resembles that
of human processing, the result is a significant re-
striction of parser search space which does not lead
to too many parsing errors.
The adjacency property described in the previous
section makes shortest common cover link sets es-
pecially suitable for incremental parsing. Consider
the example given in figure 2. When the word the
is read, the parser can already construct a link from
know to the without worrying about the continuation
of the sentence. This link is part of the correct parse
whether the sentence turns out to be I know the boy
or I know the boy sleeps. A dependency parser, on
the other hand, cannot make such a decision before
the end of the sentence is reached. If the sentence is
I know the boy then a dependency link has to be cre-
ated from know to boy while if the sentence is I know
the boy sleeps then such a link is wrong. This prob-
lem is known in psycholinguistics as the problem of
reanalysis (Sturt and Crocker, 1996).
Assume the incremental parser is processing a
prefix (x1, ... , xk) of an utterance and has already
deduced a set of links L for this prefix. It can now
only add links which have one of their ends at xk and
it may never remove any links. From the definitions
in section 2.2 it is possible to derive an exact char-
acterization of the links which may be added at each
step such that the resulting link set represents some
</bodyText>
<figure confidence="0.672653">
1
��
[ [ I ] [know
</figure>
<page confidence="0.992126">
386
</page>
<bodyText confidence="0.999955818181818">
bracketing. It can be shown that any shortest com-
mon cover link set can be constructed incrementally
under these conditions. As the full specification of
these conditions is beyond the scope of this paper, I
will only give the main condition, which is based on
adjacency. It states that a link may be added from x
to y only if for every z between x and y there is a
path of links (in L) from x to z but no link from z to
y. In the example in figure 2 this means that when
the word sleeps is first read, a link to sleeps can be
created from know, the and boy but not from I.
Given these conditions, the parsing process is
simple. At each step, the parser calculates a non-
negative weight (section 5) for every link which
may be added between the prefix hx1, ... , xk−1i and
xk. It then adds the link with the strongest positive
weight and repeats the process (adding a link can
change the set of links which may be added). When
all possible links are assigned a zero weight by the
parser, the parser reads the next word of the utter-
ance and repeats the process. This is a greedy algo-
rithm which optimizes every step separately.
</bodyText>
<sectionHeader confidence="0.996945" genericHeader="method">
4 Learning
</sectionHeader>
<bodyText confidence="0.999976">
The weight function which assigns a weight to a can-
didate link is lexicalized: the weight is calculated
based on the lexical entries of the words which are
to be connected by the link. It is the task of the learn-
ing algorithm to learn the lexicon.
</bodyText>
<subsectionHeader confidence="0.990114">
4.1 The Lexicon
</subsectionHeader>
<bodyText confidence="0.99624290625">
The lexicon stores for each word x a lexical en-
try. Each such lexical entry is a sequence of adja-
cency points, holding statistics relevant to the deci-
sion whether to link x to some other word. These
statistics are given as weights assigned to labels and
linking properties. Each adjacency point describes a
different link based at x, similar to the specification
of the arguments of a word in dependency parsing.
Let W be the set of words in the corpus. The
set of labels L(W) = W × {0, 1} consists of
two labels based on every word w: a class la-
bel (w, 0) (denoted by [w]) and an adjacency la-
bel (w, 1) (denoted by [w ] or [ w]). The two la-
bels (w, 0) and (w, 1) are said to be opposite la-
bels and, for l ∈ L(W), I write l−1 for the op-
posite of l. In addition to the labels, there is also
a finite set P = {Stop, In*, In, Out} of link-
ing properties. The Stop specifies the strength of
non-attachment, In and Out specify the strength
of inbound and outbound links and In* is an in-
termediate value in the induction of inbound and
outbound strengths. A lexicon L is a function
which assigns each word w ∈ W a lexical entry
(... , Aw−2, Aw−1, Aw1 , Aw2 , ...). Each of the Aw i is an
adjacency point.
Each Aw i is a function Aw i : L(W) ∪ P → R
which assigns each label in L(W) and each linking
property in P a real valued strength. For each Awi ,
#(Awi ) is the count of the adjacency point: the num-
ber of times the adjacency point was updated. Based
on this count, I also define a normalized version of
Awi : Awi (l) = Awi (l)�#(Aw i ).
</bodyText>
<subsectionHeader confidence="0.98988">
4.2 The Learning Process
</subsectionHeader>
<bodyText confidence="0.999993647058824">
Given a sequence of training utterances (Ut)0&lt;t, the
learner constructs a sequence of lexicons (Ls)0&lt;s
beginning with the zero lexicon L0 (which assigns
a zero strength to all labels and linking properties).
At each step, the learner uses the parsing function
PL, based on the previously learned lexicon Ls to
extend the parse L of an utterance Ut. It then uses
the result of this parse step (together with the lexi-
con Ls) to create a new lexicon Ls+1 (it may be that
Ls = Ls+1). This operation is a lexicon update. The
process then continues with the new lexicon Ls+1.
Any of the lexicons Ls constructed by the learner
may be used for parsing any utterance U, but as s
increases, parsing accuracy should improve. This
learning process is open-ended: additional training
text can always be added without having to re-run
the learner on previous training data.
</bodyText>
<subsectionHeader confidence="0.999284">
4.3 Lexicon Update
</subsectionHeader>
<bodyText confidence="0.9665431">
To define a lexicon update, I extend the definition of
an utterance to be U = h∅l, x1,... , xn, ∅ri where ∅l
and ∅r are boundary markers. The property of adja-
cency can now be extended to include the boundary
markers. A symbol α ∈ U is adjacent to a word x
relative to a set of links L over U if for every word z
between x and α there is a path of links in L from x
to z but there is no link from z to α. In the following
example, the adjacencies of x1 are ∅l, x2 and x3:
x1 0 &gt; x2 x3 x4
</bodyText>
<page confidence="0.98335">
387
</page>
<bodyText confidence="0.956765470588235">
If a link is added from x2 to x3, x4 becomes adjacent
to x1 instead of x3 (the adjacencies of x1 are then 0l,
x2 and x4):
x1 0 &gt; x2 0 &gt; x3 x4
The positions in the utterance adjacent to a word x
are indexed by an index i such that i &lt; 0 to the left
of x, i &gt; 0 to the right of x and |i |increases with the
distance from x.
The parser may only add a link from a word x to
a word y adjacent to x (relative to the set of links al-
ready constructed). Therefore, the lexical entry of x
should collect statistics about each of the adjacency
positions of x. As seen above, adjacency positions
may move, so the learner waits until the parser com-
pletes parsing the utterance and then updates each
adjacency point Axi with the symbol a at the ith ad-
jacency position of x (relative to the parse generated
by the parser). It should be stressed that this update
does not depend on whether a link was created from
x to a. In particular, whatever links the parser as-
signs, Ax (−1) and Ax1 are always updated by the sym-
bols which appear immediately before and after x.
The following example should clarify the picture.
Consider the fragment:
put 0 = the ��
�� 0 box on
All the links in this example, including the absence
of a link from box to on, depend on adjacency points
of the form Ax(−1) and Ax1 which are updated inde-
pendently of any links. Based on this alone and re-
gardless of whether a link is created from put to on,
Aput
2 will be updated by the word on, which is in-
deed the second argument of the verb put.
</bodyText>
<subsectionHeader confidence="0.9944">
4.4 Adjacency Point Update
</subsectionHeader>
<bodyText confidence="0.967938">
The update of Axi by a is given by operations
</bodyText>
<equation confidence="0.922045714285714">
Axi (p) += f(Aα(−1), Aα1 )which make the value of
Axi (p) in the new lexicon Ls+1 equal to the sum
Axi (p) + f(Aα(−1), Aα1)in the old lexicon Ls.
Let Sign(i) be 1 if 0 &lt; i and −1 otherwise. Let
true if �l E L(W) :
Aαi (l) &gt; Aαi (Stop)
false otherwise
</equation>
<bodyText confidence="0.919631428571429">
The update of Axi by a begins by incrementing
the count:
#(Axi ) += 1
If a is a boundary symbol (0l or 0r) or if x and a
are words separated by stopping punctuation (full
stop, question mark, exclamation mark, semicolon,
comma or dash):
</bodyText>
<equation confidence="0.97042">
Ax i (Stop) += 1
Otherwise, for every l E L(W):
� 1 if l = [�]
Ax i (l−1) +=
�Aα (l) otherwise
Sign(−i)
</equation>
<bodyText confidence="0.991654714285714">
(In practice, only l = [a] and the 10 strongest labels
in AαSign(−i) are updated. Because of the exponen-
tial decay in the strength of labels in Aα Sign(−i), this
is a good approximation.)
If i = −1,1 and a is not a boundary or blocked
by punctuation, simple bootstrapping takes place by
updating the following properties:
</bodyText>
<table confidence="0.673369428571429">
Axi (In∗) += { −1 if •AαSign(−i)
+1 if -,•Aα
Aα
Sign(−i) n • Sign(i)
0 otherwise
Axi (Out) += AαSign(−i)(In∗)
Ax i (In) += �AαSign(−i)(Out)
</table>
<subsectionHeader confidence="0.9141">
4.5 Discussion
</subsectionHeader>
<bodyText confidence="0.9482625">
To understand the way the labels and properties
are calculated, it is best to look at an example.
The following table gives the linking properties and
strongest labels for the determiner the as learned
from the complete Wall Street Journal corpus (only
Athe
</bodyText>
<table confidence="0.8692885">
(−1) and Athe
1 are shown):
the
A−1 A1
Stop 12897 Stop 8
In* 14898 In* 18914
In 8625 In 4764
Out -13184 Out 21922
[the] 10673 [the] 16461
[of ] 6871 [a] 3107
[in ] 5520 [ the] 2787
[a] 3407 [of] 2347
[for ] 2572 [ company] 2094
[to ] 2094 [’s] 1686
</table>
<bodyText confidence="0.946613">
A strong class label [w] indicates that the word w
frequently appears in contexts which are similar to
the. A strong adjacency label [w ] (or [ w]) indicates
</bodyText>
<equation confidence="0.648379">
•Aα i = {
</equation>
<page confidence="0.990787">
388
</page>
<bodyText confidence="0.969874647058824">
that w either frequently appears next to the or that
w frequently appears in the same contexts as words
which appear next to the.
The property Stop counts the number of times a
boundary appeared next to the. Because the can of-
ten appear at the beginning of an utterance but must
be followed by a noun or an adjective, it is not sur-
prising that Stop is stronger than any label on the
left but weaker than all labels on the right. In gen-
eral, it is unlikely that a word has an outbound link
on the side on which its Stop strength is stronger
than that of any label. The opposite is not true: a
label stronger than Stop indicates an attachment but
this may also be the result of an inbound link, as in
the following entry for to, where the strong labels on
the left are a result of an inbound link:
to
</bodyText>
<table confidence="0.951052181818182">
A−1 A1
Stop 822 Stop 48
In* -4250 In* -981
In -57 In -1791
Out -3053 Out 4010
[to] 5912 [to] 7009
[% ] 848 [ the] 3851
[in] 844 [ be] 2208
[the] 813 [will] 1414
[of] 624 [ a] 1158
[a] 599 [the] 954
</table>
<bodyText confidence="0.999686">
For this reason, the learning process is based on
the property •Ax i which indicates where a link is not
possible. Since an outbound link on one word is in-
bound on the other, the inbound/outbound properties
of each word are then calculated by a simple boot-
strapping process as an average of the opposite prop-
erties of the neighboring words.
</bodyText>
<sectionHeader confidence="0.984004" genericHeader="method">
5 The Weight Function
</sectionHeader>
<bodyText confidence="0.994396210526316">
At each step, the parser must assign a non-negative
weight to every candidate link x � y which may
d
be added to an utterance prefix (x1,... , xk), and the
link with the largest (non-zero) weight (with a pref-
erence for links between xk−1 and xk) is added to
the parse. The weight could be assigned directly
based on the In and Out properties of either x or
y but this method is not satisfactory for three rea-
sons: first, the values of these properties on low fre-
quency words are not reliable; second, the values of
the properties on x and y may conflict; third, some
words are ambiguous and require different linking
in different contexts. To solve these problems, the
weight of the link is taken from the values of In and
Out on the best matching label between x and y.
This label depends on both words and is usually a
frequent word with reliable statistics. It serves as a
prototype for the relation between x and y.
</bodyText>
<subsectionHeader confidence="0.983653">
5.1 Best Matching Label
</subsectionHeader>
<bodyText confidence="0.871620375">
A label l is a matching label between Ax i and
AySign(−i) if Axi(l) &gt; Axi (Stop) and either l = (y, 1)
or AySign(−i)(l−1) &gt; 0. The best matching label
at Axi is the matching label l such that the match
strength min(�Axi (l), �AySign(−i)(l−1)) is maximal (if
l = (y, 1) then �AySign(−i)(l−1) is defined to be 1). In
practice, as before, only the top 10 labels in Axi and
AySign(−i) are considered.
The best matching label from x to y is calculated
between Axi and AySign(−i) such that Axi is on the
same side of x as y and was either already used to
create a link or is the first adjacency point on that
side of x which was not yet used. This means that
the adjacency points on each side have to be used
one by one, but may be used more than once. The
reason is that optional arguments of x usually do
not have an adjacency point of their own but have
the same labels as obligatory arguments of x and
can share their adjacency point. The Axi with the
strongest matching label is selected, with a prefer-
ence for the unused adjacency point.
As in the learning process, label matching is
blocked between words which are separated by stop-
ping punctuation.
</bodyText>
<subsectionHeader confidence="0.999549">
5.2 Calculating the Link Weight
</subsectionHeader>
<bodyText confidence="0.999605166666667">
The best matching label l = (w, S) from x to y can
be either a class (S = 0) or an adjacency (S = 1) la-
bel at Axi . If it is a class label, w can be seen as tak-
ing the place of x and all words separating it from y
(which are already linked to x). If l is an adjacency
label, w can be seen to take the place of y. The cal-
culation of the weight Wt(x *d y) of the link from
x to y is therefore based on the strengths of the In
and Out properties of Awσ where Q = Sign(i) if
l = (w, 0) and Q = Sign(−i) if l = (w, 1). In ad-
dition, the weight is bounded from above by the best
label match strength, s(l):
</bodyText>
<listItem confidence="0.942234">
• If l = (w, 0) and Awσ (Out) &gt; 0:
</listItem>
<equation confidence="0.501934">
Wt(x __0+ y) = min(s(l), �Awσ (Out))
</equation>
<page confidence="0.996505">
389
</page>
<table confidence="0.999611230769231">
Model WSJ10 WSJ40 Negra10 Negra40
UP UR UF, UP UR UF, UP UR UF, UP UR UF,
Right-branching 55.1 70.0 61.7 35.4 47.4 40.5 33.9 60.1 43.3 17.6 35.0 23.4
Right-branching+punct. 59.1 74.4 65.8 44.5 57.7 50.2 35.4 62.5 45.2 20.9 40.4 27.6
Parsing from POS
CCM 64.2 81.6 71.9 48.1 85.5 61.6
DMV+CCM(POS) 69.3 88.0 77.6 49.6 89.7 63.9
U-DOP 70.8 88.2 78.5 63.9 51.2 90.5 65.4
UML-DOP 82.9 66.4 67.0
Parsing from plain text
DMV+CCM(DISTR.) 65.2 82.8 72.9
Incremental 75.6 76.2 75.9 58.9 55.9 57.4 51.0 69.8 59.0 34.8 48.9 40.6
Incremental (right to left) 75.9 72.5 74.2 59.3 52.2 55.6 50.4 68.3 58.0 32.9 45.5 38.2
</table>
<tableCaption confidence="0.999688">
Table 1: Parsing results on WSJ10, WSJ40, Negra10 and Negra40.
</tableCaption>
<listItem confidence="0.7658305">
• If l = (w, 1):
• If AQ (In) &gt; 0:
Wt(x �y) = min(s(l), AQ (In))
• Otherwise, if AQ (In*) &gt; |AQ (In)|:
</listItem>
<bodyText confidence="0.440756">
Wt(x �y) = min(s(l), AQ (In*))
where if AQ (In*) &lt; 0 and AQ (Out) &lt; 0 then
d = 1 and otherwise d = 0.
</bodyText>
<equation confidence="0.766481333333333">
(Out) &lt; 0 and AQ (In) &lt; 0 and either
l = (w, 1) or AQ (Out) = 0:
Wt(x __0+ y) = s(l)
</equation>
<listItem confidence="0.707088">
• In all other cases, Wt(x d* y) = 0.
A link x � y attaches x to y but does not place
</listItem>
<equation confidence="0.327724">
1
</equation>
<bodyText confidence="0.999538294117647">
y inside the smallest bracket covering x. Such links
are therefore created in the second case above, when
the attachment indication is mixed.
To explain the third case, recall that s(l) &gt; 0
means that the label l is stronger than Stop on AZ .
This implies a link unless the properties of w block
it. One way in which w can block the link is to have
a positive strength for the link in the opposite direc-
tion. Another way in which the properties of w can
block the link is if l = (w, 0) and AQ (Out) &lt; 0,
that is, if the learning process has explicitly deter-
mined that no outbound link from w (which repre-
sents x in this case) is possible. The same conclu-
sion cannot be drawn from a negative value for the
In property when l = (w, 1) because, as with stan-
dard dependencies, a word determines its outbound
links much more strongly than its inbound links.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999923862068966">
The incremental parser was tested on the Wall Street
Journal and Negra Corpora.2 Parsing accuracy was
evaluated on the subsets WSJX and NegraX of
these corpora containing sentences of length at most
X (excluding punctuation). Some of these subsets
were used for scoring in (Klein and Manning, 2004;
Bod, 2006a; Bod, 2006b). I also use the same preci-
sion and recall measures used in those papers: mul-
tiple brackets and brackets covering a single word
were not counted, but the top bracket was.
The incremental parser learns while parsing, and
it could, in principle, simply be evaluated for a sin-
gle pass of the data. But, because the quality of the
parses of the first sentences would be low, I first
trained on the full corpus and then measured pars-
ing accuracy on the corpus subset. By training on
the full corpus, the procedure differs from that of
Klein, Manning and Bod who only train on the sub-
set of bounded length sentences. However, this ex-
cludes the induction of parts-of-speech for parsing
from plain text. When Klein and Manning induce
the parts-of-speech, they do so from a much larger
corpus containing the full WSJ treebank together
with additional WSJ newswire (Klein and Manning,
2002). The comparison between the algorithms re-
mains, therefore, valid.
Table 1 gives two baselines and the parsing re-
sults for WSJ10, WSJ40, Negra10 and Negra40
for recent unsupervised parsing algorithms: CCM
</bodyText>
<footnote confidence="0.8748254">
2I also tested the incremental parser on the Chinese Tree-
bank version 5.0, achieving an F, score of 54.6 on CTB10 and
38.0 on CTB40. Because this version of the treebank is newer
and clearly different from that used by previous papers, the re-
sults are not comparable and only given here for completeness.
</footnote>
<listItem confidence="0.435431">
• If AQ
</listItem>
<page confidence="0.954212">
390
</page>
<bodyText confidence="0.89689772">
and DMV+CCM (Klein and Manning, 2004), U- 7 Conclusions
DOP (Bod, 2006b) and UML-DOP (Bod, 2006a). The unsupervised parser I presented here attempts
The middle part of the table gives results for pars- to make use of several universal properties of nat-
ing from part-of-speech sequences extracted from ural languages: it captures the skewness of syntac-
the treebank while the bottom part of the table given tic trees in its syntactic representation, restricts the
results for parsing from plain text. Results for the in- search space by processing utterances incrementally
cremental parser are given for learning and parsing (as humans do) and relies on the Zipfian distribution
from left to right and from right to left. of words to guide its parsing decisions. It uses an
The first baseline is the standard right-branching elementary bootstrapping process to deduce the ba-
baseline. The second baseline modifies right- sic properties of the language being parsed. The al-
branching by using punctuation in the same way as gorithm seems to successfully capture some of these
the incremental parser: brackets (except the top one) basic properties, but can be further refined to achieve
are not allowed to contain stopping punctuation. It high quality parsing. The current algorithm is a good
can be seen that punctuation accounts for merely a starting point for such refinement because it is so
small part of the incremental parser’s improvement very simple.
over the right-branching heuristic. Acknowledgments I would like to thank Dick de
Comparing the two algorithms parsing from plain Jongh for many hours of discussion, and Remko
text (of WSJ10), it can be seen that the incremental Scha, Reut Tsarfaty and Jelle Zuidema for reading
parser has a somewhat higher combined F1 score, and commenting on various versions of this paper.
with better precision but worse recall. This is be- References
cause Klein and Manning’s algorithms (as well as Rens Bod. 2006a. An all-subtrees approach to unsuper-
Bod’s) always generate binary parse trees, while vised parsing. In Proceedings of COLING-ACL 2006.
here no such condition is imposed. The small differ- Rens Bod. 2006b. Unsupervised parsing with U-DOP.
ence between the recall (76.2) and precision (75.6) In Proceedings of CoNLL 10.
of the incremental parser shows that the number of Alexander Clark. 2000. Inducing syntactic categories
brackets induced by the parser is very close to that by context distribution clustering. In Proceedings of
of the corpus3 and that the parser captures the same CoNLL 4.
depth of syntactic structure as that which was used Matthew W. Crocker, Martin Pickering, and Charles
by the corpus annotators. Clifton. 2000. Architectures and Mechanisms for
Incremental parsing from right to left achieves re- Language Processing. Cambridge University Press.
sults close to those of parsing from left to right. This Dan Klein and Christopher D. Manning. 2002. A gener-
shows that the incremental parser has no built-in bias ative constituent-context model for improved grammar
for right branching structures.4 The slight degra- induction. In Proceedings ofACL 40, pages 128–135.
dation in performance may suggest that language Dan Klein and Christopher D. Manning. 2004. Corpus-
should not, after all, be processed backwards. based induction of syntactic structure: Models of de-
While achieving state of the art accuracy, the algo- pendency and constituency. In Proceedings ofACL 42.
rithm also proved to be fast, parsing (on a 1.86GHz David McClosky, Eugene Charniak, and Mark Johnson.
Centrino laptop) at a rate of around 4000 words/sec. 2006. Effective self-training for parsing. In Proceed-
and learning (including parsing) at a rate of 3200 – ings ofHLT-NAACL 2006.
3600 words/sec. The effect of sentence length on Hinrich Sch¨utze. 1995. Distributional part-of-speech
parsing speed is small: the full WSJ corpus was tagging. In Proceedings ofEACL 7.
parsed at 3900 words/sec. while WSJ10 was parsed Patrick Sturt and Matthew W. Crocker. 1996. Mono-
at 4300 words/sec. tonic syntactic processing: A cross-linguistic study of
attachment and reanalysis. Language and Cognitive
Processes, 11(5):449–492.
3The algorithm produced 35588 brackets compared with
35302 brackets in the corpus.
4I would like to thank Alexander Clark for suggesting this
test.
391
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.639675">
<title confidence="0.999982">Fast Unsupervised Incremental Parsing</title>
<author confidence="0.897463">Yoav Seginer</author>
<affiliation confidence="0.8957865">Institute for Logic, Language and Computation Universiteit van Amsterdam</affiliation>
<address confidence="0.920246">Plantage Muidergracht 24 1018TV Amsterdam The Netherlands</address>
<email confidence="0.997564">yseginer@science.uva.nl</email>
<abstract confidence="0.9980445625">This paper describes an incremental parser and an unsupervised learning algorithm for inducing this parser from plain text. The parser uses a representation for syntactic structure similar to dependency links which is well-suited for incremental parsing. In contrast to previous unsupervised parsers, the parser does not use part-of-speech tags and both learning and parsing are local and fast, requiring no explicit clustering or global optimization. The parser is evaluated by converting its output into equivalent bracketing and improves on previously published results for unsupervised parsing from plain text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>