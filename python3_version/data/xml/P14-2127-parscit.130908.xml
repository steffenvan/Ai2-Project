<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99036">
Hierarchical MT Training using Max-Violation Perceptron
</title>
<author confidence="0.996876">
Kai Zhao† Liang Huang† Haitao Mi‡ Abe Ittycheriah‡
</author>
<affiliation confidence="0.851582">
†Graduate Center &amp; Queens College ‡T. J. Watson Research Center
City University of New York IBM
</affiliation>
<email confidence="0.985457">
{kzhao@gc,huang@cs.qc}.cuny.edu {hmi,abei}@us.ibm.com
</email>
<note confidence="0.705066">
Abstract Collins (02) inexact Huang et al. (12) latent Yu et al. (13)
�� ��
search variable
</note>
<bodyText confidence="0.999230888888889">
Large-scale discriminative training has be-
come promising for statistical machine
translation by leveraging the huge train-
ing corpus; for example the recent effort
in phrase-based MT (Yu et al., 2013) sig-
nificantly outperforms mainstream meth-
ods that only train on small tuning sets.
However, phrase-based MT suffers from
limited reorderings, and thus its training
can only utilize a small portion of the bi-
text due to the distortion limit. To address
this problem, we extend Yu et al. (2013)
to syntax-based MT by generalizing their
latent variable “violation-fixing” percep-
tron from graphs to hypergraphs. Exper-
iments confirm that our method leads to
up to +1.2 BLEU improvement over main-
stream methods such as MERT and PRO.
</bodyText>
<sectionHeader confidence="0.99813" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99157619047619">
Many natural language processing problems in-
cluding part-of-speech tagging (Collins, 2002),
parsing (McDonald et al., 2005), and event extrac-
tion (Li et al., 2013) have enjoyed great success us-
ing large-scale discriminative training algorithms.
However, a similar success on machine translation
has been elusive, where the mainstream methods
still tune on small datasets.
What makes large-scale MT training so hard
then? After numerous attempts by various re-
searchers (Liang et al., 2006; Watanabe et al.,
2007; Arun and Koehn, 2007; Blunsom et al.,
2008; Chiang et al., 2008; Flanigan et al., 2013;
Green et al., 2013), the recent work of Yu et al.
(2013) finally reveals a major reason: it is the vast
amount of (inevitable) search errors in MT decod-
ing that astray learning. To alleviate this prob-
lem, their work adopts the theoretically-motivated
framework of violation-fixing perceptron (Huang
et al., 2012) tailed for inexact search, yielding
great results on phrase-based MT (outperforming
</bodyText>
<note confidence="0.8096585">
1 hypergraph 1
Zhang et al. (13) this work
</note>
<figureCaption confidence="0.997813">
Figure 1: Relationship with previous work.
</figureCaption>
<bodyText confidence="0.999992904761905">
small-scale MERT/PRO by a large margin for the
first time). However, the underlying phrase-based
model suffers from limited distortion and thus can
only employ a small portion (about 1/3 in their Ch-
En experiments) of the bitext in training.
To better utilize the large training set, we
propose to generalize from phrase-based MT to
syntax-based MT, in particular the hierarchical
phrase-based translation model (HIERO) (Chiang,
2005), in order to exploit sentence pairs beyond
the expressive capacity of phrase-based MT.
The key challenge here is to extend the latent
variable violation-fixing perceptron of Yu et al.
(2013) to handle tree-structured derivations and
translation hypergraphs. Luckily, Zhang et al.
(2013) have recently generalized the underlying
violation-fixing perceptron of Huang et al. (2012)
from graphs to hypergraphs for bottom-up parsing,
which resembles syntax-based decoding. We just
need to further extend it to handle latent variables.
We make the following contributions:
</bodyText>
<listItem confidence="0.916687846153846">
1. We generalize the latent variable violation-
fixing perceptron framework to inexact
search over hypergraphs, which subsumes
previous algorithms for PBMT and bottom-
up parsing as special cases (see Fig. 1).
2. We show that syntax-based MT, with its bet-
ter handling of long-distance reordering, can
exploit a larger portion of the training set,
which facilitates sparse lexicalized features.
3. Experiments show that our training algo-
rithm outperforms mainstream tuning meth-
ods (which optimize on small devsets) by
+1.2 BLEU over MERT and PRO on FBIS.
</listItem>
<figure confidence="0.8046318">
785
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 785–790,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
id
rule
talks
X
S[0:5]
5 ]
hu`ıt´an 5j
X
ˇux´ıng 4
[2:3 ]S
h¯al´o
n giuyˇu2 S
X[1:5]
ı
X
´an5jˇ
X
n4 g
ux
ı
hu
3held2
S[
:5]
X[1
:5
yˇu 2
X4[ :
[
S
h1
0
1
:
1 ]
0B` u
:1]
X [
l´on
3]
Sh¯
3
[
:
5 with 4
Sha r
X
</figure>
<page confidence="0.175175">
1]
</page>
<equation confidence="0.891712714285714">
X[0:
h´ı1
S
]
0B`
s
n
:
0
sh´ ı1SXX talks 5he ld4X Sharon3wit h 2S X0Bush1 (a)HIEROru
[
S → hX 1 ,X 1 i
S
S → hS 1 X 2 ,S 1 X 2 i
X → hB`ush´ı, Bushi
X → hSh¯al´ong, Sharoni
X → hhu`ıt´an, talksi
0 Bus
X → hyˇu X 1 jˇux´ıng X 2 ,
held X 2 with X 1 i
X
X → hyˇu Sh¯al´ong, with Sharoni
X → hX 1 jˇux´ıng X 2 ,
X 1 held X 2 i
r0
r1
r2
r3
r4
r5
r6
r7
les(b) go ld derivat io n (c) V iterbideriva
Fi ure2: ER Otr nexam
t
</equation>
<figureCaption confidence="0.605589666666667">
Figure 3: A —LM hypergraph with two der1va-
tionFi gur e 3:A −LMhypergraph with two deriv a-tion
s:t heg oldderi vation(Fig . 2b) i n so li dlines ,andt h
</figureCaption>
<figure confidence="0.780546">
0
1
` g a
e she d lines. Viterbi derivation (Fig. 2c) in da
</figure>
<sectionHeader confidence="0.399294" genericHeader="keywords">
2 Review: Syntax-based MT Decoding
</sectionHeader>
<bodyText confidence="0.913387826086956">
r clarity reasons we will describe HIERO decod- Fo
g as a two-pass process, first without a language in
odel, ec tio and n then integrating the LM. This s m
). mostly follows Huang and Chiang (2 007
In the first, −LM phase, the decoder parses the
he rce sentence using the source projection of sou t
- synchronous grammar (see Fig. 2 (a) for an ex
ample), producing a −LM hypergraph where each
r- de has a signature N[i:j], where N is the n on no te
minal type (either X or S in HIERO) and [i : j] is
the io span, n and each hyperedge e is an appli ca t
). of the translation rule r(e) (see Figure 3
To incorporate the language model, each node
y also needs to remember its target side bou ndar
words. Thus a −LM node N[i:j] is plits into mul-
l e +LM nodes of signature Na?b [i:j], where a and
tip
b are the s. boundary For word example, with a bi-
Xheld?Sharon
tion [1:5] is a node whose transl a gram LM,
. starts with “held” and ends with “Shar on”
n More formally, the whole decoding process ca
be cast as a deductive system. Take the partial
</bodyText>
<equation confidence="0.895457666666667">
tra
iva
:5]:s2 [2:3] [ 4
</equation>
<bodyText confidence="0.964899909090909">
r5, Xheld?Sharon[1:5]:s1 +s2 +s(r5) +λ
wheres(r5) is the scoreof ruler5, and theLM
comboscore λ islog P lm(tal
k s|held)Plm(with |talks)Plm( Sha ron |w
nslation on”in of Figure “held 2 talkswithSh ar
it h).3Viola ti on-Fixi ng Perceptronfor HIERO
As mentioned in Section 1,thekey to thesuccess
ofYue tal. (2013) is thead op tio n ofv io latio
n- fixingperceptronof Huanget al.(201 2)w hichist
ailo re df or vast lyinexact se arc h. Theg en era
lideai stoupd atesom ewher einthe mi ddleof thes ea
rch (whe res earcherro rhappe ns )rath erthanatt he
verye nd (s ta ndard upda teisoftenin val id).To
adaptit toMT wherema ny deri vations c anoutputthe sa
me tra nslati on(i.e. ,s pu rious a mbigui ty),Yueta
l.(20 13)extends it tohandle late ntvariablesw hi
chc orres pond t ophra se -ba sed der iva tions.Onthe
other ha nd, Zhang e tal. (2013) ha sgeneralize
dHu angetal.( 2012)fro mgrap hs tohyper graph
sfor botto m- up pars ing, wh ichrese mbl esH
IEROdecoding .Sowejust n eedtoco mbinethe two lizing (latent directions
variable and hyper- genera
</bodyText>
<figureCaption confidence="0.610734">
Fig. 1). graph, see
</figureCaption>
<bodyText confidence="0.738927857142857">
h 3.1 Latent Variable Hypergraph Searc
g The key difference between bottom-up pa rsin
ee and MT decoding is that in parsing the gold tr
for each input sentence is unique, while in MT
ny derivations can generate the same reference ma
slatio n. In other words, he gold dr tion tr an to
update towards is a latent variable.
</bodyText>
<figure confidence="0.969564714285714">
on.
5]
nslat leofH
ion
X
0
u
[4:5]
)forexample, tion the is deduc
(b
XS
haron?Sharon : s1 Xtalks?talks
´
] 786
</figure>
<bodyText confidence="0.999933875">
Here we formally define the latent variable
“max-violation” perceptron over a hypergraph for
MT training. For a given sentence pair hx, yi, we
denote H(x) as the decoding hypergraph of HI-
ERO without any pruning. We say D ∈ H(x) if
D is a full derivation of decoding x, and D can be
derived from the hypergraph. Let good(x, y) be
the set of y-good derivations for hx, yi:
</bodyText>
<equation confidence="0.605996">
good(x, y) Δ= {D ∈ H(x)  |e(D) = y},
</equation>
<bodyText confidence="0.999897">
where e(D) is the translation from derivation D.
We then define the set of y-good partial derivations
that cover x[i:j] with root N[i:j] as
</bodyText>
<equation confidence="0.9950085">
goodN[i:j](x, y) Δ= {d ∈ D  |D ∈ good(x, y),
root(d) = N[i:j]}
</equation>
<bodyText confidence="0.999979333333333">
We further denote the real decoding hypergraph
with beam-pruning and cube-pruning as H0(x).
The set of y-bad derivations is defined as
</bodyText>
<equation confidence="0.9931405">
badN[i:j](x, y) Δ= {d ∈ D  |D ∈ H0(x, y),
root(d) = N[i:j], d ∈6 goodN[i:j](x, y)}.
</equation>
<bodyText confidence="0.99961">
Note that the y-good derivations are defined over
the unpruned whole decoding hypergraph, while
the y-bad derivations are defined over the real de-
coding hypergraph with pruning.
The max-violation method performs the update
where the model score difference between the
incorrect Viterbi partial derivation and the best
y-good partial derivation is maximal, by penaliz-
ing the incorrect Viterbi partial derivation and re-
warding the y-good partial derivation.
More formally, we first find the Viterbi partial
derivation d− and the best y-good partial deriva-
tion d+ for each N[i:j] group in the pruned +LM
hypergraph:
</bodyText>
<equation confidence="0.980762">
d++, .: (x, y)Δ=argmax w · Φ (x, d),
[`j ] d∈goodN[i:j](x,y)
d−N .. (x, y) = argmax w · Φ(x, d),
[` j] d∈badN[i:j](x,y)
</equation>
<bodyText confidence="0.9976224">
where Φ(x, d) is the feature vector for derivation
d. Then it finds the group N∗[i∗:j∗] with the max-
imal score difference between the Viterbi deriva-
tion and the best y-good derivation:
and update as follows:
</bodyText>
<equation confidence="0.975137">
w ← w + ΔΦ(x, d+ (x, y), d− (x, y)),
N∗ N∗
[i∗:j∗][i∗:j∗]
</equation>
<bodyText confidence="0.981641">
where ΔΦ(x, d, d0) Δ= Φ(x, d) − Φ(x, d0).
</bodyText>
<subsectionHeader confidence="0.573247">
3.2 Forced Decoding for HIERO
</subsectionHeader>
<bodyText confidence="0.99386">
We now describe how to find the gold derivations.1
Such derivations can be generated in way similar
to Yu et al. (2013) by using a language model tai-
lored for forced decoding:
</bodyText>
<equation confidence="0.90518">
�
1 ifq=p+1
0 otherwise ,
</equation>
<bodyText confidence="0.975005294117647">
where p and q are the indices of the boundary
words in the reference translation. The +LM node
now has signature Np?q
[i:j], where p and q are the in-
dexes of the boundary words. If a boundary word
does not occur in the reference, its index is set to
∞ so that its language model score will always be
−∞; if a boundary word occurs more than once in
the reference, its −LM node is split into multiple
+LM nodes, one for each such index.2
We have a similar deductive system for forced
decoding. For the previous example, rule r5 in
Figure 2 (a) is rewritten as
X → hyˇu X 1 jˇux´ıng X 2 ,1 X 2 4 X 1 i,
where 1 and 4 are the indexes for reference words
“held” and “with” respectively. The deduction for
X[1:5] in Figure 2 (b) is
</bodyText>
<equation confidence="0.968327666666667">
r5,
X1?5
[1:5] : s(r5) + λ + s1 + s2
</equation>
<bodyText confidence="0.978994">
where λ = log II∈{1,3,4} Pforced(i + 1  |i) = 0.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="introduction">
4 Experiments
</sectionHeader>
<bodyText confidence="0.967784454545455">
Following Yu et al. (2013), we call our max-
violation method MAXFORCE. Our implemen-
tation is mostly in Python on top of the cdec
system (Dyer et al., 2010) via the pycdec in-
terface (Chahuneau et al., 2012). In addition, we
use minibatch parallelization of (Zhao and Huang,
1We only consider single reference in this paper.
2Our formulation of index-based language model fixes a
bug in the word-based LM of Yu et al. (2013) when a sub-
string appears more than once in the reference (e.g. “the
man...the man...”); thanks to Dan Gildea for pointing it out.
</bodyText>
<equation confidence="0.968065">
Δ
N∗ = argmax
[i∗:j∗]
N[i:j]
w · ΔΦ(x, d+N[i:j](x, y), d−N[i:j](x, y)),
Pforced(q  |p) =
X5?5 X2?3
[2:3] : s1 [4:5] : s2
787
</equation>
<bodyText confidence="0.996033285714286">
2013) to speedup perceptron training. We evalu-
ate MAXFORCE for HIERO over two CH-EN cor-
pora, IWSLT09 and FBIS, and compare the per-
formance with vanilla n-best MERT (Och, 2003)
from Moses (Koehn et al., 2007), Hypergraph
MERT (Kumar et al., 2009), and PRO (Hopkins
and May, 2011) from cdec.
</bodyText>
<subsectionHeader confidence="0.91643">
4.1 Features Design
</subsectionHeader>
<bodyText confidence="0.99999625">
We use all the 18 dense features from cdec, in-
cluding language model, direct translation prob-
ability p(e|f), lexical translation probabilities
pl(e|f) and pl(f|e), length penalty, counts for the
source and target sides in the training corpus, and
flags for the glue rules and pass-through rules.
For sparse features we use Word-Edges fea-
tures (Charniak and Johnson, 2005; Huang, 2008)
which are shown to be extremely effective in
both parsing and phrase-based MT (Yu et al.,
2013). We find that even simple Word-Edges
features boost the performance significantly, and
adding complex Word-Edges features from Yu et
al. (2013) brings limited improvement and slows
down the decoding. So in the following experi-
ments we only use Word-Edges features consisting
of combinations of English and Chinese words,
and Chinese characters, and do not use word clus-
ters nor word types. For simplicity and efficiency
reasons, we also exclude all non-local features.
</bodyText>
<subsectionHeader confidence="0.980208">
4.2 Datasets and Preprocessing
</subsectionHeader>
<bodyText confidence="0.999958380952381">
Our first corpus, IWSLT09, contains —30k
short sentences collected from spoken language.
IWSLT04 is used as development set in MAX-
FORCE training, and as tuning set for n-best
MERT, Hypergraph MERT, and PRO. IWSLT05
is used as test set. Both IWSLT04 and IWSLT05
contain 16 references.We mainly use this corpus
to investigate the properties of MAXFORCE.
The second corpus, FBIS, contains —240k sen-
tences. NIST06 newswire is used as development
set for MAXFORCE training, and as tuning set
for all other tuning methods. NIST08 newswire
is used as test set. Both NIST06 newswire
and NIST08 newswire contain 4 references. We
mainly use this corpus to demonstrate the perfor-
mance of MAXFORCE in large-scale training.
For both corpora, we do standard tokeniza-
tion, alignment and rule extraction using the cdec
tools. In rule extraction, we remove all 1-count
rules but keep the rules mapping from one Chi-
nese word to one English word to help balancing
</bodyText>
<table confidence="0.9926455">
sent. words
phrase-based MT 32% 12%
HIERO 35% 30%
HIERO (all rules) 65% 55%
</table>
<tableCaption confidence="0.998465">
Table 1: Reachability comparison (on FBIS) be-
</tableCaption>
<bodyText confidence="0.914425">
tween phrase-based MT reported in Yu et al.
(2013) (without 1-count rules) and HIERO (with
and without 1-count rules).
Figure 4: Reachability vs. sent. length on FBIS.
See text below for “loose” and “tight”.
between overfitting and coverage. We use a tri-
gram language model trained from the target sides
of the two corpora respectively.
</bodyText>
<subsectionHeader confidence="0.995745">
4.3 Forced Decoding Reachability
</subsectionHeader>
<bodyText confidence="0.999994117647059">
We first report the forced decoding reachability for
HIERO on FBIS in Table 1. With the full rule set,
65% sentences and 55% words of the whole cor-
pus are forced decodable in HIERO. After pruning
1-count rules, our forced decoding covers signif-
icantly more words than phrase-based MT in Yu
et al. (2013). Furthermore, in phrase-based MT,
most decodable sentences are very short, while
in HIERO the lengths of decodable sentences are
more evenly distributed.
However, in the following experiments, due to
efficiency considerations, we use the “tight” rule
extraction in cdec that is more strict than the
standard “loose” rule extraction, which generates
a reduced rule set and, thus, a reduced reachabil-
ity. We show the reachability distributions of both
tight and loose rule extraction in Figure 4.
</bodyText>
<subsectionHeader confidence="0.997228">
4.4 Evaluation on IWSLT
</subsectionHeader>
<bodyText confidence="0.999745333333333">
For IWSLT, we first compare the performance
from various update methods in Figure 5. The
max-violation method is more than 15 BLEU
</bodyText>
<figure confidence="0.984189928571428">
20 40 60 80 100
sentence length
forced decodable ratio
0.8
0.6
0.4
0.2
0
1
loose
tight
788
2 4 6 8 10 12 14 16 18 20
iteration
</figure>
<figureCaption confidence="0.793103">
Figure 6: Sparse features (Word-Edges) contribute
</figureCaption>
<bodyText confidence="0.891407807692308">
—2 BLEU points, outperforming PRO and MERT.
points better than the standard perceptron (also
known as “bold-update” in Liang et al. (2006))
which updates at the root of the derivation tree.3,4
This can be explained by the fact that in train-
ing —58% of the standard updates are invalid (i.e.,
they do not fix any violation). We also use the
“skip” strategy of Zhang et al. (2013) which up-
dates at the root of the derivation only when it fixes
a search error, avoiding all invalid updates. This
achieves —10 BLEU better than the standard up-
date, but is still more than —5 BLEU worse than
Max-Violation update. Finally we also try the
“local-update” method from Liang et al. (2006)
which updates towards the derivation with the best
Bleu+1 in the root group S[0:|x|]. This method is
about 2 BLEU points worse than max-violation.
We further investigate the contribution of sparse
features in Figure 6. On the development set,
max-violation update without Word-Edges fea-
tures achieves BLEU similar to n-best MERT and
3We find that while MAXFORCE generates translations of
length ratio close to 1 during training, the length ratios on
dev/test sets are significantly lower, due to OOVs. So we
run a binary search for the length penalty weight after each
training iteration to tune the length ratio to ∼0.97 on dev set.
</bodyText>
<table confidence="0.929959714285714">
4 We report BLEU with averaged reference lengths.
algorithm # feats dev test
n-best MERT 18 44.9 47.9
Hypergraph MERT 18 46.6 50.7
PRO 18 45.0 49.5
local update perc. 443K 45.6 49.1
MAXFORCE 529K 47.4 51.5
</table>
<tableCaption confidence="0.9042705">
Table 2: BLEU scores (with 16 references) of var-
ious training algorithms on IWSLT09.
</tableCaption>
<table confidence="0.99910325">
algorithm # feats dev test
Hypergraph MERT 18 27.3 23.0
PRO 18 26.4 22.7
MAXFORCE 4.5M 27.7 23.9
</table>
<tableCaption confidence="0.860253">
Table 3: BLEU scores (with 4 references) of vari-
ous training algorithms on FBIS.
</tableCaption>
<bodyText confidence="0.998755666666667">
PRO, but lower than Hypergraph MERT. Adding
simple Word-Edges features improves BLEU by
—2 points, outperforming the very strong Hyper-
graph MERT baseline by —1 point. See Table 2 for
details. The results of n-best MERT, Hypergraph
MERT, and PRO are averages from 3 runs.
</bodyText>
<subsectionHeader confidence="0.986175">
4.5 Evaluation on FBIS
</subsectionHeader>
<bodyText confidence="0.999949571428571">
Table 3 shows BLEU scores of Hypergraph MERT,
PRO, and MAXFORCE on FBIS. MAXFORCE ac-
tives 4.5M features, and achieves +1.2 BLEU over
PRO and +0.9 BLEU over Hypergraph MERT. The
training time (on 32 cores) for Hypergraph MERT
and PRO is about 30 min. on the dev set, and is
about 5 hours for MAXFORCE on the training set.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="method">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999983857142857">
We have presented a latent-variable violation-
fixing framework for general structured predic-
tion problems with inexact search over hyper-
graphs. Its application on HIERO brings signif-
icant improvement in BLEU, compared to algo-
rithms that are specially designed for MT tuning
such as MERT and PRO.
</bodyText>
<sectionHeader confidence="0.939624" genericHeader="method">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.99946925">
Part of this work was done during K. Z.’s intern-
ship at IBM. We thank Martin ˇCmejrek and Lemao
Liu for discussions, David Chiang for pointing
us to pycdec, Dan Gildea for Footnote 2, and
the anonymous reviewers for comments. This
work is supported by DARPA FA8750-13-2-0041
(DEFT), DARPA HR0011-12-C-0015 (BOLT),
and a Google Faculty Research Award.
</bodyText>
<figure confidence="0.94758975">
45
40
35
30
Max-Violation
local update
skip
standard update
</figure>
<figureCaption confidence="0.989611">
Figure 5: Comparison of various update methods.
</figureCaption>
<figure confidence="0.9612061875">
2 4 6 8 10 12 14 16 18 20
iteration
BLEU on dev
47
46
45
44
43
42
sparse features
dense features
Hypergraph MERT
PRO
n-best MERT
BLEU on dev
789
</figure>
<sectionHeader confidence="0.953413" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999026556818182">
Abhishek Arun and Philipp Koehn. 2007. On-
line learning methods for discriminative training of
phrase based statistical machine translation. Proc.
of MT Summit XI, 2(5):29.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL, pages 200–208.
Victor Chahuneau, Noah Smith, and Chris Dyer. 2012.
pycdec: A python interface to cdec. Prague Bulletin
of Mathematical Linguistics, (98).
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173–180,
Ann Arbor, Michigan, June.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP
2008.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In Proceedings of NAACL 2013.
Spence Green, Sida Wang, Daniel Cer, and Christo-
pher D Manning. 2013. Fast and adaptive online
training of feature-rich translation models. to ap-
pear) ACL.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL, Prague, Czech Rep., June.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of NAACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL: HLT, Columbus, OH, June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit
for statistical machine translation. In Proceedings
ofACL.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of ACL and AFNLP.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings ofACL.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proceed-
ings of COLING-ACL, Sydney, Australia, July.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd ACL.
Franz Joseph Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160–167.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proceedings of
EMNLP-CoNLL.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In Proceedings of
EMNLP.
Hao Zhang, Liang Huang, Kai Zhao, and Ryan Mc-
Donald. 2013. Online learning with inexact hyper-
graph search. In Proceedings of EMNLP.
Kai Zhao and Liang Huang. 2013. Minibatch and par-
allelization for online large margin structured learn-
ing. In Proceedings of NAACL 2013.
</reference>
<page confidence="0.87642">
790
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.550513">
<title confidence="0.999167">Hierarchical MT Training using Max-Violation Perceptron</title>
<author confidence="0.992713">Liang Abe</author>
<affiliation confidence="0.949394">Center &amp; Queens College J. Watson Research Center City University of New York IBM</affiliation>
<abstract confidence="0.980620142857143">02) et al. (12) Yu et al. (13) �� �� search variable Large-scale discriminative training has become promising for statistical machine translation by leveraging the huge training corpus; for example the recent effort in phrase-based MT (Yu et al., 2013) significantly outperforms mainstream methods that only train on small tuning sets. However, phrase-based MT suffers from limited reorderings, and thus its training can only utilize a small portion of the bitext due to the distortion limit. To address this problem, we extend Yu et al. (2013) to syntax-based MT by generalizing their latent variable “violation-fixing” perceptron from graphs to hypergraphs. Experiments confirm that our method leads to to +1.2 over mainmethods such as</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abhishek Arun</author>
<author>Philipp Koehn</author>
</authors>
<title>Online learning methods for discriminative training of phrase based statistical machine translation.</title>
<date>2007</date>
<booktitle>Proc. of MT Summit XI,</booktitle>
<pages>2--5</pages>
<contexts>
<context position="1622" citStr="Arun and Koehn, 2007" startWordPosition="243" endWordPosition="246">p to +1.2 BLEU improvement over mainstream methods such as MERT and PRO. 1 Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1 hypergraph 1 Zhang et al. (13) this work Figure 1: Relationship with previous work. small-scale MERT/PRO by a large margin for the fi</context>
</contexts>
<marker>Arun, Koehn, 2007</marker>
<rawString>Abhishek Arun and Philipp Koehn. 2007. Online learning methods for discriminative training of phrase based statistical machine translation. Proc. of MT Summit XI, 2(5):29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>200--208</pages>
<contexts>
<context position="1644" citStr="Blunsom et al., 2008" startWordPosition="247" endWordPosition="250">ment over mainstream methods such as MERT and PRO. 1 Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1 hypergraph 1 Zhang et al. (13) this work Figure 1: Relationship with previous work. small-scale MERT/PRO by a large margin for the first time). However, th</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In ACL, pages 200–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Chahuneau</author>
<author>Noah Smith</author>
<author>Chris Dyer</author>
</authors>
<title>pycdec: A python interface to cdec.</title>
<date>2012</date>
<journal>Prague Bulletin of Mathematical Linguistics,</journal>
<volume>98</volume>
<contexts>
<context position="10514" citStr="Chahuneau et al., 2012" startWordPosition="1883" endWordPosition="1886">es, one for each such index.2 We have a similar deductive system for forced decoding. For the previous example, rule r5 in Figure 2 (a) is rewritten as X → hyˇu X 1 jˇux´ıng X 2 ,1 X 2 4 X 1 i, where 1 and 4 are the indexes for reference words “held” and “with” respectively. The deduction for X[1:5] in Figure 2 (b) is r5, X1?5 [1:5] : s(r5) + λ + s1 + s2 where λ = log II∈{1,3,4} Pforced(i + 1 |i) = 0. 4 Experiments Following Yu et al. (2013), we call our maxviolation method MAXFORCE. Our implementation is mostly in Python on top of the cdec system (Dyer et al., 2010) via the pycdec interface (Chahuneau et al., 2012). In addition, we use minibatch parallelization of (Zhao and Huang, 1We only consider single reference in this paper. 2Our formulation of index-based language model fixes a bug in the word-based LM of Yu et al. (2013) when a substring appears more than once in the reference (e.g. “the man...the man...”); thanks to Dan Gildea for pointing it out. Δ N∗ = argmax [i∗:j∗] N[i:j] w · ΔΦ(x, d+N[i:j](x, y), d−N[i:j](x, y)), Pforced(q |p) = X5?5 X2?3 [2:3] : s1 [4:5] : s2 787 2013) to speedup perceptron training. We evaluate MAXFORCE for HIERO over two CH-EN corpora, IWSLT09 and FBIS, and compare the p</context>
</contexts>
<marker>Chahuneau, Smith, Dyer, 2012</marker>
<rawString>Victor Chahuneau, Noah Smith, and Chris Dyer. 2012. pycdec: A python interface to cdec. Prague Bulletin of Mathematical Linguistics, (98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="11666" citStr="Charniak and Johnson, 2005" startWordPosition="2078" endWordPosition="2081">ORCE for HIERO over two CH-EN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al., 2007), Hypergraph MERT (Kumar et al., 2009), and PRO (Hopkins and May, 2011) from cdec. 4.1 Features Design We use all the 18 dense features from cdec, including language model, direct translation probability p(e|f), lexical translation probabilities pl(e|f) and pl(f|e), length penalty, counts for the source and target sides in the training corpus, and flags for the glue rules and pass-through rules. For sparse features we use Word-Edges features (Charniak and Johnson, 2005; Huang, 2008) which are shown to be extremely effective in both parsing and phrase-based MT (Yu et al., 2013). We find that even simple Word-Edges features boost the performance significantly, and adding complex Word-Edges features from Yu et al. (2013) brings limited improvement and slows down the decoding. So in the following experiments we only use Word-Edges features consisting of combinations of English and Chinese words, and Chinese characters, and do not use word clusters nor word types. For simplicity and efficiency reasons, we also exclude all non-local features. 4.2 Datasets and Pre</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of ACL, pages 173–180, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="1665" citStr="Chiang et al., 2008" startWordPosition="251" endWordPosition="254">ethods such as MERT and PRO. 1 Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1 hypergraph 1 Zhang et al. (13) this work Figure 1: Relationship with previous work. small-scale MERT/PRO by a large margin for the first time). However, the underlying phrase-b</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2606" citStr="Chiang, 2005" startWordPosition="400" endWordPosition="401">l., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1 hypergraph 1 Zhang et al. (13) this work Figure 1: Relationship with previous work. small-scale MERT/PRO by a large margin for the first time). However, the underlying phrase-based model suffers from limited distortion and thus can only employ a small portion (about 1/3 in their ChEn experiments) of the bitext in training. To better utilize the large training set, we propose to generalize from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (HIERO) (Chiang, 2005), in order to exploit sentence pairs beyond the expressive capacity of phrase-based MT. The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: 1. We generalize the latent vari</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1180" citStr="Collins, 2002" startWordPosition="176" endWordPosition="177">performs mainstream methods that only train on small tuning sets. However, phrase-based MT suffers from limited reorderings, and thus its training can only utilize a small portion of the bitext due to the distortion limit. To address this problem, we extend Yu et al. (2013) to syntax-based MT by generalizing their latent variable “violation-fixing” perceptron from graphs to hypergraphs. Experiments confirm that our method leads to up to +1.2 BLEU improvement over mainstream methods such as MERT and PRO. 1 Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: i</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Johnathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="10464" citStr="Dyer et al., 2010" startWordPosition="1874" endWordPosition="1877">, its −LM node is split into multiple +LM nodes, one for each such index.2 We have a similar deductive system for forced decoding. For the previous example, rule r5 in Figure 2 (a) is rewritten as X → hyˇu X 1 jˇux´ıng X 2 ,1 X 2 4 X 1 i, where 1 and 4 are the indexes for reference words “held” and “with” respectively. The deduction for X[1:5] in Figure 2 (b) is r5, X1?5 [1:5] : s(r5) + λ + s1 + s2 where λ = log II∈{1,3,4} Pforced(i + 1 |i) = 0. 4 Experiments Following Yu et al. (2013), we call our maxviolation method MAXFORCE. Our implementation is mostly in Python on top of the cdec system (Dyer et al., 2010) via the pycdec interface (Chahuneau et al., 2012). In addition, we use minibatch parallelization of (Zhao and Huang, 1We only consider single reference in this paper. 2Our formulation of index-based language model fixes a bug in the word-based LM of Yu et al. (2013) when a substring appears more than once in the reference (e.g. “the man...the man...”); thanks to Dan Gildea for pointing it out. Δ N∗ = argmax [i∗:j∗] N[i:j] w · ΔΦ(x, d+N[i:j](x, y), d−N[i:j](x, y)), Pforced(q |p) = X5?5 X2?3 [2:3] : s1 [4:5] : s2 787 2013) to speedup perceptron training. We evaluate MAXFORCE for HIERO over two </context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Flanigan</author>
<author>Chris Dyer</author>
<author>Jaime Carbonell</author>
</authors>
<title>Large-scale discriminative training for statistical machine translation using held-out line search.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="1688" citStr="Flanigan et al., 2013" startWordPosition="255" endWordPosition="258">nd PRO. 1 Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1 hypergraph 1 Zhang et al. (13) this work Figure 1: Relationship with previous work. small-scale MERT/PRO by a large margin for the first time). However, the underlying phrase-based model suffers from</context>
</contexts>
<marker>Flanigan, Dyer, Carbonell, 2013</marker>
<rawString>Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell. 2013. Large-scale discriminative training for statistical machine translation using held-out line search. In Proceedings of NAACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Sida Wang</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast and adaptive online training of feature-rich translation models. to appear) ACL.</title>
<date>2013</date>
<contexts>
<context position="1709" citStr="Green et al., 2013" startWordPosition="259" endWordPosition="262">Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1 hypergraph 1 Zhang et al. (13) this work Figure 1: Relationship with previous work. small-scale MERT/PRO by a large margin for the first time). However, the underlying phrase-based model suffers from limited distortion a</context>
</contexts>
<marker>Green, Wang, Cer, Manning, 2013</marker>
<rawString>Spence Green, Sida Wang, Daniel Cer, and Christopher D Manning. 2013. Fast and adaptive online training of feature-rich translation models. to appear) ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="11264" citStr="Hopkins and May, 2011" startWordPosition="2015" endWordPosition="2018">lation of index-based language model fixes a bug in the word-based LM of Yu et al. (2013) when a substring appears more than once in the reference (e.g. “the man...the man...”); thanks to Dan Gildea for pointing it out. Δ N∗ = argmax [i∗:j∗] N[i:j] w · ΔΦ(x, d+N[i:j](x, y), d−N[i:j](x, y)), Pforced(q |p) = X5?5 X2?3 [2:3] : s1 [4:5] : s2 787 2013) to speedup perceptron training. We evaluate MAXFORCE for HIERO over two CH-EN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al., 2007), Hypergraph MERT (Kumar et al., 2009), and PRO (Hopkins and May, 2011) from cdec. 4.1 Features Design We use all the 18 dense features from cdec, including language model, direct translation probability p(e|f), lexical translation probabilities pl(e|f) and pl(f|e), length penalty, counts for the source and target sides in the training corpus, and flags for the glue rules and pass-through rules. For sparse features we use Word-Edges features (Charniak and Johnson, 2005; Huang, 2008) which are shown to be extremely effective in both parsing and phrase-based MT (Yu et al., 2013). We find that even simple Word-Edges features boost the performance significantly, and </context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Fast decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Prague, Czech Rep.,</location>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Fast decoding with integrated language models. In Proceedings of ACL, Prague, Czech Rep., June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="2002" citStr="Huang et al., 2012" startWordPosition="306" endWordPosition="309">n has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1 hypergraph 1 Zhang et al. (13) this work Figure 1: Relationship with previous work. small-scale MERT/PRO by a large margin for the first time). However, the underlying phrase-based model suffers from limited distortion and thus can only employ a small portion (about 1/3 in their ChEn experiments) of the bitext in training. To better utilize the large training set, we propose to generalize from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (HIERO) (Chiang, 2</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL: HLT,</booktitle>
<location>Columbus, OH,</location>
<contexts>
<context position="11680" citStr="Huang, 2008" startWordPosition="2082" endWordPosition="2083">N corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al., 2007), Hypergraph MERT (Kumar et al., 2009), and PRO (Hopkins and May, 2011) from cdec. 4.1 Features Design We use all the 18 dense features from cdec, including language model, direct translation probability p(e|f), lexical translation probabilities pl(e|f) and pl(f|e), length penalty, counts for the source and target sides in the training corpus, and flags for the glue rules and pass-through rules. For sparse features we use Word-Edges features (Charniak and Johnson, 2005; Huang, 2008) which are shown to be extremely effective in both parsing and phrase-based MT (Yu et al., 2013). We find that even simple Word-Edges features boost the performance significantly, and adding complex Word-Edges features from Yu et al. (2013) brings limited improvement and slows down the decoding. So in the following experiments we only use Word-Edges features consisting of combinations of English and Chinese words, and Chinese characters, and do not use word clusters nor word types. For simplicity and efficiency reasons, we also exclude all non-local features. 4.2 Datasets and Preprocessing Our</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the ACL: HLT, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="11193" citStr="Koehn et al., 2007" startWordPosition="2003" endWordPosition="2006"> Huang, 1We only consider single reference in this paper. 2Our formulation of index-based language model fixes a bug in the word-based LM of Yu et al. (2013) when a substring appears more than once in the reference (e.g. “the man...the man...”); thanks to Dan Gildea for pointing it out. Δ N∗ = argmax [i∗:j∗] N[i:j] w · ΔΦ(x, d+N[i:j](x, y), d−N[i:j](x, y)), Pforced(q |p) = X5?5 X2?3 [2:3] : s1 [4:5] : s2 787 2013) to speedup perceptron training. We evaluate MAXFORCE for HIERO over two CH-EN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al., 2007), Hypergraph MERT (Kumar et al., 2009), and PRO (Hopkins and May, 2011) from cdec. 4.1 Features Design We use all the 18 dense features from cdec, including language model, direct translation probability p(e|f), lexical translation probabilities pl(e|f) and pl(f|e), length penalty, counts for the source and target sides in the training corpus, and flags for the glue rules and pass-through rules. For sparse features we use Word-Edges features (Charniak and Johnson, 2005; Huang, 2008) which are shown to be extremely effective in both parsing and phrase-based MT (Yu et al., 2013). We find that ev</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of ACL and AFNLP.</booktitle>
<contexts>
<context position="11231" citStr="Kumar et al., 2009" startWordPosition="2009" endWordPosition="2012">ence in this paper. 2Our formulation of index-based language model fixes a bug in the word-based LM of Yu et al. (2013) when a substring appears more than once in the reference (e.g. “the man...the man...”); thanks to Dan Gildea for pointing it out. Δ N∗ = argmax [i∗:j∗] N[i:j] w · ΔΦ(x, d+N[i:j](x, y), d−N[i:j](x, y)), Pforced(q |p) = X5?5 X2?3 [2:3] : s1 [4:5] : s2 787 2013) to speedup perceptron training. We evaluate MAXFORCE for HIERO over two CH-EN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al., 2007), Hypergraph MERT (Kumar et al., 2009), and PRO (Hopkins and May, 2011) from cdec. 4.1 Features Design We use all the 18 dense features from cdec, including language model, direct translation probability p(e|f), lexical translation probabilities pl(e|f) and pl(f|e), length penalty, counts for the source and target sides in the training corpus, and flags for the glue rules and pass-through rules. For sparse features we use Word-Edges features (Charniak and Johnson, 2005; Huang, 2008) which are shown to be extremely effective in both parsing and phrase-based MT (Yu et al., 2013). We find that even simple Word-Edges features boost th</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proceedings of the Joint Conference of ACL and AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
<author>Liang Huang</author>
</authors>
<title>Joint event extraction via structured prediction with global features.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1253" citStr="Li et al., 2013" startWordPosition="187" endWordPosition="190">er, phrase-based MT suffers from limited reorderings, and thus its training can only utilize a small portion of the bitext due to the distortion limit. To address this problem, we extend Yu et al. (2013) to syntax-based MT by generalizing their latent variable “violation-fixing” perceptron from graphs to hypergraphs. Experiments confirm that our method leads to up to +1.2 BLEU improvement over mainstream methods such as MERT and PRO. 1 Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that as</context>
</contexts>
<marker>Li, Ji, Huang, 2013</marker>
<rawString>Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<location>Sydney, Australia,</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proceedings of COLING-ACL, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context position="1213" citStr="McDonald et al., 2005" startWordPosition="179" endWordPosition="182">ds that only train on small tuning sets. However, phrase-based MT suffers from limited reorderings, and thus its training can only utilize a small portion of the bitext due to the distortion limit. To address this problem, we extend Yu et al. (2013) to syntax-based MT by generalizing their latent variable “violation-fixing” perceptron from graphs to hypergraphs. Experiments confirm that our method leads to up to +1.2 BLEU improvement over mainstream methods such as MERT and PRO. 1 Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitab</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="11161" citStr="Och, 2003" startWordPosition="1999" endWordPosition="2000">lelization of (Zhao and Huang, 1We only consider single reference in this paper. 2Our formulation of index-based language model fixes a bug in the word-based LM of Yu et al. (2013) when a substring appears more than once in the reference (e.g. “the man...the man...”); thanks to Dan Gildea for pointing it out. Δ N∗ = argmax [i∗:j∗] N[i:j] w · ΔΦ(x, d+N[i:j](x, y), d−N[i:j](x, y)), Pforced(q |p) = X5?5 X2?3 [2:3] : s1 [4:5] : s2 787 2013) to speedup perceptron training. We evaluate MAXFORCE for HIERO over two CH-EN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al., 2007), Hypergraph MERT (Kumar et al., 2009), and PRO (Hopkins and May, 2011) from cdec. 4.1 Features Design We use all the 18 dense features from cdec, including language model, direct translation probability p(e|f), lexical translation probabilities pl(e|f) and pl(f|e), length penalty, counts for the source and target sides in the training corpus, and flags for the glue rules and pass-through rules. For sparse features we use Word-Edges features (Charniak and Johnson, 2005; Huang, 2008) which are shown to be extremely effective in both parsing and phrase-based MT (Y</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Joseph Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1600" citStr="Watanabe et al., 2007" startWordPosition="239" endWordPosition="242">t our method leads to up to +1.2 BLEU improvement over mainstream methods such as MERT and PRO. 1 Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1 hypergraph 1 Zhang et al. (13) this work Figure 1: Relationship with previous work. small-scale MERT/PRO by a l</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Yu</author>
<author>Liang Huang</author>
<author>Haitao Mi</author>
<author>Kai Zhao</author>
</authors>
<title>Max-violation perceptron and forced decoding for scalable MT training.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="840" citStr="Yu et al. (2013)" startWordPosition="124" endWordPosition="127">}.cuny.edu {hmi,abei}@us.ibm.com Abstract Collins (02) inexact Huang et al. (12) latent Yu et al. (13) �� �� search variable Large-scale discriminative training has become promising for statistical machine translation by leveraging the huge training corpus; for example the recent effort in phrase-based MT (Yu et al., 2013) significantly outperforms mainstream methods that only train on small tuning sets. However, phrase-based MT suffers from limited reorderings, and thus its training can only utilize a small portion of the bitext due to the distortion limit. To address this problem, we extend Yu et al. (2013) to syntax-based MT by generalizing their latent variable “violation-fixing” perceptron from graphs to hypergraphs. Experiments confirm that our method leads to up to +1.2 BLEU improvement over mainstream methods such as MERT and PRO. 1 Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tu</context>
<context position="2797" citStr="Yu et al. (2013)" startWordPosition="428" endWordPosition="431">ale MERT/PRO by a large margin for the first time). However, the underlying phrase-based model suffers from limited distortion and thus can only employ a small portion (about 1/3 in their ChEn experiments) of the bitext in training. To better utilize the large training set, we propose to generalize from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (HIERO) (Chiang, 2005), in order to exploit sentence pairs beyond the expressive capacity of phrase-based MT. The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: 1. We generalize the latent variable violationfixing perceptron framework to inexact search over hypergraphs, which subsumes previous algorithms for PBMT and bottomup parsing as special cases (see Fig. 1). 2. We show that s</context>
<context position="9402" citStr="Yu et al. (2013)" startWordPosition="1659" endWordPosition="1662">j] group in the pruned +LM hypergraph: d++, .: (x, y)Δ=argmax w · Φ (x, d), [`j ] d∈goodN[i:j](x,y) d−N .. (x, y) = argmax w · Φ(x, d), [` j] d∈badN[i:j](x,y) where Φ(x, d) is the feature vector for derivation d. Then it finds the group N∗[i∗:j∗] with the maximal score difference between the Viterbi derivation and the best y-good derivation: and update as follows: w ← w + ΔΦ(x, d+ (x, y), d− (x, y)), N∗ N∗ [i∗:j∗][i∗:j∗] where ΔΦ(x, d, d0) Δ= Φ(x, d) − Φ(x, d0). 3.2 Forced Decoding for HIERO We now describe how to find the gold derivations.1 Such derivations can be generated in way similar to Yu et al. (2013) by using a language model tailored for forced decoding: � 1 ifq=p+1 0 otherwise , where p and q are the indices of the boundary words in the reference translation. The +LM node now has signature Np?q [i:j], where p and q are the indexes of the boundary words. If a boundary word does not occur in the reference, its index is set to ∞ so that its language model score will always be −∞; if a boundary word occurs more than once in the reference, its −LM node is split into multiple +LM nodes, one for each such index.2 We have a similar deductive system for forced decoding. For the previous example,</context>
<context position="10731" citStr="Yu et al. (2013)" startWordPosition="1919" endWordPosition="1922">for reference words “held” and “with” respectively. The deduction for X[1:5] in Figure 2 (b) is r5, X1?5 [1:5] : s(r5) + λ + s1 + s2 where λ = log II∈{1,3,4} Pforced(i + 1 |i) = 0. 4 Experiments Following Yu et al. (2013), we call our maxviolation method MAXFORCE. Our implementation is mostly in Python on top of the cdec system (Dyer et al., 2010) via the pycdec interface (Chahuneau et al., 2012). In addition, we use minibatch parallelization of (Zhao and Huang, 1We only consider single reference in this paper. 2Our formulation of index-based language model fixes a bug in the word-based LM of Yu et al. (2013) when a substring appears more than once in the reference (e.g. “the man...the man...”); thanks to Dan Gildea for pointing it out. Δ N∗ = argmax [i∗:j∗] N[i:j] w · ΔΦ(x, d+N[i:j](x, y), d−N[i:j](x, y)), Pforced(q |p) = X5?5 X2?3 [2:3] : s1 [4:5] : s2 787 2013) to speedup perceptron training. We evaluate MAXFORCE for HIERO over two CH-EN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al., 2007), Hypergraph MERT (Kumar et al., 2009), and PRO (Hopkins and May, 2011) from cdec. 4.1 Features Design We use all the 18 dense features fr</context>
<context position="13393" citStr="Yu et al. (2013)" startWordPosition="2358" endWordPosition="2361"> tuning methods. NIST08 newswire is used as test set. Both NIST06 newswire and NIST08 newswire contain 4 references. We mainly use this corpus to demonstrate the performance of MAXFORCE in large-scale training. For both corpora, we do standard tokenization, alignment and rule extraction using the cdec tools. In rule extraction, we remove all 1-count rules but keep the rules mapping from one Chinese word to one English word to help balancing sent. words phrase-based MT 32% 12% HIERO 35% 30% HIERO (all rules) 65% 55% Table 1: Reachability comparison (on FBIS) between phrase-based MT reported in Yu et al. (2013) (without 1-count rules) and HIERO (with and without 1-count rules). Figure 4: Reachability vs. sent. length on FBIS. See text below for “loose” and “tight”. between overfitting and coverage. We use a trigram language model trained from the target sides of the two corpora respectively. 4.3 Forced Decoding Reachability We first report the forced decoding reachability for HIERO on FBIS in Table 1. With the full rule set, 65% sentences and 55% words of the whole corpus are forced decodable in HIERO. After pruning 1-count rules, our forced decoding covers significantly more words than phrase-based</context>
</contexts>
<marker>Yu, Huang, Mi, Zhao, 2013</marker>
<rawString>Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao. 2013. Max-violation perceptron and forced decoding for scalable MT training. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Kai Zhao</author>
<author>Ryan McDonald</author>
</authors>
<title>Online learning with inexact hypergraph search.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2893" citStr="Zhang et al. (2013)" startWordPosition="440" endWordPosition="443">l suffers from limited distortion and thus can only employ a small portion (about 1/3 in their ChEn experiments) of the bitext in training. To better utilize the large training set, we propose to generalize from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (HIERO) (Chiang, 2005), in order to exploit sentence pairs beyond the expressive capacity of phrase-based MT. The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: 1. We generalize the latent variable violationfixing perceptron framework to inexact search over hypergraphs, which subsumes previous algorithms for PBMT and bottomup parsing as special cases (see Fig. 1). 2. We show that syntax-based MT, with its better handling of long-distance reordering, can exploit a larger porti</context>
<context position="15222" citStr="Zhang et al. (2013)" startWordPosition="2667" endWordPosition="2670">hods in Figure 5. The max-violation method is more than 15 BLEU 20 40 60 80 100 sentence length forced decodable ratio 0.8 0.6 0.4 0.2 0 1 loose tight 788 2 4 6 8 10 12 14 16 18 20 iteration Figure 6: Sparse features (Word-Edges) contribute —2 BLEU points, outperforming PRO and MERT. points better than the standard perceptron (also known as “bold-update” in Liang et al. (2006)) which updates at the root of the derivation tree.3,4 This can be explained by the fact that in training —58% of the standard updates are invalid (i.e., they do not fix any violation). We also use the “skip” strategy of Zhang et al. (2013) which updates at the root of the derivation only when it fixes a search error, avoiding all invalid updates. This achieves —10 BLEU better than the standard update, but is still more than —5 BLEU worse than Max-Violation update. Finally we also try the “local-update” method from Liang et al. (2006) which updates towards the derivation with the best Bleu+1 in the root group S[0:|x|]. This method is about 2 BLEU points worse than max-violation. We further investigate the contribution of sparse features in Figure 6. On the development set, max-violation update without Word-Edges features achieve</context>
</contexts>
<marker>Zhang, Huang, Zhao, McDonald, 2013</marker>
<rawString>Hao Zhang, Liang Huang, Kai Zhao, and Ryan McDonald. 2013. Online learning with inexact hypergraph search. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Zhao</author>
<author>Liang Huang</author>
</authors>
<title>Minibatch and parallelization for online large margin structured learning.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL</booktitle>
<marker>Zhao, Huang, 2013</marker>
<rawString>Kai Zhao and Liang Huang. 2013. Minibatch and parallelization for online large margin structured learning. In Proceedings of NAACL 2013.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>