<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.9755035">
Dimension-Reduced Estimation of Word Co-occurrence
Probability
</title>
<author confidence="0.997974">
Kilyoun Kim and Key-Sun Choi
</author>
<affiliation confidence="0.998461">
Computer Science Dept., AITrc, KORTERM
Korea Advanced Institute of Science &amp; Technology (KAIST)
</affiliation>
<address confidence="0.700545">
Kusong-Dong, Yusong-Gu Taejon, 305-701 Republic of Korea
</address>
<email confidence="0.717322">
gykim©world.kaist.ac.kr and kschoi©world.kaist.ac.kr
</email>
<sectionHeader confidence="0.945341" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999422666666666">
We investigate a novel approach
to solve the problem of sparse
data through dimension reduction.
Linear algebraic technique called
LSA/SVD is used to find co-
relationships of sparse words. Three
variant estimation methods are sug-
gested and they are evaluated for
estimating unseen noun-verb co-
occurrence probability. The model
shows possibility to be alternative
probability smoothing method.
</bodyText>
<sectionHeader confidence="0.996318" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965711864407">
One of the most suffering difficulties in sta-
tistical language processing is so-called data
sparseness problem. No matter how large
the training set is, a substantial portion of
the data is unseen. For them, the Maxi-
mum Likelihood Estimation (MLE) probabil-
ities are zero and these zeros give us bad result
all through the statistical process.
We are interested in P(x,y) and the predic-
tion task P(ylx), that is a bigram language
modeling of word co-occurrences. P(ylx) is
the conditional probability that a pair has sec-
ond element y E Y given that its first element
is x E X. In other words, P (Y I x) can be re-
garded as a measure of relationship between
word x and y. For example, for a given ob-
ject x = beer, a verb y = drink is more re-
lated than a verb y = eat, p(drinklbeer) &gt;&gt;
p(eatlbeer). Many features can be used to
predict a relationship between two words, but
we assume here that the only information we
have are the frequencies.
To overcome the difficulty of sparse data,
a smoothing technique like Good-Turing
method is widely used. Estimator com-
bining approaches such as linear interpola-
tion and Katz&apos;s back-off method are popular
also(Katz, 1987). They use unigram prob-
ability P(y) to estimate bigram probability
P (Y I x) for unseen data pair, disregarding the
relationship between two words. If unseen bi-
grams are made up of unigrams of the same
frequency, the methods give them the same
probability, causing a problem to estimate ac-
curate probability.
In addition to the classical methods,
similarity-based schemes are successfully ap-
plied to data sparseness problem. The
nearest-neighbors similarity-based method
uses a set of k most similar words x&apos; to es-
timate conditional probability P(ylx), being
said to perform almost 40% better than back-
off (Dagan et al., 1999). They use various
distributional similarity measures to find sim-
ilarity between words such as KL-divergence
or JS-divergence (Lee, 1999). For the sparse
word, however, the distribution P(ylx) itself
is sparse and it is difficult to find correct sim-
ilarity between words, since the only means
for measuring word similarity is the frequency.
The more sparse the distribution of word is,
the more difficult finding acceptable similari-
ties between words.
In this paper, we investigate a novel ap-
proach to solve the problem of sparse data
by capturing their latent relationships with
only frequency information. Through reduc-
ing dimension by linear algebraic technique
LSA/SVD&apos;, we can eliminate zero values in
</bodyText>
<page confidence="0.258879">
1-LSA - Latent Semantic Analysis, SVD - Singular
</page>
<bodyText confidence="0.999558590909091">
p(y Ix) as well as we can capture relation-
ships between words. We believe that the
dimension-reduced estimation model can be
alternative probability smoothing method.
The model consists of three parts: mak-
ing a conditional probability matrix, pro-
jecting the matrix into lower space, and
estimating probabilities on reduced space.
In the third part, three variant estimat-
ing methods are suggested and they are
compared with Katz&apos;s back-off method and
simplified nearest-neighbor similarity-based
method. We evaluated the methods in a
pseudo wrod sense disambiguation task. and
made promising result. Futher evaluation is
needed on more realistic task, though.
The optimal dimension size of subspace is
also investigated, showing the best result be-
tween 90 — 200, about 10% of the original di-
mension size. Finally, we show that the model
does not degrade performance as the sparse-
ness increases.
</bodyText>
<sectionHeader confidence="0.995034" genericHeader="method">
2 Dimension-Reduced Model
</sectionHeader>
<bodyText confidence="0.998088923076923">
Dimension-reduced model uses linear al-
gebraic technique called LSA/SVD, which
projects a matrix into reduced space.
First of all, to apply linear algebraic tech-
nique, we need to represent conditional prob-
ability P(y Ix) as a matrix form (Section 2.1).
After that, we project the matrix into lower
dimension subspace through SVD. We will
show how the resultant space represents re-
lationship between the given word x and the
predicting word y well (Section 2.2). At last,
we suggest three probability estimation meth-
ods on reduced space (Section 2.3).
</bodyText>
<subsectionHeader confidence="0.974906">
2.1 Conditional Probability Matrix
</subsectionHeader>
<bodyText confidence="0.999957125">
Any discrete conditional probability distribu-
tions can be represented by a matrix form.
For a distribution p(y Ix), given words x E X
make up row entries and predicting words
y E y make up column entries. Each element
of matrix has estimated conditional probabil-
ity value of two words p(y Ix). We define con-
ditional probability matrix and the row, col-
</bodyText>
<equation confidence="0.919334">
Value Decomposition
umn vectors:
Amxn = [ajj] = [P(Oxi)]
4 = [P(Yilxi), • • • ,P(Y.Ixi)]
= [P(Yilxi), • • • ,P(Y.ilxm)]
where m = IXI, n = IYI and 1 &lt; i &lt; m,
1 &lt; j &lt; n. For example, if we use MLE
frq(xi,y3)
estimator, au = PMLE(YjIXO = frq(xj)
</equation>
<bodyText confidence="0.999768818181818">
In the table, the noun &amp;quot;coffee&amp;quot; and &amp;quot;beer&amp;quot;
does not co-occur with the same verb and it
is difficult to find similarity between them in
this space. To find their latent relationship,
we can project each row and column vector
into lower dimension space through latent se-
mantic anaylsis.
Table 1 shows an example of MLE esti-
mating matrix. The task is predicting the
main verb with given object, that is estimat-
ing noun and verb co-occurrence probability
</bodyText>
<equation confidence="0.943809">
P(nIn)
</equation>
<bodyText confidence="0.742335666666667">
noun can be regarded as a point or a vector in
multi-dimensional space of which a dimension
size equal to I VI
</bodyText>
<tableCaption confidence="0.9662885">
Table 1: An Example of Conditional Proba-
bility Matrix
</tableCaption>
<bodyText confidence="0.8336675">
NIV swigsipdrinkdevour eat swallow\
beer 0.33 0 0.33 0.33 0 0
whiskey 0 0.5 0.5 0 0 0
coffee 0 1 0 0 0 0
bread 0 0 0 0.33 0.33 0.33
\sugar 0 0 0 0 0.5 0.5 /
</bodyText>
<subsectionHeader confidence="0.901765">
2.2 Projection - Latent Semantic
Analysis
</subsectionHeader>
<bodyText confidence="0.997147272727273">
Latent Semantic Analysis (LSA) is known
as a theory for extracting and representing
the contextual-usage meaning of words. LSA
uses singular value decomposition (SVD).
It has been widely used in information re-
trieval task as a variant of the vector space
model(Deerwester et al., 1990)(Dumais et al.,
1997).
Given the conditional probability matrix A
and rank(A) = r, the SVD of A and the rank-
where n E N,v E V. Note that each
</bodyText>
<equation confidence="0.97041325">
k approximation matrix Ak is defined as
A = UEVT =E • • vT
Ak - UkEkVkT - E • • vT
i=1
</equation>
<bodyText confidence="0.9998471">
where U and V contains left and right singu-
lar vectors of A, respectively, and the E =
diag(o-i, • • • , o-n) is the diagonal matrix of
singular values of A. Truncated SVD Ak,
which is constructed from the k-largest signu-
lar triples of A, is the closest rank-k matrix to
A 2. The left singular vector Tr, and the right
singular vector 14 corresponds to the row vec-
tor 4 and the column vector g, respectively.
By taking k elements of and 14, each given
word x and predicting wordy of P(y Ix) is rep-
resented as a vector in the reduced k-space.
Figure 1 is an example of SVD on the noun-
verb conditional probability matrix of Table
1. In Figure 1-D, both noun x and verb y are
represented by a vector in two dimensional
space. Nouns which occur with similar verbs
are grouped each other even if they never co-
occur with the same verb (diml: beverages,
dim2: foods). For example, noun &amp;quot;coffee&amp;quot; and
verb &amp;quot;beer&amp;quot; do not co-occur with the same
verb in the original matrix (Table 1); however
they are near in two dimensional space when
measured with a cosine distance. This means
that unseen word pairs (x, y) which do not co-
occur in the training data may none the less
be near in reduced k-space. This derived rep-
resentation which captures word(x)-word(y)
associations is used for estimating probabili-
ties of unseen data.
</bodyText>
<subsectionHeader confidence="0.999271">
2.3 Estimating Probabilities on
Reduced Space
</subsectionHeader>
<bodyText confidence="0.9967912">
Until now, we constructed word co-occurrence
probability matrix and projected the matrix
into lower dimension space. Now, we suggest
21n other words, the projection into the reduced
space is chosen such that the representations in the
original space are changed as little as possible when
measured by the sum of the squares of the differences.
One can prove that Ak is the best approximation to A
for any unitray invariant norm(Michael W. Berry and
Jessup, 1999)
three variant probability estimation methods
in dimension-reduced space. First is esti-
mating p(y Ix) by computing distance between
given word x and predicting word y in reduced
space. Second, we can use rank-k approx-
imation matrix. Third, the state-of-the-art
similarity-based methods can be merged to
our dimension-reduced model. Because the
first two methods are not based on statistical
theory, it should be explored
</bodyText>
<subsectionHeader confidence="0.488318">
2.3.1 Method 1: Distance-based
method
</subsectionHeader>
<bodyText confidence="0.999952111111111">
Through LSA, the matrix A is factored into
the product of three matrices as in Equa-
tion 3, and Tr, and v-3 are considered as the
row vector 4 and the column vector y-3 in k-
dimension subspace respectively. Figure 1-D
shows 2-dimensional plot of resultant U2, 172
matrix. The distance-based method use nor-
malized distance between Tr, and v-3 for esti-
mating probability P(Y3lx%):
</bodyText>
<equation confidence="0.978237333333333">
1 D k(, 2) ±
P(Y3lx%) = Zk 2
u, (t)VT (t)
Dketi,„ v 3) = , (5)
t=i N(t)2 \ Etk-1 v 3 (t) 2
\/Ek
</equation>
<bodyText confidence="0.9974115">
where Zk is normalizing factor and Dk is a
cosine distnace in k-dimensional space.
</bodyText>
<subsectionHeader confidence="0.927221">
2.3.2 Method 2: Rank-k
</subsectionHeader>
<bodyText confidence="0.9439406">
approximation matrix method
In LSA, we can create a rank-k approxi-
mation matrix Ak to the matrix A by set-
ting all but the k largest singular values of A
equal to zero (Equation 4). In this method,
we consider each element of a rank-k approx-
imation matrix Ak as probability distribution
of P(Ylx) (Figure 1-C). To satisfy the require-
ments Ep (ylx) = 1 and P(Ylx) &gt; 0, we use the
following normalizing equation:
</bodyText>
<equation confidence="0.99038675">
1
P(Yilxi) = [Ak(i,i) - minvAk(i,i)
Zk
Z (x) = E [Ak (i, - minvAk (i, j) Si (6)
</equation>
<bodyText confidence="0.97438">
where Z(n) is normalizing factor and is a
smoothing constant.
</bodyText>
<table confidence="0.995982627906977">
A. Conditional Probability Estimation Matrix by naive frequency
swig sip drink devour eat swallow \
0.3333 0 0.3333 0.3333 0 0
0 0.5000 0.5000 0 0 0
0 1.0000 0 0 0 0
0 0 0 0.3333 0.3333 0.3333
0 0 0 0 0.5000 0.5000 /
A = UEVT =
Mina dim2 dim3 dim4 dim5\ T
/1.14 0 0 0 0\ 0.02 0.06-0.42 0.31-0.84
0 0.87 0 0 0 0.96-0.03 0.21 0.13-0.02
0 0 0.64 0 0 0.25 0.07-0.71-0.62
0.13
0 0 0 0.35 0 0.03 0.29-0.45 0.67 0.49
\0 0 0 0 0.16/ 0.00 0.67 0.16 -0.12 -0.07
\ 0.00 0.67 0.16-0.12-0.07/
/
A = [P(vIn)] = beer
whiskey
coffee
bread
sugar
\
B. Singular Vector Decomposition
Mimi_ dim2 dim3 dim4 dim5\
0.09 0.16 -0.81 0.33-0.42
0.53 0.02 -0.38-0.67 0.32
0.84-0.04 0.33 0.38-0.16
0.01 0.62 -0.06 0.39 0.67
\ 0.00 0.76 0.25 -0.35 -0.47/
C. Rank-2 Approximation Matrix
swig
0.0119
0.0174
0.0233
0.0347
0.0422
sip drink devour eat swallow \
0.0959 0.0380 0.0466 0.0981 0.0981
0.5899 0.1598 0.0237 0.0153 0.0153
0.9328 0.2470 0.0175 -0.0206 -0.0206
-0.0079 0.0442 0.1639 0.3668 0.3668
-0.0206 0.0513 0.2007 0.4499 0.4499 /
</table>
<figure confidence="0.98088764">
/
beer
whiskey
coffee
bread
sugar
A2 - U2E2V2T
\
D. Two-dimensional plot of SVD Result
0.84ugar
ood-related words
vat/swa
bread
1
Nouns A
Verbs *
low
Dim2
*devoy
0.6
0.
0.2
1
0.2 0.4 0.6 0.8
Diml
</figure>
<figureCaption confidence="0.99958">
Figure 1: An Example of Singular Value Decomposition
</figureCaption>
<page confidence="0.940827">
-0.2
0
</page>
<tableCaption confidence="0.986292">
Table 2: An Illustrative Example
</tableCaption>
<table confidence="0.9914244">
MLE Katzs Similarity Dimension Reduced Model
back-off -based
method
Distance k-Rank DR-SIM
-based matrix
</table>
<equation confidence="0.843247777777778">
(k = 3) (dim = 2) (6 = 0.1) (0 = 0)
P(swig I coffee) 0 0.09 0 0.1627 0.0756 0.11
P(sip I coffee) 1 1 1 0.2425 0.5536 1
P(drink I coffee) 0 0.18 0.17 0.2359 0.1932 0.28
P(devour I coffee) 0 0.18 0.11 0.1271 0.0726 0.11
P(eat I coffee) 0 0.18 0.11 0.1159 0.0526 0
P(swallow I coffee) 0 0.18 0.11 0.1159 0.0526 0
2.3.3 Method 3: Dimension-reduced
similarity-based method
</equation>
<bodyText confidence="0.999859888888889">
The similarity-based method(Dagan et al.,
1999) and dimension reduction technique can
be merged into one model 3 . Reduced dimen-
sion can be better representation space than
the original space for finding similarities be-
tween words.
This approach finds the most k nearest
words to x in reduced space and use these
word to estimate the probability p(ylx):
</bodyText>
<equation confidence="0.934840666666667">
Pnve(Y -Ix-)
P(Yilxi)= ii
1,1Exiics-Pnile(Yilxii)
</equation>
<bodyText confidence="0.946350285714286">
where S = {4cos(4,4&apos;) &gt; 0 on k dim.},
the k is the reduced dimension size not a
count of nearest nouns. The count of nearest
nouns are determined by 0, which is threshold
of cosine value.
31n the previous similarity-based work, (Dagan et
al., 1999) used the complicated estimating equation:
</bodyText>
<equation confidence="0.99537">
x,e,s(x,k)
W(x,x1) = 10-3is(xlIx&apos;)
</equation>
<bodyText confidence="0.67084475">
where W(x,x&apos;) is a similarity measure derived from
the dissimilarity measure JS divergence. S(n, k) is
the set of k words with the smallest JS-divergence to
x. Here, however, we use more simplifed equation:
</bodyText>
<equation confidence="0.8960535">
P(yixi)=1 E
x&apos;iES(xi ,k)
</equation>
<sectionHeader confidence="0.940087" genericHeader="method">
3 An Illustrative Example
</sectionHeader>
<bodyText confidence="0.987145451612903">
Here we use a concrete example to illustrate
effectiveness of our model. The exam-
ple is based on Table 1 and the task is
estimating noun and verb co-occurrence
probability p(vIn). There are two groups
of words, beverage-related words ([beer,
whieskey, coffeel/N, [swig, sip, drinkj/V)
and food-related words (tbread,sugarl/N,
[devour,eat,swallow]/V).
In Table 1, &amp;quot;coffee&amp;quot; is a sparsely distributed
noun and we expect P(drinklcof fee) &gt;
P(eatIcof fee). With MLE, it is not pos-
sible to rank two probabilities since they
are all 0. Katz&apos;s back-off also fails to
distinguish them since unigram probabil-
ities p(drink) = p(eat) = 0.18. In
the similarity-based scheme we compute
JS-divergence to find similarity between
nouns and JS(p(vlbeer),p(vIcof fee)) 4 =
JS(p(vlbread),p(vIcof fee)) = 0.6931, which
does not discriminate &amp;quot;beer&amp;quot; and &amp;quot;bread&amp;quot;.
The distance-based model, however, solves
all these problem. When we observe the third
row in Figure 1-C, that is pre-normalized
p(vIcof fee), there are no zero values un-
like MLE. Futhermore, we can end up
with two groups of verbs: beverage-related
verbs have positive values A2 (sip, coffee),
A2 (drink, co f ee), A2 (swig, coffee) &gt; 0
and food-related verbs have negative values
A2 (eat, co f f ee), A2 (swallow, co f ee) &lt;0.
</bodyText>
<equation confidence="0.9823636">
4 J S(p, q) = ±2q) D (q11(P±2q) )1 ,
D(pllq) = Et p(t)log
Nyiixi)= E w(x,x,)
P(yixi),
Z(x)
</equation>
<bodyText confidence="0.999989769230769">
To concrete our example, p(vicof fee) is
constructed in Table 2 using probability es-
timation functions as described in the above
section. MLE shows five zero values, caus-
ing data sparseness problems. Katz&apos; back-
off methods and similarity-based method can-
not distinguish food-related verbs and drink-
related verbs. In contrast, all dimension-
reduced models resolve data sparseness prob-
lem and they cluster nouns and co-occurrence
verbs reasonably. Thus, we can expect that
dimension-reduced model will show promising
result in a real experiment.
</bodyText>
<sectionHeader confidence="0.996547" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.982693821428571">
We evaluated the dimension-reduced models
on a pseudo word sense disambiguation task
as in (Dagan et al., 1999). Each method is
presented with a noun and two verbs, deciding
which verb is more likely to have the noun as a
direct object. Data preparation method and
error counting scheme are almost similar to
that of similarity-based methods (Dagan et
al., 1999) (Lee and Pereira, 1999).
Performance is measured by the error rate,
defined as
error rate = T-1(0 of incorrect choices)
where T is the size of test set. Test instances
consist of noun-verb-verb triples (n, vl, v2),
where both (n, v1) and (n, v2) are both un-
seen in the training set. (n, v1) is selected
such that it appeared at least twice as of-
ten than (n, v2) in the original verb-object
pairs and p(n, v1) &gt; p(n, v2) is a correct an-
swer. In addition, to consider Katzs back-
off method as the baseline, v2 is choosed as
frq(v1) &lt; frq(v2) &lt;=&gt; p(v1) &lt; p(v2), and
the error rate of back-off method is always
100% 5 . Running method is three-fold cross-
validation and all results are averages over the
three test sets.
5Katz&apos;s back-off estimator is defiend as the follow-
ing eqaution. We set c(x) = 1 here.
</bodyText>
<equation confidence="0.9973535">
Pd(Yilxi) frq(xi, yi) &gt; 0
cx(xi) P(yi) frq(xi,yi) = 0
</equation>
<bodyText confidence="0.999862333333333">
For similarity-based method, the parameter
tuning is important to improve performance
but we use the simplified unweighted average
equation as in (Lee and Pereira, 1999) 6 .
Since this equation is the same as our esti-
mation method in Section 2.3.3, we can say
that the comparison is fair. Number of sim-
ilar nouns k is determined such that shows
best result on test set.
</bodyText>
<subsectionHeader confidence="0.998143">
4.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.996541">
We prepared test sets as follows:
</bodyText>
<listItem confidence="0.987037818181818">
1. Extract transitive verb and head noun
pairs from Penn Treebank
2. Select the pairs for the 1,000 most fre-
quent nouns.
3. Partition the selected pairs 70% for train-
ing set and 30% for test set. (3 fold).
4. For each test set,
(a) remove seen pairs.
(b) for each (n, v1), create (n, vl, v2)
such that f rq(n, v1) &gt; 2* frq(n, v2)
and frq(v1) &lt; frq(v2).
</listItem>
<bodyText confidence="0.976982166666667">
Step 2 makes p(v In) matrix size fixed. Since
it is difficult to find (n, vl, v2) triples that sat-
isfy Step 4-(b) criteria, average test set size is
small. Hence, we used relative large portion,
30% of the pairs for building test set. Table
3 summarizes the experiment data.
</bodyText>
<tableCaption confidence="0.999267">
Table 3: Training and Test Data.
</tableCaption>
<table confidence="0.9888912">
1.Target Corpus Penn Treebank II
3.Verb-object pairs 18843 pairs
3. INI x IVI matrix size 1000 x 2008
4.Training set size 13040 pairs
5.Test set size 713 triples
</table>
<subsectionHeader confidence="0.957508">
4.2 Result
</subsectionHeader>
<bodyText confidence="0.999606">
Table 4 shows the experimental error rate on
the three test sets, using Katz&apos;s back-off as
the baseline. Two dimension-reduced meth-
ods show much better performance than other
methods.
</bodyText>
<equation confidence="0.807929">
6/5(vin) = E,,,,Es(n,k)P(vIn&apos;)
Pbo j I
</equation>
<tableCaption confidence="0.997923">
Table 4: Experimental Result (Error Rate)
</tableCaption>
<table confidence="0.999216555555555">
Katzs Similarity Dimension Reduced Model
back-off -based
(baseline) method
Distance k-Rank DR-SIM
-based matrix
(k = 20) (dim = 90) (6 = 0.1) (0 = 0.5)
Fold 1 1.0 0.623 0.362 0.386 0.586
Fold 2 1.0 0.636 0.374 0.423 0.594
Fold 3 1.0 0.645 0.366 0.402 0.593
</table>
<bodyText confidence="0.999574071428571">
The reason of such a good performance is
that our model tries to find similarities be-
tween words toward two side (column and
row space). The state-of-the-art similarity-
based methods find similarities toward only
one side. For example, their well-known sim-
ilarity measures JS-divergence between given
word x, and another word x is defined as
JS(P(Ylx%),P(Yix)). It means that each row
P(Ylx%) is compared to another row p(y 14 in
Table 1. Comparisons on column side are not
performed. That&apos;s why the similarity-based
fail to grasp true relationships on word co-
occurrences.
On the other hand, SVD which is the math-
ematical background of our model gives us
a reduced-rank basis for both column space
and the row space simultaneously (Figure
1) (Michael W. Berry and Jessup, 1999).
As we showed in the previous example in
Section 3, the dimension-reduced model ex-
tract underlying or latent structures of word
co-occurrences well. Therefore, our model
shows successful result on estimating word co-
occurrence probabilities of sparse data. futher
However, the experiment is artificial and SVD
is not directly related to the probabilty the-
ory, futher theoretical invetigation is required.
</bodyText>
<subsectionHeader confidence="0.9779795">
4.3 Optimal subspace and Degree of
sparseness
</subsectionHeader>
<bodyText confidence="0.999970857142857">
We also investigated the change of the perfor-
mances as subspace size and degree of sparse-
ness vary. Figure 2 shows performances of
distance-based DR model as the dimension of
subspace increase. When a dimension size is
between 90 and 200, it shows the best result.
Thus, we can conclude that the subspace of
</bodyText>
<figureCaption confidence="0.998131">
Figure 2: Performance vs. Dimension size
</figureCaption>
<bodyText confidence="0.99978875">
small dimension size (90 &lt; 1000) is sufficent
to capture latent word co-occurrence relation-
ship.
Figure 3 shows the effect of the degree of
sparseness. The 1st ranked noun appears
the most frequent times and 1000th ranked
noun appears the least frequent times, in
the training set. The average error rate
does not change much as the sparseness in-
creases. Therefore it is plausible to say that
the dimension-reduced model does not show
performance degration on very sparse data.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998723857142857">
We proposed a novel approach called
dimension-reduced estimation model for deal-
ing with data sparseness problem. Three
variant models are suggested and they are
compared the performance against Katz&apos;s
back-off method and similarity-based scheme.
Dimension-reduced model can be alternative
</bodyText>
<figure confidence="0.99745025">
co
0 0 0 0 0 0 0 0 0 0
CD CO 0) N LO CO C71 &apos;C&apos;V&apos; \
0.3
Dimension Size
0.6
0.55
— 0.45
0.5
U)
CC
it!
0.4
0.35
I
0
</figure>
<figureCaption confidence="0.9923205">
Figure 3: Performance vs. Degree of sparse-
ness
</figureCaption>
<bodyText confidence="0.996059275862069">
probability smoothing scheme.
The ability of LSA that extracts and in-
fers latent relations of words makes it pos-
sible to estimate probabilities of sparse data
reasonably. LSA is a fully automatic math-
ematical technique. If we make a matrix
from any given information once, we can use
the reduced matrix for estimating probabil-
ity. While the SVD analysis is somewhat
costly in terms of time for large matrix, less
expensive alternatives such as folding-in and
SVD-updating have been suggested (Michael
W. Berry and Jessup, 1999).
Further investigation is needed in both the-
oretical and experimental side. The sug-
gested model does not have deep background
over probablity theory. Hopefully, (Hofmann,
1999) suggested probabilistic LSI which is
based on a statitical latent class model for
factor analysis of count data. In addition, we
applied our model to estimate bigram proba-
bilities only. Corpus-based NLP is so mature
and the methods must be tested with more
realistic tasks. Since any conditional proba-
bility distributions can be represented by a
matrix form, we can combines other informa-
tion in a matrix, applying our model to more
general tasks, such as word sense disambigua-
tion and word clustering.
</bodyText>
<sectionHeader confidence="0.996935" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.997193555555556">
This work was supported by KOSEF through
the &amp;quot;Multilingual Information Retrieval&amp;quot;
project at the AITrc and was supported by
Ministry of Culture and Tourism under the
program of King Sejong Project through KO-
RTERM. Many fundamental researches was
supported by the R&amp;D fund of Ministry of
Science and Technology under a project of
plan STEP2000.
</bodyText>
<sectionHeader confidence="0.997754" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998828526315789">
Ido Dagan, Lillian Lee, and Fernando C. N.
Pereira. 1999. Similarity-based models of word
cooccurrence probabilities. Machine Learning,
34:43-69.
Scott Deerwester, Susan Dumais, Goerge Fur-
nas, Thomas Landauer, and Richard Harsh-
man. 1990. Indexing by latent semantic analy-
sis. Journal of the American Society for Infor-
mation Science, 41(6):391-407.
Susan T. Dumais, Michael L. Littman, and
Thomas K. Landauer. 1997. Automatic cross-
language retrieval using latent semantic index-
ing. In In AAAI Spring Symposuim on Cross-
Language Text and Speech Retrieval, pages 18-
24, Stanford University, March.
Thomas Hofmann. 1999. Probabilistic latent se-
mantic indexing. In Proceedings of SIGIR&apos;99,
pages 50-57. ACM Press.
Slava M. Katz. 1987. Estimation of probabilities
from sparse data for the language model com-
ponent of a speech recognizer. IEEE Transac-
tions on Acoustics, Speech and Signal Process-
ing, ASSP-35(3):400-401, mar.
Lillian Lee and Fernando Pereira. 1999. Distribu-
tional similarity models: Clustering vs. nearest
neighbors. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics, pages 33-40, Somerset, New Jer-
sey. Association for Computational Linguistics.
Lillian Lee. 1999. Measures of distributional sim-
ilarity. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Lin-
guistics, pages 25-32, Somerset, New Jersey.
Association for Computational Linguistics.
Zlatko Drmac Michael W. Berry and Elizabeth R.
Jessup. 1999. Matrices, vector spaces, and in-
formation retrieval. SIAM Review, 41(2):335-
362.
</reference>
<figure confidence="0.9968835">
11 : :
1: ■
100 200 300 400 500 600 700 800 900 1000
Noun rank by frequency
Error rate(%)
100
90
80
70
60
50
40
30
20
10
0
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.758590">
<title confidence="0.988653">Dimension-Reduced Estimation of Word Co-occurrence Probability</title>
<author confidence="0.99999">Kim Choi</author>
<affiliation confidence="0.997733">Computer Science Dept., AITrc, KORTERM Advanced Institute of Science (KAIST)</affiliation>
<address confidence="0.789502">Kusong-Dong, Yusong-Gu Taejon, 305-701 Republic of Korea</address>
<email confidence="0.972297">gykim©world.kaist.ac.krandkschoi©world.kaist.ac.kr</email>
<abstract confidence="0.999473230769231">We investigate a novel approach to solve the problem of sparse data through dimension reduction. Linear algebraic technique called LSA/SVD is used to find corelationships of sparse words. Three variant estimation methods are suggested and they are evaluated for estimating unseen noun-verb cooccurrence probability. The model shows possibility to be alternative probability smoothing method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Similarity-based models of word cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--43</pages>
<contexts>
<context position="2500" citStr="Dagan et al., 1999" startWordPosition="398" endWordPosition="401">use unigram probability P(y) to estimate bigram probability P (Y I x) for unseen data pair, disregarding the relationship between two words. If unseen bigrams are made up of unigrams of the same frequency, the methods give them the same probability, causing a problem to estimate accurate probability. In addition to the classical methods, similarity-based schemes are successfully applied to data sparseness problem. The nearest-neighbors similarity-based method uses a set of k most similar words x&apos; to estimate conditional probability P(ylx), being said to perform almost 40% better than backoff (Dagan et al., 1999). They use various distributional similarity measures to find similarity between words such as KL-divergence or JS-divergence (Lee, 1999). For the sparse word, however, the distribution P(ylx) itself is sparse and it is difficult to find correct similarity between words, since the only means for measuring word similarity is the frequency. The more sparse the distribution of word is, the more difficult finding acceptable similarities between words. In this paper, we investigate a novel approach to solve the problem of sparse data by capturing their latent relationships with only frequency infor</context>
<context position="11918" citStr="Dagan et al., 1999" startWordPosition="2056" endWordPosition="2059"> Diml Figure 1: An Example of Singular Value Decomposition -0.2 0 Table 2: An Illustrative Example MLE Katzs Similarity Dimension Reduced Model back-off -based method Distance k-Rank DR-SIM -based matrix (k = 3) (dim = 2) (6 = 0.1) (0 = 0) P(swig I coffee) 0 0.09 0 0.1627 0.0756 0.11 P(sip I coffee) 1 1 1 0.2425 0.5536 1 P(drink I coffee) 0 0.18 0.17 0.2359 0.1932 0.28 P(devour I coffee) 0 0.18 0.11 0.1271 0.0726 0.11 P(eat I coffee) 0 0.18 0.11 0.1159 0.0526 0 P(swallow I coffee) 0 0.18 0.11 0.1159 0.0526 0 2.3.3 Method 3: Dimension-reduced similarity-based method The similarity-based method(Dagan et al., 1999) and dimension reduction technique can be merged into one model 3 . Reduced dimension can be better representation space than the original space for finding similarities between words. This approach finds the most k nearest words to x in reduced space and use these word to estimate the probability p(ylx): Pnve(Y -Ix-) P(Yilxi)= ii 1,1Exiics-Pnile(Yilxii) where S = {4cos(4,4&apos;) &gt; 0 on k dim.}, the k is the reduced dimension size not a count of nearest nouns. The count of nearest nouns are determined by 0, which is threshold of cosine value. 31n the previous similarity-based work, (Dagan et al., </context>
<context position="14884" citStr="Dagan et al., 1999" startWordPosition="2534" endWordPosition="2537">ucted in Table 2 using probability estimation functions as described in the above section. MLE shows five zero values, causing data sparseness problems. Katz&apos; backoff methods and similarity-based method cannot distinguish food-related verbs and drinkrelated verbs. In contrast, all dimensionreduced models resolve data sparseness problem and they cluster nouns and co-occurrence verbs reasonably. Thus, we can expect that dimension-reduced model will show promising result in a real experiment. 4 Experiment We evaluated the dimension-reduced models on a pseudo word sense disambiguation task as in (Dagan et al., 1999). Each method is presented with a noun and two verbs, deciding which verb is more likely to have the noun as a direct object. Data preparation method and error counting scheme are almost similar to that of similarity-based methods (Dagan et al., 1999) (Lee and Pereira, 1999). Performance is measured by the error rate, defined as error rate = T-1(0 of incorrect choices) where T is the size of test set. Test instances consist of noun-verb-verb triples (n, vl, v2), where both (n, v1) and (n, v2) are both unseen in the training set. (n, v1) is selected such that it appeared at least twice as often</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando C. N. Pereira. 1999. Similarity-based models of word cooccurrence probabilities. Machine Learning, 34:43-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan Dumais</author>
<author>Goerge Furnas</author>
<author>Thomas Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>41--6</pages>
<contexts>
<context position="6463" citStr="Deerwester et al., 1990" startWordPosition="1066" endWordPosition="1069"> point or a vector in multi-dimensional space of which a dimension size equal to I VI Table 1: An Example of Conditional Probability Matrix NIV swigsipdrinkdevour eat swallow\ beer 0.33 0 0.33 0.33 0 0 whiskey 0 0.5 0.5 0 0 0 coffee 0 1 0 0 0 0 bread 0 0 0 0.33 0.33 0.33 \sugar 0 0 0 0 0.5 0.5 / 2.2 Projection - Latent Semantic Analysis Latent Semantic Analysis (LSA) is known as a theory for extracting and representing the contextual-usage meaning of words. LSA uses singular value decomposition (SVD). It has been widely used in information retrieval task as a variant of the vector space model(Deerwester et al., 1990)(Dumais et al., 1997). Given the conditional probability matrix A and rank(A) = r, the SVD of A and the rankwhere n E N,v E V. Note that each k approximation matrix Ak is defined as A = UEVT =E • • vT Ak - UkEkVkT - E • • vT i=1 where U and V contains left and right singular vectors of A, respectively, and the E = diag(o-i, • • • , o-n) is the diagonal matrix of singular values of A. Truncated SVD Ak, which is constructed from the k-largest signular triples of A, is the closest rank-k matrix to A 2. The left singular vector Tr, and the right singular vector 14 corresponds to the row vector 4 a</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan Dumais, Goerge Furnas, Thomas Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
<author>Michael L Littman</author>
<author>Thomas K Landauer</author>
</authors>
<title>Automatic crosslanguage retrieval using latent semantic indexing.</title>
<date>1997</date>
<booktitle>In In AAAI Spring Symposuim on CrossLanguage Text and Speech Retrieval,</booktitle>
<pages>18--24</pages>
<institution>Stanford University,</institution>
<contexts>
<context position="6484" citStr="Dumais et al., 1997" startWordPosition="1069" endWordPosition="1072">ti-dimensional space of which a dimension size equal to I VI Table 1: An Example of Conditional Probability Matrix NIV swigsipdrinkdevour eat swallow\ beer 0.33 0 0.33 0.33 0 0 whiskey 0 0.5 0.5 0 0 0 coffee 0 1 0 0 0 0 bread 0 0 0 0.33 0.33 0.33 \sugar 0 0 0 0 0.5 0.5 / 2.2 Projection - Latent Semantic Analysis Latent Semantic Analysis (LSA) is known as a theory for extracting and representing the contextual-usage meaning of words. LSA uses singular value decomposition (SVD). It has been widely used in information retrieval task as a variant of the vector space model(Deerwester et al., 1990)(Dumais et al., 1997). Given the conditional probability matrix A and rank(A) = r, the SVD of A and the rankwhere n E N,v E V. Note that each k approximation matrix Ak is defined as A = UEVT =E • • vT Ak - UkEkVkT - E • • vT i=1 where U and V contains left and right singular vectors of A, respectively, and the E = diag(o-i, • • • , o-n) is the diagonal matrix of singular values of A. Truncated SVD Ak, which is constructed from the k-largest signular triples of A, is the closest rank-k matrix to A 2. The left singular vector Tr, and the right singular vector 14 corresponds to the row vector 4 and the column vector </context>
</contexts>
<marker>Dumais, Littman, Landauer, 1997</marker>
<rawString>Susan T. Dumais, Michael L. Littman, and Thomas K. Landauer. 1997. Automatic crosslanguage retrieval using latent semantic indexing. In In AAAI Spring Symposuim on CrossLanguage Text and Speech Retrieval, pages 18-24, Stanford University, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR&apos;99,</booktitle>
<pages>50--57</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="20994" citStr="Hofmann, 1999" startWordPosition="3588" endWordPosition="3589"> of words makes it possible to estimate probabilities of sparse data reasonably. LSA is a fully automatic mathematical technique. If we make a matrix from any given information once, we can use the reduced matrix for estimating probability. While the SVD analysis is somewhat costly in terms of time for large matrix, less expensive alternatives such as folding-in and SVD-updating have been suggested (Michael W. Berry and Jessup, 1999). Further investigation is needed in both theoretical and experimental side. The suggested model does not have deep background over probablity theory. Hopefully, (Hofmann, 1999) suggested probabilistic LSI which is based on a statitical latent class model for factor analysis of count data. In addition, we applied our model to estimate bigram probabilities only. Corpus-based NLP is so mature and the methods must be tested with more realistic tasks. Since any conditional probability distributions can be represented by a matrix form, we can combines other information in a matrix, applying our model to more general tasks, such as word sense disambiguation and word clustering. 6 Acknowledgements This work was supported by KOSEF through the &amp;quot;Multilingual Information Retrie</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of SIGIR&apos;99, pages 50-57. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<pages>35--3</pages>
<contexts>
<context position="1874" citStr="Katz, 1987" startWordPosition="299" endWordPosition="300">ts first element is x E X. In other words, P (Y I x) can be regarded as a measure of relationship between word x and y. For example, for a given object x = beer, a verb y = drink is more related than a verb y = eat, p(drinklbeer) &gt;&gt; p(eatlbeer). Many features can be used to predict a relationship between two words, but we assume here that the only information we have are the frequencies. To overcome the difficulty of sparse data, a smoothing technique like Good-Turing method is widely used. Estimator combining approaches such as linear interpolation and Katz&apos;s back-off method are popular also(Katz, 1987). They use unigram probability P(y) to estimate bigram probability P (Y I x) for unseen data pair, disregarding the relationship between two words. If unseen bigrams are made up of unigrams of the same frequency, the methods give them the same probability, causing a problem to estimate accurate probability. In addition to the classical methods, similarity-based schemes are successfully applied to data sparseness problem. The nearest-neighbors similarity-based method uses a set of k most similar words x&apos; to estimate conditional probability P(ylx), being said to perform almost 40% better than ba</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing, ASSP-35(3):400-401, mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Distributional similarity models: Clustering vs. nearest neighbors.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="15159" citStr="Lee and Pereira, 1999" startWordPosition="2581" endWordPosition="2584">rast, all dimensionreduced models resolve data sparseness problem and they cluster nouns and co-occurrence verbs reasonably. Thus, we can expect that dimension-reduced model will show promising result in a real experiment. 4 Experiment We evaluated the dimension-reduced models on a pseudo word sense disambiguation task as in (Dagan et al., 1999). Each method is presented with a noun and two verbs, deciding which verb is more likely to have the noun as a direct object. Data preparation method and error counting scheme are almost similar to that of similarity-based methods (Dagan et al., 1999) (Lee and Pereira, 1999). Performance is measured by the error rate, defined as error rate = T-1(0 of incorrect choices) where T is the size of test set. Test instances consist of noun-verb-verb triples (n, vl, v2), where both (n, v1) and (n, v2) are both unseen in the training set. (n, v1) is selected such that it appeared at least twice as often than (n, v2) in the original verb-object pairs and p(n, v1) &gt; p(n, v2) is a correct answer. In addition, to consider Katzs backoff method as the baseline, v2 is choosed as frq(v1) &lt; frq(v2) &lt;=&gt; p(v1) &lt; p(v2), and the error rate of back-off method is always 100% 5 . Running </context>
</contexts>
<marker>Lee, Pereira, 1999</marker>
<rawString>Lillian Lee and Fernando Pereira. 1999. Distributional similarity models: Clustering vs. nearest neighbors. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 33-40, Somerset, New Jersey. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="2637" citStr="Lee, 1999" startWordPosition="419" endWordPosition="420">seen bigrams are made up of unigrams of the same frequency, the methods give them the same probability, causing a problem to estimate accurate probability. In addition to the classical methods, similarity-based schemes are successfully applied to data sparseness problem. The nearest-neighbors similarity-based method uses a set of k most similar words x&apos; to estimate conditional probability P(ylx), being said to perform almost 40% better than backoff (Dagan et al., 1999). They use various distributional similarity measures to find similarity between words such as KL-divergence or JS-divergence (Lee, 1999). For the sparse word, however, the distribution P(ylx) itself is sparse and it is difficult to find correct similarity between words, since the only means for measuring word similarity is the frequency. The more sparse the distribution of word is, the more difficult finding acceptable similarities between words. In this paper, we investigate a novel approach to solve the problem of sparse data by capturing their latent relationships with only frequency information. Through reducing dimension by linear algebraic technique LSA/SVD&apos;, we can eliminate zero values in 1-LSA - Latent Semantic Analys</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 25-32, Somerset, New Jersey. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zlatko Drmac Michael W Berry</author>
<author>Elizabeth R Jessup</author>
</authors>
<title>Matrices, vector spaces, and information retrieval.</title>
<date>1999</date>
<journal>SIAM Review,</journal>
<pages>41--2</pages>
<contexts>
<context position="8512" citStr="Berry and Jessup, 1999" startWordPosition="1442" endWordPosition="1445">ed representation which captures word(x)-word(y) associations is used for estimating probabilities of unseen data. 2.3 Estimating Probabilities on Reduced Space Until now, we constructed word co-occurrence probability matrix and projected the matrix into lower dimension space. Now, we suggest 21n other words, the projection into the reduced space is chosen such that the representations in the original space are changed as little as possible when measured by the sum of the squares of the differences. One can prove that Ak is the best approximation to A for any unitray invariant norm(Michael W. Berry and Jessup, 1999) three variant probability estimation methods in dimension-reduced space. First is estimating p(y Ix) by computing distance between given word x and predicting word y in reduced space. Second, we can use rank-k approximation matrix. Third, the state-of-the-art similarity-based methods can be merged to our dimension-reduced model. Because the first two methods are not based on statistical theory, it should be explored 2.3.1 Method 1: Distance-based method Through LSA, the matrix A is factored into the product of three matrices as in Equation 3, and Tr, and v-3 are considered as the row vector 4</context>
<context position="18549" citStr="Berry and Jessup, 1999" startWordPosition="3184" endWordPosition="3187">-art similaritybased methods find similarities toward only one side. For example, their well-known similarity measures JS-divergence between given word x, and another word x is defined as JS(P(Ylx%),P(Yix)). It means that each row P(Ylx%) is compared to another row p(y 14 in Table 1. Comparisons on column side are not performed. That&apos;s why the similarity-based fail to grasp true relationships on word cooccurrences. On the other hand, SVD which is the mathematical background of our model gives us a reduced-rank basis for both column space and the row space simultaneously (Figure 1) (Michael W. Berry and Jessup, 1999). As we showed in the previous example in Section 3, the dimension-reduced model extract underlying or latent structures of word co-occurrences well. Therefore, our model shows successful result on estimating word cooccurrence probabilities of sparse data. futher However, the experiment is artificial and SVD is not directly related to the probabilty theory, futher theoretical invetigation is required. 4.3 Optimal subspace and Degree of sparseness We also investigated the change of the performances as subspace size and degree of sparseness vary. Figure 2 shows performances of distance-based DR </context>
<context position="20817" citStr="Berry and Jessup, 1999" startWordPosition="3560" endWordPosition="3563"> Size 0.6 0.55 — 0.45 0.5 U) CC it! 0.4 0.35 I 0 Figure 3: Performance vs. Degree of sparseness probability smoothing scheme. The ability of LSA that extracts and infers latent relations of words makes it possible to estimate probabilities of sparse data reasonably. LSA is a fully automatic mathematical technique. If we make a matrix from any given information once, we can use the reduced matrix for estimating probability. While the SVD analysis is somewhat costly in terms of time for large matrix, less expensive alternatives such as folding-in and SVD-updating have been suggested (Michael W. Berry and Jessup, 1999). Further investigation is needed in both theoretical and experimental side. The suggested model does not have deep background over probablity theory. Hopefully, (Hofmann, 1999) suggested probabilistic LSI which is based on a statitical latent class model for factor analysis of count data. In addition, we applied our model to estimate bigram probabilities only. Corpus-based NLP is so mature and the methods must be tested with more realistic tasks. Since any conditional probability distributions can be represented by a matrix form, we can combines other information in a matrix, applying our mod</context>
</contexts>
<marker>Berry, Jessup, 1999</marker>
<rawString>Zlatko Drmac Michael W. Berry and Elizabeth R. Jessup. 1999. Matrices, vector spaces, and information retrieval. SIAM Review, 41(2):335-362.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>