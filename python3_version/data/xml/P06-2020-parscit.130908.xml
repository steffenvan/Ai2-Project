<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000669">
<title confidence="0.998842">
Topic-Focused Multi-document Summarization
Using an Approximate Oracle Score
</title>
<author confidence="0.985694">
John M. Conroy, Judith D. Schlesinger Dianne P. O’Leary
</author>
<affiliation confidence="0.827281">
IDA Center for Computing Sciences University of Maryland
</affiliation>
<address confidence="0.620453">
Bowie, Maryland, USA College Park, Maryland, USA
</address>
<email confidence="0.997603">
conroy@super.org, judith@super.org oleary@cs.umd.edu
</email>
<sectionHeader confidence="0.993895" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919888888889">
We consider the problem of producing a
multi-document summary given a collec-
tion of documents. Since most success-
ful methods of multi-document summa-
rization are still largely extractive, in this
paper, we explore just how well an ex-
tractive method can perform. We intro-
duce an “oracle” score, based on the prob-
ability distribution of unigrams in human
summaries. We then demonstrate that with
the oracle score, we can generate extracts
which score, on average, better than the
human summaries, when evaluated with
ROUGE. In addition, we introduce an ap-
proximation to the oracle score which pro-
duces a system with the best known per-
formance for the 2005 Document Under-
standing Conference (DUC) evaluation.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962793103448">
We consider the problem of producing a multi-
document summary given a collection of doc-
uments. Most automatic methods of multi-
document summarization are largely extractive.
This mimics the behavior of humans for sin-
gle document summarization; (Kupiec, Pendersen,
and Chen 1995) reported that 79% of the sentences
in a human-generated abstract were a “direct
match” to a sentence in a document. In contrast,
for multi-document summarization, (Copeck and
Szpakowicz 2004) report that no more than 55% of
the vocabulary contained in human-generated ab-
stracts can be found in the given documents. Fur-
thermore, multiple human summaries on the same
collection of documents often have little agree-
ment. For example, (Hovy and Lin 2002) report
that unigram overlap is around 40%. (Teufel and
van Halteren 2004) used a “factoid” agreement
analysis of human summaries for a single doc-
ument and concluded that a resulting consensus
summary is stable only if 30–40 summaries are
collected.
In light of the strong evidence that nearly half
of the terms in human-generated multi-document
abstracts are not from the original documents, and
that agreement of vocabulary among human ab-
stracts is only about 40%, we pose two coupled
questions about the quality of summaries that can
be attained by document extraction:
</bodyText>
<listItem confidence="0.9957483">
1. Given the sets of unigrams used by four hu-
man summarizers, can we produce an extract
summary that is statistically indistinguish-
able from the human abstracts when mea-
sured by current automatic evaluation meth-
ods such as ROUGE?
2. If such unigram information can produce
good summaries, can we replace this infor-
mation by a statistical model and still produce
good summaries?
</listItem>
<bodyText confidence="0.999204538461538">
We will show that the answer to the first question
is, indeed, yes and, in fact, the unigram set infor-
mation gives rise to extract summaries that usually
score better than the 4 human abstractors! Sec-
ondly, we give a method to statistically approxi-
mate the set of unigrams and find it produces ex-
tracts of the DUC 05 data which outperform all
known evaluated machine entries. We conclude
with experiments on the extent that redundancy
removal improves extracts, as well as a method
of moving beyond simple extracting by employ-
ing shallow parsing techniques to shorten the sen-
tences prior to selection.
</bodyText>
<page confidence="0.968452">
152
</page>
<note confidence="0.8919535">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 152–159,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.872936" genericHeader="introduction">
2 The Data
</sectionHeader>
<bodyText confidence="0.988944083333333">
The 2005 Document Understanding Conference
(DUC 2005) data used in our experiments is par-
titioned into 50 topic sets, each containing 25–50
documents. A topic for each set was intended
to mimic a real-world complex questioning-
answering task for which the answer could not
be given in a short “nugget.” For each topic,
m
four human sumarizers were asked to provide
a 250-word summary of the topic. Topics were
labeled as either “general” or “specific”. We
present an example of one of each category.
</bodyText>
<subsectionHeader confidence="0.644002">
Set d408c
</subsectionHeader>
<bodyText confidence="0.746050571428571">
Granularity: Specific
Title: Human Toll of Tropical Storms
Narrative: What has been the human toll in death or injury
of tropical storms in recent years? Where and when have
each of the storms caused human casualties? What are the
approximate total number of casualties attributed to each of
the storms?
</bodyText>
<subsectionHeader confidence="0.503848">
Set d436j
</subsectionHeader>
<bodyText confidence="0.923731952380952">
Granularity: General
Title: Reasons for Train Wrecks
Narrative: What causes train wrecks and what can be done
to prevent them? Train wrecks are those events that result
in actual damage to the trains themselves not just accidents
where people are killed or injured.
For each topic, the goal is to produce a 250-
word summary. The basic unit we extract from
a document is a sentence.
To prepare the data for processing, we
segment each document into sentences using
a POS (part-of-speech) tagger, NLProcessor
(http://www.infogistics.com/posdemo.htm). The
newswire documents in the DUC 05 data have
markers indicating the regions of the document,
including titles, bylines, and text portions. All of
the extracted sentences in this study are taken from
the text portions of the documents only.
We define a “term” to be any “non-stop word.”
Our stop list contains the 400 most frequently oc-
curring English words.
</bodyText>
<sectionHeader confidence="0.987936" genericHeader="method">
3 The Oracle Score
</sectionHeader>
<bodyText confidence="0.999646178571429">
Recently, a crisp analysis of the frequency of
content words used by humans relative to the
high frequency content words that occur in the
relevant documents has yielded a simple and
powerful summarization method called SumBa-
sic (Nenkova and Vanderwende, 2005). SumBa-
sic produced extract summaries which performed
nearly as well as the best machine systems for
generic 100 word summaries, as evaluated in DUC
2003 and 2004, as well as the Multi-lingual Sum-
marization Evaluation (MSE 2005).
Instead of using term frequencies of the corpus
to infer highly likely terms in human summaries,
we propose to directly model the set of terms (vo-
cabulary) that is likely to occur in a sample of hu-
man summaries. We seek to estimate the proba-
bility that a term will be used by a human sum-
marizer to first get an estimate of the best possible
extract and later to produce a statistical model for
an extractive summary system. While the primary
focus of this work is “task oriented” summaries,
we will also address a comparison with SumBa-
sic and other systems on generic multi-document
summaries for the DUC 2004 dataset in Section 8.
Our extractive summarization system is given a
topic, T, specified by a text description. It then
evaluates each sentence in each document in the
set to determine its appropriateness to be included
in the summary for the topic T.
We seek a statistic which can score an individ-
ual sentence to determine if it should be included
as a candidate. We desire that this statistic take
into account the great variability that occurs in
the space of human summaries on a given topic
T. One possibility is to simply judge a sentence
based upon the expected fraction of the “human
summary”-terms that it contains. We posit an or-
acle, which answers the question “Does human
summary i contain the term t?”
By invoking this oracle over the set of terms
and a sample of human summaries, we can
readily compute the expected fraction of human
summary-terms the sentence contains. To model
the variation in human summaries, we use the or-
acle to build a probabilistic model of the space
of human abstracts. Our “oracle score” will then
compute the expected number of summary terms a
sentence contains, where the expectation is taken
from the space of all human summaries on the
topic T.
We model human variation in summary gener-
ation with a unigram bag-of-words model on the
terms. In particular, consider P(tlT) to be the
probability that a human will select term t in a
summary given a topic T. The oracle score for a
sentence x, w(x), can then be defined in terms of
</bodyText>
<page confidence="0.933271">
153
</page>
<equation confidence="0.9990696">
P :
1 X
ω(x) = x(t)P(t|τ)
|x|
t
</equation>
<bodyText confidence="0.999723375">
where |x |is the number of distinct terms sentence
x contains, T is the universal set of all terms used
in the topic τ and x(t) = 1 if the sentence x con-
tains the term t and 0 otherwise. (We affectionally
refer to this score as the “Average Jo” score, as it is
derived the average uni-gram distribution of terms
in human summaries.)
While we will consider several approximations
to P(t|τ) (and, correspondingly, ω), we first ex-
plore the maximum-likelihood estimate of P(t|τ)
given by a sample of human summaries. Suppose
we are given h sample summaries generated in-
dependently. Let cit(τ) = 1 if the i-th summary
contains the term t and 0 otherwise. Then the
maximum-likelihood estimate of P(tτ) is given
by
</bodyText>
<equation confidence="0.9873985">
h
Pˆ(t|τ) = h i=1 cit(τ).
</equation>
<bodyText confidence="0.999979">
We define ωˆ by replacing P with Pˆ in the defi-
nition of ω. Thus, ωˆ is the maximum-likelihood
estimate for ω, given a set of h human summaries.
Given the score ˆω, we can compute an extract
summary of a desired length by choosing the top
scoring sentences from the collection of docu-
ments until the desired length (250 words) is ob-
tained. We limit our selection to sentences which
have 8 or more distinct terms to avoid selecting in-
complete sentences which may have been tagged
by the sentence splitter.
Before turning to how well our idealized score,
ˆω, performs on extract summaries, we first define
the scoring mechanism used to evaluate these sum-
maries.
</bodyText>
<sectionHeader confidence="0.998723" genericHeader="method">
4 ROUGE
</sectionHeader>
<bodyText confidence="0.999715352941176">
The state-of-the-art automatic summarization
evaluation method is ROUGE (Recall Oriented
Understudy for Gisting Evaluation, (Hovy and Lin
2002)), an n-gram based comparison that was mo-
tivated by the machine translation evaluation met-
ric, Bleu (Papineni et. al. 2001). This system uses
a variety of n-gram matching approaches, some of
which allow gaps within the matches as well as
more sophistcated analyses. Surprisingly, simple
unigram and bigram matching works extremely
well. For example, at DUC 05, ROUGE-2 (bi-
gram match) had a Spearman correlation of 0.95
and a Pearson correlation of 0.97 when compared
with human evaluation of the summaries for re-
sponsiveness (Dang 2005). ROUGE-n for match-
ing n−grams of a summary X against h model
human summaries is given by:
</bodyText>
<equation confidence="0.97838675">
= Phj= z
1 &amp;ENn min(Xn(i), Mn(i j))
Rn (X)
Pjh=1 PiENn Mn(i,j),
</equation>
<bodyText confidence="0.999969647058824">
where Xn(i) is the count of the number of
times the n-gram i occurred in the summary and
Mn(i, j) is the number of times the n-gram i
occurred in the j-th model (human) summary.
(Note that for brevity of notation, we assume that
lemmatization (stemming) is done apriori on the
terms.)
When computing ROUGE scores, a jackknife
procedure is done to make comparison of machine
systems and humans more amenable. In particu-
lar, if there are k human summaries available for
a topic, then the ROUGE score is computed for a
human summary by comparing it to the remaining
k − 1 summaries, while the ROUGE score for a
machine summary is computed against all k sub-
sets of size k − 1 of the human summaries and
taking the average of these k scores.
</bodyText>
<sectionHeader confidence="0.836954" genericHeader="method">
5 The Oracle or Average Jo Summary
</sectionHeader>
<bodyText confidence="0.999980652173913">
We now present results on the performance of
the oracle method as compared with human sum-
maries. We give the ROUGE-2 (R2) scores as
well as the 95% confidence error bars. In Fig-
ure 1, the human summarizers are represented by
the letters A–H, and systems 15, 17, 8, and 4
are the top performing machine summaries from
DUC 05. The letter “O” represents the ROUGE-2
scores for extract summaries produced by the ora-
cle score, ˆω. Perhaps surprisingly, the oracle pro-
duced extracts which performed better than the hu-
man summaries! Since each human only summa-
rized 10 document clusters, the human error bars
are larger. However, even with the large error bars,
we observe that the mean ROUGE-2 scores for the
oracle extracts exceeds the 95% confidence error
bars for several humans.
While the oracle was, of course, given the un-
igram term probabilities, its performance is no-
table on two counts. First, the evaluation met-
ric scored on 2-grams, while the oracle was only
given unigram information. In a sense, optimizing
for ROUGE-1 is a “sufficient statistic” scoring at
</bodyText>
<page confidence="0.999012">
154
</page>
<bodyText confidence="0.999934380952381">
the human level for ROUGE-2. Second, the hu-
mans wrote abstracts while the oracle simply did
extracting. Consequently, the documents contain
sufficient text to produce human-quality extract
summaries as measured by ROUGE. The human
performance ROUGE scores indicate that this ap-
proach is capable of producing automatic extrac-
tive summaries that produce vocabulary compara-
ble to that chosen by humans. Human evaluation
(which we have not yet performed) is required to
determine to what extent this high ROUGE-2 per-
formance is indicative of high quality summaries
for human use.
The encouraging results of the oracle score nat-
urally lead to approximations, which, perhaps,
will give rise to strong machine system perfor-
mance. Our goal is to approximate P(t|T), the
probability that a term will be used in a human
abstract. In the next section, we present two ap-
proaches which will be used in tandem to make
this approximation.
</bodyText>
<figureCaption confidence="0.999542">
Figure 1: The Oracle (Average Jo score) Score cv
</figureCaption>
<sectionHeader confidence="0.982684" genericHeader="method">
6 Approximating P(t|T)
</sectionHeader>
<bodyText confidence="0.999987444444444">
We seek to approximate P(t|T) in an analo-
gous fashion to the maximum-likelihood estimate
P�(t|T). To this end, we devise methods to isolate
a subset of terms which would likely be included
in the human summary. These terms are gleaned
from two sources, the topic description and the
collection of documents which were judged rele-
vant to the topic. The former will give rise to query
terms and the latter to signature terms.
</bodyText>
<subsectionHeader confidence="0.998481">
6.1 Query Term Identification
</subsectionHeader>
<bodyText confidence="0.999182090909091">
A set of query terms is automatically ex-
tracted from the given topic description. We
identified individual words and phrases from
both the &lt;topic&gt; (Title) tagged paragraph as
well as whichever of the &lt;narr&gt; (Narrative)
Set d408c: approximate, casualties,
death, human, injury, number, recent,
storms, toll, total, tropical, years
Set d436j: accidents, actual, causes,
damage, events, injured, killed, prevent,
result, train, train wrecks, trains, wrecks
</bodyText>
<tableCaption confidence="0.657768">
Table 1: Query Terms for “Tropical Storms” and
“Train Wrecks” Topics
</tableCaption>
<bodyText confidence="0.988420230769231">
tagged paragraphs occurred in the topic descrip-
tion. We made no use of the &lt;granularity&gt;
paragraph marking. We tagged the topic de-
scription using the POS-tagger, NLProcessor
(http://www.infogistics.com/posdemo.htm), and
any words that were tagged with any NN (noun),
VB (verb), JJ (adjective), or RB (adverb) tag were
included in a list of words to use as query terms.
Table 1 shows a list of query terms for our two
illustrative topics.
The number of query terms extracted in this way
ranged from a low of 3 terms for document set
d360f to 20 terms for document set d324e.
</bodyText>
<subsectionHeader confidence="0.999673">
6.2 Signature Terms
</subsectionHeader>
<bodyText confidence="0.999998692307692">
The second collection of terms we use to estimate
P(t|T) are signature terms. Signature terms are
the terms that are more likely to occur in the doc-
ument set than in the background corpus. They
are generally indicative of the content contained
in the collection of documents. To identify these
terms, we use the log-likelihood statistic suggested
by Dunning (Dunning 1993) and first used in sum-
marization by Lin and Hovy (Hovy and Lin 2000).
The statistic is equivalent to a mutual information
statistic and is based on a 2-by-2 contingency ta-
ble of counts for each term. Table 2 shows a list of
signature terms for our two illustrative topics.
</bodyText>
<subsectionHeader confidence="0.998946">
6.3 An estimate of P(t|T)
</subsectionHeader>
<bodyText confidence="0.999953555555556">
To estimate P(t|T), we view both the query terms
and the signature terms as “samples” from ideal-
ized human summaries. They represent the terms
that we would most likely see in a human sum-
mary. As such, we expect that these sample terms
may approximate the underlying set of human
summary terms. Given a collection of query terms
and signature terms, we can readily estimate our
target objective, P(t|T) by the following:
</bodyText>
<equation confidence="0.9861045">
1 1
PQs(t|�) = 2qt(�) + 2st(T)
</equation>
<page confidence="0.989048">
155
</page>
<bodyText confidence="0.976204294117647">
Set d408c: ahmed, allison, andrew,
bahamas, bangladesh, bn, caribbean,
carolina, caused, cent, coast, coastal,
croix, cyclone, damage, destroyed, dev-
astated, disaster, dollars, drowned, flood,
flooded, flooding, floods, florida, gulf,
ham, hit, homeless, homes, hugo, hurri-
cane, insurance, insurers, island, islands,
lloyd, losses, louisiana, manila, miles,
nicaragua, north, port, pounds, rain,
rains, rebuild, rebuilding, relief, rem-
nants, residents, roared, salt, st, storm,
storms, supplies, tourists, trees, tropi-
cal, typhoon, virgin, volunteers, weather,
west, winds, yesterday.
Set d436j: accident, accidents, am-
munition, beach, bernardino, board,
boulevard, brake, brakes, braking, cab,
car, cargo, cars, caused, collided, col-
lision, conductor, coroner, crash, crew,
crossing, curve, derail, derailed, driver,
emergency, engineer, engineers, equip-
ment, fe, fire, freight, grade, hit, holland,
injured, injuries, investigators, killed,
line, locomotives, maintenance, mechan-
ical, miles, morning, nearby, ntsb, oc-
curred, officials, pacific, passenger, pas-
sengers, path, rail, railroad, railroads,
railway, routes, runaway, safety, san,
santa, shells, sheriff, signals, southern,
speed, station, train, trains, transporta-
tion, truck, weight, wreck
Table 2: Signature Terms for “Tropical Storms”
and “Train Wrecks” Topics
</bodyText>
<figureCaption confidence="0.998029">
Figure 2: Scatter Plot of ω� versus ωqs
</figureCaption>
<bodyText confidence="0.999949230769231">
where st(τ)=1 if t is a signature term for topic τ
and 0 otherwise and qt(τ) = 1 if t is a query term
for topic τ and 0 otherwise.
More sophisticated weightings of the query and
signature have been considered; however, for this
paper we limit our attention to the above ele-
mentary scheme. (Note, in particular, a psuedo-
relevance feedback method was employed by
(Conroy et. al. 2005), which gives improved per-
formance.)
Similarly, we estimate the oracle score of a sen-
tence’s expected number of human abstract terms
as
</bodyText>
<equation confidence="0.9936">
ωqs(x) = |1 t�x(t)Pqs(t|τ)
</equation>
<bodyText confidence="0.99997">
where |x |is the number of distinct terms that sen-
tence x contains, T is the universal set of all terms
and x(t) = 1 if the sentence x contains the term t
and 0 otherwise.
For both the oracle score and the approximation,
we form the summary by taking the top scoring
sentences among those sentences with at least 8
distinct terms, until the desired length (250 words
for the DUC05 data) is achieved or exceeded. (The
threshold of 8 was based upon previous analysis
of the sentence splitter, which indicated that sen-
tences shorter than 8 terms tended not be be well
formed sentences or had minimal, if any, content.)
If the length is too long, the last sentence chosen
is truncated to reach the target length.
Figure 2 gives a scatter plot of the oracle score
ω and its approximation ωqs for all sentences with
at least 8 unique terms. The overall Pearson corre-
lation coefficient is approximately 0.70. The cor-
relation varies substantially over the topics. Fig-
ure 3 gives a histogram of the Pearson correlation
coefficients for the 50 topic sets.
</bodyText>
<page confidence="0.99794">
156
</page>
<figureCaption confidence="0.980451">
Figure 3: Histogram of Document Set Pearson Co-
efficients of w� versus wq,
</figureCaption>
<sectionHeader confidence="0.99296" genericHeader="method">
7 Enhancements
</sectionHeader>
<bodyText confidence="0.99981075">
In the this section we explore two approaches to
improve the quality of the summary, linguistic pre-
processing (sentence trimming) and a redundancy
removal method.
</bodyText>
<subsectionHeader confidence="0.998223">
7.1 Linguistic Preprocessing
</subsectionHeader>
<bodyText confidence="0.9988177">
We developed patterns using “shallow parsing”
techniques, keying off of lexical cues in the sen-
tences after processing them with the POS-tagger.
We initially used some full sentence eliminations
along with the phrase eliminations itemized be-
low; analysis of DUC 03 results, however, demon-
strated that the full sentence eliminations were not
useful.
The following phrase eliminations were made,
when appropriate:
</bodyText>
<listItem confidence="0.999839">
• gerund clauses;
• restricted relative-clause appositives;
• intra-sentential attribution;
• lead adverbs.
</listItem>
<bodyText confidence="0.9978988125">
See (Dunlavy et. al) for the specific rules used
for these eliminations. Comparison of two runs
in DUC 04 convinced us of the benefit of applying
these phrase eliminations on the full documents,
prior to summarization, rather than on the selected
sentences after scoring and sentence selection had
been performed. See (Conroy et. al. 2004) for
details on this comparison.
After the trimmed text has been generated, we
then compute the signature terms of the document
sets and recompute the approximate oracle scores.
Note that since the sentences have usually had
some extraneous information removed, we expect
some improvement in the quality of the signature
terms and the resulting scores. Indeed, the median
ROUGE-2 score increases from 0.078 to 0.080.
</bodyText>
<subsectionHeader confidence="0.998842">
7.2 Redundancy Removal
</subsectionHeader>
<bodyText confidence="0.9997102">
The greedy sentence selection process we de-
scribed in Section 6 gives no penalty for sentences
which are redundant to information already con-
tained in the partially formed summary. A method
for reducing redundancy can be employed. One
popular method for reducing redundancy is max-
imum marginal relevance (MMR) (2). Based on
previous studies, we have found that a pivoted
QR, a method from numerical linear algebra, has
some advantages over MMR and performs some-
what better.
Pivoted QR works on a term-sentence matrix
formed from a set of candidate sentences for in-
clusion in the summary. We start with enough
sentences so the total number of terms is approx-
imately twice the desired summary length. Let B
be the term-sentence matrix with BZj = 1 if sen-
tence j contains term i.
The columns of B are then normalized so their
2-norm (Euclidean norm) is the corresponding ap-
proximate oracle score, i.e. wq,(bj), where bj is
the j-th column of B. We call this normalized term
sentence matrix A.
Given a normalized term-sentence matrix A,
QR factorization attempts to select columns of A
in the order of their importance in spanning the
subspace spanned by all of the columns. The stan-
dard implementation of pivoted QR decomposi-
tion is a “Gram-Schmidt” process. The first r sen-
tences (columns) selected by the pivoted QR are
used to form the summary. The number r is cho-
sen so that the summary length is close to the tar-
get length. A more complete description can be
found in (Conroy and O’Leary 2001).
Note, that the selection process of using the piv-
oted QR on the weighted term sentence matrix
will first choose the sentence with the highest wpq
score as was the case with the greedy selection
process. Its subsequent choices are affected by
previous choices as the weights of the columns are
decreased for any sentence which can be approxi-
mated by a linear combination of the current set of
selected sentences. This is more general than sim-
ply demanding that the sentence have small over-
lap with the set of previous chosen sentences as
</bodyText>
<page confidence="0.995429">
157
</page>
<figureCaption confidence="0.9711825">
Figure 4: ROUGE-2 Performance of Oracle Score
Approximations w� vs. Humans and Peers
</figureCaption>
<bodyText confidence="0.960108">
would be done using MMR.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.999860375">
Figure 4 gives the ROUGE-2 scores with error
bars for the approximations of the oracle score as
well as the ROUGE-2 scores for the human sum-
marizers and the top performing systems at DUC
2005. In the graph, qs is the approximate oracle,
qs(p) is the approximation using linguistic prepro-
cessing, and qs(pr) is the approximation with both
linguistic preprocessing and redundancy removal.
Note that while there is some improvement using
the linguistic preprocessing, the improvement us-
ing our redundancy removal technique is quite mi-
nor. Regardless, our system using signature terms
and query terms as estimates for the oracle score
performs comparably to the top scoring system at
DUC 05.
Table 3 gives the ROUGE-2 scores for the re-
cent DUC 06 evaluation which was essentially
the same task as for DUC 2005. The manner in
which the linguistic preprocessing is performed
has changed from DUC 2005, although the types
of removals have remained the same. In addition,
pseudo-relevance feedback was employed for re-
dundancy removal as mentioned earlier. See (Con-
roy et. al. 2005) for details.
While the main focus of this study is task-
oriented multidocument summarization, it is in-
structive to see how well such an approach would
perform for a generic summarization task as with
the 2004 DUC Task 2 dataset. Note, the w score
for generic summaries uses only the signature
term portion of the score, as no topic descrip-
tion is given. We present ROUGE-1 (rather than
</bodyText>
<table confidence="0.999785782608696">
Submission Mean 95% CI Lower 95% CI Upper
O (ω) 0.13710 0.13124 0.14299
C 0.13260 0.11596 0.15197
D 0.12380 0.10751 0.14003
B 0.11788 0.10501 0.13351
G 0.11324 0.10195 0.12366
F 0.10893 0.09310 0.12780
H 0.10777 0.09833 0.11746
J 0.10717 0.09293 0.12460
I 0.10634 0.09632 0.11628
E 0.10365 0.08935 0.11926
A 0.10361 0.09260 0.11617
24 0.09558 0.09144 0.09977
ω(pr) 0.09160 0.08729 0.09570
qs
15
0.09097 0.08671 0.09478
12 0.08987 0.08583 0.09385
8 0.08954 0.08540 0.09338
23 0.08792 0.08371 0.09204
ω(p) qs 0.08738 0.08335 0.09145
ωqs 0.08713 0.08317 0.09110
28 0.08700 0.08332 0.09096
</table>
<tableCaption confidence="0.975169">
Table 3: Average ROUGE 2 Scores for DUC06:
Humans A-I
</tableCaption>
<bodyText confidence="0.982431666666666">
ROUGE-2) scores with stop words removed for
comparison with the published results given in
(Nenkova and Vanderwende, 2005).
Table 4 gives these scores for the top perform-
ing systems at DUC04 as well as SumBasic and
wqs , the approximate oracle based on signature
</bodyText>
<equation confidence="0.497387">
�pr)
</equation>
<bodyText confidence="0.971229833333333">
terms alone with linguistic preprocess trimming
and pivot QR for redundancy removal. As dis-
played, w�pr)
qs scored second highest and within the
95% confidence intervals of the top system, peer
65, as well as SumBasic, and peer 34.
</bodyText>
<table confidence="0.999681411764706">
Submission Mean 95% CI Lower 95% CI Upper
F 0.36787 0.34442 0.39467
B 0.36126 0.33387 0.38754
O (ω) 0.35810 0.34263 0.37330
H 0.33871 0.31540 0.36423
A 0.33289 0.30591 0.35759
D 0.33212 0.30805 0.35628
E 0.33277 0.30959 0.35687
C 0.30237 0.27863 0.32496
G 0.30909 0.28847 0.32987
ω(pr) 0.308 0.294 0.322
qs 0.308 0.293 0.323
peer 65
SumBasic 0.302 0.285 0.319
peer 34 0.290 0.273 0.307
peer 124 0.286 0.268 0.303
peer 102 0.285 0.267 0.302
</table>
<tableCaption confidence="0.995338">
Table 4: Average ROUGE 1 Scores with stop
words removed for DUC04, Task 2
</tableCaption>
<page confidence="0.998037">
158
</page>
<sectionHeader confidence="0.998695" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.9999874">
We introduced an oracle score based upon the
simple model of the probability that a human
will choose to include a term in a summary.
The oracle score demonstrated that for task-based
summarization, extract summaries score as well
as human-generated abstracts using ROUGE. We
then demonstrated that an approximation of the or-
acle score based upon query terms and signature
terms gives rise to an automatic method of summa-
rization, which outperforms the systems entered
in DUC05. The approximation also performed
very well in DUC 06. Further enhancements based
upon linguistic trimming and redundancy removal
via a pivoted QR algorithm give significantly bet-
ter results.
</bodyText>
<sectionHeader confidence="0.99938" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99844003030303">
Jamie Carbonnell and Jade Goldstein “The of MMR,
diversity-based reranking for reordering documents
and producing summaries.” In Proc. ACM SIGIR,
pages 335–336.
John M. Conroy and Dianne P. O’Leary. “Text Summa-
rization via Hidden Markov Models and Pivoted QR
Matrix Decomposition”. Technical report, Univer-
sity of Maryland, College Park, Maryland, March,
2001.
John M. Conroy and Judith D. Schlesinger and
Jade Goldstein and Dianne P. O’Leary, Left-
Brain Right-Brain Multi-Document Summariza-
tion, Document Understanding Conference 2004
http://duc.nist.gov/ 2004
John M. Conroy and Judith D. Schlesinger and Jade
Goldstein, CLASSY Tasked Based Summarization:
Back to Basics, Document Understanding Confer-
ence 2005 http://duc.nist.gov/ 2005
John M. Conroy and Judith D. Schlesinger Dianne
P. O’Leary, and Jade Goldstein, Back to Basciss:
CLASSY 2006, Document Understanding Confer-
ence 2006 http://duc.nist.gov/ 2006
Terry Copeck and Stan Szpakowicz 2004 Vocabulary
Agreement Among Model Summaries and Source
Documents In ACL Text Summarization Workshop,
ACL 2004.
Hoa Trang Dang Overview of DUC 2005 Document
Understanding Conference 2005 http://duc.nist.gov
Daniel M. Dunlavy and John M. Conroy and Judith
D. Schlesinger and Sarah A. Goodman and Mary
Ellen Okurowski and Dianne P. O’Leary and Hans
van Halteren, “Performance of a Three-Stage Sys-
tem for Multi-Document Summarization”, DUC 03
Conference Proceedings, http://duc.nist.gov/, 2003
Ted Dunning, “Accurate Methods for Statistics of Sur-
prise and Coincidence”, Computational Linguistics,
19:61-74, 1993.
Julian Kupiec,, Jan Pedersen, and Francine Chen. “A
Trainable Document Summarizer”. Proceedings of
the 18th Annual International SIGIR Conference
on Research and Development in Information Re-
trieval, pages 68–73, 1995.
Chin-Yew Lin and Eduard Hovy. The automated ac-
quisition of topic signatures for text summarization.
In Proceedings of the 18th conference on Computa-
tional linguistics, pages 495–501, Morristown, NJ,
USA, 2000. Association for Computational Linguis-
tics.
Chin-Yew Lin and Eduard Hovy. Manual and Auto-
matic Evaluation of Summaries In Document Un-
derstanding Conference 2002 http:/duc.nist.gov
Multi-Lingual Summarization Evaluation
http://www.isi.edu/ cyl/MTSE2005/MLSummEval.html
NLProcessor http://www.infogistics.com/posdemo.htm
Ani Nenkova and Lucy Vanderwende. 2005. The
Impact of Frequency on Summarization,MSR-TR-
2005-101. Microsoft Research Technical Report.
Kishore Papineni and Salim Roukos and Todd Ward
and Wei-Jing Zhu, Bleu: a method for automatic
evaluation of machine translation, Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, Thomas J. Watson Research Center (2001)
Simone Teufel and Hans van Halteren. 2004. 4:
Evaluating Information Content by Factoid Analy-
sis: Human Annotation and Stability, EMNLP-04,
Barcelona
</reference>
<page confidence="0.998794">
159
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.747662">
<title confidence="0.997719">Topic-Focused Multi-document Summarization Using an Approximate Oracle Score</title>
<author confidence="0.999984">John M Conroy</author>
<author confidence="0.999984">Judith D Schlesinger Dianne P O’Leary</author>
<affiliation confidence="0.999241">IDA Center for Computing Sciences University of Maryland</affiliation>
<address confidence="0.999845">Bowie, Maryland, USA College Park, Maryland, USA</address>
<email confidence="0.997931">conroy@super.org,judith@super.orgoleary@cs.umd.edu</email>
<abstract confidence="0.986296157894737">We consider the problem of producing a multi-document summary given a collection of documents. Since most successful methods of multi-document summarization are still largely extractive, in this paper, we explore just how well an extractive method can perform. We introduce an “oracle” score, based on the probability distribution of unigrams in human summaries. We then demonstrate that with the oracle score, we can generate extracts which score, on average, better than the human summaries, when evaluated with ROUGE. In addition, we introduce an approximation to the oracle score which produces a system with the best known performance for the 2005 Document Understanding Conference (DUC) evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Jamie Carbonnell</author>
<author>Jade Goldstein</author>
</authors>
<title>The of MMR, diversity-based reranking for reordering documents and producing summaries.”</title>
<booktitle>In Proc. ACM SIGIR,</booktitle>
<pages>335--336</pages>
<marker>Carbonnell, Goldstein, </marker>
<rawString>Jamie Carbonnell and Jade Goldstein “The of MMR, diversity-based reranking for reordering documents and producing summaries.” In Proc. ACM SIGIR, pages 335–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Dianne P O’Leary</author>
</authors>
<title>Text Summarization via Hidden Markov Models and Pivoted QR Matrix Decomposition”.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>University of Maryland, College Park,</institution>
<location>Maryland,</location>
<marker>Conroy, O’Leary, 2001</marker>
<rawString>John M. Conroy and Dianne P. O’Leary. “Text Summarization via Hidden Markov Models and Pivoted QR Matrix Decomposition”. Technical report, University of Maryland, College Park, Maryland, March, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>Jade Goldstein</author>
<author>Dianne P O’Leary</author>
</authors>
<date>2004</date>
<booktitle>LeftBrain Right-Brain Multi-Document Summarization, Document Understanding Conference</booktitle>
<note>http://duc.nist.gov/</note>
<marker>Conroy, Schlesinger, Goldstein, O’Leary, 2004</marker>
<rawString>John M. Conroy and Judith D. Schlesinger and Jade Goldstein and Dianne P. O’Leary, LeftBrain Right-Brain Multi-Document Summarization, Document Understanding Conference 2004 http://duc.nist.gov/ 2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>Jade Goldstein</author>
</authors>
<title>CLASSY Tasked Based Summarization: Back to Basics, Document Understanding Conference</title>
<date>2005</date>
<note>http://duc.nist.gov/</note>
<marker>Conroy, Schlesinger, Goldstein, 2005</marker>
<rawString>John M. Conroy and Judith D. Schlesinger and Jade Goldstein, CLASSY Tasked Based Summarization: Back to Basics, Document Understanding Conference 2005 http://duc.nist.gov/ 2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger Dianne P O’Leary</author>
<author>Jade Goldstein</author>
</authors>
<date>2006</date>
<booktitle>Back to Basciss: CLASSY 2006, Document Understanding Conference</booktitle>
<note>http://duc.nist.gov/</note>
<marker>Conroy, O’Leary, Goldstein, 2006</marker>
<rawString>John M. Conroy and Judith D. Schlesinger Dianne P. O’Leary, and Jade Goldstein, Back to Basciss: CLASSY 2006, Document Understanding Conference 2006 http://duc.nist.gov/ 2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Copeck</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Vocabulary Agreement Among Model Summaries and Source Documents In</title>
<date>2004</date>
<booktitle>ACL Text Summarization Workshop, ACL</booktitle>
<contexts>
<context position="1489" citStr="Copeck and Szpakowicz 2004" startWordPosition="220" endWordPosition="223">on to the oracle score which produces a system with the best known performance for the 2005 Document Understanding Conference (DUC) evaluation. 1 Introduction We consider the problem of producing a multidocument summary given a collection of documents. Most automatic methods of multidocument summarization are largely extractive. This mimics the behavior of humans for single document summarization; (Kupiec, Pendersen, and Chen 1995) reported that 79% of the sentences in a human-generated abstract were a “direct match” to a sentence in a document. In contrast, for multi-document summarization, (Copeck and Szpakowicz 2004) report that no more than 55% of the vocabulary contained in human-generated abstracts can be found in the given documents. Furthermore, multiple human summaries on the same collection of documents often have little agreement. For example, (Hovy and Lin 2002) report that unigram overlap is around 40%. (Teufel and van Halteren 2004) used a “factoid” agreement analysis of human summaries for a single document and concluded that a resulting consensus summary is stable only if 30–40 summaries are collected. In light of the strong evidence that nearly half of the terms in human-generated multi-docu</context>
</contexts>
<marker>Copeck, Szpakowicz, 2004</marker>
<rawString>Terry Copeck and Stan Szpakowicz 2004 Vocabulary Agreement Among Model Summaries and Source Documents In ACL Text Summarization Workshop, ACL 2004.</rawString>
</citation>
<citation valid="true">
<title>Hoa Trang Dang Overview of DUC</title>
<date>2005</date>
<booktitle>Document Understanding Conference</booktitle>
<note>http://duc.nist.gov</note>
<marker>2005</marker>
<rawString>Hoa Trang Dang Overview of DUC 2005 Document Understanding Conference 2005 http://duc.nist.gov</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Dunlavy</author>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>Sarah A Goodman</author>
<author>Mary Ellen Okurowski</author>
<author>Dianne P O’Leary</author>
<author>Hans van Halteren</author>
</authors>
<title>Performance of a Three-Stage System for Multi-Document Summarization”,</title>
<date>2003</date>
<booktitle>DUC 03 Conference Proceedings,</booktitle>
<location>http://duc.nist.gov/,</location>
<marker>Dunlavy, Conroy, Schlesinger, Goodman, Okurowski, O’Leary, van Halteren, 2003</marker>
<rawString>Daniel M. Dunlavy and John M. Conroy and Judith D. Schlesinger and Sarah A. Goodman and Mary Ellen Okurowski and Dianne P. O’Leary and Hans van Halteren, “Performance of a Three-Stage System for Multi-Document Summarization”, DUC 03 Conference Proceedings, http://duc.nist.gov/, 2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate Methods for Statistics of Surprise and Coincidence”,</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--61</pages>
<contexts>
<context position="14844" citStr="Dunning 1993" startWordPosition="2478" endWordPosition="2479">as query terms. Table 1 shows a list of query terms for our two illustrative topics. The number of query terms extracted in this way ranged from a low of 3 terms for document set d360f to 20 terms for document set d324e. 6.2 Signature Terms The second collection of terms we use to estimate P(t|T) are signature terms. Signature terms are the terms that are more likely to occur in the document set than in the background corpus. They are generally indicative of the content contained in the collection of documents. To identify these terms, we use the log-likelihood statistic suggested by Dunning (Dunning 1993) and first used in summarization by Lin and Hovy (Hovy and Lin 2000). The statistic is equivalent to a mutual information statistic and is based on a 2-by-2 contingency table of counts for each term. Table 2 shows a list of signature terms for our two illustrative topics. 6.3 An estimate of P(t|T) To estimate P(t|T), we view both the query terms and the signature terms as “samples” from idealized human summaries. They represent the terms that we would most likely see in a human summary. As such, we expect that these sample terms may approximate the underlying set of human summary terms. Given </context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning, “Accurate Methods for Statistics of Surprise and Coincidence”, Computational Linguistics, 19:61-74, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A Trainable Document Summarizer”.</title>
<date>1995</date>
<booktitle>Proceedings of the 18th Annual International SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>68--73</pages>
<marker>Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec,, Jan Pedersen, and Francine Chen. “A Trainable Document Summarizer”. Proceedings of the 18th Annual International SIGIR Conference on Research and Development in Information Retrieval, pages 68–73, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<pages>495--501</pages>
<location>Morristown, NJ, USA,</location>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th conference on Computational linguistics, pages 495–501, Morristown, NJ, USA, 2000. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Manual and Automatic Evaluation of Summaries In</title>
<booktitle>Document Understanding Conference 2002 http:/duc.nist.gov Multi-Lingual Summarization Evaluation http://www.isi.edu/ cyl/MTSE2005/MLSummEval.html NLProcessor http://www.infogistics.com/posdemo.htm</booktitle>
<marker>Lin, Hovy, </marker>
<rawString>Chin-Yew Lin and Eduard Hovy. Manual and Automatic Evaluation of Summaries In Document Understanding Conference 2002 http:/duc.nist.gov Multi-Lingual Summarization Evaluation http://www.isi.edu/ cyl/MTSE2005/MLSummEval.html NLProcessor http://www.infogistics.com/posdemo.htm</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
</authors>
<title>The Impact of Frequency on Summarization,MSR-TR2005-101. Microsoft Research</title>
<date>2005</date>
<tech>Technical Report.</tech>
<contexts>
<context position="5479" citStr="Nenkova and Vanderwende, 2005" startWordPosition="873" endWordPosition="876">ire documents in the DUC 05 data have markers indicating the regions of the document, including titles, bylines, and text portions. All of the extracted sentences in this study are taken from the text portions of the documents only. We define a “term” to be any “non-stop word.” Our stop list contains the 400 most frequently occurring English words. 3 The Oracle Score Recently, a crisp analysis of the frequency of content words used by humans relative to the high frequency content words that occur in the relevant documents has yielded a simple and powerful summarization method called SumBasic (Nenkova and Vanderwende, 2005). SumBasic produced extract summaries which performed nearly as well as the best machine systems for generic 100 word summaries, as evaluated in DUC 2003 and 2004, as well as the Multi-lingual Summarization Evaluation (MSE 2005). Instead of using term frequencies of the corpus to infer highly likely terms in human summaries, we propose to directly model the set of terms (vocabulary) that is likely to occur in a sample of human summaries. We seek to estimate the probability that a term will be used by a human summarizer to first get an estimate of the best possible extract and later to produce </context>
<context position="24519" citStr="Nenkova and Vanderwende, 2005" startWordPosition="4049" endWordPosition="4052">8 0.10501 0.13351 G 0.11324 0.10195 0.12366 F 0.10893 0.09310 0.12780 H 0.10777 0.09833 0.11746 J 0.10717 0.09293 0.12460 I 0.10634 0.09632 0.11628 E 0.10365 0.08935 0.11926 A 0.10361 0.09260 0.11617 24 0.09558 0.09144 0.09977 ω(pr) 0.09160 0.08729 0.09570 qs 15 0.09097 0.08671 0.09478 12 0.08987 0.08583 0.09385 8 0.08954 0.08540 0.09338 23 0.08792 0.08371 0.09204 ω(p) qs 0.08738 0.08335 0.09145 ωqs 0.08713 0.08317 0.09110 28 0.08700 0.08332 0.09096 Table 3: Average ROUGE 2 Scores for DUC06: Humans A-I ROUGE-2) scores with stop words removed for comparison with the published results given in (Nenkova and Vanderwende, 2005). Table 4 gives these scores for the top performing systems at DUC04 as well as SumBasic and wqs , the approximate oracle based on signature �pr) terms alone with linguistic preprocess trimming and pivot QR for redundancy removal. As displayed, w�pr) qs scored second highest and within the 95% confidence intervals of the top system, peer 65, as well as SumBasic, and peer 34. Submission Mean 95% CI Lower 95% CI Upper F 0.36787 0.34442 0.39467 B 0.36126 0.33387 0.38754 O (ω) 0.35810 0.34263 0.37330 H 0.33871 0.31540 0.36423 A 0.33289 0.30591 0.35759 D 0.33212 0.30805 0.35628 E 0.33277 0.30959 0.</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>Ani Nenkova and Lucy Vanderwende. 2005. The Impact of Frequency on Summarization,MSR-TR2005-101. Microsoft Research Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation,</title>
<date>2001</date>
<journal>IBM Research Division, Thomas J. Watson Research</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<location>Center</location>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni and Salim Roukos and Todd Ward and Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center (2001)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Hans van Halteren</author>
</authors>
<title>4: Evaluating Information Content by Factoid Analysis: Human Annotation and Stability, EMNLP-04,</title>
<date>2004</date>
<location>Barcelona</location>
<marker>Teufel, van Halteren, 2004</marker>
<rawString>Simone Teufel and Hans van Halteren. 2004. 4: Evaluating Information Content by Factoid Analysis: Human Annotation and Stability, EMNLP-04, Barcelona</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>