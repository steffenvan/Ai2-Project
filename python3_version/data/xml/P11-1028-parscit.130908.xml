<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.834082">
Learning to Win by Reading Manuals in a Monte-Carlo Framework
</title>
<author confidence="0.984903">
S.R.K. Branavan David Silver * Regina Barzilay
</author>
<affiliation confidence="0.9967625">
Computer Science and Artificial Intelligence Laboratory * Department of Computer Science
Massachusetts Institute of Technology University College London
</affiliation>
<email confidence="0.99774">
{branavan, regina}@csail.mit.edu d.silver@cs.ucl.ac.uk
</email>
<sectionHeader confidence="0.997337" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999775571428571">
This paper presents a novel approach for lever-
aging automatically extracted textual knowl-
edge to improve the performance of control
applications such as games. Our ultimate goal
is to enrich a stochastic player with high-
level guidance expressed in text. Our model
jointly learns to identify text that is relevant
to a given game state in addition to learn-
ing game strategies guided by the selected
text. Our method operates in the Monte-Carlo
search framework, and learns both text anal-
ysis and game strategies based only on envi-
ronment feedback. We apply our approach to
the complex strategy game Civilization II us-
ing the official game manual as the text guide.
Our results show that a linguistically-informed
game-playing agent significantly outperforms
its language-unaware counterpart, yielding a
27% absolute improvement and winning over
78% of games when playing against the built-
in AI of Civilization II. 1
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935083333333">
In this paper, we study the task of grounding lin-
guistic analysis in control applications such as com-
puter games. In these applications, an agent attempts
to optimize a utility function (e.g., game score) by
learning to select situation-appropriate actions. In
complex domains, finding a winning strategy is chal-
lenging even for humans. Therefore, human players
typically rely on manuals and guides that describe
promising tactics and provide general advice about
the underlying task. Surprisingly, such textual infor-
mation has never been utilized in control algorithms
despite its potential to greatly improve performance.
</bodyText>
<footnote confidence="0.9493045">
1The code, data and complete experimental setup for this
work are available at http://groups.csail.mit.edu/rbg/code/civ.
</footnote>
<figure confidence="0.3483385">
The natural resources available where a population
settles affects its ability to produce food and goods.
Build your city on a plains or grassland square with
a river running through it if possible.
</figure>
<figureCaption confidence="0.985729">
Figure 1: An excerpt from the user manual of the game
Civilization II.
</figureCaption>
<bodyText confidence="0.9999616">
Consider for instance the text shown in Figure 1.
This is an excerpt from the user manual of the game
Civilization II.2 This text describes game locations
where the action “build-city” can be effectively ap-
plied. A stochastic player that does not have access
to this text would have to gain this knowledge the
hard way: it would repeatedly attempt this action in
a myriad of states, thereby learning the characteri-
zation of promising state-action pairs based on the
observed game outcomes. In games with large state
spaces, long planning horizons, and high-branching
factors, this approach can be prohibitively slow and
ineffective. An algorithm with access to the text,
however, could learn correlations between words in
the text and game attributes – e.g., the word “river”
and places with rivers in the game – thus leveraging
strategies described in text to better select actions.
The key technical challenge in leveraging textual
knowledge is to automatically extract relevant infor-
mation from text and incorporate it effectively into a
control algorithm. Approaching this task in a super-
vised framework, as is common in traditional infor-
mation extraction, is inherently difficult. Since the
game’s state space is extremely large, and the states
that will be encountered during game play cannot be
known a priori, it is impractical to manually anno-
tate the information that would be relevant to those
states. Instead, we propose to learn text analysis
based on a feedback signal inherent to the control
application, such as game score.
</bodyText>
<footnote confidence="0.994854">
2http://en.wikipedia.org/wiki/Civilization II
</footnote>
<page confidence="0.842976">
268
</page>
<note confidence="0.980894">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 268–277,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999764711111111">
Our general setup consists of a game in a stochas-
tic environment, where the goal of the player is to
maximize a given utility function R(s) at state s.
We follow a common formulation that has been the
basis of several successful applications of machine
learning to games. The player’s behavior is deter-
mined by an action-value function Q(s, a) that as-
sesses the goodness of an action a in a given state
s based on the features of s and a. This function is
learned based solely on the utility R(s) collected via
simulated game-play in a Monte-Carlo framework.
An obvious way to enrich the model with textual
information is to augment the action-value function
with word features in addition to state and action
features. However, adding all the words in the docu-
ment is unlikely to help since only a small fraction of
the text is relevant for a given state. Moreover, even
when the relevant sentence is known, the mapping
between raw text and the action-state representation
may not be apparent. This representation gap can
be bridged by inducing a predicate structure on the
sentence—e.g., by identifying words that describe
actions, and those that describe state attributes.
In this paper, we propose a method for learning an
action-value function augmented with linguistic fea-
tures, while simultaneously modeling sentence rele-
vance and predicate structure. We employ a multi-
layer neural network where the hidden layers rep-
resent sentence relevance and predicate parsing de-
cisions. Despite the added complexity, all the pa-
rameters of this non-linear model can be effectively
learned via Monte-Carlo simulations.
We test our method on the strategy game Civiliza-
tion II, a notoriously challenging game with an im-
mense action space.3 As a source of knowledge for
guiding our model, we use the official game man-
ual. As a baseline, we employ a similar Monte-
Carlo search based player which does not have ac-
cess to textual information. We demonstrate that the
linguistically-informed player significantly outper-
forms the baseline in terms of number of games won.
Moreover, we show that modeling the deeper lin-
guistic structure of sentences further improves per-
formance. In full-length games, our algorithm yields
a 27% improvement over a language unaware base-
</bodyText>
<footnote confidence="0.826293">
3Civilization II was #3 in IGN’s 2007 list of top video games
of all time (http://top100.ign.com/2007/ign top game 3.html)
</footnote>
<bodyText confidence="0.982532">
line, and wins over 78% of games against the built-
in, hand-crafted AI of Civilization II.4
</bodyText>
<sectionHeader confidence="0.999779" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999963825">
Our work fits into the broad area of grounded lan-
guage acquisition where the goal is to learn linguis-
tic analysis from a situated context (Oates, 2001;
Siskind, 2001; Yu and Ballard, 2004; Fleischman
and Roy, 2005; Mooney, 2008a; Mooney, 2008b;
Branavan et al., 2009; Vogel and Jurafsky, 2010).
Within this line of work, we are most closely related
to reinforcement learning approaches that learn lan-
guage by proactively interacting with an external en-
vironment (Branavan et al., 2009; Branavan et al.,
2010; Vogel and Jurafsky, 2010). Like the above
models, we use environment feedback (in the form
of a utility function) as the main source of supervi-
sion. The key difference, however, is in the language
interpretation task itself. Previous work has focused
on the interpretation of instruction text where input
documents specify a set of actions to be executed in
the environment. In contrast, game manuals provide
high-level advice but do not directly describe the
correct actions for every potential game state. More-
over, these documents are long, and use rich vocabu-
laries with complex grammatical constructions. We
do not aim to perform a comprehensive interpreta-
tion of such documents. Rather, our focus is on lan-
guage analysis that is sufficiently detailed to help the
underlying control task.
The area of language analysis situated in a game
domain has been studied in the past (Eisenstein et
al., 2009). Their method, however, is different both
in terms of the target interpretation task, and the su-
pervision signal it learns from. They aim to learn
the rules of a given game, such as which moves are
valid, given documents describing the rules. Our
goal is more open ended, in that we aim to learn
winning game strategies. Furthermore, Eisenstein et
al. (2009) rely on a different source of supervision –
game traces collected a priori. For complex games,
like the one considered in this paper, collecting such
game traces is prohibitively expensive. Therefore
our approach learns by actively playing the game.
</bodyText>
<footnote confidence="0.779970333333333">
4In this paper, we focus primarily on the linguistic aspects
of our task and algorithm. For a discussion and evaluation of
the non-linguistic aspects please see Branavan et al. (2011).
</footnote>
<page confidence="0.998654">
269
</page>
<sectionHeader confidence="0.9156585" genericHeader="method">
3 Monte-Carlo Framework for Computer
Games
</sectionHeader>
<bodyText confidence="0.998509414634146">
Our method operates within the Monte-Carlo search
framework (Tesauro and Galperin, 1996), which
has been successfully applied to complex computer
games such as Go, Poker, Scrabble, multi-player
card games, and real-time strategy games, among
others (Gelly et al., 2006; Tesauro and Galperin,
1996; Billings et al., 1999; Sheppard, 2002; Sch¨afer,
2008; Sturtevant, 2008; Balla and Fern, 2009).
Since Monte-Carlo search forms the foundation of
our approach, we briefly describe it in this section.
Game Representation The game is defined by a
large Markov Decision Process (S, A, T, R). Here
S is the set of possible states, A is the space of legal
actions, and T (s&apos;|s, a) is a stochastic state transition
function where s, s&apos; E S and a E A. Specifically, a
state encodes attributes of the game world, such as
available resources and city locations. At each step
of the game, a player executes an action a which
causes the current state s to change to a new state
s&apos; according to the transition function T(s&apos;|s, a).
While this function is not known a priori, the pro-
gram encoding the game can be viewed as a black
box from which transitions can be sampled. Finally,
a given utility function R(s) E R captures the like-
lihood of winning the game from state s (e.g., an
intermediate game score).
Monte-Carlo Search Algorithm The goal of the
Monte-Carlo search algorithm is to dynamically se-
lect the best action for the current state st. This se-
lection is based on the results of multiple roll-outs
which measure the outcome of a sequence of ac-
tions in a simulated game – e.g., simulations played
against the game’s built-in AI. Specifically, starting
at state st, the algorithm repeatedly selects and exe-
cutes actions, sampling state transitions from T. On
game completion at time T, we measure the final
utility R(s,).5 The actual game action is then se-
lected as the one corresponding to the roll-out with
the best final utility. See Algorithm 1 for details.
The success of Monte-Carlo search is based on
its ability to make a fast, local estimate of the ac-
</bodyText>
<footnote confidence="0.956303666666667">
5In general, roll-outs are run till game completion. However,
if simulations are expensive as is the case in our domain, roll-
outs can be truncated after a fixed number of steps.
</footnote>
<figure confidence="0.195038548387097">
procedure PlayGame ()
Initialize game state to fixed starting state
s1 +— s0
fort= 1 ... T do
Run N simulated games
for i = 1 ... N do
(ai, ri) +— SimulateGame(s)
end
Compute average observed utility for each action
Na
.V ri
z:a;=a
Execute selected action in game
st+1 +— T(s&apos;|st, at)
end
procedure SimulateGame (st)
for u = t ... Tdo
Compute Q function approximation
Q(s, a) = w � ��(s, a)
Sample action from action-value function in
e-greedy fashion:
r uniform(a E A) with probability e
au S l arg max Q (s, a) otherwise
a
Execute selected action in game:
su+1 +— T (s&apos;|su, au)
if game is won or lost break
end
Update parameters w of Q(s, a)
Return action and observed utility:
return at, R(sτ)
</figure>
<figureCaption confidence="0.333465">
Algorithm 1: The general Monte-Carlo algorithm.
</figureCaption>
<bodyText confidence="0.999889090909091">
tion quality at each step of the roll-outs. States
and actions are evaluated by an action-value func-
tion Q(s, a), which is an estimate of the expected
outcome of action a in state s. This action-value
function is used to guide action selection during the
roll-outs. While actions are usually selected to max-
imize the action-value function, sometimes other ac-
tions are also randomly explored in case they are
more valuable than predicted by the current estimate
of Q(s, a). As the accuracy of Q(s, a) improves,
the quality of action selection improves and vice
</bodyText>
<figure confidence="0.7798">
at +— arg max
a
1
</figure>
<page confidence="0.975573">
270
</page>
<bodyText confidence="0.982281806451613">
versa, in a cycle of continual improvement (Sutton
and Barto, 1998).
In many games, it is sufficient to maintain a dis-
tinct action-value for each unique state and action
in a large search tree. However, when the branch-
ing factor is large it is usually beneficial to approx-
imate the action-value function, so that the value
of many related states and actions can be learned
from a reasonably small number of simulations (Sil-
ver, 2009). One successful approach is to model
the action-value function as a linear combination of
state and action attributes (Silver et al., 2008):
Q(s, a) = w · f(s, a).
Here f(s, a) E R&apos; is a real-valued feature function,
and w is a weight vector. We take a similar approach
here, except that our feature function includes latent
structure which models language.
The parameters w of Q(s, a) are learned based on
feedback from the roll-out simulations. Specifically,
the parameters are updated by stochastic gradient
descent by comparing the current predicted Q(s, a)
against the observed utility at the end of each roll-
out. We provide details on parameter estimation in
the context of our model in Section 4.2.
The roll-outs themselves are fully guided by the
action-value function. At every step of the simula-
tion, actions are selected by an c-greedy strategy:
with probability c an action is selected uniformly
at random; otherwise the action is selected greed-
ily to maximize the current action-value function,
argmaxa Q(s, a).
</bodyText>
<sectionHeader confidence="0.8691885" genericHeader="method">
4 Adding Linguistic Knowledge to the
Monte-Carlo Framework
</sectionHeader>
<bodyText confidence="0.99997275">
In this section we describe how we inform the
simulation-based player with information automat-
ically extracted from text – in terms of both model
structure and parameter estimation.
</bodyText>
<subsectionHeader confidence="0.999866">
4.1 Model Structure
</subsectionHeader>
<bodyText confidence="0.998785">
To inform action selection with the advice provided
in game manuals, we modify the action-value func-
tion Q(s, a) to take into account words of the doc-
ument in addition to state and action information.
Conditioning Q(s, a) on all the words in the docu-
ment is unlikely to be effective since only a small
</bodyText>
<figureCaption confidence="0.959762">
Figure 2: The structure of our model. Each rectan-
</figureCaption>
<bodyText confidence="0.990004125">
gle represents a collection of units in a layer, and the
shaded trapezoids show the connections between layers.
A fixed, real-valued feature function x(s, a, d) transforms
the game state s, action a, and strategy document d into
the input vector Y. The first hidden layer contains two
disjoint sets of units y� and z� corresponding to linguis-
tic analyzes of the strategy document. These are softmax
layers, where only one unit is active at any time. The
units of the second hidden layer f(s, a, d, yi, zi) are a set
of fixed real valued feature functions on s, a, d and the
active units yi and zi of y� and z� respectively.
fraction of the document provides guidance relevant
to the current state, while the remainder of the text
is likely to be irrelevant. Since this information is
not known a priori, we model the decision about a
sentence’s relevance to the current state as a hid-
den variable. Moreover, to fully utilize the infor-
mation presented in a sentence, the model identifies
the words that describe actions and those that de-
scribe state attributes, discriminating them from the
rest of the sentence. As with the relevance decision,
we model this labeling using hidden variables.
As shown in Figure 2, our model is a four layer
neural network. The input layer x represents the
current state s, candidate action a, and document
d. The second layer consists of two disjoint sets of
units y� and z� which encode the sentence-relevance
and predicate-labeling decisions respectively. Each
of these sets of units operates as a stochastic 1-of-n
softmax selection layer (Bridle, 1990) where only a
single unit is activated. The activation function for
units in this layer is the standard softmax function:
</bodyText>
<equation confidence="0.886629">
Ayi = 1|x) = e
</equation>
<bodyText confidence="0.97845">
where yi is the ith hidden unit of y, and ui is the
weight vector corresponding to yi. Given this acti-
</bodyText>
<figure confidence="0.995198076923077">
Input layer: Deterministic feature
layer:
Hidden layer encoding
sentence relevance
Hidden layer encoding
predicate labeling
Output layer
gi·g /
k
��
9k·
,
e
</figure>
<page confidence="0.986728">
271
</page>
<bodyText confidence="0.999974722222223">
Here ej is the predicate label of the jth word being
labeled, and e1:j−1 is the partial predicate labeling
constructed so far for sentence yi.
In the second layer of the neural network, the
units z� represent a predicate labeling ei of every sen-
tence yi ∈ d. However, our intention is to incorpo-
rate, into action-value function Q, information from
only the most relevant sentence. Thus, in practice,
we only perform a predicate labeling of the sentence
selected by the relevance component of the model.
Given the sentence selected as relevant and its
predicate labeling, the output layer of the network
can now explicitly learn the correlations between
textual information, and game states and actions –
for example, between the word “grassland” in Fig-
ure 1, and the action of building a city. This allows
our method to leverage the automatically extracted
textual information to improve game play.
</bodyText>
<subsectionHeader confidence="0.94035">
4.2 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999933476190476">
Learning in our method is performed in an online
fashion: at each game state st, the algorithm per-
forms a simulated game roll-out, observes the out-
come of the game, and updates the parameters i,
V and w of the action-value function Q(st, at, d).
These three steps are repeated a fixed number of
times at each actual game state. The information
from these roll-outs is used to select the actual game
action. The algorithm re-learns Q(st, at, d) for ev-
ery new game state st. This specializes the action-
value function to the subgame starting from st.
Since our model is a non-linear approximation of
the underlying action-value function of the game,
we learn model parameters by applying non-linear
regression to the observed final utilities from the
simulated roll-outs. Specifically, we adjust the pa-
rameters by stochastic gradient descent, to mini-
mize the mean-squared error between the action-
value Q(s, a) and the final utility R(s,) for each
observed game state s and action a. The resulting
update to model parameters B is of the form:
</bodyText>
<equation confidence="0.940761">
α
OB = −� ∇o [R(s,) − Q(s, a)]2
= α [R(s,) − Q(s, a)] ∇oQ(s, a; B),
</equation>
<bodyText confidence="0.988297352941177">
where α is a learning rate parameter.
This minimization is performed via standard error
backpropagation (Bryson and Ho, 1969; Rumelhart
vation function, the second layer effectively models
sentence relevance and predicate labeling decisions
via log-linear distributions, the details of which are
described below.
�
The third feature layer f of the neural network is
deterministically computed given the active units yi
and zj of the softmax layers, and the values of the
input layer. Each unit in this layer corresponds to
a fixed feature function fk(st, at, d, yi, zj) ∈ R. Fi-
nally the output layer encodes the action-value func-
tion Q(s, a, d), which now also depends on the doc-
ument d, as a weighted linear combination of the
units of the feature layer:
</bodyText>
<equation confidence="0.831182">
Q(st, at, d) = w · �f,
</equation>
<bodyText confidence="0.9715825">
where w is the weight vector.
Modeling Sentence Relevance Given a strategy
document d, we wish to identify a sentence yi that
is most relevant to the current game state st and ac-
tion at. This relevance decision is modeled as a log-
linear distribution over sentences as follows:
</bodyText>
<equation confidence="0.581595">
p(yi|st, at, d) ∝ e 9&apos;0(yi,st,at,d)
</equation>
<bodyText confidence="0.999951631578948">
Here 0(yi, st, at, d) ∈ R&apos; is a feature function, and
i are the parameters we need to estimate.
Modeling Predicate Structure Our goal here is
to label the words of a sentence as either action-
description, state-description or background. Since
these word label assignments are likely to be mu-
tually dependent, we model predicate labeling as a
sequence prediction task. These dependencies do
not necessarily follow the order of words in a sen-
tence, and are best expressed in terms of a syn-
tactic tree. For example, words corresponding to
state-description tend to be descendants of action-
description words. Therefore, we label words in de-
pendency order — i.e., starting at the root of a given
dependency tree, and proceeding to the leaves. This
allows a word’s label decision to condition on the
label of the corresponding dependency tree parent.
Given sentence yi and its dependency parse qi, we
model the distribution over predicate labels ei as:
</bodyText>
<equation confidence="0.935092">
p(�ei |yi, qi) = � p(ej|j,�e1:j−1, yi, qi),
j
p(ej|j,e1:j−1, yi, qi) ∝ e v&apos;&apos;P(ej,j,e1:j−1,yi,qi)�
</equation>
<page confidence="0.983263">
272
</page>
<bodyText confidence="0.977868">
et al., 1986), which results in the following online
updates for the output layer parameters w:
</bodyText>
<equation confidence="0.732197">
w +_ w + a,,, [Q − R(s,)] �f(s, a, d, yz, zj),
</equation>
<bodyText confidence="0.999805333333333">
where a,,, is the learning rate, and Q = Q(s, a, d).
The corresponding updates for the sentence rele-
vance and predicate labeling parameters i and v are:
</bodyText>
<equation confidence="0.393547">
uz +_ iiz + a. [Q − R(s,)] Q x [1 − p(yz|&apos;)],
vz +_ vz + a„ [Q − R(s,)] Q x [1 − p(zz|&apos;)]-
</equation>
<sectionHeader confidence="0.995956" genericHeader="method">
5 Applying the Model
</sectionHeader>
<bodyText confidence="0.99952203030303">
We apply our model to playing the turn-based strat-
egy game, Civilization II. We use the official man-
ual 6 of the game as the source of textual strategy
advice for the language aware algorithms.
Civilization II is a multi-player game set on a grid-
based map of the world. Each grid location repre-
sents a tile of either land or sea, and has various
resources and terrain attributes. For example, land
tiles can have hills with rivers running through them.
In addition to multiple cities, each player controls
various units – e.g., settlers and explorers. Games
are won by gaining control of the entire world map.
In our experiments, we consider a two-player game
of Civilization II on a grid of 1000 squares, where
we play against the built-in AI player.
Game States and Actions We define the game state
of Civilization II to be the map of the world, the at-
tributes of each map tile, and the attributes of each
player’s cities and units. Some examples of the at-
tributes of states and actions are shown in Figure 3.
The space of possible actions for a given city or unit
is known given the current game state. The actions
of a player’s cities and units combine to form the ac-
tion space of that player. In our experiments, on av-
erage a player controls approximately 18 units, and
each unit can take one of 15 actions. This results in
a very large action space for the game – i.e., 1021.
To effectively deal with this large action space, we
assume that given the state, the actions of a single
unit are independent of the actions of all other units
of the same player.
Utility Function The Monte-Carlo algorithm uses
the utility function to evaluate the outcomes of
</bodyText>
<footnote confidence="0.938371">
6www.civfanatics.com/content/civ2/reference/Civ2manual.zip
</footnote>
<figureCaption confidence="0.999605333333333">
Figure 3: Example attributes of the game (box above),
and features computed using the game manual and these
attributes (box below).
</figureCaption>
<bodyText confidence="0.992933730769231">
simulated game roll-outs. In the typical application
of the algorithm, the final game outcome is used as
the utility function (Tesauro and Galperin, 1996).
Given the complexity of Civilization II, running sim-
ulation roll-outs until game completion is impracti-
cal. The game, however, provides each player with a
game score, which is a noisy indication of how well
they are currently playing. Since we are playing a
two-player game, we use the ratio of the game score
of the two players as our utility function.
Features The sentence relevance features and the
�
action-value function features f consider the at-
tributes of the game state and action, and the words
of the sentence. Some of these features compute text
overlap between the words of the sentence, and text
labels present in the game. The feature function 0
used for predicate labeling on the other hand oper-
ates only on a given sentence and its dependency
parse. It computes features which are the Carte-
sian product of the candidate predicate label with
word attributes such as type, part-of-speech tag, and
dependency parse information. Overall, f, �� and
0 compute approximately 306,800, 158,500, and
7,900 features respectively. Figure 3 shows some
examples of these features.
</bodyText>
<figure confidence="0.887527409090909">
1 if action=build-city
&amp; tile-has-river=true
&amp; action-words={build,city}
&amp; state-words={river,hill}
0 otherwise
1 if action=build-city
&amp; tile-has-river=true
&amp; words={build,city,river}
0 otherwise
1 if label=action
&amp; word-type=&apos;build&apos;
&amp; parent-label=action
0 otherwise
Map tile attributes:
- Terrain type (e.g. grassland, mountain, etc)
- Tile resources (e.g. wheat, coal, wildlife, etc)
City attributes:
- City population
- Amount of food produced
Unit attributes:
- Unit type (e.g., worker, explorer, archer, etc)
- Is unit in a city ?
</figure>
<page confidence="0.998271">
273
</page>
<sectionHeader confidence="0.999102" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999986477272727">
Datasets We use the official game manual for Civi-
lization II as our strategy guide. This manual uses a
large vocabulary of 3638 words, and is composed of
2083 sentences, each on average 16.9 words long.
Experimental Framework To apply our method to
the Civilization II game, we use the game’s open
source implementation Freeciv.7 We instrument the
game to allow our method to programmatically mea-
sure the current state of the game and to execute
game actions. The Stanford parser (de Marneffe et
al., 2006) was used to generate the dependency parse
information for sentences in the game manual.
Across all experiments, we start the game at the
same initial state and run it for 100 steps. At each
step, we perform 500 Monte-Carlo roll-outs. Each
roll-out is run for 20 simulated game steps before
halting the simulation and evaluating the outcome.
For our method, and for each of the baselines, we
run 200 independent games in the above manner,
with evaluations averaged across the 200 runs. We
use the same experimental settings across all meth-
ods, and all model parameters are initialized to zero.
The test environment consisted of typical PCs
with single Intel Core i7 CPUs (4 hyper-threaded
cores each), with the algorithms executing 8 simula-
tion roll-outs in parallel. In this setup, a single game
of 100 steps runs in approximately 1.5 hours.
Evaluation Metrics We wish to evaluate two as-
pects of our method: how well it leverages tex-
tual information to improve game play, and the ac-
curacy of the linguistic analysis it produces. We
evaluate the first aspect by comparing our method
against various baselines in terms of the percent-
age of games won against the built-in AI of Freeciv.
This AI is a fixed algorithm designed using exten-
sive knowledge of the game, with the intention of
challenging human players. As such, it provides a
good open-reference baseline. Since full games can
last for multiple days, we compute the percentage of
games won within the first 100 game steps as our pri-
mary evaluation. To confirm that performance under
this evaluation is meaningful, we also compute the
percentage of full games won over 50 independent
runs, where each game is run to completion.
</bodyText>
<footnote confidence="0.981579">
7http://freeciv.wikia.com. Game version 2.2
</footnote>
<table confidence="0.9876835">
Method % Win % Loss Std. Err.
Random 0 100 —
Built-in AI 0 0 —
Game only 17.3 5.3 f 2.7
Sentence relevance 46.7 2.8 f 3.5
Full model 53.7 5.9 f 3.5
Random text 40.3 4.3 f 3.4
Latent variable 26.1 3.7 f 3.1
</table>
<tableCaption confidence="0.947912857142857">
Table 1: Win rate of our method and several baselines
within the first 100 game steps, while playing against the
built-in game AI. Games that are neither won nor lost are
still ongoing. Our model’s win rate is statistically signif-
icant against all baselines except sentence relevance. All
results are averaged across 200 independent game runs.
The standard errors shown are for percentage wins.
</tableCaption>
<table confidence="0.99971125">
Method % Wins Standard Error
Game only 45.7 f 7.0
Latent variable 62.2 f 6.9
Full model 78.8 f 5.8
</table>
<tableCaption confidence="0.997957">
Table 2: Win rate of our method and two baselines on 50
full length games played against the built-in AI.
</tableCaption>
<sectionHeader confidence="0.999521" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.9999115">
Game performance As shown in Table 1, our lan-
guage aware Monte-Carlo algorithm substantially
outperforms several baselines – on average winning
53.7% of all games within the first 100 steps. The
dismal performance, on the other hand, of both the
random baseline and the game’s own built-in AI
(playing against itself) is an indicator of the diffi-
culty of the task. This evaluation is an underesti-
mate since it assumes that any game not won within
the first 100 steps is a loss. As shown in Table 2, our
method wins over 78% of full length games.
To characterize the contribution of the language
components to our model’s performance, we com-
pare our method against two ablative baselines. The
first of these, game-only, does not take advantage
of any textual information. It attempts to model the
action value function Q(s, a) only in terms of the
attributes of the game state and action. The per-
formance of this baseline – a win rate of 17.3% –
effectively confirms the benefit of automatically ex-
tracted textual information in the context of our task.
The second ablative baseline, sentence-relevance, is
</bodyText>
<page confidence="0.992649">
274
</page>
<figure confidence="0.997145333333333">
Sentence relevance
Moving average
20 40 60 80 100
Game step
0.8
0.6
0.4
0.2
0
1
S S S
A
A A
You can rename the city if you like, but we&apos;ll refer to it as washington.
There are many different strategies dictating the order in which
advances are researched
Phalanxes are twice as effective at defending cities as warriors.
Build the city on plains or grassland with a river running through it.
After the road is built, use the settlers to start improving the terrain.
S S S A A A A A
When the settlers becomes active, chose build road.
Use settlers or engineers to improve a terrain square within the city radius
A S✘ A A S A✘ S S S S
Sentence relevance accuracy
</figure>
<figureCaption confidence="0.9618845">
Figure 4: Examples of our method’s sentence relevance
and predicate labeling decisions. The box above shows
</figureCaption>
<bodyText confidence="0.953872444444445">
two sentences (identified by check marks) which were
predicted as relevant, and two which were not. The box
below shows the predicted predicate structure of three
sentences, with “S” indicating state description,“A” ac-
tion description and background words unmarked. Mis-
takes are identified with crosses.
identical to our model, but lacks the predicate label-
ing component. This method wins 46.7% of games,
showing that while identifying the text relevant to
the current game state is essential, a deeper struc-
tural analysis of the extracted text provides substan-
tial benefits.
One possible explanation for the improved perfor-
mance of our method is that the non-linear approx-
imation simply models game characteristics better,
rather than modeling textual information. We di-
rectly test this possibility with two additional base-
lines. The first, random-text, is identical to our full
model, but is given a document containing random
text. We generate this text by randomly permut-
ing the word locations of the actual game manual,
thereby maintaining the document’s overall statisti-
cal properties. The second baseline, latent variable,
extends the linear action-value function Q(s, a) of
the game only baseline with a set of latent variables
– i.e., it is a four layer neural network, where the sec-
ond layer’s units are activated only based on game
information. As shown in Table 1 both of these base-
lines significantly underperform with respect to our
model, confirming the benefit of automatically ex-
tracted textual information in the context of this task.
Sentence Relevance Figure 4 shows examples of
the sentence relevance decisions produced by our
method. To evaluate the accuracy of these decisions,
we ideally require a ground-truth relevance annota-
tion of the game’s user manual. This however, is
</bodyText>
<figureCaption confidence="0.98978">
Figure 5: Accuracy of our method’s sentence relevance
predictions, averaged over 100 independent runs.
</figureCaption>
<bodyText confidence="0.999974454545454">
impractical since the relevance decision is depen-
dent on the game context, and is hence specific to
each time step of each game instance. Therefore, for
the purposes of this evaluation, we modify the game
manual by adding to it sentences randomly selected
from the Wall Street Journal corpus (Marcus et al.,
1993) – sentences that are highly unlikely to be rel-
evant to game play. We then evaluate the accuracy
with which sentences from the original manual are
picked as relevant.
In this evaluation, our method achieves an average
accuracy of 71.8%. Given that our model only has to
differentiate between the game manual text and the
Wall Street Journal, this number may seem disap-
pointing. Furthermore, as can be seen from Figure 5,
the sentence relevance accuracy varies widely as the
game progresses, with a high average of 94.2% dur-
ing the initial 25 game steps.
In reality, this pattern of high initial accuracy fol-
lowed by a lower average is not entirely surprising:
the official game manual for Civilization II is writ-
ten for first time players. As such, it focuses on the
initial portion of the game, providing little strategy
advice relevant to subsequence game play.8 If this is
the reason for the observed sentence relevance trend,
we would also expect the final layer of the neural
network to emphasize game features over text fea-
tures after the first 25 steps of the game. This is
indeed the case, as can be seen from Figure 6.
To further test this hypothesis, we perform an ex-
periment where the first 50 steps of the game are
played using our full model, and the subsequent 50
steps are played without using any textual informa-
</bodyText>
<footnote confidence="0.990343">
8This is reminiscent of opening books for games like Chess
or Go, which aim to guide the player to a playable middle game.
</footnote>
<page confidence="0.995031">
275
</page>
<figure confidence="0.998697363636364">
20 40 60 80
Game step
Text features
dominate
Game features
dominate
1.5
1
Text feature importance
0.5
0
</figure>
<table confidence="0.9868795">
Method S/A/B S/A
Random labeling 33.3% 50.0%
Model, first 100 steps 45.1% 78.9%
Model, first 25 steps 48.0% 92.7%
</table>
<tableCaption confidence="0.9727978">
Table 3: Predicate labeling accuracy of our method and a
random baseline. Column “S/A/B” shows performance
on the three-way labeling of words as state, action or
background, while column “S/A” shows accuracy on the
task of differentiating between state and action words.
</tableCaption>
<figureCaption confidence="0.83679825">
Figure 6: Difference between the norms of the text fea-
tures and game features of the output layer of the neural
network. Beyond the initial 25 steps of the game, our
method relies increasingly on game features.
</figureCaption>
<figure confidence="0.8783545">
game attribute
state: grassland
state: grassland
action: settlers_build_city
action: set_research
word
&amp;quot;city&amp;quot;
&amp;quot;build&amp;quot;
&amp;quot;city&amp;quot;
&amp;quot;discovery&amp;quot;
</figure>
<bodyText confidence="0.999751303030303">
tion. This hybrid method performs as well as our
full model, achieving a 53.3% win rate, confirm-
ing that textual information is most useful during
the initial phase of the game. This shows that our
method is able to accurately identify relevant sen-
tences when the information they contain is most
pertinent to game play.
Predicate Labeling Figure 4 shows examples of the
predicate structure output of our model. We eval-
uate the accuracy of this labeling by comparing it
against a gold-standard annotation of the game man-
ual. Table 3 shows the performance of our method
in terms of how accurately it labels words as state,
action or background, and also how accurately it dif-
ferentiates between state and action words. In ad-
dition to showing a performance improvement over
the random baseline, these results display two clear
trends: first, under both evaluations, labeling accu-
racy is higher during the initial stages of the game.
This is to be expected since the model relies heav-
ily on textual features only during the beginning of
the game (see Figure 6). Second, the model clearly
performs better in differentiating between state and
action words, rather than in the three-way labeling.
To verify the usefulness of our method’s predi-
cate labeling, we perform a final set of experiments
where predicate labels are selected uniformly at ran-
dom within our full model. This random labeling
results in a win rate of 44% – a performance similar
to the sentence relevance model which uses no pred-
icate information. This confirms that our method
is able identify a predicate structure which, while
noisy, provides information relevant to game play.
</bodyText>
<figureCaption confidence="0.97700825">
Figure 7: Examples of word to game attribute associa-
tions that are learned via the feature weights of our model.
Figure 7 shows examples of how this textual infor-
mation is grounded in the game, by way of the asso-
</figureCaption>
<bodyText confidence="0.5621325">
ciations learned between words and game attributes
in the final layer of the full model.
</bodyText>
<sectionHeader confidence="0.999382" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.9996016">
In this paper we presented a novel approach for
improving the performance of control applications
by automatically leveraging high-level guidance ex-
pressed in text documents. Our model, which op-
erates in the Monte-Carlo framework, jointly learns
to identify text relevant to a given game state in ad-
dition to learning game strategies guided by the se-
lected text. We show that this approach substantially
outperforms language-unaware alternatives while
learning only from environment feedback.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999601333333333">
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168, grant IIS-0835652),
DARPA Machine Reading Program (FA8750-09-
C-0172) and the Microsoft Research New Faculty
Fellowship. Thanks to Michael Collins, Tommi
Jaakkola, Leslie Kaelbling, Nate Kushman, Sasha
Rush, Luke Zettlemoyer, the MIT NLP group, and
the ACL reviewers for their suggestions and com-
ments. Any opinions, findings, conclusions, or rec-
ommendations expressed in this paper are those of
the authors, and do not necessarily reflect the views
of the funding organizations.
</bodyText>
<page confidence="0.996826">
276
</page>
<sectionHeader confidence="0.998348" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999885976744186">
R. Balla and A. Fern. 2009. UCT for tactical assault
planning in real-time strategy games. In 21st Interna-
tional Joint Conference on Artificial Intelligence.
Darse Billings, Lourdes Pe˜na Castillo, Jonathan Scha-
effer, and Duane Szafron. 1999. Using probabilis-
tic knowledge and simulation to play poker. In 16th
National Conference on Artificial Intelligence, pages
697–703.
S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
ACL, pages 82–90.
S.R.K Branavan, Luke Zettlemoyer, and Regina Barzilay.
2010. Reading between the lines: Learning to map
high-level instructions to commands. In Proceedings
of ACL, pages 1268–1277.
S.R.K. Branavan, David Silver, and Regina Barzilay.
2011. Non-linear monte-carlo search in civilization ii.
In Proceedings of IJCAI.
John S. Bridle. 1990. Training stochastic model recog-
nition algorithms as networks can lead to maximum
mutual information estimation of parameters. In Ad-
vances in NIPS, pages 211–217.
Arthur E. Bryson and Yu-Chi Ho. 1969. Applied optimal
control: optimization, estimation, and control. Blais-
dell Publishing Company.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006.
Jacob Eisenstein, James Clarke, Dan Goldwasser, and
Dan Roth. 2009. Reading to learn: Constructing
features from semantic abstracts. In Proceedings of
EMNLP, pages 958–967.
Michael Fleischman and Deb Roy. 2005. Intentional
context in situated natural language learning. In Pro-
ceedings of CoNLL, pages 104–111.
S. Gelly, Y. Wang, R. Munos, and O. Teytaud. 2006.
Modification of UCT with patterns in Monte-Carlo
Go. Technical Report 6062, INRIA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313–330.
Raymond J. Mooney. 2008a. Learning language from its
perceptual context. In Proceedings of ECML/PKDD.
Raymond J. Mooney. 2008b. Learning to connect lan-
guage and perception. In Proceedings of AAAI, pages
1598–1601.
James Timothy Oates. 2001. Grounding knowledge
in sensors: Unsupervised learning for language and
planning. Ph.D. thesis, University of Massachusetts
Amherst.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323:533–536.
J. Sch¨afer. 2008. The UCT algorithm applied to games
with imperfect information. Diploma Thesis. Otto-
von-Guericke-Universit¨at Magdeburg.
B. Sheppard. 2002. World-championship-caliber Scrab-
ble. Artificial Intelligence, 134(1-2):241–275.
D. Silver, R. Sutton, and M. M¨uller. 2008. Sample-
based learning and search with permanent and tran-
sient memories. In 25th International Conference on
Machine Learning, pages 968–975.
D. Silver. 2009. Reinforcement Learning and
Simulation-Based Search in the Game of Go. Ph.D.
thesis, University of Alberta.
Jeffrey Mark Siskind. 2001. Grounding the lexical se-
mantics of verbs in visual perception using force dy-
namics and event logic. Journal of Artificial Intelli-
gence Research, 15:31–90.
N. Sturtevant. 2008. An analysis of UCT in multi-player
games. In 6th International Conference on Computers
and Games, pages 37–49.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. The MIT Press.
G. Tesauro and G. Galperin. 1996. On-line policy im-
provement using Monte-Carlo search. In Advances in
Neural Information Processing 9, pages 1068–1074.
Adam Vogel and Daniel Jurafsky. 2010. Learning to
follow navigational directions. In Proceedings of the
ACL, pages 806–814.
Chen Yu and Dana H. Ballard. 2004. On the integration
of grounding language and learning objects. In Pro-
ceedings of AAAI, pages 488–493.
</reference>
<page confidence="0.99726">
277
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.567832">
<title confidence="0.99992">Learning to Win by Reading Manuals in a Monte-Carlo Framework</title>
<author confidence="0.999487">S R K Branavan David Silver Regina Barzilay</author>
<affiliation confidence="0.998282">Computer Science and Artificial Intelligence Laboratory * Department of Computer Science Massachusetts Institute of Technology University College London</affiliation>
<email confidence="0.998316">d.silver@cs.ucl.ac.uk</email>
<abstract confidence="0.979423818181818">This paper presents a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control applications such as games. Our ultimate goal is to enrich a stochastic player with highlevel guidance expressed in text. Our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text. Our method operates in the Monte-Carlo search framework, and learns both text analysis and game strategies based only on environment feedback. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 27% absolute improvement and winning over 78% of games when playing against the built- AI of Civilization II.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Balla</author>
<author>A Fern</author>
</authors>
<title>UCT for tactical assault planning in real-time strategy games.</title>
<date>2009</date>
<booktitle>In 21st International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="9179" citStr="Balla and Fern, 2009" startWordPosition="1445" endWordPosition="1448"> this paper, we focus primarily on the linguistic aspects of our task and algorithm. For a discussion and evaluation of the non-linguistic aspects please see Branavan et al. (2011). 269 3 Monte-Carlo Framework for Computer Games Our method operates within the Monte-Carlo search framework (Tesauro and Galperin, 1996), which has been successfully applied to complex computer games such as Go, Poker, Scrabble, multi-player card games, and real-time strategy games, among others (Gelly et al., 2006; Tesauro and Galperin, 1996; Billings et al., 1999; Sheppard, 2002; Sch¨afer, 2008; Sturtevant, 2008; Balla and Fern, 2009). Since Monte-Carlo search forms the foundation of our approach, we briefly describe it in this section. Game Representation The game is defined by a large Markov Decision Process (S, A, T, R). Here S is the set of possible states, A is the space of legal actions, and T (s&apos;|s, a) is a stochastic state transition function where s, s&apos; E S and a E A. Specifically, a state encodes attributes of the game world, such as available resources and city locations. At each step of the game, a player executes an action a which causes the current state s to change to a new state s&apos; according to the transiti</context>
</contexts>
<marker>Balla, Fern, 2009</marker>
<rawString>R. Balla and A. Fern. 2009. UCT for tactical assault planning in real-time strategy games. In 21st International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darse Billings</author>
<author>Lourdes Pe˜na Castillo</author>
<author>Jonathan Schaeffer</author>
<author>Duane Szafron</author>
</authors>
<title>Using probabilistic knowledge and simulation to play poker.</title>
<date>1999</date>
<booktitle>In 16th National Conference on Artificial Intelligence,</booktitle>
<pages>697--703</pages>
<contexts>
<context position="9106" citStr="Billings et al., 1999" startWordPosition="1435" endWordPosition="1438">xpensive. Therefore our approach learns by actively playing the game. 4In this paper, we focus primarily on the linguistic aspects of our task and algorithm. For a discussion and evaluation of the non-linguistic aspects please see Branavan et al. (2011). 269 3 Monte-Carlo Framework for Computer Games Our method operates within the Monte-Carlo search framework (Tesauro and Galperin, 1996), which has been successfully applied to complex computer games such as Go, Poker, Scrabble, multi-player card games, and real-time strategy games, among others (Gelly et al., 2006; Tesauro and Galperin, 1996; Billings et al., 1999; Sheppard, 2002; Sch¨afer, 2008; Sturtevant, 2008; Balla and Fern, 2009). Since Monte-Carlo search forms the foundation of our approach, we briefly describe it in this section. Game Representation The game is defined by a large Markov Decision Process (S, A, T, R). Here S is the set of possible states, A is the space of legal actions, and T (s&apos;|s, a) is a stochastic state transition function where s, s&apos; E S and a E A. Specifically, a state encodes attributes of the game world, such as available resources and city locations. At each step of the game, a player executes an action a which causes </context>
</contexts>
<marker>Billings, Castillo, Schaeffer, Szafron, 1999</marker>
<rawString>Darse Billings, Lourdes Pe˜na Castillo, Jonathan Schaeffer, and Duane Szafron. 1999. Using probabilistic knowledge and simulation to play poker. In 16th National Conference on Artificial Intelligence, pages 697–703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>82--90</pages>
<contexts>
<context position="6798" citStr="Branavan et al., 2009" startWordPosition="1065" endWordPosition="1068">tences further improves performance. In full-length games, our algorithm yields a 27% improvement over a language unaware base3Civilization II was #3 in IGN’s 2007 list of top video games of all time (http://top100.ign.com/2007/ign top game 3.html) line, and wins over 78% of games against the builtin, hand-crafted AI of Civilization II.4 2 Related Work Our work fits into the broad area of grounded language acquisition where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to reinforcement learning approaches that learn language by proactively interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Like the above models, we use environment feedback (in the form of a utility function) as the main source of supervision. The key difference, however, is in the language interpretation task itself. Previous work has focused on the interpretation of instruction text where input documents specify a set of actions to be executed in</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of ACL, pages 82–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reading between the lines: Learning to map high-level instructions to commands.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1268--1277</pages>
<contexts>
<context position="7039" citStr="Branavan et al., 2010" startWordPosition="1103" endWordPosition="1106">3.html) line, and wins over 78% of games against the builtin, hand-crafted AI of Civilization II.4 2 Related Work Our work fits into the broad area of grounded language acquisition where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to reinforcement learning approaches that learn language by proactively interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Like the above models, we use environment feedback (in the form of a utility function) as the main source of supervision. The key difference, however, is in the language interpretation task itself. Previous work has focused on the interpretation of instruction text where input documents specify a set of actions to be executed in the environment. In contrast, game manuals provide high-level advice but do not directly describe the correct actions for every potential game state. Moreover, these documents are long, and use rich vocabularies with complex grammatical con</context>
</contexts>
<marker>Branavan, Zettlemoyer, Barzilay, 2010</marker>
<rawString>S.R.K Branavan, Luke Zettlemoyer, and Regina Barzilay. 2010. Reading between the lines: Learning to map high-level instructions to commands. In Proceedings of ACL, pages 1268–1277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>David Silver</author>
<author>Regina Barzilay</author>
</authors>
<title>Non-linear monte-carlo search in civilization ii.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="8738" citStr="Branavan et al. (2011)" startWordPosition="1381" endWordPosition="1384">me, such as which moves are valid, given documents describing the rules. Our goal is more open ended, in that we aim to learn winning game strategies. Furthermore, Eisenstein et al. (2009) rely on a different source of supervision – game traces collected a priori. For complex games, like the one considered in this paper, collecting such game traces is prohibitively expensive. Therefore our approach learns by actively playing the game. 4In this paper, we focus primarily on the linguistic aspects of our task and algorithm. For a discussion and evaluation of the non-linguistic aspects please see Branavan et al. (2011). 269 3 Monte-Carlo Framework for Computer Games Our method operates within the Monte-Carlo search framework (Tesauro and Galperin, 1996), which has been successfully applied to complex computer games such as Go, Poker, Scrabble, multi-player card games, and real-time strategy games, among others (Gelly et al., 2006; Tesauro and Galperin, 1996; Billings et al., 1999; Sheppard, 2002; Sch¨afer, 2008; Sturtevant, 2008; Balla and Fern, 2009). Since Monte-Carlo search forms the foundation of our approach, we briefly describe it in this section. Game Representation The game is defined by a large Mar</context>
</contexts>
<marker>Branavan, Silver, Barzilay, 2011</marker>
<rawString>S.R.K. Branavan, David Silver, and Regina Barzilay. 2011. Non-linear monte-carlo search in civilization ii. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Bridle</author>
</authors>
<title>Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters.</title>
<date>1990</date>
<booktitle>In Advances in NIPS,</booktitle>
<pages>211--217</pages>
<contexts>
<context position="16006" citStr="Bridle, 1990" startWordPosition="2633" endWordPosition="2634"> identifies the words that describe actions and those that describe state attributes, discriminating them from the rest of the sentence. As with the relevance decision, we model this labeling using hidden variables. As shown in Figure 2, our model is a four layer neural network. The input layer x represents the current state s, candidate action a, and document d. The second layer consists of two disjoint sets of units y� and z� which encode the sentence-relevance and predicate-labeling decisions respectively. Each of these sets of units operates as a stochastic 1-of-n softmax selection layer (Bridle, 1990) where only a single unit is activated. The activation function for units in this layer is the standard softmax function: Ayi = 1|x) = e where yi is the ith hidden unit of y, and ui is the weight vector corresponding to yi. Given this actiInput layer: Deterministic feature layer: Hidden layer encoding sentence relevance Hidden layer encoding predicate labeling Output layer gi·g / k �� 9k· , e 271 Here ej is the predicate label of the jth word being labeled, and e1:j−1 is the partial predicate labeling constructed so far for sentence yi. In the second layer of the neural network, the units z� r</context>
</contexts>
<marker>Bridle, 1990</marker>
<rawString>John S. Bridle. 1990. Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. In Advances in NIPS, pages 211–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur E Bryson</author>
<author>Yu-Chi Ho</author>
</authors>
<title>Applied optimal control: optimization, estimation, and control.</title>
<date>1969</date>
<publisher>Blaisdell Publishing Company.</publisher>
<contexts>
<context position="18561" citStr="Bryson and Ho, 1969" startWordPosition="3068" endWordPosition="3071">action-value function of the game, we learn model parameters by applying non-linear regression to the observed final utilities from the simulated roll-outs. Specifically, we adjust the parameters by stochastic gradient descent, to minimize the mean-squared error between the actionvalue Q(s, a) and the final utility R(s,) for each observed game state s and action a. The resulting update to model parameters B is of the form: α OB = −� ∇o [R(s,) − Q(s, a)]2 = α [R(s,) − Q(s, a)] ∇oQ(s, a; B), where α is a learning rate parameter. This minimization is performed via standard error backpropagation (Bryson and Ho, 1969; Rumelhart vation function, the second layer effectively models sentence relevance and predicate labeling decisions via log-linear distributions, the details of which are described below. � The third feature layer f of the neural network is deterministically computed given the active units yi and zj of the softmax layers, and the values of the input layer. Each unit in this layer corresponds to a fixed feature function fk(st, at, d, yi, zj) ∈ R. Finally the output layer encodes the action-value function Q(s, a, d), which now also depends on the document d, as a weighted linear combination of </context>
</contexts>
<marker>Bryson, Ho, 1969</marker>
<rawString>Arthur E. Bryson and Yu-Chi Ho. 1969. Applied optimal control: optimization, estimation, and control. Blaisdell Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In LREC</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Dan Roth</author>
</authors>
<title>Reading to learn: Constructing features from semantic abstracts.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>958--967</pages>
<contexts>
<context position="7945" citStr="Eisenstein et al., 2009" startWordPosition="1250" endWordPosition="1253">ruction text where input documents specify a set of actions to be executed in the environment. In contrast, game manuals provide high-level advice but do not directly describe the correct actions for every potential game state. Moreover, these documents are long, and use rich vocabularies with complex grammatical constructions. We do not aim to perform a comprehensive interpretation of such documents. Rather, our focus is on language analysis that is sufficiently detailed to help the underlying control task. The area of language analysis situated in a game domain has been studied in the past (Eisenstein et al., 2009). Their method, however, is different both in terms of the target interpretation task, and the supervision signal it learns from. They aim to learn the rules of a given game, such as which moves are valid, given documents describing the rules. Our goal is more open ended, in that we aim to learn winning game strategies. Furthermore, Eisenstein et al. (2009) rely on a different source of supervision – game traces collected a priori. For complex games, like the one considered in this paper, collecting such game traces is prohibitively expensive. Therefore our approach learns by actively playing </context>
</contexts>
<marker>Eisenstein, Clarke, Goldwasser, Roth, 2009</marker>
<rawString>Jacob Eisenstein, James Clarke, Dan Goldwasser, and Dan Roth. 2009. Reading to learn: Constructing features from semantic abstracts. In Proceedings of EMNLP, pages 958–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Deb Roy</author>
</authors>
<title>Intentional context in situated natural language learning.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="6745" citStr="Fleischman and Roy, 2005" startWordPosition="1057" endWordPosition="1060">how that modeling the deeper linguistic structure of sentences further improves performance. In full-length games, our algorithm yields a 27% improvement over a language unaware base3Civilization II was #3 in IGN’s 2007 list of top video games of all time (http://top100.ign.com/2007/ign top game 3.html) line, and wins over 78% of games against the builtin, hand-crafted AI of Civilization II.4 2 Related Work Our work fits into the broad area of grounded language acquisition where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to reinforcement learning approaches that learn language by proactively interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Like the above models, we use environment feedback (in the form of a utility function) as the main source of supervision. The key difference, however, is in the language interpretation task itself. Previous work has focused on the interpretation of instruction text where input</context>
</contexts>
<marker>Fleischman, Roy, 2005</marker>
<rawString>Michael Fleischman and Deb Roy. 2005. Intentional context in situated natural language learning. In Proceedings of CoNLL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gelly</author>
<author>Y Wang</author>
<author>R Munos</author>
<author>O Teytaud</author>
</authors>
<title>Modification of UCT with patterns in Monte-Carlo Go.</title>
<date>2006</date>
<tech>Technical Report 6062, INRIA.</tech>
<contexts>
<context position="9055" citStr="Gelly et al., 2006" startWordPosition="1427" endWordPosition="1430">, collecting such game traces is prohibitively expensive. Therefore our approach learns by actively playing the game. 4In this paper, we focus primarily on the linguistic aspects of our task and algorithm. For a discussion and evaluation of the non-linguistic aspects please see Branavan et al. (2011). 269 3 Monte-Carlo Framework for Computer Games Our method operates within the Monte-Carlo search framework (Tesauro and Galperin, 1996), which has been successfully applied to complex computer games such as Go, Poker, Scrabble, multi-player card games, and real-time strategy games, among others (Gelly et al., 2006; Tesauro and Galperin, 1996; Billings et al., 1999; Sheppard, 2002; Sch¨afer, 2008; Sturtevant, 2008; Balla and Fern, 2009). Since Monte-Carlo search forms the foundation of our approach, we briefly describe it in this section. Game Representation The game is defined by a large Markov Decision Process (S, A, T, R). Here S is the set of possible states, A is the space of legal actions, and T (s&apos;|s, a) is a stochastic state transition function where s, s&apos; E S and a E A. Specifically, a state encodes attributes of the game world, such as available resources and city locations. At each step of th</context>
</contexts>
<marker>Gelly, Wang, Munos, Teytaud, 2006</marker>
<rawString>S. Gelly, Y. Wang, R. Munos, and O. Teytaud. 2006. Modification of UCT with patterns in Monte-Carlo Go. Technical Report 6062, INRIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="31786" citStr="Marcus et al., 1993" startWordPosition="5313" endWordPosition="5316">of the sentence relevance decisions produced by our method. To evaluate the accuracy of these decisions, we ideally require a ground-truth relevance annotation of the game’s user manual. This however, is Figure 5: Accuracy of our method’s sentence relevance predictions, averaged over 100 independent runs. impractical since the relevance decision is dependent on the game context, and is hence specific to each time step of each game instance. Therefore, for the purposes of this evaluation, we modify the game manual by adding to it sentences randomly selected from the Wall Street Journal corpus (Marcus et al., 1993) – sentences that are highly unlikely to be relevant to game play. We then evaluate the accuracy with which sentences from the original manual are picked as relevant. In this evaluation, our method achieves an average accuracy of 71.8%. Given that our model only has to differentiate between the game manual text and the Wall Street Journal, this number may seem disappointing. Furthermore, as can be seen from Figure 5, the sentence relevance accuracy varies widely as the game progresses, with a high average of 94.2% during the initial 25 game steps. In reality, this pattern of high initial accur</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Learning language from its perceptual context.</title>
<date>2008</date>
<booktitle>In Proceedings of ECML/PKDD.</booktitle>
<contexts>
<context position="6759" citStr="Mooney, 2008" startWordPosition="1061" endWordPosition="1062">er linguistic structure of sentences further improves performance. In full-length games, our algorithm yields a 27% improvement over a language unaware base3Civilization II was #3 in IGN’s 2007 list of top video games of all time (http://top100.ign.com/2007/ign top game 3.html) line, and wins over 78% of games against the builtin, hand-crafted AI of Civilization II.4 2 Related Work Our work fits into the broad area of grounded language acquisition where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to reinforcement learning approaches that learn language by proactively interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Like the above models, we use environment feedback (in the form of a utility function) as the main source of supervision. The key difference, however, is in the language interpretation task itself. Previous work has focused on the interpretation of instruction text where input documents spe</context>
</contexts>
<marker>Mooney, 2008</marker>
<rawString>Raymond J. Mooney. 2008a. Learning language from its perceptual context. In Proceedings of ECML/PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to connect language and perception.</title>
<date>2008</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>1598--1601</pages>
<contexts>
<context position="6759" citStr="Mooney, 2008" startWordPosition="1061" endWordPosition="1062">er linguistic structure of sentences further improves performance. In full-length games, our algorithm yields a 27% improvement over a language unaware base3Civilization II was #3 in IGN’s 2007 list of top video games of all time (http://top100.ign.com/2007/ign top game 3.html) line, and wins over 78% of games against the builtin, hand-crafted AI of Civilization II.4 2 Related Work Our work fits into the broad area of grounded language acquisition where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to reinforcement learning approaches that learn language by proactively interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Like the above models, we use environment feedback (in the form of a utility function) as the main source of supervision. The key difference, however, is in the language interpretation task itself. Previous work has focused on the interpretation of instruction text where input documents spe</context>
</contexts>
<marker>Mooney, 2008</marker>
<rawString>Raymond J. Mooney. 2008b. Learning to connect language and perception. In Proceedings of AAAI, pages 1598–1601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Timothy Oates</author>
</authors>
<title>Grounding knowledge in sensors: Unsupervised learning for language and planning.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts Amherst.</institution>
<contexts>
<context position="6682" citStr="Oates, 2001" startWordPosition="1049" endWordPosition="1050">ne in terms of number of games won. Moreover, we show that modeling the deeper linguistic structure of sentences further improves performance. In full-length games, our algorithm yields a 27% improvement over a language unaware base3Civilization II was #3 in IGN’s 2007 list of top video games of all time (http://top100.ign.com/2007/ign top game 3.html) line, and wins over 78% of games against the builtin, hand-crafted AI of Civilization II.4 2 Related Work Our work fits into the broad area of grounded language acquisition where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to reinforcement learning approaches that learn language by proactively interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Like the above models, we use environment feedback (in the form of a utility function) as the main source of supervision. The key difference, however, is in the language interpretation task itself. Previous work ha</context>
</contexts>
<marker>Oates, 2001</marker>
<rawString>James Timothy Oates. 2001. Grounding knowledge in sensors: Unsupervised learning for language and planning. Ph.D. thesis, University of Massachusetts Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning representations by backpropagating errors.</title>
<date>1986</date>
<journal>Nature,</journal>
<pages>323--533</pages>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1986. Learning representations by backpropagating errors. Nature, 323:533–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sch¨afer</author>
</authors>
<title>The UCT algorithm applied to games with imperfect information. Diploma Thesis.</title>
<date>2008</date>
<location>Ottovon-Guericke-Universit¨at Magdeburg.</location>
<marker>Sch¨afer, 2008</marker>
<rawString>J. Sch¨afer. 2008. The UCT algorithm applied to games with imperfect information. Diploma Thesis. Ottovon-Guericke-Universit¨at Magdeburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sheppard</author>
</authors>
<title>World-championship-caliber Scrabble.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<pages>134--1</pages>
<contexts>
<context position="9122" citStr="Sheppard, 2002" startWordPosition="1439" endWordPosition="1440"> approach learns by actively playing the game. 4In this paper, we focus primarily on the linguistic aspects of our task and algorithm. For a discussion and evaluation of the non-linguistic aspects please see Branavan et al. (2011). 269 3 Monte-Carlo Framework for Computer Games Our method operates within the Monte-Carlo search framework (Tesauro and Galperin, 1996), which has been successfully applied to complex computer games such as Go, Poker, Scrabble, multi-player card games, and real-time strategy games, among others (Gelly et al., 2006; Tesauro and Galperin, 1996; Billings et al., 1999; Sheppard, 2002; Sch¨afer, 2008; Sturtevant, 2008; Balla and Fern, 2009). Since Monte-Carlo search forms the foundation of our approach, we briefly describe it in this section. Game Representation The game is defined by a large Markov Decision Process (S, A, T, R). Here S is the set of possible states, A is the space of legal actions, and T (s&apos;|s, a) is a stochastic state transition function where s, s&apos; E S and a E A. Specifically, a state encodes attributes of the game world, such as available resources and city locations. At each step of the game, a player executes an action a which causes the current stat</context>
</contexts>
<marker>Sheppard, 2002</marker>
<rawString>B. Sheppard. 2002. World-championship-caliber Scrabble. Artificial Intelligence, 134(1-2):241–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Silver</author>
<author>R Sutton</author>
<author>M M¨uller</author>
</authors>
<title>Samplebased learning and search with permanent and transient memories.</title>
<date>2008</date>
<booktitle>In 25th International Conference on Machine Learning,</booktitle>
<pages>968--975</pages>
<marker>Silver, Sutton, M¨uller, 2008</marker>
<rawString>D. Silver, R. Sutton, and M. M¨uller. 2008. Samplebased learning and search with permanent and transient memories. In 25th International Conference on Machine Learning, pages 968–975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Silver</author>
</authors>
<title>Reinforcement Learning and Simulation-Based Search in the Game of Go.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Alberta.</institution>
<contexts>
<context position="12780" citStr="Silver, 2009" startWordPosition="2088" endWordPosition="2090">y are more valuable than predicted by the current estimate of Q(s, a). As the accuracy of Q(s, a) improves, the quality of action selection improves and vice at +— arg max a 1 270 versa, in a cycle of continual improvement (Sutton and Barto, 1998). In many games, it is sufficient to maintain a distinct action-value for each unique state and action in a large search tree. However, when the branching factor is large it is usually beneficial to approximate the action-value function, so that the value of many related states and actions can be learned from a reasonably small number of simulations (Silver, 2009). One successful approach is to model the action-value function as a linear combination of state and action attributes (Silver et al., 2008): Q(s, a) = w · f(s, a). Here f(s, a) E R&apos; is a real-valued feature function, and w is a weight vector. We take a similar approach here, except that our feature function includes latent structure which models language. The parameters w of Q(s, a) are learned based on feedback from the roll-out simulations. Specifically, the parameters are updated by stochastic gradient descent by comparing the current predicted Q(s, a) against the observed utility at the e</context>
</contexts>
<marker>Silver, 2009</marker>
<rawString>D. Silver. 2009. Reinforcement Learning and Simulation-Based Search in the Game of Go. Ph.D. thesis, University of Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Mark Siskind</author>
</authors>
<title>Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic.</title>
<date>2001</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>15--31</pages>
<contexts>
<context position="6697" citStr="Siskind, 2001" startWordPosition="1051" endWordPosition="1052">f number of games won. Moreover, we show that modeling the deeper linguistic structure of sentences further improves performance. In full-length games, our algorithm yields a 27% improvement over a language unaware base3Civilization II was #3 in IGN’s 2007 list of top video games of all time (http://top100.ign.com/2007/ign top game 3.html) line, and wins over 78% of games against the builtin, hand-crafted AI of Civilization II.4 2 Related Work Our work fits into the broad area of grounded language acquisition where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to reinforcement learning approaches that learn language by proactively interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Like the above models, we use environment feedback (in the form of a utility function) as the main source of supervision. The key difference, however, is in the language interpretation task itself. Previous work has focused on th</context>
</contexts>
<marker>Siskind, 2001</marker>
<rawString>Jeffrey Mark Siskind. 2001. Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic. Journal of Artificial Intelligence Research, 15:31–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Sturtevant</author>
</authors>
<title>An analysis of UCT in multi-player games.</title>
<date>2008</date>
<booktitle>In 6th International Conference on Computers and Games,</booktitle>
<pages>37--49</pages>
<contexts>
<context position="9156" citStr="Sturtevant, 2008" startWordPosition="1443" endWordPosition="1444">ying the game. 4In this paper, we focus primarily on the linguistic aspects of our task and algorithm. For a discussion and evaluation of the non-linguistic aspects please see Branavan et al. (2011). 269 3 Monte-Carlo Framework for Computer Games Our method operates within the Monte-Carlo search framework (Tesauro and Galperin, 1996), which has been successfully applied to complex computer games such as Go, Poker, Scrabble, multi-player card games, and real-time strategy games, among others (Gelly et al., 2006; Tesauro and Galperin, 1996; Billings et al., 1999; Sheppard, 2002; Sch¨afer, 2008; Sturtevant, 2008; Balla and Fern, 2009). Since Monte-Carlo search forms the foundation of our approach, we briefly describe it in this section. Game Representation The game is defined by a large Markov Decision Process (S, A, T, R). Here S is the set of possible states, A is the space of legal actions, and T (s&apos;|s, a) is a stochastic state transition function where s, s&apos; E S and a E A. Specifically, a state encodes attributes of the game world, such as available resources and city locations. At each step of the game, a player executes an action a which causes the current state s to change to a new state s&apos; ac</context>
</contexts>
<marker>Sturtevant, 2008</marker>
<rawString>N. Sturtevant. 2008. An analysis of UCT in multi-player games. In 6th International Conference on Computers and Games, pages 37–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Sutton</author>
<author>Andrew G Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="12414" citStr="Sutton and Barto, 1998" startWordPosition="2023" endWordPosition="2026">of the roll-outs. States and actions are evaluated by an action-value function Q(s, a), which is an estimate of the expected outcome of action a in state s. This action-value function is used to guide action selection during the roll-outs. While actions are usually selected to maximize the action-value function, sometimes other actions are also randomly explored in case they are more valuable than predicted by the current estimate of Q(s, a). As the accuracy of Q(s, a) improves, the quality of action selection improves and vice at +— arg max a 1 270 versa, in a cycle of continual improvement (Sutton and Barto, 1998). In many games, it is sufficient to maintain a distinct action-value for each unique state and action in a large search tree. However, when the branching factor is large it is usually beneficial to approximate the action-value function, so that the value of many related states and actions can be learned from a reasonably small number of simulations (Silver, 2009). One successful approach is to model the action-value function as a linear combination of state and action attributes (Silver et al., 2008): Q(s, a) = w · f(s, a). Here f(s, a) E R&apos; is a real-valued feature function, and w is a weigh</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement Learning: An Introduction. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tesauro</author>
<author>G Galperin</author>
</authors>
<title>On-line policy improvement using Monte-Carlo search.</title>
<date>1996</date>
<booktitle>In Advances in Neural Information Processing 9,</booktitle>
<pages>1068--1074</pages>
<contexts>
<context position="8875" citStr="Tesauro and Galperin, 1996" startWordPosition="1400" endWordPosition="1403">g game strategies. Furthermore, Eisenstein et al. (2009) rely on a different source of supervision – game traces collected a priori. For complex games, like the one considered in this paper, collecting such game traces is prohibitively expensive. Therefore our approach learns by actively playing the game. 4In this paper, we focus primarily on the linguistic aspects of our task and algorithm. For a discussion and evaluation of the non-linguistic aspects please see Branavan et al. (2011). 269 3 Monte-Carlo Framework for Computer Games Our method operates within the Monte-Carlo search framework (Tesauro and Galperin, 1996), which has been successfully applied to complex computer games such as Go, Poker, Scrabble, multi-player card games, and real-time strategy games, among others (Gelly et al., 2006; Tesauro and Galperin, 1996; Billings et al., 1999; Sheppard, 2002; Sch¨afer, 2008; Sturtevant, 2008; Balla and Fern, 2009). Since Monte-Carlo search forms the foundation of our approach, we briefly describe it in this section. Game Representation The game is defined by a large Markov Decision Process (S, A, T, R). Here S is the set of possible states, A is the space of legal actions, and T (s&apos;|s, a) is a stochastic</context>
<context position="22991" citStr="Tesauro and Galperin, 1996" startWordPosition="3845" endWordPosition="3848">21. To effectively deal with this large action space, we assume that given the state, the actions of a single unit are independent of the actions of all other units of the same player. Utility Function The Monte-Carlo algorithm uses the utility function to evaluate the outcomes of 6www.civfanatics.com/content/civ2/reference/Civ2manual.zip Figure 3: Example attributes of the game (box above), and features computed using the game manual and these attributes (box below). simulated game roll-outs. In the typical application of the algorithm, the final game outcome is used as the utility function (Tesauro and Galperin, 1996). Given the complexity of Civilization II, running simulation roll-outs until game completion is impractical. The game, however, provides each player with a game score, which is a noisy indication of how well they are currently playing. Since we are playing a two-player game, we use the ratio of the game score of the two players as our utility function. Features The sentence relevance features and the � action-value function features f consider the attributes of the game state and action, and the words of the sentence. Some of these features compute text overlap between the words of the senten</context>
</contexts>
<marker>Tesauro, Galperin, 1996</marker>
<rawString>G. Tesauro and G. Galperin. 1996. On-line policy improvement using Monte-Carlo search. In Advances in Neural Information Processing 9, pages 1068–1074.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>806--814</pages>
<contexts>
<context position="6825" citStr="Vogel and Jurafsky, 2010" startWordPosition="1069" endWordPosition="1072"> performance. In full-length games, our algorithm yields a 27% improvement over a language unaware base3Civilization II was #3 in IGN’s 2007 list of top video games of all time (http://top100.ign.com/2007/ign top game 3.html) line, and wins over 78% of games against the builtin, hand-crafted AI of Civilization II.4 2 Related Work Our work fits into the broad area of grounded language acquisition where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to reinforcement learning approaches that learn language by proactively interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Like the above models, we use environment feedback (in the form of a utility function) as the main source of supervision. The key difference, however, is in the language interpretation task itself. Previous work has focused on the interpretation of instruction text where input documents specify a set of actions to be executed in the environment. In contra</context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>Adam Vogel and Daniel Jurafsky. 2010. Learning to follow navigational directions. In Proceedings of the ACL, pages 806–814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Yu</author>
<author>Dana H Ballard</author>
</authors>
<title>On the integration of grounding language and learning objects.</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>488--493</pages>
<contexts>
<context position="6719" citStr="Yu and Ballard, 2004" startWordPosition="1053" endWordPosition="1056">es won. Moreover, we show that modeling the deeper linguistic structure of sentences further improves performance. In full-length games, our algorithm yields a 27% improvement over a language unaware base3Civilization II was #3 in IGN’s 2007 list of top video games of all time (http://top100.ign.com/2007/ign top game 3.html) line, and wins over 78% of games against the builtin, hand-crafted AI of Civilization II.4 2 Related Work Our work fits into the broad area of grounded language acquisition where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to reinforcement learning approaches that learn language by proactively interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Like the above models, we use environment feedback (in the form of a utility function) as the main source of supervision. The key difference, however, is in the language interpretation task itself. Previous work has focused on the interpretation of in</context>
</contexts>
<marker>Yu, Ballard, 2004</marker>
<rawString>Chen Yu and Dana H. Ballard. 2004. On the integration of grounding language and learning objects. In Proceedings of AAAI, pages 488–493.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>