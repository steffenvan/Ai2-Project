<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.8709445">
The Fitted Parse:
100% Parsing Capability in a Syntactic Grammar of English
</title>
<author confidence="0.558258">
Karen Jensen and George E. Heidorn
</author>
<affiliation confidence="0.199074666666667">
Computer Sciences Department
IBM Thomas J. Watson Research Center
Yorktown Heights, New York 10598
</affiliation>
<sectionHeader confidence="0.835874" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99973125">
A technique is described for performing fitted parsing.
After the rules of a more conventional syntactic grammar are
unable to produce a parse for an input string, this technique
can be used to produce a reasonable approximate parse that
can serve as input to the remaining stages of processing. The
paper describes how fitted parsing is done in the EPISTLE
system and discusses how it can help in dealing with many
difficult problems of natural language analysis.
</bodyText>
<sectionHeader confidence="0.812506" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.963099972222222">
The EPISTLE project has as its long-range goal the ma-
chine processing of natural language text in an office environ-
ment. Ultimately we intend to have software that win be able
to parse and understand ordinary prose documents (such as
those that an office principal might expect his secretary to
cope with), and will be able to generate at least a first draft of
a business letter or memo. Our current goal is a system for
critiquing written material on points of grammar and style.
Our grammar is written in NLP (Heidorn 1972), an aug-
mented phrase structure language which is implemented in
LISP/370. The EPISTLE grammar currently uses syntactic,
but not semantic, information. Access to an on-line standard
dictionary with about 130.000 entries, including part-of-speech
and some other syntactic information (such as transitivity of
verbs), makes the system&apos;s vocabulary essentially unlimited.
We test and improve the grammar by regularly running it on a
data base of 2254 sentences from 411 actual business letters.
Most of these sentences are rather complicated; the longest
contains 63 words, and the average length is 19.2 words.
Since the subset of English which is represented in busi-
ness documents is very large, we need a very comprehensive
grammar and robust parser. In the course of this work we
have developed some new techniques to help deal with the
refractory nature of natural language syntax. In this paper we
discuss one such technique: the fitted parse, which guarantees
the production of a reasonable parse tree for any string, no
matter how unorthodox that string may be. The parse which is
produced by fitting might not be perfect; but it will always be
reasonable and useful, and will allow for later refinement by
semantic processing.
There is a certain perception of parsing that leads to the
development of techniques like this one: namely, that trying
to write a grammar to describe explicitly all and only the sen-
tences of a natural language is about as practical as trying to
find the Holy Grail. Not only will the effort expended be
Herculean, it will be doomed to failure. Instead we take a
heuristic approach and consider that a natural language parser
can be divided into three parts:
(a) a set of rules, called the core grammar, that precisely
define the central, agreed-upon grammatical structures
of a language;
(b) peripheral procedures that handle parsing ambiguity:
when the core grammar produces more than one parse,
these procedures decide which of the multiple parses is
to be preferred;
(c) peripheral procedures that handle parsing failure: when
the core grammar cannot define an acceptable parse,
these procedures assign some reasonable structure to the
input.
In EPISTLE, (a) the core grammar consists at present of a set
of about 300 syntax rules; (b) ambiguity is resolved by using a
metric that ranks alternative parses (Heidorn 1982); and (c)
parse failure is handled by the fitting procedure described here.
In using the terms core grammar and periphery we are
consciously echoing recent work in generative grammar, but we
are applying the terms in a somewhat different way. Core
grammar, in current linguistic theory, suggests the notion of a
set of very general rules which define universal properties of
human language and effectively set limits on the types of
grammars that any particular language may have; periphery
phenomena are those constructions which are peculiar to par-
ticular languages and which require added rules beyond what
the core grammar will provide (Lasnik and Freidin 1981). Our
current work is not concerned with the meta-rules of a Univer-
sal Grammar. But we have found that a distinction between
core and periphery is useful even within a grammar of a partic-
ular language â€” in this case, English.
This paper first reviews parsing in EPISTLE, and then
describes the fitting procedure, followed by several examples
of its application. Then the benefits of parse fitting and the
results of using it in our system are discussed, followed by its
relation to other work.
</bodyText>
<page confidence="0.997627">
93
</page>
<subsectionHeader confidence="0.73324">
Parsing in EPISTLE
</subsectionHeader>
<bodyText confidence="0.99990365">
EPISTLE&apos;s parser is written in the NLP programming
language, which works with augmented phrase structure rules
and with attribute-value records, which are manipulated by the
rules. When NLP is used to parse natural language text, the
records describe constituents, and the rules put these constitu-
ents together to form ever larger constituent (or record) struc-
tures. Records contain all the computational and linguistic
information associated with words, with larger constituents,
and with the parse formation. At this time our grammar is
sentence-based; we do not, for instance, create record struc-
tures to describe paragraphs. Details of the EPISTLE system
and of its core grammar may be found in Miller et al., 1981,
and Heidorn et al., 1982.
A close examination of parse trees produced by the core
grammar will often reveal branch attachments that are not
quite right: for example, semantically incongruous preposition-
al phrase attachments. In line with our pragmatic parsing
philosophy, our core grammar is designed to produce unique
approximate parses. (Recall that we currently have access
only to syntactic and morphological information about constit-
uents.) In the cases where semantic or pragmatic information
is needed before a proper attachment can be made, rather than
produce a confusion of multiple parses we force the grammar
to try to assign a single parse. This is usually done by forcing
some attachments to be made to the closest, or rightmost,
available constituent. This strategy only rarely impedes the
type of grammar-checking and style-checking that we are
working on. And we feel that a single parse with a consistent
attachment scheme will yield much more easily to later seman-
tic processing than would a large number of different struc-
tures.
The rules of the core grammar (CG) produce single ap-
proximate parses for the largest percentage of input text. The
CO can always be improved and its coverage extended; work
on improving the EPISTLE CO is continual. But the coverage
of a core grammar will never reach 100%. Natural language is
an organic symbol system; it does not submit to cast-iron
control. For those strings that cannot be fully parsed by rules
of the core grammar we use a heuristic best fit procedure that
produces a reasonable parse structure.
</bodyText>
<subsectionHeader confidence="0.956106">
The Fitting Procedure
</subsectionHeader>
<bodyText confidence="0.999963384615385">
The fitting procedure begins after the CO rules have been
applied in a bottom-up, parallel fashion, but have failed to
produce an S node that covers the string. At this point, as a
by-product of bottom-up parsing, records are available for
inspection that describe the various segments of the input
string from many perspectives, according to the rules that have
been applied. The term fitting has to do with selecting and
fitting these pieces of the analysis together in a reasonable
fashion.
The algorithm proceeds in two main stages: first, a head
constituent is chosen; next, remaining constituents are fitted in.
In our current implementation, candidates for the head are
tested preferentially as follows, from most to least desirable:
</bodyText>
<listItem confidence="0.93502475">
(a) VPs with tense and subject;
(b) VPs with tense but no subject:
(c) segments other than VP:
(d) untensed VPs.
</listItem>
<bodyText confidence="0.999913384615385">
If more than one candidate is found in any category, the one
preferred is the widest (covering most text). If there is a tie
for widest, the leftmost of those is preferred. If there is a tie
for leftmost, the one with the best value for the parse metric is
chosen. If there is still a tie (a very unlikely case). an arbi-
trary choice is made. (Note that we consider a VP to be any
segment of text that has a verb as its head element.)
The fitting process is complete if the head constituent
covers the entire input string (as would be the case if the
string contained just a noun phrase, for example. &amp;quot;Salutations
and congratulations&amp;quot;). If the head constituent does not cover
the entire string, remaining constituents are added on either
side, with the following order of preference:
</bodyText>
<listItem confidence="0.847147">
(a) segments other than VP;
(b) untensed VPs;
(c) tensed VPs.
</listItem>
<bodyText confidence="0.999926108695652">
As with the choice of head, the widest candidate is preferred
at each step. The fit moves outward from the head, both
leftward to the beginning of the string, and rightward to the
end, until the entire input string has been fitted into a best
approximate parse tree. The overall effect of the fitting proc-
ess is to select the largest chunk of sentence-like material
within a text string and consider it to be central, with left-over
chunks of text attached in some reasonable manner.
As a simple example, consider this text string which ap-
peared in one of our EPISTLE data base letters:
&amp;quot;Example: 75 percent of $250.00 is $187.50.&amp;quot;
Because this string has a capitalized first word and a period at
its end, it is submitted to the core grammar for consideration
as a sentence. But it is not a sentence, and so the CO will fail
to arrive at a completed parse. However, during processing.
the CO will have assigned many structures to its many sub-
strings. Looking for a head constituent among these struc-
tures, the fitting procedure will first seek VPs with tense and
subject. Several are present: &amp;quot;S250.00 is&amp;quot;. &amp;quot;percent of
$250.00 is&amp;quot;, &amp;quot;5250.00 is S187.50&amp;quot;, and so on. The widest aad
leftmost of these VP constituents is the one which covers the
string &amp;quot;75 percent of S250.00 is S187.50&amp;quot;, so it will be chosen
as head.
The fitting process then looks for additional constituents
to the left, favoring ones other than VP. It finds first the
colon, and then the word &amp;quot;Example&amp;quot;. In this string the only
constituent following the head is the final period, which is duly
added. The complete fitted parse is shown in Figure I.
The form of parse tree used here shows the top-down
structure of the string from left to right, with the terminal
nodes being the last item on each line. At each level of the
tree (in a vertical column), the head element of a constituent is
marked with an asterisk. The other elements above and below
are pre- and post-modifiers. The highest element of the trees
shown here is FITTED, rather than the more usual SENT. (It
is important to remember that these parse diagrams are only
shorthand representations for the NLP record structures, which
contain an abundance of information about the string proc-
essed.)
The tree of Figure 1. which would be lost if we restricted
ourselves to the precise rules of the core grammar, is now
available for examination, for grammar and style checking, and
ultimately for semantic interpretation. It can take its place in
the stream of continuous text and be analyzed for what it is â€”
a sentence fragment. interpretable only by reference to other
sentences in context.
</bodyText>
<page confidence="0.989167">
94
</page>
<figure confidence="0.564843571428571">
FITTED ---NP NOUN---&amp;quot;Example&apos;
---VP*I----NPI QUANT --NUM* &amp;quot;75&amp;quot;
NOUN*---&amp;quot;percent&amp;quot;
PPI PREP &amp;quot;of&amp;quot;
MONEY*----&amp;quot;$250.00&amp;quot;
I----VERB---&amp;quot;is&amp;quot;
I----NP MONEY--&amp;quot;$187.5O&amp;quot;
</figure>
<figureCaption confidence="0.836291">
Figure I. An example fitted parse tree.
</figureCaption>
<figure confidence="0.9739694">
FITTEDI---NP*I----NPI AJP ADJ*----&amp;quot;Good&amp;quot;
I I I NOUN*---&amp;quot;luck&amp;quot;
I I----CONJ* &amp;quot;and&amp;quot;
I I----NPI AJP ADJ*----&amp;quot;good&amp;quot;
I I NOUN*---&amp;quot;selling&amp;quot;
</figure>
<figureCaption confidence="0.993861">
Figure 2. Fined noun phrase (fragment).
</figureCaption>
<table confidence="0.846586454545454">
FITTEDI---VP*
AJP ADJ*----&amp;quot;the&amp;quot;
NP NOUN*---&amp;quot;Annual&amp;quot;
NP NOUN*---&amp;quot;Commission&amp;quot;
NP NOUN*---&amp;quot;Statement&amp;quot;
NOUN* &amp;quot;total&amp;quot;
----VERB----&amp;quot;should&amp;quot;
----VERB*---&amp;quot;be&amp;quot;
----NP MONEY*--&amp;quot;$14,682.61&amp;quot;
---AVP ADV----&amp;quot;riot&amp;quot;
---NP MONEY*--&amp;quot;$14,682.67&amp;quot;
</table>
<figureCaption confidence="0.943949">
Figure 3. Fitted sentence with ellipsis.
</figureCaption>
<subsectionHeader confidence="0.941114">
Further Examples
</subsectionHeader>
<bodyText confidence="0.8563502">
The fitted parse approach can help to deal with many
difficult natural language problems, including fragments, diffi-
cult cases of ellipsis, proliferation of rules to handle single
phenomena, phenomena for which no rule seems adequate, and
punctuation horrors. Each of these is discussed here with
examples.
Fragments. There are many of these in running text: they
are frequently NPs. as in Figure 2. and include common greet-
ings. farewells, and sentiments. (N.b., all examples in this
paper are taken from the EPISTLE data base.)
</bodyText>
<tableCaption confidence="0.657113076923077">
Difficult cases of ellipsis. In the sentence of Figure 3, what
we really have at a semantic level is a conjunction of two
propositions which, if generated directly, would read: &apos;&apos;The
Annual Commission Statement total should be $14,682.6l; the
Annual Commission Statement total should not be
S14.682.67.&amp;quot; Deletion processes operating on the second
proposition are lawful (deletion of identical elements), but
massive. It would be unwise to write a core grammar rule that
routinely allowed negativized NPs to follow main clauses.
because:
(a) the proper analysis of this sentence would be obscured:
some pieces â€” namely, the inferred concepts â€” are
missing from the second part of the surface sentence:
</tableCaption>
<listItem confidence="0.988923333333333">
(b) the linguistic generalization would be lost: any two
conjoined propositions can undergo deletion of identical
(recoverable) elements.
</listItem>
<bodyText confidence="0.934696">
A fitted parse such as Figure 3 allows us to inspect the main
clause for syntactic and stylistic deviances, and at the same
time makes clear the breaking point between the two proposi-
tions and opens the door for a later semantic processing of the
elided elements.
Proliferation of rules to handle single phenomena. There
are some English constructions which, although they have a
fairly simple and unitary form, do not hold anything like a
unitary ordering relation within clause boundaries. The voca-
tive is one of these:
</bodyText>
<note confidence="0.521184">
(a) Bill. I&apos;ve been asked to clarify the enclosed letter.
</note>
<page confidence="0.735697">
95
</page>
<figure confidence="0.9955355">
FITTED â€“NP
---VP* â€“NP PRON*---&amp;quot;I&amp;quot;
----VERB &amp;quot;&apos;ve&amp;quot;
----VERB &amp;quot;been&amp;quot;
----VERB* &amp;quot;asked&amp;quot;
----INFCLI INFT0---&amp;quot;to&amp;quot;
I VERB*---&amp;quot;clarify&amp;quot;
I NPI AJP ADJ*----&amp;quot;the&amp;quot;
AJP VERB*---&amp;quot;enclosed&amp;quot;
NOUN*___&apos; letter&amp;quot;
</figure>
<figureCaption confidence="0.990044">
Figure 4. Fitted sentence with initial vocative.
</figureCaption>
<figure confidence="0.995712529411765">
FITTEDI---NPI AJP ADJ*----&amp;quot;Good&amp;quot;
NOUN*---&amp;quot;luck&amp;quot;
---PPI PREP----&amp;quot;to&amp;quot;
NP PRON*---&amp;quot;you&amp;quot;
CONJ*---&amp;quot;and&amp;quot;
I NP PRON*---&amp;quot;yours&amp;quot;
---CONJ----&amp;quot;and&amp;quot;
---VP*I----NP PRON*---&amp;quot;I&amp;quot;
----VERB* &amp;quot;wish&amp;quot;
- NP PRON*---&amp;quot;you&amp;quot;
----NPI AJP ADJ*----&amp;quot;the&amp;quot;
ADV &amp;quot;VERY&amp;quot;
ADJ*----&amp;quot;best&amp;quot;
â€“ PPI PREP----&amp;quot;in&amp;quot;
AJP ADJ*----&amp;quot;your&amp;quot;
AJP ADJ*----&amp;quot;future&amp;quot;
NOUN*---&amp;quot;efforts&amp;quot;
</figure>
<figureCaption confidence="0.797306">
Figure 5. Fitted conjunction of noun phrase with clause.
(b) I&apos;ve been asked. Bill, to clarify the enclosed letter.
</figureCaption>
<bodyText confidence="0.961341">
(c) I&apos;ve been asked to clarify the enclosed letter, Bill.
In longer sentences there would be even more possible places
to insert the vocative, of course.
</bodyText>
<subsectionHeader confidence="0.746929">
Rules could be written that would explicitly allow the
</subsectionHeader>
<bodyText confidence="0.985940647058823">
placement of a proper name, surrounded by commas, at differ-
ent positions in the sentence â€” a different rule for each posi-
tion. But this solution lacks elegance, makes a simple phenom-
enon seem complicated, and always runs the risk of overlook-
ing yet one more position where some other writer might insert
a vocative. The parse fitting procedure provides an alternative
that preserves the integrity of the main clause and adds the
vocative at a break in the structure. which is where it belongs.
as shown in Figure 4. Other similar phenomena, such as par-
enthetical expressions, can be handled in this same fashion.
Phenomena for which no rule seems adequate. The sen-
tence &amp;quot;Good luck to you and yours and I wish you the very
best in your future efforts.&amp;quot; is. on the face of it, a conjunction
of a noun phrase (or NP plus PP) with a finite verb phrase.
Such constructions are not usually considered to be fully gram-
matical. and a core grammar which contained a rule describing
this construction ought probably to be called a faulty grammar.
Nevertheless, ordinary English correspondence abounds with
strings of this sort, and readers have no difficulty construing
them. The fitted parse for this sentence in Figure 5 presents
the finite clause as its head and adds the remaining constitu-
ents in a reasonable fashion. From this structure later seman-
tic processing could infer that &amp;quot;Good luck to you and yours&amp;quot;
really means &amp;quot;I express/send/wish good luck to you and
yours&amp;quot; â€” a special case of formalized, ritualized ellipsis.
Punctuation horrors. In any large sample of natural lan-
guage text, there will be many irregularities of punctuation
which, although perfectly understandable to readers, can com-
pletely disable an explicit computational grammar. In business
text these difficulties are frequent. Some can be caught and
corrected by punctuation checkers and balancers. But others
cannot, sometimes because, for all their trickiness, they are not
really wrong. Yet few grammarians would care to dignify, by
describing it with rules of the core grammar. a text string like:
</bodyText>
<table confidence="0.811708333333333">
&amp;quot;Options: A1-(Transmitter Clocked by Dataset)
B3-(without the 605 Recall Unit) C5-(with ABC
Ring Indicator) 138-(without Auto Answer) E10-
</table>
<subsectionHeader confidence="0.722675">
(Auto Ring Selective).&amp;quot;
</subsectionHeader>
<bodyText confidence="0.7731085">
Our parse fitting procedure handles this example by building a
string of NPs separated with punctuation marks, as shown in
Figure 6. This solution at least enables us to get a handle on
the contents of the string.
</bodyText>
<page confidence="0.969955">
96
</page>
<figure confidence="0.999412578947368">
---NP
---PP*
---NP
- â€“ â€“PP
NOUNS--- &amp;quot;Options
NOUN*---&amp;quot;A1&amp;quot;
NP NOUN*---&amp;quot;Transmitter&amp;quot;
NOUNS &amp;quot;Clocked&amp;quot;
PREP &amp;quot;by&amp;quot;
NOUN* &amp;quot;Dataset&amp;quot;
NOUN*---&amp;quot;B3&amp;quot;
----PREP----&amp;quot;without&amp;quot;
----AJP
----QUANT NUM*----&amp;quot;605&amp;quot;
----NP NOUN*---&amp;quot;Recall&amp;quot;
----NOUN* &amp;quot;Unit&amp;quot;
NOUN---&amp;quot;C5&amp;quot;
---NP
---NP
PREP----&amp;quot;with&amp;quot;
NP NOUN*--- &amp;quot;ABC&amp;quot;
NP NOUN*___ &amp;quot;Ring&amp;quot;
NOUN* &amp;quot;Indicator&amp;quot;
---NP
---PPI
---NP
---NPI
0)â€œ
NOUN--&amp;quot;D8&amp;quot;
PREP----&amp;quot;without&amp;quot;
NP NOUN---&amp;quot;Auto&amp;quot;
NOUN*---&amp;quot;Answer&amp;quot;
I, )â€œ
NOUN*---&amp;quot;E10&amp;quot;
NP NOUN---&amp;quot;Auto&amp;quot;
NP NOUN*---&amp;quot;Ring&amp;quot;
NOUN*---&amp;quot;Selective&amp;quot;
IC )
</figure>
<figureCaption confidence="0.999581">
Figure 6. Fitted list.
</figureCaption>
<bodyText confidence="0.417807">
FITTED
</bodyText>
<subsectionHeader confidence="0.960813">
Benefits
</subsectionHeader>
<bodyText confidence="0.999917266666667">
There are two main benefits to be gained from using the
fitted parse approach. First, it allows for syntactic processing
â€” for our purposes, grammar and style checking â€” to proceed
in the absence of a perfect parse. Second, it provides a prom-
ising structure to submit to later semantic processing routines.
And parenthetically, a fitted parse diagram is a great aid to
rule debugging. The place where the first break occurs be-
tween the head constituent and its pre- or post-modifiers usu-
ally indicates fairly precisely where the core grammar failed.
It should be emphasized that a fitting procedure cannot be
used as a substitute for explicit rules, and that it in no way
lessens the importance of the core grammar. There is a tight
interaction between the two components. The success of the
fitted parse depends on the accuracy and completeness of the
core rules; a fit is only as good as its grammar.
</bodyText>
<subsectionHeader confidence="0.599121">
Results
</subsectionHeader>
<bodyText confidence="0.999837375">
In December of 1981. the EPISTLE grammar, which at
that time consisted of about 250 grammar rules and did nor
include the fitted parsing technique, was run on the data base
of 2254 sentences from business letters of various types. The
input corpus was very raw; it had not been edited for spelling
or other typing errors, nor had it been manipulated in any way
that might have made parsing easier.
At that time the system failed to parse 832. or 36%, of
the input sentences. (It gave single parses for 4190, double
parses for 11%, and 3 or more parses for 12%.) Then we
added the fitting procedure and also worked to improve the
core grammar.
Concentrating only on those 832 sentences which in De-
cember Failed to parse, we ran the grammar again in July,
1982, on a subset of 163 of them. This time the number of
core grammar rules was 300. Where originally the CG could
parse none of these 163 sentences, this time it yielded parses
(mostly single or double) for 109 of them. The remaining 54
were handled by the fitting procedure.
Close analysis of the 54 fitted parses revealed that 14 of
these sentences bypass the core grammar simply because of
missing dictionary information: for example. the CG contains
a rule to parse ditransitive VPs (indirect object-taking VI&apos;s
with verbs like &amp;quot;give&amp;quot; or &amp;quot;send&amp;quot;), but that rule will not apply
if the verb is not marked as ditransitive. The EPISTLE dic-
tionary will eventually have all ditransitive verbs marked prop-
erly, but right now it does not.
Removing those 14 sentences from consideration, we are
left with a residue of 40 strings, or about 25% of the 163
sentences, which we expect always to handle by means of the
fitted parse. These strings include all of the problem types
mentioned above (fragments, ellipsis. etc.), and the fitted
parses produced were adequate for our purposes. It is not yet
clear how this 25% might extrapolate to business text at large,
but it seems safe to say that there will always be a significant
percentage of natural business correspondence which we can-
not expect to parse with the core grammar, but which responds
nicely to peripheral processing techniques like those of the
fitted parse. (A more recent run of the entire data base result-
ed in 27% fitted parses.)
</bodyText>
<subsectionHeader confidence="0.84063">
Related Work
</subsectionHeader>
<bodyText confidence="0.981111578947369">
Although we know of no approach quite like the one
described here, other related work has been done. Most of
this work suggests that unparsable or ill-formed input should
be handled by relaxation techniques, i.e., by relaxing restric-
tions in the grammar rules in some principled way. This is
undoubtedly a useful strategy â€” one which EPISTLE makes
use of. in fact, in its rules for detecting grammatical errors
(Heidorn et al. 1982). However, it is questionable whether
such a strategy can ultimately succeed in the face of the over-
whelming (for all practical purposes, infinite) variety of ill-
formedness with which we are faced when We set out to parse
truly unrestricted natural language input. If all ill-formedness
is rule-based (Weischedel and Sondheimer 1981, p. 3), it can
only be by some very loose definition of the term rule, such as
that which might apply to the fitting algorithm described here.
Thus Weischedel and Black, 1980. suggest three tech-
niques for responding intelligently to unparsable inputs:
(a) using presuppositions to determine user assumptions:
this course is not available to a syntactic grammar like
EPISTLE&apos;s:
(b) using relaxation techniques;
(et supplying the user with information about the point
where the parse blocked; this would require an interac-
tive environment, which would not be possible for every
type of natural language processing application.
Kwasny and Sondheimer, 1981. are strong.proponents of
relaxation techniques, which they use to handle both cases of
clearly ungrammatical structures, such as co-occurrence viola-
tions like subject/verb disagreement, and cases of perfectly
acceptable but difficult constructions (ellipsis and conjunc-
tion).
Weischedel and Sondheimer. 1982. describe an improved
ellipsis processor. No longer is ellipsis handled with relaxation
techniques. but by predicting transformations of previous pars-
ing paths which would allow for the matching of fragments
with plausible contexts. This plan would be appropriate as a
next step after the fitted parse, but it does not guarantee a
parse for all elided inputs.
Hayes and Mouradian. 1981. also use the relaxation me-
thod. They achieve flexibility in their parser by relaxing con-
sistency constraints (grammatical restrictions, like Kwasny and
Sondheimer&apos;s co-occurrence violations) and also by relaxing
ordering constraints. However, they are working with a
restricted-domain semantic system and their approach, as they
admit, &amp;quot;does not embody a solution for flexible parsing of
natural language in general&amp;quot; (p. 236).
The work of Wilks is heavily semantic and therefore quite
different from EPISTLE, but his general philosophy meshes
nicely with the philosophy of the fitted parse: &amp;quot;It is proper to
prefer the normal...but it would be absurd...not to accept the
abnormal if it is described&amp;quot; (Wilks 1975, p. 267). Wilks&apos;
approach to machine translation which involves doing some
amount of the translation on a phrase-by-phrase basis is rele-
vant here, too. With fitted parsing, it might be possible to get
usable translations for strings that cannot be completely parsed
with the core grammar by translating each phrase of the fitted
parse separately.
</bodyText>
<sectionHeader confidence="0.990799" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9906675">
We would like to thank Lance Miller for his interest and
encouragement in this work, and Roy Byrd, Martin Chodorow.
Robert Granville and John Sowa for their comments on an
earlier version of this paper.
</bodyText>
<sectionHeader confidence="0.991865" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999715875">
Hayes, P.J. and G.V. Mouradian. 1981. &amp;quot;Flexible Parsing&amp;quot; in
Am. J. Comp. Ling. 7.4. 232-242.
Heidorn, G.E. 1972. &amp;quot;Natural Language Inputs to a Simula-
tion Programming System.&amp;quot; Technical Report NPS-
55HD72101A. Monterey. Cal: Naval Postgraduate School.
Heidorn, G.E. 1982. &amp;quot;Experience with an Easily Computed
Metric for Ranking Alternative Parses&amp;quot; in Proc. 20t4 Annu-
al Meeting of the ACL. Toronto, Canada. 82-84.
Heidorn, G.E., K. Jensen. L.A. Miller. R.J. Byrd, and M.S.
Chodorow. 1982. &amp;quot;The EPISTLE Text-Critiquing Sys-
tem&amp;quot; in IBM Sys. J. 21.3. 305-326.
Kwasny, S.C. and N.K. Sondheimer. 1981. &amp;quot;Relaxation Tech-
niques for Parsing Ill-Formed Input&amp;quot; in Am. J. Comp. Ling.
7.2. 99-108,
Lasnik, H. and R. Freldin. 1981. &apos;&apos;Core Grammar. Case
Theory. and Markedness&amp;quot; in Proc. 1979 GLOW Conf.
Pisa, Italy.
Miller, L.A., G.E. Heidorn and K. Jensen. 1981. &amp;quot;Text-
Critiquing with the EPISTLE System: An Authors&apos;s Aid to
Better Syntax&amp;quot; in AF1PS Conf. Proc., Vol. 50. Arlington.
Va., 649-655.
Weischedel, R.M. and J.E. Black. 1980. &amp;quot;Responding Intelli-
gently to Unparsable Inputs&amp;quot; in Am. J. Comp. Ling. 6.2.
97-109.
Weischedel, R.M. and N.K. Sondheimer. 1981. &amp;quot;A Frame-
work for Processing 111-Formed Input.&amp;quot; Research Report.
Univ. of Delaware.
Weischedel. R.M. and N.K. Sondheimer. 1982. &amp;quot;An Im-
proved Heuristic for Ellipsis Processing&amp;quot; in Proc. 20th An-
nual Meeting of the ACL. Toronto. Canada, 85-68.
Wilks. Yorick. 1975. &amp;quot;An Intelligent Analyzer and Under-
slander of English&amp;quot; in Comm. ACM 18.5, 264-271.
</reference>
<page confidence="0.998447">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930023">
<title confidence="0.9731615">The Fitted Parse: 100% Parsing Capability in a Syntactic Grammar of English</title>
<author confidence="0.999993">Karen Jensen</author>
<author confidence="0.999993">George E Heidorn</author>
<affiliation confidence="0.999345">Computer Sciences Department IBM Thomas J. Watson Research Center</affiliation>
<address confidence="0.971681">Yorktown Heights, New York 10598</address>
<abstract confidence="0.999575555555555">technique is described for performing After the rules of a more conventional syntactic grammar are unable to produce a parse for an input string, this technique can be used to produce a reasonable approximate parse that can serve as input to the remaining stages of processing. The paper describes how fitted parsing is done in the EPISTLE system and discusses how it can help in dealing with many difficult problems of natural language analysis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P J Hayes</author>
<author>G V Mouradian</author>
</authors>
<title>Flexible Parsing&amp;quot;</title>
<date>1981</date>
<journal>in Am. J. Comp. Ling.</journal>
<volume>7</volume>
<pages>232--242</pages>
<marker>Hayes, Mouradian, 1981</marker>
<rawString>Hayes, P.J. and G.V. Mouradian. 1981. &amp;quot;Flexible Parsing&amp;quot; in Am. J. Comp. Ling. 7.4. 232-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidorn</author>
</authors>
<title>Natural Language Inputs to a Simulation Programming System.&amp;quot;</title>
<date>1972</date>
<tech>Technical Report NPS55HD72101A.</tech>
<location>Monterey. Cal: Naval Postgraduate School.</location>
<contexts>
<context position="1223" citStr="Heidorn 1972" startWordPosition="202" endWordPosition="203">elp in dealing with many difficult problems of natural language analysis. Introduction The EPISTLE project has as its long-range goal the machine processing of natural language text in an office environment. Ultimately we intend to have software that win be able to parse and understand ordinary prose documents (such as those that an office principal might expect his secretary to cope with), and will be able to generate at least a first draft of a business letter or memo. Our current goal is a system for critiquing written material on points of grammar and style. Our grammar is written in NLP (Heidorn 1972), an augmented phrase structure language which is implemented in LISP/370. The EPISTLE grammar currently uses syntactic, but not semantic, information. Access to an on-line standard dictionary with about 130.000 entries, including part-of-speech and some other syntactic information (such as transitivity of verbs), makes the system&apos;s vocabulary essentially unlimited. We test and improve the grammar by regularly running it on a data base of 2254 sentences from 411 actual business letters. Most of these sentences are rather complicated; the longest contains 63 words, and the average length is 19.</context>
</contexts>
<marker>Heidorn, 1972</marker>
<rawString>Heidorn, G.E. 1972. &amp;quot;Natural Language Inputs to a Simulation Programming System.&amp;quot; Technical Report NPS55HD72101A. Monterey. Cal: Naval Postgraduate School.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidorn</author>
</authors>
<title>Experience with an Easily Computed Metric for Ranking Alternative Parses&amp;quot; in</title>
<date>1982</date>
<booktitle>Proc. 20t4 Annual Meeting of the ACL.</booktitle>
<pages>82--84</pages>
<location>Toronto,</location>
<contexts>
<context position="3580" citStr="Heidorn 1982" startWordPosition="583" endWordPosition="584">ecisely define the central, agreed-upon grammatical structures of a language; (b) peripheral procedures that handle parsing ambiguity: when the core grammar produces more than one parse, these procedures decide which of the multiple parses is to be preferred; (c) peripheral procedures that handle parsing failure: when the core grammar cannot define an acceptable parse, these procedures assign some reasonable structure to the input. In EPISTLE, (a) the core grammar consists at present of a set of about 300 syntax rules; (b) ambiguity is resolved by using a metric that ranks alternative parses (Heidorn 1982); and (c) parse failure is handled by the fitting procedure described here. In using the terms core grammar and periphery we are consciously echoing recent work in generative grammar, but we are applying the terms in a somewhat different way. Core grammar, in current linguistic theory, suggests the notion of a set of very general rules which define universal properties of human language and effectively set limits on the types of grammars that any particular language may have; periphery phenomena are those constructions which are peculiar to particular languages and which require added rules be</context>
</contexts>
<marker>Heidorn, 1982</marker>
<rawString>Heidorn, G.E. 1982. &amp;quot;Experience with an Easily Computed Metric for Ranking Alternative Parses&amp;quot; in Proc. 20t4 Annual Meeting of the ACL. Toronto, Canada. 82-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Miller R J Byrd</author>
<author>M S Chodorow</author>
</authors>
<title>The EPISTLE Text-Critiquing System&amp;quot;</title>
<date>1982</date>
<journal>in IBM Sys. J.</journal>
<volume>21</volume>
<pages>305--326</pages>
<marker>Byrd, Chodorow, 1982</marker>
<rawString>Heidorn, G.E., K. Jensen. L.A. Miller. R.J. Byrd, and M.S. Chodorow. 1982. &amp;quot;The EPISTLE Text-Critiquing System&amp;quot; in IBM Sys. J. 21.3. 305-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Kwasny</author>
<author>N K Sondheimer</author>
</authors>
<title>Relaxation Techniques for Parsing Ill-Formed Input&amp;quot;</title>
<date>1981</date>
<journal>in Am. J. Comp. Ling.</journal>
<volume>7</volume>
<pages>99--108</pages>
<contexts>
<context position="22469" citStr="Kwasny and Sondheimer, 1981" startWordPosition="3646" endWordPosition="3649"> some very loose definition of the term rule, such as that which might apply to the fitting algorithm described here. Thus Weischedel and Black, 1980. suggest three techniques for responding intelligently to unparsable inputs: (a) using presuppositions to determine user assumptions: this course is not available to a syntactic grammar like EPISTLE&apos;s: (b) using relaxation techniques; (et supplying the user with information about the point where the parse blocked; this would require an interactive environment, which would not be possible for every type of natural language processing application. Kwasny and Sondheimer, 1981. are strong.proponents of relaxation techniques, which they use to handle both cases of clearly ungrammatical structures, such as co-occurrence violations like subject/verb disagreement, and cases of perfectly acceptable but difficult constructions (ellipsis and conjunction). Weischedel and Sondheimer. 1982. describe an improved ellipsis processor. No longer is ellipsis handled with relaxation techniques. but by predicting transformations of previous parsing paths which would allow for the matching of fragments with plausible contexts. This plan would be appropriate as a next step after the f</context>
</contexts>
<marker>Kwasny, Sondheimer, 1981</marker>
<rawString>Kwasny, S.C. and N.K. Sondheimer. 1981. &amp;quot;Relaxation Techniques for Parsing Ill-Formed Input&amp;quot; in Am. J. Comp. Ling. 7.2. 99-108,</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lasnik</author>
<author>R Freldin</author>
</authors>
<title>Core Grammar. Case Theory. and Markedness&amp;quot; in</title>
<date>1981</date>
<booktitle>Proc. 1979 GLOW Conf.</booktitle>
<location>Pisa, Italy.</location>
<marker>Lasnik, Freldin, 1981</marker>
<rawString>Lasnik, H. and R. Freldin. 1981. &apos;&apos;Core Grammar. Case Theory. and Markedness&amp;quot; in Proc. 1979 GLOW Conf. Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Miller</author>
<author>G E Heidorn</author>
<author>K Jensen</author>
</authors>
<title>TextCritiquing with the EPISTLE System: An Authors&apos;s Aid to Better Syntax&amp;quot;</title>
<date>1981</date>
<booktitle>in AF1PS Conf. Proc.,</booktitle>
<volume>50</volume>
<pages>649--655</pages>
<location>Arlington. Va.,</location>
<contexts>
<context position="5479" citStr="Miller et al., 1981" startWordPosition="888" endWordPosition="891">cture rules and with attribute-value records, which are manipulated by the rules. When NLP is used to parse natural language text, the records describe constituents, and the rules put these constituents together to form ever larger constituent (or record) structures. Records contain all the computational and linguistic information associated with words, with larger constituents, and with the parse formation. At this time our grammar is sentence-based; we do not, for instance, create record structures to describe paragraphs. Details of the EPISTLE system and of its core grammar may be found in Miller et al., 1981, and Heidorn et al., 1982. A close examination of parse trees produced by the core grammar will often reveal branch attachments that are not quite right: for example, semantically incongruous prepositional phrase attachments. In line with our pragmatic parsing philosophy, our core grammar is designed to produce unique approximate parses. (Recall that we currently have access only to syntactic and morphological information about constituents.) In the cases where semantic or pragmatic information is needed before a proper attachment can be made, rather than produce a confusion of multiple parse</context>
</contexts>
<marker>Miller, Heidorn, Jensen, 1981</marker>
<rawString>Miller, L.A., G.E. Heidorn and K. Jensen. 1981. &amp;quot;TextCritiquing with the EPISTLE System: An Authors&apos;s Aid to Better Syntax&amp;quot; in AF1PS Conf. Proc., Vol. 50. Arlington. Va., 649-655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Weischedel</author>
<author>J E Black</author>
</authors>
<title>Responding Intelligently to Unparsable Inputs&amp;quot;</title>
<date>1980</date>
<journal>in Am. J. Comp. Ling.</journal>
<volume>6</volume>
<pages>97--109</pages>
<contexts>
<context position="21991" citStr="Weischedel and Black, 1980" startWordPosition="3577" endWordPosition="3580">strategy â€” one which EPISTLE makes use of. in fact, in its rules for detecting grammatical errors (Heidorn et al. 1982). However, it is questionable whether such a strategy can ultimately succeed in the face of the overwhelming (for all practical purposes, infinite) variety of illformedness with which we are faced when We set out to parse truly unrestricted natural language input. If all ill-formedness is rule-based (Weischedel and Sondheimer 1981, p. 3), it can only be by some very loose definition of the term rule, such as that which might apply to the fitting algorithm described here. Thus Weischedel and Black, 1980. suggest three techniques for responding intelligently to unparsable inputs: (a) using presuppositions to determine user assumptions: this course is not available to a syntactic grammar like EPISTLE&apos;s: (b) using relaxation techniques; (et supplying the user with information about the point where the parse blocked; this would require an interactive environment, which would not be possible for every type of natural language processing application. Kwasny and Sondheimer, 1981. are strong.proponents of relaxation techniques, which they use to handle both cases of clearly ungrammatical structures,</context>
</contexts>
<marker>Weischedel, Black, 1980</marker>
<rawString>Weischedel, R.M. and J.E. Black. 1980. &amp;quot;Responding Intelligently to Unparsable Inputs&amp;quot; in Am. J. Comp. Ling. 6.2. 97-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Weischedel</author>
<author>N K Sondheimer</author>
</authors>
<title>A Framework for Processing 111-Formed Input.&amp;quot; Research Report.</title>
<date>1981</date>
<institution>Univ. of Delaware.</institution>
<contexts>
<context position="21816" citStr="Weischedel and Sondheimer 1981" startWordPosition="3545" endWordPosition="3548">nparsable or ill-formed input should be handled by relaxation techniques, i.e., by relaxing restrictions in the grammar rules in some principled way. This is undoubtedly a useful strategy â€” one which EPISTLE makes use of. in fact, in its rules for detecting grammatical errors (Heidorn et al. 1982). However, it is questionable whether such a strategy can ultimately succeed in the face of the overwhelming (for all practical purposes, infinite) variety of illformedness with which we are faced when We set out to parse truly unrestricted natural language input. If all ill-formedness is rule-based (Weischedel and Sondheimer 1981, p. 3), it can only be by some very loose definition of the term rule, such as that which might apply to the fitting algorithm described here. Thus Weischedel and Black, 1980. suggest three techniques for responding intelligently to unparsable inputs: (a) using presuppositions to determine user assumptions: this course is not available to a syntactic grammar like EPISTLE&apos;s: (b) using relaxation techniques; (et supplying the user with information about the point where the parse blocked; this would require an interactive environment, which would not be possible for every type of natural languag</context>
</contexts>
<marker>Weischedel, Sondheimer, 1981</marker>
<rawString>Weischedel, R.M. and N.K. Sondheimer. 1981. &amp;quot;A Framework for Processing 111-Formed Input.&amp;quot; Research Report. Univ. of Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M</author>
<author>N K Sondheimer</author>
</authors>
<title>An Improved Heuristic for Ellipsis Processing&amp;quot; in</title>
<date>1982</date>
<booktitle>Proc. 20th Annual Meeting of the ACL.</booktitle>
<pages>85--68</pages>
<publisher>Toronto. Canada,</publisher>
<marker>M, Sondheimer, 1982</marker>
<rawString>Weischedel. R.M. and N.K. Sondheimer. 1982. &amp;quot;An Improved Heuristic for Ellipsis Processing&amp;quot; in Proc. 20th Annual Meeting of the ACL. Toronto. Canada, 85-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick</author>
</authors>
<title>An Intelligent Analyzer and Underslander of English&amp;quot;</title>
<date>1975</date>
<journal>in Comm. ACM</journal>
<volume>18</volume>
<pages>264--271</pages>
<marker>Yorick, 1975</marker>
<rawString>Wilks. Yorick. 1975. &amp;quot;An Intelligent Analyzer and Underslander of English&amp;quot; in Comm. ACM 18.5, 264-271.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>