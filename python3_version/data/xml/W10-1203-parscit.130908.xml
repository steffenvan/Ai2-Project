<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.026451">
<title confidence="0.9751795">
Query-based Text Normalization Selection Models for Enhanced Retrieval
Accuracy
</title>
<author confidence="0.994087">
Si-Chi Chin Rhonda DeCook W. Nick Street David Eichmann
</author>
<affiliation confidence="0.8604975">
The University of Iowa
Iowa City, USA.
</affiliation>
<email confidence="0.996661">
{si-chi-chin, rhonda-decook, nick-street, david-eichmann}@uiowa.edu
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99928116">
Text normalization transforms words into a
base form so that terms from common equiv-
alent classes match. Traditionally, informa-
tion retrieval systems employ stemming tech-
niques to remove derivational affixes. Deplu-
ralization, the transformation of plurals into
singular forms, is also used as a low-level text
normalization technique to preserve more pre-
cise lexical semantics of text.
Experiment results suggest that the choice of
text normalization technique should be made
individually on each topic to enhance informa-
tion retrieval accuracy. This paper proposes a
hybrid approach, constructing a query-based
selection model to select the appropriate text
normalization technique (stemming, deplural-
ization, or not doing any text normalization).
The selection model utilized ambiguity prop-
erties extracted from queries to train a com-
posite of Support Vector Regression (SVR)
models to predict a text normalization tech-
nique that yields the highest Mean Average
Precision (MAP). Based on our study, such
a selection model holds promise in improving
retrieval accuracy.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988708619047619">
Stemming removes derivational affixes of terms
therefore allowing terms from common equivalence
classes to be clustered. However, stemming also in-
troduces noise by mapping words of different con-
cepts or meanings into one base form, thus impeding
word-sense disambiguation. Depluralization, the
conversion of plural word forms to singular form,
preserves more precise semantics of text than stem-
ming (Krovetz, 2000).
19
Empirical research has demonstrated the ambiva-
lent effect of stemming on text retrieval perfor-
mance. Hull (1996) conducted a comprehensive
case study on the effects of four stemmer tech-
niques and the removal of plural “s” 1 on retrieval
performance. Hull suggested that the adoption of
stemming is beneficial but plural removal is as well
competitive when the size of documents is small.
Prior research (Manning and Schtze, 1999; Mc-
Namee et al., 2008) indicated that traditional stem-
ming, though still benefiting some queries, would
not necessarily enhance the average retrieval perfor-
mance. In addition, stemming was considered one of
the technique failures undermining retrieval perfor-
mance in the TREC 2004 Robust Track (Voorhees,
2006). Prior research also noted the semantic differ-
ences between plurals and singulars. Riloff (1995)
indicated that plural and singular nouns are distinct
because plural nouns usually pertain to the “general
types of incidents,” while singular nouns often per-
tain to “a specific incident.”
Nevertheless, prior research has not closely ex-
amined the effect of the change of the semantics
caused by different level of text normalization tech-
niques. In our work, we conducted extensive exper-
iments on the TREC 2004 Robust track collection to
evaluate the effect of stemming and depluralization
on information retrieval. In addition, we quantify
the ambiguity of a query, extracting five ambiguity
properties from queries. The extracted ambiguity
properties are used to construct query-based selec-
tion model, a composite of multiple Support Vector
</bodyText>
<footnote confidence="0.998774666666667">
1In our work, we not only removed the plural “s” or “es”
but also changed irregular plural forms such as “children” to its
singular form “child”.
</footnote>
<note confidence="0.9684785">
Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 19–26,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999329823529412">
Regression models, to determine the most appropri-
ate text normalization technique for a given query.
To our knowledge, our work is the first study to con-
struct a query-based selection model, using ambigu-
ity properties extracted from provided queries to se-
lect an optimal text normalization technique for each
query.
The remainder of this paper is organized as fol-
lows. In section 2 we describe our experimental
setups and dataset. Section 3 describes and ana-
lyzes experiment results of different text normaliza-
tion techniques on the dataset. We discuss five am-
biguity properties and validate each property in sec-
tion 4. In section 5 we describe the framework and
the prediction results of the proposed query-based
selection model. Finally, we summarize our findings
and discuss future work in section 6.
</bodyText>
<sectionHeader confidence="0.991658" genericHeader="method">
2 Experiment Setup
</sectionHeader>
<bodyText confidence="0.9991825">
The experiment utilizes the queries and relevance
judgment results from the TREC 2004 Robust
Track to evaluate the effect of three text normal-
ization techniques – raw text, depluralized text, and
stemmed text. The TREC 2004 Robust Track used a
document set of approximately 528,000 documents
comprising 1,904 MB of text. In total, 249 query
topics were used in TREC Robust 2004.
Figure 1 illustrates the setup of the experiment.
The collection is parsed with a SAX parser and
stored in a Postgres database. Lucene is then used to
generate three indices: indices of raw text, deplural-
ized text, and stemmed text. The Postgres database
stores each document of the collection, the query
topics of the TREC 2004 Robust Track, and results
of experiments. The ambiguity properties for each
query is also computed in the Postgres system. We
query Lucene indices to obtain the top 1,000 relevant
results and compute Mean Average Precision (MAP)
with the trec eval program to evaluate performance.
We use R to analyze performance scores, generate
descriptive charts, conduct non-parametric statisti-
cal tests, and perform a paired t-test. We use Weka
(Hall et al., 2009) to construct query-based selection
model that incorporates multiple Support Vector Re-
gression (SVR) models.
</bodyText>
<subsectionHeader confidence="0.955264">
2.1 Query Models
</subsectionHeader>
<bodyText confidence="0.99992356">
The TREC 2004 Robust Track provides 249 query
topics; each includes a title, a short description,
and a narrative (usually one-paragraph). We se-
lected three basic query models as a modest base-
line to demonstrate the effect of different text nor-
malization techniques. Our future work will exploit
other ranking models such as BM25 and LMIR. The
three query models used in the experiment are: (1)
boolean search with the title words of topics con-
catenated with logical AND (e.g. hydrogen AND
fuel AND automobiles); (2) boolean search with the
title words of topics concatenated with logical OR
(e.g. hydrogen OR fuel OR automobiles); (3) co-
sine similarity with the title words of topics. Lucene
MoreLikeThis (MLT) class supports both boolean
and cosine similarity query methods for the exper-
iment. Figure 2 shows how query topics are pro-
cessed before interrogating the indices. Original
queries are first depluralized or stemmed, further
processed according to each query model, and fi-
nally run against the depluralized and stemmed in-
dices. The experiment runs unprocessed raw queries
against the index of raw text, depluralized queries
against the index of depluralized text, and stemmed
queries against the index of stemmed text.
</bodyText>
<sectionHeader confidence="0.986054" genericHeader="method">
3 Experiment Results
</sectionHeader>
<bodyText confidence="0.998958291666667">
Table 3 and Figure 3 summarize the results for the
full set of topics. Each row in Table 3 represents a
query model combined with a given text normaliza-
tion technique as described in section 2.1.
For each query model and text normalization
technique, we present the MAP value computed
across all relevant topics. We also provide the p-
value for comparing MAP between each normaliza-
tion technique and the baseline (i.e. non-normalized
(raw) queries). The p-value is generated from the
pairwise Wilcoxon signed rank sum test. Figure 3
describes the distribution of MAP across the three
text normalization techniques and three query mod-
els. The distributions are skewed and many out-
liers are observed. In general, boolean OR and MLT
query models perform similarly and stemming has
the highest median MAP value across all three query
models. The results from Table 3 for the combined
topic set show that depluralization and stemming
perform significantly better than the raw baseline.
However, the performance difference between de-
pluralization and stemming is not significant except
for the AND boolean query model. In general, the
differences of MAP among three text normalization
</bodyText>
<page confidence="0.868216">
20
</page>
<figure confidence="0.999734921052631">
TREC Robust Track 2004
Legend
Document
XML
Documents
Process
DBMS
Data
XMLParser
Weka
SVM
Regression
Ambiguity
Calculator
Ambiguity
Measures
Performance
Score
Postgres
DBMS
Trec Eval
Query
Topics
Search
Results
Text
R
raps
Graphs
Wilcoxon Tests
Paired T-test
Lexicon
Analysis
Lexicon
Analysis
Lucene
Lucene
Index Files
</figure>
<figureCaption confidence="0.974436">
Figure 1: Flow chart of experiment setup
Figure 2: Using the query “hydrogen fuel automobiles”
</figureCaption>
<bodyText confidence="0.983722441176471">
as an example, the depluralized query becomes “hydro-
gen fuel automobile” and the stemmed query becomes
“hydrogen fuel automobil.” Final boolean queries for de-
pluralized topic become “hydrogen AND fuel AND au-
tomobile” and “hydrogen OR fuel OR automobil.” More-
LikeThis (MLT) is the Lucene class used for cosine sim-
ilarity retrieval. A term vector score appends each word
in the topic.
techniques are within 2%.
To visualize the relative performances among
three text normalization techniques, we standardized
the three MAP values for a single topic (one from
each text normalization technique) to have mean 0
and standard deviation of 1. The result provides a 3-
value pattern emphasizing the ordering of the MAPs
across the text normalization techniques, rather than
the raw MAP values themselves. We then used
the K-medoids algorithm (Kaufman and Rousseeuw,
1990) to cluster the transformed data, applying Eu-
clidean distance as the distance measure. Figure 4
is an example of 9 constructed clusters based on the
MAP scores of the MLT query model. In a clus-
ter, a line represents the standardized MAP value
of a topic on each text normalization technique.
Given the small differences in aggregate MAP per-
formance, it is interesting to note that the clusters
demonstrate variable patterns, indicating that some
topics performed better as a depluralized query than
a stemmed query.
The cluster analysis suggests that the choice of
text normalization technique should be made indi-
vidually on each topic. As we choose an appropri-
ate text normalization technique for a given topic,
we would further enhance retrieval performance. In
</bodyText>
<figure confidence="0.999164512820513">
Lexicon Analysis
Query Model
Query Topic
Text Normalization
Final Query
Index
hydrogen
AND
fuel
AND
automobile
AND
boolean
Depluralizer
hydrogen
fuel
automobile
Depluralized
Index
hydrogen fuel
automobiles
OR boolean
hydrogen
OR
fuel
OR
automobile
Stemmed
Index
hydrogen
fuel
automobil
Stemmer
Lucene
MoreLikeThis
hydrogen
automobile
^0.939
fuel^0.631
</figure>
<page confidence="0.824309">
21
</page>
<note confidence="0.544548">
Mean Average Precision vs. Query Model
</note>
<figureCaption confidence="0.98054">
Figure 3: Profile plot of MAP
</figureCaption>
<figure confidence="0.977807375">
Query Model
and
or mlt
Average Precision
0.0 0.2 0.4 0.6 0.8 1.0 1.2
raw
deplural
stem
</figure>
<bodyText confidence="0.998351333333333">
the next section, we address the issue of inconsis-
tent performance by constructing regression models
to predict the mean average precision of each query
from the ambiguity measures, and choose an appro-
priate normalization method based on these predic-
tions.
</bodyText>
<sectionHeader confidence="0.997043" genericHeader="method">
4 Ambiguity Properties
</sectionHeader>
<bodyText confidence="0.999913066666667">
Research has affirmed the negative impact of query
ambiguity on an information retrieval system. As
stemming clusters terms of different concepts, it
should increase query ambiguity. To quantify the
query ambiguity potentially caused by stemming,
we compute five ambiguity properties for each
query: 1) the product of the number of senses, re-
ferred as the sense product; 2) the product of the
number of words mapped to one base form (e.g. a
stem), referred as the word product; 3) the ratio of
the sense product of depluralized query to which of
stemmed query, referred as the deplural-stem ratio;
4) the sum of the inverse document frequency for
each word in a query, referred as the idf-sum; 5) the
length of a query.
</bodyText>
<subsectionHeader confidence="0.980677">
4.1 Sense Product
</subsectionHeader>
<bodyText confidence="0.99974725">
Sense product measures the extent of query ambigu-
ity after stemming. We first find all words mapped to
a given stem and, for each word, we then count the
number of senses found in WordNet. To compute
</bodyText>
<figureCaption confidence="0.981688">
Figure 4: Example of relative performance similarities
among text normalization techniques. The cluster analy-
sis uses the MAP scores of the MLT query model
</figureCaption>
<table confidence="0.999856636363636">
Combined Topic Set
Run MAP p-value
AND Raw 0.1213 N/A
AND Dep 0.1324 5.598e-06*
AND Stem 0.1550 † 1.599e-07*
OR Raw 0.1851 N/A
OR Dep 0.1922 0.03035*
OR Stem 0.2069 0.01123*
MLT Raw 0.1893 N/A
MLT Dep 0.1959 0.04837*
MLT Stem 0.2093 0.009955*
</table>
<tableCaption confidence="0.9853295">
Table 1: Paired Wilcoxon signed-ranked test on Mean
Average Precision (MAP), utilizing raw query as the
baseline. Significant differences between query models
are labeled with *. Results labeled with † indicate sig-
nificant differences between depluralized queries and a
stemmed queries.
</tableCaption>
<page confidence="0.994551">
22
</page>
<bodyText confidence="0.988604611111111">
the number of senses for a given stem, we accumu-
late the number of senses of each word mapped to
the stem. The sense product is then the multiply-
ing of the number of senses for each stemmed query
term, computed as:
sense product =
Sj denotes the number of senses for each word
mapped to a stem i. We have m words mapped to a
stem i and have n stems in a query. As the sense
product increases, the query ambiguity increases.
Figure 5 illustrates the computation of the sense
product for the query “organic soil enhancement.”
The term “organic” has the stem organ, which is a
stem for 9 different words. The accumulated num-
ber of senses for “organ” is 39. With the same ap-
proach, we obtain 7 senses for 1 “soil” and 7 senses
for “enhanc.” Therefore, multiplication 39, 7, and 7
gives us the sense product value 1911.
</bodyText>
<subsectionHeader confidence="0.988846">
4.2 Word Product
</subsectionHeader>
<bodyText confidence="0.999975">
Word product is an alternative measure of query am-
biguity after stemming. To compute the word prod-
uct, we multiply the number of words mapped to
each stem of a given query, which is formulated as:
</bodyText>
<equation confidence="0.9783925">
word product = Yn Wi (2)
i=1
</equation>
<bodyText confidence="0.999772111111111">
Wi denotes the number of words mapped to a
stem i, and n is the number of stems in a query.
We assume that the query ambiguity increases as the
word product increases. Consider the query “organic
soil enhancement” in Figure 5. We find 9 words
mapped to the stem “organ”; 3 words mapped to the
stem “soil”; 5 words mapped to the stem “enhance-
ment”. Therefore the word product for the query is
105, the product of 9, 3, and 5.
</bodyText>
<subsectionHeader confidence="0.993168">
4.3 Deplural-Stem Ratio
</subsectionHeader>
<bodyText confidence="0.999943222222222">
Deplural-stem ratio is a variation of sense product. It
takes the ratio of the sense product of a depluralized
query to the stemmed query. As the deplural-stem
ratio increases, the query ambiguity after stemming
increases. In the example illustrated in Figure 5, the
deplural-stem ratio is the sense product of the deplu-
ralized query “organic soil enhancement” divided by
the sense product of the stemmed query “organ soil
enhanc”. The deplural-stem ratio is computed as:
</bodyText>
<equation confidence="0.9862344">
Qn Pm j=1 Smj
deplural-stem ratio = Qn
i=1 Pm (3)
j=1 Dj
i=1
</equation>
<subsectionHeader confidence="0.990096">
4.4 Idf-sum
</subsectionHeader>
<bodyText confidence="0.999888615384615">
The idf-sum is the sum of the inverse document fre-
quency (IDF) of each word in the query. The IDF of
a given word measures the importance of the word
in the document collection. Queries with high values
of IDF are more likely to return relevant documents
from the collection. For example, the term “ZX-
Turbo,” describing a series of racing cars, has a high
IDF and occurs only once in the entire TREC 2004
Robust Track collection. Therefore, searching the
collection with the term “ZX-Turb” will return the
only relevant document in the collection and achieve
high precision and recall. The idf-sum is computed
as:
</bodyText>
<equation confidence="0.98753">
idf sum = Xn IDFi (4)
i=1
</equation>
<bodyText confidence="0.999990714285714">
IDFi denotes the idf of each query term i and
n is the number of words in a query. We assume
that the query ambiguity decreases as the idf-sum in-
creases. For the query “organic soil enhancement”,
the IDF for each term is 5.97082 (organic), 5.18994
(soil), and 4.86996 (enhancement). The idf-sum of
the query is 16.0307.
</bodyText>
<subsectionHeader confidence="0.998034">
4.5 Query Length
</subsectionHeader>
<bodyText confidence="0.9997345">
The length of the query is the number of words in a
query.
</bodyText>
<subsectionHeader confidence="0.989058">
4.6 Feature Validation
</subsectionHeader>
<bodyText confidence="0.999856636363636">
We performed simple linear regression on each fea-
ture as the first step to exclude ineffectual features.
Table 2 demonstrates example results of simple lin-
ear regression from the MLT query model, using the
MAP of stemmed queries as the dependent variable.
We take the logarithm of the sense product and word
product and the square root of the deplural-stem ra-
tio (ds ratio) to mitigate skewness of the data. We
included all five ambiguity properties to construct
a query-based selection model as they demonstrate
statistical significance in prediction.
</bodyText>
<equation confidence="0.9512752">
Sj (1)
Yn
i=1
Xm
j=1
</equation>
<page confidence="0.992628">
23
</page>
<figureCaption confidence="0.998989">
Figure 5: Example of ambiguity indicators on the query
“organic soil enhancement”
</figureCaption>
<bodyText confidence="0.987183888888889">
Figure 6 demonstrates the distribution of ambigu-
ity properties against the actual best text normaliza-
tion technique. It is noted that stemming is the actual
best method when a query has lower sense product,
or lower word product, or a higher idf-sum. It im-
plies that stemming is less likely to be the actual best
method as a query is ambiguous. The results demon-
strate the potential of utilizing ambiguity measures
to select the actual best text normalization technique.
</bodyText>
<sectionHeader confidence="0.986308" genericHeader="method">
5 Query-based Selection Model
</sectionHeader>
<bodyText confidence="0.999587041666667">
The cluster analysis in Section 3 suggests that the
choice of text normalization technique should be
made individually on each topic. The retrieval per-
formance would be enhanced as we choose an ap-
propriate text normalization technique for a given
topic. Given the five ambiguity properties described
in Section 4, we constructed Support Vector Regres-
sion (SVR) (Smola and Schlkopf, 2004) models to
choose between stemming, depluralization, and not
doing any text normalization for different queries.
Regression models aim to discover the relationship
between two random variables x and y. In our work,
independent variable x is a vector of the five prop-
erties described in section 4: x = (sense product,
word product, deplural-stem ratio, Idf-sum, length),
and dependent variable y is the MAP score of a
given topic. SVR has been successfully applied
for many time series and function estimation prob-
lems. We utilized training data to construct multiple
SVR models for each of nine combinations of query
models (AND, OR, and MLT) and text normaliza-
tion techniques (raw, depluralized, and stemmed
queries). For example, the regression model for an
MLT query model using stemmed queries is:
</bodyText>
<equation confidence="0.820849666666667">
Map MLT stem = 0.0853
− 0.0849 * length
+ 0.6286 * sense prod
+ 0.0171 * word prod
− 0.0774 * gap ds
+ 0.4189 * idf sum
</equation>
<bodyText confidence="0.999954375">
For a given query model, MLT, for example, we
utilized training data to construct three SVR models
each to predict the MAP scores of raw queries, de-
pluralized queries, and stemmed queries in the test
set. We then compared the predicted MAP score of a
query and selected the text normalization technique
with the highest predicted score. Figure 5 illustrates
our experiment framework on the query-based se-
lection model. We used five-fold cross-validation to
evaluate the performance of the selection model. For
each iteration (fold) we used the 4 out of the 5 parti-
tions as training data, constructing SVR models and
using the remaining fifth partition for testing. We ac-
cumulated all testing results and computed one over-
all MAP score for evaluation. Table 3 shows the re-
sults of the five-fold cross-validation performed on
249 query topics provided by the TREC 2004 Ro-
bust Track. We utilized a paired t-test to determine
the performance difference between the query-based
selection model (hybrid model) and other three text
normalization techniques. The results in Table 3
shows that the query-based selection model attained
the highest MAP score and achieved significant im-
provement.
</bodyText>
<sectionHeader confidence="0.95582" genericHeader="conclusions">
6 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.999458333333333">
This paper evaluates the performance of stemming
and depluralization on the TREC 2004 Robust track
collection. We assume that the depluralization, as
</bodyText>
<figure confidence="0.998487039215686">
9x3x5=105
39x7x7=1911
16.0307
105 organic soil enhancement
1911
5.97082 5.18994 4.86996
organic soil enhancement
39 7 7
organ soil enhanc
organization
organize
organ
9
organized
organically
organic
8
7
6
6
3
3
3
soiling
soiled
soil
Legend
5
1
1
enhancive
enhance
enhanced
enhancement
enhancer
2
2
1
1
1
5
n number of senses
n number of words
token
n IDF
organizer
3
organism
2
organs
1
</figure>
<page confidence="0.9912">
24
</page>
<table confidence="0.9997035">
Feature Coefficient R-square P-value
length -0.06811 0.07864 5.768e-05*
log(sense prod) -0.034692 0.1482 1.819e-08*
log(word prod) -0.04557 0.09426 9.78e-06*
sqrt(ds ratio) -0.021657 0.03738 0.006088*
idf sum 0.008498 0.04165 0.003747*
</table>
<tableCaption confidence="0.99929">
Table 2: Results of simple linear regression on the MAP of stemmed queries in MLT query model.
</tableCaption>
<figureCaption confidence="0.999022">
Figure 6: Boxplots of the distribution of three ambiguity properties in each actual best text normalization technique
</figureCaption>
<figure confidence="0.996224046153846">
Sense Product(log) vs. Actual Best Method
Word Product(log) vs. Actual Best Method
IDF-sum vs. Actual Best Method
dep raw stem
Actual Best Lexical Analysis
dep raw stem
Actual Best Lexical Analysis
dep raw stem
Actual Best Lexical Analysis
0 2 4 6 8 10
log(sense_prod)
0 1 2 3 4 5
log(word_prod)
5 10 15 20 25 30 35
idf_sum
Divide all query
topics into five
groups
1
Building Support
Vector Regression
(SVR) models
Testing and
predicting Mean
Average Precision
(MAP)
MAP score from
depluralized queries
2
4
5
MAP score from
stemmed queries
3
SVR for
depluralized
queries
1
2
3
4
5
SVR for
stemmed
queries
1
1
2
3
4
5
SVR for
raw queries
Training
MAP score from
raw queries
2
3
4
5
Select the text
normalization
technique with
highest predicted
MAP score
</figure>
<figureCaption confidence="0.999755">
Figure 7: Five-fold cross validation on query-based selection model (hybrid model)
</figureCaption>
<page confidence="0.956883">
25
</page>
<table confidence="0.999578428571429">
Raw Dep Stem Hybrid
AND MAP 0.1213 0.1324 0.1550 0.2094
p-value &lt;2.2e-16* &lt;2.2e-16* &lt;2.2e-16*
OR MAP 0.1851 0.1922 0.2069 0.2131
p-value 1.286e-05* 0.0003815* 0.09
MLT MAP 0.1893 0.1959 0.2093 0.2132
p-value 3.979e-05* 0.000939* 0.09677
</table>
<tableCaption confidence="0.978226">
Table 3: Paired T-test was performed to examine the differences of each text normalization techniques (raw, deplu-
ralizer, and stemmer) and query-based selection model (hybrid model). Significant differences between models are
labeled with *.
</tableCaption>
<bodyText confidence="0.999101588235294">
a low-level text-normalization technique, introduces
less ambiguity than stemming and preserves more
precise semantics of text. The experimental re-
sults demonstrate variable patterns, indicating that
some topics performed better as a depluralized query
than as a stemmed query. From Figure 4 in Sec-
tion 3, we conclude that the choice of text nor-
malization technique should be made individually
on each topic. An effective query-based selection
model would enhance information retrieval perfor-
mance. The query-based selection model utilizes
Support Vector Regression (SVR) models to predict
the mean average precision (MAP) of each query
from the ambiguity measures, and to choose an ap-
propriate normalization technique based on these
predictions. The selection is lightweight, requiring
only analysis of the topic title itself against infor-
mation readily available regarding the corpus (e.g,
term idf values). We extracted 5 measures to quan-
tify the ambiguity of a query: 1) sense product; 2)
word product; 3) deplural-stem ratio; 4) idf-sum;
5) length of a query. The constructed query-based
selection model demonstrate positive results on en-
hanced performance. The experiments reported here
show that, even when the improvement is modest
(1%), the selection model competes well with tra-
ditional approaches. To improve the model, future
work may first explore and introduce more powerful
features to the models, considering properties such
as part of speech of text. Second, future work may
explore the effect of noise and outliers in the data
to improve the accuracy of the model. Finally, ad-
ditional data mining techniques may be adopted in
future work to further improve the prediction.
</bodyText>
<sectionHeader confidence="0.999168" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999920696969697">
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA Data Min-
ing Software: An Update. SIGKDD Explorations,
1:10–18.
David A. Hull. 1996. Stemming algorithms: A case
study for detailed evaluation. Journal of the American
Society for Information Science, 47(1):70–84.
L. Kaufman and P.J. Rousseeuw. 1990. Finding groups
in data. An introduction to cluster analysis. Wiley,
New York.
Robert Krovetz. 2000. Viewing morphology as an in-
ference process. Artificial Intelligence, 118(1-2):277–
294, April.
Christopher D Manning and Hinrich Schtze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press, Cambridge, Mass.
Paul McNamee, Charles Nicholas, and James Mayfield.
2008. Don’t have a stemmer?: be un+concern+ed.
In Proceedings of the 31st International ACM SIGIR
Conference on Research and Development in Informa-
tion retrieval, pages 813–814, Singapore, Singapore.
ACM.
Ellen Riloff. 1995. Little words can make a big differ-
ence for text classification. In Proceedings of the 18th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 130–
136, Seattle, Washington, United States. ACM.
Alex J. Smola and Bernhard Schlkopf. 2004. A tutorial
on support vector regression. Statistics and Comput-
ing, 14(3):199–222.
E. M. Voorhees. 2006. The TREC 2005 robust track.
In ACM SIGIR Forum, volume 40, pages 41–48. ACM
New York, NY, USA.
</reference>
<page confidence="0.998058">
26
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.610318">
<title confidence="0.8357195">Query-based Text Normalization Selection Models for Enhanced Retrieval Accuracy</title>
<author confidence="0.85086">Si-Chi Chin Rhonda DeCook W Nick Street David</author>
<affiliation confidence="0.9934305">The University of Iowa City,</affiliation>
<email confidence="0.92698">rhonda-decook,nick-street,</email>
<abstract confidence="0.998603115384615">Text normalization transforms words into a base form so that terms from common equivalent classes match. Traditionally, information retrieval systems employ stemming techniques to remove derivational affixes. Depluralization, the transformation of plurals into singular forms, is also used as a low-level text normalization technique to preserve more precise lexical semantics of text. Experiment results suggest that the choice of text normalization technique should be made individually on each topic to enhance information retrieval accuracy. This paper proposes a hybrid approach, constructing a query-based selection model to select the appropriate text normalization technique (stemming, depluralization, or not doing any text normalization). The selection model utilized ambiguity properties extracted from queries to train a composite of Support Vector Regression (SVR) models to predict a text normalization technique that yields the highest Mean Average Precision (MAP). Based on our study, such a selection model holds promise in improving retrieval accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>E Frank</author>
<author>G Holmes</author>
<author>B Pfahringer</author>
<author>P Reutemann</author>
<author>I H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<pages>1--10</pages>
<contexts>
<context position="5626" citStr="Hall et al., 2009" startWordPosition="858" endWordPosition="861">ee indices: indices of raw text, depluralized text, and stemmed text. The Postgres database stores each document of the collection, the query topics of the TREC 2004 Robust Track, and results of experiments. The ambiguity properties for each query is also computed in the Postgres system. We query Lucene indices to obtain the top 1,000 relevant results and compute Mean Average Precision (MAP) with the trec eval program to evaluate performance. We use R to analyze performance scores, generate descriptive charts, conduct non-parametric statistical tests, and perform a paired t-test. We use Weka (Hall et al., 2009) to construct query-based selection model that incorporates multiple Support Vector Regression (SVR) models. 2.1 Query Models The TREC 2004 Robust Track provides 249 query topics; each includes a title, a short description, and a narrative (usually one-paragraph). We selected three basic query models as a modest baseline to demonstrate the effect of different text normalization techniques. Our future work will exploit other ranking models such as BM25 and LMIR. The three query models used in the experiment are: (1) boolean search with the title words of topics concatenated with logical AND (e.</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I.H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 1:10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Hull</author>
</authors>
<title>Stemming algorithms: A case study for detailed evaluation.</title>
<date>1996</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="1869" citStr="Hull (1996)" startWordPosition="265" endWordPosition="266">a selection model holds promise in improving retrieval accuracy. 1 Introduction Stemming removes derivational affixes of terms therefore allowing terms from common equivalence classes to be clustered. However, stemming also introduces noise by mapping words of different concepts or meanings into one base form, thus impeding word-sense disambiguation. Depluralization, the conversion of plural word forms to singular form, preserves more precise semantics of text than stemming (Krovetz, 2000). 19 Empirical research has demonstrated the ambivalent effect of stemming on text retrieval performance. Hull (1996) conducted a comprehensive case study on the effects of four stemmer techniques and the removal of plural “s” 1 on retrieval performance. Hull suggested that the adoption of stemming is beneficial but plural removal is as well competitive when the size of documents is small. Prior research (Manning and Schtze, 1999; McNamee et al., 2008) indicated that traditional stemming, though still benefiting some queries, would not necessarily enhance the average retrieval performance. In addition, stemming was considered one of the technique failures undermining retrieval performance in the TREC 2004 Ro</context>
</contexts>
<marker>Hull, 1996</marker>
<rawString>David A. Hull. 1996. Stemming algorithms: A case study for detailed evaluation. Journal of the American Society for Information Science, 47(1):70–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kaufman</author>
<author>P J Rousseeuw</author>
</authors>
<title>Finding groups in data. An introduction to cluster analysis.</title>
<date>1990</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="9461" citStr="Kaufman and Rousseeuw, 1990" startWordPosition="1461" endWordPosition="1464">uel OR automobil.” MoreLikeThis (MLT) is the Lucene class used for cosine similarity retrieval. A term vector score appends each word in the topic. techniques are within 2%. To visualize the relative performances among three text normalization techniques, we standardized the three MAP values for a single topic (one from each text normalization technique) to have mean 0 and standard deviation of 1. The result provides a 3- value pattern emphasizing the ordering of the MAPs across the text normalization techniques, rather than the raw MAP values themselves. We then used the K-medoids algorithm (Kaufman and Rousseeuw, 1990) to cluster the transformed data, applying Euclidean distance as the distance measure. Figure 4 is an example of 9 constructed clusters based on the MAP scores of the MLT query model. In a cluster, a line represents the standardized MAP value of a topic on each text normalization technique. Given the small differences in aggregate MAP performance, it is interesting to note that the clusters demonstrate variable patterns, indicating that some topics performed better as a depluralized query than a stemmed query. The cluster analysis suggests that the choice of text normalization technique should</context>
</contexts>
<marker>Kaufman, Rousseeuw, 1990</marker>
<rawString>L. Kaufman and P.J. Rousseeuw. 1990. Finding groups in data. An introduction to cluster analysis. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Krovetz</author>
</authors>
<title>Viewing morphology as an inference process.</title>
<date>2000</date>
<journal>Artificial Intelligence,</journal>
<pages>118--1</pages>
<contexts>
<context position="1752" citStr="Krovetz, 2000" startWordPosition="247" endWordPosition="248">o predict a text normalization technique that yields the highest Mean Average Precision (MAP). Based on our study, such a selection model holds promise in improving retrieval accuracy. 1 Introduction Stemming removes derivational affixes of terms therefore allowing terms from common equivalence classes to be clustered. However, stemming also introduces noise by mapping words of different concepts or meanings into one base form, thus impeding word-sense disambiguation. Depluralization, the conversion of plural word forms to singular form, preserves more precise semantics of text than stemming (Krovetz, 2000). 19 Empirical research has demonstrated the ambivalent effect of stemming on text retrieval performance. Hull (1996) conducted a comprehensive case study on the effects of four stemmer techniques and the removal of plural “s” 1 on retrieval performance. Hull suggested that the adoption of stemming is beneficial but plural removal is as well competitive when the size of documents is small. Prior research (Manning and Schtze, 1999; McNamee et al., 2008) indicated that traditional stemming, though still benefiting some queries, would not necessarily enhance the average retrieval performance. In </context>
</contexts>
<marker>Krovetz, 2000</marker>
<rawString>Robert Krovetz. 2000. Viewing morphology as an inference process. Artificial Intelligence, 118(1-2):277– 294, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Schtze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="2185" citStr="Manning and Schtze, 1999" startWordPosition="315" endWordPosition="318">se form, thus impeding word-sense disambiguation. Depluralization, the conversion of plural word forms to singular form, preserves more precise semantics of text than stemming (Krovetz, 2000). 19 Empirical research has demonstrated the ambivalent effect of stemming on text retrieval performance. Hull (1996) conducted a comprehensive case study on the effects of four stemmer techniques and the removal of plural “s” 1 on retrieval performance. Hull suggested that the adoption of stemming is beneficial but plural removal is as well competitive when the size of documents is small. Prior research (Manning and Schtze, 1999; McNamee et al., 2008) indicated that traditional stemming, though still benefiting some queries, would not necessarily enhance the average retrieval performance. In addition, stemming was considered one of the technique failures undermining retrieval performance in the TREC 2004 Robust Track (Voorhees, 2006). Prior research also noted the semantic differences between plurals and singulars. Riloff (1995) indicated that plural and singular nouns are distinct because plural nouns usually pertain to the “general types of incidents,” while singular nouns often pertain to “a specific incident.” Ne</context>
</contexts>
<marker>Manning, Schtze, 1999</marker>
<rawString>Christopher D Manning and Hinrich Schtze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Charles Nicholas</author>
<author>James Mayfield</author>
</authors>
<title>Don’t have a stemmer?: be un+concern+ed.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st International ACM SIGIR Conference on Research and Development in Information retrieval,</booktitle>
<pages>813--814</pages>
<publisher>ACM.</publisher>
<location>Singapore, Singapore.</location>
<contexts>
<context position="2208" citStr="McNamee et al., 2008" startWordPosition="319" endWordPosition="323">d-sense disambiguation. Depluralization, the conversion of plural word forms to singular form, preserves more precise semantics of text than stemming (Krovetz, 2000). 19 Empirical research has demonstrated the ambivalent effect of stemming on text retrieval performance. Hull (1996) conducted a comprehensive case study on the effects of four stemmer techniques and the removal of plural “s” 1 on retrieval performance. Hull suggested that the adoption of stemming is beneficial but plural removal is as well competitive when the size of documents is small. Prior research (Manning and Schtze, 1999; McNamee et al., 2008) indicated that traditional stemming, though still benefiting some queries, would not necessarily enhance the average retrieval performance. In addition, stemming was considered one of the technique failures undermining retrieval performance in the TREC 2004 Robust Track (Voorhees, 2006). Prior research also noted the semantic differences between plurals and singulars. Riloff (1995) indicated that plural and singular nouns are distinct because plural nouns usually pertain to the “general types of incidents,” while singular nouns often pertain to “a specific incident.” Nevertheless, prior resea</context>
</contexts>
<marker>McNamee, Nicholas, Mayfield, 2008</marker>
<rawString>Paul McNamee, Charles Nicholas, and James Mayfield. 2008. Don’t have a stemmer?: be un+concern+ed. In Proceedings of the 31st International ACM SIGIR Conference on Research and Development in Information retrieval, pages 813–814, Singapore, Singapore. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Little words can make a big difference for text classification.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>130--136</pages>
<publisher>ACM.</publisher>
<location>Seattle, Washington, United States.</location>
<contexts>
<context position="2593" citStr="Riloff (1995)" startWordPosition="377" endWordPosition="378"> on retrieval performance. Hull suggested that the adoption of stemming is beneficial but plural removal is as well competitive when the size of documents is small. Prior research (Manning and Schtze, 1999; McNamee et al., 2008) indicated that traditional stemming, though still benefiting some queries, would not necessarily enhance the average retrieval performance. In addition, stemming was considered one of the technique failures undermining retrieval performance in the TREC 2004 Robust Track (Voorhees, 2006). Prior research also noted the semantic differences between plurals and singulars. Riloff (1995) indicated that plural and singular nouns are distinct because plural nouns usually pertain to the “general types of incidents,” while singular nouns often pertain to “a specific incident.” Nevertheless, prior research has not closely examined the effect of the change of the semantics caused by different level of text normalization techniques. In our work, we conducted extensive experiments on the TREC 2004 Robust track collection to evaluate the effect of stemming and depluralization on information retrieval. In addition, we quantify the ambiguity of a query, extracting five ambiguity propert</context>
</contexts>
<marker>Riloff, 1995</marker>
<rawString>Ellen Riloff. 1995. Little words can make a big difference for text classification. In Proceedings of the 18th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 130– 136, Seattle, Washington, United States. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex J Smola</author>
<author>Bernhard Schlkopf</author>
</authors>
<title>A tutorial on support vector regression.</title>
<date>2004</date>
<journal>Statistics and Computing,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="17301" citStr="Smola and Schlkopf, 2004" startWordPosition="2792" endWordPosition="2795">hat stemming is less likely to be the actual best method as a query is ambiguous. The results demonstrate the potential of utilizing ambiguity measures to select the actual best text normalization technique. 5 Query-based Selection Model The cluster analysis in Section 3 suggests that the choice of text normalization technique should be made individually on each topic. The retrieval performance would be enhanced as we choose an appropriate text normalization technique for a given topic. Given the five ambiguity properties described in Section 4, we constructed Support Vector Regression (SVR) (Smola and Schlkopf, 2004) models to choose between stemming, depluralization, and not doing any text normalization for different queries. Regression models aim to discover the relationship between two random variables x and y. In our work, independent variable x is a vector of the five properties described in section 4: x = (sense product, word product, deplural-stem ratio, Idf-sum, length), and dependent variable y is the MAP score of a given topic. SVR has been successfully applied for many time series and function estimation problems. We utilized training data to construct multiple SVR models for each of nine combi</context>
</contexts>
<marker>Smola, Schlkopf, 2004</marker>
<rawString>Alex J. Smola and Bernhard Schlkopf. 2004. A tutorial on support vector regression. Statistics and Computing, 14(3):199–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>The TREC</title>
<date>2006</date>
<journal>In ACM SIGIR Forum,</journal>
<volume>40</volume>
<pages>41--48</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2496" citStr="Voorhees, 2006" startWordPosition="363" endWordPosition="364"> comprehensive case study on the effects of four stemmer techniques and the removal of plural “s” 1 on retrieval performance. Hull suggested that the adoption of stemming is beneficial but plural removal is as well competitive when the size of documents is small. Prior research (Manning and Schtze, 1999; McNamee et al., 2008) indicated that traditional stemming, though still benefiting some queries, would not necessarily enhance the average retrieval performance. In addition, stemming was considered one of the technique failures undermining retrieval performance in the TREC 2004 Robust Track (Voorhees, 2006). Prior research also noted the semantic differences between plurals and singulars. Riloff (1995) indicated that plural and singular nouns are distinct because plural nouns usually pertain to the “general types of incidents,” while singular nouns often pertain to “a specific incident.” Nevertheless, prior research has not closely examined the effect of the change of the semantics caused by different level of text normalization techniques. In our work, we conducted extensive experiments on the TREC 2004 Robust track collection to evaluate the effect of stemming and depluralization on informatio</context>
</contexts>
<marker>Voorhees, 2006</marker>
<rawString>E. M. Voorhees. 2006. The TREC 2005 robust track. In ACM SIGIR Forum, volume 40, pages 41–48. ACM New York, NY, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>