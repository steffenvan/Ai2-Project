<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9968735">
Inducing Word and Part-of-Speech with
Pitman-Yor Hidden Semi-Markov Models
</title>
<author confidence="0.864917">
Kei Uchiumi Hiroshi Tsukahara
</author>
<affiliation confidence="0.818574">
Denso IT Laboratory, Inc.
</affiliation>
<address confidence="0.7690945">
Shibuya Cross Tower 28F
2-15-1 Shibuya, Tokyo, Japan
</address>
<email confidence="0.960858">
{kuchiumi,htsukahara}@d-itlab.co.jp
</email>
<note confidence="0.805699666666667">
Daichi Mochihashi
The Institute of Statistical Mathematics
10-3 Midori-cho, Tachikawa city
</note>
<address confidence="0.664528">
Tokyo, Japan
</address>
<email confidence="0.978355">
daichi@ism.ac.jp
</email>
<sectionHeader confidence="0.994311" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978176470588">
We propose a nonparametric Bayesian
model for joint unsupervised word seg-
mentation and part-of-speech tagging
from raw strings. Extending a previous
model for word segmentation, our model
is called a Pitman-Yor Hidden Semi-
Markov Model (PYHSMM) and consid-
ered as a method to build a class n-gram
language model directly from strings,
while integrating character and word level
information. Experimental results on stan-
dard datasets on Japanese, Chinese and
Thai revealed it outperforms previous re-
sults to yield the state-of-the-art accura-
cies. This model will also serve to analyze
a structure of a language whose words are
not identified a priori.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981653061225">
Morphological analysis is a staple of natural lan-
guage processing for broad languages. Especially
for some East Asian languages such as Japanese,
Chinese or Thai, word boundaries are not explic-
itly written, thus morphological analysis is a cru-
cial first step for further processing. Note that
also in Latin and old English, scripts were orig-
inally written with no word indications (scripta
continua), but people felt no difficulty reading
them. Here, morphological analysis means word
segmentation and part-of-speech (POS) tagging.
For this purpose, supervised methods have of-
ten been employed for training. However, to
train such supervised classifiers, we have to pre-
pare a large amount of training data with cor-
rect annotations, in this case, word segmentation
and POS tags. Creating and maintaining these
data is not only costly but also very difficult, be-
cause generally there are no clear criteria for ei-
ther “correct” segmentation or POS tags. In fact,
since there are different standards for Chinese
word segmentation, widely used SIGHAN Bake-
off dataset (Emerson, 2005) consists of multiple
parts employing different annotation schemes.
Lately, this situation has become increasingly
important because there are strong demands for
processing huge amounts of text in consumer gen-
erated media such as Twitter, Weibo or Facebook
(Figure 1). They contain a plethora of colloquial
expressions and newly coined words, including
sentiment expressions such as emoticons that can-
not be covered by fixed supervised data.
To automatically recognize such linguistic phe-
nomena beyond small “correct” supervised data,
we have to extract linguistic knowledge from the
statistics of strings themselves in an unsupervised
fashion. Needless to say, such methods will also
contribute to analyzing speech transcripts, classic
texts, or even unknown languages. From a scien-
tific point of view, it is worth while to find “words”
and their part-of-speech purely from a collection
of strings without any preconceived assumptions.
To achieve that goal, there have been two kinds
of approaches: heuristic methods and statisti-
cal generative models. Heuristic methods are
based on basic observations such that word bound-
aries will often occur at the place where predic-
tive entropy of characters is large (i.e. the next
character cannot be predicted without assuming
</bodyText>
<equation confidence="0.8166781">
ローラのときに涙かブハァってなりました∩ (´;ヮ;
`)∩~~
真樹なんてこんな中2くさい事胸張って言えるぞぉ!
今日ね! k�ラボ6&apos;るいと
いで~
(* &amp;quot; V - � r
どうせ明日の昼ごろしれっと不在表入ってるんだろ
うなぁ。
テレ東はいつものネトウヨホルホル VTR 鑑賞番組
してんのか
</equation>
<figureCaption confidence="0.934481">
Figure 1: Sample of Japanese Twitter text that
</figureCaption>
<bodyText confidence="0.799207666666667">
is difficult to analyze by ordinary supervised seg-
mentation. It contains a lot of novel words, emoti-
cons, and colloquial expressions.
</bodyText>
<page confidence="0.942403">
1774
</page>
<note confidence="0.97646">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1774–1782,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999960708333333">
the next word). By formulating such ideas as
search or MDL problems of given coding length&apos;,
word boundaries are found in an algorithmic fash-
ion (Zhikov et al., 2010; Magistry and Sagot,
2013). However, such methods have difficulty in-
corporating higher-order statistics beyond simple
heuristics, such as word transitions, word spelling
formation, or word length distribution. Moreover,
they usually depends on tuning parameters like
thresholds that cannot be learned without human
intervention.
In contrast, statistical models are ready to in-
corporate all such phenomena within a consistent
statistical generative model of a string, and often
prove to work better than heuristic methods (Gold-
water et al., 2006; Mochihashi et al., 2009). In
fact, the statistical methods often include the cri-
teria of heuristic methods at least in a conceptual
level, which is noted in (Mochihashi et al., 2009)
and also explained later in this paper. In a statisti-
cal model, each word segmentation w of a string
s is regarded as a hidden stochastic variable, and
the unsupervised learning of word segmentation is
formulated as a maximization of a probability of
</bodyText>
<equation confidence="0.983310666666667">
w given s:
argmax p(w|s) . (1)
W
</equation>
<bodyText confidence="0.99029080952381">
This means that we want the most “natural” seg-
mentation w that have a high probability in a lan-
guage model p(w|s).
Lately, Chen et al. (2014) proposed an interme-
diate model between heuristic and statistical mod-
els as a product of character and word HMMs.
However, these two models do not have informa-
tion shared between the models, which is not the
case with generative models.
So far, these approaches only find word seg-
mentation, leaving part-of-speech information be-
hind. These two problems are not actually in-
dependent but interrelated, because knowing the
part-of-speech of some infrequent or unknown
word will give contextual clues to word segmen-
tation, and vice versa. For example, in Japanese
*tttttt
can be segmented into not only *tt/t/tt/t
(plum/too/peach/too), but also into *tt/tt/
tt (plum/peach/peach), which is ungrammati-
cal. However, we could exclude the latter case
</bodyText>
<footnote confidence="0.99263475">
&apos;For example, Zhikov et al. (2010) defined a coding
length using character n-grams plus MDL penalty. Since
this can be interpreted as a crude “likelihood” and a prior,
its essence is similar but driven by a quite simplistic model.
</footnote>
<figureCaption confidence="0.89571325">
Figure 2: NPYLM represented in a hierarchical
Chinese restaurant process. Here, a character oc-
gram HPYLM is embedded in a word n-gram
HPYLM and learned jointly during inference.
</figureCaption>
<bodyText confidence="0.999751666666667">
if we leverage knowledge that a state sequence
N/P/N/P is much more plausible in Japanese than
N/N/N from the part-of-speech information. Sirts
and Alum¨ae (2012) treats a similar problem of
POS induction with unsupervised morphological
segmentation, but they know the words in advance
and only consider segmentation within a word.
For this objective, we attempt to maximize the
joint probability of words and tags:
</bodyText>
<equation confidence="0.974701">
argmax p(w, z|s) a p(w, z, s) (2)
W,Z
</equation>
<bodyText confidence="0.999865">
From the expression above, this amounts to
building a generative model of a string s with
words w and tags z along with an associated infer-
ence procedure. We solve this problem by extend-
ing previous generative model of word segmenta-
tion. Note that heuristic methods are never able to
model the hidden tags, and only statistical genera-
tive models can accommodate this objective.
This paper is organized as follows. In Sec-
tion 2, we briefly introduce NPYLM (Mochihashi
et al., 2009) on which our extension is based. Sec-
tion 3 extends it to include hidden states to yield a
hidden semi-Markov models (Murphy, 2002), and
we describe its inference procedure in Section 4.
We conduct experiments on some East Asian lan-
guages in Section 5. Section 6 discusses implica-
tions of our model and related work, and Section 7
concludes the paper.
</bodyText>
<sectionHeader confidence="0.864092" genericHeader="method">
2 Nested Pitman-Yor Language Model
</sectionHeader>
<bodyText confidence="0.995347666666667">
Our joint model of words and states is an
extension of the Nested Pitman-Yor Language
Model (Mochihashi et al., 2009) of a string, which
in turn is an extension of a Bayesian n-gram lan-
guage model called Hierarchical Pitman-Yor Lan-
guage Model (HPYLM) (Teh, 2006).
</bodyText>
<figure confidence="0.768677">
Character HPYLM
Word HPYLM
</figure>
<page confidence="0.979159">
1775
</page>
<bodyText confidence="0.994123222222222">
HPYLM is a nonparametric Bayesian model of
n-gram distribution based on the Pitman-Yor pro-
cess (Pitman and Yor, 1997) that generates a dis-
crete distribution G as G ∼ PY(G0, d, θ). Here,
d is a discount factor, “parent” distribution G0 is
called a base measure and θ controls how similar
G is to G0 in expectation. In HPYLM, n-gram
distribution Gn = {p(wt|wt−1 · · · wt−(n−1))} is
assumed to be generated from the Pitman-Yor pro-
</bodyText>
<equation confidence="0.9132385">
cess
Gn ∼ PY(Gn−1, dn, θn) , (3)
</equation>
<bodyText confidence="0.999988695652174">
where the base measure Gn−1 is an (n−1)-gram
distribution generated recursively in accordance
with (3). Note that there are different Gn for each
n-gram history h = wt−1 · · · wt−(n−1). When we
reach the unigram G1 and need to use a base mea-
sure G0, i.e. prior probabilities of words, HPYLM
usually uses a uniform distribution over the lexi-
con.
However, in the case of unsupervised word seg-
mentation, every sequence of characters could be
a word, thus the size of the lexicon is unbounded.
Moreover, prior probability of forming a word
should not be uniform over all sequences of char-
acters: for example, English words rarely begin
with ‘gme’ but tend to end with ’-ent’ like in seg-
ment. To model this property, NPYLM assumes
that word prior G0 is generated from character
HPYLM to model a well-formedness of w. In
practice, to avoid dependency on n in the charac-
ter model, we used an ∞-gram VPYLM (Mochi-
hashi and Sumita, 2008) in this research. Finally,
NPYLM gives an n-gram probability of word w
given a history h recursively by integrating out
</bodyText>
<equation confidence="0.956339333333333">
Gn,
c(w|h)−d·thw θ+d·th ·
p(w|h) = θ+c(h) + θ+c(h) p(w|h′) ,
</equation>
<bodyText confidence="0.93948375">
(4)
where h′ is the shorter history of (n −1)-grams.
c(w|h), c(h) = Ew c(w|h) are n-gram counts of
w appearing after h, and thw, th · = Ew thw are
associated latent variables explained below. In
case the history h is already empty at the unigram,
p(w|h′) = p0(w) is computed from the character
∞-grams for the word w =c1 · · · ck :
</bodyText>
<equation confidence="0.999795">
p0(w) = 7*1 ··· ck) (5)
= i is 1 p(ci|ci−1 ··· c1) .(6)
</equation>
<bodyText confidence="0.999995">
In practice, we further corrected (6) so that a word
length follows a mixture of Poisson distributions.
For details, see (Mochihashi et al., 2009).
When we know word segmentation w of the
data, the probability above can be computed by
adding each n-gram count of w given h to the
model, i.e. increment c(w|h) in accordance with
a hierarchical Chinese restaurant process associ-
ated with HPYLM (Figure 2). When each n-gram
count called a customer is inferred to be actually
generated from (n−1)-grams, we send its proxy
customer for smoothing to the parent restaurant
and increment thw, and this process will recurse.
Notice that if a word w is never seen in w, its
proxy customer is eventually sent to the parent
restaurant of unigrams. In that case2, w is decom-
posed to its character sequence c1 · · · ck and this is
added to the character HPYLM in the same way,
making it a little “clever” about possible word
spellings.
Inference Because we do not know word seg-
mentation w beforehand, we begin with a trivial
segmentation in which every sentence is a single
word3. Then, we iteratively refine it by sampling
a new word segmentation w(s) of a sentence s
in a Markov Chain Monte Carlo (MCMC) frame-
work using a dynamic programming, as is done
with PCFG by (Johnson et al., 2007) shown in Fig-
ure 3 where we omit MH steps for computational
reasons. Further note that every hyperparameter
dn, θn of NPYLM can be sampled from the poste-
rior in a Bayesian fashion, as opposed to heuristic
methods that rely on a development set for tuning.
For details, see Teh (2006).
</bodyText>
<sectionHeader confidence="0.949353" genericHeader="method">
3 Pitman-Yor Hidden Semi-Markov
Models
</sectionHeader>
<bodyText confidence="0.859918333333333">
NPYLM is a complete generative model of a
string, that is, a hierarchical Bayesian n-gram lan-
Input: a collection of strings S
Add initial segmentation w(s) to Θ
for j = 1 ··· J do
for s in randperm (S) do
Remove customers of w(s) from Θ
Sample w(s) according to p(w|s, Θ)
Add customers of w(s) to Θ
</bodyText>
<subsectionHeader confidence="0.512397">
end for
</subsectionHeader>
<bodyText confidence="0.771602">
Sample hyperparameters of Θ
</bodyText>
<listItem confidence="0.344562">
end for
</listItem>
<figureCaption confidence="0.982988">
Figure 3: MCMC inference of NPYLM Θ.
</figureCaption>
<footnote confidence="0.98289175">
2To be precise, this occurs whenever thw is incremented
in the unigram restaurant.
3Note that a child first memorizes what his mother says as
a single word and gradually learns the lexicon.
</footnote>
<page confidence="0.987847">
1776
</page>
<figureCaption confidence="0.753724">
Figure 4: Graphical model of PYHSMM in a bi-
gram case. White nodes are latent variables, and
the shaded node is the observation. We only ob-
serve a string s that is a concatenation of hidden
words w1 · · · wT.
</figureCaption>
<bodyText confidence="0.999692913043479">
guage model combining words and characters. It
can also be viewed as a way to build a Bayesian
word n-gram language model directly from a se-
quence of characters, without knowing “words” a
priori.
One possible drawback of it is a lack of part-of-
speech: as described in the introduction, grammat-
ical states will contribute much to word segmenta-
tion. Also, from a computational linguistics point
of view, it is desirable to induce not only words
from strings but also their part-of-speech purely
from the usage statistics (imagine applying it to an
unknown language or colloquial expressions). In
classical terms, it amounts to building a class n-
gram language model where both class and words
are unknown to us. Is this really possible?
Yes, we can say it is possible. The idea is sim-
ple: we augment the latent states to include a hid-
den part-of-speech zt for each word wt, which
is again unknown as displayed in Figure 4. As-
suming wt is generated from zt’-th NPYLM, we
can draw a generative model of a string s as fol-
lows:
</bodyText>
<equation confidence="0.93366075">
z0 =BOS; s=c (an empty string).
fort = 1 ··· T do
Draw zt ∼ p(zt|zt−1) ,
Draw wt ∼ p(wt|w1 · · · wt−1, zt) ,
</equation>
<bodyText confidence="0.919300142857143">
Append wt to s.
end for
Here, z0 = BOS and zT+1 = EOS are distin-
guished states for beginning and end of a sentence,
respectively. For the transition probability of hid-
den states, we put a HPY process prior as (Blun-
som and Cohn, 2011):
</bodyText>
<equation confidence="0.997662">
p(zt|zt−1) ∼ HPY(d, θ) (7)
</equation>
<bodyText confidence="0.809546333333333">
with the final base measure being a uniform dis-
tribution over the states. The word boundaries are
���M��
</bodyText>
<subsectionHeader confidence="0.309969">
&amp;quot;friends .f each c.untry&amp;quot;
</subsectionHeader>
<bodyText confidence="0.958519">
Figure 5: Graphical representation of sampling
words and POSs. Each cell corresponds to an in-
side probability α[t][k][z]. Note each cell is not
always connected to adjacent cells, because of an
overlap of substrings associated with each cell.
known in (Blunsom and Cohn, 2011), but in our
case it is also learned from data at the same time.
Note that because wt depends on already gener-
ated words w1 · · · wt−1, our model is considered
as an autoregressive HMM rather than a vanilla
HMM, as shown in Figure 4 (wt−1 → wt depen-
dency).
Since segment models like NPYLM have seg-
ment lengths as hidden states, they are called semi-
Markov models (Murphy, 2002). In contrast, our
model also has hidden part-of-speech, thus we
call it a Pitman-Yor Hidden Semi-Markov model
(PYHSMM).a Note that this is considered as a
generative counterpart of a discriminative model
known as a hidden semi-Markov CRF (Sarawagi
and Cohen, 2005).
</bodyText>
<sectionHeader confidence="0.999826" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.999928">
Inference of PYHSMM proceeds in almost the
same way as NPYLM in Figure 3: For each sen-
tence, first remove the customers associated with
the old segmentation similarly to adding them. Af-
ter sampling a new segmentation and states, the
model is updated by adding new customers in ac-
cordance with the new segmentation and hidden
states.
</bodyText>
<subsectionHeader confidence="0.999507">
4.1 Sampling words and states
</subsectionHeader>
<bodyText confidence="0.7881551">
To sample words and states (part-of-speech)
jointly, we first compute inside probabilities for-
ward from BOS to EOS and sample backwards
from EOS according to the Forward filtering-
Backward sampling algorithm (Scott, 2002). This
4Lately, Johnson et al. (2013) proposed a nonparamet-
ric Bayesian hidden semi-Markov models for general state
spaces. However, it depends on a separate distribution for a
state duration, thus is clealy different from ours for a natural
language.
</bodyText>
<figure confidence="0.9873835">
zt−1 zt zt+1
Observation s
wt−1 wt
wt+1
· · ·
· · ·
· · ·
· · ·
Word leneth
nde-
T1me t
MU]
</figure>
<page confidence="0.977212">
1777
</page>
<bodyText confidence="0.999899727272727">
can be regarded as a “stochastic Viterbi” algorithm
that has the advantage of not being trapped in local
minima, since it is a valid move of a Gibbs sampler
in a Bayesian model.
For a word bigram case for simplicity, inside
variable α[t][k][z] is a probability that a substring
c1 · · · ct of a string s = c1 · · · cN is generated with
its last k characters being a word, generated from
state z as shown in Figure 5. From the definition
of PYHSMM, this can be computed recursively as
follows:
</bodyText>
<equation confidence="0.953889333333333">
p(ctt−k|ct−k
t−k−j+1, z)
p(z|y)α[t−k][j][y] . (8)
</equation>
<bodyText confidence="0.997697333333333">
Here, cts is a substring cs · · · ct and L (&lt; t) is the
maximum length of a word, and K is the number
of hidden states.5
In Figure 5, each cell represents α[t][k][z] and
a single path connecting from EOS to BOS cor-
responds to a word sequence w and its state se-
quence z. Note that each cell is not always con-
nected to adjacent cells (we omit the arrows), be-
cause the length-k substring associated with each
cell already subsumes that of neighborhood cells.
Once w and z are sampled, each wt is added to
zt’-th NPYLM to update its statistics.
</bodyText>
<subsectionHeader confidence="0.7076885">
4.2 Efficient computation by the Negative
Binomial generalized linear model
</subsectionHeader>
<bodyText confidence="0.997519625">
Inference algorithm of PYHSMM has a computa-
tional complexity of O(K2L2N), where N is a
length of the string to analyze. To reduce com-
putations it is effective to put a small L of maxi-
mum word length, but it might also ignore occa-
sionally long words. Since these long words are
often predictable from some character level infor-
mation including suffixes or character types, in a
</bodyText>
<tableCaption confidence="0.670269333333333">
Table 1: Features used for the Negative Binomial
generalized linear model for maximum word
length prediction.
</tableCaption>
<footnote confidence="0.88422375">
5For computational reasons, we do not pursue using a
Dirichlet process to yield an infinite HMM (Van Gael et al.,
2009), but it is straightforward to extend our PYHSMM to
iHMM.
</footnote>
<bodyText confidence="0.999667166666667">
semi-supervised setting we employ a Negative Bi-
nomial generalized linear model (GLM) for set-
ting Lt adaptively for each character position t in
the corpus.
Specifically, we model the word length ℓ by a
Negative Binomial distribution (Cook, 2009):
</bodyText>
<equation confidence="0.981246">
F(r+ℓ)
ℓ � NB(ℓ|r,p) = pℓ(1 − p)r . (9)
F(r)ℓ!
</equation>
<bodyText confidence="0.999307833333333">
This counts the number of failures of Bernoulli
draws with probability (1−p) before r’th success.
For our model, note that Negative Binomial is ob-
tained from a Poisson distribution Po(λ) whose
parameter λ again follows a Gamma distribution
Ga(r, b) and integrated out:
</bodyText>
<equation confidence="0.990737">
fp(ℓ|r, b) = Po(ℓ|λ)Ga(λ|r, b)dλ (10)
F(r+ℓ)( b lP (11b) r(r) ℓ! 1+b J
</equation>
<bodyText confidence="0.970987714285714">
This construction exactly mirrors the Poisson-
Gamma word length distribution in (Mochihashi
et al., 2009) with sampled λ. Therefore, our Neg-
ative Binomial is basically a continuous analogue
of the word length distribution in NPYLM.6
Since r &gt; 0 and 0 &lt;p &lt; 1, we employ an expo-
nential and sigmoidal linear regression
</bodyText>
<equation confidence="0.942693">
r = exp(wTr f), p = σ(wTp f) (12)
</equation>
<bodyText confidence="0.939224181818182">
where σ(x) is a sigmoid function and wr, wp are
weight vectors to learn. f is a feature vector com-
puted from the substring c1 · · · ct, including f0 =1
for a bias term. Table 1 shows the features we
used for this Negative Binomial GLM. Since Neg-
ative Binomial GLM is not convex in wr and wp,
we endow a Normal prior N(0, σ2I) for them and
used a random walk MCMC for inference.
Predicting Lt Once the model is obtained, we
can set Lt adaptively as the time where the cu-
mulative probability of ℓ exceeds some threshold
θ (we used θ = 0.99). Table 2 shows the preci-
sion of predicting maximum word length learned
from 10,000 sentences from each set: it measures
whether the correct word boundary in test data is
included in the predicted Lt.
Overall it performs very well with high preci-
sion, and works better for longer words that cannot
be accommodated with a fixed maximum length.
6Because NPYLM employs a mixture of Poisson distri-
butions for each character type of a substring, this correspon-
dence is not exact.
Type
Feature
Character at time t−i (0&lt;i&lt;1)
Character type at time t−i (0&lt;i&lt;4)
# of the same character types before t
# of times character types changed
within 8 characters before t
ci
ti
cont
ch
</bodyText>
<equation confidence="0.9037066">
L
j=1
α[t][k][z] =
K
y=1
</equation>
<page confidence="0.978245">
1778
</page>
<table confidence="0.999180142857143">
Lang Dataset Training Test
Kyoto corpus 37,400 1,000
Ja BCCWJ OC 20,000 1,000
SIGHAN MSR 86,924 3,985
Zh SIGHAN CITYU 53,019 1,492
SIGHAN PKU 19,056 1,945
Th InterBEST Novel 1,000 1,000
</table>
<tableCaption confidence="0.858975666666667">
Table 3: Datasets used for evaluation. Abbrevi-
ations: Ja=Japanese, Zh=Chinese, Th=Thai lan-
guage.
</tableCaption>
<bodyText confidence="0.993642333333333">
Figure 6 shows the distribution of predicted max-
imum lengths for Japanese. Although we used
θ = 0.99, it is rather parsimonious but accurate
that makes the computation faster.
Because this cumulative Negative Binomial
prediction is language independent, we believe it
might be beneficial for other natural language pro-
cessing tasks that require some maximum lengths
within which to process the data.
</bodyText>
<sectionHeader confidence="0.999284" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99997084375">
To validate our model, we conducted experiments
on several corpora of East Asian languages with
no word boundaries.
Datasets For East Asian languages, we used
standard datasets in Japanese, Chinese and Thai
as shown in Table 3. The Kyoto corpus is a
collection of sentences from Japanese newspaper
(Kurohashi and Nagao, 1998) with both word seg-
mentation and part-of-speech annotations. BC-
CWJ (Balanced Corpus of Contemporary Writ-
ten Japanese) is a balanced corpus of written
Japanese (Maekawa, 2007) from the National
Institute of Japanese Language and Linguistics,
also with both word segmentation and part-of-
speech annotations from slightly different crite-
ria. For experiments on colloquial texts, we used
a random subset of “OC” register from this cor-
pus that is comprised of Yahoo!Japan Answers
from users. For Chinese, experiments are con-
ducted on standard datasets of SIGHAN Bakeoff
2005 (Emerson, 2005); for comparison we used
MSR and PKU datasets for simplified Chinese,
and the CITYU dataset for traditional Chinese.
SIGHAN datasets have word boundaries only, and
we conformed to original training/test splits pro-
vided with the data. InterBEST is a dataset in
Thai used in the InterBEST 2009 word segmen-
tation contest (Kosawat, 2009). For contrastive
purposes, we used a “Novel” subset of it with a
random sampling without replacement for training
and test data. Accuracies are measured in token
F-measures computed as follows:
</bodyText>
<equation confidence="0.998599285714286">
2P R
F =P +R , (13)
# of correct words
P = (14)
# of words in output ,
# of correct words
R = (15)
</equation>
<bodyText confidence="0.971540304347826">
# of words in gold standard
Unsupervised word segmentation In Table 4,
we show the accuracies of unsupervised word seg-
mentation with previous figures. We used bi-
gram PYHSMM and set L = 4 for Chinese, L =
5, 8, 10, 21 for Japanese with different types of
contiguous characters, and L = 6 for Thai. The
number of hidden states are K =10 (Chinese and
Thai), K =20 (Kyoto) and K =30 (BCCWJ).
We can see that our PYHSMM outperforms on
all the datasets. Huang and Zhao (2007) reports
that the maximum possible accuracy in unsuper-
vised Chinese word segmentation is 84.8%, de-
rived through the inconsistency between different
segmentation standards of the SIGHAN dataset.
Our PYHSMM performs nearer to this best possi-
ble accuracy, leveraging both word and character
knowledge in a consistent Bayesian fashion. Fur-
ther note that in Thai, quite high performance is
achieved with a very small data compared to pre-
vious work.
Unsupervised part-of-speech induction As
stated above, Kyoto, BCCWJ and Weibo datasets
</bodyText>
<table confidence="0.95301925">
Dataset Kyoto BCCWJ MSR CITYU BEST
Precision (All) 99.9 99.9 99.6 99.9 99.0
Precision (&gt;5) 96.7 98.4 73.6 87.0 91.7
Maximum length 15 48 23 12 21
</table>
<tableCaption confidence="0.89269075">
Table 2: Precision of maximum word length prediction with
a Negative Binomial generalized linear model (in percent).
&gt; 5 are figures for word length &gt; 5. Final row is the maxi-
mum length of a word found in each dataset.
</tableCaption>
<figure confidence="0.988286">
0
L 2 4 6 8 10 12 14 16
</figure>
<figureCaption confidence="0.998951">
Figure 6: Distribution of predicted maxi-
mum word lengths on the Kyoto corpus.
</figureCaption>
<figure confidence="0.9875555">
Frequency
14000
12000
10000
4000
8000
6000
2000
</figure>
<page confidence="0.981191">
1779
</page>
<table confidence="0.999686714285714">
Dataset PYHSMM NPY BE HMM2
Kyoto 71.5 62.1 71.3 NA
BCCWJ 70.5 NA NA NA
MSR 82.9 80.2 78.2 81.7
CITYU 82.6* 82.4 78.7 NA
PKU 81.6 NA 80.8 81.1
BEST 82.1 NA 82.1 NA
</table>
<tableCaption confidence="0.99195">
Table 4: Accuracies of unsupervised word seg-
</tableCaption>
<bodyText confidence="0.995656771428572">
mentation. BE is a Branching Entropy method of
Zhikov et al. (2010), and HMM2 is a product of
word and character HMMs of Chen et al. (2014).
* is the accuracy decoded with L = 3: it becomes
81.7 with L=4 as MSR and PKU.
have part-of-speech annotations as well. For these
data, we also evaluated the precision of part-of-
speech induction on the output of unsupervised
word segmentation above. Note that the precision
is measured only over correct word segmentation
that the system has output. Table 5 shows the
precisions; to the best of our knowledge, there
are no previous work on joint unsupervised learn-
ing of words and tags, thus we only compared
with Bayesian HMM (Goldwater and Griffiths,
2007) on both NPYLM segmentation and gold
segmentation. In this evaluation, we associated
each tag of supervised data with a latent state that
cooccurred most frequently with that tag. We
can see that the precision of joint POS tagging is
better than NPYLM+HMM, and even better than
HMM that is run over the gold segmentation.
For colloquial Chinese, we also conducted an
experiment on the Leiden Weibo Corpus (LWC), a
corpus of Chinese equivalent of Twitter7. We used
random 20,000 sentences from this corpus, and re-
sults are shown in Figure 7. In many cases plausi-
ble words are found, and assigned to syntactically
consistent states. States that are not shown here
are either just not used or consists of a mixture of
different syntactic categories. Guiding our model
to induce more accurate latent states is a common
problem to all unsupervised part-of-speech induc-
tion, but we show some semi-supervised results
next.
</bodyText>
<table confidence="0.99946875">
Dataset PYHSMM NPY+HMM HMM
Kyoto 57.4 53.8 49.5
BCCWJ 50.2 44.1 44.2
LWC 33.0 30.9 32.9
</table>
<tableCaption confidence="0.9827845">
Table 5: Precision of POS tagging on correctly
segmented words.
</tableCaption>
<footnote confidence="0.911064">
7http://lwc.daanvanesch.nl/
</footnote>
<bodyText confidence="0.993131552631579">
Semi-supervised experiments Because our
PYHSMM is a generative model, it is easily
amenable to semi-supervised segmentation and
tagging. We used random 10,000 sentences from
supervised data on Kyoto, BCCWJ, and LWC
datasets along with unsupervised datasets in
Table 3.
Results are shown in Table 6: segmentation ac-
curacies came close to 90% but do not go be-
yond. By inspecting the segmentation and POS
that PYHSMM has output, we found that this is
not necessarily a fault of our model, but it came
from the often inconsistet or incorrect tagging of
the dataset. In many cases PYHSMM found more
“natural” segmentations, but it does not always
conform to the gold annotations. On the other
hand, it often oversegments emotional expressions
(sequence of the same character, for example) and
this is one of the major sources of errors.
Finally, we note that our proposed model for un-
supervised learning is most effective for the lan-
guage which we do not know its syntactic behavior
but only know raw strings as its data. In Figure 8,
we show an excerpt of results to model a Japanese
local dialect (Mikawa-ben around Nagoya district)
collected from a specific Twitter. Even from the
surface appearance of characters, we can see that
similar words are assigned to the same state in-
cluding some emoticons (states 9,29,32), and in
fact we can identify a state of postpositions spe-
cific to that dialect (state 3). Notice that the
words themselves are not trivial before this anal-
ysis. There are also some name of local places
(state 41) and general Japanese postpositions (2)
or nouns (11,18,25,27,31). Because of the spar-
sity promoting prior (7) over the hidden states, ac-
tually used states are sparse and the results can be
considered quite satisfactory.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99942">
The characteristics of NPYLM is a Baysian inte-
gration of character and word level information,
which is related to (Blunsom and Cohn, 2011) and
the adaptor idea of (Goldwater et al., 2011). This
</bodyText>
<table confidence="0.997257">
Dataset Seg POS
Kyoto 92.1 87.1
BCCWJ 89.4 83.1
LWC 88.5 86.9
</table>
<tableCaption confidence="0.9739105">
Table 6: Semi-supervised segmentation and POS
tagging accuracies. POS is measured by precision.
</tableCaption>
<page confidence="0.606158">
1780
</page>
<table confidence="0.996983636363637">
z=1 z=3 z=10 z=11 z=18
啦 227 。 3309 , 13440 可以 207 东 68
°,f 182 ! 1901 # 5989 PE 201 大 60
去 86 T 482 的 5224 。 199 南 59
开心 65 on 226 。 3237 那么 192 , 55
走 62 PE 110 我 1504 多 192 西 53
a 53 � 93 是 1206 打 177 路 51
鸟 44 啦 69 ! 1190 才 167 海 49
喽 41 aa 56 在 900 比 165 山 49
波 31 地址 47 都 861 对 154 C 45
测试 30 晚安 43 4&apos; 742 几 146 去 39
</table>
<figureCaption confidence="0.82745175">
Figure 7: Some interesting words and states induced from Weibo corpus (K = 20). Numbers represent
frequencies that each word is generated from that class. Although not perfect, emphatic (z = 1), end-
of-sentence expressions (z = 3), and locative words (z = 18) are learned from tweets. Distinction is far
more clear in the semi-supervised experiments (not shown here).
</figureCaption>
<table confidence="0.996863681818182">
z Induced words
2 の 、 は に が で と も を 「
3 ぞん かん ね のん だに だん りん か ん だのん
9 (*ˆˆ*) !(ˆ-ˆ; (ˆ_ˆ;) (ˆˆ;; !(ˆˆ;;
10 。! !! ? 」 (≧∇≦) !!」「
11 楽 入 ど寒 大丈夫 会 受 停電 良 美味 台風が
13 に ら わ な よ ね だら じゃんね え ぁ
18 今年 最近 豊川 地元 誰 豊田 今度 次 豊川高校
19 さん ん め 食べ って よろしく ありがとう じゃん
20 これ 知 人 それ どこ まあ みんな 東京 いや 方
24 三河弁 この よ お 何 そ ほい 今日 また ほ
25 他 一緒 5 大変 頭 春 参加 指 世代 地域
26 マジ 豊橋カレー コレ トキワ コーヒー プロ ファン
27 行 」 方言 &amp; 言葉 普通 夜店 」 始 確認
29 ( !(; (´・ !!(*` ?(´・ (*ˆ_ˆ*)
30 気 うち 店 ほう ここ こっち 先生 友人 いろいろ
31 女子 無理 決 近い 安心 標準語 感動 蒲郡 試合
32 ( ( *\(ˆ \ (ˆ (ˆ !*\(ˆ ~ (ˆ_ˆ (*ˆ
34 ヤマサ マーラ オレ ハイジ イメージクッピーラムネ
35 な ー そう 好き こと らん なん ら み 意味
36 いい ど うまい 杏果 ぐろ めっちゃ かわい はよ
41 豊橋 名古屋 三河 西三河 名古屋弁 名古屋人 大阪
</table>
<figureCaption confidence="0.8924545">
Figure 8: Unsupervised analysis of a Japanese lo-
cal dialect by PYHSMM. (K =50)
</figureCaption>
<bodyText confidence="0.999960304347826">
is different from (and misunderstood in) a joint
model of Chen et al. (2014), where word and char-
acter HMMs are just multiplied. There are no in-
formation shared from the model structure, and
in fact it depends on a BIO-like heuristic tagging
scheme in the character HMM.
In the present paper, we extended it to include
a hidden state for each word. Therefore, it might
be interesting to introduce a hidden state also for
each character. Unlike western languages, there
are many kinds of Chinese characters that work
quite differently, and Japanese uses several distinct
kinds of characters, such as a Chinese character,
Hiragana, Katakana, whose mixture would consti-
tute a single word. Therefore, statistical modeling
of different types of characters is an important re-
search venue for the future.
NPYLM has already applied and extended to
speech recognition (Neubig et al., 2010), statisti-
cal machine translation (Nguyen et al., 2010), or
even robotics (Nakamura et al., 2014). For all
these research area, we believe PYHSMM would
be beneficial for their extension.
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999968454545455">
In this paper, we proposed a Pitman-Yor Hidden
Semi-Markov model for joint unsupervised word
segmentation and part-of-speech tagging on a raw
sequence of characters. It can also be viewed as
a way to build a class n-gram language model di-
rectly on strings, without any “word” information
a priori.
We applied our PYHSMM on several standard
datasets on Japanese, Chinese and Thai, and it out-
performed previous figures to yield the state-of-
the-art results, as well as automatically induced
word categories. It is especially beneficial for col-
loquial text, local languages or speech transcripts,
where not only words themselves are unknown but
their syntactic behavior is a focus of interest.
In order to adapt to human standards given in
supervised data, it is important to conduct a semi-
supervised learning with discriminative classifiers.
Since semi-supervised learning requires genera-
tive models in advance, our proposed Bayesian
generative model will also lay foundations to such
an extension.
</bodyText>
<sectionHeader confidence="0.995202" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.901564">
Phil Blunsom and Trevor Cohn. 2011. A Hierarchical
Pitman-Yor Process HMM for Unsupervised Part of
Speech Induction. In ACL 2011, pages 865–874.
</reference>
<page confidence="0.827897">
1781
</page>
<reference confidence="0.999906289719626">
Miaohong Chen, Baobao Chang, and Wenzhe Pei.
2014. A Joint Model for Unsupervised Chinese
Word Segmentation. In EMNLP 2014, pages 854–
863.
John D. Cook. 2009. Notes on the Negative Bi-
nomial Distribution. http://www.johndcook.com/
negative binomial.pdf.
Tom Emerson. 2005. The Second International Chi-
nese Word Segmentation Bakeoff. In Proceedings
of the Fourth SIGHAN Workshop on Chinese Lan-
guage Processing.
Sharon Goldwater and Tom Griffiths. 2007. A Fully
Bayesian Approach to Unsupervised Part-of-Speech
Tagging. In Proceedings of ACL 2007, pages 744–
751.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual Dependencies in Un-
supervised Word Segmentation. In Proceedings of
ACL/COLING 2006, pages 673–680.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2011. Producing Power-Law Distribu-
tions and Damping Word Frequencies with Two-
Stage Language Models. Journal of Machine Learn-
ing Research, 12:2335–2382.
Chang-Ning Huang and Hai Zhao. 2007. Chinese
word segmentation: A decade review. Journal of
Chinese Information Processing, 21(3):8–20.
Matthew J. Johnson and Alan S. Willsky. 2013.
Bayesian Nonparametric Hidden Semi-Markov
Models. Journal of Machine Learning Research,
14:673–701.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Bayesian Inference for PCFGs via
Markov Chain Monte Carlo. In Proceedings of
HLT/NAACL 2007, pages 139–146.
Krit Kosawat. 2009. InterBEST 2009: Thai Word
Segmentation Workshop. In Proceedings of 2009
Eighth International Symposium on Natural Lan-
guage Processing (SNLP2009), Thailand.
Sadao Kurohashi and Makoto Nagao. 1998. Building
a Japanese Parsed Corpus while Improving the Pars-
ing System. In Proceedings of LREC 1998, pages
719–724. http://nlp.kuee.kyoto-u.ac.jp/nl-resource/
corpus.html.
Kikuo Maekawa. 2007. Kotonoha and BCCWJ: De-
velopment of a Balanced Corpus of Contemporary
Written Japanese. In Corpora and Language Re-
search: Proceedings of the First International Con-
ference on Korean Language, Literature, and Cul-
ture, pages 158–177.
Pierre Magistry and Benoit Sagot. 2013. Can MDL
Improve Unsupervised Chinese Word Segmenta-
tion? In Proceedings of the Seventh SIGHAN Work-
shop on Chinese Language Processing, pages 2–10.
Daichi Mochihashi and Eiichiro Sumita. 2008. The In-
finite Markov Model. In Advances in Neural Infor-
mation Processing Systems 20 (NIPS 2007), pages
1017–1024.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian Unsupervised Word Seg-
mentation with Nested Pitman-Yor Language Mod-
eling. In Proceedings of ACL-IJCNLP 2009, pages
100–108.
Kevin Murphy. 2002. Hidden semi-Markov models
(segment models). http://www.cs.ubc.ca/˜murphyk/
Papers/segment.pdf.
Tomoaki Nakamura, Takayuki Nagai, Kotaro Fu-
nakoshi, Shogo Nagasaka, Tadahiro Taniguchi, and
Naoto Iwahashi. 2014. Mutual Learning of an
Object Concept and Language Model Based on
MLDA and NPYLM. In 2014 IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems
(IROS’14), pages 600–607.
Graham Neubig, Masato Mimura, Shinsuke Mori, and
Tatsuya Kawahara. 2010. Learning a Language
Model from Continuous Speech. In Proc. of INTER-
SPEECH 2010.
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric Word Segmentation for Ma-
chine Translation. In COLING 2010, pages 815–
823.
Jim Pitman and Marc Yor. 1997. The Two-Parameter
Poisson-Dirichlet Distribution Derived from a Sta-
ble Subordinator. Annals of Probability, 25(2):855–
900.
Sunita Sarawagi and William W. Cohen. 2005. Semi-
Markov Conditional Random Fields for Information
Extraction. In Advances in Neural Information Pro-
cessing Systems 17 (NIPS 2004), pages 1185–1192.
Steven L. Scott. 2002. Bayesian Methods for Hidden
Markov Models. Journal of the American Statistical
Association, 97:337–351.
Kairit Sirts and Tanel Alum¨ae. 2012. A Hierarchi-
cal Dirichlet Process Model for Joint Part-of-Speech
and Morphology Induction. In NAACL 2012, pages
407–416.
Yee Whye Teh. 2006. A Bayesian Interpretation of In-
terpolated Kneser-Ney. Technical Report TRA2/06,
School of Computing, NUS.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In EMNLP 2009, pages 678–
687.
Valentin Zhikov, Hiroya Takamura, and Manabu Oku-
mura. 2010. An Efficient Algorithm for Unsuper-
vised Word Segmentation with Branching Entropy
and MDL. In EMNLP 2010, pages 832–842.
</reference>
<page confidence="0.994276">
1782
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.490959">
<title confidence="0.9932905">Inducing Word and Part-of-Speech with Pitman-Yor Hidden Semi-Markov Models</title>
<author confidence="0.990635">Kei Uchiumi Hiroshi</author>
<affiliation confidence="0.999298">Denso IT Laboratory,</affiliation>
<address confidence="0.94736">Shibuya Cross Tower 2-15-1 Shibuya, Tokyo,</address>
<email confidence="0.573839">Daichi</email>
<affiliation confidence="0.999474">The Institute of Statistical</affiliation>
<address confidence="0.9918455">10-3 Midori-cho, Tachikawa Tokyo,</address>
<email confidence="0.997355">daichi@ism.ac.jp</email>
<abstract confidence="0.999477888888889">We propose a nonparametric Bayesian model for joint unsupervised word segmentation and part-of-speech tagging from raw strings. Extending a previous model for word segmentation, our model is called a Pitman-Yor Hidden Semi- Markov Model (PYHSMM) and considas a method to build a class language model directly from strings, while integrating character and word level information. Experimental results on standard datasets on Japanese, Chinese and Thai revealed it outperforms previous results to yield the state-of-the-art accuracies. This model will also serve to analyze a structure of a language whose words are not identified a priori.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction.</title>
<date>2011</date>
<booktitle>In ACL 2011,</booktitle>
<pages>865--874</pages>
<contexts>
<context position="13753" citStr="Blunsom and Cohn, 2011" startWordPosition="2297" endWordPosition="2301">e can say it is possible. The idea is simple: we augment the latent states to include a hidden part-of-speech zt for each word wt, which is again unknown as displayed in Figure 4. Assuming wt is generated from zt’-th NPYLM, we can draw a generative model of a string s as follows: z0 =BOS; s=c (an empty string). fort = 1 ··· T do Draw zt ∼ p(zt|zt−1) , Draw wt ∼ p(wt|w1 · · · wt−1, zt) , Append wt to s. end for Here, z0 = BOS and zT+1 = EOS are distinguished states for beginning and end of a sentence, respectively. For the transition probability of hidden states, we put a HPY process prior as (Blunsom and Cohn, 2011): p(zt|zt−1) ∼ HPY(d, θ) (7) with the final base measure being a uniform distribution over the states. The word boundaries are ���M�� &amp;quot;friends .f each c.untry&amp;quot; Figure 5: Graphical representation of sampling words and POSs. Each cell corresponds to an inside probability α[t][k][z]. Note each cell is not always connected to adjacent cells, because of an overlap of substrings associated with each cell. known in (Blunsom and Cohn, 2011), but in our case it is also learned from data at the same time. Note that because wt depends on already generated words w1 · · · wt−1, our model is considered as a</context>
<context position="27475" citStr="Blunsom and Cohn, 2011" startWordPosition="4663" endWordPosition="4666">uding some emoticons (states 9,29,32), and in fact we can identify a state of postpositions specific to that dialect (state 3). Notice that the words themselves are not trivial before this analysis. There are also some name of local places (state 41) and general Japanese postpositions (2) or nouns (11,18,25,27,31). Because of the sparsity promoting prior (7) over the hidden states, actually used states are sparse and the results can be considered quite satisfactory. 6 Discussion The characteristics of NPYLM is a Baysian integration of character and word level information, which is related to (Blunsom and Cohn, 2011) and the adaptor idea of (Goldwater et al., 2011). This Dataset Seg POS Kyoto 92.1 87.1 BCCWJ 89.4 83.1 LWC 88.5 86.9 Table 6: Semi-supervised segmentation and POS tagging accuracies. POS is measured by precision. 1780 z=1 z=3 z=10 z=11 z=18 啦 227 。 3309 , 13440 可以 207 东 68 °,f 182 ! 1901 # 5989 PE 201 大 60 去 86 T 482 的 5224 。 199 南 59 开心 65 on 226 。 3237 那么 192 , 55 走 62 PE 110 我 1504 多 192 西 53 a 53 � 93 是 1206 打 177 路 51 鸟 44 啦 69 ! 1190 才 167 海 49 喽 41 aa 56 在 900 比 165 山 49 波 31 地址 47 都 861 对 154 C 45 测试 30 晚安 43 4&apos; 742 几 146 去 39 Figure 7: Some interesting words and states induced from W</context>
</contexts>
<marker>Blunsom, Cohn, 2011</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2011. A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction. In ACL 2011, pages 865–874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miaohong Chen</author>
<author>Baobao Chang</author>
<author>Wenzhe Pei</author>
</authors>
<title>A Joint Model for Unsupervised Chinese Word Segmentation.</title>
<date>2014</date>
<booktitle>In EMNLP 2014,</booktitle>
<pages>854--863</pages>
<contexts>
<context position="5280" citStr="Chen et al. (2014)" startWordPosition="799" endWordPosition="802">l., 2006; Mochihashi et al., 2009). In fact, the statistical methods often include the criteria of heuristic methods at least in a conceptual level, which is noted in (Mochihashi et al., 2009) and also explained later in this paper. In a statistical model, each word segmentation w of a string s is regarded as a hidden stochastic variable, and the unsupervised learning of word segmentation is formulated as a maximization of a probability of w given s: argmax p(w|s) . (1) W This means that we want the most “natural” segmentation w that have a high probability in a language model p(w|s). Lately, Chen et al. (2014) proposed an intermediate model between heuristic and statistical models as a product of character and word HMMs. However, these two models do not have information shared between the models, which is not the case with generative models. So far, these approaches only find word segmentation, leaving part-of-speech information behind. These two problems are not actually independent but interrelated, because knowing the part-of-speech of some infrequent or unknown word will give contextual clues to word segmentation, and vice versa. For example, in Japanese *tttttt can be segmented into not only *</context>
<context position="23918" citStr="Chen et al. (2014)" startWordPosition="4072" endWordPosition="4075"> (in percent). &gt; 5 are figures for word length &gt; 5. Final row is the maximum length of a word found in each dataset. 0 L 2 4 6 8 10 12 14 16 Figure 6: Distribution of predicted maximum word lengths on the Kyoto corpus. Frequency 14000 12000 10000 4000 8000 6000 2000 1779 Dataset PYHSMM NPY BE HMM2 Kyoto 71.5 62.1 71.3 NA BCCWJ 70.5 NA NA NA MSR 82.9 80.2 78.2 81.7 CITYU 82.6* 82.4 78.7 NA PKU 81.6 NA 80.8 81.1 BEST 82.1 NA 82.1 NA Table 4: Accuracies of unsupervised word segmentation. BE is a Branching Entropy method of Zhikov et al. (2010), and HMM2 is a product of word and character HMMs of Chen et al. (2014). * is the accuracy decoded with L = 3: it becomes 81.7 with L=4 as MSR and PKU. have part-of-speech annotations as well. For these data, we also evaluated the precision of part-ofspeech induction on the output of unsupervised word segmentation above. Note that the precision is measured only over correct word segmentation that the system has output. Table 5 shows the precisions; to the best of our knowledge, there are no previous work on joint unsupervised learning of words and tags, thus we only compared with Bayesian HMM (Goldwater and Griffiths, 2007) on both NPYLM segmentation and gold seg</context>
<context position="29181" citStr="Chen et al. (2014)" startWordPosition="5095" endWordPosition="5098">ね え ぁ 18 今年 最近 豊川 地元 誰 豊田 今度 次 豊川高校 19 さん ん め 食べ って よろしく ありがとう じゃん 20 これ 知 人 それ どこ まあ みんな 東京 いや 方 24 三河弁 この よ お 何 そ ほい 今日 また ほ 25 他 一緒 5 大変 頭 春 参加 指 世代 地域 26 マジ 豊橋カレー コレ トキワ コーヒー プロ ファン 27 行 」 方言 &amp; 言葉 普通 夜店 」 始 確認 29 ( !(; (´・ !!(*` ?(´・ (*ˆ_ˆ*) 30 気 うち 店 ほう ここ こっち 先生 友人 いろいろ 31 女子 無理 決 近い 安心 標準語 感動 蒲郡 試合 32 ( ( *\(ˆ \ (ˆ (ˆ !*\(ˆ ~ (ˆ_ˆ (*ˆ 34 ヤマサ マーラ オレ ハイジ イメージクッピーラムネ 35 な ー そう 好き こと らん なん ら み 意味 36 いい ど うまい 杏果 ぐろ めっちゃ かわい はよ 41 豊橋 名古屋 三河 西三河 名古屋弁 名古屋人 大阪 Figure 8: Unsupervised analysis of a Japanese local dialect by PYHSMM. (K =50) is different from (and misunderstood in) a joint model of Chen et al. (2014), where word and character HMMs are just multiplied. There are no information shared from the model structure, and in fact it depends on a BIO-like heuristic tagging scheme in the character HMM. In the present paper, we extended it to include a hidden state for each word. Therefore, it might be interesting to introduce a hidden state also for each character. Unlike western languages, there are many kinds of Chinese characters that work quite differently, and Japanese uses several distinct kinds of characters, such as a Chinese character, Hiragana, Katakana, whose mixture would constitute a sin</context>
</contexts>
<marker>Chen, Chang, Pei, 2014</marker>
<rawString>Miaohong Chen, Baobao Chang, and Wenzhe Pei. 2014. A Joint Model for Unsupervised Chinese Word Segmentation. In EMNLP 2014, pages 854– 863.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Cook</author>
</authors>
<title>Notes on the Negative Binomial Distribution. http://www.johndcook.com/ negative binomial.pdf.</title>
<date>2009</date>
<contexts>
<context position="17843" citStr="Cook, 2009" startWordPosition="3017" endWordPosition="3018">m some character level information including suffixes or character types, in a Table 1: Features used for the Negative Binomial generalized linear model for maximum word length prediction. 5For computational reasons, we do not pursue using a Dirichlet process to yield an infinite HMM (Van Gael et al., 2009), but it is straightforward to extend our PYHSMM to iHMM. semi-supervised setting we employ a Negative Binomial generalized linear model (GLM) for setting Lt adaptively for each character position t in the corpus. Specifically, we model the word length ℓ by a Negative Binomial distribution (Cook, 2009): F(r+ℓ) ℓ � NB(ℓ|r,p) = pℓ(1 − p)r . (9) F(r)ℓ! This counts the number of failures of Bernoulli draws with probability (1−p) before r’th success. For our model, note that Negative Binomial is obtained from a Poisson distribution Po(λ) whose parameter λ again follows a Gamma distribution Ga(r, b) and integrated out: fp(ℓ|r, b) = Po(ℓ|λ)Ga(λ|r, b)dλ (10) F(r+ℓ)( b lP (11b) r(r) ℓ! 1+b J This construction exactly mirrors the PoissonGamma word length distribution in (Mochihashi et al., 2009) with sampled λ. Therefore, our Negative Binomial is basically a continuous analogue of the word length dis</context>
</contexts>
<marker>Cook, 2009</marker>
<rawString>John D. Cook. 2009. Notes on the Negative Binomial Distribution. http://www.johndcook.com/ negative binomial.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Emerson</author>
</authors>
<title>The Second International Chinese Word Segmentation Bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="2091" citStr="Emerson, 2005" startWordPosition="311" endWordPosition="312">l analysis means word segmentation and part-of-speech (POS) tagging. For this purpose, supervised methods have often been employed for training. However, to train such supervised classifiers, we have to prepare a large amount of training data with correct annotations, in this case, word segmentation and POS tags. Creating and maintaining these data is not only costly but also very difficult, because generally there are no clear criteria for either “correct” segmentation or POS tags. In fact, since there are different standards for Chinese word segmentation, widely used SIGHAN Bakeoff dataset (Emerson, 2005) consists of multiple parts employing different annotation schemes. Lately, this situation has become increasingly important because there are strong demands for processing huge amounts of text in consumer generated media such as Twitter, Weibo or Facebook (Figure 1). They contain a plethora of colloquial expressions and newly coined words, including sentiment expressions such as emoticons that cannot be covered by fixed supervised data. To automatically recognize such linguistic phenomena beyond small “correct” supervised data, we have to extract linguistic knowledge from the statistics of st</context>
<context position="21425" citStr="Emerson, 2005" startWordPosition="3627" endWordPosition="3628">er (Kurohashi and Nagao, 1998) with both word segmentation and part-of-speech annotations. BCCWJ (Balanced Corpus of Contemporary Written Japanese) is a balanced corpus of written Japanese (Maekawa, 2007) from the National Institute of Japanese Language and Linguistics, also with both word segmentation and part-ofspeech annotations from slightly different criteria. For experiments on colloquial texts, we used a random subset of “OC” register from this corpus that is comprised of Yahoo!Japan Answers from users. For Chinese, experiments are conducted on standard datasets of SIGHAN Bakeoff 2005 (Emerson, 2005); for comparison we used MSR and PKU datasets for simplified Chinese, and the CITYU dataset for traditional Chinese. SIGHAN datasets have word boundaries only, and we conformed to original training/test splits provided with the data. InterBEST is a dataset in Thai used in the InterBEST 2009 word segmentation contest (Kosawat, 2009). For contrastive purposes, we used a “Novel” subset of it with a random sampling without replacement for training and test data. Accuracies are measured in token F-measures computed as follows: 2P R F =P +R , (13) # of correct words P = (14) # of words in output , #</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Tom Emerson. 2005. The Second International Chinese Word Segmentation Bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>744--751</pages>
<contexts>
<context position="24478" citStr="Goldwater and Griffiths, 2007" startWordPosition="4167" endWordPosition="4170">and HMM2 is a product of word and character HMMs of Chen et al. (2014). * is the accuracy decoded with L = 3: it becomes 81.7 with L=4 as MSR and PKU. have part-of-speech annotations as well. For these data, we also evaluated the precision of part-ofspeech induction on the output of unsupervised word segmentation above. Note that the precision is measured only over correct word segmentation that the system has output. Table 5 shows the precisions; to the best of our knowledge, there are no previous work on joint unsupervised learning of words and tags, thus we only compared with Bayesian HMM (Goldwater and Griffiths, 2007) on both NPYLM segmentation and gold segmentation. In this evaluation, we associated each tag of supervised data with a latent state that cooccurred most frequently with that tag. We can see that the precision of joint POS tagging is better than NPYLM+HMM, and even better than HMM that is run over the gold segmentation. For colloquial Chinese, we also conducted an experiment on the Leiden Weibo Corpus (LWC), a corpus of Chinese equivalent of Twitter7. We used random 20,000 sentences from this corpus, and results are shown in Figure 7. In many cases plausible words are found, and assigned to sy</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging. In Proceedings of ACL 2007, pages 744– 751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual Dependencies in Unsupervised Word Segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL/COLING</booktitle>
<pages>673--680</pages>
<contexts>
<context position="4670" citStr="Goldwater et al., 2006" startWordPosition="689" endWordPosition="693">th&apos;, word boundaries are found in an algorithmic fashion (Zhikov et al., 2010; Magistry and Sagot, 2013). However, such methods have difficulty incorporating higher-order statistics beyond simple heuristics, such as word transitions, word spelling formation, or word length distribution. Moreover, they usually depends on tuning parameters like thresholds that cannot be learned without human intervention. In contrast, statistical models are ready to incorporate all such phenomena within a consistent statistical generative model of a string, and often prove to work better than heuristic methods (Goldwater et al., 2006; Mochihashi et al., 2009). In fact, the statistical methods often include the criteria of heuristic methods at least in a conceptual level, which is noted in (Mochihashi et al., 2009) and also explained later in this paper. In a statistical model, each word segmentation w of a string s is regarded as a hidden stochastic variable, and the unsupervised learning of word segmentation is formulated as a maximization of a probability of w given s: argmax p(w|s) . (1) W This means that we want the most “natural” segmentation w that have a high probability in a language model p(w|s). Lately, Chen et </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual Dependencies in Unsupervised Word Segmentation. In Proceedings of ACL/COLING 2006, pages 673–680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Producing Power-Law Distributions and Damping Word Frequencies with TwoStage Language Models.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2335</pages>
<contexts>
<context position="27524" citStr="Goldwater et al., 2011" startWordPosition="4672" endWordPosition="4675">t we can identify a state of postpositions specific to that dialect (state 3). Notice that the words themselves are not trivial before this analysis. There are also some name of local places (state 41) and general Japanese postpositions (2) or nouns (11,18,25,27,31). Because of the sparsity promoting prior (7) over the hidden states, actually used states are sparse and the results can be considered quite satisfactory. 6 Discussion The characteristics of NPYLM is a Baysian integration of character and word level information, which is related to (Blunsom and Cohn, 2011) and the adaptor idea of (Goldwater et al., 2011). This Dataset Seg POS Kyoto 92.1 87.1 BCCWJ 89.4 83.1 LWC 88.5 86.9 Table 6: Semi-supervised segmentation and POS tagging accuracies. POS is measured by precision. 1780 z=1 z=3 z=10 z=11 z=18 啦 227 。 3309 , 13440 可以 207 东 68 °,f 182 ! 1901 # 5989 PE 201 大 60 去 86 T 482 的 5224 。 199 南 59 开心 65 on 226 。 3237 那么 192 , 55 走 62 PE 110 我 1504 多 192 西 53 a 53 � 93 是 1206 打 177 路 51 鸟 44 啦 69 ! 1190 才 167 海 49 喽 41 aa 56 在 900 比 165 山 49 波 31 地址 47 都 861 对 154 C 45 测试 30 晚安 43 4&apos; 742 几 146 去 39 Figure 7: Some interesting words and states induced from Weibo corpus (K = 20). Numbers represent frequenci</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2011</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2011. Producing Power-Law Distributions and Damping Word Frequencies with TwoStage Language Models. Journal of Machine Learning Research, 12:2335–2382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang-Ning Huang</author>
<author>Hai Zhao</author>
</authors>
<title>Chinese word segmentation: A decade review.</title>
<date>2007</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="22521" citStr="Huang and Zhao (2007)" startWordPosition="3824" endWordPosition="3827">re measured in token F-measures computed as follows: 2P R F =P +R , (13) # of correct words P = (14) # of words in output , # of correct words R = (15) # of words in gold standard Unsupervised word segmentation In Table 4, we show the accuracies of unsupervised word segmentation with previous figures. We used bigram PYHSMM and set L = 4 for Chinese, L = 5, 8, 10, 21 for Japanese with different types of contiguous characters, and L = 6 for Thai. The number of hidden states are K =10 (Chinese and Thai), K =20 (Kyoto) and K =30 (BCCWJ). We can see that our PYHSMM outperforms on all the datasets. Huang and Zhao (2007) reports that the maximum possible accuracy in unsupervised Chinese word segmentation is 84.8%, derived through the inconsistency between different segmentation standards of the SIGHAN dataset. Our PYHSMM performs nearer to this best possible accuracy, leveraging both word and character knowledge in a consistent Bayesian fashion. Further note that in Thai, quite high performance is achieved with a very small data compared to previous work. Unsupervised part-of-speech induction As stated above, Kyoto, BCCWJ and Weibo datasets Dataset Kyoto BCCWJ MSR CITYU BEST Precision (All) 99.9 99.9 99.6 99.</context>
</contexts>
<marker>Huang, Zhao, 2007</marker>
<rawString>Chang-Ning Huang and Hai Zhao. 2007. Chinese word segmentation: A decade review. Journal of Chinese Information Processing, 21(3):8–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew J Johnson</author>
<author>Alan S Willsky</author>
</authors>
<title>Bayesian Nonparametric Hidden Semi-Markov Models.</title>
<date>2013</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>14--673</pages>
<marker>Johnson, Willsky, 2013</marker>
<rawString>Matthew J. Johnson and Alan S. Willsky. 2013. Bayesian Nonparametric Hidden Semi-Markov Models. Journal of Machine Learning Research, 14:673–701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian Inference for PCFGs via Markov Chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT/NAACL</booktitle>
<pages>139--146</pages>
<contexts>
<context position="11283" citStr="Johnson et al., 2007" startWordPosition="1839" endWordPosition="1842"> w, its proxy customer is eventually sent to the parent restaurant of unigrams. In that case2, w is decomposed to its character sequence c1 · · · ck and this is added to the character HPYLM in the same way, making it a little “clever” about possible word spellings. Inference Because we do not know word segmentation w beforehand, we begin with a trivial segmentation in which every sentence is a single word3. Then, we iteratively refine it by sampling a new word segmentation w(s) of a sentence s in a Markov Chain Monte Carlo (MCMC) framework using a dynamic programming, as is done with PCFG by (Johnson et al., 2007) shown in Figure 3 where we omit MH steps for computational reasons. Further note that every hyperparameter dn, θn of NPYLM can be sampled from the posterior in a Bayesian fashion, as opposed to heuristic methods that rely on a development set for tuning. For details, see Teh (2006). 3 Pitman-Yor Hidden Semi-Markov Models NPYLM is a complete generative model of a string, that is, a hierarchical Bayesian n-gram lanInput: a collection of strings S Add initial segmentation w(s) to Θ for j = 1 ··· J do for s in randperm (S) do Remove customers of w(s) from Θ Sample w(s) according to p(w|s, Θ) Add </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Bayesian Inference for PCFGs via Markov Chain Monte Carlo. In Proceedings of HLT/NAACL 2007, pages 139–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krit Kosawat</author>
</authors>
<title>InterBEST 2009: Thai Word Segmentation Workshop.</title>
<date>2009</date>
<booktitle>In Proceedings of 2009 Eighth International Symposium on Natural Language Processing (SNLP2009),</booktitle>
<contexts>
<context position="21758" citStr="Kosawat, 2009" startWordPosition="3680" endWordPosition="3681"> from slightly different criteria. For experiments on colloquial texts, we used a random subset of “OC” register from this corpus that is comprised of Yahoo!Japan Answers from users. For Chinese, experiments are conducted on standard datasets of SIGHAN Bakeoff 2005 (Emerson, 2005); for comparison we used MSR and PKU datasets for simplified Chinese, and the CITYU dataset for traditional Chinese. SIGHAN datasets have word boundaries only, and we conformed to original training/test splits provided with the data. InterBEST is a dataset in Thai used in the InterBEST 2009 word segmentation contest (Kosawat, 2009). For contrastive purposes, we used a “Novel” subset of it with a random sampling without replacement for training and test data. Accuracies are measured in token F-measures computed as follows: 2P R F =P +R , (13) # of correct words P = (14) # of words in output , # of correct words R = (15) # of words in gold standard Unsupervised word segmentation In Table 4, we show the accuracies of unsupervised word segmentation with previous figures. We used bigram PYHSMM and set L = 4 for Chinese, L = 5, 8, 10, 21 for Japanese with different types of contiguous characters, and L = 6 for Thai. The numbe</context>
</contexts>
<marker>Kosawat, 2009</marker>
<rawString>Krit Kosawat. 2009. InterBEST 2009: Thai Word Segmentation Workshop. In Proceedings of 2009 Eighth International Symposium on Natural Language Processing (SNLP2009), Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Building a Japanese Parsed Corpus while Improving the Parsing System. In</title>
<date>1998</date>
<booktitle>Proceedings of LREC</booktitle>
<pages>719--724</pages>
<note>http://nlp.kuee.kyoto-u.ac.jp/nl-resource/ corpus.html.</note>
<contexts>
<context position="20841" citStr="Kurohashi and Nagao, 1998" startWordPosition="3536" endWordPosition="3539">r parsimonious but accurate that makes the computation faster. Because this cumulative Negative Binomial prediction is language independent, we believe it might be beneficial for other natural language processing tasks that require some maximum lengths within which to process the data. 5 Experiments To validate our model, we conducted experiments on several corpora of East Asian languages with no word boundaries. Datasets For East Asian languages, we used standard datasets in Japanese, Chinese and Thai as shown in Table 3. The Kyoto corpus is a collection of sentences from Japanese newspaper (Kurohashi and Nagao, 1998) with both word segmentation and part-of-speech annotations. BCCWJ (Balanced Corpus of Contemporary Written Japanese) is a balanced corpus of written Japanese (Maekawa, 2007) from the National Institute of Japanese Language and Linguistics, also with both word segmentation and part-ofspeech annotations from slightly different criteria. For experiments on colloquial texts, we used a random subset of “OC” register from this corpus that is comprised of Yahoo!Japan Answers from users. For Chinese, experiments are conducted on standard datasets of SIGHAN Bakeoff 2005 (Emerson, 2005); for comparison</context>
</contexts>
<marker>Kurohashi, Nagao, 1998</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1998. Building a Japanese Parsed Corpus while Improving the Parsing System. In Proceedings of LREC 1998, pages 719–724. http://nlp.kuee.kyoto-u.ac.jp/nl-resource/ corpus.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kikuo Maekawa</author>
</authors>
<title>Kotonoha and BCCWJ: Development of a Balanced Corpus of Contemporary Written Japanese.</title>
<date>2007</date>
<booktitle>In Corpora and Language Research: Proceedings of the First International Conference on Korean Language, Literature, and Culture,</booktitle>
<pages>158--177</pages>
<contexts>
<context position="21015" citStr="Maekawa, 2007" startWordPosition="3564" endWordPosition="3565">atural language processing tasks that require some maximum lengths within which to process the data. 5 Experiments To validate our model, we conducted experiments on several corpora of East Asian languages with no word boundaries. Datasets For East Asian languages, we used standard datasets in Japanese, Chinese and Thai as shown in Table 3. The Kyoto corpus is a collection of sentences from Japanese newspaper (Kurohashi and Nagao, 1998) with both word segmentation and part-of-speech annotations. BCCWJ (Balanced Corpus of Contemporary Written Japanese) is a balanced corpus of written Japanese (Maekawa, 2007) from the National Institute of Japanese Language and Linguistics, also with both word segmentation and part-ofspeech annotations from slightly different criteria. For experiments on colloquial texts, we used a random subset of “OC” register from this corpus that is comprised of Yahoo!Japan Answers from users. For Chinese, experiments are conducted on standard datasets of SIGHAN Bakeoff 2005 (Emerson, 2005); for comparison we used MSR and PKU datasets for simplified Chinese, and the CITYU dataset for traditional Chinese. SIGHAN datasets have word boundaries only, and we conformed to original t</context>
</contexts>
<marker>Maekawa, 2007</marker>
<rawString>Kikuo Maekawa. 2007. Kotonoha and BCCWJ: Development of a Balanced Corpus of Contemporary Written Japanese. In Corpora and Language Research: Proceedings of the First International Conference on Korean Language, Literature, and Culture, pages 158–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Magistry</author>
<author>Benoit Sagot</author>
</authors>
<title>Can MDL Improve Unsupervised Chinese Word Segmentation?</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>2--10</pages>
<contexts>
<context position="4152" citStr="Magistry and Sagot, 2013" startWordPosition="616" endWordPosition="619"> of Japanese Twitter text that is difficult to analyze by ordinary supervised segmentation. It contains a lot of novel words, emoticons, and colloquial expressions. 1774 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1774–1782, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics the next word). By formulating such ideas as search or MDL problems of given coding length&apos;, word boundaries are found in an algorithmic fashion (Zhikov et al., 2010; Magistry and Sagot, 2013). However, such methods have difficulty incorporating higher-order statistics beyond simple heuristics, such as word transitions, word spelling formation, or word length distribution. Moreover, they usually depends on tuning parameters like thresholds that cannot be learned without human intervention. In contrast, statistical models are ready to incorporate all such phenomena within a consistent statistical generative model of a string, and often prove to work better than heuristic methods (Goldwater et al., 2006; Mochihashi et al., 2009). In fact, the statistical methods often include the cri</context>
</contexts>
<marker>Magistry, Sagot, 2013</marker>
<rawString>Pierre Magistry and Benoit Sagot. 2013. Can MDL Improve Unsupervised Chinese Word Segmentation? In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages 2–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Eiichiro Sumita</author>
</authors>
<title>The Infinite Markov Model.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 20 (NIPS</booktitle>
<pages>1017--1024</pages>
<contexts>
<context position="9436" citStr="Mochihashi and Sumita, 2008" startWordPosition="1501" endWordPosition="1505">uses a uniform distribution over the lexicon. However, in the case of unsupervised word segmentation, every sequence of characters could be a word, thus the size of the lexicon is unbounded. Moreover, prior probability of forming a word should not be uniform over all sequences of characters: for example, English words rarely begin with ‘gme’ but tend to end with ’-ent’ like in segment. To model this property, NPYLM assumes that word prior G0 is generated from character HPYLM to model a well-formedness of w. In practice, to avoid dependency on n in the character model, we used an ∞-gram VPYLM (Mochihashi and Sumita, 2008) in this research. Finally, NPYLM gives an n-gram probability of word w given a history h recursively by integrating out Gn, c(w|h)−d·thw θ+d·th · p(w|h) = θ+c(h) + θ+c(h) p(w|h′) , (4) where h′ is the shorter history of (n −1)-grams. c(w|h), c(h) = Ew c(w|h) are n-gram counts of w appearing after h, and thw, th · = Ew thw are associated latent variables explained below. In case the history h is already empty at the unigram, p(w|h′) = p0(w) is computed from the character ∞-grams for the word w =c1 · · · ck : p0(w) = 7*1 ··· ck) (5) = i is 1 p(ci|ci−1 ··· c1) .(6) In practice, we further correc</context>
</contexts>
<marker>Mochihashi, Sumita, 2008</marker>
<rawString>Daichi Mochihashi and Eiichiro Sumita. 2008. The Infinite Markov Model. In Advances in Neural Information Processing Systems 20 (NIPS 2007), pages 1017–1024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP</booktitle>
<pages>100--108</pages>
<contexts>
<context position="4696" citStr="Mochihashi et al., 2009" startWordPosition="694" endWordPosition="697"> found in an algorithmic fashion (Zhikov et al., 2010; Magistry and Sagot, 2013). However, such methods have difficulty incorporating higher-order statistics beyond simple heuristics, such as word transitions, word spelling formation, or word length distribution. Moreover, they usually depends on tuning parameters like thresholds that cannot be learned without human intervention. In contrast, statistical models are ready to incorporate all such phenomena within a consistent statistical generative model of a string, and often prove to work better than heuristic methods (Goldwater et al., 2006; Mochihashi et al., 2009). In fact, the statistical methods often include the criteria of heuristic methods at least in a conceptual level, which is noted in (Mochihashi et al., 2009) and also explained later in this paper. In a statistical model, each word segmentation w of a string s is regarded as a hidden stochastic variable, and the unsupervised learning of word segmentation is formulated as a maximization of a probability of w given s: argmax p(w|s) . (1) W This means that we want the most “natural” segmentation w that have a high probability in a language model p(w|s). Lately, Chen et al. (2014) proposed an int</context>
<context position="7366" citStr="Mochihashi et al., 2009" startWordPosition="1136" endWordPosition="1139">tation within a word. For this objective, we attempt to maximize the joint probability of words and tags: argmax p(w, z|s) a p(w, z, s) (2) W,Z From the expression above, this amounts to building a generative model of a string s with words w and tags z along with an associated inference procedure. We solve this problem by extending previous generative model of word segmentation. Note that heuristic methods are never able to model the hidden tags, and only statistical generative models can accommodate this objective. This paper is organized as follows. In Section 2, we briefly introduce NPYLM (Mochihashi et al., 2009) on which our extension is based. Section 3 extends it to include hidden states to yield a hidden semi-Markov models (Murphy, 2002), and we describe its inference procedure in Section 4. We conduct experiments on some East Asian languages in Section 5. Section 6 discusses implications of our model and related work, and Section 7 concludes the paper. 2 Nested Pitman-Yor Language Model Our joint model of words and states is an extension of the Nested Pitman-Yor Language Model (Mochihashi et al., 2009) of a string, which in turn is an extension of a Bayesian n-gram language model called Hierarchi</context>
<context position="10152" citStr="Mochihashi et al., 2009" startWordPosition="1637" endWordPosition="1640">cursively by integrating out Gn, c(w|h)−d·thw θ+d·th · p(w|h) = θ+c(h) + θ+c(h) p(w|h′) , (4) where h′ is the shorter history of (n −1)-grams. c(w|h), c(h) = Ew c(w|h) are n-gram counts of w appearing after h, and thw, th · = Ew thw are associated latent variables explained below. In case the history h is already empty at the unigram, p(w|h′) = p0(w) is computed from the character ∞-grams for the word w =c1 · · · ck : p0(w) = 7*1 ··· ck) (5) = i is 1 p(ci|ci−1 ··· c1) .(6) In practice, we further corrected (6) so that a word length follows a mixture of Poisson distributions. For details, see (Mochihashi et al., 2009). When we know word segmentation w of the data, the probability above can be computed by adding each n-gram count of w given h to the model, i.e. increment c(w|h) in accordance with a hierarchical Chinese restaurant process associated with HPYLM (Figure 2). When each n-gram count called a customer is inferred to be actually generated from (n−1)-grams, we send its proxy customer for smoothing to the parent restaurant and increment thw, and this process will recurse. Notice that if a word w is never seen in w, its proxy customer is eventually sent to the parent restaurant of unigrams. In that ca</context>
<context position="18336" citStr="Mochihashi et al., 2009" startWordPosition="3098" endWordPosition="3101"> each character position t in the corpus. Specifically, we model the word length ℓ by a Negative Binomial distribution (Cook, 2009): F(r+ℓ) ℓ � NB(ℓ|r,p) = pℓ(1 − p)r . (9) F(r)ℓ! This counts the number of failures of Bernoulli draws with probability (1−p) before r’th success. For our model, note that Negative Binomial is obtained from a Poisson distribution Po(λ) whose parameter λ again follows a Gamma distribution Ga(r, b) and integrated out: fp(ℓ|r, b) = Po(ℓ|λ)Ga(λ|r, b)dλ (10) F(r+ℓ)( b lP (11b) r(r) ℓ! 1+b J This construction exactly mirrors the PoissonGamma word length distribution in (Mochihashi et al., 2009) with sampled λ. Therefore, our Negative Binomial is basically a continuous analogue of the word length distribution in NPYLM.6 Since r &gt; 0 and 0 &lt;p &lt; 1, we employ an exponential and sigmoidal linear regression r = exp(wTr f), p = σ(wTp f) (12) where σ(x) is a sigmoid function and wr, wp are weight vectors to learn. f is a feature vector computed from the substring c1 · · · ct, including f0 =1 for a bias term. Table 1 shows the features we used for this Negative Binomial GLM. Since Negative Binomial GLM is not convex in wr and wp, we endow a Normal prior N(0, σ2I) for them and used a random wa</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling. In Proceedings of ACL-IJCNLP 2009, pages 100–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Murphy</author>
</authors>
<title>Hidden semi-Markov models (segment models).</title>
<date>2002</date>
<note>http://www.cs.ubc.ca/˜murphyk/ Papers/segment.pdf.</note>
<contexts>
<context position="7497" citStr="Murphy, 2002" startWordPosition="1161" endWordPosition="1162">Z From the expression above, this amounts to building a generative model of a string s with words w and tags z along with an associated inference procedure. We solve this problem by extending previous generative model of word segmentation. Note that heuristic methods are never able to model the hidden tags, and only statistical generative models can accommodate this objective. This paper is organized as follows. In Section 2, we briefly introduce NPYLM (Mochihashi et al., 2009) on which our extension is based. Section 3 extends it to include hidden states to yield a hidden semi-Markov models (Murphy, 2002), and we describe its inference procedure in Section 4. We conduct experiments on some East Asian languages in Section 5. Section 6 discusses implications of our model and related work, and Section 7 concludes the paper. 2 Nested Pitman-Yor Language Model Our joint model of words and states is an extension of the Nested Pitman-Yor Language Model (Mochihashi et al., 2009) of a string, which in turn is an extension of a Bayesian n-gram language model called Hierarchical Pitman-Yor Language Model (HPYLM) (Teh, 2006). Character HPYLM Word HPYLM 1775 HPYLM is a nonparametric Bayesian model of n-gra</context>
<context position="14565" citStr="Murphy, 2002" startWordPosition="2443" endWordPosition="2444">ampling words and POSs. Each cell corresponds to an inside probability α[t][k][z]. Note each cell is not always connected to adjacent cells, because of an overlap of substrings associated with each cell. known in (Blunsom and Cohn, 2011), but in our case it is also learned from data at the same time. Note that because wt depends on already generated words w1 · · · wt−1, our model is considered as an autoregressive HMM rather than a vanilla HMM, as shown in Figure 4 (wt−1 → wt dependency). Since segment models like NPYLM have segment lengths as hidden states, they are called semiMarkov models (Murphy, 2002). In contrast, our model also has hidden part-of-speech, thus we call it a Pitman-Yor Hidden Semi-Markov model (PYHSMM).a Note that this is considered as a generative counterpart of a discriminative model known as a hidden semi-Markov CRF (Sarawagi and Cohen, 2005). 4 Inference Inference of PYHSMM proceeds in almost the same way as NPYLM in Figure 3: For each sentence, first remove the customers associated with the old segmentation similarly to adding them. After sampling a new segmentation and states, the model is updated by adding new customers in accordance with the new segmentation and hid</context>
</contexts>
<marker>Murphy, 2002</marker>
<rawString>Kevin Murphy. 2002. Hidden semi-Markov models (segment models). http://www.cs.ubc.ca/˜murphyk/ Papers/segment.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoaki Nakamura</author>
</authors>
<title>Takayuki Nagai, Kotaro Funakoshi, Shogo Nagasaka, Tadahiro Taniguchi, and Naoto Iwahashi.</title>
<date>2014</date>
<booktitle>In 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS’14),</booktitle>
<pages>600--607</pages>
<marker>Nakamura, 2014</marker>
<rawString>Tomoaki Nakamura, Takayuki Nagai, Kotaro Funakoshi, Shogo Nagasaka, Tadahiro Taniguchi, and Naoto Iwahashi. 2014. Mutual Learning of an Object Concept and Language Model Based on MLDA and NPYLM. In 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS’14), pages 600–607.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Masato Mimura</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>Learning a Language Model from Continuous Speech.</title>
<date>2010</date>
<booktitle>In Proc. of INTERSPEECH</booktitle>
<contexts>
<context position="29985" citStr="Neubig et al., 2010" startWordPosition="5225" endWordPosition="5228">acter HMM. In the present paper, we extended it to include a hidden state for each word. Therefore, it might be interesting to introduce a hidden state also for each character. Unlike western languages, there are many kinds of Chinese characters that work quite differently, and Japanese uses several distinct kinds of characters, such as a Chinese character, Hiragana, Katakana, whose mixture would constitute a single word. Therefore, statistical modeling of different types of characters is an important research venue for the future. NPYLM has already applied and extended to speech recognition (Neubig et al., 2010), statistical machine translation (Nguyen et al., 2010), or even robotics (Nakamura et al., 2014). For all these research area, we believe PYHSMM would be beneficial for their extension. 7 Conclusion In this paper, we proposed a Pitman-Yor Hidden Semi-Markov model for joint unsupervised word segmentation and part-of-speech tagging on a raw sequence of characters. It can also be viewed as a way to build a class n-gram language model directly on strings, without any “word” information a priori. We applied our PYHSMM on several standard datasets on Japanese, Chinese and Thai, and it outperformed </context>
</contexts>
<marker>Neubig, Mimura, Mori, Kawahara, 2010</marker>
<rawString>Graham Neubig, Masato Mimura, Shinsuke Mori, and Tatsuya Kawahara. 2010. Learning a Language Model from Continuous Speech. In Proc. of INTERSPEECH 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ThuyLinh Nguyen</author>
<author>Stephan Vogel</author>
<author>Noah A Smith</author>
</authors>
<title>Nonparametric Word Segmentation for Machine Translation. In COLING</title>
<date>2010</date>
<pages>815--823</pages>
<contexts>
<context position="30040" citStr="Nguyen et al., 2010" startWordPosition="5233" endWordPosition="5236">ude a hidden state for each word. Therefore, it might be interesting to introduce a hidden state also for each character. Unlike western languages, there are many kinds of Chinese characters that work quite differently, and Japanese uses several distinct kinds of characters, such as a Chinese character, Hiragana, Katakana, whose mixture would constitute a single word. Therefore, statistical modeling of different types of characters is an important research venue for the future. NPYLM has already applied and extended to speech recognition (Neubig et al., 2010), statistical machine translation (Nguyen et al., 2010), or even robotics (Nakamura et al., 2014). For all these research area, we believe PYHSMM would be beneficial for their extension. 7 Conclusion In this paper, we proposed a Pitman-Yor Hidden Semi-Markov model for joint unsupervised word segmentation and part-of-speech tagging on a raw sequence of characters. It can also be viewed as a way to build a class n-gram language model directly on strings, without any “word” information a priori. We applied our PYHSMM on several standard datasets on Japanese, Chinese and Thai, and it outperformed previous figures to yield the state-ofthe-art results, </context>
</contexts>
<marker>Nguyen, Vogel, Smith, 2010</marker>
<rawString>ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith. 2010. Nonparametric Word Segmentation for Machine Translation. In COLING 2010, pages 815– 823.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
<author>Marc Yor</author>
</authors>
<title>The Two-Parameter Poisson-Dirichlet Distribution Derived from a Stable Subordinator. Annals of Probability,</title>
<date>1997</date>
<volume>25</volume>
<issue>2</issue>
<pages>900</pages>
<contexts>
<context position="8166" citStr="Pitman and Yor, 1997" startWordPosition="1271" endWordPosition="1274">tion 4. We conduct experiments on some East Asian languages in Section 5. Section 6 discusses implications of our model and related work, and Section 7 concludes the paper. 2 Nested Pitman-Yor Language Model Our joint model of words and states is an extension of the Nested Pitman-Yor Language Model (Mochihashi et al., 2009) of a string, which in turn is an extension of a Bayesian n-gram language model called Hierarchical Pitman-Yor Language Model (HPYLM) (Teh, 2006). Character HPYLM Word HPYLM 1775 HPYLM is a nonparametric Bayesian model of n-gram distribution based on the Pitman-Yor process (Pitman and Yor, 1997) that generates a discrete distribution G as G ∼ PY(G0, d, θ). Here, d is a discount factor, “parent” distribution G0 is called a base measure and θ controls how similar G is to G0 in expectation. In HPYLM, n-gram distribution Gn = {p(wt|wt−1 · · · wt−(n−1))} is assumed to be generated from the Pitman-Yor process Gn ∼ PY(Gn−1, dn, θn) , (3) where the base measure Gn−1 is an (n−1)-gram distribution generated recursively in accordance with (3). Note that there are different Gn for each n-gram history h = wt−1 · · · wt−(n−1). When we reach the unigram G1 and need to use a base measure G0, i.e. pr</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>Jim Pitman and Marc Yor. 1997. The Two-Parameter Poisson-Dirichlet Distribution Derived from a Stable Subordinator. Annals of Probability, 25(2):855– 900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>SemiMarkov Conditional Random Fields for Information Extraction.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems 17 (NIPS 2004),</booktitle>
<pages>1185--1192</pages>
<contexts>
<context position="14830" citStr="Sarawagi and Cohen, 2005" startWordPosition="2482" endWordPosition="2485">e it is also learned from data at the same time. Note that because wt depends on already generated words w1 · · · wt−1, our model is considered as an autoregressive HMM rather than a vanilla HMM, as shown in Figure 4 (wt−1 → wt dependency). Since segment models like NPYLM have segment lengths as hidden states, they are called semiMarkov models (Murphy, 2002). In contrast, our model also has hidden part-of-speech, thus we call it a Pitman-Yor Hidden Semi-Markov model (PYHSMM).a Note that this is considered as a generative counterpart of a discriminative model known as a hidden semi-Markov CRF (Sarawagi and Cohen, 2005). 4 Inference Inference of PYHSMM proceeds in almost the same way as NPYLM in Figure 3: For each sentence, first remove the customers associated with the old segmentation similarly to adding them. After sampling a new segmentation and states, the model is updated by adding new customers in accordance with the new segmentation and hidden states. 4.1 Sampling words and states To sample words and states (part-of-speech) jointly, we first compute inside probabilities forward from BOS to EOS and sample backwards from EOS according to the Forward filteringBackward sampling algorithm (Scott, 2002). T</context>
</contexts>
<marker>Sarawagi, Cohen, 2005</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2005. SemiMarkov Conditional Random Fields for Information Extraction. In Advances in Neural Information Processing Systems 17 (NIPS 2004), pages 1185–1192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven L Scott</author>
</authors>
<title>Bayesian Methods for Hidden Markov Models.</title>
<date>2002</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>97--337</pages>
<contexts>
<context position="15427" citStr="Scott, 2002" startWordPosition="2581" endWordPosition="2582">nd Cohen, 2005). 4 Inference Inference of PYHSMM proceeds in almost the same way as NPYLM in Figure 3: For each sentence, first remove the customers associated with the old segmentation similarly to adding them. After sampling a new segmentation and states, the model is updated by adding new customers in accordance with the new segmentation and hidden states. 4.1 Sampling words and states To sample words and states (part-of-speech) jointly, we first compute inside probabilities forward from BOS to EOS and sample backwards from EOS according to the Forward filteringBackward sampling algorithm (Scott, 2002). This 4Lately, Johnson et al. (2013) proposed a nonparametric Bayesian hidden semi-Markov models for general state spaces. However, it depends on a separate distribution for a state duration, thus is clealy different from ours for a natural language. zt−1 zt zt+1 Observation s wt−1 wt wt+1 · · · · · · · · · · · · Word leneth ndeT1me t MU] 1777 can be regarded as a “stochastic Viterbi” algorithm that has the advantage of not being trapped in local minima, since it is a valid move of a Gibbs sampler in a Bayesian model. For a word bigram case for simplicity, inside variable α[t][k][z] is a prob</context>
</contexts>
<marker>Scott, 2002</marker>
<rawString>Steven L. Scott. 2002. Bayesian Methods for Hidden Markov Models. Journal of the American Statistical Association, 97:337–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kairit Sirts</author>
<author>Tanel Alum¨ae</author>
</authors>
<title>A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induction.</title>
<date>2012</date>
<booktitle>In NAACL 2012,</booktitle>
<pages>407--416</pages>
<marker>Sirts, Alum¨ae, 2012</marker>
<rawString>Kairit Sirts and Tanel Alum¨ae. 2012. A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induction. In NAACL 2012, pages 407–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A Bayesian Interpretation of Interpolated Kneser-Ney.</title>
<date>2006</date>
<tech>Technical Report TRA2/06,</tech>
<institution>School of Computing, NUS.</institution>
<contexts>
<context position="8015" citStr="Teh, 2006" startWordPosition="1249" endWordPosition="1250">on 3 extends it to include hidden states to yield a hidden semi-Markov models (Murphy, 2002), and we describe its inference procedure in Section 4. We conduct experiments on some East Asian languages in Section 5. Section 6 discusses implications of our model and related work, and Section 7 concludes the paper. 2 Nested Pitman-Yor Language Model Our joint model of words and states is an extension of the Nested Pitman-Yor Language Model (Mochihashi et al., 2009) of a string, which in turn is an extension of a Bayesian n-gram language model called Hierarchical Pitman-Yor Language Model (HPYLM) (Teh, 2006). Character HPYLM Word HPYLM 1775 HPYLM is a nonparametric Bayesian model of n-gram distribution based on the Pitman-Yor process (Pitman and Yor, 1997) that generates a discrete distribution G as G ∼ PY(G0, d, θ). Here, d is a discount factor, “parent” distribution G0 is called a base measure and θ controls how similar G is to G0 in expectation. In HPYLM, n-gram distribution Gn = {p(wt|wt−1 · · · wt−(n−1))} is assumed to be generated from the Pitman-Yor process Gn ∼ PY(Gn−1, dn, θn) , (3) where the base measure Gn−1 is an (n−1)-gram distribution generated recursively in accordance with (3). No</context>
<context position="11566" citStr="Teh (2006)" startWordPosition="1892" endWordPosition="1893">know word segmentation w beforehand, we begin with a trivial segmentation in which every sentence is a single word3. Then, we iteratively refine it by sampling a new word segmentation w(s) of a sentence s in a Markov Chain Monte Carlo (MCMC) framework using a dynamic programming, as is done with PCFG by (Johnson et al., 2007) shown in Figure 3 where we omit MH steps for computational reasons. Further note that every hyperparameter dn, θn of NPYLM can be sampled from the posterior in a Bayesian fashion, as opposed to heuristic methods that rely on a development set for tuning. For details, see Teh (2006). 3 Pitman-Yor Hidden Semi-Markov Models NPYLM is a complete generative model of a string, that is, a hierarchical Bayesian n-gram lanInput: a collection of strings S Add initial segmentation w(s) to Θ for j = 1 ··· J do for s in randperm (S) do Remove customers of w(s) from Θ Sample w(s) according to p(w|s, Θ) Add customers of w(s) to Θ end for Sample hyperparameters of Θ end for Figure 3: MCMC inference of NPYLM Θ. 2To be precise, this occurs whenever thw is incremented in the unigram restaurant. 3Note that a child first memorizes what his mother says as a single word and gradually learns th</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A Bayesian Interpretation of Interpolated Kneser-Ney. Technical Report TRA2/06, School of Computing, NUS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Andreas Vlachos</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>The infinite HMM for unsupervised PoS tagging.</title>
<date>2009</date>
<booktitle>In EMNLP 2009,</booktitle>
<pages>678--687</pages>
<marker>Van Gael, Vlachos, Ghahramani, 2009</marker>
<rawString>Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahramani. 2009. The infinite HMM for unsupervised PoS tagging. In EMNLP 2009, pages 678– 687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin Zhikov</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>An Efficient Algorithm for Unsupervised Word Segmentation with Branching Entropy and MDL.</title>
<date>2010</date>
<booktitle>In EMNLP 2010,</booktitle>
<pages>832--842</pages>
<contexts>
<context position="4125" citStr="Zhikov et al., 2010" startWordPosition="612" endWordPosition="615">てんのか Figure 1: Sample of Japanese Twitter text that is difficult to analyze by ordinary supervised segmentation. It contains a lot of novel words, emoticons, and colloquial expressions. 1774 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1774–1782, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics the next word). By formulating such ideas as search or MDL problems of given coding length&apos;, word boundaries are found in an algorithmic fashion (Zhikov et al., 2010; Magistry and Sagot, 2013). However, such methods have difficulty incorporating higher-order statistics beyond simple heuristics, such as word transitions, word spelling formation, or word length distribution. Moreover, they usually depends on tuning parameters like thresholds that cannot be learned without human intervention. In contrast, statistical models are ready to incorporate all such phenomena within a consistent statistical generative model of a string, and often prove to work better than heuristic methods (Goldwater et al., 2006; Mochihashi et al., 2009). In fact, the statistical me</context>
<context position="6057" citStr="Zhikov et al. (2010)" startWordPosition="920" endWordPosition="923">mation shared between the models, which is not the case with generative models. So far, these approaches only find word segmentation, leaving part-of-speech information behind. These two problems are not actually independent but interrelated, because knowing the part-of-speech of some infrequent or unknown word will give contextual clues to word segmentation, and vice versa. For example, in Japanese *tttttt can be segmented into not only *tt/t/tt/t (plum/too/peach/too), but also into *tt/tt/ tt (plum/peach/peach), which is ungrammatical. However, we could exclude the latter case &apos;For example, Zhikov et al. (2010) defined a coding length using character n-grams plus MDL penalty. Since this can be interpreted as a crude “likelihood” and a prior, its essence is similar but driven by a quite simplistic model. Figure 2: NPYLM represented in a hierarchical Chinese restaurant process. Here, a character ocgram HPYLM is embedded in a word n-gram HPYLM and learned jointly during inference. if we leverage knowledge that a state sequence N/P/N/P is much more plausible in Japanese than N/N/N from the part-of-speech information. Sirts and Alum¨ae (2012) treats a similar problem of POS induction with unsupervised mo</context>
<context position="23846" citStr="Zhikov et al. (2010)" startWordPosition="4057" endWordPosition="4060">m word length prediction with a Negative Binomial generalized linear model (in percent). &gt; 5 are figures for word length &gt; 5. Final row is the maximum length of a word found in each dataset. 0 L 2 4 6 8 10 12 14 16 Figure 6: Distribution of predicted maximum word lengths on the Kyoto corpus. Frequency 14000 12000 10000 4000 8000 6000 2000 1779 Dataset PYHSMM NPY BE HMM2 Kyoto 71.5 62.1 71.3 NA BCCWJ 70.5 NA NA NA MSR 82.9 80.2 78.2 81.7 CITYU 82.6* 82.4 78.7 NA PKU 81.6 NA 80.8 81.1 BEST 82.1 NA 82.1 NA Table 4: Accuracies of unsupervised word segmentation. BE is a Branching Entropy method of Zhikov et al. (2010), and HMM2 is a product of word and character HMMs of Chen et al. (2014). * is the accuracy decoded with L = 3: it becomes 81.7 with L=4 as MSR and PKU. have part-of-speech annotations as well. For these data, we also evaluated the precision of part-ofspeech induction on the output of unsupervised word segmentation above. Note that the precision is measured only over correct word segmentation that the system has output. Table 5 shows the precisions; to the best of our knowledge, there are no previous work on joint unsupervised learning of words and tags, thus we only compared with Bayesian HMM</context>
</contexts>
<marker>Zhikov, Takamura, Okumura, 2010</marker>
<rawString>Valentin Zhikov, Hiroya Takamura, and Manabu Okumura. 2010. An Efficient Algorithm for Unsupervised Word Segmentation with Branching Entropy and MDL. In EMNLP 2010, pages 832–842.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>