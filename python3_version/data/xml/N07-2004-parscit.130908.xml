<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004759">
<title confidence="0.866905">
JOINT VERSUS INDEPENDENT PHONOLOGICAL FEATURE
MODELS WITHIN CRF PHONE RECOGNITION
</title>
<author confidence="0.996538">
Ilana Bromberg*, Jeremy Morris†, and Eric Fosler-Lussier*†
</author>
<affiliation confidence="0.998431">
*Department of Linguistics
†Department of Computer Science and Engineering
The Ohio State University, Columbus, OH
</affiliation>
<email confidence="0.993797">
bromberg@ling.ohio-state.edu, {morrijer, fosler}@cse.ohio-state.edu
</email>
<sectionHeader confidence="0.997349" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947733333333">
We compare the effect of joint modeling
of phonological features to independent
feature detectors in a Conditional Random
Fields framework. Joint modeling of fea-
tures is achieved by deriving phonological
feature posteriors from the posterior prob-
abilities of the phonemes. We find that
joint modeling provides superior perfor-
mance to the independent models on the
TIMIT phone recognition task. We ex-
plore the effects of varying relationships
between phonological features, and sug-
gest that in an ASR system, phonological
features should be handled as correlated,
rather than independent.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999885">
Phonological features have received attention as a
linguistically-based representation for sub-word in-
formation in automatic speech recognition. These
sub-phonetic features allow for a more refined repre-
sentation of speech by allowing for temporal desyn-
chronization between articulators, and help account
for some phonological changes common in sponta-
neous speech, such as devoicing (Kirchhoff, 1999;
Livescu, 2005). A number of methods have been de-
veloped for detecting acoustic phonological features
and related acoustic landmarks directly from data
using Multi-Layer Perceptrons (Kirchhoff, 1999),
Support Vector Machines (Hasegawa-Johnson et al.,
2005; Sharenborg et al., 2006), or Hidden Markov
Models (Li and Lee, 2005). These techniques
typically assume that acoustic phonological feature
events are independent for ease of modeling.
</bodyText>
<page confidence="0.985303">
13
</page>
<bodyText confidence="0.99977752">
In one study that broke the independence assump-
tion (Chang et al., 2001), the investigators devel-
oped conditional detectors: MLP detectors of acous-
tic phonological features that are hierarchically de-
pendent on a different phonological class. In (Ra-
jamanohar and Fosler-Lussier, 2005) it was shown
that such a conditional training of detectors tended
to have correlated frame errors, and that improve-
ments in detection could be obtained by training
joint detectors. For many features, the best detector
can be obtained by collapsing MLP phone posteriors
into feature classes by marginalizing across phones
within a class. This was shown only for frame-level
classification rather than phone recognition.
Posterior estimates of phonological feature
classes, as in Table 1, particularly those derived
from MLPs, have been used as input to HMMs
(Launay et al., 2002), Dynamic Bayesian Networks
(DBNs) (Frankel et al., 2004; Livescu, 2005),
and Conditional Random Fields (CRFs) (Morris
and Fosler-Lussier, 2006). Here we evaluate
phonological feature detectors created from MLP
phone posterior estimators (joint feature models)
rather than the independently trained MLP feature
detectors used in previous work.
</bodyText>
<sectionHeader confidence="0.963919" genericHeader="method">
2 Conditional Random Fields
</sectionHeader>
<bodyText confidence="0.999648166666667">
CRFs (Lafferty et al., 2001) are a joint model of
a label sequence conditioned on a set of inputs.
No independence is assumed among the input; the
CRF model discriminates between hypothesized la-
bel sequences according to an exponential function
of weighted feature functions:
</bodyText>
<equation confidence="0.7515125">
P(y|x) a exp � (S(x, y, i) + T(x, y, i)) (1)
�
</equation>
<note confidence="0.345726">
Proceedings of NAACL HLT 2007, Companion Volume, pages 13–16,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<table confidence="0.994943888888889">
Class Feature Values
SONORITY Vowel, Obstruent, Sonorant, Syllabic, Silence
VOICE Voiced, Unvoiced, N/A
MANNER Fricative, Stop, Stop-Closure, Flap, Nasal, Approximant, Nasalflap, N/A
PLACE Labial, Dental, Alveolar, Palatal, Velar, Glottal, Lateral, Rhotic, N/A
HEIGHT High, Mid, Low, Lowhigh, Midhigh, N/A
FRONT Front, Back, Central, Backfront, N/A
ROUND Round, Nonround, Roundnonround, Nonroundround, N/A
TENSE Tense, Lax N/A
</table>
<tableCaption confidence="0.999936">
Table 1: Phonetic feature classes and associated values
</tableCaption>
<bodyText confidence="0.991482516129032">
where P(ylx) is the probability of label sequence
y given an input frame sequence x, i is the frame
index, and S and T are a set of state feature functions
and a set of transition feature functions, defined as:
14 experiments, they are generated by training MLP
� Ajsj(y, x, i), and (2) phone detectors, by evaluating the feature informa-
5(x, y, i) = pktk(yi−1, yi, x, i) (3) tion inherent in the MLP phone posteriors, and by
j training independent MLPs to detect the various fea-
� tures within the classes described. The use of CRFs
T (x, y, i) = allows us to explore the dependencies among feature
k classes, as well as the usefulness of phone posteri
ors
versus feature classes as inputs.
where A and p are weights determined by the learn-
ing algorithm. In NLP applications, the component
feature functions sj and tk are typically realized as
binary indicator functions indicating the presence or
absence of a feature, but in ASR applications it is
more typical to utilize real-valued functions, such as
those derived from the sufficient statistics of Gaus-
sians (e.g., (Gunawardana et al., 2005)).
We can use posterior estimates of phone classes or
phonological feature classes from MLPs as feature
functions (inputs) within the CRF model. A more
detailed description of this CRF paradigm can be
found in (Morris and Fosler-Lussier, 2006), which
shows that the results of phone recognition using
CRFs is comparable to that of HMMs or Tandem
systems, with fewer constraints being imposed on
the model. State feature functions in our system are
defined such that
</bodyText>
<equation confidence="0.945455">
sφ f(yi, x, i) = S 0, otherwise (4)
NNf(xi), ifyi = 0
</equation>
<bodyText confidence="0.933514">
d 1000 hidden nodes.
</bodyText>
<sectionHeader confidence="0.895031" genericHeader="method">
4 Joint Phone Posteriors vs. Independent
</sectionHeader>
<subsectionHeader confidence="0.713645">
Feature Posteriors
</subsectionHeader>
<bodyText confidence="0.999768">
ods of generating these feature functions. In vari where the MLP output for feature f at time i is
ous NNf(xi). This allows for an association between
a phone 0 and a feature f (even if f is traditionally
not associated with 0).
In this study, we experiment with different meth-
</bodyText>
<sectionHeader confidence="0.601848" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.994335347826087">
We use the TIMIT speech corpus for all training and
testing (Garofolo et al., 1993). The acoustic data
is manually labeled at the phonetic level, and we
propagate this phonetic label information to every
frame of data. For the feature analyses, we employ
a lookup table that defines each phone in terms of
8 feature classes, as shown in Table 1. We extract
acoustic features in the form of 12th order PLP fea-
tures plus delta coefficients. We then use these as
inputs to several sets of neural networks using the
ICSI QuickNet MLP neural network software (John-
son, 2004), with the 39 acoustic features as input, a
varying number of phone or feature class posteriors
as output, an
The first experiment contrasts joint versus indepen-
dent feature modeling within the CRF system. We
compare a set of phonological feature probabilities
derived from the phone posteriors (a joint model)
with MLP phone posteriors and with independently
trained MLP phonological feature posteriors.
The inputs to the first CRF are sets of 61 state fea-
ture functions from the phonemic MLP posteriors,
each function is an estimate of the posteri
</bodyText>
<table confidence="0.9347554">
or proba-
Input Type. Phn. Accuracy Phn. Correct
Phones 67.27 68.77
Features 65.25 66.65
Phn. —* Feat. 66.45 67.94
</table>
<tableCaption confidence="0.959323">
Table 2: Results for Exp. 1: Phone and feature pos-
</tableCaption>
<bodyText confidence="0.978153428571429">
teriors as input to the CRF phone recognition
bility of one phone. The inputs to the second CRF
model are sets of 44 functions corresponding to the
phonological features listed in Table 1. The CRF
models are trained to associate these feature func-
tions with phoneme labels, incorporating the pat-
terns of variation seen in the MLPs.
The results show that phone-based posteri-
ors produce better phone recognition results than
independently-trained phonological features. This
could be due in part to the larger number of param-
eters in the system, but it could also be due to the
joint modeling that occurs in the phone classifier.
In order to equalize the feature spaces, we use the
output of the phoneme classifier to derive phonolog-
ical feature posteriors. In each frame we sum the
MLP phone posteriors of all phones that contain a
given feature. For instance, in the first frame, for
the feature LOW, we sum the posterior estimates at-
tributed to the phones aa, ae and ao. This is repeated
for each feature in each frame. The CRF model is
trained on these data and tested accordingly. The re-
sults are significantly better (p&lt;.001) than the previ-
ous features model, but are significantly worse than
the phone posteriors (p&lt;.005).
The results of Experiment 1 confirm the hypoth-
esis of (Rajamanohar and Fosler-Lussier, 2005) that
joint modeling using several types of feature infor-
mation is superior to individual modeling in phone
recognition, where only phoneme information is
used. The difference between the phone posteriors
and individual feature posteriors seems to be related
both to the larger CRF parameter space with larger
input, and the joint modeling provided by phone
posteriors.
</bodyText>
<sectionHeader confidence="0.989249" genericHeader="method">
5 Phonological Feature Class Analysis
</sectionHeader>
<bodyText confidence="0.999838">
In the second experiment, we examine the influence
of each feature class on the accuracy of the recog-
nizer. We iteratively remove the set of state fea-
ture functions corresponding to each feature class
</bodyText>
<table confidence="0.999818909090909">
Class Removed Feats. Phn. Acc. Phn. Corr.
None 44 65.25 66.65
Sonority 39 65.15 66.58
Voice 41 63.60* 65.03*
Manner 36 58.92* 60.60*
Place 35 53.22* 55.13*
Height 38 62.58* 64.07*
Front 39 64.51* 65.95*
Round 39 65.19 66.64
Tense 41 64.20* 65.65*
* p&lt;.05, different from no features removed
</table>
<tableCaption confidence="0.956723">
Table 3: Results of Exp. 2: Removing feature
classes from the input
</tableCaption>
<bodyText confidence="0.99983703030303">
from the input to the CRF. The original functions
are the output of the independently-trained feature
class MLPs. The phone recognition accuracy for the
CRF having removed each class is shown in Table 3.
In Table 4 we show how removing each feature class
affects the labeling of vowels and consonants.
Manner provides an example of the influence of a
single feature class. Both the Accuracy and Correct-
ness scores decrease significantly when features as-
sociated with Manner are removed. Manner features
distinguish consonants but not vowels, so the effect
is concentrated on the recognition of consonants.
The results of Experiment 2 show that certain fea-
ture classes are redundant from the point of view of
phone recognition. In English, Round is correlated
with Front. When we remove Round, the phonemes
remain uniquely identified by the other classes. The
same is true for the Sonority class. The results show
that the inclusion of these redundant features is not
detrimental to the recognition accuracy. Accuracy
and Correctness improve non-significantly when the
redundant features are included.
Clearly, the “independent” phonological feature
streams are not truly independent. Otherwise, per-
formance would decrease overall as we removed
each feature class, assuming predictiveness.
Removal of Place causes a slight worsening of
recognition of vowels. This is surprising, because
Place does not characterize vowels. An analysis of
the MLP activations showed that the detector for
Place=N/A is a stronger indicator for vowels than
is the Sonority=Vowel detector. This is especially
true for the vowel ax, which is frequent in the data,
</bodyText>
<page confidence="0.989204">
15
</page>
<table confidence="0.99980075">
Class Removed Percent Correct:
Vowels Consonants
None 62.68 68.91
Sonority 62.18 69.08
Voice 62.39 66.53*
Manner 61.84 59.89*
Place 60.77* 51.94*
Height 55.92* 68.69
Frontness 60.80* 68.87
Roundness 62.25 69.13
Tenseness 60.15* 68.76
* p&lt;.05, different from no features removed
</table>
<tableCaption confidence="0.9554365">
Table 4: Effect of removing each feature class on
recognition accuracy of vowels and consonants
</tableCaption>
<bodyText confidence="0.999852">
thus greatly influences the vowel recognition statis-
tic. Removing the Place detectors leads to a loss in
vowel vs. consonant information. This results in an
increased number of consonant for vowel substitu-
tions (from 560 to 976), thus a decrease in vowel
recognition accuracy.
Besides extending the findings in (Rajamanohar
and Fosler-Lussier, 2005), this provides a cautionary
tale for incorporating redundant phonological fea-
ture estimators into ASR: these systems need to be
able to handle correlated input, either by design (as
in a CRF), through full or semi-tied covariance ma-
trices in HMMs, or by including the appropriate sta-
tistical dependencies in DBNs.
</bodyText>
<sectionHeader confidence="0.999364" genericHeader="method">
6 Summary
</sectionHeader>
<bodyText confidence="0.999992625">
We have shown the effect of using joint model-
ing of phonetic feature information in conjunction
with the use of CRFs as a discriminative classifier.
Phonetic posteriors, as joint models of phonological
features, provide superior phone recognition perfor-
mance over independently-trained phonological fea-
ture models. We also find that redundant features are
often modeled well within the CRF framework.
</bodyText>
<sectionHeader confidence="0.999319" genericHeader="conclusions">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999983428571429">
The authors thank the International Computer Sci-
ence Institute for providing the neural network soft-
ware. The authors also thank four anonymous re-
viewers. This work was supported by NSF ITR
grant IIS-0427413; the opinions and conclusions ex-
pressed in this work are those of the authors and not
of any funding agency.
</bodyText>
<sectionHeader confidence="0.995666" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999240319148936">
S. Chang, S. Greenberg, and M. Wester. 2001. An eli-
tist approach to articulatory-acoustic feature classifica-
tion. In Interspeech.
J. Frankel, M. Wester, and S. King. 2004. Articulatory
feature recognition using dynamic bayesian networks.
In ICSLP.
J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, and
N. Dahlgren. 1993. DARPA TIMIT acoustic-phonetic
continuous speech corpus. Technical Report NISTIR
4930, National Institute of Standards and Technology,
Gaithersburg, MD, February. Speech Data published
on CD-ROM: NIST Speech Disc 1-1.1, October 1990.
A. Gunawardana, M. Mahajan, A. Acero, and J. Platt.
2005. Hidden conditional random fields for phone
classification. In Interspeech.
M. Hasegawa-Johnson et al. 2005. Landmark-based
speech recognition: Report of the 2004 Johns Hopkins
Summer Workshop. In ICASSP.
D. Johnson. 2004. ICSI Quicknet software package.
http://www.icsi.berkeley.edu/Speech/qn.html.
K. Kirchhoff. 1999. Robust Speech Recognition Using
Articulatory Information. Ph.D. thesis, University of
Bielefeld.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
the 18th International Conference on Machine Learn-
ing.
B. Launay, O. Siohan, A. C. Surendran, and C.-H. Lee.
2002. Towards knowledge based features for large vo-
cabulary automatic speech recognition. In ICASSP.
J. Li and C.-H. Lee. 2005. On designing and evaluating
speech event detectors. In Interspeech.
K. Livescu. 2005. Feature-Based Pronunciation Model-
ing for Automatic Speech Recognition. Ph.D. thesis,
MIT.
J. Morris and E. Fosler-Lussier. 2006. Combining pho-
netic attributes using conditional random fields. In In-
terspeech.
M. Rajamanohar and E. Fosler-Lussier. 2005. An evalu-
ation of hierarchical articulatory feature detectors. In
IEEE Automatic Speech Recogntion and Understand-
ing Workshop.
O. Sharenborg, V. Wan, and R.K. Moore. 2006. Cap-
turing fine-phonetic variation in speech through auto-
matic classification of articulatory features. In ITRW
on Speech Recognition and Intrinsic Variation.
</reference>
<page confidence="0.998668">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.154794">
<title confidence="0.969462">JOINT VERSUS INDEPENDENT PHONOLOGICAL MODELS WITHIN CRF PHONE RECOGNITION</title>
<author confidence="0.791892">Jeremy</author>
<author confidence="0.791892">Eric</author>
<affiliation confidence="0.55475">of of Computer Science and The Ohio State University, Columbus,</affiliation>
<abstract confidence="0.998937125">We compare the effect of joint modeling of phonological features to independent feature detectors in a Conditional Random Fields framework. Joint modeling of features is achieved by deriving phonological feature posteriors from the posterior probabilities of the phonemes. We find that joint modeling provides superior performance to the independent models on the TIMIT phone recognition task. We explore the effects of varying relationships between phonological features, and suggest that in an ASR system, phonological features should be handled as correlated, rather than independent.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Chang</author>
<author>S Greenberg</author>
<author>M Wester</author>
</authors>
<title>An elitist approach to articulatory-acoustic feature classification.</title>
<date>2001</date>
<booktitle>In Interspeech.</booktitle>
<contexts>
<context position="1852" citStr="Chang et al., 2001" startWordPosition="251" endWordPosition="254">account for some phonological changes common in spontaneous speech, such as devoicing (Kirchhoff, 1999; Livescu, 2005). A number of methods have been developed for detecting acoustic phonological features and related acoustic landmarks directly from data using Multi-Layer Perceptrons (Kirchhoff, 1999), Support Vector Machines (Hasegawa-Johnson et al., 2005; Sharenborg et al., 2006), or Hidden Markov Models (Li and Lee, 2005). These techniques typically assume that acoustic phonological feature events are independent for ease of modeling. 13 In one study that broke the independence assumption (Chang et al., 2001), the investigators developed conditional detectors: MLP detectors of acoustic phonological features that are hierarchically dependent on a different phonological class. In (Rajamanohar and Fosler-Lussier, 2005) it was shown that such a conditional training of detectors tended to have correlated frame errors, and that improvements in detection could be obtained by training joint detectors. For many features, the best detector can be obtained by collapsing MLP phone posteriors into feature classes by marginalizing across phones within a class. This was shown only for frame-level classification </context>
</contexts>
<marker>Chang, Greenberg, Wester, 2001</marker>
<rawString>S. Chang, S. Greenberg, and M. Wester. 2001. An elitist approach to articulatory-acoustic feature classification. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Frankel</author>
<author>M Wester</author>
<author>S King</author>
</authors>
<title>Articulatory feature recognition using dynamic bayesian networks.</title>
<date>2004</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="2698" citStr="Frankel et al., 2004" startWordPosition="378" endWordPosition="381">hat such a conditional training of detectors tended to have correlated frame errors, and that improvements in detection could be obtained by training joint detectors. For many features, the best detector can be obtained by collapsing MLP phone posteriors into feature classes by marginalizing across phones within a class. This was shown only for frame-level classification rather than phone recognition. Posterior estimates of phonological feature classes, as in Table 1, particularly those derived from MLPs, have been used as input to HMMs (Launay et al., 2002), Dynamic Bayesian Networks (DBNs) (Frankel et al., 2004; Livescu, 2005), and Conditional Random Fields (CRFs) (Morris and Fosler-Lussier, 2006). Here we evaluate phonological feature detectors created from MLP phone posterior estimators (joint feature models) rather than the independently trained MLP feature detectors used in previous work. 2 Conditional Random Fields CRFs (Lafferty et al., 2001) are a joint model of a label sequence conditioned on a set of inputs. No independence is assumed among the input; the CRF model discriminates between hypothesized label sequences according to an exponential function of weighted feature functions: P(y|x) a</context>
</contexts>
<marker>Frankel, Wester, King, 2004</marker>
<rawString>J. Frankel, M. Wester, and S. King. 2004. Articulatory feature recognition using dynamic bayesian networks. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Garofolo</author>
<author>L Lamel</author>
<author>W Fisher</author>
<author>J Fiscus</author>
<author>D Pallett</author>
<author>N Dahlgren</author>
</authors>
<title>DARPA TIMIT acoustic-phonetic continuous speech corpus.</title>
<date>1993</date>
<tech>Technical Report NISTIR 4930,</tech>
<institution>National Institute of Standards and Technology,</institution>
<location>Gaithersburg, MD,</location>
<contexts>
<context position="6035" citStr="Garofolo et al., 1993" startWordPosition="919" endWordPosition="922">fewer constraints being imposed on the model. State feature functions in our system are defined such that sφ f(yi, x, i) = S 0, otherwise (4) NNf(xi), ifyi = 0 d 1000 hidden nodes. 4 Joint Phone Posteriors vs. Independent Feature Posteriors ods of generating these feature functions. In vari where the MLP output for feature f at time i is ous NNf(xi). This allows for an association between a phone 0 and a feature f (even if f is traditionally not associated with 0). In this study, we experiment with different meth3 Experimental Setup We use the TIMIT speech corpus for all training and testing (Garofolo et al., 1993). The acoustic data is manually labeled at the phonetic level, and we propagate this phonetic label information to every frame of data. For the feature analyses, we employ a lookup table that defines each phone in terms of 8 feature classes, as shown in Table 1. We extract acoustic features in the form of 12th order PLP features plus delta coefficients. We then use these as inputs to several sets of neural networks using the ICSI QuickNet MLP neural network software (Johnson, 2004), with the 39 acoustic features as input, a varying number of phone or feature class posteriors as output, an The </context>
</contexts>
<marker>Garofolo, Lamel, Fisher, Fiscus, Pallett, Dahlgren, 1993</marker>
<rawString>J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, and N. Dahlgren. 1993. DARPA TIMIT acoustic-phonetic continuous speech corpus. Technical Report NISTIR 4930, National Institute of Standards and Technology, Gaithersburg, MD, February. Speech Data published on CD-ROM: NIST Speech Disc 1-1.1, October 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gunawardana</author>
<author>M Mahajan</author>
<author>A Acero</author>
<author>J Platt</author>
</authors>
<title>Hidden conditional random fields for phone classification. In Interspeech.</title>
<date>2005</date>
<contexts>
<context position="5053" citStr="Gunawardana et al., 2005" startWordPosition="750" endWordPosition="753">ious fea� tures within the classes described. The use of CRFs T (x, y, i) = allows us to explore the dependencies among feature k classes, as well as the usefulness of phone posteri ors versus feature classes as inputs. where A and p are weights determined by the learning algorithm. In NLP applications, the component feature functions sj and tk are typically realized as binary indicator functions indicating the presence or absence of a feature, but in ASR applications it is more typical to utilize real-valued functions, such as those derived from the sufficient statistics of Gaussians (e.g., (Gunawardana et al., 2005)). We can use posterior estimates of phone classes or phonological feature classes from MLPs as feature functions (inputs) within the CRF model. A more detailed description of this CRF paradigm can be found in (Morris and Fosler-Lussier, 2006), which shows that the results of phone recognition using CRFs is comparable to that of HMMs or Tandem systems, with fewer constraints being imposed on the model. State feature functions in our system are defined such that sφ f(yi, x, i) = S 0, otherwise (4) NNf(xi), ifyi = 0 d 1000 hidden nodes. 4 Joint Phone Posteriors vs. Independent Feature Posteriors</context>
</contexts>
<marker>Gunawardana, Mahajan, Acero, Platt, 2005</marker>
<rawString>A. Gunawardana, M. Mahajan, A. Acero, and J. Platt. 2005. Hidden conditional random fields for phone classification. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hasegawa-Johnson</author>
</authors>
<title>Landmark-based speech recognition: Report of the 2004 Johns Hopkins Summer Workshop.</title>
<date>2005</date>
<booktitle>In ICASSP.</booktitle>
<marker>Hasegawa-Johnson, 2005</marker>
<rawString>M. Hasegawa-Johnson et al. 2005. Landmark-based speech recognition: Report of the 2004 Johns Hopkins Summer Workshop. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Johnson</author>
</authors>
<date>2004</date>
<note>ICSI Quicknet software package. http://www.icsi.berkeley.edu/Speech/qn.html.</note>
<contexts>
<context position="6521" citStr="Johnson, 2004" startWordPosition="1005" endWordPosition="1007">ment with different meth3 Experimental Setup We use the TIMIT speech corpus for all training and testing (Garofolo et al., 1993). The acoustic data is manually labeled at the phonetic level, and we propagate this phonetic label information to every frame of data. For the feature analyses, we employ a lookup table that defines each phone in terms of 8 feature classes, as shown in Table 1. We extract acoustic features in the form of 12th order PLP features plus delta coefficients. We then use these as inputs to several sets of neural networks using the ICSI QuickNet MLP neural network software (Johnson, 2004), with the 39 acoustic features as input, a varying number of phone or feature class posteriors as output, an The first experiment contrasts joint versus independent feature modeling within the CRF system. We compare a set of phonological feature probabilities derived from the phone posteriors (a joint model) with MLP phone posteriors and with independently trained MLP phonological feature posteriors. The inputs to the first CRF are sets of 61 state feature functions from the phonemic MLP posteriors, each function is an estimate of the posteri or probaInput Type. Phn. Accuracy Phn. Correct Pho</context>
</contexts>
<marker>Johnson, 2004</marker>
<rawString>D. Johnson. 2004. ICSI Quicknet software package. http://www.icsi.berkeley.edu/Speech/qn.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kirchhoff</author>
</authors>
<title>Robust Speech Recognition Using Articulatory Information.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Bielefeld.</institution>
<contexts>
<context position="1335" citStr="Kirchhoff, 1999" startWordPosition="178" endWordPosition="179"> task. We explore the effects of varying relationships between phonological features, and suggest that in an ASR system, phonological features should be handled as correlated, rather than independent. 1 Introduction Phonological features have received attention as a linguistically-based representation for sub-word information in automatic speech recognition. These sub-phonetic features allow for a more refined representation of speech by allowing for temporal desynchronization between articulators, and help account for some phonological changes common in spontaneous speech, such as devoicing (Kirchhoff, 1999; Livescu, 2005). A number of methods have been developed for detecting acoustic phonological features and related acoustic landmarks directly from data using Multi-Layer Perceptrons (Kirchhoff, 1999), Support Vector Machines (Hasegawa-Johnson et al., 2005; Sharenborg et al., 2006), or Hidden Markov Models (Li and Lee, 2005). These techniques typically assume that acoustic phonological feature events are independent for ease of modeling. 13 In one study that broke the independence assumption (Chang et al., 2001), the investigators developed conditional detectors: MLP detectors of acoustic phon</context>
</contexts>
<marker>Kirchhoff, 1999</marker>
<rawString>K. Kirchhoff. 1999. Robust Speech Recognition Using Articulatory Information. Ph.D. thesis, University of Bielefeld.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="3042" citStr="Lafferty et al., 2001" startWordPosition="425" endWordPosition="428"> for frame-level classification rather than phone recognition. Posterior estimates of phonological feature classes, as in Table 1, particularly those derived from MLPs, have been used as input to HMMs (Launay et al., 2002), Dynamic Bayesian Networks (DBNs) (Frankel et al., 2004; Livescu, 2005), and Conditional Random Fields (CRFs) (Morris and Fosler-Lussier, 2006). Here we evaluate phonological feature detectors created from MLP phone posterior estimators (joint feature models) rather than the independently trained MLP feature detectors used in previous work. 2 Conditional Random Fields CRFs (Lafferty et al., 2001) are a joint model of a label sequence conditioned on a set of inputs. No independence is assumed among the input; the CRF model discriminates between hypothesized label sequences according to an exponential function of weighted feature functions: P(y|x) a exp � (S(x, y, i) + T(x, y, i)) (1) � Proceedings of NAACL HLT 2007, Companion Volume, pages 13–16, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics Class Feature Values SONORITY Vowel, Obstruent, Sonorant, Syllabic, Silence VOICE Voiced, Unvoiced, N/A MANNER Fricative, Stop, Stop-Closure, Flap, Nasal, Approximant,</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Launay</author>
<author>O Siohan</author>
<author>A C Surendran</author>
<author>C-H Lee</author>
</authors>
<title>Towards knowledge based features for large vocabulary automatic speech recognition.</title>
<date>2002</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="2642" citStr="Launay et al., 2002" startWordPosition="370" endWordPosition="373">In (Rajamanohar and Fosler-Lussier, 2005) it was shown that such a conditional training of detectors tended to have correlated frame errors, and that improvements in detection could be obtained by training joint detectors. For many features, the best detector can be obtained by collapsing MLP phone posteriors into feature classes by marginalizing across phones within a class. This was shown only for frame-level classification rather than phone recognition. Posterior estimates of phonological feature classes, as in Table 1, particularly those derived from MLPs, have been used as input to HMMs (Launay et al., 2002), Dynamic Bayesian Networks (DBNs) (Frankel et al., 2004; Livescu, 2005), and Conditional Random Fields (CRFs) (Morris and Fosler-Lussier, 2006). Here we evaluate phonological feature detectors created from MLP phone posterior estimators (joint feature models) rather than the independently trained MLP feature detectors used in previous work. 2 Conditional Random Fields CRFs (Lafferty et al., 2001) are a joint model of a label sequence conditioned on a set of inputs. No independence is assumed among the input; the CRF model discriminates between hypothesized label sequences according to an expo</context>
</contexts>
<marker>Launay, Siohan, Surendran, Lee, 2002</marker>
<rawString>B. Launay, O. Siohan, A. C. Surendran, and C.-H. Lee. 2002. Towards knowledge based features for large vocabulary automatic speech recognition. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Li</author>
<author>C-H Lee</author>
</authors>
<title>On designing and evaluating speech event detectors.</title>
<date>2005</date>
<booktitle>In Interspeech.</booktitle>
<contexts>
<context position="1661" citStr="Li and Lee, 2005" startWordPosition="222" endWordPosition="225">ion in automatic speech recognition. These sub-phonetic features allow for a more refined representation of speech by allowing for temporal desynchronization between articulators, and help account for some phonological changes common in spontaneous speech, such as devoicing (Kirchhoff, 1999; Livescu, 2005). A number of methods have been developed for detecting acoustic phonological features and related acoustic landmarks directly from data using Multi-Layer Perceptrons (Kirchhoff, 1999), Support Vector Machines (Hasegawa-Johnson et al., 2005; Sharenborg et al., 2006), or Hidden Markov Models (Li and Lee, 2005). These techniques typically assume that acoustic phonological feature events are independent for ease of modeling. 13 In one study that broke the independence assumption (Chang et al., 2001), the investigators developed conditional detectors: MLP detectors of acoustic phonological features that are hierarchically dependent on a different phonological class. In (Rajamanohar and Fosler-Lussier, 2005) it was shown that such a conditional training of detectors tended to have correlated frame errors, and that improvements in detection could be obtained by training joint detectors. For many feature</context>
</contexts>
<marker>Li, Lee, 2005</marker>
<rawString>J. Li and C.-H. Lee. 2005. On designing and evaluating speech event detectors. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Livescu</author>
</authors>
<title>Feature-Based Pronunciation Modeling for Automatic Speech Recognition.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<contexts>
<context position="1351" citStr="Livescu, 2005" startWordPosition="180" endWordPosition="181"> the effects of varying relationships between phonological features, and suggest that in an ASR system, phonological features should be handled as correlated, rather than independent. 1 Introduction Phonological features have received attention as a linguistically-based representation for sub-word information in automatic speech recognition. These sub-phonetic features allow for a more refined representation of speech by allowing for temporal desynchronization between articulators, and help account for some phonological changes common in spontaneous speech, such as devoicing (Kirchhoff, 1999; Livescu, 2005). A number of methods have been developed for detecting acoustic phonological features and related acoustic landmarks directly from data using Multi-Layer Perceptrons (Kirchhoff, 1999), Support Vector Machines (Hasegawa-Johnson et al., 2005; Sharenborg et al., 2006), or Hidden Markov Models (Li and Lee, 2005). These techniques typically assume that acoustic phonological feature events are independent for ease of modeling. 13 In one study that broke the independence assumption (Chang et al., 2001), the investigators developed conditional detectors: MLP detectors of acoustic phonological feature</context>
<context position="2714" citStr="Livescu, 2005" startWordPosition="382" endWordPosition="383"> training of detectors tended to have correlated frame errors, and that improvements in detection could be obtained by training joint detectors. For many features, the best detector can be obtained by collapsing MLP phone posteriors into feature classes by marginalizing across phones within a class. This was shown only for frame-level classification rather than phone recognition. Posterior estimates of phonological feature classes, as in Table 1, particularly those derived from MLPs, have been used as input to HMMs (Launay et al., 2002), Dynamic Bayesian Networks (DBNs) (Frankel et al., 2004; Livescu, 2005), and Conditional Random Fields (CRFs) (Morris and Fosler-Lussier, 2006). Here we evaluate phonological feature detectors created from MLP phone posterior estimators (joint feature models) rather than the independently trained MLP feature detectors used in previous work. 2 Conditional Random Fields CRFs (Lafferty et al., 2001) are a joint model of a label sequence conditioned on a set of inputs. No independence is assumed among the input; the CRF model discriminates between hypothesized label sequences according to an exponential function of weighted feature functions: P(y|x) a exp � (S(x, y, </context>
</contexts>
<marker>Livescu, 2005</marker>
<rawString>K. Livescu. 2005. Feature-Based Pronunciation Modeling for Automatic Speech Recognition. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>E Fosler-Lussier</author>
</authors>
<title>Combining phonetic attributes using conditional random fields.</title>
<date>2006</date>
<booktitle>In Interspeech.</booktitle>
<contexts>
<context position="2786" citStr="Morris and Fosler-Lussier, 2006" startWordPosition="389" endWordPosition="392"> errors, and that improvements in detection could be obtained by training joint detectors. For many features, the best detector can be obtained by collapsing MLP phone posteriors into feature classes by marginalizing across phones within a class. This was shown only for frame-level classification rather than phone recognition. Posterior estimates of phonological feature classes, as in Table 1, particularly those derived from MLPs, have been used as input to HMMs (Launay et al., 2002), Dynamic Bayesian Networks (DBNs) (Frankel et al., 2004; Livescu, 2005), and Conditional Random Fields (CRFs) (Morris and Fosler-Lussier, 2006). Here we evaluate phonological feature detectors created from MLP phone posterior estimators (joint feature models) rather than the independently trained MLP feature detectors used in previous work. 2 Conditional Random Fields CRFs (Lafferty et al., 2001) are a joint model of a label sequence conditioned on a set of inputs. No independence is assumed among the input; the CRF model discriminates between hypothesized label sequences according to an exponential function of weighted feature functions: P(y|x) a exp � (S(x, y, i) + T(x, y, i)) (1) � Proceedings of NAACL HLT 2007, Companion Volume, </context>
<context position="5296" citStr="Morris and Fosler-Lussier, 2006" startWordPosition="788" endWordPosition="791">e weights determined by the learning algorithm. In NLP applications, the component feature functions sj and tk are typically realized as binary indicator functions indicating the presence or absence of a feature, but in ASR applications it is more typical to utilize real-valued functions, such as those derived from the sufficient statistics of Gaussians (e.g., (Gunawardana et al., 2005)). We can use posterior estimates of phone classes or phonological feature classes from MLPs as feature functions (inputs) within the CRF model. A more detailed description of this CRF paradigm can be found in (Morris and Fosler-Lussier, 2006), which shows that the results of phone recognition using CRFs is comparable to that of HMMs or Tandem systems, with fewer constraints being imposed on the model. State feature functions in our system are defined such that sφ f(yi, x, i) = S 0, otherwise (4) NNf(xi), ifyi = 0 d 1000 hidden nodes. 4 Joint Phone Posteriors vs. Independent Feature Posteriors ods of generating these feature functions. In vari where the MLP output for feature f at time i is ous NNf(xi). This allows for an association between a phone 0 and a feature f (even if f is traditionally not associated with 0). In this study</context>
</contexts>
<marker>Morris, Fosler-Lussier, 2006</marker>
<rawString>J. Morris and E. Fosler-Lussier. 2006. Combining phonetic attributes using conditional random fields. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rajamanohar</author>
<author>E Fosler-Lussier</author>
</authors>
<title>An evaluation of hierarchical articulatory feature detectors.</title>
<date>2005</date>
<booktitle>In IEEE Automatic Speech Recogntion and Understanding Workshop.</booktitle>
<contexts>
<context position="2063" citStr="Rajamanohar and Fosler-Lussier, 2005" startWordPosition="279" endWordPosition="283">al features and related acoustic landmarks directly from data using Multi-Layer Perceptrons (Kirchhoff, 1999), Support Vector Machines (Hasegawa-Johnson et al., 2005; Sharenborg et al., 2006), or Hidden Markov Models (Li and Lee, 2005). These techniques typically assume that acoustic phonological feature events are independent for ease of modeling. 13 In one study that broke the independence assumption (Chang et al., 2001), the investigators developed conditional detectors: MLP detectors of acoustic phonological features that are hierarchically dependent on a different phonological class. In (Rajamanohar and Fosler-Lussier, 2005) it was shown that such a conditional training of detectors tended to have correlated frame errors, and that improvements in detection could be obtained by training joint detectors. For many features, the best detector can be obtained by collapsing MLP phone posteriors into feature classes by marginalizing across phones within a class. This was shown only for frame-level classification rather than phone recognition. Posterior estimates of phonological feature classes, as in Table 1, particularly those derived from MLPs, have been used as input to HMMs (Launay et al., 2002), Dynamic Bayesian Ne</context>
<context position="8554" citStr="Rajamanohar and Fosler-Lussier, 2005" startWordPosition="1344" endWordPosition="1347">put of the phoneme classifier to derive phonological feature posteriors. In each frame we sum the MLP phone posteriors of all phones that contain a given feature. For instance, in the first frame, for the feature LOW, we sum the posterior estimates attributed to the phones aa, ae and ao. This is repeated for each feature in each frame. The CRF model is trained on these data and tested accordingly. The results are significantly better (p&lt;.001) than the previous features model, but are significantly worse than the phone posteriors (p&lt;.005). The results of Experiment 1 confirm the hypothesis of (Rajamanohar and Fosler-Lussier, 2005) that joint modeling using several types of feature information is superior to individual modeling in phone recognition, where only phoneme information is used. The difference between the phone posteriors and individual feature posteriors seems to be related both to the larger CRF parameter space with larger input, and the joint modeling provided by phone posteriors. 5 Phonological Feature Class Analysis In the second experiment, we examine the influence of each feature class on the accuracy of the recognizer. We iteratively remove the set of state feature functions corresponding to each featu</context>
<context position="11889" citStr="Rajamanohar and Fosler-Lussier, 2005" startWordPosition="1865" endWordPosition="1868"> Voice 62.39 66.53* Manner 61.84 59.89* Place 60.77* 51.94* Height 55.92* 68.69 Frontness 60.80* 68.87 Roundness 62.25 69.13 Tenseness 60.15* 68.76 * p&lt;.05, different from no features removed Table 4: Effect of removing each feature class on recognition accuracy of vowels and consonants thus greatly influences the vowel recognition statistic. Removing the Place detectors leads to a loss in vowel vs. consonant information. This results in an increased number of consonant for vowel substitutions (from 560 to 976), thus a decrease in vowel recognition accuracy. Besides extending the findings in (Rajamanohar and Fosler-Lussier, 2005), this provides a cautionary tale for incorporating redundant phonological feature estimators into ASR: these systems need to be able to handle correlated input, either by design (as in a CRF), through full or semi-tied covariance matrices in HMMs, or by including the appropriate statistical dependencies in DBNs. 6 Summary We have shown the effect of using joint modeling of phonetic feature information in conjunction with the use of CRFs as a discriminative classifier. Phonetic posteriors, as joint models of phonological features, provide superior phone recognition performance over independent</context>
</contexts>
<marker>Rajamanohar, Fosler-Lussier, 2005</marker>
<rawString>M. Rajamanohar and E. Fosler-Lussier. 2005. An evaluation of hierarchical articulatory feature detectors. In IEEE Automatic Speech Recogntion and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Sharenborg</author>
<author>V Wan</author>
<author>R K Moore</author>
</authors>
<title>Capturing fine-phonetic variation in speech through automatic classification of articulatory features.</title>
<date>2006</date>
<booktitle>In ITRW on Speech Recognition and Intrinsic Variation.</booktitle>
<contexts>
<context position="1617" citStr="Sharenborg et al., 2006" startWordPosition="214" endWordPosition="217">stically-based representation for sub-word information in automatic speech recognition. These sub-phonetic features allow for a more refined representation of speech by allowing for temporal desynchronization between articulators, and help account for some phonological changes common in spontaneous speech, such as devoicing (Kirchhoff, 1999; Livescu, 2005). A number of methods have been developed for detecting acoustic phonological features and related acoustic landmarks directly from data using Multi-Layer Perceptrons (Kirchhoff, 1999), Support Vector Machines (Hasegawa-Johnson et al., 2005; Sharenborg et al., 2006), or Hidden Markov Models (Li and Lee, 2005). These techniques typically assume that acoustic phonological feature events are independent for ease of modeling. 13 In one study that broke the independence assumption (Chang et al., 2001), the investigators developed conditional detectors: MLP detectors of acoustic phonological features that are hierarchically dependent on a different phonological class. In (Rajamanohar and Fosler-Lussier, 2005) it was shown that such a conditional training of detectors tended to have correlated frame errors, and that improvements in detection could be obtained b</context>
</contexts>
<marker>Sharenborg, Wan, Moore, 2006</marker>
<rawString>O. Sharenborg, V. Wan, and R.K. Moore. 2006. Capturing fine-phonetic variation in speech through automatic classification of articulatory features. In ITRW on Speech Recognition and Intrinsic Variation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>