<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.956067">
Factorizing Complex Models: A Case Study in Mention
Detection
</title>
<author confidence="0.765375">
Radu Florian, Hongyan Jing, Nanda Kambhatla and Imed Zitouni
</author>
<affiliation confidence="0.548782">
IBM TJ Watson Research Center
</affiliation>
<address confidence="0.703891">
Yorktown Heights, NY 10598
</address>
<email confidence="0.998399">
{raduf,hjing,nanda,izitouni}@us.ibm.com
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999751619047619">
As natural language understanding re-
search advances towards deeper knowledge
modeling, the tasks become more and more
complex: we are interested in more nu-
anced word characteristics, more linguistic
properties, deeper semantic and syntactic
features. One such example, explored in
this article, is the mention detection and
recognition task in the Automatic Content
Extraction project, with the goal of iden-
tifying named, nominal or pronominal ref-
erences to real-world entities—mentions—
and labeling them with three types of in-
formation: entity type, entity subtype and
mention type. In this article, we investi-
gate three methods of assigning these re-
lated tags and compare them on several
data sets. A system based on the methods
presented in this article participated and
ranked very competitively in the ACE&apos;04
evaluation.
</bodyText>
<sectionHeader confidence="0.998511" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956952380953">
Information extraction is a crucial step toward un-
derstanding and processing natural language data,
its goal being to identify and categorize impor-
tant information conveyed in a discourse. Exam-
ples of information extraction tasks are identifi-
cation of the actors and the objects in written
text, the detection and classification of the rela-
tions among them, and the events they participate
in. These tasks have applications in, among other
fields, summarization, information retrieval, data
mining, question answering, and language under-
standing.
One of the basic tasks of information extraction
is the mention detection task. This task is very
similar to named entity recognition (NER), as the
objects of interest represent very similar concepts.
The main difference is that the latter will identify,
however, only named references, while mention de-
tection seeks named, nominal and pronominal ref-
erences. In this paper, we will call the identified
references mentions – using the ACE (NIST, 2003)
nomenclature – to differentiate them from entities
which are the real-world objects (the actual person,
location, etc) to which the mentions are referring
to�.
Historically, the goal of the NER task was to find
named references to entities and quantity refer-
ences – time, money (MUC-6, 1995; MUC-7, 1997).
In recent years, Automatic Content Extraction
evaluation (NIST, 2003; NIST, 2004) expanded the
task to also identify nominal and pronominal refer-
ences, and to group the mentions into sets referring
to the same entity, making the task more compli-
cated, as it requires a co-reference module. The set
of identified properties has also been extended to
include the mention type of a reference (whether it
is named, nominal or pronominal), its subtype (a
more specific type dependent on the main entity
type), and its genericity (whether the entity points
to a specific entity, or a generic one2), besides the
customary main entity type. To our knowledge,
little research has been done in the natural lan-
guage processing context or otherwise on investi-
gating the specific problem of how such multiple la-
bels are best assigned. This article compares three
methods for such an assignment.
The simplest model which can be considered for
the task is to create an atomic tag by “gluing” to-
gether the sub-task labels and considering the new
label atomic. This method transforms the prob-
lem into a regular sequence classification task, sim-
ilar to part-of-speech tagging, text chunking, and
named entity recognition tasks. We call this model
the all-in-one model. The immediate drawback
of this model is that it creates a large classifica-
tion space (the cross-product of the sub-task clas-
sification spaces) and that, during decoding, par-
tially similar classifications will compete instead of
cooperate - more details are presented in Section
3.1. Despite (or maybe due to) its relative sim-
plicity, this model obtained good results in several
instances in the past, for POS tagging in morpho-
logically rich languages (Hajic and Hladka, 1998)
</bodyText>
<footnote confidence="0.952128333333333">
&apos;In a pragmatic sense, entities are sets of mentions
which co-refer.
2This last attribute, genericity, depends only loosely
on local context. As such, it should be assigned while
examining all mentions in an entity, and for this reason
is beyond the scope of this article.
</footnote>
<page confidence="0.978658">
473
</page>
<note confidence="0.539766">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 473–480,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999878045454546">
and mention detection (Jing et al., 2003; Florian
et al., 2004).
At the opposite end of classification methodol-
ogy space, one can use a cascade model, which per-
forms the sub-tasks sequentially in a predefined or-
der. Under such a model, described in Section 3.3,
the user will build separate models for each sub-
task. For instance, it could first identify the men-
tion boundaries, then assign the entity type, sub-
type, and mention level information. Such a model
has the immediate advantage of having smaller
classification spaces, with the drawback that it re-
quires a specific model invocation path.
In between the two extremes, one can use a joint
model, which models the classification space in the
same way as the all-in-one model, but where the
classifications are not atomic. This system incor-
porates information about sub-model parts, such
as whether the current word starts an entity (of
any type), or whether the word is part of a nomi-
nal mention.
The paper presents a novel contrastive analysis
of these three models, comparing them on several
datasets in three languages selected from the ACE
2003 and 2004 evaluations. The methods described
here are independent of the underlying classifiers,
and can be used with any sequence classifiers. All
experiments in this article use our in-house imple-
mentation of a maximum entropy classifier (Flo-
rian et al., 2004), which we selected because of its
flexibility of integrating arbitrary types of features.
While we agree that the particular choice of classi-
fier will undoubtedly introduce some classifier bias,
we want to point out that the described procedures
have more to do with the organization of the search
space, and will have an impact, one way or another,
on most sequence classifiers, including conditional
random field classifiers.3
The paper is organized as follows: Section 2 de-
scribes the multi-task classification problem and
prior work, Section 3.3 presents and contrasts the
three meta-classification models. Section 4 outlines
the experimental setup and the obtained results,
and Section 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.994401" genericHeader="method">
2 Multi-Task Classification
</sectionHeader>
<bodyText confidence="0.998928428571429">
Many tasks in Natural Language Processing in-
volve labeling a word or sequence of words with
a specific property; classic examples are part-of-
speech tagging, text chunking, word sense disam-
biguation and sentiment classification. Most of the
time, the word labels are atomic labels, containing
a very specific piece of information (e.g. the word
</bodyText>
<footnote confidence="0.821381833333333">
3While not wishing to delve too deep into the issue
of label bias, we would also like to point out (as it
was done, for instance, in (Klein, 2003)) that the label
bias of MEMM classifiers can be significantly reduced
by allowing them to examine the right context of the
classification point - as we have done with our model.
</footnote>
<bodyText confidence="0.9981578125">
is noun plural, or starts a noun phrase, etc). There
are cases, though, where the labels consist of sev-
eral related, but not entirely correlated, properties;
examples include mention detection—the task we
are interested in—, syntactic parsing with func-
tional tag assignment (besides identifying the syn-
tactic parse, also label the constituent nodes with
their functional category, as defined in the Penn
Treebank (Marcus et al., 1993)), and, to a lesser
extent, part-of-speech tagging in highly inflected
languages.4
The particular type of mention detection that we
are examining in this paper follows the ACE gen-
eral definition: each mention in the text (a refer-
ence to a real-world entity) is assigned three types
of information:5
</bodyText>
<listItem confidence="0.999779916666667">
• An entity type, describing the type of the en-
tity it points to (e.g. person, location, organi-
zation, etc)
• An entity subtype, further detailing the type
(e.g. organizations can be commercial, gov-
ernmental and non-profit, while locations can
be a nation, population center, or an interna-
tional region)
• A mention type, specifying the way the en-
tity is realized – a mention can be named
(e.g. John Smith), nominal (e.g. professor),
or pronominal (e.g. she).
</listItem>
<bodyText confidence="0.988740333333333">
Such a problem – where the classification consists
of several subtasks or attributes – presents addi-
tional challenges, when compared to a standard
sequence classification task. Specifically, there are
inter-dependencies between the subtasks that need
to be modeled explicitly; predicting the tags inde-
pendently of each other will likely result in incon-
sistent classifications. For instance, in our running
example of mention detection, the subtype task is
dependent on the entity type; one could not have a
person with the subtype non-profit. On the other
hand, the mention type is relatively independent of
the entity type and/or subtype: each entity type
could be realized under any mention type and vice-
versa.
The multi-task classification problem has been
subject to investigation in the past. Caruana
et al. (1997) analyzed the multi-task learning
</bodyText>
<footnote confidence="0.754179076923077">
4The goal there is to also identify word properties
such as gender, number, and case (for nouns), mood
and tense (for verbs), etc, besides the main POS tag.
The task is slightly different, though, as these proper-
ties tend to have a stronger dependency on the lexical
form of the classified word.
5There is a fourth assigned type – a flag specifying
whether a mention is specific (i.e. it refers at a clear
entity), generic (refers to a generic type, e.g. “the sci-
entists believe ..&amp;quot;), unspecified (cannot be determined
from the text), or negative (e.g. “no person would do
this”). The classification of this type is beyond the
goal of this paper.
</footnote>
<page confidence="0.998652">
474
</page>
<bodyText confidence="0.999967824561404">
(MTL) paradigm, where individual related tasks
are trained together by sharing a common rep-
resentation of knowledge, and demonstrated that
this strategy yields better results than one-task-at-
a-time learning strategy. The authors used a back-
propagation neural network, and the paradigm was
tested on several machine learning tasks. It also
contains an excellent discussion on how and why
the MTL paradigm is superior to single-task learn-
ing. Florian and Ngai (2001) used the same multi-
task learning strategy with a transformation-based
learner to show that usually disjointly handled
tasks perform slightly better under a joint model;
the experiments there were run on POS tagging
and text chunking, Chinese word segmentation and
POS tagging. Sutton et al. (2004) investigated
the multitask classification problem and used a dy-
namic conditional random fields method, a gener-
alization of linear-chain conditional random fields,
which can be viewed as a probabilistic generaliza-
tion of cascaded, weighted finite-state transducers.
The subtasks were represented in a single graphi-
cal model that explicitly modeled the sub-task de-
pendence and the uncertainty between them. The
system, evaluated on POS tagging and base-noun
phrase segmentation, improved on the sequential
learning strategy.
In a similar spirit to the approach presented in
this article, Florian (2002) considers the task of
named entity recognition as a two-step process:
the first is the identification of mention boundaries
and the second is the classification of the identified
chunks, therefore considering a label for each word
being formed from two sub-labels: one that spec-
ifies the position of the current word relative in a
mention (outside any mentions, starts a mention, is
inside a mention) and a label specifying the men-
tion type . Experiments on the CoNLL&apos;02 data
show that the two-process model yields consider-
ably higher performance.
Hacioglu et al. (2005) explore the same task, in-
vestigating the performance of the AIO and the
cascade model, and find that the two models have
similar performance, with the AIO model having a
slight advantage. We expand their study by adding
the hybrid joint model to the mix, and further in-
vestigate different scenarios, showing that the cas-
cade model leads to superior performance most of
the time, with a few ties, and show that the cas-
cade model is especially beneficial in cases where
partially-labeled data (only some of the component
labels are given) is available. It turns out though,
(Hacioglu, 2005) that the cascade model in (Ha-
cioglu et al., 2005) did not change to a “mention
view” sequence classifications (as we did in Section
3.3) in the tasks following the entity detection, to
allow the system to use longer range features.
</bodyText>
<page confidence="0.954923">
6 A opposed to a “word view”.
</page>
<sectionHeader confidence="0.986073" genericHeader="method">
3 Classification Models
</sectionHeader>
<bodyText confidence="0.998361444444444">
This section presents the three multi-task classifi-
cation models, which we will experimentally con-
trast in Section 4. We are interested in performing
sequence classification (e.g. assigning a label to
each word in a sentence, otherwise known as tag-
ging). Let X denote the space of sequence elements
(words) and Y denote the space of classifications
(labels), both of them being finite spaces. Our goal
is to build a classifier
</bodyText>
<equation confidence="0.970516">
h : X+ → Y+
</equation>
<bodyText confidence="0.989131833333333">
which has the property that |h (�x) |= |�x |, ∀�x ∈ X+
(i.e. the size of the input sequence is preserved).
This classifier will select the a posteriori most likely
label sequence y� = arg maxi, p (�y&apos;|x); in our case
p (9|x) is computed through the standard Markov
assumption:
</bodyText>
<equation confidence="0.9936985">
�p (y1,-..+. |x) = p (yi|X, yi−n+1,i−1) (1)
i
</equation>
<bodyText confidence="0.999375">
where yi,j denotes the sequence of labels yi..yj.
Furthermore, we will assume that each label y
is composed of a number of sub-labels y =
(y1y2 . . . yk)7; in other words, we will assume the
factorization of the label space into k subspaces
</bodyText>
<equation confidence="0.615875">
Y = Y1 × Y2 × ... × Yk.
</equation>
<bodyText confidence="0.999989777777778">
The classifier we used in the experimental sec-
tion is a maximum entropy classifier (similar to
(McCallum et al., 2000))—which can integrate sev-
eral sources of information in a rigorous manner.
It is our empirical observation that, from a perfor-
mance point of view, being able to use a diverse
and abundant feature set is more important than
classifier choice, and the maximum entropy frame-
work provides such a utility.
</bodyText>
<subsectionHeader confidence="0.997378">
3.1 The All-In-One Model
</subsectionHeader>
<bodyText confidence="0.877189176470588">
As the simplest model among those presented here,
the all-in-one model ignores the natural factoriza-
tion of the output space and considers all labels as
atomic, and then performs regular sequence clas-
sification. One way to look at this process is the
following: the classification space Y = Y1 × Y2 ×
... × Yk is first mapped onto a same-dimensional
space Z through a one-to-one mapping o : Y → Z;
then the features of the system are defined on the
space X+ × Z, instead of X+ × Y.
While having the advantage of being simple, it
suffers from some theoretical disadvantages:
• The classification space can be very large, be-
ing the product of the dimensions of sub-task
spaces. In the case of the 2004 ACE data
there are 7 entity types, 4 mention types and
many subtypes; the observed number of actual
</bodyText>
<footnote confidence="0.680961">
7 W can assume, without any loss of generality, that
all labels have the same number of sub-labels.
</footnote>
<page confidence="0.995227">
475
</page>
<figure confidence="0.631127777777778">
Detect Mention Type
All-In-One Model Joint Model
B-PER B-
B-LOC
B-ORG
B-MISC
Detect Boundaries
&amp; Entity Types
Detect Entity Subtype
</figure>
<tableCaption confidence="0.7720185">
Table 1: Features predicting start of an entity in
the all-in-one and joint models
</tableCaption>
<bodyText confidence="0.999829833333333">
sub-label combinations on the training data is
401. Since the dynamic programing (Viterbi)
search’s runtime dependency on the classifica-
tion space is O (|i|n) (n is the Markov depen-
dency size), using larger spaces will negatively
impact the decoding run time.$
</bodyText>
<listItem confidence="0.994068642857143">
• The probabilities p (zi|�x, zi−n,i−1) require
large data sets to be computed properly. If
the training data is limited, the probabilities
might be poorly estimated.
• The model is not friendly to partial evaluation
or weighted sub-task evaluation: different, but
partially similar, labels will compete against
each other (because the system will return a
probability distribution over the classification
space), sometimes resulting in wrong partial
classification.9
• The model cannot directly use data that is
only partially labeled (i.e. not all sub-labels
are specified).
</listItem>
<bodyText confidence="0.998064181818182">
Despite the above disadvantages, this model has
performed well in practice: Hajic and Hladki(1998) applied it successfully to find POS se-
quences for Czech and Florian et al. (2004) re-
ports good results on the 2003 ACE task. Most
systems that participated in the CoNLL 2002 and
2003 shared tasks on named entity recognition
(Tjong Kim Sang, 2002; Tjong Kim Sang and
De Meulder, 2003) applied this model, as they
modeled the identification of mention boundaries
and the assignment of mention type at the same
time.
</bodyText>
<subsectionHeader confidence="0.992708">
3.2 The Joint Model
</subsectionHeader>
<bodyText confidence="0.898574">
The joint model differs from the all-in-one model
in the fact that the labels are no longer atomic: the
features of the system can inspect the constituent
sub-labels. This change helps alleviate the data
SFrom a practical point of view, it might not be very
important, as the search is pruned in most cases to only
a few hypotheses (beam-search); in our case, pruning
the beam only introduced an insignificant model search
error (0.1 F-measure).
</bodyText>
<footnote confidence="0.9794055">
9To exemplify, consider that the system outputs the
following classifications and probabilities: O (0.2), B-
PER-NAM (0.15), B-PER-NOM (0.15); even the latter
2 suggest that the word is the start of a person mention,
the O label will win because the two labels competed
against each other.
</footnote>
<note confidence="0.658852">
Assemble full tag
</note>
<figureCaption confidence="0.98388">
Figure 1: Cascade flow example for mention detec-
tion.
</figureCaption>
<bodyText confidence="0.988121">
sparsity encountered by the previous model by al-
lowing sub-label modeling. The joint model the-
oretically compares favorably with the all-in-one
model:
</bodyText>
<listItem confidence="0.802081">
• The probabilities p (yi|�x, yi−n,i−1) _
p C`yi , ... , yk)  |x, (ya −n ,i − 1) j =1,k/ might
</listItem>
<bodyText confidence="0.991159071428571">
require less training data to be properly
estimated, as different sub-labels can be
modeled separately.
• The joint model can use features that predict
just one or a subset of the sub-labels. Ta-
ble 1 presents the set of basic features that
predict the start of a mention for the CoNLL
shared tasks for the two models. While the
joint model can encode the start of a mention
in one feature, the all-in-one model needs to
use four features, resulting in fewer counts per
feature and, therefore, yielding less reliably es-
timated features (or, conversely, it needs more
data for the same estimation confidence).
</bodyText>
<listItem confidence="0.7609551">
• The model can predict some of the sub-tags
ahead of the others (i.e. create a dependency
structure on the sub-labels). The model used
in the experimental section predicts the sub-
labels by using only sub-labels for the previous
words, though.
• It is possible, though computationally expen-
sive, for the model to use additional data
that is only partially labeled, with the model
change presented later in Section 3.4.
</listItem>
<subsectionHeader confidence="0.989965">
3.3 The Cascade Model
</subsectionHeader>
<bodyText confidence="0.998905">
For some tasks, there might already exist a natural
hierarchy among the sub-labels: some sub-labels
could benefit from knowing the value of other,
primitive, sub-labels. For example,
</bodyText>
<listItem confidence="0.984546875">
• For mention detection, identifying the men-
tion boundaries can be considered as a primi-
tive task. Then, knowing the mention bound-
aries, one can assign an entity type, subtype,
and mention type to each mention.
• In the case of parsing with functional tags, one
can perform syntactic parsing, then assign the
functional tags to the internal constituents.
</listItem>
<page confidence="0.992128">
476
</page>
<note confidence="0.4405765">
Words Since Donna Karan International went public in 1996 ...
Labels O B-ORG I-ORG I-ORG O O O O ...
</note>
<figureCaption confidence="0.822873">
Figure 2: Sequence tagging for mention detection: the case for a cascade model.
</figureCaption>
<listItem confidence="0.723808">
• For POS tagging, one can detect the main
POS first, then detect the other specific prop-
erties, making use of the fact that one knows
the main tag.
</listItem>
<bodyText confidence="0.999944162162162">
The cascade model is essentially a factorization
of individual classifiers for the sub-tasks; in this
framework, we will assume that there is a more
or less natural dependency structure among sub-
tasks, and that models for each of the subtasks
will be built and applied in the order defined by
the dependency structure. For example, as shown
in Figure 1, one can detect mention boundaries and
entity type (at the same time), then detect mention
type and subtype in “parallel” (i.e. no dependency
exists between these last 2 sub-tags).
A very important advantage of the cascade
model is apparent in classification cases where
identifying chunks is involved (as is the case with
mention detection), similar to advantages that
rescoring hypotheses models have: in the second
stage, the chunk classification stage, it can switch
to a mention view, where the classification units
are entire mentions and words outside of mentions.
This allows the system to make use of aggregate
features over the mention words (e.g. all the words
are capitalized), and to also effectively use a larger
Markov window (instead of 2-3 words, it will use 2-
3 chunks/words around the word of interest). Fig-
ure 2 contains an example of such a case: the cas-
cade model will have to predict the type of the
entire phrase Donna Karan International, in the
context ’Since &lt;chunk&gt; went public in ..&apos;, which
will give it a better opportunity to classify it as an
organization. In contrast, because the joint model
and AIO have a word view of the sentence, will lack
the benefit of examining the larger region, and will
not have access at features that involve partial fu-
ture classifications (such as the fact that another
mention of a particular type follows).
Compared with the other two models, this clas-
sification method has the following advantages:
</bodyText>
<listItem confidence="0.996010857142857">
• The classification spaces for each subtask are
considerably smaller; this fact enables the cre-
ation of better estimated models
• The problem of partially-agreeing competing
labels is completely eliminated
• One can easily use different/additional data to
train any of the sub-task models.
</listItem>
<subsectionHeader confidence="0.983225">
3.4 Adding Partially Labeled Data
</subsectionHeader>
<bodyText confidence="0.999783894736842">
Annotated data can be sometimes expensive to
come by, especially if the label set is complex. But
not all sub-tasks were created equal: some of them
might be easier to predict than others and, there-
fore, require less data to train effectively in a cas-
cade setup. Additionally, in realistic situations,
some sub-tasks might be considered to have more
informational content than others, and have prece-
dence in evaluation. In such a scenario, one might
decide to invest resources in annotating additional
data only for the particularly interesting sub-task,
which could reduce this effort significantly.
To test this hypothesis, we annotated additional
data with the entity type only. The cascade model
can incorporate this data easily: it just adds it
to the training data for the entity type classifier
model. While it is not immediately apparent how
to incorporate this new data into the all-in-one and
joint models, in order to maintain fairness in com-
paring the models, we modified the procedures to
allow for the inclusion. Let T denote the original
training data, and T&apos; denote the additional train-
ing data.
For the all-in-one model, the additional training
data cannot be incorporated directly; this is an in-
herent deficiency of the AIO model. To facilitate a
fair comparison, we will incorporate it in an indi-
rect way: we train a classifier C on the additional
training data T&apos;, which we then use to classify the
original training data T . Then we train the all-
in-one classifier on the original training data T ,
adding the features defined on the output of ap-
plying the classifier C on T .
The situation is better for the joint model: the
new training data T&apos; can be incorporated directly
into the training data T .10 The maximum entropy
model estimates the model parameters by maxi-
mizing the data log-likelihood
</bodyText>
<equation confidence="0.952195">
11 L = P (x, y) log qλ (y|x)
(x,y)
</equation>
<bodyText confidence="0.999729">
where p (x, y) is the observed probability dis-
tribution of the pair (x, y) and qλ (y|x) =
</bodyText>
<equation confidence="0.979002">
�
1 j exp (Aj · fj (x, y)) is the conditional ME
Z
</equation>
<bodyText confidence="0.999757333333333">
probability distribution as computed by the model.
In the case where some of the data is partially an-
notated, the log-likelihood becomes
</bodyText>
<equation confidence="0.938733">
P (x, y) log qλ (y|x)
</equation>
<footnote confidence="0.757549285714286">
10The solution we present here is particular for
MEMM models (though similar solutions may exist for
other models as well). We also assume the reader is fa-
miliar with the normal MaxEnt training procedure; we
present here only the differences to the standard algo-
rithm. See (Manning and Sch�utze, 1999) for a good
description.
</footnote>
<equation confidence="0.9857185">
L= 11
(x,y)ET UT 0
</equation>
<page confidence="0.992622">
477
</page>
<table confidence="0.7454848">
Language Training Test
Arabic 511 178
Chinese 480 166
English 2003 658 139
English 2004 337 114
</table>
<equation confidence="0.961465">
X= p� (x, y) log qλ (y|x)
(x,y)ET
+ X p� (x, y) log qλ (y|x) (2)
(x,y)ET&apos;
</equation>
<bodyText confidence="0.999426125">
The only technical problem that we are faced with
here is that we cannot directly estimate the ob-
served probability p� (x, y) for examples in T, since
they are only partially labeled. Borrowing the
idea from the expectation-maximization algorithm
(Dempster et al., 1977), we can replace this proba-
bility by the re-normalized system proposed prob-
ability: for (x, yx) E T, we define
</bodyText>
<equation confidence="0.99892825">
q� (x, y) = p (x) δ (y E yx) qλ (y|x)
Py&apos;Ey. qλ (y�|x)
 |{z }
=9a(y|x)
</equation>
<bodyText confidence="0.997761933333333">
where yx is the subset of labels from Y which are
consistent with the partial classification of x in T.
δ (y E yx) is 1 if and only if y is consistent with
the partial classification yx.11 The log-likelihood
computation in Equation (2) becomes
To further simplify the evaluation, the quantities
q� (x, y) are recomputed every few steps, and are
considered constant as far as finding the optimum
λ values is concerned (the partial derivative com-
putations and numerical updates otherwise become
quite complicated, and the solution is no longer
unique). Given this new evaluation function, the
training algorithm will proceed exactly the same
way as in the normal case where all the data is
fully labeled.
</bodyText>
<sectionHeader confidence="0.999051" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9996442">
All the experiments in this section are run on the
ACE 2003 and 2004 data sets, in all the three
languages covered: Arabic, Chinese, and English.
Since the evaluation test set is not publicly avail-
able, we have split the publicly available data into
a 80%/20% data split. To facilitate future compar-
isons with work presented here, and to simulate a
realistic scenario, the splits are created based on
article dates: the test data is selected as the last
20% of the data in chronological order. This way,
the documents in the training and test data sets
do not overlap in time, and the ones in the test
data are posterior to the ones in the training data.
Table 2 presents the number of documents in the
training/test datasets for the three languages.
</bodyText>
<footnote confidence="0.8942775">
11For instance, the full label B-PER is consistent
with the partial label B, but not with O or L
</footnote>
<tableCaption confidence="0.984265">
Table 2: Datasets size (number of documents)
</tableCaption>
<bodyText confidence="0.975439">
Each word in the training data is labeled with
one of the following properties:12
</bodyText>
<listItem confidence="0.6817349">
• if it is not part of any entity, it’s labeled as O
• if it is part of an entity, it contains a tag spec-
ifying whether it starts a mention (B-) or is
inside a mention (I-). It is also labeled with
the entity type of the mention (seven possible
types: person, organization, location, facility,
geo-political entity, weapon, and vehicle), the
mention type (named, nominal, pronominal,
or premodifier13), and the entity subtype (de-
pends on the main entity type).
</listItem>
<bodyText confidence="0.999987419354839">
The underlying classifier used to run the experi-
ments in this article is a maximum entropy model
with a Gaussian prior (Chen and Rosenfeld, 1999),
making use of a large range of features, includ-
ing lexical (words and morphs in a 3-word win-
dow, prefixes and suffixes of length up to 4, Word-
Net (Miller, 1995) for English), syntactic (POS
tags, text chunks), gazetteers, and the output of
other information extraction models. These fea-
tures were described in (Florian et al., 2004), and
are not discussed here. All three methods (AIO,
joint, and cascade) instantiate classifiers based on
the same feature types whenever possible. In terms
of language-specific processing, the Arabic system
uses as input morphological segments, while the
Chinese system is a character-based model (the in-
put elements x E X are characters), but it has
access to word segments as features.
Performance in the ACE task is officially eval-
uated using a special-purpose measure, the ACE
value metric (NIST, 2003; NIST, 2004). This
metric assigns a score based on the similarity be-
tween the system’s output and the gold-standard
at both mention and entity level, and assigns dif-
ferent weights to different entity types (e.g. the
person entity weights considerably more than a fa-
cility entity, at least in the 2003 and 2004 evalu-
ations). Since this article focuses on the mention
detection task, we decided to use the more intu-
itive (unweighted) F-measure: the harmonic mean
of precision and recall.
</bodyText>
<footnote confidence="0.989294">
12The mention encoding is the IOB2 encoding pre-
sented in (Tjong Kim Sang and Veenstra, 1999) and
introduced by (Ramshaw and Marcus, 1994) for the
task of base noun phrase chunking.
13This is a special class, used for mentions that mod-
ify other labeled mentions; e.g. French in “French
wine”. This tag is specific only to ACE&apos;04.
</footnote>
<equation confidence="0.9929645">
XL = p� (x, y) log qλ (y|x)
(x,y)ET
+ X q� (x, y) log qλ (y|x)
(x,y)ET&apos;
</equation>
<page confidence="0.993597">
478
</page>
<bodyText confidence="0.9735721">
For the cascade model, the sub-task flow is pre-
sented in Figure 1. In the first step, we identify
the mention boundaries together with their entity
type (e.g. person, organization, etc). In prelimi-
nary experiments, we tried to “cascade” this task.
The performance was similar on both strategies;
the separated model would yield higher recall at
the expense of precision, while the combined model
would have higher precision, but lower recall. We
decided to use in the system with higher precision.
Once the mentions are identified and classified with
the entity type property, the data is passed, in par-
allel, to the mention type detector and the subtype
detector.
For English and Arabic, we spent three person-
weeks to annotate additional data labeled with
only the entity type information: 550k words for
English and 200k words for Arabic. As mentioned
earlier, adding this data to the cascade model is a
trivial task: the data just gets added to the train-
ing data, and the model is retrained. For the AIO
model, we have build another mention classifier on
the additional training data, and labeled the orig-
inal ACE training data with it. It is important
to note here that the ACE training data (called
T in Section 3.4) is consistent with the additional
training data T&apos;: the annotation guidelines for T&apos;
are the same as for the original ACE data, but we
only labeled entity type information. The result-
ing classifications are then used as features in the
final AIO classifier. The joint model uses the addi-
tional partially-labeled data in the way described
in Section 3.4; the probabilities 4 (x, y) are updated
every 5 iterations.
Table 3 presents the results: overall, the cascade
model performs significantly better than the all-
in-one model in four out the six tested cases - the
numbers presented in bold reflect that the differ-
ence in performance to the AIO model is statisti-
cally significant.14 The joint model, while manag-
ing to recover some ground, falls in between the
AIO and the cascade models.
When additional partially-labeled data was
available, the cascade and joint models receive a
statistically significant boost in performance, while
the all-in-one model’s performance barely changes.
This fact can be explained by the fact that the en-
tity type-only model is in itself errorful; measuring
the performance of the model on the training data
yields a performance of 82 F-measure;15 therefore
the AIO model will only access partially-correct
14To assert the statistical significance of the results,
we ran a paired Wilcoxon test over the series obtained
by computing F-measure on each document in the test
set. The results are significant at a level of at least
0.009.
15Since the additional training data is consistent in
the labeling of the entity type, such a comparison is in-
deed possible. The above mentioned score is on entity
types only.
</bodyText>
<table confidence="0.999783142857143">
Language Data+ A-I-O Joint Cascade
Arabic&apos;04 no 59.2 59.1 59.7
yes 59.4 60.0 60.7
English&apos;04 no 72.1 72.3 73.7
yes 72.5 74.1 75.2
Chinese&apos;04 no 71.2 71.7 71.7
English ’03 no 79.5 79.5 79.7
</table>
<tableCaption confidence="0.997888">
Table 3: Experimental results: F-measure on the
full label
</tableCaption>
<table confidence="0.999930285714286">
Language Data+ A-I-O Joint Cascade
Arabic&apos;04 no 66.3 66.5 67.5
yes 66.4 67.9 68.9
English&apos;04 no 77.9 78.1 79.2
yes 78.3 80.5 82.6
Chinese&apos;04 no 75.4 76.1 76.8
English ’03 no 80.4 80.4 81.1
</table>
<tableCaption confidence="0.999376">
Table 4: F-measure results on entity type only
</tableCaption>
<bodyText confidence="0.999958486486487">
data, and is unable to make effective use of it.
In contrast, the training data for the entity type
in the cascade model effectively triples, and this
change is reflected positively in the 1.5 increase in
F-measure.
Not all properties are equally valuable: the en-
tity type is arguably more interesting than the
other properties. If we restrict ourselves to eval-
uating the entity type output only (by projecting
the output label to the entity type only), the differ-
ence in performance between the all-in-one model
and cascade is even more pronounced, as shown in
Table 4. The cascade model outperforms here both
the all-in-one and joint models in all cases except
English&apos;03, where the difference is not statistically
significant.
As far as run-time speed is concerned, the AIO
and cascade models behave similarly: our imple-
mentation tags approximately 500 tokens per sec-
ond (averaged over the three languages, on a Pen-
tium 3, 1.2Ghz, 2Gb of memory). Since a MaxEnt
implementation is mostly dependent on the num-
ber of features that fire on average on a example,
and not on the total number of features, the joint
model runs twice as slow: the average number of
features firing on a particular example is consider-
ably higher. On average, the joint system can tag
approximately 240 words per second. The train
time is also considerably longer; it takes 15 times as
long to train the joint model as it takes to train the
all-in-one model (60 mins/iteration compared to
4 mins/iteration); the cascade model trains faster
than the AIO model.
One last important fact that is worth mention-
ing is that a system based on the cascade model
participated in the ACE&apos;04 competition, yielding
very competitive results in all three languages.
</bodyText>
<page confidence="0.99892">
479
</page>
<sectionHeader confidence="0.994755" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999965939393939">
As natural language processing becomes more so-
phisticated and powerful, we start focus our at-
tention on more and more properties associated
with the objects we are seeking, as they allow for
a deeper and more complex representation of the
real world. With this focus comes the question of
how this goal should be accomplished – either de-
tect all properties at once, one at a time through
a pipeline, or a hybrid model. This paper presents
three methods through which multi-label sequence
classification can be achieved, and evaluates and
contrasts them on the Automatic Content Extrac-
tion task. On the ACE mention detection task,
the cascade model which predicts first the mention
boundaries and entity types, followed by mention
type and entity subtype outperforms the simple all-
in-one model in most cases, and the joint model in
a few cases.
Among the proposed models, the cascade ap-
proach has the definite advantage that it can easily
and productively incorporate additional partially-
labeled data. We also presented a novel modifica-
tion of the joint system training that allows for the
direct incorporation of additional data, which in-
creased the system performance significantly. The
all-in-one model can only incorporate additional
data in an indirect way, resulting in little to no
overall improvement.
Finally, the performance obtained by the cas-
cade model is very competitive: when paired with a
coreference module, it ranked very well in the “En-
tity Detection and Tracking” task in the ACE&apos;04
evaluation.
</bodyText>
<sectionHeader confidence="0.997781" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999836933333334">
R. Caruana, L. Pratt, and S. Thrun. 1997. Multitask
learning. Machine Learning, 28:41.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, Computer Sci-
ence Department, Carnegie Mellon University.
A. P. Dempster, N. M. Laird,, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal statistical Soci-
ety, 39(1):1–38.
R. Florian and G. Ngai. 2001. Multidimensional
transformation-based learning. In Proceedings of
CoNLL&apos;01, pages 1–8.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N Nicolov, and S Roukos.
2004. A statistical model for multilingual entity de-
tection and tracking. In Proceedings of the Human
Language Technology Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004, pages 1–8.
R. Florian. 2002. Named entity recognition as a
house of cards: Classifier stacking. In Proceedings
of CoNLL-2002, pages 175–178.
Kadri Hacioglu, Benjamin Douglas, and Ying Chen.
2005. Detection of entity mentions occuring in en-
glish and chinese text. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 379–386, Vancouver, British Columbia,
Canada, October. Association for Computational
Linguistics.
Kadri Hacioglu. 2005. Private communication.
J. Hajic and Hladka. 1998. Tagging inflective lan-
guages: Prediction of morphological categories for a
rich, structured tagset. In Proceedings of the 36th
Annual Meeting of the ACL and the 17th ICCL,
pages 483–490, Montreal, Canada.
H. Jing, R. Florian, X. Luo, T. Zhang, and A. It-
tycheriah. 2003. HowtogetaChineseName(Entity):
Segmentation and combination issues. In Proceed-
ings of EMNLP&apos;03, pages 200–207.
Dan Klein. 2003. Maxent models, conditional estima-
tion, and optimization, without the magic. Tutorial
presented at NAACL-03 and ACL-03.
C. D. Manning and H. Sch�utze. 1999. Foundations of
Statistical Natural Language Processing. MIT Press.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 19:313–330.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov models
for information extraction and segmentation. In Pro-
ceedings of ICML-2000.
G. A. Miller. 1995. WordNet: A lexical database.
Communications of the ACM, 38(11).
MUC-6. 1995. The sixth mes-
sage understanding conference.
www.cs.nyu.edu/cs/faculty/grishman/muc6.html.
MUC-7. 1997. The seventh mes-
sage understanding conference.
www.itl.nist.gov/iad/894.02/related projects/
muc/proceedings/muc 7 toc.html.
NIST. 2003. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
NIST. 2004. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
L. Ramshaw and M. Marcus. 1994. Exploring the sta-
tistical derivation of transformational rule sequences
for part-of-speech tagging. In Proceedings of the
ACL Workshop on Combining Symbolic and Statis-
tical Approaches to Language, pages 128–135.
C. Sutton, K. Rohanimanesh, and A. McCallum.
2004. Dynamic conditional random fields: Factor-
ized probabilistic models for labeling and segment-
ing sequence data. In In Proceedings of the Twenty-
First International Conference on Machine Learning
(ICML-2004).
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 142–147. Edmonton,
Canada.
E. F. Tjong Kim Sang and J. Veenstra. 1999. Repre-
senting text chunks. In Proceedings of EACL&apos;99.
E. F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independent named en-
tity recognition. In Proceedings of CoNLL-2002,
pages 155–158.
</reference>
<page confidence="0.998228">
480
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.689383">
<title confidence="0.999549">Factorizing Complex Models: A Case Study in Mention Detection</title>
<author confidence="0.999626">Hongyan Jing Florian</author>
<author confidence="0.999626">Nanda Kambhatla Zitouni</author>
<affiliation confidence="0.999987">IBM TJ Watson Research Center</affiliation>
<address confidence="0.986904">Yorktown Heights, NY 10598</address>
<abstract confidence="0.984469818181818">As natural language understanding research advances towards deeper knowledge modeling, the tasks become more and more complex: we are interested in more nuanced word characteristics, more linguistic properties, deeper semantic and syntactic features. One such example, explored in this article, is the mention detection and recognition task in the Automatic Content Extraction project, with the goal of identifying named, nominal or pronominal references to real-world entities—mentions— and labeling them with three types of information: entity type, entity subtype and mention type. In this article, we investigate three methods of assigning these related tags and compare them on several data sets. A system based on the methods presented in this article participated and ranked very competitively in the ACE&apos;04 evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Caruana</author>
<author>L Pratt</author>
<author>S Thrun</author>
</authors>
<date>1997</date>
<booktitle>Multitask learning. Machine Learning,</booktitle>
<pages>28--41</pages>
<contexts>
<context position="9367" citStr="Caruana et al. (1997)" startWordPosition="1480" endWordPosition="1483">re inter-dependencies between the subtasks that need to be modeled explicitly; predicting the tags independently of each other will likely result in inconsistent classifications. For instance, in our running example of mention detection, the subtype task is dependent on the entity type; one could not have a person with the subtype non-profit. On the other hand, the mention type is relatively independent of the entity type and/or subtype: each entity type could be realized under any mention type and viceversa. The multi-task classification problem has been subject to investigation in the past. Caruana et al. (1997) analyzed the multi-task learning 4The goal there is to also identify word properties such as gender, number, and case (for nouns), mood and tense (for verbs), etc, besides the main POS tag. The task is slightly different, though, as these properties tend to have a stronger dependency on the lexical form of the classified word. 5There is a fourth assigned type – a flag specifying whether a mention is specific (i.e. it refers at a clear entity), generic (refers to a generic type, e.g. “the scientists believe ..&amp;quot;), unspecified (cannot be determined from the text), or negative (e.g. “no person wo</context>
</contexts>
<marker>Caruana, Pratt, Thrun, 1997</marker>
<rawString>R. Caruana, L. Pratt, and S. Thrun. 1997. Multitask learning. Machine Learning, 28:41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMU-CS-99-108,</tech>
<institution>Computer Science Department, Carnegie Mellon University.</institution>
<contexts>
<context position="27361" citStr="Chen and Rosenfeld, 1999" startWordPosition="4539" endWordPosition="4542">wing properties:12 • if it is not part of any entity, it’s labeled as O • if it is part of an entity, it contains a tag specifying whether it starts a mention (B-) or is inside a mention (I-). It is also labeled with the entity type of the mention (seven possible types: person, organization, location, facility, geo-political entity, weapon, and vehicle), the mention type (named, nominal, pronominal, or premodifier13), and the entity subtype (depends on the main entity type). The underlying classifier used to run the experiments in this article is a maximum entropy model with a Gaussian prior (Chen and Rosenfeld, 1999), making use of a large range of features, including lexical (words and morphs in a 3-word window, prefixes and suffixes of length up to 4, WordNet (Miller, 1995) for English), syntactic (POS tags, text chunks), gazetteers, and the output of other information extraction models. These features were described in (Florian et al., 2004), and are not discussed here. All three methods (AIO, joint, and cascade) instantiate classifiers based on the same feature types whenever possible. In terms of language-specific processing, the Arabic system uses as input morphological segments, while the Chinese s</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical Report CMU-CS-99-108, Computer Science Department, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal statistical Society,</journal>
<volume>39</volume>
<issue>1</issue>
<marker>Dempster, Laird, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird,, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal statistical Society, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>G Ngai</author>
</authors>
<title>Multidimensional transformation-based learning.</title>
<date>2001</date>
<booktitle>In Proceedings of CoNLL&apos;01,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="10516" citStr="Florian and Ngai (2001)" startWordPosition="1669" endWordPosition="1672">fied (cannot be determined from the text), or negative (e.g. “no person would do this”). The classification of this type is beyond the goal of this paper. 474 (MTL) paradigm, where individual related tasks are trained together by sharing a common representation of knowledge, and demonstrated that this strategy yields better results than one-task-ata-time learning strategy. The authors used a backpropagation neural network, and the paradigm was tested on several machine learning tasks. It also contains an excellent discussion on how and why the MTL paradigm is superior to single-task learning. Florian and Ngai (2001) used the same multitask learning strategy with a transformation-based learner to show that usually disjointly handled tasks perform slightly better under a joint model; the experiments there were run on POS tagging and text chunking, Chinese word segmentation and POS tagging. Sutton et al. (2004) investigated the multitask classification problem and used a dynamic conditional random fields method, a generalization of linear-chain conditional random fields, which can be viewed as a probabilistic generalization of cascaded, weighted finite-state transducers. The subtasks were represented in a s</context>
</contexts>
<marker>Florian, Ngai, 2001</marker>
<rawString>R. Florian and G. Ngai. 2001. Multidimensional transformation-based learning. In Proceedings of CoNLL&apos;01, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Hassan</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>X Luo</author>
<author>N Nicolov</author>
<author>S Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL</booktitle>
<pages>1--8</pages>
<contexts>
<context position="4639" citStr="Florian et al., 2004" startWordPosition="718" endWordPosition="721">he past, for POS tagging in morphologically rich languages (Hajic and Hladka, 1998) &apos;In a pragmatic sense, entities are sets of mentions which co-refer. 2This last attribute, genericity, depends only loosely on local context. As such, it should be assigned while examining all mentions in an entity, and for this reason is beyond the scope of this article. 473 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 473–480, Sydney, July 2006. c�2006 Association for Computational Linguistics and mention detection (Jing et al., 2003; Florian et al., 2004). At the opposite end of classification methodology space, one can use a cascade model, which performs the sub-tasks sequentially in a predefined order. Under such a model, described in Section 3.3, the user will build separate models for each subtask. For instance, it could first identify the mention boundaries, then assign the entity type, subtype, and mention level information. Such a model has the immediate advantage of having smaller classification spaces, with the drawback that it requires a specific model invocation path. In between the two extremes, one can use a joint model, which mod</context>
<context position="5943" citStr="Florian et al., 2004" startWordPosition="933" endWordPosition="937"> classifications are not atomic. This system incorporates information about sub-model parts, such as whether the current word starts an entity (of any type), or whether the word is part of a nominal mention. The paper presents a novel contrastive analysis of these three models, comparing them on several datasets in three languages selected from the ACE 2003 and 2004 evaluations. The methods described here are independent of the underlying classifiers, and can be used with any sequence classifiers. All experiments in this article use our in-house implementation of a maximum entropy classifier (Florian et al., 2004), which we selected because of its flexibility of integrating arbitrary types of features. While we agree that the particular choice of classifier will undoubtedly introduce some classifier bias, we want to point out that the described procedures have more to do with the organization of the search space, and will have an impact, one way or another, on most sequence classifiers, including conditional random field classifiers.3 The paper is organized as follows: Section 2 describes the multi-task classification problem and prior work, Section 3.3 presents and contrasts the three meta-classificat</context>
<context position="16468" citStr="Florian et al. (2004)" startWordPosition="2663" endWordPosition="2666">ies might be poorly estimated. • The model is not friendly to partial evaluation or weighted sub-task evaluation: different, but partially similar, labels will compete against each other (because the system will return a probability distribution over the classification space), sometimes resulting in wrong partial classification.9 • The model cannot directly use data that is only partially labeled (i.e. not all sub-labels are specified). Despite the above disadvantages, this model has performed well in practice: Hajic and Hladki(1998) applied it successfully to find POS sequences for Czech and Florian et al. (2004) reports good results on the 2003 ACE task. Most systems that participated in the CoNLL 2002 and 2003 shared tasks on named entity recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) applied this model, as they modeled the identification of mention boundaries and the assignment of mention type at the same time. 3.2 The Joint Model The joint model differs from the all-in-one model in the fact that the labels are no longer atomic: the features of the system can inspect the constituent sub-labels. This change helps alleviate the data SFrom a practical point of view, it might n</context>
<context position="27695" citStr="Florian et al., 2004" startWordPosition="4596" endWordPosition="4599">pon, and vehicle), the mention type (named, nominal, pronominal, or premodifier13), and the entity subtype (depends on the main entity type). The underlying classifier used to run the experiments in this article is a maximum entropy model with a Gaussian prior (Chen and Rosenfeld, 1999), making use of a large range of features, including lexical (words and morphs in a 3-word window, prefixes and suffixes of length up to 4, WordNet (Miller, 1995) for English), syntactic (POS tags, text chunks), gazetteers, and the output of other information extraction models. These features were described in (Florian et al., 2004), and are not discussed here. All three methods (AIO, joint, and cascade) instantiate classifiers based on the same feature types whenever possible. In terms of language-specific processing, the Arabic system uses as input morphological segments, while the Chinese system is a character-based model (the input elements x E X are characters), but it has access to word segments as features. Performance in the ACE task is officially evaluated using a special-purpose measure, the ACE value metric (NIST, 2003; NIST, 2004). This metric assigns a score based on the similarity between the system’s outpu</context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N Nicolov, and S Roukos. 2004. A statistical model for multilingual entity detection and tracking. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
</authors>
<title>Named entity recognition as a house of cards: Classifier stacking.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>175--178</pages>
<contexts>
<context position="11415" citStr="Florian (2002)" startWordPosition="1804" endWordPosition="1805">et al. (2004) investigated the multitask classification problem and used a dynamic conditional random fields method, a generalization of linear-chain conditional random fields, which can be viewed as a probabilistic generalization of cascaded, weighted finite-state transducers. The subtasks were represented in a single graphical model that explicitly modeled the sub-task dependence and the uncertainty between them. The system, evaluated on POS tagging and base-noun phrase segmentation, improved on the sequential learning strategy. In a similar spirit to the approach presented in this article, Florian (2002) considers the task of named entity recognition as a two-step process: the first is the identification of mention boundaries and the second is the classification of the identified chunks, therefore considering a label for each word being formed from two sub-labels: one that specifies the position of the current word relative in a mention (outside any mentions, starts a mention, is inside a mention) and a label specifying the mention type . Experiments on the CoNLL&apos;02 data show that the two-process model yields considerably higher performance. Hacioglu et al. (2005) explore the same task, inves</context>
</contexts>
<marker>Florian, 2002</marker>
<rawString>R. Florian. 2002. Named entity recognition as a house of cards: Classifier stacking. In Proceedings of CoNLL-2002, pages 175–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
<author>Benjamin Douglas</author>
<author>Ying Chen</author>
</authors>
<title>Detection of entity mentions occuring in english and chinese text.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>379--386</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="11986" citStr="Hacioglu et al. (2005)" startWordPosition="1895" endWordPosition="1898"> approach presented in this article, Florian (2002) considers the task of named entity recognition as a two-step process: the first is the identification of mention boundaries and the second is the classification of the identified chunks, therefore considering a label for each word being formed from two sub-labels: one that specifies the position of the current word relative in a mention (outside any mentions, starts a mention, is inside a mention) and a label specifying the mention type . Experiments on the CoNLL&apos;02 data show that the two-process model yields considerably higher performance. Hacioglu et al. (2005) explore the same task, investigating the performance of the AIO and the cascade model, and find that the two models have similar performance, with the AIO model having a slight advantage. We expand their study by adding the hybrid joint model to the mix, and further investigate different scenarios, showing that the cascade model leads to superior performance most of the time, with a few ties, and show that the cascade model is especially beneficial in cases where partially-labeled data (only some of the component labels are given) is available. It turns out though, (Hacioglu, 2005) that the c</context>
</contexts>
<marker>Hacioglu, Douglas, Chen, 2005</marker>
<rawString>Kadri Hacioglu, Benjamin Douglas, and Ying Chen. 2005. Detection of entity mentions occuring in english and chinese text. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 379–386, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
</authors>
<date>2005</date>
<note>Private communication.</note>
<contexts>
<context position="12575" citStr="Hacioglu, 2005" startWordPosition="1997" endWordPosition="1998">ce. Hacioglu et al. (2005) explore the same task, investigating the performance of the AIO and the cascade model, and find that the two models have similar performance, with the AIO model having a slight advantage. We expand their study by adding the hybrid joint model to the mix, and further investigate different scenarios, showing that the cascade model leads to superior performance most of the time, with a few ties, and show that the cascade model is especially beneficial in cases where partially-labeled data (only some of the component labels are given) is available. It turns out though, (Hacioglu, 2005) that the cascade model in (Hacioglu et al., 2005) did not change to a “mention view” sequence classifications (as we did in Section 3.3) in the tasks following the entity detection, to allow the system to use longer range features. 6 A opposed to a “word view”. 3 Classification Models This section presents the three multi-task classification models, which we will experimentally contrast in Section 4. We are interested in performing sequence classification (e.g. assigning a label to each word in a sentence, otherwise known as tagging). Let X denote the space of sequence elements (words) and Y </context>
</contexts>
<marker>Hacioglu, 2005</marker>
<rawString>Kadri Hacioglu. 2005. Private communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajic</author>
<author>Hladka</author>
</authors>
<title>Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the ACL and the 17th ICCL,</booktitle>
<pages>483--490</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="4101" citStr="Hajic and Hladka, 1998" startWordPosition="635" endWordPosition="638">classification task, similar to part-of-speech tagging, text chunking, and named entity recognition tasks. We call this model the all-in-one model. The immediate drawback of this model is that it creates a large classification space (the cross-product of the sub-task classification spaces) and that, during decoding, partially similar classifications will compete instead of cooperate - more details are presented in Section 3.1. Despite (or maybe due to) its relative simplicity, this model obtained good results in several instances in the past, for POS tagging in morphologically rich languages (Hajic and Hladka, 1998) &apos;In a pragmatic sense, entities are sets of mentions which co-refer. 2This last attribute, genericity, depends only loosely on local context. As such, it should be assigned while examining all mentions in an entity, and for this reason is beyond the scope of this article. 473 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 473–480, Sydney, July 2006. c�2006 Association for Computational Linguistics and mention detection (Jing et al., 2003; Florian et al., 2004). At the opposite end of classification methodology space, one</context>
</contexts>
<marker>Hajic, Hladka, 1998</marker>
<rawString>J. Hajic and Hladka. 1998. Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset. In Proceedings of the 36th Annual Meeting of the ACL and the 17th ICCL, pages 483–490, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>R Florian</author>
<author>X Luo</author>
<author>T Zhang</author>
<author>A Ittycheriah</author>
</authors>
<title>HowtogetaChineseName(Entity): Segmentation and combination issues.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP&apos;03,</booktitle>
<pages>200--207</pages>
<contexts>
<context position="4616" citStr="Jing et al., 2003" startWordPosition="714" endWordPosition="717">eral instances in the past, for POS tagging in morphologically rich languages (Hajic and Hladka, 1998) &apos;In a pragmatic sense, entities are sets of mentions which co-refer. 2This last attribute, genericity, depends only loosely on local context. As such, it should be assigned while examining all mentions in an entity, and for this reason is beyond the scope of this article. 473 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 473–480, Sydney, July 2006. c�2006 Association for Computational Linguistics and mention detection (Jing et al., 2003; Florian et al., 2004). At the opposite end of classification methodology space, one can use a cascade model, which performs the sub-tasks sequentially in a predefined order. Under such a model, described in Section 3.3, the user will build separate models for each subtask. For instance, it could first identify the mention boundaries, then assign the entity type, subtype, and mention level information. Such a model has the immediate advantage of having smaller classification spaces, with the drawback that it requires a specific model invocation path. In between the two extremes, one can use a</context>
</contexts>
<marker>Jing, Florian, Luo, Zhang, Ittycheriah, 2003</marker>
<rawString>H. Jing, R. Florian, X. Luo, T. Zhang, and A. Ittycheriah. 2003. HowtogetaChineseName(Entity): Segmentation and combination issues. In Proceedings of EMNLP&apos;03, pages 200–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
</authors>
<title>Maxent models, conditional estimation, and optimization, without the magic. Tutorial presented at NAACL-03 and ACL-03.</title>
<date>2003</date>
<contexts>
<context position="7176" citStr="Klein, 2003" startWordPosition="1131" endWordPosition="1132"> outlines the experimental setup and the obtained results, and Section 5 concludes the paper. 2 Multi-Task Classification Many tasks in Natural Language Processing involve labeling a word or sequence of words with a specific property; classic examples are part-ofspeech tagging, text chunking, word sense disambiguation and sentiment classification. Most of the time, the word labels are atomic labels, containing a very specific piece of information (e.g. the word 3While not wishing to delve too deep into the issue of label bias, we would also like to point out (as it was done, for instance, in (Klein, 2003)) that the label bias of MEMM classifiers can be significantly reduced by allowing them to examine the right context of the classification point - as we have done with our model. is noun plural, or starts a noun phrase, etc). There are cases, though, where the labels consist of several related, but not entirely correlated, properties; examples include mention detection—the task we are interested in—, syntactic parsing with functional tag assignment (besides identifying the syntactic parse, also label the constituent nodes with their functional category, as defined in the Penn Treebank (Marcus </context>
</contexts>
<marker>Klein, 2003</marker>
<rawString>Dan Klein. 2003. Maxent models, conditional estimation, and optimization, without the magic. Tutorial presented at NAACL-03 and ACL-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch�utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch�utze, 1999</marker>
<rawString>C. D. Manning and H. Sch�utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="7789" citStr="Marcus et al., 1993" startWordPosition="1227" endWordPosition="1230">, 2003)) that the label bias of MEMM classifiers can be significantly reduced by allowing them to examine the right context of the classification point - as we have done with our model. is noun plural, or starts a noun phrase, etc). There are cases, though, where the labels consist of several related, but not entirely correlated, properties; examples include mention detection—the task we are interested in—, syntactic parsing with functional tag assignment (besides identifying the syntactic parse, also label the constituent nodes with their functional category, as defined in the Penn Treebank (Marcus et al., 1993)), and, to a lesser extent, part-of-speech tagging in highly inflected languages.4 The particular type of mention detection that we are examining in this paper follows the ACE general definition: each mention in the text (a reference to a real-world entity) is assigned three types of information:5 • An entity type, describing the type of the entity it points to (e.g. person, location, organization, etc) • An entity subtype, further detailing the type (e.g. organizations can be commercial, governmental and non-profit, while locations can be a nation, population center, or an international regio</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Maximum entropy markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML-2000.</booktitle>
<contexts>
<context position="14007" citStr="McCallum et al., 2000" startWordPosition="2255" endWordPosition="2258">equence is preserved). This classifier will select the a posteriori most likely label sequence y� = arg maxi, p (�y&apos;|x); in our case p (9|x) is computed through the standard Markov assumption: �p (y1,-..+. |x) = p (yi|X, yi−n+1,i−1) (1) i where yi,j denotes the sequence of labels yi..yj. Furthermore, we will assume that each label y is composed of a number of sub-labels y = (y1y2 . . . yk)7; in other words, we will assume the factorization of the label space into k subspaces Y = Y1 × Y2 × ... × Yk. The classifier we used in the experimental section is a maximum entropy classifier (similar to (McCallum et al., 2000))—which can integrate several sources of information in a rigorous manner. It is our empirical observation that, from a performance point of view, being able to use a diverse and abundant feature set is more important than classifier choice, and the maximum entropy framework provides such a utility. 3.1 The All-In-One Model As the simplest model among those presented here, the all-in-one model ignores the natural factorization of the output space and considers all labels as atomic, and then performs regular sequence classification. One way to look at this process is the following: the classifi</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew McCallum, Dayne Freitag, and Fernando Pereira. 2000. Maximum entropy markov models for information extraction and segmentation. In Proceedings of ICML-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: A lexical database.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>6</pages>
<note>www.cs.nyu.edu/cs/faculty/grishman/muc6.html.</note>
<contexts>
<context position="27523" citStr="Miller, 1995" startWordPosition="4572" endWordPosition="4573">de a mention (I-). It is also labeled with the entity type of the mention (seven possible types: person, organization, location, facility, geo-political entity, weapon, and vehicle), the mention type (named, nominal, pronominal, or premodifier13), and the entity subtype (depends on the main entity type). The underlying classifier used to run the experiments in this article is a maximum entropy model with a Gaussian prior (Chen and Rosenfeld, 1999), making use of a large range of features, including lexical (words and morphs in a 3-word window, prefixes and suffixes of length up to 4, WordNet (Miller, 1995) for English), syntactic (POS tags, text chunks), gazetteers, and the output of other information extraction models. These features were described in (Florian et al., 2004), and are not discussed here. All three methods (AIO, joint, and cascade) instantiate classifiers based on the same feature types whenever possible. In terms of language-specific processing, the Arabic system uses as input morphological segments, while the Chinese system is a character-based model (the input elements x E X are characters), but it has access to word segments as features. Performance in the ACE task is officia</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G. A. Miller. 1995. WordNet: A lexical database. Communications of the ACM, 38(11). MUC-6. 1995. The sixth message understanding conference. www.cs.nyu.edu/cs/faculty/grishman/muc6.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-7</author>
</authors>
<title>The seventh message understanding conference.</title>
<date>1997</date>
<note>www.itl.nist.gov/iad/894.02/related projects/ muc/proceedings/muc 7 toc.html.</note>
<marker>MUC-7, 1997</marker>
<rawString>MUC-7. 1997. The seventh message understanding conference. www.itl.nist.gov/iad/894.02/related projects/ muc/proceedings/muc 7 toc.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The ACE evaluation plan.</title>
<date>2003</date>
<note>www.nist.gov/speech/tests/ace/index.htm.</note>
<contexts>
<context position="2068" citStr="NIST, 2003" startWordPosition="308" endWordPosition="309">ate in. These tasks have applications in, among other fields, summarization, information retrieval, data mining, question answering, and language understanding. One of the basic tasks of information extraction is the mention detection task. This task is very similar to named entity recognition (NER), as the objects of interest represent very similar concepts. The main difference is that the latter will identify, however, only named references, while mention detection seeks named, nominal and pronominal references. In this paper, we will call the identified references mentions – using the ACE (NIST, 2003) nomenclature – to differentiate them from entities which are the real-world objects (the actual person, location, etc) to which the mentions are referring to�. Historically, the goal of the NER task was to find named references to entities and quantity references – time, money (MUC-6, 1995; MUC-7, 1997). In recent years, Automatic Content Extraction evaluation (NIST, 2003; NIST, 2004) expanded the task to also identify nominal and pronominal references, and to group the mentions into sets referring to the same entity, making the task more complicated, as it requires a co-reference module. The</context>
<context position="28202" citStr="NIST, 2003" startWordPosition="4678" endWordPosition="4679"> output of other information extraction models. These features were described in (Florian et al., 2004), and are not discussed here. All three methods (AIO, joint, and cascade) instantiate classifiers based on the same feature types whenever possible. In terms of language-specific processing, the Arabic system uses as input morphological segments, while the Chinese system is a character-based model (the input elements x E X are characters), but it has access to word segments as features. Performance in the ACE task is officially evaluated using a special-purpose measure, the ACE value metric (NIST, 2003; NIST, 2004). This metric assigns a score based on the similarity between the system’s output and the gold-standard at both mention and entity level, and assigns different weights to different entity types (e.g. the person entity weights considerably more than a facility entity, at least in the 2003 and 2004 evaluations). Since this article focuses on the mention detection task, we decided to use the more intuitive (unweighted) F-measure: the harmonic mean of precision and recall. 12The mention encoding is the IOB2 encoding presented in (Tjong Kim Sang and Veenstra, 1999) and introduced by (R</context>
</contexts>
<marker>NIST, 2003</marker>
<rawString>NIST. 2003. The ACE evaluation plan. www.nist.gov/speech/tests/ace/index.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The ACE evaluation plan.</title>
<date>2004</date>
<note>www.nist.gov/speech/tests/ace/index.htm.</note>
<contexts>
<context position="2456" citStr="NIST, 2004" startWordPosition="368" endWordPosition="369">t the latter will identify, however, only named references, while mention detection seeks named, nominal and pronominal references. In this paper, we will call the identified references mentions – using the ACE (NIST, 2003) nomenclature – to differentiate them from entities which are the real-world objects (the actual person, location, etc) to which the mentions are referring to�. Historically, the goal of the NER task was to find named references to entities and quantity references – time, money (MUC-6, 1995; MUC-7, 1997). In recent years, Automatic Content Extraction evaluation (NIST, 2003; NIST, 2004) expanded the task to also identify nominal and pronominal references, and to group the mentions into sets referring to the same entity, making the task more complicated, as it requires a co-reference module. The set of identified properties has also been extended to include the mention type of a reference (whether it is named, nominal or pronominal), its subtype (a more specific type dependent on the main entity type), and its genericity (whether the entity points to a specific entity, or a generic one2), besides the customary main entity type. To our knowledge, little research has been done </context>
<context position="28215" citStr="NIST, 2004" startWordPosition="4680" endWordPosition="4681">ther information extraction models. These features were described in (Florian et al., 2004), and are not discussed here. All three methods (AIO, joint, and cascade) instantiate classifiers based on the same feature types whenever possible. In terms of language-specific processing, the Arabic system uses as input morphological segments, while the Chinese system is a character-based model (the input elements x E X are characters), but it has access to word segments as features. Performance in the ACE task is officially evaluated using a special-purpose measure, the ACE value metric (NIST, 2003; NIST, 2004). This metric assigns a score based on the similarity between the system’s output and the gold-standard at both mention and entity level, and assigns different weights to different entity types (e.g. the person entity weights considerably more than a facility entity, at least in the 2003 and 2004 evaluations). Since this article focuses on the mention detection task, we decided to use the more intuitive (unweighted) F-measure: the harmonic mean of precision and recall. 12The mention encoding is the IOB2 encoding presented in (Tjong Kim Sang and Veenstra, 1999) and introduced by (Ramshaw and Ma</context>
</contexts>
<marker>NIST, 2004</marker>
<rawString>NIST. 2004. The ACE evaluation plan. www.nist.gov/speech/tests/ace/index.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Exploring the statistical derivation of transformational rule sequences for part-of-speech tagging.</title>
<date>1994</date>
<booktitle>In Proceedings of the ACL Workshop on Combining Symbolic and Statistical Approaches to Language,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="28826" citStr="Ramshaw and Marcus, 1994" startWordPosition="4780" endWordPosition="4783">3; NIST, 2004). This metric assigns a score based on the similarity between the system’s output and the gold-standard at both mention and entity level, and assigns different weights to different entity types (e.g. the person entity weights considerably more than a facility entity, at least in the 2003 and 2004 evaluations). Since this article focuses on the mention detection task, we decided to use the more intuitive (unweighted) F-measure: the harmonic mean of precision and recall. 12The mention encoding is the IOB2 encoding presented in (Tjong Kim Sang and Veenstra, 1999) and introduced by (Ramshaw and Marcus, 1994) for the task of base noun phrase chunking. 13This is a special class, used for mentions that modify other labeled mentions; e.g. French in “French wine”. This tag is specific only to ACE&apos;04. XL = p� (x, y) log qλ (y|x) (x,y)ET + X q� (x, y) log qλ (y|x) (x,y)ET&apos; 478 For the cascade model, the sub-task flow is presented in Figure 1. In the first step, we identify the mention boundaries together with their entity type (e.g. person, organization, etc). In preliminary experiments, we tried to “cascade” this task. The performance was similar on both strategies; the separated model would yield high</context>
</contexts>
<marker>Ramshaw, Marcus, 1994</marker>
<rawString>L. Ramshaw and M. Marcus. 1994. Exploring the statistical derivation of transformational rule sequences for part-of-speech tagging. In Proceedings of the ACL Workshop on Combining Symbolic and Statistical Approaches to Language, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>K Rohanimanesh</author>
<author>A McCallum</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. In</title>
<date>2004</date>
<booktitle>In Proceedings of the TwentyFirst International Conference on Machine Learning (ICML-2004).</booktitle>
<contexts>
<context position="10814" citStr="Sutton et al. (2004)" startWordPosition="1715" endWordPosition="1718">s strategy yields better results than one-task-ata-time learning strategy. The authors used a backpropagation neural network, and the paradigm was tested on several machine learning tasks. It also contains an excellent discussion on how and why the MTL paradigm is superior to single-task learning. Florian and Ngai (2001) used the same multitask learning strategy with a transformation-based learner to show that usually disjointly handled tasks perform slightly better under a joint model; the experiments there were run on POS tagging and text chunking, Chinese word segmentation and POS tagging. Sutton et al. (2004) investigated the multitask classification problem and used a dynamic conditional random fields method, a generalization of linear-chain conditional random fields, which can be viewed as a probabilistic generalization of cascaded, weighted finite-state transducers. The subtasks were represented in a single graphical model that explicitly modeled the sub-task dependence and the uncertainty between them. The system, evaluated on POS tagging and base-noun phrase segmentation, improved on the sequential learning strategy. In a similar spirit to the approach presented in this article, Florian (2002</context>
</contexts>
<marker>Sutton, Rohanimanesh, McCallum, 2004</marker>
<rawString>C. Sutton, K. Rohanimanesh, and A. McCallum. 2004. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. In In Proceedings of the TwentyFirst International Conference on Machine Learning (ICML-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Walter Daelemans</booktitle>
<pages>142--147</pages>
<editor>and Miles Osborne, editors,</editor>
<location>Edmonton, Canada.</location>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL-2003, pages 142–147. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>J Veenstra</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL&apos;99.</booktitle>
<contexts>
<context position="28781" citStr="Sang and Veenstra, 1999" startWordPosition="4773" endWordPosition="4776">ose measure, the ACE value metric (NIST, 2003; NIST, 2004). This metric assigns a score based on the similarity between the system’s output and the gold-standard at both mention and entity level, and assigns different weights to different entity types (e.g. the person entity weights considerably more than a facility entity, at least in the 2003 and 2004 evaluations). Since this article focuses on the mention detection task, we decided to use the more intuitive (unweighted) F-measure: the harmonic mean of precision and recall. 12The mention encoding is the IOB2 encoding presented in (Tjong Kim Sang and Veenstra, 1999) and introduced by (Ramshaw and Marcus, 1994) for the task of base noun phrase chunking. 13This is a special class, used for mentions that modify other labeled mentions; e.g. French in “French wine”. This tag is specific only to ACE&apos;04. XL = p� (x, y) log qλ (y|x) (x,y)ET + X q� (x, y) log qλ (y|x) (x,y)ET&apos; 478 For the cascade model, the sub-task flow is presented in Figure 1. In the first step, we identify the mention boundaries together with their entity type (e.g. person, organization, etc). In preliminary experiments, we tried to “cascade” this task. The performance was similar on both str</context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>E. F. Tjong Kim Sang and J. Veenstra. 1999. Representing text chunks. In Proceedings of EACL&apos;99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
</authors>
<title>Introduction to the conll2002 shared task: Language-independent named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>155--158</pages>
<contexts>
<context position="16632" citStr="Sang, 2002" startWordPosition="2694" endWordPosition="2695"> each other (because the system will return a probability distribution over the classification space), sometimes resulting in wrong partial classification.9 • The model cannot directly use data that is only partially labeled (i.e. not all sub-labels are specified). Despite the above disadvantages, this model has performed well in practice: Hajic and Hladki(1998) applied it successfully to find POS sequences for Czech and Florian et al. (2004) reports good results on the 2003 ACE task. Most systems that participated in the CoNLL 2002 and 2003 shared tasks on named entity recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) applied this model, as they modeled the identification of mention boundaries and the assignment of mention type at the same time. 3.2 The Joint Model The joint model differs from the all-in-one model in the fact that the labels are no longer atomic: the features of the system can inspect the constituent sub-labels. This change helps alleviate the data SFrom a practical point of view, it might not be very important, as the search is pruned in most cases to only a few hypotheses (beam-search); in our case, pruning the beam only introduced an insignificant m</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>E. F. Tjong Kim Sang. 2002. Introduction to the conll2002 shared task: Language-independent named entity recognition. In Proceedings of CoNLL-2002, pages 155–158.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>