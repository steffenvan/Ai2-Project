<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002780">
<title confidence="0.59825">
Book Reviews
Theory and Practice in Corpus Linguistics
</title>
<author confidence="0.476841">
Jan Aarts and Willem Meijs, editors
</author>
<affiliation confidence="0.390835">
(University of Nijmegen and University of Amsterdam)
</affiliation>
<bodyText confidence="0.8575735">
Amsterdam: Editions Rodopi, 1990,
iii + 254 pp. (Language and Computers:
Studies in Practical Linguistics 4)
Paperbound, ISBN 90-5183-174-9, $37.50
</bodyText>
<figure confidence="0.615557666666667">
Reviewed by
Kenneth Ward Church
AT&amp;T Bell Laboratories
</figure>
<bodyText confidence="0.987348936170213">
Corpus linguistics is a hot topic, and for good reason. Text is more available than ever
before. And consequently it is easier to use corpus data more effectively than it was
in the 1950s, the last time that empiricism was in fashion.
Corpus linguistics is such a hot area that it is already splitting up into a number
of different sub-areas. Theory and Practice in Corpus Linguistics focuses on a direction
practiced in much of the U.K. and Scandinavia. This work has produced a number of
part-of-speech taggers and parsers based on probabilities derived from corpus data.
These programs work on unrestricted texts, with reasonable accuracy and efficiency. A
good example of this approach is Garside, Leech, and Sampson (1987); see Lesk (1988)
for a very positive review of this book and a strong endorsement of the approach that
it represents.
One might contrast the view(s) held by Garside, Leech, Sampson, Lesk, and others
with the Al tradition of using semantic networks and knowledge representation tech-
niques to address lexical questions. Evens (1988), a volume from the ACL book series,
is a good example of the Al approach to computational lexicography. The knowledge-
based approaches tend to assume that the representation is central, and that much of
the knowledge has to be entered into the system by hand, in contrast with probabilistic-
based approaches where much of the knowledge is acquired through various training
procedures that fit certain parameters to corpus data.
Yet a third quite distinct approach can be found among lexicographers, who
have recently become interested in computational issues because of the success of
the COBUILD dictionary:
For the first time, a dictionary has been compiled by the thorough examination
of a representative group of English texts, spoken and written, running to many
millions of words. This means that in addition to all the tools of the conventional
dictionary makers — wide reading and experience of English, other dictionaries
and of course eyes and ears — this dictionary [COBUILD] is based on hard,
measurable evidence. (Sinclair et al. 1987)
The experience of writing the COB UILD dictionary is well documented in Sinclair
(1987), a collection of articles from many of the participants of the COBUILD project;
see Boguraev (1990) for a strong positive review of this collection.
Like Sinclair (1987), Theory and Practice in Corpus Linguistics is also a collection of
papers by participants in an area of corpus linguistics that may be revolutionizing
the way we think about language. However, I would not expect this collection to
stand up as well to the test of time. Many of the articles are timely, interesting, and
Computational Linguistics Volume 17, Number 1
controversial. But, on the other hand, much of the work is still very much in progress.
I would strongly recommend the book to researchers who are actively involved in
corpus linguistics and computational lexicography. However, I would not recommend
it to people looking for a good overview of the field. This collection reads more like
a conference proceedings (which it is) than like a book. The papers are presented
in alphabetical order by first author&apos;s last name, after a short two-page preface that
starts out: &amp;quot;Like its predecessors... this new volume presents a kaleidoscope of recent
developments in this field.&amp;quot;
It is really hard to come up &apos;with useful unifying trends among the 11 papers.
They cover such a wide range of sub-areas. Nevertheless, eight of the papers can be
assigned to four topics, with two papers in each topic.
</bodyText>
<listItem confidence="0.9963028">
1. Corpus analysis on a small computer (e.g., an Apple or an IBM PC);
2. Corpus analysis (Sampson) vs. theoretical linguistics (Briscoe);
3. Corpus analysis and collocation;
4. Corpus analysis and discourse structure.
1. Corpus Analysis on a Small Computer
</listItem>
<bodyText confidence="0.990038692307692">
Two papers advocate the use of small computers for analyzing corpus material. One
of them points out that a small computer today has much better turnaround time than
mainframe computers of yesterday. Both papers are extremely enthusiastic about the
possibilities of interactive concordance programs and so forth. I think it is important
to point out that it is now possible for almost anyone to use large corpora. You no
longer need an expensive computer center to look at a concordance. It is easier than
ever before to look at data. And it isn&apos;t even expensive.
On the other hand, I do wish that the papers were a bit less enthusiastic. While
PCs do provide a lot of computer power for less than the price of a car, they do not
solve all of the world&apos;s problems. I fear that PCs may be not the best way tb deal with
the larger corpora. I also fear that PCs may not be the best way to explore various
statistical possibilities. I may be old-fashioned (and spoiled rich), but I still believe
that it is easier to work with something a little bit more powerful than a PC.
</bodyText>
<listItem confidence="0.671119">
2. Corpus Analysis vs. Theoretical Linguistics
</listItem>
<bodyText confidence="0.9993967">
There are also two papers debating corpus analysis as practiced at Leeds and Lan-
caster as compared with a more &amp;quot;traditional&amp;quot; view. In a previous conference, Sampson
(Leeds) has observed that phrase structure rules have a very skewed distribution. In
particular, if you look in a typical corpus, according to Sampson, you will find very
many instances of a few phrase structure rules, and just a few instances of a large
number of phrase structure rules.
I actually don&apos;t find this surprising at all. All kinds of language facts have &amp;quot;Zipf-
like&amp;quot; distributions (in which the probability of a type is roughly inversely proportional
to its rank). Words are the classic example. A few function words (e.g., the, a, of) are
very common, and many words occur just once in any corpus that you look at. In
</bodyText>
<page confidence="0.949049">
100
</page>
<subsectionHeader confidence="0.890124">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.99993214">
fact, Zipf&apos;s law holds for all kinds of type—token distributions, such as the allocation
of people to cities and the allocation of income to people. It is an empirical law that
has fascinated statisticians for decades. The distribution keeps coming up in nature,
but unlike other distributions, such as the normal distribution, it is not clear just what
randomness assumptions give rise to it. (In addition, the law, as stated, can&apos;t be exactly
right, since probability should sum to 1, but the integral of 1/r doesn&apos;t converge.)
In the present discussion, Clive Souter (also at Leeds, but somewhat removed
from Sampson) suggests that the Zipfian distribution implies the hopelessness of the
&amp;quot;traditional&amp;quot; approach to writing grammars. Advocates of the Leeds school would
argue that it will take a lot of effort to enumerate all of the phrase structure rules in
the language, because there are a large number that rarely occur (assuming a Zipf-
like distribution). They then turn to alternative methods such as simulated annealing,
that have a bit of a self-organizing flavor (cf. Jelinek 1990). (Apparently, there are
some important differences within the Leeds school; although Sampson and Souter
both endorse simulated annealing, Souter is sympathetic to self-organizing and/or
connectionist methods, while Sampson is not.)
The formalizing of systemic functional grammars for use in parsing rather than
generating natural language can be achieved by extracting rules from suitably
annotated English corpora. The very large size of such grammars puts into
question the real value of building small &apos;competence&apos; grammars by hand,
particularly if the grammar is to be used in the parsing of relatively unrestricted
English, which is a long-term goal of the COMMUNAL project. The frequency
distribution of extracted rules, as well as words, adheres to Zipf&apos;s law. This
open-ended characteristic of the extracted grammars suggests that a probabilistic
parsing technique which employs frequency data from the corpora may be most
suitable for the parsing of unrestricted English. (Souter, p. 195)
Briscoe counters by denying the Zipfian assumption. He argues that Sampson&apos;s ar-
gument has a failure-to-find fallacy. While any particular grammar, such as Sampson&apos;s
or Souter&apos;s, may have a Zipf-like distribution, it is possible (though maybe not very
likely) that there is another grammar that does not. Technically, one cannot rule out
the possibility of such a grammar just because one did not happen to find it. I&apos;ll grant
Briscoe the technical point and admit that it has not yet been proven that grammar
must have a Zipfian distribution. Nevertheless, I am basically convinced that grammar
probably does have a Zipf-like distribution even though I don&apos;t know how to prove
it. It just doesn&apos;t seem very likely to me that there could be a small, nicely distributed
set of phrase structure rules that would adequately describe grammar as it is actually
used in practice. The performance grammar will probably have to be at least as large
as the concise version of Quirk and Greenbaum (1973), a book of 500 pages.
On the other hand, while I agree with Sampson and others at Leeds in granting
that grammar probably does have a Zipfian distribution, I do not accept the rest
of their argument. I believe that it may still be practical to describe grammar with
traditional methods, even though the performance grammar may be large and the
distribution may be skewed. Lexicographers have managed to do a fairly good job
of describing words (without explicit probabilities), and the set of words is large and
the distribution is skewed. Of course, it will require a lot of hard work and a lot
of drudgery. But I believe it can be done, given a monumental effort like Murray&apos;s
Oxford English Dictionary project. I would rather bet on hard work than speculate on
a silver bullet like simulated annealing. I would be particularly suspicious of Souter&apos;s
attempts to appeal to self-organizing and/or connectionist methods as an alternative
to hard work.
</bodyText>
<page confidence="0.994002">
101
</page>
<note confidence="0.487276">
Computational Linguistics Volume 17, Number 1
</note>
<bodyText confidence="0.999897882352941">
I would draw a different conclusion from the observation that grammars are large
and their distribution is skewed. I think this observation points out the need to adopt
more efficient methods of collecting and evaluating evidence. Lexicographers have de-
veloped methods for writing dictionaries that resemble an expedition-style assault on
Mt. Everest. The whole enterprise is considered development, not research. Milestones
are set and taken very seriously. A lot of time is spent at the beginning of the project
on time-and-motion studies to make sure that the procedures are reasonably efficient
and that the milestones are realistic.
I suspect that computational linguists should build grammars with more or less
the same procedures. So far, most of the discussion in the literature has been on the
form of the grammar: should we use unification grammars or probabilities? I suspect
that these issues may be a bit of a red herring. I am much more concerned with how
we are going to speed up the process of collecting and interpreting the evidence. We
need to work out more efficient procedures, since it is going to be a big job and we
have very limited resources. In the long term, at least, we have the responsibility to
deliver a large grammar with broad coverage for unrestricted text. We need to start
thinking now about how we could ever hope to achieve this long-term goal.
</bodyText>
<sectionHeader confidence="0.430234" genericHeader="abstract">
3. Corpus Analysis and Collocation
</sectionHeader>
<bodyText confidence="0.993434833333333">
There are also two papers on collocation, one of the central problems in corpus lin-
guistics. The introduction to Kjellmer&apos;s paper describes the problem very well.
It is a common observation that words... tend to occur in clusters.... Mt is not
surprising, therefore, that the large-scale study of... collocations, made possible
by... computers, has come to be seen as more and more important over the last
few decades.... This is shown by the number of projects... devoted to the study
of collocations, and also... by the... emphasis... in recent... dictionaries. There is
a world of difference...between... the Concise Oxford Dictionary and the recent
COB UILD, Longman, and Oxford Advanced Learner&apos;s dictionaries.
Kjellmer then goes on to study the distribution of collocations in the Brown Cor-
pus. It is very difficult, though, to study collocations with such a small corpus. A
million words is simply not enough to see interesting word pairs. In a million words,
one can find a few of the more common two-word collocations such as of the (e.g.,
page 13 of Altenberg and Eeg-Olofsson&apos;s paper), but you can&apos;t possibly see the full
range of two-word noun phrases such as red herring and verb + prep combinations
such as give up. I suspect that it will require a huge amount of text to learn simple
facts such as these. It might require even more text to learn syntax.
All 11 papers in this volume used very small corpora, ranging from a few hundred
words up to one million words. In contrast, the COBUILD project used a corpus of
20 million words. These days, a million-word corpus is small; a corpus has to be at
least ten times larger to be considered large. And there are many corpora in use today
that are a hundred times larger than the Brown Corpus. None of the papers in this
collection discussed a corpus that could be considered large by today&apos;s standards.
This fact seriously undermines some of the papers. For example, consider the two
papers that argue for the use of small computers (e.g., an Apple or an IBM PC) in
corpus analysis work. It is clear that such a small computer is appropriate for a small
corpus. But will a small computer suffice for a reasonably sized corpus? Neither pa-
per even considers the question. Many readers are likely to be very interested in this
question, since they will have access to large corpora such as the British National
Corpus or the material to be distributed by the ACL&apos;s Data Collection Initiative. Can
</bodyText>
<page confidence="0.996493">
102
</page>
<subsectionHeader confidence="0.927326">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999907964285714">
they expect to work with so much text on a small computer? I&apos;m not sure what the
answer is, but I&apos;m sure that it is an important one that will come up again and again.
And dialectal variation adds another dimension that will surely consume even
more text. Ossi Ihalainen, for example, begins his paper by citing Nelson Francis&apos;s
observation in 1983 that there has been very little work on dialectal syntax (e.g., dis-
tributions of peculiar progressives such as she was sat in that chair) because such a
study would require larger samples of language than have been available so far. He
then cites Labov as saying more or less the same thing in 1970. After reading this
introduction, I was expecting him to come up with a much larger corpus of some tens
of millions of words, and conclude that with a larger corpus he could make observa-
tions that were not possible in 1983 and in 1970. Unfortunately, he did not have such a
rabbit to pull out of his hat. Rather he had to make do with several small corpora, no
bigger than what was available in 1983 and in 1970. I must say that I was impressed
with what he was able to do with these tiny corpora, but I would like to believe that
there is a reason why empiricism is back in fashion. My personal hunch is that the
approach is technically more feasible than it was just a few years ago, and that is why
it is back in fashion.
Although I haven&apos;t discussed all 11 papers, let me stop here. While many of my
remarks may sound critical, I do not intend them to sound negative. I found all of the
papers extremely thought-provoking. I would not have enjoyed the book so much if
I agreed with all of it.
Let me end with one final quibble: I don&apos;t like the title Theory and Practice in Corpus
Linguistics. The term theory seems badly out of place. Perhaps, the volume should have
been called Practice and Practice in Corpus Linguistics. The papers do a fairly good job
of describing, by example, how corpus linguistics is practiced, at least in parts of the
U.K. and Scandinavia. The absence of papers from my country (the United States)
is striking, though perhaps appropriate, given our history of objecting so strongly to
empiricism.
</bodyText>
<sectionHeader confidence="0.992011" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999545571428571">
Boguraev, Branimir, reviewer (1990). Review
of Sinclair (1987). Computational
Linguistics, 16(3), 184-186.
Evens, Martha, ed. (1988). Relational Models
of the Lexicon. Cambridge University
Press.
Garside, Roger; Leech, Geoffrey; and
Sampson, Geoffrey, eds. (1987). The
Computational Analysis of English: A
Corpus-Based Approach. Longman.
Jelinek, F. (1990). &amp;quot;Self-organized language
modeling for speech recognition.&amp;quot; In
Readings in Speech Recognition, edited by
A. Waibel and K. Lee. Morgan Kaufmann
Publishers.
Lesk, Michael, reviewer (1988). Review of
Garside et al. (1987). Computational
Linguistics, 14(4), 90-91.
Quirk, Randolph and Greenbaum, Sidney
(1973). A Concise Grammar of Contemporary
English. Harcourt Brace Jovanovich.
Sinclair, John; Hanks, P.; Fox, G.; Moon, R.;
and Stock, P., eds. (1987). Collins
COB UILD English Language Dictionary.
Collins.
Sinclair, John, ed. (1987). Looking up: An
Account of the COBUILD Project in Lexical
Computing. Collins.
Kenneth Ward Church received his Ph.D. in computer science from MIT in 1983, and then went
to work at AT&amp;T Bell Laboratories on problems in speech and natural language. Recently, he
has been advocating the use of statistical methods for analyzing large corpora. He is a mem-
ber of the ACL Data Collection Initiative. He is presently on sabbatical at: USC/Information
Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292; e-mail: church@isi.edu. Af-
ter September 1991, his address will be: Room 2d444, AT&amp;T Bell Laboratories, 600 Mountain
Avenue, Murray Hill, NJ 07974; e-mail: kwc@research.att.com.
</reference>
<page confidence="0.999298">
103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.210133">
<title confidence="0.998012">Book Reviews Theory and Practice in Corpus Linguistics</title>
<author confidence="0.988711">Jan Aarts</author>
<author confidence="0.988711">Willem Meijs</author>
<author confidence="0.988711">editors</author>
<affiliation confidence="0.976262">(University of Nijmegen and University of Amsterdam)</affiliation>
<address confidence="0.86754">Amsterdam: Editions Rodopi, 1990,</address>
<note confidence="0.878575">254 pp. (Language and Computers: Studies in Practical Linguistics 4) Paperbound, ISBN 90-5183-174-9, $37.50 Reviewed by</note>
<author confidence="0.996825">Kenneth Ward Church</author>
<affiliation confidence="0.998234">AT&amp;T Bell Laboratories</affiliation>
<abstract confidence="0.997978239726028">Corpus linguistics is a hot topic, and for good reason. Text is more available than ever before. And consequently it is easier to use corpus data more effectively than it was in the 1950s, the last time that empiricism was in fashion. Corpus linguistics is such a hot area that it is already splitting up into a number different sub-areas. and Practice in Corpus Linguistics on a direction practiced in much of the U.K. and Scandinavia. This work has produced a number of part-of-speech taggers and parsers based on probabilities derived from corpus data. These programs work on unrestricted texts, with reasonable accuracy and efficiency. A good example of this approach is Garside, Leech, and Sampson (1987); see Lesk (1988) for a very positive review of this book and a strong endorsement of the approach that it represents. One might contrast the view(s) held by Garside, Leech, Sampson, Lesk, and others with the Al tradition of using semantic networks and knowledge representation techniques to address lexical questions. Evens (1988), a volume from the ACL book series, is a good example of the Al approach to computational lexicography. The knowledgebased approaches tend to assume that the representation is central, and that much of the knowledge has to be entered into the system by hand, in contrast with probabilisticbased approaches where much of the knowledge is acquired through various training procedures that fit certain parameters to corpus data. Yet a third quite distinct approach can be found among lexicographers, who have recently become interested in computational issues because of the success of For the first time, a dictionary has been compiled by the thorough examination of a representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conventional dictionary makers — wide reading and experience of English, other dictionaries of course eyes and ears — this dictionary based on hard, measurable evidence. (Sinclair et al. 1987) experience of writing the UILD is well documented in Sinclair (1987), a collection of articles from many of the participants of the COBUILD project; see Boguraev (1990) for a strong positive review of this collection. Sinclair (1987), and Practice in Corpus Linguistics also a collection of papers by participants in an area of corpus linguistics that may be revolutionizing the way we think about language. However, I would not expect this collection to stand up as well to the test of time. Many of the articles are timely, interesting, and Computational Linguistics Volume 17, Number 1 controversial. But, on the other hand, much of the work is still very much in progress. I would strongly recommend the book to researchers who are actively involved in corpus linguistics and computational lexicography. However, I would not recommend it to people looking for a good overview of the field. This collection reads more like a conference proceedings (which it is) than like a book. The papers are presented in alphabetical order by first author&apos;s last name, after a short two-page preface that starts out: &amp;quot;Like its predecessors... this new volume presents a kaleidoscope of recent developments in this field.&amp;quot; It is really hard to come up &apos;with useful unifying trends among the 11 papers. They cover such a wide range of sub-areas. Nevertheless, eight of the papers can be assigned to four topics, with two papers in each topic. 1. Corpus analysis on a small computer (e.g., an Apple or an IBM PC); 2. Corpus analysis (Sampson) vs. theoretical linguistics (Briscoe); 3. Corpus analysis and collocation; 4. Corpus analysis and discourse structure. 1. Corpus Analysis on a Small Computer Two papers advocate the use of small computers for analyzing corpus material. One of them points out that a small computer today has much better turnaround time than mainframe computers of yesterday. Both papers are extremely enthusiastic about the possibilities of interactive concordance programs and so forth. I think it is important to point out that it is now possible for almost anyone to use large corpora. You no longer need an expensive computer center to look at a concordance. It is easier than ever before to look at data. And it isn&apos;t even expensive. On the other hand, I do wish that the papers were a bit less enthusiastic. While PCs do provide a lot of computer power for less than the price of a car, they do not solve all of the world&apos;s problems. I fear that PCs may be not the best way tb deal with the larger corpora. I also fear that PCs may not be the best way to explore various statistical possibilities. I may be old-fashioned (and spoiled rich), but I still believe that it is easier to work with something a little bit more powerful than a PC. 2. Corpus Analysis vs. Theoretical Linguistics There are also two papers debating corpus analysis as practiced at Leeds and Lancaster as compared with a more &amp;quot;traditional&amp;quot; view. In a previous conference, Sampson (Leeds) has observed that phrase structure rules have a very skewed distribution. In particular, if you look in a typical corpus, according to Sampson, you will find very many instances of a few phrase structure rules, and just a few instances of a large number of phrase structure rules. I actually don&apos;t find this surprising at all. All kinds of language facts have &amp;quot;Zipflike&amp;quot; distributions (in which the probability of a type is roughly inversely proportional its rank). Words are the classic example. A few function words (e.g., a, of) very common, and many words occur just once in any corpus that you look at. In 100 Book Reviews fact, Zipf&apos;s law holds for all kinds of type—token distributions, such as the allocation of people to cities and the allocation of income to people. It is an empirical law that has fascinated statisticians for decades. The distribution keeps coming up in nature, but unlike other distributions, such as the normal distribution, it is not clear just what randomness assumptions give rise to it. (In addition, the law, as stated, can&apos;t be exactly right, since probability should sum to 1, but the integral of 1/r doesn&apos;t converge.) In the present discussion, Clive Souter (also at Leeds, but somewhat removed from Sampson) suggests that the Zipfian distribution implies the hopelessness of the &amp;quot;traditional&amp;quot; approach to writing grammars. Advocates of the Leeds school would argue that it will take a lot of effort to enumerate all of the phrase structure rules in the language, because there are a large number that rarely occur (assuming a Zipfdistribution). They then turn to alternative methods such as annealing, that have a bit of a self-organizing flavor (cf. Jelinek 1990). (Apparently, there are some important differences within the Leeds school; although Sampson and Souter both endorse simulated annealing, Souter is sympathetic to self-organizing and/or connectionist methods, while Sampson is not.) The formalizing of systemic functional grammars for use in parsing rather than generating natural language can be achieved by extracting rules from suitably annotated English corpora. The very large size of such grammars puts into question the real value of building small &apos;competence&apos; grammars by hand, particularly if the grammar is to be used in the parsing of relatively unrestricted English, which is a long-term goal of the COMMUNAL project. The frequency distribution of extracted rules, as well as words, adheres to Zipf&apos;s law. This open-ended characteristic of the extracted grammars suggests that a probabilistic parsing technique which employs frequency data from the corpora may be most suitable for the parsing of unrestricted English. (Souter, p. 195) Briscoe counters by denying the Zipfian assumption. He argues that Sampson&apos;s argument has a failure-to-find fallacy. While any particular grammar, such as Sampson&apos;s or Souter&apos;s, may have a Zipf-like distribution, it is possible (though maybe not very likely) that there is another grammar that does not. Technically, one cannot rule out the possibility of such a grammar just because one did not happen to find it. I&apos;ll grant Briscoe the technical point and admit that it has not yet been proven that grammar must have a Zipfian distribution. Nevertheless, I am basically convinced that grammar probably does have a Zipf-like distribution even though I don&apos;t know how to prove it. It just doesn&apos;t seem very likely to me that there could be a small, nicely distributed set of phrase structure rules that would adequately describe grammar as it is actually used in practice. The performance grammar will probably have to be at least as large as the concise version of Quirk and Greenbaum (1973), a book of 500 pages. On the other hand, while I agree with Sampson and others at Leeds in granting that grammar probably does have a Zipfian distribution, I do not accept the rest of their argument. I believe that it may still be practical to describe grammar with traditional methods, even though the performance grammar may be large and the distribution may be skewed. Lexicographers have managed to do a fairly good job of describing words (without explicit probabilities), and the set of words is large and the distribution is skewed. Of course, it will require a lot of hard work and a lot of drudgery. But I believe it can be done, given a monumental effort like Murray&apos;s English Dictionary I would rather bet on hard work than speculate on a silver bullet like simulated annealing. I would be particularly suspicious of Souter&apos;s attempts to appeal to self-organizing and/or connectionist methods as an alternative to hard work. 101 Computational Linguistics Volume 17, Number 1 I would draw a different conclusion from the observation that grammars are large and their distribution is skewed. I think this observation points out the need to adopt more efficient methods of collecting and evaluating evidence. Lexicographers have developed methods for writing dictionaries that resemble an expedition-style assault on Mt. Everest. The whole enterprise is considered development, not research. Milestones are set and taken very seriously. A lot of time is spent at the beginning of the project on time-and-motion studies to make sure that the procedures are reasonably efficient and that the milestones are realistic. I suspect that computational linguists should build grammars with more or less the same procedures. So far, most of the discussion in the literature has been on the form of the grammar: should we use unification grammars or probabilities? I suspect that these issues may be a bit of a red herring. I am much more concerned with how we are going to speed up the process of collecting and interpreting the evidence. We need to work out more efficient procedures, since it is going to be a big job and we have very limited resources. In the long term, at least, we have the responsibility to deliver a large grammar with broad coverage for unrestricted text. We need to start thinking now about how we could ever hope to achieve this long-term goal.</abstract>
<note confidence="0.6957855">3. Corpus Analysis and Collocation There are also two papers on collocation, one of the central problems in corpus lin-</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Branimir Boguraev</author>
</authors>
<title>Review of Sinclair</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>3</issue>
<pages>184--186</pages>
<contexts>
<context position="2640" citStr="Boguraev (1990)" startWordPosition="418" endWordPosition="419">t time, a dictionary has been compiled by the thorough examination of a representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conventional dictionary makers — wide reading and experience of English, other dictionaries and of course eyes and ears — this dictionary [COBUILD] is based on hard, measurable evidence. (Sinclair et al. 1987) The experience of writing the COB UILD dictionary is well documented in Sinclair (1987), a collection of articles from many of the participants of the COBUILD project; see Boguraev (1990) for a strong positive review of this collection. Like Sinclair (1987), Theory and Practice in Corpus Linguistics is also a collection of papers by participants in an area of corpus linguistics that may be revolutionizing the way we think about language. However, I would not expect this collection to stand up as well to the test of time. Many of the articles are timely, interesting, and Computational Linguistics Volume 17, Number 1 controversial. But, on the other hand, much of the work is still very much in progress. I would strongly recommend the book to researchers who are actively involved</context>
</contexts>
<marker>Boguraev, 1990</marker>
<rawString>Boguraev, Branimir, reviewer (1990). Review of Sinclair (1987). Computational Linguistics, 16(3), 184-186.</rawString>
</citation>
<citation valid="true">
<title>Relational Models of the Lexicon.</title>
<date>1988</date>
<editor>Evens, Martha, ed.</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1091" citStr="(1988)" startWordPosition="171" endWordPosition="171">us data more effectively than it was in the 1950s, the last time that empiricism was in fashion. Corpus linguistics is such a hot area that it is already splitting up into a number of different sub-areas. Theory and Practice in Corpus Linguistics focuses on a direction practiced in much of the U.K. and Scandinavia. This work has produced a number of part-of-speech taggers and parsers based on probabilities derived from corpus data. These programs work on unrestricted texts, with reasonable accuracy and efficiency. A good example of this approach is Garside, Leech, and Sampson (1987); see Lesk (1988) for a very positive review of this book and a strong endorsement of the approach that it represents. One might contrast the view(s) held by Garside, Leech, Sampson, Lesk, and others with the Al tradition of using semantic networks and knowledge representation techniques to address lexical questions. Evens (1988), a volume from the ACL book series, is a good example of the Al approach to computational lexicography. The knowledgebased approaches tend to assume that the representation is central, and that much of the knowledge has to be entered into the system by hand, in contrast with probabili</context>
</contexts>
<marker>1988</marker>
<rawString>Evens, Martha, ed. (1988). Relational Models of the Lexicon. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<title>The Computational Analysis of English: A Corpus-Based Approach.</title>
<date>1987</date>
<editor>Garside, Roger; Leech, Geoffrey; and Sampson, Geoffrey, eds.</editor>
<publisher>Longman.</publisher>
<contexts>
<context position="1074" citStr="(1987)" startWordPosition="168" endWordPosition="168">asier to use corpus data more effectively than it was in the 1950s, the last time that empiricism was in fashion. Corpus linguistics is such a hot area that it is already splitting up into a number of different sub-areas. Theory and Practice in Corpus Linguistics focuses on a direction practiced in much of the U.K. and Scandinavia. This work has produced a number of part-of-speech taggers and parsers based on probabilities derived from corpus data. These programs work on unrestricted texts, with reasonable accuracy and efficiency. A good example of this approach is Garside, Leech, and Sampson (1987); see Lesk (1988) for a very positive review of this book and a strong endorsement of the approach that it represents. One might contrast the view(s) held by Garside, Leech, Sampson, Lesk, and others with the Al tradition of using semantic networks and knowledge representation techniques to address lexical questions. Evens (1988), a volume from the ACL book series, is a good example of the Al approach to computational lexicography. The knowledgebased approaches tend to assume that the representation is central, and that much of the knowledge has to be entered into the system by hand, in contra</context>
<context position="2540" citStr="(1987)" startWordPosition="403" endWordPosition="403">sted in computational issues because of the success of the COBUILD dictionary: For the first time, a dictionary has been compiled by the thorough examination of a representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conventional dictionary makers — wide reading and experience of English, other dictionaries and of course eyes and ears — this dictionary [COBUILD] is based on hard, measurable evidence. (Sinclair et al. 1987) The experience of writing the COB UILD dictionary is well documented in Sinclair (1987), a collection of articles from many of the participants of the COBUILD project; see Boguraev (1990) for a strong positive review of this collection. Like Sinclair (1987), Theory and Practice in Corpus Linguistics is also a collection of papers by participants in an area of corpus linguistics that may be revolutionizing the way we think about language. However, I would not expect this collection to stand up as well to the test of time. Many of the articles are timely, interesting, and Computational Linguistics Volume 17, Number 1 controversial. But, on the other hand, much of the work is still</context>
</contexts>
<marker>1987</marker>
<rawString>Garside, Roger; Leech, Geoffrey; and Sampson, Geoffrey, eds. (1987). The Computational Analysis of English: A Corpus-Based Approach. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Self-organized language modeling for speech recognition.&amp;quot;</title>
<date>1990</date>
<booktitle>In Readings in Speech Recognition, edited</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="7198" citStr="Jelinek 1990" startWordPosition="1188" endWordPosition="1189">m to 1, but the integral of 1/r doesn&apos;t converge.) In the present discussion, Clive Souter (also at Leeds, but somewhat removed from Sampson) suggests that the Zipfian distribution implies the hopelessness of the &amp;quot;traditional&amp;quot; approach to writing grammars. Advocates of the Leeds school would argue that it will take a lot of effort to enumerate all of the phrase structure rules in the language, because there are a large number that rarely occur (assuming a Zipflike distribution). They then turn to alternative methods such as simulated annealing, that have a bit of a self-organizing flavor (cf. Jelinek 1990). (Apparently, there are some important differences within the Leeds school; although Sampson and Souter both endorse simulated annealing, Souter is sympathetic to self-organizing and/or connectionist methods, while Sampson is not.) The formalizing of systemic functional grammars for use in parsing rather than generating natural language can be achieved by extracting rules from suitably annotated English corpora. The very large size of such grammars puts into question the real value of building small &apos;competence&apos; grammars by hand, particularly if the grammar is to be used in the parsing of rel</context>
</contexts>
<marker>Jelinek, 1990</marker>
<rawString>Jelinek, F. (1990). &amp;quot;Self-organized language modeling for speech recognition.&amp;quot; In Readings in Speech Recognition, edited by A. Waibel and K. Lee. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<date>1988</date>
<journal>Review of Garside</journal>
<volume>14</volume>
<issue>4</issue>
<pages>90--91</pages>
<contexts>
<context position="1091" citStr="Lesk (1988)" startWordPosition="170" endWordPosition="171"> corpus data more effectively than it was in the 1950s, the last time that empiricism was in fashion. Corpus linguistics is such a hot area that it is already splitting up into a number of different sub-areas. Theory and Practice in Corpus Linguistics focuses on a direction practiced in much of the U.K. and Scandinavia. This work has produced a number of part-of-speech taggers and parsers based on probabilities derived from corpus data. These programs work on unrestricted texts, with reasonable accuracy and efficiency. A good example of this approach is Garside, Leech, and Sampson (1987); see Lesk (1988) for a very positive review of this book and a strong endorsement of the approach that it represents. One might contrast the view(s) held by Garside, Leech, Sampson, Lesk, and others with the Al tradition of using semantic networks and knowledge representation techniques to address lexical questions. Evens (1988), a volume from the ACL book series, is a good example of the Al approach to computational lexicography. The knowledgebased approaches tend to assume that the representation is central, and that much of the knowledge has to be entered into the system by hand, in contrast with probabili</context>
</contexts>
<marker>Lesk, 1988</marker>
<rawString>Lesk, Michael, reviewer (1988). Review of Garside et al. (1987). Computational Linguistics, 14(4), 90-91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
</authors>
<title>A Concise Grammar of Contemporary English.</title>
<date>1973</date>
<publisher>Harcourt Brace Jovanovich.</publisher>
<contexts>
<context position="9189" citStr="Quirk and Greenbaum (1973)" startWordPosition="1498" endWordPosition="1501">mmar just because one did not happen to find it. I&apos;ll grant Briscoe the technical point and admit that it has not yet been proven that grammar must have a Zipfian distribution. Nevertheless, I am basically convinced that grammar probably does have a Zipf-like distribution even though I don&apos;t know how to prove it. It just doesn&apos;t seem very likely to me that there could be a small, nicely distributed set of phrase structure rules that would adequately describe grammar as it is actually used in practice. The performance grammar will probably have to be at least as large as the concise version of Quirk and Greenbaum (1973), a book of 500 pages. On the other hand, while I agree with Sampson and others at Leeds in granting that grammar probably does have a Zipfian distribution, I do not accept the rest of their argument. I believe that it may still be practical to describe grammar with traditional methods, even though the performance grammar may be large and the distribution may be skewed. Lexicographers have managed to do a fairly good job of describing words (without explicit probabilities), and the set of words is large and the distribution is skewed. Of course, it will require a lot of hard work and a lot of </context>
</contexts>
<marker>Quirk, Greenbaum, 1973</marker>
<rawString>Quirk, Randolph and Greenbaum, Sidney (1973). A Concise Grammar of Contemporary English. Harcourt Brace Jovanovich.</rawString>
</citation>
<citation valid="true">
<date>1987</date>
<journal>Collins COB UILD English Language Dictionary. Collins.</journal>
<editor>Sinclair, John; Hanks, P.; Fox, G.; Moon, R.; and Stock, P., eds.</editor>
<contexts>
<context position="1074" citStr="(1987)" startWordPosition="168" endWordPosition="168">asier to use corpus data more effectively than it was in the 1950s, the last time that empiricism was in fashion. Corpus linguistics is such a hot area that it is already splitting up into a number of different sub-areas. Theory and Practice in Corpus Linguistics focuses on a direction practiced in much of the U.K. and Scandinavia. This work has produced a number of part-of-speech taggers and parsers based on probabilities derived from corpus data. These programs work on unrestricted texts, with reasonable accuracy and efficiency. A good example of this approach is Garside, Leech, and Sampson (1987); see Lesk (1988) for a very positive review of this book and a strong endorsement of the approach that it represents. One might contrast the view(s) held by Garside, Leech, Sampson, Lesk, and others with the Al tradition of using semantic networks and knowledge representation techniques to address lexical questions. Evens (1988), a volume from the ACL book series, is a good example of the Al approach to computational lexicography. The knowledgebased approaches tend to assume that the representation is central, and that much of the knowledge has to be entered into the system by hand, in contra</context>
<context position="2540" citStr="(1987)" startWordPosition="403" endWordPosition="403">sted in computational issues because of the success of the COBUILD dictionary: For the first time, a dictionary has been compiled by the thorough examination of a representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conventional dictionary makers — wide reading and experience of English, other dictionaries and of course eyes and ears — this dictionary [COBUILD] is based on hard, measurable evidence. (Sinclair et al. 1987) The experience of writing the COB UILD dictionary is well documented in Sinclair (1987), a collection of articles from many of the participants of the COBUILD project; see Boguraev (1990) for a strong positive review of this collection. Like Sinclair (1987), Theory and Practice in Corpus Linguistics is also a collection of papers by participants in an area of corpus linguistics that may be revolutionizing the way we think about language. However, I would not expect this collection to stand up as well to the test of time. Many of the articles are timely, interesting, and Computational Linguistics Volume 17, Number 1 controversial. But, on the other hand, much of the work is still</context>
</contexts>
<marker>1987</marker>
<rawString>Sinclair, John; Hanks, P.; Fox, G.; Moon, R.; and Stock, P., eds. (1987). Collins COB UILD English Language Dictionary. Collins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sinclair</author>
<author>ed</author>
</authors>
<title>Looking up:</title>
<date>1987</date>
<booktitle>An Account of the COBUILD Project in Lexical Computing.</booktitle>
<publisher>Collins.</publisher>
<marker>Sinclair, ed, 1987</marker>
<rawString>Sinclair, John, ed. (1987). Looking up: An Account of the COBUILD Project in Lexical Computing. Collins.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kenneth Ward</author>
</authors>
<title>Church received his Ph.D. in computer science from MIT in 1983, and then went to work at AT&amp;T Bell Laboratories on problems in speech and natural language. Recently, he has been advocating the use of statistical methods for analyzing large corpora. He is a member of the ACL Data Collection Initiative. He is presently on sabbatical at: USC/Information</title>
<date>1991</date>
<booktitle>Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292; e-mail: church@isi.edu. After</booktitle>
<location>Murray Hill, NJ</location>
<note>07974; e-mail: kwc@research.att.com.</note>
<marker>Ward, 1991</marker>
<rawString>Kenneth Ward Church received his Ph.D. in computer science from MIT in 1983, and then went to work at AT&amp;T Bell Laboratories on problems in speech and natural language. Recently, he has been advocating the use of statistical methods for analyzing large corpora. He is a member of the ACL Data Collection Initiative. He is presently on sabbatical at: USC/Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292; e-mail: church@isi.edu. After September 1991, his address will be: Room 2d444, AT&amp;T Bell Laboratories, 600 Mountain Avenue, Murray Hill, NJ 07974; e-mail: kwc@research.att.com.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>