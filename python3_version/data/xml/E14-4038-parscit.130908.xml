<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.081875">
<title confidence="0.988904">
Enhancing Authorship Attribution By Utilizing Syntax Tree Profiles
</title>
<author confidence="0.99397">
Michael Tschuggnall and G¨unther Specht
</author>
<affiliation confidence="0.99197">
Institute of Computer Science, University of Innsbruck
</affiliation>
<address confidence="0.613506">
Technikerstraße 21a, 6020 Innsbruck, Austria
</address>
<email confidence="0.98836">
{michael.tschuggnall, guenther.specht}@uibk.ac.at
</email>
<sectionHeader confidence="0.993594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998405">
The aim of modern authorship attribution
approaches is to analyze known authors
and to assign authorships to previously un-
seen and unlabeled text documents based
on various features. In this paper we
present a novel feature to enhance cur-
rent attribution methods by analyzing the
grammar of authors. To extract the fea-
ture, a syntax tree of each sentence of a
document is calculated, which is then split
up into length-independent patterns using
pq-grams. The mostly used pq-grams are
then used to compose sample profiles of
authors that are compared with the pro-
file of the unlabeled document by utiliz-
ing various distance metrics and similarity
scores. An evaluation using three different
and independent data sets reveals promis-
ing results and indicate that the grammar
of authors is a significant feature to en-
hance modern authorship attribution meth-
ods.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999875910714286">
The increasing amount of documents available
from sources like publicly available literary
databases often raises the question of verifying
disputed authorships or assigning authors to un-
labeled text fragments. The original problem
was initiated already in the midst of the twenti-
eth century by Mosteller and Wallace, who tried
to find the correct authorships of The Federalist
Papers (Mosteller and Wallace, 1964), nonethe-
less authorship attribution is still a major research
topic. Especially with latest events in politics and
academia, the verification of authorships becomes
increasingly important and is used frequently in
areas like juridical applications (Forensic Linguis-
tics) or cybercrime detection (Nirkhi and Dha-
raskar, 2013). Similarily to works in the field
of plagiarism detection (e.g. (Stamatatos, 2009;
Tschuggnall and Specht, 2013b)) which aim to
find text fragments not written but claimed to be
written by an author, the problem of traditional
authorship attribution is defined as follows: Given
several authors with text samples for each of them,
the question is to label an unknown document
with the correct author. In contrast to this so-
called closed-class problem, an even harder task
is addressed in the open-class problem, where
additionally a ”none-of-them”-answer is allowed
(Juola, 2006).
In this paper we present a novel feature for the tra-
ditional, closed-class authorship attribution task,
following the assumption that different authors
have different writing styles in terms of the gram-
mar structure that is used mostly unconsciously.
Due to the fact that an author has many differ-
ent choices of how to formulate a sentence us-
ing the existing grammar rules of a natural lan-
guage, the assumption is that the way of construct-
ing sentences is significantly different for individ-
ual authors. For example, the famous Shakespeare
quote ”To be, or not to be: that is the question.”
(S1) could also be formulated as ”The question is
whether to be or not to be.” (S2) or even ”The
question is whether to be or not.” (S3) which is se-
mantically equivalent but differs significantly ac-
cording to the syntax (see Figure 1). The main idea
of this approach is to quantify those differences
by calculating grammar profiles for each candidate
author as well as for the unlabeled document, and
to assign one of the candidates as the author of the
unseen document by comparing the profiles. To
quantify the differences between profiles multiple
metrics have been implemented and evaluated.
The rest of this paper is organized as follows: Sec-
tion 2 sketches the main idea of the algorithm
which incorporates the distance metrics explained
in detail in Section 3. An extensive evaluation us-
</bodyText>
<page confidence="0.986585">
195
</page>
<note confidence="0.963487">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 195–199,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999776">
Figure 1: Syntax Trees Resulting From Parsing Sentence (S1), (S2) and (S3).
</figureCaption>
<figure confidence="0.999655518072289">
(S1) (S2)
(S3)
VB
(be)
VB
(be)
DT
(the)
NN
(question)
DT
(The)
NP
NN
(question)
SBAR
IN
(whether)
S CC
(or)
VP
VP
VB
(be)
S
TO
(to)
TO
(To)
VP CC
(or)
VP
VP
RB
(not)
S
TO
(to)
VP
S
: S
VP
DT
(that)
RBZ
(is)
VBZ
(is)
RB
VP
NP VP
S
TO
(to)
VB
(be)
VP
S
NP VP
S
NP VP
VP
DT
(The)
NN
(question)
VBZ
(is)
SBAR
S
IN
(whether)
VP
TO
(to)
NP
VB
(be)
QP
CC
(or)
RB
(not)
</figure>
<bodyText confidence="0.996554">
ing three different test sets is shown in Section 4,
while finally Section 5 and Section 6 summarize
related work and discuss future work, respectively.
</bodyText>
<sectionHeader confidence="0.880836" genericHeader="method">
2 Syntax Tree Profiles
</sectionHeader>
<bodyText confidence="0.99693223880597">
The basic idea of the approach is to utilize the syn-
tax that is used by authors to distinguish author-
ships of text documents. Based on our previous
work in the field of intrinsic plagiarism detection
(Tschuggnall and Specht, 2013c; Tschuggnall and
Specht, 2013a) we modify and enhance the algo-
rithms and apply them to be used in closed-class
authorship attribution.
The number of choices an author has to for-
mulate a sentence in terms of grammar is rather
high, and the assumption in this approach is that
the concrete choice is made mostly intuitively and
unconsciously. Evaluations shown in Section 4 re-
inforce that solely parse tree structures represent a
significant feature that can be used to distinguish
between authors.
From a global view the approach comprises the
following three steps: (A) Creating a grammar pro-
file for each author, (B) creating a grammar profile
for the unlabeled document, and (C) calculating
the distance between each author profile and the
document profile and assigning the author having
the lowest distance (or the highest similarity, de-
pending on the distance metric chosen). As this
approach is based on profiles a key criterion is the
creation of distinguishable author profiles. In or-
der to calculate a grammar profile for an author
or a document, the following procedure is applied:
(1) Concatenate all text samples for the author into
a single, large sample document, (2) split the re-
sulting document into single sentences and calcu-
late a syntax tree for each sentence, (3) calculate
the pq-gram index for each tree, and (4) compose
the final grammar profile from the normalized fre-
quencies of pq-grams.
At first the concatenated document is cleaned to
contain alphanumeric characters and punctuation
marks only, and then split into single sentences1.
Each sentence is then parsed2. For example, Fig-
ure 1 depicts the syntax trees resulting from sen-
tences (S1), (S2) and (S3). The labels of each tree
correspond to a Penn Treebank tag (Marcus et al.,
1993), where e.g NP corresponds to a noun phrase
or JJS to a superlative adjective. In order to exam-
ine solely the structure of sentences, the terminal
nodes (words) are ignored.
Having computed a syntax tree for every sentence,
the pq-gram index (Augsten et al., 2010) of each
tree is calculated in the next step. Pq-grams con-
sist of a stem (p) and a base (q) and may be re-
lated to as ”n-grams for trees”. Thereby p defines
how much nodes are included vertically, and q de-
fines the number of nodes to be considered hor-
izontally. For example, a pq-gram using p = 2
and q = 3 starting from level two of tree (S1)
would be [S-VP-VP-CC-RB]. In order to ob-
tain all pq-grams of a tree, the base is addition-
ally shifted left and right: If then less than p
nodes exist horizontally, the corresponding place
in the pq-gram is filled with *, indicating a miss-
ing node. Applying this idea to the previous exam-
ple, also the pq-grams [S-VP-*-*-VP] (base
shifted left by two), [S-VP-*-VP-CC] (base
shifted left by one), [S-VP-RB-VP-*] (base
shifted right by one) and [S-VP-VP-*-*] (base
shifted right by two) have to be considered. Fi-
nally, the pq-gram index contains all pq-grams of
</bodyText>
<footnote confidence="0.993038666666667">
1using OpenNLP, http://incubator.apache.org/opennlp,
visited October 2013
2using the Stanford Parser (Klein and Manning, 2003)
</footnote>
<page confidence="0.997214">
196
</page>
<bodyText confidence="0.999745764705882">
a syntax tree, whereby multiple occurences of the
same pq-grams are also present multiple times in
the index.
The remaining part for creating the author profile
is to compute the pq-gram index of the whole
document by combining all pq-gram indexes of all
sentences. In this step the number of occurences
is counted for each pq-gram and then normalized
by the total number of all appearing pq-grams. As
an example, the three mostly used pq-grams of
a selected document together with their normal-
ized frequencies are {[NP-NN-*-*-*],
2.7%1, {[PP-IN-*-*-*], 2.3%1, and
{[S-VP-*-*-VBD], 1.1%1. The final pq-
gram profile then consists of the complete table
of pq-grams and their occurences in the given
document.
</bodyText>
<sectionHeader confidence="0.984913" genericHeader="method">
3 Distance and Similarity Metrics
</sectionHeader>
<bodyText confidence="0.938634052631579">
With the use of the syntax tree profiles calculated
for each candidate author as well as for the unla-
beled document, the last part is to calculate a dis-
tance or similarity, respectively, for every author
profile. Finally, the unseen document is simply la-
beled with the author of the best matching profile.
To investigate on the best distance or simi-
larity metric to be used for this approach, sev-
eral metrics for this problem have been adapted
and evaluated3: 1. CNG (Keˇselj et al., 2003),
2. Stamatatos-CNG (Stamatatos, 2009), 3.
Stamatatos-CNG with Corpus Norm (Stamatatos,
2007), 4. Sentence-SPI.
For the latter, we modified the original SPI score
(Frantzeskou et al., 2006) so that each sentence
is traversed separately: Let SD be the set of sen-
tences of the document, I(s) the pq-gram-index of
sentence s and Px the profile of author X, then the
Sentence-SPI score is calculated as follows:
</bodyText>
<sectionHeader confidence="0.99916" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9998968">
The approach described in this paper has been ex-
tensively evaluated using three different English
data sets, whereby all sets are completely unre-
lated and of different types: (1.) CC04: the train-
ing set used for the Ad-hoc-Authorship Attribution
</bodyText>
<footnote confidence="0.930196">
3The algorithm names are only used as a reference for
this paper, but were not originally proposed like that
</footnote>
<bodyText confidence="0.999083947368421">
Competition workshop held in 20044 - type: nov-
els, authors: 4, documents: 8, samples per author:
1; (2.) FED: the (undisputed) federalist papers
written by Hamilton, Madison and Jay in the 18th
century - type: political essays, authors: 3, doc-
uments: 61, samples per author: 3; (3.) PAN12:
from the state-of-the-art corpus, especially created
for the use in authorship identification for the PAN
2012 workshop5 (Juola, 2012), all closed-classed
problems have been chosen - type: misc, authors:
3-16, documents: 6-16, samples per author: 2.
For the evaluation, each of the sets has been used
to optimize parameters while the remaining sets
have been used for testing. Besides examining the
discussed metrics and values for p and q (e.g. by
choosing p = 1 and q = 0 the pq-grams of a gram-
mar profile are equal to pure POS tags), two addi-
tional optimization variables have been integrated
for the similarity metric Sentence-SPI:
</bodyText>
<listItem confidence="0.914465625">
• topPQGramCount tc: by assigning a value
to this parameter, only the corresponding
amount of mostly used pq-grams of a gram-
mar profile are used.
• topPQGramOffset to: based on the idea that
all authors might have a frequently used and
common set of syntax rules that are prede-
fined by a specific language, this parameter
</listItem>
<bodyText confidence="0.982170235294118">
allows to ignore the given amount of mostly
used pq-grams. For example if to = 3 in Ta-
ble 1, the first pq-gram to be used would be
[NP-NNP-*-*-*].
The evaluation results are depicted in Table 1. It
shows the rate of correct author attributions based
on the grammar feature presented in this paper.
Generally, the algorithm worked best using the
Sentence-SPI score, which led to a rate of 72% by
using the PAN12 data set for optimization. The
optimal configuration uses p = 3 and q = 2,
which is the same configuration that was used in
(Augsten et al., 2010) to produce the best results.
The highest scores are gained by using a limit of
top pq-grams (tc ∼ 65) and by ignoring the first
three pq-grams (to = 3), which indicates that it is
sufficient to limit the number of syntax structures
</bodyText>
<footnote confidence="0.998531">
4http://www.mathcs.duq.edu/∼juola/authorship contest.html,
visited Oct. 2013
5PAN is a well-known workshop on Uncovering
Plagiarism, Authorship, and Social Software Misuses.
http://pan.webis.de, visited Oct. 2013
</footnote>
<equation confidence="0.830147">
�sPx,PD = �( 1 if p E Px
s∈SD t0 else
p∈I(s)
</equation>
<page confidence="0.954722">
197
</page>
<table confidence="0.996569">
metric p q Optimized With CC04 FED PAN12 Overall
Sentence-SPI (t, = 65, to = 3) 3 2 PAN12 57.14 86.89 (76.04) 72.02
CNG 0 2 PAN12 14.29 80.33 (57.29) 47.31
Stamatatos-CNG 2 2 PAN12 14.29 78.69 (60.42) 46.49
Stamatatos-CNG-CN 0 2 CC04 (42.86) 52.46 18.75 35.61
</table>
<tableCaption confidence="0.999943">
Table 1: Evaluation Results.
</tableCaption>
<bodyText confidence="0.999972875">
and that there exists a certain number (3) of gen-
eral grammar rules for English which are used by
all authors. I.e. those rules cannot by used to infer
information about individual authors (e.g. every
sentence starts with [S-...]).
All other metrics led to worse results, which
may also be a result of the fact that only the
Sentence-SPI metric makes use of the additional
parameters t, and to. Future work should also in-
vestigate on integrating these parameters also in
other metrics. Moreover, results are better using
the PAN12 data set for optimization, which may
be because this set is the most hetergeneous one:
The Federalist Papers contain only political essays
written some time ago, and the CC04 set only uses
literary texts written by four authors.
</bodyText>
<sectionHeader confidence="0.999849" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999902971428571">
Successful current approaches often are based on
or include character n-grams (e.g. (Hirst and
Feiguina, 2007; Stamatatos, 2009)). Several stud-
ies have shown that n-grams represent a significant
feature to identify authors, whereby the major ben-
efits are the language independency as well as the
easy computation. As a variation, word n-grams
are used in (Balaguer, 2009) to detect plagiarism
in text documents.
Using individual features, machine learning al-
gorithms are often applied to learn from au-
thor profiles and to predict unlabeled documents.
Among methods that are utilized in authorship at-
tribution as well as the related problem classes like
text categorization or intrinsic plagiarism detec-
tion are support vector machines (e.g. (Sanderson
and Guenter, 2006; Diederich et al., 2000)), neural
networks (e.g. (Tweedie et al., 1996)), naive bayes
classifiers (e.g. (McCallum and Nigam, 1998)) or
decision trees (e.g. ( ¨O. Uzuner et. al, 2005)).
Another interesting approach used in authorship
attribution that tries to detect the writing style of
authors by analyzing the occurences and varia-
tions of spelling errors is proposed in (Koppel and
Schler, 2003). It is based on the assumption that
authors tend to make similar spelling and/or gram-
mar errors and therefore uses this information to
attribute authors to unseen text documents.
Approaches in the field of genre categorization
also use NLP tools to analyze documents based
on syntactic annotations (Stamatatos et al., 2000).
Lexicalized tree-adjoining-grammars (LTAG) are
poposed in (Joshi and Schabes, 1997) as a ruleset
to construct and analyze grammar syntax by using
partial subtrees.
</bodyText>
<sectionHeader confidence="0.996182" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999737448275862">
In this paper we propose a new feature to enhance
modern authorship attribution algorithms by uti-
lizing the grammar syntax of authors. To distin-
guish between authors, syntax trees of sentences
are calculated which are split into parts by using
pq-grams. The set of pq-grams is then stored in an
author profile that is used to assign unseen docu-
ments to known authors.
The algorithm has been optimized and evalu-
ated using three different data sets, resulting in
an overall attribution rate of 72%. As the work
in this paper solely used the grammar feature and
completely ignores information like the vocabu-
lary richness or n-grams, the evaluation results are
promising. Future work should therefore concen-
trate on integrating other well-known and good-
working features as well as considering common
machine-learning techniques like support vector
machines or decision trees to predict authors based
on pq-gram features. Furthermore, the optimiza-
tion parameters currently only applied on the si-
miliarity score should also be integrated with the
distance metrics as they led to the best results. Re-
search should finally also be done on the appli-
cability to other languages, especially as syntac-
tically more complex languages like German or
French may lead to better results due to the higher
amount of grammar rules, making the writing style
of authors more unique.
</bodyText>
<page confidence="0.998">
198
</page>
<sectionHeader confidence="0.981126" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999042721649484">
Nikolaus Augsten, Michael B¨ohlen, and Johann Gam-
per. 2010. The pq-Gram Distance between Ordered
Labeled Trees. ACM Transactions on Database Sys-
tems (TODS).
Enrique Vall´es Balaguer. 2009. Putting Ourselves in
SME’s Shoes: Automatic Detection of Plagiarism
by the WCopyFind tool. In Proceedings of the SE-
PLN’09 Workshop on Uncovering Plagiarism, Au-
thorship and Social Software Misuse, pages 34–35.
Joachim Diederich, J¨org Kindermann, Edda Leopold,
and Gerhard Paass. 2000. Authorship attribution
with support vector machines. APPLIED INTELLI-
GENCE, 19:2003.
Georgia Frantzeskou, Efstathios Stamatatos, Stefanos
Gritzalis, and Sokratis Katsikas. 2006. Effective
identification of source code authors using byte-
level information. In Proceedings of the 28th inter-
national conference on Software engineering, pages
893–896. ACM.
Graeme Hirst and Ol’ga Feiguina. 2007. Bigrams
of syntactic labels for authorship discrimination of
short texts. Literary and Linguistic Computing,
22(4):405–417.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Handbook of formal lan-
guages, pages 69–123. Springer.
Patrick Juola. 2006. Authorship attribution. Founda-
tions and Trends in Information Retrieval, 1(3):233–
334.
Patrick Juola. 2012. An overview of the traditional au-
thorship attribution subtask. In CLEF (Online Work-
ing Notes/Labs/Workshop).
Vlado Keˇselj, Fuchun Peng, Nick Cercone, and Calvin
Thomas. 2003. N-gram-based author profiles for
authorship attribution. In Proceedings of the confer-
ence pacific association for computational linguis-
tics, PACLING, volume 3, pages 255–264.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 423–
430, Stroudsburg, PA, USA.
Moshe Koppel and Jonathan Schler. 2003. Exploit-
ing Stylistic Idiosyncrasies for Authorship Attribu-
tion. In IJCAI’03 Workshop On Computational Ap-
proaches To Style Analysis And Synthesis, pages 69–
72.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics, 19:313–330, June.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for naive bayes text classifi-
cation.
F. Mosteller and D. Wallace. 1964. Inference and Dis-
puted Authorship: The Federalist. Addison-Wesley.
Smita Nirkhi and RV Dharaskar. 2013. Comparative
study of authorship identification techniques for cy-
ber forensics analysis. International Journal.
¨O. Uzuner et. al. 2005. Using Syntactic Information
to Identify Plagiarism. In Proc. 2nd Workshop on
Building Educational Applications using NLP.
Conrad Sanderson and Simon Guenter. 2006. Short
text authorship attribution via sequence kernels,
markov chains and author unmasking: an investiga-
tion. In Proc. of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
482–491, Stroudsburg, PA, USA.
Efstathios Stamatatos, George Kokkinakis, and Nikos
Fakotakis. 2000. Automatic text categorization
in terms of genre and author. Comput. Linguist.,
26:471–495, December.
Efstathios Stamatatos. 2007. Author identification
using imbalanced and limited training texts. In
Database and Expert Systems Applications, 2007.
DEXA’07. 18th International Workshop on, pages
237–241. IEEE.
Efstathios Stamatatos. 2009. Intrinsic Plagiarism De-
tection Using Character n-gram Profiles. In CLEF
(Notebook Papers/Labs/Workshop).
Michael Tschuggnall and G¨unther Specht. 2013a.
Countering Plagiarism by Exposing Irregularities in
Authors Grammars. In EISIC, European Intelli-
gence and Security Informatics Conference, Upp-
sala, Sweden, pages 15–22.
Michael Tschuggnall and G¨unther Specht. 2013b.
Detecting Plagiarism in Text Documents through
Grammar-Analysis of Authors. In 15. GI-
Fachtagung Datenbanksysteme f¨ur Business, Tech-
nologie und Web, Magdeburg, Germany.
Michael Tschuggnall and G¨unther Specht. 2013c. Us-
ing grammar-profiles to intrinsically expose plagia-
rism in text documents. In NLDB, pages 297–302.
Fiona J. Tweedie, S. Singh, and David I. Holmes.
1996. Neural network applications in stylometry:
The federalist papers. Computers and the Humani-
ties, 30(1):1–10.
</reference>
<page confidence="0.998909">
199
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.374749">
<title confidence="0.999892">Enhancing Authorship Attribution By Utilizing Syntax Tree Profiles</title>
<author confidence="0.998001">Michael Tschuggnall</author>
<author confidence="0.998001">G¨unther</author>
<affiliation confidence="0.993462">Institute of Computer Science, University of</affiliation>
<address confidence="0.459577">Technikerstraße 21a, 6020 Innsbruck,</address>
<abstract confidence="0.991963956521739">The aim of modern authorship attribution approaches is to analyze known authors and to assign authorships to previously unseen and unlabeled text documents based on various features. In this paper we present a novel feature to enhance current attribution methods by analyzing the grammar of authors. To extract the feature, a syntax tree of each sentence of a document is calculated, which is then split up into length-independent patterns using pq-grams. The mostly used pq-grams are then used to compose sample profiles of authors that are compared with the profile of the unlabeled document by utilizing various distance metrics and similarity scores. An evaluation using three different and independent data sets reveals promising results and indicate that the grammar of authors is a significant feature to enhance modern authorship attribution methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nikolaus Augsten</author>
<author>Michael B¨ohlen</author>
<author>Johann Gamper</author>
</authors>
<title>The pq-Gram Distance between Ordered Labeled Trees.</title>
<date>2010</date>
<booktitle>ACM Transactions on Database Systems (TODS).</booktitle>
<marker>Augsten, B¨ohlen, Gamper, 2010</marker>
<rawString>Nikolaus Augsten, Michael B¨ohlen, and Johann Gamper. 2010. The pq-Gram Distance between Ordered Labeled Trees. ACM Transactions on Database Systems (TODS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Vall´es Balaguer</author>
</authors>
<title>Putting Ourselves in SME’s Shoes: Automatic Detection of Plagiarism by the WCopyFind tool.</title>
<date>2009</date>
<booktitle>In Proceedings of the SEPLN’09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse,</booktitle>
<pages>34--35</pages>
<contexts>
<context position="13707" citStr="Balaguer, 2009" startWordPosition="2270" endWordPosition="2271">data set for optimization, which may be because this set is the most hetergeneous one: The Federalist Papers contain only political essays written some time ago, and the CC04 set only uses literary texts written by four authors. 5 Related Work Successful current approaches often are based on or include character n-grams (e.g. (Hirst and Feiguina, 2007; Stamatatos, 2009)). Several studies have shown that n-grams represent a significant feature to identify authors, whereby the major benefits are the language independency as well as the easy computation. As a variation, word n-grams are used in (Balaguer, 2009) to detect plagiarism in text documents. Using individual features, machine learning algorithms are often applied to learn from author profiles and to predict unlabeled documents. Among methods that are utilized in authorship attribution as well as the related problem classes like text categorization or intrinsic plagiarism detection are support vector machines (e.g. (Sanderson and Guenter, 2006; Diederich et al., 2000)), neural networks (e.g. (Tweedie et al., 1996)), naive bayes classifiers (e.g. (McCallum and Nigam, 1998)) or decision trees (e.g. ( ¨O. Uzuner et. al, 2005)). Another interest</context>
</contexts>
<marker>Balaguer, 2009</marker>
<rawString>Enrique Vall´es Balaguer. 2009. Putting Ourselves in SME’s Shoes: Automatic Detection of Plagiarism by the WCopyFind tool. In Proceedings of the SEPLN’09 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse, pages 34–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Diederich</author>
<author>J¨org Kindermann</author>
<author>Edda Leopold</author>
<author>Gerhard Paass</author>
</authors>
<title>Authorship attribution with support vector machines.</title>
<date>2000</date>
<journal>APPLIED INTELLIGENCE,</journal>
<pages>19--2003</pages>
<contexts>
<context position="14130" citStr="Diederich et al., 2000" startWordPosition="2332" endWordPosition="2335">represent a significant feature to identify authors, whereby the major benefits are the language independency as well as the easy computation. As a variation, word n-grams are used in (Balaguer, 2009) to detect plagiarism in text documents. Using individual features, machine learning algorithms are often applied to learn from author profiles and to predict unlabeled documents. Among methods that are utilized in authorship attribution as well as the related problem classes like text categorization or intrinsic plagiarism detection are support vector machines (e.g. (Sanderson and Guenter, 2006; Diederich et al., 2000)), neural networks (e.g. (Tweedie et al., 1996)), naive bayes classifiers (e.g. (McCallum and Nigam, 1998)) or decision trees (e.g. ( ¨O. Uzuner et. al, 2005)). Another interesting approach used in authorship attribution that tries to detect the writing style of authors by analyzing the occurences and variations of spelling errors is proposed in (Koppel and Schler, 2003). It is based on the assumption that authors tend to make similar spelling and/or grammar errors and therefore uses this information to attribute authors to unseen text documents. Approaches in the field of genre categorization</context>
</contexts>
<marker>Diederich, Kindermann, Leopold, Paass, 2000</marker>
<rawString>Joachim Diederich, J¨org Kindermann, Edda Leopold, and Gerhard Paass. 2000. Authorship attribution with support vector machines. APPLIED INTELLIGENCE, 19:2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgia Frantzeskou</author>
</authors>
<title>Efstathios Stamatatos, Stefanos Gritzalis, and Sokratis Katsikas.</title>
<date>2006</date>
<booktitle>In Proceedings of the 28th international conference on Software engineering,</booktitle>
<pages>893--896</pages>
<publisher>ACM.</publisher>
<marker>Frantzeskou, 2006</marker>
<rawString>Georgia Frantzeskou, Efstathios Stamatatos, Stefanos Gritzalis, and Sokratis Katsikas. 2006. Effective identification of source code authors using bytelevel information. In Proceedings of the 28th international conference on Software engineering, pages 893–896. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>Ol’ga Feiguina</author>
</authors>
<title>Bigrams of syntactic labels for authorship discrimination of short texts.</title>
<date>2007</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>22--4</pages>
<contexts>
<context position="13445" citStr="Hirst and Feiguina, 2007" startWordPosition="2227" endWordPosition="2230">se results, which may also be a result of the fact that only the Sentence-SPI metric makes use of the additional parameters t, and to. Future work should also investigate on integrating these parameters also in other metrics. Moreover, results are better using the PAN12 data set for optimization, which may be because this set is the most hetergeneous one: The Federalist Papers contain only political essays written some time ago, and the CC04 set only uses literary texts written by four authors. 5 Related Work Successful current approaches often are based on or include character n-grams (e.g. (Hirst and Feiguina, 2007; Stamatatos, 2009)). Several studies have shown that n-grams represent a significant feature to identify authors, whereby the major benefits are the language independency as well as the easy computation. As a variation, word n-grams are used in (Balaguer, 2009) to detect plagiarism in text documents. Using individual features, machine learning algorithms are often applied to learn from author profiles and to predict unlabeled documents. Among methods that are utilized in authorship attribution as well as the related problem classes like text categorization or intrinsic plagiarism detection ar</context>
</contexts>
<marker>Hirst, Feiguina, 2007</marker>
<rawString>Graeme Hirst and Ol’ga Feiguina. 2007. Bigrams of syntactic labels for authorship discrimination of short texts. Literary and Linguistic Computing, 22(4):405–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Treeadjoining grammars.</title>
<date>1997</date>
<booktitle>In Handbook of formal languages,</booktitle>
<pages>69--123</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="14912" citStr="Joshi and Schabes, 1997" startWordPosition="2451" endWordPosition="2454">). Another interesting approach used in authorship attribution that tries to detect the writing style of authors by analyzing the occurences and variations of spelling errors is proposed in (Koppel and Schler, 2003). It is based on the assumption that authors tend to make similar spelling and/or grammar errors and therefore uses this information to attribute authors to unseen text documents. Approaches in the field of genre categorization also use NLP tools to analyze documents based on syntactic annotations (Stamatatos et al., 2000). Lexicalized tree-adjoining-grammars (LTAG) are poposed in (Joshi and Schabes, 1997) as a ruleset to construct and analyze grammar syntax by using partial subtrees. 6 Conclusion and Future Work In this paper we propose a new feature to enhance modern authorship attribution algorithms by utilizing the grammar syntax of authors. To distinguish between authors, syntax trees of sentences are calculated which are split into parts by using pq-grams. The set of pq-grams is then stored in an author profile that is used to assign unseen documents to known authors. The algorithm has been optimized and evaluated using three different data sets, resulting in an overall attribution rate o</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Aravind K. Joshi and Yves Schabes. 1997. Treeadjoining grammars. In Handbook of formal languages, pages 69–123. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Juola</author>
</authors>
<title>Authorship attribution. Foundations and Trends in Information Retrieval,</title>
<date>2006</date>
<volume>1</volume>
<issue>3</issue>
<pages>334</pages>
<contexts>
<context position="2458" citStr="Juola, 2006" startWordPosition="364" endWordPosition="365">ion (Nirkhi and Dharaskar, 2013). Similarily to works in the field of plagiarism detection (e.g. (Stamatatos, 2009; Tschuggnall and Specht, 2013b)) which aim to find text fragments not written but claimed to be written by an author, the problem of traditional authorship attribution is defined as follows: Given several authors with text samples for each of them, the question is to label an unknown document with the correct author. In contrast to this socalled closed-class problem, an even harder task is addressed in the open-class problem, where additionally a ”none-of-them”-answer is allowed (Juola, 2006). In this paper we present a novel feature for the traditional, closed-class authorship attribution task, following the assumption that different authors have different writing styles in terms of the grammar structure that is used mostly unconsciously. Due to the fact that an author has many different choices of how to formulate a sentence using the existing grammar rules of a natural language, the assumption is that the way of constructing sentences is significantly different for individual authors. For example, the famous Shakespeare quote ”To be, or not to be: that is the question.” (S1) co</context>
</contexts>
<marker>Juola, 2006</marker>
<rawString>Patrick Juola. 2006. Authorship attribution. Foundations and Trends in Information Retrieval, 1(3):233– 334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Juola</author>
</authors>
<title>An overview of the traditional authorship attribution subtask.</title>
<date>2012</date>
<booktitle>In CLEF (Online Working Notes/Labs/Workshop).</booktitle>
<contexts>
<context position="10396" citStr="Juola, 2012" startWordPosition="1709" endWordPosition="1710"> types: (1.) CC04: the training set used for the Ad-hoc-Authorship Attribution 3The algorithm names are only used as a reference for this paper, but were not originally proposed like that Competition workshop held in 20044 - type: novels, authors: 4, documents: 8, samples per author: 1; (2.) FED: the (undisputed) federalist papers written by Hamilton, Madison and Jay in the 18th century - type: political essays, authors: 3, documents: 61, samples per author: 3; (3.) PAN12: from the state-of-the-art corpus, especially created for the use in authorship identification for the PAN 2012 workshop5 (Juola, 2012), all closed-classed problems have been chosen - type: misc, authors: 3-16, documents: 6-16, samples per author: 2. For the evaluation, each of the sets has been used to optimize parameters while the remaining sets have been used for testing. Besides examining the discussed metrics and values for p and q (e.g. by choosing p = 1 and q = 0 the pq-grams of a grammar profile are equal to pure POS tags), two additional optimization variables have been integrated for the similarity metric Sentence-SPI: • topPQGramCount tc: by assigning a value to this parameter, only the corresponding amount of most</context>
</contexts>
<marker>Juola, 2012</marker>
<rawString>Patrick Juola. 2012. An overview of the traditional authorship attribution subtask. In CLEF (Online Working Notes/Labs/Workshop).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vlado Keˇselj</author>
<author>Fuchun Peng</author>
<author>Nick Cercone</author>
<author>Calvin Thomas</author>
</authors>
<title>N-gram-based author profiles for authorship attribution.</title>
<date>2003</date>
<booktitle>In Proceedings of the conference pacific association for computational linguistics, PACLING,</booktitle>
<volume>3</volume>
<pages>255--264</pages>
<marker>Keˇselj, Peng, Cercone, Thomas, 2003</marker>
<rawString>Vlado Keˇselj, Fuchun Peng, Nick Cercone, and Calvin Thomas. 2003. N-gram-based author profiles for authorship attribution. In Proceedings of the conference pacific association for computational linguistics, PACLING, volume 3, pages 255–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>423--430</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7962" citStr="Klein and Manning, 2003" startWordPosition="1310" endWordPosition="1313">a tree, the base is additionally shifted left and right: If then less than p nodes exist horizontally, the corresponding place in the pq-gram is filled with *, indicating a missing node. Applying this idea to the previous example, also the pq-grams [S-VP-*-*-VP] (base shifted left by two), [S-VP-*-VP-CC] (base shifted left by one), [S-VP-RB-VP-*] (base shifted right by one) and [S-VP-VP-*-*] (base shifted right by two) have to be considered. Finally, the pq-gram index contains all pq-grams of 1using OpenNLP, http://incubator.apache.org/opennlp, visited October 2013 2using the Stanford Parser (Klein and Manning, 2003) 196 a syntax tree, whereby multiple occurences of the same pq-grams are also present multiple times in the index. The remaining part for creating the author profile is to compute the pq-gram index of the whole document by combining all pq-gram indexes of all sentences. In this step the number of occurences is counted for each pq-gram and then normalized by the total number of all appearing pq-grams. As an example, the three mostly used pq-grams of a selected document together with their normalized frequencies are {[NP-NN-*-*-*], 2.7%1, {[PP-IN-*-*-*], 2.3%1, and {[S-VP-*-*-VBD], 1.1%1. The fi</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 423– 430, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
</authors>
<title>Exploiting Stylistic Idiosyncrasies for Authorship Attribution.</title>
<date>2003</date>
<booktitle>In IJCAI’03 Workshop On Computational Approaches To Style Analysis And Synthesis,</booktitle>
<pages>69--72</pages>
<contexts>
<context position="14503" citStr="Koppel and Schler, 2003" startWordPosition="2390" endWordPosition="2393">uments. Among methods that are utilized in authorship attribution as well as the related problem classes like text categorization or intrinsic plagiarism detection are support vector machines (e.g. (Sanderson and Guenter, 2006; Diederich et al., 2000)), neural networks (e.g. (Tweedie et al., 1996)), naive bayes classifiers (e.g. (McCallum and Nigam, 1998)) or decision trees (e.g. ( ¨O. Uzuner et. al, 2005)). Another interesting approach used in authorship attribution that tries to detect the writing style of authors by analyzing the occurences and variations of spelling errors is proposed in (Koppel and Schler, 2003). It is based on the assumption that authors tend to make similar spelling and/or grammar errors and therefore uses this information to attribute authors to unseen text documents. Approaches in the field of genre categorization also use NLP tools to analyze documents based on syntactic annotations (Stamatatos et al., 2000). Lexicalized tree-adjoining-grammars (LTAG) are poposed in (Joshi and Schabes, 1997) as a ruleset to construct and analyze grammar syntax by using partial subtrees. 6 Conclusion and Future Work In this paper we propose a new feature to enhance modern authorship attribution a</context>
</contexts>
<marker>Koppel, Schler, 2003</marker>
<rawString>Moshe Koppel and Jonathan Schler. 2003. Exploiting Stylistic Idiosyncrasies for Authorship Attribution. In IJCAI’03 Workshop On Computational Approaches To Style Analysis And Synthesis, pages 69– 72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="6670" citStr="Marcus et al., 1993" startWordPosition="1086" endWordPosition="1089">a single, large sample document, (2) split the resulting document into single sentences and calculate a syntax tree for each sentence, (3) calculate the pq-gram index for each tree, and (4) compose the final grammar profile from the normalized frequencies of pq-grams. At first the concatenated document is cleaned to contain alphanumeric characters and punctuation marks only, and then split into single sentences1. Each sentence is then parsed2. For example, Figure 1 depicts the syntax trees resulting from sentences (S1), (S2) and (S3). The labels of each tree correspond to a Penn Treebank tag (Marcus et al., 1993), where e.g NP corresponds to a noun phrase or JJS to a superlative adjective. In order to examine solely the structure of sentences, the terminal nodes (words) are ignored. Having computed a syntax tree for every sentence, the pq-gram index (Augsten et al., 2010) of each tree is calculated in the next step. Pq-grams consist of a stem (p) and a base (q) and may be related to as ”n-grams for trees”. Thereby p defines how much nodes are included vertically, and q defines the number of nodes to be considered horizontally. For example, a pq-gram using p = 2 and q = 3 starting from level two of tre</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for naive bayes text classification.</title>
<date>1998</date>
<contexts>
<context position="14236" citStr="McCallum and Nigam, 1998" startWordPosition="2347" endWordPosition="2350">dency as well as the easy computation. As a variation, word n-grams are used in (Balaguer, 2009) to detect plagiarism in text documents. Using individual features, machine learning algorithms are often applied to learn from author profiles and to predict unlabeled documents. Among methods that are utilized in authorship attribution as well as the related problem classes like text categorization or intrinsic plagiarism detection are support vector machines (e.g. (Sanderson and Guenter, 2006; Diederich et al., 2000)), neural networks (e.g. (Tweedie et al., 1996)), naive bayes classifiers (e.g. (McCallum and Nigam, 1998)) or decision trees (e.g. ( ¨O. Uzuner et. al, 2005)). Another interesting approach used in authorship attribution that tries to detect the writing style of authors by analyzing the occurences and variations of spelling errors is proposed in (Koppel and Schler, 2003). It is based on the assumption that authors tend to make similar spelling and/or grammar errors and therefore uses this information to attribute authors to unseen text documents. Approaches in the field of genre categorization also use NLP tools to analyze documents based on syntactic annotations (Stamatatos et al., 2000). Lexical</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for naive bayes text classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Mosteller</author>
<author>D Wallace</author>
</authors>
<title>Inference and Disputed Authorship: The Federalist.</title>
<date>1964</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="1554" citStr="Mosteller and Wallace, 1964" startWordPosition="227" endWordPosition="230">aluation using three different and independent data sets reveals promising results and indicate that the grammar of authors is a significant feature to enhance modern authorship attribution methods. 1 Introduction The increasing amount of documents available from sources like publicly available literary databases often raises the question of verifying disputed authorships or assigning authors to unlabeled text fragments. The original problem was initiated already in the midst of the twentieth century by Mosteller and Wallace, who tried to find the correct authorships of The Federalist Papers (Mosteller and Wallace, 1964), nonetheless authorship attribution is still a major research topic. Especially with latest events in politics and academia, the verification of authorships becomes increasingly important and is used frequently in areas like juridical applications (Forensic Linguistics) or cybercrime detection (Nirkhi and Dharaskar, 2013). Similarily to works in the field of plagiarism detection (e.g. (Stamatatos, 2009; Tschuggnall and Specht, 2013b)) which aim to find text fragments not written but claimed to be written by an author, the problem of traditional authorship attribution is defined as follows: Gi</context>
</contexts>
<marker>Mosteller, Wallace, 1964</marker>
<rawString>F. Mosteller and D. Wallace. 1964. Inference and Disputed Authorship: The Federalist. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Smita Nirkhi</author>
<author>RV Dharaskar</author>
</authors>
<title>Comparative study of authorship identification techniques for cyber forensics analysis.</title>
<date>2013</date>
<journal>International Journal.</journal>
<contexts>
<context position="1878" citStr="Nirkhi and Dharaskar, 2013" startWordPosition="271" endWordPosition="275">es the question of verifying disputed authorships or assigning authors to unlabeled text fragments. The original problem was initiated already in the midst of the twentieth century by Mosteller and Wallace, who tried to find the correct authorships of The Federalist Papers (Mosteller and Wallace, 1964), nonetheless authorship attribution is still a major research topic. Especially with latest events in politics and academia, the verification of authorships becomes increasingly important and is used frequently in areas like juridical applications (Forensic Linguistics) or cybercrime detection (Nirkhi and Dharaskar, 2013). Similarily to works in the field of plagiarism detection (e.g. (Stamatatos, 2009; Tschuggnall and Specht, 2013b)) which aim to find text fragments not written but claimed to be written by an author, the problem of traditional authorship attribution is defined as follows: Given several authors with text samples for each of them, the question is to label an unknown document with the correct author. In contrast to this socalled closed-class problem, an even harder task is addressed in the open-class problem, where additionally a ”none-of-them”-answer is allowed (Juola, 2006). In this paper we p</context>
</contexts>
<marker>Nirkhi, Dharaskar, 2013</marker>
<rawString>Smita Nirkhi and RV Dharaskar. 2013. Comparative study of authorship identification techniques for cyber forensics analysis. International Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>¨O Uzuner</author>
</authors>
<title>Using Syntactic Information to Identify Plagiarism.</title>
<date>2005</date>
<booktitle>In Proc. 2nd Workshop on Building Educational Applications using NLP.</booktitle>
<marker>Uzuner, 2005</marker>
<rawString>¨O. Uzuner et. al. 2005. Using Syntactic Information to Identify Plagiarism. In Proc. 2nd Workshop on Building Educational Applications using NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Conrad Sanderson</author>
<author>Simon Guenter</author>
</authors>
<title>Short text authorship attribution via sequence kernels, markov chains and author unmasking: an investigation.</title>
<date>2006</date>
<booktitle>In Proc. of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>482--491</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14105" citStr="Sanderson and Guenter, 2006" startWordPosition="2328" endWordPosition="2331">dies have shown that n-grams represent a significant feature to identify authors, whereby the major benefits are the language independency as well as the easy computation. As a variation, word n-grams are used in (Balaguer, 2009) to detect plagiarism in text documents. Using individual features, machine learning algorithms are often applied to learn from author profiles and to predict unlabeled documents. Among methods that are utilized in authorship attribution as well as the related problem classes like text categorization or intrinsic plagiarism detection are support vector machines (e.g. (Sanderson and Guenter, 2006; Diederich et al., 2000)), neural networks (e.g. (Tweedie et al., 1996)), naive bayes classifiers (e.g. (McCallum and Nigam, 1998)) or decision trees (e.g. ( ¨O. Uzuner et. al, 2005)). Another interesting approach used in authorship attribution that tries to detect the writing style of authors by analyzing the occurences and variations of spelling errors is proposed in (Koppel and Schler, 2003). It is based on the assumption that authors tend to make similar spelling and/or grammar errors and therefore uses this information to attribute authors to unseen text documents. Approaches in the fiel</context>
</contexts>
<marker>Sanderson, Guenter, 2006</marker>
<rawString>Conrad Sanderson and Simon Guenter. 2006. Short text authorship attribution via sequence kernels, markov chains and author unmasking: an investigation. In Proc. of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 482–491, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efstathios Stamatatos</author>
<author>George Kokkinakis</author>
<author>Nikos Fakotakis</author>
</authors>
<title>Automatic text categorization in terms of genre and author.</title>
<date>2000</date>
<journal>Comput. Linguist.,</journal>
<pages>26--471</pages>
<contexts>
<context position="14827" citStr="Stamatatos et al., 2000" startWordPosition="2441" endWordPosition="2444"> (e.g. (McCallum and Nigam, 1998)) or decision trees (e.g. ( ¨O. Uzuner et. al, 2005)). Another interesting approach used in authorship attribution that tries to detect the writing style of authors by analyzing the occurences and variations of spelling errors is proposed in (Koppel and Schler, 2003). It is based on the assumption that authors tend to make similar spelling and/or grammar errors and therefore uses this information to attribute authors to unseen text documents. Approaches in the field of genre categorization also use NLP tools to analyze documents based on syntactic annotations (Stamatatos et al., 2000). Lexicalized tree-adjoining-grammars (LTAG) are poposed in (Joshi and Schabes, 1997) as a ruleset to construct and analyze grammar syntax by using partial subtrees. 6 Conclusion and Future Work In this paper we propose a new feature to enhance modern authorship attribution algorithms by utilizing the grammar syntax of authors. To distinguish between authors, syntax trees of sentences are calculated which are split into parts by using pq-grams. The set of pq-grams is then stored in an author profile that is used to assign unseen documents to known authors. The algorithm has been optimized and </context>
</contexts>
<marker>Stamatatos, Kokkinakis, Fakotakis, 2000</marker>
<rawString>Efstathios Stamatatos, George Kokkinakis, and Nikos Fakotakis. 2000. Automatic text categorization in terms of genre and author. Comput. Linguist., 26:471–495, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efstathios Stamatatos</author>
</authors>
<title>Author identification using imbalanced and limited training texts.</title>
<date>2007</date>
<booktitle>In Database and Expert Systems Applications, 2007. DEXA’07. 18th International Workshop on,</booktitle>
<pages>237--241</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="9289" citStr="Stamatatos, 2007" startWordPosition="1528" endWordPosition="1529"> 3 Distance and Similarity Metrics With the use of the syntax tree profiles calculated for each candidate author as well as for the unlabeled document, the last part is to calculate a distance or similarity, respectively, for every author profile. Finally, the unseen document is simply labeled with the author of the best matching profile. To investigate on the best distance or similarity metric to be used for this approach, several metrics for this problem have been adapted and evaluated3: 1. CNG (Keˇselj et al., 2003), 2. Stamatatos-CNG (Stamatatos, 2009), 3. Stamatatos-CNG with Corpus Norm (Stamatatos, 2007), 4. Sentence-SPI. For the latter, we modified the original SPI score (Frantzeskou et al., 2006) so that each sentence is traversed separately: Let SD be the set of sentences of the document, I(s) the pq-gram-index of sentence s and Px the profile of author X, then the Sentence-SPI score is calculated as follows: 4 Evaluation The approach described in this paper has been extensively evaluated using three different English data sets, whereby all sets are completely unrelated and of different types: (1.) CC04: the training set used for the Ad-hoc-Authorship Attribution 3The algorithm names are o</context>
</contexts>
<marker>Stamatatos, 2007</marker>
<rawString>Efstathios Stamatatos. 2007. Author identification using imbalanced and limited training texts. In Database and Expert Systems Applications, 2007. DEXA’07. 18th International Workshop on, pages 237–241. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efstathios Stamatatos</author>
</authors>
<title>Intrinsic Plagiarism Detection Using Character n-gram Profiles.</title>
<date>2009</date>
<booktitle>In CLEF (Notebook Papers/Labs/Workshop).</booktitle>
<contexts>
<context position="1960" citStr="Stamatatos, 2009" startWordPosition="286" endWordPosition="287">ments. The original problem was initiated already in the midst of the twentieth century by Mosteller and Wallace, who tried to find the correct authorships of The Federalist Papers (Mosteller and Wallace, 1964), nonetheless authorship attribution is still a major research topic. Especially with latest events in politics and academia, the verification of authorships becomes increasingly important and is used frequently in areas like juridical applications (Forensic Linguistics) or cybercrime detection (Nirkhi and Dharaskar, 2013). Similarily to works in the field of plagiarism detection (e.g. (Stamatatos, 2009; Tschuggnall and Specht, 2013b)) which aim to find text fragments not written but claimed to be written by an author, the problem of traditional authorship attribution is defined as follows: Given several authors with text samples for each of them, the question is to label an unknown document with the correct author. In contrast to this socalled closed-class problem, an even harder task is addressed in the open-class problem, where additionally a ”none-of-them”-answer is allowed (Juola, 2006). In this paper we present a novel feature for the traditional, closed-class authorship attribution ta</context>
<context position="9234" citStr="Stamatatos, 2009" startWordPosition="1521" endWordPosition="1522">of pq-grams and their occurences in the given document. 3 Distance and Similarity Metrics With the use of the syntax tree profiles calculated for each candidate author as well as for the unlabeled document, the last part is to calculate a distance or similarity, respectively, for every author profile. Finally, the unseen document is simply labeled with the author of the best matching profile. To investigate on the best distance or similarity metric to be used for this approach, several metrics for this problem have been adapted and evaluated3: 1. CNG (Keˇselj et al., 2003), 2. Stamatatos-CNG (Stamatatos, 2009), 3. Stamatatos-CNG with Corpus Norm (Stamatatos, 2007), 4. Sentence-SPI. For the latter, we modified the original SPI score (Frantzeskou et al., 2006) so that each sentence is traversed separately: Let SD be the set of sentences of the document, I(s) the pq-gram-index of sentence s and Px the profile of author X, then the Sentence-SPI score is calculated as follows: 4 Evaluation The approach described in this paper has been extensively evaluated using three different English data sets, whereby all sets are completely unrelated and of different types: (1.) CC04: the training set used for the A</context>
<context position="13464" citStr="Stamatatos, 2009" startWordPosition="2231" endWordPosition="2232"> be a result of the fact that only the Sentence-SPI metric makes use of the additional parameters t, and to. Future work should also investigate on integrating these parameters also in other metrics. Moreover, results are better using the PAN12 data set for optimization, which may be because this set is the most hetergeneous one: The Federalist Papers contain only political essays written some time ago, and the CC04 set only uses literary texts written by four authors. 5 Related Work Successful current approaches often are based on or include character n-grams (e.g. (Hirst and Feiguina, 2007; Stamatatos, 2009)). Several studies have shown that n-grams represent a significant feature to identify authors, whereby the major benefits are the language independency as well as the easy computation. As a variation, word n-grams are used in (Balaguer, 2009) to detect plagiarism in text documents. Using individual features, machine learning algorithms are often applied to learn from author profiles and to predict unlabeled documents. Among methods that are utilized in authorship attribution as well as the related problem classes like text categorization or intrinsic plagiarism detection are support vector ma</context>
</contexts>
<marker>Stamatatos, 2009</marker>
<rawString>Efstathios Stamatatos. 2009. Intrinsic Plagiarism Detection Using Character n-gram Profiles. In CLEF (Notebook Papers/Labs/Workshop).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tschuggnall</author>
<author>G¨unther Specht</author>
</authors>
<title>Countering Plagiarism by Exposing Irregularities in Authors Grammars. In</title>
<date>2013</date>
<booktitle>EISIC, European Intelligence and Security Informatics Conference,</booktitle>
<pages>15--22</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1990" citStr="Tschuggnall and Specht, 2013" startWordPosition="288" endWordPosition="291">l problem was initiated already in the midst of the twentieth century by Mosteller and Wallace, who tried to find the correct authorships of The Federalist Papers (Mosteller and Wallace, 1964), nonetheless authorship attribution is still a major research topic. Especially with latest events in politics and academia, the verification of authorships becomes increasingly important and is used frequently in areas like juridical applications (Forensic Linguistics) or cybercrime detection (Nirkhi and Dharaskar, 2013). Similarily to works in the field of plagiarism detection (e.g. (Stamatatos, 2009; Tschuggnall and Specht, 2013b)) which aim to find text fragments not written but claimed to be written by an author, the problem of traditional authorship attribution is defined as follows: Given several authors with text samples for each of them, the question is to label an unknown document with the correct author. In contrast to this socalled closed-class problem, an even harder task is addressed in the open-class problem, where additionally a ”none-of-them”-answer is allowed (Juola, 2006). In this paper we present a novel feature for the traditional, closed-class authorship attribution task, following the assumption t</context>
<context position="4903" citStr="Tschuggnall and Specht, 2013" startWordPosition="796" endWordPosition="799"> VP CC (or) VP VP RB (not) S TO (to) VP S : S VP DT (that) RBZ (is) VBZ (is) RB VP NP VP S TO (to) VB (be) VP S NP VP S NP VP VP DT (The) NN (question) VBZ (is) SBAR S IN (whether) VP TO (to) NP VB (be) QP CC (or) RB (not) ing three different test sets is shown in Section 4, while finally Section 5 and Section 6 summarize related work and discuss future work, respectively. 2 Syntax Tree Profiles The basic idea of the approach is to utilize the syntax that is used by authors to distinguish authorships of text documents. Based on our previous work in the field of intrinsic plagiarism detection (Tschuggnall and Specht, 2013c; Tschuggnall and Specht, 2013a) we modify and enhance the algorithms and apply them to be used in closed-class authorship attribution. The number of choices an author has to formulate a sentence in terms of grammar is rather high, and the assumption in this approach is that the concrete choice is made mostly intuitively and unconsciously. Evaluations shown in Section 4 reinforce that solely parse tree structures represent a significant feature that can be used to distinguish between authors. From a global view the approach comprises the following three steps: (A) Creating a grammar profile f</context>
</contexts>
<marker>Tschuggnall, Specht, 2013</marker>
<rawString>Michael Tschuggnall and G¨unther Specht. 2013a. Countering Plagiarism by Exposing Irregularities in Authors Grammars. In EISIC, European Intelligence and Security Informatics Conference, Uppsala, Sweden, pages 15–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tschuggnall</author>
<author>G¨unther Specht</author>
</authors>
<title>Detecting Plagiarism in Text Documents through Grammar-Analysis of Authors.</title>
<date>2013</date>
<booktitle>In 15. GIFachtagung Datenbanksysteme f¨ur Business, Technologie und Web,</booktitle>
<location>Magdeburg, Germany.</location>
<contexts>
<context position="1990" citStr="Tschuggnall and Specht, 2013" startWordPosition="288" endWordPosition="291">l problem was initiated already in the midst of the twentieth century by Mosteller and Wallace, who tried to find the correct authorships of The Federalist Papers (Mosteller and Wallace, 1964), nonetheless authorship attribution is still a major research topic. Especially with latest events in politics and academia, the verification of authorships becomes increasingly important and is used frequently in areas like juridical applications (Forensic Linguistics) or cybercrime detection (Nirkhi and Dharaskar, 2013). Similarily to works in the field of plagiarism detection (e.g. (Stamatatos, 2009; Tschuggnall and Specht, 2013b)) which aim to find text fragments not written but claimed to be written by an author, the problem of traditional authorship attribution is defined as follows: Given several authors with text samples for each of them, the question is to label an unknown document with the correct author. In contrast to this socalled closed-class problem, an even harder task is addressed in the open-class problem, where additionally a ”none-of-them”-answer is allowed (Juola, 2006). In this paper we present a novel feature for the traditional, closed-class authorship attribution task, following the assumption t</context>
<context position="4903" citStr="Tschuggnall and Specht, 2013" startWordPosition="796" endWordPosition="799"> VP CC (or) VP VP RB (not) S TO (to) VP S : S VP DT (that) RBZ (is) VBZ (is) RB VP NP VP S TO (to) VB (be) VP S NP VP S NP VP VP DT (The) NN (question) VBZ (is) SBAR S IN (whether) VP TO (to) NP VB (be) QP CC (or) RB (not) ing three different test sets is shown in Section 4, while finally Section 5 and Section 6 summarize related work and discuss future work, respectively. 2 Syntax Tree Profiles The basic idea of the approach is to utilize the syntax that is used by authors to distinguish authorships of text documents. Based on our previous work in the field of intrinsic plagiarism detection (Tschuggnall and Specht, 2013c; Tschuggnall and Specht, 2013a) we modify and enhance the algorithms and apply them to be used in closed-class authorship attribution. The number of choices an author has to formulate a sentence in terms of grammar is rather high, and the assumption in this approach is that the concrete choice is made mostly intuitively and unconsciously. Evaluations shown in Section 4 reinforce that solely parse tree structures represent a significant feature that can be used to distinguish between authors. From a global view the approach comprises the following three steps: (A) Creating a grammar profile f</context>
</contexts>
<marker>Tschuggnall, Specht, 2013</marker>
<rawString>Michael Tschuggnall and G¨unther Specht. 2013b. Detecting Plagiarism in Text Documents through Grammar-Analysis of Authors. In 15. GIFachtagung Datenbanksysteme f¨ur Business, Technologie und Web, Magdeburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tschuggnall</author>
<author>G¨unther Specht</author>
</authors>
<title>Using grammar-profiles to intrinsically expose plagiarism in text documents.</title>
<date>2013</date>
<booktitle>In NLDB,</booktitle>
<pages>297--302</pages>
<contexts>
<context position="1990" citStr="Tschuggnall and Specht, 2013" startWordPosition="288" endWordPosition="291">l problem was initiated already in the midst of the twentieth century by Mosteller and Wallace, who tried to find the correct authorships of The Federalist Papers (Mosteller and Wallace, 1964), nonetheless authorship attribution is still a major research topic. Especially with latest events in politics and academia, the verification of authorships becomes increasingly important and is used frequently in areas like juridical applications (Forensic Linguistics) or cybercrime detection (Nirkhi and Dharaskar, 2013). Similarily to works in the field of plagiarism detection (e.g. (Stamatatos, 2009; Tschuggnall and Specht, 2013b)) which aim to find text fragments not written but claimed to be written by an author, the problem of traditional authorship attribution is defined as follows: Given several authors with text samples for each of them, the question is to label an unknown document with the correct author. In contrast to this socalled closed-class problem, an even harder task is addressed in the open-class problem, where additionally a ”none-of-them”-answer is allowed (Juola, 2006). In this paper we present a novel feature for the traditional, closed-class authorship attribution task, following the assumption t</context>
<context position="4903" citStr="Tschuggnall and Specht, 2013" startWordPosition="796" endWordPosition="799"> VP CC (or) VP VP RB (not) S TO (to) VP S : S VP DT (that) RBZ (is) VBZ (is) RB VP NP VP S TO (to) VB (be) VP S NP VP S NP VP VP DT (The) NN (question) VBZ (is) SBAR S IN (whether) VP TO (to) NP VB (be) QP CC (or) RB (not) ing three different test sets is shown in Section 4, while finally Section 5 and Section 6 summarize related work and discuss future work, respectively. 2 Syntax Tree Profiles The basic idea of the approach is to utilize the syntax that is used by authors to distinguish authorships of text documents. Based on our previous work in the field of intrinsic plagiarism detection (Tschuggnall and Specht, 2013c; Tschuggnall and Specht, 2013a) we modify and enhance the algorithms and apply them to be used in closed-class authorship attribution. The number of choices an author has to formulate a sentence in terms of grammar is rather high, and the assumption in this approach is that the concrete choice is made mostly intuitively and unconsciously. Evaluations shown in Section 4 reinforce that solely parse tree structures represent a significant feature that can be used to distinguish between authors. From a global view the approach comprises the following three steps: (A) Creating a grammar profile f</context>
</contexts>
<marker>Tschuggnall, Specht, 2013</marker>
<rawString>Michael Tschuggnall and G¨unther Specht. 2013c. Using grammar-profiles to intrinsically expose plagiarism in text documents. In NLDB, pages 297–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fiona J Tweedie</author>
<author>S Singh</author>
<author>David I Holmes</author>
</authors>
<title>Neural network applications in stylometry: The federalist papers. Computers and the Humanities,</title>
<date>1996</date>
<pages>30--1</pages>
<contexts>
<context position="14177" citStr="Tweedie et al., 1996" startWordPosition="2339" endWordPosition="2342">rs, whereby the major benefits are the language independency as well as the easy computation. As a variation, word n-grams are used in (Balaguer, 2009) to detect plagiarism in text documents. Using individual features, machine learning algorithms are often applied to learn from author profiles and to predict unlabeled documents. Among methods that are utilized in authorship attribution as well as the related problem classes like text categorization or intrinsic plagiarism detection are support vector machines (e.g. (Sanderson and Guenter, 2006; Diederich et al., 2000)), neural networks (e.g. (Tweedie et al., 1996)), naive bayes classifiers (e.g. (McCallum and Nigam, 1998)) or decision trees (e.g. ( ¨O. Uzuner et. al, 2005)). Another interesting approach used in authorship attribution that tries to detect the writing style of authors by analyzing the occurences and variations of spelling errors is proposed in (Koppel and Schler, 2003). It is based on the assumption that authors tend to make similar spelling and/or grammar errors and therefore uses this information to attribute authors to unseen text documents. Approaches in the field of genre categorization also use NLP tools to analyze documents based </context>
</contexts>
<marker>Tweedie, Singh, Holmes, 1996</marker>
<rawString>Fiona J. Tweedie, S. Singh, and David I. Holmes. 1996. Neural network applications in stylometry: The federalist papers. Computers and the Humanities, 30(1):1–10.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>