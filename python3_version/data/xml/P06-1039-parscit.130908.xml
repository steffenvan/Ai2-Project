<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009434">
<title confidence="0.898844">
Bayesian Query-Focused Summarization
</title>
<author confidence="0.9474">
Hal Daum´e III and Daniel Marcu
</author>
<affiliation confidence="0.707453">
Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
</affiliation>
<email confidence="0.997509">
me@hal3.name,marcu@isi.edu
</email>
<sectionHeader confidence="0.997368" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999785625">
We present BAYESUM (for “Bayesian
summarization”), a model for sentence ex-
traction in query-focused summarization.
BAYESUM leverages the common case in
which multiple documents are relevant to a
single query. Using these documents as re-
inforcement for query terms, BAYESUM is
not afflicted by the paucity of information
in short queries. We show that approxi-
mate inference in BAYESUM is possible
on large data sets and results in a state-
of-the-art summarization system. Further-
more, we show how BAYESUM can be
understood as a justified query expansion
technique in the language modeling for IR
framework.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980611111111">
We describe BAYESUM, an algorithm for perform-
ing query-focused summarization in the common
case that there are many relevant documents for a
given query. Given a query and a collection of rel-
evant documents, our algorithm functions by ask-
ing itself the following question: what is it about
these relevant documents that differentiates them
from the non-relevant documents? BAYESUM can
be seen as providing a statistical formulation of
this exact question.
The key requirement of BAYESUM is that mul-
tiple relevant documents are known for the query
in question. This is not a severe limitation. In two
well-studied problems, it is the de-facto standard.
In standard multidocument summarization (with
or without a query), we have access to known rel-
evant documents for some user need. Similarly, in
the case of a web-search application, an underly-
ing IR engine will retrieve multiple (presumably)
relevant documents for a given query. For both of
these tasks, BAYESUM performs well, even when
the underlying retrieval model is noisy.
The idea of leveraging known relevant docu-
ments is known as query expansion in the informa-
tion retrieval community, where it has been shown
to be successful in ad hoc retrieval tasks. Viewed
from the perspective of IR, our work can be inter-
preted in two ways. First, it can be seen as an ap-
plication of query expansion to the summarization
task (or, in IR terminology, passage retrieval); see
(Liu and Croft, 2002; Murdock and Croft, 2005).
Second, and more importantly, it can be seen as a
method for query expansion in a non-ad-hoc man-
ner. That is, BAYESUM is a statistically justified
query expansion method in the language modeling
for IR framework (Ponte and Croft, 1998).
</bodyText>
<sectionHeader confidence="0.9958305" genericHeader="method">
2 Bayesian Query-Focused
Summarization
</sectionHeader>
<bodyText confidence="0.999850444444444">
In this section, we describe our Bayesian query-
focused summarization model (BAYESUM). This
task is very similar to the standard ad-hoc IR task,
with the important distinction that we are compar-
ing query models against sentence models, rather
than against document models. The shortness of
sentences means that one must do a good job of
creating the query models.
To maintain generality, so that our model is ap-
plicable to any problem for which multiple rele-
vant documents are known for a query, we formu-
late our model in terms of relevance judgments.
For a collection of D documents and Q queries,
we assume we have a D x Q binary matrix r,
where rdQ = 1 if an only if document d is rele-
vant to query q. In multidocument summarization,
rdQ will be 1 exactly when d is in the document set
corresponding to query q; in search-engine sum-
</bodyText>
<page confidence="0.987531">
305
</page>
<note confidence="0.5349945">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 305–312,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.9939535">
marization, it will be 1 exactly when d is returned
by the search engine for query q.
</bodyText>
<subsectionHeader confidence="0.974419">
2.1 Language Modeling for IR
</subsectionHeader>
<bodyText confidence="0.999558130434783">
BAYESUM is built on the concept of language
models for information retrieval. The idea behind
the language modeling techniques used in IR is
to represent either queries or documents (or both)
as probability distributions, and then use stan-
dard probabilistic techniques for comparing them.
These probability distributions are almost always
“bag of words” distributions that assign a proba-
bility to words from a fixed vocabulary V.
One approach is to build a probability distri-
bution for a given document, pd(·), and to look
at the probability of a query under that distribu-
tion: pd(q). Documents are ranked according to
how likely they make the query (Ponte and Croft,
1998). Other researchers have built probability
distributions over queries pq(·) and ranked doc-
uments according to how likely they look under
the query model: pq(d) (Lafferty and Zhai, 2001).
A third approach builds a probability distribution
pq(·) for the query, a probability distribution pd(·)
for the document and then measures the similarity
between these two distributions using KL diver-
gence (Lavrenko et al., 2002):
</bodyText>
<equation confidence="0.99123">
pq(w) log pq(w) (1)
pd (w)
</equation>
<bodyText confidence="0.9995067">
The KL divergence between two probability
distributions is zero when they are identical and
otherwise strictly positive. It implicitly assumes
that both distributions pq and pd have the same
support: they assign non-zero probability to ex-
actly the same subset of V; in order to account
for this, the distributions pq and pd are smoothed
against a background general English model. This
final mode—the KL model—is the one on which
BAYESUM is based.
</bodyText>
<subsectionHeader confidence="0.99924">
2.2 Bayesian Statistical Model
</subsectionHeader>
<bodyText confidence="0.997479716666667">
In the language of information retrieval, the query-
focused sentence extraction task boils down to es-
timating a good query model, pq(·). Once we have
such a model, we could estimate sentence models
for each sentence in a relevant document, and rank
the sentences according to Eq (1).
The BAYESUM system is based on the follow-
ing model: we hypothesize that a sentence ap-
pears in a document because it is relevant to some
query, because it provides background informa-
tion about the document (but is not relevant to a
known query) or simply because it contains use-
less, general English filler. Similarly, we model
each word as appearing for one of those purposes.
More specifically, our model assumes that each
word can be assigned a discrete, exact source, such
as “this word is relevant to query q1” or “this word
is general English.” At the sentence level, how-
ever, sentences are assigned degrees: “this sen-
tence is 60% about query q1, 30% background
document information, and 10% general English.”
To model this, we define a general English
language model, pG(·) to capture the English
filler. Furthermore, for each document dk, we
define a background document language model,
pdk(·); similarly, for each query qj, we define
a query-specific language model pqj(·). Every
word in a document dk is modeled as being gen-
erated from a mixture of pG, pdk and {pqj :
query qj is relevant to document dk1. Supposing
there are J total queries and K total documents,
we say that the nth word from the sth sentence
in document d, wdsn, has a corresponding hidden
variable, zdsn that specifies exactly which of these
distributions is used to generate that one word. In
particular, zdsn is a vector of length 1 + J + K,
where exactly one element is 1 and the rest are 0.
At the sentence level, we introduce a second
layer of hidden variables. For the sth sentence in
document d, we let πds be a vector also of length
1 + J + K that represents our degree of belief
that this sentence came from any of the models.
The πdss lie in the J + K-dimensional simplex
ΔJ+K = 10 = (θ1, ... , θJ+K+1) : (Vi) θZ &gt;
0, EZ θZ = 11. The interpretation of the π vari-
ables is that if the “general English” component of
π is 0.9, then 90% of the words in this sentence
will be general English. The π and z variables are
constrained so that a sentence cannot be generated
by a document language model other than its own
document and cannot be generated by a query lan-
guage model for a query to which it is not relevant.
Since the πs are unknown, and it is unlikely that
there is a “true” correct value, we place a corpus-
level prior on them. Since π is a multinomial dis-
tribution over its corresponding zs, it is natural to
use a Dirichlet distribution as a prior over π. A
Dirichlet distribution is parameterized by a vector
α of equal length to the corresponding multino-
mial parameter, again with the positivity restric-
</bodyText>
<equation confidence="0.968906">
�
KL (pq pd) =
wEV
</equation>
<page confidence="0.992669">
306
</page>
<bodyText confidence="0.9974315">
tion, but no longer required to sum to one. It
has continuous density over a variable 01, ... , 0I
</bodyText>
<equation confidence="0.99591275">
given by: Dir(9  |α) = (Γ(az) nz 0α,_1�. The
first term is a normalization term that ensures that
f
ΔI d9 Dir(9  |α) = 1.
</equation>
<subsectionHeader confidence="0.991454">
2.3 Generative Story
</subsectionHeader>
<bodyText confidence="0.999893666666667">
The generative story for our model defines a distri-
bution over a corpus of queries, {qj}1:J, and doc-
uments, {dk}1:K, as follows:
</bodyText>
<listItem confidence="0.9998959">
1. For each query j = 1... J: Generate each
word qjn in qj by pqj(qjn)
2. For each document k = 1... K and each
sentence s in document k:
(a) Select the current sentence degree 7rks
by Dir(7rks  |α)rk(7rks)
(b) For each word wksn in sentence s:
• Select the word source zksn accord-
ing to Mult(z  |7rks)
• Generate the word wksn by
</listItem>
<equation confidence="0.987005666666667">
{ pG(wksn) if zksn = 0
pdk(wksn) if zksn = k + 1
pqj(wksn) if zksn = j + K + 1
</equation>
<bodyText confidence="0.996954909090909">
We used r to denote relevance judgments:
rk(7r) = 0 if any document component of 7r ex-
cept the one corresponding to k is non-zero, or if
any query component of 7r except those queries to
which document k is deemed relevant is non-zero
(this prevents a document using the “wrong” doc-
ument or query components). We have further as-
sumed that the z vector is laid out so that z0 cor-
responds to general English, zk+1 corresponds to
document dk for 0 &lt; j &lt; J and that zj+K+1 cor-
responds to query qj for 0 &lt; k &lt; K.
</bodyText>
<subsectionHeader confidence="0.9462">
2.4 Graphical Model
</subsectionHeader>
<bodyText confidence="0.998844230769231">
The graphical model corresponding to this gener-
ative story is in Figure 1. This model depicts the
four known parameters in square boxes (α, pQ, pD
and pG) with the three observed random variables
in shaded circles (the queries q, the relevance judg-
ments r and the words w) and two unobserved ran-
dom variables in empty circles (the word-level in-
dicator variables z and the sentence level degrees
7r). The rounded plates denote replication: there
are J queries and K documents, containing 5 sen-
tences in a given document and N words in a given
sentence. The joint probability over the observed
random variables is given in Eq (2):
</bodyText>
<figureCaption confidence="0.949835">
Figure 1: Graphical model for the Bayesian
Query-Focused Summarization Model.
</figureCaption>
<equation confidence="0.998499166666667">
�p (q1:J, r, d1:K) = H�
H pqj (qjn) X (2)
� H H &apos;n,
d7rks p (7rks  |α, r)
k s
�p (zksn  |7rks) p (wksn  |zksn)
</equation>
<bodyText confidence="0.99992525">
This expression computes the probability of the
data by integrating out the unknown variables. In
the case of the 7r variables, this is accomplished
by integrating over A, the multinomial simplex,
according to the prior distribution given by α. In
the case of the z variables, this is accomplished by
summing over all possible (discrete) values. The
final word probability is conditioned on the z value
by selecting the appropriate distribution from pG,
pD and pQ. Computing this expression and finding
optimal model parameters is intractable due to the
coupling of the variables under the integral.
</bodyText>
<sectionHeader confidence="0.928954" genericHeader="method">
3 Statistical Inference in BAYESUM
</sectionHeader>
<bodyText confidence="0.999952363636364">
Bayesian inference problems often give rise to in-
tractable integrals, and a large variety of tech-
niques have been proposed to deal with this. The
most popular are Markov Chain Monte Carlo
(MCMC), the Laplace (or saddle-point) approxi-
mation and the variational approximation. A third,
less common, but very effective technique, espe-
cially for dealing with mixture models, is expec-
tation propagation (Minka, 2001). In this paper,
we will focus on expectation propagation; exper-
iments not reported here have shown variational
</bodyText>
<figure confidence="0.991944210526316">
α
G
p
pQ
q
r
J
w
p
z
N
S
pD
K
j n
H
n
�
zksn
</figure>
<page confidence="0.984417">
307
</page>
<bodyText confidence="0.999061533333333">
EM to perform comparably but take roughly 50%
longer to converge.
Expectation propagation (EP) is an inference
technique introduced by Minka (2001) as a gener-
alization of both belief propagation and assumed
density filtering. In his thesis, Minka showed
that EP is very effective in mixture modeling
problems, and later demonstrated its superiority
to variational techniques in the Generative As-
pect Model (Minka and Lafferty, 2003). The key
idea is to compute an integral of a product of
terms by iteratively applying a sequence of “dele-
tion/inclusion” steps. Given an integral of the
form: fo dπ p(7r) �n tn(π), EP approximates
each term tn by a simpler term tn, giving Eq (3).
</bodyText>
<equation confidence="0.995697">
�� dπ q(π) q(π) = p(π) H
n in(π) (3)
</equation>
<bodyText confidence="0.983684">
In each deletion/inclusion step, one of the ap-
proximate terms is deleted from q(·), leaving
q−n(·) = q(·)/ in(·). A new approximation for
tn(·) is computed so that tn(·)q−n(·) has the same
integral, mean and variance as in(·)q−n(·). This
new approximation, in(·) is then included back
into the full expression for q(·) and the process re-
peats. This algorithm always has a fixed point and
there are methods for ensuring that the approxi-
mation remains in a location where the integral is
well-defined. Unlike variational EM, the approx-
imation given by EP is global, and often leads to
much more reliable estimates of the true integral.
In the case of our model, we follow Minka and
Lafferty (2003), who adapts latent Dirichlet allo-
cation of Blei et al. (2003) to EP. Due to space
constraints, we omit the inference algorithms and
instead direct the interested reader to the descrip-
tion given by Minka and Lafferty (2003).
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="method">
4 Search-Engine Experiments
</sectionHeader>
<bodyText confidence="0.9997995">
The first experiments we run are for query-focused
single document summarization, where relevant
documents are returned from a search engine, and
a short summary is desired of each document.
</bodyText>
<subsectionHeader confidence="0.950591">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.99994956097561">
The data we use to train and test BAYESUM
is drawn from the Text REtrieval Conference
(TREC) competitions. This data set consists of
queries, documents and relevance judgments, ex-
actly as required by our model. The queries are
typically broken down into four fields of increas-
ing length: the title (3-4 words), the summary (1
sentence), the narrative (2-4 sentences) and the
concepts (a list of keywords). Obviously, one
would expect that the longer the query, the better
a model would be able to do, and this is borne out
experimentally (Section 4.5).
Of the TREC data, we have trained our model
on 350 queries (queries numbered 51-350 and
401-450) and all corresponding relevant docu-
ments. This amounts to roughly 43k documents,
2.1m sentences and 65.8m words. The mean
number of relevant documents per query is 137
and the median is 81 (the most prolific query has
968 relevant documents). On the other hand, each
document is relevant to, on average, 1.11 queries
(the median is 5.5 and the most generally relevant
document is relevant to 20 different queries). In all
cases, we apply stemming using the Porter stem-
mer; for all other models, we remove stop words.
In order to evaluate our model, we had
seven human judges manually perform the query-
focused sentence extraction task. The judges were
supplied with the full TREC query and a single
document relevant to that query, and were asked to
select up to four sentences from the document that
best met the needs given by the query. Each judge
annotated 25 queries with some overlap to allow
for an evaluation of inter-annotator agreement,
yielding annotations for a total of 166 unique
query/document pairs. On the doubly annotated
data, we computed the inter-annotator agreement
using the kappa measure. The kappa value found
was 0.58, which is low, but not abysmal (also,
keep in mind that this is computed over only 25
of the 166 examples).
</bodyText>
<subsectionHeader confidence="0.977976">
4.2 Evaluation Criteria
</subsectionHeader>
<bodyText confidence="0.999871461538462">
Since there are differing numbers of sentences se-
lected per document by the human judges, one
cannot compute precision and recall; instead, we
opt for other standard IR performance measures.
We consider three related criteria: mean average
precision (MAP), mean reciprocal rank (MRR)
and precision at 2 (P@2). MAP is computed by
calculating precision at every sentence as ordered
by the system up until all relevant sentences are se-
lected and averaged. MRR is the reciprocal of the
rank of the first relevant sentence. P@2 is the pre-
cision computed at the first point that two relevant
sentences have been selected (in the rare case that
</bodyText>
<page confidence="0.99495">
308
</page>
<bodyText confidence="0.792645">
humans selected only one sentence, we use P@1).
</bodyText>
<subsectionHeader confidence="0.986403">
4.3 Baseline Models
</subsectionHeader>
<bodyText confidence="0.999933586956522">
As baselines, we consider four strawman models
and two state-of-the-art information retrieval mod-
els. The first strawman, RANDOM ranks sentences
randomly. The second strawman, POSITION,
ranks sentences according to their absolute posi-
tion (in the context of non-query-focused summa-
rization, this is an incredibly powerful baseline).
The third and fourth models are based on the vec-
tor space interpretation of IR. The third model,
JACCARD, uses standard Jaccard distance score
(intersection over union) between each sentence
and the query to rank sentences. The fourth, CO-
SINE, uses TF-IDF weighted cosine similarity.
The two state-of-the-art IR models used as com-
parative systems are based on the language mod-
eling framework described in Section 2.1. These
systems compute a language model for each query
and for each sentence in a document. Sentences
are then ranked according to the KL divergence
between the query model and the sentence model,
smoothed against a general model estimated from
the entire collection, as described in the case of
document retrieval by Lavrenko et al. (2002). This
is the first system we compare against, called KL.
The second true system, KL+REL is based on
augmenting the KL system with blind relevance
feedback (query expansion). Specifically, we first
run each query against the document set returned
by the relevance judgments and retrieve the top n
sentences. We then expand the query by interpo-
lating the original query model with a query model
estimated on these sentences. This serves as a
method of query expansion. We ran experiments
ranging n in 15, 10, 25,50, 1001 and the interpo-
lation parameter A in 10.2, 0.4, 0.6, 0.81 and used
oracle selection (on MRR) to choose the values
that performed best (the results are thus overly op-
timistic). These values were n = 25 and A = 0.4.
Of all the systems compared, only BAYESUM
and the KL+REL model use the relevance judg-
ments; however, they both have access to exactly
the same information. The other models only run
on the subset of the data used for evaluation (the
corpus language model for the KL system and the
IDF values for the COSINE model are computed
on the full data set). EP ran for 2.5 hours.
</bodyText>
<table confidence="0.999499">
MAP MRR P@2
RANDOM 19.9 37.3 16.6
POSITION 24.8 41.6 19.9
JACCARD 17.9 29.3 16.7
COSINE 29.6 50.3 23.7
KL 36.6 64.1 27.6
KL+REL 36.3 62.9 29.2
BAYESUM 44.1 70.8 33.6
</table>
<tableCaption confidence="0.883952">
Table 1: Empirical results for the baseline models
as well as BAYESUM, when all query fields are
used.
</tableCaption>
<subsectionHeader confidence="0.993045">
4.4 Performance on all Query Fields
</subsectionHeader>
<bodyText confidence="0.999643222222222">
Our first evaluation compares results when all
query fields are used (title, summary, description
and concepts1). These results are shown in Ta-
ble 1. As we can see from these results, the JAC-
CARD system alone is not sufficient to beat the
position-based baseline. The COSINE does beat
the position baseline by a bit of a margin (5 points
better in MAP, 9 points in MRR and 4 points in
P@2), and is in turn beaten by the KL system
(which is 7 points, 14 points and 4 points better
in MAP, MRR and P@2, respectively). Blind rel-
evance feedback (parameters of which were cho-
sen by an oracle to maximize the P@2 metric) ac-
tually hurts MAP and MRR performance by 0.3
and 1.2, respectively, and increases P@2 by 1.5.
Over the best performing baseline system (either
KL or KL+REL), BAYESUM wins by a margin of
7.5 points in MAP, 6.7 for MRR and 4.4 for P@2.
</bodyText>
<subsectionHeader confidence="0.997233">
4.5 Varying Query Fields
</subsectionHeader>
<bodyText confidence="0.978242529411765">
Our next experimental comparison has to do with
reducing the amount of information given in the
query. In Table 2, we show the performance
of the KL, KL-REL and BAYESUM systems, as
we use different query fields. There are several
things to notice in these results. First, the stan-
dard KL model without blind relevance feedback
performs worse than the position-based model
when only the 3-4 word title is available. Sec-
ond, BAYESUM using only the title outperform
the KL model with relevance feedback using all
fields. In fact, one can apply BAYESUM without
using any of the query fields; in this case, only the
relevance judgments are available to make sense
&apos;A reviewer pointed out that concepts were later removed
from TREC because they were “too good.” Section 4.5 con-
siders the case without the concepts field.
</bodyText>
<page confidence="0.99685">
309
</page>
<table confidence="0.999848666666667">
MAP MRR P@2
POSITION 24.8 41.6 19.9
Title KL 19.9 32.6 17.8
KL-Rel 31.9 53.8 26.1
BAYESUM 41.1 65.7 31.6
+Description KL 31.5 58.3 24.1
KL-Rel 32.6 55.0 26.2
BAYESUM 40.9 66.9 31.0
+Summary KL 31.6 56.9 23.8
KL-Rel 34.2 48.5 27.0
BAYESUM 42.0 67.8 31.8
+Concepts KL 36.7 64.2 27.6
KL-Rel 36.3 62.9 29.2
BAYESUM 44.1 70.8 33.6
No Query BAYESUM 39.4 64.7 30.4
</table>
<tableCaption confidence="0.81805">
Table 2: Empirical results for the position-based
model, the KL-based models and BAYESUM, with
different inputs.
</tableCaption>
<bodyText confidence="0.999974894736842">
of what the query might be. Even in this cir-
cumstance, BAYESUM achieves a MAP of 39.4,
an MRR of 64.7 and a P@2 of 30.4, still bet-
ter across the board than KL-REL with all query
fields. While initially this seems counterintuitive,
it is actually not so unreasonable: there is signifi-
cantly more information available in several hun-
dred positive relevance judgments than in a few
sentences. However, the simple blind relevance
feedback mechanism so popular in IR is unable to
adequately model this.
With the exception of the KL model without rel-
evance feedback, adding the description on top of
the title does not seem to make any difference for
any of the models (and, in fact, occasionally hurts
according to some metrics). Adding the summary
improves performance in most cases, but not sig-
nificantly. Adding concepts tends to improve re-
sults slightly more substantially than any other.
</bodyText>
<subsectionHeader confidence="0.996978">
4.6 Noisy Relevance Judgments
</subsectionHeader>
<bodyText confidence="0.996234142857143">
Our model hinges on the assumption that, for a
given query, we have access to a collection of
known relevant documents. In most real-world
cases, this assumption is violated. Even in multi-
document summarization as run in the DUC com-
petitions, the assumption of access to a collection
of documents all relevant to a user need is unreal-
istic. In the real world, we will have to deal with
document collections that “accidentally” contain
irrelevant documents. The experiments in this sec-
tion show that BAYESUM is comparatively robust.
For this experiment, we use the IR engine that
performed best in the TREC 1 evaluation: In-
query (Callan et al., 1992). We used the offi-
</bodyText>
<subsectionHeader confidence="0.568737">
R−precision of IR Engine
</subsectionHeader>
<bodyText confidence="0.957868444444444">
Figure 2: Performance with noisy relevance judg-
ments. The X-axis is the R-precision of the IR
engine and the Y-axis is the summarization per-
formance in MAP. Solid lines are BAYESUM, dot-
ted lines are KL-Rel. Blue/stars indicate title only,
red/circles indicated title+description+summary
and black/pluses indicate all fields.
cial TREC results of Inquery on the subset of
the TREC corpus we consider. The Inquery R-
precision on this task is 0.39 using title only, and
0.51 using all fields. In order to obtain curves
as the IR engine improves, we have linearly in-
terpolated the Inquery rankings with the true rel-
evance judgments. By tweaking the interpolation
parameter, we obtain an IR engine with improv-
ing performance, but with a reasonable bias. We
have run both BAYESUM and KL-Rel on the rel-
evance judgments obtained by this method for six
values of the interpolation parameter. The results
are shown in Figure 2.
As we can observe from the figure, the solid
lines (BAYESUM) are always above the dotted
lines (KL-Rel). Considering the KL-Rel results
alone, we can see that for a non-perfect IR engine,
it makes little difference what query fields we use
for the summarization task: they all obtain roughly
equal scores. This is because the performance in
KL-Rel is dominated by the performance of the IR
engine. Looking only at the BAYESUM results, we
can see a much stronger, and perhaps surprising
difference. For an imperfect IR system, it is better
to use only the title than to use the title, description
and summary for the summarization component.
We believe this is because the title is more on topic
than the other fields, which contain terms like “A
relevant document should describe ....” Never-
</bodyText>
<figure confidence="0.987195352941176">
0.4 0.5 0.6 0.7 0.8 0.9 1
Mean Average Precision of Sentence Extraction
44
42
40
38
36
34
32
30
28
KL−Rel (title only)
BayeSum (title only)
KL−Rel (title+desc+sum)
BayeSum (title+desc+sum)
KL−Rel (all fields)
BayeSum (all fields)
</figure>
<page confidence="0.993106">
310
</page>
<bodyText confidence="0.999804571428571">
theless, BAYESUM has a more upward trend than
KL-Rel, which indicates that improved IR will re-
sult in improved summarization for BAYESUM but
not for KL-Rel.
and sixth (depending on the Rouge parameters),
but was never statistically significantly worse than
the best performing systems.
</bodyText>
<sectionHeader confidence="0.998687" genericHeader="evaluation">
6 Discussion and Future Work
5 Multidocument Experiments
</sectionHeader>
<bodyText confidence="0.9999632">
We present two results using BAYESUM in the
multidocument summarization settings, based on
the official results from the Multilingual Summa-
rization Evaluation (MSE) and Document Under-
standing Conference (DUC) competitions in 2005.
</bodyText>
<subsectionHeader confidence="0.994088">
5.1 Performance at MSE 2005
</subsectionHeader>
<bodyText confidence="0.99999655">
We participated in the Multilingual Summariza-
tion Evaluation (MSE) workshop with a system
based on BAYESUM. The task for this competi-
tion was generic (no query) multidocument sum-
marization. Fortunately, not having a query is
not a hindrance to our model. To account for the
redundancy present in document collections, we
applied a greedy selection technique that selects
sentences central to the document cluster but far
from previously selected sentences (Daum´e III and
Marcu, 2005a). In MSE, our system performed
very well. According to the human “pyramid”
evaluation, our system came first with a score of
0.529; the next best score was 0.489. In the au-
tomatic “Basic Element” evaluation, our system
scored 0.0704 (with a 95% confidence interval of
[0.0429, 0.1057]), which was the third best score
on a site basis (out of 10 sites), and was not statis-
tically significantly different from the best system,
which scored 0.0981.
</bodyText>
<subsectionHeader confidence="0.998168">
5.2 Performance at DUC 2005
</subsectionHeader>
<bodyText confidence="0.999949435483871">
We also participated in the Document Understand-
ing Conference (DUC) competition. The chosen
task for DUC was query-focused multidocument
summarization. We entered a nearly identical sys-
tem to DUC as to MSE, with an additional rule-
based sentence compression component (Daum´e
III and Marcu, 2005b). Human evaluators consid-
ered both responsiveness (how well did the sum-
mary answer the query) and linguistic quality. Our
system achieved the highest responsiveness score
in the competition. We scored more poorly on the
linguistic quality evaluation, which (only 5 out of
about 30 systems performed worse); this is likely
due to the sentence compression we performed on
top of BAYESUM. On the automatic Rouge-based
evaluations, our system performed between third
In this paper we have described a model for au-
tomatically generating a query-focused summary,
when one has access to multiple relevance judg-
ments. Our Bayesian Query-Focused Summariza-
tion model (BAYESUM) consistently outperforms
contending, state of the art information retrieval
models, even when it is forced to work with sig-
nificantly less information (either in the complex-
ity of the query terms or the quality of relevance
judgments documents). When we applied our sys-
tem as a stand-alone summarization model in the
2005 MSE and DUC tasks, we achieved among
the highest scores in the evaluation metrics. The
primary weakness of the model is that it currently
only operates in a purely extractive setting.
One question that arises is: why does
BAYESUM so strongly outperform KL-Rel, given
that BAYESUM can be seen as Bayesian formalism
for relevance feedback (query expansion)? Both
models have access to exactly the same informa-
tion: the queries and the true relevance judgments.
This is especially interesting due to the fact that
the two relevance feedback parameters for KL-
Rel were chosen optimally in our experiments, yet
BAYESUM consistently won out. One explanation
for this performance win is that BAYESUM pro-
vides a separate weight for each word, for each
query. This gives it significantly more flexibility.
Doing something similar with ad-hoc query ex-
pansion techniques is difficult due to the enormous
number of parameters; see, for instance, (Buckley
and Salton, 1995).
One significant advantage of working in the
Bayesian statistical framework is that it gives us
a straightforward way to integrate other sources of
knowledge into our model in a coherent manner.
One could consider, for instance, to extend this
model to the multi-document setting, where one
would need to explicitly model redundancy across
documents. Alternatively, one could include user
models to account for novelty or user preferences
along the lines of Zhang et al. (2002).
Our model is similar in spirit to the random-
walk summarization model (Otterbacher et al.,
2005). However, our model has several advan-
tages over this technique. First, our model has
</bodyText>
<page confidence="0.995933">
311
</page>
<bodyText confidence="0.974404">
no tunable parameters: the random-walk method
has many (graph connectivity, various thresholds,
choice of similarity metrics, etc.). Moreover, since
our model is properly Bayesian, it is straightfor-
ward to extend it to model other aspects of the
problem, or to related problems. Doing so in a non
ad-hoc manner in the random-walk model would
be nearly impossible.
Another interesting avenue of future work is to
relax the bag-of-words assumption. Recent work
has shown, in related models, how this can be done
for moving from bag-of-words models to bag-of-
ngram models (Wallach, 2006); more interesting
than moving to ngrams would be to move to de-
pendency parse trees, which could likely be ac-
counted for in a similar fashion. One could also
potentially relax the assumption that the relevance
judgments are known, and attempt to integrate
them out as well, essentially simultaneously per-
forming IR and summarization.
Acknowledgments. We thank Dave Blei and Tom
Minka for discussions related to topic models, and to the
anonymous reviewers, whose comments have been of great
benefit. This work was partially supported by the National
Science Foundation, Grant IIS-0326276.
</bodyText>
<sectionHeader confidence="0.996054" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999776951612903">
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet allocation. Journal of Machine
Learning Research (JMLR), 3:993–1022, January.
Chris Buckley and Gerard Salton. 1995. Optimiza-
tion of relevance feedback weights. In Proceedings
of the Conference on Research and Developments in
Information Retrieval (SIGIR).
Jamie Callan, Bruce Croft, and Stephen Harding.
1992. The INQUERY retrieval system. In Pro-
ceedings of the 3rd International Conference on
Database and Expert Systems Applications.
Hal Daum´e III and Daniel Marcu. 2005a. Bayesian
multi-document summarization at MSE. In ACL
2005 Workshop on Intrinsic and Extrinsic Evalua-
tion Measures.
Hal Daum´e III and Daniel Marcu. 2005b. Bayesian
summarization at DUC and a suggestion for extrin-
sic evaluation. In Document Understanding Confer-
ence.
John Lafferty and ChengXiang Zhai. 2001. Document
language models, query models, and risk minimiza-
tion for information retrieval. In Proceedings of the
Conference on Research and Developments in Infor-
mation Retrieval (SIGIR).
Victor Lavrenko, M. Choquette, and Bruce Croft.
2002. Crosslingual relevance models. In Proceed-
ings of the Conference on Research and Develop-
ments in Information Retrieval (SIGIR).
Xiaoyong Liu and Bruce Croft. 2002. Passage re-
trieval based on language models. In Processing
of the Conference on Information and Knowledge
Management (CIKM).
Thomas Minka and John Lafferty. 2003. Expectation-
propagation for the generative aspect model. In Pro-
ceedings of the Converence on Uncertainty in Artifi-
cial Intelligence (UAI).
Thomas Minka. 2001. A family of algorithms for ap-
proximate Bayesian inference. Ph.D. thesis, Mas-
sachusetts Institute of Technology, Cambridge, MA.
Vanessa Murdock and Bruce Croft. 2005. A transla-
tion model for sentence retrieval. In Proceedings of
the Joint Conference on Human Language Technol-
ogy Conference and Empirical Methods in Natural
Language Processing (HLT/EMNLP), pages 684–
691.
Jahna Otterbacher, Gunes Erkan, and Dragomir R.
Radev. 2005. Using random walks for question-
focused sentence retrieval. In Proceedings of the
Joint Conference on Human Language Technology
Conference and Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP).
Jay M. Ponte and Bruce Croft. 1998. A language mod-
eling approach to information retrieval. In Proceed-
ings of the Conference on Research and Develop-
ments in Information Retrieval (SIGIR).
Hanna Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of the International Con-
ference on Machine Learning (ICML).
Yi Zhang, Jamie Callan, and Thomas Minka. 2002.
Novelty and redundancy detection in adaptive filter-
ing. In Proceedings of the Conference on Research
and Developments in Information Retrieval (SIGIR).
</reference>
<page confidence="0.99866">
312
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.551488">
<title confidence="0.999841">Bayesian Query-Focused Summarization</title>
<author confidence="0.767323">Daum´e Marcu</author>
<affiliation confidence="0.997817">Information Sciences Institute</affiliation>
<address confidence="0.990121">4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292</address>
<email confidence="0.990227">me@hal3.name,marcu@isi.edu</email>
<abstract confidence="0.983759058823529">present “Bayesian summarization”), a model for sentence extraction in query-focused summarization. the common case in which multiple documents are relevant to a single query. Using these documents as refor query terms, not afflicted by the paucity of information in short queries. We show that approxiinference in possible on large data sets and results in a stateof-the-art summarization system. Furtherwe show how be understood as a justified query expansion technique in the language modeling for IR framework.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>3--993</pages>
<contexts>
<context position="13018" citStr="Blei et al. (2003)" startWordPosition="2254" endWordPosition="2257">n(·) is computed so that tn(·)q−n(·) has the same integral, mean and variance as in(·)q−n(·). This new approximation, in(·) is then included back into the full expression for q(·) and the process repeats. This algorithm always has a fixed point and there are methods for ensuring that the approximation remains in a location where the integral is well-defined. Unlike variational EM, the approximation given by EP is global, and often leads to much more reliable estimates of the true integral. In the case of our model, we follow Minka and Lafferty (2003), who adapts latent Dirichlet allocation of Blei et al. (2003) to EP. Due to space constraints, we omit the inference algorithms and instead direct the interested reader to the description given by Minka and Lafferty (2003). 4 Search-Engine Experiments The first experiments we run are for query-focused single document summarization, where relevant documents are returned from a search engine, and a short summary is desired of each document. 4.1 Data The data we use to train and test BAYESUM is drawn from the Text REtrieval Conference (TREC) competitions. This data set consists of queries, documents and relevance judgments, exactly as required by our model</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research (JMLR), 3:993–1022, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Buckley</author>
<author>Gerard Salton</author>
</authors>
<title>Optimization of relevance feedback weights.</title>
<date>1995</date>
<booktitle>In Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).</booktitle>
<contexts>
<context position="28066" citStr="Buckley and Salton, 1995" startWordPosition="4753" endWordPosition="4756">ck (query expansion)? Both models have access to exactly the same information: the queries and the true relevance judgments. This is especially interesting due to the fact that the two relevance feedback parameters for KLRel were chosen optimally in our experiments, yet BAYESUM consistently won out. One explanation for this performance win is that BAYESUM provides a separate weight for each word, for each query. This gives it significantly more flexibility. Doing something similar with ad-hoc query expansion techniques is difficult due to the enormous number of parameters; see, for instance, (Buckley and Salton, 1995). One significant advantage of working in the Bayesian statistical framework is that it gives us a straightforward way to integrate other sources of knowledge into our model in a coherent manner. One could consider, for instance, to extend this model to the multi-document setting, where one would need to explicitly model redundancy across documents. Alternatively, one could include user models to account for novelty or user preferences along the lines of Zhang et al. (2002). Our model is similar in spirit to the randomwalk summarization model (Otterbacher et al., 2005). However, our model has </context>
</contexts>
<marker>Buckley, Salton, 1995</marker>
<rawString>Chris Buckley and Gerard Salton. 1995. Optimization of relevance feedback weights. In Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jamie Callan</author>
<author>Bruce Croft</author>
<author>Stephen Harding</author>
</authors>
<title>The INQUERY retrieval system.</title>
<date>1992</date>
<booktitle>In Proceedings of the 3rd International Conference on Database and Expert Systems Applications.</booktitle>
<contexts>
<context position="22276" citStr="Callan et al., 1992" startWordPosition="3824" endWordPosition="3827">ption that, for a given query, we have access to a collection of known relevant documents. In most real-world cases, this assumption is violated. Even in multidocument summarization as run in the DUC competitions, the assumption of access to a collection of documents all relevant to a user need is unrealistic. In the real world, we will have to deal with document collections that “accidentally” contain irrelevant documents. The experiments in this section show that BAYESUM is comparatively robust. For this experiment, we use the IR engine that performed best in the TREC 1 evaluation: Inquery (Callan et al., 1992). We used the offiR−precision of IR Engine Figure 2: Performance with noisy relevance judgments. The X-axis is the R-precision of the IR engine and the Y-axis is the summarization performance in MAP. Solid lines are BAYESUM, dotted lines are KL-Rel. Blue/stars indicate title only, red/circles indicated title+description+summary and black/pluses indicate all fields. cial TREC results of Inquery on the subset of the TREC corpus we consider. The Inquery Rprecision on this task is 0.39 using title only, and 0.51 using all fields. In order to obtain curves as the IR engine improves, we have linearl</context>
</contexts>
<marker>Callan, Croft, Harding, 1992</marker>
<rawString>Jamie Callan, Bruce Croft, and Stephen Harding. 1992. The INQUERY retrieval system. In Proceedings of the 3rd International Conference on Database and Expert Systems Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Bayesian multi-document summarization at MSE.</title>
<date>2005</date>
<booktitle>In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures.</booktitle>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2005a. Bayesian multi-document summarization at MSE. In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Bayesian summarization at DUC and a suggestion for extrinsic evaluation.</title>
<date>2005</date>
<booktitle>In Document Understanding Conference.</booktitle>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2005b. Bayesian summarization at DUC and a suggestion for extrinsic evaluation. In Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Document language models, query models, and risk minimization for information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).</booktitle>
<contexts>
<context position="4568" citStr="Lafferty and Zhai, 2001" startWordPosition="740" endWordPosition="743"> standard probabilistic techniques for comparing them. These probability distributions are almost always “bag of words” distributions that assign a probability to words from a fixed vocabulary V. One approach is to build a probability distribution for a given document, pd(·), and to look at the probability of a query under that distribution: pd(q). Documents are ranked according to how likely they make the query (Ponte and Croft, 1998). Other researchers have built probability distributions over queries pq(·) and ranked documents according to how likely they look under the query model: pq(d) (Lafferty and Zhai, 2001). A third approach builds a probability distribution pq(·) for the query, a probability distribution pd(·) for the document and then measures the similarity between these two distributions using KL divergence (Lavrenko et al., 2002): pq(w) log pq(w) (1) pd (w) The KL divergence between two probability distributions is zero when they are identical and otherwise strictly positive. It implicitly assumes that both distributions pq and pd have the same support: they assign non-zero probability to exactly the same subset of V; in order to account for this, the distributions pq and pd are smoothed ag</context>
</contexts>
<marker>Lafferty, Zhai, 2001</marker>
<rawString>John Lafferty and ChengXiang Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>M Choquette</author>
<author>Bruce Croft</author>
</authors>
<title>Crosslingual relevance models.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).</booktitle>
<contexts>
<context position="4800" citStr="Lavrenko et al., 2002" startWordPosition="775" endWordPosition="778"> distribution for a given document, pd(·), and to look at the probability of a query under that distribution: pd(q). Documents are ranked according to how likely they make the query (Ponte and Croft, 1998). Other researchers have built probability distributions over queries pq(·) and ranked documents according to how likely they look under the query model: pq(d) (Lafferty and Zhai, 2001). A third approach builds a probability distribution pq(·) for the query, a probability distribution pd(·) for the document and then measures the similarity between these two distributions using KL divergence (Lavrenko et al., 2002): pq(w) log pq(w) (1) pd (w) The KL divergence between two probability distributions is zero when they are identical and otherwise strictly positive. It implicitly assumes that both distributions pq and pd have the same support: they assign non-zero probability to exactly the same subset of V; in order to account for this, the distributions pq and pd are smoothed against a background general English model. This final mode—the KL model—is the one on which BAYESUM is based. 2.2 Bayesian Statistical Model In the language of information retrieval, the queryfocused sentence extraction task boils do</context>
<context position="17134" citStr="Lavrenko et al. (2002)" startWordPosition="2927" endWordPosition="2930">e score (intersection over union) between each sentence and the query to rank sentences. The fourth, COSINE, uses TF-IDF weighted cosine similarity. The two state-of-the-art IR models used as comparative systems are based on the language modeling framework described in Section 2.1. These systems compute a language model for each query and for each sentence in a document. Sentences are then ranked according to the KL divergence between the query model and the sentence model, smoothed against a general model estimated from the entire collection, as described in the case of document retrieval by Lavrenko et al. (2002). This is the first system we compare against, called KL. The second true system, KL+REL is based on augmenting the KL system with blind relevance feedback (query expansion). Specifically, we first run each query against the document set returned by the relevance judgments and retrieve the top n sentences. We then expand the query by interpolating the original query model with a query model estimated on these sentences. This serves as a method of query expansion. We ran experiments ranging n in 15, 10, 25,50, 1001 and the interpolation parameter A in 10.2, 0.4, 0.6, 0.81 and used oracle select</context>
</contexts>
<marker>Lavrenko, Choquette, Croft, 2002</marker>
<rawString>Victor Lavrenko, M. Choquette, and Bruce Croft. 2002. Crosslingual relevance models. In Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyong Liu</author>
<author>Bruce Croft</author>
</authors>
<title>Passage retrieval based on language models.</title>
<date>2002</date>
<booktitle>In Processing of the Conference on Information and Knowledge Management (CIKM).</booktitle>
<contexts>
<context position="2256" citStr="Liu and Croft, 2002" startWordPosition="355" endWordPosition="358">ication, an underlying IR engine will retrieve multiple (presumably) relevant documents for a given query. For both of these tasks, BAYESUM performs well, even when the underlying retrieval model is noisy. The idea of leveraging known relevant documents is known as query expansion in the information retrieval community, where it has been shown to be successful in ad hoc retrieval tasks. Viewed from the perspective of IR, our work can be interpreted in two ways. First, it can be seen as an application of query expansion to the summarization task (or, in IR terminology, passage retrieval); see (Liu and Croft, 2002; Murdock and Croft, 2005). Second, and more importantly, it can be seen as a method for query expansion in a non-ad-hoc manner. That is, BAYESUM is a statistically justified query expansion method in the language modeling for IR framework (Ponte and Croft, 1998). 2 Bayesian Query-Focused Summarization In this section, we describe our Bayesian queryfocused summarization model (BAYESUM). This task is very similar to the standard ad-hoc IR task, with the important distinction that we are comparing query models against sentence models, rather than against document models. The shortness of sentenc</context>
</contexts>
<marker>Liu, Croft, 2002</marker>
<rawString>Xiaoyong Liu and Bruce Croft. 2002. Passage retrieval based on language models. In Processing of the Conference on Information and Knowledge Management (CIKM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Minka</author>
<author>John Lafferty</author>
</authors>
<title>Expectationpropagation for the generative aspect model.</title>
<date>2003</date>
<booktitle>In Proceedings of the Converence on Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="11976" citStr="Minka and Lafferty, 2003" startWordPosition="2069" endWordPosition="2072">tation propagation (Minka, 2001). In this paper, we will focus on expectation propagation; experiments not reported here have shown variational α G p pQ q r J w p z N S pD K j n H n � zksn 307 EM to perform comparably but take roughly 50% longer to converge. Expectation propagation (EP) is an inference technique introduced by Minka (2001) as a generalization of both belief propagation and assumed density filtering. In his thesis, Minka showed that EP is very effective in mixture modeling problems, and later demonstrated its superiority to variational techniques in the Generative Aspect Model (Minka and Lafferty, 2003). The key idea is to compute an integral of a product of terms by iteratively applying a sequence of “deletion/inclusion” steps. Given an integral of the form: fo dπ p(7r) �n tn(π), EP approximates each term tn by a simpler term tn, giving Eq (3). �� dπ q(π) q(π) = p(π) H n in(π) (3) In each deletion/inclusion step, one of the approximate terms is deleted from q(·), leaving q−n(·) = q(·)/ in(·). A new approximation for tn(·) is computed so that tn(·)q−n(·) has the same integral, mean and variance as in(·)q−n(·). This new approximation, in(·) is then included back into the full expression for q</context>
</contexts>
<marker>Minka, Lafferty, 2003</marker>
<rawString>Thomas Minka and John Lafferty. 2003. Expectationpropagation for the generative aspect model. In Proceedings of the Converence on Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Minka</author>
</authors>
<title>A family of algorithms for approximate Bayesian inference.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology,</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="11383" citStr="Minka, 2001" startWordPosition="1967" endWordPosition="1968">stribution from pG, pD and pQ. Computing this expression and finding optimal model parameters is intractable due to the coupling of the variables under the integral. 3 Statistical Inference in BAYESUM Bayesian inference problems often give rise to intractable integrals, and a large variety of techniques have been proposed to deal with this. The most popular are Markov Chain Monte Carlo (MCMC), the Laplace (or saddle-point) approximation and the variational approximation. A third, less common, but very effective technique, especially for dealing with mixture models, is expectation propagation (Minka, 2001). In this paper, we will focus on expectation propagation; experiments not reported here have shown variational α G p pQ q r J w p z N S pD K j n H n � zksn 307 EM to perform comparably but take roughly 50% longer to converge. Expectation propagation (EP) is an inference technique introduced by Minka (2001) as a generalization of both belief propagation and assumed density filtering. In his thesis, Minka showed that EP is very effective in mixture modeling problems, and later demonstrated its superiority to variational techniques in the Generative Aspect Model (Minka and Lafferty, 2003). The k</context>
</contexts>
<marker>Minka, 2001</marker>
<rawString>Thomas Minka. 2001. A family of algorithms for approximate Bayesian inference. Ph.D. thesis, Massachusetts Institute of Technology, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Murdock</author>
<author>Bruce Croft</author>
</authors>
<title>A translation model for sentence retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technology Conference and Empirical Methods in Natural Language Processing (HLT/EMNLP),</booktitle>
<pages>684--691</pages>
<contexts>
<context position="2282" citStr="Murdock and Croft, 2005" startWordPosition="359" endWordPosition="362">g IR engine will retrieve multiple (presumably) relevant documents for a given query. For both of these tasks, BAYESUM performs well, even when the underlying retrieval model is noisy. The idea of leveraging known relevant documents is known as query expansion in the information retrieval community, where it has been shown to be successful in ad hoc retrieval tasks. Viewed from the perspective of IR, our work can be interpreted in two ways. First, it can be seen as an application of query expansion to the summarization task (or, in IR terminology, passage retrieval); see (Liu and Croft, 2002; Murdock and Croft, 2005). Second, and more importantly, it can be seen as a method for query expansion in a non-ad-hoc manner. That is, BAYESUM is a statistically justified query expansion method in the language modeling for IR framework (Ponte and Croft, 1998). 2 Bayesian Query-Focused Summarization In this section, we describe our Bayesian queryfocused summarization model (BAYESUM). This task is very similar to the standard ad-hoc IR task, with the important distinction that we are comparing query models against sentence models, rather than against document models. The shortness of sentences means that one must do </context>
</contexts>
<marker>Murdock, Croft, 2005</marker>
<rawString>Vanessa Murdock and Bruce Croft. 2005. A translation model for sentence retrieval. In Proceedings of the Joint Conference on Human Language Technology Conference and Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 684– 691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
<author>Gunes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Using random walks for questionfocused sentence retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technology Conference and Empirical Methods in Natural Language Processing (HLT/EMNLP).</booktitle>
<contexts>
<context position="28641" citStr="Otterbacher et al., 2005" startWordPosition="4844" endWordPosition="4847">ers; see, for instance, (Buckley and Salton, 1995). One significant advantage of working in the Bayesian statistical framework is that it gives us a straightforward way to integrate other sources of knowledge into our model in a coherent manner. One could consider, for instance, to extend this model to the multi-document setting, where one would need to explicitly model redundancy across documents. Alternatively, one could include user models to account for novelty or user preferences along the lines of Zhang et al. (2002). Our model is similar in spirit to the randomwalk summarization model (Otterbacher et al., 2005). However, our model has several advantages over this technique. First, our model has 311 no tunable parameters: the random-walk method has many (graph connectivity, various thresholds, choice of similarity metrics, etc.). Moreover, since our model is properly Bayesian, it is straightforward to extend it to model other aspects of the problem, or to related problems. Doing so in a non ad-hoc manner in the random-walk model would be nearly impossible. Another interesting avenue of future work is to relax the bag-of-words assumption. Recent work has shown, in related models, how this can be done </context>
</contexts>
<marker>Otterbacher, Erkan, Radev, 2005</marker>
<rawString>Jahna Otterbacher, Gunes Erkan, and Dragomir R. Radev. 2005. Using random walks for questionfocused sentence retrieval. In Proceedings of the Joint Conference on Human Language Technology Conference and Empirical Methods in Natural Language Processing (HLT/EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Ponte</author>
<author>Bruce Croft</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).</booktitle>
<contexts>
<context position="2519" citStr="Ponte and Croft, 1998" startWordPosition="399" endWordPosition="402">wn as query expansion in the information retrieval community, where it has been shown to be successful in ad hoc retrieval tasks. Viewed from the perspective of IR, our work can be interpreted in two ways. First, it can be seen as an application of query expansion to the summarization task (or, in IR terminology, passage retrieval); see (Liu and Croft, 2002; Murdock and Croft, 2005). Second, and more importantly, it can be seen as a method for query expansion in a non-ad-hoc manner. That is, BAYESUM is a statistically justified query expansion method in the language modeling for IR framework (Ponte and Croft, 1998). 2 Bayesian Query-Focused Summarization In this section, we describe our Bayesian queryfocused summarization model (BAYESUM). This task is very similar to the standard ad-hoc IR task, with the important distinction that we are comparing query models against sentence models, rather than against document models. The shortness of sentences means that one must do a good job of creating the query models. To maintain generality, so that our model is applicable to any problem for which multiple relevant documents are known for a query, we formulate our model in terms of relevance judgments. For a co</context>
<context position="4383" citStr="Ponte and Croft, 1998" startWordPosition="712" endWordPosition="715">for information retrieval. The idea behind the language modeling techniques used in IR is to represent either queries or documents (or both) as probability distributions, and then use standard probabilistic techniques for comparing them. These probability distributions are almost always “bag of words” distributions that assign a probability to words from a fixed vocabulary V. One approach is to build a probability distribution for a given document, pd(·), and to look at the probability of a query under that distribution: pd(q). Documents are ranked according to how likely they make the query (Ponte and Croft, 1998). Other researchers have built probability distributions over queries pq(·) and ranked documents according to how likely they look under the query model: pq(d) (Lafferty and Zhai, 2001). A third approach builds a probability distribution pq(·) for the query, a probability distribution pd(·) for the document and then measures the similarity between these two distributions using KL divergence (Lavrenko et al., 2002): pq(w) log pq(w) (1) pd (w) The KL divergence between two probability distributions is zero when they are identical and otherwise strictly positive. It implicitly assumes that both d</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>Jay M. Ponte and Bruce Croft. 1998. A language modeling approach to information retrieval. In Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
</authors>
<title>Topic modeling: beyond bagof-words.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="29313" citStr="Wallach, 2006" startWordPosition="4953" endWordPosition="4954">ique. First, our model has 311 no tunable parameters: the random-walk method has many (graph connectivity, various thresholds, choice of similarity metrics, etc.). Moreover, since our model is properly Bayesian, it is straightforward to extend it to model other aspects of the problem, or to related problems. Doing so in a non ad-hoc manner in the random-walk model would be nearly impossible. Another interesting avenue of future work is to relax the bag-of-words assumption. Recent work has shown, in related models, how this can be done for moving from bag-of-words models to bag-ofngram models (Wallach, 2006); more interesting than moving to ngrams would be to move to dependency parse trees, which could likely be accounted for in a similar fashion. One could also potentially relax the assumption that the relevance judgments are known, and attempt to integrate them out as well, essentially simultaneously performing IR and summarization. Acknowledgments. We thank Dave Blei and Tom Minka for discussions related to topic models, and to the anonymous reviewers, whose comments have been of great benefit. This work was partially supported by the National Science Foundation, Grant IIS-0326276. References</context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>Hanna Wallach. 2006. Topic modeling: beyond bagof-words. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Jamie Callan</author>
<author>Thomas Minka</author>
</authors>
<title>Novelty and redundancy detection in adaptive filtering.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).</booktitle>
<contexts>
<context position="28544" citStr="Zhang et al. (2002)" startWordPosition="4828" endWordPosition="4831">r with ad-hoc query expansion techniques is difficult due to the enormous number of parameters; see, for instance, (Buckley and Salton, 1995). One significant advantage of working in the Bayesian statistical framework is that it gives us a straightforward way to integrate other sources of knowledge into our model in a coherent manner. One could consider, for instance, to extend this model to the multi-document setting, where one would need to explicitly model redundancy across documents. Alternatively, one could include user models to account for novelty or user preferences along the lines of Zhang et al. (2002). Our model is similar in spirit to the randomwalk summarization model (Otterbacher et al., 2005). However, our model has several advantages over this technique. First, our model has 311 no tunable parameters: the random-walk method has many (graph connectivity, various thresholds, choice of similarity metrics, etc.). Moreover, since our model is properly Bayesian, it is straightforward to extend it to model other aspects of the problem, or to related problems. Doing so in a non ad-hoc manner in the random-walk model would be nearly impossible. Another interesting avenue of future work is to r</context>
</contexts>
<marker>Zhang, Callan, Minka, 2002</marker>
<rawString>Yi Zhang, Jamie Callan, and Thomas Minka. 2002. Novelty and redundancy detection in adaptive filtering. In Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>