<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997132">
A Unified Statistical Model for the Identification of English
BaseNP
</title>
<author confidence="0.996552">
Endong Xun
</author>
<affiliation confidence="0.994591">
Microsoft Research China
</affiliation>
<address confidence="0.9855225">
No. 49 Zhichun Road Haidian District
100080, China,
</address>
<email confidence="0.995842">
i-edxun@microsoft.com
</email>
<author confidence="0.995685">
Ming Zhou
</author>
<affiliation confidence="0.993458">
Microsoft Research China
</affiliation>
<address confidence="0.93508">
No. 49 Zhichun Road Haidian District
100080, China,
</address>
<email confidence="0.955861">
Mingzhou@microsoft.com
</email>
<sectionHeader confidence="0.99545" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999908894736842">
This paper presents a novel statistical
model for automatic identification of
English baseNP. It uses two steps: the N-
best Part-Of-Speech (POS) tagging and
baseNP identification given the N-best
POS-sequences. Unlike the other
approaches where the two steps are
separated, we integrate them into a unified
statistical framework. Our model also
integrates lexical information. Finally,
Viterbi algorithm is applied to make
global search in the entire sentence,
allowing us to obtain linear complexity for
the entire process. Compared with other
methods using the same testing set, our
approach achieves 92.3% in precision and
93.2% in recall. The result is comparable
with or better than the previously reported
results.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999369">
Finding simple and non-recursive base Noun
Phrase (baseNP) is an important subtask for
many natural language processing applications,
such as partial parsing, information retrieval and
machine translation. A baseNP is a simple noun
phrase that does not contain other noun phrase
recursively, for example, the elements within
[...] in the following example are baseNPs,
where NNS, IN VBG etc are part-of-speech tags
[as defined in M. Marcus 1993].
</bodyText>
<author confidence="0.39232">
Changning Huang
</author>
<affiliation confidence="0.472921">
Microsoft Research China
</affiliation>
<address confidence="0.5356125">
No. 49 Zhichun Road Haidian District
100080, China,
</address>
<email confidence="0.539862">
cnhuang@microsoft.com
</email>
<equation confidence="0.479409">
[Measures/NNS] of/IN [manufacturing/VBG
activity/NN] fell/VBD more/RBR than/IN
[the/DT overall/JJ measures/NNS] ./.
</equation>
<figureCaption confidence="0.9551045">
Figure 1: An example sentence with baseNP
brackets
</figureCaption>
<bodyText confidence="0.9926948">
A number of researchers have dealt with the
problem of baseNP identification (Church 1988;
Bourigault 1992; Voutilainen 1993; Justeson &amp;
Katz 1995). Recently some researchers have
made experiments with the same test corpus
extracted from the 20th section of the Penn
Treebank Wall Street Journal (Penn Treebank).
Ramshaw &amp; Markus (1998) applied transform-
based error-driven algorithm (Brill 1995) to
learn a set of transformation rules, and using
those rules to locally updates the bracket
positions. Argamon, Dagan &amp; Krymolowski
(1998) introduced a memory-based sequences
learning method, the training examples are
stored and generalization is performed at
application time by comparing subsequence of
the new text to positive and negative evidence.
Cardie &amp; Pierce (1998 1999) devised error
driven pruning approach trained on Penn
Treebank. It extracts baseNP rules from the
training corpus and prune some bad baseNP by
incremental training, and then apply the pruned
rules to identify baseNP through maximum
length matching (or dynamic program
algorithm).
Most of the prior work treats POS tagging and
baseNP identification as two separate
procedures. However, uncertainty is involved in
both steps. Using the result of the first step as if
they are certain will lead to more errors in the
second step. A better approach is to consider the
two steps together such that the final output
takes the uncertainty in both steps together. The
approaches proposed by Ramshaw &amp; Markus
and Cardie&amp;Pierce are deterministic and local,
while Argamon, Dagan &amp; Krymolowski
consider the problem globally and assigned a
score to each possible baseNP structures.
However, they did not consider any lexical
information.
This paper presents a novel statistical approach
to baseNP identification, which considers both
steps together within a unified statistical
framework. It also takes lexical information into
account. In addition, in order to make the best
choice for the entire sentence, Viterbi algorithm
is applied. Our tests with the Penn Treebank
showed that our integrated approach achieves
92.3% in precision and 93.2% in recall. The
result is comparable or better that the current
state of the art.
In the following sections, we will describe the
detail for the algorithm, parameter estimation
and search algorithms in section 2. The
experiment results are given in section 3. In
section 4 we make further analysis and
comparison. In the final section we give some
conclusions.
2 The statistical approach
In this section, we will describe the two-pass
statistical model, parameters training and Viterbi
algorithm for the search of the best sequences of
POS tagging and baseNP identification. Before
describing our algorithm, we introduce some
notations we will use
</bodyText>
<subsectionHeader confidence="0.960251">
2.1 Notation
</subsectionHeader>
<bodyText confidence="0.999195333333333">
Let us express an input sentence E as a word
sequence and a sequence of POS respectively as
follows:
</bodyText>
<equation confidence="0.99995">
E = w1 w2 ... wn−1 wn
T = t1 t2 ... tn−1 tn
</equation>
<bodyText confidence="0.9728582">
Where n is the number of words in the
sentence, ti is the POS tag of the word wi.
Given E, the result of the baseNP identification
is assumed to be a sequence, in which some
words are grouped into baseNP as follows
</bodyText>
<equation confidence="0.976858">
...wi [wi wi ... ]
wj wj
−1 + 1 +1
The corresponding tag sequence is as follows:
B t t t t t
= ... [ .. . ] ... ...
= t b t ... = n n n
...
i −1 i i j j
+ 1 + 1 i −1 , ij j+1 1 2 m
</equation>
<bodyText confidence="0.984328375">
In which bi ,j corresponds to the tag sequence of
a baseNP: [ti ti+1... tj ] . bi ,j may also be
thought of as a baseNP rule. Therefore B is a
sequence of both POS tags and baseNP rules.
Thus 1 ≤ m ≤ n, ni ∈ (POS tag set ∪ baseNP
rules set), This is the first expression of a
sentence with baseNP annotated. Sometime, we
also use the following equivalent form:
</bodyText>
<equation confidence="0.999406571428571">
. ( , ) ( , ) ( , ) ... ( , ) ( , )...
t bm t bm t bm t bm t bm = q q q
...
i − −
1 i 1 i i i+ +
1 i 1 j j j j
+1 + 1 1 2 n
</equation>
<bodyText confidence="0.994545833333333">
Where each POS tag ti is associated with its
positional information bmi with respect to
baseNPs. The positional information is one of
{F, I, E, O, S} . F, E and I mean respectively
that the word is the left boundary, right
boundary of a baseNP, or at another position
inside a baseNP. O means that the word is
outside a baseNP. S marks a single word
baseNP. This second expression is similar to that
used in [Marcus 1995].
For example, the two expressions of the example
given in Figure 1 are as follows:
</bodyText>
<figure confidence="0.548867666666667">
(a) B= [NNS] IN [VBG NN] VBD RBR IN [DT JJ NNS]
(b) Q=(NNS S) (IN O) (VBG F) (NN E) (VBD O) (RBR
O) (IN O) (DT F) (JJ I) (NNS E) (. O)
</figure>
<subsectionHeader confidence="0.998055">
2.2 An ‘integrated’ two-pass
</subsectionHeader>
<bodyText confidence="0.989114333333333">
procedure
The principle of our approach is as follows. The
most probable baseNP sequence *
</bodyText>
<equation confidence="0.9830822">
B may be
expressed generally as follows:
B* = arg max( (  |))
p B E
B
</equation>
<bodyText confidence="0.9149035">
We separate the whole procedure into two
passes, i.e.:
</bodyText>
<equation confidence="0.986214666666667">
arg max( (  |) (  |, ))
P T E × P B T E
B
</equation>
<bodyText confidence="0.999858333333333">
In order to reduce the search space and
computational complexity, we only consider the
N best POS tagging of E, i.e.
</bodyText>
<equation confidence="0.688604294117647">
− best) arg
= max( P(T
T=T TN
1 , ... ,
Therefore, we have:
(T
E)
(B
T,E)
x(P
|
×P
|
)
B T=T TN
, 1 ,...,
B* ≈ argm
</equation>
<figure confidence="0.9134263">
a
(3)
...
Q=..
B
* ≈
(1)
T(N
(2)
|))
</figure>
<equation confidence="0.932317388888889">
E
= arg max
(P(T  |E)× P(E  |B,T)× P(B)) (9)
))
)
P(B  |T, E) = P(B  |T) × P(E  |B, T
P E T
(  |)
P(B  |T, E) = P(E  |B, T) × P(T  |B) × P(B)
P(E  |T) × P(T)
i
i=1
f =
t
∏ p w t p t t t
(  |) (  |2 ,
×
i i i i− i−
</equation>
<bodyText confidence="0.9994205">
Correspondingly, the algorithm is composed of
two steps: determining the N-best POS tagging
using Equation (2). And then determining the
best baseNP sequence from those POS
sequences using Equation (3). One can see that
the two steps are integrated together, rather that
separated as in the other approaches. Let us now
examine the two steps more closely.
2.3 Determining the N best POS
sequences
The goal of the algorithm in the 1st pass is to
search for the N-best POS-sequences within the
</bodyText>
<equation confidence="0.986166">
n
P E T
(  |) ≈ ∏P(wi  |ti) (5)
H
</equation>
<bodyText confidence="0.9844832">
We then use a trigram model as an
approximation of P(T) , i.e.:
n
search space (POS lattice). According to Bayes’
Rule, we have
</bodyText>
<equation confidence="0.9994895">
P T E
(  |) =P(E)
</equation>
<bodyText confidence="0.6438935">
Since P(E) does not affect the maximizing
procedure of P(T  |E) , equation (2) becomes
</bodyText>
<equation confidence="0.909755689655172">
=argmax( (  |)) argmax( (  |) ( ))
P T E = P E T P T
×
TT T N
= 1 ,..., T T T N
= 1 ,...,
We now assume that the words in E are
independent. Thus
each search procedure, we have
n
PT B P ti t j b i j
(  |) ( , ... ,  |, ) 1. Therefore, equation
=∏ =
i=1
(3) becomes
B = argmax(P(T |E)× P(B  |T, E))
P(E  |T) × P(T)
T N best
( )
−
(4)
*
Finally we have
T N best
( − ) = argmax(P(T  |E))
B T T TN
, = 1,...,
using the independence assumption, we have
n
</equation>
<bodyText confidence="0.990162">
In Viterbi algorithm of N best search, P(wi  |ti)
is called lexical generation (or output)
probability, and P(ti  |ti−2, ti−1 ) is called
transition probability in Hidden Markov Model.
</bodyText>
<subsectionHeader confidence="0.600462">
2.3.1 Determining the baseNPs
</subsectionHeader>
<bodyText confidence="0.928433333333333">
As mentioned before, the goal of the 2nd pass is
to search the best baseNP-sequence given the N-
best POS-sequences.
Considering E ,T and B as random variables,
according to Bayes’ Rule, we have
Since P(B  |T) = P(T  |PB) × P(B) we have, T
( )
Because we search for the best baseNP sequence
for each possible POS-sequence of the given
</bodyText>
<equation confidence="0.930759789473684">
sentence E, so
P E T P T P E T const
(  |) × ( ) = ( ∩ ) = ,
Furthermore from the definition of B, during
m
P(B) (  |2, 1 )
≈ ∏ P ni ni ni
− (11)
−
i=1
Finally, we obtain
n
argmax( (  |) (  |, ) (  |,
P T E P w bm t
×∏ × ∏ P n n n
i i i i i − −
2 1
i
❑12❑
</equation>
<bodyText confidence="0.96821825">
To summarize, In the first step, Viterbi N-best
searching algorithm is applied in the POS
tagging procedure, It determines a path
probability ft for each POS sequence calculated
as follows:
i n
=1,
In the second step, for each possible POS
tagging result, Viterbi algorithm is applied again
to search for the best baseNP sequence. Every
baseNP sequence found in this pass is also
asssociated with a path probability
</bodyText>
<equation confidence="0.9100476">
n
fb =∏p(wi  |ti, bmi)× ∏p(ni  |ni−2,n
i
=1 i=
m
</equation>
<page confidence="0.804191">
1,
</page>
<bodyText confidence="0.843789333333333">
The integrated probability of a baseNP sequence
is determined by ft × fb
α , whereα is a
</bodyText>
<equation confidence="0.984527724137931">
B*
))
B T T T
, ,..
= 1 N
.
1)
i
.
−1)
P T
( ) ≈ ∏ P(ti  |ti−2, ti−1) (6)
i=1
T=T TN
1 ,...,
B,
T=T TN
1 , . . . ,
n
= argmax(∏P(wi  |ti) × P(ti  |ti−2, ti−1
,TN
i
P(E |B,T)≈∏P(wi  |ti,bm) (10)
i=1
With trigram approximation of P(B), we have:
T T
= 1
, . . .
=1
</equation>
<bodyText confidence="0.9798096">
normalization coefficient (a = 2 .4 in our
experiments). When we determine the best
baseNP sequence for the given sentenceE, we
also determine the best POS sequence of E ,
which corresponds to the best baseNP of E .
Now let us illustrate the whole process through
an example: “stock was down 9.1 points
yesterday morning.”. In the first pass, one of the
N-best POS tagging result of the sentence is: T =
NN VBD RB CD NNS NN NN. For this POS
sequence, the 2nd pass will try to determine the
baseNPs as shown in Figure 2. The details of
the path in the dash line are given in Figure 3, Its
probability calculated in the second pass is as
follows ((D is pseudo variable):
</bodyText>
<equation confidence="0.98305">
P(B  |T, E) = p(stock  |NN, S) x p(was  |VBD, O) x p(down  |RB, O) x p(NUMBER  |CD, B
x p(po int s  |NNS, E) x p( yesterday  |NN, B) x p(morning  |NN, E) x p(.  |. , O)
x p NN
([ ]  |(D, (D) x p(VBD  |(D, [NN]) x p(RB  |[NN], VBD) x p([CD NNS]  |VBD, RB)
x p([NN NN]  |RB, [CD NNS])x p(.  |[CD NNS] ,[NN NN])
</equation>
<figureCaption confidence="0.999755">
Figure 2: All possible brackets of &amp;quot;stock was down 9.1 points yesterday morning&amp;quot;
Figure 3: the transformed form of the path with dash line for the second pass processing
</figureCaption>
<bodyText confidence="0.762492">
)
</bodyText>
<subsectionHeader confidence="0.992735">
2.4 The statistical parameter
</subsectionHeader>
<bodyText confidence="0.98943725">
training
In this work, the training and testing data were
derived from the 25 sections of Penn Treebank.
We divided the whole Penn Treebank data into
two sections, one for training and the other for
testing.
As required in our statistical model, we have to
calculate the following four probabilities:
</bodyText>
<listItem confidence="0.969509">
(1) P(ti  |ti-2 , ti-1) , (2) P(wi  |ti) ,
(3) P(ni  |ni-2ni-1) and (4) P(wi  |ti , bmi) . The
</listItem>
<bodyText confidence="0.9207292">
first and the third parameters are trigrams of T
and B respectively. The second and the fourth
are lexical generation probabilities. Probabilities
(1) and (2) can be calculated from POS tagged
data with following formulae:
</bodyText>
<equation confidence="0.992140454545455">
count t t t
( )
i i i
− −
2 1
p t t t
(  |, ) = (13)
i i−2 i−1 ∑ count(ti−2ti−1tj )
j
count(wi with tag ti) p(wi  |ti) = (14)
count(ti )
</equation>
<bodyText confidence="0.999919710526316">
As each sentence in the training set has both
POS tags and baseNP boundary tags, it can be
converted to the two sequences as B (a) and Q
(b) described in the last section. Using these
sequences, parameters (3) and (4) can be
calculated, The calculation formulas are similar
with equations (13) and (14) respectively.
Before training trigram model (3), all possible
baseNP rules should be extracted from the
training corpus. For instance, the following three
sequences are among the baseNP rules extracted.
There are more than 6,000 baseNP rules in the
Penn Treebank. When training trigram model
(3), we treat those baseNP rules in two ways. (1)
Each baseNP rule is assigned a unique identifier
(UID). This means that the algorithm considers
the corresponding structure of each baseNP rule.
(2) All of those rules are assigned to the same
identifier (SID). In this case, those rules are
grouped into the same class. Nevertheless, the
identifiers of baseNP rules are still different
from the identifiers assigned to POS tags.
We used the approach of Katz (Katz.1987) for
parameter smoothing, and build a trigram model
to predict the probabilities of parameter (1) and
(3). In the case that unknown words are
encountered during baseNP identification, we
calculate parameter (2) and (4) in the following
way:
In the experiments, the training and testing sets
are derived from the 25 sections of Wall Street
Journal distributed with the Penn Treebank II,
and the definition of baseNP is the same as
Ramshaw’s, Table 1 summarizes the average
performance on both baseNP tagging and POS
tagging, each section of the whole Penn
Treebank was used as the testing data and the
other 24 sections as the training data, in this way
</bodyText>
<equation confidence="0.991186">
p(wi  |ti) = count(ti )
(16)
max ( ( )) 2
count t j
j
</equation>
<bodyText confidence="0.999715333333333">
Here, bmj indicates all possible baseNP labels
attached to ti , and tj is a POS tag guessed for
the unknown word wi.
</bodyText>
<sectionHeader confidence="0.976435" genericHeader="method">
3 Experiment result
</sectionHeader>
<bodyText confidence="0.999863">
We designed five experiments as shown in Table
1. “UID” and “SID” mean respectively that an
identifier is assigned to each baseNP rule or the
same identifier is assigned to all the baseNP
rules. “+1” and “+4” denote the number of beat
POS sequences retained in the first step. And
“UID+R” means the POS tagging result of the
given sentence is totally correct for the 2nd step.
This provides an ideal upper bound for the
system. The reason why we choose N=4 for the
N-best POS tagging can be explained in Figure
4, which shows how the precision of POS
tagging changes with the number N.
</bodyText>
<figure confidence="0.9945115">
97. 15
97. 10
97. 05
97. 00
96. 95
1 2 3 4 5 6
</figure>
<figureCaption confidence="0.9973855">
Figure 4: POS tagging precision with respect to
different number of N-best
</figureCaption>
<bodyText confidence="0.681137">
we have done the cross validation experiments
25 times.
</bodyText>
<figure confidence="0.985065545454545">
97. 45
count(bmi,ti) p w bm t =
(  |, )(15)
i i i 2
max(count(bmj , ti ))
j
97. 40
97. 35
97. 30
97. 25
97. 20
</figure>
<table confidence="0.998943666666667">
Precision Recall F-Measure P + R Precision
( baseNP %) ( baseNP %) ( baseNP %) (POS %)
2
( baseNP %)
UID+1 92.75 93.30 93.02 93.02 97.06
UID+4 92.80 93.33 93.07 93.06 97.02
SID+1 86.99 90.14 88.54 88.56 97.06
SID+4 86.99 90.16 88.55 88.58 97.13
UID+R 93.44 93.95 93.69 93.70 100
</table>
<tableCaption confidence="0.999368">
Table 1 The average performance of the five experiments
</tableCaption>
<figure confidence="0.9998018">
1 2 3 4 5 6
93. 60
93. 40
93. 20
93. 00
92. 80
92. 60
92. 40
92. 20
92. 00
91. 80
91. 60
UI DE1
UI DE4
UI DER
1 2 3 4 5 6
93. 00
92. 50
92. 00
91. 50
91. 00
90. 50
90. 00
89. 50
89. 00
88. 50
88. 00
UI DE1
UI DE4
UI DER
</figure>
<figureCaption confidence="0.998499">
Figure 5: Precision under different training sets
</figureCaption>
<bodyText confidence="0.555053">
and different POS tagging results
</bodyText>
<figure confidence="0.780824">
1 2 3 4 5 6
</figure>
<figureCaption confidence="0.938535636363636">
Figure 7: POS tagging precision under different
training sets
Figure 5 -7 summarize the outcomes of our
statistical model on various size of the training
data, x-coordinate denotes the size of the
training set, where &amp;quot;1&amp;quot; indicates that the training
set is from section 0-8th of Penn Treebank, &amp;quot;2&amp;quot;
corresponds to the corpus that add additional
three sections 9-11th into &amp;quot;1&amp;quot; and so on. In this
Figure 6: Recall under different training sets
and different POS tagging results
</figureCaption>
<bodyText confidence="0.932101708333333">
way the size of the training data becomes larger
and larger. In those cases the testing data is
always section 20 (which is excluded from the
training data).
From Figure 7, we learned that the POS tagging
and baseNP identification are influenced each
other. We conducted two experiments to study
whether the POS tagging process can make use
of baseNP information. One is UID+4, in which
the precision of POS tagging dropped slightly
with respect to the standard POS tagging with
Trigram Viterbi search. In the second
experiment SID+4, the precision of POS tagging
has increase slightly. This result shows that POS
tagging can benefit from baseNP information.
Whether or not the baseNP information can
improve the precision of POS tagging in our
approach is determined by the identifier
assignment of the baseNP rules when training
trigram model of P(ni  |ni−2 , ni−1) . In the
future, we will further study optimal baseNP
rules clustering to further improve the
performances of both baseNP identification and
POS tagging.
</bodyText>
<figure confidence="0.977032083333333">
Vi ter bi
UI DE4
SI DE4
97. 20
97. 15
97. 10
97. 05
97. 00
96. 95
96. 90
96. 85
96. 80
</figure>
<sectionHeader confidence="0.70483" genericHeader="method">
4 Comparison with other
approaches
</sectionHeader>
<bodyText confidence="0.999709846153846">
To our knowledge, three other approaches to
baseNP identification have been evaluated using
Penn Treebank-Ramshaw &amp; Marcus’s
transformation-based chunker, Argamon et al.’s
MBSL, and Cardie’s Treebank_lex in Table 2,
we give a comparison of our method with other
these three. In this experiment, we use the
testing data prepared by Ramshaw (available at
http://www.cs.biu.ac.il/~yuvalk/MBSL), the
training data is selected from the 24 sections of
Penn Treebank (excluding the section 20). We
can see that our method achieves better result
than the others
</bodyText>
<table confidence="0.93568125">
.
Transformation-Based Treebank_Lex MBSL Unified Statistical
(Training data: 200k)
Precision (%) 91.8 89.0 91.6 92.3
Recall (%) 92.3 90.9 91.6 93.2
F-Measure (%) 92.0 89.9 91.6 92.7
P + R 92.1 90.0 91.6 92.8
2
</table>
<tableCaption confidence="0.993364">
Table 2: The comparison of our statistical method with three other approaches
</tableCaption>
<table confidence="0.999835166666667">
Transforamtion-Based Treebank_Lex MBSL Unified Statistical
Unifying POS &amp; NO NO NO YES
baseNP
Lexical Information YES YES NO YES
Global Searching NO NO YES YES
Context YES NO YES YES
</table>
<tableCaption confidence="0.999928">
Table 3: The comparison of some characteristics of our statistical method with three other approaches
</tableCaption>
<bodyText confidence="0.99154392">
Table 3 summarizes some interesting aspects of
our approach and the three other methods. Our
statistical model unifies baseNP identification
and POS tagging through tracing N-best
sequences of POS tagging in the pass of baseNP
recognition, while other methods use POS
tagging as a pre-processing procedure. From
Table 1, if we reviewed 4 best output of POS
tagging, rather that only one, the F-measure of
baseNP identification is improved from 93.02 %
to 93.07%. After considering baseNP
information, the error ratio of POS tagging is
reduced by 2.4% (comparing SID+4 with
SID+1).
The transformation-based method (R&amp;M 95)
identifies baseNP within a local windows of
sentence by matching transformation rules.
Similarly to MBSL, the 2nd pass of our algorithm
traces all possible baseNP brackets, and makes
global decision through Viterbi searching. On
the other hand, unlike MSBL we take lexical
information into account. The experiments show
that lexical information is very helpful to
improve both precision and recall of baseNP
recognition. If we neglect the probability of
</bodyText>
<equation confidence="0.969456333333333">
n
P w; t; bm;
(  |, ) in the 2nd pass of our model,
</equation>
<bodyText confidence="0.999567307692308">
the precision/recall ratios are reduced to
90.0/92.4% from 92.3/93.2%. Cardie’s approach
to Treebank rule pruning may be regarded as the
special case of our statistical model, since the
maximum-matching algorithm of baseNP rules
is only a simplified processing version of our
statistical model. Compared with this rule
pruning method, all baseNP rules are kept in our
model. Therefore in principle we have less
likelihood of failing to recognize baseNP types
As to the complexity of algorithm, our approach
is determined by the Viterbi algorithm approach,
or O(n) , linear with the length.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.994697">
This paper presented a unified statistical model
to identify baseNP in English text. Compared
with other methods, our approach has following
characteristics:
∏=
; 1
</bodyText>
<listItem confidence="0.5859315">
(1) baseNP identification is implemented in two
related stages: N-best POS taggings are first
</listItem>
<bodyText confidence="0.957090230769231">
determined, then baseNPs are identified given
the N best POS-sequences. Unlike other
approaches that use POS tagging as pre-
processing, our approach is not dependant on
perfect POS-tagging, Moreover, we can apply
baseNP information to further increase the
precision of POS tagging can be improved.
These experiments triggered an interesting
future research challenge: how to cluster certain
baseNP rules into certain identifiers so as to
improve the precision of both baseNP and POS
tagging. This is one of our further research
topics.
</bodyText>
<listItem confidence="0.6895305">
(2) Our statistical model makes use of more
lexical information than other approaches. Every
word in the sentence is taken into account during
baseNP identification.
(3) Viterbi algorithm is applied to make global
search at the sentence level.
</listItem>
<bodyText confidence="0.9996824">
Experiment with the same testing data used by
the other methods showed that the precision is
92.3% and the recall is 93.2%. To our
knowledge, these results are comparable with or
better than all previously reported results.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999508289473684">
Eric Brill and Grace Ngai. (1999) Man vs. machine:
A case study in baseNP learning. In Proceedings of
the 18th International Conference on Computational
Linguistics, pp.65-72. ACL’99
S. Argamon, I. Dagan, and Y. Krymolowski (1998)
A memory-based approach to learning shallow
language patterns. In Proceedings of the 17th
International Conference on Computational
Linguistics, pp.67-73. COLING-ACL’98
Cardie and D. Pierce (1998) Error-driven pruning of
treebank grammas for baseNP identification. In
Proceedings of the 36th International Conference
on Computational Linguistics, pp.218-224.
COLING-ACL’98
Lance A. Ramshaw and Michael P. Marcus ( In
Press). Text chunking using transformation-based
learning. In Natural Language Processing Using
Very large Corpora. Kluwer. Originally appeared
in The second workshop on very large corpora
WVLC’95, pp.82-94.
Viterbi, A.J. (1967) Error bounds for convolution
codes and asymptotically optimum decoding
algorithm. IEEE Transactions on Information
Theory IT-13(2): pp.260-269, April, 1967
S.M. Katz.(1987) Estimation of probabilities from
sparse data for the language model component of
speech recognize. IEEE Transactions on Acoustics,
Speech and Signal Processing. Volume ASSP-35,
pp.400-401, March 1987
Church, Kenneth. (1988) A stochastic parts program
and noun phrase parser for unrestricted text. In
Proceedings of the Second Conference on Applied
Natural Language Processing, pages 136-143.
Association of Computational Linguistics.
M. Marcus, M. Marcinkiewicx, and B. Santorini
(1993) Building a large annotated corpus of
English: the Penn Treebank. Computational
Linguistics, 19(2): 313-330
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.810117">
<title confidence="0.9771655">A Unified Statistical Model for the Identification of English BaseNP</title>
<author confidence="0.959753">Endong Xun</author>
<affiliation confidence="0.998934">Microsoft Research China</affiliation>
<address confidence="0.9769425">No. 49 Zhichun Road Haidian District 100080, China,</address>
<email confidence="0.999866">i-edxun@microsoft.com</email>
<author confidence="0.999553">Ming Zhou</author>
<affiliation confidence="0.999873">Microsoft Research China</affiliation>
<address confidence="0.976264">No. 49 Zhichun Road Haidian District 100080, China,</address>
<email confidence="0.999283">Mingzhou@microsoft.com</email>
<abstract confidence="0.9982553">This paper presents a novel statistical model for automatic identification of English baseNP. It uses two steps: the Nbest Part-Of-Speech (POS) tagging and baseNP identification given the N-best POS-sequences. Unlike the other approaches where the two steps are separated, we integrate them into a unified statistical framework. Our model also integrates lexical information. Finally, Viterbi algorithm is applied to make global search in the entire sentence, allowing us to obtain linear complexity for the entire process. Compared with other methods using the same testing set, our approach achieves 92.3% in precision and 93.2% in recall. The result is comparable with or better than the previously reported results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Grace Ngai</author>
</authors>
<title>Man vs. machine: A case study in baseNP learning.</title>
<date>1999</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>65--72</pages>
<marker>Brill, Ngai, 1999</marker>
<rawString>Eric Brill and Grace Ngai. (1999) Man vs. machine: A case study in baseNP learning. In Proceedings of the 18th International Conference on Computational Linguistics, pp.65-72. ACL’99</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>I Dagan</author>
<author>Y Krymolowski</author>
</authors>
<title>A memory-based approach to learning shallow language patterns.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics,</booktitle>
<pages>67--73</pages>
<marker>Argamon, Dagan, Krymolowski, 1998</marker>
<rawString>S. Argamon, I. Dagan, and Y. Krymolowski (1998) A memory-based approach to learning shallow language patterns. In Proceedings of the 17th International Conference on Computational Linguistics, pp.67-73. COLING-ACL’98</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cardie</author>
<author>D Pierce</author>
</authors>
<title>Error-driven pruning of treebank grammas for baseNP identification.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th International Conference on Computational Linguistics,</booktitle>
<pages>218--224</pages>
<contexts>
<context position="2533" citStr="Cardie &amp; Pierce (1998" startWordPosition="360" endWordPosition="363">ecently some researchers have made experiments with the same test corpus extracted from the 20th section of the Penn Treebank Wall Street Journal (Penn Treebank). Ramshaw &amp; Markus (1998) applied transformbased error-driven algorithm (Brill 1995) to learn a set of transformation rules, and using those rules to locally updates the bracket positions. Argamon, Dagan &amp; Krymolowski (1998) introduced a memory-based sequences learning method, the training examples are stored and generalization is performed at application time by comparing subsequence of the new text to positive and negative evidence. Cardie &amp; Pierce (1998 1999) devised error driven pruning approach trained on Penn Treebank. It extracts baseNP rules from the training corpus and prune some bad baseNP by incremental training, and then apply the pruned rules to identify baseNP through maximum length matching (or dynamic program algorithm). Most of the prior work treats POS tagging and baseNP identification as two separate procedures. However, uncertainty is involved in both steps. Using the result of the first step as if they are certain will lead to more errors in the second step. A better approach is to consider the two steps together such that </context>
</contexts>
<marker>Cardie, Pierce, 1998</marker>
<rawString>Cardie and D. Pierce (1998) Error-driven pruning of treebank grammas for baseNP identification. In Proceedings of the 36th International Conference on Computational Linguistics, pp.218-224. COLING-ACL’98</rawString>
</citation>
<citation valid="false">
<authors>
<author>Lance A Ramshaw</author>
<author>Michael P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<booktitle>In Natural Language Processing Using Very large Corpora. Kluwer. Originally appeared in The second workshop on very large corpora WVLC’95,</booktitle>
<pages>82--94</pages>
<marker>Ramshaw, Marcus, </marker>
<rawString>Lance A. Ramshaw and Michael P. Marcus ( In Press). Text chunking using transformation-based learning. In Natural Language Processing Using Very large Corpora. Kluwer. Originally appeared in The second workshop on very large corpora WVLC’95, pp.82-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error bounds for convolution codes and asymptotically optimum decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory</journal>
<volume>13</volume>
<issue>2</issue>
<pages>260--269</pages>
<marker>Viterbi, 1967</marker>
<rawString>Viterbi, A.J. (1967) Error bounds for convolution codes and asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory IT-13(2): pp.260-269, April, 1967</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of speech recognize.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech</journal>
<booktitle>and Signal Processing. Volume ASSP-35,</booktitle>
<pages>400--401</pages>
<marker>Katz, 1987</marker>
<rawString>S.M. Katz.(1987) Estimation of probabilities from sparse data for the language model component of speech recognize. IEEE Transactions on Acoustics, Speech and Signal Processing. Volume ASSP-35, pp.400-401, March 1987</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="1851" citStr="Church 1988" startWordPosition="262" endWordPosition="263"> baseNP is a simple noun phrase that does not contain other noun phrase recursively, for example, the elements within [...] in the following example are baseNPs, where NNS, IN VBG etc are part-of-speech tags [as defined in M. Marcus 1993]. Changning Huang Microsoft Research China No. 49 Zhichun Road Haidian District 100080, China, cnhuang@microsoft.com [Measures/NNS] of/IN [manufacturing/VBG activity/NN] fell/VBD more/RBR than/IN [the/DT overall/JJ measures/NNS] ./. Figure 1: An example sentence with baseNP brackets A number of researchers have dealt with the problem of baseNP identification (Church 1988; Bourigault 1992; Voutilainen 1993; Justeson &amp; Katz 1995). Recently some researchers have made experiments with the same test corpus extracted from the 20th section of the Penn Treebank Wall Street Journal (Penn Treebank). Ramshaw &amp; Markus (1998) applied transformbased error-driven algorithm (Brill 1995) to learn a set of transformation rules, and using those rules to locally updates the bracket positions. Argamon, Dagan &amp; Krymolowski (1998) introduced a memory-based sequences learning method, the training examples are stored and generalization is performed at application time by comparing su</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, Kenneth. (1988) A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>M Marcinkiewicx</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<marker>Marcus, Marcinkiewicx, Santorini, 1993</marker>
<rawString>M. Marcus, M. Marcinkiewicx, and B. Santorini (1993) Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2): 313-330</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>