<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000293">
<title confidence="0.510463">
Obituary
</title>
<author confidence="0.716857">
Victor H. Yngve
W. John Hutchins*
</author>
<bodyText confidence="0.967920055555556">
Victor Yngve (5 July 1920 to 15 January 2012) was a major contributor in a number of
fields within computational linguistics: as the leading researcher in machine translation
(MT) at the Massachusetts Institute of Technology (MIT), as editor of its first journal,
as designer and developer of the first non-numerical programming language (COMIT),
and as an influential contributor to linguistic theory.
While still completing his Ph.D. on cosmic ray physics at the University of Chicago
during 1950–1953, Yngve had an idea for using the newly invented computers to
translate languages. He contemplated building a translation machine based on simple
dictionary lookup. At this time he knew nothing of the earlier speculations of Warren
Weaver and others (Hutchins 1997). Then during a visit to Claude Shannon at Bell
Telephone Laboratories in early 1952 he heard about a conference on machine trans-
lation to be held at MIT in June of that year. He attended the opening public meeting
and participated in conference discussions, and then, after Bar-Hillel’s departure from
MIT, he was appointed in July 1953 by Jerome Wiesner at the Research Laboratory for
Electronics (RLE) to lead the MT research effort there. (For a retrospective survey of his
MT research activities see Yngve [2000].)
Yngve, along with many others at the time, deprecated the premature publicity
around the Georgetown–IBM system demonstrated in January 1954. Yngve was ap-
palled to see research of such a limited nature reported in newspapers; his background
in physics required experiments to be carefully planned, with their assumptions made
plain, and properly tested and reviewed by other researchers. He was determined to
set the new field of MT on a proper scientific course. The first step was a journal for
the field, to be named Mechanical Translation—the field became “machine translation”
in later years. He found a collaborator for the journal in William N. Locke of the MIT
Modern Languages department. The aim was to provide a forum for information about
what research was going on in the form of abstracts, and then for peer-reviewed articles.
The first issue appeared in March 1954.
Yngve’s first experiments at MIT in October 1953 were an implementation of his
earlier ideas on word-for-word translation. The results of translating from German were
published in the collection edited by Locke and Booth (Yngve 1955b). One example of
output began:
Die CONVINCINGe CRITIQUE des CLASSICALen IDEA-OF-PROBABILITY IS eine
der REMARKABLEen WORKS des AUTHORs. Er HAS BOTHen LAWe der GREATen
NUMBERen ein DOUBLEes TO SHOWen: (1) wie sie IN seinem SYSTEM TO
INTERPRETen ARE, (2) THAT sie THROUGH THISe INTERPRETATION NOT den
CHARACTER von NOT-TRIVIALen DEMONSTRABLE PROPOSITIONen LOSen .. .
</bodyText>
<footnote confidence="0.87035">
* Previously at the University of East Anglia, Norwich, UK. E-mail: john@hutchinsweb.me.uk.
</footnote>
<note confidence="0.834496">
© 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.9997765">
It was obvious that the output was poor, but nevertheless it appeared to be good enough
for scientists with some knowledge of German grammar to read and extract relevant
information. Yngve concluded that word-for-word translation could be taken as a first
approximation. But a major problem was that many words have more than one meaning
out of context.
Erwin Reifler had suggested the use of pre-editors who would annotate texts be-
fore translation; and Victor Oswald proposed the use of microglossaries (dictionaries
limited to one specific narrow field) in order to reduce the number of homographs.
But Yngve believed that the problem of multiple meanings could be resolved with
syntactic analysis. Nonetheless, he rejected Bar-Hillel’s “operational syntax” put for-
ward at the 1952 conference (and later known as categorical syntax) as too complex for
long sentences and too remote from traditional grammatical distinctions. He had been
impressed by the work of Bloomfield and Fries, and had become convinced that lin-
guistics could provide stable and repeatable methods akin to the procedures of cosmic
ray physics. Therefore, over the following years, he appointed a number of linguists
to the RLE staff, starting with Noam Chomsky in 1955, in order to undertake basic
research. He was disappointed, however, that most of the linguists he hired were more
interested in pursuing their theoretical studies than in tackling the practical problems
of MT.
Yngve’s approach to syntactic analysis was to begin with the identification and
isolation of the different possible grammatical functions of words. The aim was to set
up mutually exclusive word-classes (i.e., one class for the noun function, another for
the verb function, and so on) The approach was to set up substitution frames to isolate
contexts. Thus walk may occur in the frame
</bodyText>
<listItem confidence="0.993374333333333">
(1) The was enjoyable
or in the frame
(2) They home every day.
</listItem>
<bodyText confidence="0.999907523809524">
Taking words from a corpus and testing in each frame produces a matrix of different
contexts (substitution frames) and the words occurring in those contexts. These words
form a word-class and their frames form context classes. As a result, each sentence
sequence of word-classes would determine a unique sequence of context classes. An
algorithm was proposed which searched left-to-right for the longest match for a se-
quence of word-classes. Sequences of word-classes formed phrases and clauses, so the
algorithm was capable of looking also for phrase sequences. On this basis, a table-driven
algorithm for syntactic analysis was designed and implemented (Yngve 1955b). This
demonstrated for the first time in MT the importance and practical value of separating
descriptive linguistic information (in this case, words and contexts) and language-
independent algorithmic procedures (searching and matching). The practice was widely
adopted by other groups in MT and in computational linguistics.
At the same time, work was going on at the RLE on investigations of coding and
switching theory and on Shannon’s information theory. Yngve decided to investigate
error correcting properties of language (Yngve 1954). Sequences of letters and words
are not random but constrained by codes specifying which sequences are legitimate.
Testing a text for deviations from randomness would help to reveal its structure. A first
step would be the location of all occurrences of a given frequent word and the deter-
mination of its effect on the occurrence of other frequent words in the neighborhood by
comparing their frequency of occurrence with what would be expected if they occurred
at random. For this investigation of what was to be called “gap analysis” (Yngve 1956)
</bodyText>
<page confidence="0.990168">
462
</page>
<bodyText confidence="0.985335862745099">
Hutchins Obituary
a computational experiment was conducted on a corpus of just 9,490 words. First, the
most frequent words were identified: the (599 instances), to (252), of (241), a (221), and
(207), and in (162). Then, the gaps between these words were determined (in numbers
of intervening words). In the case of of and the the gaps were one (the of ), two
(the of ), three (the of ), and so on, with frequencies of 72, 31, and
6, respectively. These results pointed to syntactic constraints on constructions with of
and the. Further results indicated that “structures with the can be expected to have two
or three words, and constructions with and frequently involve at least fifteen words.”
Similar observations were that “of is different from to in that it frequently follows a or the
with a gap of one or two.” And so forth, in a procedure which has now become familiar
in statistics-based computational linguistics. Yngve was a pioneer. Unfortunately, at the
time, these encouraging results could not be pursued because of the lack of large enough
corpora in machine-readable form.
The parallels between coding theory and linguistics suggested a two-stage model
for machine translation, where speakers encode a message that is decoded by a recipient
(Yngve 1955a). Between encoding and decoding there would be some representation
of the ‘message’ to be preserved in translation. Further consideration recognized that
structural representations of input would not be the same as those required for out-
put, however. Attention, therefore, focused on the transition stage where the meaning
or message of the input language would be expressed and where choices would be
made for producing output sentence structure. Thus the MIT group saw the need for
a three-stage model: syntactic analysis, structure transfer, and synthesis—a model for
many transfer-based MT systems in subsequent years. Yngve’s model was, however,
purely syntactic; no attempt was made at this time to include semantic information
(Yngve 1957).
At the same time as the development of the transfer model, Yngve and his col-
leagues tackled the problem of how linguists could be productive in MT research. They
decided that what was needed was a programming system that accepted notations
of a kind familiar to linguists, rather than systems such as FORTRAN and COBOL
designed for mathematicians and business applications. Thus, in collaboration with the
MIT Computation Center, they developed the first programming language designed
specifically for string-handling and pattern-matching, COMIT (Yngve 1961a, 1967). It
was ready for testing in late 1957—thus antedating by two years LISP, another program-
ming language devised for linguistics applications. COMIT was later used as a basis for
the SNOBOL language.
The availability of COMIT meant that the MIT group could proceed further with the
three-stage model in the development of an algorithm for sentence production. Initially
the generative grammar of Chomsky seemed to be an ideal model. What was required in
MT, however, was not the generation of all grammatical sentences from a given source
but particular specifiable sentences in context. The rules of a grammar derived from
a simple children’s story were written in COMIT in 1959 and tested in a program of
random sentence production (Yngve 1961b). Its main objective, in which it succeeded,
was to test the validity of the grammar rules, particularly rules for discontinuous
constructions and for coordination.
One outcome of programming the sentence-production algorithm with COMIT was
the “depth hypothesis,” for which Yngve is probably now best known (Yngve 1960a)
both in linguistics and in computational linguistics. The transformational approach had
already been rejected because it required too much storage space. The next question was
how much storage (push-down store) was needed for discontinuous (left-branching)
expansion and for regular (right-branching) expansion. It was clear that right expansion
</bodyText>
<page confidence="0.997656">
463
</page>
<note confidence="0.486595">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.99995044">
(or “progressive” application) was potentially infinite: the dog that worried the cat that
killed the rat that ate the malt .... On the other hand, left expansion (or ‘regressive’ appli-
cation) was limited: the malt that the rat that the cat that the dog worried killed ate. Yngve
calculated the maximum amount of extra temporary memory required to produce a
sentence (i.e., the number of unexpanded constituents at any one time [its depth]). He
found that in practice even very long sentences seldom had a depth of more than two or
three. Sentences with depths of more than three, such as the “regressive” constructions,
were often considered ungrammatical and/or unintelligible. Yngve noted the relation-
ship of this linguistic feature to the restrictions on immediate memory and processing
identified by Miller (1956). Most languages include mechanisms for restricting the depth
of regressive constructions, such as agglutination and compounding.
The depth hypothesis accounted for and predicted many syntactic features of En-
glish, including its historical changes, and also appeared to account for many features
in other languages. Yngve recognized that it arose from MT research and not from
linguistic theory, however. It was a hypothesis that needed to be tested against empirical
evidence. Its significance for linguistics was widely recognized from the beginning, but
it did not conform to the preconceptions of Chomskyan theory. Although Chomsky
had championed the rigorous statement of theory and its strict application to linguistic
material without ad hoc adjustments (Chomsky 1957, page 5), he regarded the depth
hypothesis not as a testable scientific hypothesis but as a rival linguistic theory. For
Yngve this attitude was unscientific.
Throughout his time at MIT Yngve stressed the need for a “basic, long-range
approach to translation” and not to look for “short-cut methods that might yield par-
tially adequate translations at an early date” (Yngve 1960b, page 183). No working MT
system emerged from MIT, therefore, but the quality of the research is incontestable.
Apart from Yngve’s own contributions to many aspects of syntactic analysis (the three-
stage transfer model, the depth hypothesis, and not least computer programming), his
colleagues also made significant contributions in a variety of areas: grammar theory
(Gilbert Harman and Kenneth Knowlton), semantics (Elinor Charney), logic and lan-
guage (Jared Darlington), transfer and interlingua (Edward Klima), computer storage
(William S. Cooper), Arabic translation (Arnold Satterthwait), and French translation
(David Dinneen). Citations for these contributions will be found in Yngve’s compre-
hensive survey of MT research at MIT (Yngve 1967; see also Yngve 2000).
By 1964, Yngve had come to the conclusion that “work in mechanical translation
has come up against what we will call the semantic barrier ... we will only have ade-
quate mechanical translations when the machine can ‘understand’ what it is translating
and this will be a very difficult task indeed” (Yngve 1964, page 279). Understanding
involved the background knowledge that people bring to the comprehension of com-
munication. He could see no solutions coming from linguistics. For many years, Yngve
had grown increasingly doubtful about the health of linguistic science and consequently
about the feasibility of good quality MT in general.
By 1965, funding for the MIT research group had ceased—perhaps in anticipation
of the ALPAC report which had a major impact on the funding of all US research
groups—and in that year Yngve went back to the University of Chicago as head of
the Department of Library Sciences, Linguistics, and Behavioral Sciences.
By this time, the journal Mechanical Translation, which he had founded in 1954, was
coming to an end. The aim had been to provide a public record of achievement in
MT. His ambitions for the journal could not be fulfilled, however, for various reasons.
There were relatively few outstanding contributions submitted for publication; many
MT researchers were funded by government bodies, which required the submission of
</bodyText>
<page confidence="0.997764">
464
</page>
<note confidence="0.314727">
Hutchins Obituary
</note>
<bodyText confidence="0.999959782608696">
fairly frequent reports, and these reports were distributed widely to other researchers
and public institutions. Researchers believed they had fulfilled their duties to publicize
research, and did not see the need to submit shorter articles to a peer-reviewed journal
which would probably not appear for several months. In addition the journal could not
survive solely on subscription charges, and authors were asked to contribute a page
charge towards publication costs, which they were reluctant to do.
In June 1962, the Association for Machine Translation and Computational Linguis-
tics was founded, with Yngve as its first president. The inclusion of ‘computational
linguistics’ in the title was indicative of the ever-expanding range of activities in the
field of natural language processing; MT was only part, and a diminishing proportion.
The Association took over Yngve’s journal with a changed title, Mechanical Translation
and Computational Linguistics, and Yngve remaining as editor. Even with the inclusion of
many more articles in computational linguistics and significantly fewer in MT, however,
the journal became ever more irregular and it was wound up in 1970. In 1968 machine
translation had already been dropped from the Association’s title.1
From this time on, Yngve turned away from his MT interests to questions of lin-
guistic theory that had been his increasing concern since the publication of his “depth
hypothesis.” From 1965, Yngve published a series of papers devoted to the founda-
tions of linguistic theory, many at the conferences of LACUS (Linguistic Association of
Canada and the United States). Just as when he had founded the Mechanical Translation
journal in 1954, he was determined to put studies of language on a sound scientific
footing. His recurrent theme was the unscientific nature of current linguistics. In this
period, he set forth the framework of what he called “human linguistics” (later “hard-
science linguistics”) where the units being analyzed are not the traditional properties
of sentences, verbs, noun phrases, gender, tense, phonemes, and so on, which, as he
demonstrated, are unfounded assumptions derived often from Greek philosophical
speculations. Instead the basic participants are communicating individuals and physical
observable “props” (abstractions of relevant physical objects, clocks, doors), “channels”
of communication (sound waves, writing, signs), and “settings” (relevant aspects of
the physical environment, ticket counters, rooms). Yngve’s first attempt at summa-
rizing and formulating his theory was published in his book, Linguistics as Science
(Yngve 1986).
Expansion and elaboration of his theoretical standpoint followed in further papers
and were brought together in From Grammar to Science in 1996. The opening chapters are
a cumulative rejection of all traditional and contemporary linguistics and philosophy
of language (from the Greeks to Saussure, Bloomfield, Fries, Harris, Chomsky, and
many others), including a rejection of his own widely accepted “depth hypothesis.”
The basic contention is that fundamental concepts of linguistics are intuitions and not
based on the observable behavior of people communicating. Yngve describes a detailed
comprehensive program for a new foundation of linguistics in which “we abandon
the unsupported assumptions of signs, words, meanings, and sentences” and move
completely into “the world of sound waves and the people who speak and understand
where we can test our theories and hypotheses against observations of the physical
reality all the way down to the basic assumptions of all science” (Yngve 1996, page 308).
He had not lost sight of computational treatments, and included (chapters 19 and 20)
an implementable notation for representing and testing hypotheses.
</bodyText>
<footnote confidence="0.374772">
1 For Yngve’s reflections on these changes in MT and computational linguistics see Yngve (1982).
</footnote>
<page confidence="0.99737">
465
</page>
<note confidence="0.594131">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.999889714285714">
Yngve attracted the support and collaboration of a growing number of scholars
sympathetic to his goal of a “hard-science linguistics,” and in 2004 a volume of essays
appeared dealing with a wide range of issues and topics under this title (Yngve and
W ˛asik 2004). Yngve’s own contributions included an examination of communications
in formal meetings and an essay on the foundations of a hard-science phonetics and
phonology. Others wrote about speech acts, anaphora, business negotiations, language
change, educational discourse, communication in science, and much more. The range
of applications is impressive, but it is still too early to say what the future impact of
Yngve’s “hard science” approach may have in the study of communication in all its
forms. He was himself convinced that in this “necessary reconstruction” of linguistics
on a truly scientific basis, “computational linguistics is destined to play an essential
role” (Yngve 1982, page 94).
Throughout his long career, Vic Yngve retained a modesty about his considerable
achievements in machine translation and computational linguistics and a firm com-
mitment to the highest standards of research practice which impressed everyone who
knew him. He will probably be best remembered for his depth hypothesis, but his other
papers on MT should continue to interest all who look for insights in the computational
analysis of natural language—and not just for historical reasons. His articles and books
on “hard-science linguistics” deserve to be essential reading for anyone reflecting upon
the foundations of linguistics, and indeed anyone concerned with the general health of
current and future studies of language and communication.
</bodyText>
<sectionHeader confidence="0.996134" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.999375761904762">
Chomsky, Noam.1957. Syntactic Structures.
Mouton, The Hague.
Hutchins, John. 1997. From first conception
to first demonstration: The nascent years
of machine translation, 1947–1954.
Machine Translation, 12:195–252.
Miller, George A. 1956. The magical number
seven, plus or minus two: Some limits in
our capacity for processing information.
Psychological Review, 63:81–97.
Yngve, Victor H. 1954. Language as an error
correcting code. Quarterly Progress Report of
the Research Laboratory of Electronics, MIT,
Cambridge, MA, October 1953:35–36.
Yngve, Victor H. 1955a. Sentence-for-
sentence translation. Mechanical
Translation, 2(2):29–37.
Yngve, Victor H. 1955b. Syntax and the
problem of multiple meaning. In
William N. Locke and A. Donald Booth,
editors, Machine Translation of Languages:
Fourteen Essays. Technology Press of the
Massachusetts Institute of Technology
and Wiley, Cambridge, MA, and
New York, pages 208–226.
Yngve, Victor H. 1956. Gap analysis and
syntax. IRE Transactions on Information
Theory, IT-2(3):106–112.
Yngve, Victor H. 1957. A framework
for syntactic translation. Mechanical
Translation, 4(3):59–65.
Yngve, Victor H. 1960a. A model and
an hypothesis for language structure.
Proceedings of the American Philosophical
Society, 104(5):444–466.
Yngve, Victor H. 1960b. Research
objectives. Quarterly Progress
Report of the Research Laboratory of
Electronics, MIT, Cambridge, MA,
October 1960:183.
Yngve, Victor H. 1961a. The COMIT
system. In H. P. Edmundson, editor,
Proceedings of the National Symposium
on Machine Translation, pages 439–443,
University of California, Los Angeles,
February 2–5, 1960. Prentice-Hall,
Englewood Cliffs, NJ.
Yngve, Victor H. 1961b. Random
generation of English sentences.
In 1961 International Conference on
Machine Translation of Languages and
Applied Language Analysis, pages 66–80.
Teddington, Middlesex.
Yngve, Victor H. 1964. Implications
of mechanical translation research.
Proceedings of the American Philosophical
Society, 108(4):275–281.
Yngve, Victor H. 1967. MT at M.I.T.,
1965. In A. D. Booth, editor, Machine
Translation. North-Holland, Amsterdam,
pages 451–523.
Yngve, Victor H. 1982. Our double
anniversary. In Proceedings of the
</reference>
<page confidence="0.995216">
466
</page>
<figure confidence="0.292212">
Hutchins Obituary
</figure>
<reference confidence="0.9940842">
20th Annual Meeting of the Association
for Computational Linguistics,
pages 92–94. Ontario.
Yngve, Victor H. 1986. Linguistics as
a Science. Indiana University Press,
Bloomington.
Yngve, Victor H. 1996. From Grammar to
Science: New Foundations for General
Linguistics. John Benjamins,
Amsterdam/Philadelphia.
Yngve, Victor H. 2000. Early research at
M.I.T.: In search of adequate theory.
In W. John Hutchins, editor, Early
Years in Machine Translation: Memoirs
and Biographies of Pioneers. John
Benjamins, Amsterdam/Philadelphia,
pages 39–72.
Yngve, Victor H., and Zdzisław W ˛asik,
editors. 2004. Hard-science Linguistics.
Continuum, London/New York.
</reference>
<page confidence="0.999298">
467
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.997046">Obituary</title>
<author confidence="0.994443">Victor H Yngve</author>
<degree confidence="0.5542965">John Victor Yngve (5 July 1920 to 15 January 2012) was a major contributor in a number of</degree>
<abstract confidence="0.99551388">fields within computational linguistics: as the leading researcher in machine translation (MT) at the Massachusetts Institute of Technology (MIT), as editor of its first journal, as designer and developer of the first non-numerical programming language (COMIT), and as an influential contributor to linguistic theory. While still completing his Ph.D. on cosmic ray physics at the University of Chicago during 1950–1953, Yngve had an idea for using the newly invented computers to translate languages. He contemplated building a translation machine based on simple dictionary lookup. At this time he knew nothing of the earlier speculations of Warren Weaver and others (Hutchins 1997). Then during a visit to Claude Shannon at Bell Telephone Laboratories in early 1952 he heard about a conference on machine translation to be held at MIT in June of that year. He attended the opening public meeting and participated in conference discussions, and then, after Bar-Hillel’s departure from MIT, he was appointed in July 1953 by Jerome Wiesner at the Research Laboratory for Electronics (RLE) to lead the MT research effort there. (For a retrospective survey of his MT research activities see Yngve [2000].) Yngve, along with many others at the time, deprecated the premature publicity around the Georgetown–IBM system demonstrated in January 1954. Yngve was appalled to see research of such a limited nature reported in newspapers; his background in physics required experiments to be carefully planned, with their assumptions made plain, and properly tested and reviewed by other researchers. He was determined to set the new field of MT on a proper scientific course. The first step was a journal for field, to be named field became “machine translation” in later years. He found a collaborator for the journal in William N. Locke of the MIT Modern Languages department. The aim was to provide a forum for information about what research was going on in the form of abstracts, and then for peer-reviewed articles.</abstract>
<note confidence="0.655127">The first issue appeared in March 1954. Yngve’s first experiments at MIT in October 1953 were an implementation of his</note>
<abstract confidence="0.992677322274881">earlier ideas on word-for-word translation. The results of translating from German were published in the collection edited by Locke and Booth (Yngve 1955b). One example of output began: Die CONVINCINGe CRITIQUE des CLASSICALen IDEA-OF-PROBABILITY IS eine der REMARKABLEen WORKS des AUTHORs. Er HAS BOTHen LAWe der GREATen NUMBERen ein DOUBLEes TO SHOWen: (1) wie sie IN seinem SYSTEM TO INTERPRETen ARE, (2) THAT sie THROUGH THISe INTERPRETATION NOT den CHARACTER von NOT-TRIVIALen DEMONSTRABLE PROPOSITIONen LOSen .. . at the University of East Anglia, Norwich, UK. E-mail: © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 3 It was obvious that the output was poor, but nevertheless it appeared to be good enough for scientists with some knowledge of German grammar to read and extract relevant information. Yngve concluded that word-for-word translation could be taken as a first approximation. But a major problem was that many words have more than one meaning out of context. Erwin Reifler had suggested the use of pre-editors who would annotate texts before translation; and Victor Oswald proposed the use of microglossaries (dictionaries limited to one specific narrow field) in order to reduce the number of homographs. But Yngve believed that the problem of multiple meanings could be resolved with syntactic analysis. Nonetheless, he rejected Bar-Hillel’s “operational syntax” put forward at the 1952 conference (and later known as categorical syntax) as too complex for long sentences and too remote from traditional grammatical distinctions. He had been impressed by the work of Bloomfield and Fries, and had become convinced that linguistics could provide stable and repeatable methods akin to the procedures of cosmic ray physics. Therefore, over the following years, he appointed a number of linguists to the RLE staff, starting with Noam Chomsky in 1955, in order to undertake basic research. He was disappointed, however, that most of the linguists he hired were more interested in pursuing their theoretical studies than in tackling the practical problems of MT. Yngve’s approach to syntactic analysis was to begin with the identification and isolation of the different possible grammatical functions of words. The aim was to set up mutually exclusive word-classes (i.e., one class for the noun function, another for the verb function, and so on) The approach was to set up substitution frames to isolate Thus occur in the frame (1) The was or in the frame (2) They home every day. Taking words from a corpus and testing in each frame produces a matrix of different contexts (substitution frames) and the words occurring in those contexts. These words form a word-class and their frames form context classes. As a result, each sentence sequence of word-classes would determine a unique sequence of context classes. An algorithm was proposed which searched left-to-right for the longest match for a sequence of word-classes. Sequences of word-classes formed phrases and clauses, so the algorithm was capable of looking also for phrase sequences. On this basis, a table-driven algorithm for syntactic analysis was designed and implemented (Yngve 1955b). This demonstrated for the first time in MT the importance and practical value of separating descriptive linguistic information (in this case, words and contexts) and languageindependent algorithmic procedures (searching and matching). The practice was widely adopted by other groups in MT and in computational linguistics. At the same time, work was going on at the RLE on investigations of coding and switching theory and on Shannon’s information theory. Yngve decided to investigate error correcting properties of language (Yngve 1954). Sequences of letters and words are not random but constrained by codes specifying which sequences are legitimate. Testing a text for deviations from randomness would help to reveal its structure. A first step would be the location of all occurrences of a given frequent word and the determination of its effect on the occurrence of other frequent words in the neighborhood by comparing their frequency of occurrence with what would be expected if they occurred at random. For this investigation of what was to be called “gap analysis” (Yngve 1956) 462 Hutchins Obituary a computational experiment was conducted on a corpus of just 9,490 words. First, the frequent words were identified: instances), and Then, the gaps between these words were determined (in numbers intervening words). In the case of gaps were one of two of three of and so on, with frequencies of 72, 31, respectively. These results pointed to syntactic constraints on constructions with Further results indicated that “structures with be expected to have two three words, and constructions with involve at least fifteen words.” observations were that different from that it frequently follows with a gap of one or two.” And so forth, in a procedure which has now become familiar in statistics-based computational linguistics. Yngve was a pioneer. Unfortunately, at the time, these encouraging results could not be pursued because of the lack of large enough corpora in machine-readable form. The parallels between coding theory and linguistics suggested a two-stage model for machine translation, where speakers encode a message that is decoded by a recipient (Yngve 1955a). Between encoding and decoding there would be some representation of the ‘message’ to be preserved in translation. Further consideration recognized that structural representations of input would not be the same as those required for output, however. Attention, therefore, focused on the transition stage where the meaning or message of the input language would be expressed and where choices would be made for producing output sentence structure. Thus the MIT group saw the need for a three-stage model: syntactic analysis, structure transfer, and synthesis—a model for many transfer-based MT systems in subsequent years. Yngve’s model was, however, purely syntactic; no attempt was made at this time to include semantic information (Yngve 1957). At the same time as the development of the transfer model, Yngve and his colleagues tackled the problem of how linguists could be productive in MT research. They decided that what was needed was a programming system that accepted notations of a kind familiar to linguists, rather than systems such as FORTRAN and COBOL designed for mathematicians and business applications. Thus, in collaboration with the MIT Computation Center, they developed the first programming language designed specifically for string-handling and pattern-matching, COMIT (Yngve 1961a, 1967). It was ready for testing in late 1957—thus antedating by two years LISP, another programming language devised for linguistics applications. COMIT was later used as a basis for the SNOBOL language. The availability of COMIT meant that the MIT group could proceed further with the three-stage model in the development of an algorithm for sentence production. Initially the generative grammar of Chomsky seemed to be an ideal model. What was required in MT, however, was not the generation of all grammatical sentences from a given source but particular specifiable sentences in context. The rules of a grammar derived from a simple children’s story were written in COMIT in 1959 and tested in a program of random sentence production (Yngve 1961b). Its main objective, in which it succeeded, was to test the validity of the grammar rules, particularly rules for discontinuous constructions and for coordination. One outcome of programming the sentence-production algorithm with COMIT was the “depth hypothesis,” for which Yngve is probably now best known (Yngve 1960a) both in linguistics and in computational linguistics. The transformational approach had already been rejected because it required too much storage space. The next question was how much storage (push-down store) was needed for discontinuous (left-branching) expansion and for regular (right-branching) expansion. It was clear that right expansion 463 Computational Linguistics Volume 38, Number 3 “progressive” application) was potentially infinite: dog that worried the cat that the rat that ate the malt On the other hand, left expansion (or ‘regressive’ appliwas limited: malt that the rat that the cat that the dog worried killed Yngve calculated the maximum amount of extra temporary memory required to produce a sentence (i.e., the number of unexpanded constituents at any one time [its depth]). He found that in practice even very long sentences seldom had a depth of more than two or three. Sentences with depths of more than three, such as the “regressive” constructions, were often considered ungrammatical and/or unintelligible. Yngve noted the relationship of this linguistic feature to the restrictions on immediate memory and processing identified by Miller (1956). Most languages include mechanisms for restricting the depth of regressive constructions, such as agglutination and compounding. The depth hypothesis accounted for and predicted many syntactic features of English, including its historical changes, and also appeared to account for many features in other languages. Yngve recognized that it arose from MT research and not from linguistic theory, however. It was a hypothesis that needed to be tested against empirical evidence. Its significance for linguistics was widely recognized from the beginning, but it did not conform to the preconceptions of Chomskyan theory. Although Chomsky had championed the rigorous statement of theory and its strict application to linguistic material without ad hoc adjustments (Chomsky 1957, page 5), he regarded the depth hypothesis not as a testable scientific hypothesis but as a rival linguistic theory. For Yngve this attitude was unscientific. Throughout his time at MIT Yngve stressed the need for a “basic, long-range approach to translation” and not to look for “short-cut methods that might yield partially adequate translations at an early date” (Yngve 1960b, page 183). No working MT system emerged from MIT, therefore, but the quality of the research is incontestable. Apart from Yngve’s own contributions to many aspects of syntactic analysis (the threestage transfer model, the depth hypothesis, and not least computer programming), his colleagues also made significant contributions in a variety of areas: grammar theory (Gilbert Harman and Kenneth Knowlton), semantics (Elinor Charney), logic and language (Jared Darlington), transfer and interlingua (Edward Klima), computer storage (William S. Cooper), Arabic translation (Arnold Satterthwait), and French translation (David Dinneen). Citations for these contributions will be found in Yngve’s comprehensive survey of MT research at MIT (Yngve 1967; see also Yngve 2000). By 1964, Yngve had come to the conclusion that “work in mechanical translation has come up against what we will call the semantic barrier ... we will only have adequate mechanical translations when the machine can ‘understand’ what it is translating and this will be a very difficult task indeed” (Yngve 1964, page 279). Understanding involved the background knowledge that people bring to the comprehension of communication. He could see no solutions coming from linguistics. For many years, Yngve had grown increasingly doubtful about the health of linguistic science and consequently about the feasibility of good quality MT in general. By 1965, funding for the MIT research group had ceased—perhaps in anticipation of the ALPAC report which had a major impact on the funding of all US research groups—and in that year Yngve went back to the University of Chicago as head of the Department of Library Sciences, Linguistics, and Behavioral Sciences. this time, the journal which he had founded in 1954, was coming to an end. The aim had been to provide a public record of achievement in MT. His ambitions for the journal could not be fulfilled, however, for various reasons. There were relatively few outstanding contributions submitted for publication; many MT researchers were funded by government bodies, which required the submission of 464 Hutchins Obituary fairly frequent reports, and these reports were distributed widely to other researchers and public institutions. Researchers believed they had fulfilled their duties to publicize research, and did not see the need to submit shorter articles to a peer-reviewed journal which would probably not appear for several months. In addition the journal could not survive solely on subscription charges, and authors were asked to contribute a page charge towards publication costs, which they were reluctant to do. In June 1962, the Association for Machine Translation and Computational Linguistics was founded, with Yngve as its first president. The inclusion of ‘computational linguistics’ in the title was indicative of the ever-expanding range of activities in the field of natural language processing; MT was only part, and a diminishing proportion. Association took over Yngve’s journal with a changed title, Translation Computational and Yngve remaining as editor. Even with the inclusion of many more articles in computational linguistics and significantly fewer in MT, however, the journal became ever more irregular and it was wound up in 1970. In 1968 machine had already been dropped from the Association’s From this time on, Yngve turned away from his MT interests to questions of linguistic theory that had been his increasing concern since the publication of his “depth hypothesis.” From 1965, Yngve published a series of papers devoted to the foundations of linguistic theory, many at the conferences of LACUS (Linguistic Association of and the United States). Just as when he had founded the Translation journal in 1954, he was determined to put studies of language on a sound scientific footing. His recurrent theme was the unscientific nature of current linguistics. In this period, he set forth the framework of what he called “human linguistics” (later “hardscience linguistics”) where the units being analyzed are not the traditional properties of sentences, verbs, noun phrases, gender, tense, phonemes, and so on, which, as he demonstrated, are unfounded assumptions derived often from Greek philosophical speculations. Instead the basic participants are communicating individuals and physical observable “props” (abstractions of relevant physical objects, clocks, doors), “channels” of communication (sound waves, writing, signs), and “settings” (relevant aspects of the physical environment, ticket counters, rooms). Yngve’s first attempt at summaand formulating his theory was published in his book, as Science (Yngve 1986). Expansion and elaboration of his theoretical standpoint followed in further papers were brought together in Grammar to Science 1996. The opening chapters are a cumulative rejection of all traditional and contemporary linguistics and philosophy of language (from the Greeks to Saussure, Bloomfield, Fries, Harris, Chomsky, and many others), including a rejection of his own widely accepted “depth hypothesis.” The basic contention is that fundamental concepts of linguistics are intuitions and not based on the observable behavior of people communicating. Yngve describes a detailed comprehensive program for a new foundation of linguistics in which “we abandon the unsupported assumptions of signs, words, meanings, and sentences” and move completely into “the world of sound waves and the people who speak and understand where we can test our theories and hypotheses against observations of the physical reality all the way down to the basic assumptions of all science” (Yngve 1996, page 308). He had not lost sight of computational treatments, and included (chapters 19 and 20) an implementable notation for representing and testing hypotheses.</abstract>
<note confidence="0.650097">1 For Yngve’s reflections on these changes in MT and computational linguistics see Yngve (1982). 465 Computational Linguistics Volume 38, Number 3</note>
<abstract confidence="0.992873">Yngve attracted the support and collaboration of a growing number of scholars sympathetic to his goal of a “hard-science linguistics,” and in 2004 a volume of essays appeared dealing with a wide range of issues and topics under this title (Yngve and 2004). Yngve’s own contributions included an examination of communications in formal meetings and an essay on the foundations of a hard-science phonetics and phonology. Others wrote about speech acts, anaphora, business negotiations, language change, educational discourse, communication in science, and much more. The range of applications is impressive, but it is still too early to say what the future impact of Yngve’s “hard science” approach may have in the study of communication in all its forms. He was himself convinced that in this “necessary reconstruction” of linguistics on a truly scientific basis, “computational linguistics is destined to play an essential role” (Yngve 1982, page 94). Throughout his long career, Vic Yngve retained a modesty about his considerable achievements in machine translation and computational linguistics and a firm commitment to the highest standards of research practice which impressed everyone who knew him. He will probably be best remembered for his depth hypothesis, but his other papers on MT should continue to interest all who look for insights in the computational analysis of natural language—and not just for historical reasons. His articles and books on “hard-science linguistics” deserve to be essential reading for anyone reflecting upon the foundations of linguistics, and indeed anyone concerned with the general health of current and future studies of language and communication.</abstract>
<note confidence="0.952612">References Noam.1957.</note>
<degree confidence="0.22506">Mouton, The Hague. Hutchins, John. 1997. From first conception</degree>
<abstract confidence="0.804799333333333">to first demonstration: The nascent years of machine translation, 1947–1954. 12:195–252. Miller, George A. 1956. The magical number seven, plus or minus two: Some limits in our capacity for processing information.</abstract>
<note confidence="0.603883">63:81–97. Yngve, Victor H. 1954. Language as an error code. Progress Report of Research Laboratory of MIT, Cambridge, MA, October 1953:35–36. Yngve, Victor H. 1955a. Sentence-fortranslation. 2(2):29–37. Yngve, Victor H. 1955b. Syntax and the problem of multiple meaning. In</note>
<author confidence="0.735246">William N Locke</author>
<author confidence="0.735246">A Donald Booth</author>
<affiliation confidence="0.979802666666667">Translation of Languages: Technology Press of the Massachusetts Institute of Technology</affiliation>
<address confidence="0.936119">and Wiley, Cambridge, MA, and</address>
<note confidence="0.8763413">New York, pages 208–226. Yngve, Victor H. 1956. Gap analysis and Transactions on Information IT-2(3):106–112. Yngve, Victor H. 1957. A framework syntactic translation. 4(3):59–65. Yngve, Victor H. 1960a. A model and an hypothesis for language structure. Proceedings of the American Philosophical 104(5):444–466. Yngve, Victor H. 1960b. Research Progress Report of the Research Laboratory of MIT, Cambridge, MA, October 1960:183. Yngve, Victor H. 1961a. The COMIT system. In H. P. Edmundson, editor, Proceedings of the National Symposium Machine pages 439–443,</note>
<affiliation confidence="0.913814">University of California, Los Angeles,</affiliation>
<address confidence="0.773483">February 2–5, 1960. Prentice-Hall, Englewood Cliffs, NJ.</address>
<note confidence="0.926204363636364">Yngve, Victor H. 1961b. Random generation of English sentences. International Conference on Machine Translation of Languages and Language pages 66–80. Teddington, Middlesex. Yngve, Victor H. 1964. Implications of mechanical translation research. Proceedings of the American Philosophical 108(4):275–281. Yngve, Victor H. 1967. MT at M.I.T., In A. D. Booth, editor, North-Holland, Amsterdam, pages 451–523. Yngve, Victor H. 1982. Our double In of the 466 Hutchins Obituary 20th Annual Meeting of the Association Computational pages 92–94. Ontario. Victor H. 1986. as</note>
<affiliation confidence="0.999426">Indiana University Press,</affiliation>
<address confidence="0.986488">Bloomington.</address>
<author confidence="0.800181">H Victor</author>
<title confidence="0.74142">Science: New Foundations for General</title>
<author confidence="0.750504">John Benjamins</author>
<note confidence="0.85120125">Amsterdam/Philadelphia. Yngve, Victor H. 2000. Early research at M.I.T.: In search of adequate theory. W. John Hutchins, editor, Years in Machine Translation: Memoirs Biographies of John Benjamins, Amsterdam/Philadelphia, pages 39–72. Victor H., and Zdzisław W 2004. Continuum, London/New York. 467</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Noam 1957 Chomsky</author>
</authors>
<title>Syntactic Structures.</title>
<publisher>Mouton, The Hague.</publisher>
<marker>Chomsky, </marker>
<rawString>Chomsky, Noam.1957. Syntactic Structures. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hutchins</author>
</authors>
<title>From first conception to first demonstration: The nascent years of machine translation,</title>
<date>1997</date>
<booktitle>Machine Translation,</booktitle>
<pages>12--195</pages>
<contexts>
<context position="811" citStr="Hutchins 1997" startWordPosition="123" endWordPosition="124"> machine translation (MT) at the Massachusetts Institute of Technology (MIT), as editor of its first journal, as designer and developer of the first non-numerical programming language (COMIT), and as an influential contributor to linguistic theory. While still completing his Ph.D. on cosmic ray physics at the University of Chicago during 1950–1953, Yngve had an idea for using the newly invented computers to translate languages. He contemplated building a translation machine based on simple dictionary lookup. At this time he knew nothing of the earlier speculations of Warren Weaver and others (Hutchins 1997). Then during a visit to Claude Shannon at Bell Telephone Laboratories in early 1952 he heard about a conference on machine translation to be held at MIT in June of that year. He attended the opening public meeting and participated in conference discussions, and then, after Bar-Hillel’s departure from MIT, he was appointed in July 1953 by Jerome Wiesner at the Research Laboratory for Electronics (RLE) to lead the MT research effort there. (For a retrospective survey of his MT research activities see Yngve [2000].) Yngve, along with many others at the time, deprecated the premature publicity ar</context>
</contexts>
<marker>Hutchins, 1997</marker>
<rawString>Hutchins, John. 1997. From first conception to first demonstration: The nascent years of machine translation, 1947–1954. Machine Translation, 12:195–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>The magical number seven, plus or minus two: Some limits in our capacity for processing information.</title>
<date>1956</date>
<journal>Psychological Review,</journal>
<pages>63--81</pages>
<contexts>
<context position="11526" citStr="Miller (1956)" startWordPosition="1806" endWordPosition="1807"> the malt that the rat that the cat that the dog worried killed ate. Yngve calculated the maximum amount of extra temporary memory required to produce a sentence (i.e., the number of unexpanded constituents at any one time [its depth]). He found that in practice even very long sentences seldom had a depth of more than two or three. Sentences with depths of more than three, such as the “regressive” constructions, were often considered ungrammatical and/or unintelligible. Yngve noted the relationship of this linguistic feature to the restrictions on immediate memory and processing identified by Miller (1956). Most languages include mechanisms for restricting the depth of regressive constructions, such as agglutination and compounding. The depth hypothesis accounted for and predicted many syntactic features of English, including its historical changes, and also appeared to account for many features in other languages. Yngve recognized that it arose from MT research and not from linguistic theory, however. It was a hypothesis that needed to be tested against empirical evidence. Its significance for linguistics was widely recognized from the beginning, but it did not conform to the preconceptions of</context>
</contexts>
<marker>Miller, 1956</marker>
<rawString>Miller, George A. 1956. The magical number seven, plus or minus two: Some limits in our capacity for processing information. Psychological Review, 63:81–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>Language as an error correcting code.</title>
<date>1954</date>
<booktitle>Quarterly Progress Report of the Research Laboratory of Electronics, MIT,</booktitle>
<location>Cambridge, MA,</location>
<contexts>
<context position="6106" citStr="Yngve 1954" startWordPosition="953" endWordPosition="954">orithm for syntactic analysis was designed and implemented (Yngve 1955b). This demonstrated for the first time in MT the importance and practical value of separating descriptive linguistic information (in this case, words and contexts) and languageindependent algorithmic procedures (searching and matching). The practice was widely adopted by other groups in MT and in computational linguistics. At the same time, work was going on at the RLE on investigations of coding and switching theory and on Shannon’s information theory. Yngve decided to investigate error correcting properties of language (Yngve 1954). Sequences of letters and words are not random but constrained by codes specifying which sequences are legitimate. Testing a text for deviations from randomness would help to reveal its structure. A first step would be the location of all occurrences of a given frequent word and the determination of its effect on the occurrence of other frequent words in the neighborhood by comparing their frequency of occurrence with what would be expected if they occurred at random. For this investigation of what was to be called “gap analysis” (Yngve 1956) 462 Hutchins Obituary a computational experiment w</context>
</contexts>
<marker>Yngve, 1954</marker>
<rawString>Yngve, Victor H. 1954. Language as an error correcting code. Quarterly Progress Report of the Research Laboratory of Electronics, MIT, Cambridge, MA, October 1953:35–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>Sentence-forsentence translation.</title>
<date>1955</date>
<journal>Mechanical Translation,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="2443" citStr="Yngve 1955" startWordPosition="388" endWordPosition="389">eld, to be named Mechanical Translation—the field became “machine translation” in later years. He found a collaborator for the journal in William N. Locke of the MIT Modern Languages department. The aim was to provide a forum for information about what research was going on in the form of abstracts, and then for peer-reviewed articles. The first issue appeared in March 1954. Yngve’s first experiments at MIT in October 1953 were an implementation of his earlier ideas on word-for-word translation. The results of translating from German were published in the collection edited by Locke and Booth (Yngve 1955b). One example of output began: Die CONVINCINGe CRITIQUE des CLASSICALen IDEA-OF-PROBABILITY IS eine der REMARKABLEen WORKS des AUTHORs. Er HAS BOTHen LAWe der GREATen NUMBERen ein DOUBLEes TO SHOWen: (1) wie sie IN seinem SYSTEM TO INTERPRETen ARE, (2) THAT sie THROUGH THISe INTERPRETATION NOT den CHARACTER von NOT-TRIVIALen DEMONSTRABLE PROPOSITIONen LOSen .. . * Previously at the University of East Anglia, Norwich, UK. E-mail: john@hutchinsweb.me.uk. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 3 It was obvious that the output was poor, but n</context>
<context position="5565" citStr="Yngve 1955" startWordPosition="873" endWordPosition="874">duces a matrix of different contexts (substitution frames) and the words occurring in those contexts. These words form a word-class and their frames form context classes. As a result, each sentence sequence of word-classes would determine a unique sequence of context classes. An algorithm was proposed which searched left-to-right for the longest match for a sequence of word-classes. Sequences of word-classes formed phrases and clauses, so the algorithm was capable of looking also for phrase sequences. On this basis, a table-driven algorithm for syntactic analysis was designed and implemented (Yngve 1955b). This demonstrated for the first time in MT the importance and practical value of separating descriptive linguistic information (in this case, words and contexts) and languageindependent algorithmic procedures (searching and matching). The practice was widely adopted by other groups in MT and in computational linguistics. At the same time, work was going on at the RLE on investigations of coding and switching theory and on Shannon’s information theory. Yngve decided to investigate error correcting properties of language (Yngve 1954). Sequences of letters and words are not random but constra</context>
<context position="7929" citStr="Yngve 1955" startWordPosition="1256" endWordPosition="1257">tly involve at least fifteen words.” Similar observations were that “of is different from to in that it frequently follows a or the with a gap of one or two.” And so forth, in a procedure which has now become familiar in statistics-based computational linguistics. Yngve was a pioneer. Unfortunately, at the time, these encouraging results could not be pursued because of the lack of large enough corpora in machine-readable form. The parallels between coding theory and linguistics suggested a two-stage model for machine translation, where speakers encode a message that is decoded by a recipient (Yngve 1955a). Between encoding and decoding there would be some representation of the ‘message’ to be preserved in translation. Further consideration recognized that structural representations of input would not be the same as those required for output, however. Attention, therefore, focused on the transition stage where the meaning or message of the input language would be expressed and where choices would be made for producing output sentence structure. Thus the MIT group saw the need for a three-stage model: syntactic analysis, structure transfer, and synthesis—a model for many transfer-based MT syst</context>
</contexts>
<marker>Yngve, 1955</marker>
<rawString>Yngve, Victor H. 1955a. Sentence-forsentence translation. Mechanical Translation, 2(2):29–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>Syntax and the problem of multiple meaning. In</title>
<date>1955</date>
<contexts>
<context position="2443" citStr="Yngve 1955" startWordPosition="388" endWordPosition="389">eld, to be named Mechanical Translation—the field became “machine translation” in later years. He found a collaborator for the journal in William N. Locke of the MIT Modern Languages department. The aim was to provide a forum for information about what research was going on in the form of abstracts, and then for peer-reviewed articles. The first issue appeared in March 1954. Yngve’s first experiments at MIT in October 1953 were an implementation of his earlier ideas on word-for-word translation. The results of translating from German were published in the collection edited by Locke and Booth (Yngve 1955b). One example of output began: Die CONVINCINGe CRITIQUE des CLASSICALen IDEA-OF-PROBABILITY IS eine der REMARKABLEen WORKS des AUTHORs. Er HAS BOTHen LAWe der GREATen NUMBERen ein DOUBLEes TO SHOWen: (1) wie sie IN seinem SYSTEM TO INTERPRETen ARE, (2) THAT sie THROUGH THISe INTERPRETATION NOT den CHARACTER von NOT-TRIVIALen DEMONSTRABLE PROPOSITIONen LOSen .. . * Previously at the University of East Anglia, Norwich, UK. E-mail: john@hutchinsweb.me.uk. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 3 It was obvious that the output was poor, but n</context>
<context position="5565" citStr="Yngve 1955" startWordPosition="873" endWordPosition="874">duces a matrix of different contexts (substitution frames) and the words occurring in those contexts. These words form a word-class and their frames form context classes. As a result, each sentence sequence of word-classes would determine a unique sequence of context classes. An algorithm was proposed which searched left-to-right for the longest match for a sequence of word-classes. Sequences of word-classes formed phrases and clauses, so the algorithm was capable of looking also for phrase sequences. On this basis, a table-driven algorithm for syntactic analysis was designed and implemented (Yngve 1955b). This demonstrated for the first time in MT the importance and practical value of separating descriptive linguistic information (in this case, words and contexts) and languageindependent algorithmic procedures (searching and matching). The practice was widely adopted by other groups in MT and in computational linguistics. At the same time, work was going on at the RLE on investigations of coding and switching theory and on Shannon’s information theory. Yngve decided to investigate error correcting properties of language (Yngve 1954). Sequences of letters and words are not random but constra</context>
<context position="7929" citStr="Yngve 1955" startWordPosition="1256" endWordPosition="1257">tly involve at least fifteen words.” Similar observations were that “of is different from to in that it frequently follows a or the with a gap of one or two.” And so forth, in a procedure which has now become familiar in statistics-based computational linguistics. Yngve was a pioneer. Unfortunately, at the time, these encouraging results could not be pursued because of the lack of large enough corpora in machine-readable form. The parallels between coding theory and linguistics suggested a two-stage model for machine translation, where speakers encode a message that is decoded by a recipient (Yngve 1955a). Between encoding and decoding there would be some representation of the ‘message’ to be preserved in translation. Further consideration recognized that structural representations of input would not be the same as those required for output, however. Attention, therefore, focused on the transition stage where the meaning or message of the input language would be expressed and where choices would be made for producing output sentence structure. Thus the MIT group saw the need for a three-stage model: syntactic analysis, structure transfer, and synthesis—a model for many transfer-based MT syst</context>
</contexts>
<marker>Yngve, 1955</marker>
<rawString>Yngve, Victor H. 1955b. Syntax and the problem of multiple meaning. In</rawString>
</citation>
<citation valid="false">
<authors>
<author>N William</author>
</authors>
<booktitle>Machine Translation of Languages: Fourteen Essays. Technology Press of the Massachusetts Institute of Technology and Wiley,</booktitle>
<pages>208--226</pages>
<editor>Locke and A. Donald Booth, editors,</editor>
<location>Cambridge, MA, and New York,</location>
<marker>William, </marker>
<rawString>William N. Locke and A. Donald Booth, editors, Machine Translation of Languages: Fourteen Essays. Technology Press of the Massachusetts Institute of Technology and Wiley, Cambridge, MA, and New York, pages 208–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>Gap analysis and syntax.</title>
<date>1956</date>
<journal>IRE Transactions on Information Theory,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="6655" citStr="Yngve 1956" startWordPosition="1044" endWordPosition="1045">vestigate error correcting properties of language (Yngve 1954). Sequences of letters and words are not random but constrained by codes specifying which sequences are legitimate. Testing a text for deviations from randomness would help to reveal its structure. A first step would be the location of all occurrences of a given frequent word and the determination of its effect on the occurrence of other frequent words in the neighborhood by comparing their frequency of occurrence with what would be expected if they occurred at random. For this investigation of what was to be called “gap analysis” (Yngve 1956) 462 Hutchins Obituary a computational experiment was conducted on a corpus of just 9,490 words. First, the most frequent words were identified: the (599 instances), to (252), of (241), a (221), and (207), and in (162). Then, the gaps between these words were determined (in numbers of intervening words). In the case of of and the the gaps were one (the of ), two (the of ), three (the of ), and so on, with frequencies of 72, 31, and 6, respectively. These results pointed to syntactic constraints on constructions with of and the. Further results indicated that “structures with the can be expecte</context>
</contexts>
<marker>Yngve, 1956</marker>
<rawString>Yngve, Victor H. 1956. Gap analysis and syntax. IRE Transactions on Information Theory, IT-2(3):106–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>A framework for syntactic translation.</title>
<date>1957</date>
<journal>Mechanical Translation,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="8677" citStr="Yngve 1957" startWordPosition="1368" endWordPosition="1369">on recognized that structural representations of input would not be the same as those required for output, however. Attention, therefore, focused on the transition stage where the meaning or message of the input language would be expressed and where choices would be made for producing output sentence structure. Thus the MIT group saw the need for a three-stage model: syntactic analysis, structure transfer, and synthesis—a model for many transfer-based MT systems in subsequent years. Yngve’s model was, however, purely syntactic; no attempt was made at this time to include semantic information (Yngve 1957). At the same time as the development of the transfer model, Yngve and his colleagues tackled the problem of how linguists could be productive in MT research. They decided that what was needed was a programming system that accepted notations of a kind familiar to linguists, rather than systems such as FORTRAN and COBOL designed for mathematicians and business applications. Thus, in collaboration with the MIT Computation Center, they developed the first programming language designed specifically for string-handling and pattern-matching, COMIT (Yngve 1961a, 1967). It was ready for testing in lat</context>
</contexts>
<marker>Yngve, 1957</marker>
<rawString>Yngve, Victor H. 1957. A framework for syntactic translation. Mechanical Translation, 4(3):59–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>A model and an hypothesis for language structure.</title>
<date>1960</date>
<journal>Proceedings of the American Philosophical Society,</journal>
<volume>104</volume>
<issue>5</issue>
<contexts>
<context position="10309" citStr="Yngve 1960" startWordPosition="1621" endWordPosition="1622">, however, was not the generation of all grammatical sentences from a given source but particular specifiable sentences in context. The rules of a grammar derived from a simple children’s story were written in COMIT in 1959 and tested in a program of random sentence production (Yngve 1961b). Its main objective, in which it succeeded, was to test the validity of the grammar rules, particularly rules for discontinuous constructions and for coordination. One outcome of programming the sentence-production algorithm with COMIT was the “depth hypothesis,” for which Yngve is probably now best known (Yngve 1960a) both in linguistics and in computational linguistics. The transformational approach had already been rejected because it required too much storage space. The next question was how much storage (push-down store) was needed for discontinuous (left-branching) expansion and for regular (right-branching) expansion. It was clear that right expansion 463 Computational Linguistics Volume 38, Number 3 (or “progressive” application) was potentially infinite: the dog that worried the cat that killed the rat that ate the malt .... On the other hand, left expansion (or ‘regressive’ application) was limi</context>
<context position="12678" citStr="Yngve 1960" startWordPosition="1980" endWordPosition="1981">beginning, but it did not conform to the preconceptions of Chomskyan theory. Although Chomsky had championed the rigorous statement of theory and its strict application to linguistic material without ad hoc adjustments (Chomsky 1957, page 5), he regarded the depth hypothesis not as a testable scientific hypothesis but as a rival linguistic theory. For Yngve this attitude was unscientific. Throughout his time at MIT Yngve stressed the need for a “basic, long-range approach to translation” and not to look for “short-cut methods that might yield partially adequate translations at an early date” (Yngve 1960b, page 183). No working MT system emerged from MIT, therefore, but the quality of the research is incontestable. Apart from Yngve’s own contributions to many aspects of syntactic analysis (the threestage transfer model, the depth hypothesis, and not least computer programming), his colleagues also made significant contributions in a variety of areas: grammar theory (Gilbert Harman and Kenneth Knowlton), semantics (Elinor Charney), logic and language (Jared Darlington), transfer and interlingua (Edward Klima), computer storage (William S. Cooper), Arabic translation (Arnold Satterthwait), and </context>
</contexts>
<marker>Yngve, 1960</marker>
<rawString>Yngve, Victor H. 1960a. A model and an hypothesis for language structure. Proceedings of the American Philosophical Society, 104(5):444–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>Research objectives.</title>
<date>1960</date>
<booktitle>Quarterly Progress Report of the Research Laboratory of Electronics, MIT,</booktitle>
<location>Cambridge, MA,</location>
<contexts>
<context position="10309" citStr="Yngve 1960" startWordPosition="1621" endWordPosition="1622">, however, was not the generation of all grammatical sentences from a given source but particular specifiable sentences in context. The rules of a grammar derived from a simple children’s story were written in COMIT in 1959 and tested in a program of random sentence production (Yngve 1961b). Its main objective, in which it succeeded, was to test the validity of the grammar rules, particularly rules for discontinuous constructions and for coordination. One outcome of programming the sentence-production algorithm with COMIT was the “depth hypothesis,” for which Yngve is probably now best known (Yngve 1960a) both in linguistics and in computational linguistics. The transformational approach had already been rejected because it required too much storage space. The next question was how much storage (push-down store) was needed for discontinuous (left-branching) expansion and for regular (right-branching) expansion. It was clear that right expansion 463 Computational Linguistics Volume 38, Number 3 (or “progressive” application) was potentially infinite: the dog that worried the cat that killed the rat that ate the malt .... On the other hand, left expansion (or ‘regressive’ application) was limi</context>
<context position="12678" citStr="Yngve 1960" startWordPosition="1980" endWordPosition="1981">beginning, but it did not conform to the preconceptions of Chomskyan theory. Although Chomsky had championed the rigorous statement of theory and its strict application to linguistic material without ad hoc adjustments (Chomsky 1957, page 5), he regarded the depth hypothesis not as a testable scientific hypothesis but as a rival linguistic theory. For Yngve this attitude was unscientific. Throughout his time at MIT Yngve stressed the need for a “basic, long-range approach to translation” and not to look for “short-cut methods that might yield partially adequate translations at an early date” (Yngve 1960b, page 183). No working MT system emerged from MIT, therefore, but the quality of the research is incontestable. Apart from Yngve’s own contributions to many aspects of syntactic analysis (the threestage transfer model, the depth hypothesis, and not least computer programming), his colleagues also made significant contributions in a variety of areas: grammar theory (Gilbert Harman and Kenneth Knowlton), semantics (Elinor Charney), logic and language (Jared Darlington), transfer and interlingua (Edward Klima), computer storage (William S. Cooper), Arabic translation (Arnold Satterthwait), and </context>
</contexts>
<marker>Yngve, 1960</marker>
<rawString>Yngve, Victor H. 1960b. Research objectives. Quarterly Progress Report of the Research Laboratory of Electronics, MIT, Cambridge, MA, October 1960:183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>The COMIT system. In</title>
<date>1961</date>
<booktitle>Proceedings of the National Symposium on Machine Translation,</booktitle>
<pages>439--443</pages>
<editor>H. P. Edmundson, editor,</editor>
<publisher>Prentice-Hall,</publisher>
<location>University of California, Los Angeles,</location>
<contexts>
<context position="9236" citStr="Yngve 1961" startWordPosition="1452" endWordPosition="1453">s time to include semantic information (Yngve 1957). At the same time as the development of the transfer model, Yngve and his colleagues tackled the problem of how linguists could be productive in MT research. They decided that what was needed was a programming system that accepted notations of a kind familiar to linguists, rather than systems such as FORTRAN and COBOL designed for mathematicians and business applications. Thus, in collaboration with the MIT Computation Center, they developed the first programming language designed specifically for string-handling and pattern-matching, COMIT (Yngve 1961a, 1967). It was ready for testing in late 1957—thus antedating by two years LISP, another programming language devised for linguistics applications. COMIT was later used as a basis for the SNOBOL language. The availability of COMIT meant that the MIT group could proceed further with the three-stage model in the development of an algorithm for sentence production. Initially the generative grammar of Chomsky seemed to be an ideal model. What was required in MT, however, was not the generation of all grammatical sentences from a given source but particular specifiable sentences in context. The r</context>
</contexts>
<marker>Yngve, 1961</marker>
<rawString>Yngve, Victor H. 1961a. The COMIT system. In H. P. Edmundson, editor, Proceedings of the National Symposium on Machine Translation, pages 439–443, University of California, Los Angeles, February 2–5, 1960. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>Random generation of English sentences.</title>
<date>1961</date>
<booktitle>In 1961 International Conference on Machine Translation of Languages and Applied Language Analysis,</booktitle>
<pages>66--80</pages>
<location>Teddington, Middlesex.</location>
<contexts>
<context position="9236" citStr="Yngve 1961" startWordPosition="1452" endWordPosition="1453">s time to include semantic information (Yngve 1957). At the same time as the development of the transfer model, Yngve and his colleagues tackled the problem of how linguists could be productive in MT research. They decided that what was needed was a programming system that accepted notations of a kind familiar to linguists, rather than systems such as FORTRAN and COBOL designed for mathematicians and business applications. Thus, in collaboration with the MIT Computation Center, they developed the first programming language designed specifically for string-handling and pattern-matching, COMIT (Yngve 1961a, 1967). It was ready for testing in late 1957—thus antedating by two years LISP, another programming language devised for linguistics applications. COMIT was later used as a basis for the SNOBOL language. The availability of COMIT meant that the MIT group could proceed further with the three-stage model in the development of an algorithm for sentence production. Initially the generative grammar of Chomsky seemed to be an ideal model. What was required in MT, however, was not the generation of all grammatical sentences from a given source but particular specifiable sentences in context. The r</context>
</contexts>
<marker>Yngve, 1961</marker>
<rawString>Yngve, Victor H. 1961b. Random generation of English sentences. In 1961 International Conference on Machine Translation of Languages and Applied Language Analysis, pages 66–80. Teddington, Middlesex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>Implications of mechanical translation research.</title>
<date>1964</date>
<journal>Proceedings of the American Philosophical Society,</journal>
<volume>108</volume>
<issue>4</issue>
<contexts>
<context position="13759" citStr="Yngve 1964" startWordPosition="2144" endWordPosition="2145">n), transfer and interlingua (Edward Klima), computer storage (William S. Cooper), Arabic translation (Arnold Satterthwait), and French translation (David Dinneen). Citations for these contributions will be found in Yngve’s comprehensive survey of MT research at MIT (Yngve 1967; see also Yngve 2000). By 1964, Yngve had come to the conclusion that “work in mechanical translation has come up against what we will call the semantic barrier ... we will only have adequate mechanical translations when the machine can ‘understand’ what it is translating and this will be a very difficult task indeed” (Yngve 1964, page 279). Understanding involved the background knowledge that people bring to the comprehension of communication. He could see no solutions coming from linguistics. For many years, Yngve had grown increasingly doubtful about the health of linguistic science and consequently about the feasibility of good quality MT in general. By 1965, funding for the MIT research group had ceased—perhaps in anticipation of the ALPAC report which had a major impact on the funding of all US research groups—and in that year Yngve went back to the University of Chicago as head of the Department of Library Scie</context>
</contexts>
<marker>Yngve, 1964</marker>
<rawString>Yngve, Victor H. 1964. Implications of mechanical translation research. Proceedings of the American Philosophical Society, 108(4):275–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<date>1967</date>
<booktitle>Machine Translation.</booktitle>
<pages>451--523</pages>
<editor>MT at M.I.T.,</editor>
<publisher>North-Holland,</publisher>
<location>Amsterdam,</location>
<contexts>
<context position="13427" citStr="Yngve 1967" startWordPosition="2087" endWordPosition="2088">ntributions to many aspects of syntactic analysis (the threestage transfer model, the depth hypothesis, and not least computer programming), his colleagues also made significant contributions in a variety of areas: grammar theory (Gilbert Harman and Kenneth Knowlton), semantics (Elinor Charney), logic and language (Jared Darlington), transfer and interlingua (Edward Klima), computer storage (William S. Cooper), Arabic translation (Arnold Satterthwait), and French translation (David Dinneen). Citations for these contributions will be found in Yngve’s comprehensive survey of MT research at MIT (Yngve 1967; see also Yngve 2000). By 1964, Yngve had come to the conclusion that “work in mechanical translation has come up against what we will call the semantic barrier ... we will only have adequate mechanical translations when the machine can ‘understand’ what it is translating and this will be a very difficult task indeed” (Yngve 1964, page 279). Understanding involved the background knowledge that people bring to the comprehension of communication. He could see no solutions coming from linguistics. For many years, Yngve had grown increasingly doubtful about the health of linguistic science and co</context>
</contexts>
<marker>Yngve, 1967</marker>
<rawString>Yngve, Victor H. 1967. MT at M.I.T., 1965. In A. D. Booth, editor, Machine Translation. North-Holland, Amsterdam, pages 451–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>Our double anniversary.</title>
<date>1982</date>
<booktitle>In Proceedings of the 20th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>92--94</pages>
<location>Ontario.</location>
<contexts>
<context position="18728" citStr="Yngve (1982)" startWordPosition="2895" endWordPosition="2896">nguistics in which “we abandon the unsupported assumptions of signs, words, meanings, and sentences” and move completely into “the world of sound waves and the people who speak and understand where we can test our theories and hypotheses against observations of the physical reality all the way down to the basic assumptions of all science” (Yngve 1996, page 308). He had not lost sight of computational treatments, and included (chapters 19 and 20) an implementable notation for representing and testing hypotheses. 1 For Yngve’s reflections on these changes in MT and computational linguistics see Yngve (1982). 465 Computational Linguistics Volume 38, Number 3 Yngve attracted the support and collaboration of a growing number of scholars sympathetic to his goal of a “hard-science linguistics,” and in 2004 a volume of essays appeared dealing with a wide range of issues and topics under this title (Yngve and W ˛asik 2004). Yngve’s own contributions included an examination of communications in formal meetings and an essay on the foundations of a hard-science phonetics and phonology. Others wrote about speech acts, anaphora, business negotiations, language change, educational discourse, communication in</context>
</contexts>
<marker>Yngve, 1982</marker>
<rawString>Yngve, Victor H. 1982. Our double anniversary. In Proceedings of the 20th Annual Meeting of the Association for Computational Linguistics, pages 92–94. Ontario.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>Linguistics as a Science.</title>
<date>1986</date>
<publisher>Indiana University Press,</publisher>
<location>Bloomington.</location>
<contexts>
<context position="17472" citStr="Yngve 1986" startWordPosition="2706" endWordPosition="2707">operties of sentences, verbs, noun phrases, gender, tense, phonemes, and so on, which, as he demonstrated, are unfounded assumptions derived often from Greek philosophical speculations. Instead the basic participants are communicating individuals and physical observable “props” (abstractions of relevant physical objects, clocks, doors), “channels” of communication (sound waves, writing, signs), and “settings” (relevant aspects of the physical environment, ticket counters, rooms). Yngve’s first attempt at summarizing and formulating his theory was published in his book, Linguistics as Science (Yngve 1986). Expansion and elaboration of his theoretical standpoint followed in further papers and were brought together in From Grammar to Science in 1996. The opening chapters are a cumulative rejection of all traditional and contemporary linguistics and philosophy of language (from the Greeks to Saussure, Bloomfield, Fries, Harris, Chomsky, and many others), including a rejection of his own widely accepted “depth hypothesis.” The basic contention is that fundamental concepts of linguistics are intuitions and not based on the observable behavior of people communicating. Yngve describes a detailed comp</context>
</contexts>
<marker>Yngve, 1986</marker>
<rawString>Yngve, Victor H. 1986. Linguistics as a Science. Indiana University Press, Bloomington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>From Grammar to Science: New Foundations for General Linguistics.</title>
<date>1996</date>
<location>John Benjamins, Amsterdam/Philadelphia.</location>
<contexts>
<context position="18468" citStr="Yngve 1996" startWordPosition="2856" endWordPosition="2857"> widely accepted “depth hypothesis.” The basic contention is that fundamental concepts of linguistics are intuitions and not based on the observable behavior of people communicating. Yngve describes a detailed comprehensive program for a new foundation of linguistics in which “we abandon the unsupported assumptions of signs, words, meanings, and sentences” and move completely into “the world of sound waves and the people who speak and understand where we can test our theories and hypotheses against observations of the physical reality all the way down to the basic assumptions of all science” (Yngve 1996, page 308). He had not lost sight of computational treatments, and included (chapters 19 and 20) an implementable notation for representing and testing hypotheses. 1 For Yngve’s reflections on these changes in MT and computational linguistics see Yngve (1982). 465 Computational Linguistics Volume 38, Number 3 Yngve attracted the support and collaboration of a growing number of scholars sympathetic to his goal of a “hard-science linguistics,” and in 2004 a volume of essays appeared dealing with a wide range of issues and topics under this title (Yngve and W ˛asik 2004). Yngve’s own contributio</context>
</contexts>
<marker>Yngve, 1996</marker>
<rawString>Yngve, Victor H. 1996. From Grammar to Science: New Foundations for General Linguistics. John Benjamins, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>Early research at M.I.T.: In search of adequate theory.</title>
<date>2000</date>
<booktitle>Early Years in Machine Translation: Memoirs and Biographies of Pioneers. John Benjamins, Amsterdam/Philadelphia,</booktitle>
<pages>39--72</pages>
<editor>In W. John Hutchins, editor,</editor>
<contexts>
<context position="13449" citStr="Yngve 2000" startWordPosition="2091" endWordPosition="2092">spects of syntactic analysis (the threestage transfer model, the depth hypothesis, and not least computer programming), his colleagues also made significant contributions in a variety of areas: grammar theory (Gilbert Harman and Kenneth Knowlton), semantics (Elinor Charney), logic and language (Jared Darlington), transfer and interlingua (Edward Klima), computer storage (William S. Cooper), Arabic translation (Arnold Satterthwait), and French translation (David Dinneen). Citations for these contributions will be found in Yngve’s comprehensive survey of MT research at MIT (Yngve 1967; see also Yngve 2000). By 1964, Yngve had come to the conclusion that “work in mechanical translation has come up against what we will call the semantic barrier ... we will only have adequate mechanical translations when the machine can ‘understand’ what it is translating and this will be a very difficult task indeed” (Yngve 1964, page 279). Understanding involved the background knowledge that people bring to the comprehension of communication. He could see no solutions coming from linguistics. For many years, Yngve had grown increasingly doubtful about the health of linguistic science and consequently about the f</context>
</contexts>
<marker>Yngve, 2000</marker>
<rawString>Yngve, Victor H. 2000. Early research at M.I.T.: In search of adequate theory. In W. John Hutchins, editor, Early Years in Machine Translation: Memoirs and Biographies of Pioneers. John Benjamins, Amsterdam/Philadelphia, pages 39–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>Hard-science Linguistics. Continuum,</title>
<date>2004</date>
<editor>and Zdzisław W ˛asik, editors.</editor>
<location>London/New York.</location>
<marker>Yngve, 2004</marker>
<rawString>Yngve, Victor H., and Zdzisław W ˛asik, editors. 2004. Hard-science Linguistics. Continuum, London/New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>