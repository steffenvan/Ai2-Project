<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000205">
<title confidence="0.797371">
Determining Term Subjectivity and Term Orientation for Opinion Mining
</title>
<note confidence="0.6781138">
Andrea Esulii and Fabrizio Sebastiani&apos;
(1) Istituto di Scienza e Tecnologie dell’Informazione – Consiglio Nazionale delle Ricerche
Via G Moruzzi, 1 – 56124 Pisa, Italy
andrea.esuli@isti.cnr.it
(2) Dipartimento di Matematica Pura e Applicata – Universit`a di Padova
</note>
<address confidence="0.485999">
Via GB Belzoni, 7 – 35131 Padova, Italy
</address>
<email confidence="0.984869">
fabrizio.sebastiani@unipd.it
</email>
<sectionHeader confidence="0.997165" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999936583333333">
Opinion mining is a recent subdiscipline
of computational linguistics which is con-
cerned not with the topic a document is
about, but with the opinion it expresses.
To aid the extraction of opinions from
text, recent work has tackled the issue
of determining the orientation of “subjec-
tive” terms contained in text, i.e. decid-
ing whether a term that carries opinion-
ated content has a positive or a negative
connotation. This is believed to be of key
importance for identifying the orientation
of documents, i.e. determining whether a
document expresses a positive or negative
opinion about its subject matter.
We contend that the plain determination
of the orientation of terms is not a realis-
tic problem, since it starts from the non-
realistic assumption that we already know
whether a term is subjective or not; this
would imply that a linguistic resource that
marks terms as “subjective” or “objective”
is available, which is usually not the case.
In this paper we confront the task of de-
ciding whether a given term has a positive
connotation, or a negative connotation, or
has no subjective connotation at all; this
problem thus subsumes the problem of de-
termining subjectivity and the problem of
determining orientation. We tackle this
problem by testing three different variants
of a semi-supervised method previously
proposed for orientation detection. Our
results show that determining subjectivity
and orientation is a much harder problem
than determining orientation alone.
</bodyText>
<sectionHeader confidence="0.999509" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999874083333333">
Opinion mining is a recent subdiscipline of com-
putational linguistics which is concerned not with
the topic a document is about, but with the opinion
it expresses. Opinion-driven content management
has several important applications, such as deter-
mining critics’ opinions about a given product by
classifying online product reviews, or tracking the
shifting attitudes of the general public toward a po-
litical candidate by mining online forums.
Within opinion mining, several subtasks can be
identified, all of them having to do with tagging a
given document according to expressed opinion:
</bodyText>
<listItem confidence="0.841320789473684">
1. determining document subjectivity, as in de-
ciding whether a given text has a factual na-
ture (i.e. describes a given situation or event,
without expressing a positive or a negative
opinion on it) or expresses an opinion on its
subject matter. This amounts to performing
binary text categorization under categories
Objective and Subjective (Pang and Lee,
2004; Yu and Hatzivassiloglou, 2003);
2. determining document orientation (or polar-
ity), as in deciding if a given Subjective text
expresses a Positive or a Negative opinion
on its subject matter (Pang and Lee, 2004;
Turney, 2002);
3. determining the strength of document orien-
tation, as in deciding e.g. whether the Posi-
tive opinion expressed by a text on its subject
matter is Weakly Positive, Mildly Positive,
or Strongly Positive (Wilson et al., 2004).
</listItem>
<bodyText confidence="0.99858275">
To aid these tasks, recent work (Esuli and Se-
bastiani, 2005; Hatzivassiloglou and McKeown,
1997; Kamps et al., 2004; Kim and Hovy, 2004;
Takamura et al., 2005; Turney and Littman, 2003)
has tackled the issue of identifying the orientation
of subjective terms contained in text, i.e. determin-
ing whether a term that carries opinionated content
has a positive or a negative connotation (e.g. de-
ciding that — using Turney and Littman’s (2003)
examples — honest and intrepid have a
positive connotation while disturbing and
superfluous have a negative connotation).
</bodyText>
<page confidence="0.99857">
193
</page>
<bodyText confidence="0.999907711111112">
This is believed to be of key importance for iden-
tifying the orientation of documents, since it is
by considering the combined contribution of these
terms that one may hope to solve Tasks 1, 2 and 3
above. The conceptually simplest approach to this
latter problem is probably Turney’s (2002), who
has obtained interesting results on Task 2 by con-
sidering the algebraic sum of the orientations of
terms as representative of the orientation of the
document they belong to; but more sophisticated
approaches are also possible (Hatzivassiloglou and
Wiebe, 2000; Riloff et al., 2003; Wilson et al.,
2004).
Implicit in most works dealing with term orien-
tation is the assumption that, for many languages
for which one would like to perform opinion min-
ing, there is no available lexical resource where
terms are tagged as having either a Positive or a
Negative connotation, and that in the absence of
such a resource the only available route is to gen-
erate such a resource automatically.
However, we think this approach lacks real-
ism, since it is also true that, for the very same
languages, there is no available lexical resource
where terms are tagged as having either a Subjec-
tive or an Objective connotation. Thus, the avail-
ability of an algorithm that tags Subjective terms
as being either Positive or Negative is of little
help, since determining if a term is Subjective is
itself non-trivial.
In this paper we confront the task of de-
termining whether a given term has a Pos-
itive connotation (e.g. honest, intrepid),
or a Negative connotation (e.g. disturbing,
superfluous), or has instead no Subjective
connotation at all (e.g. white, triangular);
this problem thus subsumes the problem of decid-
ing between Subjective and Objective and the
problem of deciding between Positive and Neg-
ative. We tackle this problem by testing three dif-
ferent variants of the semi-supervised method for
orientation detection proposed in (Esuli and Se-
bastiani, 2005). Our results show that determining
subjectivity and orientation is a much harder prob-
lem than determining orientation alone.
</bodyText>
<subsectionHeader confidence="0.996704">
1.1 Outline of the paper
</subsectionHeader>
<bodyText confidence="0.999873727272727">
The rest of the paper is structured as follows. Sec-
tion 2 reviews related work dealing with term ori-
entation and/or subjectivity detection. Section 3
briefly reviews the semi-supervised method for
orientation detection presented in (Esuli and Se-
bastiani, 2005). Section 4 describes in detail three
different variants of it we propose for determining,
at the same time, subjectivity and orientation, and
describes the general setup of our experiments. In
Section 5 we discuss the results we have obtained.
Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.999597" genericHeader="introduction">
2 Related work
</sectionHeader>
<subsectionHeader confidence="0.999708">
2.1 Determining term orientation
</subsectionHeader>
<bodyText confidence="0.999979630434783">
Most previous works dealing with the properties
of terms within an opinion mining perspective
have focused on determining term orientation.
Hatzivassiloglou and McKeown (1997) attempt
to predict the orientation of subjective adjectives
by analysing pairs of adjectives (conjoined by
and, or, but, either-or, or neither-nor)
extracted from a large unlabelled document set.
The underlying intuition is that the act of conjoin-
ing adjectives is subject to linguistic constraints
on the orientation of the adjectives involved; e.g.
and usually conjoins adjectives of equal orienta-
tion, while but conjoins adjectives of opposite
orientation. The authors generate a graph where
terms are nodes connected by “equal-orientation”
or “opposite-orientation” edges, depending on the
conjunctions extracted from the document set. A
clustering algorithm then partitions the graph into
a Positive cluster and a Negative cluster, based
on a relation of similarity induced by the edges.
Turney and Littman (2003) determine term ori-
entation by bootstrapping from two small sets of
subjective “seed” terms (with the seed set for Pos-
itive containing terms such as good and nice,
and the seed set for Negative containing terms
such as bad and nasty). Their method is based
on computing the pointwise mutual information
(PMI) of the target term t with each seed term
ti as a measure of their semantic association.
Given a target term t, its orientation value O(t)
(where positive value means positive orientation,
and higher absolute value means stronger orien-
tation) is given by the sum of the weights of its
semantic association with the seed positive terms
minus the sum of the weights of its semantic as-
sociation with the seed negative terms. For com-
puting PMI, term frequencies and co-occurrence
frequencies are measured by querying a document
set by means of the AltaVista search engine1 with
a “t” query, a “ti” query, and a “t NEAR ti” query,
and using the number of matching documents re-
turned by the search engine as estimates of the
probabilities needed for the computation of PMI.
Kamps et al. (2004) consider instead the graph
defined on adjectives by the WordNet2 synonymy
relation, and determine the orientation of a target
</bodyText>
<footnote confidence="0.999977">
1http://www.altavista.com/
2http://wordnet.princeton.edu/
</footnote>
<page confidence="0.998477">
194
</page>
<bodyText confidence="0.999903605633803">
adjective t contained in the graph by comparing
the lengths of (i) the shortest path between t and
the seed term good, and (ii) the shortest path be-
tween t and the seed term bad: if the former is
shorter than the latter, than t is deemed to be Pos-
itive, otherwise it is deemed to be Negative.
Takamura et al. (2005) determine term orienta-
tion (for Japanese) according to a “spin model”,
i.e. a physical model of a set of electrons each
endowed with one between two possible spin di-
rections, and where electrons propagate their spin
direction to neighbouring electrons until the sys-
tem reaches a stable configuration. The authors
equate terms with electrons and term orientation
to spin direction. They build a neighbourhood ma-
trix connecting each pair of terms if one appears in
the gloss of the other, and iteratively apply the spin
model on the matrix until a “minimum energy”
configuration is reached. The orientation assigned
to a term then corresponds to the spin direction as-
signed to electrons.
The system of Kim and Hovy (2004) tackles ori-
entation detection by attributing, to each term, a
positivity score and a negativity score; interest-
ingly, terms may thus be deemed to have both a
positive and a negative correlation, maybe with
different degrees, and some terms may be deemed
to carry a stronger positive (or negative) orienta-
tion than others. Their system starts from a set
of positive and negative seed terms, and expands
the positive (resp. negative) seed set by adding to
it the synonyms of positive (resp. negative) seed
terms and the antonyms of negative (resp. positive)
seed terms. The system classifies then a target
term t into either Positive or Negative by means
of two alternative learning-free methods based on
the probabilities that synonyms of t also appear in
the respective expanded seed sets. A problem with
this method is that it can classify only terms that
share some synonyms with the expanded seed sets.
Kim and Hovy also report an evaluation of human
inter-coder agreement. We compare this evalua-
tion with our results in Section 5.
The approach we have proposed for determin-
ing term orientation (Esuli and Sebastiani, 2005)
is described in more detail in Section 3, since it
will be extensively used in this paper.
All these works evaluate the performance of
the proposed algorithms by checking them against
precompiled sets of Positive and Negative terms,
i.e. checking how good the algorithms are at clas-
sifying a term known to be subjective into either
Positive or Negative. When tested on the same
benchmarks, the methods of (Esuli and Sebastiani,
2005; Turney and Littman, 2003) have performed
with comparable accuracies (however, the method
of (Esuli and Sebastiani, 2005) is much more effi-
cient than the one of (Turney and Littman, 2003)),
and have outperformed the method of (Hatzivas-
siloglou and McKeown, 1997) by a wide margin
and the one by (Kamps et al., 2004) by a very
wide margin. The methods described in (Hatzi-
vassiloglou and McKeown, 1997) is also limited
by the fact that it can only decide the orientation
of adjectives, while the method of (Kamps et al.,
2004) is further limited in that it can only work
on adjectives that are present in WordNet. The
methods of (Kim and Hovy, 2004; Takamura et
al., 2005) are instead difficult to compare with the
other ones since they were not evaluated on pub-
licly available datasets.
</bodyText>
<subsectionHeader confidence="0.998057">
2.2 Determining term subjectivity
</subsectionHeader>
<bodyText confidence="0.99997375">
Riloff et al. (2003) develop a method to determine
whether a term has a Subjective or an Objective
connotation, based on bootstrapping algorithms.
The method identifies patterns for the extraction
of subjective nouns from text, bootstrapping from
a seed set of 20 terms that the authors judge to be
strongly subjective and have found to have high
frequency in the text collection from which the
subjective nouns must be extracted. The results
of this method are not easy to compare with the
ones we present in this paper because of the dif-
ferent evaluation methodologies. While we adopt
the evaluation methodology used in all of the pa-
pers reviewed so far (i.e. checking how good our
system is at replicating an existing, independently
motivated lexical resource), the authors do not test
their method on an independently identified set of
labelled terms, but on the set of terms that the algo-
rithm itself extracts. This evaluation methodology
only allows to test precision, and not accuracy tout
court, since no quantification can be made of false
negatives (i.e. the subjective terms that the algo-
rithm should have spotted but has not spotted). In
Section 5 this will prevent us from drawing com-
parisons between this method and our own.
Baroni and Vegnaduzzo (2004) apply the PMI
method, first used by Turney and Littman (2003)
to determine term orientation, to determine term
subjectivity. Their method uses a small set S,
of 35 adjectives, marked as subjective by human
judges, to assign a subjectivity score to each adjec-
tive to be classified. Therefore, their method, un-
like our own, does not classify terms (i.e. take firm
classification decisions), but ranks them according
to a subjectivity score, on which they evaluate pre-
cision at various level of recall.
</bodyText>
<page confidence="0.997776">
195
</page>
<sectionHeader confidence="0.899634" genericHeader="method">
3 Determining term subjectivity and
</sectionHeader>
<bodyText confidence="0.989790519230769">
term orientation by semi-supervised
learning
The method we use in this paper for determining
term subjectivity and term orientation is a variant
of the method proposed in (Esuli and Sebastiani,
2005) for determining term orientation alone.
This latter method relies on training, in a semi-
supervised way, a binary classifier that labels
terms as either Positive or Negative. A semi-
supervised method is a learning process whereby
only a small subset L C Tr of the training data
Tr are human-labelled. In origin the training
data in U = Tr − L are instead unlabelled; it
is the process itself that labels them, automati-
cally, by using L (with the possible addition of
other publicly available resources) as input. The
method of (Esuli and Sebastiani, 2005) starts from
two small seed (i.e. training) sets Lp and Ln of
known Positive and Negative terms, respectively,
and expands them into the two final training sets
Trp D Lp and Trn D Ln by adding them new sets
of terms Up and Un found by navigating the Word-
Net graph along the synonymy and antonymy re-
lations3. This process is based on the hypothesis
that synonymy and antonymy, in addition to defin-
ing a relation of meaning, also define a relation of
orientation, i.e. that two synonyms typically have
the same orientation and two antonyms typically
have opposite orientation. The method is iterative,
generating two sets Trkp and Trknat each iteration
k, where Trkp D Trk−1
p D ... D Tr1 p = Lp
and Trkn D Trk−1 n D ... D Tr1 n = Ln. At
iteration k, Trkp is obtained by adding to Trk−1
p
all synonyms of terms in Trk−1 pand all antonyms
of terms in Trk−1
n ; similarly, Trknis obtained by
adding to Trk−1
n all synonyms of terms in Trk−1
n
and all antonyms of terms in Trk−1
p . If a total of K
iterations are performed, then Tr = TrKp U TrKn .
The second main feature of the method pre-
sented in (Esuli and Sebastiani, 2005) is that terms
are given vectorial representations based on their
WordNet glosses (i.e. textual definitions). For
each term ti in Tr U Te (Te being the test set, i.e.
the set of terms to be classified), a textual represen-
tation of ti is generated by collating all the glosses
of ti as found in WordNet4. Each such represen-
</bodyText>
<footnote confidence="0.479645714285714">
3Several other WordNet lexical relations, and several
combinations of them, are tested in (Esuli and Sebastiani,
2005). In the present paper we only use the best-performing
such combination, as described in detail in Section 4.2. The
version of WordNet used here and in (Esuli and Sebastiani,
2005) is 2.0.
4In general a term ti may have more than one gloss, since
</footnote>
<bodyText confidence="0.9963845">
tation is converted into vectorial form by standard
text indexing techniques (in (Esuli and Sebastiani,
2005) and in the present work, stop words are
removed and the remaining words are weighted
by cosine-normalized tfidf; no stemming is per-
formed)5. This representation method is based on
the assumption that terms with a similar orienta-
tion tend to have “similar” glosses: for instance,
that the glosses of honest and intrepid will
both contain appreciative expressions, while the
glosses of disturbing and superfluous
will both contain derogative expressions. Note
that this method allows to classify any term, in-
dependently of its POS, provided there is a gloss
for it in the lexical resource.
Once the vectorial representations for all terms
in TrUT e have been generated, those for the terms
in Tr are fed to a supervised learner, which thus
generates a binary classifier. This latter, once fed
with the vectorial representations of the terms in
Te, classifies each of them as either Positive or
Negative.
</bodyText>
<sectionHeader confidence="0.999621" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.977396333333333">
In this paper we extend the method of (Esuli and
Sebastiani, 2005) to the determination of term sub-
jectivity and term orientation altogether.
</bodyText>
<subsectionHeader confidence="0.999381">
4.1 Test sets
</subsectionHeader>
<bodyText confidence="0.967485923076923">
The benchmark (i.e. test set) we use for our exper-
iments is the General Inquirer (GI) lexicon (Stone
et al., 1966). This is a lexicon of terms labelled
according to a large set of categories6, each one
denoting the presence of a specific trait in the
term. The two main categories, and the ones we
will be concerned with, are Positive/Negative,
which contain 1,915/2,291 terms having a posi-
tive/negative orientation (in what follows we will
also refer to the category Subjective, which we
define as the union of the two categories Positive
and Negative). In opinion mining research the GI
was first used by Turney and Littman (2003), who
reduced the list of terms to 1,614/1,982 entries af-
it may have more than one sense; dictionaries normally asso-
ciate one gloss to each sense.
5Several combinations of subparts of a WordNet gloss are
tested as textual representations of terms in (Esuli and Sebas-
tiani, 2005). Of all those combinations, in the present paper
we always use the DGS- combination, since this is the one
that has been shown to perform best in (Esuli and Sebastiani,
2005). DGS- corresponds to using the entire gloss and per-
forming negation propagation on its text, i.e. replacing all the
terms that occur after a negation in a sentence with negated
versions of the term (see (Esuli and Sebastiani, 2005) for de-
tails).
</bodyText>
<footnote confidence="0.9834545">
6The definitions of all such categories are available at
http://www.webuse.umd.edu:9090/
</footnote>
<page confidence="0.997189">
196
</page>
<bodyText confidence="0.998708846153846">
ter removing 17 terms appearing in both categories
(e.g. deal) and reducing all the multiple entries
of the same term in a category, caused by multi-
ple senses, to a single entry. Likewise, we take
all the 7,582 GI terms that are not labelled as ei-
ther Positive or Negative, as being (implicitly)
labelled as Objective, and reduce them to 5,009
terms after combining multiple entries of the same
term, caused by multiple senses, to a single entry.
The effectiveness of our classifiers will thus be
evaluated in terms of their ability to assign the to-
tal 8,605 GI terms to the correct category among
Positive, Negative, and Objective7.
</bodyText>
<subsectionHeader confidence="0.999236">
4.2 Seed sets and training sets
</subsectionHeader>
<bodyText confidence="0.999857756756757">
Similarly to (Esuli and Sebastiani, 2005), our
training set is obtained by expanding initial seed
sets by means of WordNet lexical relations. The
main difference is that our training set is now
the union of three sets of training terms Tr =
TrKp ∪TrKn ∪TrKo obtained by expanding, through
K iterations, three seed sets Tr1p, Tr1n, Tr1o, one
for each of the categories Positive, Negative, and
Objective, respectively.
Concerning categories Positive and Negative,
we have used the seed sets, expansion policy, and
number of iterations, that have performed best in
the experiments of (Esuli and Sebastiani, 2005),
i.e. the seed sets Tr1p = {good} and Tr1 n =
{bad} expanded by using the union of synonymy
and indirect antonymy, restricting the relations
only to terms with the same POS of the original
terms (i.e. adjectives), for a total of K = 4 itera-
tions. The final expanded sets contain 6,053 Pos-
itive terms and 6,874 Negative terms.
Concerning the category Objective, the pro-
cess we have followed is similar, but with a few
key differences. These are motivated by the fact
that the Objective category coincides with the
complement of the union of Positive and Neg-
ative; therefore, Objective terms are more var-
ied and diverse in meaning than the terms in the
other two categories. To obtain a representative
expanded set TrKo , we have chosen the seed set
Tr1o = {entity} and we have expanded it by
using, along with synonymy and antonymy, the
WordNet relation of hyponymy (e.g. vehicle /
car), and without imposing the restriction that the
two related terms must have the same POS. These
choices are strictly related to each other: the term
entity is the root term of the largest generaliza-
tion hierarchy in WordNet, with more than 40,000
</bodyText>
<footnote confidence="0.887833666666667">
7We make this labelled term set available for download at
http://patty.isti.cnr.it/˜esuli/software/
SentiGI.tgz.
</footnote>
<bodyText confidence="0.999150142857143">
terms (Devitt and Vogel, 2004), thus allowing to
reach a very large number of terms by using the
hyponymy relation8. Moreover, it seems reason-
able to assume that terms that refer to entities are
likely to have an “objective” nature, and that hy-
ponyms (and also synonyms and antonyms) of an
objective term are also objective. Note that, at
each iteration k, a given term t is added to Trko
only if it does not already belong to either Trp or
Trn. We experiment with two different choices
for the Tro set, corresponding to the sets gener-
ated in K = 3 and K = 4 iterations, respectively;
this yields sets Tr3o and Tr4o consisting of 8,353
and 33,870 training terms, respectively.
</bodyText>
<subsectionHeader confidence="0.975224">
4.3 Learning approaches and evaluation
measures
</subsectionHeader>
<bodyText confidence="0.999553028571428">
We experiment with three “philosophically” dif-
ferent learning approaches to the problem of dis-
tinguishing between Positive, Negative, and Ob-
jective terms.
Approach I is a two-stage method which con-
sists in learning two binary classifiers: the first
classifier places terms into either Subjective or
Objective, while the second classifier places
terms that have been classified as Subjective by
the first classifier into either Positive or Negative.
In the training phase, the terms in TrKp ∪ TrKn are
used as training examples of category Subjective.
Approach II is again based on learning two bi-
nary classifiers. Here, one of them must discrim-
inate between terms that belong to the Positive
category and ones that belong to its complement
(not Positive), while the other must discriminate
between terms that belong to the Negative cate-
gory and ones that belong to its complement (not
Negative). Terms that have been classified both
into Positive by the former classifier and into (not
Negative) by the latter are deemed to be positive,
and terms that have been classified both into (not
Positive) by the former classifier and into Nega-
tive by the latter are deemed to be negative. The
terms that have been classified (i) into both (not
Positive) and (not Negative), or (ii) into both
Positive and Negative, are taken to be Objec-
tive. In the training phase of Approach II, the
terms in TrKn ∪ TrKo are used as training exam-
ples of category (not Positive), and the terms in
TrKp ∪ TrKo are used as training examples of cat-
egory (not Negative).
Approach III consists instead in viewing Posi-
tive, Negative, and Objective as three categories
</bodyText>
<footnote confidence="0.99332">
8The synonymy relation connects instead only 10,992
terms at most (Kamps et al., 2004).
</footnote>
<page confidence="0.997981">
197
</page>
<bodyText confidence="0.9999804375">
with equal status, and in learning a ternary clas-
sifier that classifies each term into exactly one
among the three categories.
There are several differences among these three
approaches. A first difference, of a conceptual
nature, is that only Approaches I and III view
Objective as a category, or concept, in its own
right, while Approach II views objectivity as a
nonexistent entity, i.e. as the “absence of subjec-
tivity” (in fact, in Approach II the training exam-
ples of Objective are only used as training exam-
ples of the complements of Positive and Nega-
tive). A second difference is that Approaches I and
II are based on standard binary classification tech-
nology, while Approach III requires “multiclass”
(i.e. 1-of-m) classification. As a consequence,
while for the former we use well-known learn-
ers for binary classification (the naive Bayesian
learner using the multinomial model (McCallum
and Nigam, 1998), support vector machines us-
ing linear kernels (Joachims, 1998), the Roc-
chio learner, and its PrTFIDF probabilistic version
(Joachims, 1997)), for Approach III we use their
multiclass versions9.
Before running our learners we make a pass of
feature selection, with the intent of retaining only
those features that are good at discriminating our
categories, while discarding those which are not.
Feature selection is implemented by scoring each
feature fk (i.e. each term that occurs in the glosses
of at least one training term) by means of the mu-
tual information (MI) function, defined as
</bodyText>
<equation confidence="0.998045">
Pr(f, c)
Pr(f, c) · log (1)
Pr(f) Pr(c)
</equation>
<bodyText confidence="0.99918625">
and discarding the x% features fk that minimize
it. We will call x% the reduction factor. Note that
the set {c1, ... , cm} from Equation 1 is interpreted
differently in Approaches I to III, and always con-
sistently with who the categories at stake are.
Since the task we aim to solve is manifold, we
will evaluate our classifiers according to two eval-
uation measures:
</bodyText>
<listItem confidence="0.9666318">
• SO-accuracy, i.e. the accuracy of a classifier
in separating Subjective from Objective, i.e.
in deciding term subjectivity alone;
• PNO-accuracy, the accuracy of a classifier
in discriminating among Positive, Negative,
</listItem>
<footnote confidence="0.879111">
9The naive Bayesian, Rocchio, and PrTFIDF learners
we have used are from Andrew McCallum’s Bow package
(http://www-2.cs.cmu.edu/˜mccallum/bow/),
while the SVMs learner we have used is Thorsten Joachims’
SVMlight (http://svmlight.joachims.org/),
version 6.01. Both packages allow the respective learners to
be run in “multiclass” fashion.
</footnote>
<tableCaption confidence="0.975842">
Table 1: Average and best accuracy values over
the four dimensions analysed in the experiments.
</tableCaption>
<table confidence="0.999261826086957">
Dimension SO-accuracy PNO-accuracy
Avg (v) Best Avg (v) Best
Approach
I .635 (.020) .668 .595 (.029) .635
II .636 (.033) .676 .614 (.037) .660
III .635 (.036) .674 .600 (.039) .648
Learner
NB .653 (.014) .674 .619 (.022) .647
SVMs .627 (.033) .671 .601 (.037) .658
Rocchio .624 (.030) .654 .585 (.033) .616
PrTFIDF .637 (.031) .676 .606 (.042) .660
TSR
0% .649 (.025) .676 .619 (.027) .660
50% .650 (.022) .670 .622 (.022) .657
80% .646 (.023) .674 .621 (.021) .647
90% .642 (.024) .667 .616 (.024) .651
95% .635 (.027) .671 .606 (.031) .658
99% .612 (.036) .661 .570 (.049) .647
TrKo set
Tr3 .645 (.006) .676 .608 (.007) .658
o
Tr4 .633 (.013) .674 .610 (.018) .660
o
</table>
<bodyText confidence="0.6973715">
and Objective, i.e. in deciding both term ori-
entation and subjectivity.
</bodyText>
<sectionHeader confidence="0.999812" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999619034482759">
We present results obtained from running every
combination of (i) the three approaches to classifi-
cation described in Section 4.3, (ii) the four learn-
ers mentioned in the same section, (iii) five dif-
ferent reduction factors for feature selection (0%,
50%, 90%, 95%, 99%), and (iv) the two different
training sets (Tr3o and Tr4o) for Objective men-
tioned in Section 4.2. We discuss each of these
four dimensions of the problem individually, for
each one reporting results averaged across all the
experiments we have run (see Table 1).
The first and most important observation is that,
with respect to a pure term orientation task, ac-
curacy drops significantly. In fact, the best SO-
accuracy and the best PNO-accuracy results ob-
tained across the 120 different experiments are
.676 and .660, respectively (these were obtained
by using Approach II with the PrTFIDF learner
and no feature selection, with Tro = Tr3o for the
.676 SO-accuracy result and Tro = Tr4o for the
.660 PNO-accuracy result); this contrasts sharply
with the accuracy obtained in (Esuli and Sebas-
tiani, 2005) on discriminating Positive from Neg-
ative (where the best run obtained .830 accuracy),
on the same benchmarks and essentially the same
algorithms. This suggests that good performance
at orientation detection (as e.g. in (Esuli and Se-
bastiani, 2005; Hatzivassiloglou and McKeown,
1997; Turney and Littman, 2003)) may not be a
</bodyText>
<equation confidence="0.8666395">
�
MI(fk) =
</equation>
<bodyText confidence="0.6211875">
cE{cl,...,cm},
fE{fk,fk}
</bodyText>
<page confidence="0.998597">
198
</page>
<tableCaption confidence="0.9780905">
Table 2: Human inter-coder agreement values re-
ported by Kim and Hovy (2004).
</tableCaption>
<table confidence="0.9988194">
Agreement Adjectives (462) Verbs (502)
measure
Hum1 vs Hum2 Hum2 vs Hum3
Strict .762 .623
Lenient .890 .851
</table>
<bodyText confidence="0.995860043478261">
guarantee of good performance at subjectivity de-
tection, quite evidently a harder (and, as we have
suggested, more realistic) task.
This hypothesis is confirmed by an experiment
performed by Kim and Hovy (2004) on testing
the agreement of two human coders at tagging
words with the Positive, Negative, and Objec-
tive labels. The authors define two measures of
such agreement: strict agreement, equivalent to
our PNO-accuracy, and lenient agreement, which
measures the accuracy at telling Negative against
the rest. For any experiment, strict agreement val-
ues are then going to be, by definition, lower or
equal than the corresponding lenient ones. The au-
thors use two sets of 462 adjectives and 502 verbs,
respectively, randomly extracted from the basic
English word list of the TOEFL test. The inter-
coder agreement results (see Table 2) show a de-
terioration in agreement (from lenient to strict) of
16.77% for adjectives and 36.42% for verbs. Fol-
lowing this, we evaluated our best experiment ac-
cording to these measures, and obtained a “strict”
accuracy value of .660 and a “lenient” accuracy
value of .821, with a relative deterioration of
24.39%, in line with Kim and Hovy’s observa-
tion10. This confirms that determining subjectivity
and orientation is a much harder task than deter-
mining orientation alone.
The second important observation is that there
is very little variance in the results: across all 120
experiments, average SO-accuracy and PNO-
accuracy results were .635 (with standard devia-
tion Q = .030) and .603 (Q = .036), a mere
6.06% and 8.64% deterioration from the best re-
sults reported above. This seems to indicate that
the levels of performance obtained may be hard to
improve upon, especially if working in a similar
framework.
Let us analyse the individual dimensions of the
problem. Concerning the three approaches to clas-
sification described in Section 4.3, Approach II
outperforms the other two, but by an extremely
narrow margin. As for the choice of learners, on
average the best performer is NB, but again by a
very small margin wrt the others. On average, the
10We observed this trend in all of our experiments.
best reduction factor for feature selection turns out
to be 50%, but the performance drop we witness
in approaching 99% (a dramatic reduction factor)
is extremely graceful. As for the choice of TrKo ,
we note that Tro and Tr4o elicit comparable levels
of performance, with the former performing best
at SO-accuracy and the latter performing best at
PNO-accuracy.
An interesting observation on the learners we
have used is that NB, PrTFIDF and SVMs, un-
like Rocchio, generate classifiers that depend on
P(ci), the prior probabilities of the classes, which
are normally estimated as the proportion of train-
ing documents that belong to ci. In many classi-
fication applications this is reasonable, as we may
assume that the training data are sampled from the
same distribution from which the test data are sam-
pled, and that these proportions are thus indica-
tive of the proportions that we are going to en-
counter in the test data. However, in our appli-
cation this is not the case, since we do not have a
“natural” sample of training terms. What we have
is one human-labelled training term for each cat-
egory in {Positive,Negative,Objective}, and as
many machine-labelled terms as we deem reason-
able to include, in possibly different numbers for
the different categories; and we have no indica-
tion whatsoever as to what the “natural” propor-
tions among the three might be. This means that
the proportions of Positive, Negative, and Ob-
jective terms we decide to include in the train-
ing set will strongly bias the classification results
if the learner is one of NB, PrTFIDF and SVMs.
We may notice this by looking at Table 3, which
shows the average proportion of test terms classi-
fied as Objective by each learner, depending on
whether we have chosen Tro to coincide with Tr3o
or Tr4o; note that the former (resp. latter) choice
means having roughly as many (resp. roughly five
times as many) Objective training terms as there
are Positive and Negative ones. Table 3 shows
that, the more Objective training terms there are,
the more test terms NB, PrTFIDF and (in partic-
ular) SVMs will classify as Objective; this is not
true for Rocchio, which is basically unaffected by
the variation in size of Tro.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999968166666667">
We have presented a method for determining both
term subjectivity and term orientation for opinion
mining applications. This is a valuable advance
with respect to the state of the art, since past work
in this area had mostly confined to determining
term orientation alone, a task that (as we have ar-
</bodyText>
<page confidence="0.999085">
199
</page>
<tableCaption confidence="0.939702">
Table 3: Average proportion of test terms classi-
</tableCaption>
<bodyText confidence="0.7712045">
fied as Objective, for each learner and for each
choice of the Tro set.
</bodyText>
<table confidence="0.861707">
Learner T r3 T r4 Variation
o o
NB .564 (v = .069) .693 (.069) +23.0%
SVMs .601 (.108) .814 (.083) +35.4%
Rocchio .572 (.043) .544 (.061) -4.8%
PrTFIDF .636 (.059) .763 (.085) +20.0%
</table>
<bodyText confidence="0.999675952380952">
gued) has limited practical significance in itself,
given the generalized absence of lexical resources
that tag terms as being either Subjective or Ob-
jective. Our algorithms have tagged by orienta-
tion and subjectivity the entire General Inquirer
lexicon, a complete general-purpose lexicon that
is the de facto standard benchmark for researchers
in this field. Our results thus constitute, for this
task, the first baseline for other researchers to im-
prove upon.
Unfortunately, our results have shown that
an algorithm that had shown excellent, state-
of-the-art performance in deciding term orienta-
tion (Esuli and Sebastiani, 2005), once modified
for the purposes of deciding term subjectivity, per-
forms more poorly. This has been shown by test-
ing several variants of the basic algorithm, some
of them involving radically different supervised
learning policies. The results suggest that decid-
ing term subjectivity is a substantially harder task
that deciding term orientation alone.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999215">
M. Baroni and S. Vegnaduzzo. 2004. Identifying subjec-
tive adjectives through Web-based mutual information. In
Proceedings of KONVENS-04, 7th Konferenz zur Verar-
beitung Nat¨urlicher Sprache (German Conference on Nat-
ural Language Processing), pages 17–24, Vienna, AU.
Ann Devitt and Carl Vogel. 2004. The topology of WordNet:
Some metrics. In Proceedings of GWC-04, 2nd Global
WordNet Conference, pages 106–111, Brno, CZ.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determin-
ing the semantic orientation of terms through gloss analy-
sis. In Proceedings of CIKM-05, 14th ACM International
Conference on Information and Knowledge Management,
pages 617–624, Bremen, DE.
Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997.
Predicting the semantic orientation of adjectives. In Pro-
ceedings of ACL-97, 35th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 174–181,
Madrid, ES.
Vasileios Hatzivassiloglou and Janyce M. Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sentence
subjectivity. In Proceedings of COLING-00, 18th Inter-
national Conference on Computational Linguistics, pages
174–181, Saarbr¨ucken, DE.
Thorsten Joachims. 1997. A probabilistic analysis of the
Rocchio algorithm with TFIDF for text categorization. In
Proceedings of ICML-97, 14th International Conference
on Machine Learning, pages 143–151, Nashville, US.
Thorsten Joachims. 1998. Text categorization with support
vector machines: learning with many relevant features. In
Proceedings of ECML-98, 10th European Conference on
Machine Learning, pages 137–142, Chemnitz, DE.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten
De Rijke. 2004. Using WordNet to measure semantic ori-
entation of adjectives. In Proceedings of LREC-04, 4th In-
ternational Conference on Language Resources and Eval-
uation, volume IV, pages 1115–1118, Lisbon, PT.
Soo-Min Kim and Eduard Hovy. 2004. Determining the sen-
timent of opinions. In Proceedings of COLING-04, 20th
International Conference on Computational Linguistics,
pages 1367–1373, Geneva, CH.
Andrew K. McCallum and Kamal Nigam. 1998. A compari-
son of event models for naive Bayes text classification. In
Proceedings of the AAAI Workshop on Learning for Text
Categorization, pages 41–48, Madison, US.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of ACL-04, 42nd
Meeting of the Association for Computational Linguistics,
pages 271–278, Barcelona, ES.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern boot-
strapping. In Proceedings of CONLL-03, 7th Conference
on Natural Language Learning, pages 25–32, Edmonton,
CA.
P. J. Stone, D. C. Dunphy, M. S. Smith, and D. M. Ogilvie.
1966. The General Inquirer: A Computer Approach to
Content Analysis. MIT Press, Cambridge, US.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting emotional polarity of words using spin
model. In Proceedings of ACL-05, 43rd Annual Meeting
of the Association for Computational Linguistics, pages
133–140, Ann Arbor, US.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information Sys-
tems, 21(4):315–346.
Peter Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification of re-
views. In Proceedings of ACL-02, 40th Annual Meeting
of the Association for Computational Linguistics, pages
417–424, Philadelphia, US.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004.
Just how mad are you? Finding strong and weak opinion
clauses. In Proceedings of AAAI-04, 21st Conference of
the American Association for Artificial Intelligence, pages
761–769, San Jose, US.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from opin-
ions and identifying the polarity of opinion sentences. In
Proceedings of EMNLP-03, 8th Conference on Empiri-
cal Methods in Natural Language Processing, pages 129–
136, Sapporo, JP.
</reference>
<page confidence="0.996626">
200
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.513593">
<title confidence="0.998328">Determining Term Subjectivity and Term Orientation for Opinion Mining</title>
<author confidence="0.84276">Fabrizio</author>
<affiliation confidence="0.676723">(1) Istituto di Scienza e Tecnologie dell’Informazione – Consiglio Nazionale delle Ricerche</affiliation>
<address confidence="0.752449">Via G Moruzzi, 1 – 56124 Pisa, Italy</address>
<email confidence="0.978298">andrea.esuli@isti.cnr.it</email>
<affiliation confidence="0.944837">(2) Dipartimento di Matematica Pura e Applicata – Universit`a di Padova</affiliation>
<address confidence="0.953852">Via GB Belzoni, 7 – 35131 Padova, Italy</address>
<email confidence="0.999227">fabrizio.sebastiani@unipd.it</email>
<abstract confidence="0.996015081081081">mining a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. To aid the extraction of opinions from text, recent work has tackled the issue determining the “subjective” terms contained in text, i.e. deciding whether a term that carries opinionated content has a positive or a negative connotation. This is believed to be of key importance for identifying the orientation of documents, i.e. determining whether a document expresses a positive or negative opinion about its subject matter. We contend that the plain determination of the orientation of terms is not a realistic problem, since it starts from the nonrealistic assumption that we already know whether a term is subjective or not; this would imply that a linguistic resource that marks terms as “subjective” or “objective” is available, which is usually not the case. In this paper we confront the task of deciding whether a given term has a positive or a negative connotation, no subjective connotation at this problem thus subsumes the problem of desubjectivity problem of determining orientation. We tackle this problem by testing three different variants of a semi-supervised method previously proposed for orientation detection. Our results show that determining subjectivity is a much harder problem than determining orientation alone.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>S Vegnaduzzo</author>
</authors>
<title>Identifying subjective adjectives through Web-based mutual information.</title>
<date>2004</date>
<booktitle>In Proceedings of KONVENS-04, 7th Konferenz zur Verarbeitung Nat¨urlicher Sprache (German Conference on Natural Language Processing),</booktitle>
<pages>17--24</pages>
<location>Vienna, AU.</location>
<contexts>
<context position="13510" citStr="Baroni and Vegnaduzzo (2004)" startWordPosition="2182" endWordPosition="2185"> reviewed so far (i.e. checking how good our system is at replicating an existing, independently motivated lexical resource), the authors do not test their method on an independently identified set of labelled terms, but on the set of terms that the algorithm itself extracts. This evaluation methodology only allows to test precision, and not accuracy tout court, since no quantification can be made of false negatives (i.e. the subjective terms that the algorithm should have spotted but has not spotted). In Section 5 this will prevent us from drawing comparisons between this method and our own. Baroni and Vegnaduzzo (2004) apply the PMI method, first used by Turney and Littman (2003) to determine term orientation, to determine term subjectivity. Their method uses a small set S, of 35 adjectives, marked as subjective by human judges, to assign a subjectivity score to each adjective to be classified. Therefore, their method, unlike our own, does not classify terms (i.e. take firm classification decisions), but ranks them according to a subjectivity score, on which they evaluate precision at various level of recall. 195 3 Determining term subjectivity and term orientation by semi-supervised learning The method we </context>
</contexts>
<marker>Baroni, Vegnaduzzo, 2004</marker>
<rawString>M. Baroni and S. Vegnaduzzo. 2004. Identifying subjective adjectives through Web-based mutual information. In Proceedings of KONVENS-04, 7th Konferenz zur Verarbeitung Nat¨urlicher Sprache (German Conference on Natural Language Processing), pages 17–24, Vienna, AU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Devitt</author>
<author>Carl Vogel</author>
</authors>
<title>The topology of WordNet: Some metrics.</title>
<date>2004</date>
<booktitle>In Proceedings of GWC-04, 2nd Global WordNet Conference,</booktitle>
<pages>106--111</pages>
<location>Brno, CZ.</location>
<contexts>
<context position="21772" citStr="Devitt and Vogel, 2004" startWordPosition="3589" endWordPosition="3592">ther two categories. To obtain a representative expanded set TrKo , we have chosen the seed set Tr1o = {entity} and we have expanded it by using, along with synonymy and antonymy, the WordNet relation of hyponymy (e.g. vehicle / car), and without imposing the restriction that the two related terms must have the same POS. These choices are strictly related to each other: the term entity is the root term of the largest generalization hierarchy in WordNet, with more than 40,000 7We make this labelled term set available for download at http://patty.isti.cnr.it/˜esuli/software/ SentiGI.tgz. terms (Devitt and Vogel, 2004), thus allowing to reach a very large number of terms by using the hyponymy relation8. Moreover, it seems reasonable to assume that terms that refer to entities are likely to have an “objective” nature, and that hyponyms (and also synonyms and antonyms) of an objective term are also objective. Note that, at each iteration k, a given term t is added to Trko only if it does not already belong to either Trp or Trn. We experiment with two different choices for the Tro set, corresponding to the sets generated in K = 3 and K = 4 iterations, respectively; this yields sets Tr3o and Tr4o consisting of </context>
</contexts>
<marker>Devitt, Vogel, 2004</marker>
<rawString>Ann Devitt and Carl Vogel. 2004. The topology of WordNet: Some metrics. In Proceedings of GWC-04, 2nd Global WordNet Conference, pages 106–111, Brno, CZ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Determining the semantic orientation of terms through gloss analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of CIKM-05, 14th ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>617--624</pages>
<location>Bremen, DE.</location>
<contexts>
<context position="3372" citStr="Esuli and Sebastiani, 2005" startWordPosition="522" endWordPosition="526">his amounts to performing binary text categorization under categories Objective and Subjective (Pang and Lee, 2004; Yu and Hatzivassiloglou, 2003); 2. determining document orientation (or polarity), as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter (Pang and Lee, 2004; Turney, 2002); 3. determining the strength of document orientation, as in deciding e.g. whether the Positive opinion expressed by a text on its subject matter is Weakly Positive, Mildly Positive, or Strongly Positive (Wilson et al., 2004). To aid these tasks, recent work (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Kamps et al., 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003) has tackled the issue of identifying the orientation of subjective terms contained in text, i.e. determining whether a term that carries opinionated content has a positive or a negative connotation (e.g. deciding that — using Turney and Littman’s (2003) examples — honest and intrepid have a positive connotation while disturbing and superfluous have a negative connotation). 193 This is believed to be of key importance for identifying the orientation of documents, since </context>
<context position="5824" citStr="Esuli and Sebastiani, 2005" startWordPosition="923" endWordPosition="927">le help, since determining if a term is Subjective is itself non-trivial. In this paper we confront the task of determining whether a given term has a Positive connotation (e.g. honest, intrepid), or a Negative connotation (e.g. disturbing, superfluous), or has instead no Subjective connotation at all (e.g. white, triangular); this problem thus subsumes the problem of deciding between Subjective and Objective and the problem of deciding between Positive and Negative. We tackle this problem by testing three different variants of the semi-supervised method for orientation detection proposed in (Esuli and Sebastiani, 2005). Our results show that determining subjectivity and orientation is a much harder problem than determining orientation alone. 1.1 Outline of the paper The rest of the paper is structured as follows. Section 2 reviews related work dealing with term orientation and/or subjectivity detection. Section 3 briefly reviews the semi-supervised method for orientation detection presented in (Esuli and Sebastiani, 2005). Section 4 describes in detail three different variants of it we propose for determining, at the same time, subjectivity and orientation, and describes the general setup of our experiments</context>
<context position="10989" citStr="Esuli and Sebastiani, 2005" startWordPosition="1759" endWordPosition="1762">seed terms and the antonyms of negative (resp. positive) seed terms. The system classifies then a target term t into either Positive or Negative by means of two alternative learning-free methods based on the probabilities that synonyms of t also appear in the respective expanded seed sets. A problem with this method is that it can classify only terms that share some synonyms with the expanded seed sets. Kim and Hovy also report an evaluation of human inter-coder agreement. We compare this evaluation with our results in Section 5. The approach we have proposed for determining term orientation (Esuli and Sebastiani, 2005) is described in more detail in Section 3, since it will be extensively used in this paper. All these works evaluate the performance of the proposed algorithms by checking them against precompiled sets of Positive and Negative terms, i.e. checking how good the algorithms are at classifying a term known to be subjective into either Positive or Negative. When tested on the same benchmarks, the methods of (Esuli and Sebastiani, 2005; Turney and Littman, 2003) have performed with comparable accuracies (however, the method of (Esuli and Sebastiani, 2005) is much more efficient than the one of (Turn</context>
<context position="14250" citStr="Esuli and Sebastiani, 2005" startWordPosition="2301" endWordPosition="2304">ubjectivity. Their method uses a small set S, of 35 adjectives, marked as subjective by human judges, to assign a subjectivity score to each adjective to be classified. Therefore, their method, unlike our own, does not classify terms (i.e. take firm classification decisions), but ranks them according to a subjectivity score, on which they evaluate precision at various level of recall. 195 3 Determining term subjectivity and term orientation by semi-supervised learning The method we use in this paper for determining term subjectivity and term orientation is a variant of the method proposed in (Esuli and Sebastiani, 2005) for determining term orientation alone. This latter method relies on training, in a semisupervised way, a binary classifier that labels terms as either Positive or Negative. A semisupervised method is a learning process whereby only a small subset L C Tr of the training data Tr are human-labelled. In origin the training data in U = Tr − L are instead unlabelled; it is the process itself that labels them, automatically, by using L (with the possible addition of other publicly available resources) as input. The method of (Esuli and Sebastiani, 2005) starts from two small seed (i.e. training) se</context>
<context position="15927" citStr="Esuli and Sebastiani, 2005" startWordPosition="2612" endWordPosition="2615">ave the same orientation and two antonyms typically have opposite orientation. The method is iterative, generating two sets Trkp and Trknat each iteration k, where Trkp D Trk−1 p D ... D Tr1 p = Lp and Trkn D Trk−1 n D ... D Tr1 n = Ln. At iteration k, Trkp is obtained by adding to Trk−1 p all synonyms of terms in Trk−1 pand all antonyms of terms in Trk−1 n ; similarly, Trknis obtained by adding to Trk−1 n all synonyms of terms in Trk−1 n and all antonyms of terms in Trk−1 p . If a total of K iterations are performed, then Tr = TrKp U TrKn . The second main feature of the method presented in (Esuli and Sebastiani, 2005) is that terms are given vectorial representations based on their WordNet glosses (i.e. textual definitions). For each term ti in Tr U Te (Te being the test set, i.e. the set of terms to be classified), a textual representation of ti is generated by collating all the glosses of ti as found in WordNet4. Each such represen3Several other WordNet lexical relations, and several combinations of them, are tested in (Esuli and Sebastiani, 2005). In the present paper we only use the best-performing such combination, as described in detail in Section 4.2. The version of WordNet used here and in (Esuli a</context>
<context position="17706" citStr="Esuli and Sebastiani, 2005" startWordPosition="2908" endWordPosition="2911">ssions, while the glosses of disturbing and superfluous will both contain derogative expressions. Note that this method allows to classify any term, independently of its POS, provided there is a gloss for it in the lexical resource. Once the vectorial representations for all terms in TrUT e have been generated, those for the terms in Tr are fed to a supervised learner, which thus generates a binary classifier. This latter, once fed with the vectorial representations of the terms in Te, classifies each of them as either Positive or Negative. 4 Experiments In this paper we extend the method of (Esuli and Sebastiani, 2005) to the determination of term subjectivity and term orientation altogether. 4.1 Test sets The benchmark (i.e. test set) we use for our experiments is the General Inquirer (GI) lexicon (Stone et al., 1966). This is a lexicon of terms labelled according to a large set of categories6, each one denoting the presence of a specific trait in the term. The two main categories, and the ones we will be concerned with, are Positive/Negative, which contain 1,915/2,291 terms having a positive/negative orientation (in what follows we will also refer to the category Subjective, which we define as the union o</context>
<context position="19113" citStr="Esuli and Sebastiani, 2005" startWordPosition="3147" endWordPosition="3150">t may have more than one sense; dictionaries normally associate one gloss to each sense. 5Several combinations of subparts of a WordNet gloss are tested as textual representations of terms in (Esuli and Sebastiani, 2005). Of all those combinations, in the present paper we always use the DGS- combination, since this is the one that has been shown to perform best in (Esuli and Sebastiani, 2005). DGS- corresponds to using the entire gloss and performing negation propagation on its text, i.e. replacing all the terms that occur after a negation in a sentence with negated versions of the term (see (Esuli and Sebastiani, 2005) for details). 6The definitions of all such categories are available at http://www.webuse.umd.edu:9090/ 196 ter removing 17 terms appearing in both categories (e.g. deal) and reducing all the multiple entries of the same term in a category, caused by multiple senses, to a single entry. Likewise, we take all the 7,582 GI terms that are not labelled as either Positive or Negative, as being (implicitly) labelled as Objective, and reduce them to 5,009 terms after combining multiple entries of the same term, caused by multiple senses, to a single entry. The effectiveness of our classifiers will thu</context>
<context position="20496" citStr="Esuli and Sebastiani, 2005" startWordPosition="3373" endWordPosition="3376">and training sets Similarly to (Esuli and Sebastiani, 2005), our training set is obtained by expanding initial seed sets by means of WordNet lexical relations. The main difference is that our training set is now the union of three sets of training terms Tr = TrKp ∪TrKn ∪TrKo obtained by expanding, through K iterations, three seed sets Tr1p, Tr1n, Tr1o, one for each of the categories Positive, Negative, and Objective, respectively. Concerning categories Positive and Negative, we have used the seed sets, expansion policy, and number of iterations, that have performed best in the experiments of (Esuli and Sebastiani, 2005), i.e. the seed sets Tr1p = {good} and Tr1 n = {bad} expanded by using the union of synonymy and indirect antonymy, restricting the relations only to terms with the same POS of the original terms (i.e. adjectives), for a total of K = 4 iterations. The final expanded sets contain 6,053 Positive terms and 6,874 Negative terms. Concerning the category Objective, the process we have followed is similar, but with a few key differences. These are motivated by the fact that the Objective category coincides with the complement of the union of Positive and Negative; therefore, Objective terms are more </context>
<context position="28586" citStr="Esuli and Sebastiani, 2005" startWordPosition="4717" endWordPosition="4721">ch one reporting results averaged across all the experiments we have run (see Table 1). The first and most important observation is that, with respect to a pure term orientation task, accuracy drops significantly. In fact, the best SOaccuracy and the best PNO-accuracy results obtained across the 120 different experiments are .676 and .660, respectively (these were obtained by using Approach II with the PrTFIDF learner and no feature selection, with Tro = Tr3o for the .676 SO-accuracy result and Tro = Tr4o for the .660 PNO-accuracy result); this contrasts sharply with the accuracy obtained in (Esuli and Sebastiani, 2005) on discriminating Positive from Negative (where the best run obtained .830 accuracy), on the same benchmarks and essentially the same algorithms. This suggests that good performance at orientation detection (as e.g. in (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003)) may not be a � MI(fk) = cE{cl,...,cm}, fE{fk,fk} 198 Table 2: Human inter-coder agreement values reported by Kim and Hovy (2004). Agreement Adjectives (462) Verbs (502) measure Hum1 vs Hum2 Hum2 vs Hum3 Strict .762 .623 Lenient .890 .851 guarantee of good performance at subjectivity dete</context>
</contexts>
<marker>Esuli, Sebastiani, 2005</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2005. Determining the semantic orientation of terms through gloss analysis. In Proceedings of CIKM-05, 14th ACM International Conference on Information and Knowledge Management, pages 617–624, Bremen, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL-97, 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>174--181</pages>
<location>Madrid, ES.</location>
<contexts>
<context position="3408" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="527" endWordPosition="530">nary text categorization under categories Objective and Subjective (Pang and Lee, 2004; Yu and Hatzivassiloglou, 2003); 2. determining document orientation (or polarity), as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter (Pang and Lee, 2004; Turney, 2002); 3. determining the strength of document orientation, as in deciding e.g. whether the Positive opinion expressed by a text on its subject matter is Weakly Positive, Mildly Positive, or Strongly Positive (Wilson et al., 2004). To aid these tasks, recent work (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Kamps et al., 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003) has tackled the issue of identifying the orientation of subjective terms contained in text, i.e. determining whether a term that carries opinionated content has a positive or a negative connotation (e.g. deciding that — using Turney and Littman’s (2003) examples — honest and intrepid have a positive connotation while disturbing and superfluous have a negative connotation). 193 This is believed to be of key importance for identifying the orientation of documents, since it is by considering the combined co</context>
<context position="6724" citStr="Hatzivassiloglou and McKeown (1997)" startWordPosition="1060" endWordPosition="1063">r subjectivity detection. Section 3 briefly reviews the semi-supervised method for orientation detection presented in (Esuli and Sebastiani, 2005). Section 4 describes in detail three different variants of it we propose for determining, at the same time, subjectivity and orientation, and describes the general setup of our experiments. In Section 5 we discuss the results we have obtained. Section 6 concludes. 2 Related work 2.1 Determining term orientation Most previous works dealing with the properties of terms within an opinion mining perspective have focused on determining term orientation. Hatzivassiloglou and McKeown (1997) attempt to predict the orientation of subjective adjectives by analysing pairs of adjectives (conjoined by and, or, but, either-or, or neither-nor) extracted from a large unlabelled document set. The underlying intuition is that the act of conjoining adjectives is subject to linguistic constraints on the orientation of the adjectives involved; e.g. and usually conjoins adjectives of equal orientation, while but conjoins adjectives of opposite orientation. The authors generate a graph where terms are nodes connected by “equal-orientation” or “opposite-orientation” edges, depending on the conju</context>
<context position="11685" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="1871" endWordPosition="1875">xtensively used in this paper. All these works evaluate the performance of the proposed algorithms by checking them against precompiled sets of Positive and Negative terms, i.e. checking how good the algorithms are at classifying a term known to be subjective into either Positive or Negative. When tested on the same benchmarks, the methods of (Esuli and Sebastiani, 2005; Turney and Littman, 2003) have performed with comparable accuracies (however, the method of (Esuli and Sebastiani, 2005) is much more efficient than the one of (Turney and Littman, 2003)), and have outperformed the method of (Hatzivassiloglou and McKeown, 1997) by a wide margin and the one by (Kamps et al., 2004) by a very wide margin. The methods described in (Hatzivassiloglou and McKeown, 1997) is also limited by the fact that it can only decide the orientation of adjectives, while the method of (Kamps et al., 2004) is further limited in that it can only work on adjectives that are present in WordNet. The methods of (Kim and Hovy, 2004; Takamura et al., 2005) are instead difficult to compare with the other ones since they were not evaluated on publicly available datasets. 2.2 Determining term subjectivity Riloff et al. (2003) develop a method to d</context>
<context position="28869" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="4760" endWordPosition="4763"> obtained across the 120 different experiments are .676 and .660, respectively (these were obtained by using Approach II with the PrTFIDF learner and no feature selection, with Tro = Tr3o for the .676 SO-accuracy result and Tro = Tr4o for the .660 PNO-accuracy result); this contrasts sharply with the accuracy obtained in (Esuli and Sebastiani, 2005) on discriminating Positive from Negative (where the best run obtained .830 accuracy), on the same benchmarks and essentially the same algorithms. This suggests that good performance at orientation detection (as e.g. in (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003)) may not be a � MI(fk) = cE{cl,...,cm}, fE{fk,fk} 198 Table 2: Human inter-coder agreement values reported by Kim and Hovy (2004). Agreement Adjectives (462) Verbs (502) measure Hum1 vs Hum2 Hum2 vs Hum3 Strict .762 .623 Lenient .890 .851 guarantee of good performance at subjectivity detection, quite evidently a harder (and, as we have suggested, more realistic) task. This hypothesis is confirmed by an experiment performed by Kim and Hovy (2004) on testing the agreement of two human coders at tagging words with the Positive, Negative, and Objective labels. The autho</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of ACL-97, 35th Annual Meeting of the Association for Computational Linguistics, pages 174–181, Madrid, ES.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Janyce M Wiebe</author>
</authors>
<title>Effects of adjective orientation and gradability on sentence subjectivity.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING-00, 18th International Conference on Computational Linguistics,</booktitle>
<pages>174--181</pages>
<location>Saarbr¨ucken, DE.</location>
<contexts>
<context position="4435" citStr="Hatzivassiloglou and Wiebe, 2000" startWordPosition="693" endWordPosition="696">notation while disturbing and superfluous have a negative connotation). 193 This is believed to be of key importance for identifying the orientation of documents, since it is by considering the combined contribution of these terms that one may hope to solve Tasks 1, 2 and 3 above. The conceptually simplest approach to this latter problem is probably Turney’s (2002), who has obtained interesting results on Task 2 by considering the algebraic sum of the orientations of terms as representative of the orientation of the document they belong to; but more sophisticated approaches are also possible (Hatzivassiloglou and Wiebe, 2000; Riloff et al., 2003; Wilson et al., 2004). Implicit in most works dealing with term orientation is the assumption that, for many languages for which one would like to perform opinion mining, there is no available lexical resource where terms are tagged as having either a Positive or a Negative connotation, and that in the absence of such a resource the only available route is to generate such a resource automatically. However, we think this approach lacks realism, since it is also true that, for the very same languages, there is no available lexical resource where terms are tagged as having </context>
</contexts>
<marker>Hatzivassiloglou, Wiebe, 2000</marker>
<rawString>Vasileios Hatzivassiloglou and Janyce M. Wiebe. 2000. Effects of adjective orientation and gradability on sentence subjectivity. In Proceedings of COLING-00, 18th International Conference on Computational Linguistics, pages 174–181, Saarbr¨ucken, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of ICML-97, 14th International Conference on Machine Learning,</booktitle>
<pages>143--151</pages>
<location>Nashville, US.</location>
<contexts>
<context position="25251" citStr="Joachims, 1997" startWordPosition="4171" endWordPosition="4172">proach II the training examples of Objective are only used as training examples of the complements of Positive and Negative). A second difference is that Approaches I and II are based on standard binary classification technology, while Approach III requires “multiclass” (i.e. 1-of-m) classification. As a consequence, while for the former we use well-known learners for binary classification (the naive Bayesian learner using the multinomial model (McCallum and Nigam, 1998), support vector machines using linear kernels (Joachims, 1998), the Rocchio learner, and its PrTFIDF probabilistic version (Joachims, 1997)), for Approach III we use their multiclass versions9. Before running our learners we make a pass of feature selection, with the intent of retaining only those features that are good at discriminating our categories, while discarding those which are not. Feature selection is implemented by scoring each feature fk (i.e. each term that occurs in the glosses of at least one training term) by means of the mutual information (MI) function, defined as Pr(f, c) Pr(f, c) · log (1) Pr(f) Pr(c) and discarding the x% features fk that minimize it. We will call x% the reduction factor. Note that the set {c</context>
</contexts>
<marker>Joachims, 1997</marker>
<rawString>Thorsten Joachims. 1997. A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. In Proceedings of ICML-97, 14th International Conference on Machine Learning, pages 143–151, Nashville, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with support vector machines: learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of ECML-98, 10th European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<location>Chemnitz, DE.</location>
<contexts>
<context position="25174" citStr="Joachims, 1998" startWordPosition="4160" endWordPosition="4161">s a nonexistent entity, i.e. as the “absence of subjectivity” (in fact, in Approach II the training examples of Objective are only used as training examples of the complements of Positive and Negative). A second difference is that Approaches I and II are based on standard binary classification technology, while Approach III requires “multiclass” (i.e. 1-of-m) classification. As a consequence, while for the former we use well-known learners for binary classification (the naive Bayesian learner using the multinomial model (McCallum and Nigam, 1998), support vector machines using linear kernels (Joachims, 1998), the Rocchio learner, and its PrTFIDF probabilistic version (Joachims, 1997)), for Approach III we use their multiclass versions9. Before running our learners we make a pass of feature selection, with the intent of retaining only those features that are good at discriminating our categories, while discarding those which are not. Feature selection is implemented by scoring each feature fk (i.e. each term that occurs in the glosses of at least one training term) by means of the mutual information (MI) function, defined as Pr(f, c) Pr(f, c) · log (1) Pr(f) Pr(c) and discarding the x% features fk</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with support vector machines: learning with many relevant features. In Proceedings of ECML-98, 10th European Conference on Machine Learning, pages 137–142, Chemnitz, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaap Kamps</author>
<author>Maarten Marx</author>
<author>Robert J Mokken</author>
<author>Maarten De Rijke</author>
</authors>
<title>Using WordNet to measure semantic orientation of adjectives.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC-04, 4th International Conference on Language Resources and Evaluation, volume IV,</booktitle>
<pages>1115--1118</pages>
<location>Lisbon, PT.</location>
<marker>Kamps, Marx, Mokken, De Rijke, 2004</marker>
<rawString>Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten De Rijke. 2004. Using WordNet to measure semantic orientation of adjectives. In Proceedings of LREC-04, 4th International Conference on Language Resources and Evaluation, volume IV, pages 1115–1118, Lisbon, PT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04, 20th International Conference on Computational Linguistics,</booktitle>
<pages>1367--1373</pages>
<location>Geneva, CH.</location>
<contexts>
<context position="3448" citStr="Kim and Hovy, 2004" startWordPosition="535" endWordPosition="538">Subjective (Pang and Lee, 2004; Yu and Hatzivassiloglou, 2003); 2. determining document orientation (or polarity), as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter (Pang and Lee, 2004; Turney, 2002); 3. determining the strength of document orientation, as in deciding e.g. whether the Positive opinion expressed by a text on its subject matter is Weakly Positive, Mildly Positive, or Strongly Positive (Wilson et al., 2004). To aid these tasks, recent work (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Kamps et al., 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003) has tackled the issue of identifying the orientation of subjective terms contained in text, i.e. determining whether a term that carries opinionated content has a positive or a negative connotation (e.g. deciding that — using Turney and Littman’s (2003) examples — honest and intrepid have a positive connotation while disturbing and superfluous have a negative connotation). 193 This is believed to be of key importance for identifying the orientation of documents, since it is by considering the combined contribution of these terms that one may h</context>
<context position="9862" citStr="Kim and Hovy (2004)" startWordPosition="1572" endWordPosition="1575">t of electrons each endowed with one between two possible spin directions, and where electrons propagate their spin direction to neighbouring electrons until the system reaches a stable configuration. The authors equate terms with electrons and term orientation to spin direction. They build a neighbourhood matrix connecting each pair of terms if one appears in the gloss of the other, and iteratively apply the spin model on the matrix until a “minimum energy” configuration is reached. The orientation assigned to a term then corresponds to the spin direction assigned to electrons. The system of Kim and Hovy (2004) tackles orientation detection by attributing, to each term, a positivity score and a negativity score; interestingly, terms may thus be deemed to have both a positive and a negative correlation, maybe with different degrees, and some terms may be deemed to carry a stronger positive (or negative) orientation than others. Their system starts from a set of positive and negative seed terms, and expands the positive (resp. negative) seed set by adding to it the synonyms of positive (resp. negative) seed terms and the antonyms of negative (resp. positive) seed terms. The system classifies then a ta</context>
<context position="12069" citStr="Kim and Hovy, 2004" startWordPosition="1944" endWordPosition="1947">ave performed with comparable accuracies (however, the method of (Esuli and Sebastiani, 2005) is much more efficient than the one of (Turney and Littman, 2003)), and have outperformed the method of (Hatzivassiloglou and McKeown, 1997) by a wide margin and the one by (Kamps et al., 2004) by a very wide margin. The methods described in (Hatzivassiloglou and McKeown, 1997) is also limited by the fact that it can only decide the orientation of adjectives, while the method of (Kamps et al., 2004) is further limited in that it can only work on adjectives that are present in WordNet. The methods of (Kim and Hovy, 2004; Takamura et al., 2005) are instead difficult to compare with the other ones since they were not evaluated on publicly available datasets. 2.2 Determining term subjectivity Riloff et al. (2003) develop a method to determine whether a term has a Subjective or an Objective connotation, based on bootstrapping algorithms. The method identifies patterns for the extraction of subjective nouns from text, bootstrapping from a seed set of 20 terms that the authors judge to be strongly subjective and have found to have high frequency in the text collection from which the subjective nouns must be extrac</context>
<context position="29026" citStr="Kim and Hovy (2004)" startWordPosition="4787" endWordPosition="4790">with Tro = Tr3o for the .676 SO-accuracy result and Tro = Tr4o for the .660 PNO-accuracy result); this contrasts sharply with the accuracy obtained in (Esuli and Sebastiani, 2005) on discriminating Positive from Negative (where the best run obtained .830 accuracy), on the same benchmarks and essentially the same algorithms. This suggests that good performance at orientation detection (as e.g. in (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003)) may not be a � MI(fk) = cE{cl,...,cm}, fE{fk,fk} 198 Table 2: Human inter-coder agreement values reported by Kim and Hovy (2004). Agreement Adjectives (462) Verbs (502) measure Hum1 vs Hum2 Hum2 vs Hum3 Strict .762 .623 Lenient .890 .851 guarantee of good performance at subjectivity detection, quite evidently a harder (and, as we have suggested, more realistic) task. This hypothesis is confirmed by an experiment performed by Kim and Hovy (2004) on testing the agreement of two human coders at tagging words with the Positive, Negative, and Objective labels. The authors define two measures of such agreement: strict agreement, equivalent to our PNO-accuracy, and lenient agreement, which measures the accuracy at telling Neg</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of COLING-04, 20th International Conference on Computational Linguistics, pages 1367–1373, Geneva, CH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew K McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for naive Bayes text classification.</title>
<date>1998</date>
<booktitle>In Proceedings of the AAAI Workshop on Learning for Text Categorization,</booktitle>
<pages>41--48</pages>
<location>Madison, US.</location>
<contexts>
<context position="25111" citStr="McCallum and Nigam, 1998" startWordPosition="4149" endWordPosition="4152">gory, or concept, in its own right, while Approach II views objectivity as a nonexistent entity, i.e. as the “absence of subjectivity” (in fact, in Approach II the training examples of Objective are only used as training examples of the complements of Positive and Negative). A second difference is that Approaches I and II are based on standard binary classification technology, while Approach III requires “multiclass” (i.e. 1-of-m) classification. As a consequence, while for the former we use well-known learners for binary classification (the naive Bayesian learner using the multinomial model (McCallum and Nigam, 1998), support vector machines using linear kernels (Joachims, 1998), the Rocchio learner, and its PrTFIDF probabilistic version (Joachims, 1997)), for Approach III we use their multiclass versions9. Before running our learners we make a pass of feature selection, with the intent of retaining only those features that are good at discriminating our categories, while discarding those which are not. Feature selection is implemented by scoring each feature fk (i.e. each term that occurs in the glosses of at least one training term) by means of the mutual information (MI) function, defined as Pr(f, c) P</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew K. McCallum and Kamal Nigam. 1998. A comparison of event models for naive Bayes text classification. In Proceedings of the AAAI Workshop on Learning for Text Categorization, pages 41–48, Madison, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-04, 42nd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>271--278</pages>
<location>Barcelona, ES.</location>
<contexts>
<context position="2860" citStr="Pang and Lee, 2004" startWordPosition="439" endWordPosition="442"> or tracking the shifting attitudes of the general public toward a political candidate by mining online forums. Within opinion mining, several subtasks can be identified, all of them having to do with tagging a given document according to expressed opinion: 1. determining document subjectivity, as in deciding whether a given text has a factual nature (i.e. describes a given situation or event, without expressing a positive or a negative opinion on it) or expresses an opinion on its subject matter. This amounts to performing binary text categorization under categories Objective and Subjective (Pang and Lee, 2004; Yu and Hatzivassiloglou, 2003); 2. determining document orientation (or polarity), as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter (Pang and Lee, 2004; Turney, 2002); 3. determining the strength of document orientation, as in deciding e.g. whether the Positive opinion expressed by a text on its subject matter is Weakly Positive, Mildly Positive, or Strongly Positive (Wilson et al., 2004). To aid these tasks, recent work (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Kamps et al., 2004; Kim and Hovy, 2004; Takamura e</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL-04, 42nd Meeting of the Association for Computational Linguistics, pages 271–278, Barcelona, ES.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
</authors>
<title>Learning subjective nouns using extraction pattern bootstrapping.</title>
<date>2003</date>
<booktitle>In Proceedings of CONLL-03, 7th Conference on Natural Language Learning,</booktitle>
<pages>25--32</pages>
<location>Edmonton, CA.</location>
<contexts>
<context position="4456" citStr="Riloff et al., 2003" startWordPosition="697" endWordPosition="700">rfluous have a negative connotation). 193 This is believed to be of key importance for identifying the orientation of documents, since it is by considering the combined contribution of these terms that one may hope to solve Tasks 1, 2 and 3 above. The conceptually simplest approach to this latter problem is probably Turney’s (2002), who has obtained interesting results on Task 2 by considering the algebraic sum of the orientations of terms as representative of the orientation of the document they belong to; but more sophisticated approaches are also possible (Hatzivassiloglou and Wiebe, 2000; Riloff et al., 2003; Wilson et al., 2004). Implicit in most works dealing with term orientation is the assumption that, for many languages for which one would like to perform opinion mining, there is no available lexical resource where terms are tagged as having either a Positive or a Negative connotation, and that in the absence of such a resource the only available route is to generate such a resource automatically. However, we think this approach lacks realism, since it is also true that, for the very same languages, there is no available lexical resource where terms are tagged as having either a Subjective o</context>
<context position="12263" citStr="Riloff et al. (2003)" startWordPosition="1975" endWordPosition="1978">od of (Hatzivassiloglou and McKeown, 1997) by a wide margin and the one by (Kamps et al., 2004) by a very wide margin. The methods described in (Hatzivassiloglou and McKeown, 1997) is also limited by the fact that it can only decide the orientation of adjectives, while the method of (Kamps et al., 2004) is further limited in that it can only work on adjectives that are present in WordNet. The methods of (Kim and Hovy, 2004; Takamura et al., 2005) are instead difficult to compare with the other ones since they were not evaluated on publicly available datasets. 2.2 Determining term subjectivity Riloff et al. (2003) develop a method to determine whether a term has a Subjective or an Objective connotation, based on bootstrapping algorithms. The method identifies patterns for the extraction of subjective nouns from text, bootstrapping from a seed set of 20 terms that the authors judge to be strongly subjective and have found to have high frequency in the text collection from which the subjective nouns must be extracted. The results of this method are not easy to compare with the ones we present in this paper because of the different evaluation methodologies. While we adopt the evaluation methodology used i</context>
</contexts>
<marker>Riloff, Wiebe, Wilson, 2003</marker>
<rawString>Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. Learning subjective nouns using extraction pattern bootstrapping. In Proceedings of CONLL-03, 7th Conference on Natural Language Learning, pages 25–32, Edmonton, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Stone</author>
<author>D C Dunphy</author>
<author>M S Smith</author>
<author>D M Ogilvie</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, US.</location>
<contexts>
<context position="17910" citStr="Stone et al., 1966" startWordPosition="2943" endWordPosition="2946">he lexical resource. Once the vectorial representations for all terms in TrUT e have been generated, those for the terms in Tr are fed to a supervised learner, which thus generates a binary classifier. This latter, once fed with the vectorial representations of the terms in Te, classifies each of them as either Positive or Negative. 4 Experiments In this paper we extend the method of (Esuli and Sebastiani, 2005) to the determination of term subjectivity and term orientation altogether. 4.1 Test sets The benchmark (i.e. test set) we use for our experiments is the General Inquirer (GI) lexicon (Stone et al., 1966). This is a lexicon of terms labelled according to a large set of categories6, each one denoting the presence of a specific trait in the term. The two main categories, and the ones we will be concerned with, are Positive/Negative, which contain 1,915/2,291 terms having a positive/negative orientation (in what follows we will also refer to the category Subjective, which we define as the union of the two categories Positive and Negative). In opinion mining research the GI was first used by Turney and Littman (2003), who reduced the list of terms to 1,614/1,982 entries afit may have more than one</context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>P. J. Stone, D. C. Dunphy, M. S. Smith, and D. M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press, Cambridge, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Extracting emotional polarity of words using spin model.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05, 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>133--140</pages>
<location>Ann Arbor, US.</location>
<contexts>
<context position="3471" citStr="Takamura et al., 2005" startWordPosition="539" endWordPosition="542"> Lee, 2004; Yu and Hatzivassiloglou, 2003); 2. determining document orientation (or polarity), as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter (Pang and Lee, 2004; Turney, 2002); 3. determining the strength of document orientation, as in deciding e.g. whether the Positive opinion expressed by a text on its subject matter is Weakly Positive, Mildly Positive, or Strongly Positive (Wilson et al., 2004). To aid these tasks, recent work (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Kamps et al., 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003) has tackled the issue of identifying the orientation of subjective terms contained in text, i.e. determining whether a term that carries opinionated content has a positive or a negative connotation (e.g. deciding that — using Turney and Littman’s (2003) examples — honest and intrepid have a positive connotation while disturbing and superfluous have a negative connotation). 193 This is believed to be of key importance for identifying the orientation of documents, since it is by considering the combined contribution of these terms that one may hope to solve Tasks 1, 2</context>
<context position="9142" citStr="Takamura et al. (2005)" startWordPosition="1453" endWordPosition="1456">returned by the search engine as estimates of the probabilities needed for the computation of PMI. Kamps et al. (2004) consider instead the graph defined on adjectives by the WordNet2 synonymy relation, and determine the orientation of a target 1http://www.altavista.com/ 2http://wordnet.princeton.edu/ 194 adjective t contained in the graph by comparing the lengths of (i) the shortest path between t and the seed term good, and (ii) the shortest path between t and the seed term bad: if the former is shorter than the latter, than t is deemed to be Positive, otherwise it is deemed to be Negative. Takamura et al. (2005) determine term orientation (for Japanese) according to a “spin model”, i.e. a physical model of a set of electrons each endowed with one between two possible spin directions, and where electrons propagate their spin direction to neighbouring electrons until the system reaches a stable configuration. The authors equate terms with electrons and term orientation to spin direction. They build a neighbourhood matrix connecting each pair of terms if one appears in the gloss of the other, and iteratively apply the spin model on the matrix until a “minimum energy” configuration is reached. The orient</context>
<context position="12093" citStr="Takamura et al., 2005" startWordPosition="1948" endWordPosition="1951">omparable accuracies (however, the method of (Esuli and Sebastiani, 2005) is much more efficient than the one of (Turney and Littman, 2003)), and have outperformed the method of (Hatzivassiloglou and McKeown, 1997) by a wide margin and the one by (Kamps et al., 2004) by a very wide margin. The methods described in (Hatzivassiloglou and McKeown, 1997) is also limited by the fact that it can only decide the orientation of adjectives, while the method of (Kamps et al., 2004) is further limited in that it can only work on adjectives that are present in WordNet. The methods of (Kim and Hovy, 2004; Takamura et al., 2005) are instead difficult to compare with the other ones since they were not evaluated on publicly available datasets. 2.2 Determining term subjectivity Riloff et al. (2003) develop a method to determine whether a term has a Subjective or an Objective connotation, based on bootstrapping algorithms. The method identifies patterns for the extraction of subjective nouns from text, bootstrapping from a seed set of 20 terms that the authors judge to be strongly subjective and have found to have high frequency in the text collection from which the subjective nouns must be extracted. The results of this</context>
</contexts>
<marker>Takamura, Inui, Okumura, 2005</marker>
<rawString>Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting emotional polarity of words using spin model. In Proceedings of ACL-05, 43rd Annual Meeting of the Association for Computational Linguistics, pages 133–140, Ann Arbor, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="3498" citStr="Turney and Littman, 2003" startWordPosition="543" endWordPosition="546">ivassiloglou, 2003); 2. determining document orientation (or polarity), as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter (Pang and Lee, 2004; Turney, 2002); 3. determining the strength of document orientation, as in deciding e.g. whether the Positive opinion expressed by a text on its subject matter is Weakly Positive, Mildly Positive, or Strongly Positive (Wilson et al., 2004). To aid these tasks, recent work (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Kamps et al., 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003) has tackled the issue of identifying the orientation of subjective terms contained in text, i.e. determining whether a term that carries opinionated content has a positive or a negative connotation (e.g. deciding that — using Turney and Littman’s (2003) examples — honest and intrepid have a positive connotation while disturbing and superfluous have a negative connotation). 193 This is believed to be of key importance for identifying the orientation of documents, since it is by considering the combined contribution of these terms that one may hope to solve Tasks 1, 2 and 3 above. The conceptua</context>
<context position="7543" citStr="Turney and Littman (2003)" startWordPosition="1180" endWordPosition="1183">t. The underlying intuition is that the act of conjoining adjectives is subject to linguistic constraints on the orientation of the adjectives involved; e.g. and usually conjoins adjectives of equal orientation, while but conjoins adjectives of opposite orientation. The authors generate a graph where terms are nodes connected by “equal-orientation” or “opposite-orientation” edges, depending on the conjunctions extracted from the document set. A clustering algorithm then partitions the graph into a Positive cluster and a Negative cluster, based on a relation of similarity induced by the edges. Turney and Littman (2003) determine term orientation by bootstrapping from two small sets of subjective “seed” terms (with the seed set for Positive containing terms such as good and nice, and the seed set for Negative containing terms such as bad and nasty). Their method is based on computing the pointwise mutual information (PMI) of the target term t with each seed term ti as a measure of their semantic association. Given a target term t, its orientation value O(t) (where positive value means positive orientation, and higher absolute value means stronger orientation) is given by the sum of the weights of its semanti</context>
<context position="11449" citStr="Turney and Littman, 2003" startWordPosition="1835" endWordPosition="1838">oder agreement. We compare this evaluation with our results in Section 5. The approach we have proposed for determining term orientation (Esuli and Sebastiani, 2005) is described in more detail in Section 3, since it will be extensively used in this paper. All these works evaluate the performance of the proposed algorithms by checking them against precompiled sets of Positive and Negative terms, i.e. checking how good the algorithms are at classifying a term known to be subjective into either Positive or Negative. When tested on the same benchmarks, the methods of (Esuli and Sebastiani, 2005; Turney and Littman, 2003) have performed with comparable accuracies (however, the method of (Esuli and Sebastiani, 2005) is much more efficient than the one of (Turney and Littman, 2003)), and have outperformed the method of (Hatzivassiloglou and McKeown, 1997) by a wide margin and the one by (Kamps et al., 2004) by a very wide margin. The methods described in (Hatzivassiloglou and McKeown, 1997) is also limited by the fact that it can only decide the orientation of adjectives, while the method of (Kamps et al., 2004) is further limited in that it can only work on adjectives that are present in WordNet. The methods of</context>
<context position="13572" citStr="Turney and Littman (2003)" startWordPosition="2193" endWordPosition="2196">ting an existing, independently motivated lexical resource), the authors do not test their method on an independently identified set of labelled terms, but on the set of terms that the algorithm itself extracts. This evaluation methodology only allows to test precision, and not accuracy tout court, since no quantification can be made of false negatives (i.e. the subjective terms that the algorithm should have spotted but has not spotted). In Section 5 this will prevent us from drawing comparisons between this method and our own. Baroni and Vegnaduzzo (2004) apply the PMI method, first used by Turney and Littman (2003) to determine term orientation, to determine term subjectivity. Their method uses a small set S, of 35 adjectives, marked as subjective by human judges, to assign a subjectivity score to each adjective to be classified. Therefore, their method, unlike our own, does not classify terms (i.e. take firm classification decisions), but ranks them according to a subjectivity score, on which they evaluate precision at various level of recall. 195 3 Determining term subjectivity and term orientation by semi-supervised learning The method we use in this paper for determining term subjectivity and term o</context>
<context position="18428" citStr="Turney and Littman (2003)" startWordPosition="3030" endWordPosition="3033">chmark (i.e. test set) we use for our experiments is the General Inquirer (GI) lexicon (Stone et al., 1966). This is a lexicon of terms labelled according to a large set of categories6, each one denoting the presence of a specific trait in the term. The two main categories, and the ones we will be concerned with, are Positive/Negative, which contain 1,915/2,291 terms having a positive/negative orientation (in what follows we will also refer to the category Subjective, which we define as the union of the two categories Positive and Negative). In opinion mining research the GI was first used by Turney and Littman (2003), who reduced the list of terms to 1,614/1,982 entries afit may have more than one sense; dictionaries normally associate one gloss to each sense. 5Several combinations of subparts of a WordNet gloss are tested as textual representations of terms in (Esuli and Sebastiani, 2005). Of all those combinations, in the present paper we always use the DGS- combination, since this is the one that has been shown to perform best in (Esuli and Sebastiani, 2005). DGS- corresponds to using the entire gloss and performing negation propagation on its text, i.e. replacing all the terms that occur after a negat</context>
<context position="28896" citStr="Turney and Littman, 2003" startWordPosition="4764" endWordPosition="4767">xperiments are .676 and .660, respectively (these were obtained by using Approach II with the PrTFIDF learner and no feature selection, with Tro = Tr3o for the .676 SO-accuracy result and Tro = Tr4o for the .660 PNO-accuracy result); this contrasts sharply with the accuracy obtained in (Esuli and Sebastiani, 2005) on discriminating Positive from Negative (where the best run obtained .830 accuracy), on the same benchmarks and essentially the same algorithms. This suggests that good performance at orientation detection (as e.g. in (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003)) may not be a � MI(fk) = cE{cl,...,cm}, fE{fk,fk} 198 Table 2: Human inter-coder agreement values reported by Kim and Hovy (2004). Agreement Adjectives (462) Verbs (502) measure Hum1 vs Hum2 Hum2 vs Hum3 Strict .762 .623 Lenient .890 .851 guarantee of good performance at subjectivity detection, quite evidently a harder (and, as we have suggested, more realistic) task. This hypothesis is confirmed by an experiment performed by Kim and Hovy (2004) on testing the agreement of two human coders at tagging words with the Positive, Negative, and Objective labels. The authors define two measures of s</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21(4):315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-02, 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>417--424</pages>
<location>Philadelphia, US.</location>
<contexts>
<context position="3086" citStr="Turney, 2002" startWordPosition="477" endWordPosition="478">ding to expressed opinion: 1. determining document subjectivity, as in deciding whether a given text has a factual nature (i.e. describes a given situation or event, without expressing a positive or a negative opinion on it) or expresses an opinion on its subject matter. This amounts to performing binary text categorization under categories Objective and Subjective (Pang and Lee, 2004; Yu and Hatzivassiloglou, 2003); 2. determining document orientation (or polarity), as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter (Pang and Lee, 2004; Turney, 2002); 3. determining the strength of document orientation, as in deciding e.g. whether the Positive opinion expressed by a text on its subject matter is Weakly Positive, Mildly Positive, or Strongly Positive (Wilson et al., 2004). To aid these tasks, recent work (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Kamps et al., 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003) has tackled the issue of identifying the orientation of subjective terms contained in text, i.e. determining whether a term that carries opinionated content has a positive or a negative c</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of ACL-02, 40th Annual Meeting of the Association for Computational Linguistics, pages 417–424, Philadelphia, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Rebecca Hwa</author>
</authors>
<title>Just how mad are you? Finding strong and weak opinion clauses.</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI-04, 21st Conference of the American Association for Artificial Intelligence,</booktitle>
<pages>761--769</pages>
<location>San Jose, US.</location>
<contexts>
<context position="3311" citStr="Wilson et al., 2004" startWordPosition="512" endWordPosition="515">on it) or expresses an opinion on its subject matter. This amounts to performing binary text categorization under categories Objective and Subjective (Pang and Lee, 2004; Yu and Hatzivassiloglou, 2003); 2. determining document orientation (or polarity), as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter (Pang and Lee, 2004; Turney, 2002); 3. determining the strength of document orientation, as in deciding e.g. whether the Positive opinion expressed by a text on its subject matter is Weakly Positive, Mildly Positive, or Strongly Positive (Wilson et al., 2004). To aid these tasks, recent work (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Kamps et al., 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003) has tackled the issue of identifying the orientation of subjective terms contained in text, i.e. determining whether a term that carries opinionated content has a positive or a negative connotation (e.g. deciding that — using Turney and Littman’s (2003) examples — honest and intrepid have a positive connotation while disturbing and superfluous have a negative connotation). 193 This is believed to be of key im</context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004. Just how mad are you? Finding strong and weak opinion clauses. In Proceedings of AAAI-04, 21st Conference of the American Association for Artificial Intelligence, pages 761–769, San Jose, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-03, 8th Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>129--136</pages>
<location>Sapporo, JP.</location>
<contexts>
<context position="2892" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="443" endWordPosition="446">fting attitudes of the general public toward a political candidate by mining online forums. Within opinion mining, several subtasks can be identified, all of them having to do with tagging a given document according to expressed opinion: 1. determining document subjectivity, as in deciding whether a given text has a factual nature (i.e. describes a given situation or event, without expressing a positive or a negative opinion on it) or expresses an opinion on its subject matter. This amounts to performing binary text categorization under categories Objective and Subjective (Pang and Lee, 2004; Yu and Hatzivassiloglou, 2003); 2. determining document orientation (or polarity), as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter (Pang and Lee, 2004; Turney, 2002); 3. determining the strength of document orientation, as in deciding e.g. whether the Positive opinion expressed by a text on its subject matter is Weakly Positive, Mildly Positive, or Strongly Positive (Wilson et al., 2004). To aid these tasks, recent work (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Kamps et al., 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman,</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of EMNLP-03, 8th Conference on Empirical Methods in Natural Language Processing, pages 129– 136, Sapporo, JP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>