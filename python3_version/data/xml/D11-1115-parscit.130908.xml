<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000054">
<title confidence="0.976692">
Semi-supervised CCG Lexicon Extension
</title>
<author confidence="0.998886">
Emily Thomforde Mark Steedman
</author>
<affiliation confidence="0.999926">
University of Edinburgh University of Edinburgh
</affiliation>
<email confidence="0.992019">
e.j.thomforde@sms.ed.ac.uk steedman@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997296" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999821823529412">
This paper introduces Chart Inference (CI),
an algorithm for deriving a CCG category
for an unknown word from a partial parse
chart. It is shown to be faster and more pre-
cise than a baseline brute-force method, and
to achieve wider coverage than a rule-based
system. In addition, we show the application
of CI to a domain adaptation task for ques-
tion words, which are largely missing in the
Penn Treebank. When used in combination
with self-training, CI increases the precision
of the baseline StatCCG parser over subject-
extraction questions by 50%. An error analy-
sis shows that CI contributes to the increase by
expanding the number of category types avail-
able to the parser, while self-training adjusts
the counts.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994834">
Unseen lexical items are a major cause of error in
strongly lexicalised parsers such as those based on
CCG (Clark and Curran, 2003; Hockenmaier, 2003).
The problem is especially acute for less privileged
languages, but even in the case of English, we are
aware of many category types entirely missing from
the Penn Treebank (Clark et al., 2004).
In the case of totally unseen words, the standard
method used by StatCCG (Hockenmaier, 2003) and
many other treebank parsers is part-of-speech back-
off, which is quite effective, affording an F-score of
93% over dependencies in §00 in the optimal config-
uration. It is difficult to say how backing off affects
dependency errors, but when we examine category
match accuracy of the CCGBank-trained parser, we
find that POS backoff has been used on 19.6% of to-
kens, which means that those tokens are unseen, or
too infrequent in the training data to be included in
the lexicon. Of the 3320 items the parser labelled
incorrectly, 675 (20.3%) are words that are miss-
ing from the lexicon entirely.1 In the best case, if
we were able to learn lexical entries for those 675,
we could transfer them to lexical treatment, which
is 93.5% accurate, rather than POS backoff, which
is 89.3% accurate. Under these conditions, we pre-
dict a further 631 word/category pairs to be tagged
correctly by the parser, reducing the error rate from
7.4% to 6% on §00. Further to reducing parsing er-
ror, a robust method for learning words from un-
labelled data would result in the recovery of inter-
esting and important category types that are missing
from our standard lexical resources.
This paper introduces Chart Inference (CI) as
a strategy for deducing a ranked set of possible
categories for an unknown word using the partial
chart formed from the known words that surround
it. CCG (Steedman, 2000) is particularly suited to
this problem, because category types can be inferred
from the types of the surrounding constituents. CI
is designed to take advantage of this property of
generative CCGBank-trained parser, and of access
to the full inventory of CCG combinators and non-
combinatory unary rules from the trained model. It
is capable of learning category types that are com-
pletely missing from the lexicon, and is superior to
existing learning systems in both precision and effi-
ciency.
Four experiments are discussed in this paper. The
first compares three word-learning methods for their
ability to converge to a toy target lexicon. The sec-
</bodyText>
<footnote confidence="0.951732">
1A further 269 (8%) are cases where the word is known, but
has not been seen with the correct category.
</footnote>
<page confidence="0.802715">
1246
</page>
<note confidence="0.9584175">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1246–1256,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999629">
ond and third compare the three methods based on
their ability to correctly tag the all the words in a
small natural language corpus. The final experiment
shows how Chart Induction can be effectively used
in a domain adaptation task where a small number
of category types are known to be missing from the
lexicon.
</bodyText>
<sectionHeader confidence="0.983285" genericHeader="method">
2 Learning Words
</sectionHeader>
<bodyText confidence="0.999871928571429">
The methods used in this paper all operate under
a restricted learning setting, over sentences where
all but one word is in the lexicon. Since the learn-
ing portion of the algorithm is unsupervised, it has
access to an essentially unlimited amount of unla-
belled data, and it can afford to skip any sentence
that does not conform to the one-unseen-word re-
striction. Attempting two or more OOL words at a
time from one sentence would compound the search
space and the error rate. We do not address the much
harder problem of hypothesising missing categories
for known words, which should presumably be han-
dled by quite other methods, such as prior offline
generalization of the lexicon.
</bodyText>
<subsectionHeader confidence="0.996842">
2.1 A Brute-force System
</subsectionHeader>
<bodyText confidence="0.999976707317073">
One of the early lexical acquisition systems us-
ing Categorial Grammar was that of Watkinson and
Manandhar (1999; 2000; 2001a; 2001b). This sys-
tem attempted to simultaneously learn a CG lexicon
and annotate unlabelled text with parse derivations.
Using a stripped-down parser that only utilised the
forward- and backward-application rules, they iter-
atively learned the lexicon from the feedback from
online parsing. The system decided which parse was
best based on the lexicon, and then decided which
additions to the lexicon to make based on principles
of compression. After each change, the system re-
examined the parses for previous sentences and up-
dated them to reflect the new lexicon.
They report fully convergent results on two toy
corpora, but the parsing accuracy of the system
trained on natural language data was far below
the state of the art. However, they do show cat-
egorial grammar to be a promising basis for ar-
tificial language acquisition, because CCG makes
learning the lexicon and learning the grammar the
same task (Watkinson and Manandhar, 1999). They
also showed that seeding the lexicon with examples
of lexical items (closed-class words in their case),
rather than just a list of possible category types, in-
creased its chances of converging. This approach of
automating the learning process differs from the pre-
vious language learning methods described, in that
it doesn’t require the specification of any particular
patterns, only knowledge of the grammar formalism.
For this paper, as a baseline, we implement
a generalised version of Watkinson and Manand-
har’s mechanism for determining the category γ
of a single OOL word in a sentence where the
rest of the words C1...CN are in the lexicon: γ =
argmaxParse(C1...Cn,γ). This is equivalent to
backing off to the set of all known category types;
the learner returns the category that maximises the
probability of the completed parse tree. We ignore
the optimisation and compression steps of the origi-
nal system.
</bodyText>
<subsectionHeader confidence="0.999925">
2.2 A Rule-based System
</subsectionHeader>
<bodyText confidence="0.9997972">
Yao et al. (2009a; 2009b) developed a learning sys-
tem based on handwritten translation rules for de-
ducing the category (X) of a single unknown word
in a sentence consisting of a sequence of partially-
parsed constituents (A..N).
Their system was based on a small inventory of
inference rules that eliminated ambiguity in the or-
dering of arguments. For example, one of the Level
3 inference rules specifies the order of the arguments
in the deduced category:
</bodyText>
<equation confidence="0.643851">
A X B C → D ⇒ X = ((D\A)/C)/B
</equation>
<bodyText confidence="0.9994308125">
Without this inductive bias the learner would
have to deal with the ambiguity of the options
((D/C)/B)\A and ((D/C)\A)/B at minimum. In
addition they limited their learner to CG-compatible
parse structures and their constituent strings to
length 4.
Their argument is that only this minimal bias is
needed to learn syntactic structures, including the
fronting of polar interrogative auxiliaries and aux-
iliary word order (should &gt; have &gt; been), from a
training set that did not explicitly contain full evi-
dence for them.
Although Yao et al. (2009b) used the full set of
CCG combinators to generate learned categories,
they employed a post-processing step to filter spu-
rious categories by checking whether the category
</bodyText>
<page confidence="0.696979">
1247
</page>
<equation confidence="0.992076875">
DERIVE([C1...Cn],β,γ)
if β = 0/
then return (γ)
else if C1 = Cn = X
�γ = γ +β;
DERIVE(X, /0,γ)
{ if C1 ∈/ S,X
then DERIVE([C2...Cn],β\C1,γ)
</equation>
<construct confidence="0.865913">
if Cn ∈/ S,X then DERIVE([C1...Cn−1],β/Cn,γ)
if β - B and C1 - B/A and C1 ∈/ S,X
then DERIVE([C2...Cn],A,γ)
if β - B and Cn - B\A and Cn ∈/S,X
then DERIVE([C1...Cn−1],A,γ)
</construct>
<figureCaption confidence="0.961217">
Figure 1: Generalised recursive rule-based algorithm,
</figureCaption>
<bodyText confidence="0.944339647058824">
where [C1...Cn] is a sequence of categories, one of which
is X, β is a result category, and γ is the (initially empty)
category set.
participated in a CG-only derivation (using applica-
tion rules only). This is effective in limiting spuri-
ous derivations, but at the expense of reduced recall
on those sentences for whose analysis CCG rules of
composition etc. are crucial.
Their rules were effective for their toy-scale
datasets, but for the purposes of this paper we have
implemented a generalised version of the recursive
algorithm for use in wide-coverage parsing. This al-
gorithm is outlined in Figure 1. It takes a sequence
of categorial constituents, all known except one (X),
and builds a candidate set of categories (γ) for the
unknown word by recursively applying Yao’s Level
0 and Level 1 inference rules.
</bodyText>
<subsectionHeader confidence="0.998826">
2.3 Chart Inference
</subsectionHeader>
<bodyText confidence="0.99996">
Both Watkinson’s and Yao’s experiments were fully
convergent over toy datasets, but did not scale to re-
alistic corpora. Watkinson attempted to learn from
the LLL corpus (Kazakov et al., 1998), but attributed
the failure to the small amount of training data rela-
tive to the corpus, and the naive initial category set.
Yao’s method was only ever designed as a proof-of-
concept to show how much of the language can be
learned from partial evidence, and was not meant to
be run in earnest in a real-world learning setting. For
one, the rules do not cover the full set of partial parse
conditions, and further to that, they do not allow for
partial parses to be reanalysed within the learning
framework.
To that end, we have developed a learning algo-
rithm that is capable of operating within the one-
unknown-word-per-sentence learning setting estab-
lished by the two baseline systems, that is able to
invent new category types, and that is able to take
advantage of the full generality of CCG. This sec-
tion shows that it performs as well as the previous
two systems on a toy corpus, and the next section
proves that it more readily scales to natural language
domains.
Mellish (1989) established a two-stage bidirec-
tional chart parser for diagnosing errors in input text.
His method relied heavily on heuristic rules, and
the only evaluation he did was on number of cy-
cles needed for each type of error, and number of
solutions produced. His method was designed for
use in producing parses where the original parser
failed, dealing with omissions, insertions, and mis-
spelled/unknown words. The only method used to
rank the possible solutions was heuristic scores.
Kato (1994) implemented a revised system that
used a generalised top-down parser, rather than a
chart, and was able to get the number of cycles to
decrease.
In both cases the evaluation was only on a toy cor-
pus, and they did not evaluate on whether the sys-
tems diagnosed the errors correctly, or whether the
solution they offered was accurate. They also had
to deal with cases where the error was ambiguous,
for example, where an inserted word could be inter-
preted as a misspelling or vice-versa.
Where Mellish uses the two-stage parsing process
to complete malformed parses, we use it to diagnose
unknown lexical items. In addition, we work on the
scale of a full grammar and wide-coverage parser,
using modern lexical corpora.
Our method is a wrapper for a naive generative
CCG parser StatOpenCCG (Christodoulopoulos,
2008), a statistical extension to OpenCCG (White
and Baldridge, 2003). In the general case, the parser
is trained on all the labelled data available in a par-
ticular learning setting, then the learner discovers
new lexical items from unlabelled text. Like the
brute force and rule-based systems, it is vulnerable
</bodyText>
<figure confidence="0.7594615">
then
else
1248
CCG Combinator Inverse Combinator
</figure>
<equation confidence="0.987654">
B A X = A/B if v(B) &lt; 1
X A X = B
A\B A X = B
X A X = A\B if v(B) &lt; 1
C/B A/B X = A/C
X A/B X = C/B
A\C A\B X = C\B
X A\B X = A\C
</equation>
<figureCaption confidence="0.999847">
Figure 2: Derivation of inverse combinators
</figureCaption>
<figure confidence="0.99592975">
A/B B A (&gt;) X
A/B
B A\B A (&lt;) X
B
A/C C/B A/B (&gt;B) X
A/C
C\B A\C A\B (&lt;B) X
C\B
</figure>
<equation confidence="0.9536">
�
�P(HeadRight|R)
P(target = C|R,S) = max
P(HeadLeft|R)
P(HeadRight|R) = I P[outside](R)* }
P(HeadLeft|R) = I P[inside](S)* }
P(exp = left|R)*
P(C|R,exp = left)*
P(S|R,exp = left,C)
P[outside](R)*
P[inside](S)*
P(exp = right|R)*
P(S|R,exp = right)*
P(C|R,exp = right,S)
</equation>
<figureCaption confidence="0.935037">
Figure 3: CI probability that the target is category C,
given possible categories for result (R) and sister (S).
</figureCaption>
<bodyText confidence="0.999295733333333">
to attachment errors and ambiguity from adverbials.
The learning step consists in presenting the parser
with sentences all of whose words but one are in-
lexicon. The parser must have a statistical parsing
model, which contains a seed lexicon, a set of CCG
combinators, and an optional set of unary and binary
rules learned from the training corpus.
First the baseline bottom-up parser is called upon
to produce a partial parse chart. The learner takes
this partial chart and fills the top right cell with a
distribution for the result category based on the end
punctuation.2
Using this partial chart that contains at least one
entry for every leaf cell (except the one OOL tar-
get cell) and at least one entry for the result, the
</bodyText>
<footnote confidence="0.807536333333333">
2For simple corpora, only S is required, but realistic corpora
necessitate a distribution over all result types, including noun
phrases and fragments.
</footnote>
<bodyText confidence="0.957535972222222">
learner steps through the chart in a top-down ver-
sion of CYK (Younger, 1967). For the top-down
process, the standard combinators have to be refor-
mulated to take an argument and a result as inputs,
rather than two arguments as in the standard bottom-
up case. In addition, the learner has access to the
non-combinator rules from the parse model, which
have been similarly inverted for top-down use. This
process continues until the target cell has been filled,
and the ranked set of categories is returned.
The probability that the target has a given cat-
egory is calculated as the greater of the right- or
left-headed derivations, according to Figure 3. At
training time, the StatOpenCCG parser creates a
head-dependency model from the training corpus, in
which we can look up the values for the expansion
probabilities. Where a value is unavailable, it backs
off to a pre-specified value (default 0.0001).3 The
system requires a pruning parameter that limits each
cell to the top N most probable categories. Here, we
set N=10, to limit the search space and complexity. 4
Figure 2 sets out the inventory of inverse combi-
nators used in the top-down learning step. Each stan-
dard binary CCG combinator motivates two inverse
combinators: one for each possible missing item. In
the two permissive instances where the sister cate-
gory’s form is the unrestricted B, we limit the sis-
ter’s valency to 1, in order to keep the learner from
generating spurious categories that could result from
these two rules being overapplied.
Figure 4 illustrates the workings of the learning
3This backoff parameter allows adjustment of the expecta-
tion of new category types and could be replaced with another
smoothing method in subsequent implementations.
4Further testing on the McGuffey corpus has shown the av-
erage rank of correct tags in the category set to be 1.4.
</bodyText>
<page confidence="0.981484">
1249
</page>
<bodyText confidence="0.99986925">
algorithm for the sentence The cat X her. The grey
cells are filled as a partial chart by the parser, and the
white cells are filled by the top-down learner. Note
that taking rule probabilities into account makes the
algorithm robust to ambiguity. The highest-ranking
lexical category for her is NP[nb]/N, but the next
highest (NP) is preferred in the derivation of the
highest-ranking category for the unknown word X.
</bodyText>
<sectionHeader confidence="0.996551" genericHeader="method">
3 Experiment I: Convergence
</sectionHeader>
<bodyText confidence="0.999954666666667">
In the following experiments, we compare Chart In-
ference to the two baseline methods: Brute Force
(BF), derived from Watkinson and Manandhar, and
Rule-Based (RB), derived from Yao et al. This sec-
tion investigates how robust the three systems are to
changes in theoriginal seed lexicon.
</bodyText>
<subsectionHeader confidence="0.993754">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999986111111111">
For this experiment we test the three systems on a
reconstructed version of Corpus 1 from Watkinson
and Manandhar’s experiments.5 The lexicon con-
tains 40 word-category pairs, including the full stop
(S\S), which was not in Watkinson’s experiment,
and one example of noun-verb ambiguity (saw). The
test sentences are randomly generated from a simple
PCFG over the lexicon, and are always presented to
the learners in the same order.
</bodyText>
<subsectionHeader confidence="0.994865">
3.2 Methods
</subsectionHeader>
<bodyText confidence="0.958871210526316">
In order to directly compare the three learning meth-
ods, we use the evaluation setting from Watkinson
and Manandhar (1999), which consists of a 40-entry
target lexicon and a PCFG language model used to
randomly generate 1000 sentences. We then specify
a seed lexicon and run the learner incrementally, so
that it deals with one sentence at a time, then feeds
the learned material back into the lexicon. Watkin-
son’s system was shown to fully convergent (they
defined convergence as cosine similarity between
the seed lexicon (S) and the target lexicon (T) ex-
ceeding 0.99), whenever the seed lexicon contained
at least one instance of each of the category types in
the target lexicon (Watkinson and Manandhar, 1999)
5The full corpus was not included in any of Watkinson’s pa-
pers, but its properties were outlined to such an extent that it
was straightforward to recreate, though the reconstruction may
differ from the original in the distribution of category types. The
reconstructed corpus will be released shortly.
</bodyText>
<figureCaption confidence="0.993927">
Figure 5: Learning curve for all three methods when the
seed contains no ditransitives. CI and RB are identical.
</figureCaption>
<figure confidence="0.9695925">
1 10 100 1000
Sentences seen
</figure>
<figureCaption confidence="0.972857">
Figure 6: Learning curve for all three methods when seed
contains only three determiners and one noun.
</figureCaption>
<subsectionHeader confidence="0.809805">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.997494714285714">
When run incrementally over this toy corpus, both
the RB and CI algorithms converge to the target
lexicon in an identical sigmoid learning curve (not
shown). However, when we start with an im-
poverishedg seed, the algorithms’ behaviours start
to diverge. Figure 5 shows the learning curve
for the three methods when the seed lexicon
omits all instances of the ditransitive category type
((S\NP) /NP) /NP. Both RB and CI converge iden-
tically as expected, but BF, the lower curve, cannot
learn any category types that are not attested in the
seed, so it plateaus at 95% similarity.
When the seed is reduced to only three deter-
miners and a noun, CI can still learn the complete
</bodyText>
<figure confidence="0.998505892857143">
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
1
R
B
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
CI
RB
BF
I
</figure>
<page confidence="0.738736">
1250
</page>
<table confidence="0.9545165">
0 1 2 3
NP[nb]/N : 0.08428 NP[nb] : 0.00138 S[dcl]\NP : 2.90E-5 S[dcl] : 1.0 0
S[dcl]/NP : 2.90E-10
The N : 0.01890 (S[dcl]\NP)/NP : 1.35E-7 S[dcl]\NP : 1.00E-4
NP : 0.00174 1
S/(S\NP) : 0.00152
cat (S[dcl]\NP)/NP : 2.21E-9 S[dcl]\NP : 1.64E-6
(S[dcl]\NP)\NP[nb] : 6.06E-19 S[dcl]\NP[nb] : 7.41E-17 2
(S[dcl]/NP)\NP[nb] : 4.00E-23 (S[dcl]\NP)\N : 2.87E-17
... ...
X NP[nb]/N : 0.05467
NP : 0.02439 3
S/(S\NP) : 0.02124
her .
</table>
<figureCaption confidence="0.998615">
Figure 4: Example of a two-stage derivation using Chart Inference: Grey boxes are filled bottom-up by the partial
parser; white boxes top-down by the learner. The target cell (2,2) shows the correct category type as the highest
probability solution.
</figureCaption>
<bodyText confidence="0.9999633">
lexicon, despite some initial missteps and a steeper
curve. However, the other two methods fail catas-
trophically (Figure 6). BF never gets going, since it
can only correctly learn the remaining nouns. RB is
partially successful, but is thwarted by a bad deci-
sion at 80% that quickly compounds to diverge from
the target lexicon, ending up with higher coverage
in the form of more lexical entries, but lower preci-
sion, as the final similarity plateaus at the same level
as the original seed.
</bodyText>
<sectionHeader confidence="0.99512" genericHeader="method">
4 Experiments II and III: Coverage
</sectionHeader>
<bodyText confidence="0.9999395">
Next, we compare the three learning methods on a
larger corpus of natural language, to investigate how
well they perform at recovering a wide range of cat-
egory types in complex settings.
</bodyText>
<subsectionHeader confidence="0.994474">
4.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999971333333333">
We have constructed a small natural language lex-
icon based on the first volume of a 6-volume 1836
children’s primer, McGuffey’s Eclectic Reader.6
Volume 1 of the McGuffey corpus (MG1) consists
of 546 sentences that have been manually annotated
with CCG categories, automatically parsed, and then
corrected. Volume 2 (MG2) comprises 801 sen-
tences, annotated in the same manner as Volume 1,
though not as reliably. The McGuffey corpus makes
</bodyText>
<footnote confidence="0.99265625">
6The raw text of William Holmes McGuffey’s Eclectic
Reader is available as an e-book from Project Gutenberg at
http://www.gutenberg.org/ebooks/14640. The annotated corpus
will be released shortly.
</footnote>
<bodyText confidence="0.99936775">
an ideal seed for development purposes, as it con-
tains a high proportion of simple declarative sen-
tences, but also touches on questions, quotations,
passives, and other complex constructions.
</bodyText>
<subsectionHeader confidence="0.971812">
4.2 Methods
</subsectionHeader>
<bodyText confidence="0.986582083333333">
In the first of these two experiments we train and
test on the same corpus in one pass, attempting to
learn each word token in turn and comparing the
learned category set to the gold standard annota-
tion. Because we know that the lexicon contains all
the necessary entries to correctly parse all the sen-
tences, this addresses the lexical coverage problem
discussed in Section 1 of this paper.
The second of these two experiments looks at a
more realistic environment for word learning: the
parser is initially trained on MG1, then tested on
MG2. We evaluate on the gold standard categories
in MG2. Since we are not guaranteed to have ac-
cess to all the necessary word/category pairs in the
seed lexicon, the precision and recall values for this
second experiment will inevitably be lower than the
first.
Figure 7 outlines the process of producing new
parsed sentences out of raw text. The process be-
gins like the previous experiment, but then the cat-
egory set generated by the learner is passed back to
the parser, so it can incorporate this new informa-
tion into its lexicon and produce a full parse. The
Hypothesis lexicon is cleared after every sentence.
</bodyText>
<page confidence="0.994728">
1251
</page>
<figureCaption confidence="0.999322">
Figure 7: Learning framework for Experiments II-IV.
</figureCaption>
<subsectionHeader confidence="0.767787">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999933261904762">
Table 1 compares the category match accuracy
across the three systems in experiment II, as well
as the baseline that chose the the most probable
category for the target word’s POS. Two tasks are
scored: Top One, where we evaluate the single
highest-scoring category against the gold-standard
tag, and Top Ten, where we check to see if the gold
tag is in the set of the ten highest-probability cate-
gories returned by the learner.
CI achieves the best F-scores in both tasks, reach-
ing 76% for Top One and 94% for Top Ten. POS
backoff has an advantage in the Top Ten task, es-
pecially in recall, since it returns an answer in ev-
ery case, but CI still outperforms it on F-score. BF
achieves the highest precision in the Top One task,
but takes 30 hours to do so, since it is searching over
all possible categories. RB is markedly worse in
both precision and recall, but also remarkably fast.
CI combines the merits of both BF and RB, yield-
ing a higher F-score than BF and a processing time
similar to RB.
In Experiment III, to test the limits of the learners
on truly OOL words, we again train on MG1, but test
instead on MG2. We can then perform a meaningful
error analysis on the results, showing how the three
word-learning methods compare in actual practice,
in a realistic setting.
Out of its 801 sentences in MG2, only 32 present
learning opportunities for the learners, being be-
tween 2 and 10 tokens long, containing no inter-
nal punctuation or coordination, and containing only
one OOL word.
Table 2 shows the category match results of the
three systems on MG2. Recall is calculated over
the set of learning opportunities, of which there are
only 32. BF performs best in all metrics, but the
CI results are reasonable. The underlying reason for
this behaviour is that the 32 learning targets are all
of common categories: over half of them are N or
NP. Since the Brute Force learner seeks simply to
maximise the tree probability, N and NP are its most
common guesses in general.
</bodyText>
<sectionHeader confidence="0.998311" genericHeader="evaluation">
5 Experiment IV: Domain Adaptation
</sectionHeader>
<bodyText confidence="0.99991872">
Clark et al. (2004) identified the problem with using
news data to train a parser for a question answering
task as the lack of lexical support for question words.
Some lexical types were missing entirely. The lexi-
con for CCGBank §02-21 contains 12 WH-question
types, notably lacking some important ones. Clark
et al. note the absence of one category in partic-
ular: (S[wq]/(S[dcl]\NP))/N, the category needed
for What President became Chief Justice after his
presidency?
They attempt to adapt the discriminative C&amp;C
parser (Clark and Curran, 2007) to the QA do-
main by retraining on 500 hand-labelled question
sentences, then automatically parsing and hand-
correcting an additional 671. The entire set was
then used in conjunction with CCGBank §02-21 to
train a final parsing model. Their per-word accuracy
rose from a 68.5% baseline to 94.6% for the newly
trained model.
In this experiment, we examine how close we
can get to those results by using Chart Inference to
learn WH-question words from the unlabelled ques-
tion corpus. If successful, this would eliminate the
human-annotation step for domain adaptation of the
kind investigated by (Clark et al., 2004).
</bodyText>
<subsectionHeader confidence="0.98384">
5.1 Corpora
</subsectionHeader>
<bodyText confidence="0.99996075">
We trained the initial parser on the CCG-
Bank (Hockenmaier and Steedman, 2007; Hock-
enmaier, 2003) training set (§02-21), consisting of
39603 sentences of Wall Street Journal text (Marcus
et al., 1993). It is important to note that this training
corpus contains only 93 questions in total, so it is
not surprising that several category types for ques-
tion words are entirely unrepresented. It also rein-
</bodyText>
<figure confidence="0.991152357142857">
Raw
sentence
Original
lexicon
Parsed
sentence
Hypothesis
lexicon
Parser
Category
Set
Partial
Parse
Learner
</figure>
<page confidence="0.95567">
1252
</page>
<table confidence="0.997059833333333">
P Top One F P Top Ten F Time (m)
R R
POS 64.91 64.91 64.91 92.55 92.55 92.55 1
BF 80.53 65.84 72.45 95.97 78.32 86.25 1740
RB 39.77 37.92 38.82 68.46 65.28 66.83 12
CI 78.63 74.16 76.33 97.03 91.52 94.20 22
</table>
<tableCaption confidence="0.999087">
Table 1: Exp. II: Category match results for the three systems on the McGuffey corpus, training and testing on MG1.
</tableCaption>
<table confidence="0.9997276">
P Top One F P Top Ten F
R R
BF 70.83 53.13 60.72 83.33 62.50 71.43
RB 16.13 15.63 15.87 29.03 28.13 28.57
CI 61.90 40.63 49.06 76.19 50.00 60.38
</table>
<tableCaption confidence="0.983883666666667">
Table 2: Exp. III: Category match results for the three systems on the McGuffey corpus, training on MG1 and testing
on MG2. Common categories (N, NP, N/N) are overrepresented in the test data, leading to higher BF scores. POS
tags not available for MG2, so no POS baseline is reported.
</tableCaption>
<bodyText confidence="0.979413823529412">
forces the fact that this is a domain-adaptation task.
We use the same 500-sentence test set as Rimell
and Clark (2008b). The test corpus consists of 488
questions, each starting with What, When, How,
Who or Where. The learning corpus contains 1328
questions in a similar distribution.
Only three out of the five categories needed
to parse What-questions are present in the
CCGBank seed lexicon: S[wq]/(S[q]/NP),7,
S[wq]/(S[dcl]\NP),8 and S[wq]/(S[q]/NP)/N.9
For this experiment we focus on the
subject WH-element extraction category
(S[wq]/(S[dcl]\NP))/N, as in Which cat is the
grandmother?. This particular category was chosen
as a point of investigation because it is OOL in
CCGBank and is common enough to meaningfully
evaluate.
</bodyText>
<subsectionHeader confidence="0.997382">
5.2 Methods
</subsectionHeader>
<bodyText confidence="0.999927714285714">
The baseline is the original StatCCG parser and lexi-
con. We also employ self-training (Charniak, 1997),
in which a parser is used to parse a set of sentences,
and then retrained using those output trees. Self-
training has had very little success in CCG appli-
cations hitherto. McClosky et al (2006) attribute
success in self-training to a confluence of circum-
</bodyText>
<footnote confidence="0.991679">
7Object question category as in What is the Keystone State?
8Subject question category as in What lays blue eggs?
9Object WH-element extraction category as in What conti-
nent is Scotland in?
</footnote>
<bodyText confidence="0.999742153846154">
stances particular to their learning setting, which has
the benefit of a discriminative re-ranker, both in the
parsing case and in the learning case (McClosky et
al., 2008). We follow their recommendations that
the best performance is achieved when all the train-
ing sentences are parsed at once, rather than incre-
mentally.
We evaluate the success of CI in bootstrapping
Wh-question categories from the out-of-domain cor-
pus in two ways. First, we compare the CI output to
the gold standard categories labelled in Rimmell and
Clark (2008a). Second, we add the parsed questions
into the training set, then retrain and finally retest the
parser.
The parser was initially trained on CCGBank §02-
21 with a word frequency threshold of 5.10 It pro-
duces partial parse charts in the cases where all
words in the sentence are in-lexicon, except for the
WH-word target, for which the learner attempts to
return a category motivated by that context.
We run the learner on the set of 149 sentences
from the TREC Question-Answering corpus (Rimell
and Clark, 2008b) that contain the word/category
pair What:(S[wq]/(S[dcl]\NP))/N. For this exper-
iment the end-punctuation distribution derived from
the training corpus is replaced with a single value:
</bodyText>
<equation confidence="0.583329">
P(S[wq]|?) = 1.
</equation>
<footnote confidence="0.8807725">
10StatCCG requires a parameter to trade off between training
the lexicon and the POS-backoff.
</footnote>
<page confidence="0.834776">
1253
</page>
<table confidence="0.9996738">
BL CI CI+ST
All Words 84.31 86.59 87.03
POS=WHQ 53.40 56.19 59.54
Word=What 55.87 60.83 65.42
Cat=SubjExt 7.84 52.94 58.82
</table>
<tableCaption confidence="0.9980765">
Table 3: F-score over individual category matches. Bold
means significantly different from the Baseline.
</tableCaption>
<subsectionHeader confidence="0.866981">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999991125">
Table 3 shows the change in F-score throughout this
experiment. BL is the baseline condition, where
the accuracy is predictably high over all the words
in the sentence, but lower when we examine the
question words only. It is most telling that the
baseline F-score over words that should be tagged
with the subject WH-element-extraction category
((S[wq]/(S[dcl]\NP))/N) is extremely low. In fact,
that seven percent represents only a handful of in-
stances of Which, and none of What. Applying Chart
Inference to the problem results in statistically sig-
nificant increases in all metrics, but the biggest gain
is in the last. When we first apply CI, then self-train
over the full training corpus, we further increase all
metrics, and again the largest gain is over the target
category type specifically. 11 The reason for this can
be clearly seen when we evaluate the lexicons cre-
ated by each method.
Table 4 shows the differences in the impact on
the lexicon between baseline (BL), Chart Induc-
tion (CI), and the combined method of CI and self-
training (CI+ST).12 CI leaves the initial distribution
unchanged while adding seven more category types.
One of these is the category we are interested in:
(S[wq]/(S[dcl]\NP))/N, which is previously asso-
ciated with Which in the baseline lexicon. The other
six are spurious categories, and have low counts.
Combining the learning mechanisms by running first
CI, and then ST, has the effect of introducing the cat-
egory we need, and then elevating the counts. The
probability for S[wq] is elevated as well, as a result
of misparses, but the whole process results in bet-
</bodyText>
<footnote confidence="0.997539">
11We also ran the experiment using ST only, which per-
formed better than CI alone, but only over a different set consist-
ing entirely of seen categories. We do not report those figures
here because they are not commensurable with the CI results.
12What has 31 categories in total in the baseline lexicon; here
we show only the [wq] types.
</footnote>
<tableCaption confidence="0.9116475">
ter category matches over the test set, as we saw in
Table 3.
</tableCaption>
<subsectionHeader confidence="0.805036">
5.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.986683477272727">
Of the previously known categories, the ST
step overwhelmingly prefers three categories:
one subject extraction category S[wq]/(S[dcl]\NP)
and two object extraction S[wq]/(S[q]/NP) and
(S[wq]/(S[q]/NP))/N. The remaining categories
are classified in Table 4 as either rare (R), spu-
rious (*), or duplicate (D). Rare categories, like
S[wq] are used for specialised cases (the sentence
What?) which occur in PTB, but not in the QA
corpus. Spurious categories, like (S[wq]/PP)/N
exist in the baseline parser, arising from er-
rors in either the original PTB, or the transla-
tion to CCGBank. S[wq]/S[q] is only used where
S[wq]/(S[q]/NP) is meant, but fails to capture the
extraction. S[wq]/(S[dcl]/NP) is a misinterpretation
of sentences requiring (S[wq]/(S[dcl]\NP))/N, but
without capturing the extracted N.
Five spurious categories are also introduced
by the CI learning step. (S[wq]/S[dcl])/N and
(S[wq]/((S[dcl]\NP[expl])/NP))/N are spurious
forms of (S[wq]/(S[dcl]\NP))/N that arise when
the constituent directly right of the target is mis-
parsed; the former misses the extraction and the lat-
ter adds an extra dummy subject. S[wq]/N occurs
when the main verb of the sentence is treated as
a participle, forming a complex nominal argument.
(S[wq]/N)/N and (S[wq]/(S[dcl]/(S[pt]\NP)))/N
are caused by similar verbal ambiguity.
The classification of (S[wq]/S[inv])/N as a du-
plicate category is linguistically motivated. Rather
than interpret the embedded sentence as declarative,
the parser uses has:S[inv]/NP to interpret it instead
as an inverted sentence. In essence, it cannot see
the difference between What companies have them?
and What choice have they? when the NPs lack a
case distinction. As such, it duplicates the work of
the target (S[wq]/(S[dcl]\NP))/N, because the con-
stituents S[dcl]\NP and S[inv] are often synonymous
in practice.
As seen in Table 4, the distinction between rare
and spurious categories cannot be made on fre-
quency alone, but the best categories are the ones
with the highest frequency. Duplicate categories can
be considered spurious for the sake of parsing, but
</bodyText>
<page confidence="0.963259">
1254
</page>
<table confidence="0.999719809523809">
P(W|C) F P(C|W)
? C BL CI CI+ST BL CI CI+ST BL CI CI+ST
R S[wq] 0.09 0.09 0.17 1 1 2 0.006 0.005 0.002
R S[wq]/PP 0.6 0.6 0.6 3 3 3 0.019 0.016 0.003
* (S[wq]/PP)/N 1 1 1 1 1 4 0.006 0.005 0.004
* S[wq]/(S[ad j]\NP) 0.5 0.5 0.5 1 1 1 0.006 0.005 0.001
S[wq]/(S[dcl]\NP) 0.37 0.37 0.86 22 22 239 0.137 0.118 0.225
S[wq]/(S[dcl]/NP) 1 1 1 1 1 8 0.006 0.005 0.008
(S[wq]/(S[dcl]/NP))/N 0.5 0.5 0.5 1 1 1 0.006 0.005 0.001
* S[wq]/(S[ng]\NP) 1 1 1 1 1 2 0.006 0.005 0.002
R S[wq]/S[poss] 0.83 0.83 0.83 5 5 5 0.031 0.027 0.005
* S[wq]/S[q] 0.03 0.03 0.12 2 2 9 0.012 0.011 0.008
S[wq]/(S[q]/NP) 0.64 0.64 0.97 16 16 331 0.099 0.086 0.312
(S[wq]/(S[q]/NP))/N 0.36 0.36 0.95 4 4 136 0.025 0.021 0.128
(S[wq]/(S[dcl]\NP))/N - 0.5 0.96 - 4 75 - 0.021 0.071
* S[wq]/N - 1 1 - 8 12 - 0.043 0.011
* (S[wq]/S[dcl])/N - 1 1 - 8 28 - 0.043 0.026
* (S[wq]/N)/N - 1 1 - 4 7 - 0.021 0.007
D (S[wq]/S[inv])/N - 1 1 - 3 78 - 0.016 0.074
* (S[wq]/(S[dcl]/(S[pt]\NP)))/N - 1 1 - 1 2 - 0.005 0.002
* (S[wq]/((S[dcl]\NP[expl])/NP))/N - 1 1 - 1 12 - 0.005 0.011
</table>
<tableCaption confidence="0.982272">
Table 4: Exp. IV: Lexical category distribution for the word What in the baseline §02-21 of CCGBank (BL), after
Chart Inference (CI), and after first applying Chart Inference, then self-training (CI+ST). Column 1 classifies low-
frequency categories as rare (R), spurious (*) or duplicate (D). Cateogories above the middle line are present in the
Baseline lexicon; below are induced.
</tableCaption>
<bodyText confidence="0.999726333333333">
are linguistically interesting, and if they are frequent
enough, that is possibly an indication that the struc-
ture of the lexicon or the grammar is non-optimal.
</bodyText>
<sectionHeader confidence="0.998418" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99992425">
Chart Inference is a useful tool for finding OOL cat-
egories. It has been shown to outperform both the
brute-force and rule-based systems. When used in
conjunction with self-training, CI presents a valu-
able framework for domain adaptation in the case
where whole category types are missing from the
lexicon.
It remains to put Chart Inference into an appro-
priate framework for improving coverage over the
baseline WSJ-trained StatCCG parser. We estimate
an upper bound of 20% error reduction possible over
CCGBank §00, if the lexicon is expanded to cover
all the necessary word/category pairs. Improving
global F-score for §23 is of course very difficult. The
lexical entries CI finds are by definition rare and at
the scale we are running, they are unlikely to occur
in those 2000 sentences. We believe our analysis of
the lexical items themselves shows that we are learn-
ing a high proportion of good lexical entries.
The problem of discovering missing categories
for known words remains. We have shown through
adapting to the question domain that it is possible to
make focused improvements when we can identify
the gaps in coverage (as in wh-question words), but
in order to address the challenge of automatic lex-
icon extension fully, quite different techniques for
generalising lexical entries for seen words will be
require.
</bodyText>
<sectionHeader confidence="0.998056" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9761771">
The authors would like to thank Steve Clark
and Laura Rimell for provision of the annotated
QA corpus, Christos Christodoulopoulos for the
StatOpenCCG parser, Tejaswini Deoskar for edito-
rial advice, and Luke Zettlemoyer for considerable
mathematical assistance. This work was partially
funded by EU ERC Advanced Fellowship 249520
GRAMPLUS, IST Cognitive Systems IP EC-FP7-
270273 “XPERIENCE” and a grant from Wolfson
Microelectronics.
</bodyText>
<page confidence="0.982474">
1255
</page>
<sectionHeader confidence="0.998332" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923787234043">
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of AAAI ’97, pages 598–603.
Christos Christodoulopoulos. 2008. Creating a natu-
ral logic inference system with combinatory categorial
grammar. Master’s thesis, School of Informatics, Uni-
versity of Edinburgh.
Stephen Clark and James R. Curran. 2003. Log-linear
models for wide-coverage CCG parsing. In Proceed-
ings of EMNLP ’03, pages 97–104, Morristown, NJ,
USA.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493–552.
Stephen Clark, Mark Steedman, and James Curran. 2004.
Object-extraction and question-parsing using CCG. In
Proceedings of EMNLP ’04, pages 111–118.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355–396.
Julia Hockenmaier. 2003. Data and models for statis-
tical parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh, Edinburgh, UK.
Tsuneaki Kato. 1994. Yet another chart-based technique
for parsing ill-formed input. In Proceedings of ANLC
’94, pages 107–112, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
D. Kazakov, S. Pulman, and S. Muggleton. 1998. The
FraCas dataset and the LLL challenge. Technical re-
port, SRI International.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313–330, June.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of NAACL-HLT ’06, pages 152–159.
David McClosky, Eugene Charniak, and Mark Johnson.
2008. When is self-training effective for parsing? In
Proceedings of COLING ’08, pages 561–568, Morris-
town, NJ, USA.
Chris S. Mellish. 1989. Some chart based techniques for
parsing ill-formed input. In Proceedings of the ACL
’89, pages 102–109, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Laura Rimell and Stephen Clark. 2008a. Adapting a
lexicalized-grammar parser to contrasting domains. In
Proceedings of EMNLP ’08, pages 475–484, Strouds-
burg, PA, USA.
Laura Rimell and Stephen Clark. 2008b. Constructing a
parser evaluation scheme. In COLING ’08: Proceed-
ings of the workshop on Cross-Framework and Cross-
Domain Parser Evaluation, pages 44–50, Stroudsburg,
PA, USA.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
Stephen Watkinson and Suresh Manandhar. 1999. Un-
supervised lexical learning of categorial grammars. In
ACL’99: Workshop in Unsupervised Learning in Nat-
ural Language Processing.
Stephen Watkinson and Suresh Manandhar. 2000. Un-
supervised lexical learning with categorial grammars
using the LLL corpus. In James Cussens and Savso
Dvzeroski, editors, Learning Language in Logic, vol-
ume 1925 of Lecture Notes in Arti�cial Intelligence.
Springer.
Stephen Watkinson and Suresh Manandhar. 2001a. Ac-
quisition of large scale categorial grammar lexicons.
In Proceedings of PACLING ’01.
Stephen Watkinson and Suresh Manandhar. 2001b. A
psychologically plausible and computationally effec-
tive approach to learning syntax. In Walter Daelemans
and R’emi Zajac, editors, Proceedings of CoNLL ’01),
pages 160 – 167.
Michael White and Jason Baldridge. 2003. Adapt-
ing chart realization to CCG. In Proceedings of the
9th European Workshop on Natural Language Gener-
ation, pages 119–126.
Xuchen Yao, Jianqiang Ma, Sergio Duarte, and Cagri
Coltekin. 2009a. An inference-rules based catego-
rial grammar learner for simulating language acquisi-
tion. In Proceedings of the 18th Annual Belgian-Dutch
Conference on Machine Learning, Tillburg.
Xuchen Yao, Jianqiang Ma, Sergio Duarte, and Cagri
Coltekin. 2009b. Unsupervised syntax learning with
categorial grammars using inference rules. In Pro-
ceedings of The 14th Student Session of the European
Summer School for Logic, Language, and Information,
Bordeaux.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189–208.
</reference>
<page confidence="0.991175">
1256
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.947298">
<title confidence="0.999017">Semi-supervised CCG Lexicon Extension</title>
<author confidence="0.999975">Emily Thomforde Mark Steedman</author>
<affiliation confidence="0.999876">University of Edinburgh University of Edinburgh</affiliation>
<email confidence="0.982343">e.j.thomforde@sms.ed.ac.uksteedman@inf.ed.ac.uk</email>
<abstract confidence="0.997918277777778">This paper introduces Chart Inference (CI), an algorithm for deriving a CCG category for an unknown word from a partial parse chart. It is shown to be faster and more precise than a baseline brute-force method, and to achieve wider coverage than a rule-based system. In addition, we show the application of CI to a domain adaptation task for question words, which are largely missing in the Penn Treebank. When used in combination with self-training, CI increases the precision of the baseline StatCCG parser over subjectextraction questions by 50%. An error analysis shows that CI contributes to the increase by expanding the number of category types available to the parser, while self-training adjusts the counts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of AAAI ’97,</booktitle>
<pages>598--603</pages>
<contexts>
<context position="27348" citStr="Charniak, 1997" startWordPosition="4604" endWordPosition="4605">s contains 1328 questions in a similar distribution. Only three out of the five categories needed to parse What-questions are present in the CCGBank seed lexicon: S[wq]/(S[q]/NP),7, S[wq]/(S[dcl]\NP),8 and S[wq]/(S[q]/NP)/N.9 For this experiment we focus on the subject WH-element extraction category (S[wq]/(S[dcl]\NP))/N, as in Which cat is the grandmother?. This particular category was chosen as a point of investigation because it is OOL in CCGBank and is common enough to meaningfully evaluate. 5.2 Methods The baseline is the original StatCCG parser and lexicon. We also employ self-training (Charniak, 1997), in which a parser is used to parse a set of sentences, and then retrained using those output trees. Selftraining has had very little success in CCG applications hitherto. McClosky et al (2006) attribute success in self-training to a confluence of circum7Object question category as in What is the Keystone State? 8Subject question category as in What lays blue eggs? 9Object WH-element extraction category as in What continent is Scotland in? stances particular to their learning setting, which has the benefit of a discriminative re-ranker, both in the parsing case and in the learning case (McClo</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of AAAI ’97, pages 598–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
</authors>
<title>Creating a natural logic inference system with combinatory categorial grammar. Master’s thesis,</title>
<date>2008</date>
<institution>School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="11616" citStr="Christodoulopoulos, 2008" startWordPosition="1929" endWordPosition="1930">toy corpus, and they did not evaluate on whether the systems diagnosed the errors correctly, or whether the solution they offered was accurate. They also had to deal with cases where the error was ambiguous, for example, where an inserted word could be interpreted as a misspelling or vice-versa. Where Mellish uses the two-stage parsing process to complete malformed parses, we use it to diagnose unknown lexical items. In addition, we work on the scale of a full grammar and wide-coverage parser, using modern lexical corpora. Our method is a wrapper for a naive generative CCG parser StatOpenCCG (Christodoulopoulos, 2008), a statistical extension to OpenCCG (White and Baldridge, 2003). In the general case, the parser is trained on all the labelled data available in a particular learning setting, then the learner discovers new lexical items from unlabelled text. Like the brute force and rule-based systems, it is vulnerable then else 1248 CCG Combinator Inverse Combinator B A X = A/B if v(B) &lt; 1 X A X = B A\B A X = B X A X = A\B if v(B) &lt; 1 C/B A/B X = A/C X A/B X = C/B A\C A\B X = C\B X A\B X = A\C Figure 2: Derivation of inverse combinators A/B B A (&gt;) X A/B B A\B A (&lt;) X B A/C C/B A/B (&gt;B) X A/C C\B A\C A\B (</context>
</contexts>
<marker>Christodoulopoulos, 2008</marker>
<rawString>Christos Christodoulopoulos. 2008. Creating a natural logic inference system with combinatory categorial grammar. Master’s thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Log-linear models for wide-coverage CCG parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP ’03,</booktitle>
<pages>97--104</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1036" citStr="Clark and Curran, 2003" startWordPosition="158" endWordPosition="161"> than a rule-based system. In addition, we show the application of CI to a domain adaptation task for question words, which are largely missing in the Penn Treebank. When used in combination with self-training, CI increases the precision of the baseline StatCCG parser over subjectextraction questions by 50%. An error analysis shows that CI contributes to the increase by expanding the number of category types available to the parser, while self-training adjusts the counts. 1 Introduction Unseen lexical items are a major cause of error in strongly lexicalised parsers such as those based on CCG (Clark and Curran, 2003; Hockenmaier, 2003). The problem is especially acute for less privileged languages, but even in the case of English, we are aware of many category types entirely missing from the Penn Treebank (Clark et al., 2004). In the case of totally unseen words, the standard method used by StatCCG (Hockenmaier, 2003) and many other treebank parsers is part-of-speech backoff, which is quite effective, affording an F-score of 93% over dependencies in §00 in the optimal configuration. It is difficult to say how backing off affects dependency errors, but when we examine category match accuracy of the CCGBan</context>
</contexts>
<marker>Clark, Curran, 2003</marker>
<rawString>Stephen Clark and James R. Curran. 2003. Log-linear models for wide-coverage CCG parsing. In Proceedings of EMNLP ’03, pages 97–104, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="24608" citStr="Clark and Curran, 2007" startWordPosition="4145" endWordPosition="4148">P are its most common guesses in general. 5 Experiment IV: Domain Adaptation Clark et al. (2004) identified the problem with using news data to train a parser for a question answering task as the lack of lexical support for question words. Some lexical types were missing entirely. The lexicon for CCGBank §02-21 contains 12 WH-question types, notably lacking some important ones. Clark et al. note the absence of one category in particular: (S[wq]/(S[dcl]\NP))/N, the category needed for What President became Chief Justice after his presidency? They attempt to adapt the discriminative C&amp;C parser (Clark and Curran, 2007) to the QA domain by retraining on 500 hand-labelled question sentences, then automatically parsing and handcorrecting an additional 671. The entire set was then used in conjunction with CCGBank §02-21 to train a final parsing model. Their per-word accuracy rose from a 68.5% baseline to 94.6% for the newly trained model. In this experiment, we examine how close we can get to those results by using Chart Inference to learn WH-question words from the unlabelled question corpus. If successful, this would eliminate the human-annotation step for domain adaptation of the kind investigated by (Clark </context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James Curran</author>
</authors>
<title>Object-extraction and question-parsing using CCG.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP ’04,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="1250" citStr="Clark et al., 2004" startWordPosition="193" endWordPosition="196">reases the precision of the baseline StatCCG parser over subjectextraction questions by 50%. An error analysis shows that CI contributes to the increase by expanding the number of category types available to the parser, while self-training adjusts the counts. 1 Introduction Unseen lexical items are a major cause of error in strongly lexicalised parsers such as those based on CCG (Clark and Curran, 2003; Hockenmaier, 2003). The problem is especially acute for less privileged languages, but even in the case of English, we are aware of many category types entirely missing from the Penn Treebank (Clark et al., 2004). In the case of totally unseen words, the standard method used by StatCCG (Hockenmaier, 2003) and many other treebank parsers is part-of-speech backoff, which is quite effective, affording an F-score of 93% over dependencies in §00 in the optimal configuration. It is difficult to say how backing off affects dependency errors, but when we examine category match accuracy of the CCGBank-trained parser, we find that POS backoff has been used on 19.6% of tokens, which means that those tokens are unseen, or too infrequent in the training data to be included in the lexicon. Of the 3320 items the par</context>
<context position="24081" citStr="Clark et al. (2004)" startWordPosition="4062" endWordPosition="4065">ng no internal punctuation or coordination, and containing only one OOL word. Table 2 shows the category match results of the three systems on MG2. Recall is calculated over the set of learning opportunities, of which there are only 32. BF performs best in all metrics, but the CI results are reasonable. The underlying reason for this behaviour is that the 32 learning targets are all of common categories: over half of them are N or NP. Since the Brute Force learner seeks simply to maximise the tree probability, N and NP are its most common guesses in general. 5 Experiment IV: Domain Adaptation Clark et al. (2004) identified the problem with using news data to train a parser for a question answering task as the lack of lexical support for question words. Some lexical types were missing entirely. The lexicon for CCGBank §02-21 contains 12 WH-question types, notably lacking some important ones. Clark et al. note the absence of one category in particular: (S[wq]/(S[dcl]\NP))/N, the category needed for What President became Chief Justice after his presidency? They attempt to adapt the discriminative C&amp;C parser (Clark and Curran, 2007) to the QA domain by retraining on 500 hand-labelled question sentences, </context>
</contexts>
<marker>Clark, Steedman, Curran, 2004</marker>
<rawString>Stephen Clark, Mark Steedman, and James Curran. 2004. Object-extraction and question-parsing using CCG. In Proceedings of EMNLP ’04, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="25311" citStr="Hockenmaier and Steedman, 2007" startWordPosition="4260" endWordPosition="4263">then automatically parsing and handcorrecting an additional 671. The entire set was then used in conjunction with CCGBank §02-21 to train a final parsing model. Their per-word accuracy rose from a 68.5% baseline to 94.6% for the newly trained model. In this experiment, we examine how close we can get to those results by using Chart Inference to learn WH-question words from the unlabelled question corpus. If successful, this would eliminate the human-annotation step for domain adaptation of the kind investigated by (Clark et al., 2004). 5.1 Corpora We trained the initial parser on the CCGBank (Hockenmaier and Steedman, 2007; Hockenmaier, 2003) training set (§02-21), consisting of 39603 sentences of Wall Street Journal text (Marcus et al., 1993). It is important to note that this training corpus contains only 93 questions in total, so it is not surprising that several category types for question words are entirely unrepresented. It also reinRaw sentence Original lexicon Parsed sentence Hypothesis lexicon Parser Category Set Partial Parse Learner 1252 P Top One F P Top Ten F Time (m) R R POS 64.91 64.91 64.91 92.55 92.55 92.55 1 BF 80.53 65.84 72.45 95.97 78.32 86.25 1740 RB 39.77 37.92 38.82 68.46 65.28 66.83 12 </context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and models for statistical parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh,</institution>
<location>Edinburgh, UK.</location>
<contexts>
<context position="1056" citStr="Hockenmaier, 2003" startWordPosition="162" endWordPosition="163">m. In addition, we show the application of CI to a domain adaptation task for question words, which are largely missing in the Penn Treebank. When used in combination with self-training, CI increases the precision of the baseline StatCCG parser over subjectextraction questions by 50%. An error analysis shows that CI contributes to the increase by expanding the number of category types available to the parser, while self-training adjusts the counts. 1 Introduction Unseen lexical items are a major cause of error in strongly lexicalised parsers such as those based on CCG (Clark and Curran, 2003; Hockenmaier, 2003). The problem is especially acute for less privileged languages, but even in the case of English, we are aware of many category types entirely missing from the Penn Treebank (Clark et al., 2004). In the case of totally unseen words, the standard method used by StatCCG (Hockenmaier, 2003) and many other treebank parsers is part-of-speech backoff, which is quite effective, affording an F-score of 93% over dependencies in §00 in the optimal configuration. It is difficult to say how backing off affects dependency errors, but when we examine category match accuracy of the CCGBank-trained parser, we</context>
<context position="25331" citStr="Hockenmaier, 2003" startWordPosition="4264" endWordPosition="4266">andcorrecting an additional 671. The entire set was then used in conjunction with CCGBank §02-21 to train a final parsing model. Their per-word accuracy rose from a 68.5% baseline to 94.6% for the newly trained model. In this experiment, we examine how close we can get to those results by using Chart Inference to learn WH-question words from the unlabelled question corpus. If successful, this would eliminate the human-annotation step for domain adaptation of the kind investigated by (Clark et al., 2004). 5.1 Corpora We trained the initial parser on the CCGBank (Hockenmaier and Steedman, 2007; Hockenmaier, 2003) training set (§02-21), consisting of 39603 sentences of Wall Street Journal text (Marcus et al., 1993). It is important to note that this training corpus contains only 93 questions in total, so it is not surprising that several category types for question words are entirely unrepresented. It also reinRaw sentence Original lexicon Parsed sentence Hypothesis lexicon Parser Category Set Partial Parse Learner 1252 P Top One F P Top Ten F Time (m) R R POS 64.91 64.91 64.91 92.55 92.55 92.55 1 BF 80.53 65.84 72.45 95.97 78.32 86.25 1740 RB 39.77 37.92 38.82 68.46 65.28 66.83 12 CI 78.63 74.16 76.33</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and models for statistical parsing with Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsuneaki Kato</author>
</authors>
<title>Yet another chart-based technique for parsing ill-formed input.</title>
<date>1994</date>
<booktitle>In Proceedings of ANLC ’94,</booktitle>
<pages>107--112</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10802" citStr="Kato (1994)" startWordPosition="1792" endWordPosition="1793">ms on a toy corpus, and the next section proves that it more readily scales to natural language domains. Mellish (1989) established a two-stage bidirectional chart parser for diagnosing errors in input text. His method relied heavily on heuristic rules, and the only evaluation he did was on number of cycles needed for each type of error, and number of solutions produced. His method was designed for use in producing parses where the original parser failed, dealing with omissions, insertions, and misspelled/unknown words. The only method used to rank the possible solutions was heuristic scores. Kato (1994) implemented a revised system that used a generalised top-down parser, rather than a chart, and was able to get the number of cycles to decrease. In both cases the evaluation was only on a toy corpus, and they did not evaluate on whether the systems diagnosed the errors correctly, or whether the solution they offered was accurate. They also had to deal with cases where the error was ambiguous, for example, where an inserted word could be interpreted as a misspelling or vice-versa. Where Mellish uses the two-stage parsing process to complete malformed parses, we use it to diagnose unknown lexic</context>
</contexts>
<marker>Kato, 1994</marker>
<rawString>Tsuneaki Kato. 1994. Yet another chart-based technique for parsing ill-formed input. In Proceedings of ANLC ’94, pages 107–112, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kazakov</author>
<author>S Pulman</author>
<author>S Muggleton</author>
</authors>
<title>The FraCas dataset and the LLL challenge.</title>
<date>1998</date>
<tech>Technical report, SRI International.</tech>
<contexts>
<context position="9327" citStr="Kazakov et al., 1998" startWordPosition="1537" endWordPosition="1540">heir toy-scale datasets, but for the purposes of this paper we have implemented a generalised version of the recursive algorithm for use in wide-coverage parsing. This algorithm is outlined in Figure 1. It takes a sequence of categorial constituents, all known except one (X), and builds a candidate set of categories (γ) for the unknown word by recursively applying Yao’s Level 0 and Level 1 inference rules. 2.3 Chart Inference Both Watkinson’s and Yao’s experiments were fully convergent over toy datasets, but did not scale to realistic corpora. Watkinson attempted to learn from the LLL corpus (Kazakov et al., 1998), but attributed the failure to the small amount of training data relative to the corpus, and the naive initial category set. Yao’s method was only ever designed as a proof-ofconcept to show how much of the language can be learned from partial evidence, and was not meant to be run in earnest in a real-world learning setting. For one, the rules do not cover the full set of partial parse conditions, and further to that, they do not allow for partial parses to be reanalysed within the learning framework. To that end, we have developed a learning algorithm that is capable of operating within the o</context>
</contexts>
<marker>Kazakov, Pulman, Muggleton, 1998</marker>
<rawString>D. Kazakov, S. Pulman, and S. Muggleton. 1998. The FraCas dataset and the LLL challenge. Technical report, SRI International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="25434" citStr="Marcus et al., 1993" startWordPosition="4279" endWordPosition="4282">rain a final parsing model. Their per-word accuracy rose from a 68.5% baseline to 94.6% for the newly trained model. In this experiment, we examine how close we can get to those results by using Chart Inference to learn WH-question words from the unlabelled question corpus. If successful, this would eliminate the human-annotation step for domain adaptation of the kind investigated by (Clark et al., 2004). 5.1 Corpora We trained the initial parser on the CCGBank (Hockenmaier and Steedman, 2007; Hockenmaier, 2003) training set (§02-21), consisting of 39603 sentences of Wall Street Journal text (Marcus et al., 1993). It is important to note that this training corpus contains only 93 questions in total, so it is not surprising that several category types for question words are entirely unrepresented. It also reinRaw sentence Original lexicon Parsed sentence Hypothesis lexicon Parser Category Set Partial Parse Learner 1252 P Top One F P Top Ten F Time (m) R R POS 64.91 64.91 64.91 92.55 92.55 92.55 1 BF 80.53 65.84 72.45 95.97 78.32 86.25 1740 RB 39.77 37.92 38.82 68.46 65.28 66.83 12 CI 78.63 74.16 76.33 97.03 91.52 94.20 22 Table 1: Exp. II: Category match results for the three systems on the McGuffey co</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19:313–330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL-HLT ’06,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="27542" citStr="McClosky et al (2006)" startWordPosition="4637" endWordPosition="4640">(S[dcl]\NP),8 and S[wq]/(S[q]/NP)/N.9 For this experiment we focus on the subject WH-element extraction category (S[wq]/(S[dcl]\NP))/N, as in Which cat is the grandmother?. This particular category was chosen as a point of investigation because it is OOL in CCGBank and is common enough to meaningfully evaluate. 5.2 Methods The baseline is the original StatCCG parser and lexicon. We also employ self-training (Charniak, 1997), in which a parser is used to parse a set of sentences, and then retrained using those output trees. Selftraining has had very little success in CCG applications hitherto. McClosky et al (2006) attribute success in self-training to a confluence of circum7Object question category as in What is the Keystone State? 8Subject question category as in What lays blue eggs? 9Object WH-element extraction category as in What continent is Scotland in? stances particular to their learning setting, which has the benefit of a discriminative re-ranker, both in the parsing case and in the learning case (McClosky et al., 2008). We follow their recommendations that the best performance is achieved when all the training sentences are parsed at once, rather than incrementally. We evaluate the success of</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of NAACL-HLT ’06, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>When is self-training effective for parsing?</title>
<date>2008</date>
<booktitle>In Proceedings of COLING ’08,</booktitle>
<pages>561--568</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="27965" citStr="McClosky et al., 2008" startWordPosition="4705" endWordPosition="4708">1997), in which a parser is used to parse a set of sentences, and then retrained using those output trees. Selftraining has had very little success in CCG applications hitherto. McClosky et al (2006) attribute success in self-training to a confluence of circum7Object question category as in What is the Keystone State? 8Subject question category as in What lays blue eggs? 9Object WH-element extraction category as in What continent is Scotland in? stances particular to their learning setting, which has the benefit of a discriminative re-ranker, both in the parsing case and in the learning case (McClosky et al., 2008). We follow their recommendations that the best performance is achieved when all the training sentences are parsed at once, rather than incrementally. We evaluate the success of CI in bootstrapping Wh-question categories from the out-of-domain corpus in two ways. First, we compare the CI output to the gold standard categories labelled in Rimmell and Clark (2008a). Second, we add the parsed questions into the training set, then retrain and finally retest the parser. The parser was initially trained on CCGBank §02- 21 with a word frequency threshold of 5.10 It produces partial parse charts in th</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2008</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2008. When is self-training effective for parsing? In Proceedings of COLING ’08, pages 561–568, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris S Mellish</author>
</authors>
<title>Some chart based techniques for parsing ill-formed input.</title>
<date>1989</date>
<booktitle>In Proceedings of the ACL ’89,</booktitle>
<pages>102--109</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10310" citStr="Mellish (1989)" startWordPosition="1713" endWordPosition="1714">et of partial parse conditions, and further to that, they do not allow for partial parses to be reanalysed within the learning framework. To that end, we have developed a learning algorithm that is capable of operating within the oneunknown-word-per-sentence learning setting established by the two baseline systems, that is able to invent new category types, and that is able to take advantage of the full generality of CCG. This section shows that it performs as well as the previous two systems on a toy corpus, and the next section proves that it more readily scales to natural language domains. Mellish (1989) established a two-stage bidirectional chart parser for diagnosing errors in input text. His method relied heavily on heuristic rules, and the only evaluation he did was on number of cycles needed for each type of error, and number of solutions produced. His method was designed for use in producing parses where the original parser failed, dealing with omissions, insertions, and misspelled/unknown words. The only method used to rank the possible solutions was heuristic scores. Kato (1994) implemented a revised system that used a generalised top-down parser, rather than a chart, and was able to </context>
</contexts>
<marker>Mellish, 1989</marker>
<rawString>Chris S. Mellish. 1989. Some chart based techniques for parsing ill-formed input. In Proceedings of the ACL ’89, pages 102–109, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Adapting a lexicalized-grammar parser to contrasting domains.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP ’08,</booktitle>
<pages>475--484</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="26618" citStr="Rimell and Clark (2008" startWordPosition="4494" endWordPosition="4497">r the three systems on the McGuffey corpus, training and testing on MG1. P Top One F P Top Ten F R R BF 70.83 53.13 60.72 83.33 62.50 71.43 RB 16.13 15.63 15.87 29.03 28.13 28.57 CI 61.90 40.63 49.06 76.19 50.00 60.38 Table 2: Exp. III: Category match results for the three systems on the McGuffey corpus, training on MG1 and testing on MG2. Common categories (N, NP, N/N) are overrepresented in the test data, leading to higher BF scores. POS tags not available for MG2, so no POS baseline is reported. forces the fact that this is a domain-adaptation task. We use the same 500-sentence test set as Rimell and Clark (2008b). The test corpus consists of 488 questions, each starting with What, When, How, Who or Where. The learning corpus contains 1328 questions in a similar distribution. Only three out of the five categories needed to parse What-questions are present in the CCGBank seed lexicon: S[wq]/(S[q]/NP),7, S[wq]/(S[dcl]\NP),8 and S[wq]/(S[q]/NP)/N.9 For this experiment we focus on the subject WH-element extraction category (S[wq]/(S[dcl]\NP))/N, as in Which cat is the grandmother?. This particular category was chosen as a point of investigation because it is OOL in CCGBank and is common enough to meaning</context>
<context position="28841" citStr="Rimell and Clark, 2008" startWordPosition="4851" endWordPosition="4854">two ways. First, we compare the CI output to the gold standard categories labelled in Rimmell and Clark (2008a). Second, we add the parsed questions into the training set, then retrain and finally retest the parser. The parser was initially trained on CCGBank §02- 21 with a word frequency threshold of 5.10 It produces partial parse charts in the cases where all words in the sentence are in-lexicon, except for the WH-word target, for which the learner attempts to return a category motivated by that context. We run the learner on the set of 149 sentences from the TREC Question-Answering corpus (Rimell and Clark, 2008b) that contain the word/category pair What:(S[wq]/(S[dcl]\NP))/N. For this experiment the end-punctuation distribution derived from the training corpus is replaced with a single value: P(S[wq]|?) = 1. 10StatCCG requires a parameter to trade off between training the lexicon and the POS-backoff. 1253 BL CI CI+ST All Words 84.31 86.59 87.03 POS=WHQ 53.40 56.19 59.54 Word=What 55.87 60.83 65.42 Cat=SubjExt 7.84 52.94 58.82 Table 3: F-score over individual category matches. Bold means significantly different from the Baseline. 5.3 Results Table 3 shows the change in F-score throughout this experim</context>
</contexts>
<marker>Rimell, Clark, 2008</marker>
<rawString>Laura Rimell and Stephen Clark. 2008a. Adapting a lexicalized-grammar parser to contrasting domains. In Proceedings of EMNLP ’08, pages 475–484, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Constructing a parser evaluation scheme.</title>
<date>2008</date>
<booktitle>In COLING ’08: Proceedings of the workshop on Cross-Framework and CrossDomain Parser Evaluation,</booktitle>
<pages>44--50</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="26618" citStr="Rimell and Clark (2008" startWordPosition="4494" endWordPosition="4497">r the three systems on the McGuffey corpus, training and testing on MG1. P Top One F P Top Ten F R R BF 70.83 53.13 60.72 83.33 62.50 71.43 RB 16.13 15.63 15.87 29.03 28.13 28.57 CI 61.90 40.63 49.06 76.19 50.00 60.38 Table 2: Exp. III: Category match results for the three systems on the McGuffey corpus, training on MG1 and testing on MG2. Common categories (N, NP, N/N) are overrepresented in the test data, leading to higher BF scores. POS tags not available for MG2, so no POS baseline is reported. forces the fact that this is a domain-adaptation task. We use the same 500-sentence test set as Rimell and Clark (2008b). The test corpus consists of 488 questions, each starting with What, When, How, Who or Where. The learning corpus contains 1328 questions in a similar distribution. Only three out of the five categories needed to parse What-questions are present in the CCGBank seed lexicon: S[wq]/(S[q]/NP),7, S[wq]/(S[dcl]\NP),8 and S[wq]/(S[q]/NP)/N.9 For this experiment we focus on the subject WH-element extraction category (S[wq]/(S[dcl]\NP))/N, as in Which cat is the grandmother?. This particular category was chosen as a point of investigation because it is OOL in CCGBank and is common enough to meaning</context>
<context position="28841" citStr="Rimell and Clark, 2008" startWordPosition="4851" endWordPosition="4854">two ways. First, we compare the CI output to the gold standard categories labelled in Rimmell and Clark (2008a). Second, we add the parsed questions into the training set, then retrain and finally retest the parser. The parser was initially trained on CCGBank §02- 21 with a word frequency threshold of 5.10 It produces partial parse charts in the cases where all words in the sentence are in-lexicon, except for the WH-word target, for which the learner attempts to return a category motivated by that context. We run the learner on the set of 149 sentences from the TREC Question-Answering corpus (Rimell and Clark, 2008b) that contain the word/category pair What:(S[wq]/(S[dcl]\NP))/N. For this experiment the end-punctuation distribution derived from the training corpus is replaced with a single value: P(S[wq]|?) = 1. 10StatCCG requires a parameter to trade off between training the lexicon and the POS-backoff. 1253 BL CI CI+ST All Words 84.31 86.59 87.03 POS=WHQ 53.40 56.19 59.54 Word=What 55.87 60.83 65.42 Cat=SubjExt 7.84 52.94 58.82 Table 3: F-score over individual category matches. Bold means significantly different from the Baseline. 5.3 Results Table 3 shows the change in F-score throughout this experim</context>
</contexts>
<marker>Rimell, Clark, 2008</marker>
<rawString>Laura Rimell and Stephen Clark. 2008b. Constructing a parser evaluation scheme. In COLING ’08: Proceedings of the workshop on Cross-Framework and CrossDomain Parser Evaluation, pages 44–50, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="2726" citStr="Steedman, 2000" startWordPosition="446" endWordPosition="447"> which is 89.3% accurate. Under these conditions, we predict a further 631 word/category pairs to be tagged correctly by the parser, reducing the error rate from 7.4% to 6% on §00. Further to reducing parsing error, a robust method for learning words from unlabelled data would result in the recovery of interesting and important category types that are missing from our standard lexical resources. This paper introduces Chart Inference (CI) as a strategy for deducing a ranked set of possible categories for an unknown word using the partial chart formed from the known words that surround it. CCG (Steedman, 2000) is particularly suited to this problem, because category types can be inferred from the types of the surrounding constituents. CI is designed to take advantage of this property of generative CCGBank-trained parser, and of access to the full inventory of CCG combinators and noncombinatory unary rules from the trained model. It is capable of learning category types that are completely missing from the lexicon, and is superior to existing learning systems in both precision and efficiency. Four experiments are discussed in this paper. The first compares three word-learning methods for their abili</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Watkinson</author>
<author>Suresh Manandhar</author>
</authors>
<title>Unsupervised lexical learning of categorial grammars.</title>
<date>1999</date>
<booktitle>In ACL’99: Workshop in Unsupervised Learning in Natural Language Processing.</booktitle>
<contexts>
<context position="4830" citStr="Watkinson and Manandhar (1999" startWordPosition="791" endWordPosition="794">vised, it has access to an essentially unlimited amount of unlabelled data, and it can afford to skip any sentence that does not conform to the one-unseen-word restriction. Attempting two or more OOL words at a time from one sentence would compound the search space and the error rate. We do not address the much harder problem of hypothesising missing categories for known words, which should presumably be handled by quite other methods, such as prior offline generalization of the lexicon. 2.1 A Brute-force System One of the early lexical acquisition systems using Categorial Grammar was that of Watkinson and Manandhar (1999; 2000; 2001a; 2001b). This system attempted to simultaneously learn a CG lexicon and annotate unlabelled text with parse derivations. Using a stripped-down parser that only utilised the forward- and backward-application rules, they iteratively learned the lexicon from the feedback from online parsing. The system decided which parse was best based on the lexicon, and then decided which additions to the lexicon to make based on principles of compression. After each change, the system reexamined the parses for previous sentences and updated them to reflect the new lexicon. They report fully conv</context>
<context position="16647" citStr="Watkinson and Manandhar (1999)" startWordPosition="2788" endWordPosition="2791"> systems are to changes in theoriginal seed lexicon. 3.1 Corpus For this experiment we test the three systems on a reconstructed version of Corpus 1 from Watkinson and Manandhar’s experiments.5 The lexicon contains 40 word-category pairs, including the full stop (S\S), which was not in Watkinson’s experiment, and one example of noun-verb ambiguity (saw). The test sentences are randomly generated from a simple PCFG over the lexicon, and are always presented to the learners in the same order. 3.2 Methods In order to directly compare the three learning methods, we use the evaluation setting from Watkinson and Manandhar (1999), which consists of a 40-entry target lexicon and a PCFG language model used to randomly generate 1000 sentences. We then specify a seed lexicon and run the learner incrementally, so that it deals with one sentence at a time, then feeds the learned material back into the lexicon. Watkinson’s system was shown to fully convergent (they defined convergence as cosine similarity between the seed lexicon (S) and the target lexicon (T) exceeding 0.99), whenever the seed lexicon contained at least one instance of each of the category types in the target lexicon (Watkinson and Manandhar, 1999) 5The ful</context>
</contexts>
<marker>Watkinson, Manandhar, 1999</marker>
<rawString>Stephen Watkinson and Suresh Manandhar. 1999. Unsupervised lexical learning of categorial grammars. In ACL’99: Workshop in Unsupervised Learning in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Watkinson</author>
<author>Suresh Manandhar</author>
</authors>
<title>Unsupervised lexical learning with categorial grammars using the LLL corpus.</title>
<date>2000</date>
<booktitle>In James Cussens and Savso Dvzeroski, editors, Learning Language in Logic, volume 1925 of Lecture Notes in Arti�cial Intelligence.</booktitle>
<publisher>Springer.</publisher>
<marker>Watkinson, Manandhar, 2000</marker>
<rawString>Stephen Watkinson and Suresh Manandhar. 2000. Unsupervised lexical learning with categorial grammars using the LLL corpus. In James Cussens and Savso Dvzeroski, editors, Learning Language in Logic, volume 1925 of Lecture Notes in Arti�cial Intelligence. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Watkinson</author>
<author>Suresh Manandhar</author>
</authors>
<title>Acquisition of large scale categorial grammar lexicons.</title>
<date>2001</date>
<booktitle>In Proceedings of PACLING ’01.</booktitle>
<marker>Watkinson, Manandhar, 2001</marker>
<rawString>Stephen Watkinson and Suresh Manandhar. 2001a. Acquisition of large scale categorial grammar lexicons. In Proceedings of PACLING ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Watkinson</author>
<author>Suresh Manandhar</author>
</authors>
<title>A psychologically plausible and computationally effective approach to learning syntax.</title>
<date>2001</date>
<booktitle>In Walter Daelemans and R’emi Zajac, editors, Proceedings of CoNLL ’01),</booktitle>
<pages>160--167</pages>
<marker>Watkinson, Manandhar, 2001</marker>
<rawString>Stephen Watkinson and Suresh Manandhar. 2001b. A psychologically plausible and computationally effective approach to learning syntax. In Walter Daelemans and R’emi Zajac, editors, Proceedings of CoNLL ’01), pages 160 – 167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Jason Baldridge</author>
</authors>
<title>Adapting chart realization to CCG.</title>
<date>2003</date>
<booktitle>In Proceedings of the 9th European Workshop on Natural Language Generation,</booktitle>
<pages>119--126</pages>
<contexts>
<context position="11680" citStr="White and Baldridge, 2003" startWordPosition="1936" endWordPosition="1939">agnosed the errors correctly, or whether the solution they offered was accurate. They also had to deal with cases where the error was ambiguous, for example, where an inserted word could be interpreted as a misspelling or vice-versa. Where Mellish uses the two-stage parsing process to complete malformed parses, we use it to diagnose unknown lexical items. In addition, we work on the scale of a full grammar and wide-coverage parser, using modern lexical corpora. Our method is a wrapper for a naive generative CCG parser StatOpenCCG (Christodoulopoulos, 2008), a statistical extension to OpenCCG (White and Baldridge, 2003). In the general case, the parser is trained on all the labelled data available in a particular learning setting, then the learner discovers new lexical items from unlabelled text. Like the brute force and rule-based systems, it is vulnerable then else 1248 CCG Combinator Inverse Combinator B A X = A/B if v(B) &lt; 1 X A X = B A\B A X = B X A X = A\B if v(B) &lt; 1 C/B A/B X = A/C X A/B X = C/B A\C A\B X = C\B X A\B X = A\C Figure 2: Derivation of inverse combinators A/B B A (&gt;) X A/B B A\B A (&lt;) X B A/C C/B A/B (&gt;B) X A/C C\B A\C A\B (&lt;B) X C\B � �P(HeadRight|R) P(target = C|R,S) = max P(HeadLeft|R</context>
</contexts>
<marker>White, Baldridge, 2003</marker>
<rawString>Michael White and Jason Baldridge. 2003. Adapting chart realization to CCG. In Proceedings of the 9th European Workshop on Natural Language Generation, pages 119–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Jianqiang Ma</author>
<author>Sergio Duarte</author>
<author>Cagri Coltekin</author>
</authors>
<title>An inference-rules based categorial grammar learner for simulating language acquisition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th Annual Belgian-Dutch Conference on Machine Learning,</booktitle>
<location>Tillburg.</location>
<contexts>
<context position="6745" citStr="Yao et al. (2009" startWordPosition="1102" endWordPosition="1105">ecification of any particular patterns, only knowledge of the grammar formalism. For this paper, as a baseline, we implement a generalised version of Watkinson and Manandhar’s mechanism for determining the category γ of a single OOL word in a sentence where the rest of the words C1...CN are in the lexicon: γ = argmaxParse(C1...Cn,γ). This is equivalent to backing off to the set of all known category types; the learner returns the category that maximises the probability of the completed parse tree. We ignore the optimisation and compression steps of the original system. 2.2 A Rule-based System Yao et al. (2009a; 2009b) developed a learning system based on handwritten translation rules for deducing the category (X) of a single unknown word in a sentence consisting of a sequence of partiallyparsed constituents (A..N). Their system was based on a small inventory of inference rules that eliminated ambiguity in the ordering of arguments. For example, one of the Level 3 inference rules specifies the order of the arguments in the deduced category: A X B C → D ⇒ X = ((D\A)/C)/B Without this inductive bias the learner would have to deal with the ambiguity of the options ((D/C)/B)\A and ((D/C)\A)/B at minimu</context>
</contexts>
<marker>Yao, Ma, Duarte, Coltekin, 2009</marker>
<rawString>Xuchen Yao, Jianqiang Ma, Sergio Duarte, and Cagri Coltekin. 2009a. An inference-rules based categorial grammar learner for simulating language acquisition. In Proceedings of the 18th Annual Belgian-Dutch Conference on Machine Learning, Tillburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Jianqiang Ma</author>
<author>Sergio Duarte</author>
<author>Cagri Coltekin</author>
</authors>
<title>Unsupervised syntax learning with categorial grammars using inference rules.</title>
<date>2009</date>
<booktitle>In Proceedings of The 14th Student Session of the European Summer School for Logic, Language, and Information,</booktitle>
<location>Bordeaux.</location>
<contexts>
<context position="6745" citStr="Yao et al. (2009" startWordPosition="1102" endWordPosition="1105">ecification of any particular patterns, only knowledge of the grammar formalism. For this paper, as a baseline, we implement a generalised version of Watkinson and Manandhar’s mechanism for determining the category γ of a single OOL word in a sentence where the rest of the words C1...CN are in the lexicon: γ = argmaxParse(C1...Cn,γ). This is equivalent to backing off to the set of all known category types; the learner returns the category that maximises the probability of the completed parse tree. We ignore the optimisation and compression steps of the original system. 2.2 A Rule-based System Yao et al. (2009a; 2009b) developed a learning system based on handwritten translation rules for deducing the category (X) of a single unknown word in a sentence consisting of a sequence of partiallyparsed constituents (A..N). Their system was based on a small inventory of inference rules that eliminated ambiguity in the ordering of arguments. For example, one of the Level 3 inference rules specifies the order of the arguments in the deduced category: A X B C → D ⇒ X = ((D\A)/C)/B Without this inductive bias the learner would have to deal with the ambiguity of the options ((D/C)/B)\A and ((D/C)\A)/B at minimu</context>
</contexts>
<marker>Yao, Ma, Duarte, Coltekin, 2009</marker>
<rawString>Xuchen Yao, Jianqiang Ma, Sergio Duarte, and Cagri Coltekin. 2009b. Unsupervised syntax learning with categorial grammars using inference rules. In Proceedings of The 14th Student Session of the European Summer School for Logic, Language, and Information, Bordeaux.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time n3.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="13564" citStr="Younger, 1967" startWordPosition="2282" endWordPosition="2283">orpus. First the baseline bottom-up parser is called upon to produce a partial parse chart. The learner takes this partial chart and fills the top right cell with a distribution for the result category based on the end punctuation.2 Using this partial chart that contains at least one entry for every leaf cell (except the one OOL target cell) and at least one entry for the result, the 2For simple corpora, only S is required, but realistic corpora necessitate a distribution over all result types, including noun phrases and fragments. learner steps through the chart in a top-down version of CYK (Younger, 1967). For the top-down process, the standard combinators have to be reformulated to take an argument and a result as inputs, rather than two arguments as in the standard bottomup case. In addition, the learner has access to the non-combinator rules from the parse model, which have been similarly inverted for top-down use. This process continues until the target cell has been filled, and the ranked set of categories is returned. The probability that the target has a given category is calculated as the greater of the right- or left-headed derivations, according to Figure 3. At training time, the Sta</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Daniel H. Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10(2):189–208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>