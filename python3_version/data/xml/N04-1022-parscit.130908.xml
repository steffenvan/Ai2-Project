<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005190">
<title confidence="0.995249">
Minimum Bayes-Risk Decoding for Statistical Machine Translation
</title>
<author confidence="0.964856">
Shankar Kumar and William Byrne
</author>
<affiliation confidence="0.789587">
Center for Language and Speech Processing, Johns Hopkins University,
3400 North Charles Street, Baltimore, MD, 21218, USA
</affiliation>
<email confidence="0.997044">
skumar,byrne @jhu.edu
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945733333334">
We present Minimum Bayes-Risk (MBR) de-
coding for statistical machine translation. This
statistical approach aims to minimize expected
loss of translation errors under loss functions
that measure translation performance. We de-
scribe a hierarchy of loss functions that incor-
porate different levels of linguistic information
from word strings, word-to-word alignments
from an MT system, and syntactic structure
from parse-trees of source and target language
sentences. We report the performance of the
MBR decoders on a Chinese-to-English trans-
lation task. Our results show that MBR decod-
ing can be used to tune statistical MT perfor-
mance for specific loss functions.
</bodyText>
<sectionHeader confidence="0.999121" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997891583333333">
Statistical Machine Translation systems have achieved
considerable progress in recent years as seen from their
performance on international competitions in standard
evaluation tasks (NIST, 2003). This rapid progress has
been greatly facilitated by the development of automatic
translation evaluation metrics such as BLEU score (Pa-
pineni et al., 2001), NIST score (Doddington, 2002)
and Position Independent Word Error Rate (PER) (Och,
2002). However, given the many factors that influence
translation quality, it is unlikely that we will find a single
translation metric that will be able to judge all these fac-
tors. For example, the BLEU, NIST and the PER metrics,
</bodyText>
<footnote confidence="0.946619333333333">
This work was supported by the National Science Foun-
dation under Grant No. 0121285 and an ONR MURI Grant
N00014-01-1-0685. Any opinions, findings, and conclusions or
recommendations expressed in this material are those of the au-
thors and do not necessarily reflect the views of the National
Science Foundation or the Office of Naval Research.
</footnote>
<bodyText confidence="0.999829476190476">
though effective, do not take into account explicit syntac-
tic information when measuring translation quality.
Given that different Machine Translation (MT) eval-
uation metrics are useful for capturing different aspects
of translation quality, it becomes desirable to create MT
systems tuned with respect to each individual criterion. In
contrast, the maximum likelihood techniques that under-
lie the decision processes of most current MT systems do
not take into account these application specific goals. We
apply the Minimum Bayes-Risk (MBR) techniques devel-
oped for automatic speech recognition (Goel and Byrne,
2000) and bitext word alignment for statistical MT (Ku-
mar and Byrne, 2002), to the problem of building au-
tomatic MT systems tuned for specific metrics. This is
a framework that can be used with statistical models of
speech and language to develop decision processes opti-
mized for specific loss functions.
We will show that MBR decoding can be applied to
machine translation in two scenarios. Given an automatic
MT metric, we design a loss function based on the met-
ric and use MBR decoding to tune MT performance un-
der the metric. We also show how MBR decoding can
be used to incorporate syntactic structure into a statistical
MT system by building specialized loss functions. These
loss functions can use information from word strings,
word-to-word alignments and parse-trees of the source
sentence and its translation. In particular we describe
the design of a Bilingual Tree Loss Function that can ex-
plicitly use syntactic structure for measuring translation
quality. MBR decoding under this loss function allows
us to integrate syntactic knowledge into a statistical MT
system without building detailed models of linguistic fea-
tures, and retraining the system from scratch.
We first present a hierarchy of loss functions for trans-
lation based on different levels of lexical and syntactic
information from source and target language sentences.
This hierarchy includes the loss functions useful in both
situations where we intend to apply MBR decoding. We
then present the MBR framework for statistical machine
translation under the various translation loss functions.
We finally report the performance of MBR decoders op-
timized for each loss function.
</bodyText>
<sectionHeader confidence="0.940211" genericHeader="method">
2 Translation Loss Functions
</sectionHeader>
<bodyText confidence="0.999924764705883">
We now introduce translation loss functions to measure
the quality of automatically generated translations. Sup-
pose we have a sentence in a source language for
which we have generated an automatic translation
with word-to-word alignment relative to. The word-
to-word alignment specifies the words in the source
sentence that are aligned to each word in the transla-
tion . We wish to compare this automatic translation
with a reference translation with word-to-word align-
ment relative to.
We will now present a three-tier hierarchy of trans-
lation loss functions of the form
that measure against . These loss func-
tions will make use of different levels of information from
word strings, MT alignments and syntactic structure from
parse-trees of both the source and target strings as illus-
trated in the following table.
</bodyText>
<table confidence="0.6191695">
Loss Function Functional Form
Lexical
Target Language Parse-Tree
Bilingual Parse-Tree
</table>
<bodyText confidence="0.999944">
We start with an example of two competing English
translations for a Chinese sentence (in Pinyin without
tones), with their word-to-word alignments in Figure 1.
The reference translation for the Chinese sentence with
its word-to-word alignment is shown in Figure 2. In this
section, we will show the computation of different loss
functions for this example.
</bodyText>
<subsectionHeader confidence="0.986792">
2.1 Lexical Loss Functions
</subsectionHeader>
<bodyText confidence="0.990920676470588">
The first class of loss functions uses no informa-
tion about word alignments or parse-trees, so that
can be reduced to . We
consider three loss functions in this category: The BLEU
score (Papineni et al., 2001), word-error rate, and the
position-independent word-error rate (Och, 2002). An-
other example of a loss function in this class is the MT-
eval metric introduced in Melamed et al. (2003). A loss
function of this type depends only on information from
word strings.
BLEU score (Papineni et al., 2001) computes the
geometric mean of the precision of-grams of vari-
ous lengths ( ) between a hypothesis and
a reference translation, and includes a brevity penalty
( ) if the hypothesis is shorter than the refer-
ence. We use .
where is the precision of-grams in the hy-
pothesis . The BLEU score is zero if any of the n-gram
precisions is zero for that sentence pair. We
note that . We derive a loss
function from BLEU score as
BLEU .
Word Error Rate (WER) is the ratio of the string-edit
distance between the reference and the hypothesis word
strings to the number of words in the reference. String-
edit distance is measured as the minimum number of edit
operations needed to transform a word string to the other
word string.
Position-independent Word Error Rate (PER) mea-
sures the minimum number of edit operations needed
to transform a word string to any permutation of the
other word string. The PER score (Och, 2002) is then
computed as a ratio of this distance to the number of
words in the reference word string.
</bodyText>
<subsectionHeader confidence="0.999352">
2.2 Target Language Parse-Tree Loss Functions
</subsectionHeader>
<bodyText confidence="0.999910470588235">
The second class of translation loss functions uses infor-
mation only from the parse-trees of the two translations,
so that . This loss
function has no access to any information from the source
sentence or the word alignments.
Examples of such loss functions are tree-edit distances
between parse-trees, string-edit distances between event
representation of parse-trees (Tang et al., 2002), and tree-
kernels (Collins and Duffy, 2002). The computation of
tree-edit distance involves an unconstrained alignment of
the two English parse-trees. We can simplify this prob-
lem once we have a third parse tree (for the Chinese sen-
tence) with node-to-node alignment relative to the two
English trees. We will introduce such a loss function in
the next section. We did not perform experiments involv-
ing this class of loss functions, but mention them for com-
pleteness in the hierarchy of loss functions.
</bodyText>
<subsectionHeader confidence="0.999361">
2.3 Bilingual Parse-Tree Loss Functions
</subsectionHeader>
<bodyText confidence="0.986666833333333">
The third class of loss functions uses information from
word strings, alignments and parse-trees in both lan-
guages, and can be described by
.
We will now describe one such loss function using the
example in Figures 1 and 2. Figure 3 shows a tree-
to-tree mapping between the source (Chinese) parse-tree
and parse-trees of its reference translation and two com-
peting hypothesis (English) translations.
the first two months of this year guangdong ’s high−tech products 3.76 billion US dollars
jin−nian qian liangyue guangdong gao xinjishu chanpin chukou sanqidianliuyi meiyuan
the first two months of this year guangdong exported high−tech products 3.76 billion US dollars
</bodyText>
<figureCaption confidence="0.999402">
Figure 1: Two competing English translations for a Chinese sentence with their word-to-word alignments.
</figureCaption>
<figure confidence="0.979936666666667">
E export of high−tech products in guangdong in first two months this year reached 3.76 billion US dollars
A
F jin−nian qian liangyue guangdong gao xinjishu chanpin chukou sanqidianliuyi meiyuan
</figure>
<figureCaption confidence="0.9252235">
Figure 2: The reference translation for the Chinese sentence from Figure 1 with its word-to-word alignments. Words in
the Chinese (English) sentence shown as unaligned are aligned to the NULL word in the English (Chinese) sentence.
</figureCaption>
<equation confidence="0.467253166666667">
E1
A1
F
2
A
E 2
</equation>
<bodyText confidence="0.9982885">
We first assume that a nodein the source tree can
be mapped to a node in (and a node in ) using
word alignment (and respectively). We denote the
subtree of rooted at node by and the subtree of
rooted at node by. We will now describe a
simple procedure that makes use of the word alignment
to construct node-to-node alignment between nodes in
the source tree and the target tree.
</bodyText>
<subsectionHeader confidence="0.996801">
2.3.1 Alignment of Parse-Trees
</subsectionHeader>
<bodyText confidence="0.996213">
For each nodein the source tree we consider the
subtreerooted at. We first read off the source word
sequence corresponding to the leaves of. We next con-
sider the subset of words in the target sentence that are
aligned to any word in this source word sequence, and
select the leftmost and rightmost words from this sub-
set. We locate the leaf nodes corresponding to these two
words in the target parse tree, and obtain their closest
common ancestor node . This procedure gives us
a mapping from a node to a node and this
mapping associates one subtree to one subtree
.
</bodyText>
<subsectionHeader confidence="0.966712">
2.3.2 Loss Computation between Aligned
Parse-Trees
</subsectionHeader>
<bodyText confidence="0.997413666666667">
Given the subtree alignment between and, and
and, we first identify the subset of nodes in for
which we can identify a corresponding node in both
and .
The Bilingual Parse-Tree (BiTree) Loss Function can
then be computed as
</bodyText>
<subsectionHeader confidence="0.702792">
BiTreeLoss
</subsectionHeader>
<bodyText confidence="0.99976062962963">
where is a distance measure between sub-trees
and. Specific Bi-tree loss functions are determined
through particular choices of. In our experiments, we
used a 0/1 loss function between sub-treesand.
otherwise
We note that other tree-to-tree distance measures can
also be used to compute, e.g. the distance function
could compare if the subtrees andhave the same
headword/non-terminal tag.
The Bitree loss function measures the distance be-
tween two trees in terms of distances between their cor-
responding subtrees. In this way, we replace the string-
to-string (Levenshtein) alignments (for WER) or-gram
matches (for BLEU/PER) with subtree-to-subtree align-
ments.
The Bitree Error Rate (in %) is computed as a ratio of
the Bi-tree Loss function to the number of nodes in the
set .
The complete node-to-node alignment between the
parse-tree of the source (Chinese) sentence and the parse
trees of its reference translation and the two hypothesis
translations (English) is given in Table 1. Each row in
this table shows the alignment between a node in the Chi-
nese parse-tree and nodes in the reference and the two hy-
pothesis parse-trees. The computation of the Bitree Loss
function and the Bitree Error Rate is presented in the last
two rows of the table.
</bodyText>
<figure confidence="0.999769493589743">
:Reference Translation (English) S
VP1
NP1
NP2
PP1
PP2
VBD1
NP9
reached
IN1
NP3
IN3
NP6
CD2
CD3
NNP2
NNS3
NN1
export
of
in
billion
dollars
NP4
PP2
NP7
NP8
3.76
US
JJ2
CD1
NNS2
DT1
NN2
JJ1
NNS1
IN2
NP5
high-tech
products
in
first
months
this
NNP1
guangdong
two
year
: Source Sentence(Chinese) VP1
VP2
LCP1
NP1
LC1
VV1
NP2
QP1
qian
liangyue
CLP1
M1
CD1
sanqidianliuyi
NT1
jin-nian
NP3
ADJP1
NP3
NN1
NN2
JJ1
NR2
NN3
meiyuan
chanpin
xinjishu
guangdong
chukou
gao
S
NP1
VP1
NP2
PP1
VBD1
NP4
NP5
NP1
NP2
NP4
NP5
PP1
DT1
CD1
NNS1
JJ1
IN1
NP3
QP1
NNP2
NNS3
the
first
months
of
DT2
NN1
year
NP4
JJ2
NNS2
CD2
CD3
US
dollars
two
exported
DT1
JJ1
CD1
NNS1
IN1
NP3
JJ2
NNS2
CD1
CD2
NNP2
NNS3
this
NNP1
POS1
high-tech
products
3.76
billion
the
first
months
of
high-tech
products
billion
dollars
DT2
NN1
NNP1
3.76
US
two
Guangdong
’s
: Hypothesis Translation 2 (English)
this
year
Guangdong
: Hypothesis Translation 1 (English)
</figure>
<figureCaption confidence="0.98512175">
Figure 3: An example showing a parse-tree fora Chinese sentence and parse-trees for its reference translation and
two competing hypothesis translations. We show a sample alignment for one of the nodes in the Chinese tree with its
corresponding nodes in the three English trees. The complete node-to-node alignment between the parse-trees of the
Chinese sentence and the three English sentences is given in Table 1.
</figureCaption>
<table confidence="0.999968689655172">
Node Node Node Node
VP1 S S 1 NP1 1
LCP NP6 NP1 1 NP1 1
NP1 NP8 NP3 1 NP3 1
NT1 NP8 NP3 1 NP3 1
jin-nian NP8 NP3 1 NP3 1
LC1 first NP1 1 NP2 1
qian first NP1 1 NP2 1
VP2 S S 1 NP1 1
VV NP7 NP2 1 NP2 1
liangyue NP7 NP2 1 NP2 1
NP2 S S 1 NP1 1
NP3 Guangdong Guangdong 0 NP4 1
NR2 Guangdong Guangdong 0 NP4 1
guangdong Guangdong Guangdong 0 NP4 1
ADJP1 reached high-tech 1 high-tech 1
JJ1 reached high-tech 1 high-tech 1
gao reached high-tech 1 high-tech 1
NP3 NP1 VP1 1 NP3 1
NN2 products products 0 products 0
chanpin products products 0 products 0
NN3 export exported 1 products 1
chukou export exported 1 products 1
QP1 NP9 NP5 0 NP1 1
CLP1 NP9 NP5 0 NP1 1
M1 NP9 NP5 0 NP1 1
meiyuan NP9 NP5 0 NP1 1
BiTree Loss Loss 17 Loss 24
BiTree Error Rate (%) 17/26 = 65.4 24/26 = 92.3
</table>
<tableCaption confidence="0.778823333333333">
Table 1: Bi-Tree Loss Computation for the parse-trees shown in Figure 3. Each row shows a mapping between a node
in the parse-tree of the Chinese sentence and the nodes in parse-trees of its reference translation, hypothesis translation
1 and hypothesis translation 2.
</tableCaption>
<subsectionHeader confidence="0.999321">
2.4 Comparison of Loss Functions
</subsectionHeader>
<bodyText confidence="0.999611095238096">
In Table 2 we compare various translation loss functions
for the example from Figure 1. The two hypothesis trans-
lations are very similar at the word level and therefore the
BLEU score, PER and the WER are identical. However
we observe that the sentences differ substantially in their
syntactic structure (as seen from Parse-Trees in Figure 3),
and to a lesser extent in their word-to-word alignments
(Figure 1) to the source sentence. The first hypothesis
translation is parsed as a sentence while
the second translation is parsed as a noun phrase. The Bi-
tree loss function which depends both on the parse-trees
and the word-to-word alignments, is therefore very differ-
ent for the two translations (Table 2). While string based
metrics such as BLEU, WER and PER are insensitive to
the syntactic structure of the translations, BiTree Loss is
able to measure this aspect of translation quality, and as-
signs different scores to the two translations.
We provide this example to show how a loss function
which makes use of syntactic structure from source and
target parse trees, can capture properties of translations
that string based loss functions are unable to measure.
</bodyText>
<table confidence="0.997396">
Loss Functions
BLEU (%) 26.4 26.4
WER (%) 70.6 70.6
PER (%) 23.5 23.5
BiTree Error Rate (%) 65.4 92.3
</table>
<tableCaption confidence="0.9895975">
Table 2: Comparison of the different loss functions for
hypothesis and reference translations from Figures 1, 2.
</tableCaption>
<sectionHeader confidence="0.982052" genericHeader="method">
3 Minimum Bayes-Risk Decoding
</sectionHeader>
<bodyText confidence="0.999791454545455">
Statistical Machine Translation (Brown et al., 1990) can
be formulated as a mapping of a word sequence in a
source language to word sequence in the target lan-
guage that has a word-to-word alignment relative to.
Given the source sentence, the MT decoder pro-
duces a target word string with word-to-word align-
ment . Relative to a reference translation with word
alignment, the decoder performance is measured as
. Our goal is to find the decoder that has
the best performance over all translations. This is mea-
sured through Bayes-Risk :
The expectation is taken under the true distribution
that describes translations of human quality.
Given a loss function and a distribution, it is well
known that the decision rule that minimizes the Bayes-
Risk is given by (Bickel and Doksum, 1977; Goel and
Byrne, 2000):
We shall refer to the decoder given by this equation
as the Minimum Bayes-Risk (MBR) decoder. The MBR
decoder can be thought of as selecting a consensus trans-
lation: For each sentence, Equation 3 selects the trans-
lation that is closest on an average to all the likely trans-
lations and alignments. The closeness is measured under
the loss function of interest.
This optimal decoder has the difficulties of search
(minimization) and computing the expectation under the
true distribution. In practice, we will consider the space
of translations to be an -best list of translation alterna-
tives generated under a baseline translation model. Of
course, we do not have access to the true distribution
over translations. We therefore use statistical transla-
tion models (Och, 2002) to approximate the distribution
.
</bodyText>
<subsectionHeader confidence="0.367156">
Decoder Implementation: The MBR decoder (Equa-
</subsectionHeader>
<bodyText confidence="0.994926052631579">
tion 3) on the -best List is implemented as
and . This is a rescoring procedure that
searches for consensus under a given loss function. The
posterior probability of each hypothesis in the -best list
is derived from the joint probability assigned by the base-
line translation model.
The conventional Maximum A Posteriori (MAP) de-
coder can be derived as a special case of the MBR de-
coder by considering a loss function that assigns a equal
cost (say 1) to all misclassifications. Under the 0/1 loss
function,
the decoder of Equation 3 reduces to the MAP decoder
MAP
This illustrates why we are interested in MBR decoders
based on other loss functions: the MAP decoder is opti-
mal with respect to a loss function that is very harsh. It
does not distinguish between different types of translation
errors and good translations receive the same penalty as
poor translations.
</bodyText>
<sectionHeader confidence="0.60782" genericHeader="method">
4 Performance of MBR Decoders
</sectionHeader>
<bodyText confidence="0.999944375">
We performed our experiments on the Large-Data Track
of the NIST Chinese-to-English MT task (NIST, 2003).
The goal of this task is the translation of news stories
from Chinese to English. The test set has a total of 1791
sentences, consisting of 993 sentences from the NIST
2001 MT-eval set and 878 sentences from the NIST 2002
MT-eval set. Each Chinese sentence in this set has four
reference translations.
</bodyText>
<subsectionHeader confidence="0.988646">
4.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999888933333334">
The performance of the baseline and the MBR decoders
under the different loss functions was measured with re-
spect to the four reference translations provided for the
test set. Four evaluation metrics were used. These
were multi-reference Word Error Rate (mWER) (Och,
2002), multi-reference Position-independent word Error
Rate (mPER) (Och, 2002) , BLEU and multi-reference
BiTree Error Rate.
Among these evaluation metrics, the BLEU score
directly takes into account multiple reference transla-
tions (Papineni et al., 2001). In case of the other metrics,
we consider multiple references in the following way. For
each sentence, we compute the error rate of the hypothe-
sis translation with respect to the most similar reference
translation under the corresponding loss function.
</bodyText>
<subsectionHeader confidence="0.993669">
4.2 Decoder Performance
</subsectionHeader>
<bodyText confidence="0.999132244444444">
In our experiments, a baseline translation model (JHU,
2003), trained on a Chinese-English parallel cor-
pus (NIST, 2003) ( English words and Chi-
nese words), was used to generate 1000-best translation
hypotheses for each Chinese sentence in the test set. The
1000-best lists were then rescored using the different
translation loss functions described in Section 2.
The English sentences in the -best lists were parsed
using the Collins parser (Collins, 1999), and the Chinese
sentences were parsed using a Chinese parser provided to
us by D. Bikel (Bikel and Chiang, 2000). The English
parser was trained on the Penn Treebank and the Chinese
parser on the Penn Chinese treebank.
Under each loss function, the MBR decoding was per-
formed using Equation 3. We say we have a matched
condition when the same loss function is used in both the
error rate and the decoder design. The performance of
the MBR decoders on the NIST 2001+2002 test set is re-
ported in Table 3. For all performance metrics, we show
the 70% confidence interval with respect to the MAP
baseline computed using bootstrap resampling (Press et
al., 2002; Och, 2003). We note that this significance level
if
otherwise,
does meet the customary criteria for minimum signifi-
cance intervals of 68.3% (Press et al., 2002).
We observe in most cases that the MBR decoder under
a loss function performs the best under the correspond-
ing error metric i.e. matched conditions perform the best.
The gains from MBR decoding under matched conditions
are statistically significant in most cases. We note that the
MAP decoder is not optimal in any of the cases. In partic-
ular, the translation performance under the BLEU metric
can be improved by using MBR relative to MAP decod-
ing. This shows the value of finding decoding procedure
matched to the performance criterion of interest.
We also notice some affinity among the loss functions.
The MBR decoding under the Bitree Loss function per-
forms better under the WER relative to the MAP decoder,
but perform poorly under the BLEU metric. The MBR
decoder under WER and PER perform better than the
MAP decoder under all error metrics. The MBR decoder
under BLEU loss function obtains a similar (or worse)
performance relative to MAP decoder on all metrics other
than BLEU.
</bodyText>
<sectionHeader confidence="0.999627" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999932588235294">
We have described the formulation of Minimum Bayes-
Risk decoders for machine translation. This is a general
framework that allows us to build special purpose de-
coders from general purpose models. The procedure aims
at direct minimization of the expected risk of translation
errors under a given loss function. In this paper we have
focused on two situations where this framework could be
applied.
Given an MT evaluation metric of interest such as
BLEU, PER or WER, we can use this metric as a loss
function within the MBR framework to design decoders
optimized for the evaluation criterion. In particular, the
MBR decoding under the BLEU loss function can yield
further improvements on top of MAP decoding.
Suppose we are interested in improving syntactic struc-
ture of automatic translations and would like to use an
existing statistical MT system that is trained without any
linguistic features. We have shown in such a situation
how MBR decoding can be applied to the MT system.
This can be done by the design of translation loss func-
tions from varied linguistic analyzes. We have shown the
construction of a Bitree loss function to compare parse-
trees of any two translations using alignments with re-
spect to a parse-tree for the source sentence. The loss
function therefore avoids the problem of unconstrained
tree-to-tree alignment. Using an example, we have shown
that this loss function can measure qualities of transla-
tion that string (and ngram) based metrics cannot cap-
ture. The MBR decoder under this loss function gives
improvements under an evaluation metric based on the
loss function.
We present results under the Bitree loss function as
an example of incorporating linguistic information into
a loss function; we have not yet measured its correla-
tion with human assessments of translation quality. This
loss function allows us to integrate syntactic structure into
the statistical MT framework without building detailed
models of syntactic features and retraining models from
scratch. However, we emphasize that the MBR tech-
niques do not preclude the construction of complex mod-
els of syntactic structure. Translation models that have
been trained with linguistic features could still benefit by
the application of MBR decoding procedures.
That machine translation evaluation continues to be
an active area of research is evident from recent work-
shops (AMTA, 2003). We expect new automatic MT
evaluation metrics to emerge frequently in the future.
Given any translation metric, the MBR decoding frame-
work will allow us to optimize existing MT systems for
the new criterion. This is intended to compensate for any
mismatch between decoding strategy of MT systems and
their evaluation criteria. While we have focused on de-
veloping MBR procedures for loss functions that mea-
sure various aspects of translation quality, this frame-
work can also be used with loss functions which measure
application-specific error criteria.
We now describe related training and search proce-
dures for NLP that explicitly take into consideration task-
specific performance metrics. Och (2003) developed a
training procedure that incorporates various MT evalua-
tion criteria in the training procedure of log-linear MT
models. Foster et al. (2002) developed a text-prediction
system for translators that maximizes expected benefit to
the translator under a statistical user model. In parsing,
Goodman (1996) developed parsing algorithms that are
appropriate for specific parsing metrics. There has also
been recent work that combines 1-best hypotheses from
multiple translation systems (Bangalore et al., 2002); this
approach uses string-edit distance to align the hypotheses
and rescores the resulting lattice with a language model.
In future work we plan to extend the search space of
MBR decoders to translation lattices produced by the
baseline system. Translation lattices (Ueffing et al., 2002;
Kumar and Byrne, 2003) are a compact representation of
a large set of most likely translations generated by an MT
system. While an -best list contains only a limited re-
ordering of hypotheses, a translation lattice will contain
hypotheses with a vastly greater number of re-orderings.
We are developing efficient lattice search procedures for
MBR decoders. By extending the search space of the de-
coder to a much larger space than the -best list, we ex-
pect further performance improvements.
MBR is a promising modeling framework for statisti-
cal machine translation. It is a simple model rescoring
framework that improves well-trained statistical models
</bodyText>
<table confidence="0.998149">
Performance Metrics
Decoder BLEU (%) mWER(%) mPER (%) mBiTree Error Rate(%)
70% Confidence Intervals +/-0.3 +/-0.9 +/-0.6 +/-1.0
MAP(baseline) 31.2 64.9 41.3 69.0
MBR
BLEU 31.5 65.1 41.1 68.9
WER 31.3 64.3 40.8 68.5
PER 31.3 64.6 40.4 68.6
BiTree Loss 30.7 64.1 41.1 68.0
</table>
<tableCaption confidence="0.999521">
Table 3: Translation performance of the MBR decoder under various loss functions on the NIST 2001+2002 Test set.
</tableCaption>
<bodyText confidence="0.994010666666667">
For each metric, the performance under a matched condition is shown in bold. Note that better results correspond to
higher BLEU scores and to lower error rates.
by tuning them for particular criteria. These criteria could
come from evaluation metrics or from other desiderata
(such as syntactic well-formedness) that we wish to see
in automatic translations.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999375">
This work was performed as part of the 2003 Johns Hop-
kins Summer Workshop research group on Syntax for
Statistical Machine Translation. We would like to thank
all the group members for providing various resources
and tools and contributing to useful discussions during
the course of the workshop.
</bodyText>
<sectionHeader confidence="0.998804" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998388614285714">
AMTA. 2003. Workshop on Machine
Translation Evaluation, MT Summit IX.
www.issco.unige.ch/projects/isle/MTE-at-MTS9.html.
S. Bangalore, V. Murdock, and G. Riccardi. 2002. Boot-
strapping bilingual data using consensus translation for
a multilingual instant messaging system. In Proceed-
ings of COLING, Taipei, Taiwan.
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected topics. Holden-
Day Inc., Oakland, CA, USA.
D. Bikel and D. Chiang. 2000. Two statistical pars-
ing models applied to the chinese treebank. In Pro-
ceedings of the Second Chinese Language Processing
Workshop, pages 1–6, Hong Kong.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79–85.
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: Kernels over discrete struc-
tures, and the weighted perceptron. In Proceedings of
EMNLP, Philadelphia, PA, USA.
M. J. Collins. 1999. Head-driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ofHLT 2002, San Diego, CA. USA.
G. Foster, P. Langlais, and G. Lapalme. 2002. User-
friendly text prediction for translators. In Proc. of
EMNLP, Philadelphia, PA, USA.
V. Goel and W. Byrne. 2000. Minimum Bayes-risk auto-
matic speech recognition. Computer Speech and Lan-
guage, 14(2):115–135.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL-1996, pages 177–183, Santa Cruz, CA,
USA.
JHU. 2003. Syntax for statistical machine
translation, Final report, JHU summer workshop.
http://www.clsp.jhu.edu/ws2003/groups/translate/.
S. Kumar and W. Byrne. 2002. Minimum Bayes-Risk
alignment of bilingual texts. In Proc. of EMNLP,
Philadelphia, PA, USA.
S. Kumar and W. Byrne. 2003. A weighted finite state
transducer implementation of the alignment template
model for statistical machine translation. In Proceed-
ings ofHLT-NAACL, Edmonton, Canada.
I. D. Melamed, R. Green, and J. P. Turian. 2003. Preci-
sion and recall of machine translation. In Proceedings
of the HLT-NAACL, Edmonton, Canada.
NIST. 2003. The NIST Machine Translation Evalua-
tions. http://www.nist.gov/speech/tests/mt/.
F. Och. 2002. Statistical Machine Translation: From
Single Word Models to Alignment Templates. Ph.D.
thesis, RWTH Aachen, Germany.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ofACL, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Division.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.
Flannery. 2002. Numerical Recipes in C++. Cam-
bridge University Press, Cambridge, UK.
M. Tang, X. Luo, and S. Roukos. 2002. Active learning
for statistical natural language parsing. In Proceedings
ofACL 2002, Philadelphia, PA, USA.
N. Ueffing, F. Och, and H. Ney. 2002. Generation of
word graphs in statistical machine translation. In Proc.
of EMNLP, pages 156–163, Philadelphia, PA, USA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.922436">
<title confidence="0.999601">Minimum Bayes-Risk Decoding for Statistical Machine Translation</title>
<author confidence="0.979681">Shankar Kumar</author>
<author confidence="0.979681">William</author>
<affiliation confidence="0.948853">Center for Language and Speech Processing, Johns Hopkins</affiliation>
<address confidence="0.999743">3400 North Charles Street, Baltimore, MD, 21218,</address>
<email confidence="0.998691">skumar,byrne@jhu.edu</email>
<abstract confidence="0.9990403125">We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation. This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AMTA</author>
</authors>
<date>2003</date>
<booktitle>Workshop on Machine Translation Evaluation, MT Summit IX. www.issco.unige.ch/projects/isle/MTE-at-MTS9.html.</booktitle>
<contexts>
<context position="24001" citStr="AMTA, 2003" startWordPosition="3957" endWordPosition="3958">th human assessments of translation quality. This loss function allows us to integrate syntactic structure into the statistical MT framework without building detailed models of syntactic features and retraining models from scratch. However, we emphasize that the MBR techniques do not preclude the construction of complex models of syntactic structure. Translation models that have been trained with linguistic features could still benefit by the application of MBR decoding procedures. That machine translation evaluation continues to be an active area of research is evident from recent workshops (AMTA, 2003). We expect new automatic MT evaluation metrics to emerge frequently in the future. Given any translation metric, the MBR decoding framework will allow us to optimize existing MT systems for the new criterion. This is intended to compensate for any mismatch between decoding strategy of MT systems and their evaluation criteria. While we have focused on developing MBR procedures for loss functions that measure various aspects of translation quality, this framework can also be used with loss functions which measure application-specific error criteria. We now describe related training and search p</context>
</contexts>
<marker>AMTA, 2003</marker>
<rawString>AMTA. 2003. Workshop on Machine Translation Evaluation, MT Summit IX. www.issco.unige.ch/projects/isle/MTE-at-MTS9.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>V Murdock</author>
<author>G Riccardi</author>
</authors>
<title>Bootstrapping bilingual data using consensus translation for a multilingual instant messaging system.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="25218" citStr="Bangalore et al., 2002" startWordPosition="4137" endWordPosition="4140">arch procedures for NLP that explicitly take into consideration taskspecific performance metrics. Och (2003) developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear MT models. Foster et al. (2002) developed a text-prediction system for translators that maximizes expected benefit to the translator under a statistical user model. In parsing, Goodman (1996) developed parsing algorithms that are appropriate for specific parsing metrics. There has also been recent work that combines 1-best hypotheses from multiple translation systems (Bangalore et al., 2002); this approach uses string-edit distance to align the hypotheses and rescores the resulting lattice with a language model. In future work we plan to extend the search space of MBR decoders to translation lattices produced by the baseline system. Translation lattices (Ueffing et al., 2002; Kumar and Byrne, 2003) are a compact representation of a large set of most likely translations generated by an MT system. While an -best list contains only a limited reordering of hypotheses, a translation lattice will contain hypotheses with a vastly greater number of re-orderings. We are developing efficie</context>
</contexts>
<marker>Bangalore, Murdock, Riccardi, 2002</marker>
<rawString>S. Bangalore, V. Murdock, and G. Riccardi. 2002. Bootstrapping bilingual data using consensus translation for a multilingual instant messaging system. In Proceedings of COLING, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Bickel</author>
<author>K A Doksum</author>
</authors>
<title>Mathematical Statistics: Basic Ideas and Selected topics. HoldenDay Inc.,</title>
<date>1977</date>
<location>Oakland, CA, USA.</location>
<contexts>
<context position="16339" citStr="Bickel and Doksum, 1977" startWordPosition="2706" endWordPosition="2709">nguage that has a word-to-word alignment relative to. Given the source sentence, the MT decoder produces a target word string with word-to-word alignment . Relative to a reference translation with word alignment, the decoder performance is measured as . Our goal is to find the decoder that has the best performance over all translations. This is measured through Bayes-Risk : The expectation is taken under the true distribution that describes translations of human quality. Given a loss function and a distribution, it is well known that the decision rule that minimizes the BayesRisk is given by (Bickel and Doksum, 1977; Goel and Byrne, 2000): We shall refer to the decoder given by this equation as the Minimum Bayes-Risk (MBR) decoder. The MBR decoder can be thought of as selecting a consensus translation: For each sentence, Equation 3 selects the translation that is closest on an average to all the likely translations and alignments. The closeness is measured under the loss function of interest. This optimal decoder has the difficulties of search (minimization) and computing the expectation under the true distribution. In practice, we will consider the space of translations to be an -best list of translatio</context>
</contexts>
<marker>Bickel, Doksum, 1977</marker>
<rawString>P. J. Bickel and K. A. Doksum. 1977. Mathematical Statistics: Basic Ideas and Selected topics. HoldenDay Inc., Oakland, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>D Chiang</author>
</authors>
<title>Two statistical parsing models applied to the chinese treebank.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second Chinese Language Processing Workshop,</booktitle>
<pages>1--6</pages>
<location>Hong Kong.</location>
<contexts>
<context position="19916" citStr="Bikel and Chiang, 2000" startWordPosition="3283" endWordPosition="3286">corresponding loss function. 4.2 Decoder Performance In our experiments, a baseline translation model (JHU, 2003), trained on a Chinese-English parallel corpus (NIST, 2003) ( English words and Chinese words), was used to generate 1000-best translation hypotheses for each Chinese sentence in the test set. The 1000-best lists were then rescored using the different translation loss functions described in Section 2. The English sentences in the -best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). The English parser was trained on the Penn Treebank and the Chinese parser on the Penn Chinese treebank. Under each loss function, the MBR decoding was performed using Equation 3. We say we have a matched condition when the same loss function is used in both the error rate and the decoder design. The performance of the MBR decoders on the NIST 2001+2002 test set is reported in Table 3. For all performance metrics, we show the 70% confidence interval with respect to the MAP baseline computed using bootstrap resampling (Press et al., 2002; Och, 2003). We note that this significance level if ot</context>
</contexts>
<marker>Bikel, Chiang, 2000</marker>
<rawString>D. Bikel and D. Chiang. 2000. Two statistical parsing models applied to the chinese treebank. In Proceedings of the Second Chinese Language Processing Workshop, pages 1–6, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J Cocke</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="15611" citStr="Brown et al., 1990" startWordPosition="2581" endWordPosition="2584">o measure this aspect of translation quality, and assigns different scores to the two translations. We provide this example to show how a loss function which makes use of syntactic structure from source and target parse trees, can capture properties of translations that string based loss functions are unable to measure. Loss Functions BLEU (%) 26.4 26.4 WER (%) 70.6 70.6 PER (%) 23.5 23.5 BiTree Error Rate (%) 65.4 92.3 Table 2: Comparison of the different loss functions for hypothesis and reference translations from Figures 1, 2. 3 Minimum Bayes-Risk Decoding Statistical Machine Translation (Brown et al., 1990) can be formulated as a mapping of a word sequence in a source language to word sequence in the target language that has a word-to-word alignment relative to. Given the source sentence, the MT decoder produces a target word string with word-to-word alignment . Relative to a reference translation with word alignment, the decoder performance is measured as . Our goal is to find the decoder that has the best performance over all translations. This is measured through Bayes-Risk : The expectation is taken under the true distribution that describes translations of human quality. Given a loss functi</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the weighted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="7510" citStr="Collins and Duffy, 2002" startWordPosition="1183" endWordPosition="1186">the other word string. The PER score (Och, 2002) is then computed as a ratio of this distance to the number of words in the reference word string. 2.2 Target Language Parse-Tree Loss Functions The second class of translation loss functions uses information only from the parse-trees of the two translations, so that . This loss function has no access to any information from the source sentence or the word alignments. Examples of such loss functions are tree-edit distances between parse-trees, string-edit distances between event representation of parse-trees (Tang et al., 2002), and treekernels (Collins and Duffy, 2002). The computation of tree-edit distance involves an unconstrained alignment of the two English parse-trees. We can simplify this problem once we have a third parse tree (for the Chinese sentence) with node-to-node alignment relative to the two English trees. We will introduce such a loss function in the next section. We did not perform experiments involving this class of loss functions, but mention them for completeness in the hierarchy of loss functions. 2.3 Bilingual Parse-Tree Loss Functions The third class of loss functions uses information from word strings, alignments and parse-trees in </context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>M. Collins and N. Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the weighted perceptron. In Proceedings of EMNLP, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="19802" citStr="Collins, 1999" startWordPosition="3265" endWordPosition="3266">rror rate of the hypothesis translation with respect to the most similar reference translation under the corresponding loss function. 4.2 Decoder Performance In our experiments, a baseline translation model (JHU, 2003), trained on a Chinese-English parallel corpus (NIST, 2003) ( English words and Chinese words), was used to generate 1000-best translation hypotheses for each Chinese sentence in the test set. The 1000-best lists were then rescored using the different translation loss functions described in Section 2. The English sentences in the -best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). The English parser was trained on the Penn Treebank and the Chinese parser on the Penn Chinese treebank. Under each loss function, the MBR decoding was performed using Equation 3. We say we have a matched condition when the same loss function is used in both the error rate and the decoder design. The performance of the MBR decoders on the NIST 2001+2002 test set is reported in Table 3. For all performance metrics, we show the 70% confidence interval with respect to the MAP baseli</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. J. Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. ofHLT 2002,</booktitle>
<publisher>USA.</publisher>
<location>San Diego, CA.</location>
<contexts>
<context position="1310" citStr="Doddington, 2002" startWordPosition="184" endWordPosition="185">t language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions. 1 Introduction Statistical Machine Translation systems have achieved considerable progress in recent years as seen from their performance on international competitions in standard evaluation tasks (NIST, 2003). This rapid progress has been greatly facilitated by the development of automatic translation evaluation metrics such as BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002) and Position Independent Word Error Rate (PER) (Och, 2002). However, given the many factors that influence translation quality, it is unlikely that we will find a single translation metric that will be able to judge all these factors. For example, the BLEU, NIST and the PER metrics, This work was supported by the National Science Foundation under Grant No. 0121285 and an ONR MURI Grant N00014-01-1-0685. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation or th</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proc. ofHLT 2002, San Diego, CA. USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster</author>
<author>P Langlais</author>
<author>G Lapalme</author>
</authors>
<title>Userfriendly text prediction for translators.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="24855" citStr="Foster et al. (2002)" startWordPosition="4087" endWordPosition="4090">ensate for any mismatch between decoding strategy of MT systems and their evaluation criteria. While we have focused on developing MBR procedures for loss functions that measure various aspects of translation quality, this framework can also be used with loss functions which measure application-specific error criteria. We now describe related training and search procedures for NLP that explicitly take into consideration taskspecific performance metrics. Och (2003) developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear MT models. Foster et al. (2002) developed a text-prediction system for translators that maximizes expected benefit to the translator under a statistical user model. In parsing, Goodman (1996) developed parsing algorithms that are appropriate for specific parsing metrics. There has also been recent work that combines 1-best hypotheses from multiple translation systems (Bangalore et al., 2002); this approach uses string-edit distance to align the hypotheses and rescores the resulting lattice with a language model. In future work we plan to extend the search space of MBR decoders to translation lattices produced by the baselin</context>
</contexts>
<marker>Foster, Langlais, Lapalme, 2002</marker>
<rawString>G. Foster, P. Langlais, and G. Lapalme. 2002. Userfriendly text prediction for translators. In Proc. of EMNLP, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Goel</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayes-risk automatic speech recognition.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="2555" citStr="Goel and Byrne, 2000" startWordPosition="375" endWordPosition="378">ch. though effective, do not take into account explicit syntactic information when measuring translation quality. Given that different Machine Translation (MT) evaluation metrics are useful for capturing different aspects of translation quality, it becomes desirable to create MT systems tuned with respect to each individual criterion. In contrast, the maximum likelihood techniques that underlie the decision processes of most current MT systems do not take into account these application specific goals. We apply the Minimum Bayes-Risk (MBR) techniques developed for automatic speech recognition (Goel and Byrne, 2000) and bitext word alignment for statistical MT (Kumar and Byrne, 2002), to the problem of building automatic MT systems tuned for specific metrics. This is a framework that can be used with statistical models of speech and language to develop decision processes optimized for specific loss functions. We will show that MBR decoding can be applied to machine translation in two scenarios. Given an automatic MT metric, we design a loss function based on the metric and use MBR decoding to tune MT performance under the metric. We also show how MBR decoding can be used to incorporate syntactic structur</context>
<context position="16362" citStr="Goel and Byrne, 2000" startWordPosition="2710" endWordPosition="2713">-word alignment relative to. Given the source sentence, the MT decoder produces a target word string with word-to-word alignment . Relative to a reference translation with word alignment, the decoder performance is measured as . Our goal is to find the decoder that has the best performance over all translations. This is measured through Bayes-Risk : The expectation is taken under the true distribution that describes translations of human quality. Given a loss function and a distribution, it is well known that the decision rule that minimizes the BayesRisk is given by (Bickel and Doksum, 1977; Goel and Byrne, 2000): We shall refer to the decoder given by this equation as the Minimum Bayes-Risk (MBR) decoder. The MBR decoder can be thought of as selecting a consensus translation: For each sentence, Equation 3 selects the translation that is closest on an average to all the likely translations and alignments. The closeness is measured under the loss function of interest. This optimal decoder has the difficulties of search (minimization) and computing the expectation under the true distribution. In practice, we will consider the space of translations to be an -best list of translation alternatives generate</context>
</contexts>
<marker>Goel, Byrne, 2000</marker>
<rawString>V. Goel and W. Byrne. 2000. Minimum Bayes-risk automatic speech recognition. Computer Speech and Language, 14(2):115–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proc. of ACL-1996,</booktitle>
<pages>177--183</pages>
<location>Santa Cruz, CA, USA.</location>
<contexts>
<context position="25015" citStr="Goodman (1996)" startWordPosition="4111" endWordPosition="4112">hat measure various aspects of translation quality, this framework can also be used with loss functions which measure application-specific error criteria. We now describe related training and search procedures for NLP that explicitly take into consideration taskspecific performance metrics. Och (2003) developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear MT models. Foster et al. (2002) developed a text-prediction system for translators that maximizes expected benefit to the translator under a statistical user model. In parsing, Goodman (1996) developed parsing algorithms that are appropriate for specific parsing metrics. There has also been recent work that combines 1-best hypotheses from multiple translation systems (Bangalore et al., 2002); this approach uses string-edit distance to align the hypotheses and rescores the resulting lattice with a language model. In future work we plan to extend the search space of MBR decoders to translation lattices produced by the baseline system. Translation lattices (Ueffing et al., 2002; Kumar and Byrne, 2003) are a compact representation of a large set of most likely translations generated b</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>J. Goodman. 1996. Parsing algorithms and metrics. In Proc. of ACL-1996, pages 177–183, Santa Cruz, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JHU</author>
</authors>
<title>Syntax for statistical machine translation,</title>
<date>2003</date>
<note>Final report, JHU summer workshop. http://www.clsp.jhu.edu/ws2003/groups/translate/.</note>
<contexts>
<context position="19406" citStr="JHU, 2003" startWordPosition="3203" endWordPosition="3204">te (mWER) (Och, 2002), multi-reference Position-independent word Error Rate (mPER) (Och, 2002) , BLEU and multi-reference BiTree Error Rate. Among these evaluation metrics, the BLEU score directly takes into account multiple reference translations (Papineni et al., 2001). In case of the other metrics, we consider multiple references in the following way. For each sentence, we compute the error rate of the hypothesis translation with respect to the most similar reference translation under the corresponding loss function. 4.2 Decoder Performance In our experiments, a baseline translation model (JHU, 2003), trained on a Chinese-English parallel corpus (NIST, 2003) ( English words and Chinese words), was used to generate 1000-best translation hypotheses for each Chinese sentence in the test set. The 1000-best lists were then rescored using the different translation loss functions described in Section 2. The English sentences in the -best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). The English parser was trained on the Penn Treebank and the Chinese parser on the Penn C</context>
</contexts>
<marker>JHU, 2003</marker>
<rawString>JHU. 2003. Syntax for statistical machine translation, Final report, JHU summer workshop. http://www.clsp.jhu.edu/ws2003/groups/translate/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayes-Risk alignment of bilingual texts.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="2624" citStr="Kumar and Byrne, 2002" startWordPosition="386" endWordPosition="390">formation when measuring translation quality. Given that different Machine Translation (MT) evaluation metrics are useful for capturing different aspects of translation quality, it becomes desirable to create MT systems tuned with respect to each individual criterion. In contrast, the maximum likelihood techniques that underlie the decision processes of most current MT systems do not take into account these application specific goals. We apply the Minimum Bayes-Risk (MBR) techniques developed for automatic speech recognition (Goel and Byrne, 2000) and bitext word alignment for statistical MT (Kumar and Byrne, 2002), to the problem of building automatic MT systems tuned for specific metrics. This is a framework that can be used with statistical models of speech and language to develop decision processes optimized for specific loss functions. We will show that MBR decoding can be applied to machine translation in two scenarios. Given an automatic MT metric, we design a loss function based on the metric and use MBR decoding to tune MT performance under the metric. We also show how MBR decoding can be used to incorporate syntactic structure into a statistical MT system by building specialized loss functions</context>
</contexts>
<marker>Kumar, Byrne, 2002</marker>
<rawString>S. Kumar and W. Byrne. 2002. Minimum Bayes-Risk alignment of bilingual texts. In Proc. of EMNLP, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>A weighted finite state transducer implementation of the alignment template model for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="25531" citStr="Kumar and Byrne, 2003" startWordPosition="4186" endWordPosition="4189">at maximizes expected benefit to the translator under a statistical user model. In parsing, Goodman (1996) developed parsing algorithms that are appropriate for specific parsing metrics. There has also been recent work that combines 1-best hypotheses from multiple translation systems (Bangalore et al., 2002); this approach uses string-edit distance to align the hypotheses and rescores the resulting lattice with a language model. In future work we plan to extend the search space of MBR decoders to translation lattices produced by the baseline system. Translation lattices (Ueffing et al., 2002; Kumar and Byrne, 2003) are a compact representation of a large set of most likely translations generated by an MT system. While an -best list contains only a limited reordering of hypotheses, a translation lattice will contain hypotheses with a vastly greater number of re-orderings. We are developing efficient lattice search procedures for MBR decoders. By extending the search space of the decoder to a much larger space than the -best list, we expect further performance improvements. MBR is a promising modeling framework for statistical machine translation. It is a simple model rescoring framework that improves wel</context>
</contexts>
<marker>Kumar, Byrne, 2003</marker>
<rawString>S. Kumar and W. Byrne. 2003. A weighted finite state transducer implementation of the alignment template model for statistical machine translation. In Proceedings ofHLT-NAACL, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
<author>R Green</author>
<author>J P Turian</author>
</authors>
<title>Precision and recall of machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="5910" citStr="Melamed et al. (2003)" startWordPosition="909" endWordPosition="912">re 1. The reference translation for the Chinese sentence with its word-to-word alignment is shown in Figure 2. In this section, we will show the computation of different loss functions for this example. 2.1 Lexical Loss Functions The first class of loss functions uses no information about word alignments or parse-trees, so that can be reduced to . We consider three loss functions in this category: The BLEU score (Papineni et al., 2001), word-error rate, and the position-independent word-error rate (Och, 2002). Another example of a loss function in this class is the MTeval metric introduced in Melamed et al. (2003). A loss function of this type depends only on information from word strings. BLEU score (Papineni et al., 2001) computes the geometric mean of the precision of-grams of various lengths ( ) between a hypothesis and a reference translation, and includes a brevity penalty ( ) if the hypothesis is shorter than the reference. We use . where is the precision of-grams in the hypothesis . The BLEU score is zero if any of the n-gram precisions is zero for that sentence pair. We note that . We derive a loss function from BLEU score as BLEU . Word Error Rate (WER) is the ratio of the string-edit distanc</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. D. Melamed, R. Green, and J. P. Turian. 2003. Precision and recall of machine translation. In Proceedings of the HLT-NAACL, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The NIST Machine Translation Evaluations.</title>
<date>2003</date>
<note>http://www.nist.gov/speech/tests/mt/.</note>
<contexts>
<context position="1123" citStr="NIST, 2003" startWordPosition="156" endWordPosition="157">hat incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions. 1 Introduction Statistical Machine Translation systems have achieved considerable progress in recent years as seen from their performance on international competitions in standard evaluation tasks (NIST, 2003). This rapid progress has been greatly facilitated by the development of automatic translation evaluation metrics such as BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002) and Position Independent Word Error Rate (PER) (Och, 2002). However, given the many factors that influence translation quality, it is unlikely that we will find a single translation metric that will be able to judge all these factors. For example, the BLEU, NIST and the PER metrics, This work was supported by the National Science Foundation under Grant No. 0121285 and an ONR MURI Grant N00014-01-1-0685. Any o</context>
<context position="18218" citStr="NIST, 2003" startWordPosition="3018" endWordPosition="3019">nsidering a loss function that assigns a equal cost (say 1) to all misclassifications. Under the 0/1 loss function, the decoder of Equation 3 reduces to the MAP decoder MAP This illustrates why we are interested in MBR decoders based on other loss functions: the MAP decoder is optimal with respect to a loss function that is very harsh. It does not distinguish between different types of translation errors and good translations receive the same penalty as poor translations. 4 Performance of MBR Decoders We performed our experiments on the Large-Data Track of the NIST Chinese-to-English MT task (NIST, 2003). The goal of this task is the translation of news stories from Chinese to English. The test set has a total of 1791 sentences, consisting of 993 sentences from the NIST 2001 MT-eval set and 878 sentences from the NIST 2002 MT-eval set. Each Chinese sentence in this set has four reference translations. 4.1 Evaluation Metrics The performance of the baseline and the MBR decoders under the different loss functions was measured with respect to the four reference translations provided for the test set. Four evaluation metrics were used. These were multi-reference Word Error Rate (mWER) (Och, 2002),</context>
<context position="19465" citStr="NIST, 2003" startWordPosition="3212" endWordPosition="3213">t word Error Rate (mPER) (Och, 2002) , BLEU and multi-reference BiTree Error Rate. Among these evaluation metrics, the BLEU score directly takes into account multiple reference translations (Papineni et al., 2001). In case of the other metrics, we consider multiple references in the following way. For each sentence, we compute the error rate of the hypothesis translation with respect to the most similar reference translation under the corresponding loss function. 4.2 Decoder Performance In our experiments, a baseline translation model (JHU, 2003), trained on a Chinese-English parallel corpus (NIST, 2003) ( English words and Chinese words), was used to generate 1000-best translation hypotheses for each Chinese sentence in the test set. The 1000-best lists were then rescored using the different translation loss functions described in Section 2. The English sentences in the -best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). The English parser was trained on the Penn Treebank and the Chinese parser on the Penn Chinese treebank. Under each loss function, the MBR decoding</context>
</contexts>
<marker>NIST, 2003</marker>
<rawString>NIST. 2003. The NIST Machine Translation Evaluations. http://www.nist.gov/speech/tests/mt/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Statistical Machine Translation: From Single Word Models to Alignment Templates.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>RWTH Aachen,</institution>
<contexts>
<context position="1369" citStr="Och, 2002" startWordPosition="193" endWordPosition="194">s on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions. 1 Introduction Statistical Machine Translation systems have achieved considerable progress in recent years as seen from their performance on international competitions in standard evaluation tasks (NIST, 2003). This rapid progress has been greatly facilitated by the development of automatic translation evaluation metrics such as BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002) and Position Independent Word Error Rate (PER) (Och, 2002). However, given the many factors that influence translation quality, it is unlikely that we will find a single translation metric that will be able to judge all these factors. For example, the BLEU, NIST and the PER metrics, This work was supported by the National Science Foundation under Grant No. 0121285 and an ONR MURI Grant N00014-01-1-0685. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation or the Office of Naval Research. though effective, do not take i</context>
<context position="5803" citStr="Och, 2002" startWordPosition="890" endWordPosition="891">ons for a Chinese sentence (in Pinyin without tones), with their word-to-word alignments in Figure 1. The reference translation for the Chinese sentence with its word-to-word alignment is shown in Figure 2. In this section, we will show the computation of different loss functions for this example. 2.1 Lexical Loss Functions The first class of loss functions uses no information about word alignments or parse-trees, so that can be reduced to . We consider three loss functions in this category: The BLEU score (Papineni et al., 2001), word-error rate, and the position-independent word-error rate (Och, 2002). Another example of a loss function in this class is the MTeval metric introduced in Melamed et al. (2003). A loss function of this type depends only on information from word strings. BLEU score (Papineni et al., 2001) computes the geometric mean of the precision of-grams of various lengths ( ) between a hypothesis and a reference translation, and includes a brevity penalty ( ) if the hypothesis is shorter than the reference. We use . where is the precision of-grams in the hypothesis . The BLEU score is zero if any of the n-gram precisions is zero for that sentence pair. We note that . We der</context>
<context position="17136" citStr="Och, 2002" startWordPosition="2837" endWordPosition="2838">on: For each sentence, Equation 3 selects the translation that is closest on an average to all the likely translations and alignments. The closeness is measured under the loss function of interest. This optimal decoder has the difficulties of search (minimization) and computing the expectation under the true distribution. In practice, we will consider the space of translations to be an -best list of translation alternatives generated under a baseline translation model. Of course, we do not have access to the true distribution over translations. We therefore use statistical translation models (Och, 2002) to approximate the distribution . Decoder Implementation: The MBR decoder (Equation 3) on the -best List is implemented as and . This is a rescoring procedure that searches for consensus under a given loss function. The posterior probability of each hypothesis in the -best list is derived from the joint probability assigned by the baseline translation model. The conventional Maximum A Posteriori (MAP) decoder can be derived as a special case of the MBR decoder by considering a loss function that assigns a equal cost (say 1) to all misclassifications. Under the 0/1 loss function, the decoder o</context>
<context position="18817" citStr="Och, 2002" startWordPosition="3117" endWordPosition="3118">NIST, 2003). The goal of this task is the translation of news stories from Chinese to English. The test set has a total of 1791 sentences, consisting of 993 sentences from the NIST 2001 MT-eval set and 878 sentences from the NIST 2002 MT-eval set. Each Chinese sentence in this set has four reference translations. 4.1 Evaluation Metrics The performance of the baseline and the MBR decoders under the different loss functions was measured with respect to the four reference translations provided for the test set. Four evaluation metrics were used. These were multi-reference Word Error Rate (mWER) (Och, 2002), multi-reference Position-independent word Error Rate (mPER) (Och, 2002) , BLEU and multi-reference BiTree Error Rate. Among these evaluation metrics, the BLEU score directly takes into account multiple reference translations (Papineni et al., 2001). In case of the other metrics, we consider multiple references in the following way. For each sentence, we compute the error rate of the hypothesis translation with respect to the most similar reference translation under the corresponding loss function. 4.2 Decoder Performance In our experiments, a baseline translation model (JHU, 2003), trained o</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>F. Och. 2002. Statistical Machine Translation: From Single Word Models to Alignment Templates. Ph.D. thesis, RWTH Aachen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ofACL,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="20472" citStr="Och, 2003" startWordPosition="3383" endWordPosition="3384">er provided to us by D. Bikel (Bikel and Chiang, 2000). The English parser was trained on the Penn Treebank and the Chinese parser on the Penn Chinese treebank. Under each loss function, the MBR decoding was performed using Equation 3. We say we have a matched condition when the same loss function is used in both the error rate and the decoder design. The performance of the MBR decoders on the NIST 2001+2002 test set is reported in Table 3. For all performance metrics, we show the 70% confidence interval with respect to the MAP baseline computed using bootstrap resampling (Press et al., 2002; Och, 2003). We note that this significance level if otherwise, does meet the customary criteria for minimum significance intervals of 68.3% (Press et al., 2002). We observe in most cases that the MBR decoder under a loss function performs the best under the corresponding error metric i.e. matched conditions perform the best. The gains from MBR decoding under matched conditions are statistically significant in most cases. We note that the MAP decoder is not optimal in any of the cases. In particular, the translation performance under the BLEU metric can be improved by using MBR relative to MAP decoding. </context>
<context position="24703" citStr="Och (2003)" startWordPosition="4066" endWordPosition="4067">y translation metric, the MBR decoding framework will allow us to optimize existing MT systems for the new criterion. This is intended to compensate for any mismatch between decoding strategy of MT systems and their evaluation criteria. While we have focused on developing MBR procedures for loss functions that measure various aspects of translation quality, this framework can also be used with loss functions which measure application-specific error criteria. We now describe related training and search procedures for NLP that explicitly take into consideration taskspecific performance metrics. Och (2003) developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear MT models. Foster et al. (2002) developed a text-prediction system for translators that maximizes expected benefit to the translator under a statistical user model. In parsing, Goodman (1996) developed parsing algorithms that are appropriate for specific parsing metrics. There has also been recent work that combines 1-best hypotheses from multiple translation systems (Bangalore et al., 2002); this approach uses string-edit distance to align the hypotheses and rescores the re</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ofACL, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>Technical Report RC22176 (W0109-022),</tech>
<institution>IBM Research Division.</institution>
<contexts>
<context position="1279" citStr="Papineni et al., 2001" startWordPosition="177" endWordPosition="181">from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions. 1 Introduction Statistical Machine Translation systems have achieved considerable progress in recent years as seen from their performance on international competitions in standard evaluation tasks (NIST, 2003). This rapid progress has been greatly facilitated by the development of automatic translation evaluation metrics such as BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002) and Position Independent Word Error Rate (PER) (Och, 2002). However, given the many factors that influence translation quality, it is unlikely that we will find a single translation metric that will be able to judge all these factors. For example, the BLEU, NIST and the PER metrics, This work was supported by the National Science Foundation under Grant No. 0121285 and an ONR MURI Grant N00014-01-1-0685. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Na</context>
<context position="5728" citStr="Papineni et al., 2001" startWordPosition="879" endWordPosition="882">e-Tree Bilingual Parse-Tree We start with an example of two competing English translations for a Chinese sentence (in Pinyin without tones), with their word-to-word alignments in Figure 1. The reference translation for the Chinese sentence with its word-to-word alignment is shown in Figure 2. In this section, we will show the computation of different loss functions for this example. 2.1 Lexical Loss Functions The first class of loss functions uses no information about word alignments or parse-trees, so that can be reduced to . We consider three loss functions in this category: The BLEU score (Papineni et al., 2001), word-error rate, and the position-independent word-error rate (Och, 2002). Another example of a loss function in this class is the MTeval metric introduced in Melamed et al. (2003). A loss function of this type depends only on information from word strings. BLEU score (Papineni et al., 2001) computes the geometric mean of the precision of-grams of various lengths ( ) between a hypothesis and a reference translation, and includes a brevity penalty ( ) if the hypothesis is shorter than the reference. We use . where is the precision of-grams in the hypothesis . The BLEU score is zero if any of </context>
<context position="19067" citStr="Papineni et al., 2001" startWordPosition="3149" endWordPosition="3152">eval set. Each Chinese sentence in this set has four reference translations. 4.1 Evaluation Metrics The performance of the baseline and the MBR decoders under the different loss functions was measured with respect to the four reference translations provided for the test set. Four evaluation metrics were used. These were multi-reference Word Error Rate (mWER) (Och, 2002), multi-reference Position-independent word Error Rate (mPER) (Och, 2002) , BLEU and multi-reference BiTree Error Rate. Among these evaluation metrics, the BLEU score directly takes into account multiple reference translations (Papineni et al., 2001). In case of the other metrics, we consider multiple references in the following way. For each sentence, we compute the error rate of the hypothesis translation with respect to the most similar reference translation under the corresponding loss function. 4.2 Decoder Performance In our experiments, a baseline translation model (JHU, 2003), trained on a Chinese-English parallel corpus (NIST, 2003) ( English words and Chinese words), was used to generate 1000-best translation hypotheses for each Chinese sentence in the test set. The 1000-best lists were then rescored using the different translati</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Press</author>
<author>S A Teukolsky</author>
<author>W T Vetterling</author>
<author>B P Flannery</author>
</authors>
<title>Numerical Recipes in C++.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="20460" citStr="Press et al., 2002" startWordPosition="3379" endWordPosition="3382">using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). The English parser was trained on the Penn Treebank and the Chinese parser on the Penn Chinese treebank. Under each loss function, the MBR decoding was performed using Equation 3. We say we have a matched condition when the same loss function is used in both the error rate and the decoder design. The performance of the MBR decoders on the NIST 2001+2002 test set is reported in Table 3. For all performance metrics, we show the 70% confidence interval with respect to the MAP baseline computed using bootstrap resampling (Press et al., 2002; Och, 2003). We note that this significance level if otherwise, does meet the customary criteria for minimum significance intervals of 68.3% (Press et al., 2002). We observe in most cases that the MBR decoder under a loss function performs the best under the corresponding error metric i.e. matched conditions perform the best. The gains from MBR decoding under matched conditions are statistically significant in most cases. We note that the MAP decoder is not optimal in any of the cases. In particular, the translation performance under the BLEU metric can be improved by using MBR relative to MA</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 2002</marker>
<rawString>W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. 2002. Numerical Recipes in C++. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tang</author>
<author>X Luo</author>
<author>S Roukos</author>
</authors>
<title>Active learning for statistical natural language parsing.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL 2002,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="7467" citStr="Tang et al., 2002" startWordPosition="1176" endWordPosition="1179"> a word string to any permutation of the other word string. The PER score (Och, 2002) is then computed as a ratio of this distance to the number of words in the reference word string. 2.2 Target Language Parse-Tree Loss Functions The second class of translation loss functions uses information only from the parse-trees of the two translations, so that . This loss function has no access to any information from the source sentence or the word alignments. Examples of such loss functions are tree-edit distances between parse-trees, string-edit distances between event representation of parse-trees (Tang et al., 2002), and treekernels (Collins and Duffy, 2002). The computation of tree-edit distance involves an unconstrained alignment of the two English parse-trees. We can simplify this problem once we have a third parse tree (for the Chinese sentence) with node-to-node alignment relative to the two English trees. We will introduce such a loss function in the next section. We did not perform experiments involving this class of loss functions, but mention them for completeness in the hierarchy of loss functions. 2.3 Bilingual Parse-Tree Loss Functions The third class of loss functions uses information from w</context>
</contexts>
<marker>Tang, Luo, Roukos, 2002</marker>
<rawString>M. Tang, X. Luo, and S. Roukos. 2002. Active learning for statistical natural language parsing. In Proceedings ofACL 2002, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Generation of word graphs in statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>156--163</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="25507" citStr="Ueffing et al., 2002" startWordPosition="4182" endWordPosition="4185">tem for translators that maximizes expected benefit to the translator under a statistical user model. In parsing, Goodman (1996) developed parsing algorithms that are appropriate for specific parsing metrics. There has also been recent work that combines 1-best hypotheses from multiple translation systems (Bangalore et al., 2002); this approach uses string-edit distance to align the hypotheses and rescores the resulting lattice with a language model. In future work we plan to extend the search space of MBR decoders to translation lattices produced by the baseline system. Translation lattices (Ueffing et al., 2002; Kumar and Byrne, 2003) are a compact representation of a large set of most likely translations generated by an MT system. While an -best list contains only a limited reordering of hypotheses, a translation lattice will contain hypotheses with a vastly greater number of re-orderings. We are developing efficient lattice search procedures for MBR decoders. By extending the search space of the decoder to a much larger space than the -best list, we expect further performance improvements. MBR is a promising modeling framework for statistical machine translation. It is a simple model rescoring fra</context>
</contexts>
<marker>Ueffing, Och, Ney, 2002</marker>
<rawString>N. Ueffing, F. Och, and H. Ney. 2002. Generation of word graphs in statistical machine translation. In Proc. of EMNLP, pages 156–163, Philadelphia, PA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>