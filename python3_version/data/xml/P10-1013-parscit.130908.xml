<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.991329">
Open Information Extraction using Wikipedia
</title>
<author confidence="0.99874">
Fei Wu
</author>
<affiliation confidence="0.999086">
University of Washington
</affiliation>
<address confidence="0.948209">
Seattle, WA, USA
</address>
<email confidence="0.997801">
wufei@cs.washington.edu
</email>
<author confidence="0.94931">
Daniel S. Weld
</author>
<affiliation confidence="0.986044">
University of Washington
</affiliation>
<address confidence="0.9474">
Seattle, WA, USA
</address>
<email confidence="0.998592">
weld@cs.washington.edu
</email>
<sectionHeader confidence="0.993887" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99978076">
Information-extraction (IE) systems seek
to distill semantic relations from natural-
language text, but most systems use super-
vised learning of relation-specific examples
and are thus limited by the availability of
training data. Open IE systems such as
TextRunner, on the other hand, aim to handle
the unbounded number of relations found
on the Web. But how well can these open
systems perform?
This paper presents WOE, an open IE system
which improves dramatically on TextRunner’s
precision and recall. The key to WOE’s per-
formance is a novel form of self-supervised
learning for open extractors — using heuris-
tic matches between Wikipedia infobox at-
tribute values and corresponding sentences to
construct training data. Like TextRunner,
WOE’s extractor eschews lexicalized features
and handles an unbounded set of semantic
relations. WOE can operate in two modes:
when restricted to POS tag features, it runs
as quickly as TextRunner, but when set to use
dependency-parse features its precision and
recall rise even higher.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999848795918368">
The problem of information-extraction (IE), gen-
erating relational data from natural-language text,
has received increasing attention in recent years.
A large, high-quality repository of extracted tu-
ples can potentially benefit a wide range of NLP
tasks such as question answering, ontology learn-
ing, and summarization. The vast majority of
IE work uses supervised learning of relation-
specific examples. For example, the WebKB
project (Craven et al., 1998) used labeled exam-
ples of the courses-taught-by relation to in-
duce rules for identifying additional instances of
the relation. While these methods can achieve
high precision and recall, they are limited by the
availability of training data and are unlikely to
scale to the thousands of relations found in text
on the Web.
An alternative paradigm, Open IE, pioneered
by the TextRunner system (Banko et al., 2007)
and the “preemptive IE” in (Shinyama and Sekine,
2006), aims to handle an unbounded number of
relations and run quickly enough to process Web-
scale corpora. Domain independence is achieved
by extracting the relation name as well as its
two arguments. Most open IE systems use self-
supervised learning, in which automatic heuristics
generate labeled data for training the extractor. For
example, TextRunner uses a small set of hand-
written rules to heuristically label training exam-
ples from sentences in the Penn Treebank.
This paper presents WOE (Wikipedia-based
Open Extractor), the first system that au-
tonomously transfers knowledge from random ed-
itors’ effort of collaboratively editing Wikipedia to
train an open information extractor. Specifically,
WOE generates relation-specific training examples
by matching Infobox1 attribute values to corre-
sponding sentences (as done in Kylin (Wu and
Weld, 2007) and Luchs (Hoffmann et al., 2010)),
but WOE abstracts these examples to relation-
independent training data to learn an unlexical-
ized extractor, akin to that of TextRunner. WOE
can operate in two modes: when restricted to
shallow features like part-of-speech (POS) tags, it
runs as quickly as Textrunner, but when set to use
dependency-parse features its precision and recall
rise even higher. We present a thorough experi-
mental evaluation, making the following contribu-
tions:
</bodyText>
<listItem confidence="0.871636">
• We present WOE, a new approach to open IE
that uses Wikipedia for self-supervised learn-
</listItem>
<footnote confidence="0.945568">
1An infobox is a set of tuples summarizing the key at-
tributes of the subject in a Wikipedia article. For example,
the infobox in the article on “Sweden” contains attributes like
Capital, Population and GDP.
</footnote>
<page confidence="0.823832">
118
</page>
<note confidence="0.9482785">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 118–127,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.757848421052632">
ing of unlexicalized extractors. Compared
with TextRunner (the state of the art) on three
corpora, WOE yields between 72% and 91%
improved F-measure — generalizing well be-
yond Wikipedia.
• Using the same learning algorithm and fea-
tures as TextRunner, we compare four dif-
ferent ways to generate positive and negative
training data with TextRunner’s method, con-
cluding that our Wikipedia heuristic is respon-
sible for the bulk of WOE’s improved accuracy.
• The biggest win arises from using parser fea-
tures. Previous work (Jiang and Zhai, 2007)
concluded that parser-based features are un-
necessary for information extraction, but that
work assumed the presence of lexical features.
We show that abstract dependency paths are
a highly informative feature when performing
unlexicalized extraction.
</bodyText>
<sectionHeader confidence="0.990668" genericHeader="method">
2 Problem Definition
</sectionHeader>
<bodyText confidence="0.999885434782609">
An open information extractor is a function
from a document, d, to a set of triples,
{(argi, rel, arg0j, where the args are noun
phrases and rel is a textual fragment indicat-
ing an implicit, semantic relation between the two
noun phrases. The extractor should produce one
triple for every relation stated explicitly in the text,
but is not required to infer implicit facts. In this
paper, we assume that all relational instances are
stated within a single sentence. Note the dif-
ference between open IE and the traditional ap-
proaches (e.g., as in WebKB), where the task is
to decide whether some pre-defined relation holds
between (two) arguments in the sentence.
We wish to learn an open extractor without di-
rect supervision, i.e. without annotated training
examples or hand-crafted patterns. Our input is
Wikipedia, a collaboratively-constructed encyclo-
pedia2. As output, WOE produces an unlexicalized
and relation-independent open extractor. Our ob-
jective is an extractor which generalizes beyond
Wikipedia, handling other corpora such as the gen-
eral Web.
</bodyText>
<sectionHeader confidence="0.985648" genericHeader="method">
3 Wikipedia-based Open IE
</sectionHeader>
<bodyText confidence="0.9985195">
The key idea underlying WOE is the automatic
construction of training examples by heuristically
matching Wikipedia infobox values and corre-
sponding text; these examples are used to generate
</bodyText>
<footnote confidence="0.9654545">
2We also use DBpedia (Auer and Lehmann, 2007) as a
collection of conveniently parsed Wikipedia infoboxes
</footnote>
<figureCaption confidence="0.999955">
Figure 1: Architecture of WOE.
</figureCaption>
<bodyText confidence="0.656180666666667">
an unlexicalized, relation-independent (open) ex-
tractor. As shown in Figure 1, WOE has three main
components: preprocessor, matcher, and learner.
</bodyText>
<subsectionHeader confidence="0.985464">
3.1 Preprocessor
</subsectionHeader>
<bodyText confidence="0.992751416666667">
The preprocessor converts the raw Wikipedia text
into a sequence of sentences, attaches NLP anno-
tations, and builds synonym sets for key entities.
The resulting data is fed to the matcher, described
in Section 3.2, which generates the training set.
Sentence Splitting: The preprocessor first renders
each Wikipedia article into HTML, then splits the
article into sentences using OpenNLP.
NLP Annotation: As we discuss fully in Sec-
tion 4 (Experiments), we consider several varia-
tions of our system; one version, WOEparse, uses
parser-based features, while another, WOEpos, uses
shallow features like POS tags, which may be
more quickly computed. Depending on which
version is being trained, the preprocessor uses
OpenNLP to supply POS tags and NP-chunk an-
notations — or uses the Stanford Parser to create a
dependency parse. When parsing, we force the hy-
perlinked anchor texts to be a single token by con-
necting the words with an underscore; this trans-
formation improves parsing performance in many
cases.
Compiling Synonyms: As a final step, the pre-
processor builds sets of synonyms to help the
matcher find sentences that correspond to infobox
relations. This is useful because Wikipedia edi-
tors frequently use multiple names for an entity;
for example, in the article titled “University of
Washington” the token “UW” is widely used to
refer the university. Additionally, attribute values
are often described differently within the infobox
than they are in surrounding text. Without knowl-
edge of these synonyms, it is impossible to con-
struct good matches. Following (Wu and Weld,
2007; Nakayama and Nishio, 2008), the prepro-
cessor uses Wikipedia redirection pages and back-
</bodyText>
<figure confidence="0.993742444444444">
Sentence Splitting Preprocessor
NLP Annotating
Synonyms Compiling
Primary Entity Matching Matcher
Sentence Matching
Pattern Classifier over Parser Features
CRF Extractor over Shallow Features
Triples
Learner
</figure>
<page confidence="0.997127">
119
</page>
<bodyText confidence="0.999902555555555">
ward links to automatically construct synonym
sets. Redirection pages are a natural choice, be-
cause they explicitly encode synonyms; for ex-
ample, “USA” is redirected to the article on the
“United States.” Backward links for a Wiki-
pedia entity such as the “Massachusetts Institute of
Technology” are hyperlinks pointing to this entity
from other articles; the anchor text of such links
(e.g., “MIT”) forms another source of synonyms.
</bodyText>
<subsectionHeader confidence="0.997654">
3.2 Matcher
</subsectionHeader>
<bodyText confidence="0.995477363636364">
The matcher constructs training data for the
learner component by heuristically matching
attribute-value pairs from Wikipedia articles con-
taining infoboxes with corresponding sentences in
the article. Given the article on “Stanford Univer-
sity,” for example, the matcher should associate
(established, 1891i with the sentence “The
university was founded in 1891 by ... ” Given a
Wikipedia page with an infobox, the matcher iter-
ates through all its attributes looking for a unique
sentence that contains references to both the sub-
ject of the article and the attribute value; these
noun phrases will be annotated arg1 and arg2
in the training set. The matcher considers a sen-
tence to contain the attribute value if the value or
its synonym is present. Matching the article sub-
ject, however, is more involved.
Matching Primary Entities: In order to match
shorthand terms like “MIT” with more complete
names, the matcher uses an ordered set of heuris-
tics like those of (Wu and Weld, 2007; Nguyen et
al., 2007):
</bodyText>
<listItem confidence="0.987775777777778">
• Full match: strings matching the full name of
the entity are selected.
• Synonym set match: strings appearing in the
entity’s synonym set are selected.
• Partial match: strings matching a prefix or suf-
fix of the entity’s name are selected. If the
full name contains punctuation, only a prefix
is allowed. For example, “Amherst” matches
“Amherst, Mass,” but “Mass” does not.
• Patterns of “the &lt;type&gt;”: The matcher first
identifies the type of the entity (e.g., “city” for
“Ithaca”), then instantiates the pattern to create
the string “the city.” Since the first sentence of
most Wikipedia articles is stylized (e.g. “The
city of Ithaca sits ... ”), a few patterns suffice
to extract most entity types.
• The most frequent pronoun: The matcher as-
sumes that the article’s most frequent pronoun
</listItem>
<bodyText confidence="0.999276384615385">
denotes the primary entity, e.g., “he” for the
page on “Albert Einstein.” This heuristic is
dropped when “it” is most common, because
the word is used in too many other ways.
When there are multiple matches to the primary
entity in a sentence, the matcher picks the one
which is closest to the matched infobox attribute
value in the parser dependency graph.
Matching Sentences: The matcher seeks a unique
sentence to match the attribute value. To produce
the best training set, the matcher performs three
filterings. First, it skips the attribute completely
when multiple sentences mention the value or its
synonym. Second, it rejects the sentence if the
subject and/or attribute value are not heads of the
noun phrases containing them. Third, it discards
the sentence if the subject and the attribute value
do not appear in the same clause (or in parent/child
clauses) in the parse tree.
Since Wikipedia’s Wikimarkup language is se-
mantically ambiguous, parsing infoboxes is sur-
prisingly complex. Fortunately, DBpedia (Auer
and Lehmann, 2007) provides a cleaned set of in-
foboxes from 1,027,744 articles. The matcher uses
this data for attribute values, generating a training
dataset with a total of 301,962 labeled sentences.
</bodyText>
<subsectionHeader confidence="0.999945">
3.3 Learning Extractors
</subsectionHeader>
<bodyText confidence="0.999998727272727">
We learn two kinds of extractors, one (WOEparse)
using features from dependency-parse trees and
the other (WOEp°s) limited to shallow features like
POS tags. WOEparse uses a pattern learner to
classify whether the shortest dependency path be-
tween two noun phrases indicates a semantic rela-
tion. In contrast, WOEp°s (like TextRunner) trains
a conditional random field (CRF) to output certain
text between noun phrases when the text denotes
such a relation. Neither extractor uses individual
words or lexical information for features.
</bodyText>
<subsectionHeader confidence="0.989821">
3.3.1 Extraction with Parser Features
</subsectionHeader>
<bodyText confidence="0.963365818181818">
Despite some evidence that parser-based features
have limited utility in IE (Jiang and Zhai, 2007),
we hoped dependency paths would improve preci-
sion on long sentences.
Shortest Dependency Path as Relation: Unless
otherwise noted, WOE uses the Stanford Parser
to create dependencies in the “collapsedDepen-
dency” format. Dependencies involving preposi-
tions, conjuncts as well as information about the
referent of relative clauses are collapsed to get
direct dependencies between content words. As
</bodyText>
<page confidence="0.970569">
120
</page>
<bodyText confidence="0.973475">
noted in (de Marneffe and Manning, 2008), this
collapsed format often yields simplified patterns
which are useful for relation extraction. Consider
the sentence:
Dan was not born in Berkeley.
</bodyText>
<equation confidence="0.8565704">
The Stanford Parser dependencies are:
nsubjpass(born-4, Dan-1)
auxpass(born-4, was-2)
neg(born-4, not-3)
prep in(born-4, Berkeley-6)
</equation>
<bodyText confidence="0.981871716417911">
where each atomic formula represents a binary de-
pendence from dependent token to the governor
token.
These dependencies form a directed graph,
(V, E), where each token is a vertex in V , and E
is the set of dependencies. For any pair of tokens,
such as “Dan” and “Berkeley”, we use the shortest
connecting path to represent the possible relation
between them:
−−−−−−−−−&apos; &apos;-−−−−−−
Dan nsubjpass born prep in Berkeley
We call such a path a corePath. While we will
see that corePaths are useful for indicating when
a relation exists between tokens, they don’t neces-
sarily capture the semantics of that relation. For
example, the path shown above doesn’t indicate
the existence of negation! In order to capture the
meaning of the relation, the learner augments the
corePath into a tree by adding all adverbial and
adjectival modifiers as well as dependencies like
“neg” and “auxpass”. We call the result an ex-
pandPath as shown below:
WOE traverses the expandPath with respect to the
token orders in the original sentence when out-
putting the final expression of rel.
Building a Database of Patterns: For each of the
301,962 sentences selected and annotated by the
matcher, the learner generates a corePath between
the tokens denoting the subject and the infobox at-
tribute value. Since we are interested in eventu-
ally extracting “subject, relation, object” triples,
the learner rejects corePaths that don’t start with
subject-like dependencies, such as nsubj, nsubj-
pass, partmod and rcmod. This leads to a collec-
tion of 259,046 corePaths.
To combat data sparsity and improve learn-
ing performance, the learner further generalizes
the corePaths in this set to create a smaller set
of generalized-corePaths. The idea is to elimi-
nate distinctions which are irrelevant for recog-
nizing (domain-independent) relations. Lexical
words in corePaths are replaced with their POS
tags. Further, all Noun POS tags and “PRP”
are abstracted to “N”, all Verb POS tags to “V”,
all Adverb POS tags to “RB” and all Adjective
POS tags to “J”. The preposition dependencies
such as “prep in” are generalized to “prep”. Take
−−−−−−−−−&apos; &apos;-−−−−−−
the corePath “Dan nsubjpass born prep in
Berkeley” for example, its generalized-corePath
is “N −−−−−−−−−&apos;
nsubjpass V &apos;-−−−−
prep N”. We call such
a generalized-corePath an extraction pattern. In
total, WOE builds a database (named DBp) of
15,333 distinct patterns and each pattern p is asso-
ciated with a frequency — the number of matching
sentences containing p. Specifically, 185 patterns
have fp &gt; 100 and 1929 patterns have fp &gt; 5.
Learning a Pattern Classifier: Given the large
number of patterns in DBp, we assume few valid
open extraction patterns are left behind. The
learner builds a simple pattern classifier, named
WOEparse, which checks whether the generalized-
corePath from a test triple is present in DBp, and
computes the normalized logarithmic frequency as
the probability3:
</bodyText>
<equation confidence="0.667729">
max(log(fp) − log(fmin), 0)
</equation>
<bodyText confidence="0.996767">
and abstracts to p=“N nsubjpass V &apos;-−−−−
&apos; prep
N”. It then queries DBp to retrieve the fre-
quency fp = 29112 and assigns a probabil-
ity of 0.95. Finally, WOEparse traverses the
triple’s expandPath to output the final expression
(Dan, wasNotBornIn, Berkeley). As shown
in the experiments on three corpora, WOEparse
achieves an F-measure which is between 72% to
91% greater than TextRunner’s.
</bodyText>
<subsectionHeader confidence="0.832946">
3.3.2 Extraction with Shallow Features
</subsectionHeader>
<bodyText confidence="0.8241214">
WOEparse has a dramatic performance improve-
ment over TextRunner. However, the improve-
ment comes at the cost of speed — TextRunner
3How to learn a more sophisticated weighting function is
left as a future topic.
</bodyText>
<equation confidence="0.997638">
w(p) = log(fmax) − log(fmin)
</equation>
<bodyText confidence="0.9725106">
where fmax (50,259 in this paper) is the maximal
frequency of pattern in DBp, and fmin (set 1 in
this work) is the controlling threshold that deter-
mines the minimal frequency of a valid pattern.
Take the previous sentence “Dan was not born
in Berkeley” for example. WOEparse first identi-
fies Dan as arg1 and Berkeley as arg2 based
on NP-chunking. It then computes the corePath
−−−−−−−−−&apos; &apos;-−−−−−−
“Dan nsubjpass born prep in Berkeley”
</bodyText>
<page confidence="0.882274">
121
</page>
<figure confidence="0.996297833333333">
0.0 0.2 0.4 0.6 0.8 1.0
precision
P/R Curve on WSJ
P/R Curve on Web
WOEparse
WOEpos
TextRunner
0.0 0.1 0.2 0.3 0.4 0.5 0.6
recall
P/R Curve on Wikipedia
WOEparse
WOEpos
TextRunner
0.0 0.1 0.2 0.3 0.4 0.5 0.6
recall
0.0 0.2 0.4 0.6 0.8 1.0
precision
WOEparse
WOEpos
TextRunner
0.0 0.1 0.2 0.3 0.4 0.5 0.6
recall
0.0 0.2 0.4 0.6 0.8 1.0
precision
</figure>
<figureCaption confidence="0.9983365">
Figure 2: WOEposperforms better than TextRunner, especially on precision. WOEparsedramatically im-
proves performance, especially on recall.
</figureCaption>
<bodyText confidence="0.999923451612903">
runs about 30X faster by only using shallow fea-
tures. Since high speed can be crucial when pro-
cessing Web-scale corpora, we additionally learn a
CRF extractor WOEpos based on shallow features
like POS-tags. In both cases, however, we gen-
erate training data from Wikipedia by matching
sentences with infoboxes, while TextRunner used
a small set of hand-written rules to label training
examples from the Penn Treebank.
We use the same matching sentence set behind
DBP to generate positive examples for WOEpos.
Specifically, for each matching sentence, we label
the subject and infobox attribute value as arg1
and arg2 to serve as the ends of a linear CRF
chain. Tokens involved in the expandPath are la-
beled as rel. Negative examples are generated
from random noun-phrase pairs in other sentences
when their generalized-CorePaths are not in DBP.
WOEpos uses the same learning algorithm and
selection of features as TextRunner: a two-order
CRF chain model is trained with the Mallet pack-
age (McCallum, 2002). WOEpos’s features include
POS-tags, regular expressions (e.g., for detecting
capitalization, punctuation, etc..), and conjunc-
tions of features occurring in adjacent positions
within six words to the left and to the right of the
current word.
As shown in the experiments, WOEpos achieves
an improved F-measure over TextRunner between
18% to 34% on three corpora, and this is mainly
due to the increase on precision.
</bodyText>
<sectionHeader confidence="0.999807" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9998935">
We used three corpora for experiments: WSJ from
Penn Treebank, Wikipedia, and the general Web.
For each dataset, we randomly selected 300 sen-
tences. Each sentence was examined by two peo-
ple to label all reasonable triples. These candidate
triples are mixed with pseudo-negative ones and
submitted to Amazon Mechanical Turk for veri-
fication. Each triple was examined by 5 Turk-
ers. We mark a triple’s final label as positive when
more than 3 Turkers marked them as positive.
</bodyText>
<subsectionHeader confidence="0.989231">
4.1 Overall Performance Analysis
</subsectionHeader>
<bodyText confidence="0.99991025">
In this section, we compare the overall perfor-
mance of WOEparse, WOEpos and TextRunner
(shared by the Turing Center at the University of
Washington). In particular, we are going to answer
the following questions: 1) How do these systems
perform against each other? 2) How does perfor-
mance vary w.r.t. sentence length? 3) How does
extraction speed vary w.r.t. sentence length?
</bodyText>
<sectionHeader confidence="0.341878" genericHeader="method">
Overall Performance Comparison
</sectionHeader>
<bodyText confidence="0.986111">
The detailed P/R curves are shown in Figure 2.
To have a close look, for each corpus, we ran-
domly divided the 300 sentences into 5 groups and
compared the best F-measures of three systems in
Figure 3. We can see that:
</bodyText>
<listItem confidence="0.937457833333333">
• WOEpos is better than TextRunner, especially
on precision. This is due to better training
data from Wikipedia via self-supervision. Sec-
tion 4.2 discusses this in more detail.
• WOEparse achieves the best performance, es-
pecially on recall. This is because the parser
</listItem>
<bodyText confidence="0.871583818181818">
features help to handle complicated and long-
distance relations in difficult sentences. In par-
ticular, WOEparse outputs 1.42 triples per sen-
tence on average, while WOEpos outputs 1.05
and TextRunner outputs 0.75.
Note that we measure TextRunner’s precision
&amp; recall differently than (Banko et al., 2007)
did. Specifically, we compute the precision &amp; re-
call based on all extractions, while Banko et al.
counted only concrete triples where arg1 is a
proper noun, arg2 is a proper noun or date, and
</bodyText>
<page confidence="0.996199">
122
</page>
<figureCaption confidence="0.8632865">
Figure 3: WOEposachieves an F-measure, which is
between 18% and 34% better than TextRunner’s.
</figureCaption>
<bodyText confidence="0.96187106060606">
WOEparseachieves an improvement between 72%
and 91% over TextRunner. The error bar indicates
one standard deviation.
the frequency of rel is over a threshold. Our ex-
periments show that focussing on concrete triples
generally improves precision at the expense of re-
call.4 Of course, one can apply a concreteness fil-
ter to any open extractor in order to trade recall for
precision.
The extraction errors by WOEparse can be cat-
egorized into four classes. We illustrate them
with the WSJ corpus. In total, WOEparse got
85 wrong extractions on WSJ, and they are
caused by: 1) Incorrect arg1 and/or arg2
from NP-Chunking (18.6%); 2) A erroneous de-
pendency parse from Stanford Parser (11.9%);
3) Inaccurate meaning (27.1%) — for exam-
ple, (she, isNominatedBy, PresidentBush) is
wrongly extracted from the sentence “If she is
nominated by President Bush ...”5; 4) A pattern
inapplicable for the test sentence (42.4%).
Note WOEparse is worse than WOEpos in the low
recall region. This is mainly due to parsing er-
rors (especially on long-distance dependencies),
which misleads WOEparse to extract false high-
confidence triples. WOEpos won’t suffer from such
parsing errors. Therefore it has better precision on
high-confidence extractions.
We noticed that TextRunner has a dip point
in the low recall region. There are two typical
errors responsible for this. A sample error of
the first type is (Sources, sold, theCompany)
extracted from the sentence “Sources said
</bodyText>
<footnote confidence="0.996440142857143">
4For example, consider the Wikipedia corpus. From
our 300 test sentences, TextRunner extracted 257 triples (at
72.0% precision) but only extracted 16 concrete triples (with
87.5% precision).
5These kind of errors might be excluded by monitor-
ing whether sentences contain words such as ‘if,’ ‘suspect,’
‘doubt,’ etc.. We leave this as a topic for the future.
</footnote>
<figureCaption confidence="0.9605325">
Figure 4: WOEparse’s F-measure decreases more
slowly with sentence length than WOEpos and Tex-
tRunner, due to its better handling of difficult sen-
tences using parser features.
</figureCaption>
<bodyText confidence="0.989117727272727">
he sold the company”, where “Sources” is
wrongly treated as the subject of the object
clause. A sample error of the second type is
(thisYear, willStarIn, theMovie) extracted
from the sentence “Coming up this year, Long
will star in the new movie.”, where “this year” is
wrongly treated as part of a compound subject.
Taking the WSJ corpus for example, at the dip
point with recall=0.002 and precision=0.059,
these two types of errors account for 70% of all
errors.
</bodyText>
<subsectionHeader confidence="0.688658">
Extraction Performance vs. Sentence Length
</subsectionHeader>
<bodyText confidence="0.999993230769231">
We tested how extractors’ performance varies
with sentence length; the results are shown in Fig-
ure 4. TextRunner and WOEpos have good perfor-
mance on short sentences, but their performance
deteriorates quickly as sentences get longer. This
is because long sentences tend to have compli-
cated and long-distance relations which are diffi-
cult for shallow features to capture. In contrast,
WOEparse’s performance decreases more slowly
w.r.t. sentence length. This is mainly because
parser features are more useful for handling diffi-
cult sentences and they help WOEparse to maintain
a good recall with only moderate loss of precision.
</bodyText>
<subsectionHeader confidence="0.711452">
Extraction Speed vs. Sentence Length
</subsectionHeader>
<bodyText confidence="0.999992090909091">
We also tested the extraction speed of different
extractors. We used Java for implementing the
extractors, and tested on a Linux platform with
a 2.4GHz CPU and 4G memory. On average, it
takes WOEparse 0.679 seconds to process a sen-
tence. For TextRunner and WOEpos, it only takes
0.022 seconds — 30X times faster. The detailed
extraction speed vs. sentence length is in Figure 5,
showing that TextRunner and WOEpos’s extraction
time grows approximately linearly with sentence
length, while WOEparse’s extraction time grows
</bodyText>
<page confidence="0.998901">
123
</page>
<figureCaption confidence="0.984740666666667">
Figure 5: Textrnner and WOEpos’s running time
seems to grow linearly with sentence length, while
WOEparse’s time grows quadratically.
</figureCaption>
<bodyText confidence="0.849792">
quadratically (R2 = 0.935) due to its reliance on
parsing.
</bodyText>
<subsectionHeader confidence="0.981397">
4.2 Self-supervision with Wikipedia Results
in Better Training Data
</subsectionHeader>
<bodyText confidence="0.999979033333333">
In this section, we consider how the process of
matching Wikipedia infobox values to correspond-
ing sentences results in better training data than
the hand-written rules used by TextRunner.
To compare with TextRunner, we tested four
different ways to generate training examples from
Wikipedia for learning a CRF extractor. Specif-
ically, positive and/or negative examples are se-
lected by TextRunner’s hand-written rules (tr for
short), by WOE’s heuristic of matching sentences
with infoboxes (w for short), or randomly (r for
short). We use CRF+h1_h2 to denote a particu-
lar approach, where “+” means positive samples,
“-” means negative samples, and hi E {tr, w, r}.
In particular, “+w” results in 221,205 positive ex-
amples based on the matching sentence set6. All
extractors are trained using about the same num-
ber of positive and negative examples. In contrast,
TextRunner was trained with 91,687 positive ex-
amples and 96,795 negative examples generated
from the WSJ dataset in Penn Treebank.
The CRF extractors are trained using the same
learning algorithm and feature selection as Tex-
tRunner. The detailed P/R curves are in Fig-
ure 6, showing that using WOE heuristics to la-
bel positive examples gives the biggest perfor-
mance boost. CRF+tr_tr (trained using TextRun-
ner’s heuristics) is slightly worse than TextRunner.
Most likely, this is because TextRunner’s heuris-
tics rely on parse trees to label training examples,
</bodyText>
<footnote confidence="0.984032333333333">
6This number is smaller than the total number of
corePaths (259,046) because we require arg1 to appear be-
fore arg2 in a sentence — as specified by TextRunner.
</footnote>
<bodyText confidence="0.9831415">
and the Stanford parse on Wikipedia is less accu-
rate than the gold parse on WSJ.
</bodyText>
<subsectionHeader confidence="0.982283">
4.3 Design Desiderata of WOEparse
</subsectionHeader>
<bodyText confidence="0.999994727272727">
There are two interesting design choices in
WOEparse: 1) whether to require arg1 to appear
before arg2 (denoted as 1�2) in the sentence;
2) whether to allow corePaths to contain prepo-
sitional phrase (PP) attachments (denoted as PPa).
We tested how they affect the extraction perfor-
mance; the results are shown in Figure 7.
We can see that filtering PP attachments (PPa)
gives a large precision boost with a noticeable loss
in recall; enforcing a lexical ordering of relation
arguments (1�2) yields a smaller improvement in
precision with small loss in recall. Take the WSJ
corpus for example: setting 1≺2 and PPa achieves
a precision of 0.792 (with recall of 0.558). By
changing 1≺2 to 1-2, the precision decreases to
0.773 (with recall of 0.595). By changing PPa to
PPa and keeping 1�2, the precision decreases to
0.642 (with recall of 0.687) — in particular, if we
use gold parse, the precision decreases to 0.672
(with recall of 0.685). We set 1≺2 and PPa as de-
fault in WOEparse as a logical consequence of our
preference for high precision over high recall.
</bodyText>
<subsectionHeader confidence="0.867213">
4.3.1 Different parsing options
</subsectionHeader>
<bodyText confidence="0.999846">
We also tested how different parsing might ef-
fect WOEparse’s performance. We used three pars-
ing options on the WSJ dataset: Stanford parsing,
CJ50 parsing (Charniak and Johnson, 2005), and
the gold parses from the Penn Treebank. The Stan-
ford Parser is used to derive dependencies from
CJ50 and gold parse trees. Figure 8 shows the
detailed P/R curves. We can see that although
today’s statistical parsers make errors, they have
negligible effect on the accuracy of WOE.
</bodyText>
<sectionHeader confidence="0.999882" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.992164307692308">
Open or Traditional Information Extraction:
Most existing work on IE is relation-specific.
Occurrence-statistical models (Agichtein and Gra-
vano, 2000; M. Ciaramita, 2005), graphical mod-
els (Peng and McCallum, 2004; Poon and Domin-
gos, 2008), and kernel-based methods (Bunescu
and R.Mooney, 2005) have been studied. Snow
et al. (Snow et al., 2005) utilize WordNet to
learn dependency path patterns for extracting the
hypernym relation from text. Some seed-based
frameworks are proposed for open-domain extrac-
tion (Pasca, 2008; Davidov et al., 2007; Davi-
dov and Rappoport, 2008). These works focus
</bodyText>
<page confidence="0.989186">
124
</page>
<figure confidence="0.998133166666667">
0.0 0.2 0.4 0.6 0.8 1.0
precision
P/R Curve on WSJ
P/R Curve on Web
CRF+w−w=WOEpos
CRF+w−tr
CRF+w−r
CRF+tr−tr
TextRunner
0.0 0.1 0.2 0.3 0.4
recall
P/R Curve on Wikipedia
0.0 0.1 0.2 0.3 0.4
recall
CRF+w−w=WOEpos
CRF+w−tr
CRF+w−r
CRF+tr−tr
TextRunner
0.0 0.2 0.4 0.6 0.8 1.0
precision
0.0 0.1 0.2 0.3 0.4
recall
CRF+w−w=WOEpos
CRF+w−tr
CRF+w−r
CRF+tr−tr
TextRunner
0.0 0.2 0.4 0.6 0.8 1.0
precision
</figure>
<figureCaption confidence="0.924754">
Figure 6: Matching sentences with Wikipedia infoboxes results in better training data than the hand-
written rules used by TextRunner.
Figure 7: Filtering prepositional phrase attachments (PPa) shows a strong boost to precision, and we see
a smaller boost from enforcing a lexical ordering of relation arguments (1�2).
</figureCaption>
<figure confidence="0.570966">
P/R Curve on WSJ
</figure>
<figureCaption confidence="0.913341">
Figure 8: Although today’s statistical parsers
</figureCaption>
<bodyText confidence="0.989326583333334">
make errors, they have negligible effect on the
accuracy of WOE compared to operation on gold
standard, human-annotated data.
on identifying general relations such as class at-
tributes, while open IE aims to extract relation
instances from given sentences. Another seed-
based system StatSnowball (Zhu et al., 2009)
can perform both relation-specific and open IE
by iteratively generating weighted extraction pat-
terns. Different from WOE, StatSnowball only em-
ploys shallow features and uses L1-normalization
to weight patterns. Shinyama and Sekine pro-
posed the “preemptive IE” framework to avoid
relation-specificity (Shinyama and Sekine, 2006).
They first group documents based on pairwise
vector-space clustering, then apply an additional
clustering to group entities based on documents
clusters. The two clustering steps make it dif-
ficult to meet the scalability requirement neces-
sary to process the Web. Mintz et al. (Mintz et
al., 2009) uses Freebase to provide distant su-
pervision for relation extraction. They applied
a similar heuristic by matching Freebase tuples
with unstructured sentences (Wikipedia articles in
their experiments) to create features for learning
relation extractors. Matching Freebase with ar-
bitrary sentences instead of matching Wikipedia
infobox with corresponding Wikipedia articles
will potentially increase the size of matched sen-
tences at a cost of accuracy. Also, their learned
extractors are relation-specific. Alan Akbik et
al. (Akbik and Broß, 2009) annotated 10,000 sen-
tences parsed with LinkGrammar and selected 46
general linkpaths as patterns for relation extrac-
tion. In contrast, WOE learns 15,333 general pat-
terns based on an automatically annotated set of
</bodyText>
<figure confidence="0.9958802">
0.0 0.1 0.2 0.3 0.4 0.5 0.6
recall
precision
0.4 0.6 0.8 1.0
WOEstanford
parse =WOEparse
parse
WOECJ50
parse
WOEgold
</figure>
<page confidence="0.993202">
125
</page>
<bodyText confidence="0.999151425531915">
301,962 Wikipedia sentences. The KNext sys-
tem (Durme and Schubert, 2008) performs open
knowledge extraction via significant heuristics. Its
output is knowledge represented as logical state-
ments instead of information represented as seg-
mented text fragments.
Information Extraction with Wikipedia: The
YAGO system (Suchanek et al., 2007) extends
WordNet using facts extracted from Wikipedia
categories. It only targets a limited number of pre-
defined relations. Nakayama et al. (Nakayama and
Nishio, 2008) parse selected Wikipedia sentences
and perform extraction over the phrase structure
trees based on several handcrafted patterns. Wu
and Weld proposed the KYLIN system (Wu and
Weld, 2007; Wu et al., 2008) which has the same
spirit of matching Wikipedia sentences with in-
foboxes to learn CRF extractors. However, it
only works for relations defined in Wikipedia in-
foboxes.
Shallow or Deep Parsing: Shallow features, like
POS tags, enable fast extraction over large-scale
corpora (Davidov et al., 2007; Banko et al., 2007).
Deep features are derived from parse trees with
the hope of training better extractors (Zhang et
al., 2006; Zhao and Grishman, 2005; Bunescu
and Mooney, 2005; Wang, 2008). Jiang and
Zhai (Jiang and Zhai, 2007) did a systematic ex-
ploration of the feature space for relation extrac-
tion on the ACE corpus. Their results showed lim-
ited advantage of parser features over shallow fea-
tures for IE. However, our results imply that ab-
stracted dependency path features are highly in-
formative for open IE. There might be several rea-
sons for the different observations. First, Jiang and
Zhai’s results are tested for traditional IE where lo-
cal lexicalized tokens might contain sufficient in-
formation to trigger a correct classification. The
situation is different when features are completely
unlexicalized in open IE. Second, as they noted,
many relations defined in the ACE corpus are
short-range relations which are easier for shallow
features to capture. In practical corpora like the
general Web, many sentences contain complicated
long-distance relations. As we have shown ex-
perimentally, parser features are more powerful in
handling such cases.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999716372093023">
This paper introduces WOE, a new approach to
open IE that uses self-supervised learning over un-
lexicalized features, based on a heuristic match
between Wikipedia infoboxes and corresponding
text. WOE can run in two modes: a CRF extrac-
tor (WOEp°s) trained with shallow features like
POS tags; a pattern classfier (WOEp&amp;quot;&apos;) learned
from dependency path patterns. Comparing with
TextRunner, WOEp°s runs at the same speed, but
achieves an F-measure which is between 18% and
34% greater on three corpora; WOEp&amp;quot;&apos; achieves
an F-measure which is between 72% and 91%
higher than that of TextRunner, but runs about
30X times slower due to the time required for
parsing.
Our experiments uncovered two sources of
WOE’s strong performance: 1) the Wikipedia
heuristic is responsible for the bulk of WOE’s im-
proved accuracy, but 2) dependency-parse features
are highly informative when performing unlexi-
calized extraction. We note that this second con-
clusion disagrees with the findings in (Jiang and
Zhai, 2007).
In the future, we plan to run WOE over the bil-
lion document CMU ClueWeb09 corpus to com-
pile a giant knowledge base for distribution to the
NLP community. There are several ways to further
improve WOE’s performance. Other data sources,
such as Freebase, could be used to create an ad-
ditional training dataset via self-supervision. For
example, Mintz et al. consider all sentences con-
taining both the subject and object of a Freebase
record as matching sentences (Mintz et al., 2009);
while they use this data to learn relation-specific
extractors, one could also learn an open extrac-
tor. We are also interested in merging lexical-
ized and open extraction methods; the use of some
domain-specific lexical features might help to im-
prove WOE’s practical performance, but the best
way to do this is unclear. Finally, we wish to com-
bine WOEpII&apos; with WOEp°s (&apos;.g., with voting) to
produce a system which maximizes precision at
low recall.
</bodyText>
<sectionHeader confidence="0.996552" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.903437909090909">
We thank Oren Etzioni and Michele Banko from
Turing Center at the University of Washington for
providing the code of their software and useful dis-
cussions. We also thank Alan Ritter, Mausam,
Peng Dai, Raphael Hoffmann, Xiao Ling, Ste-
fan Schoenmackers, Andrey Kolobov and Daniel
Suskin for valuable comments. This material is
based upon work supported by the WRF / TJ Cable
Professorship, a gift from Google and by the Air
Force Research Laboratory (AFRL) under prime
contract no. FA8750-09-C-0181. Any opinions,
</bodyText>
<page confidence="0.995664">
126
</page>
<bodyText confidence="0.99667375">
findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s)
and do not necessarily reflect the view of the Air
Force Research Laboratory (AFRL).
</bodyText>
<sectionHeader confidence="0.9959" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999824968421053">
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections.
In ICDL.
Alan Akbik and J¨ugen Broß. 2009. Wanderlust: Ex-
tracting semantic relations from natural language
text using dependency grammar patterns. In WWW
Workshop.
S¨oren Auer and Jens Lehmann. 2007. What have inns-
bruck and leipzig in common? extracting semantics
from wiki content. In ESWC.
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence kernels for relation extraction. In
NIPS.
R. Bunescu and R.Mooney. 2005. A shortest
path dependency kernel for relation extraction. In
HLT/EMNLP.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In ACL.
M. Craven, D. DiPasquo, D. Freitag, A. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1998. Learn-
ing to extract symbolic knowledge from the world
wide web. In AAAI.
Dmitry Davidov and Ari Rappoport. 2008. Unsuper-
vised discovery of generic relationships using pat-
tern clusters and its evaluation by automatically gen-
erated sat analogy questions. In ACL.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In ACL.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
http://nlp.stanford.edu/downloads/lex-parser.shtml.
Benjamin Van Durme and Lenhart K. Schubert. 2008.
Open knowledge extraction using compositional
language processing. In STEP.
R. Hoffmann, C. Zhang, and D. Weld. 2010. Learning
5000 relational extractors. In ACL.
Jing Jiang and ChengXiang Zhai. 2007. A systematic
exploration of the feature space for relation extrac-
tion. In HLT/NAACL.
A. Gangemi M. Ciaramita. 2005. Unsupervised learn-
ing of semantic relations between concepts of a
molecular biology ontology. In IJCAI.
Andrew Kachites McCallum. 2002. Mallet:
A machine learning for language toolkit. In
http://mallet.cs.umass.edu.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In ACL-IJCNLP.
T. H. Kotaro Nakayama and S. Nishio. 2008. Wiki-
pedia link structure and text mining for semantic re-
lation extraction. In CEUR Workshop.
Dat P.T Nguyen, Yutaka Matsuo, and Mitsuru Ishizuka.
2007. Exploiting syntactic and semantic informa-
tion for relation extraction from wikipedia. In
IJCAI07-TextLinkWS.
Marius Pasca. 2008. Turning web text and search
queries into factual knowledge: Hierarchical class
attribute extraction. In AAAI.
Fuchun Peng and Andrew McCallum. 2004. Accurate
Information Extraction from Research Papers using
Conditional Random Fields. In HLT-NAACL.
Hoifung Poon and Pedro Domingos. 2008. Joint Infer-
ence in Information Extraction. In AAAI.
Y. Shinyama and S. Sekine. 2006. Preemptive infor-
mation extraction using unristricted relation discov-
ery. In HLT-NAACL.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge - unifying WordNet and Wikipedia. In WWW.
Mengqiu Wang. 2008. A re-examination of depen-
dency path kernels for relation extraction. In IJC-
NLP.
Fei Wu and Daniel Weld. 2007. Autonomouslly Se-
mantifying Wikipedia. In CIKM.
Fei Wu, Raphael Hoffmann, and Danel S. Weld. 2008.
Information extraction from Wikipedia: Moving
down the long tail. In KDD.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In ACL.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In ACL.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and
Ji-Rong Wen. 2009. Statsnowball: a statistical ap-
proach to extracting entity relationships. In WWW.
</reference>
<page confidence="0.997303">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970218">
<title confidence="0.999652">Open Information Extraction using Wikipedia</title>
<author confidence="0.999867">Fei Wu</author>
<affiliation confidence="0.999971">University of Washington</affiliation>
<address confidence="0.999683">Seattle, WA, USA</address>
<email confidence="0.999843">wufei@cs.washington.edu</email>
<author confidence="0.999917">Daniel S Weld</author>
<affiliation confidence="0.99996">University of Washington</affiliation>
<address confidence="0.999675">Seattle, WA, USA</address>
<email confidence="0.999865">weld@cs.washington.edu</email>
<abstract confidence="0.998900769230769">Information-extraction (IE) systems seek to distill semantic relations from naturallanguage text, but most systems use supervised learning of relation-specific examples and are thus limited by the availability of data. systems such as TextRunner, on the other hand, aim to handle the unbounded number of relations found on the Web. But how well can these open systems perform? paper presents an open IE system which improves dramatically on TextRunner’s and recall. The key to performance is a novel form of self-supervised learning for open extractors — using heuristic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data. Like TextRunner, extractor eschews lexicalized features and handles an unbounded set of semantic operate in two modes: when restricted to POS tag features, it runs as quickly as TextRunner, but when set to use dependency-parse features its precision and recall rise even higher.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>L Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In ICDL.</booktitle>
<contexts>
<context position="28464" citStr="Agichtein and Gravano, 2000" startWordPosition="4512" endWordPosition="4516"> tested how different parsing might effect WOEparse’s performance. We used three parsing options on the WSJ dataset: Stanford parsing, CJ50 parsing (Charniak and Johnson, 2005), and the gold parses from the Penn Treebank. The Stanford Parser is used to derive dependencies from CJ50 and gold parse trees. Figure 8 shows the detailed P/R curves. We can see that although today’s statistical parsers make errors, they have negligible effect on the accuracy of WOE. 5 Related Work Open or Traditional Information Extraction: Most existing work on IE is relation-specific. Occurrence-statistical models (Agichtein and Gravano, 2000; M. Ciaramita, 2005), graphical models (Peng and McCallum, 2004; Poon and Domingos, 2008), and kernel-based methods (Bunescu and R.Mooney, 2005) have been studied. Snow et al. (Snow et al., 2005) utilize WordNet to learn dependency path patterns for extracting the hypernym relation from text. Some seed-based frameworks are proposed for open-domain extraction (Pasca, 2008; Davidov et al., 2007; Davidov and Rappoport, 2008). These works focus 124 0.0 0.2 0.4 0.6 0.8 1.0 precision P/R Curve on WSJ P/R Curve on Web CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.1 0.2 0.3 0.4 recall P/</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>E. Agichtein and L. Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In ICDL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Akbik</author>
<author>J¨ugen Broß</author>
</authors>
<title>Wanderlust: Extracting semantic relations from natural language text using dependency grammar patterns.</title>
<date>2009</date>
<booktitle>In WWW Workshop.</booktitle>
<contexts>
<context position="31179" citStr="Akbik and Broß, 2009" startWordPosition="4924" endWordPosition="4927">equirement necessary to process the Web. Mintz et al. (Mintz et al., 2009) uses Freebase to provide distant supervision for relation extraction. They applied a similar heuristic by matching Freebase tuples with unstructured sentences (Wikipedia articles in their experiments) to create features for learning relation extractors. Matching Freebase with arbitrary sentences instead of matching Wikipedia infobox with corresponding Wikipedia articles will potentially increase the size of matched sentences at a cost of accuracy. Also, their learned extractors are relation-specific. Alan Akbik et al. (Akbik and Broß, 2009) annotated 10,000 sentences parsed with LinkGrammar and selected 46 general linkpaths as patterns for relation extraction. In contrast, WOE learns 15,333 general patterns based on an automatically annotated set of 0.0 0.1 0.2 0.3 0.4 0.5 0.6 recall precision 0.4 0.6 0.8 1.0 WOEstanford parse =WOEparse parse WOECJ50 parse WOEgold 125 301,962 Wikipedia sentences. The KNext system (Durme and Schubert, 2008) performs open knowledge extraction via significant heuristics. Its output is knowledge represented as logical statements instead of information represented as segmented text fragments. Informa</context>
</contexts>
<marker>Akbik, Broß, 2009</marker>
<rawString>Alan Akbik and J¨ugen Broß. 2009. Wanderlust: Extracting semantic relations from natural language text using dependency grammar patterns. In WWW Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Jens Lehmann</author>
</authors>
<title>What have innsbruck and leipzig in common? extracting semantics from wiki content.</title>
<date>2007</date>
<booktitle>In ESWC.</booktitle>
<contexts>
<context position="6113" citStr="Auer and Lehmann, 2007" startWordPosition="935" endWordPosition="938">r without direct supervision, i.e. without annotated training examples or hand-crafted patterns. Our input is Wikipedia, a collaboratively-constructed encyclopedia2. As output, WOE produces an unlexicalized and relation-independent open extractor. Our objective is an extractor which generalizes beyond Wikipedia, handling other corpora such as the general Web. 3 Wikipedia-based Open IE The key idea underlying WOE is the automatic construction of training examples by heuristically matching Wikipedia infobox values and corresponding text; these examples are used to generate 2We also use DBpedia (Auer and Lehmann, 2007) as a collection of conveniently parsed Wikipedia infoboxes Figure 1: Architecture of WOE. an unlexicalized, relation-independent (open) extractor. As shown in Figure 1, WOE has three main components: preprocessor, matcher, and learner. 3.1 Preprocessor The preprocessor converts the raw Wikipedia text into a sequence of sentences, attaches NLP annotations, and builds synonym sets for key entities. The resulting data is fed to the matcher, described in Section 3.2, which generates the training set. Sentence Splitting: The preprocessor first renders each Wikipedia article into HTML, then splits </context>
<context position="11539" citStr="Auer and Lehmann, 2007" startWordPosition="1796" endWordPosition="1799">tch the attribute value. To produce the best training set, the matcher performs three filterings. First, it skips the attribute completely when multiple sentences mention the value or its synonym. Second, it rejects the sentence if the subject and/or attribute value are not heads of the noun phrases containing them. Third, it discards the sentence if the subject and the attribute value do not appear in the same clause (or in parent/child clauses) in the parse tree. Since Wikipedia’s Wikimarkup language is semantically ambiguous, parsing infoboxes is surprisingly complex. Fortunately, DBpedia (Auer and Lehmann, 2007) provides a cleaned set of infoboxes from 1,027,744 articles. The matcher uses this data for attribute values, generating a training dataset with a total of 301,962 labeled sentences. 3.3 Learning Extractors We learn two kinds of extractors, one (WOEparse) using features from dependency-parse trees and the other (WOEp°s) limited to shallow features like POS tags. WOEparse uses a pattern learner to classify whether the shortest dependency path between two noun phrases indicates a semantic relation. In contrast, WOEp°s (like TextRunner) trains a conditional random field (CRF) to output certain t</context>
</contexts>
<marker>Auer, Lehmann, 2007</marker>
<rawString>S¨oren Auer and Jens Lehmann. 2007. What have innsbruck and leipzig in common? extracting semantics from wiki content. In ESWC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open information extraction from the Web. In Procs. of IJCAI.</title>
<date>2007</date>
<contexts>
<context position="2112" citStr="Banko et al., 2007" startWordPosition="316" endWordPosition="319">tasks such as question answering, ontology learning, and summarization. The vast majority of IE work uses supervised learning of relationspecific examples. For example, the WebKB project (Craven et al., 1998) used labeled examples of the courses-taught-by relation to induce rules for identifying additional instances of the relation. While these methods can achieve high precision and recall, they are limited by the availability of training data and are unlikely to scale to the thousands of relations found in text on the Web. An alternative paradigm, Open IE, pioneered by the TextRunner system (Banko et al., 2007) and the “preemptive IE” in (Shinyama and Sekine, 2006), aims to handle an unbounded number of relations and run quickly enough to process Webscale corpora. Domain independence is achieved by extracting the relation name as well as its two arguments. Most open IE systems use selfsupervised learning, in which automatic heuristics generate labeled data for training the extractor. For example, TextRunner uses a small set of handwritten rules to heuristically label training examples from sentences in the Penn Treebank. This paper presents WOE (Wikipedia-based Open Extractor), the first system that</context>
<context position="20804" citStr="Banko et al., 2007" startWordPosition="3280" endWordPosition="3283">s of three systems in Figure 3. We can see that: • WOEpos is better than TextRunner, especially on precision. This is due to better training data from Wikipedia via self-supervision. Section 4.2 discusses this in more detail. • WOEparse achieves the best performance, especially on recall. This is because the parser features help to handle complicated and longdistance relations in difficult sentences. In particular, WOEparse outputs 1.42 triples per sentence on average, while WOEpos outputs 1.05 and TextRunner outputs 0.75. Note that we measure TextRunner’s precision &amp; recall differently than (Banko et al., 2007) did. Specifically, we compute the precision &amp; recall based on all extractions, while Banko et al. counted only concrete triples where arg1 is a proper noun, arg2 is a proper noun or date, and 122 Figure 3: WOEposachieves an F-measure, which is between 18% and 34% better than TextRunner’s. WOEparseachieves an improvement between 72% and 91% over TextRunner. The error bar indicates one standard deviation. the frequency of rel is over a threshold. Our experiments show that focussing on concrete triples generally improves precision at the expense of recall.4 Of course, one can apply a concretenes</context>
<context position="32537" citStr="Banko et al., 2007" startWordPosition="5132" endWordPosition="5135"> only targets a limited number of predefined relations. Nakayama et al. (Nakayama and Nishio, 2008) parse selected Wikipedia sentences and perform extraction over the phrase structure trees based on several handcrafted patterns. Wu and Weld proposed the KYLIN system (Wu and Weld, 2007; Wu et al., 2008) which has the same spirit of matching Wikipedia sentences with infoboxes to learn CRF extractors. However, it only works for relations defined in Wikipedia infoboxes. Shallow or Deep Parsing: Shallow features, like POS tags, enable fast extraction over large-scale corpora (Davidov et al., 2007; Banko et al., 2007). Deep features are derived from parse trees with the hope of training better extractors (Zhang et al., 2006; Zhao and Grishman, 2005; Bunescu and Mooney, 2005; Wang, 2008). Jiang and Zhai (Jiang and Zhai, 2007) did a systematic exploration of the feature space for relation extraction on the ACE corpus. Their results showed limited advantage of parser features over shallow features for IE. However, our results imply that abstracted dependency path features are highly informative for open IE. There might be several reasons for the different observations. First, Jiang and Zhai’s results are test</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>M. Banko, M. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open information extraction from the Web. In Procs. of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Subsequence kernels for relation extraction.</title>
<date>2005</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="32696" citStr="Bunescu and Mooney, 2005" startWordPosition="5158" endWordPosition="5161">ion over the phrase structure trees based on several handcrafted patterns. Wu and Weld proposed the KYLIN system (Wu and Weld, 2007; Wu et al., 2008) which has the same spirit of matching Wikipedia sentences with infoboxes to learn CRF extractors. However, it only works for relations defined in Wikipedia infoboxes. Shallow or Deep Parsing: Shallow features, like POS tags, enable fast extraction over large-scale corpora (Davidov et al., 2007; Banko et al., 2007). Deep features are derived from parse trees with the hope of training better extractors (Zhang et al., 2006; Zhao and Grishman, 2005; Bunescu and Mooney, 2005; Wang, 2008). Jiang and Zhai (Jiang and Zhai, 2007) did a systematic exploration of the feature space for relation extraction on the ACE corpus. Their results showed limited advantage of parser features over shallow features for IE. However, our results imply that abstracted dependency path features are highly informative for open IE. There might be several reasons for the different observations. First, Jiang and Zhai’s results are tested for traditional IE where local lexicalized tokens might contain sufficient information to trigger a correct classification. The situation is different when </context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. Subsequence kernels for relation extraction. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>R Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="32696" citStr="Bunescu and Mooney, 2005" startWordPosition="5158" endWordPosition="5161">ion over the phrase structure trees based on several handcrafted patterns. Wu and Weld proposed the KYLIN system (Wu and Weld, 2007; Wu et al., 2008) which has the same spirit of matching Wikipedia sentences with infoboxes to learn CRF extractors. However, it only works for relations defined in Wikipedia infoboxes. Shallow or Deep Parsing: Shallow features, like POS tags, enable fast extraction over large-scale corpora (Davidov et al., 2007; Banko et al., 2007). Deep features are derived from parse trees with the hope of training better extractors (Zhang et al., 2006; Zhao and Grishman, 2005; Bunescu and Mooney, 2005; Wang, 2008). Jiang and Zhai (Jiang and Zhai, 2007) did a systematic exploration of the feature space for relation extraction on the ACE corpus. Their results showed limited advantage of parser features over shallow features for IE. However, our results imply that abstracted dependency path features are highly informative for open IE. There might be several reasons for the different observations. First, Jiang and Zhai’s results are tested for traditional IE where local lexicalized tokens might contain sufficient information to trigger a correct classification. The situation is different when </context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>R. Bunescu and R.Mooney. 2005. A shortest path dependency kernel for relation extraction. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28013" citStr="Charniak and Johnson, 2005" startWordPosition="4442" endWordPosition="4445">ecall of 0.558). By changing 1≺2 to 1-2, the precision decreases to 0.773 (with recall of 0.595). By changing PPa to PPa and keeping 1�2, the precision decreases to 0.642 (with recall of 0.687) — in particular, if we use gold parse, the precision decreases to 0.672 (with recall of 0.685). We set 1≺2 and PPa as default in WOEparse as a logical consequence of our preference for high precision over high recall. 4.3.1 Different parsing options We also tested how different parsing might effect WOEparse’s performance. We used three parsing options on the WSJ dataset: Stanford parsing, CJ50 parsing (Charniak and Johnson, 2005), and the gold parses from the Penn Treebank. The Stanford Parser is used to derive dependencies from CJ50 and gold parse trees. Figure 8 shows the detailed P/R curves. We can see that although today’s statistical parsers make errors, they have negligible effect on the accuracy of WOE. 5 Related Work Open or Traditional Information Extraction: Most existing work on IE is relation-specific. Occurrence-statistical models (Agichtein and Gravano, 2000; M. Ciaramita, 2005), graphical models (Peng and McCallum, 2004; Poon and Domingos, 2008), and kernel-based methods (Bunescu and R.Mooney, 2005) hav</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Craven</author>
<author>D DiPasquo</author>
<author>D Freitag</author>
<author>A McCallum</author>
<author>T Mitchell</author>
<author>K Nigam</author>
<author>S Slattery</author>
</authors>
<title>Learning to extract symbolic knowledge from the world wide web. In</title>
<date>1998</date>
<publisher>AAAI.</publisher>
<contexts>
<context position="1701" citStr="Craven et al., 1998" startWordPosition="249" endWordPosition="252">icted to POS tag features, it runs as quickly as TextRunner, but when set to use dependency-parse features its precision and recall rise even higher. 1 Introduction The problem of information-extraction (IE), generating relational data from natural-language text, has received increasing attention in recent years. A large, high-quality repository of extracted tuples can potentially benefit a wide range of NLP tasks such as question answering, ontology learning, and summarization. The vast majority of IE work uses supervised learning of relationspecific examples. For example, the WebKB project (Craven et al., 1998) used labeled examples of the courses-taught-by relation to induce rules for identifying additional instances of the relation. While these methods can achieve high precision and recall, they are limited by the availability of training data and are unlikely to scale to the thousands of relations found in text on the Web. An alternative paradigm, Open IE, pioneered by the TextRunner system (Banko et al., 2007) and the “preemptive IE” in (Shinyama and Sekine, 2006), aims to handle an unbounded number of relations and run quickly enough to process Webscale corpora. Domain independence is achieved </context>
</contexts>
<marker>Craven, DiPasquo, Freitag, McCallum, Mitchell, Nigam, Slattery, 1998</marker>
<rawString>M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. Mitchell, K. Nigam, and S. Slattery. 1998. Learning to extract symbolic knowledge from the world wide web. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated sat analogy questions.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28890" citStr="Davidov and Rappoport, 2008" startWordPosition="4578" endWordPosition="4582">igible effect on the accuracy of WOE. 5 Related Work Open or Traditional Information Extraction: Most existing work on IE is relation-specific. Occurrence-statistical models (Agichtein and Gravano, 2000; M. Ciaramita, 2005), graphical models (Peng and McCallum, 2004; Poon and Domingos, 2008), and kernel-based methods (Bunescu and R.Mooney, 2005) have been studied. Snow et al. (Snow et al., 2005) utilize WordNet to learn dependency path patterns for extracting the hypernym relation from text. Some seed-based frameworks are proposed for open-domain extraction (Pasca, 2008; Davidov et al., 2007; Davidov and Rappoport, 2008). These works focus 124 0.0 0.2 0.4 0.6 0.8 1.0 precision P/R Curve on WSJ P/R Curve on Web CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.1 0.2 0.3 0.4 recall P/R Curve on Wikipedia 0.0 0.1 0.2 0.3 0.4 recall CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.2 0.4 0.6 0.8 1.0 precision 0.0 0.1 0.2 0.3 0.4 recall CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.2 0.4 0.6 0.8 1.0 precision Figure 6: Matching sentences with Wikipedia infoboxes results in better training data than the handwritten rules used by TextRunner. Figure 7: Filtering prepositional phrase att</context>
</contexts>
<marker>Davidov, Rappoport, 2008</marker>
<rawString>Dmitry Davidov and Ari Rappoport. 2008. Unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated sat analogy questions. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
<author>Moshe Koppel</author>
</authors>
<title>Fully unsupervised discovery of conceptspecific relationships by web mining.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28860" citStr="Davidov et al., 2007" startWordPosition="4574" endWordPosition="4577">errors, they have negligible effect on the accuracy of WOE. 5 Related Work Open or Traditional Information Extraction: Most existing work on IE is relation-specific. Occurrence-statistical models (Agichtein and Gravano, 2000; M. Ciaramita, 2005), graphical models (Peng and McCallum, 2004; Poon and Domingos, 2008), and kernel-based methods (Bunescu and R.Mooney, 2005) have been studied. Snow et al. (Snow et al., 2005) utilize WordNet to learn dependency path patterns for extracting the hypernym relation from text. Some seed-based frameworks are proposed for open-domain extraction (Pasca, 2008; Davidov et al., 2007; Davidov and Rappoport, 2008). These works focus 124 0.0 0.2 0.4 0.6 0.8 1.0 precision P/R Curve on WSJ P/R Curve on Web CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.1 0.2 0.3 0.4 recall P/R Curve on Wikipedia 0.0 0.1 0.2 0.3 0.4 recall CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.2 0.4 0.6 0.8 1.0 precision 0.0 0.1 0.2 0.3 0.4 recall CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.2 0.4 0.6 0.8 1.0 precision Figure 6: Matching sentences with Wikipedia infoboxes results in better training data than the handwritten rules used by TextRunner. Figure 7: Filt</context>
<context position="32516" citStr="Davidov et al., 2007" startWordPosition="5128" endWordPosition="5131">kipedia categories. It only targets a limited number of predefined relations. Nakayama et al. (Nakayama and Nishio, 2008) parse selected Wikipedia sentences and perform extraction over the phrase structure trees based on several handcrafted patterns. Wu and Weld proposed the KYLIN system (Wu and Weld, 2007; Wu et al., 2008) which has the same spirit of matching Wikipedia sentences with infoboxes to learn CRF extractors. However, it only works for relations defined in Wikipedia infoboxes. Shallow or Deep Parsing: Shallow features, like POS tags, enable fast extraction over large-scale corpora (Davidov et al., 2007; Banko et al., 2007). Deep features are derived from parse trees with the hope of training better extractors (Zhang et al., 2006; Zhao and Grishman, 2005; Bunescu and Mooney, 2005; Wang, 2008). Jiang and Zhai (Jiang and Zhai, 2007) did a systematic exploration of the feature space for relation extraction on the ACE corpus. Their results showed limited advantage of parser features over shallow features for IE. However, our results imply that abstracted dependency path features are highly informative for open IE. There might be several reasons for the different observations. First, Jiang and Zh</context>
</contexts>
<marker>Davidov, Rappoport, Koppel, 2007</marker>
<rawString>Dmitry Davidov, Ari Rappoport, and Moshe Koppel. 2007. Fully unsupervised discovery of conceptspecific relationships by web mining. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<date>2008</date>
<note>Stanford typed dependencies manual. http://nlp.stanford.edu/downloads/lex-parser.shtml.</note>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. Stanford typed dependencies manual. http://nlp.stanford.edu/downloads/lex-parser.shtml.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Lenhart K Schubert</author>
</authors>
<title>Open knowledge extraction using compositional language processing.</title>
<date>2008</date>
<booktitle>In STEP.</booktitle>
<marker>Van Durme, Schubert, 2008</marker>
<rawString>Benjamin Van Durme and Lenhart K. Schubert. 2008. Open knowledge extraction using compositional language processing. In STEP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hoffmann</author>
<author>C Zhang</author>
<author>D Weld</author>
</authors>
<title>Learning 5000 relational extractors.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3051" citStr="Hoffmann et al., 2010" startWordPosition="458" endWordPosition="461">ic heuristics generate labeled data for training the extractor. For example, TextRunner uses a small set of handwritten rules to heuristically label training examples from sentences in the Penn Treebank. This paper presents WOE (Wikipedia-based Open Extractor), the first system that autonomously transfers knowledge from random editors’ effort of collaboratively editing Wikipedia to train an open information extractor. Specifically, WOE generates relation-specific training examples by matching Infobox1 attribute values to corresponding sentences (as done in Kylin (Wu and Weld, 2007) and Luchs (Hoffmann et al., 2010)), but WOE abstracts these examples to relationindependent training data to learn an unlexicalized extractor, akin to that of TextRunner. WOE can operate in two modes: when restricted to shallow features like part-of-speech (POS) tags, it runs as quickly as Textrunner, but when set to use dependency-parse features its precision and recall rise even higher. We present a thorough experimental evaluation, making the following contributions: • We present WOE, a new approach to open IE that uses Wikipedia for self-supervised learn1An infobox is a set of tuples summarizing the key attributes of the </context>
</contexts>
<marker>Hoffmann, Zhang, Weld, 2010</marker>
<rawString>R. Hoffmann, C. Zhang, and D. Weld. 2010. Learning 5000 relational extractors. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>A systematic exploration of the feature space for relation extraction.</title>
<date>2007</date>
<booktitle>In HLT/NAACL.</booktitle>
<contexts>
<context position="4521" citStr="Jiang and Zhai, 2007" startWordPosition="690" endWordPosition="693">sala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics ing of unlexicalized extractors. Compared with TextRunner (the state of the art) on three corpora, WOE yields between 72% and 91% improved F-measure — generalizing well beyond Wikipedia. • Using the same learning algorithm and features as TextRunner, we compare four different ways to generate positive and negative training data with TextRunner’s method, concluding that our Wikipedia heuristic is responsible for the bulk of WOE’s improved accuracy. • The biggest win arises from using parser features. Previous work (Jiang and Zhai, 2007) concluded that parser-based features are unnecessary for information extraction, but that work assumed the presence of lexical features. We show that abstract dependency paths are a highly informative feature when performing unlexicalized extraction. 2 Problem Definition An open information extractor is a function from a document, d, to a set of triples, {(argi, rel, arg0j, where the args are noun phrases and rel is a textual fragment indicating an implicit, semantic relation between the two noun phrases. The extractor should produce one triple for every relation stated explicitly in the text</context>
<context position="12416" citStr="Jiang and Zhai, 2007" startWordPosition="1929" endWordPosition="1932"> using features from dependency-parse trees and the other (WOEp°s) limited to shallow features like POS tags. WOEparse uses a pattern learner to classify whether the shortest dependency path between two noun phrases indicates a semantic relation. In contrast, WOEp°s (like TextRunner) trains a conditional random field (CRF) to output certain text between noun phrases when the text denotes such a relation. Neither extractor uses individual words or lexical information for features. 3.3.1 Extraction with Parser Features Despite some evidence that parser-based features have limited utility in IE (Jiang and Zhai, 2007), we hoped dependency paths would improve precision on long sentences. Shortest Dependency Path as Relation: Unless otherwise noted, WOE uses the Stanford Parser to create dependencies in the “collapsedDependency” format. Dependencies involving prepositions, conjuncts as well as information about the referent of relative clauses are collapsed to get direct dependencies between content words. As 120 noted in (de Marneffe and Manning, 2008), this collapsed format often yields simplified patterns which are useful for relation extraction. Consider the sentence: Dan was not born in Berkeley. The St</context>
<context position="32748" citStr="Jiang and Zhai, 2007" startWordPosition="5167" endWordPosition="5170">ndcrafted patterns. Wu and Weld proposed the KYLIN system (Wu and Weld, 2007; Wu et al., 2008) which has the same spirit of matching Wikipedia sentences with infoboxes to learn CRF extractors. However, it only works for relations defined in Wikipedia infoboxes. Shallow or Deep Parsing: Shallow features, like POS tags, enable fast extraction over large-scale corpora (Davidov et al., 2007; Banko et al., 2007). Deep features are derived from parse trees with the hope of training better extractors (Zhang et al., 2006; Zhao and Grishman, 2005; Bunescu and Mooney, 2005; Wang, 2008). Jiang and Zhai (Jiang and Zhai, 2007) did a systematic exploration of the feature space for relation extraction on the ACE corpus. Their results showed limited advantage of parser features over shallow features for IE. However, our results imply that abstracted dependency path features are highly informative for open IE. There might be several reasons for the different observations. First, Jiang and Zhai’s results are tested for traditional IE where local lexicalized tokens might contain sufficient information to trigger a correct classification. The situation is different when features are completely unlexicalized in open IE. Se</context>
<context position="34689" citStr="Jiang and Zhai, 2007" startWordPosition="5473" endWordPosition="5476">TextRunner, WOEp°s runs at the same speed, but achieves an F-measure which is between 18% and 34% greater on three corpora; WOEp&amp;quot;&apos; achieves an F-measure which is between 72% and 91% higher than that of TextRunner, but runs about 30X times slower due to the time required for parsing. Our experiments uncovered two sources of WOE’s strong performance: 1) the Wikipedia heuristic is responsible for the bulk of WOE’s improved accuracy, but 2) dependency-parse features are highly informative when performing unlexicalized extraction. We note that this second conclusion disagrees with the findings in (Jiang and Zhai, 2007). In the future, we plan to run WOE over the billion document CMU ClueWeb09 corpus to compile a giant knowledge base for distribution to the NLP community. There are several ways to further improve WOE’s performance. Other data sources, such as Freebase, could be used to create an additional training dataset via self-supervision. For example, Mintz et al. consider all sentences containing both the subject and object of a Freebase record as matching sentences (Mintz et al., 2009); while they use this data to learn relation-specific extractors, one could also learn an open extractor. We are also</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. A systematic exploration of the feature space for relation extraction. In HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gangemi M Ciaramita</author>
</authors>
<title>Unsupervised learning of semantic relations between concepts of a molecular biology ontology.</title>
<date>2005</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="28485" citStr="Ciaramita, 2005" startWordPosition="4518" endWordPosition="4519">ght effect WOEparse’s performance. We used three parsing options on the WSJ dataset: Stanford parsing, CJ50 parsing (Charniak and Johnson, 2005), and the gold parses from the Penn Treebank. The Stanford Parser is used to derive dependencies from CJ50 and gold parse trees. Figure 8 shows the detailed P/R curves. We can see that although today’s statistical parsers make errors, they have negligible effect on the accuracy of WOE. 5 Related Work Open or Traditional Information Extraction: Most existing work on IE is relation-specific. Occurrence-statistical models (Agichtein and Gravano, 2000; M. Ciaramita, 2005), graphical models (Peng and McCallum, 2004; Poon and Domingos, 2008), and kernel-based methods (Bunescu and R.Mooney, 2005) have been studied. Snow et al. (Snow et al., 2005) utilize WordNet to learn dependency path patterns for extracting the hypernym relation from text. Some seed-based frameworks are proposed for open-domain extraction (Pasca, 2008; Davidov et al., 2007; Davidov and Rappoport, 2008). These works focus 124 0.0 0.2 0.4 0.6 0.8 1.0 precision P/R Curve on WSJ P/R Curve on Web CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.1 0.2 0.3 0.4 recall P/R Curve on Wikipedia </context>
</contexts>
<marker>Ciaramita, 2005</marker>
<rawString>A. Gangemi M. Ciaramita. 2005. Unsupervised learning of semantic relations between concepts of a molecular biology ontology. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>In http://mallet.cs.umass.edu.</note>
<contexts>
<context position="18672" citStr="McCallum, 2002" startWordPosition="2938" endWordPosition="2939">les from the Penn Treebank. We use the same matching sentence set behind DBP to generate positive examples for WOEpos. Specifically, for each matching sentence, we label the subject and infobox attribute value as arg1 and arg2 to serve as the ends of a linear CRF chain. Tokens involved in the expandPath are labeled as rel. Negative examples are generated from random noun-phrase pairs in other sentences when their generalized-CorePaths are not in DBP. WOEpos uses the same learning algorithm and selection of features as TextRunner: a two-order CRF chain model is trained with the Mallet package (McCallum, 2002). WOEpos’s features include POS-tags, regular expressions (e.g., for detecting capitalization, punctuation, etc..), and conjunctions of features occurring in adjacent positions within six words to the left and to the right of the current word. As shown in the experiments, WOEpos achieves an improved F-measure over TextRunner between 18% to 34% on three corpora, and this is mainly due to the increase on precision. 4 Experiments We used three corpora for experiments: WSJ from Penn Treebank, Wikipedia, and the general Web. For each dataset, we randomly selected 300 sentences. Each sentence was ex</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. In http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="30632" citStr="Mintz et al., 2009" startWordPosition="4847" endWordPosition="4850">rform both relation-specific and open IE by iteratively generating weighted extraction patterns. Different from WOE, StatSnowball only employs shallow features and uses L1-normalization to weight patterns. Shinyama and Sekine proposed the “preemptive IE” framework to avoid relation-specificity (Shinyama and Sekine, 2006). They first group documents based on pairwise vector-space clustering, then apply an additional clustering to group entities based on documents clusters. The two clustering steps make it difficult to meet the scalability requirement necessary to process the Web. Mintz et al. (Mintz et al., 2009) uses Freebase to provide distant supervision for relation extraction. They applied a similar heuristic by matching Freebase tuples with unstructured sentences (Wikipedia articles in their experiments) to create features for learning relation extractors. Matching Freebase with arbitrary sentences instead of matching Wikipedia infobox with corresponding Wikipedia articles will potentially increase the size of matched sentences at a cost of accuracy. Also, their learned extractors are relation-specific. Alan Akbik et al. (Akbik and Broß, 2009) annotated 10,000 sentences parsed with LinkGrammar a</context>
<context position="35172" citStr="Mintz et al., 2009" startWordPosition="5555" endWordPosition="5558">tive when performing unlexicalized extraction. We note that this second conclusion disagrees with the findings in (Jiang and Zhai, 2007). In the future, we plan to run WOE over the billion document CMU ClueWeb09 corpus to compile a giant knowledge base for distribution to the NLP community. There are several ways to further improve WOE’s performance. Other data sources, such as Freebase, could be used to create an additional training dataset via self-supervision. For example, Mintz et al. consider all sentences containing both the subject and object of a Freebase record as matching sentences (Mintz et al., 2009); while they use this data to learn relation-specific extractors, one could also learn an open extractor. We are also interested in merging lexicalized and open extraction methods; the use of some domain-specific lexical features might help to improve WOE’s practical performance, but the best way to do this is unclear. Finally, we wish to combine WOEpII&apos; with WOEp°s (&apos;.g., with voting) to produce a system which maximizes precision at low recall. Acknowledgements We thank Oren Etzioni and Michele Banko from Turing Center at the University of Washington for providing the code of their software a</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Kotaro Nakayama</author>
<author>S Nishio</author>
</authors>
<title>Wikipedia link structure and text mining for semantic relation extraction.</title>
<date>2008</date>
<booktitle>In CEUR Workshop.</booktitle>
<contexts>
<context position="7980" citStr="Nakayama and Nishio, 2008" startWordPosition="1226" endWordPosition="1229">ce in many cases. Compiling Synonyms: As a final step, the preprocessor builds sets of synonyms to help the matcher find sentences that correspond to infobox relations. This is useful because Wikipedia editors frequently use multiple names for an entity; for example, in the article titled “University of Washington” the token “UW” is widely used to refer the university. Additionally, attribute values are often described differently within the infobox than they are in surrounding text. Without knowledge of these synonyms, it is impossible to construct good matches. Following (Wu and Weld, 2007; Nakayama and Nishio, 2008), the preprocessor uses Wikipedia redirection pages and backSentence Splitting Preprocessor NLP Annotating Synonyms Compiling Primary Entity Matching Matcher Sentence Matching Pattern Classifier over Parser Features CRF Extractor over Shallow Features Triples Learner 119 ward links to automatically construct synonym sets. Redirection pages are a natural choice, because they explicitly encode synonyms; for example, “USA” is redirected to the article on the “United States.” Backward links for a Wikipedia entity such as the “Massachusetts Institute of Technology” are hyperlinks pointing to this e</context>
<context position="32017" citStr="Nakayama and Nishio, 2008" startWordPosition="5050" endWordPosition="5053">f 0.0 0.1 0.2 0.3 0.4 0.5 0.6 recall precision 0.4 0.6 0.8 1.0 WOEstanford parse =WOEparse parse WOECJ50 parse WOEgold 125 301,962 Wikipedia sentences. The KNext system (Durme and Schubert, 2008) performs open knowledge extraction via significant heuristics. Its output is knowledge represented as logical statements instead of information represented as segmented text fragments. Information Extraction with Wikipedia: The YAGO system (Suchanek et al., 2007) extends WordNet using facts extracted from Wikipedia categories. It only targets a limited number of predefined relations. Nakayama et al. (Nakayama and Nishio, 2008) parse selected Wikipedia sentences and perform extraction over the phrase structure trees based on several handcrafted patterns. Wu and Weld proposed the KYLIN system (Wu and Weld, 2007; Wu et al., 2008) which has the same spirit of matching Wikipedia sentences with infoboxes to learn CRF extractors. However, it only works for relations defined in Wikipedia infoboxes. Shallow or Deep Parsing: Shallow features, like POS tags, enable fast extraction over large-scale corpora (Davidov et al., 2007; Banko et al., 2007). Deep features are derived from parse trees with the hope of training better ex</context>
</contexts>
<marker>Nakayama, Nishio, 2008</marker>
<rawString>T. H. Kotaro Nakayama and S. Nishio. 2008. Wikipedia link structure and text mining for semantic relation extraction. In CEUR Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dat P T Nguyen</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Exploiting syntactic and semantic information for relation extraction from wikipedia.</title>
<date>2007</date>
<booktitle>In IJCAI07-TextLinkWS.</booktitle>
<contexts>
<context position="9701" citStr="Nguyen et al., 2007" startWordPosition="1495" endWordPosition="1498">dia page with an infobox, the matcher iterates through all its attributes looking for a unique sentence that contains references to both the subject of the article and the attribute value; these noun phrases will be annotated arg1 and arg2 in the training set. The matcher considers a sentence to contain the attribute value if the value or its synonym is present. Matching the article subject, however, is more involved. Matching Primary Entities: In order to match shorthand terms like “MIT” with more complete names, the matcher uses an ordered set of heuristics like those of (Wu and Weld, 2007; Nguyen et al., 2007): • Full match: strings matching the full name of the entity are selected. • Synonym set match: strings appearing in the entity’s synonym set are selected. • Partial match: strings matching a prefix or suffix of the entity’s name are selected. If the full name contains punctuation, only a prefix is allowed. For example, “Amherst” matches “Amherst, Mass,” but “Mass” does not. • Patterns of “the &lt;type&gt;”: The matcher first identifies the type of the entity (e.g., “city” for “Ithaca”), then instantiates the pattern to create the string “the city.” Since the first sentence of most Wikipedia article</context>
</contexts>
<marker>Nguyen, Matsuo, Ishizuka, 2007</marker>
<rawString>Dat P.T Nguyen, Yutaka Matsuo, and Mitsuru Ishizuka. 2007. Exploiting syntactic and semantic information for relation extraction from wikipedia. In IJCAI07-TextLinkWS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
</authors>
<title>Turning web text and search queries into factual knowledge: Hierarchical class attribute extraction.</title>
<date>2008</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="28838" citStr="Pasca, 2008" startWordPosition="4572" endWordPosition="4573">parsers make errors, they have negligible effect on the accuracy of WOE. 5 Related Work Open or Traditional Information Extraction: Most existing work on IE is relation-specific. Occurrence-statistical models (Agichtein and Gravano, 2000; M. Ciaramita, 2005), graphical models (Peng and McCallum, 2004; Poon and Domingos, 2008), and kernel-based methods (Bunescu and R.Mooney, 2005) have been studied. Snow et al. (Snow et al., 2005) utilize WordNet to learn dependency path patterns for extracting the hypernym relation from text. Some seed-based frameworks are proposed for open-domain extraction (Pasca, 2008; Davidov et al., 2007; Davidov and Rappoport, 2008). These works focus 124 0.0 0.2 0.4 0.6 0.8 1.0 precision P/R Curve on WSJ P/R Curve on Web CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.1 0.2 0.3 0.4 recall P/R Curve on Wikipedia 0.0 0.1 0.2 0.3 0.4 recall CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.2 0.4 0.6 0.8 1.0 precision 0.0 0.1 0.2 0.3 0.4 recall CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.2 0.4 0.6 0.8 1.0 precision Figure 6: Matching sentences with Wikipedia infoboxes results in better training data than the handwritten rules used by Text</context>
</contexts>
<marker>Pasca, 2008</marker>
<rawString>Marius Pasca. 2008. Turning web text and search queries into factual knowledge: Hierarchical class attribute extraction. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Andrew McCallum</author>
</authors>
<title>Accurate Information Extraction from Research Papers using Conditional Random Fields.</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="28528" citStr="Peng and McCallum, 2004" startWordPosition="4523" endWordPosition="4526">e used three parsing options on the WSJ dataset: Stanford parsing, CJ50 parsing (Charniak and Johnson, 2005), and the gold parses from the Penn Treebank. The Stanford Parser is used to derive dependencies from CJ50 and gold parse trees. Figure 8 shows the detailed P/R curves. We can see that although today’s statistical parsers make errors, they have negligible effect on the accuracy of WOE. 5 Related Work Open or Traditional Information Extraction: Most existing work on IE is relation-specific. Occurrence-statistical models (Agichtein and Gravano, 2000; M. Ciaramita, 2005), graphical models (Peng and McCallum, 2004; Poon and Domingos, 2008), and kernel-based methods (Bunescu and R.Mooney, 2005) have been studied. Snow et al. (Snow et al., 2005) utilize WordNet to learn dependency path patterns for extracting the hypernym relation from text. Some seed-based frameworks are proposed for open-domain extraction (Pasca, 2008; Davidov et al., 2007; Davidov and Rappoport, 2008). These works focus 124 0.0 0.2 0.4 0.6 0.8 1.0 precision P/R Curve on WSJ P/R Curve on Web CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.1 0.2 0.3 0.4 recall P/R Curve on Wikipedia 0.0 0.1 0.2 0.3 0.4 recall CRF+w−w=WOEpos C</context>
</contexts>
<marker>Peng, McCallum, 2004</marker>
<rawString>Fuchun Peng and Andrew McCallum. 2004. Accurate Information Extraction from Research Papers using Conditional Random Fields. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<date>2008</date>
<booktitle>Joint Inference in Information Extraction. In AAAI.</booktitle>
<contexts>
<context position="28554" citStr="Poon and Domingos, 2008" startWordPosition="4527" endWordPosition="4531">ons on the WSJ dataset: Stanford parsing, CJ50 parsing (Charniak and Johnson, 2005), and the gold parses from the Penn Treebank. The Stanford Parser is used to derive dependencies from CJ50 and gold parse trees. Figure 8 shows the detailed P/R curves. We can see that although today’s statistical parsers make errors, they have negligible effect on the accuracy of WOE. 5 Related Work Open or Traditional Information Extraction: Most existing work on IE is relation-specific. Occurrence-statistical models (Agichtein and Gravano, 2000; M. Ciaramita, 2005), graphical models (Peng and McCallum, 2004; Poon and Domingos, 2008), and kernel-based methods (Bunescu and R.Mooney, 2005) have been studied. Snow et al. (Snow et al., 2005) utilize WordNet to learn dependency path patterns for extracting the hypernym relation from text. Some seed-based frameworks are proposed for open-domain extraction (Pasca, 2008; Davidov et al., 2007; Davidov and Rappoport, 2008). These works focus 124 0.0 0.2 0.4 0.6 0.8 1.0 precision P/R Curve on WSJ P/R Curve on Web CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.1 0.2 0.3 0.4 recall P/R Curve on Wikipedia 0.0 0.1 0.2 0.3 0.4 recall CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr </context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008. Joint Inference in Information Extraction. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Shinyama</author>
<author>S Sekine</author>
</authors>
<title>Preemptive information extraction using unristricted relation discovery.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="2167" citStr="Shinyama and Sekine, 2006" startWordPosition="325" endWordPosition="328">ing, and summarization. The vast majority of IE work uses supervised learning of relationspecific examples. For example, the WebKB project (Craven et al., 1998) used labeled examples of the courses-taught-by relation to induce rules for identifying additional instances of the relation. While these methods can achieve high precision and recall, they are limited by the availability of training data and are unlikely to scale to the thousands of relations found in text on the Web. An alternative paradigm, Open IE, pioneered by the TextRunner system (Banko et al., 2007) and the “preemptive IE” in (Shinyama and Sekine, 2006), aims to handle an unbounded number of relations and run quickly enough to process Webscale corpora. Domain independence is achieved by extracting the relation name as well as its two arguments. Most open IE systems use selfsupervised learning, in which automatic heuristics generate labeled data for training the extractor. For example, TextRunner uses a small set of handwritten rules to heuristically label training examples from sentences in the Penn Treebank. This paper presents WOE (Wikipedia-based Open Extractor), the first system that autonomously transfers knowledge from random editors’ </context>
<context position="30335" citStr="Shinyama and Sekine, 2006" startWordPosition="4800" endWordPosition="4803">ey have negligible effect on the accuracy of WOE compared to operation on gold standard, human-annotated data. on identifying general relations such as class attributes, while open IE aims to extract relation instances from given sentences. Another seedbased system StatSnowball (Zhu et al., 2009) can perform both relation-specific and open IE by iteratively generating weighted extraction patterns. Different from WOE, StatSnowball only employs shallow features and uses L1-normalization to weight patterns. Shinyama and Sekine proposed the “preemptive IE” framework to avoid relation-specificity (Shinyama and Sekine, 2006). They first group documents based on pairwise vector-space clustering, then apply an additional clustering to group entities based on documents clusters. The two clustering steps make it difficult to meet the scalability requirement necessary to process the Web. Mintz et al. (Mintz et al., 2009) uses Freebase to provide distant supervision for relation extraction. They applied a similar heuristic by matching Freebase tuples with unstructured sentences (Wikipedia articles in their experiments) to create features for learning relation extractors. Matching Freebase with arbitrary sentences inste</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Y. Shinyama and S. Sekine. 2006. Preemptive information extraction using unristricted relation discovery. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2005</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="28660" citStr="Snow et al., 2005" startWordPosition="4545" endWordPosition="4548">e Penn Treebank. The Stanford Parser is used to derive dependencies from CJ50 and gold parse trees. Figure 8 shows the detailed P/R curves. We can see that although today’s statistical parsers make errors, they have negligible effect on the accuracy of WOE. 5 Related Work Open or Traditional Information Extraction: Most existing work on IE is relation-specific. Occurrence-statistical models (Agichtein and Gravano, 2000; M. Ciaramita, 2005), graphical models (Peng and McCallum, 2004; Poon and Domingos, 2008), and kernel-based methods (Bunescu and R.Mooney, 2005) have been studied. Snow et al. (Snow et al., 2005) utilize WordNet to learn dependency path patterns for extracting the hypernym relation from text. Some seed-based frameworks are proposed for open-domain extraction (Pasca, 2008; Davidov et al., 2007; Davidov and Rappoport, 2008). These works focus 124 0.0 0.2 0.4 0.6 0.8 1.0 precision P/R Curve on WSJ P/R Curve on Web CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.1 0.2 0.3 0.4 recall P/R Curve on Wikipedia 0.0 0.1 0.2 0.3 0.4 recall CRF+w−w=WOEpos CRF+w−tr CRF+w−r CRF+tr−tr TextRunner 0.0 0.2 0.4 0.6 0.8 1.0 precision 0.0 0.1 0.2 0.3 0.4 recall CRF+w−w=WOEpos CRF+w−tr CRF+w−r CR</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A core of semantic knowledge - unifying WordNet and Wikipedia.</title>
<date>2007</date>
<booktitle>In WWW.</booktitle>
<contexts>
<context position="31850" citStr="Suchanek et al., 2007" startWordPosition="5025" endWordPosition="5028">ar and selected 46 general linkpaths as patterns for relation extraction. In contrast, WOE learns 15,333 general patterns based on an automatically annotated set of 0.0 0.1 0.2 0.3 0.4 0.5 0.6 recall precision 0.4 0.6 0.8 1.0 WOEstanford parse =WOEparse parse WOECJ50 parse WOEgold 125 301,962 Wikipedia sentences. The KNext system (Durme and Schubert, 2008) performs open knowledge extraction via significant heuristics. Its output is knowledge represented as logical statements instead of information represented as segmented text fragments. Information Extraction with Wikipedia: The YAGO system (Suchanek et al., 2007) extends WordNet using facts extracted from Wikipedia categories. It only targets a limited number of predefined relations. Nakayama et al. (Nakayama and Nishio, 2008) parse selected Wikipedia sentences and perform extraction over the phrase structure trees based on several handcrafted patterns. Wu and Weld proposed the KYLIN system (Wu and Weld, 2007; Wu et al., 2008) which has the same spirit of matching Wikipedia sentences with infoboxes to learn CRF extractors. However, it only works for relations defined in Wikipedia infoboxes. Shallow or Deep Parsing: Shallow features, like POS tags, ena</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A core of semantic knowledge - unifying WordNet and Wikipedia. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
</authors>
<title>A re-examination of dependency path kernels for relation extraction.</title>
<date>2008</date>
<booktitle>In IJCNLP.</booktitle>
<contexts>
<context position="32709" citStr="Wang, 2008" startWordPosition="5162" endWordPosition="5163">ure trees based on several handcrafted patterns. Wu and Weld proposed the KYLIN system (Wu and Weld, 2007; Wu et al., 2008) which has the same spirit of matching Wikipedia sentences with infoboxes to learn CRF extractors. However, it only works for relations defined in Wikipedia infoboxes. Shallow or Deep Parsing: Shallow features, like POS tags, enable fast extraction over large-scale corpora (Davidov et al., 2007; Banko et al., 2007). Deep features are derived from parse trees with the hope of training better extractors (Zhang et al., 2006; Zhao and Grishman, 2005; Bunescu and Mooney, 2005; Wang, 2008). Jiang and Zhai (Jiang and Zhai, 2007) did a systematic exploration of the feature space for relation extraction on the ACE corpus. Their results showed limited advantage of parser features over shallow features for IE. However, our results imply that abstracted dependency path features are highly informative for open IE. There might be several reasons for the different observations. First, Jiang and Zhai’s results are tested for traditional IE where local lexicalized tokens might contain sufficient information to trigger a correct classification. The situation is different when features are </context>
</contexts>
<marker>Wang, 2008</marker>
<rawString>Mengqiu Wang. 2008. A re-examination of dependency path kernels for relation extraction. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel Weld</author>
</authors>
<title>Autonomouslly Semantifying Wikipedia.</title>
<date>2007</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="3017" citStr="Wu and Weld, 2007" startWordPosition="452" endWordPosition="455">sed learning, in which automatic heuristics generate labeled data for training the extractor. For example, TextRunner uses a small set of handwritten rules to heuristically label training examples from sentences in the Penn Treebank. This paper presents WOE (Wikipedia-based Open Extractor), the first system that autonomously transfers knowledge from random editors’ effort of collaboratively editing Wikipedia to train an open information extractor. Specifically, WOE generates relation-specific training examples by matching Infobox1 attribute values to corresponding sentences (as done in Kylin (Wu and Weld, 2007) and Luchs (Hoffmann et al., 2010)), but WOE abstracts these examples to relationindependent training data to learn an unlexicalized extractor, akin to that of TextRunner. WOE can operate in two modes: when restricted to shallow features like part-of-speech (POS) tags, it runs as quickly as Textrunner, but when set to use dependency-parse features its precision and recall rise even higher. We present a thorough experimental evaluation, making the following contributions: • We present WOE, a new approach to open IE that uses Wikipedia for self-supervised learn1An infobox is a set of tuples summ</context>
<context position="7952" citStr="Wu and Weld, 2007" startWordPosition="1222" endWordPosition="1225">s parsing performance in many cases. Compiling Synonyms: As a final step, the preprocessor builds sets of synonyms to help the matcher find sentences that correspond to infobox relations. This is useful because Wikipedia editors frequently use multiple names for an entity; for example, in the article titled “University of Washington” the token “UW” is widely used to refer the university. Additionally, attribute values are often described differently within the infobox than they are in surrounding text. Without knowledge of these synonyms, it is impossible to construct good matches. Following (Wu and Weld, 2007; Nakayama and Nishio, 2008), the preprocessor uses Wikipedia redirection pages and backSentence Splitting Preprocessor NLP Annotating Synonyms Compiling Primary Entity Matching Matcher Sentence Matching Pattern Classifier over Parser Features CRF Extractor over Shallow Features Triples Learner 119 ward links to automatically construct synonym sets. Redirection pages are a natural choice, because they explicitly encode synonyms; for example, “USA” is redirected to the article on the “United States.” Backward links for a Wikipedia entity such as the “Massachusetts Institute of Technology” are h</context>
<context position="9679" citStr="Wu and Weld, 2007" startWordPosition="1491" endWordPosition="1494">.. ” Given a Wikipedia page with an infobox, the matcher iterates through all its attributes looking for a unique sentence that contains references to both the subject of the article and the attribute value; these noun phrases will be annotated arg1 and arg2 in the training set. The matcher considers a sentence to contain the attribute value if the value or its synonym is present. Matching the article subject, however, is more involved. Matching Primary Entities: In order to match shorthand terms like “MIT” with more complete names, the matcher uses an ordered set of heuristics like those of (Wu and Weld, 2007; Nguyen et al., 2007): • Full match: strings matching the full name of the entity are selected. • Synonym set match: strings appearing in the entity’s synonym set are selected. • Partial match: strings matching a prefix or suffix of the entity’s name are selected. If the full name contains punctuation, only a prefix is allowed. For example, “Amherst” matches “Amherst, Mass,” but “Mass” does not. • Patterns of “the &lt;type&gt;”: The matcher first identifies the type of the entity (e.g., “city” for “Ithaca”), then instantiates the pattern to create the string “the city.” Since the first sentence of </context>
<context position="32203" citStr="Wu and Weld, 2007" startWordPosition="5078" endWordPosition="5081"> performs open knowledge extraction via significant heuristics. Its output is knowledge represented as logical statements instead of information represented as segmented text fragments. Information Extraction with Wikipedia: The YAGO system (Suchanek et al., 2007) extends WordNet using facts extracted from Wikipedia categories. It only targets a limited number of predefined relations. Nakayama et al. (Nakayama and Nishio, 2008) parse selected Wikipedia sentences and perform extraction over the phrase structure trees based on several handcrafted patterns. Wu and Weld proposed the KYLIN system (Wu and Weld, 2007; Wu et al., 2008) which has the same spirit of matching Wikipedia sentences with infoboxes to learn CRF extractors. However, it only works for relations defined in Wikipedia infoboxes. Shallow or Deep Parsing: Shallow features, like POS tags, enable fast extraction over large-scale corpora (Davidov et al., 2007; Banko et al., 2007). Deep features are derived from parse trees with the hope of training better extractors (Zhang et al., 2006; Zhao and Grishman, 2005; Bunescu and Mooney, 2005; Wang, 2008). Jiang and Zhai (Jiang and Zhai, 2007) did a systematic exploration of the feature space for </context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel Weld. 2007. Autonomouslly Semantifying Wikipedia. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Raphael Hoffmann</author>
<author>Danel S Weld</author>
</authors>
<title>Information extraction from Wikipedia: Moving down the long tail.</title>
<date>2008</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="32221" citStr="Wu et al., 2008" startWordPosition="5082" endWordPosition="5085">ledge extraction via significant heuristics. Its output is knowledge represented as logical statements instead of information represented as segmented text fragments. Information Extraction with Wikipedia: The YAGO system (Suchanek et al., 2007) extends WordNet using facts extracted from Wikipedia categories. It only targets a limited number of predefined relations. Nakayama et al. (Nakayama and Nishio, 2008) parse selected Wikipedia sentences and perform extraction over the phrase structure trees based on several handcrafted patterns. Wu and Weld proposed the KYLIN system (Wu and Weld, 2007; Wu et al., 2008) which has the same spirit of matching Wikipedia sentences with infoboxes to learn CRF extractors. However, it only works for relations defined in Wikipedia infoboxes. Shallow or Deep Parsing: Shallow features, like POS tags, enable fast extraction over large-scale corpora (Davidov et al., 2007; Banko et al., 2007). Deep features are derived from parse trees with the hope of training better extractors (Zhang et al., 2006; Zhao and Grishman, 2005; Bunescu and Mooney, 2005; Wang, 2008). Jiang and Zhai (Jiang and Zhai, 2007) did a systematic exploration of the feature space for relation extractio</context>
</contexts>
<marker>Wu, Hoffmann, Weld, 2008</marker>
<rawString>Fei Wu, Raphael Hoffmann, and Danel S. Weld. 2008. Information extraction from Wikipedia: Moving down the long tail. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
</authors>
<title>A composite kernel to extract relations between entities with both flat and structured features.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="32645" citStr="Zhang et al., 2006" startWordPosition="5150" endWordPosition="5153">ected Wikipedia sentences and perform extraction over the phrase structure trees based on several handcrafted patterns. Wu and Weld proposed the KYLIN system (Wu and Weld, 2007; Wu et al., 2008) which has the same spirit of matching Wikipedia sentences with infoboxes to learn CRF extractors. However, it only works for relations defined in Wikipedia infoboxes. Shallow or Deep Parsing: Shallow features, like POS tags, enable fast extraction over large-scale corpora (Davidov et al., 2007; Banko et al., 2007). Deep features are derived from parse trees with the hope of training better extractors (Zhang et al., 2006; Zhao and Grishman, 2005; Bunescu and Mooney, 2005; Wang, 2008). Jiang and Zhai (Jiang and Zhai, 2007) did a systematic exploration of the feature space for relation extraction on the ACE corpus. Their results showed limited advantage of parser features over shallow features for IE. However, our results imply that abstracted dependency path features are highly informative for open IE. There might be several reasons for the different observations. First, Jiang and Zhai’s results are tested for traditional IE where local lexicalized tokens might contain sufficient information to trigger a corre</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou. 2006. A composite kernel to extract relations between entities with both flat and structured features. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubin Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="32670" citStr="Zhao and Grishman, 2005" startWordPosition="5154" endWordPosition="5157">ences and perform extraction over the phrase structure trees based on several handcrafted patterns. Wu and Weld proposed the KYLIN system (Wu and Weld, 2007; Wu et al., 2008) which has the same spirit of matching Wikipedia sentences with infoboxes to learn CRF extractors. However, it only works for relations defined in Wikipedia infoboxes. Shallow or Deep Parsing: Shallow features, like POS tags, enable fast extraction over large-scale corpora (Davidov et al., 2007; Banko et al., 2007). Deep features are derived from parse trees with the hope of training better extractors (Zhang et al., 2006; Zhao and Grishman, 2005; Bunescu and Mooney, 2005; Wang, 2008). Jiang and Zhai (Jiang and Zhai, 2007) did a systematic exploration of the feature space for relation extraction on the ACE corpus. Their results showed limited advantage of parser features over shallow features for IE. However, our results imply that abstracted dependency path features are highly informative for open IE. There might be several reasons for the different observations. First, Jiang and Zhai’s results are tested for traditional IE where local lexicalized tokens might contain sufficient information to trigger a correct classification. The si</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Shubin Zhao and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Zaiqing Nie</author>
<author>Xiaojiang Liu</author>
<author>Bo Zhang</author>
<author>Ji-Rong Wen</author>
</authors>
<title>Statsnowball: a statistical approach to extracting entity relationships.</title>
<date>2009</date>
<booktitle>In WWW.</booktitle>
<contexts>
<context position="30006" citStr="Zhu et al., 2009" startWordPosition="4755" endWordPosition="4758">ning data than the handwritten rules used by TextRunner. Figure 7: Filtering prepositional phrase attachments (PPa) shows a strong boost to precision, and we see a smaller boost from enforcing a lexical ordering of relation arguments (1�2). P/R Curve on WSJ Figure 8: Although today’s statistical parsers make errors, they have negligible effect on the accuracy of WOE compared to operation on gold standard, human-annotated data. on identifying general relations such as class attributes, while open IE aims to extract relation instances from given sentences. Another seedbased system StatSnowball (Zhu et al., 2009) can perform both relation-specific and open IE by iteratively generating weighted extraction patterns. Different from WOE, StatSnowball only employs shallow features and uses L1-normalization to weight patterns. Shinyama and Sekine proposed the “preemptive IE” framework to avoid relation-specificity (Shinyama and Sekine, 2006). They first group documents based on pairwise vector-space clustering, then apply an additional clustering to group entities based on documents clusters. The two clustering steps make it difficult to meet the scalability requirement necessary to process the Web. Mintz e</context>
</contexts>
<marker>Zhu, Nie, Liu, Zhang, Wen, 2009</marker>
<rawString>Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen. 2009. Statsnowball: a statistical approach to extracting entity relationships. In WWW.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>