<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.976054">
Creating a CCGbank and a wide-coverage CCG lexicon for German
</title>
<author confidence="0.995993">
Julia Hockenmaier
</author>
<affiliation confidence="0.9981125">
Institute for Research in Cognitive Science
University of Pennsylvania
</affiliation>
<address confidence="0.521214">
Philadelphia, PA 19104, USA
</address>
<email confidence="0.99948">
juliahr@cis.upenn.edu
</email>
<sectionHeader confidence="0.994816" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999688333333333">
We present an algorithm which creates a
German CCGbank by translating the syn-
tax graphs in the German Tiger corpus into
CCG derivation trees. The resulting cor-
pus contains 46,628 derivations, covering
95% of all complete sentences in Tiger.
Lexicons extracted from this corpus con-
tain correct lexical entries for 94% of all
known tokens in unseen text.
</bodyText>
<sectionHeader confidence="0.998425" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974536231884">
A number of wide-coverage TAG, CCG, LFG and
HPSG grammars (Xia, 1999; Chen et al., 2005;
Hockenmaier and Steedman, 2002a; O’Donovan
et al., 2005; Miyao et al., 2004) have been ex-
tracted from the Penn Treebank (Marcus et al.,
1993), and have enabled the creation of wide-
coverage parsers for English which recover local
and non-local dependencies that approximate the
underlying predicate-argument structure (Hocken-
maier and Steedman, 2002b; Clark and Curran,
2004; Miyao and Tsujii, 2005; Shen and Joshi,
2005). However, many corpora (B¨ohomv´a et al.,
2003; Skut et al., 1997; Brants et al., 2002) use
dependency graphs or other representations, and
the extraction algorithms that have been developed
for Penn Treebank style corpora may not be im-
mediately applicable to this representation. As a
consequence, research on statistical parsing with
“deep” grammars has largely been confined to En-
glish. Free-word order languages typically pose
greater challenges for syntactic theories (Rambow,
1994), and the richer inflectional morphology of
these languages creates additional problems both
for the coverage of lexicalized formalisms such
as CCG or TAG, and for the usefulness of de-
pendency counts extracted from the training data.
On the other hand, formalisms such as CCG and
TAG are particularly suited to capture the cross-
ing dependencies that arise in languages such as
Dutch or German, and by choosing an appropriate
linguistic representation, some of these problems
may be mitigated.
Here, we present an algorithm which translates
the German Tiger corpus (Brants et al., 2002) into
CCG derivations. Similar algorithms have been
developed by Hockenmaier and Steedman (2002a)
to create CCGbank, a corpus of CCG derivations
(Hockenmaier and Steedman, 2005) from the Penn
Treebank, by C¸ akici (2005) to extract a CCG lex-
icon from a Turkish dependency corpus, and by
Moortgat and Moot (2002) to induce a type-logical
grammar for Dutch.
The annotation scheme used in Tiger is an ex-
tension of that used in the earlier, and smaller,
German Negra corpus (Skut et al., 1997). Tiger
is better suited for the extraction of subcatego-
rization information (and thus the translation into
“deep” grammars of any kind), since it distin-
guishes between PP complements and modifiers,
and includes “secondary” edges to indicate shared
arguments in coordinate constructions. Tiger also
includes morphology and lemma information.
Negra is also provided with a “Penn Treebank”-
style representation, which uses flat phrase struc-
ture trees instead of the crossing dependency
structures in the original corpus. This version
has been used by Cahill et al. (2005) to extract a
German LFG. However, Dubey and Keller (2003)
have demonstrated that lexicalization does not
help a Collins-style parser that is trained on this
corpus, and Levy and Manning (2004) have shown
that its context-free representation is a poor ap-
proximation to the underlying dependency struc-
ture. The resource presented here will enable
future research to address the question whether
“deep” grammars such as CCG, which capture the
underlying dependencies directly, are better suited
to parsing German than linguistically inadequate
context-free approximations.
</bodyText>
<page confidence="0.974717">
505
</page>
<note confidence="0.8918975">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 505–512,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<figure confidence="0.963585791666667">
1. Standard main clause
Peter gibt Maria das Buch
NP[n] ((S[vl]/NP[a])/NP[d])/NP[n] NP[d] NP[a]
�� �� ��
S[dc�]/(S[vl]/NP[n]) (S[vl]/NP[a])\((S[vl]/NP[a])/NP[d]) S[vl]\(S[vl]/NP[a])
&lt;B
(S[vl]/NP[a])/NP[n] &lt;B
S[vl]/NP[n]
S[dc�]
2. Main clause with fronted adjunct 3. Main clause with fronted complement
dann gibt Peter Maria das Buch Maria gibt Peter das Buch
�
NP[d] ((S[vl]/NP[a])/NP[d])/NP[n] NP[n] NP[a]
�� � ��
S[dc�]/(S[vl]\NP[d]) (S[vl]/NP[a])/NP[d] S[vl]\(S[vl]/NP[a])
S/S ((S[vl]/NP[a])/NP[d])/NP[n] NP[n] NP[d] NP[a]
�
S[dcl]/S[vl] (S[vl]/NP[a])/NP[d]
� &gt;B
S[vl]/NP[a] S[vl]/NP[d]
� &gt;B
�
S[vl] S[dc�]
S[dc�]
</figure>
<figureCaption confidence="0.995684">
Figure 1: CCG uses topicalization (1.), a type-changing rule (2.), and type-raising (3.) to capture the
different variants of German main clause order with the same lexical category for the verb.
</figureCaption>
<sectionHeader confidence="0.413211" genericHeader="method">
2 German syntax and morphology
</sectionHeader>
<bodyText confidence="0.999672">
Morphology German verbs are inflected for
person, number, tense and mood. German nouns
and adjectives are inflected for number, case and
gender, and noun compounding is very productive.
Word order German has three different word
orders that depend on the clause type. Main
clauses (1) are verb-second. Imperatives and ques-
tions are verb-initial (2). If a modifier or one of
the objects is moved to the front, the word order
becomes verb-initial (2). Subordinate and relative
clauses are verb-final (3):
</bodyText>
<equation confidence="0.566707666666667">
(1) a. Peter gibt Maria das Buch.
Peter gives Mary the book.
b. ein Buch gibt Peter Maria.
</equation>
<listItem confidence="0.9826286">
c. dann gibt Peter Maria das Buch.
(2) a. Gibt Peter Maria das Buch?
b. Gib Maria das Buch!
(3) a. dass Peter Maria das Buch gibt.
b. das Buch, das Peter Maria gibt.
</listItem>
<bodyText confidence="0.76171025">
Local Scrambling In the so-called “Mittelfeld”
all orders of arguments and adjuncts are poten-
tially possible. In the following example, all 5!
permutations are grammatical (Rambow, 1994):
</bodyText>
<footnote confidence="0.716208727272727">
(4) dass [eine Firma] [meinem Onkel] [die M¨obel] [vor
drei Tagen] [ohne Voranmeldung] zugestellt hat.
that [a company] [to my uncle] [the furniture] [three
days ago] [without notice] delivered has.
Long-distance scrambling Objects of embed-
ded verbs can also be extraposed unboundedly
within the same sentence (Rambow, 1994):
(5) dass [den Schrank] [niemand] [zu reparieren] ver-
sprochen hat.
that [the wardrobe] [nobody] [to repair] promised
has.
</footnote>
<sectionHeader confidence="0.995801" genericHeader="method">
3 A CCG for German
</sectionHeader>
<subsectionHeader confidence="0.998962">
3.1 Combinatory Categorial Grammar
</subsectionHeader>
<bodyText confidence="0.999425578947369">
CCG (Steedman (1996; 2000)) is a lexicalized
grammar formalism with a completely transparent
syntax-semantics interface. Since CCG is mildly
context-sensitive, it can capture the crossing de-
pendencies that arise in Dutch or German, yet is
efficiently parseable.
In categorial grammar, words are associ-
ated with syntactic categories, such as S\NP or
(S\NP)/NP for English intransitive and transitive
verbs. Categories of the form X/Y or X\Y are func-
tors, which take an argument Y to their left or right
(depending on the the direction of the slash) and
yield a result X. Every syntactic category is paired
with a semantic interpretation (usually a A-term).
Like all variants of categorial grammar, CCG
uses function application to combine constituents,
but it also uses a set of combinatory rules such as
composition (B) and type-raising (l). Non-order-
preserving type-raising is used for topicalization:
</bodyText>
<equation confidence="0.97837025">
Application: X/Y Y X
Y X\Y X
Composition: X/Y Y/Z =:�B X/Z
X/Y Y\Z =:�B X\Z
Y\Z X\Y =:�B X\Z
Y/Z X\Y =:�B X/Z
Type-raising: X =z&gt;T T\(T/X)
Topicalization: X =z&gt;T T/(T/X)
</equation>
<bodyText confidence="0.998961166666667">
Hockenmaier and Steedman (2005) advocate
the use of additional “type-changing” rules to deal
with complex adjunct categories (e.g. (NP\NP) ==&gt;
S[ng]\NP for ing-VPs that act as noun phrase mod-
ifiers). Here, we also use a small number of such
rules to deal with similar adjunct cases.
</bodyText>
<page confidence="0.99616">
506
</page>
<subsectionHeader confidence="0.999726">
3.2 Capturing German word order
</subsectionHeader>
<bodyText confidence="0.99963425">
We follow Steedman (2000) in assuming that the
underlying word order in main clauses is always
verb-initial, and that the sententce-initial subject is
in fact topicalized. This enables us to capture dif-
ferent word orders with the same lexical category
(Figure 1). We use the features S[v1] and S[vlast] to
distinguish verbs in main and subordinate clauses.
Main clauses have the feature S[dc�], requiring ei-
ther a sentential modifierwith category S[dcl]/S[vl],
a topicalized subject (S[dc�]/(S[vl]/NP[nom])), or a
type-raised argument (S[dc�]/(S[vl]\x)), where x
can be any argument category, such as a noun
phrase, prepositional phrase, or a non-finite VP.
Here is the CCG derivation for the subordinate
clause (S[emb]) example:
continuous. 7.3% of the sentences have one or
more “secondary” edges, which are used to indi-
cate double dependencies that arise in coordinated
structures which are difficult to bracket, such as
right node raising, argument cluster coordination
or gapping. There are no traces or null elements to
indicate non-local dependencies or wh-movement.
Figure 2 shows the Tiger graph for a PP whose
NP argument is modified by a relative clause.
There is no NP level inside PPs (and no noun level
inside NPs). Punctuation marks are often attached
at the so-called “virtual” root (VROOT) of the en-
tire graph. The relative pronoun is a dative object
(edge label DA) of the embedded infinitive, and
is therefore attached at the VP level. The relative
clause itself has the category S; the incoming edge
is labeled RC (relative clause).
</bodyText>
<figure confidence="0.954822">
dass Peter Maria das Buch gibt
S[emb]/S[vlast] NP[n] NP[d] NP[a] ((S[vlast]\NP[n])\NP[d]�NP[a]
&lt;
&lt; Our translation algorithm has the following steps:
4.2 The translation algorithm
(S[vlast]\NP[n])\NP[d]
S[vlast]\NP[n]
S[vlast]
S[emb]
</figure>
<bodyText confidence="0.998961">
For simplicity’s sake our extraction algorithm
ignores the issues that arise through local scram-
bling, and assumes that there are different lexical
category for each permutation.1
Type-raising and composition are also used to
deal with wh-extraction and with long-distance
scrambling (Figure 2).
</bodyText>
<sectionHeader confidence="0.885796" genericHeader="method">
4 Translating Tiger graphs into CCG
</sectionHeader>
<subsectionHeader confidence="0.99948">
4.1 The Tiger corpus
</subsectionHeader>
<bodyText confidence="0.999953333333333">
The Tiger corpus (Brants et al., 2002) is a pub-
licly available2 corpus of ca. 50,000 sentences (al-
most 900,000 tokens) taken from the Frankfurter
Rundschau newspaper. The annotation is based
on a hybrid framework which contains features of
phrase-structure and dependency grammar. Each
sentence is represented as a graph whose nodes
are labeled with syntactic categories (NP, VP, S,
PP, etc.) and POS tags. Edges are directed and la-
beled with syntactic functions (e.g. head, subject,
accusative object, conjunct, appositive). The edge
labels are similar to the Penn Treebank function
tags, but provide richer and more explicit infor-
mation. Only 72.5% of the graphs have no cross-
ing edges; the remaining 27.5% are marked as dis-
</bodyText>
<footnote confidence="0.9727885">
1Variants of CCG, such as Set-CCG (Hoffman, 1995) and
Multimodal-CCG (Baldridge, 2002), allow a more compact
lexicon for free word order languages.
2http://www.ims.uni-stuttgart.de/projekte/TIGER
</footnote>
<construct confidence="0.973895818181818">
translate(TigerGraph g):
TigerTree t = createTree(g);
preprocess(t);
if (t :A null)
CCGderiv d = translateToCCG(t);
if (d :A null);
if (isCCGderivation(d))
return d;
else fail;
else fail;
else fail;
</construct>
<listItem confidence="0.813165">
1. Creating a planar tree: After an initial pre-
</listItem>
<bodyText confidence="0.999082761904762">
processing step which inserts punctuation that is
attached to the “virtual” root (VROOT) of the
graph in the appropriate locations, discontinuous
graphs are transformed into planar trees. Starting
at the lowest nonterminal nodes, this step turns
the Tiger graph into a planar tree without cross-
ing edges, where every node spans a contiguous
substring. This is required as input to the actual
translation step, since CCG derivations are pla-
nar binary trees. If the first to the ith child of a
node X span a contiguous substring that ends in
the jth word, and the (i+1)th child spans a sub-
string starting at h &gt; j +1, we attempt to move
the first i children of X to its parent P (if the
head position of P is greater than i). Punctuation
marks and adjuncts are simply moved up the tree
and treated as if they were originally attached to
P. This changes the syntactic scope of adjuncts,
but typically only VP modifiers are affected which
could also be attached at a higher VP or S node
without a change in meaning. The main exception
</bodyText>
<page confidence="0.969283">
507
</page>
<figure confidence="0.795494">
1. The original Tiger graph:
</figure>
<figureCaption confidence="0.9755">
Figure 2: From Tiger graphs to CCG derivations
</figureCaption>
<figure confidence="0.999914160377359">
PP
AC NK NK RC
S
SB OC
VP
OA
NP
PM HD
kleine Mensch sich fraglos zu unterwerfen habe
small human refl. without to submit have
questions
ADJA NN PRF ADJD PTKZU VVVIN VAFIN
MO
HD
VZ
NK NK NK
HD
der
the
ART
DA
Höchsten
Highest
, dem
whom
NN $, PRELS
einem
a
APPR
ART
an
in
APPR-AC
2. After transformation into a planar tree and preprocessing:
PP
NP-ARG
NOUN-ARG
NN-NK
H¨ochsten
ART-HD
einem
PKT
SBAR-RC
an
PRELS-EXTRA-DA
S-ARG
,
dem
NP-SB
VAFIN-HD
ART-NK
VZ-HD
habe
der
ADJA-NK
kleine
NN-HD
Mensch
PTKZU-PM
NOUN-ARG
VVINF
unterwerfen
VP-OC
PRF-ADJ ADJD-MO
sich fraglos
zu
NP[dat]
PP/NP[dat]
an
NP[dat]
NP\NP
NP[dat]
�
(NP\NP)/(S[vlast]\NP[dat])
dem
S[vlast]/(S[vlast]\NP[nom])
NP[nom]
(S[z]\NP)\NP[dat]
(S[vlast]\NP[nom])\(S[z]\NP)
NP[dat]/N[dat]
N[dat]
H¨ochsten
einem
3. The resulting CCG derivation
PP
S[vlast]\NP[dat]
(S[vlast]\NP[nom])\NP[dat]
,
N[nom]
NP[nom]/N[nom]
der
(S\NP)/(S\NP)
habe
(S[z]\NP)/(S[b]\NP)
zu
fraglos
(S\NP)/(S\NP)
(S[b]\NP)\NP[dat]
unterwerfen
sich
N[nom]
Mensch
N/N
kleine
(S[z]\NP)\NP[dat]
(S[z]\NP)\NP[dat]
</figure>
<bodyText confidence="0.991222411764706">
are extraposed relative clauses, which CCG treats
as sentential modifiers with an anaphoric depen-
dency. Arguments that are moved up are marked
as extracted, and an additional “extraction” edge
(explained below) from the original head is intro-
duced to capture the correct dependencies in the
CCG derivation. Discontinuous dependencies be-
tween resumptive pronouns (“place holders”, PH)
and their antecedents (“repeated elements”, RE)
are also dissolved.
2. Additional preprocessing: In order to obtain
the desired CCG analysis, a certain amount of pre-
processing is required. We insert NPs into PPs,
nouns into NPs3, and change sentences whose
first element is a complementizer (dass, ob, etc.)
into an SBAR (a category which does not ex-
ist in the original Tiger annotation) with S argu-
</bodyText>
<footnote confidence="0.478202">
3The span of nouns is given by the NK edge label.
</footnote>
<bodyText confidence="0.998497375">
ment. This is necessary to obtain the desired CCG
derivations where complementizers and preposi-
tions take a sentential or nominal argument to their
right, whereas they appear at the same level as
their arguments in the Tiger corpus. Further pre-
processing is required to create the required struc-
tures for wh-extraction and certain coordination
phenomena (see below).
In figure 2, preprocessing of the original Tiger
graph (top) yields the tree shown in the middle
(edge labels are shown as Penn Treebank-style
function tags).4
We will first present the basic translation algo-
rithm before we explain how we obtain a deriva-
tion which captures the dependency between the
relative pronoun and the embedded verb.
</bodyText>
<footnote confidence="0.902381">
4We treat reflexive pronouns as modifiers.
</footnote>
<page confidence="0.983779">
508
</page>
<bodyText confidence="0.957431111111111">
3. The basic translation step Our basic transla-
tion algorithm is very similar to Hockenmaier and
Steedman (2005). It requires a planar tree with-
out crossing edges, where each node is marked as
head, complement or adjunct. The latter informa-
tion is represented in the Tiger edge labels, and
only a small number of additional head rules is re-
quired. Each individual translation step operates
on local trees, which are typically flat.
</bodyText>
<equation confidence="0.8749125">
N
Cl C2 ... Cg ... Cn_1 C,
</equation>
<bodyText confidence="0.999806142857143">
Assuming the CCG category of N is x, and its
head position is i, the algorithm traverses first the
left nodes C1 ...Ci_1 from left to right to create a
right-branching derivation tree, and then the right
nodes (C,,,...Ci+1) from right to left to create a
left-branching tree. The algorithm starts at the root
category and recursively traverses the tree.
</bodyText>
<equation confidence="0.966948666666667">
N
C1 L
C~ L2
... R
Hi R ...R CI
-1 Cn
</equation>
<bodyText confidence="0.999834727272727">
The CCG category of complements and of the
root of the graph is determined from their Tiger
label. VPs are S[.]\NP, where the feature [.] dis-
tinguishes bare infinitives, zu-infinitives, passives,
and (active) past participles. With the exception
of passives, these features can be determined from
the POS tags alone.5 Embedded sentences (under
an SBAR-node) are always S[vlast]. NPs and nouns
(NP and N) have a case feature, e.g. [nom].6 Like
the English CCGbank, our grammar ignores num-
ber and person agreement.
</bodyText>
<subsectionHeader confidence="0.788637">
Special cases: Wh-extraction and extraposition
</subsectionHeader>
<bodyText confidence="0.999708416666667">
In Tiger, wh-extraction is not explicitly marked.
Relative clauses, wh-questions and free relatives
are all annotated as S-nodes,and the wh-word is
a normal argument of the verb. After turning the
graph into a planar tree, we can identify these
constructions by searching for a relative pronoun
in the leftmost child of an S node (which may
be marked as extraposed in the case of extrac-
tion from an embedded verb). As shown in fig-
ure 2, we turn this S into an SBAR (a category
which does not exist in Tiger) with the first edge
as complementizer and move the remaining chil-
</bodyText>
<footnote confidence="0.9940016">
5Eventive (“werden”) passive is easily identified by con-
text; however, we found that not all stative (“sein”) passives
seem to be annotated as such.
6In some contexts, measure nouns (e.g. Mark, Kilometer)
lack case annotation.
</footnote>
<bodyText confidence="0.994477809523809">
dren under a new S node which becomes the sec-
ond daughter of the SBAR. The relative pronoun
is the head of this SBAR and takes the S-node as
argument. Its category is S[vlast], since all clauses
with a complementizer are verb-final. In order to
capture the long-range dependency, a “trace” is
introduced, and percolated down the tree, much
like in the algorithm of Hockenmaier and Steed-
man (2005), and similar to GPSG’s slash-passing
(Gazdar et al., 1985). These trace categories are
appended to the category of the head node (and
other arguments are type-raised as necessary). In
our case, the trace is also associated with the verb
whose argument it is. If the span of this verb
is within the span of a complement, the trace is
percolated down this complement. When the VP
that is headed by this verb is reached, we assume
a canonical order of arguments in order to “dis-
charge” the trace.
If a complement node is marked as extraposed,
it is also percolated down the head tree until the
constituent whose argument it is is found. When
another complement is found whose span includes
the span of the constituent whose argument the ex-
traposed edge is, the extraposed category is perco-
lated down this tree (we assume extraction out of
adjuncts is impossible).7 In order to capture the
topicalization analysis, main clause subjects also
introduce a trace. Fronted complements or sub-
jects, and the first adjunct in main clauses are ana-
lyzed as described in figure 1.
Special case: coordination – secondary edges
Tiger uses “secondary edges” to represent the de-
pendencies that arise in coordinate constructions
such as gapping, argument cluster coordination
and right (or left) node raising (Figure 3). In right
(left) node raising, the shared elements are argu-
ments or adjuncts that appear on the right periph-
ery of the last, (or left periphery of the first) con-
junct. CCG uses type-raising and composition to
combine the incomplete conjuncts into one con-
stituent which combines with the shared element:
</bodyText>
<figure confidence="0.641434">
liest immer und beantwortet gerne jeden Brief.
always reads and gladly replies to every letter.
(S[vl]/NP)/NP coni (S[vl]/NP)/NP NP
(S[vl]/NP)/NP «&gt;
�
S[vl]/NP
</figure>
<footnote confidence="0.6616855">
7In our current implementation, each node cannot have
more than one forward and one backward extraposed element
and one forward and one backward trace. It may be preferable
to use list structures instead, especially for extraposition.
</footnote>
<page confidence="0.992184">
509
</page>
<note confidence="0.485388">
Complex coordinations: a Tiger graph with secondary edges
</note>
<figureCaption confidence="0.993453">
Figure 3: Processing secondary edges in Tiger
</figureCaption>
<figure confidence="0.998369519607843">
S-ARG
KOUS-HD
w¨ahrend
ARGCLUSTER
VVFIN-HD
aussprachen
S-CJ
S-CJ
NP-SB
KON-CD
und
PP-MO
NP-SB
PP-MO
PRF-MO
sich
während
while
78
78
Prozent
percent
sich
refl.
für
for
Bush
Bush
und
and
vier
vier
Prozent
percent
für
for
Clinton
Clinton
aussprachen
argued
KOUS
CARD
NN
PRF
APPR
NE
KON
CARD
NN
APPR
NE
VVFIN
78 Prozent
f¨ur Bush
vier Prozent
f¨ur Clinton
(S\S)/S[dcl]
w¨ahrend
S[vlast]/(S[vlast]\NP[nom])
f¨ur Clinton
78 Prozent
vier Prozent
The planar tree after preprocessing:
SBAR
The resulting CCG derivation:
S\S
S[vlast]/(S[vlast]\NP[nom])
(S\NP)/(S\NP)
f¨ur Bush
(S\NP)/(S\NP)
sich
S[vlast]/(S[vlast]\NP[nom])
NP[nom]
S[vlast]/(S[vlast]\NP[nom])
NP[nom]
(S\NP)/(S\NP)
conj
und
S[vlast]/(S[vlast]\NP[nom])
S[vlast]/(S[vlast]\NP[nom])
S[vlast]
S[vlast]/(S[vlast]\NP[nom])[conj]
S[vlast]\NP[nom]
aussprachen
CS
S
S
CP SB
OA
SB
HD
MO
MO
NP
NK NK
PP
AC NK
NP
NK NK
PP
AC NK
CJ CD CJ
</figure>
<bodyText confidence="0.973133394736842">
In order to obtain this analysis, we lift such
shared peripheral constituents inside the conjuncts
of conjoined sentences CS (or verb phrases, CVP)
to new S (VP) level that we insert in between the
CS and its parent.
In argument cluster coordination (Figure 3), the
shared peripheral element (aussprachen) is the
head.8 In CCG, the remaining arguments and ad-
juncts combine via composition and typeraising
into a functor category which takes the category of
the head as argument (e.g. a ditransitive verb), and
returns the same category that would result from
a non-coordinated structure (e.g. a VP). The re-
sult category of the furthest element in each con-
junct is equal to the category of the entire VP (or
sentence), and all other elements are type-raised
and composed with this to yield a category which
takes as argument a verb with the required subcat
frame and returns a verb phrase (sentence). Tiger
assumes instead that there are two conjuncts (one
of which is headless), and uses secondary edges
8W¨ahrend has scope over the entire coordinated structure.
to indicate the dependencies between the head and
the elements in the distant conjunct. Coordinated
sentences and VPs (CS and CVP) that have this
annotation are rebracketed to obtain the CCG con-
stituent structure, and the conjuncts are marked as
argument clusters. Since the edges in the argu-
ment cluster are labeled with their correct syntac-
tic functions, we are able to mimic the derivation
during category assignment.
In sentential gapping, the main verb is shared
and appears in the middle of the Þrst conjunct:
(6) Er trinkt Bier und sie Wein.
He drinks beer and she wine.
As in the English CCGbank, we ignore this con-
struction, which requires a non-combinatory “de-
composition” rule (Steedman, 1990).
</bodyText>
<sectionHeader confidence="0.999295" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.98019075">
Translation coverage The algorithm can fail at
several stages. If the graph cannot be turned into a
tree, it cannot be translated. This happens in 1.3%
(647) of all sentences. In many cases, this is due
</bodyText>
<page confidence="0.980672">
510
</page>
<bodyText confidence="0.999988869565217">
to coordinated NPs or PPs where one or more con-
juncts are extraposed. We believe that these are
anaphoric, and further preprocessing could take
care of this. In other cases, this is due to verb top-
icalization (gegeben hat Peter Maria das Buch),
which our algorithm cannot currently deal with.9
For 1.9% of the sentences, the algorithm cannot
obtain a correct CCG derivation. Mostly this is
the case because some traces and extraposed el-
ements cannot be discharged properly. Typically
this happens either in local scrambling, where an
object of the main verb appears between the aux-
iliary and the subject (hat das Buch Peter...)10, or
when an argument of a noun that appears in a rel-
ative clause is extraposed to the right. There are
also a small number of constituents whose head is
not annotated. We ignore any gapping construc-
tion or argument cluster coordination that we can-
not get into the right shape (1.5%), 732 sentences).
There are also a number of other constructions
that we do not currently deal with. We do not pro-
cess sentences if the root of the graph is a “virtual
root” that does not expand into a sentence (1.7%,
869). This is mostly the case for strings such as
Frankfurt (Reuters)), or if we cannot identify a
head child of the root node (1.3%, 648; mostly
fragments or elliptical constructions).
Overall, we obtain CCG derivations for 92.4%
(46,628) of all 54,0474 sentences, including
88.4% (12,122) of those whose Tiger graphs are
marked as discontinuous (13,717), and 95.2%
of all 48,957 full sentences (excluding headless
roots, and fragments, but counting coordinate
structures such as gapping).
Lexicon size There are 2,506 lexical category
types, but 1,018 of these appear only once. 933
category types appear more than 5 times.
Lexical coverage In order to evaluate coverage
of the extracted lexicon on unseen data, we split
the corpus into segments of 5,000 sentences (ig-
noring the last 474), and perform 10-fold cross-
validation, using 9 segments to extract a lexicon
and the 10th to test its coverage. Average cover-
age is 86.7% (by token) of all lexical categories.
Coverage varies between 84.4% and 87.6%. On
average, 92% (90.3%-92.6%) of the lexical tokens
</bodyText>
<footnote confidence="0.642339">
9The corresponding CCG derivation combines the rem-
nant complements as in argument cluster coordination.
</footnote>
<bodyText confidence="0.9704368">
10This problem arises because Tiger annotates subjects as
arguments of the auxiliary. We believe this problem could be
avoided if they were instead arguments of the non-finite verb.
that appear in the held-out data also appear in the
training data. On these seen tokens, coverage is
94.2% (93.5%-92.6%). More than half of all miss-
ing lexical entries are nouns.
In the English CCGbank, a lexicon extracted
from section 02-21 (930,000 tokens) has 94% cov-
erage on all tokens in section 00, and 97.7% cov-
erage on all seen tokens (Hockenmaier and Steed-
man, 2005). In the English data set, the proportion
of seen tokens (96.2%) is much higher, most likely
because of the relative lack of derivational and in-
flectional morphology. The better lexical coverage
on seen tokens is also to be expected, given that the
flexible word order of German requires case mark-
ings on all nouns as well as at least two different
categories for each tensed verb, and more in order
to account for local scrambling.
</bodyText>
<sectionHeader confidence="0.98961" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.9999965625">
We have presented an algorithm which converts
the syntax graphs in the German Tiger corpus
(Brants et al., 2002) into Combinatory Catego-
rial Grammar derivation trees. This algorithm is
currently able to translate 92.4% of all graphs in
Tiger, or 95.2% of all full sentences. Lexicons
extracted from this corpus contain the correct en-
tries for 86.7% of all and 94.2% of all seen to-
kens. Good lexical coverage is essential for the
performance of statistical CCG parsers (Hocken-
maier and Steedman, 2002a). Since the Tiger cor-
pus contains complete morphological and lemma
information for all words, future work will address
the question of how to identify and apply a set of
(non-recursive) lexical rules (Carpenter, 1992) to
the extracted CCG lexicon to create a much larger
lexicon. The number of lexical category types is
almost twice as large as that of the English CCG-
bank. This is to be expected, since our gram-
mar includes case features, and German verbs re-
quire different categories for main and subordinate
clauses. We currently perform only the most es-
sential preprocessing steps, although there are a
number of constructions that might benefit from
additional changes (e.g. comparatives, parentheti-
cals, or fragments), both to increase coverage and
accuracy of the extracted grammar.
Since Tiger corpus is of comparable size to the
Penn Treebank, we hope that the work presented
here will stimulate research into statistical wide-
coverage parsing of free word order languages
such as German with deep grammars like CCG.
</bodyText>
<page confidence="0.996077">
511
</page>
<sectionHeader confidence="0.99896" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999873666666667">
I would like to thank Mark Steedman and Aravind
Joshi for many helpful discussions. This research
is supported by NSF ITR grant 0205456.
</bodyText>
<sectionHeader confidence="0.994327" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999752339622642">
Jason Baldridge. 2002. Lexically SpeciÞed Derivational
Control in Combinatory Categorial Grammar. Ph.D. the-
sis, School of Informatics, University of Edinburgh.
Alena B¨ohomv´a, Jan Hajiÿc, Eva Hajiÿcov´a, and Barbora
Hladk´a. 2003. The Prague Dependency Treebank: Three-
level annotation scenario. In Anne Abeill´e, editor, Tree-
banks: Building and Using Syntactially Annotated Cor-
pora. Kluwer.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lexius, and George Smith. 2002. The TIGER tree-
bank. In Workshop on Treebanks and Linguistic Theories,
Sozpol.
Aoife Cahill, Martin Forst, Mairead McCarthy, Ruth
O’Donovan, Christian Rohrer, Josef van Genabith, and
Andy Way. 2005. Treebank-based acquisition of multilin-
gual unification-grammar resources. Journal of Research
on Language and Computation.
Ruken C¸ akici. 2005. Automatic induction of a CCG gram-
mar for Turkish. In ACL Student Research Workshop,
pages 73–78, Ann Arbor, MI, June.
Bob Carpenter. 1992. Categorial grammars, lexical rules,
and the English predicative. In Robert Levine, editor, For-
mal Grammar: Theory and Implementation, chapter 3.
Oxford University Press.
John Chen, Srinivas Bangalore, and K. Vijay-Shanker. 2005.
Automated extraction of Tree-Adjoining Grammars from
treebanks. Natural Language Engineering.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceedings
of the 42nd Annual Meeting of the Association for Com-
putational Linguistics, Barcelona, Spain.
Amit Dubey and Frank Keller. 2003. Probabilistic parsing
for German using Sister-Head dependencies. In Erhard
Hinrichs and Dan Roth, editors, Proceedings of the 41st
Annual Meeting of the Association for Computational Lin-
guistics, pages 96–103, Sapporo, Japan.
Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and Ivan A.
Sag. 1985. Generalised Phrase Structure Grammar.
Blackwell, Oxford.
Julia Hockenmaier and Mark Steedman. 2002a. Acquir-
ing compact lexicalized grammars from a cleaner Tree-
bank. In Proceedings of the Third International Con-
ference on Language Resources and Evaluation (LREC),
pages 1974–1981, Las Palmas, Spain, May.
Julia Hockenmaier and Mark Steedman. 2002b. Generative
models for statistical parsing with Combinatory Categorial
Grammar. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, pages 335–
342, Philadelphia, PA.
Julia Hockenmaier and Mark Steedman. 2005. CCGbank:
Users’ manual. Technical Report MS-CIS-05-09, Com-
puter and Information Science, University of Pennsylva-
nia.
Beryl Hoffman. 1995. Computational Analysis of the Syntax
and Interpretation of ‘Free’ Word-order in Turkish. Ph.D.
thesis, University of Pennsylvania. IRCS Report 95-17.
Roger Levy and Christopher Manning. 2004. Deep depen-
dencies from context-free statistical parsers: correcting
the surface dependency approximation. In Proceedings
of the 42nd Annual Meeting of the Association for Com-
putational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Linguis-
tics, 19:313–330.
Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic dis-
ambiguation models for wide-coverage HPSG parsing. In
Proceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 83–90, Ann Ar-
bor, MI.
Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii. 2004.
Corpus-oriented grammar development for acquiring a
Head-driven Phrase Structure Grammar from the Penn
Treebank. In Proceedings of the First International Joint
Conference on Natural Language Processing (IJCNLP-
04).
Michael Moortgat and Richard Moot. 2002. Using the Spo-
ken Dutch Corpus for type-logical grammar induction.
In Proceedings of the Third International Conference on
Language Resources and Evaluation (LREC).
Ruth O’Donovan, Michael Burke, Aoife Cahill, Josef van
Genabith, and Andy Way. 2005. Large-scale induc-
tion and evaluation of lexical resources from the Penn-
II and Penn-III Treebanks. Computational Linguistics,
31(3):329 – 365, September.
Owen Rambow. 1994. Formal and Computational Aspects
of Natural Language Syntax. Ph.D. thesis, University of
Pennsylvania, Philadelphia PA.
Libin Shen and Aravind K. Joshi. 2005. Incremental LTAG
parsing. In Proceedings of the Human Language Tech-
nology Conference / Conference of Empirical Methods in
Natural Language Processing (HLT/EMNLP).
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans
Uszkoreit. 1997. An annotation scheme for free word
order languages. In Fifth Conference on Applied Natural
Language Processing.
Mark Steedman. 1990. Gapping as constituent coordination.
Linguistics and Philosophy, 13:207–263.
Mark Steedman. 1996. Surface Structure and Interpretation.
MIT Press, Cambridge, MA. Linguistic Inquiry Mono-
graph, 30.
Mark Steedman. 2000. The Syntactic Process. MIT Press,
Cambridge, MA.
Fei Xia. 1999. Extracting Tree Adjoining Grammars from
bracketed corpora. In Proceedings of the 5th Natural Lan-
guage Processing PaciÞc Rim Symposium (NLPRS-99).
</reference>
<page confidence="0.996976">
512
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.988036">
<title confidence="0.99968">Creating a CCGbank and a wide-coverage CCG lexicon for German</title>
<author confidence="0.999857">Julia Hockenmaier</author>
<affiliation confidence="0.9997985">Institute for Research in Cognitive Science University of Pennsylvania</affiliation>
<address confidence="0.999443">Philadelphia, PA 19104, USA</address>
<email confidence="0.99986">juliahr@cis.upenn.edu</email>
<abstract confidence="0.9981641">We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees. The resulting corpus contains 46,628 derivations, covering 95% of all complete sentences in Tiger. Lexicons extracted from this corpus contain correct lexical entries for 94% of all known tokens in unseen text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
</authors>
<title>Lexically SpeciÞed Derivational Control in Combinatory Categorial Grammar.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="10628" citStr="Baldridge, 2002" startWordPosition="1631" endWordPosition="1632">hybrid framework which contains features of phrase-structure and dependency grammar. Each sentence is represented as a graph whose nodes are labeled with syntactic categories (NP, VP, S, PP, etc.) and POS tags. Edges are directed and labeled with syntactic functions (e.g. head, subject, accusative object, conjunct, appositive). The edge labels are similar to the Penn Treebank function tags, but provide richer and more explicit information. Only 72.5% of the graphs have no crossing edges; the remaining 27.5% are marked as dis1Variants of CCG, such as Set-CCG (Hoffman, 1995) and Multimodal-CCG (Baldridge, 2002), allow a more compact lexicon for free word order languages. 2http://www.ims.uni-stuttgart.de/projekte/TIGER translate(TigerGraph g): TigerTree t = createTree(g); preprocess(t); if (t :A null) CCGderiv d = translateToCCG(t); if (d :A null); if (isCCGderivation(d)) return d; else fail; else fail; else fail; 1. Creating a planar tree: After an initial preprocessing step which inserts punctuation that is attached to the “virtual” root (VROOT) of the graph in the appropriate locations, discontinuous graphs are transformed into planar trees. Starting at the lowest nonterminal nodes, this step turn</context>
</contexts>
<marker>Baldridge, 2002</marker>
<rawString>Jason Baldridge. 2002. Lexically SpeciÞed Derivational Control in Combinatory Categorial Grammar. Ph.D. thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alena B¨ohomv´a</author>
<author>Jan Hajiÿc</author>
<author>Eva Hajiÿcov´a</author>
<author>Barbora Hladk´a</author>
</authors>
<title>The Prague Dependency Treebank: Threelevel annotation scenario.</title>
<date>2003</date>
<editor>In Anne Abeill´e, editor,</editor>
<publisher>Kluwer.</publisher>
<marker>B¨ohomv´a, Hajiÿc, Hajiÿcov´a, Hladk´a, 2003</marker>
<rawString>Alena B¨ohomv´a, Jan Hajiÿc, Eva Hajiÿcov´a, and Barbora Hladk´a. 2003. The Prague Dependency Treebank: Threelevel annotation scenario. In Anne Abeill´e, editor, Treebanks: Building and Using Syntactially Annotated Corpora. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lexius</author>
<author>George Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Workshop on Treebanks and Linguistic Theories,</booktitle>
<location>Sozpol.</location>
<contexts>
<context position="1175" citStr="Brants et al., 2002" startWordPosition="178" endWordPosition="181">n text. 1 Introduction A number of wide-coverage TAG, CCG, LFG and HPSG grammars (Xia, 1999; Chen et al., 2005; Hockenmaier and Steedman, 2002a; O’Donovan et al., 2005; Miyao et al., 2004) have been extracted from the Penn Treebank (Marcus et al., 1993), and have enabled the creation of widecoverage parsers for English which recover local and non-local dependencies that approximate the underlying predicate-argument structure (Hockenmaier and Steedman, 2002b; Clark and Curran, 2004; Miyao and Tsujii, 2005; Shen and Joshi, 2005). However, many corpora (B¨ohomv´a et al., 2003; Skut et al., 1997; Brants et al., 2002) use dependency graphs or other representations, and the extraction algorithms that have been developed for Penn Treebank style corpora may not be immediately applicable to this representation. As a consequence, research on statistical parsing with “deep” grammars has largely been confined to English. Free-word order languages typically pose greater challenges for syntactic theories (Rambow, 1994), and the richer inflectional morphology of these languages creates additional problems both for the coverage of lexicalized formalisms such as CCG or TAG, and for the usefulness of dependency counts </context>
<context position="9854" citStr="Brants et al., 2002" startWordPosition="1507" endWordPosition="1510">Peter Maria das Buch gibt S[emb]/S[vlast] NP[n] NP[d] NP[a] ((S[vlast]\NP[n])\NP[d]�NP[a] &lt; &lt; Our translation algorithm has the following steps: 4.2 The translation algorithm (S[vlast]\NP[n])\NP[d] S[vlast]\NP[n] S[vlast] S[emb] For simplicity’s sake our extraction algorithm ignores the issues that arise through local scrambling, and assumes that there are different lexical category for each permutation.1 Type-raising and composition are also used to deal with wh-extraction and with long-distance scrambling (Figure 2). 4 Translating Tiger graphs into CCG 4.1 The Tiger corpus The Tiger corpus (Brants et al., 2002) is a publicly available2 corpus of ca. 50,000 sentences (almost 900,000 tokens) taken from the Frankfurter Rundschau newspaper. The annotation is based on a hybrid framework which contains features of phrase-structure and dependency grammar. Each sentence is represented as a graph whose nodes are labeled with syntactic categories (NP, VP, S, PP, etc.) and POS tags. Edges are directed and labeled with syntactic functions (e.g. head, subject, accusative object, conjunct, appositive). The edge labels are similar to the Penn Treebank function tags, but provide richer and more explicit information</context>
<context position="25726" citStr="Brants et al., 2002" startWordPosition="4099" endWordPosition="4102">l seen tokens (Hockenmaier and Steedman, 2005). In the English data set, the proportion of seen tokens (96.2%) is much higher, most likely because of the relative lack of derivational and inflectional morphology. The better lexical coverage on seen tokens is also to be expected, given that the flexible word order of German requires case markings on all nouns as well as at least two different categories for each tensed verb, and more in order to account for local scrambling. 6 Conclusion and future work We have presented an algorithm which converts the syntax graphs in the German Tiger corpus (Brants et al., 2002) into Combinatory Categorial Grammar derivation trees. This algorithm is currently able to translate 92.4% of all graphs in Tiger, or 95.2% of all full sentences. Lexicons extracted from this corpus contain the correct entries for 86.7% of all and 94.2% of all seen tokens. Good lexical coverage is essential for the performance of statistical CCG parsers (Hockenmaier and Steedman, 2002a). Since the Tiger corpus contains complete morphological and lemma information for all words, future work will address the question of how to identify and apply a set of (non-recursive) lexical rules (Carpenter,</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lexius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lexius, and George Smith. 2002. The TIGER treebank. In Workshop on Treebanks and Linguistic Theories, Sozpol.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Martin Forst</author>
<author>Mairead McCarthy</author>
<author>Ruth O’Donovan</author>
<author>Christian Rohrer</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Treebank-based acquisition of multilingual unification-grammar resources.</title>
<date>2005</date>
<journal>Journal of Research on Language and Computation.</journal>
<marker>Cahill, Forst, McCarthy, O’Donovan, Rohrer, van Genabith, Way, 2005</marker>
<rawString>Aoife Cahill, Martin Forst, Mairead McCarthy, Ruth O’Donovan, Christian Rohrer, Josef van Genabith, and Andy Way. 2005. Treebank-based acquisition of multilingual unification-grammar resources. Journal of Research on Language and Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruken C¸ akici</author>
</authors>
<title>Automatic induction of a CCG grammar for Turkish.</title>
<date>2005</date>
<booktitle>In ACL Student Research Workshop,</booktitle>
<pages>73--78</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="2380" citStr="akici (2005)" startWordPosition="363" endWordPosition="364">counts extracted from the training data. On the other hand, formalisms such as CCG and TAG are particularly suited to capture the crossing dependencies that arise in languages such as Dutch or German, and by choosing an appropriate linguistic representation, some of these problems may be mitigated. Here, we present an algorithm which translates the German Tiger corpus (Brants et al., 2002) into CCG derivations. Similar algorithms have been developed by Hockenmaier and Steedman (2002a) to create CCGbank, a corpus of CCG derivations (Hockenmaier and Steedman, 2005) from the Penn Treebank, by C¸ akici (2005) to extract a CCG lexicon from a Turkish dependency corpus, and by Moortgat and Moot (2002) to induce a type-logical grammar for Dutch. The annotation scheme used in Tiger is an extension of that used in the earlier, and smaller, German Negra corpus (Skut et al., 1997). Tiger is better suited for the extraction of subcategorization information (and thus the translation into “deep” grammars of any kind), since it distinguishes between PP complements and modifiers, and includes “secondary” edges to indicate shared arguments in coordinate constructions. Tiger also includes morphology and lemma in</context>
</contexts>
<marker>akici, 2005</marker>
<rawString>Ruken C¸ akici. 2005. Automatic induction of a CCG grammar for Turkish. In ACL Student Research Workshop, pages 73–78, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>Categorial grammars, lexical rules, and the English predicative.</title>
<date>1992</date>
<booktitle>Formal Grammar: Theory and Implementation, chapter 3.</booktitle>
<editor>In Robert Levine, editor,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="26332" citStr="Carpenter, 1992" startWordPosition="4199" endWordPosition="4200">al., 2002) into Combinatory Categorial Grammar derivation trees. This algorithm is currently able to translate 92.4% of all graphs in Tiger, or 95.2% of all full sentences. Lexicons extracted from this corpus contain the correct entries for 86.7% of all and 94.2% of all seen tokens. Good lexical coverage is essential for the performance of statistical CCG parsers (Hockenmaier and Steedman, 2002a). Since the Tiger corpus contains complete morphological and lemma information for all words, future work will address the question of how to identify and apply a set of (non-recursive) lexical rules (Carpenter, 1992) to the extracted CCG lexicon to create a much larger lexicon. The number of lexical category types is almost twice as large as that of the English CCGbank. This is to be expected, since our grammar includes case features, and German verbs require different categories for main and subordinate clauses. We currently perform only the most essential preprocessing steps, although there are a number of constructions that might benefit from additional changes (e.g. comparatives, parentheticals, or fragments), both to increase coverage and accuracy of the extracted grammar. Since Tiger corpus is of co</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Bob Carpenter. 1992. Categorial grammars, lexical rules, and the English predicative. In Robert Levine, editor, Formal Grammar: Theory and Implementation, chapter 3. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>Srinivas Bangalore</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Automated extraction of Tree-Adjoining Grammars from treebanks. Natural Language Engineering.</title>
<date>2005</date>
<contexts>
<context position="665" citStr="Chen et al., 2005" startWordPosition="99" endWordPosition="102">on for German Julia Hockenmaier Institute for Research in Cognitive Science University of Pennsylvania Philadelphia, PA 19104, USA juliahr@cis.upenn.edu Abstract We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees. The resulting corpus contains 46,628 derivations, covering 95% of all complete sentences in Tiger. Lexicons extracted from this corpus contain correct lexical entries for 94% of all known tokens in unseen text. 1 Introduction A number of wide-coverage TAG, CCG, LFG and HPSG grammars (Xia, 1999; Chen et al., 2005; Hockenmaier and Steedman, 2002a; O’Donovan et al., 2005; Miyao et al., 2004) have been extracted from the Penn Treebank (Marcus et al., 1993), and have enabled the creation of widecoverage parsers for English which recover local and non-local dependencies that approximate the underlying predicate-argument structure (Hockenmaier and Steedman, 2002b; Clark and Curran, 2004; Miyao and Tsujii, 2005; Shen and Joshi, 2005). However, many corpora (B¨ohomv´a et al., 2003; Skut et al., 1997; Brants et al., 2002) use dependency graphs or other representations, and the extraction algorithms that have b</context>
</contexts>
<marker>Chen, Bangalore, Vijay-Shanker, 2005</marker>
<rawString>John Chen, Srinivas Bangalore, and K. Vijay-Shanker. 2005. Automated extraction of Tree-Adjoining Grammars from treebanks. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1040" citStr="Clark and Curran, 2004" startWordPosition="155" endWordPosition="158">all complete sentences in Tiger. Lexicons extracted from this corpus contain correct lexical entries for 94% of all known tokens in unseen text. 1 Introduction A number of wide-coverage TAG, CCG, LFG and HPSG grammars (Xia, 1999; Chen et al., 2005; Hockenmaier and Steedman, 2002a; O’Donovan et al., 2005; Miyao et al., 2004) have been extracted from the Penn Treebank (Marcus et al., 1993), and have enabled the creation of widecoverage parsers for English which recover local and non-local dependencies that approximate the underlying predicate-argument structure (Hockenmaier and Steedman, 2002b; Clark and Curran, 2004; Miyao and Tsujii, 2005; Shen and Joshi, 2005). However, many corpora (B¨ohomv´a et al., 2003; Skut et al., 1997; Brants et al., 2002) use dependency graphs or other representations, and the extraction algorithms that have been developed for Penn Treebank style corpora may not be immediately applicable to this representation. As a consequence, research on statistical parsing with “deep” grammars has largely been confined to English. Free-word order languages typically pose greater challenges for syntactic theories (Rambow, 1994), and the richer inflectional morphology of these languages creat</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCG and log-linear models. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Dubey</author>
<author>Frank Keller</author>
</authors>
<title>Probabilistic parsing for German using Sister-Head dependencies.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>96--103</pages>
<editor>In Erhard Hinrichs and Dan Roth, editors,</editor>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3277" citStr="Dubey and Keller (2003)" startWordPosition="505" endWordPosition="508">er is better suited for the extraction of subcategorization information (and thus the translation into “deep” grammars of any kind), since it distinguishes between PP complements and modifiers, and includes “secondary” edges to indicate shared arguments in coordinate constructions. Tiger also includes morphology and lemma information. Negra is also provided with a “Penn Treebank”- style representation, which uses flat phrase structure trees instead of the crossing dependency structures in the original corpus. This version has been used by Cahill et al. (2005) to extract a German LFG. However, Dubey and Keller (2003) have demonstrated that lexicalization does not help a Collins-style parser that is trained on this corpus, and Levy and Manning (2004) have shown that its context-free representation is a poor approximation to the underlying dependency structure. The resource presented here will enable future research to address the question whether “deep” grammars such as CCG, which capture the underlying dependencies directly, are better suited to parsing German than linguistically inadequate context-free approximations. 505 Proceedings of the 21st International Conference on Computational Linguistics and 4</context>
</contexts>
<marker>Dubey, Keller, 2003</marker>
<rawString>Amit Dubey and Frank Keller. 2003. Probabilistic parsing for German using Sister-Head dependencies. In Erhard Hinrichs and Dan Roth, editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 96–103, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey K Pullum</author>
<author>Ivan A Sag</author>
</authors>
<title>Generalised Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="17383" citStr="Gazdar et al., 1985" startWordPosition="2733" endWordPosition="2736">however, we found that not all stative (“sein”) passives seem to be annotated as such. 6In some contexts, measure nouns (e.g. Mark, Kilometer) lack case annotation. dren under a new S node which becomes the second daughter of the SBAR. The relative pronoun is the head of this SBAR and takes the S-node as argument. Its category is S[vlast], since all clauses with a complementizer are verb-final. In order to capture the long-range dependency, a “trace” is introduced, and percolated down the tree, much like in the algorithm of Hockenmaier and Steedman (2005), and similar to GPSG’s slash-passing (Gazdar et al., 1985). These trace categories are appended to the category of the head node (and other arguments are type-raised as necessary). In our case, the trace is also associated with the verb whose argument it is. If the span of this verb is within the span of a complement, the trace is percolated down this complement. When the VP that is headed by this verb is reached, we assume a canonical order of arguments in order to “discharge” the trace. If a complement node is marked as extraposed, it is also percolated down the head tree until the constituent whose argument it is is found. When another complement </context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and Ivan A. Sag. 1985. Generalised Phrase Structure Grammar. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Acquiring compact lexicalized grammars from a cleaner Treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1974--1981</pages>
<location>Las Palmas, Spain,</location>
<contexts>
<context position="697" citStr="Hockenmaier and Steedman, 2002" startWordPosition="103" endWordPosition="106"> Hockenmaier Institute for Research in Cognitive Science University of Pennsylvania Philadelphia, PA 19104, USA juliahr@cis.upenn.edu Abstract We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees. The resulting corpus contains 46,628 derivations, covering 95% of all complete sentences in Tiger. Lexicons extracted from this corpus contain correct lexical entries for 94% of all known tokens in unseen text. 1 Introduction A number of wide-coverage TAG, CCG, LFG and HPSG grammars (Xia, 1999; Chen et al., 2005; Hockenmaier and Steedman, 2002a; O’Donovan et al., 2005; Miyao et al., 2004) have been extracted from the Penn Treebank (Marcus et al., 1993), and have enabled the creation of widecoverage parsers for English which recover local and non-local dependencies that approximate the underlying predicate-argument structure (Hockenmaier and Steedman, 2002b; Clark and Curran, 2004; Miyao and Tsujii, 2005; Shen and Joshi, 2005). However, many corpora (B¨ohomv´a et al., 2003; Skut et al., 1997; Brants et al., 2002) use dependency graphs or other representations, and the extraction algorithms that have been developed for Penn Treebank </context>
<context position="2255" citStr="Hockenmaier and Steedman (2002" startWordPosition="341" endWordPosition="344">languages creates additional problems both for the coverage of lexicalized formalisms such as CCG or TAG, and for the usefulness of dependency counts extracted from the training data. On the other hand, formalisms such as CCG and TAG are particularly suited to capture the crossing dependencies that arise in languages such as Dutch or German, and by choosing an appropriate linguistic representation, some of these problems may be mitigated. Here, we present an algorithm which translates the German Tiger corpus (Brants et al., 2002) into CCG derivations. Similar algorithms have been developed by Hockenmaier and Steedman (2002a) to create CCGbank, a corpus of CCG derivations (Hockenmaier and Steedman, 2005) from the Penn Treebank, by C¸ akici (2005) to extract a CCG lexicon from a Turkish dependency corpus, and by Moortgat and Moot (2002) to induce a type-logical grammar for Dutch. The annotation scheme used in Tiger is an extension of that used in the earlier, and smaller, German Negra corpus (Skut et al., 1997). Tiger is better suited for the extraction of subcategorization information (and thus the translation into “deep” grammars of any kind), since it distinguishes between PP complements and modifiers, and inc</context>
<context position="26113" citStr="Hockenmaier and Steedman, 2002" startWordPosition="4162" endWordPosition="4166">ast two different categories for each tensed verb, and more in order to account for local scrambling. 6 Conclusion and future work We have presented an algorithm which converts the syntax graphs in the German Tiger corpus (Brants et al., 2002) into Combinatory Categorial Grammar derivation trees. This algorithm is currently able to translate 92.4% of all graphs in Tiger, or 95.2% of all full sentences. Lexicons extracted from this corpus contain the correct entries for 86.7% of all and 94.2% of all seen tokens. Good lexical coverage is essential for the performance of statistical CCG parsers (Hockenmaier and Steedman, 2002a). Since the Tiger corpus contains complete morphological and lemma information for all words, future work will address the question of how to identify and apply a set of (non-recursive) lexical rules (Carpenter, 1992) to the extracted CCG lexicon to create a much larger lexicon. The number of lexical category types is almost twice as large as that of the English CCGbank. This is to be expected, since our grammar includes case features, and German verbs require different categories for main and subordinate clauses. We currently perform only the most essential preprocessing steps, although the</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002a. Acquiring compact lexicalized grammars from a cleaner Treebank. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC), pages 1974–1981, Las Palmas, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>335--342</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="697" citStr="Hockenmaier and Steedman, 2002" startWordPosition="103" endWordPosition="106"> Hockenmaier Institute for Research in Cognitive Science University of Pennsylvania Philadelphia, PA 19104, USA juliahr@cis.upenn.edu Abstract We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees. The resulting corpus contains 46,628 derivations, covering 95% of all complete sentences in Tiger. Lexicons extracted from this corpus contain correct lexical entries for 94% of all known tokens in unseen text. 1 Introduction A number of wide-coverage TAG, CCG, LFG and HPSG grammars (Xia, 1999; Chen et al., 2005; Hockenmaier and Steedman, 2002a; O’Donovan et al., 2005; Miyao et al., 2004) have been extracted from the Penn Treebank (Marcus et al., 1993), and have enabled the creation of widecoverage parsers for English which recover local and non-local dependencies that approximate the underlying predicate-argument structure (Hockenmaier and Steedman, 2002b; Clark and Curran, 2004; Miyao and Tsujii, 2005; Shen and Joshi, 2005). However, many corpora (B¨ohomv´a et al., 2003; Skut et al., 1997; Brants et al., 2002) use dependency graphs or other representations, and the extraction algorithms that have been developed for Penn Treebank </context>
<context position="2255" citStr="Hockenmaier and Steedman (2002" startWordPosition="341" endWordPosition="344">languages creates additional problems both for the coverage of lexicalized formalisms such as CCG or TAG, and for the usefulness of dependency counts extracted from the training data. On the other hand, formalisms such as CCG and TAG are particularly suited to capture the crossing dependencies that arise in languages such as Dutch or German, and by choosing an appropriate linguistic representation, some of these problems may be mitigated. Here, we present an algorithm which translates the German Tiger corpus (Brants et al., 2002) into CCG derivations. Similar algorithms have been developed by Hockenmaier and Steedman (2002a) to create CCGbank, a corpus of CCG derivations (Hockenmaier and Steedman, 2005) from the Penn Treebank, by C¸ akici (2005) to extract a CCG lexicon from a Turkish dependency corpus, and by Moortgat and Moot (2002) to induce a type-logical grammar for Dutch. The annotation scheme used in Tiger is an extension of that used in the earlier, and smaller, German Negra corpus (Skut et al., 1997). Tiger is better suited for the extraction of subcategorization information (and thus the translation into “deep” grammars of any kind), since it distinguishes between PP complements and modifiers, and inc</context>
<context position="26113" citStr="Hockenmaier and Steedman, 2002" startWordPosition="4162" endWordPosition="4166">ast two different categories for each tensed verb, and more in order to account for local scrambling. 6 Conclusion and future work We have presented an algorithm which converts the syntax graphs in the German Tiger corpus (Brants et al., 2002) into Combinatory Categorial Grammar derivation trees. This algorithm is currently able to translate 92.4% of all graphs in Tiger, or 95.2% of all full sentences. Lexicons extracted from this corpus contain the correct entries for 86.7% of all and 94.2% of all seen tokens. Good lexical coverage is essential for the performance of statistical CCG parsers (Hockenmaier and Steedman, 2002a). Since the Tiger corpus contains complete morphological and lemma information for all words, future work will address the question of how to identify and apply a set of (non-recursive) lexical rules (Carpenter, 1992) to the extracted CCG lexicon to create a much larger lexicon. The number of lexical category types is almost twice as large as that of the English CCGbank. This is to be expected, since our grammar includes case features, and German verbs require different categories for main and subordinate clauses. We currently perform only the most essential preprocessing steps, although the</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002b. Generative models for statistical parsing with Combinatory Categorial Grammar. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 335– 342, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: Users’ manual.</title>
<date>2005</date>
<tech>Technical Report MS-CIS-05-09,</tech>
<institution>Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="2337" citStr="Hockenmaier and Steedman, 2005" startWordPosition="353" endWordPosition="356">isms such as CCG or TAG, and for the usefulness of dependency counts extracted from the training data. On the other hand, formalisms such as CCG and TAG are particularly suited to capture the crossing dependencies that arise in languages such as Dutch or German, and by choosing an appropriate linguistic representation, some of these problems may be mitigated. Here, we present an algorithm which translates the German Tiger corpus (Brants et al., 2002) into CCG derivations. Similar algorithms have been developed by Hockenmaier and Steedman (2002a) to create CCGbank, a corpus of CCG derivations (Hockenmaier and Steedman, 2005) from the Penn Treebank, by C¸ akici (2005) to extract a CCG lexicon from a Turkish dependency corpus, and by Moortgat and Moot (2002) to induce a type-logical grammar for Dutch. The annotation scheme used in Tiger is an extension of that used in the earlier, and smaller, German Negra corpus (Skut et al., 1997). Tiger is better suited for the extraction of subcategorization information (and thus the translation into “deep” grammars of any kind), since it distinguishes between PP complements and modifiers, and includes “secondary” edges to indicate shared arguments in coordinate constructions. </context>
<context position="7388" citStr="Hockenmaier and Steedman (2005)" startWordPosition="1126" endWordPosition="1129">ent Y to their left or right (depending on the the direction of the slash) and yield a result X. Every syntactic category is paired with a semantic interpretation (usually a A-term). Like all variants of categorial grammar, CCG uses function application to combine constituents, but it also uses a set of combinatory rules such as composition (B) and type-raising (l). Non-orderpreserving type-raising is used for topicalization: Application: X/Y Y X Y X\Y X Composition: X/Y Y/Z =:�B X/Z X/Y Y\Z =:�B X\Z Y\Z X\Y =:�B X\Z Y/Z X\Y =:�B X/Z Type-raising: X =z&gt;T T\(T/X) Topicalization: X =z&gt;T T/(T/X) Hockenmaier and Steedman (2005) advocate the use of additional “type-changing” rules to deal with complex adjunct categories (e.g. (NP\NP) ==&gt; S[ng]\NP for ing-VPs that act as noun phrase modifiers). Here, we also use a small number of such rules to deal with similar adjunct cases. 506 3.2 Capturing German word order We follow Steedman (2000) in assuming that the underlying word order in main clauses is always verb-initial, and that the sententce-initial subject is in fact topicalized. This enables us to capture different word orders with the same lexical category (Figure 1). We use the features S[v1] and S[vlast] to distin</context>
<context position="14829" citStr="Hockenmaier and Steedman (2005)" startWordPosition="2296" endWordPosition="2299">ther preprocessing is required to create the required structures for wh-extraction and certain coordination phenomena (see below). In figure 2, preprocessing of the original Tiger graph (top) yields the tree shown in the middle (edge labels are shown as Penn Treebank-style function tags).4 We will first present the basic translation algorithm before we explain how we obtain a derivation which captures the dependency between the relative pronoun and the embedded verb. 4We treat reflexive pronouns as modifiers. 508 3. The basic translation step Our basic translation algorithm is very similar to Hockenmaier and Steedman (2005). It requires a planar tree without crossing edges, where each node is marked as head, complement or adjunct. The latter information is represented in the Tiger edge labels, and only a small number of additional head rules is required. Each individual translation step operates on local trees, which are typically flat. N Cl C2 ... Cg ... Cn_1 C, Assuming the CCG category of N is x, and its head position is i, the algorithm traverses first the left nodes C1 ...Ci_1 from left to right to create a right-branching derivation tree, and then the right nodes (C,,,...Ci+1) from right to left to create </context>
<context position="17324" citStr="Hockenmaier and Steedman (2005)" startWordPosition="2723" endWordPosition="2727">ing chil5Eventive (“werden”) passive is easily identified by context; however, we found that not all stative (“sein”) passives seem to be annotated as such. 6In some contexts, measure nouns (e.g. Mark, Kilometer) lack case annotation. dren under a new S node which becomes the second daughter of the SBAR. The relative pronoun is the head of this SBAR and takes the S-node as argument. Its category is S[vlast], since all clauses with a complementizer are verb-final. In order to capture the long-range dependency, a “trace” is introduced, and percolated down the tree, much like in the algorithm of Hockenmaier and Steedman (2005), and similar to GPSG’s slash-passing (Gazdar et al., 1985). These trace categories are appended to the category of the head node (and other arguments are type-raised as necessary). In our case, the trace is also associated with the verb whose argument it is. If the span of this verb is within the span of a complement, the trace is percolated down this complement. When the VP that is headed by this verb is reached, we assume a canonical order of arguments in order to “discharge” the trace. If a complement node is marked as extraposed, it is also percolated down the head tree until the constitu</context>
<context position="25152" citStr="Hockenmaier and Steedman, 2005" startWordPosition="3999" endWordPosition="4003">ion combines the remnant complements as in argument cluster coordination. 10This problem arises because Tiger annotates subjects as arguments of the auxiliary. We believe this problem could be avoided if they were instead arguments of the non-finite verb. that appear in the held-out data also appear in the training data. On these seen tokens, coverage is 94.2% (93.5%-92.6%). More than half of all missing lexical entries are nouns. In the English CCGbank, a lexicon extracted from section 02-21 (930,000 tokens) has 94% coverage on all tokens in section 00, and 97.7% coverage on all seen tokens (Hockenmaier and Steedman, 2005). In the English data set, the proportion of seen tokens (96.2%) is much higher, most likely because of the relative lack of derivational and inflectional morphology. The better lexical coverage on seen tokens is also to be expected, given that the flexible word order of German requires case markings on all nouns as well as at least two different categories for each tensed verb, and more in order to account for local scrambling. 6 Conclusion and future work We have presented an algorithm which converts the syntax graphs in the German Tiger corpus (Brants et al., 2002) into Combinatory Categori</context>
</contexts>
<marker>Hockenmaier, Steedman, 2005</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2005. CCGbank: Users’ manual. Technical Report MS-CIS-05-09, Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beryl Hoffman</author>
</authors>
<title>Computational Analysis of the Syntax and Interpretation of ‘Free’ Word-order in Turkish.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="10591" citStr="Hoffman, 1995" startWordPosition="1627" endWordPosition="1628">aper. The annotation is based on a hybrid framework which contains features of phrase-structure and dependency grammar. Each sentence is represented as a graph whose nodes are labeled with syntactic categories (NP, VP, S, PP, etc.) and POS tags. Edges are directed and labeled with syntactic functions (e.g. head, subject, accusative object, conjunct, appositive). The edge labels are similar to the Penn Treebank function tags, but provide richer and more explicit information. Only 72.5% of the graphs have no crossing edges; the remaining 27.5% are marked as dis1Variants of CCG, such as Set-CCG (Hoffman, 1995) and Multimodal-CCG (Baldridge, 2002), allow a more compact lexicon for free word order languages. 2http://www.ims.uni-stuttgart.de/projekte/TIGER translate(TigerGraph g): TigerTree t = createTree(g); preprocess(t); if (t :A null) CCGderiv d = translateToCCG(t); if (d :A null); if (isCCGderivation(d)) return d; else fail; else fail; else fail; 1. Creating a planar tree: After an initial preprocessing step which inserts punctuation that is attached to the “virtual” root (VROOT) of the graph in the appropriate locations, discontinuous graphs are transformed into planar trees. Starting at the low</context>
</contexts>
<marker>Hoffman, 1995</marker>
<rawString>Beryl Hoffman. 1995. Computational Analysis of the Syntax and Interpretation of ‘Free’ Word-order in Turkish. Ph.D. thesis, University of Pennsylvania. IRCS Report 95-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher Manning</author>
</authors>
<title>Deep dependencies from context-free statistical parsers: correcting the surface dependency approximation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3412" citStr="Levy and Manning (2004)" startWordPosition="526" endWordPosition="529">nce it distinguishes between PP complements and modifiers, and includes “secondary” edges to indicate shared arguments in coordinate constructions. Tiger also includes morphology and lemma information. Negra is also provided with a “Penn Treebank”- style representation, which uses flat phrase structure trees instead of the crossing dependency structures in the original corpus. This version has been used by Cahill et al. (2005) to extract a German LFG. However, Dubey and Keller (2003) have demonstrated that lexicalization does not help a Collins-style parser that is trained on this corpus, and Levy and Manning (2004) have shown that its context-free representation is a poor approximation to the underlying dependency structure. The resource presented here will enable future research to address the question whether “deep” grammars such as CCG, which capture the underlying dependencies directly, are better suited to parsing German than linguistically inadequate context-free approximations. 505 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 505–512, Sydney, July 2006. c�2006 Association for Computational Linguistics 1. Standard main clau</context>
</contexts>
<marker>Levy, Manning, 2004</marker>
<rawString>Roger Levy and Christopher Manning. 2004. Deep dependencies from context-free statistical parsers: correcting the surface dependency approximation. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="808" citStr="Marcus et al., 1993" startWordPosition="123" endWordPosition="126">is.upenn.edu Abstract We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees. The resulting corpus contains 46,628 derivations, covering 95% of all complete sentences in Tiger. Lexicons extracted from this corpus contain correct lexical entries for 94% of all known tokens in unseen text. 1 Introduction A number of wide-coverage TAG, CCG, LFG and HPSG grammars (Xia, 1999; Chen et al., 2005; Hockenmaier and Steedman, 2002a; O’Donovan et al., 2005; Miyao et al., 2004) have been extracted from the Penn Treebank (Marcus et al., 1993), and have enabled the creation of widecoverage parsers for English which recover local and non-local dependencies that approximate the underlying predicate-argument structure (Hockenmaier and Steedman, 2002b; Clark and Curran, 2004; Miyao and Tsujii, 2005; Shen and Joshi, 2005). However, many corpora (B¨ohomv´a et al., 2003; Skut et al., 1997; Brants et al., 2002) use dependency graphs or other representations, and the extraction algorithms that have been developed for Penn Treebank style corpora may not be immediately applicable to this representation. As a consequence, research on statistic</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>83--90</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="1064" citStr="Miyao and Tsujii, 2005" startWordPosition="159" endWordPosition="162">n Tiger. Lexicons extracted from this corpus contain correct lexical entries for 94% of all known tokens in unseen text. 1 Introduction A number of wide-coverage TAG, CCG, LFG and HPSG grammars (Xia, 1999; Chen et al., 2005; Hockenmaier and Steedman, 2002a; O’Donovan et al., 2005; Miyao et al., 2004) have been extracted from the Penn Treebank (Marcus et al., 1993), and have enabled the creation of widecoverage parsers for English which recover local and non-local dependencies that approximate the underlying predicate-argument structure (Hockenmaier and Steedman, 2002b; Clark and Curran, 2004; Miyao and Tsujii, 2005; Shen and Joshi, 2005). However, many corpora (B¨ohomv´a et al., 2003; Skut et al., 1997; Brants et al., 2002) use dependency graphs or other representations, and the extraction algorithms that have been developed for Penn Treebank style corpora may not be immediately applicable to this representation. As a consequence, research on statistical parsing with “deep” grammars has largely been confined to English. Free-word order languages typically pose greater challenges for syntactic theories (Rambow, 1994), and the richer inflectional morphology of these languages creates additional problems b</context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 83–90, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus-oriented grammar development for acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank.</title>
<date>2004</date>
<booktitle>In Proceedings of the First International Joint Conference on Natural Language Processing (IJCNLP04).</booktitle>
<contexts>
<context position="743" citStr="Miyao et al., 2004" startWordPosition="111" endWordPosition="114">University of Pennsylvania Philadelphia, PA 19104, USA juliahr@cis.upenn.edu Abstract We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees. The resulting corpus contains 46,628 derivations, covering 95% of all complete sentences in Tiger. Lexicons extracted from this corpus contain correct lexical entries for 94% of all known tokens in unseen text. 1 Introduction A number of wide-coverage TAG, CCG, LFG and HPSG grammars (Xia, 1999; Chen et al., 2005; Hockenmaier and Steedman, 2002a; O’Donovan et al., 2005; Miyao et al., 2004) have been extracted from the Penn Treebank (Marcus et al., 1993), and have enabled the creation of widecoverage parsers for English which recover local and non-local dependencies that approximate the underlying predicate-argument structure (Hockenmaier and Steedman, 2002b; Clark and Curran, 2004; Miyao and Tsujii, 2005; Shen and Joshi, 2005). However, many corpora (B¨ohomv´a et al., 2003; Skut et al., 1997; Brants et al., 2002) use dependency graphs or other representations, and the extraction algorithms that have been developed for Penn Treebank style corpora may not be immediately applicabl</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2004</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii. 2004. Corpus-oriented grammar development for acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank. In Proceedings of the First International Joint Conference on Natural Language Processing (IJCNLP04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Moortgat</author>
<author>Richard Moot</author>
</authors>
<title>Using the Spoken Dutch Corpus for type-logical grammar induction.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="2471" citStr="Moortgat and Moot (2002)" startWordPosition="378" endWordPosition="381">CCG and TAG are particularly suited to capture the crossing dependencies that arise in languages such as Dutch or German, and by choosing an appropriate linguistic representation, some of these problems may be mitigated. Here, we present an algorithm which translates the German Tiger corpus (Brants et al., 2002) into CCG derivations. Similar algorithms have been developed by Hockenmaier and Steedman (2002a) to create CCGbank, a corpus of CCG derivations (Hockenmaier and Steedman, 2005) from the Penn Treebank, by C¸ akici (2005) to extract a CCG lexicon from a Turkish dependency corpus, and by Moortgat and Moot (2002) to induce a type-logical grammar for Dutch. The annotation scheme used in Tiger is an extension of that used in the earlier, and smaller, German Negra corpus (Skut et al., 1997). Tiger is better suited for the extraction of subcategorization information (and thus the translation into “deep” grammars of any kind), since it distinguishes between PP complements and modifiers, and includes “secondary” edges to indicate shared arguments in coordinate constructions. Tiger also includes morphology and lemma information. Negra is also provided with a “Penn Treebank”- style representation, which uses </context>
</contexts>
<marker>Moortgat, Moot, 2002</marker>
<rawString>Michael Moortgat and Richard Moot. 2002. Using the Spoken Dutch Corpus for type-logical grammar induction. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruth O’Donovan</author>
<author>Michael Burke</author>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Large-scale induction and evaluation of lexical resources from the PennII and Penn-III Treebanks.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<pages>365</pages>
<marker>O’Donovan, Burke, Cahill, van Genabith, Way, 2005</marker>
<rawString>Ruth O’Donovan, Michael Burke, Aoife Cahill, Josef van Genabith, and Andy Way. 2005. Large-scale induction and evaluation of lexical resources from the PennII and Penn-III Treebanks. Computational Linguistics, 31(3):329 – 365, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
</authors>
<title>Formal and Computational Aspects of Natural Language Syntax.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia PA.</location>
<contexts>
<context position="1575" citStr="Rambow, 1994" startWordPosition="237" endWordPosition="238">te-argument structure (Hockenmaier and Steedman, 2002b; Clark and Curran, 2004; Miyao and Tsujii, 2005; Shen and Joshi, 2005). However, many corpora (B¨ohomv´a et al., 2003; Skut et al., 1997; Brants et al., 2002) use dependency graphs or other representations, and the extraction algorithms that have been developed for Penn Treebank style corpora may not be immediately applicable to this representation. As a consequence, research on statistical parsing with “deep” grammars has largely been confined to English. Free-word order languages typically pose greater challenges for syntactic theories (Rambow, 1994), and the richer inflectional morphology of these languages creates additional problems both for the coverage of lexicalized formalisms such as CCG or TAG, and for the usefulness of dependency counts extracted from the training data. On the other hand, formalisms such as CCG and TAG are particularly suited to capture the crossing dependencies that arise in languages such as Dutch or German, and by choosing an appropriate linguistic representation, some of these problems may be mitigated. Here, we present an algorithm which translates the German Tiger corpus (Brants et al., 2002) into CCG deriv</context>
<context position="5781" citStr="Rambow, 1994" startWordPosition="878" endWordPosition="879">erb-initial (2). If a modifier or one of the objects is moved to the front, the word order becomes verb-initial (2). Subordinate and relative clauses are verb-final (3): (1) a. Peter gibt Maria das Buch. Peter gives Mary the book. b. ein Buch gibt Peter Maria. c. dann gibt Peter Maria das Buch. (2) a. Gibt Peter Maria das Buch? b. Gib Maria das Buch! (3) a. dass Peter Maria das Buch gibt. b. das Buch, das Peter Maria gibt. Local Scrambling In the so-called “Mittelfeld” all orders of arguments and adjuncts are potentially possible. In the following example, all 5! permutations are grammatical (Rambow, 1994): (4) dass [eine Firma] [meinem Onkel] [die M¨obel] [vor drei Tagen] [ohne Voranmeldung] zugestellt hat. that [a company] [to my uncle] [the furniture] [three days ago] [without notice] delivered has. Long-distance scrambling Objects of embedded verbs can also be extraposed unboundedly within the same sentence (Rambow, 1994): (5) dass [den Schrank] [niemand] [zu reparieren] versprochen hat. that [the wardrobe] [nobody] [to repair] promised has. 3 A CCG for German 3.1 Combinatory Categorial Grammar CCG (Steedman (1996; 2000)) is a lexicalized grammar formalism with a completely transparent synt</context>
</contexts>
<marker>Rambow, 1994</marker>
<rawString>Owen Rambow. 1994. Formal and Computational Aspects of Natural Language Syntax. Ph.D. thesis, University of Pennsylvania, Philadelphia PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>Incremental LTAG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference / Conference of Empirical Methods in Natural Language Processing (HLT/EMNLP).</booktitle>
<contexts>
<context position="1087" citStr="Shen and Joshi, 2005" startWordPosition="163" endWordPosition="166">ted from this corpus contain correct lexical entries for 94% of all known tokens in unseen text. 1 Introduction A number of wide-coverage TAG, CCG, LFG and HPSG grammars (Xia, 1999; Chen et al., 2005; Hockenmaier and Steedman, 2002a; O’Donovan et al., 2005; Miyao et al., 2004) have been extracted from the Penn Treebank (Marcus et al., 1993), and have enabled the creation of widecoverage parsers for English which recover local and non-local dependencies that approximate the underlying predicate-argument structure (Hockenmaier and Steedman, 2002b; Clark and Curran, 2004; Miyao and Tsujii, 2005; Shen and Joshi, 2005). However, many corpora (B¨ohomv´a et al., 2003; Skut et al., 1997; Brants et al., 2002) use dependency graphs or other representations, and the extraction algorithms that have been developed for Penn Treebank style corpora may not be immediately applicable to this representation. As a consequence, research on statistical parsing with “deep” grammars has largely been confined to English. Free-word order languages typically pose greater challenges for syntactic theories (Rambow, 1994), and the richer inflectional morphology of these languages creates additional problems both for the coverage of</context>
</contexts>
<marker>Shen, Joshi, 2005</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2005. Incremental LTAG parsing. In Proceedings of the Human Language Technology Conference / Conference of Empirical Methods in Natural Language Processing (HLT/EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Skut</author>
<author>Brigitte Krenn</author>
<author>Thorsten Brants</author>
<author>Hans Uszkoreit</author>
</authors>
<title>An annotation scheme for free word order languages.</title>
<date>1997</date>
<booktitle>In Fifth Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="1153" citStr="Skut et al., 1997" startWordPosition="174" endWordPosition="177">own tokens in unseen text. 1 Introduction A number of wide-coverage TAG, CCG, LFG and HPSG grammars (Xia, 1999; Chen et al., 2005; Hockenmaier and Steedman, 2002a; O’Donovan et al., 2005; Miyao et al., 2004) have been extracted from the Penn Treebank (Marcus et al., 1993), and have enabled the creation of widecoverage parsers for English which recover local and non-local dependencies that approximate the underlying predicate-argument structure (Hockenmaier and Steedman, 2002b; Clark and Curran, 2004; Miyao and Tsujii, 2005; Shen and Joshi, 2005). However, many corpora (B¨ohomv´a et al., 2003; Skut et al., 1997; Brants et al., 2002) use dependency graphs or other representations, and the extraction algorithms that have been developed for Penn Treebank style corpora may not be immediately applicable to this representation. As a consequence, research on statistical parsing with “deep” grammars has largely been confined to English. Free-word order languages typically pose greater challenges for syntactic theories (Rambow, 1994), and the richer inflectional morphology of these languages creates additional problems both for the coverage of lexicalized formalisms such as CCG or TAG, and for the usefulness</context>
<context position="2649" citStr="Skut et al., 1997" startWordPosition="410" endWordPosition="413"> of these problems may be mitigated. Here, we present an algorithm which translates the German Tiger corpus (Brants et al., 2002) into CCG derivations. Similar algorithms have been developed by Hockenmaier and Steedman (2002a) to create CCGbank, a corpus of CCG derivations (Hockenmaier and Steedman, 2005) from the Penn Treebank, by C¸ akici (2005) to extract a CCG lexicon from a Turkish dependency corpus, and by Moortgat and Moot (2002) to induce a type-logical grammar for Dutch. The annotation scheme used in Tiger is an extension of that used in the earlier, and smaller, German Negra corpus (Skut et al., 1997). Tiger is better suited for the extraction of subcategorization information (and thus the translation into “deep” grammars of any kind), since it distinguishes between PP complements and modifiers, and includes “secondary” edges to indicate shared arguments in coordinate constructions. Tiger also includes morphology and lemma information. Negra is also provided with a “Penn Treebank”- style representation, which uses flat phrase structure trees instead of the crossing dependency structures in the original corpus. This version has been used by Cahill et al. (2005) to extract a German LFG. Howe</context>
</contexts>
<marker>Skut, Krenn, Brants, Uszkoreit, 1997</marker>
<rawString>Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans Uszkoreit. 1997. An annotation scheme for free word order languages. In Fifth Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Gapping as constituent coordination. Linguistics and Philosophy,</title>
<date>1990</date>
<pages>13--207</pages>
<contexts>
<context position="22080" citStr="Steedman, 1990" startWordPosition="3487" endWordPosition="3488">oordinated sentences and VPs (CS and CVP) that have this annotation are rebracketed to obtain the CCG constituent structure, and the conjuncts are marked as argument clusters. Since the edges in the argument cluster are labeled with their correct syntactic functions, we are able to mimic the derivation during category assignment. In sentential gapping, the main verb is shared and appears in the middle of the Þrst conjunct: (6) Er trinkt Bier und sie Wein. He drinks beer and she wine. As in the English CCGbank, we ignore this construction, which requires a non-combinatory “decomposition” rule (Steedman, 1990). 5 Evaluation Translation coverage The algorithm can fail at several stages. If the graph cannot be turned into a tree, it cannot be translated. This happens in 1.3% (647) of all sentences. In many cases, this is due 510 to coordinated NPs or PPs where one or more conjuncts are extraposed. We believe that these are anaphoric, and further preprocessing could take care of this. In other cases, this is due to verb topicalization (gegeben hat Peter Maria das Buch), which our algorithm cannot currently deal with.9 For 1.9% of the sentences, the algorithm cannot obtain a correct CCG derivation. Mos</context>
</contexts>
<marker>Steedman, 1990</marker>
<rawString>Mark Steedman. 1990. Gapping as constituent coordination. Linguistics and Philosophy, 13:207–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Surface Structure and Interpretation.</title>
<date>1996</date>
<journal>Linguistic Inquiry Monograph,</journal>
<volume>30</volume>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6303" citStr="Steedman (1996" startWordPosition="956" endWordPosition="957">ially possible. In the following example, all 5! permutations are grammatical (Rambow, 1994): (4) dass [eine Firma] [meinem Onkel] [die M¨obel] [vor drei Tagen] [ohne Voranmeldung] zugestellt hat. that [a company] [to my uncle] [the furniture] [three days ago] [without notice] delivered has. Long-distance scrambling Objects of embedded verbs can also be extraposed unboundedly within the same sentence (Rambow, 1994): (5) dass [den Schrank] [niemand] [zu reparieren] versprochen hat. that [the wardrobe] [nobody] [to repair] promised has. 3 A CCG for German 3.1 Combinatory Categorial Grammar CCG (Steedman (1996; 2000)) is a lexicalized grammar formalism with a completely transparent syntax-semantics interface. Since CCG is mildly context-sensitive, it can capture the crossing dependencies that arise in Dutch or German, yet is efficiently parseable. In categorial grammar, words are associated with syntactic categories, such as S\NP or (S\NP)/NP for English intransitive and transitive verbs. Categories of the form X/Y or X\Y are functors, which take an argument Y to their left or right (depending on the the direction of the slash) and yield a result X. Every syntactic category is paired with a semanti</context>
</contexts>
<marker>Steedman, 1996</marker>
<rawString>Mark Steedman. 1996. Surface Structure and Interpretation. MIT Press, Cambridge, MA. Linguistic Inquiry Monograph, 30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7701" citStr="Steedman (2000)" startWordPosition="1180" endWordPosition="1181">s composition (B) and type-raising (l). Non-orderpreserving type-raising is used for topicalization: Application: X/Y Y X Y X\Y X Composition: X/Y Y/Z =:�B X/Z X/Y Y\Z =:�B X\Z Y\Z X\Y =:�B X\Z Y/Z X\Y =:�B X/Z Type-raising: X =z&gt;T T\(T/X) Topicalization: X =z&gt;T T/(T/X) Hockenmaier and Steedman (2005) advocate the use of additional “type-changing” rules to deal with complex adjunct categories (e.g. (NP\NP) ==&gt; S[ng]\NP for ing-VPs that act as noun phrase modifiers). Here, we also use a small number of such rules to deal with similar adjunct cases. 506 3.2 Capturing German word order We follow Steedman (2000) in assuming that the underlying word order in main clauses is always verb-initial, and that the sententce-initial subject is in fact topicalized. This enables us to capture different word orders with the same lexical category (Figure 1). We use the features S[v1] and S[vlast] to distinguish verbs in main and subordinate clauses. Main clauses have the feature S[dc�], requiring either a sentential modifierwith category S[dcl]/S[vl], a topicalized subject (S[dc�]/(S[vl]/NP[nom])), or a type-raised argument (S[dc�]/(S[vl]\x)), where x can be any argument category, such as a noun phrase, prepositi</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Extracting Tree Adjoining Grammars from bracketed corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 5th Natural Language Processing PaciÞc Rim Symposium (NLPRS-99).</booktitle>
<contexts>
<context position="646" citStr="Xia, 1999" startWordPosition="97" endWordPosition="98">e CCG lexicon for German Julia Hockenmaier Institute for Research in Cognitive Science University of Pennsylvania Philadelphia, PA 19104, USA juliahr@cis.upenn.edu Abstract We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees. The resulting corpus contains 46,628 derivations, covering 95% of all complete sentences in Tiger. Lexicons extracted from this corpus contain correct lexical entries for 94% of all known tokens in unseen text. 1 Introduction A number of wide-coverage TAG, CCG, LFG and HPSG grammars (Xia, 1999; Chen et al., 2005; Hockenmaier and Steedman, 2002a; O’Donovan et al., 2005; Miyao et al., 2004) have been extracted from the Penn Treebank (Marcus et al., 1993), and have enabled the creation of widecoverage parsers for English which recover local and non-local dependencies that approximate the underlying predicate-argument structure (Hockenmaier and Steedman, 2002b; Clark and Curran, 2004; Miyao and Tsujii, 2005; Shen and Joshi, 2005). However, many corpora (B¨ohomv´a et al., 2003; Skut et al., 1997; Brants et al., 2002) use dependency graphs or other representations, and the extraction alg</context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>Fei Xia. 1999. Extracting Tree Adjoining Grammars from bracketed corpora. In Proceedings of the 5th Natural Language Processing PaciÞc Rim Symposium (NLPRS-99).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>