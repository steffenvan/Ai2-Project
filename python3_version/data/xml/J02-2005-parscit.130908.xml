<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.774099">
Book Reviews
Spotting and Discovering Terms through Natural Language
Processing
</title>
<author confidence="0.96771">
Christian Jacquemin
</author>
<affiliation confidence="0.941221">
(University of Paris 11)
</affiliation>
<address confidence="0.707526">
Cambridge, MA: The MIT Press, 2001,
viii+378 pp; hardbound, ISBN
</address>
<figure confidence="0.501643666666667">
0-262-10085-1, $52.95, £36.50
Reviewed by
Sophia Ananiadou
</figure>
<affiliation confidence="0.750226">
University of Salford
</affiliation>
<subsubsectionHeader confidence="0.77122">
Christian Jacquemin’s book Spotting and Discovering Terms through Natural Language Pro-
</subsubsectionHeader>
<bodyText confidence="0.9999609375">
cessing is a much needed and most welcome addition to the field of computational
terminology. The central issue of this book is the in-depth examination of term vari-
ation, that is, the morphological, syntactic, or semantic transformation of multiword
terms, capturing the fact that the same concept can be expressed through term vari-
ants. From an application point of view, dealing with term variation would result in
more efficient indexing engines and better term extraction tools, among other applica-
tions. The book also describes in depth the author’s FASTR system, a natural language
processor for term variation. There are plenty of examples throughout the book and
a set of metarules in the appendix for the interested reader who would like to use
FASTR.
Chapter 2 provides an excellent overview and a comparison of numerous algo-
rithms for automatic term extraction systems as well as techniques for recognizing
multiword terms. Six systems are examined in detail. The author also discusses the
related area of automatic indexing, the main purpose of which is to assign content de-
scriptors to documents. He describes 11 studies of phrase indexing (indexing through
multiword units). A useful comparison of the different indexers is provided (p. 113).
The aim of this extended discussion of concerns, methods, and systems is to introduce
the main ideas behind FASTR, which is used for both term recognition and termi-
nological enrichment. The identification of variants of terms is crucial in Jacquemin’s
term-spotting technique, and because such variation occurs in corpora frequently, “it
makes sense to build an indexer on a variational mechanism” (p. 115). In addition,
variants represent on average 28% of multiword term occurrences (p. 219).
Since term spotting uses various linguistic features, FASTR’s grammar formal-
ism is unification-based, inspired by PATR-II, in order to represent different types
of features. Information is embedded in nontyped feature structures, which include
two additional facilities: disjunction and negation. Term rules in FASTR are composed
of a context-free skeleton that describes the constituent structure of terms and logi-
cal constraints (feature structures) that denote the information linked to the nodes of
the context-free skeleton. The morphological model of FASTR is concatenative mor-
phology enriched with feature structures and a list of suffixes. This applies to both
inflectional and derivational morphology, although in FASTR there are two ways of
describing derivational morphology: dynamically (similar to inflection) and statically,
</bodyText>
<note confidence="0.534118">
Computational Linguistics Volume 28, Number 2
</note>
<bodyText confidence="0.999946647058824">
where derivational links between words and their stems are explicitly stated in the
single-word lexicon. To represent the syntax of multiword terms and to cope with term
variability, FASTR’s formalism uses the notions of extended domain of locality and lexical-
ization from lexicalized tree adjoining grammars (LTAGs) (Abeill´e and Rambow 2000).
Jacquemin’s approach relies on metarules that exploit syntagmatic and paradig-
matic information. Syntagmatically, they describe structural mappings between, say, a
multiword term and its syntactic or phrasal variants. Paradigmatically, they describe
lexical relationships between the words of a term and the words of its variants that par-
ticipate in the mappings. These lexical relationships may be morphological or semantic
in nature or both. Morphological relationships cover words that belong to the same
derivational family and express the fact that such words share a common root. Seman-
tic relationships cover words that are linked (by synonymy or antonymy, for example);
that is, they capture the fact that the words have something in common semantically.
Taken together, the syntagmatic and paradigmatic elements of metarules provide com-
plementary constraints that prevent the generation of undesirable mappings.
Chapter 4 is dedicated to the metagrammar of FASTR, which operates on top of
the lexical and terminological data. Metarules dynamically transform rules written for
controlled terms into new rules that, in turn, can be used to extract variants from
texts. The concept of metarule is inspired by the work of Harris (Harris et al. 1989)
and is further influenced by generalized phase structure grammars (Gazdar et al.
1985) and feature-based LTAGs (Srinivas et al. 1994) and by work on lexically based
formalisms that seek to reduce the complexity and size of the grammar. The author
contrasts his use of metarules with other formalisms (p. 157). The main features are
that metarules in FASTR are more generic than those in other formalisms and, since
they are dedicated to term and variant extraction, they are also much simpler. Of
further interest in FASTR’s metarule formalism is that it contains a compiler and
an interpreter of regular expressions, and this allows the output of more complex
structures (for example, coordinated structures).
Term variation is explicitly described for English through four types of elemen-
tary term variation: coordination, permutation, modification/substitution, and eli-
sion. Chapter 5 provides a practical description of how to build, tune, and evaluate
metarules for syntactic term variants in English that can be produced from a controlled
vocabulary (this vocabulary would typically be the result of the term extraction phase
of FASTR). The chapter begins by describing variations of binary terms, for example,
how to link a binary compound term such as tooth root with a noun phrase having the
same meaning, the root of a lower premolar tooth, by means of permutation metarules.
Section 5.2 describes how to extend variations from binary to n-ary terms, taking into
account all structural ambiguities. A detailed description is given for an example of
variation involving coordination, as this is one of the most difficult phenomena to
tackle. Section 5.4 illustrates how the metagrammar can be further tuned experimen-
tally, using occurrences extracted from the corpus through paradigmatic rules. These
paradigmatic rules are refined and transformed into filtering metarules (i.e., metarules
augmented with constraints), which are then used by FASTR for extracting term vari-
ants. Evaluation of the different variants extracted by FASTR shows high precision and
recall. The author reports poor performance in the case of elliptic variants, however,
which he notes is largely due to the noncontextuality of FASTR’s parsing approach
(p. 169).
The recognition of morphosyntactic variants (as opposed to the syntactic variants
of Chapter 5) is dealt with in Chapter 7. FASTR’s formalism is extended to include
metarules for morphosyntactic variations; for example, development of mouse embryos is
considered a morphosyntactic rather than a syntactic variant of embryonic development,
</bodyText>
<page confidence="0.996348">
218
</page>
<subsectionHeader confidence="0.774724">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.99992805">
because the adjective embryonic is transformed into the noun embryo in the variant (p.
273). Single words contain morphological links between words and their root lemma
(for English, derived from the CELEX database); these links are also expressed in the
metagrammar of morphosyntactic variation. Variants of binary terms are presented
one morphological transformation at a time: adjective to noun variation, noun to verb
variation, and so on. The analysis of these variants is important, as terminology work
has concentrated mostly on nouns, leaving unexplored non-nominal terminological
occurrences, which are equally important, as they capture relations between terms.
Semantic variation, described briefly in Chapter 8, concludes the study of types
of term variation. FASTR’s formalism is further extended to account for this type of
variation. For example, benign mouse skin tumors can be recognized as a semantic variant
of benign neoplasms provided that there is a semantic link between the words tumors
and neoplasms and that the insertion of the modifier mouse skin is accepted (p. 299).
The semantic links used for extracting semantic variants in English are the synonymy
links of WordNet 1.6. Morphosyntactic term variation can be seen as a special case of
semantic variation, since legitimate morphological links can be established between
semantically related words.
I found it more useful to read Chapter 6 after Chapters 5, 7, and 8. It describes how
FASTR can be used for “incremental term enrichment,” taking into account existing
terms and trying to relate newly acquired terms to the previously existing terminolo-
gies. The idea is that, by deconstructing term variants, we can detect possible term
associations and acquire new terms. “The variant uterine and carotid artery of uterine
artery is the opportunity for discovering the term carotid artery. The context of acquisi-
tion shows that both terms can be coordinated, indicating that the meanings of artery
in the original term uterine artery and in the candidate term carotid artery are similar: a
blood vessel” (p. 241). The analysis of such variants produces a set of patterns, called
pattern extractors. Each pattern extractor is then attached to a specific metarule. Starting
from any variant detected by such a metarule, the extractor outputs the corresponding
candidate term. This technique of term enrichment is also very useful for producing
conceptual relations that can be used for automatic thesaurus construction.
In conclusion, this book is clearly written and explores an interesting and very
applicable area of computational terminology: term variation. An in-depth analysis of
current techniques in terminology, a detailed bibliography, and a plethora of examples
demonstrating how to write rules for and use FASTR make this book an important
acquisition not only for researchers interested in computational terminology but also
for the wider natural language processing community. Anyone concerned with issues
of automatic indexing, automatic thesaurus construction, rapid adaptation to new do-
mains, and robust processing of not only domain-specific terminology in the classical
sense but also the phrases in which variants of terms are found will learn much from
this book.
</bodyText>
<sectionHeader confidence="0.97708" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.78451005">
Abeill ´e, Anne and Owen Rambow, editors.
2000. Tree Adjoining Grammars: Formalisms,
Linguistic Analysis and Processing. CSLI,
Stanford, CA.
Gazdar, Gerald, Ewan Klein, Geoffrey K.
Pullum, and Ivan A. Sag. 1985. Generalized
Phrase Structure Grammar. Harvard
University Press.
Harris, Zellig S., Michael Gottfried, Thomas
Ryckman, Paul Mattick, Jr., Anne Daladier,
T. N. Harris, and S. Harris. 1989. The Form
of Information in Science: Analysis of
Immunology Sublanguage (Boston Studies
in the Philosophy of Science, volume 104).
Kluwer Academic.
Srinivas, B., Dania Egedi, Christine Doran,
and Tilman Becker. 1994. Lexicalization
and grammar development. In Harald
Trost, editor, Proceedings of KONVENS ’94,
Vienna, pages 310–319.
</reference>
<page confidence="0.998697">
219
</page>
<bodyText confidence="0.859284">
Computational Linguistics Volume 28, Number 2
Sophia Ananiadou is a Senior Lecturer in Computer Science at the University of Salford, U.K.
She teaches natural language processing, and her main research interests are in computational
terminology, ontologies, and information extraction. Currently, she is applying natural language
processing techniques to bioinformatics. Ananiadou’s address is: Computer Science, School of
Sciences, University of Salford, Salford M5 4WT, U.K.; e-mail: S.Ananiadou@salford.ac.uk.
</bodyText>
<page confidence="0.991328">
220
</page>
<figure confidence="0.3232272">
Book Reviews
Automatic Summarization
Inderjeet Mani
(MITRE Corporation and Georgetown University)
Amsterdam: John Benjamins (Natural
</figure>
<footnote confidence="0.4366045">
language processing series, edited by
Ruslan Mitkov, volume 3), 2001,
xi+285 pp; hardbound, ISBN
1-58811-059-1, $82.00; paperbound,
ISBN 1-58811-060-5, $29.95
Reviewed by
Chris D. Paice
Lancaster University
</footnote>
<bodyText confidence="0.999904228571429">
Though the first attempts at automatic abstracting were made about 45 years ago, this
area was until quite recently a rather obscure specialism. No doubt, this was due to the
nonavailability of machine-readable texts: It seemed absurd to go to the trouble and
expense of keypunching complete texts and then use a program to throw away 90%
of them! Since about 1990, with the urgent need to manage the mass of information
available on the World Wide Web, the field of automatic summarization has exploded
into prominence.
Automatic Summarization is the first monograph on the subject, and as such it is
sure to be widely read and cited. The author, Inderjeet Mani, is currently one of the
field’s most active researchers, and it is therefore no surprise that the book has an
authoritative feel. In general, it provides good, balanced coverage of the field (except
that cross-lingual summarization is not covered) and describes a great deal of the
published research in the area, with a bibliography of well over 200 items. Mani
succeeds in conveying a sense that this is a most challenging field for system designers.
With a grim topicality, many of the illustrations used refer to the summarization of
reports on terrorist incidents.
The book is structured sensibly into nine main chapters. Chapter 1, “Preliminar-
ies,” introduces a range of fundamental terms and ideas. A distinction is made between
extracts, consisting of material (typically sentences) taken straight from the source, and
abstracts, “at least some of whose material is not present in the input.” Chapter 2 looks
at professional (i.e., nonautomatic) summarizing.
Chapter 3 is about extraction, which involves assigning an importance score to
each sentence in a text and outputting those with the highest score. Various importance
clues may be used, including the presence of concentrations of content words, use of
cue expressions such as we see that and certainly, and the location of the sentence in
the text. A long section describes and discusses the work by Edmundson and by
later workers pursuing similar approaches. Coming up to date, the chapter continues
with a discussion of corpus-based approaches to sentence extraction. Chapter 4 is
entitled “Revision” and discusses how to deal with problems of cohesion in extracted
sentences that arise from the occurrence of anaphors and other awkward features.
Chapter 5 discusses the potential use of discourse-level information in approaches
such as cohesion graphs, text tiling, lexical chains, and rhetorical structure theory.
In Chapter 6, the author turns to abstraction, as opposed to sentence extraction.
Typical approaches here have involved the use of sketchy scripts or frames tailored
to a particular domain. Analysis of a text results in instantiation of the appropriate
</bodyText>
<page confidence="0.993174">
221
</page>
<note confidence="0.443466">
Computational Linguistics Volume 28, Number 2
</note>
<bodyText confidence="0.999945392156863">
frame, and when this process is complete, output can be generated, either as stylized
“canned text” or by the use of proper text generation techniques. A major problem
with this approach is its inflexibility: A text can be summarized only if the correct
frame is available and can be identified.
Chapter 7 deals with multidocument summarization, which is now seen as a most
important area (terrorism news reports, again). This is not simply a matter of merging
a number of separate descriptions of the same issue or event: There is a need to high-
light discrepancies between different reports and to deal with time-related issues (e.g.,
changing estimates of numbers of casualties). Another nontrivial problem is handling
cross-document coreferences—for example, ensuring that mentions of Johnson in two
different documents actually refer to the same person.
Chapter 8, “Multimedia Summarization,” is in my view the weakest in the book.
Mani starts by acknowledging that this area is evolving rapidly, so that “no clear
principles have emerged” (p. 209). But he then takes this as an excuse for a rather
haphazard collection of topics (summarization of dialog, of video, and of diagrams,
and multimedia briefing generation) dealt with in varying degrees of detail. The chap-
ter is only 12 pages long (nine pages of actual text), whereas to begin to tackle this
area, at least twice the space is surely required. To give a proper structure, the chapter
should begin with a discussion of possibilities for the direct analysis of various nontext
media, followed by an examination, with examples, of how different media (especially
text) can support one another in building useful systems.
Chapter 9, on the evaluation of summarization systems, is fully three times as
long as the previous chapter, and there is no space here to go into details. Though
the exposition gets a bit tangled in places, the account is generally authoritative and
thorough and provides a valuable overview of this problematic area.
All the main chapters from Chapter 3 onward finish with (1) a table summarizing
the various approaches discussed, with their strengths and weaknesses, and (2) a
glossary giving a brief definition of the various terms introduced in the chapter. These
are both valuable features, though it seems structurally inconsistent that the former
are included as ordinary numbered tables and the latter as separate “Review” sections.
Since this is the only textbook in its field, it will probably become required read-
ing for new researchers, who will assume that it provides a full and balanced view.
Its very patchy presentation of historical research therefore seems regrettable. Surely,
the seminal 1958 paper by H. P. Luhn deserves a section of its own? Apart from its
historical importance, this work provides a clear and clean introduction to the sen-
tence extraction paradigm. Luhn’s paper is in fact mentioned twice in the book, but in
neither case is it very prominent. Neither is there any mention of James E. Rush and
his work on the ADAM system (Rush, Salvador, and Zamora 1971) (though the suc-
cessor system for the Chemical Abstracts Service is discussed). Chapter 4, on sentence
revision, is short and should certainly have included a summary of the pioneering
efforts by Mathis, Rush, and Young (1973). So little research was carried out prior to
the 1980s that it seems a pity to ignore so much of it! (By contrast, the ideas of Skoro-
chod’ko [1972] on cohesion graphs are—quite properly—discussed at some length in
Chapter 5.)
Although (Chapter 8 apart) I have no problems with the general structure and
content of the book, there were various minor defects that for me rather spoiled the
overall impression. First, although on the whole the book is readable enough, there
are a few places where the description of processes or ideas becomes so convoluted or
condensed that even repeated rereading failed to reveal the meaning. In some places,
intelligibility would have been much improved by proper use of examples (p. 82 is a
glaring example). I also felt that some of the formulae could have been justified and
</bodyText>
<page confidence="0.994659">
222
</page>
<subsectionHeader confidence="0.949404">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999978833333333">
explained a bit more fully. The diagrams vary between clear and pertinent to obscure
or curious (two figures in Chapter 7 seem to show a succession of aquaria with various
documents suspended inside them).
A book like this is certainly going to be used very much for reference purposes
and thus the reader should be able to find her way around it. Here, in fact, is my
biggest gripe. In a short space of time, I was able to compile a list of 10 phrasal terms
that were discussed in the text but did not appear either in the index or in the relevant
Review section. Moreover, cross-referencing within the text is minimal. In some cases,
there are references to equations or diagrams that are many pages away, necessitating
a tedious search. Finally, neither the section numbers nor the page headers include the
chapter number, making it hard to find one’s way about (though this was probably
not the author’s decision).
I have actually found this quite a hard book to review. As a textbook, it provides
a valuable map of the field, and yet, as far as I can detect, there is nothing here that
is notably innovative or controversial; there is very little to focus on except the minor
flaws. But taken as a whole, the book certainly provides a timely and informative
overview of automatic summarization, and all researchers with an interest in this
important field will wish to have it on their bookshelves.
</bodyText>
<sectionHeader confidence="0.551179" genericHeader="keywords">
References
</sectionHeader>
<bodyText confidence="0.504373275862069">
Luhn, H. P. 1958. The automatic creation of
literature abstracts. IBM Journal of Research
and Development, 2(2):159–165. Reprinted in
Inderjeet Mani and Mark T. Maybury,
editors, Advances in Text Summarization,
MIT Press, 1999, pages 15–21.
Mathis, B. A., J. E. Rush, and C. E. Young.
1973. Improvement of automatic abstracts
by the use of structural analysis. Journal of
the American Society for Information Science,
24(2):101–109.
Rush, J. E., J. R. Salvador, and A. Zamora.
1971. Automatic abstracting and indexing.
II. Production of indicative abstracts by
application of contextual inference and
syntactic coherence criteria. Journal of the
American Society for Information Science,
22(4):260–274.
Skorochod’ko, E. F. 1972. Adaptive method
of automatic abstracting and indexing. In
C. V. Freiman, editor, Information
Processing 71: Proceedings of the IFIP
Congress 71, Ljubljana, Yugoslavia, North
Holland, 1179–1182.
Chris Paice teaches in the Computing Department at Lancaster University, U.K. He has re-
search interests in various aspects of information retrieval and text processing and has pub-
lished several papers on automatic abstracting, the first appearing in 1981. Paice’s address
is: Computing Department, Lancaster University, Bailrigg, Lancaster LA1 4YR, U.K.; e-mail:
cdp@comp.lancs.ac.uk.
</bodyText>
<page confidence="0.991483">
223
</page>
<note confidence="0.325902">
Computational Linguistics Volume 28, Number 2
</note>
<subsectionHeader confidence="0.710491">
Efficient Processing with Constraint-Logic Grammars Using
Grammar Compilation
</subsectionHeader>
<bodyText confidence="0.862248">
Guido Minnen
(Motorola Labs)
Stanford: CSLI Publications (Stanford
monographs in linguistics), 2001,
viii+255 pp; distributed by University
of Chicago Press; hardbound, ISBN
1-57586-305-7, $55.00, £35.00;
paperbound, ISBN 0-57586-306-5,
$20.00, £13.00
</bodyText>
<subsubsectionHeader confidence="0.325646">
Reviewed by
Suresh Manandhar
University of York
</subsubsectionHeader>
<sectionHeader confidence="0.861693" genericHeader="introduction">
1. Book Content
</sectionHeader>
<bodyText confidence="0.99953548">
This monograph should be of interest to researchers working on building practical
and efficient methods for processing highly abstract declarative constraint-based gram-
mars, primarily head-driven phrase structure grammar (HPSG) (Pollard and Sag 1994).
The work should also be of interest to researchers in the logic-programming commu-
nity. The monograph is accessible to anyone with a background in logic programming.
Background in grammar formalisms or HPSG is not essential to follow the monograph.
Minnen has done a commendable job in making the monograph relatively easy to fol-
low by using numerous well-explained examples. A number of typographic errors in
the example programs, however, and a crucial missing figure (Figure 4.17) make the
reading somewhat more difficult than it need have been.
Declarative constraint-based grammars are notorious for being highly inefficient
from a processing point of view. Minnen’s techniques, several of which are explored
in the monograph, automatically transform the input grammar into a more specialized
grammar that efficiently realizes the user’s goal. The techniques can be viewed as per-
forming equivalence transformations on the input logic program to derive a logic pro-
gram that is more efficient with respect to the input goal. These transformations range
from simple strategies such as literal rearrangement to more complex ones such as build-
ing recursion reversal and magic templates. Although most of these techniques are closely
related to work by other researchers (Strzalkowski 1994, Ramakrishnan 1991), Minnen
has extended them to make them suitable for dealing with feature-based grammars.
The monograph is divided into three parts: top-down control, bottom-up control,
and lexical rules. Whereas the top-down and bottom-up control chapters primarily
deal with grammar rule compilation, the lexical-rules chapter deals with methods for
processing lexical rules.
Central to the extraction of control information are two notions:
</bodyText>
<listItem confidence="0.99753525">
• the adornment of a literal, which identifies those arguments in a literal that
are bound.
• the degree of nondeterminism (DoN) of a literal, which is the number of
alternatives or choice points available when evaluating the literal.
</listItem>
<page confidence="0.991764">
224
</page>
<subsectionHeader confidence="0.893062">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.983303333333333">
Given a user-specified goal and a user-specified global DoN, Minnen’s grammar com-
pilation procedures attempt to transform the grammar nondeterministically into an
equivalent grammar until the transformed grammar meets the global DoN require-
ment.
Minnen’s method can be understood generally in terms of two mutually recursive
methods:
</bodyText>
<listItem confidence="0.998983">
• Adornment of a goal is used to perform a static abstract interpretation of
the program to determine the DoN of each literal.
• The information gained from abstract interpretation is employed to
perform a program transformation.
</listItem>
<bodyText confidence="0.997375285714286">
Minnen couples program transformation with relatively sophisticated tabulation tech-
niques to store partial solutions and hence minimize the cost of processing recursive
goals with shared subgoals. His tabulation method stores failed goals, successful goals,
and currently “opened” goals.
Chapter 3 describes literal rearrangement, in which the literals of a clause are re-
arranged to make the goal more deterministic. Literal rearrangement employs adorn-
ment information specified in the user goal to determine the best literal rearrangement
to achieve the specified DoN. Literal rearrangement, although fairly simple in princi-
ple, requires a recursive search through the clauses of each literal and hence can be
costly. Tabulation becomes essential here. In addition, Minnen describes a local heuris-
tic that chooses the literal with the most arguments (or feature paths, in the case of
constraint-based grammars) instantiated to minimize the cost of this search. A simple
iterative deepening search over the literal rearrangement procedure starting with DoN
= 1 finds the literal rearrangement program transformation with the smallest possible
DoN.
When information from two literals is simultaneously required to successfully
constrain the processing task, such as in certain treatments of German partial verb
phrase topicalization (Example (1)), then literal rearrangement alone is insufficient. In
fact, in such cases, Minnen states that coroutining or parallel processing of literals is
required. He then goes on to show that coroutining can be simulated by unfolding the
literals and applying literal transformation to the resultant clause.
</bodyText>
<listItem confidence="0.901749">
(1) [Anna lieben]i wird i Karl.
</listItem>
<bodyText confidence="0.9999594">
The second problem that literal rearrangement cannot solve is left recursion, since
in a grammar such as the one shown in Figure 1, described by Minnen (p. 72), it is
not the literal order that is problematic but that the base case needs to be processed as
early as possible. Chapter 4 explores techniques for automatically detecting when such
a building recursion reversal (BRR) transformation can be applied and describes methods
for implementing it correctly. This type of transformation is more involved than the
simpler literal rearrangement transformation of the previous chapter, as it involves
analyzing argument instantiation patterns called argument sequences. For example, in
the program in Figure 1, one possible argument sequence (if we trace the second
argument of vp/4) is the sequence &lt;Comps,[Comp|Comps]&gt; (given in simplified form
here). Roughly speaking, a building recursion reversal changes the order of argument
sequences from a sequence such as &lt;Comps,[Comp|Comps]&gt;, which builds structure, to
the reversed sequence &lt;[Comp|Comps],Comps&gt;, which consumes structure.
Part 2 covers bottom-up processing using magic transformations, which transform
the original program by adding an additional literal, known as a magic rule, to the
</bodyText>
<page confidence="0.992248">
225
</page>
<note confidence="0.34907">
Computational Linguistics Volume 28, Number 2
</note>
<construct confidence="0.817871777777778">
vp(Subj, Comps, VSem, P0, P):-
vp(Subj, [Comp|Comps], VSem, P0, P1),
np(Comp, P1, P0).
vp(Subj, Comps, VSem, P0, P):-
v(Subj, Comps, VSem, P0, P).
v(Subj, [Obj,IObj], bring(Subj,IObj,Obj),[bring|P],P).
np(john, [john|P], P).
np(flowers, [flowers|P], P).
np(mary, [mary|P], P).
</construct>
<figureCaption confidence="0.916969">
Figure 1
</figureCaption>
<bodyText confidence="0.98570248">
Example program from Minnen (p. 86).
start of the right-hand side of each clause. In the example, magic transformation of
the predicate vp/4 of the program shown in Figure 1 with respect to the goal shown
in Figure 2 would result in the clauses shown in Figure 3. The magic rule magic_vp/4
acts as a guard by instantiating variables and filters out subgoals that cannot be part
of the main goal. In a naive bottom-up strategy, all facts are used to deduce the goal.
Naive bottom-up processing is expensive in terms of both space and time. The magic
rule provides a top-down filtering component and hence makes an otherwise purely
data-driven control more goal driven. Minnen explores both the Earley deduction
strategy and an improved seminaive bottom-up processing strategy and concludes
that the two are very similar, with the semi-naive strategy being slightly better in
terms of space requirements. He concedes, however, that to ensure termination, both
a subsumption check and an abstraction function are necessary. Subsumption checks
are computationally expensive, and abstraction functions have to be user specified,
which is a big disadvantage.
The final part of the monograph deals with the treatment of HPSG lexical rules.
This work builds upon work reported by Meurers and Minnen (1997). The nice part
is that, in Minnen’s setup, lexical rules can be viewed as definite clauses, so that
techniques from the previous chapters directly apply to lexical rules. As in Meurers
and Minnen (1997), lexical rule interaction (through which the output of a lexical
rule can be the input of another lexical rule) is modeled by means of a finite-state
automaton, computed off-line, that precompiles all the possible interactions between
lexical rules. Minnen shows that nondeterminism in lexical rule expansion can be
minimized by combining partial unfolding with lexical rule interaction. In this way, a
large number of choice points that lead to failure can be eliminated at compile time.
</bodyText>
<sectionHeader confidence="0.992088" genericHeader="method">
2. Final Analysis
</sectionHeader>
<bodyText confidence="0.999218">
Minnen’s monograph provides a refreshing entry point for someone wanting to pur-
sue a research program in efficient implementation of constraint-based grammars.
Minnen’s work complements work on compilation techniques for typed-feature hi-
</bodyText>
<page confidence="0.246849">
vp(Subj,Comps,bring(john,flowers,mary),P0,P1)
</page>
<figureCaption confidence="0.754311">
Figure 2
User goal.
</figureCaption>
<page confidence="0.97135">
226
</page>
<table confidence="0.826202818181818">
Book Reviews
vp(Subj, Comps, VSem, P0, P):-
magic_vp(Subj,Comps,VSem, P0, P1),
vp(Subj, [Comp|Comps], VSem, P0, P1),
np(Comp, P1, P0).
vp(Subj, Comps, VSem, P0, P):-
magic_vp(Subj,Comps,VSem, P0, P1),
v(Subj, Comps, VSem, P0, P).
magic_vp(Subj,Comps,bring(john,flowers,mary), P0, P1).
Figure 3
Application of magic transformation to vp/4 from Figure 1.
</table>
<bodyText confidence="0.998581095238095">
erarchies (cf. Fall 1996, Wintner and Francez 1995). Coupling Minnen’s techniques
with compilation methods for typed-feature hierarchies should provide the necessary
mechanisms for efficient implementation of large HPSG grammars.
A fair amount of work still remains: For an automated grammar-compilation sys-
tem, it is essential that as much of the control information be extracted automatically as
possible. Minnen’s work, however, falls short of achieving this objective. His top-down
processing strategy employing literal transformation and BRR transformation comes
close to being a fully automated strategy, but I suspect that space requirements from
tabulation becomes a factor, and hence (heuristic) techniques for making tabulation
decisions need to be explored before the approach can be made practical. Somewhat
surprisingly, Minnen does not explore the behavior of other variations on the basic
top-down strategy, such as deterministic closure. It was not clear to me why the BRR
transformation could not be used along with top-down control.
To ensure termination, Minnen’s bottom-up control requires additional user-
supplied control information in the form of parse types and delay patterns, which is not
very desirable. Either automated generation of such control information or methods to
eliminate it are needed. Although it is clear that Minnen’s transformation techniques
apply to typed-feature-structure grammars, there are hardly any examples of this in
the monograph. Results of evaluation on a realistic grammar are only glossed over or
missing, making it impossible to assess performance on large HPSG grammars such
as the LinGo grammar (Copestake and Flickinger 2000).
</bodyText>
<sectionHeader confidence="0.740424" genericHeader="method">
References
</sectionHeader>
<bodyText confidence="0.95928425">
Copestake, Ann and Dan Flickinger. 2000.
An open-source grammar development
environment and broad-coverage English
grammar using HPSG. In Proceedings of the
</bodyText>
<subsubsectionHeader confidence="0.437615">
Second Linguistic Resources and Evaluation
</subsubsectionHeader>
<reference confidence="0.425121923076923">
Conference, Athens, Greece, pages 591–600.
Fall, Andrew. 1996. Reasoning with
Taxonomies. Ph.D. dissertation, Department
of Computer Science, Simon Fraser
University, July 1996.
Meurers, Detmar and Guido Minnen. 1997.
A computational treatment of lexical rules
in HPSG as covariation in lexical entries.
Computational Linguistics, 23(4):543–568.
Pollard, Carl and Ivan Andrew Sag. 1994.
Head-Driven Phrase Structure Grammar.
Chicago: University of Chicago Press and
Stanford: CSLI Publications.
Ramakrishnan, Raghu. 1991. Magic
templates: A spellbinding approach to
logic programs. Journal of Logic
Programming, 11:189–216.
Strzalkowski, Tomek, editor. 1994. Reversible
Grammar in Natural Language Processing.
Kluwer Academic.
Wintner, Shuly and Nissim Francez. 1995.
An abstract machine for typed feature
structures. In Proceedings of the Fifth
Workshop on Natural Language
Understanding and Logic Programming,
Lisbon, pages 205–220.
</reference>
<page confidence="0.691517">
227
</page>
<table confidence="0.501373">
Computational Linguistics Volume 28, Number 2
</table>
<tableCaption confidence="0.679295428571429">
Suresh Manandhar is a lecturer in Computer Science at the University of York. He has worked
on the implementation and formalization of constraint-based grammars. His published work
includes papers on constraint logics and efficient compilation methods for constraint-based
grammars, unsupervised learning of categorial grammars, learning of WordNet relations, and
applications of inductive logic programming to natural language processing. Manandhar’s ad-
dress is: Department of Computer Science, University of York, Heslington YO1 5DD, York, U.K.;
e-mail: suresh@cs.york.ac.uk.
</tableCaption>
<page confidence="0.982326">
228
</page>
<subsectionHeader confidence="0.5160625">
Book Reviews
The Language of Word Meaning
</subsectionHeader>
<bodyText confidence="0.926958166666667">
Pierrette Bouillon and Federica Busa (editors)
(University of Geneva and LingoMotors Inc.)
Cambridge University Press (Studies in
natural language processing, edited by
Branimir Boguraev), 2001, xvi+387 pp;
hardbound, ISBN 0-521-78048-9, $69.95
</bodyText>
<figure confidence="0.827419666666667">
Reviewed by
Mari Broman Olsen
Microsoft Corporation
</figure>
<sectionHeader confidence="0.948277" genericHeader="method">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999879548387097">
The papers collected in The Language of Word Meaning resemble nothing so much
as a holiday celebration in a large, heterogeneous family, with echoes of old feuds,
marginally relevant contributions from distant relatives, and fresh stories from re-
cent friends. Although the feuds are entertaining (opening the festivities as Section
I, “Linguistic Creativity and the Lexicon”), and the outsider perspectives insightful,
from a computational-linguistic perspective many of the most valuable contributions
come from guests who have traveled far, and with other companions, before finding
common ground at this gathering.
The volume’s title conceals the specificity of its subject: a toast and roast of Gen-
erative Lexicon theory (henceforth GL), originally proposed by James Pustejovsky
(henceforth JP) (Pustejovsky 1993, 1995, inter alia), who contributed two chapters and
the preface. The editors also contribute a joint article, and Busa is coauthor in a second.
Others, certainly, would be included in a general discussion on “the language of word
meaning.” Indeed (with rare and sometimes confusing exceptions) the discussion as-
sumes GL as the “semantic vocabulary” (p. xv) and focuses on the converse—“the
word meaning of language”1—specifically, how and whether GL’s finite number of
generative devices and rules can be used to construct semantic expressions composi-
tionally. Several contributors (Fodor and Lepore, papers in Section II; Vossen, p. 373)
raise interesting ancillary questions: how (and whether) to divide word meaning from
world knowledge.
Pustejovsky (Chapter 7) and Busa and Bouillon provide especially clear reviews
of basic GL concepts. These (and other) surveys would be more useful, however, as
a condensed introduction for newcomers (which this review will present with great
reduction as part of Section 2). The book claims (according to JP’s preface) that the GL
approach to language synthesizes traditions and ideas from ordinary language philos-
ophy (a focus on words and word use), analytic semantics (formalization of rules and
types), and generative linguistics (infinite generation of meanings from finite, recursive
resources). It claims that GL can tackle empirically difficult problems, both for compu-
tational applications and for other formal, even descriptive, systems: specifically, how
words vary in context, how new senses emerge, and what the underpinnings are of
the systematic mapping of semantic types onto syntactic forms across languages.
</bodyText>
<footnote confidence="0.611589">
1 Here I’m agreeing with my six-year-old that this would “make more sense” as the title.
</footnote>
<page confidence="0.982007">
229
</page>
<note confidence="0.633366">
Computational Linguistics Volume 28, Number 2
</note>
<bodyText confidence="0.999758111111111">
The editors begin the volume and each of four sections with short introductions
(somewhat confusingly numbered as chapters). The introductions bring harmony and
clarity to the four sections (except in Section III, the introduction to which takes issue
with Kilgarriff’s contribution). Nevertheless, the sections do not clearly form a whole,
beyond their family relation to GL, computational lexicography, and/or lexical seman-
tics. JP points out that not all contributors even agree with the generativity of lexical
senses and composition. Some who do, but in a different framework (Moravcsik) pro-
vide explicit mappings from their representation to GL; others (Asher and Lascarides,
Hobbs) leave relationships implicit. Several papers focus on practical natural language
processing (NLP) (Section IV and others) and a few on questions of philosophical re-
ality, which, though an interesting diversion, leads to further diffusion of focus.
One frequently wants more explication of criteria. When are new subrelations or
attribute values required (cf. Ruimy, Gola, and Monachini)? Why is book a physical
object in I am waiting for her next book (p. 137)? Why does waiting for the car anticipate
a construction event, but not repairing the car, washing the car, or waiting for the car to
be returned Eby a borrower] (p. 159)? How do we know when senses are different and
require different generative mechanisms (p. 367)? Why are some predicted senses rare
(pp. 153–154)?
In some chapters, editorial lapses and typos are frequent enough to be distracting:
non-English finish to read for finish reading, and on one side for on one hand; Schank and
Abelson cited as 1997 instead of 1977; unexpanded acronyms (LCS by Saint-Dizier,
AFT by Moravcsik) and multiply expanded acronyms (JP by Fodor and Lepore); and
a missing abstract (Moravcsik). A few sentences are uninterpretable (the sentence in
Yorick Wilks’s abstract beginning “It is argued ... ”; a couple of sentences by Piek
Vossen (p. 36) on “disjunctive” labels). A shared bibliography would have improved
the book’s value as a GL reference and reduced the number of pages (all but one paper
cites Pustejovsky 1995, for example).
</bodyText>
<sectionHeader confidence="0.998461" genericHeader="method">
2. Generative Lexicon Theory
</sectionHeader>
<bodyText confidence="0.999944095238095">
GL employs four levels of linguistic representation. ARGUMENT STRUCTURE encodes
obligatory and optional arity of predicates. EVENT STRUCTURE describes the aspectual
nature of the events, whether states, processes, or transitions (cf. Mourelatos [1978],
inter alia), and their subevents, if any, as well as the temporal relations between
subevents. QUALIA STRUCTURE links the first two representation levels by assign-
ing arguments to participant slots in (sub)events. The four qualia roles, inspired by
Moravcsik’s interpretations of Aristotle (p. 56), minimally encode the linguistic be-
havior of lexical items, primarily nominals and predicates with which they compose.
The CONSTITUTIVE role of an object represents the relation between it and its parts
(and what it is part of, according to Busa and Bouillon), the FORMAL role categorizes
it within a larger domain, the TELIC role represents its purpose, and the AGENTIVE
role its origins. Finally, LEXICAL INHERITANCE STRUCTURE relates lexical structures
to other structures in the type lattice. Generative devices, including type coercion,
subselection, and co-composition, connect all four levels in restricted ways, providing
for compositional interpretation of words in context (p. 56).
Qualia structure work has proved especially fertile for lexical semantic research,
including many clear, data-rich papers in this volume. The qualia roles challenge re-
searchers to consider how far a minimal lexical representation can go toward account-
ing for semantic productivity, much of which had been assigned to world knowledge.
For example, JP suggests (Pustejovsky 1993) that both possible meanings for I began
the book (‘began to read/write’) derive from qualia structure, one from the AGENTIVE
</bodyText>
<page confidence="0.985081">
230
</page>
<subsectionHeader confidence="0.849668">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999694266666667">
(books come into being by writing), and one from the TELIC (books are intended to
be read).
Julius M. Moravcsik aligns his fourfold structure (m-factor, s-factor, a-factor, and f-
factor) with the four qualia roles above, respectively, to partially decompose metaphors.
Salvador Climent argues that the CONSTITUTIVE role accounts for the behavior of
Spanish partitives; Patrick Saint-Dizier augments the TELIC role with a small set of
rules relating predicates and arguments for more explanatory representation of ad-
jectival modification, as well as some sense variations, metaphors, and metonymies;
Jacques Jayez finds qualia structure inadequate to account for the distributional proper-
ties of several French synonym classes—for example, to encode the idea that sugg´erer
‘suggest’ permits both animate and inanimate subjects, but the former only if the
complement is a “choice” among items in the propositional context. Adam Kilgarriff
concludes from the SENSEVAL data (Kilgarriff and Palmer 2000) that qualia structure
fails to account for many senses (perhaps metaphorical) not encoded in dictionaries,
for which GL would seem to be naturally suited.
</bodyText>
<sectionHeader confidence="0.928246" genericHeader="method">
3. Section I: Family Feuds
</sectionHeader>
<bodyText confidence="0.999836888888889">
The first section, “Linguistic Creativity and the Lexicon,” considers whether words
have internal syntax and inferential meaning (as in GL) or whether they are concepts
as particulars with denotational semantics. James McGilvray, in “Chomsky on the
Creative Aspect of Language Use and Its Implications for Lexical Semantic Studies,”
situates GL as Relation R in Chomskyan syntax, as an internalist semantics, “a branch
of syntax, broadly speaking” (p. 19). His leap from the usefulness to the innateness
of qualia structure (p. 17) is jarring, as is the sudden appearance of minimalism 14
pages into the 23-page chapter (p. 18). But he clearly and cogently defends Chomsky
as grounding a computational approach: obvious to some (p. xii), yet hotly debated
in some computational circles.
As they have in the past,2 Jerry A. Fodor and Ernie Lepore, in “The Emptiness of
the Lexicon: Critical Reflections on J. Pustejovsky’s ‘The Generative Lexicon’ ”, chal-
lenge JP on the possibility of the GL enterprise (or indeed any nonatomic representa-
tion structure). Fodor and Lepore (henceforth F&amp;L) lack faith that lexical generaliza-
tions are ultimately interesting or powerful and not merely one of “all sorts of ways”
in which “the meanings of words can partially overlap” (p. 48). Claiming that the
null hypothesis (p. 29) is as “everybody always thought: the lexicons of natural lan-
guages are just lists” (p. 44),3 they question whether GL reaches its aims: to account
for semantic well-formedness and generativity and to provide evidence that meaning
is inferential (rather than atomistic and denotative). JP responds (“Generativity and
Explanation in Semantics: A Reply to Fodor and Lepore”) that F&amp;L misread him in
substance and detail and are “negative and unconstructive” (p. 51), artificially raising
the bar for semantic theory while avoiding the theoretical issues raised by the data.
According to F&amp;L, GL is descriptively inadequate, both over- and undergenerating
interpretations. JP claims that “the modes of inheritance for concepts associated with
linguistic expressions are not arbitrary” (p. 61). F&amp;L say those given are incomplete.
F&amp;L’s alternative (want a beer → want to have a beer → want to drink a beer derived by
</bodyText>
<footnote confidence="0.952845">
2 See, for example, discussion between Fodor and Weinberg in CUNY (1998), as well as references in the
papers by Fodor and Lepore and Wilks in the volume under review.
3 Although, as F&amp;L point out, much effort has been expended studying semantic relations among lexical
items, for example, synonymy and antonymy, which are phenomena that they foresee will turn out
also to be conceptual rather than lexical.
</footnote>
<page confidence="0.991202">
231
</page>
<note confidence="0.645716">
Computational Linguistics Volume 28, Number 2
</note>
<bodyText confidence="0.999616666666667">
logical-form rules) should be subjected to the same adequacy standard to which they
subject the GL discussion of bake (on which JP says, in a footnote on p. 64, that they
miss the point anyway).
Wilks stands with JP in “The ‘Fodor’-FODOR Fallacy Bites Back,”4 rejecting the
F&amp;L position as untenable for analytic reasons (there is no one-to-one mapping be-
tween words and meanings) and unhelpful in organizing a useful dictionary, for NLP
or otherwise. Although F&amp;L don’t claim “real” representations should be useful, Wilks
also disagrees with F&amp;L and JP that an adequate theory of the lexicon should be able
to judge semantic well-formedness.
</bodyText>
<sectionHeader confidence="0.929876" genericHeader="method">
4. Section II: Finding Common Ground
</sectionHeader>
<bodyText confidence="0.974048078947369">
The papers in the second section, “The Syntax of Word Meaning,” refine and gently
challenge the vocabulary, framework, and crosslinguistic data analysis required for an
explanatorily adequate, language-independent lexical semantics. JP, in “Type Construc-
tion and the Logic of Concepts,” a revision of a 1998 paper, discusses well-formedness
directly and convincingly, arguing that constraints on concepts (i.e., thought) are re-
vealed through lexicalization strategies. He proposes a tripartite-concept subtyping—
natural, functional, and complex, discussed further in Section IV by Busa, Calzolari,
and Lenci—and addresses criteria for choosing among competing representations and
deciding about feature admission, appropriateness, and composition.
In “Underspecification, Context Selection, and Generativity,” Jayez shows that con-
text provides cues to polysemous word meaning as well as imposing restrictions on
synonym selection (French sugg´erer and attendre ‘wait’ classes). It is not clear that the
data presented are inherently incompatible with the GL framework, as he argues. He
also makes the general point that GL practitioners must demonstrate generative ade-
quacy not only against sense enumeration lexicons (the current focus in the literature)
but also against other decompositional and underspecified lexical theories.
The next two papers propose extensions to qualia structure. For Bouillon and Busa,
in “Qualia and the Structuring of Verb Meaning,” qualia roles are dynamic, varying
by speaker and context; for example, the constitutive relation has subtypes (IS PART
OF, HAS AS PART, IS IN, LIVES IN, IS A MEMBER OF, HAS AS MEMBER). They argue that
the restrictions on the variety of noun phrases that can occur as objects of attendre
can be derived compositionally: The verb selects an event, and some nominals encode
events, for example, creation (such as journal, book, car), possession (all nominals with
telic roles, inter alia), and beginning or culminating. Saint-Dizier (“Sense Variation
and Lexical Semantics: Generative Operations”) augments the telic role, the “most
productive role to novel sense variations” (p. 168).
In “Individuation by Partitive Constructions in Spanish,” Climent argues that the
semantics of portions is grounded in the entailment that entities (individuated, mass,
etc.) are composed of something. He argues that qualia are syntactic, with composi-
tional operations creating selectionally constrained complex nominals. Through inter-
esting (though constructed) data, in “Event Coreference in Causal Discourses,” Lau-
rence Danlos argues persuasively that causatives can be interpreted from a linguistic
perspective with internally complex events, although she rejects the extended event
structure for unaccusatives.
4 Wilks says F&amp;L claim to be part of informational role semantics (IRS), as he is. This is confusing, since
in their chapter they express doubt that IRS can be sustained, challenge the cogency of arguments that
take it as a premise, and go on to address JP, who purports to provide an argument for the complexity
of lexical entries that do not presuppose IRS (p. 29).
</bodyText>
<page confidence="0.978911">
232
</page>
<figure confidence="0.367001">
Book Reviews
</figure>
<sectionHeader confidence="0.70738" genericHeader="method">
5. Section III: Forbears and New Friends
</sectionHeader>
<bodyText confidence="0.999946225806452">
The gratingly titled third section, “Interfacing the Lexicon” (with what?), examines
metonymy and metaphor in the GL framework. The stimulating studies in this sec-
tion decompose structures generally considered lexically atomic. In “Metaphor, Cre-
ative Understanding, and the Generative Lexicon,” Moravcsik uses GL’s polysemy
account (whose qualia spring, in part, from Moravcsik’s work) to uncover the lexical
information structure by means of metaphor. Moravcsik argues that simile also can be
treated “as a literal statement” (p. 260) of comparison.
Nicholas Asher and Alex Lascarides examine “Metaphor in Discourse” in terms of
lexical semantic structures, inspired by GL and others (Copestake and Briscoe 1995),
focusing on change-of-location verbs, metaphorical extension of properties from objects
to persons, and the relation between metaphoric interpretation and discourse structure.
They constrain interpretations by means of lexical rules that restrict meaning shifts and
a discourse structure theory in which truth conditions can be evaluated.
Jerry Hobbs presents his interpretation-as-abduction framework in “Syntax and
Metonymy.” He accounts for Nunberg’s (1995) metonymic data (I’m parked out back) by
means of a set of coercion relations, selected on the basis of saliency by least-cost abduc-
tive interpretation. He extends his account to a wide range of intriguing data, old and
new, including extraposed modifiers (A jolly old man arrived with an armload of presents),
Bolinger’s (1988) ataxis (John smokes an occasional cigarette = John occasionally smokes
cigarettes), container nouns (cf. Jayez), and disguised small clauses (I have a sore throat
= My throat is sore).
Kilgarriff, in “Generative Lexicon Meets Corpus Data: The Case of Nonstandard
Word Uses,” brings to the discussion a refreshing quantitative, corpus linguistics per-
spective. He asks to what extent GL is able to generate, for a sample set of words,
senses found in corpora but not represented in a dictionary (“nonstandard” senses).
As mentioned above, he argues that contextual sense variation is too large and un-
predictable for such a model of lexical structures. Some might question his strategy
for determining nonstandard senses and his rigorous interpretation of GL mecha-
nisms, but his methodology is clear, reproducible, and a fair challenge to GL, po-
tentially accounted for by Bouillon and Busa’s expansion of the qualia relations, for
example.
</bodyText>
<sectionHeader confidence="0.90233" genericHeader="method">
6. Section IV: Practical Matters
</sectionHeader>
<bodyText confidence="0.999318733333333">
The fourth section, “Building Resources,” chronicles practical experiences for a range
of NLP applications within the SIMPLE and EuroWordNet frameworks. The papers
in this section, as the introduction states, leave open the question of whether they are
useful or usable; they furthermore neglect to evaluate whether they are practical to
construct: Most involve significant manual work.
In “Generative Lexicon and the SIMPLE Model: Developing Semantic Resources
for NLP,” Busa, Nicoletta Calzolari, and Alessandro Lenci highlight the need to cap-
ture richness of language (apparently independent of applications) within a testable
model for building large-scale resources. They employ qualia roles for structuring
the semantic/conceptual types, assuming that words vary in complexity (expanding
on Pustejovsky in Section II). Nilda Ruimy, Elisabetta Gola, and Monica Monachini,
in “Lexicography Informs Lexical Semantics: The SIMPLE Experience,” describe how
they use GL to categorize concepts, without defending or critiquing the theory or their
particular attribute–value assignments. They argue that encoding their methodology
serves to explicate the theory.
</bodyText>
<page confidence="0.995873">
233
</page>
<note confidence="0.733594">
Computational Linguistics Volume 28, Number 2
</note>
<bodyText confidence="0.99910825">
Piek Vossen describes his work on “Condensed Meaning in EuroWordNet,” fo-
cusing on the challenge of developing GL types of relations from resources that are
sense-enumerative. He sets forth how EuroWordNet links semantic representations
across languages via an Inter-Lingual-Index that connects their structures.
</bodyText>
<sectionHeader confidence="0.936031" genericHeader="method">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.999457166666667">
The Language of Word Meaning offers glimpses into the variety of work spawned by GL:
philosophical, computational, theoretical, and multilingual. Section introductions and
GL overviews by Pustejovsky (Chapter 7) and Busa and Bouillon lay out important
themes in computational and theoretical lexical semantics. Other chapters offer practi-
cal, often entertaining exemplars of lexical work unfolding—like family celebrations—
somehow, noisily, and with great diversity.
</bodyText>
<sectionHeader confidence="0.969902" genericHeader="conclusions">
References
</sectionHeader>
<reference confidence="0.992633388888889">
Bolinger, Dwight. 1988. Ataxis. In Rokko
Linguistic Society, editors, Gendai no Gengo
Kenkyu [Linguistics Today], Kinseido,
Tokyo, 1–17.
Copestake, Ann and Ted Briscoe. 1995.
Semi-productive polysemy and sense
extension. Journal of Semantics, 12(1):15–67.
Reprinted in James Pustejovsky and
Branimir Boguraev, editors, Lexical
Semantics: The Problem of Polysemy, Oxford
University Press, 1996, 15–67.
CUNY. 1998. Proceedings of the 11th CUNY
Conference on Human Sentence Processing,
Rutgers University, New Brunswick, NJ.
Fodor, Jerrold. 1970. Three reasons for not
deriving kill from cause to die. Linguistic
Inquiry, 1:429–438.
Kilgarriff, Adam and Martha Palmer,
editors. 2000. Special Issue on
SENSEVAL: Evaluating Word Sense
Disambiguation Programs. Computers and
the Humanities, 34.
Mourelatos, Alexander. 1978. Events,
processes, and states. Linguistics and
Philosophy, 2:415–434. Also published in
Philip Tedeschi and Annie Zaenen,
editors, Syntax and Semantics 14: Tense and
Aspect, Academic Press, 1981, 191–212.
Nunberg, Geoffrey. 1995. Transfers of
meaning. Journal of Semantics, 12:109–132.
Pustejovsky, James. 1993. Type coercion and
lexical selection. In James Pustejovsky,
editor, Semantics and the Lexicon. Kluwer
Academic, pages 73–96.
Pustejovsky, James. 1995. The Generative
Lexicon. MIT Press.
</reference>
<bodyText confidence="0.927946571428571">
Mari Broman Olsen has worked as lexical semanticist in Microsoft Corporation’s Natural Lan-
guage Group since 1999. From 1996 to 1999, she was a postdoctoral fellow at the University
of Maryland Institute for Advanced Computer Studies, researching interlingual lexical repre-
sentation for Chinese–English machine translation, inter alia. Her Ph.D. is from Northwestern
University (1994); her dissertation, A Semantic and Pragmatic Model of Lexical and Grammatical As-
pect, was published by Garland Press. Olsen’s address is: Microsoft Corporation, One Microsoft
Way, Redmond, WA 98052; e-mail: molsen@microsoft.com.
</bodyText>
<page confidence="0.994561">
234
</page>
<figure confidence="0.905702222222222">
Book Reviews
Empirical Methods for Exploiting Parallel Texts
I. Dan Melamed
(New York University)
Cambridge, MA: MIT Press, 2001,
xi+195 pp; hardbound, ISBN
0-262-13380-6, $32.95, £22.95
Reviewed by
Ted Pedersen
</figure>
<affiliation confidence="0.559585">
University of Minnesota, Duluth
</affiliation>
<bodyText confidence="0.999691675675676">
Parallel translations of written texts have long been useful tools for human students
of language and have begun to serve as an intriguing source of data for corpus-based
approaches to natural language processing. A source text and its translation can be
viewed as a coarse map between the two languages, and an industrious student or
clever computer program may wish to refine that mapping so that it shows which
sentences, phrases, and words are translations of one another.
Humans are very adept at finding such relations in parallel text. This is true even
when one or both of the languages is unfamiliar, as can be seen in a simple but
convincing exercise by Knight (1997). Although there was considerable early success
in automatically identifying sentences in parallel text that are translations of each other
(e.g., Brown, Lai, and Mercer 1991, Gale and Church 1993), a variety of challenging
problems has emerged since that time.
Empirical Methodsfor Exploiting Parallel Texts is a revision of author I. Dan Melamed’s
1998 Ph.D. dissertation (University of Pennsylvania) and succeeds in capturing the
range of problems inherent in parallel text. It presents a variety of techniques for find-
ing translation equivalents and demonstrates that once these are available, they can
be used to align text segments, detect omissions in translations, identify noncomposi-
tional compounds, and discriminate among word senses.
The book is arranged in three parts, the guiding organizational principle of which
is the distinction between tokens and types. (A token represents each occurrence of
a linguistic entity in a text, whereas a type consists of every occurrence of identical
tokens.) The author casts his own work in terms of pattern recognition techniques that
acquire information about specific tokens and statistical learning methods that induce
generalized models of word types on the basis of token data.
Part I consists of three chapters that focus on tokens and pattern recognition.
Chapter 2 presents an algorithm that finds a token-by-token mapping of a parallel
text in which each token in the source text is aligned with its translation in the target
text. This algorithm is presented in terms of pattern recognition concepts such as signal
generation, filtering, and search.
The signals generated are candidate token alignments, and these come from cog-
nates identified in the parallel text and optionally from a user-supplied seed lexicon
of word translations. High-frequency words create noise that is filtered by means of
a localized search procedure that allows the algorithm to consider the parallel text
piece by piece as it performs token alignment. The next two chapters (3 and 4) show
that once such a mapping is available, it can be used to find segment alignments in
parallel text and to detect portions of text that have been erroneously omitted from a
translation.
</bodyText>
<page confidence="0.994554">
235
</page>
<note confidence="0.733616">
Computational Linguistics Volume 28, Number 2
</note>
<bodyText confidence="0.999862901960785">
Part II is a two-chapter interlude that touches on both tokens and types. Chapter 5
is a discussion of the issues involved in counting co-occurrences in parallel text. Al-
though Melamed’s conclusions may seem fairly intuitive, he shows that previous work
in translation modeling has often been based on less-than-optimal counting schemes.
In Chapter 6, Melamed describes the creation of a manually word-aligned version
of the Bible in French and English. This is a nontechnical chapter that captures the
difficulties of manual alignment in general and in dealing with the Bible in particu-
lar.
The three chapters in Part III consider word types and statistical learning tech-
niques. Chapter 7 develops three statistical models of translation that represent un-
ordered word-by-word translation. These models rely on the observation that a source
word will most often translate into a single target word (as opposed to multiple words)
or may have no translation equivalent at all. Parameter estimation schemes are devel-
oped that capture these properties, and empirical results show that the inclusion of
parameters developed according to these schemes steadily improves the translation
models, all of which significantly outperform the historically important IBM Model 1
(Brown et al. 1993).
Melamed argues that word-by-word translation models can also be used to iden-
tify linguistic entities that are normally not translated word for word. In Chapter 8
he shows how noncompositional compounds such as beat a dead horse can be dis-
covered in parallel text by means of such a model. In Chapter 9, an unsupervised
word sense discrimination algorithm is introduced that is evaluated by treating the
different discovered senses of a word as distinct types. The incorporation of these
additional sense distinctions as types is shown to improve the quality of a translation
model.
Several of the chapters in this book have appeared in preliminary form in this
journal (Chapters 2, 3, and 7) and conference proceedings (Chapters 4 and 8). As a
result, the individual chapters are well polished and clearly the product of careful
reviewing and rewriting. Since this material retains its original style and form, each
chapter is relatively self-contained, and readers may wish to approach this book as
a collection of related papers rather than as a work that must be read from start to
finish.
Readers not familiar with corpus-based methods, pattern recognition, and sta-
tistical learning may struggle at times. There is not a great deal of preliminary or
background material presented in each chapter before moving into more advanced
concepts. This is particularly true of Chapters 2 and 7, which present the token align-
ment algorithm and translation model development. This should not discourage the
novice from approaching this book, however. Since each chapter can be read indepen-
dently, it is possible to appreciate the related applications described in Chapters 3, 4,
8, and 9 without fully understanding all of the technical details in Chapters 2 and 7.
The author also includes sufficient discussion of related work throughout to provide
a general picture of the field.
To conclude, this is a useful book for anyone interested in parallel text. It offers
enough improvements and clarifications over previously published material that it
will appeal to a reader already familiar with the author’s work. It also has a wide
scope and strikes a reasonable balance between theory and application, so newcomers
will get a sense of the possibilities and problems associated with parallel text. These
readers may wish to supplement this book with pattern recognition and statistical-
learning readings, as found in Duda and Hart (1973), for example. Finally, in an age in
which academic texts tend to be expensive and poorly printed, this book is a notable
exception, as the production quality is high and the price is reasonable.
</bodyText>
<page confidence="0.993535">
236
</page>
<figure confidence="0.280335">
Book Reviews
</figure>
<sectionHeader confidence="0.824886" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9819836">
Brown, Peter F., Stephen Della Pietra,
Vincent Della Pietra, and Robert L. Mercer.
1993. The mathematics of statistical
machine translation: Parameter estimation.
Computational Linguistics, 19(2):263–311.
Brown, Peter F., Jennifer Lai, and Robert L.
Mercer. 1991. Aligning sentences in
parallel corpora. In Proceedings of the 29th
Annual Meeting of the Association for
Computational Linguistics, pages 169–176,
Berkeley, CA.
Duda, Richard O. and Peter E. Hart. 1973.
Pattern Classification and Scene Analysis.
Wiley.
Gale, William and Kenneth Church. 1993. A
program for aligning sentences in
bilingual corpora. Computational
Linguistics, 19(1):75–102.
Knight, Kevin. 1997. Automating
knowledge acquisition for machine
translation. AI Magazine, 18(4):81–96.
Ted Pedersen is an Assistant Professor of Computer Science at the University of Minnesota,
Duluth. His research focuses on word sense disambiguation. He is the recipient of a National
Science Foundation CAREER award. Pedersen’s address is: Department of Computer Science,
University of Minnesota, Duluth, MN 55812; e-mail: tpederse@d.umn.edu.
</reference>
<page confidence="0.995334">
237
</page>
<note confidence="0.741657666666667">
Computational Linguistics Volume 28, Number 2
Sentence Comprehension: The Integration of Habits and Rules
David J. Townsend and Thomas G. Bever
</note>
<bodyText confidence="0.749146142857143">
(Montclair State University and University of Arizona)
Cambridge, MA: MIT Press (Language,
speech, and communication series),
2001, xi+445 pp; hardbound, ISBN
0-262-20132-1, $65.00, £44.95;
paperbound, ISBN 0-262-70080-8,
$24.95, £16.00
</bodyText>
<subsubsectionHeader confidence="0.277287666666667">
Reviewed by
Matthew W. Crocker
Saarland University
</subsubsectionHeader>
<bodyText confidence="0.981532857142857">
The last two decades have witnessed a great deal of activity in the development of
computational theories of human sentence processing. The aim of such research is to
explicitly identify and model the architectures and mechanisms that underlie human
comprehension (see Crocker, Pickering, and Clifton [2000] for a recent collection on
the topic). Although there is often relatively little interaction between the computa-
tional psycholinguistics and natural language processing (NLP) communities, both have
been concerned with how various symbolic, connectionist, probabilistic, and hybrid
techniques can be applied to the task of recovering interpretations of ambiguous lin-
guistic input according to some set of underlying linguistic rules and principles. The
difference between the two enterprises concerns their respective goals. Models of hu-
man sentence comprehension are concerned with replicating human behavior during
the on-line, incremental (word-by-word) processing of an utterance as it unfolds. Par-
ticular emphasis is therefore placed on the resolution of local or temporary ambiguities,
which occur as each initial substring of an utterance is encountered. Identifying human
preferences in the face of local ambiguity is not a goal in itself but provides critical
insight into the kinds of mechanisms (parallel, serial, underspecified) and informa-
tion sources that are used (e.g., syntactic, semantic, visual, probabilistic). Traditional
NLP systems, on the other hand, are typically concerned with trying to resolve global
ambiguities (language models for speech recognition being a notable exception) and
often strive only to recover partial interpretations (e.g., keyword finding, chunking,
template filling). Interestingly, the work reviewed here can be seen as arguing for
greater convergence between psychological and technical models in that it argues for
the existence of a shallow, partial parser within the human sentence-processing mech-
anism.
Sentence Comprehension: The Integration of Habits and Rules is an ambitious work that
aims to integrate ideas and evidence from sentence-processing research that have ac-
crued over the last 40 years. This is essentially two books in one: an introductory text
and a research monograph. The need for these two subbooks arises from the highly in-
terdisciplinary nature of the topic: The development of an algorithmic model of human
sentence processing based on state-of-the-art linguistic theory necessarily draws upon
background in linguistics, computational linguistics, and psycholinguistics. The first
150 pages of the book are devoted to providing this. After a brief introductory chapter
sketching the perspective and goals of the book, we are presented with a rather histor-
ical review of early theories of sentence processing (Chapter 2) and an introduction to
some of the key aspects of current linguistic theory (Chapter 3). The latter concludes
</bodyText>
<page confidence="0.989101">
238
</page>
<subsectionHeader confidence="0.837257">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999991235294118">
with an overview of minimalist syntactic theory (Chomsky 1995), which the authors
later assume in developing their own model.
Of these introductory chapters, Chapter 4 is worthy of particular mention. It gives
a truly comprehensive and relatively detailed review of the prevailing theories of sen-
tence processing. Not only are the models well presented, but similarities, differences,
advantages, and disadvantages are also clearly identified. The chapter begins with the
deterministic parser of Marcus (1980) and Frazier’s Garden Path theory (1978) and
moves on through principle-based parsing and reanalysis (e.g., Pritchett 1992; Crocker
1996). The final half of the chapter then considers more-recent probabilistic models,
which I loosely take to encompass statistical parsing (Jurafsky 1996), connectionist,
and competitive constraint-based approaches (Spivey and Tanenhaus 1998) as well as
hybrid models (Stevenson 1994). This part of the book is particularly interesting in
revealing how probabilistic mechanisms can be used in extremely different ways and
how such models face fundamental challenges, for example, in establishing the “level
of grain” at which statistical frequency information is accrued (Mitchell et al. 1995),
and the scalability of recent connectionist proposals (Spivey and Tanenhaus 1998).
We now turn our attention to the model proposed by the authors, which is intro-
duced in Chapter 5. The central hypothesis of the book, and core idea of the model,
is that sentence comprehension involves associative and syntactic components. An
associative component is assumed to be necessary for rapid, shallow recognition of
major phrases. Configurational surface schemata, or canonical sentence templates, and
verb-frame information are then exploited to construct a preliminary conceptual rep-
resentation. The syntactic component, in turn, is responsible for recovering the full
cyclic structure of the sentence, including the longer-distance relationships introduced
by movement transformations. The syntactic component takes the shallow “quick and
dirty” analysis as its input, derives a complete syntactic parse, and then checks that this
is still consistent with the input string. The result is the so-called analysis-by-synthesis
model.
This two-tiered model of processing is broadly motivated by an apparent duality
in the evidence from human language processing. On the one hand it is clear that
people are sensitive to certain kinds of probabilistic information in building interpre-
tations of ambiguous linguistic input and even in generating predictions of what is
to come next (and recovering material that was unclear in the original, noisy input
signal). On the other hand, people clearly demonstrate the ability to apply rich and
productive linguistic principles in dealing with complex and novel input. The authors
argue that trying to recover the interpretation of a sentence directly by means of the
grammar is intractable. In particular, they argue that reversing the transformational
derivations of the grammar incrementally would simply not be possible, given the
recursive complexity of language. The idea is that the associative component “isolates
a candidate analysis based on surface cues, and syntactic processes check that the
analysis can derive the sequence” (p. 164, para. 1). Since full parsing follows an initial
syntactic and semantic analysis, it is argued that the tremendous search space required
for full parsing can be substantially reduced.
The following chapters further develop the Late Assignment of Syntax Theory (LAST).
Chapter 6 presents a wide range of empirical data, which are interpreted as support-
ing the model, and also takes the opportunity to introduce a range of experimental
psycholinguistic techniques for the uninitiated. Chapter 7 delves into greater detail
concerning the associative component and the use of canonical sentence templates
in particular. Chapter 8 then situates the model of sentence processing within lan-
guage processing more generally (e.g., discourse and conceptual understanding). Fi-
nally, Chapter 9 considers a range of other, more peripheral issues, such as evidence
</bodyText>
<page confidence="0.983598">
239
</page>
<note confidence="0.478788">
Computational Linguistics Volume 28, Number 2
</note>
<bodyText confidence="0.999842627450981">
from language acquisition and neurological support for the model. Chapter 10 gives
a concise summary of the theory and its motivation and some further rationalization.
To their credit (and the benefit of the seriously interested reader), these chap-
ters cover a tremendous amount of psycholinguistic data in building support for the
authors’ proposals. It must be accepted, of course, that empirical facts are typically
consistent with many possible interpretations, and it will be up to readers to decide
whether they are persuaded by the authors’ attempt to interpret the facts presented as
supporting the LAST model. One serious criticism of the empirical discussion, how-
ever, is that the book considers only evidence from English. Sentence processing has
seen an increasing emphasis on crosslinguistic research over the last 10 years, as it
became evident that earlier models were “overfitted” to English. LAST seems to fall
prey to this in its heavy reliance on two properties of English, namely that (1) verbs
precede their complements and (2) clauses have a relatively fixed word order. The
former is essential for the prediction of complements on the basis of the verb’s most
frequent subcategorization frame (which the authors propose guides the associative
component), and the latter seems crucial in order for canonical sentence templates to
have any substantive value. An additional (and important) consequence of the model
is that it sacrifices full, word-by-word incremental interpretation. Although the au-
thors attempt to justify this move, it is certainly at odds with current consensus in the
sentence-processing community. Experimental evidence suggests that people can and
do rapidly integrate the full complement of linguistic constraints that follow from a left
context in processing the words that follow. I remain unconvinced that the associative
stage of the LAST model would be able to account for the full range of facts here.
Although the theory’s ability to explain the empirical phenomena it is intended to
elucidate will be crucial to its ultimate success, it is worth making some observations
on the theory itself. It has to be said that, despite its being couched in terms of modern
minimalist linguistic theory, and despite all the attention paid in the book to the
full complement of modern models (symbolic, probabilistic, and connectionist), the
proposed model has a decidedly “retro” feel to it. In particular, it resurrects notions like
the old N-V-N (agent-action-patient) canonical sentence templates (Fodor, Bever, and
Garrett 1974), not to mention something closely resembling the transformational theory
of grammar, in that the syntactic component is assumed to perform some direct form of
reverse transformation operations. The rather literal interpretation of transformations
has all but been abandoned even in principle-based models (Crocker 1996). I was also
disappointed to discover that the claimed reconciliation and “integration of symbolic-
computational and associative-connectionist” approaches amounted to simply having
one of each. Although this is a possible position to take, it is certainly at odds with
Occam’s razor to have two mechanisms doing essentially the same task. A theory that
can explain the evidence with a single mechanism would surely be preferable. Whereas
earlier models indeed failed to account adequately for both symbolic and probabilistic
behavior, recent models have shown that they can be integrated by applying ideas
from probabilistic language modeling, such as hidden Markov models and stochastic
parsing (Jurafsky 1996; Crocker and Brants 2000).
Furthermore, although the model might be considered algorithmic, it has not been
implemented computationally, in contrast with many competing models. Given the
complexity of human language comprehension and the range of interacting mecha-
nisms and information sources, it is difficult to ensure that the theory is being in-
voked consistently and uniformly in accounting for the full range of empirical find-
ings. This potential pitfall seems especially important for LAST given that two almost
autonomous mechanisms are involved. The only way to achieve the necessary consis-
tency in evaluating a model, and to make clear predictions, is through implementation.
</bodyText>
<page confidence="0.973539">
240
</page>
<subsectionHeader confidence="0.753387">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999709733333333">
To conclude, Sentence Comprehension functions as an excellent introduction to the
history of sentence processing, the methods used, and prevailing theories. It strives
to identify both the truths that have been revealed and the outstanding conundrums
and presents evidence and concepts in both a detailed and comprehensible fashion.
The book is well written and organized, including a detailed bibliography, as well
as subject and author indices. Other psycholinguists may not always be convinced
by the authors’ interpretation of competing theories and empirical data, but that will
always be the case. This book nonetheless provides exceptional coverage and would
function as a thorough introduction for newcomers and a valued resource to those
already in the field. It could certainly be used as a text, although, given the emphasis
on what I take to be a rather controversial proposal, I would probably prefer to use
it alongside other material. Despite the criticisms detailed above, the authors’ pro-
posals certainly offer numerous insights and put into sharp focus the importance of
considering probabilistic, experience-based aspects of language use as well as general
linguistic principles.
</bodyText>
<sectionHeader confidence="0.996795" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999716120689655">
Chomsky, Noam. 1995. The Minimalist
Program. MIT Press.
Crocker, Matthew W. 1996. Computational
Psycholinguistics: An Interdisciplinary
Approach to the Study of Language. Kluwer
Academic.
Crocker, Matthew W. and Thorsten Brants.
2000. Wide coverage probabilistic sentence
processing. Journal of Psycholinguistic
Research, 29(6):647–669.
Crocker, Matthew W., Martin Pickering, and
Charles Clifton Jr. 2000. Architectures and
Mechanisms for Language Processing.
Cambridge University Press.
Fodor, Jerry A., Thomas G. Bever, and
Merrill F. Garrett. 1974. The Psychology of
Language: An Introduction to
Psycholinguistics and Generative Grammar.
New York: McGraw-Hill.
Frazier, Lyn. 1978. On Comprehending
Sentences: Syntactic Parsing Strategies. Ph.D.
dissertation, University of Connecticut.
Published by the Indiana University
Linguistics Club, Bloomington, IN.
Jurafsky, Daniel A. 1996. A probabilistic
model of lexical and syntactic access and
disambiguation. Cognitive Science,
20:137–194.
Marcus, Mitchell P. 1980. A Theory of
Syntactic Recognition for Natural Language.
MIT Press.
Mitchell, Don C., Fernando Cuetos, Martin
M. B. Corley, and Marc Brysbaert. 1995.
Exposure based models of human
parsing: Evidence for the use of
coarse-grained (non-lexical) statistical
records. Journal of Psycholinguistic Research,
24:469–488.
Pritchett, Bradley L. 1992. Grammatical
Competence and Parsing Performance.
University of Chicago Press.
Spivey, Michael and Michael Tanenhaus.
1998. Syntactic ambiguity resolution in
discourse: Modeling the effects of
referential context and lexical frequency.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 24:1521–1543.
Stevenson, Suzanne. 1994. Competition and
recency in a hybrid network model of
syntactic disambiguation. Journal of
Psycholinguistic Research, 23:295–322.
Matthew W. Crocker is a Professor of Psycholinguistics in the Department of Computational
Linguistics, Saarland University. His research is concerned with computational models of hu-
man sentence processing, focusing in particular on the development of probabilistic mod-
els of sentence comprehension, statistical language processing, and experimental psycholin-
guistics. Crocker’s address is: Department of Computational Linguistics, Building 17, Saar-
land University, 66041 Saarbr¨ucken, Germany; e-mail: crocker@coli.uni-sb.de; http://www.coli.
uni-sb.de/∼crocker/.
</reference>
<page confidence="0.998097">
241
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.003540">
<title confidence="0.99846">Book Reviews Spotting and Discovering Terms through Natural Language Processing</title>
<author confidence="0.999987">Christian Jacquemin</author>
<affiliation confidence="0.884124">(University of Paris 11)</affiliation>
<address confidence="0.8845545">Cambridge, MA: The MIT Press, 2001, viii+378 pp; hardbound, ISBN</address>
<email confidence="0.552368">$52.95,</email>
<note confidence="0.908387">Reviewed by</note>
<author confidence="0.992095">Sophia Ananiadou</author>
<affiliation confidence="0.984431">University of Salford</affiliation>
<abstract confidence="0.997488787401575">Jacquemin’s book and Discovering Terms through Natural Language Proa much needed and most welcome addition to the field of computational terminology. The central issue of this book is the in-depth examination of term variation, that is, the morphological, syntactic, or semantic transformation of multiword terms, capturing the fact that the same concept can be expressed through term variants. From an application point of view, dealing with term variation would result in more efficient indexing engines and better term extraction tools, among other applications. The book also describes in depth the author’s FASTR system, a natural language processor for term variation. There are plenty of examples throughout the book and a set of metarules in the appendix for the interested reader who would like to use FASTR. Chapter 2 provides an excellent overview and a comparison of numerous algorithms for automatic term extraction systems as well as techniques for recognizing multiword terms. Six systems are examined in detail. The author also discusses the related area of automatic indexing, the main purpose of which is to assign content descriptors to documents. He describes 11 studies of phrase indexing (indexing through multiword units). A useful comparison of the different indexers is provided (p. 113). The aim of this extended discussion of concerns, methods, and systems is to introduce the main ideas behind FASTR, which is used for both term recognition and terminological enrichment. The identification of variants of terms is crucial in Jacquemin’s term-spotting technique, and because such variation occurs in corpora frequently, “it makes sense to build an indexer on a variational mechanism” (p. 115). In addition, variants represent on average 28% of multiword term occurrences (p. 219). Since term spotting uses various linguistic features, FASTR’s grammar formalism is unification-based, inspired by PATR-II, in order to represent different types of features. Information is embedded in nontyped feature structures, which include two additional facilities: disjunction and negation. Term rules in FASTR are composed of a context-free skeleton that describes the constituent structure of terms and logical constraints (feature structures) that denote the information linked to the nodes of the context-free skeleton. The morphological model of FASTR is concatenative morphology enriched with feature structures and a list of suffixes. This applies to both inflectional and derivational morphology, although in FASTR there are two ways of describing derivational morphology: dynamically (similar to inflection) and statically, Computational Linguistics Volume 28, Number 2 where derivational links between words and their stems are explicitly stated in the single-word lexicon. To represent the syntax of multiword terms and to cope with term FASTR’s formalism uses the notions of domain of locality lexicallexicalized tree adjoining grammars (LTAGs) (Abeill´e and Rambow 2000). Jacquemin’s approach relies on metarules that exploit syntagmatic and paradigmatic information. Syntagmatically, they describe structural mappings between, say, a multiword term and its syntactic or phrasal variants. Paradigmatically, they describe lexical relationships between the words of a term and the words of its variants that participate in the mappings. These lexical relationships may be morphological or semantic in nature or both. Morphological relationships cover words that belong to the same derivational family and express the fact that such words share a common root. Semantic relationships cover words that are linked (by synonymy or antonymy, for example); that is, they capture the fact that the words have something in common semantically. Taken together, the syntagmatic and paradigmatic elements of metarules provide complementary constraints that prevent the generation of undesirable mappings. Chapter 4 is dedicated to the metagrammar of FASTR, which operates on top of the lexical and terminological data. Metarules dynamically transform rules written for controlled terms into new rules that, in turn, can be used to extract variants from texts. The concept of metarule is inspired by the work of Harris (Harris et al. 1989) and is further influenced by generalized phase structure grammars (Gazdar et al. 1985) and feature-based LTAGs (Srinivas et al. 1994) and by work on lexically based formalisms that seek to reduce the complexity and size of the grammar. The author contrasts his use of metarules with other formalisms (p. 157). The main features are that metarules in FASTR are more generic than those in other formalisms and, since they are dedicated to term and variant extraction, they are also much simpler. Of further interest in FASTR’s metarule formalism is that it contains a compiler and an interpreter of regular expressions, and this allows the output of more complex structures (for example, coordinated structures). Term variation is explicitly described for English through four types of elementary term variation: coordination, permutation, modification/substitution, and elision. Chapter 5 provides a practical description of how to build, tune, and evaluate metarules for syntactic term variants in English that can be produced from a controlled vocabulary (this vocabulary would typically be the result of the term extraction phase of FASTR). The chapter begins by describing variations of binary terms, for example, to link a binary compound term such as root a noun phrase having the meaning, root of a lower premolar by means of permutation metarules. 5.2 describes how to extend variations from binary to terms, taking into account all structural ambiguities. A detailed description is given for an example of variation involving coordination, as this is one of the most difficult phenomena to tackle. Section 5.4 illustrates how the metagrammar can be further tuned experimentally, using occurrences extracted from the corpus through paradigmatic rules. These paradigmatic rules are refined and transformed into filtering metarules (i.e., metarules augmented with constraints), which are then used by FASTR for extracting term variants. Evaluation of the different variants extracted by FASTR shows high precision and recall. The author reports poor performance in the case of elliptic variants, however, which he notes is largely due to the noncontextuality of FASTR’s parsing approach (p. 169). The recognition of morphosyntactic variants (as opposed to the syntactic variants of Chapter 5) is dealt with in Chapter 7. FASTR’s formalism is extended to include for morphosyntactic variations; for example, of mouse embryos a morphosyntactic rather than a syntactic variant of 218 Book Reviews the adjective transformed into the noun the variant (p. 273). Single words contain morphological links between words and their root lemma (for English, derived from the CELEX database); these links are also expressed in the metagrammar of morphosyntactic variation. Variants of binary terms are presented one morphological transformation at a time: adjective to noun variation, noun to verb variation, and so on. The analysis of these variants is important, as terminology work has concentrated mostly on nouns, leaving unexplored non-nominal terminological occurrences, which are equally important, as they capture relations between terms. Semantic variation, described briefly in Chapter 8, concludes the study of types of term variation. FASTR’s formalism is further extended to account for this type of For example, mouse skin tumors be recognized as a semantic variant neoplasms that there is a semantic link between the words that the insertion of the modifier skin accepted (p. 299). The semantic links used for extracting semantic variants in English are the synonymy links of WordNet 1.6. Morphosyntactic term variation can be seen as a special case of semantic variation, since legitimate morphological links can be established between semantically related words. I found it more useful to read Chapter 6 after Chapters 5, 7, and 8. It describes how FASTR can be used for “incremental term enrichment,” taking into account existing terms and trying to relate newly acquired terms to the previously existing terminologies. The idea is that, by deconstructing term variants, we can detect possible term and acquire new terms. “The variant and carotid artery the opportunity for discovering the term The context of acquisishows that both terms can be coordinated, indicating that the meanings of the original term artery in the candidate term artery similar: a blood vessel” (p. 241). The analysis of such variants produces a set of patterns, called Each pattern extractor is then attached to a specific metarule. Starting from any variant detected by such a metarule, the extractor outputs the corresponding candidate term. This technique of term enrichment is also very useful for producing conceptual relations that can be used for automatic thesaurus construction. In conclusion, this book is clearly written and explores an interesting and very applicable area of computational terminology: term variation. An in-depth analysis of current techniques in terminology, a detailed bibliography, and a plethora of examples demonstrating how to write rules for and use FASTR make this book an important acquisition not only for researchers interested in computational terminology but also for the wider natural language processing community. Anyone concerned with issues of automatic indexing, automatic thesaurus construction, rapid adaptation to new domains, and robust processing of not only domain-specific terminology in the classical sense but also the phrases in which variants of terms are found will learn much from this book.</abstract>
<note confidence="0.698986448275862">References Abeill ´e, Anne and Owen Rambow, editors. Adjoining Grammars: Formalisms, Analysis and CSLI, Stanford, CA. Gazdar, Gerald, Ewan Klein, Geoffrey K. and Ivan A. Sag. 1985. Structure Harvard University Press. Harris, Zellig S., Michael Gottfried, Thomas Ryckman, Paul Mattick, Jr., Anne Daladier, N. Harris, and S. Harris. 1989. Form of Information in Science: Analysis of Sublanguage Studies in the Philosophy of Science, volume 104). Kluwer Academic. Srinivas, B., Dania Egedi, Christine Doran, and Tilman Becker. 1994. Lexicalization and grammar development. In Harald editor, of KONVENS Vienna, pages 310–319. 219 Computational Linguistics Volume 28, Number 2 Ananiadou a Senior Lecturer in Computer Science at the University of Salford, U.K. She teaches natural language processing, and her main research interests are in computational terminology, ontologies, and information extraction. Currently, she is applying natural language processing techniques to bioinformatics. Ananiadou’s address is: Computer Science, School of Sciences, University of Salford, Salford M5 4WT, U.K.; e-mail: S.Ananiadou@salford.ac.uk. 220</note>
<title confidence="0.8933705">Book Reviews Automatic Summarization</title>
<author confidence="0.998192">Inderjeet Mani</author>
<affiliation confidence="0.999778">(MITRE Corporation and Georgetown University)</affiliation>
<address confidence="0.901727">Amsterdam: John Benjamins (Natural</address>
<note confidence="0.961482833333333">language processing series, edited by Ruslan Mitkov, volume 3), 2001, xi+285 pp; hardbound, ISBN 1-58811-059-1, $82.00; paperbound, ISBN 1-58811-060-5, $29.95 Reviewed by</note>
<author confidence="0.997406">Chris D Paice</author>
<affiliation confidence="0.999938">Lancaster University</affiliation>
<abstract confidence="0.995329857142858">Though the first attempts at automatic abstracting were made about 45 years ago, this area was until quite recently a rather obscure specialism. No doubt, this was due to the nonavailability of machine-readable texts: It seemed absurd to go to the trouble and expense of keypunching complete texts and then use a program to throw away 90% of them! Since about 1990, with the urgent need to manage the mass of information available on the World Wide Web, the field of automatic summarization has exploded into prominence. Summarization the first monograph on the subject, and as such it is sure to be widely read and cited. The author, Inderjeet Mani, is currently one of the field’s most active researchers, and it is therefore no surprise that the book has an authoritative feel. In general, it provides good, balanced coverage of the field (except that cross-lingual summarization is not covered) and describes a great deal of the published research in the area, with a bibliography of well over 200 items. Mani succeeds in conveying a sense that this is a most challenging field for system designers. With a grim topicality, many of the illustrations used refer to the summarization of reports on terrorist incidents. The book is structured sensibly into nine main chapters. Chapter 1, “Preliminaries,” introduces a range of fundamental terms and ideas. A distinction is made between consisting of material (typically sentences) taken straight from the source, and “at least some of whose material is not present in the input.” Chapter 2 looks at professional (i.e., nonautomatic) summarizing. Chapter 3 is about extraction, which involves assigning an importance score to each sentence in a text and outputting those with the highest score. Various importance clues may be used, including the presence of concentrations of content words, use of expressions such as see that and the location of the sentence in the text. A long section describes and discusses the work by Edmundson and by later workers pursuing similar approaches. Coming up to date, the chapter continues with a discussion of corpus-based approaches to sentence extraction. Chapter 4 is entitled “Revision” and discusses how to deal with problems of cohesion in extracted sentences that arise from the occurrence of anaphors and other awkward features. Chapter 5 discusses the potential use of discourse-level information in approaches such as cohesion graphs, text tiling, lexical chains, and rhetorical structure theory. In Chapter 6, the author turns to abstraction, as opposed to sentence extraction. Typical approaches here have involved the use of sketchy scripts or frames tailored to a particular domain. Analysis of a text results in instantiation of the appropriate 221 Computational Linguistics Volume 28, Number 2 frame, and when this process is complete, output can be generated, either as stylized “canned text” or by the use of proper text generation techniques. A major problem with this approach is its inflexibility: A text can be summarized only if the correct frame is available and can be identified. Chapter 7 deals with multidocument summarization, which is now seen as a most important area (terrorism news reports, again). This is not simply a matter of merging a number of separate descriptions of the same issue or event: There is a need to highlight discrepancies between different reports and to deal with time-related issues (e.g., changing estimates of numbers of casualties). Another nontrivial problem is handling coreferences—for example, ensuring that mentions of two different documents actually refer to the same person. Chapter 8, “Multimedia Summarization,” is in my view the weakest in the book. Mani starts by acknowledging that this area is evolving rapidly, so that “no clear principles have emerged” (p. 209). But he then takes this as an excuse for a rather haphazard collection of topics (summarization of dialog, of video, and of diagrams, and multimedia briefing generation) dealt with in varying degrees of detail. The chapter is only 12 pages long (nine pages of actual text), whereas to begin to tackle this area, at least twice the space is surely required. To give a proper structure, the chapter should begin with a discussion of possibilities for the direct analysis of various nontext media, followed by an examination, with examples, of how different media (especially text) can support one another in building useful systems. Chapter 9, on the evaluation of summarization systems, is fully three times as long as the previous chapter, and there is no space here to go into details. Though the exposition gets a bit tangled in places, the account is generally authoritative and thorough and provides a valuable overview of this problematic area. the main chapters from Chapter 3 onward finish with table summarizing various approaches discussed, with their strengths and weaknesses, and glossary giving a brief definition of the various terms introduced in the chapter. These are both valuable features, though it seems structurally inconsistent that the former are included as ordinary numbered tables and the latter as separate “Review” sections. Since this is the only textbook in its field, it will probably become required reading for new researchers, who will assume that it provides a full and balanced view. Its very patchy presentation of historical research therefore seems regrettable. Surely,</abstract>
<intro confidence="0.494944">the seminal 1958 paper by H. P. Luhn deserves a section of its own? Apart from its</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2000</date>
<booktitle>Tree Adjoining Grammars: Formalisms, Linguistic Analysis and Processing. CSLI,</booktitle>
<editor>Abeill ´e, Anne and Owen Rambow, editors.</editor>
<location>Stanford, CA.</location>
<marker>2000</marker>
<rawString>Abeill ´e, Anne and Owen Rambow, editors. 2000. Tree Adjoining Grammars: Formalisms, Linguistic Analysis and Processing. CSLI, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey K Pullum</author>
<author>Ivan A Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="4679" citStr="Gazdar et al. 1985" startWordPosition="695" endWordPosition="698">s have something in common semantically. Taken together, the syntagmatic and paradigmatic elements of metarules provide complementary constraints that prevent the generation of undesirable mappings. Chapter 4 is dedicated to the metagrammar of FASTR, which operates on top of the lexical and terminological data. Metarules dynamically transform rules written for controlled terms into new rules that, in turn, can be used to extract variants from texts. The concept of metarule is inspired by the work of Harris (Harris et al. 1989) and is further influenced by generalized phase structure grammars (Gazdar et al. 1985) and feature-based LTAGs (Srinivas et al. 1994) and by work on lexically based formalisms that seek to reduce the complexity and size of the grammar. The author contrasts his use of metarules with other formalisms (p. 157). The main features are that metarules in FASTR are more generic than those in other formalisms and, since they are dedicated to term and variant extraction, they are also much simpler. Of further interest in FASTR’s metarule formalism is that it contains a compiler and an interpreter of regular expressions, and this allows the output of more complex structures (for example, </context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, Gerald, Ewan Klein, Geoffrey K. Pullum, and Ivan A. Sag. 1985. Generalized Phrase Structure Grammar. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
<author>Michael Gottfried</author>
<author>Thomas Ryckman</author>
<author>Paul Mattick</author>
<author>Anne Daladier</author>
<author>T N Harris</author>
<author>S Harris</author>
</authors>
<date>1989</date>
<booktitle>The Form of Information in Science: Analysis of Immunology Sublanguage (Boston Studies in the Philosophy of Science,</booktitle>
<volume>104</volume>
<publisher>Kluwer Academic.</publisher>
<contexts>
<context position="4592" citStr="Harris et al. 1989" startWordPosition="682" endWordPosition="685">ed (by synonymy or antonymy, for example); that is, they capture the fact that the words have something in common semantically. Taken together, the syntagmatic and paradigmatic elements of metarules provide complementary constraints that prevent the generation of undesirable mappings. Chapter 4 is dedicated to the metagrammar of FASTR, which operates on top of the lexical and terminological data. Metarules dynamically transform rules written for controlled terms into new rules that, in turn, can be used to extract variants from texts. The concept of metarule is inspired by the work of Harris (Harris et al. 1989) and is further influenced by generalized phase structure grammars (Gazdar et al. 1985) and feature-based LTAGs (Srinivas et al. 1994) and by work on lexically based formalisms that seek to reduce the complexity and size of the grammar. The author contrasts his use of metarules with other formalisms (p. 157). The main features are that metarules in FASTR are more generic than those in other formalisms and, since they are dedicated to term and variant extraction, they are also much simpler. Of further interest in FASTR’s metarule formalism is that it contains a compiler and an interpreter of re</context>
</contexts>
<marker>Harris, Gottfried, Ryckman, Mattick, Daladier, Harris, Harris, 1989</marker>
<rawString>Harris, Zellig S., Michael Gottfried, Thomas Ryckman, Paul Mattick, Jr., Anne Daladier, T. N. Harris, and S. Harris. 1989. The Form of Information in Science: Analysis of Immunology Sublanguage (Boston Studies in the Philosophy of Science, volume 104). Kluwer Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas</author>
<author>Dania Egedi</author>
<author>Christine Doran</author>
<author>Tilman Becker</author>
</authors>
<title>Lexicalization and grammar development.</title>
<date>1994</date>
<booktitle>Proceedings of KONVENS ’94,</booktitle>
<pages>310--319</pages>
<editor>In Harald Trost, editor,</editor>
<location>Vienna,</location>
<contexts>
<context position="4726" citStr="Srinivas et al. 1994" startWordPosition="702" endWordPosition="705">n together, the syntagmatic and paradigmatic elements of metarules provide complementary constraints that prevent the generation of undesirable mappings. Chapter 4 is dedicated to the metagrammar of FASTR, which operates on top of the lexical and terminological data. Metarules dynamically transform rules written for controlled terms into new rules that, in turn, can be used to extract variants from texts. The concept of metarule is inspired by the work of Harris (Harris et al. 1989) and is further influenced by generalized phase structure grammars (Gazdar et al. 1985) and feature-based LTAGs (Srinivas et al. 1994) and by work on lexically based formalisms that seek to reduce the complexity and size of the grammar. The author contrasts his use of metarules with other formalisms (p. 157). The main features are that metarules in FASTR are more generic than those in other formalisms and, since they are dedicated to term and variant extraction, they are also much simpler. Of further interest in FASTR’s metarule formalism is that it contains a compiler and an interpreter of regular expressions, and this allows the output of more complex structures (for example, coordinated structures). Term variation is expl</context>
</contexts>
<marker>Srinivas, Egedi, Doran, Becker, 1994</marker>
<rawString>Srinivas, B., Dania Egedi, Christine Doran, and Tilman Becker. 1994. Lexicalization and grammar development. In Harald Trost, editor, Proceedings of KONVENS ’94, Vienna, pages 310–319. Conference, Athens, Greece, pages 591–600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Fall</author>
</authors>
<title>Reasoning with Taxonomies.</title>
<date>1996</date>
<tech>Ph.D. dissertation,</tech>
<institution>Department of Computer Science, Simon Fraser University,</institution>
<marker>Fall, 1996</marker>
<rawString>Fall, Andrew. 1996. Reasoning with Taxonomies. Ph.D. dissertation, Department of Computer Science, Simon Fraser University, July 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detmar Meurers</author>
<author>Guido Minnen</author>
</authors>
<title>A computational treatment of lexical rules in HPSG as covariation in lexical entries.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<marker>Meurers, Minnen, 1997</marker>
<rawString>Meurers, Detmar and Guido Minnen. 1997. A computational treatment of lexical rules in HPSG as covariation in lexical entries. Computational Linguistics, 23(4):543–568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Andrew Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar. Chicago: University of Chicago Press and Stanford:</title>
<date>1994</date>
<publisher>CSLI Publications.</publisher>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, Carl and Ivan Andrew Sag. 1994. Head-Driven Phrase Structure Grammar. Chicago: University of Chicago Press and Stanford: CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raghu Ramakrishnan</author>
</authors>
<title>Magic templates: A spellbinding approach to logic programs.</title>
<date>1991</date>
<journal>Journal of Logic Programming,</journal>
<pages>11--189</pages>
<marker>Ramakrishnan, 1991</marker>
<rawString>Ramakrishnan, Raghu. 1991. Magic templates: A spellbinding approach to logic programs. Journal of Logic Programming, 11:189–216.</rawString>
</citation>
<citation valid="true">
<date>1994</date>
<booktitle>Reversible Grammar in Natural Language Processing.</booktitle>
<editor>Strzalkowski, Tomek, editor.</editor>
<publisher>Kluwer Academic.</publisher>
<marker>1994</marker>
<rawString>Strzalkowski, Tomek, editor. 1994. Reversible Grammar in Natural Language Processing. Kluwer Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuly Wintner</author>
<author>Nissim Francez</author>
</authors>
<title>An abstract machine for typed feature structures.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fifth Workshop on Natural Language Understanding and Logic Programming,</booktitle>
<pages>205--220</pages>
<location>Lisbon,</location>
<marker>Wintner, Francez, 1995</marker>
<rawString>Wintner, Shuly and Nissim Francez. 1995. An abstract machine for typed feature structures. In Proceedings of the Fifth Workshop on Natural Language Understanding and Logic Programming, Lisbon, pages 205–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dwight Bolinger</author>
</authors>
<title>Ataxis.</title>
<date>1988</date>
<booktitle>In Rokko Linguistic Society, editors, Gendai no Gengo Kenkyu [Linguistics Today],</booktitle>
<pages>1--17</pages>
<location>Kinseido, Tokyo,</location>
<marker>Bolinger, 1988</marker>
<rawString>Bolinger, Dwight. 1988. Ataxis. In Rokko Linguistic Society, editors, Gendai no Gengo Kenkyu [Linguistics Today], Kinseido, Tokyo, 1–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Ted Briscoe</author>
</authors>
<title>Semi-productive polysemy and sense extension.</title>
<date>1995</date>
<journal>Journal of Semantics,</journal>
<booktitle>Reprinted in James Pustejovsky and Branimir Boguraev, editors, Lexical Semantics: The Problem of Polysemy,</booktitle>
<volume>12</volume>
<issue>1</issue>
<pages>15--67</pages>
<publisher>University Press,</publisher>
<location>Oxford</location>
<marker>Copestake, Briscoe, 1995</marker>
<rawString>Copestake, Ann and Ted Briscoe. 1995. Semi-productive polysemy and sense extension. Journal of Semantics, 12(1):15–67. Reprinted in James Pustejovsky and Branimir Boguraev, editors, Lexical Semantics: The Problem of Polysemy, Oxford University Press, 1996, 15–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CUNY</author>
</authors>
<date>1998</date>
<booktitle>Proceedings of the 11th CUNY Conference on Human Sentence Processing,</booktitle>
<institution>Rutgers University,</institution>
<location>New Brunswick, NJ.</location>
<marker>CUNY, 1998</marker>
<rawString>CUNY. 1998. Proceedings of the 11th CUNY Conference on Human Sentence Processing, Rutgers University, New Brunswick, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold Fodor</author>
</authors>
<title>Three reasons for not deriving kill from cause to die. Linguistic Inquiry,</title>
<date>1970</date>
<pages>1--429</pages>
<marker>Fodor, 1970</marker>
<rawString>Fodor, Jerrold. 1970. Three reasons for not deriving kill from cause to die. Linguistic Inquiry, 1:429–438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Martha Palmer</author>
<author>editors</author>
</authors>
<date>2000</date>
<booktitle>Special Issue on SENSEVAL: Evaluating Word Sense Disambiguation Programs. Computers and the Humanities,</booktitle>
<pages>34</pages>
<marker>Kilgarriff, Palmer, editors, 2000</marker>
<rawString>Kilgarriff, Adam and Martha Palmer, editors. 2000. Special Issue on SENSEVAL: Evaluating Word Sense Disambiguation Programs. Computers and the Humanities, 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Mourelatos</author>
</authors>
<title>Events, processes, and states.</title>
<date>1978</date>
<booktitle>Linguistics and Philosophy, 2:415–434. Also published in Philip Tedeschi and Annie Zaenen, editors, Syntax and Semantics 14: Tense and Aspect,</booktitle>
<pages>191--212</pages>
<publisher>Academic Press,</publisher>
<marker>Mourelatos, 1978</marker>
<rawString>Mourelatos, Alexander. 1978. Events, processes, and states. Linguistics and Philosophy, 2:415–434. Also published in Philip Tedeschi and Annie Zaenen, editors, Syntax and Semantics 14: Tense and Aspect, Academic Press, 1981, 191–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Nunberg</author>
</authors>
<title>Transfers of meaning.</title>
<date>1995</date>
<journal>Journal of Semantics,</journal>
<pages>12--109</pages>
<marker>Nunberg, 1995</marker>
<rawString>Nunberg, Geoffrey. 1995. Transfers of meaning. Journal of Semantics, 12:109–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>Type coercion and lexical selection.</title>
<date>1993</date>
<booktitle>Semantics and the Lexicon.</booktitle>
<pages>73--96</pages>
<editor>In James Pustejovsky, editor,</editor>
<publisher>Kluwer Academic,</publisher>
<marker>Pustejovsky, 1993</marker>
<rawString>Pustejovsky, James. 1993. Type coercion and lexical selection. In James Pustejovsky, editor, Semantics and the Lexicon. Kluwer Academic, pages 73–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The Generative Lexicon.</title>
<date>1995</date>
<publisher>MIT Press.</publisher>
<marker>Pustejovsky, 1995</marker>
<rawString>Pustejovsky, James. 1995. The Generative Lexicon. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen Della Pietra, Vincent Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Jennifer Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Aligning sentences in parallel corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>169--176</pages>
<marker>Brown, Lai, Mercer, 1991</marker>
<rawString>Brown, Peter F., Jennifer Lai, and Robert L. Mercer. 1991. Aligning sentences in parallel corpora. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 169–176,</rawString>
</citation>
<citation valid="false">
<authors>
<author>CA Berkeley</author>
</authors>
<marker>Berkeley, </marker>
<rawString>Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard O Duda</author>
<author>Peter E Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>Wiley.</publisher>
<marker>Duda, Hart, 1973</marker>
<rawString>Duda, Richard O. and Peter E. Hart. 1973. Pattern Classification and Scene Analysis. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<marker>Gale, Church, 1993</marker>
<rawString>Gale, William and Kenneth Church. 1993. A program for aligning sentences in bilingual corpora. Computational Linguistics, 19(1):75–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Automating knowledge acquisition for machine translation.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<issue>4</issue>
<marker>Knight, 1997</marker>
<rawString>Knight, Kevin. 1997. Automating knowledge acquisition for machine translation. AI Magazine, 18(4):81–96.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>is an Assistant Professor of Computer Science at the University of Minnesota, Duluth. His research focuses on word sense disambiguation. He is the recipient of a National Science Foundation CAREER award. Pedersen’s address is:</title>
<institution>Department of Computer Science, University of Minnesota,</institution>
<location>Duluth, MN</location>
<note>55812; e-mail: tpederse@d.umn.edu.</note>
<marker>Pedersen, </marker>
<rawString>Ted Pedersen is an Assistant Professor of Computer Science at the University of Minnesota, Duluth. His research focuses on word sense disambiguation. He is the recipient of a National Science Foundation CAREER award. Pedersen’s address is: Department of Computer Science, University of Minnesota, Duluth, MN 55812; e-mail: tpederse@d.umn.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>The Minimalist Program.</title>
<date>1995</date>
<publisher>MIT Press.</publisher>
<marker>Chomsky, 1995</marker>
<rawString>Chomsky, Noam. 1995. The Minimalist Program. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew W Crocker</author>
</authors>
<title>Computational Psycholinguistics: An Interdisciplinary Approach to the Study of Language.</title>
<date>1996</date>
<publisher>Kluwer Academic.</publisher>
<marker>Crocker, 1996</marker>
<rawString>Crocker, Matthew W. 1996. Computational Psycholinguistics: An Interdisciplinary Approach to the Study of Language. Kluwer Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew W Crocker</author>
<author>Thorsten Brants</author>
</authors>
<title>Wide coverage probabilistic sentence processing.</title>
<date>2000</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>29</volume>
<issue>6</issue>
<marker>Crocker, Brants, 2000</marker>
<rawString>Crocker, Matthew W. and Thorsten Brants. 2000. Wide coverage probabilistic sentence processing. Journal of Psycholinguistic Research, 29(6):647–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew W Crocker</author>
<author>Martin Pickering</author>
<author>Charles Clifton Jr</author>
</authors>
<title>Architectures and Mechanisms for Language Processing.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<marker>Crocker, Pickering, Jr, 2000</marker>
<rawString>Crocker, Matthew W., Martin Pickering, and Charles Clifton Jr. 2000. Architectures and Mechanisms for Language Processing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry A Fodor</author>
<author>Thomas G Bever</author>
<author>Merrill F Garrett</author>
</authors>
<title>The Psychology of Language: An Introduction to Psycholinguistics and Generative Grammar.</title>
<date>1974</date>
<publisher>McGraw-Hill.</publisher>
<location>New York:</location>
<marker>Fodor, Bever, Garrett, 1974</marker>
<rawString>Fodor, Jerry A., Thomas G. Bever, and Merrill F. Garrett. 1974. The Psychology of Language: An Introduction to Psycholinguistics and Generative Grammar. New York: McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
</authors>
<title>On Comprehending Sentences: Syntactic Parsing Strategies.</title>
<date>1978</date>
<institution>University of Connecticut.</institution>
<location>Bloomington, IN.</location>
<note>Ph.D. dissertation,</note>
<marker>Frazier, 1978</marker>
<rawString>Frazier, Lyn. 1978. On Comprehending Sentences: Syntactic Parsing Strategies. Ph.D. dissertation, University of Connecticut. Published by the Indiana University Linguistics Club, Bloomington, IN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel A Jurafsky</author>
</authors>
<title>A probabilistic model of lexical and syntactic access and disambiguation.</title>
<date>1996</date>
<journal>Cognitive Science,</journal>
<pages>20--137</pages>
<marker>Jurafsky, 1996</marker>
<rawString>Jurafsky, Daniel A. 1996. A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science, 20:137–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MIT Press.</publisher>
<marker>Marcus, 1980</marker>
<rawString>Marcus, Mitchell P. 1980. A Theory of Syntactic Recognition for Natural Language. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don C Mitchell</author>
<author>Fernando Cuetos</author>
<author>Martin M B Corley</author>
<author>Marc Brysbaert</author>
</authors>
<title>Exposure based models of human parsing: Evidence for the use of coarse-grained (non-lexical) statistical records.</title>
<date>1995</date>
<journal>Journal of Psycholinguistic Research,</journal>
<pages>24--469</pages>
<marker>Mitchell, Cuetos, Corley, Brysbaert, 1995</marker>
<rawString>Mitchell, Don C., Fernando Cuetos, Martin M. B. Corley, and Marc Brysbaert. 1995. Exposure based models of human parsing: Evidence for the use of coarse-grained (non-lexical) statistical records. Journal of Psycholinguistic Research, 24:469–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley L Pritchett</author>
</authors>
<title>Grammatical Competence and Parsing Performance.</title>
<date>1992</date>
<publisher>University of Chicago Press.</publisher>
<marker>Pritchett, 1992</marker>
<rawString>Pritchett, Bradley L. 1992. Grammatical Competence and Parsing Performance. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Spivey</author>
<author>Michael Tanenhaus</author>
</authors>
<title>Syntactic ambiguity resolution in discourse: Modeling the effects of referential context and lexical frequency.</title>
<date>1998</date>
<journal>Journal of Experimental Psychology: Learning, Memory, and Cognition,</journal>
<pages>24--1521</pages>
<marker>Spivey, Tanenhaus, 1998</marker>
<rawString>Spivey, Michael and Michael Tanenhaus. 1998. Syntactic ambiguity resolution in discourse: Modeling the effects of referential context and lexical frequency. Journal of Experimental Psychology: Learning, Memory, and Cognition, 24:1521–1543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzanne Stevenson</author>
</authors>
<title>Competition and recency in a hybrid network model of syntactic disambiguation.</title>
<date>1994</date>
<journal>Journal of Psycholinguistic Research,</journal>
<pages>23--295</pages>
<marker>Stevenson, 1994</marker>
<rawString>Stevenson, Suzanne. 1994. Competition and recency in a hybrid network model of syntactic disambiguation. Journal of Psycholinguistic Research, 23:295–322.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W Matthew</author>
</authors>
<title>Crocker is a Professor of Psycholinguistics in the Department of Computational Linguistics, Saarland University. His research is concerned with computational models of human sentence processing, focusing in particular on the development of probabilistic models of sentence comprehension, statistical language processing, and experimental psycholinguistics. Crocker’s address is: Department of Computational Linguistics, Building 17, Saarland University, 66041 Saarbr¨ucken, Germany; e-mail: crocker@coli.uni-sb.de;</title>
<note>http://www.coli. uni-sb.de/∼crocker/.</note>
<marker>Matthew, </marker>
<rawString>Matthew W. Crocker is a Professor of Psycholinguistics in the Department of Computational Linguistics, Saarland University. His research is concerned with computational models of human sentence processing, focusing in particular on the development of probabilistic models of sentence comprehension, statistical language processing, and experimental psycholinguistics. Crocker’s address is: Department of Computational Linguistics, Building 17, Saarland University, 66041 Saarbr¨ucken, Germany; e-mail: crocker@coli.uni-sb.de; http://www.coli. uni-sb.de/∼crocker/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>