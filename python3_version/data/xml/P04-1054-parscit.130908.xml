<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000241">
<title confidence="0.995441">
Dependency Tree Kernels for Relation Extraction
</title>
<author confidence="0.997906">
Aron Culotta
</author>
<affiliation confidence="0.996055">
University of Massachusetts
</affiliation>
<address confidence="0.700711">
Amherst, MA 01002
USA
</address>
<email confidence="0.998336">
culotta@cs.umass.edu
</email>
<note confidence="0.4901925">
Jeffrey Sorensen
IBM T.J. Watson Research Center
</note>
<author confidence="0.497898">
Yorktown Heights, NY 10598
</author>
<affiliation confidence="0.453854">
USA
</affiliation>
<email confidence="0.997048">
sorenj@us.ibm.com
</email>
<sectionHeader confidence="0.99384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999812">
We extend previous work on tree kernels to estimate
the similarity between the dependency trees of sen-
tences. Using this kernel within a Support Vector
Machine, we detect and classify relations between
entities in the Automatic Content Extraction (ACE)
corpus of news articles. We examine the utility of
different features such as Wordnet hypernyms, parts
of speech, and entity types, and find that the depen-
dency tree kernel achieves a 20% F1 improvement
over a “bag-of-words” kernel.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968827586207">
The ability to detect complex patterns in data is lim-
ited by the complexity of the data’s representation.
In the case of text, a more structured data source
(e.g. a relational database) allows richer queries
than does an unstructured data source (e.g. a col-
lection of news articles). For example, current web
search engines would not perform well on the query,
“list all California-based CEOs who have social ties
with a United States Senator.” Only a structured
representation of the data can effectively provide
such a list.
The goal of Information Extraction (IE) is to dis-
cover relevant segments of information in a data
stream that will be useful for structuring the data.
In the case of text, this usually amounts to finding
mentions of interesting entities and the relations that
join them, transforming a large corpus of unstruc-
tured text into a relational database with entries such
as those in Table 1.
IE is commonly viewed as a three stage process:
first, an entity tagger detects all mentions of interest;
second, coreference resolution resolves disparate
mentions of the same entity; third, a relation extrac-
tor finds relations between these entities. Entity tag-
ging has been thoroughly addressed by many statis-
tical machine learning techniques, obtaining greater
than 90% F1 on many datasets (Tjong Kim Sang
and De Meulder, 2003). Coreference resolution is
an active area of research not investigated here (Pa-
</bodyText>
<table confidence="0.965231333333333">
Entity Type Location
Apple Organization Cupertino, CA
Microsoft Organization Redmond, WA
</table>
<tableCaption confidence="0.999866">
Table 1: An example of extracted fields
</tableCaption>
<bodyText confidence="0.9955385">
sula et al., 2002; McCallum and Wellner, 2003).
We describe a relation extraction technique based
on kernel methods. Kernel methods are non-
parametric density estimation techniques that com-
pute a kernel function between data instances,
where a kernel function can be thought of as a sim-
ilarity measure. Given a set of labeled instances,
kernel methods determine the label of a novel in-
stance by comparing it to the labeled training in-
stances using this kernel function. Nearest neighbor
classification and support-vector machines (SVMs)
are two popular examples of kernel methods (Fuku-
naga, 1990; Cortes and Vapnik, 1995).
An advantage of kernel methods is that they can
search a feature space much larger than could be
represented by a feature extraction-based approach.
This is possible because the kernel function can ex-
plore an implicit feature space when calculating the
similarity between two instances, as described in the
Section 3.
Working in such a large feature space can lead to
over-fitting in many machine learning algorithms.
To address this problem, we apply SVMs to the task
of relation extraction. SVMs find a boundary be-
tween instances of different classes such that the
distance between the boundary and the nearest in-
stances is maximized. This characteristic, in addi-
tion to empirical validation, indicates that SVMs are
particularly robust to over-fitting.
Here we are interested in detecting and classify-
ing instances of relations, where a relation is some
meaningful connection between two entities (Table
2). We represent each relation instance as an aug-
mented dependency tree. A dependency tree repre-
sents the grammatical dependencies in a sentence;
we augment this tree with features for each node
</bodyText>
<table confidence="0.996467">
AT NEAR PART ROLE SOCIAL
Based-In Relative-location Part-of Affiliate, Founder Associate, Grandparent
Located Subsidiary Citizen-of, Management Parent, Sibling
Residence Other Client, Member Spouse, Other-professional
Owner, Other, Staff Other-relative, Other-personal
</table>
<tableCaption confidence="0.999449">
Table 2: Relation types and subtypes.
</tableCaption>
<bodyText confidence="0.999790833333333">
(e.g. part of speech) We choose this representation
because we hypothesize that instances containing
similar relations will share similar substructures in
their dependency trees. The task of the kernel func-
tion is to find these similarities.
We define a tree kernel over dependency trees and
incorporate this kernel within an SVM to extract
relations from newswire documents. The tree ker-
nel approach consistently outperforms the bag-of-
words kernel, suggesting that this highly-structured
representation of sentences is more informative for
detecting and distinguishing relations.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999954666666667">
Kernel methods (Vapnik, 1998; Cristianini and
Shawe-Taylor, 2000) have become increasingly
popular because of their ability to map arbitrary ob-
jects to a Euclidian feature space. Haussler (1999)
describes a framework for calculating kernels over
discrete structures such as strings and trees. String
kernels for text classification are explored in Lodhi
et al. (2000), and tree kernel variants are described
in (Zelenko et al., 2003; Collins and Duffy, 2002;
Cumby and Roth, 2003). Our algorithm is similar
to that described by Zelenko et al. (2003). Our
contributions are a richer sentence representation, a
more general framework to allow feature weighting,
as well as the use of composite kernels to reduce
kernel sparsity.
Brin (1998) and Agichtein and Gravano (2000)
apply pattern matching and wrapper techniques for
relation extraction, but these approaches do not
scale well to fastly evolving corpora. Miller et al.
(2000) propose an integrated statistical parsing tech-
nique that augments parse trees with semantic la-
bels denoting entity and relation types. Whereas
Miller et al. (2000) use a generative model to pro-
duce parse information as well as relation informa-
tion, we hypothesize that a technique discrimina-
tively trained to classify relations will achieve bet-
ter performance. Also, Roth and Yih (2002) learn a
Bayesian network to tag entities and their relations
simultaneously. We experiment with a more chal-
lenging set of relation types and a larger corpus.
</bodyText>
<sectionHeader confidence="0.999818" genericHeader="method">
3 Kernel Methods
</sectionHeader>
<bodyText confidence="0.9964998">
In traditional machine learning, we are provided
a set of training instances S = {x1 ... xN},
where each instance xi is represented by some d-
dimensional feature vector. Much time is spent on
the task of feature engineering – searching for the
optimal feature set either manually by consulting
domain experts or automatically through feature in-
duction and selection (Scott and Matwin, 1999).
For example, in entity detection the original in-
stance representation is generally a word vector cor-
responding to a sentence. Feature extraction and
induction may result in features such as part-of-
speech, word n-grams, character n-grams, capital-
ization, and conjunctions of these features. In the
case of more structured objects, such as parse trees,
features may include some description of the ob-
ject’s structure, such as “has an NP-VP subtree.”
Kernel methods can be particularly effective at re-
ducing the feature engineering burden for structured
objects. By calculating the similarity between two
objects, kernel methods can employ dynamic pro-
gramming solutions to efficiently enumerate over
substructures that would be too costly to explicitly
include as features.
Formally, a kernel function K is a mapping
</bodyText>
<equation confidence="0.858763666666667">
K : X x X —* [0, oc] from instance space X
to a similarity score K(x, y) = Ei Oi(x)Oi(y) =
O(x) · O(y). Here, Oi(x) is some feature func-
</equation>
<bodyText confidence="0.967323773584906">
tion over the instance x. The kernel function must
be symmetric [K(x, y) = K(y, x)] and positive-
semidefinite. By positive-semidefinite, we require
that the if x1, ... , xn E X, then the n x n matrix
G defined by Gij = K(xi, xj) is positive semi-
definite. It has been shown that any function that
takes the dot product of feature vectors is a kernel
function (Haussler, 1999).
A simple kernel function takes the dot product of
the vector representation of instances being com-
pared. For example, in document classification,
each document can be represented by a binary vec-
tor, where each element corresponds to the presence
or absence of a particular word in that document.
Here, Oi(x) = 1 if word i occurs in document x.
Thus, the kernel function K(x, y) returns the num-
ber of words in common between x and y. We refer
to this kernel as the “bag-of-words” kernel, since it
ignores word order.
When instances are more structured, as in the
case of dependency trees, more complex kernels
become necessary. Haussler (1999) describes con-
volution kernels, which find the similarity between
two structures by summing the similarity of their
substructures. As an example, consider a kernel
over strings. To determine the similarity between
two strings, string kernels (Lodhi et al., 2000) count
the number of common subsequences in the two
strings, and weight these matches by their length.
Thus, Oi(x) is the number of times string x contains
the subsequence referenced by i. These matches can
be found efficiently through a dynamic program,
allowing string kernels to examine long-range fea-
tures that would be computationally infeasible in a
feature-based method.
Given a training set S = {xs ... xN}, kernel
methods compute the Gram matrix G such that
Gij = K(xi,xj). Given G, the classifier finds a
hyperplane which separates instances of different
classes. To classify an unseen instance x, the classi-
fier first projects x into the feature space defined by
the kernel function. Classification then consists of
determining on which side of the separating hyper-
plane x lies.
A support vector machine (SVM) is a type of
classifier that formulates the task of finding the sep-
arating hyperplane as the solution to a quadratic pro-
gramming problem (Cristianini and Shawe-Taylor,
2000). Support vector machines attempt to find a
hyperplane that not only separates the classes but
also maximizes the margin between them. The hope
is that this will lead to better generalization perfor-
mance on unseen instances.
</bodyText>
<sectionHeader confidence="0.978365" genericHeader="method">
4 Augmented Dependency Trees
</sectionHeader>
<bodyText confidence="0.999942666666667">
Our task is to detect and classify relations between
entities in text. We assume that entity tagging has
been performed; so to generate potential relation
instances, we iterate over all pairs of entities oc-
curring in the same sentence. For each entity pair,
we create an augmented dependency tree (described
below) representing this instance. Given a labeled
training set of potential relations, we define a tree
kernel over dependency trees which we then use in
an SVM to classify test instances.
A dependency tree is a representation that de-
notes grammatical relations between words in a sen-
tence (Figure 1). A set of rules maps a parse tree to
a dependency tree. For example, subjects are de-
pendent on their verbs and adjectives are dependent
</bodyText>
<figureCaption confidence="0.850986">
Figure 1: A dependency tree for the sentence
Troops advanced near Tikrit.
</figureCaption>
<table confidence="0.932042777777778">
Feature Example
word troops, Tikrit
part-of-speech (24 values) NN, NNP
general-pos (S values) noun, verb, adj
chunk-tag NP, VP, ADJP
entity-type person, geo-political-entity
entity-level name, nominal, pronoun
Wordnet hypernyms social group, city
relation-argument ARG A, ARG B
</table>
<tableCaption confidence="0.940427">
Table 3: List of features assigned to each node in
</tableCaption>
<bodyText confidence="0.977893565217391">
the dependency tree.
on the nouns they modify. Note that for the pur-
poses of this paper, we do not consider the link la-
bels (e.g. “object”, “subject”); instead we use only
the dependency structure. To generate the parse tree
of each sentence, we use MXPOST, a maximum en-
tropy statistical parser1; we then convert this parse
tree to a dependency tree. Note that the left-to-right
ordering of the sentence is maintained in the depen-
dency tree only among siblings (i.e. the dependency
tree does not specify an order to traverse the tree to
recover the original sentence).
For each pair of entities in a sentence, we find
the smallest common subtree in the dependency tree
that includes both entities. We choose to use this
subtree instead of the entire tree to reduce noise
and emphasize the local characteristics of relations.
We then augment each node of the tree with a fea-
ture vector (Table 3). The relation-argument feature
specifies whether an entity is the first or second ar-
gument in a relation. This is required to learn asym-
metric relations (e.g. X OWNS Y).
Formally, a relation instance is a dependency tree
</bodyText>
<footnote confidence="0.760961">
1http://www.cis.upenn.edu/˜adwait/statnlp.html
</footnote>
<figure confidence="0.557846888888889">
Troops
t
t
1 2
advanced
t0
near
Tikrit
t3
</figure>
<bodyText confidence="0.953755125">
T with nodes It0 ... tn}. The features of node ti
are given by 0(ti) = Iv1 ... vd}. We refer to the
jth child of node ti as ti[j], and we denote the set
of all children of node ti as ti[c]. We reference a
subset j of children of ti by ti[j] C_ ti[c]. Finally, we
refer to the parent of node ti as ti.p.
From the example in Figure 1, t0[1] = t2,
t0[I0,1}] = It1, t2}, and t1.p = t0.
</bodyText>
<sectionHeader confidence="0.858031" genericHeader="method">
5 Tree kernels for dependency trees
</sectionHeader>
<bodyText confidence="0.997791066666667">
We now define a kernel function for dependency
trees. The tree kernel is a function K(T1, T2) that
returns a normalized, symmetric similarity score in
the range (0, 1) for two trees T1 and T2. We de-
fine a slightly more general version of the kernel
described by Zelenko et al. (2003).
We first define two functions over the features of
tree nodes: a matching function m(ti, tj) E I0, 1}
and a similarity function s(ti, tj) E (0, oc]. Let the
feature vector 0(ti) = Iv1 ... vd} consist of two
possibly overlapping subsets 0m(ti) C_ 0(ti) and
0s(ti) C_ 0(ti). We use 0m(ti) in the matching
function and 0s(ti) in the similarity function. We
define
and
</bodyText>
<equation confidence="0.9464605">
s(ti, tj) = X X C(vq, vr)
vq∈φ.(ti) v,∈φ.(tj)
</equation>
<bodyText confidence="0.999449533333333">
where C(vq, vr) is some compatibility function
between two feature values. For example, in the
simplest case where
s(ti, tj) returns the number of feature values in
common between feature vectors 0s(ti) and 0s(tj).
We can think of the distinction between functions
m(ti, tj) and s(ti, tj) as a way to discretize the sim-
ilarity between two nodes. If 0m(ti) =� 0m(tj),
then we declare the two nodes completely dissimi-
lar. However, if 0m(ti) = 0m(tj), then we proceed
to compute the similarity s(ti, tj). Thus, restrict-
ing nodes by m(ti, tj) is a way to prune the search
space of matching subtrees, as shown below.
For two dependency trees T1, T2, with root nodes
r1 and r2, we define the tree kernel K(T1, T2) as
</bodyText>
<equation confidence="0.971251666666667">
0 if m(r1, r2) = 0
s(r1, r2)+
Kc(r1[c], r2[c]) otherwise
</equation>
<bodyText confidence="0.9999344">
where Kc is a kernel function over children. Let
a and b be sequences of indices such that a is a
sequence a1 &lt; a2 &lt; ... &lt; an, and likewise for b.
Let d(a) = an − a1 + 1 and l(a) be the length of a.
Then we have Kc(ti[c], tj[c]) =
</bodyText>
<equation confidence="0.981958">
X Ad(a)Ad(b)K(ti[a], tj[b])
a,b,l(a)=l(b)
</equation>
<bodyText confidence="0.999924970588235">
The constant 0 &lt; A &lt; 1 is a decay factor that
penalizes matching subsequences that are spread
out within the child sequences. See Zelenko et al.
(2003) for a proof that K is kernel function.
Intuitively, whenever we find a pair of matching
nodes, we search for all matching subsequences of
the children of each node. A matching subsequence
of children is a sequence of children a and b such
that m(ai, bi) = 1 (bi &lt; n). For each matching
pair of nodes (ai, bi) in a matching subsequence,
we accumulate the result of the similarity function
s(ai, bj) and then recursively search for matching
subsequences of their children ai[c], bj[c].
We implement two types of tree kernels. A
contiguous kernel only matches children subse-
quences that are uninterrupted by non-matching
nodes. Therefore, d(a) = l(a). A sparse tree ker-
nel, by contrast, allows non-matching nodes within
matching subsequences.
Figure 2 shows two relation instances, where
each node contains the original text plus the features
used for the matching function, 0m(ti) = Igeneral-
pos, entity-type, relation-argument}. (“NA” de-
notes the feature is not present for this node.) The
contiguous kernel matches the following substruc-
tures: It0[0], u0[0]}, It0[2], u0[1]}, It3[0], u2[0]}.
Because the sparse kernel allows non-contiguous
matching sequences, it matches an additional sub-
structure It0[0, *, 2], u0[0, *,1]}, where (*) indi-
cates an arbitrary number of non-matching nodes.
Zelenko et al. (2003) have shown the contiguous
kernel to be computable in O(mn) and the sparse
kernel in O(mn3), where m and n are the number
of children in trees T1 and T2 respectively.
</bodyText>
<sectionHeader confidence="0.999304" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999609666666667">
We extract relations from the Automatic Content
Extraction (ACE) corpus provided by the National
Institute for Standards and Technology (NIST). The
</bodyText>
<equation confidence="0.975565857142857">
m(ti, tj) =
(0 otherwise
1 if 0m(ti) = 0m(tj)
(
1 if vq = v
C(vq, vr)
r
</equation>
<figure confidence="0.82034">
0 otherwise
follows:
K(T1,T2) =
⎧
⎨⎪
⎪⎩
</figure>
<figureCaption confidence="0.999033">
Figure 2: Two instances of the NEAR relation.
</figureCaption>
<bodyText confidence="0.99984476">
data consists of about 800 annotated text documents
gathered from various newspapers and broadcasts.
Five entities have been annotated (PERSON, ORGA-
NIZATION, GEO-POLITICAL ENTITY, LOCATION,
FACILITY), along with 24 types of relations (Table
2). As noted from the distribution of relationship
types in the training data (Figure 3), data imbalance
and sparsity are potential problems.
In addition to the contiguous and sparse tree
kernels, we also implement a bag-of-words ker-
nel, which treats the tree as a vector of features
over nodes, disregarding any structural informa-
tion. We also create composite kernels by combin-
ing the sparse and contiguous kernels with the bag-
of-words kernel. Joachims et al. (2001) have shown
that given two kernels K1, K2, the composite ker-
nel K12(xi, xj) = K1(xi, xj)+K2(xi, xj) is also a
kernel. We find that this composite kernel improves
performance when the Gram matrix G is sparse (i.e.
our instances are far apart in the kernel space).
The features used to represent each node are
shown in Table 3. After initial experimentation,
the set of features we use in the matching func-
tion is φm(ti) = {general-pos, entity-type, relation-
argument}, and the similarity function examines the
</bodyText>
<figureCaption confidence="0.997795">
Figure 3: Distribution over relation types in train-
ing data.
</figureCaption>
<bodyText confidence="0.94681">
remaining features.
In our experiments we tested the following five
kernels:
</bodyText>
<equation confidence="0.9992718">
K0 = sparse kernel
K1 = contiguous kernel
K2 = bag-of-words kernel
K3 = K0 + K2
K4 = K1 + K2
</equation>
<bodyText confidence="0.952554642857143">
We also experimented with the function C(vq, vr),
the compatibility function between two feature val-
ues. For example, we can increase the importance
of two nodes having the same Wordnet hypernym2.
If vq, vr are hypernym features, then we can define
When
&gt; 1, we increase the similarity of
nodes that share a hypernym. We tested a num-
ber of weighting schemes, but did not obtain a set
of weights that produced consistent significant im-
provements. See Section 8 for altern
α
ate approaches
to setting C.
</bodyText>
<footnote confidence="0.722345">
2http://www.cogsci.princeton.edu/˜wn/
</footnote>
<figure confidence="0.994821789473684">
ARG_A
forces
noun
person
ARG_A
troops
noun
person
t
1
u
1
moved
verb
NA
NA
quickly
adverb
NA
NA
advanced
verb
NA
NA
t
0
t t
2 3
u
0
Baghdad
noun
geo−political
ARG_B
geo−political
ARG_B
toward
prep
NA
NA
Tikrit
noun
t
4
near
prep
NA
NA
u
2
u
3
�
α if vq = v
C(vq, vr)
r
0 otherwise
</figure>
<table confidence="0.9980984">
Avg. Prec. Avg. Rec. Avg. F1
K1 69.6 25.3 36.8
K2 47.0 10.0 14.2
K3 68.9 24.3 35.5
K4 70.3 26.3 38.0
</table>
<tableCaption confidence="0.999611">
Table 4: Kernel performance comparison.
</tableCaption>
<bodyText confidence="0.998788">
Table 4 shows the results of each kernel within
an SVM. (We augment the LibSVM3 implementa-
tion to include our dependency tree kernel.) Note
that, although training was done over all 24 rela-
tion subtypes, we evaluate only over the 5 high-level
relation types. Thus, classifying a RESIDENCE re-
lation as a LOCATED relation is deemed correct4.
Note also that K0 is not included in Table 4 because
of burdensome computational time. Table 4 shows
that precision is adequate, but recall is low. This
is a result of the aforementioned class imbalance –
very few of the training examples are relations, so
the classifier is less likely to identify a testing in-
stances as a relation. Because we treat every pair
of mentions in a sentence as a possible relation, our
training set contains fewer than 15% positive rela-
tion instances.
To remedy this, we retrain each SVMs for a bi-
nary classification task. Here, we detect, but do not
classify, relations. This allows us to combine all
positive relation instances into one class, which pro-
vides us more training samples to estimate the class
boundary. We then threshold our output to achieve
an optimal operating point. As seen in Table 5, this
method of relation detection outperforms that of the
multi-class classifier.
We then use these binary classifiers in a cascading
scheme as follows: First, we use the binary SVM
to detect possible relations. Then, we use the SVM
trained only on positive relation instances to classify
each predicted relation. These results are shown in
Table 6.
The first result of interest is that the sparse tree
kernel, K0, does not perform as well as the con-
tiguous tree kernel, K1. Suspecting that noise was
introduced by the non-matching nodes allowed in
the sparse tree kernel, we performed the experi-
ment with different values for the decay factor A =
{.9,.5,.1}, but obtained no improvement.
The second result of interest is that all tree ker-
nels outperform the bag-of-words kernel, K2, most
noticeably in recall performance, implying that the
</bodyText>
<footnote confidence="0.989827666666667">
3http://www.csie.ntu.edu.tw/˜cjlin/libsvm/
4This is to compensate for the small amount of training data
for many classes.
</footnote>
<table confidence="0.999428272727273">
Prec. Rec. F1
K0 – – –
K0 (B) 83.4 45.5 58.8
K1 91.4 37.1 52.8
K1 (B) 84.7 49.3 62.3
K2 92.7 10.6 19.0
K2 (B) 72.5 40.2 51.7
K3 91.3 35.1 50.8
K3 (B) 80.1 49.9 61.5
K4 91.8 37.5 53.3
K4 (B) 81.2 51.8 63.2
</table>
<tableCaption confidence="0.993847">
Table 5: Relation detection performance. (B) de-
notes binary classification.
</tableCaption>
<table confidence="0.9998355">
D C Avg. Prec. Avg. Rec. Avg. F1
K0 K0 66.0 29.0 40.1
K1 K1 66.6 32.4 43.5
K2 K2 62.5 27.7 38.1
K3 K3 67.5 34.3 45.3
K4 K4 67.1 35.0 45.8
K1 K4 67.4 33.9 45.0
K4 K1 65.3 32.5 43.3
</table>
<tableCaption confidence="0.980627">
Table 6: Results on the cascading classification. D
</tableCaption>
<bodyText confidence="0.996893375">
and C denote the kernel used for relation detection
and classification, respectively.
structural information the tree kernel provides is ex-
tremely useful for relation detection.
Note that the average results reported here are
representative of the performance per relation, ex-
cept for the NEAR relation, which had slightly lower
results overall due to its infrequency in training.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="method">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999956416666667">
We have shown that using a dependency tree ker-
nel for relation extraction provides a vast improve-
ment over a bag-of-words kernel. While the de-
pendency tree kernel appears to perform well at the
task of classifying relations, recall is still relatively
low. Detecting relations is a difficult task for a ker-
nel method because the set of all non-relation in-
stances is extremely heterogeneous, and is therefore
difficult to characterize with a similarity metric. An
improved system might use a different method to
detect candidate relations and then use this kernel
method to classify the relations.
</bodyText>
<sectionHeader confidence="0.999247" genericHeader="discussions">
8 Future Work
</sectionHeader>
<bodyText confidence="0.999978222222222">
The most immediate extension is to automatically
learn the feature compatibility function C(vq, vr).
A first approach might use tf-idf to weight each fea-
ture. Another approach might be to calculate the
information gain for each feature and use that as
its weight. A more complex system might learn a
weight for each pair of features; however this seems
computationally infeasible for large numbers of fea-
tures.
One could also perform latent semantic indexing
to collapse feature values into similar “categories”
— for example, the words “football” and “baseball”
might fall into the same category. Here, C(vq, vr)
might return α1 if vq = vr, and α2 if vq and vr are
in the same category, where α1 &gt; α2 &gt; 0. Any
method that provides a “soft” match between fea-
ture values will sharpen the granularity of the kernel
and enhance its modeling power.
Further investigation is also needed to understand
why the sparse kernel performs worse than the con-
tiguous kernel. These results contradict those given
in Zelenko et al. (2003), where the sparse kernel
achieves 2-3% better F1 performance than the con-
tiguous kernel. It is worthwhile to characterize rela-
tion types that are better captured by the sparse ker-
nel, and to determine when using the sparse kernel
is worth the increased computational burden.
</bodyText>
<sectionHeader confidence="0.99885" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998372256756757">
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text
collections. In Proceedings of the Fifth ACMIn-
ternational Conference on Digital Libraries.
Sergey Brin. 1998. Extracting patterns and rela-
tions from the world wide web. In WebDB Work-
shop at 6th International Conference on Extend-
ing Database Technology, EDBT’98.
M. Collins and N. Duffy. 2002. Convolution ker-
nels for natural language. In T. G. Dietterich,
S. Becker, and Z. Ghahramani, editors, Advances
in Neural Information Processing Systems 14,
Cambridge, MA. MIT Press.
Corinna Cortes and Vladimir Vapnik. 1995.
Support-vector networks. Machine Learning,
20(3):273–297.
N. Cristianini and J. Shawe-Taylor. 2000. An intro-
duction to support vector machines. Cambridge
University Press.
Chad M. Cumby and Dan Roth. 2003. On kernel
methods for relational learning. In Tom Fawcett
and Nina Mishra, editors, Machine Learning,
Proceedings of the Twentieth International Con-
ference (ICML 2003), August 21-24, 2003, Wash-
ington, DC, USA. AAAI Press.
K. Fukunaga. 1990. Introduction to Statistical Pat-
tern Recognition. Academic Press, second edi-
tion.
D. Haussler. 1999. Convolution kernels on dis-
crete structures. Technical Report UCS-CRL-99-
10, University of California, Santa Cruz.
Thorsten Joachims, Nello Cristianini, and John
Shawe-Taylor. 2001. Composite kernels for hy-
pertext categorisation. In Carla Brodley and An-
drea Danyluk, editors, Proceedings of ICML-
01, 18th International Conference on Machine
Learning, pages 250–257, Williams College, US.
Morgan Kaufmann Publishers, San Francisco,
US.
Huma Lodhi, John Shawe-Taylor, Nello Cristian-
ini, and Christopher J. C. H. Watkins. 2000. Text
classification using string kernels. In NIPS, pages
563–569.
A. McCallum and B. Wellner. 2003. Toward con-
ditional models of identity uncertainty with ap-
plication to proper noun coreference. In IJCAI
Workshop on Information Integration on the Web.
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel.
2000. A novel use of statistical parsing to ex-
tract information from text. In 6th Applied Nat-
ural Language Processing Conference.
H. Pasula, B. Marthi, B. Milch, S. Russell, and
I. Shpitser. 2002. Identity uncertainty and cita-
tion matching.
Dan Roth and Wen-tau Yih. 2002. Probabilistic
reasoning for entity and relation recognition. In
19th International Conference on Computational
Linguistics.
Sam Scott and Stan Matwin. 1999. Feature engi-
neering for text classification. In Proceedings of
ICML-99, 16th International Conference on Ma-
chine Learning.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared
task: Language-independent named entity recog-
nition. In Walter Daelemans and Miles Osborne,
editors, Proceedings of CoNLL-2003, pages 142–
147. Edmonton, Canada.
Vladimir Vapnik. 1998. Statistical Learning The-
ory. Whiley, Chichester, GB.
D. Zelenko, C. Aone, and A. Richardella. 2003.
Kernel methods for relation extraction. Jour-
nal of Machine Learning Research, pages 1083–
1106.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.973398">
<title confidence="0.999866">Dependency Tree Kernels for Relation Extraction</title>
<author confidence="0.999973">Aron Culotta</author>
<affiliation confidence="0.999987">University of Massachusetts</affiliation>
<address confidence="0.9981785">Amherst, MA 01002 USA</address>
<email confidence="0.999448">culotta@cs.umass.edu</email>
<author confidence="0.999997">Jeffrey Sorensen</author>
<affiliation confidence="0.999982">IBM T.J. Watson Research Center</affiliation>
<address confidence="0.993108">Yorktown Heights, NY 10598 USA</address>
<email confidence="0.999881">sorenj@us.ibm.com</email>
<abstract confidence="0.999143818181818">We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences. Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles. We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a “bag-of-words” kernel.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fifth ACMInternational Conference on Digital Libraries.</booktitle>
<contexts>
<context position="5691" citStr="Agichtein and Gravano (2000)" startWordPosition="868" endWordPosition="871">jects to a Euclidian feature space. Haussler (1999) describes a framework for calculating kernels over discrete structures such as strings and trees. String kernels for text classification are explored in Lodhi et al. (2000), and tree kernel variants are described in (Zelenko et al., 2003; Collins and Duffy, 2002; Cumby and Roth, 2003). Our algorithm is similar to that described by Zelenko et al. (2003). Our contributions are a richer sentence representation, a more general framework to allow feature weighting, as well as the use of composite kernels to reduce kernel sparsity. Brin (1998) and Agichtein and Gravano (2000) apply pattern matching and wrapper techniques for relation extraction, but these approaches do not scale well to fastly evolving corpora. Miller et al. (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types. Whereas Miller et al. (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance. Also, Roth and Yih (2002) learn a Bayesian network to tag entities and their re</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of the Fifth ACMInternational Conference on Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
</authors>
<title>Extracting patterns and relations from the world wide web.</title>
<date>1998</date>
<booktitle>In WebDB Workshop at 6th International Conference on Extending Database Technology, EDBT’98.</booktitle>
<contexts>
<context position="5658" citStr="Brin (1998)" startWordPosition="865" endWordPosition="866">map arbitrary objects to a Euclidian feature space. Haussler (1999) describes a framework for calculating kernels over discrete structures such as strings and trees. String kernels for text classification are explored in Lodhi et al. (2000), and tree kernel variants are described in (Zelenko et al., 2003; Collins and Duffy, 2002; Cumby and Roth, 2003). Our algorithm is similar to that described by Zelenko et al. (2003). Our contributions are a richer sentence representation, a more general framework to allow feature weighting, as well as the use of composite kernels to reduce kernel sparsity. Brin (1998) and Agichtein and Gravano (2000) apply pattern matching and wrapper techniques for relation extraction, but these approaches do not scale well to fastly evolving corpora. Miller et al. (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types. Whereas Miller et al. (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance. Also, Roth and Yih (2002) learn a Bayesian net</context>
</contexts>
<marker>Brin, 1998</marker>
<rawString>Sergey Brin. 1998. Extracting patterns and relations from the world wide web. In WebDB Workshop at 6th International Conference on Extending Database Technology, EDBT’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution kernels for natural language. In</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems 14,</booktitle>
<editor>T. G. Dietterich, S. Becker, and Z. Ghahramani, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5377" citStr="Collins and Duffy, 2002" startWordPosition="818" endWordPosition="821">bag-ofwords kernel, suggesting that this highly-structured representation of sentences is more informative for detecting and distinguishing relations. 2 Related Work Kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) have become increasingly popular because of their ability to map arbitrary objects to a Euclidian feature space. Haussler (1999) describes a framework for calculating kernels over discrete structures such as strings and trees. String kernels for text classification are explored in Lodhi et al. (2000), and tree kernel variants are described in (Zelenko et al., 2003; Collins and Duffy, 2002; Cumby and Roth, 2003). Our algorithm is similar to that described by Zelenko et al. (2003). Our contributions are a richer sentence representation, a more general framework to allow feature weighting, as well as the use of composite kernels to reduce kernel sparsity. Brin (1998) and Agichtein and Gravano (2000) apply pattern matching and wrapper techniques for relation extraction, but these approaches do not scale well to fastly evolving corpora. Miller et al. (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relatio</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>M. Collins and N. Duffy. 2002. Convolution kernels for natural language. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="2913" citStr="Cortes and Vapnik, 1995" startWordPosition="453" endWordPosition="456">lds sula et al., 2002; McCallum and Wellner, 2003). We describe a relation extraction technique based on kernel methods. Kernel methods are nonparametric density estimation techniques that compute a kernel function between data instances, where a kernel function can be thought of as a similarity measure. Given a set of labeled instances, kernel methods determine the label of a novel instance by comparing it to the labeled training instances using this kernel function. Nearest neighbor classification and support-vector machines (SVMs) are two popular examples of kernel methods (Fukunaga, 1990; Cortes and Vapnik, 1995). An advantage of kernel methods is that they can search a feature space much larger than could be represented by a feature extraction-based approach. This is possible because the kernel function can explore an implicit feature space when calculating the similarity between two instances, as described in the Section 3. Working in such a large feature space can lead to over-fitting in many machine learning algorithms. To address this problem, we apply SVMs to the task of relation extraction. SVMs find a boundary between instances of different classes such that the distance between the boundary a</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine Learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
</authors>
<title>An introduction to support vector machines.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4985" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="757" endWordPosition="760">hoose this representation because we hypothesize that instances containing similar relations will share similar substructures in their dependency trees. The task of the kernel function is to find these similarities. We define a tree kernel over dependency trees and incorporate this kernel within an SVM to extract relations from newswire documents. The tree kernel approach consistently outperforms the bag-ofwords kernel, suggesting that this highly-structured representation of sentences is more informative for detecting and distinguishing relations. 2 Related Work Kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) have become increasingly popular because of their ability to map arbitrary objects to a Euclidian feature space. Haussler (1999) describes a framework for calculating kernels over discrete structures such as strings and trees. String kernels for text classification are explored in Lodhi et al. (2000), and tree kernel variants are described in (Zelenko et al., 2003; Collins and Duffy, 2002; Cumby and Roth, 2003). Our algorithm is similar to that described by Zelenko et al. (2003). Our contributions are a richer sentence representation, a more general framework to allow feature weighting, as we</context>
<context position="10027" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="1582" endWordPosition="1585">ture-based method. Given a training set S = {xs ... xN}, kernel methods compute the Gram matrix G such that Gij = K(xi,xj). Given G, the classifier finds a hyperplane which separates instances of different classes. To classify an unseen instance x, the classifier first projects x into the feature space defined by the kernel function. Classification then consists of determining on which side of the separating hyperplane x lies. A support vector machine (SVM) is a type of classifier that formulates the task of finding the separating hyperplane as the solution to a quadratic programming problem (Cristianini and Shawe-Taylor, 2000). Support vector machines attempt to find a hyperplane that not only separates the classes but also maximizes the margin between them. The hope is that this will lead to better generalization performance on unseen instances. 4 Augmented Dependency Trees Our task is to detect and classify relations between entities in text. We assume that entity tagging has been performed; so to generate potential relation instances, we iterate over all pairs of entities occurring in the same sentence. For each entity pair, we create an augmented dependency tree (described below) representing this instance. Giv</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>N. Cristianini and J. Shawe-Taylor. 2000. An introduction to support vector machines. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chad M Cumby</author>
<author>Dan Roth</author>
</authors>
<title>On kernel methods for relational learning.</title>
<date>2003</date>
<booktitle>Machine Learning, Proceedings of the Twentieth International Conference (ICML</booktitle>
<editor>In Tom Fawcett and Nina Mishra, editors,</editor>
<publisher>AAAI Press.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="5400" citStr="Cumby and Roth, 2003" startWordPosition="822" endWordPosition="825">sting that this highly-structured representation of sentences is more informative for detecting and distinguishing relations. 2 Related Work Kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) have become increasingly popular because of their ability to map arbitrary objects to a Euclidian feature space. Haussler (1999) describes a framework for calculating kernels over discrete structures such as strings and trees. String kernels for text classification are explored in Lodhi et al. (2000), and tree kernel variants are described in (Zelenko et al., 2003; Collins and Duffy, 2002; Cumby and Roth, 2003). Our algorithm is similar to that described by Zelenko et al. (2003). Our contributions are a richer sentence representation, a more general framework to allow feature weighting, as well as the use of composite kernels to reduce kernel sparsity. Brin (1998) and Agichtein and Gravano (2000) apply pattern matching and wrapper techniques for relation extraction, but these approaches do not scale well to fastly evolving corpora. Miller et al. (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types. Whereas Miller</context>
</contexts>
<marker>Cumby, Roth, 2003</marker>
<rawString>Chad M. Cumby and Dan Roth. 2003. On kernel methods for relational learning. In Tom Fawcett and Nina Mishra, editors, Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003), August 21-24, 2003, Washington, DC, USA. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Fukunaga</author>
</authors>
<title>Introduction to Statistical Pattern Recognition.</title>
<date>1990</date>
<publisher>Academic Press,</publisher>
<note>second edition.</note>
<contexts>
<context position="2887" citStr="Fukunaga, 1990" startWordPosition="450" endWordPosition="452">of extracted fields sula et al., 2002; McCallum and Wellner, 2003). We describe a relation extraction technique based on kernel methods. Kernel methods are nonparametric density estimation techniques that compute a kernel function between data instances, where a kernel function can be thought of as a similarity measure. Given a set of labeled instances, kernel methods determine the label of a novel instance by comparing it to the labeled training instances using this kernel function. Nearest neighbor classification and support-vector machines (SVMs) are two popular examples of kernel methods (Fukunaga, 1990; Cortes and Vapnik, 1995). An advantage of kernel methods is that they can search a feature space much larger than could be represented by a feature extraction-based approach. This is possible because the kernel function can explore an implicit feature space when calculating the similarity between two instances, as described in the Section 3. Working in such a large feature space can lead to over-fitting in many machine learning algorithms. To address this problem, we apply SVMs to the task of relation extraction. SVMs find a boundary between instances of different classes such that the dista</context>
</contexts>
<marker>Fukunaga, 1990</marker>
<rawString>K. Fukunaga. 1990. Introduction to Statistical Pattern Recognition. Academic Press, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical Report UCS-CRL-99-10,</tech>
<institution>University of California,</institution>
<location>Santa Cruz.</location>
<contexts>
<context position="5114" citStr="Haussler (1999)" startWordPosition="779" endWordPosition="780">es. The task of the kernel function is to find these similarities. We define a tree kernel over dependency trees and incorporate this kernel within an SVM to extract relations from newswire documents. The tree kernel approach consistently outperforms the bag-ofwords kernel, suggesting that this highly-structured representation of sentences is more informative for detecting and distinguishing relations. 2 Related Work Kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) have become increasingly popular because of their ability to map arbitrary objects to a Euclidian feature space. Haussler (1999) describes a framework for calculating kernels over discrete structures such as strings and trees. String kernels for text classification are explored in Lodhi et al. (2000), and tree kernel variants are described in (Zelenko et al., 2003; Collins and Duffy, 2002; Cumby and Roth, 2003). Our algorithm is similar to that described by Zelenko et al. (2003). Our contributions are a richer sentence representation, a more general framework to allow feature weighting, as well as the use of composite kernels to reduce kernel sparsity. Brin (1998) and Agichtein and Gravano (2000) apply pattern matching</context>
<context position="8127" citStr="Haussler, 1999" startWordPosition="1273" endWordPosition="1274">es that would be too costly to explicitly include as features. Formally, a kernel function K is a mapping K : X x X —* [0, oc] from instance space X to a similarity score K(x, y) = Ei Oi(x)Oi(y) = O(x) · O(y). Here, Oi(x) is some feature function over the instance x. The kernel function must be symmetric [K(x, y) = K(y, x)] and positivesemidefinite. By positive-semidefinite, we require that the if x1, ... , xn E X, then the n x n matrix G defined by Gij = K(xi, xj) is positive semidefinite. It has been shown that any function that takes the dot product of feature vectors is a kernel function (Haussler, 1999). A simple kernel function takes the dot product of the vector representation of instances being compared. For example, in document classification, each document can be represented by a binary vector, where each element corresponds to the presence or absence of a particular word in that document. Here, Oi(x) = 1 if word i occurs in document x. Thus, the kernel function K(x, y) returns the number of words in common between x and y. We refer to this kernel as the “bag-of-words” kernel, since it ignores word order. When instances are more structured, as in the case of dependency trees, more compl</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>D. Haussler. 1999. Convolution kernels on discrete structures. Technical Report UCS-CRL-99-10, University of California, Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Composite kernels for hypertext categorisation.</title>
<date>2001</date>
<booktitle>Proceedings of ICML01, 18th International Conference on Machine Learning,</booktitle>
<pages>250--257</pages>
<editor>In Carla Brodley and Andrea Danyluk, editors,</editor>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>Williams College, US.</location>
<contexts>
<context position="17446" citStr="Joachims et al. (2001)" startWordPosition="2856" endWordPosition="2859">d broadcasts. Five entities have been annotated (PERSON, ORGANIZATION, GEO-POLITICAL ENTITY, LOCATION, FACILITY), along with 24 types of relations (Table 2). As noted from the distribution of relationship types in the training data (Figure 3), data imbalance and sparsity are potential problems. In addition to the contiguous and sparse tree kernels, we also implement a bag-of-words kernel, which treats the tree as a vector of features over nodes, disregarding any structural information. We also create composite kernels by combining the sparse and contiguous kernels with the bagof-words kernel. Joachims et al. (2001) have shown that given two kernels K1, K2, the composite kernel K12(xi, xj) = K1(xi, xj)+K2(xi, xj) is also a kernel. We find that this composite kernel improves performance when the Gram matrix G is sparse (i.e. our instances are far apart in the kernel space). The features used to represent each node are shown in Table 3. After initial experimentation, the set of features we use in the matching function is φm(ti) = {general-pos, entity-type, relationargument}, and the similarity function examines the Figure 3: Distribution over relation types in training data. remaining features. In our expe</context>
</contexts>
<marker>Joachims, Cristianini, Shawe-Taylor, 2001</marker>
<rawString>Thorsten Joachims, Nello Cristianini, and John Shawe-Taylor. 2001. Composite kernels for hypertext categorisation. In Carla Brodley and Andrea Danyluk, editors, Proceedings of ICML01, 18th International Conference on Machine Learning, pages 250–257, Williams College, US. Morgan Kaufmann Publishers, San Francisco, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Christopher J C H Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2000</date>
<booktitle>In NIPS,</booktitle>
<pages>563--569</pages>
<contexts>
<context position="5287" citStr="Lodhi et al. (2000)" startWordPosition="803" endWordPosition="806">ations from newswire documents. The tree kernel approach consistently outperforms the bag-ofwords kernel, suggesting that this highly-structured representation of sentences is more informative for detecting and distinguishing relations. 2 Related Work Kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) have become increasingly popular because of their ability to map arbitrary objects to a Euclidian feature space. Haussler (1999) describes a framework for calculating kernels over discrete structures such as strings and trees. String kernels for text classification are explored in Lodhi et al. (2000), and tree kernel variants are described in (Zelenko et al., 2003; Collins and Duffy, 2002; Cumby and Roth, 2003). Our algorithm is similar to that described by Zelenko et al. (2003). Our contributions are a richer sentence representation, a more general framework to allow feature weighting, as well as the use of composite kernels to reduce kernel sparsity. Brin (1998) and Agichtein and Gravano (2000) apply pattern matching and wrapper techniques for relation extraction, but these approaches do not scale well to fastly evolving corpora. Miller et al. (2000) propose an integrated statistical pa</context>
<context position="9033" citStr="Lodhi et al., 2000" startWordPosition="1420" endWordPosition="1423"> document. Here, Oi(x) = 1 if word i occurs in document x. Thus, the kernel function K(x, y) returns the number of words in common between x and y. We refer to this kernel as the “bag-of-words” kernel, since it ignores word order. When instances are more structured, as in the case of dependency trees, more complex kernels become necessary. Haussler (1999) describes convolution kernels, which find the similarity between two structures by summing the similarity of their substructures. As an example, consider a kernel over strings. To determine the similarity between two strings, string kernels (Lodhi et al., 2000) count the number of common subsequences in the two strings, and weight these matches by their length. Thus, Oi(x) is the number of times string x contains the subsequence referenced by i. These matches can be found efficiently through a dynamic program, allowing string kernels to examine long-range features that would be computationally infeasible in a feature-based method. Given a training set S = {xs ... xN}, kernel methods compute the Gram matrix G such that Gij = K(xi,xj). Given G, the classifier finds a hyperplane which separates instances of different classes. To classify an unseen inst</context>
</contexts>
<marker>Lodhi, Shawe-Taylor, Cristianini, Watkins, 2000</marker>
<rawString>Huma Lodhi, John Shawe-Taylor, Nello Cristianini, and Christopher J. C. H. Watkins. 2000. Text classification using string kernels. In NIPS, pages 563–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Toward conditional models of identity uncertainty with application to proper noun coreference.</title>
<date>2003</date>
<booktitle>In IJCAI Workshop on Information Integration on the Web.</booktitle>
<contexts>
<context position="2339" citStr="McCallum and Wellner, 2003" startWordPosition="363" endWordPosition="366">ntity tagger detects all mentions of interest; second, coreference resolution resolves disparate mentions of the same entity; third, a relation extractor finds relations between these entities. Entity tagging has been thoroughly addressed by many statistical machine learning techniques, obtaining greater than 90% F1 on many datasets (Tjong Kim Sang and De Meulder, 2003). Coreference resolution is an active area of research not investigated here (PaEntity Type Location Apple Organization Cupertino, CA Microsoft Organization Redmond, WA Table 1: An example of extracted fields sula et al., 2002; McCallum and Wellner, 2003). We describe a relation extraction technique based on kernel methods. Kernel methods are nonparametric density estimation techniques that compute a kernel function between data instances, where a kernel function can be thought of as a similarity measure. Given a set of labeled instances, kernel methods determine the label of a novel instance by comparing it to the labeled training instances using this kernel function. Nearest neighbor classification and support-vector machines (SVMs) are two popular examples of kernel methods (Fukunaga, 1990; Cortes and Vapnik, 1995). An advantage of kernel m</context>
</contexts>
<marker>McCallum, Wellner, 2003</marker>
<rawString>A. McCallum and B. Wellner. 2003. Toward conditional models of identity uncertainty with application to proper noun coreference. In IJCAI Workshop on Information Integration on the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>H Fox</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>A novel use of statistical parsing to extract information from text.</title>
<date>2000</date>
<booktitle>In 6th Applied Natural Language Processing Conference.</booktitle>
<contexts>
<context position="5850" citStr="Miller et al. (2000)" startWordPosition="892" endWordPosition="895">r text classification are explored in Lodhi et al. (2000), and tree kernel variants are described in (Zelenko et al., 2003; Collins and Duffy, 2002; Cumby and Roth, 2003). Our algorithm is similar to that described by Zelenko et al. (2003). Our contributions are a richer sentence representation, a more general framework to allow feature weighting, as well as the use of composite kernels to reduce kernel sparsity. Brin (1998) and Agichtein and Gravano (2000) apply pattern matching and wrapper techniques for relation extraction, but these approaches do not scale well to fastly evolving corpora. Miller et al. (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types. Whereas Miller et al. (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance. Also, Roth and Yih (2002) learn a Bayesian network to tag entities and their relations simultaneously. We experiment with a more challenging set of relation types and a larger corpus. 3 Kernel Methods In traditional machine learning, we a</context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 2000</marker>
<rawString>S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 2000. A novel use of statistical parsing to extract information from text. In 6th Applied Natural Language Processing Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Pasula</author>
<author>B Marthi</author>
<author>B Milch</author>
<author>S Russell</author>
<author>I Shpitser</author>
</authors>
<title>Identity uncertainty and citation matching.</title>
<date>2002</date>
<marker>Pasula, Marthi, Milch, Russell, Shpitser, 2002</marker>
<rawString>H. Pasula, B. Marthi, B. Milch, S. Russell, and I. Shpitser. 2002. Identity uncertainty and citation matching.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>Probabilistic reasoning for entity and relation recognition.</title>
<date>2002</date>
<booktitle>In 19th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="6237" citStr="Roth and Yih (2002)" startWordPosition="953" endWordPosition="956">s to reduce kernel sparsity. Brin (1998) and Agichtein and Gravano (2000) apply pattern matching and wrapper techniques for relation extraction, but these approaches do not scale well to fastly evolving corpora. Miller et al. (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types. Whereas Miller et al. (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance. Also, Roth and Yih (2002) learn a Bayesian network to tag entities and their relations simultaneously. We experiment with a more challenging set of relation types and a larger corpus. 3 Kernel Methods In traditional machine learning, we are provided a set of training instances S = {x1 ... xN}, where each instance xi is represented by some ddimensional feature vector. Much time is spent on the task of feature engineering – searching for the optimal feature set either manually by consulting domain experts or automatically through feature induction and selection (Scott and Matwin, 1999). For example, in entity detection </context>
</contexts>
<marker>Roth, Yih, 2002</marker>
<rawString>Dan Roth and Wen-tau Yih. 2002. Probabilistic reasoning for entity and relation recognition. In 19th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam Scott</author>
<author>Stan Matwin</author>
</authors>
<title>Feature engineering for text classification.</title>
<date>1999</date>
<booktitle>In Proceedings of ICML-99, 16th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="6802" citStr="Scott and Matwin, 1999" startWordPosition="1046" endWordPosition="1049">will achieve better performance. Also, Roth and Yih (2002) learn a Bayesian network to tag entities and their relations simultaneously. We experiment with a more challenging set of relation types and a larger corpus. 3 Kernel Methods In traditional machine learning, we are provided a set of training instances S = {x1 ... xN}, where each instance xi is represented by some ddimensional feature vector. Much time is spent on the task of feature engineering – searching for the optimal feature set either manually by consulting domain experts or automatically through feature induction and selection (Scott and Matwin, 1999). For example, in entity detection the original instance representation is generally a word vector corresponding to a sentence. Feature extraction and induction may result in features such as part-ofspeech, word n-grams, character n-grams, capitalization, and conjunctions of these features. In the case of more structured objects, such as parse trees, features may include some description of the object’s structure, such as “has an NP-VP subtree.” Kernel methods can be particularly effective at reducing the feature engineering burden for structured objects. By calculating the similarity between </context>
</contexts>
<marker>Scott, Matwin, 1999</marker>
<rawString>Sam Scott and Stan Matwin. 1999. Feature engineering for text classification. In Proceedings of ICML-99, 16th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Walter Daelemans</booktitle>
<pages>142--147</pages>
<editor>and Miles Osborne, editors,</editor>
<location>Edmonton, Canada.</location>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL-2003, pages 142– 147. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Whiley,</publisher>
<location>Chichester, GB.</location>
<contexts>
<context position="4948" citStr="Vapnik, 1998" startWordPosition="755" endWordPosition="756">f speech) We choose this representation because we hypothesize that instances containing similar relations will share similar substructures in their dependency trees. The task of the kernel function is to find these similarities. We define a tree kernel over dependency trees and incorporate this kernel within an SVM to extract relations from newswire documents. The tree kernel approach consistently outperforms the bag-ofwords kernel, suggesting that this highly-structured representation of sentences is more informative for detecting and distinguishing relations. 2 Related Work Kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) have become increasingly popular because of their ability to map arbitrary objects to a Euclidian feature space. Haussler (1999) describes a framework for calculating kernels over discrete structures such as strings and trees. String kernels for text classification are explored in Lodhi et al. (2000), and tree kernel variants are described in (Zelenko et al., 2003; Collins and Duffy, 2002; Cumby and Roth, 2003). Our algorithm is similar to that described by Zelenko et al. (2003). Our contributions are a richer sentence representation, a more general framew</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir Vapnik. 1998. Statistical Learning Theory. Whiley, Chichester, GB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>A Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>1083--1106</pages>
<contexts>
<context position="5352" citStr="Zelenko et al., 2003" startWordPosition="814" endWordPosition="817">ently outperforms the bag-ofwords kernel, suggesting that this highly-structured representation of sentences is more informative for detecting and distinguishing relations. 2 Related Work Kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) have become increasingly popular because of their ability to map arbitrary objects to a Euclidian feature space. Haussler (1999) describes a framework for calculating kernels over discrete structures such as strings and trees. String kernels for text classification are explored in Lodhi et al. (2000), and tree kernel variants are described in (Zelenko et al., 2003; Collins and Duffy, 2002; Cumby and Roth, 2003). Our algorithm is similar to that described by Zelenko et al. (2003). Our contributions are a richer sentence representation, a more general framework to allow feature weighting, as well as the use of composite kernels to reduce kernel sparsity. Brin (1998) and Agichtein and Gravano (2000) apply pattern matching and wrapper techniques for relation extraction, but these approaches do not scale well to fastly evolving corpora. Miller et al. (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels de</context>
<context position="13336" citStr="Zelenko et al. (2003)" startWordPosition="2152" endWordPosition="2155">d}. We refer to the jth child of node ti as ti[j], and we denote the set of all children of node ti as ti[c]. We reference a subset j of children of ti by ti[j] C_ ti[c]. Finally, we refer to the parent of node ti as ti.p. From the example in Figure 1, t0[1] = t2, t0[I0,1}] = It1, t2}, and t1.p = t0. 5 Tree kernels for dependency trees We now define a kernel function for dependency trees. The tree kernel is a function K(T1, T2) that returns a normalized, symmetric similarity score in the range (0, 1) for two trees T1 and T2. We define a slightly more general version of the kernel described by Zelenko et al. (2003). We first define two functions over the features of tree nodes: a matching function m(ti, tj) E I0, 1} and a similarity function s(ti, tj) E (0, oc]. Let the feature vector 0(ti) = Iv1 ... vd} consist of two possibly overlapping subsets 0m(ti) C_ 0(ti) and 0s(ti) C_ 0(ti). We use 0m(ti) in the matching function and 0s(ti) in the similarity function. We define and s(ti, tj) = X X C(vq, vr) vq∈φ.(ti) v,∈φ.(tj) where C(vq, vr) is some compatibility function between two feature values. For example, in the simplest case where s(ti, tj) returns the number of feature values in common between feature</context>
<context position="14941" citStr="Zelenko et al. (2003)" startWordPosition="2450" endWordPosition="2453">ce of matching subtrees, as shown below. For two dependency trees T1, T2, with root nodes r1 and r2, we define the tree kernel K(T1, T2) as 0 if m(r1, r2) = 0 s(r1, r2)+ Kc(r1[c], r2[c]) otherwise where Kc is a kernel function over children. Let a and b be sequences of indices such that a is a sequence a1 &lt; a2 &lt; ... &lt; an, and likewise for b. Let d(a) = an − a1 + 1 and l(a) be the length of a. Then we have Kc(ti[c], tj[c]) = X Ad(a)Ad(b)K(ti[a], tj[b]) a,b,l(a)=l(b) The constant 0 &lt; A &lt; 1 is a decay factor that penalizes matching subsequences that are spread out within the child sequences. See Zelenko et al. (2003) for a proof that K is kernel function. Intuitively, whenever we find a pair of matching nodes, we search for all matching subsequences of the children of each node. A matching subsequence of children is a sequence of children a and b such that m(ai, bi) = 1 (bi &lt; n). For each matching pair of nodes (ai, bi) in a matching subsequence, we accumulate the result of the similarity function s(ai, bj) and then recursively search for matching subsequences of their children ai[c], bj[c]. We implement two types of tree kernels. A contiguous kernel only matches children subsequences that are uninterrupt</context>
<context position="16250" citStr="Zelenko et al. (2003)" startWordPosition="2658" endWordPosition="2661">ows non-matching nodes within matching subsequences. Figure 2 shows two relation instances, where each node contains the original text plus the features used for the matching function, 0m(ti) = Igeneralpos, entity-type, relation-argument}. (“NA” denotes the feature is not present for this node.) The contiguous kernel matches the following substructures: It0[0], u0[0]}, It0[2], u0[1]}, It3[0], u2[0]}. Because the sparse kernel allows non-contiguous matching sequences, it matches an additional substructure It0[0, *, 2], u0[0, *,1]}, where (*) indicates an arbitrary number of non-matching nodes. Zelenko et al. (2003) have shown the contiguous kernel to be computable in O(mn) and the sparse kernel in O(mn3), where m and n are the number of children in trees T1 and T2 respectively. 6 Experiments We extract relations from the Automatic Content Extraction (ACE) corpus provided by the National Institute for Standards and Technology (NIST). The m(ti, tj) = (0 otherwise 1 if 0m(ti) = 0m(tj) ( 1 if vq = v C(vq, vr) r 0 otherwise follows: K(T1,T2) = ⎧ ⎨⎪ ⎪⎩ Figure 2: Two instances of the NEAR relation. data consists of about 800 annotated text documents gathered from various newspapers and broadcasts. Five entitie</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel methods for relation extraction. Journal of Machine Learning Research, pages 1083– 1106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>