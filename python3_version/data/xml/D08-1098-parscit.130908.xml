<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000042">
<title confidence="0.996457">
CoCQA: Co-Training Over Questions and Answers
with an Application to Predicting Question Subjectivity Orientation
</title>
<author confidence="0.998727">
Baoli Li Yandong Liu Eugene Agichtein
</author>
<affiliation confidence="0.999703">
Emory University Emory University Emory University
</affiliation>
<email confidence="0.998982">
csblli@gmail.com yliu49@emory.edu eugene@mathcs.emory.edu
</email>
<sectionHeader confidence="0.99739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954548387097">
An increasingly popular method for
finding information online is via the
Community Question Answering
(CQA) portals such as Yahoo! An-
swers, Naver, and Baidu Knows.
Searching the CQA archives, and rank-
ing, filtering, and evaluating the sub-
mitted answers requires intelligent
processing of the questions and an-
swers posed by the users. One impor-
tant task is automatically detecting the
question’s subjectivity orientation:
namely, whether a user is searching for
subjective or objective information.
Unfortunately, real user questions are
often vague, ill-posed, poorly stated.
Furthermore, there has been little la-
beled training data available for real
user questions. To address these prob-
lems, we present CoCQA, a co-training
system that exploits the association be-
tween the questions and contributed
answers for question analysis tasks.
The co-training approach allows
CoCQA to use the effectively unlim-
ited amounts of unlabeled data readily
available in CQA archives. In this pa-
per we study the effectiveness of
CoCQA for the question subjectivity
classification task by experimenting
over thousands of real users’ questions.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946642857143">
Automatic question answering (QA) has been
one of the long-standing goals of natural lan-
guage processing, information retrieval, and
artificial intelligence research. For a natural
language question we would like to respond
with a specific, accurate, and complete an-
swer that addresses the question. Although
much progress has been made, answering
complex, opinion, and even many factual
questions automatically is still beyond the
current state-of-the-art. At the same time, the
rise of popularity in social media and collabo-
rative content creation services provides a
promising alternative to web search or com-
pletely automated QA. The explicit support
for social interactions between participants,
such as posting comments, rating content, and
responding to questions and comments makes
this medium particularly amenable to Ques-
tion Answering. Some very successful exam-
ples of Community Question Answering
(CQA) sites are Yahoo! Answers 1 and
Naver2, and Baidu Knows3. Yahoo! Answers
alone has already amassed hundreds of mil-
lions of answers posted by millions of par-
ticipants on thousands of topics.
The questions posted to such CQA portals
are typically complex, subjective, and rely on
human interpretation to understand the corre-
sponding information need. At the same time,
the questions are also usually ill-phrased,
vague, and often subjective in nature. Hence,
analysis of the questions (and of the corre-
sponding user intent) in this setting is a par-
ticularly difficult task. At the same time,
CQA content incorporates the relationships
between questions and the corresponding an-
swers. Because of the various incentives pro-
vided by the CQA sites, answers posted by
users tend to be, at least to some degree, re-
sponsive to the question. This observation
suggests investigating whether the relation-
</bodyText>
<footnote confidence="0.999172666666667">
1 http://answers.yahoo.com
2 http://www.naver.com
3 http://www.baidu.com
</footnote>
<page confidence="0.871996">
937
</page>
<note confidence="0.9626575">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 937–946,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.997174390243902">
ship between questions and answers can be
exploited to improve automated analysis of the
CQA content and the user intent behind the
questions posted.
To this end, we exploit the ideas of co-
training, a general semi-supervised learning
approach naturally applicable to cases of com-
plementary views on a domain, for example,
web page links and content (Blum and
Mitchell, 1998). In our setting, we focus on the
complimentary views for a question, namely
the text of the question and the text of the as-
sociated answers.
As a concrete case-study of our approach
we focus on one particularly important aspect
of intent detection: the subjectivity orientation.
We attempt to predict whether a question
posted in a CQA site is subjective or objective.
Objective questions are expected to be an-
swered with reliable or authoritative informa-
tion, typically published online and possibly
referenced as part of the answer, whereas sub-
jective questions seek answers containing pri-
vate states, e.g. personal opinions, judgment,
experiences. If we could automatically predict
the orientation of a question, we would be able
to better rank or filter the answers, improve
search over the archives, and more accurately
identify similar questions. For example, if a
question is objective, we could try to find a
few highly relevant articles as references,
whereas if a question is subjective, useful an-
swers are not expected to be found in authori-
tative sources and tend to rank low with cur-
rent question answering and CQA search tech-
niques. Finally, learning how to identify ques-
tion orientation is a crucial component of in-
ferring user intent, a long-standing problem in
web information access settings.
In particular, we focus on the following re-
search questions:
</bodyText>
<listItem confidence="0.998633">
• Can we utilize the inherent structure of the
CQA interactions and use the unlimited
amounts of unlabeled data to improve classi-
fication performance, and/or reduce the
amount of manual labeling required?
• Can we automatically predict question sub-
jectivity in Community Question Answering
– and which features are useful for this task
in the real CQA setting?
</listItem>
<figureCaption confidence="0.998384">
Figure 1: Example question (Yahoo! Answers)
</figureCaption>
<bodyText confidence="0.999864857142857">
The rest of the paper is structured as fol-
lows. We first overview the community ques-
tion answering setting, and state the question
orientation classification problem, which we
use as the motivating application for our sys-
tem, more precisely. We then introduce our
CoCQA system for semi-supervised classifi-
cation of questions and answers in CQA com-
munities (Section 3). We report the results of
our experiments over thousands of real user
questions in Section 4, showing the effective-
ness of our approach. Finally, we review re-
lated work in Section 5, and discuss our con-
clusions and future work in Section 6.
</bodyText>
<sectionHeader confidence="0.87399" genericHeader="method">
2 Question Orientation in CQA
</sectionHeader>
<bodyText confidence="0.999811">
We first briefly describe the essential features
of question answering communities such as
Yahoo! Answers or Naver. Then, we formally
state the problem addressed in this paper, and
the features used for this setting.
</bodyText>
<page confidence="0.996129">
938
</page>
<subsectionHeader confidence="0.982208">
2.1 Community Question Answering
</subsectionHeader>
<bodyText confidence="0.997085823529412">
Online social media content and associated
services comprise one of the fastest growing
segments on the Web. The explicit support for
social interactions between participants, such
as posting comments, rating content, and re-
sponding to questions and comments makes
the social media unique. Question answering
has been particularly amenable to social media
by directly connecting information seekers
with the community members willing to share
the information. Yahoo! Answers, with mil-
lions of users and hundreds of millions of an-
swers for millions of questions is a very suc-
cessful implementation of CQA.
For example, consider two example user-
contributed questions, objective and subjective
respectively:
</bodyText>
<listItem confidence="0.996403285714286">
Q1: What’s the difference between
chemotherapy and radiation treat-
ments?
Q2: Has anyone got one of those
home blood pressure monitors? and
if so what make is it and do you
think they are worth getting?
</listItem>
<bodyText confidence="0.9995275">
Figure 1 shows an example of community
interactions in Yahoo! Answers around the
question Q2 above. A user posted the question
in the Health category of the site, and was able
to obtain 10 responses from other users. Even-
tually, the asker chooses the best answer. Fail-
ing that, as shown in the example, the best an-
swer can also be chosen according to the votes
from other users. Many of the interactions de-
pend on the perceived goals of the asker: if the
participants interpret the question as subjec-
tive, they will tend to share their experiences
and opinions, and if they interpret the question
as objective, they may still share their experi-
ences but may also provide more factual in-
formation.
</bodyText>
<subsectionHeader confidence="0.998315">
2.2 Problem Definition
</subsectionHeader>
<bodyText confidence="0.999925111111111">
We now state our problem of question orienta-
tion more precisely. We consider question ori-
entation from the perspective of user goals:
authors of objective questions request authori-
tative, objective information (e.g., published
literature or expert opinion), whereas authors
of subjective questions seek opinions or judg-
ments of other users in the community. We
state our problem as follows.
</bodyText>
<subsectionHeader confidence="0.623817">
Question Subjectivity Problem: Given a
</subsectionHeader>
<construct confidence="0.991158">
question Q in a question answering com-
munity, predict whether Q has objective
or subjective orientation, based on ques-
tion and answer text as well as the user
and community feedback.
</construct>
<sectionHeader confidence="0.912521" genericHeader="method">
3 CoCQA: A Co-Training Frame-
</sectionHeader>
<subsectionHeader confidence="0.752502">
work over Questions and Answers
</subsectionHeader>
<bodyText confidence="0.9965713">
In the CQA setting we could easily obtain
thousands or millions of unlabeled examples
from the online CQA archives. On the other
hand, it is difficult to create a labeled dataset
with a reasonable size, which could be used
to train a perfect classifier to analyze ques-
tions from different domains and sub-
domains. Therefore, semi-supervised learning
(Chapelle et al., 2006) is a natural approach
for this setting.
Intuitively, we can consider the text of the
question itself or answers to it. In other
words, we have multiple (at least two) natural
views of the data, which satisfies the condi-
tions of the co-training approach (Blum and
Mitchell, 1998). In co-training, two separate
classifiers are trained on two sets of features,
respectively. By automatically labeling the
unlabeled examples, these two classifiers it-
eratively “teach” each other by giving their
partners a newly labeled data that they can
predict with high confidence. Based on the
original co-training algorithm in (Blum and
Mitchell, 1998) and other implementations,
we develop our algorithm CoCQA shown in
Figure 2.
At Steps 1 and 2, the K examples are com-
ing from different feature spaces, and each
category (for example, Subjective and Objec-
tive) has top Kj most confident examples cho-
sen, where Kj corresponds to the distribution
of class in the current set of labeled examples
L. CoCQA will terminate when the incre-
ments of both classifiers are less than a speci-
fied threshold X or the maximum number of
iterations are exceeded. Following the co-
training approach, we include the most confi-
dently predicted examples as additional “la-
beled” data. The SVM output margin value
was used to estimate confidence; alternative
</bodyText>
<page confidence="0.9926">
939
</page>
<bodyText confidence="0.811405">
Input:
</bodyText>
<listItem confidence="0.997678923076923">
• FQ and FA are Question and Answer feature views
• CQ and CA are classifiers trained on FQ and FA respec-
tively
• L is a set of labeled training examples
• U is a set of unlabeled examples
• K: Number of unlabeled examples to choose on
each iteration
• X: the threshold for increment
• R: the maximal number of iterations
Algorithm CoCQA
1. Train CQ ,0 on L: FQ , and record resulting ACCQ,0
2. Train CA ,0 on L: FA , and record resulting ACCA ,0
3. for j=1 to R do:
</listItem>
<figure confidence="0.886871222222222">
Use CQ,j-1 to predict labels for U and choose
top K items with highest confidence 4 EQ, , j-1
Use CA,j-1 to predict labels for U and choose
top K items with highest confidence 4 EA, , j-1
Move examples EQ, , j-1 U EA, , j-1 4 L
Train CQ,j on L: FQ and record training ACCQ,j
Train CA,j on L: FA and record training ACCA,j
if Max(∆ACCQ,j, ∆ ACCA,j) &lt; X break
4. return final classifiers CQ,j 4 CQ and CA,j 4 CA
</figure>
<figureCaption confidence="0.955803333333333">
Figure 2: Algorithm CoCQA: A co-training algo-
rithm for exploiting redundant feature sets in
community question answering.
</figureCaption>
<bodyText confidence="0.964462333333334">
methods (including reliability of this confi-
dence prediction) could further improve per-
formance, and we will explore these issues in
future work. Finally, the next question is how
to estimate classification performance with
training data. For each pass, we randomly split
the original training data into N folds (N=10 in
our experiments), and keep one part for valida-
tion and the rest, augmented with the newly
added examples, as the expanded training set.
After CoCQA terminates, we obtain two
classifiers. When a new example arrives, we
will classify it with these two classifiers based
on both of the feature sets, and combine the
predictions of these two classifiers. We ex-
plored two strategies to make the final deci-
sion based on the confidence values given by
two classifiers:
■ Choose the class with higher confidence
■ Multiply the confidence values, and
choose the class that has the highest
product.
We found the second heuristic to be more
effective than the first in our experiments. As
the base classifier we use SVM in the current
implementation, but other classifiers could be
incorporated as well.
</bodyText>
<sectionHeader confidence="0.996086" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999748">
We experiment with supervised and semi-
supervised methods on a relatively large data
set from Yahoo! Answers.
</bodyText>
<subsectionHeader confidence="0.974781">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999902658536585">
To our knowledge, there is no standard data-
set of real questions and answers posted by
online users, labeled for subjectivity orienta-
tion. Hence, we had to create a dataset our-
selves. To create our dataset, we downloaded
more than 30,000 resolved questions from
each of the following top-level categories of
Yahoo! Answers: Arts, Education, Health,
Science, and Sports. We randomly chose 200
questions from each category to create a raw
dataset with 1,000 questions total. Then, we
labeled the dataset with annotators from the
Amazon’s Mechanical Turk service4.
For annotation, each question was judged
by 5 Mechanical Turk workers who passed a
qualification test of 10 questions (labeled by
ourselves) with at least 9 of them correctly
marked. The qualification test was required to
ensure that the raters were sufficiently com-
petent to make reasonable judgments. We
grouped the tasks into 25 question batches,
where the whole batch was submitted as the
Mechanical Turk’s Human Intelligence Task
(HIT). The batching of questions was done to
easily detect the “random” ratings produced
by irresponsible workers. That is, each
worker rated a batch of 25 questions.
While precise definition of subjectivity is
elusive, we decided to take the practical per-
spective, namely the &amp;quot;majority&amp;quot; interpreta-
tion. The annotators were instructed to guess
orientation according to how the question
would be answered by most people. We did
not deal with multi-part questions: if any part
of question was subjective, the whole ques-
tion was labeled as subjective. The gold stan-
dard was thus derived with the majority strat-
egy, followed by manual inspection as a “san-
ity check”. At this stage we removed 22 ques-
tions with undeterminable meaning, including
gems such as “Upward Soccer
</bodyText>
<footnote confidence="0.964801">
4 http://www.mturk.com
</footnote>
<page confidence="0.993191">
940
</page>
<bodyText confidence="0.983369666666667">
Shorts?”5 and “1+1=?fdgdgdfg?”6. Fi-
nally, we create a labeled dataset consisting of
978 resolved questions, available online7.
</bodyText>
<table confidence="0.998760375">
Num. of Num. of Total Annotator
SUB. Q OBJ. Q Num. agreement
Arts 137 (70%) 58 (30%) 195 0.841
Education 127 (64%) 70 (36%) 197 0.716
Health 125 (64%) 69 (36%) 194 0.833
Science 103 (52%) 94 (48%) 197 0.618
Sports 154 (79%) 41 (21%) 195 0.877
Total 646 (66%) 332 (34%) 978 0.777
</table>
<tableCaption confidence="0.999784">
Table 1: Labeled dataset statistics.
</tableCaption>
<bodyText confidence="0.98501">
Table 1 reports the statistics of the annotated
dataset. The overall inter-annotator percentage
agreement between Mechanical Turk workers
and final annotation is 0.777, indicating that
the task is difficult, but feasible for humans to
annotate manually.
The statistics of our labeled sample show
that the vast majority of the questions in all
categories except for Science are subjective in
nature. The relatively high ratio of subjective
questions in the Science category is surprising.
However, we find that users often post polem-
ics and statements instead of questions, using
CQA as a forum to share their opinions on
controversial topics. Overall, we were struck
by the expressed need in Subjective informa-
tion, even for categories such as Health and
Education, where objective information would
intuitively seem more desirable.
</bodyText>
<subsectionHeader confidence="0.977743">
4.2 Features Used in Experiments
</subsectionHeader>
<bodyText confidence="0.994139875">
For the subjectivied experiments to follow,
we attempt to capture the linguistic
characteristics identified in previous work
(Section 5) in a lightweight and robust manner,
due to the informal and noisy nature of CQA.
In particular, we use the following feature
classes, computed separately over question and
answer content:
</bodyText>
<listItem confidence="0.9584714">
■ Character 3-grams
■ Words
■ Word with Character 3-grams
■ Word n-grams (n&lt;=3, i.e. Wi, WiWi+,,
WiWi+,Wi+2)
</listItem>
<footnote confidence="0.90817075">
5http://answers.yahoo.com/question/?qid=20060829074901AA
DBRJ4
6 http://answers.yahoo.com/question/?qid=1006012003651
7 Available at http://ir.mathcs.emory.edu/datasets/.
</footnote>
<listItem confidence="0.848941666666667">
■ Word and POS n-gram (n&lt;=3, i.e. Wi,
WiWi+,, Wi POSi+,, POSiWi+, ,
POSiPOSi+,, etc.).
</listItem>
<bodyText confidence="0.999989178571428">
We use the character 3-grams features to
overcome spelling errors and problems of ill-
formatted or ungrammatical questions, and
the POS information to capture common pat-
terns across domains, as words, especially the
content words, are quite diverse in different
topical domains. For word and character 3-
gram features, we consider two different ver-
sions: case-sensitive and case-insensitive.
Case-insensitive features are assumed to be
helpful for mitigating negative effects of ill-
formatted text.
Moreover, we experimented with three
term weighting schemes: Binary, TF, and
TF*IDF. Term frequency (TF) exhibited bet-
ter performance in our development experi-
ments, so we use this weighting scheme for
all the experiments in Section 4. Regarding
features: both words and structure of the text
(e.g., word order) can be used to infer subjec-
tivity. Therefore, the features we employ,
such as words and word n-grams, are ex-
pected to be useful as a (coarse) proxy to
grammatical and phrase features. Unlike tra-
ditional work on news-like text, the text of
CQA and has poor spelling, grammar, and
heavily uses non-standard abbreviations,
hence our decision to use character n-grams.
</bodyText>
<subsectionHeader confidence="0.994723">
4.3 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.942878555555555">
Metrics: Since the prediction on both sub-
jective questions and objective questions is
equally important, we use the macro-
averaged F1 measure as the evaluation met-
ric. This is computed as the macro average of
F1 measures computed for the Subjective and
Objective classes individually. The F1 meas-
ure for either class is computed
2 Precision Recall
</bodyText>
<equation confidence="0.6583245">
⋅ ⋅
.
Precision Recall
+
</equation>
<bodyText confidence="0.70608225">
Methods compared: We compare our ap-
proach with both the base supervised learning,
as well as GE, a state-of-the-art semi-
supervised method:
</bodyText>
<listItem confidence="0.758760333333333">
■ Supervised: we use the LibSVM im-
plementation (Chang and Lin, 2001)
with linear kernel.
</listItem>
<bodyText confidence="0.63512">
as
</bodyText>
<page confidence="0.969441">
941
</page>
<bodyText confidence="0.735661818181818">
■ GE: This is a state-of-the-art semi-
supervised learning algorithm, General-
ized Expectation (GE), introduced in
(McCallum et al., 2007) that incorporates
model expectations into the objective
functions for parameter estimation.
■ CoCQA: Our method (Section 3).
For semi-supervised learning experiments,
we selected a random subset of 2,000 unla-
beled questions for each of the topical catego-
ries, for the total of 10,000 unlabeled questions.
</bodyText>
<subsectionHeader confidence="0.994698">
4.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.998646333333333">
First we report the performance of our Super-
vised baseline system with a variety of fea-
tures, reporting the average results of 5-fold
cross validation. Then we investigate the per-
formance to our new CoCQA framework under
a variety of settings.
</bodyText>
<subsectionHeader confidence="0.776658">
4.4.1 Supervised Learning
</subsectionHeader>
<bodyText confidence="0.99973672972973">
Table 2 reports the classification perform-
ance for varying units of representation (e.g.,
question text vs. answer text) and the varying
feature sets. We used case-insensitive features
and TF (term frequency within the text unit) as
feature weights, as these two settings achieved
the best results in our development experi-
ments. The rows show performance consider-
ing only the question text (question), the best
answer (best_ans), text of all answers to a
question (all_ans), the text of the question and
the best answer (q_bestans), and the text of
the question with all answers (q_allans), re-
spectively. In particular, using the words in
the question alone achieves F1 of 0.717, com-
pared to using words in the question and the
best answers text (F1 of 0.695). For compari-
son, a naïve baseline that always guesses the
majority class (Subjective) obtains F1 of 0.398.
With character 3-gram, our system achieves
performance comparable with words as fea-
tures, but combining them together does not
improve performance. We observe a slight
gain with more complicated features, e.g. word
n-gram, or word and POS n-grams, but the
gain is not significant, and hence not worth the
increased complexity of the feature generation.
Finally, combining question text with answer
text does not improve performance.
Interestingly, the best answer itself is not as
effective as the question for subjectivity
analysis, nor is using all of the answers sub-
mitted. One possible reason is that approxi-
mately 40% of the best answers were chosen
by the community and not the asker herself,
are hence not necessarily representative of the
asker intent.
</bodyText>
<table confidence="0.9990024">
Feature Char set Word+ Word Word
Unit 3- Word Char n-gram POS
gram 3-gram (n&lt;=3) n-gram
(n&lt;=3)
question 0.700 0.717 0.694 0.716 0.720
best ans 0.587 0.597 0.578 0.580 0.565
all_ans 0.603 0.628 0.607 0.648 0.630
q bestans 0.681 0.695 0.662 0.687 0.712
q allans 0.679 0.677 0.676 0.708 0.689
Naïve (majority class) baseline: 0.398
</table>
<tableCaption confidence="0.993478333333333">
Table 2. Performance of predicting question
orientation on the mixed dataset with varying
feature sets (Supervised).
</tableCaption>
<bodyText confidence="0.991976333333333">
Table 3 reports the supervised subjectivity
classification performance for each question
category with word features. The overall clas-
sification results are significantly lower com-
pared to training and testing on the mixture of
the questions drawn from all categories,
likely caused by the small amount of labeled
training data for each category. Another pos-
sibility is that the subjectivity clues are not
topical and hence are not category dependent,
with the possible exception of the questions
in the Health domain.
</bodyText>
<subsectionHeader confidence="0.727074">
Category Arts Edu. Health Science Sports
</subsectionHeader>
<bodyText confidence="0.58944">
F1 0.448 0.572 0.711 0.647 0.441
</bodyText>
<tableCaption confidence="0.860277">
Table 3. Experiment results on sub-categories
with supervised SVM (q_bestans features).
</tableCaption>
<bodyText confidence="0.999976555555556">
As words are simple and effective features
in this experiment, we will use them in the
subsequent experiments. Furthermore, the
feature set using the words in the question
with best answer together (q_bestans) exhibit
higher performance than question with all
answers (q_allans). Thus, we will only con-
sider questions and best answers in the fol-
lowing experiments with GE and CoCQA.
</bodyText>
<subsectionHeader confidence="0.562411">
4.4.2 Semi-Supervised Learning
</subsectionHeader>
<bodyText confidence="0.9998905">
We now focus on investigating the effec-
tiveness of CoCQA, our co-training-based
framework for community question answer-
ing analysis. Table 4 summarizes the main
</bodyText>
<page confidence="0.995438">
942
</page>
<bodyText confidence="0.999607">
results of this section. The values for CoCQA
are derived with the parameter settings: K=100,
X=0.001. These optimal settings are chosen
after comprehensive experiments with differ-
ent combinations, described later in this sec-
tion. GE does not exhibit a significant im-
provement over Supervised. In contrast,
CoCQA performs significantly better than the
purely supervised method, with F1 of 0.745
compared to the F1 of 0.717 for Supervised.
While it may seem surprising that a semi-
supervised method outperforms a supervised
one, note that we use all of the available la-
beled data as provided to the Supervised
method, as well as a large amount of unlabeled
data, that is ultimately responsible for the per-
formance improvement.
</bodyText>
<table confidence="0.996143">
Features Question Question+
Method Best Answer
Supervised 0.717 0.695
GE 0.712 (-0.7%) 0.717 (+3.2%)
CoCQA 0.731 (+1.9%) 0.745 (+7.2%)
</table>
<tableCaption confidence="0.909436">
Table 4. Performance of CoCQA, GE, and Su-
pervised with the same feature and data settings.
</tableCaption>
<bodyText confidence="0.999863820512821">
As an added advantage, CoCQA approach is
also practical. In a realistic application, we
have two different situations: offline and
online. With online processing, we may not
have best answers available to predict ques-
tion’s orientation, whereas we can employ in-
formation from best answers in offline setting.
Co-training is a solution that is applicable to
both situations. With CoCQA, we have two
classifiers using the question text and the best
answer text, respectively. We can use both of
them to obtain better results in the offline set-
ting, while in online setting, we can use the
text of the question alone. In contrast, GE may
not have this flexibility.
We now analyze the performance of
CoCQA under a variety of settings to derive
optimal parameters and to better understand
the performance. Figure 3 reports the perform-
ance of CoCQA with varying the K parameter
from 20 to 200. In this experiment, we fix X to
be 0.001. The combination of question and
best answer is superior to that of question and
all answers. When K is 100, the system obtains
the best result, 0.745.
Figure 4 reports the number of co-training
iterations needed to converge to optimal per-
formance. After 13 iterations (and 2500 unla-
beled examples added), CoCQA achieves op-
timal performance, and eventually terminates
after an additional 3 iterations. While a vali-
dation set should have been used for CoCQA
parameter tuning, Figures 3 and 4 indicate
that CoCQA is not sensitive to the specific
parameter settings. In particular, we observe
that any K is greater than 100, and for any
number of iterations R is greater than 10,
CoCQA exhibits in effectively equivalent per-
formance.
</bodyText>
<figureCaption confidence="0.960731666666667">
Figure 3: Performance of CoCQA for varying
the K (number of examples added on each it-
eration of co-training).
</figureCaption>
<bodyText confidence="0.993457875">
Figure 5 reports the performance of
CoCQA for varying the number of labeled
examples from 50 to 400 (that is, up to 50%
of the available labeled training data). Note
that for this comparison we use the same fea-
ture sets (words in question and best answer
text), but using only the (varying) fractions of
the manually labeled data. Surprisingly,
CoCQA exhibits comparable performance of
F1=0.685 with only 200 labeled examples are
used, compared to the F1=0.695 for Super-
vised with all 800 labeled training examples
on this feature set. In other words, CoCQA is
able to achieve comparable performance to
supervised SVM with only one quarter of the
labeled training data.
</bodyText>
<figure confidence="0.998171809523809">
F 1
0.76
0.75
0.74
0.73
0.72
0.71
0.69
0.68
0.67
0.66
0.65
0.64
0.7
20 40 60 80 100 120 140 160 180 200
K: # labeled examples adde1 on each
co-training iteration
CoCQA(Question and Best Answer)
Supervised Q_bestans
CoCQA(Question and All Answers)
Supervised Q_allans
</figure>
<page confidence="0.938549">
943
</page>
<figureCaption confidence="0.960698857142857">
Figure 4: Performance and the total number of
unlabeled examples added for varying number
of co-training iterations (K=100, using q_bestans
features)
Figure 5: Performance of CoCQA with varying
number of labeled examples used, compared to
Supervised method, on same features.
</figureCaption>
<sectionHeader confidence="0.999846" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999899046153846">
Question analysis, especially question classifi-
cation, has been long studied in the question
answering research community. However,
most of the previous research primarily con-
sidered factual questions, with the notable ex-
ception of the most recent TREC opinion QA
track. Furthermore, the questions were specifi-
cally designed for benchmark evaluation. A
related thread of research considered deep
analysis of the questions (and corresponding
sentences) by manually classifying questions
along several orientation dimensions, notably
(Stoyanov et al., 2005). In contrast, our work
focuses on analyzing real user questions
posted in a question answering community.
These questions are often complex or subjec-
tive, and are typically difficult to answer
automatically as the question author probably
was not able to find satisfactory answers with
quick web search.
Automatic complex question answering has
been an active area of research, ranging from
simple modification to factoid QA techniques
(e.g., Soricut and Brill, 2003) to knowledge
intensive approaches for specific domains
(e.g., Harabagiu et al. 2001, Fushman and Lin
2007). However, the technology does not yet
exist to automatically answer open-domain
complex and subjective questions. While
there has been some recent research (e.g.,
Agichtein et al. 2008, Bian et al. 2008) on
retrieving high quality answers from CQA
archives, the subjectivity orientation of the
questions has not been considered as a feature
for ranking.
A related corresponding problem is com-
plex QA evaluation. Recent efforts at auto-
matic evaluation show that even for well-
defined, objective, complex questions,
evaluation is extremely labor-intensive and
introduces many challenges (Lin and
Fushman 2006, Lin and Zhang 2007). As part
of our contribution we showed that it is feasi-
ble to use the Amazon Mechanical Turk ser-
vice for evaluation by combining large degree
of annotator redundancy (5 annotators per
question) with more sparse but higher-quality
expert annotation.
The problem of automatic subjective ques-
tion answering has recently started to be ad-
dressed in the question answering commu-
nity, most recently as the first opinion QA
track in (Dang et al., 2007). Unlike the con-
trolled TREC opinion track (introduced in
2007), many of the questions in Yahoo! An-
swers community are inherently subjective,
complex, ill-formed, or all of the above. To
our knowledge, this paper is the first large-
scale study of subjective/objective orientation
of information needs, and certainly the first in
the CQA environment.
A closely related research thread is subjec-
tivity analysis at document and sentence
level. For example, reference (Yu, H., and
Hatzivassiloglou, V. 2003; Somasundaran et
</bodyText>
<figure confidence="0.998235472222222">
Total # Unlabeled Added
3500
3000
2500
2000
1500
1000
500
0
6 6 6 6 7 7 7 7 13 16
# co-training iterations
CoCQA (Question + Best Answer)
Supervised
Total # Unlabeled
0.75
0.74
0.73
0.72
0.71
F1
F1
0.72
0.68
0.66
0.64
0.62
0.58
0.56
0.54
0.52
0.7
0.6
50 100 150 200 250 300 350 400
# of labeled data used
CoCQA (Question + Best Answer)
Supervised Q_Best Ans
</figure>
<page confidence="0.995284">
944
</page>
<bodyText confidence="0.999881162162162">
al. 2007) attempted to classify sentences into
those reporting facts or opinions. Also related
is research on sentiment analysis (e.g., Pang et
al., 2004) where the goal is to classify a sen-
tence or text fragment as being overall positive
or negative. More generally, (Wiebe et al.
2004) and subsequent work focused on the
analysis of subjective language in narrative
text, primarily news. Our problem is quite dif-
ferent in the sense that we are trying to iden-
tify the orientation of a question. Nevertheless,
our baseline method is similar to the methods
and features used for sentiment analysis, and
one of our contributions is evaluating the use-
fulness of the established features and tech-
niques to the new CQA setting.
In order to predict question orientation, we
build on co-training, one of the known semi-
supervised learning techniques. Many models
and techniques have been proposed for classi-
fication, including support vector machines,
decision tree based techniques, boosting-based
techniques, and many others. We use LIBSVM
(Chang and Lin, 2001) as a robust implemen-
tation of SVM algorithms.
In summary, while we draw on many tech-
niques in question answering, natural language
processing, and text classification, our work
differs from previous research in that a) de-
velop a novel co-training based algorithm for
question and answer classification; b) we ad-
dress a relatively new problem of automatic
question subjectivity prediction; c) demon-
strate the effectiveness of our techniques in the
new CQA setting and d) explore the character-
istics unique to CQA – while showing good
results for a quite difficult task.
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999995727272727">
We presented CoCQA, a co-training frame-
work for modeling the textual interactions in
question answer communities. Unlike previous
work, we have focused on real user questions
(often noisy, ungrammatical, and vague) sub-
mitted in Yahoo! Answers, a popular commu-
nity question answering portal. We demon-
strated CoCQA for one particularly important
task of automatically identifying question sub-
jectivity orientation, showing that CoCQA is
able to exploit the structure of questions and
corresponding answers. Despite the inherent
difficulties of subjectivity analysis for real user
questions, we have shown that by applying
CoCQA to this task we can significantly im-
prove prediction performance, and substan-
tially reduce the size of the required training
data, while outperforming a general state-of-
the-art semi-supervised algorithm that does
not take advantage of the CQA characteris-
tics.
In the future we plan to explore more so-
phisticated features such semantic concepts
and relationships (e.g., derived from WordNet
or Wikipedia), and richer syntactic and lin-
guistic information. We also plan to explore
related variants of semi-supervised learning
such as co-boosting methods to further im-
prove classification performance. We will
also investigate other applications of our co-
training framework to tasks such as sentiment
analysis in community question answering
and similar social media content.
</bodyText>
<sectionHeader confidence="0.997761" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999819428571429">
This research was partially supported by the
Emory University Research Committee
(URC) grant, and by the Emory College Seed
grant. We thank the Yahoo! Answers team for
providing access to the Answers API, and
anonymous reviewers for their excellent sug-
gestions.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999798454545455">
Agichtein, E., Castillo, C., Donato, D., Gionis, A., and
Mishne, G. 2008. Finding High-Quality Content in
Social Media with an Application to Community-
Based Question Answering. WSDM2008
Bian, J., Liu, Y., Agichtein, E., and H. Zha. 2008, to
appear. Finding the Right Facts in the Crowd: Fac-
toid Question Answering over Social Media, Pro-
ceedings of the Inter-national World Wide Web Con-
ference (WWW), 2008
Blum, A., and Mitchell, T. 1998. Combining Labeled
and Unlabeled Data with Co-Training. Proc. of the
Annual Conference on Computational Learning
Theory.
Chang, C. C. and Lin, C. J. 2001. LIBSVM : a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm.
Chapelle, O., Scholkopf, B., and Zien, A. 2006. Semi-
supervised Learning. The MIT Press, Cambridge,
Mas-sachusetts.
Dang, H. T., Kelly, D., and Lin, J. 2007. Overview of
the TREC 2007 Question Answering track. In Pro-
ceedings of TREC-2007.
</reference>
<page confidence="0.988936">
945
</page>
<reference confidence="0.988370309090909">
Zhu, X. 2005. Semi-supervised Learning Literature
Survey. Technical Report 1530, Computer Sciences,
University of Wisconsin-Madison.
Demner-Fushman, D. and Lin, J. 2007. Answering clini-
cal questions with knowledge-based and statistical
techniques. Computational Linguistics, 33(1):63–103.
Harabagiu, S., Moldovan, D., Pasca, M., Surdeanu, M. ,
Mihalcea, R., Girju, R., Rusa, V., Lacatusu, F.,
Morarescu, P., and Bunescu, R. 2001. Answering
Complex, List and Context Questions with LCC&apos;s
Question-Answering Server. In Proc. of TREC 2001.
Lin, J. and Demner-Fushman, D. 2006. Methods for
automatically evaluating answers to complex ques-
tions. In-formation Retrieval, 9(5):565–587
Lin, J. and Zhang, P. 2007. Deconstructing nuggets: the
stability and reliability of complex question answering
evaluation. In Proceedings of the 30th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 327–334.
Mann, G., and McCallum, A. 2007. Simple, Robust,
Scalable Semi-supervised Learning via Expectation
Regularization. Proceedings of ICML 2007.
Pang, B., and Lee, L. 2004. A Sentimental Education:
Sen-timent Analysis Using Subjective Summarization
Based on Minimum Cuts. In Proc. of ACL.
Prager, J. 2006. Open-Domain Question-Answering.
Foundations and Trends in Information Retrieval.
Sindhwani, V., Keerthi, S. 2006. Large Scale Semi-
supervised Linear SVMs. Proceedings of SIGIR 2006.
Somasundaran, S., Wilson, T., Wiebe, J. and Stoyanov,
V. 2007. QA with Attitude: Exploiting Opinion Type
Analysis for Improving Question Answering in On-
line Discussions and the News. In proceedings of In-
ternational Conference on Weblogs and Social Media
(ICWSM-2007).
Soricut, R. and Brill, E. 2004. Automatic question an-
swering: Beyond the factoid. Proceedings of HLT-
NAACL.
Stoyanov, V., Cardie, C., and Wiebe, J. 2005. Multi-
Perspective question answering using the OpQA cor-
pus. In Proceedings of EMNLP.
Tri, N. T., Le, N. M., and Shimazu, A. 2006. Using
Semi-supervised Learning for Question Classification.
In Proceedings of ICCPOL-2006.
Wiebe, J., Wilson, T., Bruce R., Bell M., and Martin M.
2004. Learning subjective language. Computational
Linguistics, 30 (3).
Yu, H., and Hatzivassiloglou, V. 2003. Towards Answer-
ing Opinion Questions: Separating Facts from Opin-
ions and Identifying the Polarity of Opinion Sentences.
In Proceedings of EMNLP-2003.
Zhang, D., and Lee, W.S. 2003. Question Classification
Using Support Vector Machines. Proceedings of the
26th Annual International ACM SIGIR Conference on
Re-search and Development in Information Retrieval.
</reference>
<page confidence="0.998782">
946
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.983103">
<title confidence="0.9993975">Co-Training Over Questions and with an Application to Predicting Question Subjectivity Orientation</title>
<author confidence="0.996554">Baoli Li Yandong Liu Eugene Agichtein</author>
<affiliation confidence="0.998448">Emory University Emory University Emory University</affiliation>
<email confidence="0.996364">csblli@gmail.comyliu49@emory.edueugene@mathcs.emory.edu</email>
<abstract confidence="0.999741032258064">An increasingly popular method for finding information online is via the Community Question Answering (CQA) portals such as Yahoo! Answers, Naver, and Baidu Knows. Searching the CQA archives, and ranking, filtering, and evaluating the submitted answers requires intelligent processing of the questions and answers posed by the users. One important task is automatically detecting the namely, whether a user is searching for subjective or objective information. Unfortunately, real user questions are often vague, ill-posed, poorly stated. Furthermore, there has been little labeled training data available for real user questions. To address these probwe present a co-training system that exploits the association between the questions and contributed answers for question analysis tasks. The co-training approach allows use the effectively unlimamounts of readily available in CQA archives. In this paper we study the effectiveness of the question subjectivity classification task by experimenting over thousands of real users’ questions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>C Castillo</author>
<author>D Donato</author>
<author>A Gionis</author>
<author>G Mishne</author>
</authors>
<title>Finding High-Quality Content in Social Media with an Application to CommunityBased Question Answering.</title>
<date>2008</date>
<pages>2008</pages>
<contexts>
<context position="27722" citStr="Agichtein et al. 2008" startWordPosition="4399" endWordPosition="4402">x or subjective, and are typically difficult to answer automatically as the question author probably was not able to find satisfactory answers with quick web search. Automatic complex question answering has been an active area of research, ranging from simple modification to factoid QA techniques (e.g., Soricut and Brill, 2003) to knowledge intensive approaches for specific domains (e.g., Harabagiu et al. 2001, Fushman and Lin 2007). However, the technology does not yet exist to automatically answer open-domain complex and subjective questions. While there has been some recent research (e.g., Agichtein et al. 2008, Bian et al. 2008) on retrieving high quality answers from CQA archives, the subjectivity orientation of the questions has not been considered as a feature for ranking. A related corresponding problem is complex QA evaluation. Recent efforts at automatic evaluation show that even for welldefined, objective, complex questions, evaluation is extremely labor-intensive and introduces many challenges (Lin and Fushman 2006, Lin and Zhang 2007). As part of our contribution we showed that it is feasible to use the Amazon Mechanical Turk service for evaluation by combining large degree of annotator re</context>
</contexts>
<marker>Agichtein, Castillo, Donato, Gionis, Mishne, 2008</marker>
<rawString>Agichtein, E., Castillo, C., Donato, D., Gionis, A., and Mishne, G. 2008. Finding High-Quality Content in Social Media with an Application to CommunityBased Question Answering. WSDM2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bian</author>
<author>Y Liu</author>
<author>E Agichtein</author>
<author>H Zha</author>
</authors>
<title>to appear. Finding the Right Facts in the Crowd: Factoid Question Answering over Social Media,</title>
<date>2008</date>
<booktitle>Proceedings of the Inter-national World Wide Web Conference (WWW),</booktitle>
<contexts>
<context position="27741" citStr="Bian et al. 2008" startWordPosition="4403" endWordPosition="4406">e typically difficult to answer automatically as the question author probably was not able to find satisfactory answers with quick web search. Automatic complex question answering has been an active area of research, ranging from simple modification to factoid QA techniques (e.g., Soricut and Brill, 2003) to knowledge intensive approaches for specific domains (e.g., Harabagiu et al. 2001, Fushman and Lin 2007). However, the technology does not yet exist to automatically answer open-domain complex and subjective questions. While there has been some recent research (e.g., Agichtein et al. 2008, Bian et al. 2008) on retrieving high quality answers from CQA archives, the subjectivity orientation of the questions has not been considered as a feature for ranking. A related corresponding problem is complex QA evaluation. Recent efforts at automatic evaluation show that even for welldefined, objective, complex questions, evaluation is extremely labor-intensive and introduces many challenges (Lin and Fushman 2006, Lin and Zhang 2007). As part of our contribution we showed that it is feasible to use the Amazon Mechanical Turk service for evaluation by combining large degree of annotator redundancy (5 annotat</context>
</contexts>
<marker>Bian, Liu, Agichtein, Zha, 2008</marker>
<rawString>Bian, J., Liu, Y., Agichtein, E., and H. Zha. 2008, to appear. Finding the Right Facts in the Crowd: Factoid Question Answering over Social Media, Proceedings of the Inter-national World Wide Web Conference (WWW), 2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>Proc. of the Annual Conference on Computational Learning Theory.</booktitle>
<contexts>
<context position="3846" citStr="Blum and Mitchell, 1998" startWordPosition="561" endWordPosition="564">http://answers.yahoo.com 2 http://www.naver.com 3 http://www.baidu.com 937 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 937–946, Honolulu, October 2008.c�2008 Association for Computational Linguistics ship between questions and answers can be exploited to improve automated analysis of the CQA content and the user intent behind the questions posted. To this end, we exploit the ideas of cotraining, a general semi-supervised learning approach naturally applicable to cases of complementary views on a domain, for example, web page links and content (Blum and Mitchell, 1998). In our setting, we focus on the complimentary views for a question, namely the text of the question and the text of the associated answers. As a concrete case-study of our approach we focus on one particularly important aspect of intent detection: the subjectivity orientation. We attempt to predict whether a question posted in a CQA site is subjective or objective. Objective questions are expected to be answered with reliable or authoritative information, typically published online and possibly referenced as part of the answer, whereas subjective questions seek answers containing private sta</context>
<context position="9468" citStr="Blum and Mitchell, 1998" startWordPosition="1473" endWordPosition="1476"> easily obtain thousands or millions of unlabeled examples from the online CQA archives. On the other hand, it is difficult to create a labeled dataset with a reasonable size, which could be used to train a perfect classifier to analyze questions from different domains and subdomains. Therefore, semi-supervised learning (Chapelle et al., 2006) is a natural approach for this setting. Intuitively, we can consider the text of the question itself or answers to it. In other words, we have multiple (at least two) natural views of the data, which satisfies the conditions of the co-training approach (Blum and Mitchell, 1998). In co-training, two separate classifiers are trained on two sets of features, respectively. By automatically labeling the unlabeled examples, these two classifiers iteratively “teach” each other by giving their partners a newly labeled data that they can predict with high confidence. Based on the original co-training algorithm in (Blum and Mitchell, 1998) and other implementations, we develop our algorithm CoCQA shown in Figure 2. At Steps 1 and 2, the K examples are coming from different feature spaces, and each category (for example, Subjective and Objective) has top Kj most confident exam</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Blum, A., and Mitchell, T. 1998. Combining Labeled and Unlabeled Data with Co-Training. Proc. of the Annual Conference on Computational Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C Chang</author>
<author>C J Lin</author>
</authors>
<title>LIBSVM : a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="18355" citStr="Chang and Lin, 2001" startWordPosition="2914" endWordPosition="2917">racter n-grams. 4.3 Experimental Setting Metrics: Since the prediction on both subjective questions and objective questions is equally important, we use the macroaveraged F1 measure as the evaluation metric. This is computed as the macro average of F1 measures computed for the Subjective and Objective classes individually. The F1 measure for either class is computed 2 Precision Recall ⋅ ⋅ . Precision Recall + Methods compared: We compare our approach with both the base supervised learning, as well as GE, a state-of-the-art semisupervised method: ■ Supervised: we use the LibSVM implementation (Chang and Lin, 2001) with linear kernel. as 941 ■ GE: This is a state-of-the-art semisupervised learning algorithm, Generalized Expectation (GE), introduced in (McCallum et al., 2007) that incorporates model expectations into the objective functions for parameter estimation. ■ CoCQA: Our method (Section 3). For semi-supervised learning experiments, we selected a random subset of 2,000 unlabeled questions for each of the topical categories, for the total of 10,000 unlabeled questions. 4.4 Experimental Results First we report the performance of our Supervised baseline system with a variety of features, reporting th</context>
<context position="30545" citStr="Chang and Lin, 2001" startWordPosition="4862" endWordPosition="4865">hat we are trying to identify the orientation of a question. Nevertheless, our baseline method is similar to the methods and features used for sentiment analysis, and one of our contributions is evaluating the usefulness of the established features and techniques to the new CQA setting. In order to predict question orientation, we build on co-training, one of the known semisupervised learning techniques. Many models and techniques have been proposed for classification, including support vector machines, decision tree based techniques, boosting-based techniques, and many others. We use LIBSVM (Chang and Lin, 2001) as a robust implementation of SVM algorithms. In summary, while we draw on many techniques in question answering, natural language processing, and text classification, our work differs from previous research in that a) develop a novel co-training based algorithm for question and answer classification; b) we address a relatively new problem of automatic question subjectivity prediction; c) demonstrate the effectiveness of our techniques in the new CQA setting and d) explore the characteristics unique to CQA – while showing good results for a quite difficult task. 6 Conclusions We presented CoC</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chang, C. C. and Lin, C. J. 2001. LIBSVM : a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Chapelle</author>
<author>B Scholkopf</author>
<author>A Zien</author>
</authors>
<title>Semisupervised Learning.</title>
<date>2006</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Mas-sachusetts.</location>
<contexts>
<context position="9189" citStr="Chapelle et al., 2006" startWordPosition="1425" endWordPosition="1428">iven a question Q in a question answering community, predict whether Q has objective or subjective orientation, based on question and answer text as well as the user and community feedback. 3 CoCQA: A Co-Training Framework over Questions and Answers In the CQA setting we could easily obtain thousands or millions of unlabeled examples from the online CQA archives. On the other hand, it is difficult to create a labeled dataset with a reasonable size, which could be used to train a perfect classifier to analyze questions from different domains and subdomains. Therefore, semi-supervised learning (Chapelle et al., 2006) is a natural approach for this setting. Intuitively, we can consider the text of the question itself or answers to it. In other words, we have multiple (at least two) natural views of the data, which satisfies the conditions of the co-training approach (Blum and Mitchell, 1998). In co-training, two separate classifiers are trained on two sets of features, respectively. By automatically labeling the unlabeled examples, these two classifiers iteratively “teach” each other by giving their partners a newly labeled data that they can predict with high confidence. Based on the original co-training </context>
</contexts>
<marker>Chapelle, Scholkopf, Zien, 2006</marker>
<rawString>Chapelle, O., Scholkopf, B., and Zien, A. 2006. Semisupervised Learning. The MIT Press, Cambridge, Mas-sachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
<author>D Kelly</author>
<author>J Lin</author>
</authors>
<title>Question Answering track.</title>
<date>2007</date>
<journal>Overview of the TREC</journal>
<booktitle>In Proceedings of TREC-2007.</booktitle>
<contexts>
<context position="28609" citStr="Dang et al., 2007" startWordPosition="4540" endWordPosition="4543">ow that even for welldefined, objective, complex questions, evaluation is extremely labor-intensive and introduces many challenges (Lin and Fushman 2006, Lin and Zhang 2007). As part of our contribution we showed that it is feasible to use the Amazon Mechanical Turk service for evaluation by combining large degree of annotator redundancy (5 annotators per question) with more sparse but higher-quality expert annotation. The problem of automatic subjective question answering has recently started to be addressed in the question answering community, most recently as the first opinion QA track in (Dang et al., 2007). Unlike the controlled TREC opinion track (introduced in 2007), many of the questions in Yahoo! Answers community are inherently subjective, complex, ill-formed, or all of the above. To our knowledge, this paper is the first largescale study of subjective/objective orientation of information needs, and certainly the first in the CQA environment. A closely related research thread is subjectivity analysis at document and sentence level. For example, reference (Yu, H., and Hatzivassiloglou, V. 2003; Somasundaran et Total # Unlabeled Added 3500 3000 2500 2000 1500 1000 500 0 6 6 6 6 7 7 7 7 13 16</context>
</contexts>
<marker>Dang, Kelly, Lin, 2007</marker>
<rawString>Dang, H. T., Kelly, D., and Lin, J. 2007. Overview of the TREC 2007 Question Answering track. In Proceedings of TREC-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
</authors>
<title>Semi-supervised Learning Literature Survey.</title>
<date>2005</date>
<tech>Technical Report 1530,</tech>
<institution>Computer Sciences, University of Wisconsin-Madison.</institution>
<marker>Zhu, 2005</marker>
<rawString>Zhu, X. 2005. Semi-supervised Learning Literature Survey. Technical Report 1530, Computer Sciences, University of Wisconsin-Madison.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Demner-Fushman</author>
<author>J Lin</author>
</authors>
<title>Answering clinical questions with knowledge-based and statistical techniques.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<marker>Demner-Fushman, Lin, 2007</marker>
<rawString>Demner-Fushman, D. and Lin, J. 2007. Answering clinical questions with knowledge-based and statistical techniques. Computational Linguistics, 33(1):63–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>R Girju</author>
<author>V Rusa</author>
<author>F Lacatusu</author>
<author>P Morarescu</author>
<author>R Bunescu</author>
</authors>
<title>Answering Complex, List and Context Questions with LCC&apos;s Question-Answering Server.</title>
<date>2001</date>
<booktitle>In Proc. of TREC</booktitle>
<marker>Mihalcea, Girju, Rusa, Lacatusu, Morarescu, Bunescu, 2001</marker>
<rawString>Harabagiu, S., Moldovan, D., Pasca, M., Surdeanu, M. , Mihalcea, R., Girju, R., Rusa, V., Lacatusu, F., Morarescu, P., and Bunescu, R. 2001. Answering Complex, List and Context Questions with LCC&apos;s Question-Answering Server. In Proc. of TREC 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Demner-Fushman</author>
</authors>
<title>Methods for automatically evaluating answers to complex questions.</title>
<date>2006</date>
<journal>In-formation Retrieval,</journal>
<volume>9</volume>
<issue>5</issue>
<marker>Lin, Demner-Fushman, 2006</marker>
<rawString>Lin, J. and Demner-Fushman, D. 2006. Methods for automatically evaluating answers to complex questions. In-formation Retrieval, 9(5):565–587</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>P Zhang</author>
</authors>
<title>Deconstructing nuggets: the stability and reliability of complex question answering evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>327--334</pages>
<contexts>
<context position="28164" citStr="Lin and Zhang 2007" startWordPosition="4466" endWordPosition="4469">r, the technology does not yet exist to automatically answer open-domain complex and subjective questions. While there has been some recent research (e.g., Agichtein et al. 2008, Bian et al. 2008) on retrieving high quality answers from CQA archives, the subjectivity orientation of the questions has not been considered as a feature for ranking. A related corresponding problem is complex QA evaluation. Recent efforts at automatic evaluation show that even for welldefined, objective, complex questions, evaluation is extremely labor-intensive and introduces many challenges (Lin and Fushman 2006, Lin and Zhang 2007). As part of our contribution we showed that it is feasible to use the Amazon Mechanical Turk service for evaluation by combining large degree of annotator redundancy (5 annotators per question) with more sparse but higher-quality expert annotation. The problem of automatic subjective question answering has recently started to be addressed in the question answering community, most recently as the first opinion QA track in (Dang et al., 2007). Unlike the controlled TREC opinion track (introduced in 2007), many of the questions in Yahoo! Answers community are inherently subjective, complex, ill-</context>
</contexts>
<marker>Lin, Zhang, 2007</marker>
<rawString>Lin, J. and Zhang, P. 2007. Deconstructing nuggets: the stability and reliability of complex question answering evaluation. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 327–334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mann</author>
<author>A McCallum</author>
</authors>
<title>Simple, Robust, Scalable Semi-supervised Learning via Expectation Regularization.</title>
<date>2007</date>
<booktitle>Proceedings of ICML</booktitle>
<marker>Mann, McCallum, 2007</marker>
<rawString>Mann, G., and McCallum, A. 2007. Simple, Robust, Scalable Semi-supervised Learning via Expectation Regularization. Proceedings of ICML 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A Sentimental Education: Sen-timent Analysis Using Subjective Summarization Based on Minimum Cuts.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Pang, Lee, 2004</marker>
<rawString>Pang, B., and Lee, L. 2004. A Sentimental Education: Sen-timent Analysis Using Subjective Summarization Based on Minimum Cuts. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
</authors>
<title>Open-Domain Question-Answering. Foundations and Trends in Information Retrieval.</title>
<date>2006</date>
<marker>Prager, 2006</marker>
<rawString>Prager, J. 2006. Open-Domain Question-Answering. Foundations and Trends in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sindhwani</author>
<author>S Keerthi</author>
</authors>
<title>Large Scale Semisupervised Linear SVMs.</title>
<date>2006</date>
<booktitle>Proceedings of SIGIR</booktitle>
<marker>Sindhwani, Keerthi, 2006</marker>
<rawString>Sindhwani, V., Keerthi, S. 2006. Large Scale Semisupervised Linear SVMs. Proceedings of SIGIR 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>V Stoyanov</author>
</authors>
<title>QA with Attitude: Exploiting Opinion Type Analysis for Improving Question Answering</title>
<date>2007</date>
<booktitle>in Online Discussions and the News. In proceedings of International Conference on Weblogs and Social Media (ICWSM-2007).</booktitle>
<marker>Somasundaran, Wilson, Wiebe, Stoyanov, 2007</marker>
<rawString>Somasundaran, S., Wilson, T., Wiebe, J. and Stoyanov, V. 2007. QA with Attitude: Exploiting Opinion Type Analysis for Improving Question Answering in Online Discussions and the News. In proceedings of International Conference on Weblogs and Social Media (ICWSM-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>E Brill</author>
</authors>
<title>Automatic question answering: Beyond the factoid.</title>
<date>2004</date>
<booktitle>Proceedings of HLTNAACL.</booktitle>
<marker>Soricut, Brill, 2004</marker>
<rawString>Soricut, R. and Brill, E. 2004. Automatic question answering: Beyond the factoid. Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>C Cardie</author>
<author>J Wiebe</author>
</authors>
<title>MultiPerspective question answering using the OpQA corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="26962" citStr="Stoyanov et al., 2005" startWordPosition="4285" endWordPosition="4288">d, compared to Supervised method, on same features. 5 Related Work Question analysis, especially question classification, has been long studied in the question answering research community. However, most of the previous research primarily considered factual questions, with the notable exception of the most recent TREC opinion QA track. Furthermore, the questions were specifically designed for benchmark evaluation. A related thread of research considered deep analysis of the questions (and corresponding sentences) by manually classifying questions along several orientation dimensions, notably (Stoyanov et al., 2005). In contrast, our work focuses on analyzing real user questions posted in a question answering community. These questions are often complex or subjective, and are typically difficult to answer automatically as the question author probably was not able to find satisfactory answers with quick web search. Automatic complex question answering has been an active area of research, ranging from simple modification to factoid QA techniques (e.g., Soricut and Brill, 2003) to knowledge intensive approaches for specific domains (e.g., Harabagiu et al. 2001, Fushman and Lin 2007). However, the technology</context>
</contexts>
<marker>Stoyanov, Cardie, Wiebe, 2005</marker>
<rawString>Stoyanov, V., Cardie, C., and Wiebe, J. 2005. MultiPerspective question answering using the OpQA corpus. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N T Tri</author>
<author>N M Le</author>
<author>A Shimazu</author>
</authors>
<title>Using Semi-supervised Learning for Question Classification.</title>
<date>2006</date>
<booktitle>In Proceedings of ICCPOL-2006.</booktitle>
<marker>Tri, Le, Shimazu, 2006</marker>
<rawString>Tri, N. T., Le, N. M., and Shimazu, A. 2006. Using Semi-supervised Learning for Question Classification. In Proceedings of ICCPOL-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>R Bruce</author>
<author>M Bell</author>
<author>M Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="29777" citStr="Wiebe et al. 2004" startWordPosition="4741" endWordPosition="4744">00 2500 2000 1500 1000 500 0 6 6 6 6 7 7 7 7 13 16 # co-training iterations CoCQA (Question + Best Answer) Supervised Total # Unlabeled 0.75 0.74 0.73 0.72 0.71 F1 F1 0.72 0.68 0.66 0.64 0.62 0.58 0.56 0.54 0.52 0.7 0.6 50 100 150 200 250 300 350 400 # of labeled data used CoCQA (Question + Best Answer) Supervised Q_Best Ans 944 al. 2007) attempted to classify sentences into those reporting facts or opinions. Also related is research on sentiment analysis (e.g., Pang et al., 2004) where the goal is to classify a sentence or text fragment as being overall positive or negative. More generally, (Wiebe et al. 2004) and subsequent work focused on the analysis of subjective language in narrative text, primarily news. Our problem is quite different in the sense that we are trying to identify the orientation of a question. Nevertheless, our baseline method is similar to the methods and features used for sentiment analysis, and one of our contributions is evaluating the usefulness of the established features and techniques to the new CQA setting. In order to predict question orientation, we build on co-training, one of the known semisupervised learning techniques. Many models and techniques have been propose</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Wiebe, J., Wilson, T., Bruce R., Bell M., and Martin M. 2004. Learning subjective language. Computational Linguistics, 30 (3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-2003.</booktitle>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Yu, H., and Hatzivassiloglou, V. 2003. Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences. In Proceedings of EMNLP-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhang</author>
<author>W S Lee</author>
</authors>
<title>Question Classification Using Support Vector Machines.</title>
<date>2003</date>
<booktitle>Proceedings of the 26th Annual International ACM SIGIR Conference on Re-search and Development in Information Retrieval.</booktitle>
<marker>Zhang, Lee, 2003</marker>
<rawString>Zhang, D., and Lee, W.S. 2003. Question Classification Using Support Vector Machines. Proceedings of the 26th Annual International ACM SIGIR Conference on Re-search and Development in Information Retrieval.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>