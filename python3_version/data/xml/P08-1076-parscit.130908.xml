<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<note confidence="0.54342275">
Semi-Supervised Sequential Labeling and Segmentation
using Giga-word Scale Unlabeled Data
Jun Suzuki and Hideki Isozaki
NTT Communication Science Laboratories, NTT Corp.
</note>
<address confidence="0.858129">
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
</address>
<email confidence="0.994336">
{jun, isozaki}@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.994607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999838368421053">
This paper provides evidence that the use of
more unlabeled data in semi-supervised learn-
ing can improve the performance of Natu-
ral Language Processing (NLP) tasks, such
as part-of-speech tagging, syntactic chunking,
and named entity recognition. We first pro-
pose a simple yet powerful semi-supervised
discriminative model appropriate for handling
large scale unlabeled data. Then, we describe
experiments performed on widely used test
collections, namely, PTB III data, CoNLL’00
and ’03 shared task data for the above three
NLP tasks, respectively. We incorporate up
to 1G-words (one billion tokens) of unlabeled
data, which is the largest amount of unlabeled
data ever used for these tasks, to investigate
the performance improvement. In addition,
our results are superior to the best reported re-
sults for all of the above test collections.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946660377359">
Today, we can easily find a large amount of un-
labeled data for many supervised learning applica-
tions in Natural Language Processing (NLP). There-
fore, to improve performance, the development of
an effective framework for semi-supervised learning
(SSL) that uses both labeled and unlabeled data is at-
tractive for both the machine learning and NLP com-
munities. We expect that such SSL will replace most
supervised learning in real world applications.
In this paper, we focus on traditional and impor-
tant NLP tasks, namely part-of-speech (POS) tag-
ging, syntactic chunking, and named entity recog-
nition (NER). These are also typical supervised
learning applications in NLP, and are referred to
as sequential labeling and segmentation problems.
In some cases, these tasks have relatively large
amounts of labeled training data. In this situation,
supervised learning can provide competitive results,
and it is difficult to improve them any further by
using SSL. In fact, few papers have succeeded in
showing significantly better results than state-of-the-
art supervised learning. Ando and Zhang (2005) re-
ported a substantial performance improvement com-
pared with state-of-the-art supervised learning re-
sults for syntactic chunking with the CoNLL’00
shared task data (Tjong Kim Sang and Buchholz,
2000) and NER with the CoNLL’03 shared task
data (Tjong Kim Sang and Meulder, 2003).
One remaining question is the behavior of SSL
when using as much labeled and unlabeled data
as possible. This paper investigates this question,
namely, the use of a large amount of unlabeled data
in the presence of (fixed) large labeled data.
To achieve this, it is paramount to make the SSL
method scalable with regard to the size of unlabeled
data. We first propose a scalable model for SSL.
Then, we apply our model to widely used test collec-
tions, namely Penn Treebank (PTB) III data (Mar-
cus et al., 1994) for POS tagging, CoNLL’00 shared
task data for syntactic chunking, and CoNLL’03
shared task data for NER. We used up to 1G-words
(one billion tokens) of unlabeled data to explore the
performance improvement with respect to the unla-
beled data size. In addition, we investigate the per-
formance improvement for ‘unseen data’ from the
viewpoint of unlabeled data coverage. Finally, we
compare our results with those provided by the best
current systems.
The contributions of this paper are threefold.
First, we present a simple, scalable, but power-
ful task-independent model for semi-supervised se-
quential labeling and segmentation. Second, we re-
port the best current results for the widely used test
</bodyText>
<page confidence="0.976007">
665
</page>
<note confidence="0.71485">
Proceedings of ACL-08: HLT, pages 665–673,
</note>
<page confidence="0.53748">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.99681">
collections described above. Third, we confirm that
the use of more unlabeled data in SSL can really lead
to further improvements.
</bodyText>
<sectionHeader confidence="0.980898" genericHeader="method">
2 Conditional Model for SSL
</sectionHeader>
<bodyText confidence="0.999144666666667">
We design our model for SSL as a natural semi-
supervised extension of conventional supervised
conditional random fields (CRFs) (Lafferty et al.,
2001). As our approach for incorporating unla-
beled data, we basically follow the idea proposed in
(Suzuki et al., 2007).
</bodyText>
<subsectionHeader confidence="0.984409">
2.1 Conventional Supervised CRFs
</subsectionHeader>
<bodyText confidence="0.99973425">
Let x ∈ X and y ∈ Y be an input and output, where
X and Y represent the set of possible inputs and out-
puts, respectively. C stands for the set of cliques in
an undirected graphical model G(x, y), which indi-
cates the interdependency of a given x and y. yc
denotes the output from the corresponding clique c.
Each clique c∈C has a potential function IFc. Then,
the CRFs define the conditional probability p(y|x)
as a product of IFcs. In addition, let f = (f1, ..., fI)
be a feature vector, and A = (A1, ..., AI) be a pa-
rameter vector, whose lengths are I. p(y|x; A) on a
CRF is defined as follows:
</bodyText>
<equation confidence="0.641023">
p(y |x;A) = Z 1x)Yc `&apos;c(yc, x; A), (1)
</equation>
<bodyText confidence="0.9962444">
where Z(x) = Py∈Y Qc∈C &apos;Fc(yc, x; A) is the par-
tition function. We generally assume that the po-
tential function is a non-negative real value func-
tion. Therefore, the exponentiated weighted sum
over the features of a clique is widely used, so that,
</bodyText>
<construct confidence="0.789180333333333">
IFc(yc, x; A)=exp(A · fc(yc, x)) where fc(yc, x)
is a feature vector obtained from the corresponding
clique c in G(x, y).
</construct>
<subsectionHeader confidence="0.999378">
2.2 Semi-supervised Extension for CRFs
</subsectionHeader>
<bodyText confidence="0.9998105">
Suppose we have J kinds of probability mod-
els (PMs). The j-th joint PM is represented by
pj(xj, y; 0j) where 0j is a model parameter. xj =
Tj(x) is simply an input x transformed by a pre-
defined function Tj. We assume xj has the same
graph structure as x. This means pj(xj, y) can
be factorized by the cliques c in G(x, y). That is,
pj(xj, y; 0j)=Qc pj(xjc, yc; 0j). Thus, we can in-
corporate generative models such as Bayesian net-
works including (1D and 2D) hidden Markov mod-
els (HMMs) as these joint PMs. Actually, there is
a difference in that generative models are directed
graphical models while our conditional PM is an
undirected. However, this difference causes no vi-
olations when we construct our approach.
Let us introduce A0=(A1, ..., AI, AI+1, . . ., AI+J),
and h = (f1, ..., fI, log p1, ..., log pJ), which is
the concatenation of feature vector f and the log-
likelihood of J-joint PMs. Then, we can define a
new potential function by embedding the joint PMs;
</bodyText>
<equation confidence="0.99787475">
V c(yc, x; A&apos;, Θ)
Y
= exp(A · fc(yc, x)) · j pj(xjc, yc; 0j)λ���
= exp(A&apos; · hc(yc, x)).
</equation>
<bodyText confidence="0.999657333333333">
where Θ = {0j}Jj=1, and hc(yc, x) is h obtained
from the corresponding clique c in G(x, y). Since
each pj(xjc, yc) has range [0, 1], which is non-
negative, IF0c can also be used as a potential func-
tion. Thus, the conditional model for our SSL can
be written as:
</bodyText>
<equation confidence="0.96229">
1 P(y |x; A&apos;, Θ) = Z&apos;(x) Yc IF&apos; (yc, x; A&apos;, Θ), (2)
</equation>
<bodyText confidence="0.999608285714286">
where Z0(x) = Py∈YQc∈C V (yc, x; A0, Θ). Here-
after in this paper, we refer to this conditional model
as a ‘Joint probability model Embedding style Semi-
Supervised Conditional Model’, or JESS-CM for
short.
Given labeled data, Dl={(xn, yn)}Nn=1, the MAP
estimation of A0 under a fixed Θ can be written as:
</bodyText>
<construct confidence="0.357505">
log P(yn|xn; A&apos;, Θ) + logp(A&apos;),
</construct>
<bodyText confidence="0.999981636363636">
where p(A0) is a prior probability distribution of A0.
Clearly, JESS-CM shown in Equation 2 has exactly
the same form as Equation 1. With a fixed Θ, the
log-likelihood, log pj, can be seen simply as the fea-
ture functions of JESS-CM as with fi. Therefore,
embedded joint PMs do not violate the global con-
vergence conditions. As a result, as with super-
vised CRFs, it is guaranteed that A0 has a value that
achieves the global maximum of L1(A0|Θ). More-
over, we can obtain the same form of gradient as that
of supervised CRFs (Sha and Pereira, 2003), that is,
</bodyText>
<equation confidence="0.998368">
∇L1(A&apos;|Θ) = EP�(Y,X;a&apos;,d) £h(Y, X)¤
EP(Y|xn;a&apos;,©) £h(Y, xn)¤+∇ log p(A&apos;).
</equation>
<bodyText confidence="0.9804345">
Thus, we can easily optimize L1 by using the
forward-backward algorithm since this paper solely
</bodyText>
<equation confidence="0.9711606">
X
L1(A&apos;|Θ) =
n
X−
n
</equation>
<page confidence="0.982283">
666
</page>
<bodyText confidence="0.998896333333333">
focuses on a sequence model and a gradient-based
optimization algorithm in the same manner as those
used in supervised CRF parameter estimation.
We cannot naturally incorporate unlabeled data
into standard discriminative learning methods since
the correct outputs y for unlabeled data are un-
known. On the other hand with a generative ap-
proach, a well-known way to achieve this incorpora-
tion is to use maximum marginal likelihood (MML)
parameter estimation, i.e., (Nigam et al., 2000).
Given unlabeled data Du = {xm}Mm=1, MML esti-
mation in our setting maximizes the marginal distri-
bution of a joint PM over a missing (hidden) variable
y, namely, it maximizes Em log Ey∈Y p(xm, y; θ).
Following this idea, there have been introduced
a parameter estimation approach for non-generative
approaches that can effectively incorporate unla-
beled data (Suzuki et al., 2007). Here, we refer to it
as ‘Maximum Discriminant Functions sum’ (MDF)
parameter estimation. MDF estimation substitutes
p(x, y) with discriminant functions g(x, y). There-
fore, to estimate the parameter Θ of JESS-CM by
using MDF estimation, the following objective func-
tion is maximized with a fixed λ0:
</bodyText>
<equation confidence="0.995372333333333">
� �
L2(Θ|λ&apos;) = log
m yEY
</equation>
<bodyText confidence="0.999960888888889">
where p(Θ) is a prior probability distribution of
Θ. Since the normalization factor does not af-
fect the determination of y, the discriminant func-
tion of JESS-CM shown in Equation 2 is defined
as g(x, y; λ0, Θ) = Hc∈C Ψ0 c(yc, x; λ0, Θ). With
a fixed λ0, the local maximum of L2(Θ|λ0) around
the initialized value of Θ can be estimated by an iter-
ative computation such as the EM algorithm (Demp-
ster et al., 1977).
</bodyText>
<subsectionHeader confidence="0.996268">
2.3 Scalability: Efficient Training Algorithm
</subsectionHeader>
<bodyText confidence="0.999073916666667">
A parameter estimation algorithm of λ0 and Θ can
be obtained by maximizing the objective functions
L1(λ0|Θ) and L2(Θ|λ0) iteratively and alternately.
Figure 1 summarizes an algorithm for estimating λ0
and Θ for JESS-CM.
This paper considers a situation where there are
many more unlabeled data M than labeled data N,
that is, N &lt;&lt; M. This means that the calculation
cost for unlabeled data is dominant. Thus, in order
to make the overall parameter estimation procedure
Input: training data D = {Dl, Du}
where labeled data Dl = {(xn, yn)}Nn=1,
</bodyText>
<figure confidence="0.930427230769231">
and unlabeled data Du = {xm}Mm=1
Initialize: Θ(0) ← uniform distribution, t ← 0
do
1. t ← t + 1
2. (Re)estimate A0:
maximize L1(A0|Θ) with fixed Θ←Θ(t−1) using Dl.
3. Estimate Θ(t): (Initial values = Θ(t−1))
update one step toward maximizing L2(Θ|A0)
with fixed A0 using Du.
do until |Θ&lt; &lt; e.
(t
Reestimate A0: perform the same procedure as 1.
Output: a JESS-CM, P(y|x, A0, Θ(t)).
</figure>
<figureCaption confidence="0.999963">
Figure 1: Parameter estimation algorithm for JESS-CM.
</figureCaption>
<bodyText confidence="0.999971">
scalable for handling large scale unlabeled data, we
only perform one step of MDF estimation for each t
as explained on 3. in Figure 1. In addition, the cal-
culation cost for estimating parameters of embedded
joint PMs (HMMs) is independent of the number of
HMMs, J, that we used (Suzuki et al., 2007). As a
result, the cost for calculating the JESS-CM param-
eters, λ0 and Θ, is essentially the same as execut-
ing T iterations of the MML estimation for a single
HMM using the EM algorithm plus T + 1 time opti-
mizations of the MAP estimation for a conventional
supervised CRF if it converged when t = T. In
addition, our parameter estimation algorithm can be
easily performed in parallel computation.
</bodyText>
<subsectionHeader confidence="0.999412">
2.4 Comparison with Hybrid Model
</subsectionHeader>
<bodyText confidence="0.9999816">
SSL based on a hybrid generative/discriminative ap-
proach proposed in (Suzuki et al., 2007) has been
defined as a log-linear model that discriminatively
combines several discriminative models, pDi , and
generative models, pGj , such that:
</bodyText>
<equation confidence="0.96376725">
R(y|x; Λ, Θ, Γ)
Hi pDi (y|x;λi)γi Hj pGj (xj,y;θj)γj
� Hi pD i (y|x;λi)γi Hj pG j (xj,y;θj)γj ,
y
</equation>
<bodyText confidence="0.984302111111111">
where Λ={λi}Ii=1, and Γ={{γi}Ii=1, {γj}I+J
j=I+1}.
With the hybrid model, if we use the same labeled
training data to estimate both Λ and Γ, γjs will be-
come negligible (zero or nearly zero) since pDi is al-
ready fitted to the labeled training data while pGj are
trained by using unlabeled data. As a solution, a
given amount of labeled training data is divided into
two distinct sets, i.e., 4/5 for estimating Λ, and the
</bodyText>
<equation confidence="0.908196">
g(xm, y; λ&apos;, Θ) + log p(Θ),
=
</equation>
<page confidence="0.955946">
667
</page>
<bodyText confidence="0.999947363636364">
remaining 1/5 for estimating F (Suzuki et al., 2007).
Moreover, it is necessary to split features into sev-
eral sets, and then train several corresponding dis-
criminative models separately and preliminarily. In
contrast, JESS-CM is free from this kind of addi-
tional process, and the entire parameter estimation
procedure can be performed in a single pass. Sur-
prisingly, although JESS-CM is a simpler version of
the hybrid model in terms of model structure and
parameter estimation procedure, JESS-CM provides
F-scores of 94.45 and 88.03 for CoNLL’00 and ’03
data, respectively, which are 0.15 and 0.83 points
higher than those reported in (Suzuki et al., 2007)
for the same configurations. This performance im-
provement is basically derived from the full bene-
fit of using labeled training data for estimating the
parameter of the conditional model while the com-
bination weights, F, of the hybrid model are esti-
mated solely by using 1/5 of the labeled training
data. These facts indicate that JESS-CM has sev-
eral advantageous characteristics compared with the
hybrid model.
</bodyText>
<sectionHeader confidence="0.999032" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999908666666667">
In our experiments, we report POS tagging, syntac-
tic chunking and NER performance incorporating up
to 1G-words of unlabeled data.
</bodyText>
<subsectionHeader confidence="0.999334">
3.1 Data Set
</subsectionHeader>
<bodyText confidence="0.999590875">
To compare the performance with that of previ-
ous studies, we selected widely used test collec-
tions. For our POS tagging experiments, we used
the Wall Street Journal in PTB III (Marcus et al.,
1994) with the same data split as used in (Shen et
al., 2007). For our syntactic chunking and NER ex-
periments, we used exactly the same training, devel-
opment and test data as those provided for the shared
tasks of CoNLL’00 (Tjong Kim Sang and Buchholz,
2000) and CoNLL’03 (Tjong Kim Sang and Meul-
der, 2003), respectively. The training, development
and test data are detailed in Table 11 .
The unlabeled data for our experiments was
taken from the Reuters corpus, TIPSTER corpus
(LDC93T3C) and the English Gigaword corpus,
third edition (LDC2007T07). As regards the TIP-
</bodyText>
<footnote confidence="0.994754333333333">
1The second-order encoding used in our NER experiments
is the same as that described in (Sha and Pereira, 2003) except
removing IOB-tag of previous position label.
</footnote>
<table confidence="0.986777055555556">
(a) POS-tagging: (WSJ in PTB III)
# of labels 45
Data set (WSJ sec. IDs) # of sent. # of words
Training 0–18 38,219 912,344
Development 19–21 5,527 131,768
Test 22–24 5,462 129,654
(b) Chunking: (WSJ in PTB III: CoNLL’00 shared task data)
# of labels 23 (w/ IOB-tagging)
Data set (WSJ sec. IDs) # of sent. # of words
Training 15–18 8,936 211,727
Development N/A N/A N/A
Test 20 2,012 47,377
(c) NER: (Reuters Corpus: CoNLL’03 shared task data)
# of labels 29 (w/ IOB-tagging+2nd-order encoding)
Data set (time period) # of sent. # of words
Training 22–30/08/96 14,987 203,621
Development 30–31/08/96 3,466 51,362
Test 06–07/12/96 3,684 46,435
</table>
<tableCaption confidence="0.993864">
Table 1: Details of training, development, and test data
(labeled data set) used in our experiments
</tableCaption>
<table confidence="0.9997191">
data abbr. (time period) # of sent. # of words
Tipster wsj 04/90–03/92 1,624,744 36,725,301
Reuters reu 09/96–08/97* 13,747,227 215,510,564
Corpus *(excluding 06–07/12/96)
English afp 05/94–12/96 5,510,730 135,041,450
Gigaword apw 11/94–12/96 7,207,790 154,024,679
ltw 04/94–12/96 3,094,290 72,928,537
nyt 07/94–12/96 15,977,991 357,952,297
xin 01/95–12/96 1,740,832 40,078,312
total all 48,903,604 1,012,261,140
</table>
<tableCaption confidence="0.999344">
Table 2: Unlabeled data used in our experiments
</tableCaption>
<bodyText confidence="0.999874571428571">
STER corpus, we extracted all the Wall Street Jour-
nal articles published between 1990 and 1992. With
the English Gigaword corpus, we extracted articles
from five news sources published between 1994 and
1996. The unlabeled data used in this paper is de-
tailed in Table 2. Note that the total size of the unla-
beled data reaches 1G-words (one billion tokens).
</bodyText>
<subsectionHeader confidence="0.999744">
3.2 Design of JESS-CM
</subsectionHeader>
<bodyText confidence="0.997325">
We used the same graph structure as the linear chain
CRF for JESS-CM. As regards the design of the fea-
ture functions fi, Table 3 shows the feature tem-
plates used in our experiments. In the table, s indi-
cates a focused token position. Xs_1.s represents the
bi-gram of feature X obtained from s − 1 and s po-
sitions. {Xu}Bu�A indicates that u ranges from A to
B. For example, {Xu}s+2
u�s_2 is equal to five feature
templates, {Xs_2i Xs_1i Xsi Xs+1i Xs+2}. ‘word
type’ or wtp represents features of a word such as
capitalization, the existence of digits, and punctua-
tion as shown in (Sutton et al., 2006) without regular
expressions. Although it is common to use external
</bodyText>
<page confidence="0.958367">
668
</page>
<figure confidence="0.6163385">
Entire entence accuracy
(a) POS tagging:(total 47 templates)
</figure>
<equation confidence="0.884821818181818">
[ys], [ys−1:s], {[ys, pf-Ns], [ys, sf-Ns]}9 N=1,
{[ys, wdu], [ys, wtpu], [ys−1:s, wtpu]}s+2
u=s−2,
{[ys, wdu−1:u], [ys, wtpu−1:u], [ys−1:s, wtpu−1:u]}s+2
u=s−1
(b) Syntactic chunking: (total 39 templates)
[ys], [ys−1:s], {[ys, wdu], [ys, posu], [ys, wdu, posu],
[ys−1:s, wdu], [ys−1:s, posu]}s+2
u=s−2, {[ys, wdu−1:u],
]}s+2
[ys, posu−1:u], {[ys
−1: s, posu−1:u u=s−1,
(c) NER: (total 79 templates)
[ys], [ys−1:s], {[ys,wdu], [ys,lwdu], [ys, posu], [ys,wtpu],
[ys−1:s, lwdu], [ys−1:s, posu], [ys−1:s, wtpu]}s+2
u=s−2,
{[ys, lwdu−1:u], [ys, posu−1:u], [ys, wtpu−1:u],
[ys−1:s, posu−1:u], [ys−1:s, wtpu−1:u]}s+2
u=s−1,
[ys, poss−1:s:s+1], [ys, wtps−1:s:s+1], [ys−1:s, poss−1:s:s+1],
[ys−1:s, wtps−1:s:s+1], [ys, wd4ls], [ys, wd4rs],
{[ys, pf-Ns], [ys, sf-Ns], [ys−1:s, pf-Ns], [ys−1:s, sf-Ns]}4 N=1
</equation>
<tableCaption confidence="0.743323">
wd: word, pos: part-of-speech lwd : lowercase of word,
wtp: ‘word type’, wd4{l,r}: words within the left or right 4 tokens
{pf,sf}-N: N character prefix or suffix of word
Table 3: Feature templates used in our experiments
</tableCaption>
<figure confidence="0.97947">

o�,


o�o�

o�oo1
o�ooo�
�� � � �� o s m u ao
    Yofi .U_
(a) Influence of 77 (b) Changes in performance
in Dirichlet prior and convergence property
</figure>
<figureCaption confidence="0.999204">
Figure 2: Typical behavior of tunable parameters
</figureCaption>
<bodyText confidence="0.871234666666667">
resources such as gazetteers for NER, we used none.
All our features can be automatically extracted from
the given training data.
</bodyText>
<subsectionHeader confidence="0.998861">
3.3 Design of Joint PMs (HMMs)
</subsectionHeader>
<bodyText confidence="0.999872928571429">
We used first order HMMs for embedded joint PMs
since we assume that they have the same graph struc-
ture as JESS-CM as described in Section 2.2.
To reduce the required human effort, we simply
used the feature templates shown in Table 3 to gener-
ate the features of the HMMs. With our design, one
feature template corresponded to one HMM. This
design preserves the feature whereby each HMM
emits a single symbol from a single state (or transi-
tion). We can easily ignore overlapping features that
appear in a single HMM. As a result, 47, 39 and 79
distinct HMMs are embedded in the potential func-
tions of JESS-CM for POS tagging, chunking and
NER experiments, respectively.
</bodyText>
<subsectionHeader confidence="0.936417">
3.4 Tunable Parameters
</subsectionHeader>
<bodyText confidence="0.999972941176471">
In our experiments, we selected Gaussian and
Dirichlet priors as the prior distributions in G1 and
G2, respectively. This means that JESS-CM has two
tunable parameters, Q2 and q, in the Gaussian and
Dirichlet priors, respectively. The values of these
tunable parameters are chosen by employing a bi-
nary line search. We used the value for the best per-
formance with the development set2. However, it
may be computationally unrealistic to retrain the en-
tire procedure several times using 1G-words of unla-
beled data. Therefore, these tunable parameter val-
ues are selected using a relatively small amount of
unlabeled data (17M-words), and we used the se-
lected values in all our experiments. The left graph
in Figure 2 shows typical q behavior. The left end
is equivalent to optimizing G2 without a prior, and
the right end is almost equivalent to considering
pj(xj, y) for all j to be a uniform distribution. This
is why it appears to be bounded by the performance
obtained from supervised CRF. We omitted the in-
fluence of Q2 because of space constraints, but its be-
havior is nearly the same as that of supervised CRF.
Unfortunately, G2(O|A0) may have two or more
local maxima. Our parameter estimation procedure
does not guarantee to provide either the global opti-
mum or a convergence solution in O and A0 space.
An example of non-convergence is the oscillation of
the estimated O. That is, O traverses two or more
local maxima. Therefore, we examined its con-
vergence property experimentally. The right graph
in Figure 2 shows a typical convergence property.
Fortunately, in all our experiments, JESS-CM con-
verged in a small number of iterations. No oscilla-
tion is observed here.
</bodyText>
<sectionHeader confidence="0.999906" genericHeader="method">
4 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.999874">
4.1 Impact of Unlabeled Data Size
</subsectionHeader>
<bodyText confidence="0.999944333333333">
Table 4 shows the performance of JESS-CM us-
ing 1G-words of unlabeled data and the perfor-
mance gain compared with supervised CRF, which
is trained under the same conditions as JESS-CM ex-
cept that joint PMs are not incorporated. We empha-
size that our model achieved these large improve-
ments solely using unlabeled data as additional re-
sources, without introducing a sophisticated model,
deep feature engineering, handling external hand-
</bodyText>
<footnote confidence="0.99273875">
2Since CoNLL’00 shared task data has no development set,
we divided the labeled training data into two distinct sets, 4/5
for training and the remainder for the development set, and de-
termined the tunable parameters in preliminary experiments.
</footnote>
<equation confidence="0.507567833333333">

 
 
Entire sentence accuracy
o��1
o�s9
o�s1
� ����� � ������� � ��������
� �������
� ���������� � ����
w  
,
</equation>
<page confidence="0.889539">
669
</page>
<table confidence="0.9491802">
(a) POS tagging (b) Chunking (c) NER
measures label accuracy entire sent. acc. Fa=1 sent. acc. Fa=1 entire sent. acc.
eval. data dev. test dev. test test test dev. test dev. test
JESS-CM (CRF/HMM) 97.35 97.40 56.34 57.01 95.15 65.06 94.48 89.92 91.17 85.12
(gain from supervised CRF) (+0.17) (+0.19) (+1.90) (+1.63) (+1.27) (+4.92) (+2.74) (+3.57) (+3.46) (+3.96)
</table>
<tableCaption confidence="0.988603">
Table 4: Results for POS tagging (PTB III data), syntactic chunking (CoNLL’00 data), and NER (CoNLL’03 data)
incorporated with 1G-words of unlabeled data, and the performance gain from supervised CRF
</tableCaption>
<figure confidence="0.991789">
(a) POS tagging (b) Syntactic chunking (c) NER
</figure>
<figureCaption confidence="0.982148">
Figure 3: Performance changes with respect to unlabeled data size in JESS-CM
</figureCaption>
<figure confidence="0.9982906">
 
�� ��o ��oo ��ooo
    
test
dev.
Supervimd CRF
perfar -e(t 0
F—measure
(devJ
     
Unlabeled data size (Mega words) : [log—scale]
    
     
�


 
     0 
Unlabeled data size (Mega words) : rlog—scale]
    
      
dev.
test


  



  

     
Unlabeled data size (Mega words): dog—scale]






Label Accuracy





F—measure




</figure>
<bodyText confidence="0.998552022222223">
crafted resources, or task dependent human knowl-
edge (except for the feature design). Our method can
greatly reduce the human effort needed to obtain a
high performance tagger or chunker.
Figure 3 shows the learning curves of JESS-CM
with respect to the size of the unlabeled data, where
the x-axis is on the logarithmic scale of the unla-
beled data size (Mega-word). The scale at the top
of the graph shows the ratio of the unlabeled data
size to the labeled data size. We observe that a small
amount of unlabeled data hardly improved the per-
formance since the supervised CRF results are com-
petitive. It seems that we require at least dozens
of times more unlabeled data than labeled training
data to provide a significant performance improve-
ment. The most important and interesting behav-
ior is that the performance improvements against the
unlabeled data size are almost linear on a logarith-
mic scale within the size of the unlabeled data used
in our experiments. Moreover, there is a possibil-
ity that the performance is still unsaturated at the
1G-word unlabeled data point. This suggests that
increasing the unlabeled data in JESS-CM may fur-
ther improve the performance.
Suppose J=1, the discriminant function of JESS-
CM is g(x, y) = A(x, y)p1(x1, y; 01)λI+1 where
A(x, y) = exp(A · &amp; fc(yc, x)). Note that both
A(x, y) and AI+j are given and fixed during the
MDF estimation of joint PM parameters O. There-
fore, the MDF estimation in JESS-CM can be re-
garded as a variant of the MML estimation (see Sec-
tion 2.2), namely, it is MML estimation with a bias,
A(x, y), and smooth factors, AI+j. MML estima-
tion can be seen as modeling p(x) since it is equiv-
alent to maximizing Em log p(xm) with marginal-
ized hidden variables y, where EYEY p(x, y) =
p(x). Generally, more data will lead to a more ac-
curate model of p(x). With our method, as with
modeling p(x) in MML estimation, more unlabeled
data is preferable since it may provide more accurate
modeling. This also means that it provides better
‘clusters’ over the output space since Y is used as
hidden states in HMMs. These are intuitive expla-
nations as to why more unlabeled data in JESS-CM
produces better performance.
</bodyText>
<subsectionHeader confidence="0.995177">
4.2 Expected Performance for Unseen Data
</subsectionHeader>
<bodyText confidence="0.999848153846154">
We try to investigate the impact of unlabeled data
on the performance of unseen data. We divide the
test set (or the development set) into two disjoint
sets: L.app and L.neg app. L.app is a set of sen-
tences constructed by words that all appeared in the
Labeled training data. L.-,app is a set of sentences
that have at least one word that does not appear in
the Labeled training data.
Table 5 shows the performance with these two
sets obtained from both supervised CRF and JESS-
CM with 1G-word unlabeled data. As the super-
vised CRF results, the performance of the L.-,app
sets is consistently much lower than that of the cor-
</bodyText>
<page confidence="0.993757">
670
</page>
<table confidence="0.99340775">
(a) POS tagging (b) Chunking (c) NER
eval. data development test test development test
L.-app L.app L.-app L.app L.-app L.app L.-app L.app L.-app L.app
rates of sentences (46.1%) (53.9%) (40.4%) (59.6%) (70.7%) (29.3%) (54.3%) (45.7%) (64.3%) (35.7%)
supervised CRF (baseline) 46.78 60.99 48.57 60.01 56.92 67.91 79.60 97.35 75.69 91.03
JESS-CM (CRF/HMM) 49.02 62.60 50.79 61.24 62.47 71.30 85.87 97.47 80.84 92.85
(gain from supervised CRF) (+2.24) (+1.61) (+2.22) (+1.23) (+5.55) (+3.40) (+6.27) (+0.12) (+5.15) (+1.82)
U.app 83.7% 96.3% 84.3% 95.8% 89.5% 99.2% 95.3% 99.8% 94.9% 100.0%
</table>
<tableCaption confidence="0.989463">
Table 5: Comparison with L.¬app and L.app sets obtained from both supervised CRF and JESS-CM with 1G-word
unlabeled data evaluated by the entire sentence accuracies, and the ratio of U.app.
</tableCaption>
<table confidence="0.997777833333333">
unlab. data dev (Aug. 30-31) test (Dec. 06-07)
(period) #sent. #wds Fa=1 U.app Fa=1 U.app
reu(Sep.) 1.0M 17M 93.50 82.0% 88.27 69.7%
reu(Oct.) 1.3M 20M 93.04 71.0% 88.82 72.0%
reu(Nov.) 1.2M 18M 92.94 68.7% 89.08 74.3%
reu(Dec.)* 9M 15M 92.91 67.0% 89.29 84.4%
</table>
<tableCaption confidence="0.9921695">
Table 6: Influence of U.app in NER experiments: *(ex-
cluding Dec. 06-07)
</tableCaption>
<bodyText confidence="0.983681322580645">
responding L.app sets. Moreover, we can observe
that the ratios of L.¬app are not so small; nearly half
(46.1% and 40.4%) in the PTB III data, and more
than half (70.7%, 54.3% and 64.3%) in CoNLL’00
and ’03 data, respectively. This indicates that words
not appearing in the labeled training data are really
harmful for supervised learning. Although the per-
formance with L.¬app sets is still poorer than with
L.app sets, the JESS-CM results indicate that the in-
troduction of unlabeled data effectively improves the
performance of L.¬app sets, even more than that of
L.app sets. These improvements are essentially very
important; when a tagger and chunker are actually
used, input data can be obtained from anywhere and
this may mostly include words that do not appear
in the given labeled training data since the labeled
training data is limited and difficult to increase. This
means that the improved performance of L.¬app can
link directly to actual use.
Table 5 also shows the ratios of sentences that
are constructed from words that all appeared in the
1G-word Unlabeled data used in our experiments
(U.app) in the L.¬app and L.app. This indicates that
most of the words in the development or test sets are
covered by the 1G-word unlabeled data. This may
be the main reason for JESS-CM providing large
performance gains for both the overall and L.¬app
set performance of all three tasks.
Table 6 shows the relation between JESS-CM per-
formance and U.app in the NER experiments. The
development data and test data were obtained from
</bodyText>
<table confidence="0.9976814">
system dev. test additional resources
JESS-CM (CRF/HMM) 97.35 97.40 1G-word unlabeled data
(Shen et al., 2007) 97.28 97.33 –
(Toutanova et al., 2003) 97.15 97.24 crude company name detector
[sup. CRF (baseline)] 97.18 97.21 –
</table>
<tableCaption confidence="0.834258">
Table 7: POS tagging results of the previous top systems
for PTB III data evaluated by label accuracy
</tableCaption>
<table confidence="0.999869125">
system test additional resources
JESS-CM (CRF/HMM) 95.15 1G-word unlabeled data
94.67 15M-word unlabeled data
(Ando and Zhang, 2005) 94.39 15M-word unlabeled data
(Suzuki et al., 2007) 94.36 17M-word unlabeled data
(Zhang et al., 2002) 94.17 full parser output
(Kudo and Matsumoto, 2001) 93.91 –
[supervised CRF (baseline)] 93.88 –
</table>
<tableCaption confidence="0.998913">
Table 8: Syntactic chunking results of the previous top
systems for CoNLL’00 shared task data (F0=1 score)
</tableCaption>
<bodyText confidence="0.999061416666667">
30-31 Aug. 1996 and 6-7 Dec. 1996 Reuters news
articles, respectively. We find that temporal proxim-
ity leads to better performance. This aspect can also
be explained as U.app. Basically, the U.app increase
leads to improved performance.
The evidence provided by the above experiments
implies that increasing the coverage of unlabeled
data offers the strong possibility of increasing the
expected performance of unseen data. Thus, it
strongly encourages us to use an SSL approach that
includes JESS-CM to construct a general tagger and
chunker for actual use.
</bodyText>
<sectionHeader confidence="0.999495" genericHeader="method">
5 Comparison with Previous Top Systems
and Related Work
</sectionHeader>
<bodyText confidence="0.999400375">
In POS tagging, the previous best performance was
reported by (Shen et al., 2007) as summarized in
Table 7. Their method uses a novel sophisticated
model that learns both decoding order and labeling,
while our model uses a standard first order Markov
model. Despite using such a simple model, our
method can provide a better result with the help of
unlabeled data.
</bodyText>
<page confidence="0.99567">
671
</page>
<table confidence="0.992924125">
system dev. test additional resources
JESS-CM (CRF/HMM) 94.48 89.92 1G-word unlabeled data
93.66 89.36 37M-word unlabeled data
(Ando and Zhang, 2005) 93.15 89.31 27M-word unlabeled data
(Florian et al., 2003) 93.87 88.76 own large gazetteers,
2M-word labeled data
(Suzuki et al., 2007) N/A 88.41 27M-word unlabeled data
[sup. CRF (baseline)] 91.74 86.35 –
</table>
<tableCaption confidence="0.9888575">
Table 9: NER results of the previous top systems for
CoNLL’03 shared task data evaluated by F0=1 score
</tableCaption>
<bodyText confidence="0.999965106060606">
As shown in Tables 8 and 9, the previous best
performance for syntactic chunking and NER was
reported by (Ando and Zhang, 2005), and is re-
ferred to as ‘ASO-semi’. ASO-semi also incorpo-
rates unlabeled data solely as additional informa-
tion in the same way as JESS-CM. ASO-semi uses
unlabeled data for constructing auxiliary problems
that are expected to capture a good feature repre-
sentation of the target problem. As regards syntac-
tic chunking, JESS-CM significantly outperformed
ASO-semi for the same 15M-word unlabeled data
size obtained from the Wall Street Journal in 1991
as described in (Ando and Zhang, 2005). Unfor-
tunately with NER, JESS-CM is slightly inferior to
ASO-semi for the same 27M-word unlabeled data
size extracted from the Reuters corpus. In fact,
JESS-CM using 37M-words of unlabeled data pro-
vided a comparable result. We observed that ASO-
semi prefers ‘nugget extraction’ tasks to ’field seg-
mentation’ tasks (Grenager et al., 2005). We can-
not provide details here owing to the space limi-
tation. Intuitively, their word prediction auxiliary
problems can capture only a limited number of char-
acteristic behaviors because the auxiliary problems
are constructed by a limited number of ‘binary’ clas-
sifiers. Moreover, we should remember that ASO-
semi used the human knowledge that ‘named en-
tities mostly consist of nouns or adjectives’ during
the auxiliary problem construction in their NER ex-
periments. In contrast, our results require no such
additional knowledge or limitation. In addition, the
design and training of auxiliary problems as well as
calculating SVD are too costly when the size of the
unlabeled data increases. These facts imply that our
SSL framework is rather appropriate for handling
large scale unlabeled data.
On the other hand, ASO-semi and JESS-CM have
an important common feature. That is, both meth-
ods discriminatively combine models trained by us-
ing unlabeled data in order to create informative fea-
ture representation for discriminative learning. Un-
like self/co-training approaches (Blum and Mitchell,
1998), which use estimated labels as ‘correct la-
bels’, this approach automatically judges the relia-
bility of additional features obtained from unlabeled
data in terms of discriminative training. Ando and
Zhang (2007) have also pointed out that this method-
ology seems to be one key to achieving higher per-
formance in NLP applications.
There is an approach that combines individually
and independently trained joint PMs into a discrimi-
native model (Li and McCallum, 2005). There is an
essential difference between this method and JESS-
CM. We categorize their approach as an ‘indirect
approach’ since the outputs of the target task, y,
are not considered during the unlabeled data incor-
poration. Note that ASO-semi is also an ‘indirect
approach’. On the other hand, our approach is a
‘direct approach’ because the distribution of y ob-
tained from JESS-CM is used as ‘seeds’ of hidden
states during MDF estimation for join PM param-
eters (see Section 4.1). In addition, MDF estima-
tion over unlabeled data can effectively incorporate
the ‘labeled’ training data information via a ‘bias’
since A included in A(x, y) is estimated from la-
beled training data.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999991058823529">
We proposed a simple yet powerful semi-supervised
conditional model, which we call JESS-CM. It is
applicable to large amounts of unlabeled data, for
example, at the giga-word level. Experimental re-
sults obtained by using JESS-CM incorporating 1G-
words of unlabeled data have provided the current
best performance as regards POS tagging, syntactic
chunking, and NER for widely used large test col-
lections such as PTB III, CoNLL’00 and ’03 shared
task data, respectively. We also provided evidence
that the use of more unlabeled data in SSL can lead
to further improvements. Moreover, our experimen-
tal analysis revealed that it may also induce an im-
provement in the expected performance for unseen
data in terms of the unlabeled data coverage. Our re-
sults may encourage the adoption of the SSL method
for many other real world applications.
</bodyText>
<page confidence="0.998072">
672
</page>
<sectionHeader confidence="0.99388" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999941923076923">
R. Ando and T. Zhang. 2005. A High-Performance
Semi-Supervised Learning Method for Text Chunking.
In Proc. ofACL-2005, pages 1–9.
R. Ando and T. Zhang. 2007. Two-view Feature Genera-
tion Model for Semi-supervised Learning. In Proc. of
ICML-2007, pages 25–32.
A. Blum and T. Mitchell. 1998. Combining Labeled and
Unlabeled Data with Co-Training. In Conference on
Computational Learning Theory 11.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical Soci-
ety, Series B, 39:1–38.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named Entity Recognition through Classifier Combi-
nation. In Proc. of CoNLL-2003, pages 168–171.
T. Grenager, D. Klein, and C. Manning. 2005. Unsu-
pervised Learning of Field Segmentation Models for
Information Extraction. In Proc. of ACL-2005, pages
371–378.
T. Kudo and Y. Matsumoto. 2001. Chunking with Sup-
port Vector Machines. In Proc. ofNAACL 2001, pages
192–199.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
ICML-2001, pages 282–289.
W. Li and A. McCallum. 2005. Semi-Supervised Se-
quence Modeling with Syntactic Topic Models. In
Proc. ofAAAI-2005, pages 813–818.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313–330.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000. Text Classification from Labeled and Unlabeled
Documents using EM. Machine Learning, 39:103–
134.
F. Sha and F. Pereira. 2003. Shallow Parsing with Condi-
tional Random Fields. In Proc. of HLT/NAACL-2003,
pages 213–220.
L. Shen, G. Satta, and A. Joshi. 2007. Guided Learning
for Bidirectional Sequence Classification. In Proc. of
ACL-2007, pages 760–767.
C. Sutton, M. Sindelar, and A. McCallum. 2006. Reduc-
ing Weight Undertraining in Structured Discriminative
Learning. In Proc. ofHTL-NAACL 2006, pages 89–95.
J Suzuki, A Fujino, and H Isozaki. 2007. Semi-
Supervised Structured Output Learning Based on a
Hybrid Generative and Discriminative Approach. In
Proc. of EMNLP-CoNLL, pages 791–800.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduc-
tion to the CoNLL-2000 Shared Task: Chunking. In
Proc. of CoNLL-2000 and LLL-2000, pages 127–132.
E. T. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proc. of
CoNLL-2003, pages 142–147.
K. Toutanova, D. Klein, C.D. Manning, and
Y. Yoram Singer. 2003. Feature-rich Part-of-
speech Tagging with a Cyclic Dependency Network.
In Proc. ofHLT-NAACL-2003, pages 252–259.
T. Zhang, F. Damerau, and D. Johnson. 2002. Text
Chunking based on a Generalization of Winnow. Ma-
chine Learning Research, 2:615–637.
</reference>
<page confidence="0.999275">
673
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.892423">
<title confidence="0.9953005">Semi-Supervised Sequential Labeling and Segmentation using Giga-word Scale Unlabeled Data</title>
<author confidence="0.994386">Jun Suzuki</author>
<author confidence="0.994386">Hideki Isozaki</author>
<affiliation confidence="0.991816">NTT Communication Science Laboratories, NTT Corp.</affiliation>
<address confidence="0.929626">2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan</address>
<abstract confidence="0.9986804">This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition. We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data. Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL’00 and ’03 shared task data for the above three NLP tasks, respectively. We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement. In addition, our results are superior to the best reported results for all of the above test collections.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Ando</author>
<author>T Zhang</author>
</authors>
<title>A High-Performance Semi-Supervised Learning Method for Text Chunking. In</title>
<date>2005</date>
<booktitle>Proc. ofACL-2005,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="2227" citStr="Ando and Zhang (2005)" startWordPosition="327" endWordPosition="330">n traditional and important NLP tasks, namely part-of-speech (POS) tagging, syntactic chunking, and named entity recognition (NER). These are also typical supervised learning applications in NLP, and are referred to as sequential labeling and segmentation problems. In some cases, these tasks have relatively large amounts of labeled training data. In this situation, supervised learning can provide competitive results, and it is difficult to improve them any further by using SSL. In fact, few papers have succeeded in showing significantly better results than state-of-theart supervised learning. Ando and Zhang (2005) reported a substantial performance improvement compared with state-of-the-art supervised learning results for syntactic chunking with the CoNLL’00 shared task data (Tjong Kim Sang and Buchholz, 2000) and NER with the CoNLL’03 shared task data (Tjong Kim Sang and Meulder, 2003). One remaining question is the behavior of SSL when using as much labeled and unlabeled data as possible. This paper investigates this question, namely, the use of a large amount of unlabeled data in the presence of (fixed) large labeled data. To achieve this, it is paramount to make the SSL method scalable with regard </context>
<context position="28429" citStr="Ando and Zhang, 2005" startWordPosition="4742" endWordPosition="4745"> all three tasks. Table 6 shows the relation between JESS-CM performance and U.app in the NER experiments. The development data and test data were obtained from system dev. test additional resources JESS-CM (CRF/HMM) 97.35 97.40 1G-word unlabeled data (Shen et al., 2007) 97.28 97.33 – (Toutanova et al., 2003) 97.15 97.24 crude company name detector [sup. CRF (baseline)] 97.18 97.21 – Table 7: POS tagging results of the previous top systems for PTB III data evaluated by label accuracy system test additional resources JESS-CM (CRF/HMM) 95.15 1G-word unlabeled data 94.67 15M-word unlabeled data (Ando and Zhang, 2005) 94.39 15M-word unlabeled data (Suzuki et al., 2007) 94.36 17M-word unlabeled data (Zhang et al., 2002) 94.17 full parser output (Kudo and Matsumoto, 2001) 93.91 – [supervised CRF (baseline)] 93.88 – Table 8: Syntactic chunking results of the previous top systems for CoNLL’00 shared task data (F0=1 score) 30-31 Aug. 1996 and 6-7 Dec. 1996 Reuters news articles, respectively. We find that temporal proximity leads to better performance. This aspect can also be explained as U.app. Basically, the U.app increase leads to improved performance. The evidence provided by the above experiments implies t</context>
<context position="29869" citStr="Ando and Zhang, 2005" startWordPosition="4970" endWordPosition="4973">t a general tagger and chunker for actual use. 5 Comparison with Previous Top Systems and Related Work In POS tagging, the previous best performance was reported by (Shen et al., 2007) as summarized in Table 7. Their method uses a novel sophisticated model that learns both decoding order and labeling, while our model uses a standard first order Markov model. Despite using such a simple model, our method can provide a better result with the help of unlabeled data. 671 system dev. test additional resources JESS-CM (CRF/HMM) 94.48 89.92 1G-word unlabeled data 93.66 89.36 37M-word unlabeled data (Ando and Zhang, 2005) 93.15 89.31 27M-word unlabeled data (Florian et al., 2003) 93.87 88.76 own large gazetteers, 2M-word labeled data (Suzuki et al., 2007) N/A 88.41 27M-word unlabeled data [sup. CRF (baseline)] 91.74 86.35 – Table 9: NER results of the previous top systems for CoNLL’03 shared task data evaluated by F0=1 score As shown in Tables 8 and 9, the previous best performance for syntactic chunking and NER was reported by (Ando and Zhang, 2005), and is referred to as ‘ASO-semi’. ASO-semi also incorporates unlabeled data solely as additional information in the same way as JESS-CM. ASO-semi uses unlabeled </context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>R. Ando and T. Zhang. 2005. A High-Performance Semi-Supervised Learning Method for Text Chunking. In Proc. ofACL-2005, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ando</author>
<author>T Zhang</author>
</authors>
<title>Two-view Feature Generation Model for Semi-supervised Learning.</title>
<date>2007</date>
<booktitle>In Proc. of ICML-2007,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="32440" citStr="Ando and Zhang (2007)" startWordPosition="5374" endWordPosition="5377">s. These facts imply that our SSL framework is rather appropriate for handling large scale unlabeled data. On the other hand, ASO-semi and JESS-CM have an important common feature. That is, both methods discriminatively combine models trained by using unlabeled data in order to create informative feature representation for discriminative learning. Unlike self/co-training approaches (Blum and Mitchell, 1998), which use estimated labels as ‘correct labels’, this approach automatically judges the reliability of additional features obtained from unlabeled data in terms of discriminative training. Ando and Zhang (2007) have also pointed out that this methodology seems to be one key to achieving higher performance in NLP applications. There is an approach that combines individually and independently trained joint PMs into a discriminative model (Li and McCallum, 2005). There is an essential difference between this method and JESSCM. We categorize their approach as an ‘indirect approach’ since the outputs of the target task, y, are not considered during the unlabeled data incorporation. Note that ASO-semi is also an ‘indirect approach’. On the other hand, our approach is a ‘direct approach’ because the distri</context>
</contexts>
<marker>Ando, Zhang, 2007</marker>
<rawString>R. Ando and T. Zhang. 2007. Two-view Feature Generation Model for Semi-supervised Learning. In Proc. of ICML-2007, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>In Conference on Computational Learning Theory</booktitle>
<volume>11</volume>
<contexts>
<context position="32229" citStr="Blum and Mitchell, 1998" startWordPosition="5343" endWordPosition="5346">ast, our results require no such additional knowledge or limitation. In addition, the design and training of auxiliary problems as well as calculating SVD are too costly when the size of the unlabeled data increases. These facts imply that our SSL framework is rather appropriate for handling large scale unlabeled data. On the other hand, ASO-semi and JESS-CM have an important common feature. That is, both methods discriminatively combine models trained by using unlabeled data in order to create informative feature representation for discriminative learning. Unlike self/co-training approaches (Blum and Mitchell, 1998), which use estimated labels as ‘correct labels’, this approach automatically judges the reliability of additional features obtained from unlabeled data in terms of discriminative training. Ando and Zhang (2007) have also pointed out that this methodology seems to be one key to achieving higher performance in NLP applications. There is an approach that combines individually and independently trained joint PMs into a discriminative model (Li and McCallum, 2005). There is an essential difference between this method and JESSCM. We categorize their approach as an ‘indirect approach’ since the outp</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining Labeled and Unlabeled Data with Co-Training. In Conference on Computational Learning Theory 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood from Incomplete Data via the EM Algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<pages>39--1</pages>
<contexts>
<context position="9398" citStr="Dempster et al., 1977" startWordPosition="1583" endWordPosition="1587">es p(x, y) with discriminant functions g(x, y). Therefore, to estimate the parameter Θ of JESS-CM by using MDF estimation, the following objective function is maximized with a fixed λ0: � � L2(Θ|λ&apos;) = log m yEY where p(Θ) is a prior probability distribution of Θ. Since the normalization factor does not affect the determination of y, the discriminant function of JESS-CM shown in Equation 2 is defined as g(x, y; λ0, Θ) = Hc∈C Ψ0 c(yc, x; λ0, Θ). With a fixed λ0, the local maximum of L2(Θ|λ0) around the initialized value of Θ can be estimated by an iterative computation such as the EM algorithm (Dempster et al., 1977). 2.3 Scalability: Efficient Training Algorithm A parameter estimation algorithm of λ0 and Θ can be obtained by maximizing the objective functions L1(λ0|Θ) and L2(Θ|λ0) iteratively and alternately. Figure 1 summarizes an algorithm for estimating λ0 and Θ for JESS-CM. This paper considers a situation where there are many more unlabeled data M than labeled data N, that is, N &lt;&lt; M. This means that the calculation cost for unlabeled data is dominant. Thus, in order to make the overall parameter estimation procedure Input: training data D = {Dl, Du} where labeled data Dl = {(xn, yn)}Nn=1, and unlab</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, Series B, 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>T Zhang</author>
</authors>
<title>Named Entity Recognition through Classifier Combination.</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL-2003,</booktitle>
<pages>168--171</pages>
<contexts>
<context position="29928" citStr="Florian et al., 2003" startWordPosition="4979" endWordPosition="4982"> with Previous Top Systems and Related Work In POS tagging, the previous best performance was reported by (Shen et al., 2007) as summarized in Table 7. Their method uses a novel sophisticated model that learns both decoding order and labeling, while our model uses a standard first order Markov model. Despite using such a simple model, our method can provide a better result with the help of unlabeled data. 671 system dev. test additional resources JESS-CM (CRF/HMM) 94.48 89.92 1G-word unlabeled data 93.66 89.36 37M-word unlabeled data (Ando and Zhang, 2005) 93.15 89.31 27M-word unlabeled data (Florian et al., 2003) 93.87 88.76 own large gazetteers, 2M-word labeled data (Suzuki et al., 2007) N/A 88.41 27M-word unlabeled data [sup. CRF (baseline)] 91.74 86.35 – Table 9: NER results of the previous top systems for CoNLL’03 shared task data evaluated by F0=1 score As shown in Tables 8 and 9, the previous best performance for syntactic chunking and NER was reported by (Ando and Zhang, 2005), and is referred to as ‘ASO-semi’. ASO-semi also incorporates unlabeled data solely as additional information in the same way as JESS-CM. ASO-semi uses unlabeled data for constructing auxiliary problems that are expected </context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003. Named Entity Recognition through Classifier Combination. In Proc. of CoNLL-2003, pages 168–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Grenager</author>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Unsupervised Learning of Field Segmentation Models for Information Extraction.</title>
<date>2005</date>
<booktitle>In Proc. of ACL-2005,</booktitle>
<pages>371--378</pages>
<contexts>
<context position="31130" citStr="Grenager et al., 2005" startWordPosition="5173" endWordPosition="5176">s that are expected to capture a good feature representation of the target problem. As regards syntactic chunking, JESS-CM significantly outperformed ASO-semi for the same 15M-word unlabeled data size obtained from the Wall Street Journal in 1991 as described in (Ando and Zhang, 2005). Unfortunately with NER, JESS-CM is slightly inferior to ASO-semi for the same 27M-word unlabeled data size extracted from the Reuters corpus. In fact, JESS-CM using 37M-words of unlabeled data provided a comparable result. We observed that ASOsemi prefers ‘nugget extraction’ tasks to ’field segmentation’ tasks (Grenager et al., 2005). We cannot provide details here owing to the space limitation. Intuitively, their word prediction auxiliary problems can capture only a limited number of characteristic behaviors because the auxiliary problems are constructed by a limited number of ‘binary’ classifiers. Moreover, we should remember that ASOsemi used the human knowledge that ‘named entities mostly consist of nouns or adjectives’ during the auxiliary problem construction in their NER experiments. In contrast, our results require no such additional knowledge or limitation. In addition, the design and training of auxiliary proble</context>
</contexts>
<marker>Grenager, Klein, Manning, 2005</marker>
<rawString>T. Grenager, D. Klein, and C. Manning. 2005. Unsupervised Learning of Field Segmentation Models for Information Extraction. In Proc. of ACL-2005, pages 371–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Chunking with Support Vector Machines.</title>
<date>2001</date>
<booktitle>In Proc. ofNAACL</booktitle>
<pages>192--199</pages>
<contexts>
<context position="28584" citStr="Kudo and Matsumoto, 2001" startWordPosition="4766" endWordPosition="4769">ined from system dev. test additional resources JESS-CM (CRF/HMM) 97.35 97.40 1G-word unlabeled data (Shen et al., 2007) 97.28 97.33 – (Toutanova et al., 2003) 97.15 97.24 crude company name detector [sup. CRF (baseline)] 97.18 97.21 – Table 7: POS tagging results of the previous top systems for PTB III data evaluated by label accuracy system test additional resources JESS-CM (CRF/HMM) 95.15 1G-word unlabeled data 94.67 15M-word unlabeled data (Ando and Zhang, 2005) 94.39 15M-word unlabeled data (Suzuki et al., 2007) 94.36 17M-word unlabeled data (Zhang et al., 2002) 94.17 full parser output (Kudo and Matsumoto, 2001) 93.91 – [supervised CRF (baseline)] 93.88 – Table 8: Syntactic chunking results of the previous top systems for CoNLL’00 shared task data (F0=1 score) 30-31 Aug. 1996 and 6-7 Dec. 1996 Reuters news articles, respectively. We find that temporal proximity leads to better performance. This aspect can also be explained as U.app. Basically, the U.app increase leads to improved performance. The evidence provided by the above experiments implies that increasing the coverage of unlabeled data offers the strong possibility of increasing the expected performance of unseen data. Thus, it strongly encour</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>T. Kudo and Y. Matsumoto. 2001. Chunking with Support Vector Machines. In Proc. ofNAACL 2001, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML-2001,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="4150" citStr="Lafferty et al., 2001" startWordPosition="637" endWordPosition="640">ent a simple, scalable, but powerful task-independent model for semi-supervised sequential labeling and segmentation. Second, we report the best current results for the widely used test 665 Proceedings of ACL-08: HLT, pages 665–673, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics collections described above. Third, we confirm that the use of more unlabeled data in SSL can really lead to further improvements. 2 Conditional Model for SSL We design our model for SSL as a natural semisupervised extension of conventional supervised conditional random fields (CRFs) (Lafferty et al., 2001). As our approach for incorporating unlabeled data, we basically follow the idea proposed in (Suzuki et al., 2007). 2.1 Conventional Supervised CRFs Let x ∈ X and y ∈ Y be an input and output, where X and Y represent the set of possible inputs and outputs, respectively. C stands for the set of cliques in an undirected graphical model G(x, y), which indicates the interdependency of a given x and y. yc denotes the output from the corresponding clique c. Each clique c∈C has a potential function IFc. Then, the CRFs define the conditional probability p(y|x) as a product of IFcs. In addition, let f </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proc. of ICML-2001, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Li</author>
<author>A McCallum</author>
</authors>
<title>Semi-Supervised Sequence Modeling with Syntactic Topic Models.</title>
<date>2005</date>
<booktitle>In Proc. ofAAAI-2005,</booktitle>
<pages>813--818</pages>
<contexts>
<context position="32693" citStr="Li and McCallum, 2005" startWordPosition="5416" endWordPosition="5419">unlabeled data in order to create informative feature representation for discriminative learning. Unlike self/co-training approaches (Blum and Mitchell, 1998), which use estimated labels as ‘correct labels’, this approach automatically judges the reliability of additional features obtained from unlabeled data in terms of discriminative training. Ando and Zhang (2007) have also pointed out that this methodology seems to be one key to achieving higher performance in NLP applications. There is an approach that combines individually and independently trained joint PMs into a discriminative model (Li and McCallum, 2005). There is an essential difference between this method and JESSCM. We categorize their approach as an ‘indirect approach’ since the outputs of the target task, y, are not considered during the unlabeled data incorporation. Note that ASO-semi is also an ‘indirect approach’. On the other hand, our approach is a ‘direct approach’ because the distribution of y obtained from JESS-CM is used as ‘seeds’ of hidden states during MDF estimation for join PM parameters (see Section 4.1). In addition, MDF estimation over unlabeled data can effectively incorporate the ‘labeled’ training data information via</context>
</contexts>
<marker>Li, McCallum, 2005</marker>
<rawString>W. Li and A. McCallum. 2005. Semi-Supervised Sequence Modeling with Syntactic Topic Models. In Proc. ofAAAI-2005, pages 813–818.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="3016" citStr="Marcus et al., 1994" startWordPosition="460" endWordPosition="464">ng Kim Sang and Buchholz, 2000) and NER with the CoNLL’03 shared task data (Tjong Kim Sang and Meulder, 2003). One remaining question is the behavior of SSL when using as much labeled and unlabeled data as possible. This paper investigates this question, namely, the use of a large amount of unlabeled data in the presence of (fixed) large labeled data. To achieve this, it is paramount to make the SSL method scalable with regard to the size of unlabeled data. We first propose a scalable model for SSL. Then, we apply our model to widely used test collections, namely Penn Treebank (PTB) III data (Marcus et al., 1994) for POS tagging, CoNLL’00 shared task data for syntactic chunking, and CoNLL’03 shared task data for NER. We used up to 1G-words (one billion tokens) of unlabeled data to explore the performance improvement with respect to the unlabeled data size. In addition, we investigate the performance improvement for ‘unseen data’ from the viewpoint of unlabeled data coverage. Finally, we compare our results with those provided by the best current systems. The contributions of this paper are threefold. First, we present a simple, scalable, but powerful task-independent model for semi-supervised sequenti</context>
<context position="13368" citStr="Marcus et al., 1994" startWordPosition="2258" endWordPosition="2261">g the parameter of the conditional model while the combination weights, F, of the hybrid model are estimated solely by using 1/5 of the labeled training data. These facts indicate that JESS-CM has several advantageous characteristics compared with the hybrid model. 3 Experiments In our experiments, we report POS tagging, syntactic chunking and NER performance incorporating up to 1G-words of unlabeled data. 3.1 Data Set To compare the performance with that of previous studies, we selected widely used test collections. For our POS tagging experiments, we used the Wall Street Journal in PTB III (Marcus et al., 1994) with the same data split as used in (Shen et al., 2007). For our syntactic chunking and NER experiments, we used exactly the same training, development and test data as those provided for the shared tasks of CoNLL’00 (Tjong Kim Sang and Buchholz, 2000) and CoNLL’03 (Tjong Kim Sang and Meulder, 2003), respectively. The training, development and test data are detailed in Table 11 . The unlabeled data for our experiments was taken from the Reuters corpus, TIPSTER corpus (LDC93T3C) and the English Gigaword corpus, third edition (LDC2007T07). As regards the TIP1The second-order encoding used in ou</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1994. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A McCallum</author>
<author>S Thrun</author>
<author>T Mitchell</author>
</authors>
<title>Text Classification from Labeled and Unlabeled Documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--103</pages>
<contexts>
<context position="8282" citStr="Nigam et al., 2000" startWordPosition="1391" endWordPosition="1394">)¤+∇ log p(A&apos;). Thus, we can easily optimize L1 by using the forward-backward algorithm since this paper solely X L1(A&apos;|Θ) = n X− n 666 focuses on a sequence model and a gradient-based optimization algorithm in the same manner as those used in supervised CRF parameter estimation. We cannot naturally incorporate unlabeled data into standard discriminative learning methods since the correct outputs y for unlabeled data are unknown. On the other hand with a generative approach, a well-known way to achieve this incorporation is to use maximum marginal likelihood (MML) parameter estimation, i.e., (Nigam et al., 2000). Given unlabeled data Du = {xm}Mm=1, MML estimation in our setting maximizes the marginal distribution of a joint PM over a missing (hidden) variable y, namely, it maximizes Em log Ey∈Y p(xm, y; θ). Following this idea, there have been introduced a parameter estimation approach for non-generative approaches that can effectively incorporate unlabeled data (Suzuki et al., 2007). Here, we refer to it as ‘Maximum Discriminant Functions sum’ (MDF) parameter estimation. MDF estimation substitutes p(x, y) with discriminant functions g(x, y). Therefore, to estimate the parameter Θ of JESS-CM by using</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000. Text Classification from Labeled and Unlabeled Documents using EM. Machine Learning, 39:103– 134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow Parsing with Conditional Random Fields.</title>
<date>2003</date>
<booktitle>In Proc. of HLT/NAACL-2003,</booktitle>
<pages>213--220</pages>
<contexts>
<context position="7594" citStr="Sha and Pereira, 2003" startWordPosition="1282" endWordPosition="1285">ion of A0 under a fixed Θ can be written as: log P(yn|xn; A&apos;, Θ) + logp(A&apos;), where p(A0) is a prior probability distribution of A0. Clearly, JESS-CM shown in Equation 2 has exactly the same form as Equation 1. With a fixed Θ, the log-likelihood, log pj, can be seen simply as the feature functions of JESS-CM as with fi. Therefore, embedded joint PMs do not violate the global convergence conditions. As a result, as with supervised CRFs, it is guaranteed that A0 has a value that achieves the global maximum of L1(A0|Θ). Moreover, we can obtain the same form of gradient as that of supervised CRFs (Sha and Pereira, 2003), that is, ∇L1(A&apos;|Θ) = EP�(Y,X;a&apos;,d) £h(Y, X)¤ EP(Y|xn;a&apos;,©) £h(Y, xn)¤+∇ log p(A&apos;). Thus, we can easily optimize L1 by using the forward-backward algorithm since this paper solely X L1(A&apos;|Θ) = n X− n 666 focuses on a sequence model and a gradient-based optimization algorithm in the same manner as those used in supervised CRF parameter estimation. We cannot naturally incorporate unlabeled data into standard discriminative learning methods since the correct outputs y for unlabeled data are unknown. On the other hand with a generative approach, a well-known way to achieve this incorporation is t</context>
<context position="14042" citStr="Sha and Pereira, 2003" startWordPosition="2372" endWordPosition="2375">007). For our syntactic chunking and NER experiments, we used exactly the same training, development and test data as those provided for the shared tasks of CoNLL’00 (Tjong Kim Sang and Buchholz, 2000) and CoNLL’03 (Tjong Kim Sang and Meulder, 2003), respectively. The training, development and test data are detailed in Table 11 . The unlabeled data for our experiments was taken from the Reuters corpus, TIPSTER corpus (LDC93T3C) and the English Gigaword corpus, third edition (LDC2007T07). As regards the TIP1The second-order encoding used in our NER experiments is the same as that described in (Sha and Pereira, 2003) except removing IOB-tag of previous position label. (a) POS-tagging: (WSJ in PTB III) # of labels 45 Data set (WSJ sec. IDs) # of sent. # of words Training 0–18 38,219 912,344 Development 19–21 5,527 131,768 Test 22–24 5,462 129,654 (b) Chunking: (WSJ in PTB III: CoNLL’00 shared task data) # of labels 23 (w/ IOB-tagging) Data set (WSJ sec. IDs) # of sent. # of words Training 15–18 8,936 211,727 Development N/A N/A N/A Test 20 2,012 47,377 (c) NER: (Reuters Corpus: CoNLL’03 shared task data) # of labels 29 (w/ IOB-tagging+2nd-order encoding) Data set (time period) # of sent. # of words Trainin</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow Parsing with Conditional Random Fields. In Proc. of HLT/NAACL-2003, pages 213–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>G Satta</author>
<author>A Joshi</author>
</authors>
<title>Guided Learning for Bidirectional Sequence Classification.</title>
<date>2007</date>
<booktitle>In Proc. of ACL-2007,</booktitle>
<pages>760--767</pages>
<contexts>
<context position="13424" citStr="Shen et al., 2007" startWordPosition="2270" endWordPosition="2273">tion weights, F, of the hybrid model are estimated solely by using 1/5 of the labeled training data. These facts indicate that JESS-CM has several advantageous characteristics compared with the hybrid model. 3 Experiments In our experiments, we report POS tagging, syntactic chunking and NER performance incorporating up to 1G-words of unlabeled data. 3.1 Data Set To compare the performance with that of previous studies, we selected widely used test collections. For our POS tagging experiments, we used the Wall Street Journal in PTB III (Marcus et al., 1994) with the same data split as used in (Shen et al., 2007). For our syntactic chunking and NER experiments, we used exactly the same training, development and test data as those provided for the shared tasks of CoNLL’00 (Tjong Kim Sang and Buchholz, 2000) and CoNLL’03 (Tjong Kim Sang and Meulder, 2003), respectively. The training, development and test data are detailed in Table 11 . The unlabeled data for our experiments was taken from the Reuters corpus, TIPSTER corpus (LDC93T3C) and the English Gigaword corpus, third edition (LDC2007T07). As regards the TIP1The second-order encoding used in our NER experiments is the same as that described in (Sha </context>
<context position="28079" citStr="Shen et al., 2007" startWordPosition="4687" endWordPosition="4690">ords that all appeared in the 1G-word Unlabeled data used in our experiments (U.app) in the L.¬app and L.app. This indicates that most of the words in the development or test sets are covered by the 1G-word unlabeled data. This may be the main reason for JESS-CM providing large performance gains for both the overall and L.¬app set performance of all three tasks. Table 6 shows the relation between JESS-CM performance and U.app in the NER experiments. The development data and test data were obtained from system dev. test additional resources JESS-CM (CRF/HMM) 97.35 97.40 1G-word unlabeled data (Shen et al., 2007) 97.28 97.33 – (Toutanova et al., 2003) 97.15 97.24 crude company name detector [sup. CRF (baseline)] 97.18 97.21 – Table 7: POS tagging results of the previous top systems for PTB III data evaluated by label accuracy system test additional resources JESS-CM (CRF/HMM) 95.15 1G-word unlabeled data 94.67 15M-word unlabeled data (Ando and Zhang, 2005) 94.39 15M-word unlabeled data (Suzuki et al., 2007) 94.36 17M-word unlabeled data (Zhang et al., 2002) 94.17 full parser output (Kudo and Matsumoto, 2001) 93.91 – [supervised CRF (baseline)] 93.88 – Table 8: Syntactic chunking results of the previou</context>
<context position="29432" citStr="Shen et al., 2007" startWordPosition="4900" endWordPosition="4903"> that temporal proximity leads to better performance. This aspect can also be explained as U.app. Basically, the U.app increase leads to improved performance. The evidence provided by the above experiments implies that increasing the coverage of unlabeled data offers the strong possibility of increasing the expected performance of unseen data. Thus, it strongly encourages us to use an SSL approach that includes JESS-CM to construct a general tagger and chunker for actual use. 5 Comparison with Previous Top Systems and Related Work In POS tagging, the previous best performance was reported by (Shen et al., 2007) as summarized in Table 7. Their method uses a novel sophisticated model that learns both decoding order and labeling, while our model uses a standard first order Markov model. Despite using such a simple model, our method can provide a better result with the help of unlabeled data. 671 system dev. test additional resources JESS-CM (CRF/HMM) 94.48 89.92 1G-word unlabeled data 93.66 89.36 37M-word unlabeled data (Ando and Zhang, 2005) 93.15 89.31 27M-word unlabeled data (Florian et al., 2003) 93.87 88.76 own large gazetteers, 2M-word labeled data (Suzuki et al., 2007) N/A 88.41 27M-word unlabel</context>
</contexts>
<marker>Shen, Satta, Joshi, 2007</marker>
<rawString>L. Shen, G. Satta, and A. Joshi. 2007. Guided Learning for Bidirectional Sequence Classification. In Proc. of ACL-2007, pages 760–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>M Sindelar</author>
<author>A McCallum</author>
</authors>
<title>Reducing Weight Undertraining in Structured Discriminative Learning.</title>
<date>2006</date>
<booktitle>In Proc. ofHTL-NAACL</booktitle>
<pages>89--95</pages>
<contexts>
<context position="16277" citStr="Sutton et al., 2006" startWordPosition="2736" endWordPosition="2739">2 Design of JESS-CM We used the same graph structure as the linear chain CRF for JESS-CM. As regards the design of the feature functions fi, Table 3 shows the feature templates used in our experiments. In the table, s indicates a focused token position. Xs_1.s represents the bi-gram of feature X obtained from s − 1 and s positions. {Xu}Bu�A indicates that u ranges from A to B. For example, {Xu}s+2 u�s_2 is equal to five feature templates, {Xs_2i Xs_1i Xsi Xs+1i Xs+2}. ‘word type’ or wtp represents features of a word such as capitalization, the existence of digits, and punctuation as shown in (Sutton et al., 2006) without regular expressions. Although it is common to use external 668 Entire entence accuracy (a) POS tagging:(total 47 templates) [ys], [ys−1:s], {[ys, pf-Ns], [ys, sf-Ns]}9 N=1, {[ys, wdu], [ys, wtpu], [ys−1:s, wtpu]}s+2 u=s−2, {[ys, wdu−1:u], [ys, wtpu−1:u], [ys−1:s, wtpu−1:u]}s+2 u=s−1 (b) Syntactic chunking: (total 39 templates) [ys], [ys−1:s], {[ys, wdu], [ys, posu], [ys, wdu, posu], [ys−1:s, wdu], [ys−1:s, posu]}s+2 u=s−2, {[ys, wdu−1:u], ]}s+2 [ys, posu−1:u], {[ys −1: s, posu−1:u u=s−1, (c) NER: (total 79 templates) [ys], [ys−1:s], {[ys,wdu], [ys,lwdu], [ys, posu], [ys,wtpu], [ys−1:s</context>
</contexts>
<marker>Sutton, Sindelar, McCallum, 2006</marker>
<rawString>C. Sutton, M. Sindelar, and A. McCallum. 2006. Reducing Weight Undertraining in Structured Discriminative Learning. In Proc. ofHTL-NAACL 2006, pages 89–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>A Fujino</author>
<author>H Isozaki</author>
</authors>
<title>SemiSupervised Structured Output Learning Based on a Hybrid Generative and Discriminative Approach.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>791--800</pages>
<contexts>
<context position="4264" citStr="Suzuki et al., 2007" startWordPosition="656" endWordPosition="659">. Second, we report the best current results for the widely used test 665 Proceedings of ACL-08: HLT, pages 665–673, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics collections described above. Third, we confirm that the use of more unlabeled data in SSL can really lead to further improvements. 2 Conditional Model for SSL We design our model for SSL as a natural semisupervised extension of conventional supervised conditional random fields (CRFs) (Lafferty et al., 2001). As our approach for incorporating unlabeled data, we basically follow the idea proposed in (Suzuki et al., 2007). 2.1 Conventional Supervised CRFs Let x ∈ X and y ∈ Y be an input and output, where X and Y represent the set of possible inputs and outputs, respectively. C stands for the set of cliques in an undirected graphical model G(x, y), which indicates the interdependency of a given x and y. yc denotes the output from the corresponding clique c. Each clique c∈C has a potential function IFc. Then, the CRFs define the conditional probability p(y|x) as a product of IFcs. In addition, let f = (f1, ..., fI) be a feature vector, and A = (A1, ..., AI) be a parameter vector, whose lengths are I. p(y|x; A) o</context>
<context position="8661" citStr="Suzuki et al., 2007" startWordPosition="1452" endWordPosition="1455">e the correct outputs y for unlabeled data are unknown. On the other hand with a generative approach, a well-known way to achieve this incorporation is to use maximum marginal likelihood (MML) parameter estimation, i.e., (Nigam et al., 2000). Given unlabeled data Du = {xm}Mm=1, MML estimation in our setting maximizes the marginal distribution of a joint PM over a missing (hidden) variable y, namely, it maximizes Em log Ey∈Y p(xm, y; θ). Following this idea, there have been introduced a parameter estimation approach for non-generative approaches that can effectively incorporate unlabeled data (Suzuki et al., 2007). Here, we refer to it as ‘Maximum Discriminant Functions sum’ (MDF) parameter estimation. MDF estimation substitutes p(x, y) with discriminant functions g(x, y). Therefore, to estimate the parameter Θ of JESS-CM by using MDF estimation, the following objective function is maximized with a fixed λ0: � � L2(Θ|λ&apos;) = log m yEY where p(Θ) is a prior probability distribution of Θ. Since the normalization factor does not affect the determination of y, the discriminant function of JESS-CM shown in Equation 2 is defined as g(x, y; λ0, Θ) = Hc∈C Ψ0 c(yc, x; λ0, Θ). With a fixed λ0, the local maximum of</context>
<context position="10724" citStr="Suzuki et al., 2007" startWordPosition="1810" endWordPosition="1813"> maximize L1(A0|Θ) with fixed Θ←Θ(t−1) using Dl. 3. Estimate Θ(t): (Initial values = Θ(t−1)) update one step toward maximizing L2(Θ|A0) with fixed A0 using Du. do until |Θ&lt; &lt; e. (t Reestimate A0: perform the same procedure as 1. Output: a JESS-CM, P(y|x, A0, Θ(t)). Figure 1: Parameter estimation algorithm for JESS-CM. scalable for handling large scale unlabeled data, we only perform one step of MDF estimation for each t as explained on 3. in Figure 1. In addition, the calculation cost for estimating parameters of embedded joint PMs (HMMs) is independent of the number of HMMs, J, that we used (Suzuki et al., 2007). As a result, the cost for calculating the JESS-CM parameters, λ0 and Θ, is essentially the same as executing T iterations of the MML estimation for a single HMM using the EM algorithm plus T + 1 time optimizations of the MAP estimation for a conventional supervised CRF if it converged when t = T. In addition, our parameter estimation algorithm can be easily performed in parallel computation. 2.4 Comparison with Hybrid Model SSL based on a hybrid generative/discriminative approach proposed in (Suzuki et al., 2007) has been defined as a log-linear model that discriminatively combines several d</context>
<context position="11996" citStr="Suzuki et al., 2007" startWordPosition="2036" endWordPosition="2039">Gj , such that: R(y|x; Λ, Θ, Γ) Hi pDi (y|x;λi)γi Hj pGj (xj,y;θj)γj � Hi pD i (y|x;λi)γi Hj pG j (xj,y;θj)γj , y where Λ={λi}Ii=1, and Γ={{γi}Ii=1, {γj}I+J j=I+1}. With the hybrid model, if we use the same labeled training data to estimate both Λ and Γ, γjs will become negligible (zero or nearly zero) since pDi is already fitted to the labeled training data while pGj are trained by using unlabeled data. As a solution, a given amount of labeled training data is divided into two distinct sets, i.e., 4/5 for estimating Λ, and the g(xm, y; λ&apos;, Θ) + log p(Θ), = 667 remaining 1/5 for estimating F (Suzuki et al., 2007). Moreover, it is necessary to split features into several sets, and then train several corresponding discriminative models separately and preliminarily. In contrast, JESS-CM is free from this kind of additional process, and the entire parameter estimation procedure can be performed in a single pass. Surprisingly, although JESS-CM is a simpler version of the hybrid model in terms of model structure and parameter estimation procedure, JESS-CM provides F-scores of 94.45 and 88.03 for CoNLL’00 and ’03 data, respectively, which are 0.15 and 0.83 points higher than those reported in (Suzuki et al.,</context>
<context position="28481" citStr="Suzuki et al., 2007" startWordPosition="4750" endWordPosition="4753">JESS-CM performance and U.app in the NER experiments. The development data and test data were obtained from system dev. test additional resources JESS-CM (CRF/HMM) 97.35 97.40 1G-word unlabeled data (Shen et al., 2007) 97.28 97.33 – (Toutanova et al., 2003) 97.15 97.24 crude company name detector [sup. CRF (baseline)] 97.18 97.21 – Table 7: POS tagging results of the previous top systems for PTB III data evaluated by label accuracy system test additional resources JESS-CM (CRF/HMM) 95.15 1G-word unlabeled data 94.67 15M-word unlabeled data (Ando and Zhang, 2005) 94.39 15M-word unlabeled data (Suzuki et al., 2007) 94.36 17M-word unlabeled data (Zhang et al., 2002) 94.17 full parser output (Kudo and Matsumoto, 2001) 93.91 – [supervised CRF (baseline)] 93.88 – Table 8: Syntactic chunking results of the previous top systems for CoNLL’00 shared task data (F0=1 score) 30-31 Aug. 1996 and 6-7 Dec. 1996 Reuters news articles, respectively. We find that temporal proximity leads to better performance. This aspect can also be explained as U.app. Basically, the U.app increase leads to improved performance. The evidence provided by the above experiments implies that increasing the coverage of unlabeled data offers</context>
<context position="30005" citStr="Suzuki et al., 2007" startWordPosition="4991" endWordPosition="4994">performance was reported by (Shen et al., 2007) as summarized in Table 7. Their method uses a novel sophisticated model that learns both decoding order and labeling, while our model uses a standard first order Markov model. Despite using such a simple model, our method can provide a better result with the help of unlabeled data. 671 system dev. test additional resources JESS-CM (CRF/HMM) 94.48 89.92 1G-word unlabeled data 93.66 89.36 37M-word unlabeled data (Ando and Zhang, 2005) 93.15 89.31 27M-word unlabeled data (Florian et al., 2003) 93.87 88.76 own large gazetteers, 2M-word labeled data (Suzuki et al., 2007) N/A 88.41 27M-word unlabeled data [sup. CRF (baseline)] 91.74 86.35 – Table 9: NER results of the previous top systems for CoNLL’03 shared task data evaluated by F0=1 score As shown in Tables 8 and 9, the previous best performance for syntactic chunking and NER was reported by (Ando and Zhang, 2005), and is referred to as ‘ASO-semi’. ASO-semi also incorporates unlabeled data solely as additional information in the same way as JESS-CM. ASO-semi uses unlabeled data for constructing auxiliary problems that are expected to capture a good feature representation of the target problem. As regards sy</context>
</contexts>
<marker>Suzuki, Fujino, Isozaki, 2007</marker>
<rawString>J Suzuki, A Fujino, and H Isozaki. 2007. SemiSupervised Structured Output Learning Based on a Hybrid Generative and Discriminative Approach. In Proc. of EMNLP-CoNLL, pages 791–800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 Shared Task: Chunking.</title>
<date>2000</date>
<booktitle>In Proc. of CoNLL-2000 and LLL-2000,</booktitle>
<pages>127--132</pages>
<contexts>
<context position="2427" citStr="Sang and Buchholz, 2000" startWordPosition="356" endWordPosition="359">nd are referred to as sequential labeling and segmentation problems. In some cases, these tasks have relatively large amounts of labeled training data. In this situation, supervised learning can provide competitive results, and it is difficult to improve them any further by using SSL. In fact, few papers have succeeded in showing significantly better results than state-of-theart supervised learning. Ando and Zhang (2005) reported a substantial performance improvement compared with state-of-the-art supervised learning results for syntactic chunking with the CoNLL’00 shared task data (Tjong Kim Sang and Buchholz, 2000) and NER with the CoNLL’03 shared task data (Tjong Kim Sang and Meulder, 2003). One remaining question is the behavior of SSL when using as much labeled and unlabeled data as possible. This paper investigates this question, namely, the use of a large amount of unlabeled data in the presence of (fixed) large labeled data. To achieve this, it is paramount to make the SSL method scalable with regard to the size of unlabeled data. We first propose a scalable model for SSL. Then, we apply our model to widely used test collections, namely Penn Treebank (PTB) III data (Marcus et al., 1994) for POS ta</context>
<context position="13621" citStr="Sang and Buchholz, 2000" startWordPosition="2304" endWordPosition="2307">he hybrid model. 3 Experiments In our experiments, we report POS tagging, syntactic chunking and NER performance incorporating up to 1G-words of unlabeled data. 3.1 Data Set To compare the performance with that of previous studies, we selected widely used test collections. For our POS tagging experiments, we used the Wall Street Journal in PTB III (Marcus et al., 1994) with the same data split as used in (Shen et al., 2007). For our syntactic chunking and NER experiments, we used exactly the same training, development and test data as those provided for the shared tasks of CoNLL’00 (Tjong Kim Sang and Buchholz, 2000) and CoNLL’03 (Tjong Kim Sang and Meulder, 2003), respectively. The training, development and test data are detailed in Table 11 . The unlabeled data for our experiments was taken from the Reuters corpus, TIPSTER corpus (LDC93T3C) and the English Gigaword corpus, third edition (LDC2007T07). As regards the TIP1The second-order encoding used in our NER experiments is the same as that described in (Sha and Pereira, 2003) except removing IOB-tag of previous position label. (a) POS-tagging: (WSJ in PTB III) # of labels 45 Data set (WSJ sec. IDs) # of sent. # of words Training 0–18 38,219 912,344 De</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to the CoNLL-2000 Shared Task: Chunking. In Proc. of CoNLL-2000 and LLL-2000, pages 127–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Tjong Kim Sang</author>
<author>F De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition.</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL-2003,</booktitle>
<pages>142--147</pages>
<marker>Sang, De Meulder, 2003</marker>
<rawString>E. T. Tjong Kim Sang and F. De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition. In Proc. of CoNLL-2003, pages 142–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Yoram Singer</author>
</authors>
<title>Feature-rich Part-ofspeech Tagging with a Cyclic Dependency Network. In</title>
<date>2003</date>
<booktitle>Proc. ofHLT-NAACL-2003,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="28118" citStr="Toutanova et al., 2003" startWordPosition="4694" endWordPosition="4697">ord Unlabeled data used in our experiments (U.app) in the L.¬app and L.app. This indicates that most of the words in the development or test sets are covered by the 1G-word unlabeled data. This may be the main reason for JESS-CM providing large performance gains for both the overall and L.¬app set performance of all three tasks. Table 6 shows the relation between JESS-CM performance and U.app in the NER experiments. The development data and test data were obtained from system dev. test additional resources JESS-CM (CRF/HMM) 97.35 97.40 1G-word unlabeled data (Shen et al., 2007) 97.28 97.33 – (Toutanova et al., 2003) 97.15 97.24 crude company name detector [sup. CRF (baseline)] 97.18 97.21 – Table 7: POS tagging results of the previous top systems for PTB III data evaluated by label accuracy system test additional resources JESS-CM (CRF/HMM) 95.15 1G-word unlabeled data 94.67 15M-word unlabeled data (Ando and Zhang, 2005) 94.39 15M-word unlabeled data (Suzuki et al., 2007) 94.36 17M-word unlabeled data (Zhang et al., 2002) 94.17 full parser output (Kudo and Matsumoto, 2001) 93.91 – [supervised CRF (baseline)] 93.88 – Table 8: Syntactic chunking results of the previous top systems for CoNLL’00 shared task </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C.D. Manning, and Y. Yoram Singer. 2003. Feature-rich Part-ofspeech Tagging with a Cyclic Dependency Network. In Proc. ofHLT-NAACL-2003, pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
<author>F Damerau</author>
<author>D Johnson</author>
</authors>
<title>Text Chunking based on a Generalization of Winnow.</title>
<date>2002</date>
<booktitle>Machine Learning Research,</booktitle>
<pages>2--615</pages>
<contexts>
<context position="28532" citStr="Zhang et al., 2002" startWordPosition="4758" endWordPosition="4761">. The development data and test data were obtained from system dev. test additional resources JESS-CM (CRF/HMM) 97.35 97.40 1G-word unlabeled data (Shen et al., 2007) 97.28 97.33 – (Toutanova et al., 2003) 97.15 97.24 crude company name detector [sup. CRF (baseline)] 97.18 97.21 – Table 7: POS tagging results of the previous top systems for PTB III data evaluated by label accuracy system test additional resources JESS-CM (CRF/HMM) 95.15 1G-word unlabeled data 94.67 15M-word unlabeled data (Ando and Zhang, 2005) 94.39 15M-word unlabeled data (Suzuki et al., 2007) 94.36 17M-word unlabeled data (Zhang et al., 2002) 94.17 full parser output (Kudo and Matsumoto, 2001) 93.91 – [supervised CRF (baseline)] 93.88 – Table 8: Syntactic chunking results of the previous top systems for CoNLL’00 shared task data (F0=1 score) 30-31 Aug. 1996 and 6-7 Dec. 1996 Reuters news articles, respectively. We find that temporal proximity leads to better performance. This aspect can also be explained as U.app. Basically, the U.app increase leads to improved performance. The evidence provided by the above experiments implies that increasing the coverage of unlabeled data offers the strong possibility of increasing the expected </context>
</contexts>
<marker>Zhang, Damerau, Johnson, 2002</marker>
<rawString>T. Zhang, F. Damerau, and D. Johnson. 2002. Text Chunking based on a Generalization of Winnow. Machine Learning Research, 2:615–637.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>