<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004830">
<title confidence="0.85738">
Multilingual, Efficient and Easy NLP Processing with IXA Pipeline
</title>
<author confidence="0.682671">
Rodrigo Agerri
</author>
<note confidence="0.806084333333333">
IXA NLP Group
Univ. of the Basque Country
UPV/EHU
</note>
<address confidence="0.387077">
Donostia San-Sebasti´an
</address>
<email confidence="0.977084">
rodrigo.agerri@ehu.es
</email>
<author confidence="0.969693">
Josu Bermudez
</author>
<affiliation confidence="0.936133666666667">
Deusto Institute of Technology
Deustotech
Univ. of Deusto
</affiliation>
<address confidence="0.506147">
Bilbao
</address>
<email confidence="0.989353">
josu.bermudez@deusto.es
</email>
<note confidence="0.8396022">
German Rigau
IXA NLP Group
Univ. of the Basque Country
UPV/EHU
Donostia-San Sebasti´an
</note>
<email confidence="0.981238">
german.rigau@ehu.es
</email>
<sectionHeader confidence="0.993465" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999728285714286">
IXA pipeline is a modular set of Natural Lan-
guage Processing tools (or pipes) which pro-
vide easy access to NLP technology. It aims at
lowering the barriers of using NLP technology
both for research purposes and for small indus-
trial developers and SMEs by offering robust
and efficient linguistic annotation to both re-
searchers and non-NLP experts. IXA pipeline
can be used “as is” or exploit its modularity
to pick and change different components. This
paper describes the general data-centric archi-
tecture of IXA pipeline and presents competi-
tive results in several NLP annotations for En-
glish and Spanish.
</bodyText>
<sectionHeader confidence="0.998756" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999703605633803">
Many Natural Language Processing (NLP) applica-
tions demand some basic linguistic processing (Tok-
enization, Part of Speech (POS) tagging, Named Entity
Recognition and Classification (NER), Syntactic Pars-
ing, Coreference Resolution, etc.) to be able to further
undertake more complex tasks. Generally, NLP anno-
tation is required to be as accurate and efficient as pos-
sible and existing tools, quite righly, have mostly fo-
cused on performance. However, this generally means
that NLP suites and tools usually require researchers
to do complex compilation/installation/configuration in
order to use such tools. At the same time, in the indus-
try, there are currently many Small and Medium Enter-
prises (SMEs) offering services that one way or another
depend on NLP annotations.
In both cases, in research and industry, acquiring, de-
ploying or developing such base qualifying technolo-
gies is an expensive undertaking that redirects their
original central focus: In research, much time is spent
in the preliminaries of a particular research experiment
trying to obtain the required basic linguistic annota-
tion, whereas in an industrial environment SMEs see
their already limited resources taken away from of-
fering products and services that the market demands.
IXA pipeline provides ready to use modules to per-
form efficient and accurate linguistic annotation to al-
low users to focus on their original, central task. When
designing the architecture, we took several decisions
with respect to what IXA pipeline had to be:
Simple and ready to use: Every module of the IXA
pipeline can be up an running after two simple steps.
Portable: The modules come with “all batteries in-
cluded” which means that no classpath configurations
or installing of any third-party dependencies is re-
quired. The modules will will run on any platform as
long as a JVM 1.7+ and/or Python 2.7 are available.
Modular: Unlike other NLP toolkits, which often are
built in a monolithic architecture, IXA pipeline is built
in a data centric architecture so that modules can be
picked and changed (even from other NLP toolkits).
The modules behave like Unix pipes, they all take stan-
dard input, do some annotation, and produce standard
output which in turn is the input for the next module.
The data-centric architecture of IXA pipeline means
that any module is highly independent and can there-
fore be used with other tools from other toolkits if re-
quired.
Efficient: Piping the tokenizer (250K words per sec-
ond) POS tagger and lemmatizer all in one process
annotates over 5K words/second. The NERC mod-
ule annotates over 5K words/second. In a multi-core
machine, these times are dramatically reduced due to
multi-threading.
Multilingual: Currently we offer NLP annotations for
both English and Spanish, but other languages are be-
ing included in the pipeline. Tokenization already
works for several languages, including Dutch, French,
Italian, German, Spanish and English.
Accurate: For example, POS tagging and NERC for
English and Spanish are comparable with other state
of the art systems, as it is the coreference resolution
module for English.
Apache License 2.0: IXA Pipeline is licensed under
the Apache License 2.0, an open-source license that fa-
cilitates source code use, distribution and integration,
also for commercial purposes.1
Next section describes the IXA pipeline architecture,
section 3 the modules so far developed. Whenever
available, we also present empirical evaluation. Sec-
tion 4 describes the various ways of using the tools.
Finally, section 5 discusses some concluding remarks.
</bodyText>
<footnote confidence="0.989442">
1http://www.apache.org/licenses/LICENSE-2.0.html
</footnote>
<page confidence="0.954615">
5
</page>
<note confidence="0.9203165">
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 5–8,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.959343" genericHeader="introduction">
2 Architecture
</sectionHeader>
<bodyText confidence="0.999978574468085">
IXA pipeline is primarily conceived as a set of
ready to use tools that can provide efficient and
accurate linguistic annotation without any installa-
tion/configuration/compilation effort. As in Unix-like
operative systems, IXA pipeline consists of a set of pro-
cesses chained by their standard streams, in a way that
the output of each process feeds directly as input to the
next one. The Unix pipeline metaphor has been ap-
plied for NLP tools by adopting a very simple and well
known data centric architecture, in which every mod-
ule/pipe is interchangeable for another one as long as it
takes and produces the required data format.
The data format in which both the input and output of
the modules needs to be formatted to represent and fil-
ter linguistic annotations is KAF (Bosma et al., 2009).
KAF is a language neutral annotation format represent-
ing both morpho-syntactic and semantic annotation in a
structured format. KAF was originally designed in the
Kyoto European project2, but it has since been in con-
tinuous development3. Our Java modules all use kaflib4
library for easy integration.
Every module in the IXA pipeline, except the coref-
erence resolution, is implemented in Java, and re-
quires Java JDK1.7+ to compile. The integration of
the Java modules in the IXA pipeline is performed us-
ing Maven5. Maven is used to take care of classpaths
configurations and third-party tool dependencies. This
means that the binaries produced and distributed will
work off-the-self. The coreference module uses pip6
to provide an easy, one step installation. If the source
code of an ixa-pipe-$module is cloned from the remote
repository, one command to compile and have ready the
tools will suffice.
Some modules in IXA pipeline provide linguistic an-
notation based on probabilistic supervised approaches
such as POS tagging, NER and Syntactic Parsing. IXA
pipeline uses two well known machine learning algo-
rithms, namely, Maximum Entropy and the Percep-
tron. Both Perceptron (Collins, 2002; Collins, 2003)
and Maximum Entropy models (Ratnaparkhi, 1999) are
adaptable algorithms which have been successfully ap-
plied to NLP tasks such as POS tagging, NER and
Parsing with state of the art results. To avoid dupli-
cation of efforts, IXA pipeline uses the already avail-
able open-source Apache OpenNLP API7 to train POS,
NER and parsing probabilistic models using these two
approaches.
</bodyText>
<footnote confidence="0.999459833333333">
2http://kyoto-project.eu
3http://www.opener-project.org/kaf/
4https://github.com/ixa-ehu/kaflib
5http://maven.apache.org/
6https://pypi.python.org/pypi/pip
7http://opennlp.apache.org
</footnote>
<sectionHeader confidence="0.975722" genericHeader="background">
3 Pipes
</sectionHeader>
<bodyText confidence="0.9985655">
IXA pipeline currently provides the following linguis-
tic annotations: Sentence segmentation, tokenization,
Part of Speech (POS) tagging, Lemmatization, Named
Entity Recognition and Classification (NER), Con-
stituent Parsing and Coreference Resolution. Every
module works for English and Spanish and is imple-
mented in Java/Maven as described above. The only
exception is the coreference resolution module, which
currently is available in Python 2.7 and for English only
(Spanish version will comme soon). We will now de-
scribe which annotation services are provided by each
module of the pipeline.
</bodyText>
<subsectionHeader confidence="0.998448">
3.1 ixa-pipe-tok
</subsectionHeader>
<bodyText confidence="0.999979333333333">
This module provides rule-based Sentence Segmenta-
tion and Tokenization for French, Dutch, English, Ital-
ian and Spanish. It produces tokenized and segmented
text in KAF, running text and CoNLL formats. The
rules are originally based on the Stanford English To-
kenizer8, but with substantial modifications and addi-
tions. These include tokenization for other languages
such as French and Italian, normalization according
the Spanish Ancora Corpus (Taul´e et al., 2008), para-
graph treatment, and more comprehensive gazeteers
of non breaking prefixes. The tokenizer depends on
a JFlex9 specification file which compiles in seconds
and performs at a very reasonable speed (around 250K
word/second, and much quicker with Java multithread-
ing).
</bodyText>
<subsectionHeader confidence="0.999018">
3.2 ixa-pipe-pos
</subsectionHeader>
<bodyText confidence="0.996203818181818">
ixa-pipe-pos provides POS tagging and lemmatization
for English and Spanish. We have obtained the best
results so far with the same featureset as in Collins’s
(2002) paper. Perceptron models for English have been
trained and evaluated on the WSJ treebank using the
usual partitions (e.g., as explained in Toutanova et al.
(2003). We currently obtain a performance of 97.07%
vs 97.24% obtained by Toutanova et al., (2003)). For
Spanish, Maximum Entropy models have been trained
and evaluated using the Ancora corpus; it was ran-
domly divided in 90% for training and 10% for test-
ing. This corresponds to 440K words used for train-
ing and 70K words for testing. We obtain a perfor-
mance of 98.88% (the corpus partitions are available
for reproducibility). Gim´enez and Marquez (2004) re-
port 98.86%, although they train and test on a different
subset of the Ancora corpus.
Lemmatization is currently performed via 3 different
dictionary lookup methods: (i) Simple Lemmatizer: It
is based on HashMap lookups on a plain text dictionary.
Currently we use dictionaries from the LanguageTool
project10 under their distribution licenses. The English
</bodyText>
<footnote confidence="0.999389">
8http://www-nlp.stanford.edu/software/tokenizer.shtml
9http://jflex.de/
10http://languagetool.org/
</footnote>
<page confidence="0.999038">
6
</page>
<bodyText confidence="0.999920333333333">
dictionary contains 300K lemmas whereas the Spanish
provides over 600K; (ii) Morfologik-stemming11: The
Morfologik library provides routines to produce binary
dictionaries, from dictionaries such as the one used by
the Simple Lemmatizer above, as finite state automata.
This method is convenient whenever lookups on very
large dictionaries are required because it reduces the
memory footprint to 10% of the memory required for
the equivalent plain text dictionary; and (iii) We also
provide lemmatization by lookup in WordNet-3.0 (Fell-
baum and Miller, 1998) via the JWNL API12. Note that
this method is only available for English.
</bodyText>
<subsectionHeader confidence="0.997952">
3.3 ixa-pipe-nerc
</subsectionHeader>
<bodyText confidence="0.999282172413793">
Most of the NER systems nowdays consist of language
independent systems (sometimes enriched with gaze-
teers) based on automatic learning of statistical mod-
els. ixa-pipe-nerc provides Named Entity Recogni-
tion (NER) for English and Spanish. The named en-
tity types are based on the CONLL 200213 and 200314
tasks which were focused on language-independent su-
pervised named entity recognition (NER) for four types
of named entities: persons, locations, organizations and
names of miscellaneous entities that do not belong to
the previous three groups. We currently provide two
very fast language independent models using a rather
simple baseline featureset (e.g., similar to that of Cur-
ran and Clark (2003), except POS tag features).
For English, perceptron models have been trained
using CoNLL 2003 dataset. We currenly obtain 84.80
F1 which is coherent with other results reported with
these features (Clark and Curran, 2003; Ratinov and
Roth, 2009). The best Stanford NER model reported
on this dataset achieves 86.86 F1 (Finkel et al., 2005),
whereas the best system on this dataset achieves 90.80
F1 (Ratinov and Roth, 2009), using non local features
and substantial external knowledge.
For Spanish we currently obtain best results train-
ing Maximum Entropy models on the CoNLL 2002
dataset. Our best model obtains 79.92 F1 vs 81.39
F1 (Carreras et al., 2002), the best result so far on this
dataset. Their result uses external knowledge and with-
out it, their system obtains 79.28 F1.
</bodyText>
<subsectionHeader confidence="0.992219">
3.4 ixa-pipe-parse
</subsectionHeader>
<bodyText confidence="0.999784777777778">
ixa-pipe-parse provides statistical constituent parsing
for English and Spanish. Maximum Entropy models
are trained to build shift reduce bottom up parsers (Rat-
naparkhi, 1999) as provided by the Apache OpenNLP
API. Parsing models for English have been trained us-
ing the Penn treebank and for Spanish using the Ancora
corpus (Taul´e et al., 2008).
Furthermore, ixa-pipe-parse provides two methods
of HeadWord finders: one based on Collins’ head rules
</bodyText>
<footnote confidence="0.9998045">
11https://github.com/morfologik/morfologik-stemming
12http://jwordnet.sourceforge.net/
13http://www.clips.ua.ac.be/conll2002/ner/
14http://www.clips.ua.ac.be/conll2003/ner/
</footnote>
<bodyText confidence="0.999867545454545">
as defined in his PhD thesis (1999), and another one
based on Stanford’s parser Semantic Head Rules15.
The latter are a modification of Collins’ head rules ac-
cording to lexical and semantic criteria. These head
rules are particularly useful for the Coreference reso-
lution module and for projecting the constituents into
dependency graphs.
As far as we know, and although previous ap-
proaches exist (Cowan and Collins, 2005), ixa-pipe-
parse provides the first publicly available statistical
parser for Spanish.
</bodyText>
<subsectionHeader confidence="0.993214">
3.5 Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.99982588">
The module of coreference resolution included in the
IXA pipeline is loosely based on the Stanford Multi
Sieve Pass system (Lee et al., 2013). The module takes
every linguistic information it requires from the KAF
layers annotated by all the previously described mod-
ules. The system consists of a number of rule-based
sieves. Each sieve pass is applied in a deterministic
manner, reusing the information generated by the pre-
vious sieve and the mention processing. The order in
which the sieves are applied favours a highest precision
approach and aims at improving the recall with the sub-
sequent application of each of the sieve passes. This
is illustrated by the evaluation results of the CoNLL
2011 Coreference Evaluation task (Lee et al., 2013), in
which the Stanford’s system obtained the best results.
So far we have evaluated our module on the CoNLL
2011 testset and we are a 5% behind the Stanford’s sys-
tem (52.8 vs 57.6 CoNLL F1), the best on that task (Lee
et al., 2013). It is interesting that in our current imple-
mentation, mention-based metrics are favoured (CEAF
and B3). Still, note that these results are comparable
with the results obtained by the best CoNLL 2011 par-
ticipants. Currently the module performs coreference
resolution only for English, although a Spanish version
will be coming soon.
</bodyText>
<sectionHeader confidence="0.99991" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999889214285714">
Other NLP toolkits exist providing similar or more ex-
tensive functionalities than the IXA pipeline tools, al-
though not many of them provide multilingual support.
GATE (Cunningham, 2002) is an extensive framework
supporting annotation of text. GATE has some capacity
for wrapping Apache UIMA components16, so should
be able to manage distributed NLP components. How-
ever, GATE is a very large and complex system, with a
corresponding steep learning curve.
Freeling (Padr´o and Stanilovsky, 2012) provides
multilingual processing for a number of languages,
incluing Spanish and English. As opposed to IXA
pipeline, Freeling is a monolithic toolkit written in C++
which needs to be compiled natively. The Stanford
</bodyText>
<footnote confidence="0.998074">
15http://www-nlp.stanford.edu/software/lex-parser.shtml
16http://uima.apache.org/
</footnote>
<page confidence="0.999262">
7
</page>
<bodyText confidence="0.999227625">
CoreNLP17 is a monolithic suite, which makes it dif-
ficult to integrate other tools in its chain.
IXA pipeline tools can easily be used piping the in-
put with the output of another too, and it is also pos-
sible to easily replace or extend the toolchain with a
third-party tool. IXA pipeline is already being used to
do extensive parallel processing in the FP7 European
projects OpeNER18 and NewsReader19.
</bodyText>
<sectionHeader confidence="0.992895" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999975705882353">
IXA pipeline provides a simple, efficient, accurate and
ready to use set of NLP tools. Its modularity and data
centric architecture makes it flexible to pick and change
or integrate new linguistic annotators. Currently we of-
fer linguistic annotation for English and Spanish, but
more languages are being integrated. Furthermore,
other annotations such as Semantic Role Labelling and
Named Entity Disambiguation are being included in
the pipeline following the same principles.
Additionally, current integrated modules are be-
ing improved: both on the quality and variety of
the probabilistic models, and on specific issues such
as lemmatization, and treatment of time expressions.
Finally, we are adding server-mode execution into
the pipeline to provide faster processing. IXA
pipeline is publicly available under Apache 2.0 license:
http://adimen.si.ehu.es/web/ixa-pipes.
</bodyText>
<sectionHeader confidence="0.994728" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999573714285714">
TThis work has been supported by the OpeNER FP7
project under Grant No. 296451, the FP7 NewsReader
project, Grant No. 316404, and by the SKATER Span-
ish MICINN project No TIN2012-38584-C06-01. The
work of Josu Bermudez on coreference resolution is
supported by a PhD Grant of the University of Deusto
(http://www.deusto.es).
</bodyText>
<sectionHeader confidence="0.9755" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.816129214285714">
Wauter Bosma, Piek Vossen, Aitor Soroa, German
Rigau, Maurizio Tesconi, Andrea Marchetti, Mon-
ica Monachini, and Carlo Aliprandi. 2009. Kaf: a
generic semantic annotation format. In Proceedings
of the GL2009 Workshop on Semantic Annotation.
X. Carreras, L. Marquez, and L. Padro. 2002. Named
entity extraction using AdaBoost. In proceedings
of the 6th conference on Natural language learning-
Volume 20, pages 1–4.
Stephen Clark and James Curran. 2003. Language In-
dependent NER using a Maximum Entropy Tagger.
In Proceedings of the Seventh Conference on Nat-
ural Language Learning (CoNLL-03), pages 164–
167, Edmonton, Canada.
</bodyText>
<footnote confidence="0.999427">
17http://nlp.stanford.edu/software/corenlp.shtml
18http://www.opener-project.org
19http://www.newsreader-project.eu
</footnote>
<bodyText confidence="0.8604972">
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1–8.
</bodyText>
<reference confidence="0.954171729166667">
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589–637.
Brooke Cowan and Michael Collins. 2005. Mor-
phology and reranking for the statistical parsing of
spanish. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 795–802.
Association for Computational Linguistics.
Hamish Cunningham. 2002. Gate, a general architec-
ture for text engineering. Computers and the Hu-
manities, 36(2):223–254.
C. Fellbaum and G. Miller, editors. 1998. Wordnet: An
Electronic Lexical Database. MIT Press, Cambridge
(MA).
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, pages 363–370.
Jes´us Gim´enez and Lluis Marquez. 2004. Svmtool: A
general pos tagger generator based on support vector
machines. In In Proceedings of the 4th International
Conference on Language Resources and Evaluation.
Citeseer.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, pages 1–54, January.
Llu´ıs Padr´o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning, page 147155.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
learning, 34(1-3):151–175.
Mariona Taul´e, Maria Ant`onia Mart´ı, and Marta Re-
casens. 2008. Ancora: Multilevel annotated corpora
for catalan and spanish. In LREC.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proceedings of HLT-NAACL, pages 252–259.
</reference>
<page confidence="0.998489">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.069147">
<title confidence="0.999872">Multilingual, Efficient and Easy NLP Processing with IXA Pipeline</title>
<author confidence="0.997193">Rodrigo</author>
<affiliation confidence="0.856593333333333">IXA NLP Univ. of the Basque Donostia San-Sebasti´an</affiliation>
<email confidence="0.867232">rodrigo.agerri@ehu.es</email>
<author confidence="0.544062">Josu</author>
<affiliation confidence="0.997671">Deusto Institute of Univ. of</affiliation>
<address confidence="0.774837">Bilbao</address>
<email confidence="0.847641">josu.bermudez@deusto.es</email>
<author confidence="0.959334">German</author>
<affiliation confidence="0.9307875">IXA NLP Univ. of the Basque</affiliation>
<author confidence="0.649828">Donostia-San Sebasti´an</author>
<email confidence="0.992541">german.rigau@ehu.es</email>
<abstract confidence="0.9809538">IXA pipeline is a modular set of Natural Language Processing tools (or pipes) which provide easy access to NLP technology. It aims at lowering the barriers of using NLP technology both for research purposes and for small industrial developers and SMEs by offering robust and efficient linguistic annotation to both researchers and non-NLP experts. IXA pipeline can be used “as is” or exploit its modularity to pick and change different components. This paper describes the general data-centric architecture of IXA pipeline and presents competitive results in several NLP annotations for English and Spanish.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<booktitle>Computational linguistics,</booktitle>
<pages>29--4</pages>
<contexts>
<context position="6877" citStr="Collins, 2003" startWordPosition="1067" endWordPosition="1068">dependencies. This means that the binaries produced and distributed will work off-the-self. The coreference module uses pip6 to provide an easy, one step installation. If the source code of an ixa-pipe-$module is cloned from the remote repository, one command to compile and have ready the tools will suffice. Some modules in IXA pipeline provide linguistic annotation based on probabilistic supervised approaches such as POS tagging, NER and Syntactic Parsing. IXA pipeline uses two well known machine learning algorithms, namely, Maximum Entropy and the Perceptron. Both Perceptron (Collins, 2002; Collins, 2003) and Maximum Entropy models (Ratnaparkhi, 1999) are adaptable algorithms which have been successfully applied to NLP tasks such as POS tagging, NER and Parsing with state of the art results. To avoid duplication of efforts, IXA pipeline uses the already available open-source Apache OpenNLP API7 to train POS, NER and parsing probabilistic models using these two approaches. 2http://kyoto-project.eu 3http://www.opener-project.org/kaf/ 4https://github.com/ixa-ehu/kaflib 5http://maven.apache.org/ 6https://pypi.python.org/pypi/pip 7http://opennlp.apache.org 3 Pipes IXA pipeline currently provides th</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
<author>Michael Collins</author>
</authors>
<title>Morphology and reranking for the statistical parsing of spanish.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>795--802</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13241" citStr="Cowan and Collins, 2005" startWordPosition="1992" endWordPosition="1995">based on Collins’ head rules 11https://github.com/morfologik/morfologik-stemming 12http://jwordnet.sourceforge.net/ 13http://www.clips.ua.ac.be/conll2002/ner/ 14http://www.clips.ua.ac.be/conll2003/ner/ as defined in his PhD thesis (1999), and another one based on Stanford’s parser Semantic Head Rules15. The latter are a modification of Collins’ head rules according to lexical and semantic criteria. These head rules are particularly useful for the Coreference resolution module and for projecting the constituents into dependency graphs. As far as we know, and although previous approaches exist (Cowan and Collins, 2005), ixa-pipeparse provides the first publicly available statistical parser for Spanish. 3.5 Coreference Resolution The module of coreference resolution included in the IXA pipeline is loosely based on the Stanford Multi Sieve Pass system (Lee et al., 2013). The module takes every linguistic information it requires from the KAF layers annotated by all the previously described modules. The system consists of a number of rule-based sieves. Each sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the </context>
</contexts>
<marker>Cowan, Collins, 2005</marker>
<rawString>Brooke Cowan and Michael Collins. 2005. Morphology and reranking for the statistical parsing of spanish. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 795–802. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
</authors>
<title>Gate, a general architecture for text engineering.</title>
<date>2002</date>
<journal>Computers and the Humanities,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="14867" citStr="Cunningham, 2002" startWordPosition="2258" endWordPosition="2259">Stanford’s system (52.8 vs 57.6 CoNLL F1), the best on that task (Lee et al., 2013). It is interesting that in our current implementation, mention-based metrics are favoured (CEAF and B3). Still, note that these results are comparable with the results obtained by the best CoNLL 2011 participants. Currently the module performs coreference resolution only for English, although a Spanish version will be coming soon. 4 Related Work Other NLP toolkits exist providing similar or more extensive functionalities than the IXA pipeline tools, although not many of them provide multilingual support. GATE (Cunningham, 2002) is an extensive framework supporting annotation of text. GATE has some capacity for wrapping Apache UIMA components16, so should be able to manage distributed NLP components. However, GATE is a very large and complex system, with a corresponding steep learning curve. Freeling (Padr´o and Stanilovsky, 2012) provides multilingual processing for a number of languages, incluing Spanish and English. As opposed to IXA pipeline, Freeling is a monolithic toolkit written in C++ which needs to be compiled natively. The Stanford 15http://www-nlp.stanford.edu/software/lex-parser.shtml 16http://uima.apach</context>
</contexts>
<marker>Cunningham, 2002</marker>
<rawString>Hamish Cunningham. 2002. Gate, a general architecture for text engineering. Computers and the Humanities, 36(2):223–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
<author>G Miller</author>
<author>editors</author>
</authors>
<title>Wordnet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge (MA).</location>
<marker>Fellbaum, Miller, editors, 1998</marker>
<rawString>C. Fellbaum and G. Miller, editors. 1998. Wordnet: An Electronic Lexical Database. MIT Press, Cambridge (MA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="11733" citStr="Finkel et al., 2005" startWordPosition="1776" endWordPosition="1779">tities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. We currently provide two very fast language independent models using a rather simple baseline featureset (e.g., similar to that of Curran and Clark (2003), except POS tag features). For English, perceptron models have been trained using CoNLL 2003 dataset. We currenly obtain 84.80 F1 which is coherent with other results reported with these features (Clark and Curran, 2003; Ratinov and Roth, 2009). The best Stanford NER model reported on this dataset achieves 86.86 F1 (Finkel et al., 2005), whereas the best system on this dataset achieves 90.80 F1 (Ratinov and Roth, 2009), using non local features and substantial external knowledge. For Spanish we currently obtain best results training Maximum Entropy models on the CoNLL 2002 dataset. Our best model obtains 79.92 F1 vs 81.39 F1 (Carreras et al., 2002), the best result so far on this dataset. Their result uses external knowledge and without it, their system obtains 79.28 F1. 3.4 ixa-pipe-parse ixa-pipe-parse provides statistical constituent parsing for English and Spanish. Maximum Entropy models are trained to build shift reduce</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. R. Finkel, T. Grenager, and C. Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis Marquez</author>
</authors>
<title>Svmtool: A general pos tagger generator based on support vector machines. In</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation. Citeseer.</booktitle>
<marker>Gim´enez, Marquez, 2004</marker>
<rawString>Jes´us Gim´enez and Lluis Marquez. 2004. Svmtool: A general pos tagger generator based on support vector machines. In In Proceedings of the 4th International Conference on Language Resources and Evaluation. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics,</title>
<date>2013</date>
<pages>1--54</pages>
<contexts>
<context position="13495" citStr="Lee et al., 2013" startWordPosition="2030" endWordPosition="2033">on Stanford’s parser Semantic Head Rules15. The latter are a modification of Collins’ head rules according to lexical and semantic criteria. These head rules are particularly useful for the Coreference resolution module and for projecting the constituents into dependency graphs. As far as we know, and although previous approaches exist (Cowan and Collins, 2005), ixa-pipeparse provides the first publicly available statistical parser for Spanish. 3.5 Coreference Resolution The module of coreference resolution included in the IXA pipeline is loosely based on the Stanford Multi Sieve Pass system (Lee et al., 2013). The module takes every linguistic information it requires from the KAF layers annotated by all the previously described modules. The system consists of a number of rule-based sieves. Each sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the sieves are applied favours a highest precision approach and aims at improving the recall with the subsequent application of each of the sieve passes. This is illustrated by the evaluation results of the CoNLL 2011 Coreference Evaluation task (Lee et al.,</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics, pages 1–54, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs Padr´o</author>
<author>Evgeny Stanilovsky</author>
</authors>
<title>Freeling 3.0: Towards wider multilinguality.</title>
<date>2012</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC 2012),</booktitle>
<publisher>ELRA.</publisher>
<location>Istanbul, Turkey,</location>
<marker>Padr´o, Stanilovsky, 2012</marker>
<rawString>Llu´ıs Padr´o and Evgeny Stanilovsky. 2012. Freeling 3.0: Towards wider multilinguality. In Proceedings of the Language Resources and Evaluation Conference (LREC 2012), Istanbul, Turkey, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>147155</pages>
<contexts>
<context position="11639" citStr="Ratinov and Roth, 2009" startWordPosition="1760" endWordPosition="1763">used on language-independent supervised named entity recognition (NER) for four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. We currently provide two very fast language independent models using a rather simple baseline featureset (e.g., similar to that of Curran and Clark (2003), except POS tag features). For English, perceptron models have been trained using CoNLL 2003 dataset. We currenly obtain 84.80 F1 which is coherent with other results reported with these features (Clark and Curran, 2003; Ratinov and Roth, 2009). The best Stanford NER model reported on this dataset achieves 86.86 F1 (Finkel et al., 2005), whereas the best system on this dataset achieves 90.80 F1 (Ratinov and Roth, 2009), using non local features and substantial external knowledge. For Spanish we currently obtain best results training Maximum Entropy models on the CoNLL 2002 dataset. Our best model obtains 79.92 F1 vs 81.39 F1 (Carreras et al., 2002), the best result so far on this dataset. Their result uses external knowledge and without it, their system obtains 79.28 F1. 3.4 ixa-pipe-parse ixa-pipe-parse provides statistical constit</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>L. Ratinov and D. Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, page 147155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="6924" citStr="Ratnaparkhi, 1999" startWordPosition="1073" endWordPosition="1074">produced and distributed will work off-the-self. The coreference module uses pip6 to provide an easy, one step installation. If the source code of an ixa-pipe-$module is cloned from the remote repository, one command to compile and have ready the tools will suffice. Some modules in IXA pipeline provide linguistic annotation based on probabilistic supervised approaches such as POS tagging, NER and Syntactic Parsing. IXA pipeline uses two well known machine learning algorithms, namely, Maximum Entropy and the Perceptron. Both Perceptron (Collins, 2002; Collins, 2003) and Maximum Entropy models (Ratnaparkhi, 1999) are adaptable algorithms which have been successfully applied to NLP tasks such as POS tagging, NER and Parsing with state of the art results. To avoid duplication of efforts, IXA pipeline uses the already available open-source Apache OpenNLP API7 to train POS, NER and parsing probabilistic models using these two approaches. 2http://kyoto-project.eu 3http://www.opener-project.org/kaf/ 4https://github.com/ixa-ehu/kaflib 5http://maven.apache.org/ 6https://pypi.python.org/pypi/pip 7http://opennlp.apache.org 3 Pipes IXA pipeline currently provides the following linguistic annotations: Sentence se</context>
<context position="12371" citStr="Ratnaparkhi, 1999" startWordPosition="1877" endWordPosition="1879">ystem on this dataset achieves 90.80 F1 (Ratinov and Roth, 2009), using non local features and substantial external knowledge. For Spanish we currently obtain best results training Maximum Entropy models on the CoNLL 2002 dataset. Our best model obtains 79.92 F1 vs 81.39 F1 (Carreras et al., 2002), the best result so far on this dataset. Their result uses external knowledge and without it, their system obtains 79.28 F1. 3.4 ixa-pipe-parse ixa-pipe-parse provides statistical constituent parsing for English and Spanish. Maximum Entropy models are trained to build shift reduce bottom up parsers (Ratnaparkhi, 1999) as provided by the Apache OpenNLP API. Parsing models for English have been trained using the Penn treebank and for Spanish using the Ancora corpus (Taul´e et al., 2008). Furthermore, ixa-pipe-parse provides two methods of HeadWord finders: one based on Collins’ head rules 11https://github.com/morfologik/morfologik-stemming 12http://jwordnet.sourceforge.net/ 13http://www.clips.ua.ac.be/conll2002/ner/ 14http://www.clips.ua.ac.be/conll2003/ner/ as defined in his PhD thesis (1999), and another one based on Stanford’s parser Semantic Head Rules15. The latter are a modification of Collins’ head ru</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine learning, 34(1-3):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taul´e</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Marta Recasens</author>
</authors>
<title>Ancora: Multilevel annotated corpora for catalan and spanish.</title>
<date>2008</date>
<booktitle>In LREC.</booktitle>
<marker>Taul´e, Mart´ı, Recasens, 2008</marker>
<rawString>Mariona Taul´e, Maria Ant`onia Mart´ı, and Marta Recasens. 2008. Ancora: Multilevel annotated corpora for catalan and spanish. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="9135" citStr="Toutanova et al. (2003)" startWordPosition="1384" endWordPosition="1387">, 2008), paragraph treatment, and more comprehensive gazeteers of non breaking prefixes. The tokenizer depends on a JFlex9 specification file which compiles in seconds and performs at a very reasonable speed (around 250K word/second, and much quicker with Java multithreading). 3.2 ixa-pipe-pos ixa-pipe-pos provides POS tagging and lemmatization for English and Spanish. We have obtained the best results so far with the same featureset as in Collins’s (2002) paper. Perceptron models for English have been trained and evaluated on the WSJ treebank using the usual partitions (e.g., as explained in Toutanova et al. (2003). We currently obtain a performance of 97.07% vs 97.24% obtained by Toutanova et al., (2003)). For Spanish, Maximum Entropy models have been trained and evaluated using the Ancora corpus; it was randomly divided in 90% for training and 10% for testing. This corresponds to 440K words used for training and 70K words for testing. We obtain a performance of 98.88% (the corpus partitions are available for reproducibility). Gim´enez and Marquez (2004) report 98.86%, although they train and test on a different subset of the Ancora corpus. Lemmatization is currently performed via 3 different dictionar</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-NAACL, pages 252–259.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>