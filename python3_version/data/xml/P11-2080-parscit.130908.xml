<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.905307">
Two Easy Improvements to Lexical Weighting
</title>
<author confidence="0.984462">
David Chiang and Steve DeNeefe and Michael Pust
</author>
<affiliation confidence="0.830601">
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.743167">
Marina del Rey, CA 90292
</address>
<email confidence="0.999436">
{chiang,sdeneefe,pust}@isi.edu
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999904818181818">
We introduce two simple improvements to the
lexical weighting features of Koehn, Och, and
Marcu (2003) for machine translation: one
which smooths the probability of translating
word f to word e by simplifying English mor-
phology, and one which conditions it on the
kind of training data that f and e co-occurred
in. These new variations lead to improvements
of up to +0.8 BLEU, with an average improve-
ment of +0.6 BLEU across two language pairs,
two genres, and two translation systems.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999845727272727">
Lexical weighting features (Koehn et al., 2003) es-
timate the probability of a phrase pair or translation
rule word-by-word. In this paper, we introduce two
simple improvements to these features: one which
smooths the probability of translating word f to
word e using English morphology, and one which
conditions it on the kind of training data that f and
e co-occurred in. These new variations lead to im-
provements of up to +0.8 BLEU, with an average im-
provement of +0.6 BLEU across two language pairs,
two genres, and two translation systems.
</bodyText>
<sectionHeader confidence="0.996996" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.996926625">
Since there are slight variations in how the lexi-
cal weighting features are computed, we begin by
defining the baseline lexical weighting features. If
f = f1 · · · fn and e = e1 · · · em are a training sentence
pair, let ai (1 &lt;— i &lt;— n) be the (possibly empty) set of
positions in f that ei is aligned to.
First, compute a word translation table from the
word-aligned parallel text: for each sentence pair and
</bodyText>
<equation confidence="0.97213475">
each i, let
1
c(fj,ei) (-- c(fj,ei) + |ai |for j E ai (1)
c(NULL, ei) (-- c(NULL, ei) + 1 if |ai |= 0 (2)
Then
c(f, e)
t(e  |f) = (3)
Ee c(f, e)
</equation>
<bodyText confidence="0.999185">
where f can be NULL.
Second, during phrase-pair extraction, store with
each phrase pair the alignments between the words
in the phrase pair. If it is observed with more than
one word alignment pattern, store the most frequent
pattern.
Third, for each phrase pair (f¯, ¯e, a), compute
</bodyText>
<equation confidence="0.9811025">
|ai |jEai
1 t(¯ei  |¯fj) if |ai |&gt; 0
Z (4)
t(¯ei  |NULL) otherwise
</equation>
<bodyText confidence="0.99840025">
This generalizes to synchronous CFG rules in the ob-
vious way.
Similarly, compute the reverse probability t(f¯  |¯e).
Then add two new model features
</bodyText>
<equation confidence="0.997419">
− log t(¯e  |f¯) and − log t(f¯  |¯e)
� � ����
�����
t(¯e |
|e¯|
H
i=1
f¯ ) =
</equation>
<page confidence="0.979001">
455
</page>
<note confidence="0.8439165">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 455–460,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<table confidence="0.977641666666667">
feature translation
(7) (8)
small LM 26.7 24.3
large LM 31.4 28.2
− log t(¯e  |f¯) 9.3 9.9
− log t(f¯  |¯e) 5.8 6.3
</table>
<tableCaption confidence="0.9933112">
Table 1: Although the language models prefer translation
(8), which translates 朋友 and 伙伴 as singular nouns, the
lexical weighting features prefer translation (7), which in-
correctly generates plural nouns. All features are negative
log-probabilities, so lower numbers indicate preference.
</tableCaption>
<sectionHeader confidence="0.943195" genericHeader="method">
3 Morphological smoothing
</sectionHeader>
<bodyText confidence="0.964663">
Consider the following example Chinese sentence:
</bodyText>
<equation confidence="0.913662444444444">
(5) 温家宝 表示
Wēn Jiābǎo biǎoshì
Wen Jiabao said
中国 在 非洲
Zhōngguó zài Fēizhōu
China in Africa
伙伴
huǒbàn
partner
</equation>
<listItem confidence="0.990798777777778">
(6) Human: Wen Jiabao said that Côte d’Ivoire is
a good friend and a good partner of China’s in
Africa.
(7) MT (baseline): Wen Jiabao said that Cote
d’Ivoire is China’s good friends, and good
partners in Africa.
(8) MT (better): Wen Jiabao said that Cote d’Ivoire
is China’s good friend and good partner in
Africa.
</listItem>
<bodyText confidence="0.980751777777778">
The baseline machine translation (7) incorrectly gen-
erates plural nouns. Even though the language mod-
els (LMs) prefer singular nouns, the lexical weight-
ing features prefer plural nouns (Table 1).1
The reason for this is that the Chinese words do not
have any marking for number. Therefore the infor-
mation needed to mark friend and partner for num-
ber must come from the context. The LMs are able
to capture this context: the 5-gram is China’s good
</bodyText>
<footnote confidence="0.998795333333333">
1The presence of an extra comma in translation (7) affects
the LM scores only slightly; removing the comma would make
them 26.4 and 32.0.
</footnote>
<table confidence="0.985222">
f e t(e  |f) t(f  |e) tm(e  |f) tm(f  |e)
朋友 friends 0.44 0.44 0.47 0.48
朋友 friend 0.21 0.58 0.19 0.48
伙伴 partners 0.44 0.60 0.40 0.53
伙伴 partner 0.13 0.40 0.17 0.53
</table>
<tableCaption confidence="0.794781666666667">
Table 2: The morphologically-smoothed lexical weight-
ing features weaken the preference for singular or plural
translations, with the exception of t(friends  |朋友).
</tableCaption>
<bodyText confidence="0.999481571428572">
friend is observed in our large LM, and the 4-gram
China’s good friend in our small LM, but China’s
good friends is not observed in either LM. Likewise,
the 5-grams good friend and good partner and good
friends and good partners are both observed in our
LMs, but neither good friend and good partners nor
good friends and good partner is.
By contrast, the lexical weighting tables (Table 2,
columns 3–4), which ignore context, have a strong
preference for plural translations, except in the case
of t(朋友  |friend). Therefore we hypothesize that,
for Chinese-English translation, we should weaken
the lexical weighting features’ morphological pref-
erences so that more contextual features can do their
work.
Running a morphological stemmer (Porter, 1980)
on the English side of the parallel data gives a
three-way parallel text: for each sentence, we have
French f, English e, and stemmed English e′. We can
then build two word translation tables, t(e′  |f) and
t(e  |e′), and form their product
</bodyText>
<equation confidence="0.991524">
tm(e  |f) = � t(e′  |f)t(e  |e′) (9)
e′
</equation>
<bodyText confidence="0.939738909090909">
Similarly, we can compute tm(f  |e) in the opposite
direction.2 (See Table 2, columns 5–6.) These tables
can then be extended to phrase pairs or synchronous
CFG rules as before and added as two new features
of the model:
−logtm(¯e  |f¯) and − logtm(f¯  |¯e)
The feature tm(¯e  |f¯) does still prefer certain word-
forms, as can be seen in Table 2. But because e is
generated from e′ and not from f , we are protected
from the situation where a rare f leads to poor esti-
mates for the e.
</bodyText>
<footnote confidence="0.88848">
2Since the Porter stemmer is deterministic, we always have
</footnote>
<construct confidence="0.289783">
t(e′  |e) = 1.0, so that tm(f  |e) = t(f  |e′), as seen in the last
column of Table 2.
</construct>
<figure confidence="0.947554833333334">
, 科特迪瓦 是
,Kētèdíwǎ shì
, Côte d’Ivoire is
的 好 朋友 ,
de hǎo péngyǒu ,
’s good friend ,
好
hǎo
good
.
.
.
</figure>
<page confidence="0.997343">
456
</page>
<bodyText confidence="0.999962636363636">
When we applied an analogous approach to
Arabic-English translation, stemming both Arabic
and English, we generated very large lexicon tables,
but saw no statistically significant change in BLEU.
Perhaps this is not surprising, because in Arabic-
English translation (unlike Chinese-English transla-
tion), the source language is morphologically richer
than the target language. So we may benefit from fea-
tures that preserve this information, while smoothing
over morphological differences blurs important dis-
tinctions.
</bodyText>
<sectionHeader confidence="0.932916" genericHeader="method">
4 Conditioning on provenance
</sectionHeader>
<bodyText confidence="0.998347285714286">
Typical machine translation systems are trained on
a fixed set of training data ranging over a variety of
genres, and if the genre of an input sentence is known
in advance, it is usually advantageous to use model
parameters tuned for that genre.
Consider the following Arabic sentence, from a
weblog (words written left-to-right):
</bodyText>
<listItem confidence="0.888911">
(10) ﻞﻌﻟو اﺬھ ﺪﺣا ﻢھا قوﺮﻔﻟا ﻦﯿﺑ
wlEl h*A AHd Ahm Alfrwq byn
perhaps this one main differences between
رﻮﺻ ﺔﻤﻈﻧا ﻢﻜﺤﻟا ﺔﺣﺮﺘﻘﻤﻟا .
Swr AnZmp AlHkm AlmqtrHp .
images systems ruling proposed .
(11) Human: Perhaps this is one of the most impor-
tant differences between the images of the pro-
posed ruling systems.
(12) MT (baseline): This may be one of the most
important differences between pictures of the
proposed ruling regimes.
(13) MT (better): Perhaps this is one of the most im-
</listItem>
<bodyText confidence="0.964404">
portant differences between the images of the
proposed regimes.
The Arabic word ﻞﻌﻟو can be translated as may or per-
haps (among others), with the latter more common
according to t(e  |f), as shown in Table 3. But some
genres favor perhaps more or less strongly. Thus,
both translations (12) and (13) are good, but the lat-
ter uses a slightly more informal register appropriate
to the genre.
Following Matsoukas et al. (2009), we assign each
training sentence pair a set of binary features which
we call s-features:
</bodyText>
<table confidence="0.983971">
f e t(e  |f) nw ts(e  |f) un
– web bn
ﻞﻌﻟو may 0.13 0.12 0.16 0.09 0.13
ﻞﻌﻟو perhaps 0.20 0.23 0.32 0.42 0.19
</table>
<tableCaption confidence="0.939367666666667">
Table 3: Different genres have different preferences for
word translations. Key: nw = newswire, web = Web, bn =
broadcast news, un = United Nations proceedings.
</tableCaption>
<listItem confidence="0.88147675">
• Whether the sentence pair came from a particu-
lar genre, for example, newswire or web
• Whether the sentence pair came from a particu-
lar collection, for example, FBIS or UN
</listItem>
<bodyText confidence="0.999877608695652">
Matsoukas et al. (2009) use these s-features to
compute weights for each training sentence pair,
which are in turn used for computing various model
features. They found that the sentence-level weights
were most helpful for computing the lexical weight-
ing features (p.c.). The mapping from s-features
to sentence weights was chosen to optimize ex-
pected TER on held-out data. A drawback of this
method is that we must now learn the mapping from
s-features to sentence-weights and then the model
feature weights. Therefore, we tried an alternative
that incorporates s-features into the model itself.
For each s-feature s, we compute new word trans-
lation tables ts(e  |f) and ts(f  |e) estimated from
only those sentence pairsf on which s fires, and ex-
tend them to phrases/rules as before. The idea is to
use these probabilities as new features in the model.
However, two challenges arise: first, many word
pairs are unseen for a given s, resulting in zero or
undefined probabilities; second, this adds many new
features for each rule, which requires a lot of space.
To address the problem of unseen word pairs, we
use Witten-Bell smoothing (Witten and Bell, 1991):
</bodyText>
<equation confidence="0.999745">
ˆts(e  |f) = Afsts(e  |f) + (1 − Afs)t(e  |f) (14)
c(f, s) (15)
c(f, s) + d(f, s)
</equation>
<bodyText confidence="0.999975">
where c(f, s) is the number of times f has been ob-
served in sentences with s-feature s, and d(f, s) is the
number of e types observed aligned to f in sentences
with s-feature s.
</bodyText>
<equation confidence="0.923477888888889">
For each s-feature s, we add two model features
Afs =
ˆts(f¯  |¯e)
ˆts(¯e  |f¯)
f¯)
and − log
− log
t(¯e |
t(f¯  |¯e)
</equation>
<page confidence="0.997167">
457
</page>
<table confidence="0.999432857142857">
system features Arabic-English Chinese-English Test
newswire web newswire web
Dev Test Dev Test Dev Test Dev
string-to-string baseline 47.1 43.8 37.1 38.4 28.7 26.0 23.2 25.9
full2 47.7 44.2* 37.4 39.0 29.5 26.8 23.8 26.3
string-to-tree baseline 47.3 43.6 37.7 39.6 29.2 26.4 23.0 26.0
full 47.7 44.3 38.3 40.2 29.8 27.1 23.4 26.6
</table>
<tableCaption confidence="0.996532">
Table 4: Our variations on lexical weighting improve translation quality significantly across 16 different test conditions.
All improvements are significant at the p &lt; 0.01 level, except where marked with an asterisk (*), indicating p &lt; 0.05.
</tableCaption>
<bodyText confidence="0.99846">
In order to address the space problem, we use the
following heuristic: for any given rule, if the absolute
value of one of these features is less than log 2, we
discard it for that rule.
</bodyText>
<sectionHeader confidence="0.999792" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.948412315789474">
Setup We tested these features on two ma-
chine translation systems: a hierarchical phrase-
based (string-to-string) system (Chiang, 2005) and
a syntax-based (string-to-tree) system (Galley et al.,
2004; Galley et al., 2006). For Arabic-English trans-
lation, both systems were trained on 190+220 mil-
lion words of parallel data; for Chinese-English, the
string-to-string system was trained on 240+260 mil-
lion words of parallel data, and the string-to-tree sys-
tem, 58+65 million words. Both used two language
models, one trained on the combined English sides
of the Arabic-English and Chinese-English data, and
one trained on 4 billion words of English data.
The baseline string-to-string system already incor-
porates some simple provenance features: for each
s-feature s, there is a feature P(s  |rule). Both base-
line also include a variety of other features (Chiang
et al., 2008; Chiang et al., 2009; Chiang, 2010).
Both systems were trained using MIRA (Cram-
mer et al., 2006; Watanabe et al., 2007; Chiang et al.,
2008) on a held-out set, then tested on two more sets
(Dev and Test) disjoint from the data used for rule
extraction and for MIRA training. These datasets
have roughly 1000–3000 sentences (30,000–70,000
words) and are drawn from test sets from the NIST
MT evaluation and development sets from the GALE
program.
Individual tests We first tested morphological
smoothing using the string-to-string system on
Chinese-English translation. The morphologically
smoothed system generated the improved translation
(8) above, and generally gave a small improvement:
task features Dev
Chi-Eng nw baseline 28.7
morph 29.1
We then tested the provenance-conditioned fea-
tures on both Arabic-English and Chinese-English,
again using the string-to-string system:
</bodyText>
<table confidence="0.558025">
task features Dev
Ara-Eng nw baseline 47.1
(Matsoukas et al., 2009) 47.3
provenance2 47.7
Chi-Eng nw baseline 28.7
provenance2 29.4
</table>
<bodyText confidence="0.999872944444445">
The translations (12) and (13) come from the
Arabic-English baseline and provenance systems.
For Arabic-English, we also compared against lex-
ical weighting features that use sentence weights
kindly provided to us by Matsoukas et al. Our fea-
tures performed better, although it should be noted
that those sentence weights had been optimized for
a different translation model.
Combined tests Finally, we tested the features
across a wider range of tasks. For Chinese-English
translation, we combined the morphologically-
smoothed and provenance-conditioned lexical
weighting features; for Arabic-English, we con-
tinued to use only the provenance-conditioned
features. We tested using both systems, and on
both newswire and web genres. The results are
shown in Table 4. The features produce statistically
significant improvements across all 16 conditions.
</bodyText>
<footnote confidence="0.9947665">
2In these systems, an error crippled the t(f  |e), tm(f  |e), and
ts(f  |e) features. Time did not permit rerunning all of these sys-
tems with the error fixed, but partial results suggest that it did
not have a significant impact.
</footnote>
<page confidence="0.996752">
458
</page>
<figure confidence="0.998829692307692">
0.5
0.4
0.3
0.2
0.1
Web
0
-0.1
-0.2
-0.3
-0.4
-0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8
Newswire
</figure>
<figureCaption confidence="0.95183025">
Figure 1: Feature weights for provenance-conditioned features: string-to-string, Chinese-English, web versus
newswire. A higher weight indicates a more useful source of information, while a negative weight indicates a less
useful or possibly problematic source. For clarity, only selected points are labeled. The diagonal line indicates where
the two weights would be equal relative to the original t(e I f) feature weight.
</figureCaption>
<figure confidence="0.999004294117647">
LDC2006G05
NameEntity
wl
LDC2006E92
ng
UN
LDC2007E101
lexicon
bc
bn
LDC2006E24
nw
LDC2005T06
web
NewsExplorer
LDC2007E08
LDC2007E103 LDC2008G05
</figure>
<figureCaption confidence="0.591474">
Figure 1 shows the feature weights obtained for
</figureCaption>
<bodyText confidence="0.9903435">
the provenance-conditioned features tr(f I e) in the
string-to-string Chinese-English system, trained on
newswire and web data. On the diagonal are cor-
pora that were equally useful in either genre. Surpris-
ingly, the UN data received strong positive weights,
indicating usefulness in both genres. Two lists of
named entities received large weights: the LDC list
(LDC2005T34) in the positive direction and the
NewsExplorer list in the negative direction, sug-
gesting that there are noisy entries in the latter.
The corpus LDC2007E08, which contains parallel
data mined from comparable corpora (Munteanu and
Marcu, 2005), received strong negative weights.
Off the diagonal are corpora favored in only one
genre or the other: above, we see that the wt (we-
blog) and ng (newsgroup) genres are more help-
ful for web translation, as expected (although web
oddly seems less helpful), as well as LDC2006G05
(LDC/FBIS/NVTC Parallel Text V2.0). Below are
corpora more helpful for newswire translation,
like LDC2005T06 (Chinese News Translation Text
Part 1).
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99998">
Many different approaches to morphology and
provenance in machine translation are possible. We
have chosen to implement our approach as exten-
sions to lexical weighting (Koehn et al., 2003),
which is nearly ubiquitous, because it is defined at
the level of word alignments. For this reason, the
features we have introduced should be easily ap-
plicable to a wide range of phrase-based, hierarchi-
cal phrase-based, and syntax-based systems. While
the improvements obtained using them are not enor-
mous, we have demonstrated that they help signif-
icantly across many different conditions, and over
very strong baselines. We therefore fully expect that
these new features would yield similar improve-
ments in other systems as well.
</bodyText>
<sectionHeader confidence="0.998711" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999597333333333">
We would like to thank Spyros Matsoukas and col-
leagues at BBN for providing their sentence-level
weights and important insights into their corpus-
weighting work. This work was supported in part by
DARPA contract HR0011-06-C-0022 under subcon-
tract to BBN Technologies.
</bodyText>
<page confidence="0.998913">
459
</page>
<sectionHeader confidence="0.998344" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999773733333333">
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. EMNLP 2008, pages
224–233.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proc. NAACL HLT, pages 218–226.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL 2005,
pages 263–270.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. ACL, pages 1443–1452.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551–585.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Proc.
HLT-NAACL 2004, pages 273–280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich
syntactic translation models. In Proc. COLING-ACL
2006, pages 961–968.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003, pages 127–133.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proc. EMNLP 2009,
pages 708–717.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31:477–504.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proc. EMNLP-CoNLL
2007, pages 764–773.
Ian H. Witten and Timothy C. Bell. 1991. The
zero-frequency problem: Estimating the probabilities
of novel events in adaptive text compression. IEEE
Trans. Information Theory, 37(4):1085–1094.
</reference>
<page confidence="0.999211">
460
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.743130">
<title confidence="0.997614">Two Easy Improvements to Lexical Weighting</title>
<author confidence="0.820431">Chiang DeNeefe</author>
<affiliation confidence="0.947161">USC Information Sciences</affiliation>
<address confidence="0.965145">4676 Admiralty Way, Suite</address>
<author confidence="0.925367">Marina del Rey</author>
<author confidence="0.925367">CA</author>
<email confidence="0.998977">chiang@isi.edu</email>
<email confidence="0.998977">sdeneefe@isi.edu</email>
<email confidence="0.998977">pust@isi.edu</email>
<abstract confidence="0.99933825">We introduce two simple improvements to the lexical weighting features of Koehn, Och, and Marcu (2003) for machine translation: one which smooths the probability of translating word simplifying English morphology, and one which conditions it on the of training data that in. These new variations lead to improvements up to with an average improveof across two language pairs, two genres, and two translation systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP</booktitle>
<pages>224--233</pages>
<contexts>
<context position="11672" citStr="Chiang et al., 2008" startWordPosition="2003" endWordPosition="2006">, both systems were trained on 190+220 million words of parallel data; for Chinese-English, the string-to-string system was trained on 240+260 million words of parallel data, and the string-to-tree system, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data. The baseline string-to-string system already incorporates some simple provenance features: for each s-feature s, there is a feature P(s |rule). Both baseline also include a variety of other features (Chiang et al., 2008; Chiang et al., 2009; Chiang, 2010). Both systems were trained using MIRA (Crammer et al., 2006; Watanabe et al., 2007; Chiang et al., 2008) on a held-out set, then tested on two more sets (Dev and Test) disjoint from the data used for rule extraction and for MIRA training. These datasets have roughly 1000–3000 sentences (30,000–70,000 words) and are drawn from test sets from the NIST MT evaluation and development sets from the GALE program. Individual tests We first tested morphological smoothing using the string-to-string system on Chinese-English translation. The morphologically smoothed s</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proc. EMNLP 2008, pages 224–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. NAACL HLT,</booktitle>
<pages>218--226</pages>
<contexts>
<context position="11693" citStr="Chiang et al., 2009" startWordPosition="2007" endWordPosition="2010">rained on 190+220 million words of parallel data; for Chinese-English, the string-to-string system was trained on 240+260 million words of parallel data, and the string-to-tree system, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data. The baseline string-to-string system already incorporates some simple provenance features: for each s-feature s, there is a feature P(s |rule). Both baseline also include a variety of other features (Chiang et al., 2008; Chiang et al., 2009; Chiang, 2010). Both systems were trained using MIRA (Crammer et al., 2006; Watanabe et al., 2007; Chiang et al., 2008) on a held-out set, then tested on two more sets (Dev and Test) disjoint from the data used for rule extraction and for MIRA training. These datasets have roughly 1000–3000 sentences (30,000–70,000 words) and are drawn from test sets from the NIST MT evaluation and development sets from the GALE program. Individual tests We first tested morphological smoothing using the string-to-string system on Chinese-English translation. The morphologically smoothed system generated the i</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proc. NAACL HLT, pages 218–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="10935" citStr="Chiang, 2005" startWordPosition="1890" endWordPosition="1891">44.3 38.3 40.2 29.8 27.1 23.4 26.6 Table 4: Our variations on lexical weighting improve translation quality significantly across 16 different test conditions. All improvements are significant at the p &lt; 0.01 level, except where marked with an asterisk (*), indicating p &lt; 0.05. In order to address the space problem, we use the following heuristic: for any given rule, if the absolute value of one of these features is less than log 2, we discard it for that rule. 5 Experiments Setup We tested these features on two machine translation systems: a hierarchical phrasebased (string-to-string) system (Chiang, 2005) and a syntax-based (string-to-tree) system (Galley et al., 2004; Galley et al., 2006). For Arabic-English translation, both systems were trained on 190+220 million words of parallel data; for Chinese-English, the string-to-string system was trained on 240+260 million words of parallel data, and the string-to-tree system, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data. The baseline string-to-string system already incorporates some simple provenance f</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. ACL 2005, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1443--1452</pages>
<contexts>
<context position="11708" citStr="Chiang, 2010" startWordPosition="2011" endWordPosition="2012">lion words of parallel data; for Chinese-English, the string-to-string system was trained on 240+260 million words of parallel data, and the string-to-tree system, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data. The baseline string-to-string system already incorporates some simple provenance features: for each s-feature s, there is a feature P(s |rule). Both baseline also include a variety of other features (Chiang et al., 2008; Chiang et al., 2009; Chiang, 2010). Both systems were trained using MIRA (Crammer et al., 2006; Watanabe et al., 2007; Chiang et al., 2008) on a held-out set, then tested on two more sets (Dev and Test) disjoint from the data used for rule extraction and for MIRA training. These datasets have roughly 1000–3000 sentences (30,000–70,000 words) and are drawn from test sets from the NIST MT evaluation and development sets from the GALE program. Individual tests We first tested morphological smoothing using the string-to-string system on Chinese-English translation. The morphologically smoothed system generated the improved transla</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In Proc. ACL, pages 1443–1452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="11768" citStr="Crammer et al., 2006" startWordPosition="2019" endWordPosition="2023"> string-to-string system was trained on 240+260 million words of parallel data, and the string-to-tree system, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data. The baseline string-to-string system already incorporates some simple provenance features: for each s-feature s, there is a feature P(s |rule). Both baseline also include a variety of other features (Chiang et al., 2008; Chiang et al., 2009; Chiang, 2010). Both systems were trained using MIRA (Crammer et al., 2006; Watanabe et al., 2007; Chiang et al., 2008) on a held-out set, then tested on two more sets (Dev and Test) disjoint from the data used for rule extraction and for MIRA training. These datasets have roughly 1000–3000 sentences (30,000–70,000 words) and are drawn from test sets from the NIST MT evaluation and development sets from the GALE program. Individual tests We first tested morphological smoothing using the string-to-string system on Chinese-English translation. The morphologically smoothed system generated the improved translation (8) above, and generally gave a small improvement: task</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL</booktitle>
<pages>273--280</pages>
<contexts>
<context position="10999" citStr="Galley et al., 2004" startWordPosition="1897" endWordPosition="1900"> on lexical weighting improve translation quality significantly across 16 different test conditions. All improvements are significant at the p &lt; 0.01 level, except where marked with an asterisk (*), indicating p &lt; 0.05. In order to address the space problem, we use the following heuristic: for any given rule, if the absolute value of one of these features is less than log 2, we discard it for that rule. 5 Experiments Setup We tested these features on two machine translation systems: a hierarchical phrasebased (string-to-string) system (Chiang, 2005) and a syntax-based (string-to-tree) system (Galley et al., 2004; Galley et al., 2006). For Arabic-English translation, both systems were trained on 190+220 million words of parallel data; for Chinese-English, the string-to-string system was trained on 240+260 million words of parallel data, and the string-to-tree system, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data. The baseline string-to-string system already incorporates some simple provenance features: for each s-feature s, there is a feature P(s |rule). Bo</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. HLT-NAACL 2004, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL</booktitle>
<pages>961--968</pages>
<contexts>
<context position="11021" citStr="Galley et al., 2006" startWordPosition="1901" endWordPosition="1904"> improve translation quality significantly across 16 different test conditions. All improvements are significant at the p &lt; 0.01 level, except where marked with an asterisk (*), indicating p &lt; 0.05. In order to address the space problem, we use the following heuristic: for any given rule, if the absolute value of one of these features is less than log 2, we discard it for that rule. 5 Experiments Setup We tested these features on two machine translation systems: a hierarchical phrasebased (string-to-string) system (Chiang, 2005) and a syntax-based (string-to-tree) system (Galley et al., 2004; Galley et al., 2006). For Arabic-English translation, both systems were trained on 190+220 million words of parallel data; for Chinese-English, the string-to-string system was trained on 240+260 million words of parallel data, and the string-to-tree system, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data. The baseline string-to-string system already incorporates some simple provenance features: for each s-feature s, there is a feature P(s |rule). Both baseline also inclu</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. COLING-ACL 2006, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL</booktitle>
<pages>127--133</pages>
<contexts>
<context position="770" citStr="Koehn et al., 2003" startWordPosition="117" endWordPosition="120"> 1001 Marina del Rey, CA 90292 {chiang,sdeneefe,pust}@isi.edu Abstract We introduce two simple improvements to the lexical weighting features of Koehn, Och, and Marcu (2003) for machine translation: one which smooths the probability of translating word f to word e by simplifying English morphology, and one which conditions it on the kind of training data that f and e co-occurred in. These new variations lead to improvements of up to +0.8 BLEU, with an average improvement of +0.6 BLEU across two language pairs, two genres, and two translation systems. 1 Introduction Lexical weighting features (Koehn et al., 2003) estimate the probability of a phrase pair or translation rule word-by-word. In this paper, we introduce two simple improvements to these features: one which smooths the probability of translating word f to word e using English morphology, and one which conditions it on the kind of training data that f and e co-occurred in. These new variations lead to improvements of up to +0.8 BLEU, with an average improvement of +0.6 BLEU across two language pairs, two genres, and two translation systems. 2 Background Since there are slight variations in how the lexical weighting features are computed, we b</context>
<context position="15720" citStr="Koehn et al., 2003" startWordPosition="2620" endWordPosition="2623"> strong negative weights. Off the diagonal are corpora favored in only one genre or the other: above, we see that the wt (weblog) and ng (newsgroup) genres are more helpful for web translation, as expected (although web oddly seems less helpful), as well as LDC2006G05 (LDC/FBIS/NVTC Parallel Text V2.0). Below are corpora more helpful for newswire translation, like LDC2005T06 (Chinese News Translation Text Part 1). 6 Conclusion Many different approaches to morphology and provenance in machine translation are possible. We have chosen to implement our approach as extensions to lexical weighting (Koehn et al., 2003), which is nearly ubiquitous, because it is defined at the level of word alignments. For this reason, the features we have introduced should be easily applicable to a wide range of phrase-based, hierarchical phrase-based, and syntax-based systems. While the improvements obtained using them are not enormous, we have demonstrated that they help significantly across many different conditions, and over very strong baselines. We therefore fully expect that these new features would yield similar improvements in other systems as well. Acknowledgements We would like to thank Spyros Matsoukas and colle</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. HLT-NAACL 2003, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for machine translation.</title>
<date>2009</date>
<booktitle>In Proc. EMNLP</booktitle>
<pages>708--717</pages>
<contexts>
<context position="7956" citStr="Matsoukas et al. (2009)" startWordPosition="1369" endWordPosition="1372"> the proposed ruling systems. (12) MT (baseline): This may be one of the most important differences between pictures of the proposed ruling regimes. (13) MT (better): Perhaps this is one of the most important differences between the images of the proposed regimes. The Arabic word ﻞﻌﻟو can be translated as may or perhaps (among others), with the latter more common according to t(e |f), as shown in Table 3. But some genres favor perhaps more or less strongly. Thus, both translations (12) and (13) are good, but the latter uses a slightly more informal register appropriate to the genre. Following Matsoukas et al. (2009), we assign each training sentence pair a set of binary features which we call s-features: f e t(e |f) nw ts(e |f) un – web bn ﻞﻌﻟو may 0.13 0.12 0.16 0.09 0.13 ﻞﻌﻟو perhaps 0.20 0.23 0.32 0.42 0.19 Table 3: Different genres have different preferences for word translations. Key: nw = newswire, web = Web, bn = broadcast news, un = United Nations proceedings. • Whether the sentence pair came from a particular genre, for example, newswire or web • Whether the sentence pair came from a particular collection, for example, FBIS or UN Matsoukas et al. (2009) use these s-features to compute weights fo</context>
<context position="12621" citStr="Matsoukas et al., 2009" startWordPosition="2146" endWordPosition="2149">0,000–70,000 words) and are drawn from test sets from the NIST MT evaluation and development sets from the GALE program. Individual tests We first tested morphological smoothing using the string-to-string system on Chinese-English translation. The morphologically smoothed system generated the improved translation (8) above, and generally gave a small improvement: task features Dev Chi-Eng nw baseline 28.7 morph 29.1 We then tested the provenance-conditioned features on both Arabic-English and Chinese-English, again using the string-to-string system: task features Dev Ara-Eng nw baseline 47.1 (Matsoukas et al., 2009) 47.3 provenance2 47.7 Chi-Eng nw baseline 28.7 provenance2 29.4 The translations (12) and (13) come from the Arabic-English baseline and provenance systems. For Arabic-English, we also compared against lexical weighting features that use sentence weights kindly provided to us by Matsoukas et al. Our features performed better, although it should be noted that those sentence weights had been optimized for a different translation model. Combined tests Finally, we tested the features across a wider range of tasks. For Chinese-English translation, we combined the morphologicallysmoothed and proven</context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proc. EMNLP 2009, pages 708–717.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--477</pages>
<contexts>
<context position="15091" citStr="Munteanu and Marcu, 2005" startWordPosition="2521" endWordPosition="2524">ined for the provenance-conditioned features tr(f I e) in the string-to-string Chinese-English system, trained on newswire and web data. On the diagonal are corpora that were equally useful in either genre. Surprisingly, the UN data received strong positive weights, indicating usefulness in both genres. Two lists of named entities received large weights: the LDC list (LDC2005T34) in the positive direction and the NewsExplorer list in the negative direction, suggesting that there are noisy entries in the latter. The corpus LDC2007E08, which contains parallel data mined from comparable corpora (Munteanu and Marcu, 2005), received strong negative weights. Off the diagonal are corpora favored in only one genre or the other: above, we see that the wt (weblog) and ng (newsgroup) genres are more helpful for web translation, as expected (although web oddly seems less helpful), as well as LDC2006G05 (LDC/FBIS/NVTC Parallel Text V2.0). Below are corpora more helpful for newswire translation, like LDC2005T06 (Chinese News Translation Text Part 1). 6 Conclusion Many different approaches to morphology and provenance in machine translation are possible. We have chosen to implement our approach as extensions to lexical w</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31:477–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="5174" citStr="Porter, 1980" startWordPosition="882" endWordPosition="883"> LM. Likewise, the 5-grams good friend and good partner and good friends and good partners are both observed in our LMs, but neither good friend and good partners nor good friends and good partner is. By contrast, the lexical weighting tables (Table 2, columns 3–4), which ignore context, have a strong preference for plural translations, except in the case of t(朋友 |friend). Therefore we hypothesize that, for Chinese-English translation, we should weaken the lexical weighting features’ morphological preferences so that more contextual features can do their work. Running a morphological stemmer (Porter, 1980) on the English side of the parallel data gives a three-way parallel text: for each sentence, we have French f, English e, and stemmed English e′. We can then build two word translation tables, t(e′ |f) and t(e |e′), and form their product tm(e |f) = � t(e′ |f)t(e |e′) (9) e′ Similarly, we can compute tm(f |e) in the opposite direction.2 (See Table 2, columns 5–6.) These tables can then be extended to phrase pairs or synchronous CFG rules as before and added as two new features of the model: −logtm(¯e |f¯) and − logtm(f¯ |¯e) The feature tm(¯e |f¯) does still prefer certain wordforms, as can b</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukuda</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL</booktitle>
<pages>764--773</pages>
<contexts>
<context position="11791" citStr="Watanabe et al., 2007" startWordPosition="2024" endWordPosition="2027">em was trained on 240+260 million words of parallel data, and the string-to-tree system, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data. The baseline string-to-string system already incorporates some simple provenance features: for each s-feature s, there is a feature P(s |rule). Both baseline also include a variety of other features (Chiang et al., 2008; Chiang et al., 2009; Chiang, 2010). Both systems were trained using MIRA (Crammer et al., 2006; Watanabe et al., 2007; Chiang et al., 2008) on a held-out set, then tested on two more sets (Dev and Test) disjoint from the data used for rule extraction and for MIRA training. These datasets have roughly 1000–3000 sentences (30,000–70,000 words) and are drawn from test sets from the NIST MT evaluation and development sets from the GALE program. Individual tests We first tested morphological smoothing using the string-to-string system on Chinese-English translation. The morphologically smoothed system generated the improved translation (8) above, and generally gave a small improvement: task features Dev Chi-Eng n</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukuda, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proc. EMNLP-CoNLL 2007, pages 764–773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Timothy C Bell</author>
</authors>
<title>The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<journal>IEEE Trans. Information Theory,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="9648" citStr="Witten and Bell, 1991" startWordPosition="1659" endWordPosition="1662">ernative that incorporates s-features into the model itself. For each s-feature s, we compute new word translation tables ts(e |f) and ts(f |e) estimated from only those sentence pairsf on which s fires, and extend them to phrases/rules as before. The idea is to use these probabilities as new features in the model. However, two challenges arise: first, many word pairs are unseen for a given s, resulting in zero or undefined probabilities; second, this adds many new features for each rule, which requires a lot of space. To address the problem of unseen word pairs, we use Witten-Bell smoothing (Witten and Bell, 1991): ˆts(e |f) = Afsts(e |f) + (1 − Afs)t(e |f) (14) c(f, s) (15) c(f, s) + d(f, s) where c(f, s) is the number of times f has been observed in sentences with s-feature s, and d(f, s) is the number of e types observed aligned to f in sentences with s-feature s. For each s-feature s, we add two model features Afs = ˆts(f¯ |¯e) ˆts(¯e |f¯) f¯) and − log − log t(¯e | t(f¯ |¯e) 457 system features Arabic-English Chinese-English Test newswire web newswire web Dev Test Dev Test Dev Test Dev string-to-string baseline 47.1 43.8 37.1 38.4 28.7 26.0 23.2 25.9 full2 47.7 44.2* 37.4 39.0 29.5 26.8 23.8 26.3 </context>
</contexts>
<marker>Witten, Bell, 1991</marker>
<rawString>Ian H. Witten and Timothy C. Bell. 1991. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Trans. Information Theory, 37(4):1085–1094.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>