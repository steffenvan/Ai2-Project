<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.036464">
<title confidence="0.759383">
Improved Statistical Alignment Models
</title>
<note confidence="0.91354225">
Franz Josef Och and Hermann Ney
Lehrstuhl fiir Informatik VI, Computer Science Department
RWTH Aachen - University of Technology
D-52056 Aachen, Germany
</note>
<email confidence="0.782793">
foch,neyl@informatik.rwth-aachen.de
</email>
<sectionHeader confidence="0.934416" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929210526316">
In this paper, we present and com-
pare various single-word based align-
ment models for statistical machine
translation. We discuss the five
IBM alignment models, the Hidden-
Markov alignment model, smooth-
ing techniques and various modifica-
tions. We present different methods
to combine alignments. As evalua-
tion criterion we use the quality of
the resulting Viterbi alignment com-
pared to a manually produced refer-
ence alignment. We show that mod-
els with a first-order dependence and
a fertility model lead to significantly
better results than the simple mod-
els IBM-1 or IBM-2, which are not
able to go beyond zero-order depen-
dencies.
</bodyText>
<sectionHeader confidence="0.979386" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998730868421053">
In statistical machine translation we set up a
statistical translation model Pr(fillef) which
describes the relationship between a source
language (SL) string f and a target lan-
guage (TL) string ef. In (statistical) align-
ment models Pr(fil , aflef), a &apos;hidden&apos; align-
ment a is introduced which describes a map-
ping from source word fi to a target word Ca,.
We discuss here the IBM translation mod-
els IBM-1 to IBM-5 (Brown et al., 1993b) and
the Hidden-Markov alignment model (Vogel
et al., 1996; Och and Ney, 2000). The differ-
ent alignment models we present provide dif-
ferent decompositions of Pr(fil ,41e{). An
alignment â for which holds
= arg max Pr(fil
for a specific model is called Viterbi alignment
of this model.
So far, no well established evaluation crite-
rion exists in the literature for these align-
ment models. For various reasons (non-
unique reference translation, over-fitting and
statistically deficient models) it seems hard
to use training/test perplexity as in language
modeling. Using translation quality is prob-
lematic, as translation quality is not well de-
fined and as there are additional influences
such as language model or decoder proper-
ties. We propose in this paper to measure the
quality of an alignment model using the qual-
ity of the Viterbi alignment compared to a
manually produced alignment. This allows an
automatic evaluation, once a reference align-
ment has been produced. In addition, it re-
sults in a very precise and reliable evaluation
criterion that is well suited to assess various
design decisions in modeling and training of
statistical alignment models.
</bodyText>
<sectionHeader confidence="0.915823" genericHeader="introduction">
2 Models
</sectionHeader>
<bodyText confidence="0.997269571428571">
In this paper we use the models IBM-1
to IBM-5 from (Brown et al., 1993b) and
the Hidden-Markov alignment model (HMM)
from (Vogel et al., 1996; Och and Ney, 2000).
All these models provide different decompo-
sitions of the probability Pr(fil ,41e{). The
alignment a may contain alignments ai =
0 with the &apos;empty&apos; word co to account for
French words that are not aligned to any En-
glish word. All models include lexicon pa-
rameters p(f le) and additional parameters de-
scribing the probability of an alignment.
We now sketch the structure of the six mod-
els:
</bodyText>
<listItem confidence="0.963814789473684">
• In IBM-1 all alignments have the same
probability.
• IBM-2 uses a zero-order alignment model
P(aili,I, -I) where different alignment
positions are independent from each
other.
• The HMM uses a first-order model
where the alignment position
ai depends on the previous alignment po-
sition ai_l.
• In IBM-3 we have an (inverted) zero-
order alignment model p(jlaj , I, J) with
an additional fertility model p(Ole) which
describes the number of words çf aligned
to an English word e.
• In IBM-4 we have an (inverted) first-
order alignment model p(jIji) and a fer-
tility model *Ole).
• The models IBM-3 and IBM-4 are defi-
</listItem>
<bodyText confidence="0.928343">
cient as they waste probability mass on
non-strings. IBM-5 is a reformulation of
IBM-4 with a suitably refined alignment
model in order to avoid deficiency.
So the main differences of these models lie
in the alignment model (which may be zero-
order or first-order), in the existence of an
explicit fertility model and whether the model
is deficient or not.
For HMM, IBM-4 and IBM-5 it is straight-
forward to extend the alignment parameters
to include a dependence on the word classes of
the words around the alignment position. In
the HMM alignment model we allow for a de-
pendence from the class E = C(ea,_,). Cor-
respondingly, we can include similar depen-
dencies on French and English word classes in
IBM-4 and IBM-5 (Brown et al., 1993b). The
classification of the words into a given number
of classes (here: 50) is performed automati-
cally by another statistical learning procedure
(Kneser and Ney, 1991).
</bodyText>
<sectionHeader confidence="0.946791" genericHeader="method">
3 Training&apos;
</sectionHeader>
<bodyText confidence="0.999181333333333">
The training of all alignment models is done
by the EM-algorithm using a parallel training
corpus (f(8), e(s)), s = 1, . . . ,S . In the E-
step the counts for one sentence pair (f, e)
are calculated. For the lexicon parameters the
counts are:
</bodyText>
<equation confidence="0.87288175">
cu le; f, e) = E Pr(alf, e) E (5(f f)5 (e, ea„)
a
In the M-step the lexicon parameters are:
P(fle) cxE c(fle; f(,), e(s))
</equation>
<bodyText confidence="0.942445904761905">
Correspondingly, the alignment and fertility
probabilities can be estimated.
The models IBM-1, IBM-2 and HMM have
a particularly simple mathematical form so
that the EM algorithm can be performed ex-
actly, i.e. in the E-step it is possible to effi-
ciently consider all alignments. For the HMM
we do this using the Baum-Welch algorithm
(Baum, 1972).
Since there is no efficient way in the fertil-
ity models IBM-3 to 5 to avoid the explicit
summation over all alignments in the EM-
algorithm, the counts are collected only over
a subset of promising alignments. For IBM-
3, IBM-4 and IBM-5 we perform the count
collection only over a small number of good
alignments. In order to keep the training fast
we can take into account only a small fraction
of all alignments. We will compare three dif-
ferent possibilities of using subsets of different
size:
</bodyText>
<listItem confidence="0.992302">
• The simplest possibility is to perform
Viterbi training using only the best align-
ment that can be found. As the calcula-
tion of the Viterbi alignment itself is very
time-consuming it is computed only ap-
proximately using the method described
in (Brown et al., 1993b).
• In (Al-Onaizan et al., 1999) it was sug-
gested to use also the neighboring align-
ments (i.e. alignments differing by one
</listItem>
<tableCaption confidence="0.89915075">
1-Our implementation of the IBM translation mod-
els is based on GIZA which is part of the publicly avail-
able toolkit for statistical machine translation EGYPT
(Al-Onaizan et al., 1999).
</tableCaption>
<figure confidence="0.99945695">
3.5
3
2.5
2
Average fertility
e
➬
➬
1.5
1
0.5
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22
#Letters
.
that
at
it
leave
us
let
,
say
would
I
then
,
yes
ja
,
.
dann
w&amp;quot;urde
ich
sagen
,
verbleiben
wir
so
Model
0.14
viterbi
+neighbors
+pegging
0.12
✷
✷
E
0.1
AER
0.08
0.06
1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 5 5
0.14
1-2-3-4-5
1-HMM-3-4-5
1-HMM-4
0.12
AER
✷
0.1
E
✷
0.08
0.06
1 1 1 1 1 2/H 2/H 2/H 2/H 2/H 3/4 3/4 3/4 3/4 3/4 4 4 4 4 4 5 5 5
Model
1 1 1 1 1 2/H 2/H 2/H 2/H 2/H 3/4 3/4 3/4 3/4 3/4 4
4 4 4 4 5 5 5
1 1 1 1 1 H H H H H 4 4 4 4 4 4 4 4
Model
0.3
1-2-3-4-5
1-HMM-3-4-5
1-HMM-4
0.28
0.26
0.24
✷
0.22
AER
E
✷
0.14
0.2
0.18
0.16
Model
0.14
0.12
0.1
AER
E
✷
0.08
0.06
✷
1-H-4:standard
1-H-4:modifiedModel4
1-H-4:modifiedModel4+smoothing
</figure>
<tableCaption confidence="0.996678">
Table 5: Alignment quality in last iteration of IBM-4 of both translation directions.
</tableCaption>
<table confidence="0.999345285714286">
SL-TL TL -&gt; SL
Corpus prec rec AER prec rec AER
VERBMOBIL 93.2 95.5 5.8 90.0 87.9 10.9
HANSARDS(50k) 80.5 91.2 15.6 80.0 90.8 16.0
HANSARDS(200k) 84.3 93.1 12.5 84.2 93.4 12.4
HANSARDS(500k) 86.5 94.2 10.7 86.9 94.4 10.3
HANsARDs(1500k) 88.1 94.9 9.4 88.5 95.0 9.0
</table>
<tableCaption confidence="0.956774">
Table 6: Effect of combination of IBM-4 Viterbi alignments from both translation directions.
</tableCaption>
<table confidence="0.999806857142857">
Intersection Union Refined
Corpus prec rec AER prec rec AER prec rec AER
VERBMOBIL 97.9 85.4 8.0 87.3 98.0 8.6 93.3 96.4 5.4
HANSARDS(50k) 95.7 85.6 9.0 72.6 96.6 20.2 85.9 92.3 11.7
HANSARDS(200k) 96.7 89.0 6.8 77.5 97.5 16.3 88.3 94.5 9.4
HANSARDS(500k) 96.9 90.9 5.8 80.7 97.8 13.8 90.1 95.1 8.0
HANSARDS (1500k) 96.8 91.9 5.3 83.0 98.0 12.1 90.4 95.6 7.6
</table>
<sectionHeader confidence="0.935079" genericHeader="method">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999945142857143">
This work has been partially supported as
part of the Verbmobil project (contract num-
ber 01 IV 701 T4) by the German Fed-
eral Ministry of Education, Science, Research
and Technology and as part of the EuTrans
project by the by the European Community
(ESPRIT project number 30268).
</bodyText>
<sectionHeader confidence="0.993079" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999949204545455">
L. Ahrenberg, M. Merkel, H. A. Sagvall, and
J. Tiedemann. 2000. Evaluation of word
alignment systems. In Proceedings of the Sec-
ond International Conference on Language Re-
sources and Evaluation (LREC), pages 1255-
1261, Athens, Greece, May/June.
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight,
J. Lafferty, I. D. Melamed, F. J.
Och, D. Purdy, N. A. Smith, and
D. Yarowsky. 1999. Statistical machine
translation, final report, JHU workshop.
http : //www. clsp . jhu. edu/ws99/projects/
mt/f inal_report/mt-f inal-report .ps.
L.E. Baum. 1972. An Inequality and Associated
Maximization Technique in Statistical Estima-
tion for Probabilistic Functions of Markov Pro-
cesses. Inequalities, 3:1-8.
P. Brown, S. A. Della Pietra, V. 1 Della Pietra,
M. 1 Goldsmith, J. Hajic, R. L. Mercer, and
S. Mohanty. 1993a. But dictionaries are data
too. In Human Language Technology, pages
202-205.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993b. The mathematics of
statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263-
311.
R. Kneser and H. Ney. 1991. Forming Word
Classes by Statistical Clustering for Statistical
Language Modelling. In I. Quantitative Lin-
guistics Conference, September.
I. D. Melamed. 1998. Manual annotation of
translational equivalence: The Blinker project.
Technical Report 98-07, IRCS.
F. J. Och and H. Ney. 2000. A comparison of
alignment models for statistical machine trans-
lation. In Proc. of the 18th Int. Conf. on Com-
putational Linguistics, Saarbrficken, Germany,
August.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation.
In COLING &apos;96: The 16th Int. Conf. on Com-
putational Linguistics, pages 836-841, Copen-
hagen, August.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.728959">
<title confidence="0.998732">Improved Statistical Alignment Models</title>
<author confidence="0.999961">Franz Josef Och</author>
<author confidence="0.999961">Hermann Ney</author>
<affiliation confidence="0.99392">Lehrstuhl fiir Informatik VI, Computer Science Department RWTH Aachen - University of Technology</affiliation>
<address confidence="0.999787">D-52056 Aachen, Germany</address>
<abstract confidence="0.9868336">In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden- Markov alignment model, smoothing techniques and various modifications. We present different methods to combine alignments. As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Ahrenberg</author>
<author>M Merkel</author>
<author>H A Sagvall</author>
<author>J Tiedemann</author>
</authors>
<title>Evaluation of word alignment systems.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1255--1261</pages>
<location>Athens, Greece, May/June.</location>
<marker>Ahrenberg, Merkel, Sagvall, Tiedemann, 2000</marker>
<rawString>L. Ahrenberg, M. Merkel, H. A. Sagvall, and J. Tiedemann. 2000. Evaluation of word alignment systems. In Proceedings of the Second International Conference on Language Resources and Evaluation (LREC), pages 1255-1261, Athens, Greece, May/June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>J Curin</author>
<author>M Jahr</author>
<author>K Knight</author>
<author>J Lafferty</author>
<author>I D Melamed</author>
<author>F J Och</author>
<author>D Purdy</author>
<author>N A Smith</author>
<author>D Yarowsky</author>
</authors>
<title>Statistical machine translation, final report, JHU workshop. http : //www. clsp . jhu. edu/ws99/projects/ mt/f inal_report/mt-f inal-report .ps.</title>
<date>1999</date>
<contexts>
<context position="6068" citStr="Al-Onaizan et al., 1999" startWordPosition="1021" endWordPosition="1024"> over a subset of promising alignments. For IBM3, IBM-4 and IBM-5 we perform the count collection only over a small number of good alignments. In order to keep the training fast we can take into account only a small fraction of all alignments. We will compare three different possibilities of using subsets of different size: • The simplest possibility is to perform Viterbi training using only the best alignment that can be found. As the calculation of the Viterbi alignment itself is very time-consuming it is computed only approximately using the method described in (Brown et al., 1993b). • In (Al-Onaizan et al., 1999) it was suggested to use also the neighboring alignments (i.e. alignments differing by one 1-Our implementation of the IBM translation models is based on GIZA which is part of the publicly available toolkit for statistical machine translation EGYPT (Al-Onaizan et al., 1999). 3.5 3 2.5 2 Average fertility e ➬ ➬ 1.5 1 0.5 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #Letters . that at it leave us let , say would I then , yes ja , . dann w&amp;quot;urde ich sagen , verbleiben wir so Model 0.14 viterbi +neighbors +pegging 0.12 ✷ ✷ E 0.1 AER 0.08 0.06 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 5 5 </context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty, I. D. Melamed, F. J. Och, D. Purdy, N. A. Smith, and D. Yarowsky. 1999. Statistical machine translation, final report, JHU workshop. http : //www. clsp . jhu. edu/ws99/projects/ mt/f inal_report/mt-f inal-report .ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
</authors>
<title>An Inequality and Associated Maximization Technique</title>
<date>1972</date>
<booktitle>in Statistical Estimation for Probabilistic Functions of Markov Processes. Inequalities,</booktitle>
<pages>3--1</pages>
<contexts>
<context position="5274" citStr="Baum, 1972" startWordPosition="883" endWordPosition="884">(f(8), e(s)), s = 1, . . . ,S . In the Estep the counts for one sentence pair (f, e) are calculated. For the lexicon parameters the counts are: cu le; f, e) = E Pr(alf, e) E (5(f f)5 (e, ea„) a In the M-step the lexicon parameters are: P(fle) cxE c(fle; f(,), e(s)) Correspondingly, the alignment and fertility probabilities can be estimated. The models IBM-1, IBM-2 and HMM have a particularly simple mathematical form so that the EM algorithm can be performed exactly, i.e. in the E-step it is possible to efficiently consider all alignments. For the HMM we do this using the Baum-Welch algorithm (Baum, 1972). Since there is no efficient way in the fertility models IBM-3 to 5 to avoid the explicit summation over all alignments in the EMalgorithm, the counts are collected only over a subset of promising alignments. For IBM3, IBM-4 and IBM-5 we perform the count collection only over a small number of good alignments. In order to keep the training fast we can take into account only a small fraction of all alignments. We will compare three different possibilities of using subsets of different size: • The simplest possibility is to perform Viterbi training using only the best alignment that can be foun</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>L.E. Baum. 1972. An Inequality and Associated Maximization Technique in Statistical Estimation for Probabilistic Functions of Markov Processes. Inequalities, 3:1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>M</author>
</authors>
<title>But dictionaries are data too. In Human Language Technology,</title>
<date>1993</date>
<pages>202--205</pages>
<marker>Pietra, M, 1993</marker>
<rawString>P. Brown, S. A. Della Pietra, V. 1 Della Pietra, M. 1 Goldsmith, J. Hajic, R. L. Mercer, and S. Mohanty. 1993a. But dictionaries are data too. In Human Language Technology, pages 202-205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="1313" citStr="Brown et al., 1993" startWordPosition="203" endWordPosition="206">rder dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies. 1 Introduction In statistical machine translation we set up a statistical translation model Pr(fillef) which describes the relationship between a source language (SL) string f and a target language (TL) string ef. In (statistical) alignment models Pr(fil , aflef), a &apos;hidden&apos; alignment a is introduced which describes a mapping from source word fi to a target word Ca,. We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000). The different alignment models we present provide different decompositions of Pr(fil ,41e{). An alignment â for which holds = arg max Pr(fil for a specific model is called Viterbi alignment of this model. So far, no well established evaluation criterion exists in the literature for these alignment models. For various reasons (nonunique reference translation, over-fitting and statistically deficient models) it seems hard to use training/test perplexity as in language modeling. Using translation quality is problema</context>
<context position="2554" citStr="Brown et al., 1993" startWordPosition="406" endWordPosition="409">lity is not well defined and as there are additional influences such as language model or decoder properties. We propose in this paper to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually produced alignment. This allows an automatic evaluation, once a reference alignment has been produced. In addition, it results in a very precise and reliable evaluation criterion that is well suited to assess various design decisions in modeling and training of statistical alignment models. 2 Models In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000). All these models provide different decompositions of the probability Pr(fil ,41e{). The alignment a may contain alignments ai = 0 with the &apos;empty&apos; word co to account for French words that are not aligned to any English word. All models include lexicon parameters p(f le) and additional parameters describing the probability of an alignment. We now sketch the structure of the six models: • In IBM-1 all alignments have the same probability. • IBM-2 uses a zero-order alignment model P(aili,I, -I) where diff</context>
<context position="4382" citStr="Brown et al., 1993" startWordPosition="724" endWordPosition="727">ment model in order to avoid deficiency. So the main differences of these models lie in the alignment model (which may be zeroorder or first-order), in the existence of an explicit fertility model and whether the model is deficient or not. For HMM, IBM-4 and IBM-5 it is straightforward to extend the alignment parameters to include a dependence on the word classes of the words around the alignment position. In the HMM alignment model we allow for a dependence from the class E = C(ea,_,). Correspondingly, we can include similar dependencies on French and English word classes in IBM-4 and IBM-5 (Brown et al., 1993b). The classification of the words into a given number of classes (here: 50) is performed automatically by another statistical learning procedure (Kneser and Ney, 1991). 3 Training&apos; The training of all alignment models is done by the EM-algorithm using a parallel training corpus (f(8), e(s)), s = 1, . . . ,S . In the Estep the counts for one sentence pair (f, e) are calculated. For the lexicon parameters the counts are: cu le; f, e) = E Pr(alf, e) E (5(f f)5 (e, ea„) a In the M-step the lexicon parameters are: P(fle) cxE c(fle; f(,), e(s)) Correspondingly, the alignment and fertility probabil</context>
<context position="6034" citStr="Brown et al., 1993" startWordPosition="1015" endWordPosition="1018">he counts are collected only over a subset of promising alignments. For IBM3, IBM-4 and IBM-5 we perform the count collection only over a small number of good alignments. In order to keep the training fast we can take into account only a small fraction of all alignments. We will compare three different possibilities of using subsets of different size: • The simplest possibility is to perform Viterbi training using only the best alignment that can be found. As the calculation of the Viterbi alignment itself is very time-consuming it is computed only approximately using the method described in (Brown et al., 1993b). • In (Al-Onaizan et al., 1999) it was suggested to use also the neighboring alignments (i.e. alignments differing by one 1-Our implementation of the IBM translation models is based on GIZA which is part of the publicly available toolkit for statistical machine translation EGYPT (Al-Onaizan et al., 1999). 3.5 3 2.5 2 Average fertility e ➬ ➬ 1.5 1 0.5 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #Letters . that at it leave us let , say would I then , yes ja , . dann w&amp;quot;urde ich sagen , verbleiben wir so Model 0.14 viterbi +neighbors +pegging 0.12 ✷ ✷ E 0.1 AER 0.08 0.06 1 1 1 1 </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993b. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Forming Word Classes by Statistical Clustering for Statistical Language Modelling.</title>
<date>1991</date>
<booktitle>In I. Quantitative Linguistics Conference,</booktitle>
<contexts>
<context position="4551" citStr="Kneser and Ney, 1991" startWordPosition="750" endWordPosition="753">of an explicit fertility model and whether the model is deficient or not. For HMM, IBM-4 and IBM-5 it is straightforward to extend the alignment parameters to include a dependence on the word classes of the words around the alignment position. In the HMM alignment model we allow for a dependence from the class E = C(ea,_,). Correspondingly, we can include similar dependencies on French and English word classes in IBM-4 and IBM-5 (Brown et al., 1993b). The classification of the words into a given number of classes (here: 50) is performed automatically by another statistical learning procedure (Kneser and Ney, 1991). 3 Training&apos; The training of all alignment models is done by the EM-algorithm using a parallel training corpus (f(8), e(s)), s = 1, . . . ,S . In the Estep the counts for one sentence pair (f, e) are calculated. For the lexicon parameters the counts are: cu le; f, e) = E Pr(alf, e) E (5(f f)5 (e, ea„) a In the M-step the lexicon parameters are: P(fle) cxE c(fle; f(,), e(s)) Correspondingly, the alignment and fertility probabilities can be estimated. The models IBM-1, IBM-2 and HMM have a particularly simple mathematical form so that the EM algorithm can be performed exactly, i.e. in the E-ste</context>
</contexts>
<marker>Kneser, Ney, 1991</marker>
<rawString>R. Kneser and H. Ney. 1991. Forming Word Classes by Statistical Clustering for Statistical Language Modelling. In I. Quantitative Linguistics Conference, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Manual annotation of translational equivalence: The Blinker project.</title>
<date>1998</date>
<tech>Technical Report 98-07, IRCS.</tech>
<marker>Melamed, 1998</marker>
<rawString>I. D. Melamed. 1998. Manual annotation of translational equivalence: The Blinker project. Technical Report 98-07, IRCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proc. of the 18th Int. Conf. on Computational Linguistics,</booktitle>
<location>Saarbrficken, Germany,</location>
<contexts>
<context position="1393" citStr="Och and Ney, 2000" startWordPosition="216" endWordPosition="219">the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies. 1 Introduction In statistical machine translation we set up a statistical translation model Pr(fillef) which describes the relationship between a source language (SL) string f and a target language (TL) string ef. In (statistical) alignment models Pr(fil , aflef), a &apos;hidden&apos; alignment a is introduced which describes a mapping from source word fi to a target word Ca,. We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000). The different alignment models we present provide different decompositions of Pr(fil ,41e{). An alignment â for which holds = arg max Pr(fil for a specific model is called Viterbi alignment of this model. So far, no well established evaluation criterion exists in the literature for these alignment models. For various reasons (nonunique reference translation, over-fitting and statistically deficient models) it seems hard to use training/test perplexity as in language modeling. Using translation quality is problematic, as translation quality is not well defined and as there are additional infl</context>
<context position="2645" citStr="Och and Ney, 2000" startWordPosition="421" endWordPosition="424">ecoder properties. We propose in this paper to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually produced alignment. This allows an automatic evaluation, once a reference alignment has been produced. In addition, it results in a very precise and reliable evaluation criterion that is well suited to assess various design decisions in modeling and training of statistical alignment models. 2 Models In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000). All these models provide different decompositions of the probability Pr(fil ,41e{). The alignment a may contain alignments ai = 0 with the &apos;empty&apos; word co to account for French words that are not aligned to any English word. All models include lexicon parameters p(f le) and additional parameters describing the probability of an alignment. We now sketch the structure of the six models: • In IBM-1 all alignments have the same probability. • IBM-2 uses a zero-order alignment model P(aili,I, -I) where different alignment positions are independent from each other. • The HMM uses a first-order mod</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. A comparison of alignment models for statistical machine translation. In Proc. of the 18th Int. Conf. on Computational Linguistics, Saarbrficken, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMMbased word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING &apos;96: The 16th Int. Conf. on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen,</location>
<contexts>
<context position="1373" citStr="Vogel et al., 1996" startWordPosition="212" endWordPosition="215">better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies. 1 Introduction In statistical machine translation we set up a statistical translation model Pr(fillef) which describes the relationship between a source language (SL) string f and a target language (TL) string ef. In (statistical) alignment models Pr(fil , aflef), a &apos;hidden&apos; alignment a is introduced which describes a mapping from source word fi to a target word Ca,. We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000). The different alignment models we present provide different decompositions of Pr(fil ,41e{). An alignment â for which holds = arg max Pr(fil for a specific model is called Viterbi alignment of this model. So far, no well established evaluation criterion exists in the literature for these alignment models. For various reasons (nonunique reference translation, over-fitting and statistically deficient models) it seems hard to use training/test perplexity as in language modeling. Using translation quality is problematic, as translation quality is not well defined and as there</context>
<context position="2625" citStr="Vogel et al., 1996" startWordPosition="417" endWordPosition="420"> language model or decoder properties. We propose in this paper to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually produced alignment. This allows an automatic evaluation, once a reference alignment has been produced. In addition, it results in a very precise and reliable evaluation criterion that is well suited to assess various design decisions in modeling and training of statistical alignment models. 2 Models In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000). All these models provide different decompositions of the probability Pr(fil ,41e{). The alignment a may contain alignments ai = 0 with the &apos;empty&apos; word co to account for French words that are not aligned to any English word. All models include lexicon parameters p(f le) and additional parameters describing the probability of an alignment. We now sketch the structure of the six models: • In IBM-1 all alignments have the same probability. • IBM-2 uses a zero-order alignment model P(aili,I, -I) where different alignment positions are independent from each other. • The HMM us</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMMbased word alignment in statistical translation. In COLING &apos;96: The 16th Int. Conf. on Computational Linguistics, pages 836-841, Copenhagen, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>