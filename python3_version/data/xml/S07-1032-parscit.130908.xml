<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028741">
<title confidence="0.921664">
GPLSI: Word Coarse-grained Disambiguation aided by Basic Level
Concepts*
</title>
<author confidence="0.997185">
Rub´en Izquierdo Armando Su´arez
</author>
<affiliation confidence="0.884988666666667">
GPLSI Group, DLSI
University of Alicante
Spain
</affiliation>
<email confidence="0.994043">
{ruben, armando}@dlsi.ua.es
</email>
<author confidence="0.561584">
German Rigau
</author>
<affiliation confidence="0.592394333333333">
IXA NLP Group
EHU/UPV
Donostia, Basque Country
</affiliation>
<email confidence="0.993439">
german.rigau@ehu.es
</email>
<sectionHeader confidence="0.995564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999890941176471">
We present a corpus-based supervised lear-
ning system for coarse-grained sense disam-
biguation. In addition to usual features for
training in word sense disambiguation, our
system also uses Base Level Concepts au-
tomatically obtained from WordNet. Base
Level Concepts are some synsets that gene-
ralize a hyponymy sub–hierarchy, and pro-
vides an extra level of abstraction as well as
relevant information about the context of a
word to be disambiguated. Our experiments
proved that using this type of features re-
sults on a significant improvement of preci-
sion. Our system has achieved almost 0.8 F1
(fifth place) in the coarse–grained English
all-words task using a very simple set of fea-
tures plus Base Level Concepts annotation.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.975367333333333">
The GPLSI system in SemEval’s task 7, coarse–
grained English all-words, consists of a corpus-
based supervised-learning method which uses lo-
cal context information. The system uses Base Le-
vel Concepts (BLC) (Rosch, 1977) as features. In
short, BLC are synsets of WordNet (WN) (Fell-
baum, 1998) that are representative of a certain hy-
ponymy sub–hierarchy. The synsets that are se-
lected to be BLC must accomplish certain condi-
tions that will be explained in next section. BLC
This paper has been supported by the European Union un-
der the project QALL-ME (FP6 IST-033860) and the Spanish
Government under the project Text-Mess (TIN2006-15265-
C06-01) and KNOW (TIN2006-15049-C03-01)
are slightly different from Base Concepts of Eu-
roWordNet1 (EWN) (Vossen et al., 1998), Balkanet2
or Meaning Project3 because of the selection crite-
ria but also because our method is capable to define
them automatically. This type of features helps our
system to achieve 0.79550 F1 (over the First–Sense
baseline, 0.78889) while only four systems outper-
formed ours being the F1 of the best one 0.83208.
WordNet has been widely criticised for being a
sense repository that often offers too fine–grained
sense distinctions for higher level applications like
Machine Translation or Question &amp; Answering. In
fact, WSD at this level of granularity, has resisted
all attempts of inferring robust broad-coverage mo-
dels. It seems that many word–sense distinctions are
too subtle to be captured by automatic systems with
the current small volumes of word–sense annotated
examples. Possibly, building class-based classifiers
would allow to avoid the data sparseness problem of
the word-based approach.
Thus, some research has been focused on deri-
ving different sense groupings to overcome the fine–
grained distinctions of WN (Hearst and Sch¨utze,
1993) (Peters et al., 1998) (Mihalcea and Moldo-
van, 2001) (Agirre et al., 2003) and on using predefi-
ned sets of sense-groupings for learning class-based
classifiers for WSD (Segond et al., 1997) (Ciaramita
and Johnson, 2003) (Villarejo et al., 2005) (Curran,
2005) (Ciaramita and Altun, 2006). However, most
of the later approaches used the original Lexico-
graphical Files of WN (more recently called Super-
</bodyText>
<footnote confidence="0.999993666666667">
1http://www.illc.uva.nl/EuroWordNet/
2http://www.ceid.upatras.gr/Balkanet
3http://www.lsi.upc.es/ nlp/meaning
</footnote>
<page confidence="0.971992">
157
</page>
<bodyText confidence="0.986271565217391">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160,
Prague, June 2007. c�2007 Association for Computational Linguistics
senses) as very coarse–grained sense distinctions.
However, not so much attention has been paid on
learning class-based classifiers from other available
sense–groupings such as WordNet Domains (Mag-
nini and Cavaglia, 2000), SUMO labels (Niles and
Pease, 2001), EuroWordNet Base Concepts or Top
Concept Ontology labels (Atserias et al., 2004). Ob-
viously, these resources relate senses at some level
of abstraction using different semantic criteria and
properties that could be of interest for WSD. Pos-
sibly, their combination could improve the overall
results since they offer different semantic perspecti-
ves of the data. Furthermore, to our knowledge, to
date no comparative evaluation have been performed
exploring different sense–groupings.
This paper is organized as follows. In section 2,
we present a method for deriving fully automatica-
lly a number of Base Level Concepts from any WN
version. Section 3 shows the details of the whole
system and finally, in section 4 some concluding re-
marks are provided.
</bodyText>
<sectionHeader confidence="0.894641" genericHeader="method">
2 Automatic Selection of Base Level
Concepts
</sectionHeader>
<bodyText confidence="0.997484833333333">
The notion of Base Concepts (hereinafter BC) was
introduced in EWN. The BC are supposed to be the
concepts that play the most important role in the va-
rious wordnets4 (Fellbaum, 1998) of different lan-
guages. This role was measured in terms of two
main criteria:
</bodyText>
<listItem confidence="0.999863">
• A high position in the semantic hierarchy;
• Having many relations to other concepts;
</listItem>
<bodyText confidence="0.982857083333333">
Thus, the BC are the fundamental building blocks
for establishing the relations in a wordnet and give
information about the dominant lexicalization pat-
terns in languages. BC are generalizations of featu-
res or semantic components and thus apply to a ma-
ximum number of concepts. Thus, the Lexicografic
Files (or Supersenses) of WN could be considered
the most basic set of BC.
Basic Level Concepts (Rosch, 1977) should not
be confused with Base Concepts. BLC are the result
of a compromise between two conflicting principles
of characterization:
</bodyText>
<footnote confidence="0.941238">
4http://wordnet.princeton.edu
</footnote>
<table confidence="0.999266272727273">
#rel. synset
18 group 1,grouping 1
19 social group 1
37 organisation 2,organization 1
10 establishment 2,institution 1
12 faith 3,religion 2
5 Christianity 2,church 1,Christian church 1
#rel. synset
14 entity 1,something 1
29 object 1,physical object 1
39 artifact 1,artefact 1
63 construction 3,structure 1
79 building 1,edifice 1
11 place of worship 1, ...
19 church 2,church building 1
#rel. synset
20 act 2,human action 1,human activity 1
69 activity 1
5 ceremony 3
11 religious ceremony 1,religious ritual 1
7 service 3,religious service 1,divine service 1
1 church 3,church service 1
</table>
<tableCaption confidence="0.6568005">
Table 1: Possible Base Level Concepts for the noun
Church
</tableCaption>
<listItem confidence="0.9998435">
• Represent as many concepts as possible;
• Represent as many features as possible;
</listItem>
<bodyText confidence="0.999964826086956">
As a result of this, Basic Level Concepts typically
occur in the middle of hierarchies and less than the
maximum number of relations. BC mostly involve
the first principle of the Basic Level Concepts only.
Our work focuses on devising simple methods for
selecting automatically an accurate set of Basic Le-
vel Concepts from WN. In particular, our method se-
lects the appropriate BLC of a particular synset con-
sidering the relative number of relations encoded in
WN of their hypernyms.
The process follows a bottom-up approach using
the chain of hypernym relations. For each synset
in WN, the process selects as its Base Level Con-
cept the first local maximum according to the rela-
tive number of relations. For synsets having multi-
ple hypernyms, the path having the local maximum
with higher number of relations is selected. Usually,
this process finishes having a number of “fake” Base
Level Concepts. That is, synsets having no descen-
dants (or with a very small number) but being the
first local maximum according to the number of re-
lations considered. Thus, the process finishes che-
cking if the number of concepts subsumed by the
</bodyText>
<page confidence="0.982273">
158
</page>
<table confidence="0.9995775">
Senses BLC SuperSenses
Nouns 4.92 4.10 3.01
Verbs 11.00 8.67 1.03
Nouns + Verbs 7.66 6.16 3.47
</table>
<tableCaption confidence="0.999418">
Table 2: Polysemy degree over SensEval–3
</tableCaption>
<bodyText confidence="0.999744833333333">
preliminary list of BLC is higher than a certain th-
reshold. For those BLC not representing enough
concepts according to a certain threshold, the pro-
cess selects the next local maximum following the
hypernym hierarchy.
An example is provided in table 1. This table
shows the possible BLC for the noun “church” using
WN1.6. The table presents the hypernym chain for
each synset together with the number of relations en-
coded in WN for the synset. The local maxima along
the hypernym chain of each synset appears in bold.
Table 2 presents the polysemy degree for nouns
and verbs of the different words when grouping its
senses with respect the different semantic classes on
SensEval–3. Senses stand for the WN senses, BLC
for the Automatic BLC derived using a threshold of
20 and SuperSenses for the Lexicographic Files of
WN.
</bodyText>
<sectionHeader confidence="0.995061" genericHeader="method">
3 The GPLSI system
</sectionHeader>
<bodyText confidence="0.999958666666667">
The GPLSI system uses a publicly available imple-
mentation of Support Vector Machines, SVMLight5
(Joachims, 2002), and Semcor as learning corpus.
Semcor has been properly mapped and labelled with
both BLC6 and sense-clusters.
Actually, the process of training-classification has
two phases: first, one classifier is trained for each
possible BLC class and then the SemEval test data
is classified and enriched with them, and second, a
classifier for each target word is built using as addi-
tional features the BLC tags in Semcor and SemE-
val’s test.
Then, the features used for training the classifiers
are: lemmas, word forms, PoS tags7, BLC tags, and
first sense class of target word (S1TW). All features
</bodyText>
<footnote confidence="0.998184625">
5http://svmlight.joachims.org/
6Because BLC are automatically defined from WN, some tu-
ning must be performed due to the nature of the task 7. We have
not enough room to present the complete study but threshold 20
has been chosen, using SENSEVAL-3 English all-words as test
data. Moreover, our tests showed roughly 5% of improvement
against not using these features.
7TreeTagger (Schmid, 1994) was used
</footnote>
<bodyText confidence="0.99995584375">
were extracted from a window [−3.. + 3] except for
the last type (S1TW). The reason of using S1TW
features is to assure the learning of the baseline. It is
well known that Semcor presents a higher frequency
on first senses (and it is also the baseline of the task
finally provided by the organizers).
Besides, these are the same features for both first
and second phases (obviously except for S1TW be-
cause of the different target set of classes). Nevert-
heless, the training in both cases are quite different:
the first phase is class-based while the second is
word-based. By word-based we mean that the lear-
ning is performed using just the examples in Semcor
that contains the target word. We obtain one classi-
fier per polysemous word are in the SemEval test
corpus. The output of these classifiers is a sense-
cluster. In class-based learning all the examples in
Semcor are used, tagging those ones belonging to a
specific class (BLC in our case) as positive exam-
ples while the rest are tagged as negatives. We ob-
tain so many binary classifiers as BLC are in Se-
mEval test corpus. The output of these classifiers
is true or false, “the example belongs to a class”
or not. When dealing with a concrete target word,
only those BLC classifiers that are related to it are
“activated” (i.e, “animal” classifier will be not used
to classify “church”), ensuring that the word will be
tagged with coherent labels. In order to avoid statis-
tical bias because of very large set of negative exam-
ples, the features are defined from positive examples
only (although they are obviously used to characte-
rize all the examples).
</bodyText>
<sectionHeader confidence="0.973444" genericHeader="conclusions">
4 Conclusions and further work
</sectionHeader>
<bodyText confidence="0.999854923076923">
The WSD task seems to have reached its maxi-
mum accuracy figures with the usual framework.
Some of its limitations could come from the sense–
granularity of WN. In particular, SemEval’s coarse-
grained English all-words task represents a solution
in this direction.
Nevertheless, the task still remains oriented to
words rather than classes. Then, other problems
arise like data sparseness just because the lack of
adequate and enough examples. Changing the set of
classes could be a solution to enrich training corpora
with many more examples Another option seems to
be incorporating more semantic information.
</bodyText>
<page confidence="0.996488">
159
</page>
<bodyText confidence="0.99944275">
Base Level Concepts (BLC) are concepts that are
representative for a set of other concepts. A simple
method for automatically selecting BLC from WN
based on the hypernym hierarchy and the number of
stored relationships between synsets have been used
to define features for training a supervised system.
Although in our system BLC play a simple role
aiding to the disambiguation just as additional fea-
tures, the good results achieved with such simple
features confirm us that an appropriate set of BLC
will be a better semantic discriminator than senses
or even sense-clusters.
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999720178082192">
E. Agirre, I. Aldezabal, and E. Pociello. 2003. A pi-
lot study of english selectional preferences and their
cross-lingual compatibility with basque. In Procee-
dings of the International Conference on Text Speech
and Dialogue (TSD’2003), CeskBudojovice, Czech
Republic.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The meaning mul-
tilingual central repository. In Proceedings of Global
WordNet Conference (GWC’04), Brno, Czech Repu-
blic.
M. Ciaramita and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction with
a supersense sequence tagger. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP’06), pages 594–602, Syd-
ney, Australia. ACL.
M. Ciaramita and M. Johnson. 2003. Supersense tagging
of unknown nouns in wordnet. In Proceedings of the
Conference on Empirical methods in natural language
processing (EMNLP’03), pages 168–175. ACL.
J. Curran. 2005. Supersense tagging of unknown nouns
using semantic similarity. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics (ACL’05), pages 26–33. ACL.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
M. Hearst and H. Sch¨utze. 1993. Customizing a lexicon
to better suit a computational task. In Proceedingns
of the ACL SIGLEX Workshop on Lexical Acquisition,
Stuttgart, Germany.
Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines. Kluwer Academic
Publishers.
B. Magnini and G. Cavaglia. 2000. Integrating subject
fields codes into wordnet. In Proceedings of the Se-
cond International Conference on Language Resour-
ces and Evaluation (LREC’00).
R. Mihalcea and D. Moldovan. 2001. Automatic ge-
neration of coarse grained wordnet. In Proceding of
the NAACL workshop on WordNet and Other Lexical
Resources: Applications, Extensions and Customiza-
tions, Pittsburg, USA.
I. Niles and A. Pease. 2001. Towards a standard up-
per ontology. In Proceedings of the 2nd International
Conference on Formal Ontology in Information Sys-
tems (FOIS-2001), pages 17–19. Chris Welty and Ba-
rry Smith, eds.
W. Peters, I. Peters, and P. Vossen. 1998. Automatic
sense clustering in eurowordnet. In First Internatio-
nal Conference on Language Resources and Evalua-
tion (LREC’98), Granada, Spain.
E. Rosch. 1977. Human categorisation. Studies in
Cross-Cultural Psychology, I(1):1–49.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings ofNemLap-
94, pages 44–49, Manchester, England.
F. Segond, A. Schiller, G. Greffenstette, and J. Chanod.
1997. An experiment in semantic tagging using hid-
den markov model tagging. In ACL Workshop on Au-
tomatic Information Extraction and Building of Lexi-
cal Semantic Resources for NLP Applications, pages
78–81. ACL, New Brunswick, New Jersey.
L. Villarejo, L. M`arquez, and G. Rigau. 2005. Explo-
ring the construction of semantic class classifiers for
wsd. In Proceedings of the 21th Annual Meeting of
Sociedad Espaola para el Procesamiento del Lenguaje
Natural SEPLN’05, pages 195–202, Granada, Spain,
September. ISSN 1136-5948.
P. Vossen, L. Bloksma, H. Rodriguez, S. Climent, N. Cal-
zolari, A. Roventini, F. Bertagna, A. Alonge, and
W. Peters. 1998. The eurowordnet base concepts and
top ontology. Technical report, Paris, France, France.
</reference>
<page confidence="0.997469">
160
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.285816">
<title confidence="0.99979">GPLSI: Word Coarse-grained Disambiguation aided by Basic Level</title>
<author confidence="0.999913">Rub´en Izquierdo Armando Su´arez</author>
<affiliation confidence="0.821343666666667">GPLSI Group, DLSI University of Alicante Spain</affiliation>
<author confidence="0.962372">German Rigau</author>
<affiliation confidence="0.979886">IXA NLP Group EHU/UPV</affiliation>
<address confidence="0.808248">Donostia, Basque Country</address>
<email confidence="0.962792">german.rigau@ehu.es</email>
<abstract confidence="0.991893111111111">We present a corpus-based supervised learning system for coarse-grained sense disambiguation. In addition to usual features for training in word sense disambiguation, our system also uses Base Level Concepts automatically obtained from WordNet. Base Level Concepts are some synsets that generalize a hyponymy sub–hierarchy, and provides an extra level of abstraction as well as relevant information about the context of a word to be disambiguated. Our experiments proved that using this type of features results on a significant improvement of precision. Our system has achieved almost 0.8 F1 (fifth place) in the coarse–grained English all-words task using a very simple set of features plus Base Level Concepts annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>I Aldezabal</author>
<author>E Pociello</author>
</authors>
<title>A pilot study of english selectional preferences and their cross-lingual compatibility with basque.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Text Speech and Dialogue (TSD’2003),</booktitle>
<location>CeskBudojovice, Czech Republic.</location>
<contexts>
<context position="2904" citStr="Agirre et al., 2003" startWordPosition="443" endWordPosition="446">ng. In fact, WSD at this level of granularity, has resisted all attempts of inferring robust broad-coverage models. It seems that many word–sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word–sense annotated examples. Possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach. Thus, some research has been focused on deriving different sense groupings to overcome the fine– grained distinctions of WN (Hearst and Sch¨utze, 1993) (Peters et al., 1998) (Mihalcea and Moldovan, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006). However, most of the later approaches used the original Lexicographical Files of WN (more recently called Super1http://www.illc.uva.nl/EuroWordNet/ 2http://www.ceid.upatras.gr/Balkanet 3http://www.lsi.upc.es/ nlp/meaning 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, Prague, June 2007. c�2007 Association for Computational Lin</context>
</contexts>
<marker>Agirre, Aldezabal, Pociello, 2003</marker>
<rawString>E. Agirre, I. Aldezabal, and E. Pociello. 2003. A pilot study of english selectional preferences and their cross-lingual compatibility with basque. In Proceedings of the International Conference on Text Speech and Dialogue (TSD’2003), CeskBudojovice, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Atserias</author>
<author>L Villarejo</author>
<author>G Rigau</author>
<author>E Agirre</author>
<author>J Carroll</author>
<author>B Magnini</author>
<author>P Vossen</author>
</authors>
<title>The meaning multilingual central repository.</title>
<date>2004</date>
<booktitle>In Proceedings of Global WordNet Conference (GWC’04),</booktitle>
<location>Brno, Czech Republic.</location>
<contexts>
<context position="3853" citStr="Atserias et al., 2004" startWordPosition="567" endWordPosition="570">http://www.illc.uva.nl/EuroWordNet/ 2http://www.ceid.upatras.gr/Balkanet 3http://www.lsi.upc.es/ nlp/meaning 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, Prague, June 2007. c�2007 Association for Computational Linguistics senses) as very coarse–grained sense distinctions. However, not so much attention has been paid on learning class-based classifiers from other available sense–groupings such as WordNet Domains (Magnini and Cavaglia, 2000), SUMO labels (Niles and Pease, 2001), EuroWordNet Base Concepts or Top Concept Ontology labels (Atserias et al., 2004). Obviously, these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for WSD. Possibly, their combination could improve the overall results since they offer different semantic perspectives of the data. Furthermore, to our knowledge, to date no comparative evaluation have been performed exploring different sense–groupings. This paper is organized as follows. In section 2, we present a method for deriving fully automatically a number of Base Level Concepts from any WN version. Section 3 shows the details of the whole s</context>
</contexts>
<marker>Atserias, Villarejo, Rigau, Agirre, Carroll, Magnini, Vossen, 2004</marker>
<rawString>J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll, B. Magnini, and P. Vossen. 2004. The meaning multilingual central repository. In Proceedings of Global WordNet Conference (GWC’04), Brno, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>Y Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’06),</booktitle>
<pages>594--602</pages>
<publisher>ACL.</publisher>
<location>Sydney, Australia.</location>
<contexts>
<context position="3117" citStr="Ciaramita and Altun, 2006" startWordPosition="475" endWordPosition="478">ms with the current small volumes of word–sense annotated examples. Possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach. Thus, some research has been focused on deriving different sense groupings to overcome the fine– grained distinctions of WN (Hearst and Sch¨utze, 1993) (Peters et al., 1998) (Mihalcea and Moldovan, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006). However, most of the later approaches used the original Lexicographical Files of WN (more recently called Super1http://www.illc.uva.nl/EuroWordNet/ 2http://www.ceid.upatras.gr/Balkanet 3http://www.lsi.upc.es/ nlp/meaning 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, Prague, June 2007. c�2007 Association for Computational Linguistics senses) as very coarse–grained sense distinctions. However, not so much attention has been paid on learning class-based classifiers from other available sense–groupings such as WordNet Domains (Magnini an</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>M. Ciaramita and Y. Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’06), pages 594–602, Sydney, Australia. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>M Johnson</author>
</authors>
<title>Supersense tagging of unknown nouns in wordnet.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP’03),</booktitle>
<pages>168--175</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3049" citStr="Ciaramita and Johnson, 2003" startWordPosition="465" endWordPosition="468">rd–sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word–sense annotated examples. Possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach. Thus, some research has been focused on deriving different sense groupings to overcome the fine– grained distinctions of WN (Hearst and Sch¨utze, 1993) (Peters et al., 1998) (Mihalcea and Moldovan, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006). However, most of the later approaches used the original Lexicographical Files of WN (more recently called Super1http://www.illc.uva.nl/EuroWordNet/ 2http://www.ceid.upatras.gr/Balkanet 3http://www.lsi.upc.es/ nlp/meaning 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, Prague, June 2007. c�2007 Association for Computational Linguistics senses) as very coarse–grained sense distinctions. However, not so much attention has been paid on learning class-based classifiers from</context>
</contexts>
<marker>Ciaramita, Johnson, 2003</marker>
<rawString>M. Ciaramita and M. Johnson. 2003. Supersense tagging of unknown nouns in wordnet. In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP’03), pages 168–175. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Curran</author>
</authors>
<title>Supersense tagging of unknown nouns using semantic similarity.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL’05),</booktitle>
<pages>26--33</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3089" citStr="Curran, 2005" startWordPosition="473" endWordPosition="474">automatic systems with the current small volumes of word–sense annotated examples. Possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach. Thus, some research has been focused on deriving different sense groupings to overcome the fine– grained distinctions of WN (Hearst and Sch¨utze, 1993) (Peters et al., 1998) (Mihalcea and Moldovan, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006). However, most of the later approaches used the original Lexicographical Files of WN (more recently called Super1http://www.illc.uva.nl/EuroWordNet/ 2http://www.ceid.upatras.gr/Balkanet 3http://www.lsi.upc.es/ nlp/meaning 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, Prague, June 2007. c�2007 Association for Computational Linguistics senses) as very coarse–grained sense distinctions. However, not so much attention has been paid on learning class-based classifiers from other available sense–groupings such as</context>
</contexts>
<marker>Curran, 2005</marker>
<rawString>J. Curran. 2005. Supersense tagging of unknown nouns using semantic similarity. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL’05), pages 26–33. ACL.</rawString>
</citation>
<citation valid="true">
<title>WordNet. An Electronic Lexical Database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet. An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
<author>H Sch¨utze</author>
</authors>
<title>Customizing a lexicon to better suit a computational task.</title>
<date>1993</date>
<booktitle>In Proceedingns of the ACL SIGLEX Workshop on Lexical Acquisition,</booktitle>
<location>Stuttgart, Germany.</location>
<marker>Hearst, Sch¨utze, 1993</marker>
<rawString>M. Hearst and H. Sch¨utze. 1993. Customizing a lexicon to better suit a computational task. In Proceedingns of the ACL SIGLEX Workshop on Lexical Acquisition, Stuttgart, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Learning to Classify Text Using Support Vector Machines.</title>
<date>2002</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="8446" citStr="Joachims, 2002" startWordPosition="1325" endWordPosition="1326">s the hypernym chain for each synset together with the number of relations encoded in WN for the synset. The local maxima along the hypernym chain of each synset appears in bold. Table 2 presents the polysemy degree for nouns and verbs of the different words when grouping its senses with respect the different semantic classes on SensEval–3. Senses stand for the WN senses, BLC for the Automatic BLC derived using a threshold of 20 and SuperSenses for the Lexicographic Files of WN. 3 The GPLSI system The GPLSI system uses a publicly available implementation of Support Vector Machines, SVMLight5 (Joachims, 2002), and Semcor as learning corpus. Semcor has been properly mapped and labelled with both BLC6 and sense-clusters. Actually, the process of training-classification has two phases: first, one classifier is trained for each possible BLC class and then the SemEval test data is classified and enriched with them, and second, a classifier for each target word is built using as additional features the BLC tags in Semcor and SemEval’s test. Then, the features used for training the classifiers are: lemmas, word forms, PoS tags7, BLC tags, and first sense class of target word (S1TW). All features 5http://</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Learning to Classify Text Using Support Vector Machines. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>G Cavaglia</author>
</authors>
<title>Integrating subject fields codes into wordnet.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources and Evaluation (LREC’00).</booktitle>
<contexts>
<context position="3734" citStr="Magnini and Cavaglia, 2000" startWordPosition="548" endWordPosition="552">tun, 2006). However, most of the later approaches used the original Lexicographical Files of WN (more recently called Super1http://www.illc.uva.nl/EuroWordNet/ 2http://www.ceid.upatras.gr/Balkanet 3http://www.lsi.upc.es/ nlp/meaning 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, Prague, June 2007. c�2007 Association for Computational Linguistics senses) as very coarse–grained sense distinctions. However, not so much attention has been paid on learning class-based classifiers from other available sense–groupings such as WordNet Domains (Magnini and Cavaglia, 2000), SUMO labels (Niles and Pease, 2001), EuroWordNet Base Concepts or Top Concept Ontology labels (Atserias et al., 2004). Obviously, these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for WSD. Possibly, their combination could improve the overall results since they offer different semantic perspectives of the data. Furthermore, to our knowledge, to date no comparative evaluation have been performed exploring different sense–groupings. This paper is organized as follows. In section 2, we present a method for deriv</context>
</contexts>
<marker>Magnini, Cavaglia, 2000</marker>
<rawString>B. Magnini and G. Cavaglia. 2000. Integrating subject fields codes into wordnet. In Proceedings of the Second International Conference on Language Resources and Evaluation (LREC’00).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>D Moldovan</author>
</authors>
<title>Automatic generation of coarse grained wordnet.</title>
<date>2001</date>
<booktitle>In Proceding of the NAACL workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations,</booktitle>
<location>Pittsburg, USA.</location>
<contexts>
<context position="2882" citStr="Mihalcea and Moldovan, 2001" startWordPosition="438" endWordPosition="442">nslation or Question &amp; Answering. In fact, WSD at this level of granularity, has resisted all attempts of inferring robust broad-coverage models. It seems that many word–sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word–sense annotated examples. Possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach. Thus, some research has been focused on deriving different sense groupings to overcome the fine– grained distinctions of WN (Hearst and Sch¨utze, 1993) (Peters et al., 1998) (Mihalcea and Moldovan, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006). However, most of the later approaches used the original Lexicographical Files of WN (more recently called Super1http://www.illc.uva.nl/EuroWordNet/ 2http://www.ceid.upatras.gr/Balkanet 3http://www.lsi.upc.es/ nlp/meaning 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, Prague, June 2007. c�2007 Association</context>
</contexts>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>R. Mihalcea and D. Moldovan. 2001. Automatic generation of coarse grained wordnet. In Proceding of the NAACL workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, Pittsburg, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Niles</author>
<author>A Pease</author>
</authors>
<title>Towards a standard upper ontology.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd International Conference on Formal Ontology in Information Systems (FOIS-2001),</booktitle>
<pages>17--19</pages>
<editor>Chris Welty and Barry Smith, eds.</editor>
<contexts>
<context position="3771" citStr="Niles and Pease, 2001" startWordPosition="555" endWordPosition="558">roaches used the original Lexicographical Files of WN (more recently called Super1http://www.illc.uva.nl/EuroWordNet/ 2http://www.ceid.upatras.gr/Balkanet 3http://www.lsi.upc.es/ nlp/meaning 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, Prague, June 2007. c�2007 Association for Computational Linguistics senses) as very coarse–grained sense distinctions. However, not so much attention has been paid on learning class-based classifiers from other available sense–groupings such as WordNet Domains (Magnini and Cavaglia, 2000), SUMO labels (Niles and Pease, 2001), EuroWordNet Base Concepts or Top Concept Ontology labels (Atserias et al., 2004). Obviously, these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for WSD. Possibly, their combination could improve the overall results since they offer different semantic perspectives of the data. Furthermore, to our knowledge, to date no comparative evaluation have been performed exploring different sense–groupings. This paper is organized as follows. In section 2, we present a method for deriving fully automatically a number of B</context>
</contexts>
<marker>Niles, Pease, 2001</marker>
<rawString>I. Niles and A. Pease. 2001. Towards a standard upper ontology. In Proceedings of the 2nd International Conference on Formal Ontology in Information Systems (FOIS-2001), pages 17–19. Chris Welty and Barry Smith, eds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Peters</author>
<author>I Peters</author>
<author>P Vossen</author>
</authors>
<title>Automatic sense clustering in eurowordnet.</title>
<date>1998</date>
<booktitle>In First International Conference on Language Resources and Evaluation (LREC’98),</booktitle>
<location>Granada,</location>
<contexts>
<context position="2852" citStr="Peters et al., 1998" startWordPosition="434" endWordPosition="437">tions like Machine Translation or Question &amp; Answering. In fact, WSD at this level of granularity, has resisted all attempts of inferring robust broad-coverage models. It seems that many word–sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word–sense annotated examples. Possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach. Thus, some research has been focused on deriving different sense groupings to overcome the fine– grained distinctions of WN (Hearst and Sch¨utze, 1993) (Peters et al., 1998) (Mihalcea and Moldovan, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006). However, most of the later approaches used the original Lexicographical Files of WN (more recently called Super1http://www.illc.uva.nl/EuroWordNet/ 2http://www.ceid.upatras.gr/Balkanet 3http://www.lsi.upc.es/ nlp/meaning 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, Prague,</context>
</contexts>
<marker>Peters, Peters, Vossen, 1998</marker>
<rawString>W. Peters, I. Peters, and P. Vossen. 1998. Automatic sense clustering in eurowordnet. In First International Conference on Language Resources and Evaluation (LREC’98), Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rosch</author>
</authors>
<title>Human categorisation.</title>
<date>1977</date>
<booktitle>Studies in Cross-Cultural Psychology, I(1):1–49.</booktitle>
<contexts>
<context position="1229" citStr="Rosch, 1977" startWordPosition="183" endWordPosition="184">evel of abstraction as well as relevant information about the context of a word to be disambiguated. Our experiments proved that using this type of features results on a significant improvement of precision. Our system has achieved almost 0.8 F1 (fifth place) in the coarse–grained English all-words task using a very simple set of features plus Base Level Concepts annotation. 1 Introduction The GPLSI system in SemEval’s task 7, coarse– grained English all-words, consists of a corpusbased supervised-learning method which uses local context information. The system uses Base Level Concepts (BLC) (Rosch, 1977) as features. In short, BLC are synsets of WordNet (WN) (Fellbaum, 1998) that are representative of a certain hyponymy sub–hierarchy. The synsets that are selected to be BLC must accomplish certain conditions that will be explained in next section. BLC This paper has been supported by the European Union under the project QALL-ME (FP6 IST-033860) and the Spanish Government under the project Text-Mess (TIN2006-15265- C06-01) and KNOW (TIN2006-15049-C03-01) are slightly different from Base Concepts of EuroWordNet1 (EWN) (Vossen et al., 1998), Balkanet2 or Meaning Project3 because of the selection</context>
<context position="5326" citStr="Rosch, 1977" startWordPosition="810" endWordPosition="811">e various wordnets4 (Fellbaum, 1998) of different languages. This role was measured in terms of two main criteria: • A high position in the semantic hierarchy; • Having many relations to other concepts; Thus, the BC are the fundamental building blocks for establishing the relations in a wordnet and give information about the dominant lexicalization patterns in languages. BC are generalizations of features or semantic components and thus apply to a maximum number of concepts. Thus, the Lexicografic Files (or Supersenses) of WN could be considered the most basic set of BC. Basic Level Concepts (Rosch, 1977) should not be confused with Base Concepts. BLC are the result of a compromise between two conflicting principles of characterization: 4http://wordnet.princeton.edu #rel. synset 18 group 1,grouping 1 19 social group 1 37 organisation 2,organization 1 10 establishment 2,institution 1 12 faith 3,religion 2 5 Christianity 2,church 1,Christian church 1 #rel. synset 14 entity 1,something 1 29 object 1,physical object 1 39 artifact 1,artefact 1 63 construction 3,structure 1 79 building 1,edifice 1 11 place of worship 1, ... 19 church 2,church building 1 #rel. synset 20 act 2,human action 1,human act</context>
</contexts>
<marker>Rosch, 1977</marker>
<rawString>E. Rosch. 1977. Human categorisation. Studies in Cross-Cultural Psychology, I(1):1–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings ofNemLap94,</booktitle>
<pages>44--49</pages>
<location>Manchester, England.</location>
<contexts>
<context position="9430" citStr="Schmid, 1994" startWordPosition="1483" endWordPosition="1484"> additional features the BLC tags in Semcor and SemEval’s test. Then, the features used for training the classifiers are: lemmas, word forms, PoS tags7, BLC tags, and first sense class of target word (S1TW). All features 5http://svmlight.joachims.org/ 6Because BLC are automatically defined from WN, some tuning must be performed due to the nature of the task 7. We have not enough room to present the complete study but threshold 20 has been chosen, using SENSEVAL-3 English all-words as test data. Moreover, our tests showed roughly 5% of improvement against not using these features. 7TreeTagger (Schmid, 1994) was used were extracted from a window [−3.. + 3] except for the last type (S1TW). The reason of using S1TW features is to assure the learning of the baseline. It is well known that Semcor presents a higher frequency on first senses (and it is also the baseline of the task finally provided by the organizers). Besides, these are the same features for both first and second phases (obviously except for S1TW because of the different target set of classes). Nevertheless, the training in both cases are quite different: the first phase is class-based while the second is word-based. By word-based we m</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings ofNemLap94, pages 44–49, Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Segond</author>
<author>A Schiller</author>
<author>G Greffenstette</author>
<author>J Chanod</author>
</authors>
<title>An experiment in semantic tagging using hidden markov model tagging.</title>
<date>1997</date>
<booktitle>In ACL Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications,</booktitle>
<pages>78--81</pages>
<publisher>ACL,</publisher>
<location>New Brunswick, New Jersey.</location>
<contexts>
<context position="3019" citStr="Segond et al., 1997" startWordPosition="461" endWordPosition="464"> It seems that many word–sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word–sense annotated examples. Possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach. Thus, some research has been focused on deriving different sense groupings to overcome the fine– grained distinctions of WN (Hearst and Sch¨utze, 1993) (Peters et al., 1998) (Mihalcea and Moldovan, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006). However, most of the later approaches used the original Lexicographical Files of WN (more recently called Super1http://www.illc.uva.nl/EuroWordNet/ 2http://www.ceid.upatras.gr/Balkanet 3http://www.lsi.upc.es/ nlp/meaning 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, Prague, June 2007. c�2007 Association for Computational Linguistics senses) as very coarse–grained sense distinctions. However, not so much attention has been paid on learnin</context>
</contexts>
<marker>Segond, Schiller, Greffenstette, Chanod, 1997</marker>
<rawString>F. Segond, A. Schiller, G. Greffenstette, and J. Chanod. 1997. An experiment in semantic tagging using hidden markov model tagging. In ACL Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications, pages 78–81. ACL, New Brunswick, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Villarejo</author>
<author>L M`arquez</author>
<author>G Rigau</author>
</authors>
<title>Exploring the construction of semantic class classifiers for wsd.</title>
<date>2005</date>
<journal>ISSN</journal>
<booktitle>In Proceedings of the 21th Annual Meeting of Sociedad Espaola para el Procesamiento del Lenguaje Natural SEPLN’05,</booktitle>
<pages>195--202</pages>
<location>Granada, Spain,</location>
<marker>Villarejo, M`arquez, Rigau, 2005</marker>
<rawString>L. Villarejo, L. M`arquez, and G. Rigau. 2005. Exploring the construction of semantic class classifiers for wsd. In Proceedings of the 21th Annual Meeting of Sociedad Espaola para el Procesamiento del Lenguaje Natural SEPLN’05, pages 195–202, Granada, Spain, September. ISSN 1136-5948.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
<author>L Bloksma</author>
<author>H Rodriguez</author>
<author>S Climent</author>
<author>N Calzolari</author>
<author>A Roventini</author>
<author>F Bertagna</author>
<author>A Alonge</author>
<author>W Peters</author>
</authors>
<title>The eurowordnet base concepts and top ontology.</title>
<date>1998</date>
<tech>Technical report,</tech>
<location>Paris, France, France.</location>
<contexts>
<context position="1773" citStr="Vossen et al., 1998" startWordPosition="268" endWordPosition="271">context information. The system uses Base Level Concepts (BLC) (Rosch, 1977) as features. In short, BLC are synsets of WordNet (WN) (Fellbaum, 1998) that are representative of a certain hyponymy sub–hierarchy. The synsets that are selected to be BLC must accomplish certain conditions that will be explained in next section. BLC This paper has been supported by the European Union under the project QALL-ME (FP6 IST-033860) and the Spanish Government under the project Text-Mess (TIN2006-15265- C06-01) and KNOW (TIN2006-15049-C03-01) are slightly different from Base Concepts of EuroWordNet1 (EWN) (Vossen et al., 1998), Balkanet2 or Meaning Project3 because of the selection criteria but also because our method is capable to define them automatically. This type of features helps our system to achieve 0.79550 F1 (over the First–Sense baseline, 0.78889) while only four systems outperformed ours being the F1 of the best one 0.83208. WordNet has been widely criticised for being a sense repository that often offers too fine–grained sense distinctions for higher level applications like Machine Translation or Question &amp; Answering. In fact, WSD at this level of granularity, has resisted all attempts of inferring rob</context>
</contexts>
<marker>Vossen, Bloksma, Rodriguez, Climent, Calzolari, Roventini, Bertagna, Alonge, Peters, 1998</marker>
<rawString>P. Vossen, L. Bloksma, H. Rodriguez, S. Climent, N. Calzolari, A. Roventini, F. Bertagna, A. Alonge, and W. Peters. 1998. The eurowordnet base concepts and top ontology. Technical report, Paris, France, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>