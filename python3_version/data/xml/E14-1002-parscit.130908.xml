<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000080">
<title confidence="0.9991155">
Undirected Machine Translation with
Discriminative Reinforcement Learning
</title>
<author confidence="0.984483">
Andrea Gesmundo James Henderson
</author>
<affiliation confidence="0.932202">
Google Inc. Xerox Research Centre Europe
</affiliation>
<email confidence="0.997228">
andrea.gesmundo@gmail.com james.henderson@xrce.xerox.com
</email>
<sectionHeader confidence="0.993864" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999996166666667">
We present a novel Undirected Machine
Translation model of Hierarchical MT that
is not constrained to the standard bottom-
up inference order. Removing the order-
ing constraint makes it possible to condi-
tion on top-down structure and surround-
ing context. This allows the introduc-
tion of a new class of contextual features
that are not constrained to condition only
on the bottom-up context. The model
builds translation-derivations efficiently in
a greedy fashion. It is trained to learn
to choose jointly the best action and the
best inference order. Experiments show
that the decoding time is halved and forest-
rescoring is 6 times faster, while reaching
accuracy not significantly different from
state of the art.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999800740740741">
Machine Translation (MT) can be addressed as a
structured prediction task (Brown et al., 1993; Ya-
mada and Knight, 2001; Koehn et al., 2003). MT’s
goal is to learn a mapping function, f, from an in-
put sentence, x, into y = (t, h), where t is the
sentence translated into the target language, and
h is the hidden correspondence structure (Liang
et al., 2006). In Hierarchical MT (HMT) (Chi-
ang, 2005) the hidden correspondence structure is
the synchronous-tree composed by instantiations
of synchronous rules from the input grammar, G.
Statistical models usually define f as: f(x) =
arg maxy∈Y Score(x, y), where Score(x, y) is a
function whose parameters can be learned with a
specialized learning algorithm. In MT applica-
tions, it is not possible to enumerate all y ∈ Y.
HMT decoding applies pruning (e.g. Cube Prun-
ing (Huang and Chiang, 2005)), but even then
HMT has higher complexity than Phrase Based
MT (PbMT) (Koehn et al., 2003). On the other
hand, HMT improves over PbMT by introducing
the possibility of exploiting a more sophisticated
reordering model not bounded by a window size,
and producing translations with higher syntactic-
semantic quality. In this paper, we present the
Undirected Machine Translation (UMT) frame-
work, which retains the advantages of HMT and
allows the use of a greedy decoder whose com-
plexity is lower than standard quadratic beam-
search PbMT.
UMT’s fast decoding is made possible through
even stronger pruning: the decoder chooses a sin-
gle action at each step, never retracts that action,
and prunes all incompatible alternatives to that ac-
tion. If this extreme level of pruning was ap-
plied to the CKY-like beam-decoding used in stan-
dard HMT, translation quality would be severely
degraded. This is because the bottom-up infer-
ence order imposed by CKY-like beam-decoding
means that all pruning decisions must be based on
a bottom-up approximation of contextual features,
which leads to search errors that affect the qual-
ity of reordering and lexical-choice (Gesmundo
and Henderson, 2011). UMT solves this problem
by removing the bottom-up inference order con-
straint, allowing many different inference orders
for the same tree structure, and learning the in-
ference order where the decoder can be the most
confident in its pruning decisions.
Removing the bottom-up inference order con-
straint makes it possible to condition on top-down
structure and surrounding context. This undirected
approach allows us to integrate contextual features
such as the Language Model (LM) in a more flex-
</bodyText>
<page confidence="0.987167">
10
</page>
<note confidence="0.9929035">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 10–19,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999591025">
ible way. It also allows us to introduce a new class
of undirected features. In particular, we introduce
the Context-Free Factor (CFF) features. CFF fea-
tures compute exactly and efficiently a bound on
the context-free cost of a partial derivation’s miss-
ing branches, thereby estimating the future cost of
partial derivations. The new class of undirected
features is fundamental for the success of a greedy
approach to HMT, because the additional non-
bottom-up context is sometimes crucial to have the
necessary information to make greedy decisions.
Because UMT prunes all but the single cho-
sen action at each step, both choosing a good in-
ference order and choosing a correct action re-
duce to a single choice of what action to take
next. To learn this decoding policy, we propose
a novel Discriminative Reinforcement Learning
(DRL) framework. DRL is used to train mod-
els that construct incrementally structured out-
put using a local discriminative function, with
the goal of optimizing a global loss function.
We apply DRL to learn the UMT scoring func-
tion’s parameters, using the BLEU score as the
global loss function. DRL learns a weight vector
for a linear classifier that discriminates between
decisions based on which one leads to a com-
plete translation-derivation with a better BLEU
score. Promotions/demotions of translations are
performed by applying a Perceptron-style update
on the sequence of decisions that produced the
translation, thereby training local decisions to op-
timize the global BLEU score of the final trans-
lation, while keeping the efficiency and simplic-
ity of the Perceptron Algorithm (Rosenblatt, 1958;
Collins, 2002).
Our experiments show that UMT with DRL re-
duces decoding time by over half, and the time to
rescore translations with the Language Model by
6 times, while reaching accuracy non-significantly
different from the state of the art.
</bodyText>
<sectionHeader confidence="0.960895" genericHeader="method">
2 Undirected Machine Translation
</sectionHeader>
<bodyText confidence="0.893089508474577">
In this section, we present the UMT frame-
work. For ease of presentation, and following
synchronous-grammar based MT practice, we will
henceforth restrict our focus to binary grammars
(Zhang et al., 2006; Wang et al., 2007).
A UMT decoder can be formulated as a func-
tion, f, that maps a source sentence, x ∈ X, into
a structure defined by y = (t, h) ∈ Y, where t
is the translation in the target language, and h
is the synchronous tree structure generating the
input sentence on the source side and its trans-
lation on the target side. Synchronous-trees are
composed of instantiations of synchronous-rules,
r, from a grammar, G. A UMT decoder builds
synchronous-trees, h, by recursively expanding
partial synchronous-trees, τ. τ includes a partial
translation. Each τ is required to be a connected
sub-graph of some synchronous-tree h. Thus, τ
is composed of a subset of the rules from any h
that generates x on the source side, such that there
is a connected path between any two rules in τ.
Differently from the partial structures built by a
bottom-up decoder, τ does not have to cover a
contiguous span on x. Formally, τ is defined by:
1) The set of synchronous-rule instantiations in τ:
I ≡ {r1,r2,···,rk|ri ∈ G,1 ≤ i ≤ k};
2) The set of connections among the synchronous-
rule instantiations, C.
Let ci = (ri, rji) be the notation to represent the
connection between the i-th rule and the rule rji.
The set of connections can be expressed as:
C ≡ {(r1, rj1), (r2, rj2), ··· , (rk−1, rjk−1)}
3) The postcondition set, P, which specifies
the non-terminals in τ that are available for
creating new connections. Each postcondition,
pi = (rx,Xy)i, indicates that the rule rx has the
non-terminal Xy available for connections. The
index y identifies the non-terminal in the rule. In
a binary grammar y can take only 3 values: 1 for
the first non-terminal (the left child of the source
side), 2 for the second non-terminal, and h for the
head. The postcondition set can be expressed as:
P≡{(rx1,Xy1)1,··· ,(rxm,Xym)m}
4) The set of carries, K. We define a different
carry, κi, for each non-terminal available for
connections. Each carry stores the extra infor-
mation required to correctly score the non-local
interactions between τ and the rule that will be
connected at that non-terminal. Thus |K |= |P|.
Let κi be the carry associated with the postcon-
dition pi. The set of carries can be expressed as:
K ≡ {κ1, κ2, ··· , κm}
Partial synchronous-trees, τ, are expanded by
performing connection-actions. Given a τ we can
connect to it a new rule, ˆr, using one available non-
terminal represented by postcondition, pi ∈ P,
and obtain a new partial synchronous-tree ˆτ. For-
mally: τˆ ≡ h τ ⋖ aˆ i, where, aˆ = [ˆr, pi],
represents the connection-action.
</bodyText>
<page confidence="0.998614">
11
</page>
<construct confidence="0.43554">
Algorithm 1 UMT Decoding
</construct>
<listItem confidence="0.998124761904762">
1: function Decoder (x; w, G) : (t,h)
2: T.{I, C, P, K} ← {∅, ∅, ∅, ∅} ;
3: Q ← LeafRules(G);
4: while |Q |&gt; 0 do
5: [ˆr, pi] ← PopBestAction (Q,w);
6: T ← CreateConnection(T, ˆr, pi);
7: UpdateQueue(Q, ˆr, pi);
8: end while
9: Return(T);
10: procedure CreateConnection(T, ˆr, pi ) : Tˆ
11: ˆT.I ← T.I + ˆr;
12: ˆT.C ← T.C + (ˆr, rpi);
13: ˆT.P ← T.P − pi;
14: ˆT.K ← T.K − κi;
15: ˆT.K.UpdateCarries(ˆr,pi);
16: ˆT.P.AddAvailableConnectionsFrom(ˆr,pi);
17: ˆT.K.AddCarriesForNewConnections(ˆr,pi);
18: Return(ˆT);
19: procedure UpdateQueue( Q, ˆr, pi ) :
20: Q.RemoveActionsWith(pi);
21: Q.AddNewActions(ˆr, pi);
</listItem>
<subsectionHeader confidence="0.996244">
2.1 Decoding Algorithm
</subsectionHeader>
<bodyText confidence="0.994439816901408">
Algorithm 1 gives details of the UMT decoding
algorithm. The decoder takes as input the source
sentence, x, the parameters of the scoring func-
tion, w, and the synchronous-grammar, G. At
line 2 the partial synchronous-tree T is initialized
by setting I, C, P and K to empty sets ∅. At
line 3 the queue of candidate connection-actions
is initialized as Q ≡ { [rleaf, null]  |rleaf is a
leaf rule}, where null means that there is no post-
condition specified, since the first rule does not
need to connect to anything. A leaf rule rleaf is
any synchronous rule with only terminals on the
right-hand sides. At line 4 the main loop starts.
Each iteration of the main loop will expand T us-
ing one connection-action. The loop ends when
Q is empty, implying that T covers the full sen-
tence and has no more missing branches or par-
ents. The best scoring action according to the
parameter vector w is popped from the queue at
line 5. The scoring of connection-actions is dis-
cussed in details in Section 3.2. At line 6 the se-
lected connection-action is used to expand T. At
line 7 the queue of candidates is updated accord-
ingly (see lines 19-21). At line 8 the decoder it-
erates the main loop, until T is complete and is
returned at line 9.
Lines 10-18 describe the CreateConnection(·)
procedure, that connects the partial synchronous-
tree T to the selected rule rˆ via the postcondi-
tion pi specified by the candidate-action selected
in line 5. This procedure returns the resulting par-
tial synchronous-tree: Tˆ ≡ h T ⋖ [ˆr, pi] i. At
line 11, rˆ is added to the rule set I. At line 12 the
connection between rˆ and rpz (the rule specified
in the postcondition) is added to the set of connec-
tions C. At line 13, pi is removed from P. At
line 14 the carry ki matching with pi is removed
from K. At line 15 the set of carries K is updated,
in order to update those carries that need to pro-
vide information about the new action. At line 16
new postconditions representing the non-terminals
in rˆ that are available for subsequent connections
are added in P. At line 17 the carries associated
with these new postconditions are computed and
added to K. Finally at line 18 the updated partial
synchronous-tree is returned.
In the very first iteration, the
CreateConnection(·) procedure has nothing
to compute for some lines. Line 11 is not exe-
cuted since the first leaf rule needs no connection
and has nothing to connect to. lines 12-13 are
not executed since P and K are ∅ and pi is not
specified for the first action. Line 15 is not
executed since there are no carries to be updated.
Lines 16-17 only add the postcondition and carry
relative to the leaf rule head link.
The procedure used to update Q is reported in
lines 19-21. At line 20 all the connection-actions
involving the expansion of pi are removed from
Q. These actions are the incompatible alternatives
to the selected action. In the very first iteration,
all actions in Q are removed because they are all
incompatible with the connected-graph constraint.
At line 21 new connection-actions are added to
Q. These are the candidate actions proposing a
connection to the available non-terminals of the
selected action’s new rule ˆr. The rules used for
these new candidate-actions must not be in con-
flict with the current structure of T (e.g. the rule
cannot generate a source side terminal that is al-
ready covered by T).
</bodyText>
<page confidence="0.997393">
12
</page>
<sectionHeader confidence="0.986456" genericHeader="method">
3 Discriminative Reinforcement
</sectionHeader>
<subsectionHeader confidence="0.677777">
Learning
</subsectionHeader>
<bodyText confidence="0.999962307692307">
Training a UMT model simply means training the
parameter vector w that is used to choose the best
scoring action during decoding. We propose a
novel method to apply a kind of minimum error
rate training (MERT) to w. Because each ac-
tion choice must be evaluated in the context of
the complete translation-derivation, we formalize
this method in terms of Reinforcement Learning.
We propose Discriminative Reinforcement Learn-
ing as an appropriate way to train a UMT model to
maximize the BLEU score of the complete deriva-
tion. First we define DRL as a novel generic train-
ing framework.
</bodyText>
<subsectionHeader confidence="0.902025">
3.1 Generic Framework of DRL
</subsectionHeader>
<bodyText confidence="0.705117">
RL can be applied to any task, T, that can be for-
malized in terms of:
</bodyText>
<listItem confidence="0.989026">
1) The set of states S1;
2) A set of actions As for each state s E S;
3) The transition function T : S x As —* S, that
specifies the next state given a source state and
performed action2;
4) The reward function, R : S x As —* R;
</listItem>
<bodyText confidence="0.81505275">
5) The discount factor, γ E [0, 1].
A policy is defined as any map π : S —* A. Its
value function is given by:
σ
</bodyText>
<equation confidence="0.9517695">
V π(s0) = E γiR(si,π(si)) (1)
i=0
</equation>
<bodyText confidence="0.9999266">
where path(s0|π) - (s0, s1, · · · , sσ|π) is the se-
quence of states determined by following policy π
starting at state s0. The Q-function is the total fu-
ture reward of performing action a0 in state s0 and
then following policy π:
</bodyText>
<equation confidence="0.96371">
Qπ(s0, a0) = R(s0, a0) + γVπ(s1) (2)
</equation>
<bodyText confidence="0.998832222222222">
Standard RL algorithms search for a policy that
maximizes the given reward.
Because we are taking a discriminative ap-
proach to learn w, we formalize our optimization
task similarly to an inverse reinforcement learning
problem (Ng and Russell, 2000): we are given in-
formation about the optimal action sequence and
we want to learn a discriminative reward func-
tion. As in other discriminative approaches, this
</bodyText>
<footnote confidence="0.99223875">
1S can be either finite or infinite.
2For simplicity we describe a deterministic process. To
generalize to the stochastic process, replace the transition
function with the transition probability: Psa(s′), s′ ∈ S.
</footnote>
<construct confidence="0.511809">
Algorithm 2 Discriminative RL
</construct>
<listItem confidence="0.9973391">
1: function Trainer (φ, T , D ) : w
2: repeat
3: s +—SampleState(S);
4: aˆ +— π,(s);
5: a′ +—SampleAction(As);
6: if QπW(s, ˆa) &lt; QπW(s, a′) in D then
7: w +— w + 4b,(s, a′) − 4b-(s, ˆa);
8: end if
9: until convergence
10: Return(w);
</listItem>
<bodyText confidence="0.9861471">
approach simplifies the task of learning the re-
ward function in two respects: the learned reward
function only needs to be monotonically related
to the true reward function, and this property only
needs to hold for the best competing alternatives.
This is all we need in order to use the discrimina-
tive reward function in an optimal classifier, and
this simplification makes learning easier in cases
where the true reward function is too complicated
to model directly.
In RL, an optimal policy π∗ is one which, at
each state s, chooses the action which maximizes
the future reward Qπ∗(s, a). We assume that the
future discriminative reward can be approximated
with a linear function ˜Qπ(s, a) in some feature-
vector representation φ : S x As —* Rd that maps
a state-action pair to a d-dimensional features vec-
tor:
˜Qπ(s, a) = w φ(s, a) (3)
where w E Rd. This gives us the following policy:
</bodyText>
<equation confidence="0.7623185">
π,(s) = arg max w φ(s, a) (4)
a∈As
</equation>
<bodyText confidence="0.9999288">
The set of parameters of this policy is the vec-
tor w. With this formalization, all we need to
learn is a vector w such that the resulting deci-
sions are compatible with the given information
about the optimal action sequence. We propose a
Perceptron-like algorithm to learn these parame-
ters.
Algorithm 2 describes the DRL meta-algorithm.
The Trainer takes as input φ, the task T, and a
generic set of data D describing the behaviors we
want to learn. The output is the weight vector w
of the learned policy that fits the data D. The al-
gorithm consists in a single training loop that is
repeated until convergence (lines 2-9). At line 3
a state, s, is sampled from S. At line 4, aˆ is set to
</bodyText>
<page confidence="0.995631">
13
</page>
<bodyText confidence="0.999822428571428">
be the action that would be preferred by the cur-
rent w-policy. At line 5 an action, a′, is sampled
from As such that a′ =6 ˆa. At line 6 the algo-
rithm checks if preferring path(T (s, ˆa), πw) over
path(T(s, a′), πw) is a correct choice according
to the behaviors data D that the algorithm aims to
learn. If the current w-policy contradicts D, line 7
is executed to update the weight vector to promote
Φw(s, a′) and penalize Φw(s, ˆa), where Φw(s, a)
is the summation of the features vectors of the en-
tire derivation path starting at (s, a) and following
policy πw. This way of updating w has the ef-
fect of increasing the ˜Q(·) value associated with
all the actions in the sequence that generated the
promoted structure, and reducing the ˜Q(·) value
of the actions in the sequence that generated the
penalized structure3.
We have described the DRL meta-algorithm to
be as general as possible. When applied to a spe-
cific problem, more details can be specified: 1) it
is possible to choose specific sampling techniques
to implement lines 3 and 5; 2) the test at line 6
needs to be detailed according to the nature of T
and D; 3) the update statement at line 7 can be re-
placed with a more sophisticated update approach.
We address these issues and describe a range of
alternatives as we apply DRL to UMT in Section
3.2.
</bodyText>
<subsectionHeader confidence="0.999777">
3.2 Application of DRL to UMT
</subsectionHeader>
<bodyText confidence="0.9914075">
To apply DRL we formalize the task of translating
x with UMT as T ≡ {5, {As}, T, R, ry}:
</bodyText>
<listItem confidence="0.9102422">
1) The set of states 5 is the space of all possible
UMT partial synchronous-trees, τ;
2) The set Aτ,x is the set of connection-actions
that can expand τ connecting new synchronous-
rule instantiations matching the input sentence x
on the source side;
3) The transition function T is the connection
function τˆ ≡ h τ ⋖ a i formalized in Section 2
and detailed by the procedure CreateConnection(·)
in Algorithm 1;
</listItem>
<bodyText confidence="0.869896857142857">
4) The true reward function R is the BLEU score.
BLEU is a loss function that quantifies the differ-
ence between the reference translation and the out-
put translation t. The BLEU score can be com-
puted only when a terminal state is reached and a
full translation is available. Thus, the rewards are
all zero except at terminal states, called a Pure De-
</bodyText>
<footnote confidence="0.641690666666667">
3Preliminary experiments with updating only the features
for aˆ and a′ produced substantially worse results.
layed Reward function;
</footnote>
<listItem confidence="0.57947">
5) Considering the nature of the problem and re-
ward function, we choose an undiscounted setting:
ry = 1.
</listItem>
<bodyText confidence="0.9999038">
Next we specify the details of the DRL algo-
rithm. The data D consists of a set of pairs of
sentences, D ≡ {(x, t*)}, where x is the source
sentence and t* is the reference translation. The
feature-vector representation function φ maps a
pair (τ, a) to a real valued vector having any num-
ber of dimensions. Each dimension corresponds
to a distinct feature function that maps: {τ} ×
Aτ,x → R. Details of the features functions im-
plemented for our model are given in Section 4.
Each loop of the DRL algorithm analyzes a single
sample (x, t*) ∈ D. The state s is sampled from a
uniform distribution over hs0, si, · · · , sσ|πi. The
action a′ is sampled from a Zipfian distribution
over {Aτ,x − ˆa} sorted with the ˜QπW(s, a) func-
tion. In this way actions with higher score have
higher probability to be drawn, while actions at the
bottom of the rank still have a small probability to
be selected. The if at line 6 tests if the translation
produced by path(T(s, a′), πw) has higher BLEU
score than the one produced by path(T (s, ˆa), πw).
For the update statement at line 7 we use
the Averaged Perceptron technique (Freund and
Schapire, 1999). Algorithm 2 can be eas-
ily adapted to implement the efficient Averaged
Perceptron updates (e.g. see Section 2.1.1 of
(Daum´e III, 2006)). In preliminary experiments,
we found that other more aggressive update tech-
nique, such as Passive-Aggressive (Crammer et
al., 2006), Aggressive (Shen et al., 2007), or
MIRA (Crammer and Singer, 2003), lead to worst
accuracy. To see why this might be, consider that
a MT decoder needs to learn to construct struc-
tures (t, h), while the training data specifies the
gold translation t* but gives no information on the
hidden-correspondence structure h. As discussed
in (Liang et al., 2006), there are output structures
that match the reference translation using a wrong
internal structure (e.g. assuming wrong internal
alignment). While in other cases the output trans-
lation can be a valid alternative translation but gets
a low BLEU score because it differs from t*. Ag-
gressively promoting/penalizing structures whose
correctness can be only partially verified can be
expected to harm generalization ability.
</bodyText>
<page confidence="0.999221">
14
</page>
<sectionHeader confidence="0.99464" genericHeader="method">
4 Undirected Features
</sectionHeader>
<bodyText confidence="0.999990027027027">
In this section we show how the features designed
for bottom-up HMT can be adapted to the undi-
rected approach, and we introduce a new feature
from the class of undirected features that are made
possible by the undirected approach.
Local features depend only on the action rule r.
These features can be used in the undirected ap-
proach without adaptation, since they are indepen-
dent of the surrounding structure. For our experi-
ments we use a standard set of local features: the
probability of the source phrase given the target
phrase; the lexical translation probabilities of the
source words given the target words; the lexical
translation probabilities of the target words given
the source words; and the Word Penalty feature.
Contextual features are dependent on the inter-
action between the action rule r and the avail-
able context. In UMT all the needed information
about the available context is stored in the carry
κZ. Therefore, the computation of contextual fea-
tures whose carry’s size is bounded (like the LM)
requires constant time.
The undirected adaptation of the LM feature
computes the scores of the new n-grams formed
by adding the terminals of the action rule r to the
current partial translation T. In the case that the
action rule r is connected to T via a child non-
terminal, the carry is expressed as κZ - ([WL ⋆
WR]). Where WL and WR are respectively the left
and right boundary target words of the span cov-
ered by T. This notation is analogous to the stan-
dard star notation used for the bottom-up decoder
(e.g. (Chiang, 2007) Section 5.3.2). In the case
that r is connected to T via the head non-terminal,
the carry is expressed as κZ - (WR]-[WL). Where
WL and WR are respectively the left and right
boundary target words of the surrounding context
provided by T. The boundary words stored in the
carry and the terminals of the action rule are all the
information needed to compute and score the new
n-grams generated by the connection-action.
In addition, we introduce the Context-Free Fac-
tor (CFF) features. An action rule r is connected
to T via one of r’s non-terminals, Xr,τ. Thus, the
score of the interaction between r and the context
structure attached to Xr,τ can be computed ex-
actly, while the score of the structures attached to
other r nonterminals (i.e. those in postconditions)
cannot be computed since these branches are miss-
ing. Each of these postcondition nonterminals
has an associated CFF feature, which is an upper
bound on the score of its missing branch. More
precisely, it is an upper bound on the context-free
component of this score. This upper bound can be
exactly and efficiently computed using the Forest
Rescoring Framework (Huang and Chiang, 2007;
Huang, 2008). This framework separates the MT
decoding in two steps. In the first step only the
context-free factors are considered. The output of
the first step is a hypergraph called the context-
free-forest, which compactly represents an expo-
nential number of synchronous-trees. The second
step introduces contextual features by applying a
process of state-splitting to the context-free-forest,
rescoring with non-context-free factors, and effi-
ciently pruning the search space.
To efficiently compute CFF features we run
the Inside-Outside algorithm with the (max, +)
semiring (Goodman, 1999) over the context-free-
forest. The result is a map that gives the maxi-
mum Inside and Outside scores for each node in
the context-free forest. This map is used to get the
value of the CFF features in constant time while
running the forest rescoring step.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999558">
We implement our model on top of Cdec (Dyer et
al., 2010). Cdec provides a standard implemen-
tation of the HMT decoder (Chiang, 2007) and
MERT training (Och, 2003) that we use as base-
line.
We experiment on the NIST Chinese-English
parallel corpus. The training corpus contains
239k sentence pairs with 6.9M Chinese words and
8.9M English words. The test set contains 919
sentence pairs. The hierarchical translation gram-
mar was extracted using the Joshua toolkit (Li et
al., 2009) implementation of the suffix array rule
extractor algorithm (Callison-Burch et al., 2005;
Lopez, 2007).
Table 1 reports the decoding time measures.
HMT with beam1 is the fastest possible configu-
ration for HMT, but it is 71.59% slower than UMT.
This is because HMT b1 constructs O(n2) sub-
trees, many of which end up not being used in
the final result, whereas UMT only constructs the
rule instantiations that are required. HMT with
beam30 is the fastest configuration that reaches
state of the art accuracy, but increases the aver-
age time per sentence by an additional 131.36%
when compared with UMT. The rescoring time is
</bodyText>
<page confidence="0.996176">
15
</page>
<table confidence="0.99808475">
Model sent. t. sent. t. var. resc. t. resc. t. var.
UMT 135.2ms - 38.9 ms -
HMT b1 232.0ms +71.59% 141.3 ms +263.23%
HMT b30 312.8ms +131.36% 226.9 ms +483.29%
</table>
<tableCaption confidence="0.995436">
Table 1: Decoding speed comparison.
</tableCaption>
<table confidence="0.998594">
Model sent. t. sent. t. var.
UMT with DRL 267.4 ms -
HMT b1 765.2 ms +186.16%
HMT b30 1153.5 ms +331.37%
</table>
<tableCaption confidence="0.989749">
Table 2: Training speed comparison.
</tableCaption>
<table confidence="0.999811">
Model BLEU relative loss p-value
UMT with DRL 30.14 6.33% 0.18
HMT b1 30.87 4.07% 0.21
HMT b30 32.18 - -
</table>
<tableCaption confidence="0.999925">
Table 3: Accuracy comparison.
</tableCaption>
<bodyText confidence="0.999934301587302">
the average time spent on the forest rescoring step,
which is the only step where the decoders actu-
ally differ. This is the step that involves the inte-
gration of the Language Model and other contex-
tual features. For HMT b30, rescoring takes two
thirds of the total decoding time. Thus rescoring
is the most time consuming step in the pipeline.
The rescoring time comparison shows even bigger
gains for UMT. HMT b30 is almost 6 times slower
than UMT.
Table 2 reports the training time measures.
These results show HMT b30 training is more
than 4 times slower than UMT training with DRL.
Comparing with Table 1, we notice that the rela-
tive gain on average training time is higher than
the gain measured at decoding time. This is be-
cause MERT has an higher complexity than DRL.
Both of the training algorithms requires 10 train-
ing epochs to reach convergence.
Table 3 reports the accuracy measures. As ex-
pected, accuracy degrades the more aggressively
the search space is pruned. UMT trained with
DRL loses 2.0 BLEU points compared to HMT
b30. This corresponds to a relative-loss of 6.33%.
Although not inconsequential, this variation is
not considered big (e.g. at the WMT-11 Ma-
chine Translation shared task (Callison-Burch et
al., 2011)). To measure the significance of the
variation, we compute the sign test and measure
the one-tail p-value for the presented models in
comparison to HMT b30. From the values re-
ported in the fourth column, we can observe that
the BLEU score variations would not normally be
considered significant. For example, at WMT-11
two systems were considered equivalent if p &gt;
0.1, as in these cases. The accuracy cannot be
compared in terms of search score since the mod-
els we are comparing are trained with distinct al-
gorithms and thus the search scores are not com-
parable.
To test the impact of the CFF features, we
trained and tested UMT with DRL with and with-
out these features. This resulted in an accuracy de-
crease of 2.3 BLEU points. Thus these features are
important for the success of the greedy approach.
They provide an estimate of the score of the miss-
ing branches, thus helping to avoid some actions
that have a good local score but lead to final trans-
lations with low global score.
To validate the results, additional experiments
were executed on the French to Italian portion
of the Europarl corpus v6. This portion contains
190k pairs of sentences. The first 186k sentences
were used to extract the grammar and train the two
models. The final tests were performed on the re-
maining 4k sentence pairs. With this corpus we
measured a similar speed gain. HMT b30 is 2.3
times slower at decoding compared to UMT, and
6.1 times slower at rescoring, while UMT loses
1.1 BLEU points in accuracy. But again the ac-
curacy differences are not considered significant.
We measured a p-value of 0.25, which is not sig-
nificant at the 0.1 level.
</bodyText>
<sectionHeader confidence="0.999903" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9996802">
Models sharing similar intuitions have been pre-
viously applied to other structure prediction tasks.
For example, Nivre et al. (2006) presents a linear
time syntactic dependency parser, which is con-
strained in a left-to-right decoding order. This
model offers a different accuracy/complexity bal-
ance than the quadratic time graph-based parser of
Mcdonald et al. (2005).
Other approaches learning a model specifically
for greedy decoding have been applied with suc-
</bodyText>
<page confidence="0.993518">
16
</page>
<bodyText confidence="0.999948139534884">
cess to other less complex tasks. Shen et al. (2007)
present the Guided Learning (GL) framework for
bidirectional sequence classification. GL success-
fully combines the tasks of learning the order of
inference and training the local classifier in a sin-
gle Perceptron-like algorithm, reaching state of the
art accuracy with complexity lower than the ex-
haustive counterpart (Collins, 2002).
Goldberg and Elhadad (2010) present a simi-
lar training approach for a Dependency Parser that
builds the tree-structure by recursively creating
the easiest arc in a non-directional manner. This
model also integrates the tasks of learning the or-
der of inference and training the parser in a single
Perceptron. By “non-directional” they mean the
removal of the constraint of scanning the sentence
from left to right, which is typical of shift-reduce
models. However this algorithm still builds the
tree structures in a bottom-up fashion. This model
has a O(n log n) decoding complexity and accu-
racy performance close to the O(n2) graph-based
parsers (Mcdonald et al., 2005).
Similarities can be found between DRL and pre-
vious work that applies discriminative training to
structured prediction: Collins and Roark (2004)
present an Incremental Parser trained with the Per-
ceptron algorithm. Their approach is specific to
dependency parsing and requires a function to test
exact match of tree structures to trigger parameter
updates. On the other hand, DRL can be applied to
any structured prediction task and can handle any
kind of reward function. LASO (Daum´e III and
Marcu, 2005; Daum´e III et al., 2005) and SEARN
(Daum´e III et al., 2009; Daum´e III et al., 2006)
are generic frameworks for discriminative training
for structured prediction: LASO requires a func-
tion that tests correctness of partial structures to
trigger early updates, while SEARN requires an
optimal policy to initialize the learning algorithm.
Such a test function or optimal policy cannot be
computed for tasks such as MT where the hidden
correspondence structure h is not provided in the
training data.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="evaluation">
7 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999973384615385">
In general, we believe that greedy-discriminative
solutions are promising for tasks like MT, where
there is not a single correct solution: normally
there are many correct ways to translate the same
sentence, and for each correct translation there
are many different derivation-trees generating that
translation, and each correct derivation tree can be
built greedily following different inference orders.
Therefore, the set of correct decoding paths is a
reasonable portion of UMT’s search space, giving
a well-designed greedy algorithm a chance to find
a good translation even without beam search.
In order to directly evaluate the impact of our
proposed decoding strategy, in this paper the only
novel features that we consider are the CFF fea-
tures. But to take full advantage of the power
of discriminative training and the lower decoding
complexity, it would be possible to vastly increase
the number of features. The UMT’s undirected na-
ture allows the integration of non-bottom-up con-
textual features, which cannot be used by stan-
dard HMT and PbMT. And the use of a history-
based model allows features from an arbitrarily
wide context, since the model does not need to be
factorized. Exploring the impact of this advantage
is left for future work.
</bodyText>
<sectionHeader confidence="0.997291" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999977944444445">
The main contribution of this work is the pro-
posal of a new MT model that offers an accu-
racy/complexity balance that was previously un-
available among the choices of hierarchical mod-
els.
We have presented the first Undirected frame-
work for MT. This model combines advantages
given by the use of hierarchical synchronous-
grammars with a more efficient decoding algo-
rithm. UMT’s nature allows us to design novel
undirected features that better approximate con-
textual features (such as the LM), and to introduce
a new class of undirected features that cannot be
used by standard bottom-up decoders. Further-
more, we generalize the training algorithm into
a generic Discriminative Reinforcement Learning
meta-algorithm that can be applied to any struc-
tured prediction task.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985050625">
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19:263–311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
</reference>
<page confidence="0.995241">
17
</page>
<reference confidence="0.999512057142857">
phrases. In ACL ’05: Proceedings of the 43rd Con-
ference of the Association for Computational Lin-
guistics, Ann Arbor, MI, USA.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In WMT ’11:
Proceedings of the 6th Workshop on Statistical Ma-
chine Translation, Edinburgh, Scotland.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL
’05: Proceedings of the 43rd Conference of the As-
sociation for Computational Linguistics, Ann Arbor,
MI, USA.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In ACL ’04:
Proceedings of the 42rd Conference of the Associa-
tion for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP ’02:
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, Philadel-
phia, PA, USA.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal ofMachine Learning Research, 3:951–991.
Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and
Yoram Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551–585.
Hal Daum´e III and Daniel Marcu. 2005. Learning
as search optimization: approximate large margin
methods for structured prediction. In ICML ’05:
Proceedings of the 22nd International Conference
on Machine Learning, Bonn, Germany.
Hal Daum´e III, John Langford, and Daniel Marcu.
2005. Search-based structured prediction as clas-
sification. In ASLTSP ’05: Proceedings of the
NIPS Workshop on Advances in Structured Learn-
ingfor Text and Speech Processing, Whistler, British
Columbia, Canada.
Hal Daum´e III, John Langford, and Daniel Marcu.
2006. Searn in practice. Technical report.
Hal Daum´e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Submit-
ted to Machine Learning Journal.
Hal Daum´e III. 2006. Practical structured learning
techniques for natural language processing. Ph.D.
thesis, University of Southern California.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Hendra Setiawan, Ferhan Ture, Vladimir Ei-
delman, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In ACL ’10: Proceedings of the ACL 2010 System
Demonstrations, Uppsala, Sweden.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277–296.
Andrea Gesmundo and James Henderson. 2011.
Heuristic Search for Non-Bottom-Up Tree Structure
Prediction. In EMNLP ’11: Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, Edinburgh, Scotland, UK.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In NAACL ’10: Proceedings of the
11th Conference of the North American Chapter of
the Association for Computational Linguistics, Los
Angeles, CA, USA.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25:573–605.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In IWPT ’05: Proceedings of the 9th Inter-
national Workshop on Parsing Technology, Vancou-
ver, British Columbia, Canada.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL ’07: Proceedings of the 45th Confer-
ence of the Association for Computational Linguis-
tics, Prague, Czech Republic.
Liang Huang. 2008. Forest-based algorithms in natu-
ral language processing. Ph.D. thesis, University of
Pennsylvania.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 4th Conference of
the North American Chapter of the Association for
Computational Linguistics, Edmonton, Canada.
Zhifei Li, Chris Callison-Burch, Chris Dyer, San-
jeev Khudanpur, Lane Schwartz, Wren Thornton,
Jonathan Weese, and Omar Zaidan. 2009. Joshua:
An open source toolkit for parsing-based machine
translation. In WMT ’09: Proceedings of the
4th Workshop on Statistical Machine Translation,
Athens, Greece.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In COLING-
ACL ’06: Proceedings ofthe 21st International Con-
ference on Computational Linguistics and the 44th
Conference of the Association for Computational
Linguistics, Sydney, Australia.
</reference>
<page confidence="0.984776">
18
</page>
<reference confidence="0.99972986">
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL ’07:
Proceedings of the 2007Joint Conference on Empir-
ical Methods in Natural Language Processing and
ComputationalNatural Language Learning, Prague,
Czech Republic.
Ryan Mcdonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In ACL ’05: Proceedings of the
43rd Conference of the Association for Computa-
tional Linguistics, Ann Arbor, MI, USA.
Andrew Y. Ng and Stuart Russell. 2000. Algorithms
for inverse reinforcement learning. In ICML ’00:
Proceedings of the 17th International Conference on
Machine Learning, Stanford University, CA, USA.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In LREC ’06: Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, Genoa, Italy.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ’03: Pro-
ceedings of the 41st Conference of the Association
for Computational Linguistics, Sapporo, Japan.
Frank Rosenblatt. 1958. The Perceptron: A proba-
bilistic model for information storage and organiza-
tion in the brain. Psychological Review, 65(6):386–
408.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifi-
cation. In ACL ’07: Proceedings of the 45th Confer-
ence of the Association for Computational Linguis-
tics, Prague, Czech Republic.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In EMNLP-CoNLL ’07:
Proceedings of the 2007Joint Conference on Empir-
ical Methods in Natural Language Processing and
ComputationalNatural Language Learning, Prague,
Czech Republic.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In ACL ’01: Pro-
ceedings of the 39th Conference of the Association
for Computational Linguistics, Toulouse, France.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In NAACL ’06: Proceedings ofthe
7th Conference of the North American Chapter of
the Association for Computational Linguistics, New
York, New York.
</reference>
<page confidence="0.999332">
19
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968424">
<title confidence="0.9995385">Undirected Machine Translation Discriminative Reinforcement Learning</title>
<author confidence="0.999947">Andrea Gesmundo James Henderson</author>
<affiliation confidence="0.990476">Google Inc. Xerox Research Centre</affiliation>
<email confidence="0.998541">andrea.gesmundo@gmail.comjames.henderson@xrce.xerox.com</email>
<abstract confidence="0.998926789473684">We present a novel Undirected Machine Translation model of Hierarchical MT that is not constrained to the standard bottomup inference order. Removing the ordering constraint makes it possible to condition on top-down structure and surrounding context. This allows the introduction of a new class of contextual features that are not constrained to condition only on the bottom-up context. The model builds translation-derivations efficiently in a greedy fashion. It is trained to learn to choose jointly the best action and the best inference order. Experiments show that the decoding time is halved and forestrescoring is 6 times faster, while reaching accuracy not significantly different from state of the art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1034" citStr="Brown et al., 1993" startWordPosition="147" endWordPosition="150">n on top-down structure and surrounding context. This allows the introduction of a new class of contextual features that are not constrained to condition only on the bottom-up context. The model builds translation-derivations efficiently in a greedy fashion. It is trained to learn to choose jointly the best action and the best inference order. Experiments show that the decoding time is halved and forestrescoring is 6 times faster, while reaching accuracy not significantly different from state of the art. 1 Introduction Machine Translation (MT) can be addressed as a structured prediction task (Brown et al., 1993; Yamada and Knight, 2001; Koehn et al., 2003). MT’s goal is to learn a mapping function, f, from an input sentence, x, into y = (t, h), where t is the sentence translated into the target language, and h is the hidden correspondence structure (Liang et al., 2006). In Hierarchical MT (HMT) (Chiang, 2005) the hidden correspondence structure is the synchronous-tree composed by instantiations of synchronous rules from the input grammar, G. Statistical models usually define f as: f(x) = arg maxy∈Y Score(x, y), where Score(x, y) is a function whose parameters can be learned with a specialized learni</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Colin Bannard</author>
<author>Josh Schroeder</author>
</authors>
<title>Scaling phrase-based statistical machine translation to larger corpora and longer phrases.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Conference of the Association for Computational Linguistics,</booktitle>
<location>Ann Arbor, MI, USA.</location>
<contexts>
<context position="24971" citStr="Callison-Burch et al., 2005" startWordPosition="4288" endWordPosition="4291">ime while running the forest rescoring step. 5 Experiments We implement our model on top of Cdec (Dyer et al., 2010). Cdec provides a standard implementation of the HMT decoder (Chiang, 2007) and MERT training (Och, 2003) that we use as baseline. We experiment on the NIST Chinese-English parallel corpus. The training corpus contains 239k sentence pairs with 6.9M Chinese words and 8.9M English words. The test set contains 919 sentence pairs. The hierarchical translation grammar was extracted using the Joshua toolkit (Li et al., 2009) implementation of the suffix array rule extractor algorithm (Callison-Burch et al., 2005; Lopez, 2007). Table 1 reports the decoding time measures. HMT with beam1 is the fastest possible configuration for HMT, but it is 71.59% slower than UMT. This is because HMT b1 constructs O(n2) subtrees, many of which end up not being used in the final result, whereas UMT only constructs the rule instantiations that are required. HMT with beam30 is the fastest configuration that reaches state of the art accuracy, but increases the average time per sentence by an additional 131.36% when compared with UMT. The rescoring time is 15 Model sent. t. sent. t. var. resc. t. resc. t. var. UMT 135.2ms</context>
</contexts>
<marker>Callison-Burch, Bannard, Schroeder, 2005</marker>
<rawString>Chris Callison-Burch, Colin Bannard, and Josh Schroeder. 2005. Scaling phrase-based statistical machine translation to larger corpora and longer phrases. In ACL ’05: Proceedings of the 43rd Conference of the Association for Computational Linguistics, Ann Arbor, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In WMT ’11: Proceedings of the 6th Workshop on Statistical Machine Translation,</booktitle>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="27215" citStr="Callison-Burch et al., 2011" startWordPosition="4681" endWordPosition="4684">1, we notice that the relative gain on average training time is higher than the gain measured at decoding time. This is because MERT has an higher complexity than DRL. Both of the training algorithms requires 10 training epochs to reach convergence. Table 3 reports the accuracy measures. As expected, accuracy degrades the more aggressively the search space is pruned. UMT trained with DRL loses 2.0 BLEU points compared to HMT b30. This corresponds to a relative-loss of 6.33%. Although not inconsequential, this variation is not considered big (e.g. at the WMT-11 Machine Translation shared task (Callison-Burch et al., 2011)). To measure the significance of the variation, we compute the sign test and measure the one-tail p-value for the presented models in comparison to HMT b30. From the values reported in the fourth column, we can observe that the BLEU score variations would not normally be considered significant. For example, at WMT-11 two systems were considered equivalent if p &gt; 0.1, as in these cases. The accuracy cannot be compared in terms of search score since the models we are comparing are trained with distinct algorithms and thus the search scores are not comparable. To test the impact of the CFF featu</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In WMT ’11: Proceedings of the 6th Workshop on Statistical Machine Translation, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Conference of the Association for Computational Linguistics,</booktitle>
<location>Ann Arbor, MI, USA.</location>
<contexts>
<context position="1338" citStr="Chiang, 2005" startWordPosition="205" endWordPosition="207">action and the best inference order. Experiments show that the decoding time is halved and forestrescoring is 6 times faster, while reaching accuracy not significantly different from state of the art. 1 Introduction Machine Translation (MT) can be addressed as a structured prediction task (Brown et al., 1993; Yamada and Knight, 2001; Koehn et al., 2003). MT’s goal is to learn a mapping function, f, from an input sentence, x, into y = (t, h), where t is the sentence translated into the target language, and h is the hidden correspondence structure (Liang et al., 2006). In Hierarchical MT (HMT) (Chiang, 2005) the hidden correspondence structure is the synchronous-tree composed by instantiations of synchronous rules from the input grammar, G. Statistical models usually define f as: f(x) = arg maxy∈Y Score(x, y), where Score(x, y) is a function whose parameters can be learned with a specialized learning algorithm. In MT applications, it is not possible to enumerate all y ∈ Y. HMT decoding applies pruning (e.g. Cube Pruning (Huang and Chiang, 2005)), but even then HMT has higher complexity than Phrase Based MT (PbMT) (Koehn et al., 2003). On the other hand, HMT improves over PbMT by introducing the p</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In ACL ’05: Proceedings of the 43rd Conference of the Association for Computational Linguistics, Ann Arbor, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="22390" citStr="Chiang, 2007" startWordPosition="3869" endWordPosition="3870">e, the computation of contextual features whose carry’s size is bounded (like the LM) requires constant time. The undirected adaptation of the LM feature computes the scores of the new n-grams formed by adding the terminals of the action rule r to the current partial translation T. In the case that the action rule r is connected to T via a child nonterminal, the carry is expressed as κZ - ([WL ⋆ WR]). Where WL and WR are respectively the left and right boundary target words of the span covered by T. This notation is analogous to the standard star notation used for the bottom-up decoder (e.g. (Chiang, 2007) Section 5.3.2). In the case that r is connected to T via the head non-terminal, the carry is expressed as κZ - (WR]-[WL). Where WL and WR are respectively the left and right boundary target words of the surrounding context provided by T. The boundary words stored in the carry and the terminals of the action rule are all the information needed to compute and score the new n-grams generated by the connection-action. In addition, we introduce the Context-Free Factor (CFF) features. An action rule r is connected to T via one of r’s non-terminals, Xr,τ. Thus, the score of the interaction between r</context>
<context position="24535" citStr="Chiang, 2007" startWordPosition="4222" endWordPosition="4223">ext-free-forest, rescoring with non-context-free factors, and efficiently pruning the search space. To efficiently compute CFF features we run the Inside-Outside algorithm with the (max, +) semiring (Goodman, 1999) over the context-freeforest. The result is a map that gives the maximum Inside and Outside scores for each node in the context-free forest. This map is used to get the value of the CFF features in constant time while running the forest rescoring step. 5 Experiments We implement our model on top of Cdec (Dyer et al., 2010). Cdec provides a standard implementation of the HMT decoder (Chiang, 2007) and MERT training (Och, 2003) that we use as baseline. We experiment on the NIST Chinese-English parallel corpus. The training corpus contains 239k sentence pairs with 6.9M Chinese words and 8.9M English words. The test set contains 919 sentence pairs. The hierarchical translation grammar was extracted using the Joshua toolkit (Li et al., 2009) implementation of the suffix array rule extractor algorithm (Callison-Burch et al., 2005; Lopez, 2007). Table 1 reports the decoding time measures. HMT with beam1 is the fastest possible configuration for HMT, but it is 71.59% slower than UMT. This is </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42rd Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="30533" citStr="Collins and Roark (2004)" startWordPosition="5229" endWordPosition="5232">s model also integrates the tasks of learning the order of inference and training the parser in a single Perceptron. By “non-directional” they mean the removal of the constraint of scanning the sentence from left to right, which is typical of shift-reduce models. However this algorithm still builds the tree structures in a bottom-up fashion. This model has a O(n log n) decoding complexity and accuracy performance close to the O(n2) graph-based parsers (Mcdonald et al., 2005). Similarities can be found between DRL and previous work that applies discriminative training to structured prediction: Collins and Roark (2004) present an Incremental Parser trained with the Perceptron algorithm. Their approach is specific to dependency parsing and requires a function to test exact match of tree structures to trigger parameter updates. On the other hand, DRL can be applied to any structured prediction task and can handle any kind of reward function. LASO (Daum´e III and Marcu, 2005; Daum´e III et al., 2005) and SEARN (Daum´e III et al., 2009; Daum´e III et al., 2006) are generic frameworks for discriminative training for structured prediction: LASO requires a function that tests correctness of partial structures to t</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In ACL ’04: Proceedings of the 42rd Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP ’02: Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="5309" citStr="Collins, 2002" startWordPosition="833" endWordPosition="834">n the UMT scoring function’s parameters, using the BLEU score as the global loss function. DRL learns a weight vector for a linear classifier that discriminates between decisions based on which one leads to a complete translation-derivation with a better BLEU score. Promotions/demotions of translations are performed by applying a Perceptron-style update on the sequence of decisions that produced the translation, thereby training local decisions to optimize the global BLEU score of the final translation, while keeping the efficiency and simplicity of the Perceptron Algorithm (Rosenblatt, 1958; Collins, 2002). Our experiments show that UMT with DRL reduces decoding time by over half, and the time to rescore translations with the Language Model by 6 times, while reaching accuracy non-significantly different from the state of the art. 2 Undirected Machine Translation In this section, we present the UMT framework. For ease of presentation, and following synchronous-grammar based MT practice, we will henceforth restrict our focus to binary grammars (Zhang et al., 2006; Wang et al., 2007). A UMT decoder can be formulated as a function, f, that maps a source sentence, x ∈ X, into a structure defined by </context>
<context position="29716" citStr="Collins, 2002" startWordPosition="5103" endWordPosition="5104">er. This model offers a different accuracy/complexity balance than the quadratic time graph-based parser of Mcdonald et al. (2005). Other approaches learning a model specifically for greedy decoding have been applied with suc16 cess to other less complex tasks. Shen et al. (2007) present the Guided Learning (GL) framework for bidirectional sequence classification. GL successfully combines the tasks of learning the order of inference and training the local classifier in a single Perceptron-like algorithm, reaching state of the art accuracy with complexity lower than the exhaustive counterpart (Collins, 2002). Goldberg and Elhadad (2010) present a similar training approach for a Dependency Parser that builds the tree-structure by recursively creating the easiest arc in a non-directional manner. This model also integrates the tasks of learning the order of inference and training the parser in a single Perceptron. By “non-directional” they mean the removal of the constraint of scanning the sentence from left to right, which is typical of shift-reduce models. However this algorithm still builds the tree structures in a bottom-up fashion. This model has a O(n log n) decoding complexity and accuracy pe</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In EMNLP ’02: Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="20113" citStr="Crammer and Singer, 2003" startWordPosition="3484" endWordPosition="3487">l have a small probability to be selected. The if at line 6 tests if the translation produced by path(T(s, a′), πw) has higher BLEU score than the one produced by path(T (s, ˆa), πw). For the update statement at line 7 we use the Averaged Perceptron technique (Freund and Schapire, 1999). Algorithm 2 can be easily adapted to implement the efficient Averaged Perceptron updates (e.g. see Section 2.1.1 of (Daum´e III, 2006)). In preliminary experiments, we found that other more aggressive update technique, such as Passive-Aggressive (Crammer et al., 2006), Aggressive (Shen et al., 2007), or MIRA (Crammer and Singer, 2003), lead to worst accuracy. To see why this might be, consider that a MT decoder needs to learn to construct structures (t, h), while the training data specifies the gold translation t* but gives no information on the hidden-correspondence structure h. As discussed in (Liang et al., 2006), there are output structures that match the reference translation using a wrong internal structure (e.g. assuming wrong internal alignment). While in other cases the output translation can be a valid alternative translation but gets a low BLEU score because it differs from t*. Aggressively promoting/penalizing </context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal ofMachine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="20045" citStr="Crammer et al., 2006" startWordPosition="3473" endWordPosition="3476">bility to be drawn, while actions at the bottom of the rank still have a small probability to be selected. The if at line 6 tests if the translation produced by path(T(s, a′), πw) has higher BLEU score than the one produced by path(T (s, ˆa), πw). For the update statement at line 7 we use the Averaged Perceptron technique (Freund and Schapire, 1999). Algorithm 2 can be easily adapted to implement the efficient Averaged Perceptron updates (e.g. see Section 2.1.1 of (Daum´e III, 2006)). In preliminary experiments, we found that other more aggressive update technique, such as Passive-Aggressive (Crammer et al., 2006), Aggressive (Shen et al., 2007), or MIRA (Crammer and Singer, 2003), lead to worst accuracy. To see why this might be, consider that a MT decoder needs to learn to construct structures (t, h), while the training data specifies the gold translation t* but gives no information on the hidden-correspondence structure h. As discussed in (Liang et al., 2006), there are output structures that match the reference translation using a wrong internal structure (e.g. assuming wrong internal alignment). While in other cases the output translation can be a valid alternative translation but gets a low BLEU </context>
</contexts>
<marker>Crammer, Dekel, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Learning as search optimization: approximate large margin methods for structured prediction.</title>
<date>2005</date>
<booktitle>In ICML ’05: Proceedings of the 22nd International Conference on Machine Learning,</booktitle>
<location>Bonn, Germany.</location>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2005. Learning as search optimization: approximate large margin methods for structured prediction. In ICML ’05: Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e John Langford</author>
<author>Daniel Marcu</author>
</authors>
<title>Search-based structured prediction as classification.</title>
<date>2005</date>
<booktitle>In ASLTSP ’05: Proceedings of the NIPS Workshop on Advances in Structured Learningfor Text and Speech Processing,</booktitle>
<location>Whistler, British Columbia, Canada.</location>
<marker>Langford, Marcu, 2005</marker>
<rawString>Hal Daum´e III, John Langford, and Daniel Marcu. 2005. Search-based structured prediction as classification. In ASLTSP ’05: Proceedings of the NIPS Workshop on Advances in Structured Learningfor Text and Speech Processing, Whistler, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e John Langford</author>
<author>Daniel Marcu</author>
</authors>
<date>2006</date>
<note>Searn in practice. Technical report.</note>
<marker>Langford, Marcu, 2006</marker>
<rawString>Hal Daum´e III, John Langford, and Daniel Marcu. 2006. Searn in practice. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e John Langford</author>
<author>Daniel Marcu</author>
</authors>
<date>2009</date>
<note>Search-based structured prediction. Submitted to Machine Learning Journal.</note>
<marker>Langford, Marcu, 2009</marker>
<rawString>Hal Daum´e III, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. Submitted to Machine Learning Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Practical structured learning techniques for natural language processing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern California.</institution>
<marker>Daum´e, 2006</marker>
<rawString>Hal Daum´e III. 2006. Practical structured learning techniques for natural language processing. Ph.D. thesis, University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Hendra Setiawan</author>
<author>Ferhan Ture</author>
<author>Vladimir Eidelman</author>
<author>Phil Blunsom</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In ACL ’10: Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="24460" citStr="Dyer et al., 2010" startWordPosition="4208" endWordPosition="4211">roduces contextual features by applying a process of state-splitting to the context-free-forest, rescoring with non-context-free factors, and efficiently pruning the search space. To efficiently compute CFF features we run the Inside-Outside algorithm with the (max, +) semiring (Goodman, 1999) over the context-freeforest. The result is a map that gives the maximum Inside and Outside scores for each node in the context-free forest. This map is used to get the value of the CFF features in constant time while running the forest rescoring step. 5 Experiments We implement our model on top of Cdec (Dyer et al., 2010). Cdec provides a standard implementation of the HMT decoder (Chiang, 2007) and MERT training (Och, 2003) that we use as baseline. We experiment on the NIST Chinese-English parallel corpus. The training corpus contains 239k sentence pairs with 6.9M Chinese words and 8.9M English words. The test set contains 919 sentence pairs. The hierarchical translation grammar was extracted using the Joshua toolkit (Li et al., 2009) implementation of the suffix array rule extractor algorithm (Callison-Burch et al., 2005; Lopez, 2007). Table 1 reports the decoding time measures. HMT with beam1 is the fastest</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Setiawan, Ture, Eidelman, Blunsom, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Hendra Setiawan, Ferhan Ture, Vladimir Eidelman, Phil Blunsom, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In ACL ’10: Proceedings of the ACL 2010 System Demonstrations, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="19775" citStr="Freund and Schapire, 1999" startWordPosition="3432" endWordPosition="3435">zes a single sample (x, t*) ∈ D. The state s is sampled from a uniform distribution over hs0, si, · · · , sσ|πi. The action a′ is sampled from a Zipfian distribution over {Aτ,x − ˆa} sorted with the ˜QπW(s, a) function. In this way actions with higher score have higher probability to be drawn, while actions at the bottom of the rank still have a small probability to be selected. The if at line 6 tests if the translation produced by path(T(s, a′), πw) has higher BLEU score than the one produced by path(T (s, ˆa), πw). For the update statement at line 7 we use the Averaged Perceptron technique (Freund and Schapire, 1999). Algorithm 2 can be easily adapted to implement the efficient Averaged Perceptron updates (e.g. see Section 2.1.1 of (Daum´e III, 2006)). In preliminary experiments, we found that other more aggressive update technique, such as Passive-Aggressive (Crammer et al., 2006), Aggressive (Shen et al., 2007), or MIRA (Crammer and Singer, 2003), lead to worst accuracy. To see why this might be, consider that a MT decoder needs to learn to construct structures (t, h), while the training data specifies the gold translation t* but gives no information on the hidden-correspondence structure h. As discusse</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Gesmundo</author>
<author>James Henderson</author>
</authors>
<title>Heuristic Search for Non-Bottom-Up Tree Structure Prediction.</title>
<date>2011</date>
<booktitle>In EMNLP ’11: Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="2961" citStr="Gesmundo and Henderson, 2011" startWordPosition="464" endWordPosition="467">ding is made possible through even stronger pruning: the decoder chooses a single action at each step, never retracts that action, and prunes all incompatible alternatives to that action. If this extreme level of pruning was applied to the CKY-like beam-decoding used in standard HMT, translation quality would be severely degraded. This is because the bottom-up inference order imposed by CKY-like beam-decoding means that all pruning decisions must be based on a bottom-up approximation of contextual features, which leads to search errors that affect the quality of reordering and lexical-choice (Gesmundo and Henderson, 2011). UMT solves this problem by removing the bottom-up inference order constraint, allowing many different inference orders for the same tree structure, and learning the inference order where the decoder can be the most confident in its pruning decisions. Removing the bottom-up inference order constraint makes it possible to condition on top-down structure and surrounding context. This undirected approach allows us to integrate contextual features such as the Language Model (LM) in a more flex10 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Lingui</context>
</contexts>
<marker>Gesmundo, Henderson, 2011</marker>
<rawString>Andrea Gesmundo and James Henderson. 2011. Heuristic Search for Non-Bottom-Up Tree Structure Prediction. In EMNLP ’11: Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In NAACL ’10: Proceedings of the 11th Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Los Angeles, CA, USA.</location>
<contexts>
<context position="29745" citStr="Goldberg and Elhadad (2010)" startWordPosition="5105" endWordPosition="5108">ffers a different accuracy/complexity balance than the quadratic time graph-based parser of Mcdonald et al. (2005). Other approaches learning a model specifically for greedy decoding have been applied with suc16 cess to other less complex tasks. Shen et al. (2007) present the Guided Learning (GL) framework for bidirectional sequence classification. GL successfully combines the tasks of learning the order of inference and training the local classifier in a single Perceptron-like algorithm, reaching state of the art accuracy with complexity lower than the exhaustive counterpart (Collins, 2002). Goldberg and Elhadad (2010) present a similar training approach for a Dependency Parser that builds the tree-structure by recursively creating the easiest arc in a non-directional manner. This model also integrates the tasks of learning the order of inference and training the parser in a single Perceptron. By “non-directional” they mean the removal of the constraint of scanning the sentence from left to right, which is typical of shift-reduce models. However this algorithm still builds the tree structures in a bottom-up fashion. This model has a O(n log n) decoding complexity and accuracy performance close to the O(n2) </context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In NAACL ’10: Proceedings of the 11th Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<date>1999</date>
<booktitle>Semiring parsing. Computational Linguistics,</booktitle>
<pages>25--573</pages>
<contexts>
<context position="24136" citStr="Goodman, 1999" startWordPosition="4150" endWordPosition="4151">ng and Chiang, 2007; Huang, 2008). This framework separates the MT decoding in two steps. In the first step only the context-free factors are considered. The output of the first step is a hypergraph called the contextfree-forest, which compactly represents an exponential number of synchronous-trees. The second step introduces contextual features by applying a process of state-splitting to the context-free-forest, rescoring with non-context-free factors, and efficiently pruning the search space. To efficiently compute CFF features we run the Inside-Outside algorithm with the (max, +) semiring (Goodman, 1999) over the context-freeforest. The result is a map that gives the maximum Inside and Outside scores for each node in the context-free forest. This map is used to get the value of the CFF features in constant time while running the forest rescoring step. 5 Experiments We implement our model on top of Cdec (Dyer et al., 2010). Cdec provides a standard implementation of the HMT decoder (Chiang, 2007) and MERT training (Och, 2003) that we use as baseline. We experiment on the NIST Chinese-English parallel corpus. The training corpus contains 239k sentence pairs with 6.9M Chinese words and 8.9M Engl</context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>Joshua Goodman. 1999. Semiring parsing. Computational Linguistics, 25:573–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In IWPT ’05: Proceedings of the 9th International Workshop on Parsing Technology,</booktitle>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="1783" citStr="Huang and Chiang, 2005" startWordPosition="276" endWordPosition="279"> = (t, h), where t is the sentence translated into the target language, and h is the hidden correspondence structure (Liang et al., 2006). In Hierarchical MT (HMT) (Chiang, 2005) the hidden correspondence structure is the synchronous-tree composed by instantiations of synchronous rules from the input grammar, G. Statistical models usually define f as: f(x) = arg maxy∈Y Score(x, y), where Score(x, y) is a function whose parameters can be learned with a specialized learning algorithm. In MT applications, it is not possible to enumerate all y ∈ Y. HMT decoding applies pruning (e.g. Cube Pruning (Huang and Chiang, 2005)), but even then HMT has higher complexity than Phrase Based MT (PbMT) (Koehn et al., 2003). On the other hand, HMT improves over PbMT by introducing the possibility of exploiting a more sophisticated reordering model not bounded by a window size, and producing translations with higher syntacticsemantic quality. In this paper, we present the Undirected Machine Translation (UMT) framework, which retains the advantages of HMT and allows the use of a greedy decoder whose complexity is lower than standard quadratic beamsearch PbMT. UMT’s fast decoding is made possible through even stronger pruning</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In IWPT ’05: Proceedings of the 9th International Workshop on Parsing Technology, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In ACL ’07: Proceedings of the 45th Conference of the Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="23541" citStr="Huang and Chiang, 2007" startWordPosition="4062" endWordPosition="4065">f r’s non-terminals, Xr,τ. Thus, the score of the interaction between r and the context structure attached to Xr,τ can be computed exactly, while the score of the structures attached to other r nonterminals (i.e. those in postconditions) cannot be computed since these branches are missing. Each of these postcondition nonterminals has an associated CFF feature, which is an upper bound on the score of its missing branch. More precisely, it is an upper bound on the context-free component of this score. This upper bound can be exactly and efficiently computed using the Forest Rescoring Framework (Huang and Chiang, 2007; Huang, 2008). This framework separates the MT decoding in two steps. In the first step only the context-free factors are considered. The output of the first step is a hypergraph called the contextfree-forest, which compactly represents an exponential number of synchronous-trees. The second step introduces contextual features by applying a process of state-splitting to the context-free-forest, rescoring with non-context-free factors, and efficiently pruning the search space. To efficiently compute CFF features we run the Inside-Outside algorithm with the (max, +) semiring (Goodman, 1999) over</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In ACL ’07: Proceedings of the 45th Conference of the Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest-based algorithms in natural language processing.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="23555" citStr="Huang, 2008" startWordPosition="4066" endWordPosition="4067">τ. Thus, the score of the interaction between r and the context structure attached to Xr,τ can be computed exactly, while the score of the structures attached to other r nonterminals (i.e. those in postconditions) cannot be computed since these branches are missing. Each of these postcondition nonterminals has an associated CFF feature, which is an upper bound on the score of its missing branch. More precisely, it is an upper bound on the context-free component of this score. This upper bound can be exactly and efficiently computed using the Forest Rescoring Framework (Huang and Chiang, 2007; Huang, 2008). This framework separates the MT decoding in two steps. In the first step only the context-free factors are considered. The output of the first step is a hypergraph called the contextfree-forest, which compactly represents an exponential number of synchronous-trees. The second step introduces contextual features by applying a process of state-splitting to the context-free-forest, rescoring with non-context-free factors, and efficiently pruning the search space. To efficiently compute CFF features we run the Inside-Outside algorithm with the (max, +) semiring (Goodman, 1999) over the context-f</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest-based algorithms in natural language processing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 4th Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1080" citStr="Koehn et al., 2003" startWordPosition="156" endWordPosition="159">xt. This allows the introduction of a new class of contextual features that are not constrained to condition only on the bottom-up context. The model builds translation-derivations efficiently in a greedy fashion. It is trained to learn to choose jointly the best action and the best inference order. Experiments show that the decoding time is halved and forestrescoring is 6 times faster, while reaching accuracy not significantly different from state of the art. 1 Introduction Machine Translation (MT) can be addressed as a structured prediction task (Brown et al., 1993; Yamada and Knight, 2001; Koehn et al., 2003). MT’s goal is to learn a mapping function, f, from an input sentence, x, into y = (t, h), where t is the sentence translated into the target language, and h is the hidden correspondence structure (Liang et al., 2006). In Hierarchical MT (HMT) (Chiang, 2005) the hidden correspondence structure is the synchronous-tree composed by instantiations of synchronous rules from the input grammar, G. Statistical models usually define f as: f(x) = arg maxy∈Y Score(x, y), where Score(x, y) is a function whose parameters can be learned with a specialized learning algorithm. In MT applications, it is not po</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL ’03: Proceedings of the 4th Conference of the North American Chapter of the Association for Computational Linguistics, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In WMT ’09: Proceedings of the 4th Workshop on Statistical Machine Translation,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="24882" citStr="Li et al., 2009" startWordPosition="4276" endWordPosition="4279">ee forest. This map is used to get the value of the CFF features in constant time while running the forest rescoring step. 5 Experiments We implement our model on top of Cdec (Dyer et al., 2010). Cdec provides a standard implementation of the HMT decoder (Chiang, 2007) and MERT training (Och, 2003) that we use as baseline. We experiment on the NIST Chinese-English parallel corpus. The training corpus contains 239k sentence pairs with 6.9M Chinese words and 8.9M English words. The test set contains 919 sentence pairs. The hierarchical translation grammar was extracted using the Joshua toolkit (Li et al., 2009) implementation of the suffix array rule extractor algorithm (Callison-Burch et al., 2005; Lopez, 2007). Table 1 reports the decoding time measures. HMT with beam1 is the fastest possible configuration for HMT, but it is 71.59% slower than UMT. This is because HMT b1 constructs O(n2) subtrees, many of which end up not being used in the final result, whereas UMT only constructs the rule instantiations that are required. HMT with beam30 is the fastest configuration that reaches state of the art accuracy, but increases the average time per sentence by an additional 131.36% when compared with UMT.</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In WMT ’09: Proceedings of the 4th Workshop on Statistical Machine Translation, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In COLINGACL ’06: Proceedings ofthe 21st International Conference on Computational Linguistics and the 44th Conference of the Association for Computational Linguistics,</booktitle>
<location>Sydney, Australia.</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In COLINGACL ’06: Proceedings ofthe 21st International Conference on Computational Linguistics and the 44th Conference of the Association for Computational Linguistics, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL ’07: Proceedings of the 2007Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="24985" citStr="Lopez, 2007" startWordPosition="4292" endWordPosition="4293">rescoring step. 5 Experiments We implement our model on top of Cdec (Dyer et al., 2010). Cdec provides a standard implementation of the HMT decoder (Chiang, 2007) and MERT training (Och, 2003) that we use as baseline. We experiment on the NIST Chinese-English parallel corpus. The training corpus contains 239k sentence pairs with 6.9M Chinese words and 8.9M English words. The test set contains 919 sentence pairs. The hierarchical translation grammar was extracted using the Joshua toolkit (Li et al., 2009) implementation of the suffix array rule extractor algorithm (Callison-Burch et al., 2005; Lopez, 2007). Table 1 reports the decoding time measures. HMT with beam1 is the fastest possible configuration for HMT, but it is 71.59% slower than UMT. This is because HMT b1 constructs O(n2) subtrees, many of which end up not being used in the final result, whereas UMT only constructs the rule instantiations that are required. HMT with beam30 is the fastest configuration that reaches state of the art accuracy, but increases the average time per sentence by an additional 131.36% when compared with UMT. The rescoring time is 15 Model sent. t. sent. t. var. resc. t. resc. t. var. UMT 135.2ms - 38.9 ms - H</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In EMNLP-CoNLL ’07: Proceedings of the 2007Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Mcdonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Conference of the Association for Computational Linguistics,</booktitle>
<location>Ann Arbor, MI, USA.</location>
<contexts>
<context position="29232" citStr="Mcdonald et al. (2005)" startWordPosition="5027" endWordPosition="5030">g compared to UMT, and 6.1 times slower at rescoring, while UMT loses 1.1 BLEU points in accuracy. But again the accuracy differences are not considered significant. We measured a p-value of 0.25, which is not significant at the 0.1 level. 6 Related Work Models sharing similar intuitions have been previously applied to other structure prediction tasks. For example, Nivre et al. (2006) presents a linear time syntactic dependency parser, which is constrained in a left-to-right decoding order. This model offers a different accuracy/complexity balance than the quadratic time graph-based parser of Mcdonald et al. (2005). Other approaches learning a model specifically for greedy decoding have been applied with suc16 cess to other less complex tasks. Shen et al. (2007) present the Guided Learning (GL) framework for bidirectional sequence classification. GL successfully combines the tasks of learning the order of inference and training the local classifier in a single Perceptron-like algorithm, reaching state of the art accuracy with complexity lower than the exhaustive counterpart (Collins, 2002). Goldberg and Elhadad (2010) present a similar training approach for a Dependency Parser that builds the tree-struc</context>
</contexts>
<marker>Mcdonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan Mcdonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In ACL ’05: Proceedings of the 43rd Conference of the Association for Computational Linguistics, Ann Arbor, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Stuart Russell</author>
</authors>
<title>Algorithms for inverse reinforcement learning.</title>
<date>2000</date>
<booktitle>In ICML ’00: Proceedings of the 17th International Conference on Machine Learning,</booktitle>
<location>Stanford University, CA, USA.</location>
<contexts>
<context position="13954" citStr="Ng and Russell, 2000" startWordPosition="2371" endWordPosition="2374">icy is defined as any map π : S —* A. Its value function is given by: σ V π(s0) = E γiR(si,π(si)) (1) i=0 where path(s0|π) - (s0, s1, · · · , sσ|π) is the sequence of states determined by following policy π starting at state s0. The Q-function is the total future reward of performing action a0 in state s0 and then following policy π: Qπ(s0, a0) = R(s0, a0) + γVπ(s1) (2) Standard RL algorithms search for a policy that maximizes the given reward. Because we are taking a discriminative approach to learn w, we formalize our optimization task similarly to an inverse reinforcement learning problem (Ng and Russell, 2000): we are given information about the optimal action sequence and we want to learn a discriminative reward function. As in other discriminative approaches, this 1S can be either finite or infinite. 2For simplicity we describe a deterministic process. To generalize to the stochastic process, replace the transition function with the transition probability: Psa(s′), s′ ∈ S. Algorithm 2 Discriminative RL 1: function Trainer (φ, T , D ) : w 2: repeat 3: s +—SampleState(S); 4: aˆ +— π,(s); 5: a′ +—SampleAction(As); 6: if QπW(s, ˆa) &lt; QπW(s, a′) in D then 7: w +— w + 4b,(s, a′) − 4b-(s, ˆa); 8: end if</context>
</contexts>
<marker>Ng, Russell, 2000</marker>
<rawString>Andrew Y. Ng and Stuart Russell. 2000. Algorithms for inverse reinforcement learning. In ICML ’00: Proceedings of the 17th International Conference on Machine Learning, Stanford University, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In LREC ’06: Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="28997" citStr="Nivre et al. (2006)" startWordPosition="4992" endWordPosition="4995">t 186k sentences were used to extract the grammar and train the two models. The final tests were performed on the remaining 4k sentence pairs. With this corpus we measured a similar speed gain. HMT b30 is 2.3 times slower at decoding compared to UMT, and 6.1 times slower at rescoring, while UMT loses 1.1 BLEU points in accuracy. But again the accuracy differences are not considered significant. We measured a p-value of 0.25, which is not significant at the 0.1 level. 6 Related Work Models sharing similar intuitions have been previously applied to other structure prediction tasks. For example, Nivre et al. (2006) presents a linear time syntactic dependency parser, which is constrained in a left-to-right decoding order. This model offers a different accuracy/complexity balance than the quadratic time graph-based parser of Mcdonald et al. (2005). Other approaches learning a model specifically for greedy decoding have been applied with suc16 cess to other less complex tasks. Shen et al. (2007) present the Guided Learning (GL) framework for bidirectional sequence classification. GL successfully combines the tasks of learning the order of inference and training the local classifier in a single Perceptron-l</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In LREC ’06: Proceedings of the 5th International Conference on Language Resources and Evaluation, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Conference of the Association for Computational Linguistics,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="24565" citStr="Och, 2003" startWordPosition="4227" endWordPosition="4228">on-context-free factors, and efficiently pruning the search space. To efficiently compute CFF features we run the Inside-Outside algorithm with the (max, +) semiring (Goodman, 1999) over the context-freeforest. The result is a map that gives the maximum Inside and Outside scores for each node in the context-free forest. This map is used to get the value of the CFF features in constant time while running the forest rescoring step. 5 Experiments We implement our model on top of Cdec (Dyer et al., 2010). Cdec provides a standard implementation of the HMT decoder (Chiang, 2007) and MERT training (Och, 2003) that we use as baseline. We experiment on the NIST Chinese-English parallel corpus. The training corpus contains 239k sentence pairs with 6.9M Chinese words and 8.9M English words. The test set contains 919 sentence pairs. The hierarchical translation grammar was extracted using the Joshua toolkit (Li et al., 2009) implementation of the suffix array rule extractor algorithm (Callison-Burch et al., 2005; Lopez, 2007). Table 1 reports the decoding time measures. HMT with beam1 is the fastest possible configuration for HMT, but it is 71.59% slower than UMT. This is because HMT b1 constructs O(n2</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Conference of the Association for Computational Linguistics, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Rosenblatt</author>
</authors>
<title>The Perceptron: A probabilistic model for information storage and organization in the brain.</title>
<date>1958</date>
<journal>Psychological Review,</journal>
<volume>65</volume>
<issue>6</issue>
<pages>408</pages>
<contexts>
<context position="5293" citStr="Rosenblatt, 1958" startWordPosition="831" endWordPosition="832"> apply DRL to learn the UMT scoring function’s parameters, using the BLEU score as the global loss function. DRL learns a weight vector for a linear classifier that discriminates between decisions based on which one leads to a complete translation-derivation with a better BLEU score. Promotions/demotions of translations are performed by applying a Perceptron-style update on the sequence of decisions that produced the translation, thereby training local decisions to optimize the global BLEU score of the final translation, while keeping the efficiency and simplicity of the Perceptron Algorithm (Rosenblatt, 1958; Collins, 2002). Our experiments show that UMT with DRL reduces decoding time by over half, and the time to rescore translations with the Language Model by 6 times, while reaching accuracy non-significantly different from the state of the art. 2 Undirected Machine Translation In this section, we present the UMT framework. For ease of presentation, and following synchronous-grammar based MT practice, we will henceforth restrict our focus to binary grammars (Zhang et al., 2006; Wang et al., 2007). A UMT decoder can be formulated as a function, f, that maps a source sentence, x ∈ X, into a struc</context>
</contexts>
<marker>Rosenblatt, 1958</marker>
<rawString>Frank Rosenblatt. 1958. The Perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6):386– 408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Giorgio Satta</author>
<author>Aravind Joshi</author>
</authors>
<title>Guided learning for bidirectional sequence classification.</title>
<date>2007</date>
<booktitle>In ACL ’07: Proceedings of the 45th Conference of the Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="20077" citStr="Shen et al., 2007" startWordPosition="3478" endWordPosition="3481">t the bottom of the rank still have a small probability to be selected. The if at line 6 tests if the translation produced by path(T(s, a′), πw) has higher BLEU score than the one produced by path(T (s, ˆa), πw). For the update statement at line 7 we use the Averaged Perceptron technique (Freund and Schapire, 1999). Algorithm 2 can be easily adapted to implement the efficient Averaged Perceptron updates (e.g. see Section 2.1.1 of (Daum´e III, 2006)). In preliminary experiments, we found that other more aggressive update technique, such as Passive-Aggressive (Crammer et al., 2006), Aggressive (Shen et al., 2007), or MIRA (Crammer and Singer, 2003), lead to worst accuracy. To see why this might be, consider that a MT decoder needs to learn to construct structures (t, h), while the training data specifies the gold translation t* but gives no information on the hidden-correspondence structure h. As discussed in (Liang et al., 2006), there are output structures that match the reference translation using a wrong internal structure (e.g. assuming wrong internal alignment). While in other cases the output translation can be a valid alternative translation but gets a low BLEU score because it differs from t*</context>
<context position="29382" citStr="Shen et al. (2007)" startWordPosition="5052" endWordPosition="5055">ignificant. We measured a p-value of 0.25, which is not significant at the 0.1 level. 6 Related Work Models sharing similar intuitions have been previously applied to other structure prediction tasks. For example, Nivre et al. (2006) presents a linear time syntactic dependency parser, which is constrained in a left-to-right decoding order. This model offers a different accuracy/complexity balance than the quadratic time graph-based parser of Mcdonald et al. (2005). Other approaches learning a model specifically for greedy decoding have been applied with suc16 cess to other less complex tasks. Shen et al. (2007) present the Guided Learning (GL) framework for bidirectional sequence classification. GL successfully combines the tasks of learning the order of inference and training the local classifier in a single Perceptron-like algorithm, reaching state of the art accuracy with complexity lower than the exhaustive counterpart (Collins, 2002). Goldberg and Elhadad (2010) present a similar training approach for a Dependency Parser that builds the tree-structure by recursively creating the easiest arc in a non-directional manner. This model also integrates the tasks of learning the order of inference and </context>
</contexts>
<marker>Shen, Satta, Joshi, 2007</marker>
<rawString>Libin Shen, Giorgio Satta, and Aravind Joshi. 2007. Guided learning for bidirectional sequence classification. In ACL ’07: Proceedings of the 45th Conference of the Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Binarizing syntax trees to improve syntax-based machine translation accuracy.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL ’07: Proceedings of the 2007Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5793" citStr="Wang et al., 2007" startWordPosition="909" endWordPosition="912">re of the final translation, while keeping the efficiency and simplicity of the Perceptron Algorithm (Rosenblatt, 1958; Collins, 2002). Our experiments show that UMT with DRL reduces decoding time by over half, and the time to rescore translations with the Language Model by 6 times, while reaching accuracy non-significantly different from the state of the art. 2 Undirected Machine Translation In this section, we present the UMT framework. For ease of presentation, and following synchronous-grammar based MT practice, we will henceforth restrict our focus to binary grammars (Zhang et al., 2006; Wang et al., 2007). A UMT decoder can be formulated as a function, f, that maps a source sentence, x ∈ X, into a structure defined by y = (t, h) ∈ Y, where t is the translation in the target language, and h is the synchronous tree structure generating the input sentence on the source side and its translation on the target side. Synchronous-trees are composed of instantiations of synchronous-rules, r, from a grammar, G. A UMT decoder builds synchronous-trees, h, by recursively expanding partial synchronous-trees, τ. τ includes a partial translation. Each τ is required to be a connected sub-graph of some synchron</context>
</contexts>
<marker>Wang, Knight, Marcu, 2007</marker>
<rawString>Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Binarizing syntax trees to improve syntax-based machine translation accuracy. In EMNLP-CoNLL ’07: Proceedings of the 2007Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In ACL ’01: Proceedings of the 39th Conference of the Association for Computational Linguistics,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="1059" citStr="Yamada and Knight, 2001" startWordPosition="151" endWordPosition="155">ure and surrounding context. This allows the introduction of a new class of contextual features that are not constrained to condition only on the bottom-up context. The model builds translation-derivations efficiently in a greedy fashion. It is trained to learn to choose jointly the best action and the best inference order. Experiments show that the decoding time is halved and forestrescoring is 6 times faster, while reaching accuracy not significantly different from state of the art. 1 Introduction Machine Translation (MT) can be addressed as a structured prediction task (Brown et al., 1993; Yamada and Knight, 2001; Koehn et al., 2003). MT’s goal is to learn a mapping function, f, from an input sentence, x, into y = (t, h), where t is the sentence translated into the target language, and h is the hidden correspondence structure (Liang et al., 2006). In Hierarchical MT (HMT) (Chiang, 2005) the hidden correspondence structure is the synchronous-tree composed by instantiations of synchronous rules from the input grammar, G. Statistical models usually define f as: f(x) = arg maxy∈Y Score(x, y), where Score(x, y) is a function whose parameters can be learned with a specialized learning algorithm. In MT appli</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In ACL ’01: Proceedings of the 39th Conference of the Association for Computational Linguistics, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In NAACL ’06: Proceedings ofthe 7th Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>New York, New York.</location>
<contexts>
<context position="5773" citStr="Zhang et al., 2006" startWordPosition="905" endWordPosition="908"> the global BLEU score of the final translation, while keeping the efficiency and simplicity of the Perceptron Algorithm (Rosenblatt, 1958; Collins, 2002). Our experiments show that UMT with DRL reduces decoding time by over half, and the time to rescore translations with the Language Model by 6 times, while reaching accuracy non-significantly different from the state of the art. 2 Undirected Machine Translation In this section, we present the UMT framework. For ease of presentation, and following synchronous-grammar based MT practice, we will henceforth restrict our focus to binary grammars (Zhang et al., 2006; Wang et al., 2007). A UMT decoder can be formulated as a function, f, that maps a source sentence, x ∈ X, into a structure defined by y = (t, h) ∈ Y, where t is the translation in the target language, and h is the synchronous tree structure generating the input sentence on the source side and its translation on the target side. Synchronous-trees are composed of instantiations of synchronous-rules, r, from a grammar, G. A UMT decoder builds synchronous-trees, h, by recursively expanding partial synchronous-trees, τ. τ includes a partial translation. Each τ is required to be a connected sub-gr</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In NAACL ’06: Proceedings ofthe 7th Conference of the North American Chapter of the Association for Computational Linguistics, New York, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>