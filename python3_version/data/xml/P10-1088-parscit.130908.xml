<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998017">
Bucking the Trend: Large-Scale Cost-Focused Active Learning for
Statistical Machine Translation
</title>
<author confidence="0.984009">
Michael Bloodgood
</author>
<affiliation confidence="0.930790333333333">
Human Language Technology
Center of Excellence
Johns Hopkins University
</affiliation>
<address confidence="0.954528">
Baltimore, MD 21211
</address>
<email confidence="0.999492">
bloodgood@jhu.edu
</email>
<sectionHeader confidence="0.993913" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996475">
We explore how to improve machine trans-
lation systems by adding more translation
data in situations where we already have
substantial resources. The main challenge
is how to buck the trend of diminishing re-
turns that is commonly encountered. We
present an active learning-style data solic-
itation algorithm to meet this challenge.
We test it, gathering annotations via Ama-
zon Mechanical Turk, and find that we get
an order of magnitude increase in perfor-
mance rates of improvement.
</bodyText>
<sectionHeader confidence="0.998737" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998646">
Figure 1 shows the learning curves for two state of
the art statistical machine translation (SMT) sys-
tems for Urdu-English translation. Observe how
the learning curves rise rapidly at first but then a
trend of diminishing returns occurs: put simply,
the curves flatten.
This paper investigates whether we can buck the
trend of diminishing returns, and if so, how we can
do it effectively. Active learning (AL) has been ap-
plied to SMT recently (Haffari et al., 2009; Haffari
and Sarkar, 2009) but they were interested in start-
ing with a tiny seed set of data, and they stopped
their investigations after only adding a relatively
tiny amount of data as depicted in Figure 1.
In contrast, we are interested in applying AL
when a large amount of data already exists as is
the case for many important lanuage pairs. We de-
velop an AL algorithm that focuses on keeping an-
notation costs (measured by time in seconds) low.
It succeeds in doing this by only soliciting trans-
lations for parts of sentences. We show that this
gets a savings in human annotation time above and
beyond what the reduction in # words annotated
would have indicated by a factor of about three
and speculate as to why.
</bodyText>
<note confidence="0.6324264">
Chris Callison-Burch
Center for Language and
Speech Processing
Johns Hopkins University
Baltimore, MD 21211
</note>
<email confidence="0.965184">
ccb@cs.jhu.edu
</email>
<bodyText confidence="0.3533225">
JSyntax and JHier Learning Curves on the LDC
Urdu−English Language Pack (BLEU vs Sentences)
</bodyText>
<figureCaption confidence="0.995316">
Figure 1: Syntax-based and Hierarchical Phrase-
</figureCaption>
<bodyText confidence="0.968842478260869">
Based MT systems’ learning curves on the LDC
Urdu-English language pack. The x-axis measures
the number of sentence pairs in the training data.
The y-axis measures BLEU score. Note the di-
minishing returns as more data is added. Also
note how relatively early on in the process pre-
vious studies were terminated. In contrast, the
focus of our main experiments doesn’t even be-
gin until much higher performance has already
been achieved with a period of diminishing returns
firmly established.
We conduct experiments for Urdu-English
translation, gathering annotations via Amazon
Mechanical Turk (MTurk) and show that we can
indeed buck the trend of diminishing returns,
achieving an order of magnitude increase in the
rate of improvement in performance.
Section 2 discusses related work; Section 3
discusses preliminary experiments that show the
guiding principles behind the algorithm we use;
Section 4 explains our method for soliciting new
translation data; Section 5 presents our main re-
sults; and Section 6 concludes.
</bodyText>
<figure confidence="0.824881842105263">
30
25
20
where we begin our
main investigations into
bucking the trend
of diminishing returns
10
as far as previous
AL for SMT research
studies were conducted
5
00 2 4 6 8 10
x 104
jHier
jSyntax
Number of Sentences in Training Data
BLEU Score
15
</figure>
<page confidence="0.987007">
854
</page>
<note confidence="0.969491">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 854–864,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.99742" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999611640000001">
Active learning has been shown to be effective
for improving NLP systems and reducing anno-
tation burdens for a number of NLP tasks (see,
e.g., (Hwa, 2000; Sassano, 2002; Bloodgood
and Vijay-Shanker, 2008; Bloodgood and Vijay-
Shanker, 2009b; Mairesse et al., 2010; Vickrey et
al., 2010)). The current paper is most highly re-
lated to previous work falling into three main ar-
eas: use of AL when large corpora already exist;
cost-focused AL; and AL for SMT.
In a sense, the work of Banko and Brill (2001)
is closely related to ours. Though their focus is
mainly on investigating the performance of learn-
ing methods on giant corpora many orders of mag-
nitude larger than previously used, they do lay out
how AL might be useful to apply to acquire data
to augment a large set cheaply because they rec-
ognize the problem of diminishing returns that we
discussed in Section 1.
The second area of work that is related to ours is
previous work on AL that is cost-conscious. The
vast majority of AL research has not focused on
accurate cost accounting and a typical assumption
is that each annotatable has equal annotation cost.
An early exception in the AL for NLP field was
the work of Hwa (2000), which makes a point of
using # of brackets to measure cost for a syntac-
tic analysis task instead of using # of sentences.
Another relatively early work in our field along
these lines was the work of Ngai and Yarowsky
(2000), which measured actual times of annota-
tion to compare the efficacy of rule writing ver-
sus annotation with AL for the task of BaseNP
chunking. Osborne and Baldridge (2004) argued
for the use of discriminant cost over unit cost for
the task of Head Phrase Structure Grammar parse
selection. King et al. (2004) design a robot that
tests gene functions. The robot chooses which
experiments to conduct by using AL and takes
monetary costs (in pounds sterling) into account
during AL selection and evaluation. Unlike our
situation for SMT, their costs are all known be-
forehand because they are simply the cost of ma-
terials to conduct the experiments, which are al-
ready known to the robot. Hachey et al. (2005)
showed that selectively sampled examples for an
NER task took longer to annotate and had lower
inter-annotator agreement. This work is related to
ours because it shows that how examples are se-
lected can impact the cost of annotation, an idea
we turn around to use for our advantage when de-
veloping our data selection algorithm. Haertel et
al. (2008) emphasize measuring costs carefully for
AL for POS tagging. They develop a model based
on a user study that can estimate the time required
for POS annotating. Kapoor et al. (2007) assign
costs for AL based on message length for a voice-
mail classification task. In contrast, we show for
SMT that annotation times do not scale according
to length in words and we show our method can
achieve a speedup in annotation time above and
beyond what the reduction in words would indi-
cate. Tomanek and Hahn (2009) measure cost by #
of tokens for an NER task. Their AL method only
solicits labels for parts of sentences in the interest
of reducing annotation effort. Along these lines,
our method is similar in the respect that we also
will only solicit annotation for parts of sentences,
though we prefer to measure cost with time and
we show that time doesn’t track with token length
for SMT.
Haffari et al. (2009), Haffari and Sarkar (2009),
and Ambati et al. (2010) investigate AL for SMT.
There are two major differences between our work
and this previous work. One is that our intended
use cases are very different. They deal with the
more traditional AL setting of starting from an ex-
tremely small set of seed data. Also, by SMT stan-
dards, they only add a very tiny amount of data
during AL. All their simulations top out at 10,000
sentences of labeled data and the models learned
have relatively low translation quality compared to
the state of the art.
On the other hand, in the current paper, we
demonstrate how to apply AL in situations where
we already have large corpora. Our goal is to buck
the trend of diminishing returns and use AL to
add data to build some of the highest-performing
MT systems in the world while keeping annota-
tion costs low. See Figure 1 from Section 1, which
contrasts where (Haffari et al., 2009; Haffari and
Sarkar, 2009) stop their investigations with where
we begin our studies.
The other major difference is that (Haffari et al.,
2009; Haffari and Sarkar, 2009) measure annota-
tion cost by # of sentences. In contrast, we bring
to light some potential drawbacks of this practice,
showing it can lead to different conclusions than
if other annotation cost metrics are used, such as
time and money, which are the metrics that we use.
</bodyText>
<page confidence="0.999224">
855
</page>
<sectionHeader confidence="0.987889" genericHeader="method">
3 Simulation Experiments
</sectionHeader>
<bodyText confidence="0.999811655172414">
Here we report on results of simulation experi-
ments that help to illustrate and motivate the de-
sign decisions of the algorithm we present in Sec-
tion 4. We use the Urdu-English language pack1
from the Linguistic Data Consortium (LDC),
which contains Pz� 88000 Urdu-English sentence
translation pairs, amounting to Pz� 1.7 million Urdu
words translated into English. All experiments in
this paper evaluate on a genre-balanced split of the
NIST2008 Urdu-English test set. In addition, the
language pack contains an Urdu-English dictio-
nary consisting of Pz� 114000 entries. In all the ex-
periments, we use the dictionary at every iteration
of training. This will make it harder for us to show
our methods providing substantial gains since the
dictionary will provide a higher base performance
to begin with. However, it would be artificial to
ignore dictionary resources when they exist.
We experiment with two translation models: hi-
erarchical phrase-based translation (Chiang, 2007)
and syntax augmented translation (Zollmann and
Venugopal, 2006), both of which are implemented
in the Joshua decoder (Li et al., 2009). We here-
after refer to these systems as jHier and jSyntax,
respectively.
We will now present results of experiments with
different methods for growing MT training data.
The results are organized into three areas of inves-
tigations:
</bodyText>
<listItem confidence="0.998529">
1. annotation costs;
2. managing uncertainty; and
3. how to automatically detect when to stop so-
liciting annotations from a pool of data.
</listItem>
<subsectionHeader confidence="0.999525">
3.1 Annotation Costs
</subsectionHeader>
<bodyText confidence="0.99976675">
We begin our cost investigations with four sim-
ple methods for growing MT training data: ran-
dom, shortest, longest, and VocabGrowth sen-
tence selection. The first three methods are self-
explanatory. VocabGrowth (hereafter VG) selec-
tion is modeled after the best methods from previ-
ous work (Haffari et al., 2009; Haffari and Sarkar,
2009), which are based on preferring sentences
that contain phrases that occur frequently in un-
labeled data and infrequently in the so-far labeled
data. Our VG method selects sentences for transla-
tion that contain n-grams (for n in 11,2,3,4}) that
</bodyText>
<footnote confidence="0.5482">
1LDC Catalog No.: LDC2006E110.
</footnote>
<sectionHeader confidence="0.402888" genericHeader="method">
Init:
</sectionHeader>
<bodyText confidence="0.917149666666667">
Go through all available training
data (labeled and unlabeled)
and obtain frequency counts for
every n-gram (n in 11, 2, 3, 4})
that occurs.
sortedNGrams +— Sort n-grams by
frequency in descending order.
Loop
until stopping criterion (see Section 3.3) is met
</bodyText>
<listItem confidence="0.994795625">
1. trigger +— Go down sortedNGrams list
and find the first n-gram that isn’t covered in
the so far labeled training data.
2. selected5entence +— Find a sentence
that contains trigger.
3. Remove selected5entence from unlabeled
data and add it to labeled training data.
End Loop
</listItem>
<figureCaption confidence="0.997514">
Figure 2: The VG sentence selection algorithm
</figureCaption>
<bodyText confidence="0.978933">
do not occur at all in our so-far labeled data. We
call an n-gram “covered” if it occurs at least once
in our so-far labeled data. VG has a preference
for covering frequent n-grams before covering in-
frequent n-grams. The VG method is depicted in
Figure 2.
Figure 3 shows the learning curves for both
jHier and jSyntax for VG selection and random
selection. The y-axis measures BLEU score (Pap-
ineni et al., 2002),which is a fast automatic way of
measuring translation quality that has been shown
to correlate with human judgments and is perhaps
the most widely used metric in the MT commu-
nity. The x-axis measures the number of sen-
tence translation pairs in the training data. The VG
curves are cut off at the point at which the stopping
criterion in Section 3.3 is met. From Figure 3 it
might appear that VG selection is better than ran-
dom selection, achieving higher-performing sys-
tems with fewer translations in the labeled data.
However, it is important to take care when mea-
suring annotation costs (especially for relatively
complicated tasks such as translation). Figure 4
shows the learning curves for the same systems
and selection methods as in Figure 3 but now the
x-axis measures the number of foreign words in
the training data. The difference between VG and
random selection now appears smaller.
For an extreme case, to illustrate the ramifica-
</bodyText>
<page confidence="0.9884">
856
</page>
<figure confidence="0.878401685714286">
jHier and jSyntax: VG vs Random selection (BLEU vs FWords)
30
25
20
jHier: random selection
jHier: VG selection
jSyntax: random selection
jSyntax: VG selection
5
00 0.5 1 1.5 2
x 106
Number of Foreign Words in Training Data
BLEU Score
15
10
jHier and jSyntax: VG vs Random selection (BLEU vs Sents)
30
25
20
where previous AL for
SMT research stopped
their experiments
where we will
start our main experiments
5
jHier: random selection
jHier: VG selection
jSyntax: random selection
jSyntax: VG selection
0
0 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000
BLEU Score
15
10
Number of Sentence Pairs in the Training Data
</figure>
<figureCaption confidence="0.998935">
Figure 3: Random vs VG selection. The x-axis
measures the number of sentence pairs in the train-
ing data. The y-axis measures BLEU score.
Figure 4: Random vs VG selection. The x-axis
measures the number of foreign words in the train-
ing data. The y-axis measures BLEU score.
</figureCaption>
<bodyText confidence="0.9996195">
tions of measuring translation annotation cost by #
of sentences versus # of words, consider Figures 5
and 6. They both show the same three selection
methods but Figure 5 measures the x-axis by # of
sentences and Figure 6 measures by # of words. In
Figure 5, one would conclude that shortest is a far
inferior selection method to longest but in Figure 6
one would conclude the opposite.
Measuring annotation time and cost in dol-
lars are probably the most important measures
of annotation cost. We can’t measure these for
the simulated experiments but we will use time
(in seconds) and money (in US dollars) as cost
measures in Section 5, which discusses our non-
simulated AL experiments. If # sentences or #
words track these other more relevant costs in pre-
dictable known relationships, then it would suffice
to measure # sentences or # words instead. But it’s
clear that different sentences can have very differ-
ent annotation time requirements according to how
long and complicated they are so we will not use
# sentences as an annotation cost any more. It is
not as clear how # words tracks with annotation
time. In Section 5 we will present evidence show-
ing that time per word can vary considerably and
also show a method for soliciting annotations that
reduces time per word by nearly a factor of three.
As it is prudent to evaluate using accurate cost
accounting, so it is also prudent to develop new
AL algorithms that take costs carefully into ac-
count. Hence, reducing annotation time burdens
instead of the # of sentences translated (which
might be quite a different thing) will be a corner-
stone of the algorithm we describe in Section 4.
</bodyText>
<subsectionHeader confidence="0.999909">
3.2 Managing Uncertainty
</subsectionHeader>
<bodyText confidence="0.999991576923077">
One of the most successful of all AL methods de-
veloped to date is uncertainty sampling and it has
been applied successfully many times (e.g.,(Lewis
and Gale, 1994; Tong and Koller, 2002)). The
intuition is clear: much can be learned (poten-
tially) if there is great uncertainty. However, with
MT being a relatively complicated task (compared
with binary classification, for example), it might
be the case that the uncertainty approach has to
be re-considered. If words have never occurred
in the training data, then uncertainty can be ex-
pected to be high. But we are concerned that if a
sentence is translated for which (almost) no words
have been seen in training yet, though uncertainty
will be high (which is usually considered good for
AL), the word alignments may be incorrect and
then subsequent learning from that translation pair
will be severely hampered.
We tested this hypothesis and Figure 7 shows
empirical evidence that it is true. Along with VG,
two other selection methods’ learning curves are
charted in Figure 7: mostNew, which prefers to
select those sentences which have the largest # of
unseen words in them; and moderateNew, which
aims to prefer sentences that have a moderate #
of unseen words, preferring sentences with Pz� ten
</bodyText>
<page confidence="0.976497">
857
</page>
<figure confidence="0.950225777777778">
jHiero: Random, Shortest, and Longest selection
25
20
15
10
5
00 2 4 6 8 10
x
104
</figure>
<figureCaption confidence="0.9714705">
Figure 5: Random vs Shortest vs Longest selec-
tion. The x-axis measures the number of sentence
pairs in the training data. The y-axis measures
BLEU score.
</figureCaption>
<note confidence="0.564983">
jHiero: Longest, Shortest, and Random Selection
</note>
<figureCaption confidence="0.99158675">
Figure 6: Random vs Shortest vs Longest selec-
tion. The x-axis measures the number of foreign
words in the training data. The y-axis measures
BLEU score.
</figureCaption>
<figure confidence="0.995662529411765">
25
20
random
shortest
longest
5
00 0.5 1 1.5 2
6
Number of Foreign Words in Training Data 10
BLEU Score
15
10
BLEU Score
random
shortest
longest
Number of Sentences in Training Data
</figure>
<bodyText confidence="0.9996671">
unknown words in them. One can see that most-
New underperforms VG. This could have been due
to VG’s frequency component, which mostNew
doesn’t have. But moderateNew also doesn’t have
a frequency preference so it is likely that mostNew
winds up overwhelming the MT training system,
word alignments are incorrect, and less is learned
as a result. In light of this, the algorithm we de-
velop in Section 4 will be designed to avoid this
word alignment danger.
</bodyText>
<subsectionHeader confidence="0.999027">
3.3 Automatic Stopping
</subsectionHeader>
<bodyText confidence="0.999889619047619">
The problem of automatically detecting when to
stop AL is a substantial one, discussed at length
in the literature (e.g., (Bloodgood and Vijay-
Shanker, 2009a; Schohn and Cohn, 2000; Vla-
chos, 2008)). In our simulation, we stop VG once
all n-grams (n in {1,2,3,4}) have been covered.
Though simple, this stopping criterion seems to
work well as can be seen by where the curve for
VG is cut off in Figures 3 and 4. It stops af-
ter 1,293,093 words have been translated, with
jHier’s BLEU=21.92 and jSyntax’s BLEU=26.10
at the stopping point. The ending BLEU scores
(with the full corpus annotated) are 21.87 and
26.01 for jHier and jSyntax, respectively. So
our stopping criterion saves 22.3% of the anno-
tation (in terms of words) and actually achieves
slightly higher BLEU scores than if all the data
were used. Note: this ”less is more” phenomenon
has been commonly observed in AL settings (e.g.,
(Bloodgood and Vijay-Shanker, 2009a; Schohn
and Cohn, 2000)).
</bodyText>
<sectionHeader confidence="0.99105" genericHeader="method">
4 Highlighted N-Gram Method
</sectionHeader>
<bodyText confidence="0.99996444">
In this section we describe a method for solicit-
ing human translations that we have applied suc-
cessfully to improving translation quality in real
(not simulated) conditions. We call the method the
Highlighted N-Gram method, or HNG, for short.
HNG solicits translations only for trigger n-grams
and not for entire sentences. We provide senten-
tial context, highlight the trigger n-gram that we
want translated, and ask for a translation of just the
highlighted trigger n-gram. HNG asks for transla-
tions for triggers in the same order that the triggers
are encountered by the algorithm in Figure 2. A
screenshot of our interface is depicted in Figure 8.
The same stopping criterion is used as was used in
the last section. When the stopping criterion be-
comes true, it is time to tap a new unlabeled pool
of foreign text, if available.
Our motivations for soliciting translations for
only parts of sentences are twofold, corresponding
to two possible cases. Case one is that a translation
model learned from the so-far labeled data will be
able to translate most of the non-trigger words in
the sentence correctly. Thus, by asking a human
to translate only the trigger words, we avoid wast-
ing human translation effort. (We will show in
</bodyText>
<page confidence="0.984097">
858
</page>
<figure confidence="0.499743">
jHiero: VG vs mostNew vs moderateNew
</figure>
<figureCaption confidence="0.92436825">
Figure 7: VG vs MostNew vs ModerateNew se-
lection. The x-axis measures the number of sen-
tence pairs in the training data. The y-axis mea-
sures BLEU score.
</figureCaption>
<table confidence="0.990352111111111">
$+ 86:#5 &amp;&amp;quot; %$,&apos;C )&apos; &amp;&apos; &apos; 54.3 &apos; I:G *&lt; &amp;+ )0H/ $+ *&apos; )#E )+C )K ):&apos; *@e *&lt;.
)K ):&apos; Z:(&amp;quot; f !-&amp;quot; ! 3i&amp;quot; $&apos; 1 ! C. ! Xh. )&apos; IU IP
&amp;+ &amp;&apos;
9) &apos; 8! $76 ),(&apos; )&apos; 1!
)0H/ &apos;
.
)0/.-,+ *&apos; )(&apos; &amp;&amp;quot; %$#&amp;quot;!
A/ 3@?/&gt; $&apos; =&lt;; +* ) (&amp; $&apos;&amp;$% $#&amp;quot;! ! -:9 8765 432 1
&amp;+ )0+ )D )&amp;quot;3&apos; I?HG FE C)D C!B+ .&gt;.
</table>
<figureCaption confidence="0.9844705">
Figure 8: Screenshot of the interface we used for
soliciting translations for triggers.
</figureCaption>
<bodyText confidence="0.999831444444445">
the next section that we even get a much larger
speedup above and beyond what the reduction in
number of translated words would give us.) Case
two is that a translation model learned from the so-
far labeled data will (in addition to not being able
to translate the trigger words correctly) also not be
able to translate most of the non-trigger words cor-
rectly. One might think then that this would be a
great sentence to have translated because the ma-
chine can potentially learn a lot from the transla-
tion. Indeed, one of the overarching themes of AL
research is to query examples where uncertainty is
greatest. But, as we showed evidence for in the
last section, for the case of SMT, too much un-
certainty could in a sense overwhelm the machine
and it might be better to provide new training data
in a more gradual manner. A sentence with large
#s of unseen words is likely to get word-aligned
incorrectly and then learning from that translation
could be hampered. By asking for a translation
of only the trigger words, we expect to be able to
circumvent this problem in large part.
The next section presents the results of experi-
ments that show that the HNG algorithm is indeed
practically effective. Also, the next section ana-
lyzes results regarding various aspects of HNG’s
behavior in more depth.
</bodyText>
<sectionHeader confidence="0.996359" genericHeader="evaluation">
5 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.948124">
5.1 General Setup
</subsectionHeader>
<bodyText confidence="0.9998801">
We set out to see whether we could use the HNG
method to achieve translation quality improve-
ments by gathering additional translations to add
to the training data of the entire LDC language
pack, including its dictionary. In particular, we
wanted to see if we could achieve translation im-
provements on top of already state-of-the-art per-
forming systems trained already on the entire LDC
corpus. Note that at the outset this is an ambitious
endeavor (recall the flattening of the curves in Fig-
ure 1 from Section 1).
Snow et al. (2008) explored the use of the Ama-
zon Mechanical Turk (MTurk) web service for
gathering annotations for a variety of natural lan-
guage processing tasks and recently MTurk has
been shown to be a quick, cost-effective way to
gather Urdu-English translations (Bloodgood and
Callison-Burch, 2010). We used the MTurk web
service to gather our annotations. Specifically, we
first crawled a large set of BBC articles on the in-
ternet in Urdu and used this as our unlabeled pool
from which to gather annotations. We applied the
HNG method from Section 4 to determine what to
post on MTurk for workers to translate.2 We gath-
ered 20,580 n-gram translations for which we paid
$0.01 USD per translation, giving us a total cost
of $205.80 USD. We also gathered 1632 randomly
chosen Urdu sentence translations as a control set,
for which we paid $0.10 USD per sentence trans-
lation.3
</bodyText>
<footnote confidence="0.965923857142857">
2For practical reasons we restricted ourselves to not con-
sidering sentences that were longer than 60 Urdu words, how-
ever.
3The prices we paid were not market-driven. We just
chose prices we thought were reasonable. In hindsight, given
how much quicker the phrase translations are for people we
could have had a greater disparity in price.
</footnote>
<figure confidence="0.990568717391304">
25
20
VG
mostNew
moderateNew
5
00 0.5 1 1.5 2
6
Number of Foreign Words in Training Datax 10
BLEU Score
15
10
1
XW )VU
&amp;P)#O !
$! K$&amp;quot; &amp;J&amp;quot; &amp;J&amp;quot;
&amp;&apos; -6QG
N3?:&apos; 8M
8,9 ! &amp;TS &amp;TS &amp;/ *@7R
8&apos;
\3R 8&apos; %$[-&amp;quot; ! Z:G 3HM
C. ! 432
8&apos; 3&lt;.
- )*&amp;quot;,
&amp;quot;.0/ (.!
)YE 8#D ! 432
&amp;&lt;&gt; 8P )#&apos;&gt; &amp;E3&apos; ])#5 =T )#5 .
S :
*&apos; Z:+ &amp;+C
b.-,5 &amp;&amp;quot;
*&lt; &amp;/
)&lt;$! #Hd
8&apos;
^
_
XG$HU a&gt;$`/
_S *9 c$&amp;quot;
*(&apos; *&lt;
f
g&lt;
3e
)@(e ? &amp;+.
:
2.1#
2.1#
Z:(&amp;quot;
</figure>
<page confidence="0.988303">
859
</page>
<subsectionHeader confidence="0.970625">
5.2 Accounting for Translation Time
</subsectionHeader>
<bodyText confidence="0.999853393939394">
MTurk returns with each assignment the “Work-
TimeInSeconds.” This is the amount of time be-
tween when a worker accepts an assignment and
when the worker submits the completed assign-
ment. We use this value to estimate annotation
times.4
Figure 9 shows HNG collection versus random
collection from MTurk. The x-axis measures the
number of seconds of annotation time. Note that
HNG is more effective. A result that may be par-
ticularly interesting is that HNG results in a time
speedup by more than just the reduction in trans-
lated words would indicate. The average time to
translate a word of Urdu with the sentence post-
ings to MTurk was 32.92 seconds. The average
time to translate a word with the HNG postings to
MTurk was 11.98 seconds. This is nearly three
times faster. Figure 10 shows the distribution of
speeds (in seconds per word) for HNG postings
versus complete sentence postings. Note that the
HNG postings consistently result in faster transla-
tion speeds than the sentence postings5.
We hypothesize that this speedup comes about
because when translating a full sentence, there’s
the time required to examine each word and trans-
late them in some sense (even if not one-to-one)
and then there is an extra significant overhead time
to put it all together and synthesize into a larger
sentence translation. The factor of three speedup
is evidence that this overhead is significant effort
compared to just quickly translating short n-grams
from a sentence. This speedup is an additional
benefit of the HNG approach.
</bodyText>
<subsectionHeader confidence="0.999476">
5.3 Bucking the Trend
</subsectionHeader>
<bodyText confidence="0.999535">
We gathered translations for Pz� 54,500 Urdu words
via the use of HNG on MTurk. This is a rela-
tively small amount, Pz� 3% of the LDC corpus.
Figure 11 shows the performance when we add
this training data to the LDC corpus. The rect-
</bodyText>
<footnote confidence="0.995669461538462">
4It’s imperfect because of network delays and if a person
is multitasking or pausing between their accept and submit
times. Nonetheless, the times ought to be better estimates as
they are taken over larger samples.
5The average speed for the HNG postings seems to be
slower than the histogram indicates. This is because there
were a few extremely slow outlier speeds for a handful of
HNG postings. These are almost certainly not cases when the
turker is working continuously on the task and so the average
speed we computed for the HNG postings might be slower
than the actual speed and hence the true speedup may even
be faster than indicated by the difference between the aver-
age speeds we reported.
</footnote>
<note confidence="0.835721">
jHier: HNG Collection vs Random Collection of
Annotations from MTurk
</note>
<figureCaption confidence="0.839764666666667">
Figure 9: HNG vs Random collection of new data
via MTurk. y-axis measures BLEU. x-axis mea-
sures annotation time in seconds.
</figureCaption>
<bodyText confidence="0.999909777777778">
angle around the last 700,000 words of the LDC
data is wide and short (it has a height of 0.9 BLEU
points and a width of 700,000 words) but the rect-
angle around the newly added translations is nar-
row and tall (a height of 1 BLEU point and a
width of 54,500 words). Visually, it appears we
are succeeding in bucking the trend of diminish-
ing returns. We further confirmed this by running
a least-squares linear regression on the points of
the last 700,000 words annotated in the LDC data
and also for the points in the new data that we ac-
quired via MTurk for $205.80 USD. We find that
the slope fit to our new data is 6.6245E-06 BLEU
points per Urdu word, or 6.6245 BLEU points for
a million Urdu words. The slope fit to the LDC
data is only 7.4957E-07 BLEU points per word,
or only 0.74957 BLEU points for a million words.
This is already an order of magnitude difference
that would make the difference between it being
worth adding more data and not being worth it;
and this is leaving aside the added time speedup
that our method enjoys.
Still, we wondered why we could not have
raised BLEU scores even faster. The main hur-
dle seems to be one of coverage. Of the 20,580 n-
grams we collected, only 571 (i.e., 2.77%) of them
ever even occur in the test set.
</bodyText>
<subsectionHeader confidence="0.985082">
5.4 Beyond BLEU Scores
</subsectionHeader>
<bodyText confidence="0.9587015">
BLEU is an imperfect metric (Callison-Burch et
al., 2006). One reason is that it rates all ngram
</bodyText>
<figure confidence="0.938174333333333">
22
21.8
21.60 1 2 3 4 5 6
x 105
random
HNG
Number of Seconds of Annotation Time
BLEU Score
22.2
22.8
22.6
22.4
</figure>
<page confidence="0.985789">
860
</page>
<bodyText confidence="0.954156657142857">
Histogram showing the distribution of translation speeds
(in seconds per foreign word) when translations
are collected via n−grams versus via complete sentences
Time (in seconds) per foreign word translated
Figure 10: Distribution of translation speeds (in
seconds per word) for HNG postings versus com-
plete sentence postings. The y-axis measures rel-
ative frequency. The x-axis measures translation
speed in seconds per word (so farther to the left is
faster).
mismatches equally although some are much more
important than others. Another reason is it’s not
intuitive what a gain of x BLEU points means in
practice. Here we show some concrete example
translations to show the types of improvements
we’re achieving and also some examples which
suggest improvements we can make to our AL se-
lection algorithm in the future. Figure 12 shows a
prototypical example of our system working.
Figure 13 shows an example where the strategy
is working partially but not as well as it might. The
Urdu phrase was translated by turkers as “gowned
veil”. However, since the word aligner just aligns
the word to “gowned”, we only see “gowned” in
our output. This prompts a number of discussion
points. First, the ‘after system’ has better transla-
tions but they’re not rewarded by BLEU scores be-
cause the references use the words ‘burqah’ or just
‘veil’ without ‘gowned’. Second, we hypothesize
that we may be able to see improvements by over-
riding the automatic alignment software when-
ever we obtain a many-to-one or one-to-many (in
terms of words) translation for one of our trigger
phrases. In such cases, we’d like to make sure that
every word on the ‘many’ side is aligned to the
</bodyText>
<note confidence="0.671569">
Bucking the Trend: JHiero Translation Quality versus
Number of Foreign Words Annotated
</note>
<figureCaption confidence="0.997019666666667">
Figure 11: Bucking the trend: performance of
HNG-selected additional data from BBC web
crawl data annotated via Amazon Mechanical
Turk. y-axis measures BLEU. x-axis measures
number of words annotated.
Figure 12: Example of strategy working.
</figureCaption>
<bodyText confidence="0.999190071428571">
single word on the ‘one’ side. For example, we
would force both ‘gowned’ and ‘veil’ to be aligned
to the single Urdu word instead of allowing the au-
tomatic aligner to only align ‘gowned’.
Figure 14 shows an example where our “before”
system already got the translation correct without
the need for the additional phrase translation. This
is because though the “before” system had never
seen the Urdu expression for “12 May”, it had seen
the Urdu words for “12” and “May” in isolation
and was able to successfully compose them. An
area of future work is to use the “before” system to
determine such cases automatically and avoid ask-
ing humans to provide translations in such cases.
</bodyText>
<figure confidence="0.99317425">
Relative Frequency
0.25
0.15
0.05
0.2
0.1
00 20 40 60 80 100 120
n−grams
sentences
average time per
word for sentences
average time per
word for n−grams
1 1.2 1.4 1.6 1.8
Number of Foreign Words Annotated x 106
BLEU Score 23.5
23
22.5
22
21.5
21
the approx. 54,500 foreign words
we selectively sampled
for annotation
cost = $205.80
last approx. 700,000
foreign words
annotated in LDC data
</figure>
<page confidence="0.8112">
861
</page>
<figureCaption confidence="0.99870475">
Figure 13: Example showing where we can im-
prove our selection strategy.
Figure 14: Example showing where we can im-
prove our selection strategy.
</figureCaption>
<sectionHeader confidence="0.995865" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999995333333333">
We succeeded in bucking the trend of diminishing
returns and improving translation quality while
keeping annotation costs low. In future work we
would like to apply these ideas to domain adap-
tation (say, general-purpose MT system to work
for scientific domain such as chemistry). Also, we
would like to test with more languages, increase
the amount of data we can gather, and investigate
stopping criteria further. Also, we would like to
investigate increasing the efficiency of the selec-
tion algorithm by addressing issues such as the one
raised by the 12 May example presented earlier.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999938666666667">
This work was supported by the Johns Hopkins
University Human Language Technology Center
of Excellence. Any opinions, findings, conclu-
sions, or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the sponsor.
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.943101175438596">
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. In Proceedings of the Seventh con-
ference on International Language Resources and
Evaluation (LREC’10), Valletta, Malta, may. Euro-
pean Language Resources Association (ELRA).
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics, pages
26–33, Toulouse, France, July. Association for Com-
putational Linguistics.
Michael Bloodgood and Chris Callison-Burch. 2010.
Using mechanical turk to build machine translation
evaluation sets. In Proceedings of the Workshop on
Creating Speech and Language Data With Amazon’s
Mechanical Turk, Los Angeles, California, June.
Association for Computational Linguistics.
Michael Bloodgood and K Vijay-Shanker. 2008. An
approach to reducing annotation costs for bionlp.
In Proceedings of the Workshop on Current Trends
in Biomedical Natural Language Processing, pages
104–105, Columbus, Ohio, June. Association for
Computational Linguistics.
Michael Bloodgood and K Vijay-Shanker. 2009a. A
method for stopping active learning based on stabi-
lizing predictions and the need for user-adjustable
stopping. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 39–47, Boulder, Colorado,
June. Association for Computational Linguistics.
Michael Bloodgood and K Vijay-Shanker. 2009b. Tak-
ing into account the differences between actively
and passively acquired data: The case of active
learning with support vector machines for imbal-
anced datasets. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL), pages 137–
140, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2006), Trento, Italy.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 144–151, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-
roll, and Peter McClanahan. 2008. Assessing the
</reference>
<page confidence="0.977185">
862
</page>
<reference confidence="0.999828469565217">
costs of sampling methods in active learning for an-
notation. In Proceedings ofACL-08: HLT, Short Pa-
pers, pages 65–68, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Gholamreza Haffari and Anoop Sarkar. 2009. Active
learning for multilingual statistical machine trans-
lation. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 181–189, Suntec,
Singapore, August. Association for Computational
Linguistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 415–423,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Rebecca Hwa. 2000. Sample selection for statistical
grammar induction. In Hinrich Sch¨utze and Keh-
Yih Su, editors, Proceedings of the 2000 Joint SIG-
DAT Conference on Empirical Methods in Natural
Language Processing, pages 45–53. Association for
Computational Linguistics, Somerset, New Jersey.
Ashish Kapoor, Eric Horvitz, and Sumit Basu. 2007.
Selective supervision: Guiding supervised learn-
ing with decision-theoretic active learning. In
Manuela M. Veloso, editor, IJCAI 2007, Proceed-
ings of the 20th International Joint Conference on
Artificial Intelligence, Hyderabad, India, January 6-
12, 2007, pages 877–882.
Ross D. King, Kenneth E. Whelan, Ffion M.
Jones, Philip G. K. Reiser, Christopher H. Bryant,
Stephen H. Muggleton, Douglas B. Kell, and
Stephen G. Oliver. 2004. Functional genomic hy-
pothesis generation and experimentation by a robot
scientist. Nature, 427:247–252, 15 January.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In SI-
GIR ’94: Proceedings of the 17th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 3–12, New
York, NY, USA. Springer-Verlag New York, Inc.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based
machine translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
135–139, Athens, Greece, March. Association for
Computational Linguistics.
Francois Mairesse, Milica Gasic, Filip Jurcicek, Simon
Keizer, Jorge Prombonas, Blaise Thomson, Kai Yu,
and Steve Young. 2010. Phrase-based statistical
language generation using graphical models and ac-
tive learning. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Uppsala, Sweden, July. Association
for Computational Linguistics.
Grace Ngai and David Yarowsky. 2000. Rule writ-
ing or annotation: cost-efficient resource usage for
base noun phrase chunking. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Miles Osborne and Jason Baldridge. 2004. Ensemble-
based active learning for parse selection. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
89–96, Boston, Massachusetts, USA, May 2 - May
7. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for japanese
word segmentation. In ACL ’02: Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 505–512, Morristown, NJ,
USA. Association for Computational Linguistics.
Greg Schohn and David Cohn. 2000. Less is more:
Active learning with support vector machines. In
Proc. 17th International Conf. on Machine Learn-
ing, pages 839–846. Morgan Kaufmann, San Fran-
cisco, CA.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast – but is it
good? evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 254–263, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Katrin Tomanek and Udo Hahn. 2009. Semi-
supervised active learning for sequence labeling. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of theAFNLP, pages 1039–1047, Suntec, Singapore,
August. Association for Computational Linguistics.
Simon Tong and Daphne Koller. 2002. Support vec-
tor machine active learning with applications to text
classification. Journal of Machine Learning Re-
search (JMLR), 2:45–66.
David Vickrey, Oscar Kipersztok, and Daphne Koller.
2010. An active learning approach to finding related
terms. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), Uppsala, Sweden, July. Association for
Computational Linguistics.
</reference>
<page confidence="0.989561">
863
</page>
<reference confidence="0.9995785">
Andreas Vlachos. 2008. A stopping criterion for
active learning. Computer Speech and Language,
22(3):295–312.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT06), New
York, New York.
</reference>
<page confidence="0.998636">
864
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.893166">
<title confidence="0.9998015">Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</title>
<author confidence="0.999904">Michael Bloodgood</author>
<affiliation confidence="0.967036333333333">Human Language Technology Center of Excellence Johns Hopkins University</affiliation>
<address confidence="0.999872">Baltimore, MD 21211</address>
<email confidence="0.999812">bloodgood@jhu.edu</email>
<abstract confidence="0.999357230769231">We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Vamshi Ambati</author>
<author>Stephan Vogel</author>
<author>Jaime Carbonell</author>
</authors>
<title>Active learning and crowd-sourcing for machine translation.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta, Malta,</location>
<contexts>
<context position="7067" citStr="Ambati et al. (2010)" startWordPosition="1184" endWordPosition="1187"> length in words and we show our method can achieve a speedup in annotation time above and beyond what the reduction in words would indicate. Tomanek and Hahn (2009) measure cost by # of tokens for an NER task. Their AL method only solicits labels for parts of sentences in the interest of reducing annotation effort. Along these lines, our method is similar in the respect that we also will only solicit annotation for parts of sentences, though we prefer to measure cost with time and we show that time doesn’t track with token length for SMT. Haffari et al. (2009), Haffari and Sarkar (2009), and Ambati et al. (2010) investigate AL for SMT. There are two major differences between our work and this previous work. One is that our intended use cases are very different. They deal with the more traditional AL setting of starting from an extremely small set of seed data. Also, by SMT standards, they only add a very tiny amount of data during AL. All their simulations top out at 10,000 sentences of labeled data and the models learned have relatively low translation quality compared to the state of the art. On the other hand, in the current paper, we demonstrate how to apply AL in situations where we already have</context>
</contexts>
<marker>Ambati, Vogel, Carbonell, 2010</marker>
<rawString>Vamshi Ambati, Stephan Vogel, and Jaime Carbonell. 2010. Active learning and crowd-sourcing for machine translation. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Toulouse, France,</location>
<contexts>
<context position="4145" citStr="Banko and Brill (2001)" startWordPosition="668" endWordPosition="671">guistics, pages 854–864, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics 2 Related Work Active learning has been shown to be effective for improving NLP systems and reducing annotation burdens for a number of NLP tasks (see, e.g., (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)). The current paper is most highly related to previous work falling into three main areas: use of AL when large corpora already exist; cost-focused AL; and AL for SMT. In a sense, the work of Banko and Brill (2001) is closely related to ours. Though their focus is mainly on investigating the performance of learning methods on giant corpora many orders of magnitude larger than previously used, they do lay out how AL might be useful to apply to acquire data to augment a large set cheaply because they recognize the problem of diminishing returns that we discussed in Section 1. The second area of work that is related to ours is previous work on AL that is cost-conscious. The vast majority of AL research has not focused on accurate cost accounting and a typical assumption is that each annotatable has equal a</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 26–33, Toulouse, France, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bloodgood</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Using mechanical turk to build machine translation evaluation sets.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="22327" citStr="Bloodgood and Callison-Burch, 2010" startWordPosition="3804" endWordPosition="3807">nguage pack, including its dictionary. In particular, we wanted to see if we could achieve translation improvements on top of already state-of-the-art performing systems trained already on the entire LDC corpus. Note that at the outset this is an ambitious endeavor (recall the flattening of the curves in Figure 1 from Section 1). Snow et al. (2008) explored the use of the Amazon Mechanical Turk (MTurk) web service for gathering annotations for a variety of natural language processing tasks and recently MTurk has been shown to be a quick, cost-effective way to gather Urdu-English translations (Bloodgood and Callison-Burch, 2010). We used the MTurk web service to gather our annotations. Specifically, we first crawled a large set of BBC articles on the internet in Urdu and used this as our unlabeled pool from which to gather annotations. We applied the HNG method from Section 4 to determine what to post on MTurk for workers to translate.2 We gathered 20,580 n-gram translations for which we paid $0.01 USD per translation, giving us a total cost of $205.80 USD. We also gathered 1632 randomly chosen Urdu sentence translations as a control set, for which we paid $0.10 USD per sentence translation.3 2For practical reasons w</context>
</contexts>
<marker>Bloodgood, Callison-Burch, 2010</marker>
<rawString>Michael Bloodgood and Chris Callison-Burch. 2010. Using mechanical turk to build machine translation evaluation sets. In Proceedings of the Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bloodgood</author>
<author>K Vijay-Shanker</author>
</authors>
<title>An approach to reducing annotation costs for bionlp.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,</booktitle>
<pages>104--105</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3849" citStr="Bloodgood and Vijay-Shanker, 2008" startWordPosition="614" endWordPosition="617">n our main investigations into bucking the trend of diminishing returns 10 as far as previous AL for SMT research studies were conducted 5 00 2 4 6 8 10 x 104 jHier jSyntax Number of Sentences in Training Data BLEU Score 15 854 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 854–864, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics 2 Related Work Active learning has been shown to be effective for improving NLP systems and reducing annotation burdens for a number of NLP tasks (see, e.g., (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)). The current paper is most highly related to previous work falling into three main areas: use of AL when large corpora already exist; cost-focused AL; and AL for SMT. In a sense, the work of Banko and Brill (2001) is closely related to ours. Though their focus is mainly on investigating the performance of learning methods on giant corpora many orders of magnitude larger than previously used, they do lay out how AL might be useful to apply to acquire data to augment a large set cheaply because they recognize the </context>
</contexts>
<marker>Bloodgood, Vijay-Shanker, 2008</marker>
<rawString>Michael Bloodgood and K Vijay-Shanker. 2008. An approach to reducing annotation costs for bionlp. In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing, pages 104–105, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bloodgood</author>
<author>K Vijay-Shanker</author>
</authors>
<title>A method for stopping active learning based on stabilizing predictions and the need for user-adjustable stopping.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>39--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="18316" citStr="Bloodgood and Vijay-Shanker, 2009" startWordPosition="3091" endWordPosition="3094">, this stopping criterion seems to work well as can be seen by where the curve for VG is cut off in Figures 3 and 4. It stops after 1,293,093 words have been translated, with jHier’s BLEU=21.92 and jSyntax’s BLEU=26.10 at the stopping point. The ending BLEU scores (with the full corpus annotated) are 21.87 and 26.01 for jHier and jSyntax, respectively. So our stopping criterion saves 22.3% of the annotation (in terms of words) and actually achieves slightly higher BLEU scores than if all the data were used. Note: this ”less is more” phenomenon has been commonly observed in AL settings (e.g., (Bloodgood and Vijay-Shanker, 2009a; Schohn and Cohn, 2000)). 4 Highlighted N-Gram Method In this section we describe a method for soliciting human translations that we have applied successfully to improving translation quality in real (not simulated) conditions. We call the method the Highlighted N-Gram method, or HNG, for short. HNG solicits translations only for trigger n-grams and not for entire sentences. We provide sentential context, highlight the trigger n-gram that we want translated, and ask for a translation of just the highlighted trigger n-gram. HNG asks for translations for triggers in the same order that the tri</context>
</contexts>
<marker>Bloodgood, Vijay-Shanker, 2009</marker>
<rawString>Michael Bloodgood and K Vijay-Shanker. 2009a. A method for stopping active learning based on stabilizing predictions and the need for user-adjustable stopping. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009), pages 39–47, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael Bloodgood</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Taking into account the differences between actively and passively acquired data: The case of active learning with support vector machines for imbalanced datasets.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>137--140</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="18316" citStr="Bloodgood and Vijay-Shanker, 2009" startWordPosition="3091" endWordPosition="3094">, this stopping criterion seems to work well as can be seen by where the curve for VG is cut off in Figures 3 and 4. It stops after 1,293,093 words have been translated, with jHier’s BLEU=21.92 and jSyntax’s BLEU=26.10 at the stopping point. The ending BLEU scores (with the full corpus annotated) are 21.87 and 26.01 for jHier and jSyntax, respectively. So our stopping criterion saves 22.3% of the annotation (in terms of words) and actually achieves slightly higher BLEU scores than if all the data were used. Note: this ”less is more” phenomenon has been commonly observed in AL settings (e.g., (Bloodgood and Vijay-Shanker, 2009a; Schohn and Cohn, 2000)). 4 Highlighted N-Gram Method In this section we describe a method for soliciting human translations that we have applied successfully to improving translation quality in real (not simulated) conditions. We call the method the Highlighted N-Gram method, or HNG, for short. HNG solicits translations only for trigger n-grams and not for entire sentences. We provide sentential context, highlight the trigger n-gram that we want translated, and ask for a translation of just the highlighted trigger n-gram. HNG asks for translations for triggers in the same order that the tri</context>
</contexts>
<marker>Bloodgood, Vijay-Shanker, 2009</marker>
<rawString>Michael Bloodgood and K Vijay-Shanker. 2009b. Taking into account the differences between actively and passively acquired data: The case of active learning with support vector machines for imbalanced datasets. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 137– 140, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of Bleu in machine translation research.</title>
<date>2006</date>
<booktitle>In 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006),</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="27680" citStr="Callison-Burch et al., 2006" startWordPosition="4777" endWordPosition="4780">ope fit to the LDC data is only 7.4957E-07 BLEU points per word, or only 0.74957 BLEU points for a million words. This is already an order of magnitude difference that would make the difference between it being worth adding more data and not being worth it; and this is leaving aside the added time speedup that our method enjoys. Still, we wondered why we could not have raised BLEU scores even faster. The main hurdle seems to be one of coverage. Of the 20,580 ngrams we collected, only 571 (i.e., 2.77%) of them ever even occur in the test set. 5.4 Beyond BLEU Scores BLEU is an imperfect metric (Callison-Burch et al., 2006). One reason is that it rates all ngram 22 21.8 21.60 1 2 3 4 5 6 x 105 random HNG Number of Seconds of Annotation Time BLEU Score 22.2 22.8 22.6 22.4 860 Histogram showing the distribution of translation speeds (in seconds per foreign word) when translations are collected via n−grams versus via complete sentences Time (in seconds) per foreign word translated Figure 10: Distribution of translation speeds (in seconds per word) for HNG postings versus complete sentence postings. The y-axis measures relative frequency. The x-axis measures translation speed in seconds per word (so farther to the l</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the role of Bleu in machine translation research. In 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006), Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="9378" citStr="Chiang, 2007" startWordPosition="1573" endWordPosition="1574">All experiments in this paper evaluate on a genre-balanced split of the NIST2008 Urdu-English test set. In addition, the language pack contains an Urdu-English dictionary consisting of Pz� 114000 entries. In all the experiments, we use the dictionary at every iteration of training. This will make it harder for us to show our methods providing substantial gains since the dictionary will provide a higher base performance to begin with. However, it would be artificial to ignore dictionary resources when they exist. We experiment with two translation models: hierarchical phrase-based translation (Chiang, 2007) and syntax augmented translation (Zollmann and Venugopal, 2006), both of which are implemented in the Joshua decoder (Li et al., 2009). We hereafter refer to these systems as jHier and jSyntax, respectively. We will now present results of experiments with different methods for growing MT training data. The results are organized into three areas of investigations: 1. annotation costs; 2. managing uncertainty; and 3. how to automatically detect when to stop soliciting annotations from a pool of data. 3.1 Annotation Costs We begin our cost investigations with four simple methods for growing MT t</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Beatrice Alex</author>
<author>Markus Becker</author>
</authors>
<title>Investigating the effects of selective sampling on the annotation task.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005),</booktitle>
<pages>144--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="5759" citStr="Hachey et al. (2005)" startWordPosition="954" endWordPosition="957">versus annotation with AL for the task of BaseNP chunking. Osborne and Baldridge (2004) argued for the use of discriminant cost over unit cost for the task of Head Phrase Structure Grammar parse selection. King et al. (2004) design a robot that tests gene functions. The robot chooses which experiments to conduct by using AL and takes monetary costs (in pounds sterling) into account during AL selection and evaluation. Unlike our situation for SMT, their costs are all known beforehand because they are simply the cost of materials to conduct the experiments, which are already known to the robot. Hachey et al. (2005) showed that selectively sampled examples for an NER task took longer to annotate and had lower inter-annotator agreement. This work is related to ours because it shows that how examples are selected can impact the cost of annotation, an idea we turn around to use for our advantage when developing our data selection algorithm. Haertel et al. (2008) emphasize measuring costs carefully for AL for POS tagging. They develop a model based on a user study that can estimate the time required for POS annotating. Kapoor et al. (2007) assign costs for AL based on message length for a voicemail classific</context>
</contexts>
<marker>Hachey, Alex, Becker, 2005</marker>
<rawString>Ben Hachey, Beatrice Alex, and Markus Becker. 2005. Investigating the effects of selective sampling on the annotation task. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages 144–151, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robbie Haertel</author>
<author>Eric Ringger</author>
<author>Kevin Seppi</author>
<author>James Carroll</author>
<author>Peter McClanahan</author>
</authors>
<title>Assessing the costs of sampling methods in active learning for annotation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT, Short Papers,</booktitle>
<pages>65--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="6109" citStr="Haertel et al. (2008)" startWordPosition="1014" endWordPosition="1017">s (in pounds sterling) into account during AL selection and evaluation. Unlike our situation for SMT, their costs are all known beforehand because they are simply the cost of materials to conduct the experiments, which are already known to the robot. Hachey et al. (2005) showed that selectively sampled examples for an NER task took longer to annotate and had lower inter-annotator agreement. This work is related to ours because it shows that how examples are selected can impact the cost of annotation, an idea we turn around to use for our advantage when developing our data selection algorithm. Haertel et al. (2008) emphasize measuring costs carefully for AL for POS tagging. They develop a model based on a user study that can estimate the time required for POS annotating. Kapoor et al. (2007) assign costs for AL based on message length for a voicemail classification task. In contrast, we show for SMT that annotation times do not scale according to length in words and we show our method can achieve a speedup in annotation time above and beyond what the reduction in words would indicate. Tomanek and Hahn (2009) measure cost by # of tokens for an NER task. Their AL method only solicits labels for parts of s</context>
</contexts>
<marker>Haertel, Ringger, Seppi, Carroll, McClanahan, 2008</marker>
<rawString>Robbie Haertel, Eric Ringger, Kevin Seppi, James Carroll, and Peter McClanahan. 2008. Assessing the costs of sampling methods in active learning for annotation. In Proceedings ofACL-08: HLT, Short Papers, pages 65–68, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Active learning for multilingual statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>181--189</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1220" citStr="Haffari and Sarkar, 2009" startWordPosition="185" endWordPosition="188">s via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement. 1 Introduction Figure 1 shows the learning curves for two state of the art statistical machine translation (SMT) systems for Urdu-English translation. Observe how the learning curves rise rapidly at first but then a trend of diminishing returns occurs: put simply, the curves flatten. This paper investigates whether we can buck the trend of diminishing returns, and if so, how we can do it effectively. Active learning (AL) has been applied to SMT recently (Haffari et al., 2009; Haffari and Sarkar, 2009) but they were interested in starting with a tiny seed set of data, and they stopped their investigations after only adding a relatively tiny amount of data as depicted in Figure 1. In contrast, we are interested in applying AL when a large amount of data already exists as is the case for many important lanuage pairs. We develop an AL algorithm that focuses on keeping annotation costs (measured by time in seconds) low. It succeeds in doing this by only soliciting translations for parts of sentences. We show that this gets a savings in human annotation time above and beyond what the reduction i</context>
<context position="7041" citStr="Haffari and Sarkar (2009)" startWordPosition="1179" endWordPosition="1182">times do not scale according to length in words and we show our method can achieve a speedup in annotation time above and beyond what the reduction in words would indicate. Tomanek and Hahn (2009) measure cost by # of tokens for an NER task. Their AL method only solicits labels for parts of sentences in the interest of reducing annotation effort. Along these lines, our method is similar in the respect that we also will only solicit annotation for parts of sentences, though we prefer to measure cost with time and we show that time doesn’t track with token length for SMT. Haffari et al. (2009), Haffari and Sarkar (2009), and Ambati et al. (2010) investigate AL for SMT. There are two major differences between our work and this previous work. One is that our intended use cases are very different. They deal with the more traditional AL setting of starting from an extremely small set of seed data. Also, by SMT standards, they only add a very tiny amount of data during AL. All their simulations top out at 10,000 sentences of labeled data and the models learned have relatively low translation quality compared to the state of the art. On the other hand, in the current paper, we demonstrate how to apply AL in situat</context>
<context position="10238" citStr="Haffari and Sarkar, 2009" startWordPosition="1709" endWordPosition="1712">s of experiments with different methods for growing MT training data. The results are organized into three areas of investigations: 1. annotation costs; 2. managing uncertainty; and 3. how to automatically detect when to stop soliciting annotations from a pool of data. 3.1 Annotation Costs We begin our cost investigations with four simple methods for growing MT training data: random, shortest, longest, and VocabGrowth sentence selection. The first three methods are selfexplanatory. VocabGrowth (hereafter VG) selection is modeled after the best methods from previous work (Haffari et al., 2009; Haffari and Sarkar, 2009), which are based on preferring sentences that contain phrases that occur frequently in unlabeled data and infrequently in the so-far labeled data. Our VG method selects sentences for translation that contain n-grams (for n in 11,2,3,4}) that 1LDC Catalog No.: LDC2006E110. Init: Go through all available training data (labeled and unlabeled) and obtain frequency counts for every n-gram (n in 11, 2, 3, 4}) that occurs. sortedNGrams +— Sort n-grams by frequency in descending order. Loop until stopping criterion (see Section 3.3) is met 1. trigger +— Go down sortedNGrams list and find the first n-</context>
</contexts>
<marker>Haffari, Sarkar, 2009</marker>
<rawString>Gholamreza Haffari and Anoop Sarkar. 2009. Active learning for multilingual statistical machine translation. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 181–189, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gholamreza Haffari</author>
<author>Maxim Roy</author>
<author>Anoop Sarkar</author>
</authors>
<title>Active learning for statistical phrase-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>415--423</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="1193" citStr="Haffari et al., 2009" startWordPosition="181" endWordPosition="184">, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement. 1 Introduction Figure 1 shows the learning curves for two state of the art statistical machine translation (SMT) systems for Urdu-English translation. Observe how the learning curves rise rapidly at first but then a trend of diminishing returns occurs: put simply, the curves flatten. This paper investigates whether we can buck the trend of diminishing returns, and if so, how we can do it effectively. Active learning (AL) has been applied to SMT recently (Haffari et al., 2009; Haffari and Sarkar, 2009) but they were interested in starting with a tiny seed set of data, and they stopped their investigations after only adding a relatively tiny amount of data as depicted in Figure 1. In contrast, we are interested in applying AL when a large amount of data already exists as is the case for many important lanuage pairs. We develop an AL algorithm that focuses on keeping annotation costs (measured by time in seconds) low. It succeeds in doing this by only soliciting translations for parts of sentences. We show that this gets a savings in human annotation time above and </context>
<context position="7014" citStr="Haffari et al. (2009)" startWordPosition="1175" endWordPosition="1178">or SMT that annotation times do not scale according to length in words and we show our method can achieve a speedup in annotation time above and beyond what the reduction in words would indicate. Tomanek and Hahn (2009) measure cost by # of tokens for an NER task. Their AL method only solicits labels for parts of sentences in the interest of reducing annotation effort. Along these lines, our method is similar in the respect that we also will only solicit annotation for parts of sentences, though we prefer to measure cost with time and we show that time doesn’t track with token length for SMT. Haffari et al. (2009), Haffari and Sarkar (2009), and Ambati et al. (2010) investigate AL for SMT. There are two major differences between our work and this previous work. One is that our intended use cases are very different. They deal with the more traditional AL setting of starting from an extremely small set of seed data. Also, by SMT standards, they only add a very tiny amount of data during AL. All their simulations top out at 10,000 sentences of labeled data and the models learned have relatively low translation quality compared to the state of the art. On the other hand, in the current paper, we demonstrat</context>
<context position="10211" citStr="Haffari et al., 2009" startWordPosition="1705" endWordPosition="1708">ill now present results of experiments with different methods for growing MT training data. The results are organized into three areas of investigations: 1. annotation costs; 2. managing uncertainty; and 3. how to automatically detect when to stop soliciting annotations from a pool of data. 3.1 Annotation Costs We begin our cost investigations with four simple methods for growing MT training data: random, shortest, longest, and VocabGrowth sentence selection. The first three methods are selfexplanatory. VocabGrowth (hereafter VG) selection is modeled after the best methods from previous work (Haffari et al., 2009; Haffari and Sarkar, 2009), which are based on preferring sentences that contain phrases that occur frequently in unlabeled data and infrequently in the so-far labeled data. Our VG method selects sentences for translation that contain n-grams (for n in 11,2,3,4}) that 1LDC Catalog No.: LDC2006E110. Init: Go through all available training data (labeled and unlabeled) and obtain frequency counts for every n-gram (n in 11, 2, 3, 4}) that occurs. sortedNGrams +— Sort n-grams by frequency in descending order. Loop until stopping criterion (see Section 3.3) is met 1. trigger +— Go down sortedNGrams</context>
</contexts>
<marker>Haffari, Roy, Sarkar, 2009</marker>
<rawString>Gholamreza Haffari, Maxim Roy, and Anoop Sarkar. 2009. Active learning for statistical phrase-based machine translation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 415–423, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample selection for statistical grammar induction.</title>
<date>2000</date>
<booktitle>In Hinrich Sch¨utze and KehYih Su, editors, Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>45--53</pages>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="3799" citStr="Hwa, 2000" startWordPosition="610" endWordPosition="611">es. 30 25 20 where we begin our main investigations into bucking the trend of diminishing returns 10 as far as previous AL for SMT research studies were conducted 5 00 2 4 6 8 10 x 104 jHier jSyntax Number of Sentences in Training Data BLEU Score 15 854 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 854–864, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics 2 Related Work Active learning has been shown to be effective for improving NLP systems and reducing annotation burdens for a number of NLP tasks (see, e.g., (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)). The current paper is most highly related to previous work falling into three main areas: use of AL when large corpora already exist; cost-focused AL; and AL for SMT. In a sense, the work of Banko and Brill (2001) is closely related to ours. Though their focus is mainly on investigating the performance of learning methods on giant corpora many orders of magnitude larger than previously used, they do lay out how AL might be useful to apply to acquire data to augme</context>
</contexts>
<marker>Hwa, 2000</marker>
<rawString>Rebecca Hwa. 2000. Sample selection for statistical grammar induction. In Hinrich Sch¨utze and KehYih Su, editors, Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing, pages 45–53. Association for Computational Linguistics, Somerset, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Kapoor</author>
<author>Eric Horvitz</author>
<author>Sumit Basu</author>
</authors>
<title>Selective supervision: Guiding supervised learning with decision-theoretic active learning.</title>
<date>2007</date>
<booktitle>IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>877--882</pages>
<editor>In Manuela M. Veloso, editor,</editor>
<location>Hyderabad, India,</location>
<contexts>
<context position="6289" citStr="Kapoor et al. (2007)" startWordPosition="1045" endWordPosition="1048">als to conduct the experiments, which are already known to the robot. Hachey et al. (2005) showed that selectively sampled examples for an NER task took longer to annotate and had lower inter-annotator agreement. This work is related to ours because it shows that how examples are selected can impact the cost of annotation, an idea we turn around to use for our advantage when developing our data selection algorithm. Haertel et al. (2008) emphasize measuring costs carefully for AL for POS tagging. They develop a model based on a user study that can estimate the time required for POS annotating. Kapoor et al. (2007) assign costs for AL based on message length for a voicemail classification task. In contrast, we show for SMT that annotation times do not scale according to length in words and we show our method can achieve a speedup in annotation time above and beyond what the reduction in words would indicate. Tomanek and Hahn (2009) measure cost by # of tokens for an NER task. Their AL method only solicits labels for parts of sentences in the interest of reducing annotation effort. Along these lines, our method is similar in the respect that we also will only solicit annotation for parts of sentences, th</context>
</contexts>
<marker>Kapoor, Horvitz, Basu, 2007</marker>
<rawString>Ashish Kapoor, Eric Horvitz, and Sumit Basu. 2007. Selective supervision: Guiding supervised learning with decision-theoretic active learning. In Manuela M. Veloso, editor, IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007, pages 877–882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross D King</author>
<author>Kenneth E Whelan</author>
<author>Ffion M Jones</author>
<author>Philip G K Reiser</author>
<author>Christopher H Bryant</author>
<author>Stephen H Muggleton</author>
<author>Douglas B Kell</author>
<author>Stephen G Oliver</author>
</authors>
<title>Functional genomic hypothesis generation and experimentation by a robot scientist.</title>
<date>2004</date>
<journal>Nature,</journal>
<volume>427</volume>
<pages>15</pages>
<contexts>
<context position="5363" citStr="King et al. (2004)" startWordPosition="886" endWordPosition="889">nnotation cost. An early exception in the AL for NLP field was the work of Hwa (2000), which makes a point of using # of brackets to measure cost for a syntactic analysis task instead of using # of sentences. Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. Osborne and Baldridge (2004) argued for the use of discriminant cost over unit cost for the task of Head Phrase Structure Grammar parse selection. King et al. (2004) design a robot that tests gene functions. The robot chooses which experiments to conduct by using AL and takes monetary costs (in pounds sterling) into account during AL selection and evaluation. Unlike our situation for SMT, their costs are all known beforehand because they are simply the cost of materials to conduct the experiments, which are already known to the robot. Hachey et al. (2005) showed that selectively sampled examples for an NER task took longer to annotate and had lower inter-annotator agreement. This work is related to ours because it shows that how examples are selected can </context>
</contexts>
<marker>King, Whelan, Jones, Reiser, Bryant, Muggleton, Kell, Oliver, 2004</marker>
<rawString>Ross D. King, Kenneth E. Whelan, Ffion M. Jones, Philip G. K. Reiser, Christopher H. Bryant, Stephen H. Muggleton, Douglas B. Kell, and Stephen G. Oliver. 2004. Functional genomic hypothesis generation and experimentation by a robot scientist. Nature, 427:247–252, 15 January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In SIGIR ’94: Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>3--12</pages>
<publisher>Springer-Verlag</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="15200" citStr="Lewis and Gale, 1994" startWordPosition="2560" endWordPosition="2563">w a method for soliciting annotations that reduces time per word by nearly a factor of three. As it is prudent to evaluate using accurate cost accounting, so it is also prudent to develop new AL algorithms that take costs carefully into account. Hence, reducing annotation time burdens instead of the # of sentences translated (which might be quite a different thing) will be a cornerstone of the algorithm we describe in Section 4. 3.2 Managing Uncertainty One of the most successful of all AL methods developed to date is uncertainty sampling and it has been applied successfully many times (e.g.,(Lewis and Gale, 1994; Tong and Koller, 2002)). The intuition is clear: much can be learned (potentially) if there is great uncertainty. However, with MT being a relatively complicated task (compared with binary classification, for example), it might be the case that the uncertainty approach has to be re-considered. If words have never occurred in the training data, then uncertainty can be expected to be high. But we are concerned that if a sentence is translated for which (almost) no words have been seen in training yet, though uncertainty will be high (which is usually considered good for AL), the word alignment</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and William A. Gale. 1994. A sequential algorithm for training text classifiers. In SIGIR ’94: Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, pages 3–12, New York, NY, USA. Springer-Verlag New York, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>135--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="9513" citStr="Li et al., 2009" startWordPosition="1592" endWordPosition="1595">ck contains an Urdu-English dictionary consisting of Pz� 114000 entries. In all the experiments, we use the dictionary at every iteration of training. This will make it harder for us to show our methods providing substantial gains since the dictionary will provide a higher base performance to begin with. However, it would be artificial to ignore dictionary resources when they exist. We experiment with two translation models: hierarchical phrase-based translation (Chiang, 2007) and syntax augmented translation (Zollmann and Venugopal, 2006), both of which are implemented in the Joshua decoder (Li et al., 2009). We hereafter refer to these systems as jHier and jSyntax, respectively. We will now present results of experiments with different methods for growing MT training data. The results are organized into three areas of investigations: 1. annotation costs; 2. managing uncertainty; and 3. how to automatically detect when to stop soliciting annotations from a pool of data. 3.1 Annotation Costs We begin our cost investigations with four simple methods for growing MT training data: random, shortest, longest, and VocabGrowth sentence selection. The first three methods are selfexplanatory. VocabGrowth (</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 135–139, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francois Mairesse</author>
<author>Milica Gasic</author>
<author>Filip Jurcicek</author>
<author>Simon Keizer</author>
<author>Jorge Prombonas</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
<author>Steve Young</author>
</authors>
<title>Phrase-based statistical language generation using graphical models and active learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="3907" citStr="Mairesse et al., 2010" startWordPosition="623" endWordPosition="626">s 10 as far as previous AL for SMT research studies were conducted 5 00 2 4 6 8 10 x 104 jHier jSyntax Number of Sentences in Training Data BLEU Score 15 854 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 854–864, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics 2 Related Work Active learning has been shown to be effective for improving NLP systems and reducing annotation burdens for a number of NLP tasks (see, e.g., (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)). The current paper is most highly related to previous work falling into three main areas: use of AL when large corpora already exist; cost-focused AL; and AL for SMT. In a sense, the work of Banko and Brill (2001) is closely related to ours. Though their focus is mainly on investigating the performance of learning methods on giant corpora many orders of magnitude larger than previously used, they do lay out how AL might be useful to apply to acquire data to augment a large set cheaply because they recognize the problem of diminishing returns that we discussed in Sectio</context>
</contexts>
<marker>Mairesse, Gasic, Jurcicek, Keizer, Prombonas, Thomson, Yu, Young, 2010</marker>
<rawString>Francois Mairesse, Milica Gasic, Filip Jurcicek, Simon Keizer, Jorge Prombonas, Blaise Thomson, Kai Yu, and Steve Young. 2010. Phrase-based statistical language generation using graphical models and active learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>David Yarowsky</author>
</authors>
<title>Rule writing or annotation: cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5055" citStr="Ngai and Yarowsky (2000)" startWordPosition="833" endWordPosition="836">hey recognize the problem of diminishing returns that we discussed in Section 1. The second area of work that is related to ours is previous work on AL that is cost-conscious. The vast majority of AL research has not focused on accurate cost accounting and a typical assumption is that each annotatable has equal annotation cost. An early exception in the AL for NLP field was the work of Hwa (2000), which makes a point of using # of brackets to measure cost for a syntactic analysis task instead of using # of sentences. Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. Osborne and Baldridge (2004) argued for the use of discriminant cost over unit cost for the task of Head Phrase Structure Grammar parse selection. King et al. (2004) design a robot that tests gene functions. The robot chooses which experiments to conduct by using AL and takes monetary costs (in pounds sterling) into account during AL selection and evaluation. Unlike our situation for SMT, their costs are all known beforehand because they are simply the </context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>Grace Ngai and David Yarowsky. 2000. Rule writing or annotation: cost-efficient resource usage for base noun phrase chunking. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
<author>Jason Baldridge</author>
</authors>
<title>Ensemblebased active learning for parse selection.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>89--96</pages>
<editor>In Daniel Marcu Susan Dumais and Salim Roukos, editors,</editor>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="5226" citStr="Osborne and Baldridge (2004)" startWordPosition="862" endWordPosition="865">nscious. The vast majority of AL research has not focused on accurate cost accounting and a typical assumption is that each annotatable has equal annotation cost. An early exception in the AL for NLP field was the work of Hwa (2000), which makes a point of using # of brackets to measure cost for a syntactic analysis task instead of using # of sentences. Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. Osborne and Baldridge (2004) argued for the use of discriminant cost over unit cost for the task of Head Phrase Structure Grammar parse selection. King et al. (2004) design a robot that tests gene functions. The robot chooses which experiments to conduct by using AL and takes monetary costs (in pounds sterling) into account during AL selection and evaluation. Unlike our situation for SMT, their costs are all known beforehand because they are simply the cost of materials to conduct the experiments, which are already known to the robot. Hachey et al. (2005) showed that selectively sampled examples for an NER task took long</context>
</contexts>
<marker>Osborne, Baldridge, 2004</marker>
<rawString>Miles Osborne and Jason Baldridge. 2004. Ensemblebased active learning for parse selection. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 89–96, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="11511" citStr="Papineni et al., 2002" startWordPosition="1921" endWordPosition="1925">ing data. 2. selected5entence +— Find a sentence that contains trigger. 3. Remove selected5entence from unlabeled data and add it to labeled training data. End Loop Figure 2: The VG sentence selection algorithm do not occur at all in our so-far labeled data. We call an n-gram “covered” if it occurs at least once in our so-far labeled data. VG has a preference for covering frequent n-grams before covering infrequent n-grams. The VG method is depicted in Figure 2. Figure 3 shows the learning curves for both jHier and jSyntax for VG selection and random selection. The y-axis measures BLEU score (Papineni et al., 2002),which is a fast automatic way of measuring translation quality that has been shown to correlate with human judgments and is perhaps the most widely used metric in the MT community. The x-axis measures the number of sentence translation pairs in the training data. The VG curves are cut off at the point at which the stopping criterion in Section 3.3 is met. From Figure 3 it might appear that VG selection is better than random selection, achieving higher-performing systems with fewer translations in the labeled data. However, it is important to take care when measuring annotation costs (especial</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Sassano</author>
</authors>
<title>An empirical study of active learning with support vector machines for japanese word segmentation.</title>
<date>2002</date>
<booktitle>In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>505--512</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3814" citStr="Sassano, 2002" startWordPosition="612" endWordPosition="613">0 where we begin our main investigations into bucking the trend of diminishing returns 10 as far as previous AL for SMT research studies were conducted 5 00 2 4 6 8 10 x 104 jHier jSyntax Number of Sentences in Training Data BLEU Score 15 854 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 854–864, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics 2 Related Work Active learning has been shown to be effective for improving NLP systems and reducing annotation burdens for a number of NLP tasks (see, e.g., (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)). The current paper is most highly related to previous work falling into three main areas: use of AL when large corpora already exist; cost-focused AL; and AL for SMT. In a sense, the work of Banko and Brill (2001) is closely related to ours. Though their focus is mainly on investigating the performance of learning methods on giant corpora many orders of magnitude larger than previously used, they do lay out how AL might be useful to apply to acquire data to augment a large set </context>
</contexts>
<marker>Sassano, 2002</marker>
<rawString>Manabu Sassano. 2002. An empirical study of active learning with support vector machines for japanese word segmentation. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 505–512, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Schohn</author>
<author>David Cohn</author>
</authors>
<title>Less is more: Active learning with support vector machines.</title>
<date>2000</date>
<booktitle>In Proc. 17th International Conf. on Machine Learning,</booktitle>
<pages>839--846</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="17568" citStr="Schohn and Cohn, 2000" startWordPosition="2963" endWordPosition="2966">that mostNew underperforms VG. This could have been due to VG’s frequency component, which mostNew doesn’t have. But moderateNew also doesn’t have a frequency preference so it is likely that mostNew winds up overwhelming the MT training system, word alignments are incorrect, and less is learned as a result. In light of this, the algorithm we develop in Section 4 will be designed to avoid this word alignment danger. 3.3 Automatic Stopping The problem of automatically detecting when to stop AL is a substantial one, discussed at length in the literature (e.g., (Bloodgood and VijayShanker, 2009a; Schohn and Cohn, 2000; Vlachos, 2008)). In our simulation, we stop VG once all n-grams (n in {1,2,3,4}) have been covered. Though simple, this stopping criterion seems to work well as can be seen by where the curve for VG is cut off in Figures 3 and 4. It stops after 1,293,093 words have been translated, with jHier’s BLEU=21.92 and jSyntax’s BLEU=26.10 at the stopping point. The ending BLEU scores (with the full corpus annotated) are 21.87 and 26.01 for jHier and jSyntax, respectively. So our stopping criterion saves 22.3% of the annotation (in terms of words) and actually achieves slightly higher BLEU scores than</context>
</contexts>
<marker>Schohn, Cohn, 2000</marker>
<rawString>Greg Schohn and David Cohn. 2000. Less is more: Active learning with support vector machines. In Proc. 17th International Conf. on Machine Learning, pages 839–846. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>254--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 254–263, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Udo Hahn</author>
</authors>
<title>Semisupervised active learning for sequence labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of theAFNLP,</booktitle>
<pages>1039--1047</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="6612" citStr="Tomanek and Hahn (2009)" startWordPosition="1103" endWordPosition="1106">ion, an idea we turn around to use for our advantage when developing our data selection algorithm. Haertel et al. (2008) emphasize measuring costs carefully for AL for POS tagging. They develop a model based on a user study that can estimate the time required for POS annotating. Kapoor et al. (2007) assign costs for AL based on message length for a voicemail classification task. In contrast, we show for SMT that annotation times do not scale according to length in words and we show our method can achieve a speedup in annotation time above and beyond what the reduction in words would indicate. Tomanek and Hahn (2009) measure cost by # of tokens for an NER task. Their AL method only solicits labels for parts of sentences in the interest of reducing annotation effort. Along these lines, our method is similar in the respect that we also will only solicit annotation for parts of sentences, though we prefer to measure cost with time and we show that time doesn’t track with token length for SMT. Haffari et al. (2009), Haffari and Sarkar (2009), and Ambati et al. (2010) investigate AL for SMT. There are two major differences between our work and this previous work. One is that our intended use cases are very dif</context>
</contexts>
<marker>Tomanek, Hahn, 2009</marker>
<rawString>Katrin Tomanek and Udo Hahn. 2009. Semisupervised active learning for sequence labeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of theAFNLP, pages 1039–1047, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tong</author>
<author>Daphne Koller</author>
</authors>
<title>Support vector machine active learning with applications to text classification.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>2--45</pages>
<contexts>
<context position="15224" citStr="Tong and Koller, 2002" startWordPosition="2564" endWordPosition="2567">ing annotations that reduces time per word by nearly a factor of three. As it is prudent to evaluate using accurate cost accounting, so it is also prudent to develop new AL algorithms that take costs carefully into account. Hence, reducing annotation time burdens instead of the # of sentences translated (which might be quite a different thing) will be a cornerstone of the algorithm we describe in Section 4. 3.2 Managing Uncertainty One of the most successful of all AL methods developed to date is uncertainty sampling and it has been applied successfully many times (e.g.,(Lewis and Gale, 1994; Tong and Koller, 2002)). The intuition is clear: much can be learned (potentially) if there is great uncertainty. However, with MT being a relatively complicated task (compared with binary classification, for example), it might be the case that the uncertainty approach has to be re-considered. If words have never occurred in the training data, then uncertainty can be expected to be high. But we are concerned that if a sentence is translated for which (almost) no words have been seen in training yet, though uncertainty will be high (which is usually considered good for AL), the word alignments may be incorrect and t</context>
</contexts>
<marker>Tong, Koller, 2002</marker>
<rawString>Simon Tong and Daphne Koller. 2002. Support vector machine active learning with applications to text classification. Journal of Machine Learning Research (JMLR), 2:45–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vickrey</author>
<author>Oscar Kipersztok</author>
<author>Daphne Koller</author>
</authors>
<title>An active learning approach to finding related terms.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="3930" citStr="Vickrey et al., 2010" startWordPosition="627" endWordPosition="630"> AL for SMT research studies were conducted 5 00 2 4 6 8 10 x 104 jHier jSyntax Number of Sentences in Training Data BLEU Score 15 854 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 854–864, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics 2 Related Work Active learning has been shown to be effective for improving NLP systems and reducing annotation burdens for a number of NLP tasks (see, e.g., (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)). The current paper is most highly related to previous work falling into three main areas: use of AL when large corpora already exist; cost-focused AL; and AL for SMT. In a sense, the work of Banko and Brill (2001) is closely related to ours. Though their focus is mainly on investigating the performance of learning methods on giant corpora many orders of magnitude larger than previously used, they do lay out how AL might be useful to apply to acquire data to augment a large set cheaply because they recognize the problem of diminishing returns that we discussed in Section 1. The second area of</context>
</contexts>
<marker>Vickrey, Kipersztok, Koller, 2010</marker>
<rawString>David Vickrey, Oscar Kipersztok, and Daphne Koller. 2010. An active learning approach to finding related terms. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
</authors>
<title>A stopping criterion for active learning.</title>
<date>2008</date>
<journal>Computer Speech and Language,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="17584" citStr="Vlachos, 2008" startWordPosition="2967" endWordPosition="2969">rms VG. This could have been due to VG’s frequency component, which mostNew doesn’t have. But moderateNew also doesn’t have a frequency preference so it is likely that mostNew winds up overwhelming the MT training system, word alignments are incorrect, and less is learned as a result. In light of this, the algorithm we develop in Section 4 will be designed to avoid this word alignment danger. 3.3 Automatic Stopping The problem of automatically detecting when to stop AL is a substantial one, discussed at length in the literature (e.g., (Bloodgood and VijayShanker, 2009a; Schohn and Cohn, 2000; Vlachos, 2008)). In our simulation, we stop VG once all n-grams (n in {1,2,3,4}) have been covered. Though simple, this stopping criterion seems to work well as can be seen by where the curve for VG is cut off in Figures 3 and 4. It stops after 1,293,093 words have been translated, with jHier’s BLEU=21.92 and jSyntax’s BLEU=26.10 at the stopping point. The ending BLEU scores (with the full corpus annotated) are 21.87 and 26.01 for jHier and jSyntax, respectively. So our stopping criterion saves 22.3% of the annotation (in terms of words) and actually achieves slightly higher BLEU scores than if all the data</context>
</contexts>
<marker>Vlachos, 2008</marker>
<rawString>Andreas Vlachos. 2008. A stopping criterion for active learning. Computer Speech and Language, 22(3):295–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the NAACL-2006 Workshop on Statistical Machine Translation (WMT06),</booktitle>
<location>New York, New York.</location>
<contexts>
<context position="9442" citStr="Zollmann and Venugopal, 2006" startWordPosition="1579" endWordPosition="1582">e-balanced split of the NIST2008 Urdu-English test set. In addition, the language pack contains an Urdu-English dictionary consisting of Pz� 114000 entries. In all the experiments, we use the dictionary at every iteration of training. This will make it harder for us to show our methods providing substantial gains since the dictionary will provide a higher base performance to begin with. However, it would be artificial to ignore dictionary resources when they exist. We experiment with two translation models: hierarchical phrase-based translation (Chiang, 2007) and syntax augmented translation (Zollmann and Venugopal, 2006), both of which are implemented in the Joshua decoder (Li et al., 2009). We hereafter refer to these systems as jHier and jSyntax, respectively. We will now present results of experiments with different methods for growing MT training data. The results are organized into three areas of investigations: 1. annotation costs; 2. managing uncertainty; and 3. how to automatically detect when to stop soliciting annotations from a pool of data. 3.1 Annotation Costs We begin our cost investigations with four simple methods for growing MT training data: random, shortest, longest, and VocabGrowth sentenc</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the NAACL-2006 Workshop on Statistical Machine Translation (WMT06), New York, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>