<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005845">
<title confidence="0.9984135">
The Domain Restriction Hypothesis:
Relating Term Similarity and Semantic Consistency
</title>
<author confidence="0.987524">
Alfio Massimiliano Gliozzo Marco Pennacchiotti Patrick Pantel
</author>
<affiliation confidence="0.912457">
ITC-irst University of Rome Tor Vergata USC, Information Sciences Institute
Trento, Italy Rome, Italy Marina del Rey, CA
</affiliation>
<email confidence="0.993561">
gliozzo@itc.it pennacchiotti@info.uniroma2.it pantel@isi.edu
</email>
<sectionHeader confidence="0.998557" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934642857143">
In this paper, we empirically demonstrate
what we call the domain restriction hy-
pothesis, claiming that semantically re-
lated terms extracted from a corpus tend
to be semantically coherent. We apply
this hypothesis to define a post-processing
module for the output of Espresso, a state
of the art relation extraction system, show-
ing that irrelevant and erroneous relations
can be filtered out by our module, in-
creasing the precision of the final output.
Results are confirmed by both quantita-
tive and qualitative analyses, showing that
very high precision can be reached.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999624632653061">
Relation extraction is a fundamental step in
many natural language processing applications such
as learning ontologies from texts (Buitelaar et
al., 2005) and Question Answering (Pasca and
Harabagiu, 2001).
The most common approach for acquiring con-
cepts, instances and relations is to harvest semantic
knowledge from texts. These techniques have been
largely explored and today they achieve reasonable
accuracy. Harvested lexical resources, such as con-
cept lists (Pantel and Lin, 2002), facts (Etzioni et
al., 2002) and semantic relations (Pantel and Pen-
nacchiotti, 2006) could be then successfully used in
different frameworks and applications.
The state of the art technology for relation extrac-
tion primarily relies on pattern-based approaches
(Snow et al., 2006). These techniques are based on
the recognition of the typical patterns that express
a particular relation in text (e.g. “X such as Y”
usually expresses an is-a relation). Yet, text-based
algorithms for relation extraction, in particular
pattern-based algorithms, still suffer from a number
of limitations due to complexities of natural lan-
guage, some of which we describe below.
Irrelevant relations. These are valid relations
that are not of interest in the domain at hand. For
example, in a political domain, “Condoleezza Rice
is a football fan” is not as relevant as “Condoleezza
Rice is the Secretary of State of the United States”.
Irrelevant relations are ubiquitous, and affect ontol-
ogy reliability, if used to populate it, as the relation
drives the wrong type of ontological knowledge.
Erroneous or false relations. These are particu-
larly harmful, since they directly affect algorithm
precision. A pattern-based relation extraction
algorithm is particularly likely to extract erroneous
relations if it uses generic patterns, which are
defined in (Pantel and Pennacchiotti, 2006) as
broad coverage, noisy patterns with high recall and
low precision (e.g. “X of Y” for part-of relation).
Harvesting algorithms either ignore generic patterns
(Hearst, 1992) (affecting system recall) or use man-
ually supervised filtering approaches (Girju et al.,
2006) or use completely unsupervised Web-filtering
methods (Pantel and Pennacchiotti, 2006). Yet,
these methods still do not sufficiently mitigate the
problem of erroneous relations.
Background knowledge. Another aspect that
makes relation harvesting difficult is related to the
</bodyText>
<page confidence="0.979533">
131
</page>
<note confidence="0.798075">
Proceedings of NAACL HLT 2007, pages 131–138,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.998644673469388">
nature of semantic relations: relations among enti-
ties are mostly paradigmatic (de Saussure, 1922),
and are usually established in absentia (i.e., they are
not made explicit in text). According to Eco’s posi-
tion (Eco, 1979), the background knowledge (e.g.
“persons are humans”) is often assumed by the
writer, and thus is not explicitly mentioned in text.
In some cases, such widely-known relations can be
captured by distributional similarity techniques but
not by pattern-based approaches.
Metaphorical language. Even when paradigmatic
relations are explicitly expressed in texts, it can
be very difficult to distinguish between facts and
metaphoric usage (e.g. the expression “My mind is
a pearl” occurs 17 times on the Web, but it is clear
that mind is not a pearl, at least from an ontological
perspective).
The considerations above outline some of the dif-
ficulties of taking a purely lexico-syntactic approach
to relation extraction. Pragmatic issues (background
knowledge and metaphorical language) and onto-
logical issues (irrelevant relation) can not be solved
at the syntactic level. Also, erroneous relations can
always arise. These considerations lead us to the
intuition that extraction can benefit from imposing
some additional constraints.
In this paper, we integrate Espresso with a lex-
ical distribution technique modeling semantic co-
herence through semantic domains (Magnini et al.,
2002). These are defined as common discourse top-
ics which demonstrate lexical coherence, such as
ECONOMICS or POLITICS. We explore whether se-
mantic domains can provide the needed additional
constraints to mitigate the acceptance of erroneous
relations. At the lexical level, semantic domains
identify clusters of (domain) paradigmatically re-
lated terms. We believe that the main advantage of
adopting semantic domains in relation extraction is
that relations are established mainly among terms in
the same Domain, while concepts belonging to dif-
ferent fields are mostly unrelated (Gliozzo, 2005),
as described in Section 2. For example, in a chem-
istry domain, an is-a will tend to relate only terms of
that domain (e.g., nitrogen is-a element), while out-
of-domain relations are likely to be erroneous e.g.,
driver is-a element.
By integrating pattern-based and distributional ap-
proaches we aim to capture the two characteristic
properties of semantic relations:
</bodyText>
<listItem confidence="0.905322933333333">
• Syntagmatic properties: if two terms X and
Y are in a given relation, they tend to co-
occur in texts, and are mostly connected by spe-
cific lexical-syntactic patterns (e.g., the patter
“X is a Y&amp;quot; connects terms in is-a relations).
This aspect is captured using a pattern-based
approach.
• Domain properties: if a semantic relation
among two terms X and Y holds, both X
and Y should belong to the same semantic
domain (i.e. they are semantically coherent),
where semantic domains are sets of terms
characterized by very similar distributional
properties in a (possibly domain specific)
corpus.
</listItem>
<bodyText confidence="0.996970083333333">
In Section 2, we develop the concept of semantic do-
main and an automatic acquisition procedure based
on Latent Semantic Analysis (LSA) and we provide
empirical evidence of the connection between rela-
tion extraction and domain modelling. Section 3 de-
scribes the Espresso system. Section 4 concerns our
integration of semantic domains and Espresso. In
Section 5, we evaluate the impact of our LSA do-
main restriction module on improving a state of the
art relation extraction system. In Section 6 we draw
some interesting research directions opened by our
work.
</bodyText>
<sectionHeader confidence="0.975736" genericHeader="method">
2 Semantic Domains
</sectionHeader>
<bodyText confidence="0.998583714285714">
Semantic domains are common areas of human
discussion, which demonstrate lexical coherence,
such as ECONOMICS, POLITICS, LAW, SCIENCE,
(Magnini et al., 2002). At the lexical level, se-
mantic domains identify clusters of (domain) related
lexical-concepts, i.e. sets of highly paradigmatically
related words also known as Semantic Fields.
In the literature, semantic domains have been
inferred from corpora by adopting term clustering
methodologies (Gliozzo, 2005), and have been used
for several NLP tasks, such as Text Categorization
and Ontology Learning (Gliozzo, 2006).
Semantic domains can be described by Domain
Models (DMs) (Gliozzo, 2005). A DM is a com-
</bodyText>
<page confidence="0.989901">
132
</page>
<bodyText confidence="0.99956625">
putational model for semantic domains, that repre-
sents domain information at the term level, by defin-
ing a set of term clusters. Each cluster represents a
Semantic Domain, i.e. a set of terms that often co-
occur in texts having similar topics. A DM is repre-
sented by a k x k0 rectangular matrix D, containing
the domain relevance for each term with respect to
each domain, as illustrated in Table 1.
</bodyText>
<table confidence="0.964876">
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
</table>
<tableCaption confidence="0.999784">
Table 1: Example of a Domain Model
</tableCaption>
<bodyText confidence="0.9986018">
Once a DM has been defined by the matrix D, the
Domain Space is a k0 dimensional space, in which
both texts and terms are associated to Domain Vec-
tors (DVs), i.e. vectors representing their domain
relevancies with respect to each domain. The DV
�t0i for the term ti E V is the ith row of D, where
V = {t1, t2, ... , tk} is the vocabulary of the corpus.
The domain similarity Od(ti, tj) among terms is then
estimated by the cosine among their corresponding
DVs in the Domain Space, defined as follows:
</bodyText>
<equation confidence="0.988536">
Od(ti, tj) = (�ti, Cj) (2)
✓(�ti, �ti)(�tj, 6)
</equation>
<bodyText confidence="0.998959125">
DMs can be acquired from texts in a completely
unsupervised way by exploiting a lexical coherence
assumption. To this end, term clustering algorithms
can be used with each cluster representing a Se-
mantic Domain. The degree of association among
terms and clusters, estimated by the learning algo-
rithm, provides a domain relevance function. For
our experiments we adopted a clustering strategy
based on LSA (Deerwester et al., 1990), following
the methodology described in (Gliozzo, 2005). The
input of the LSA process is a term-by-document ma-
trix T reporting the term frequencies in the whole
corpus for each term. The matrix is decomposed by
means of a Singular Value Decomposition (SVD),
identifying the principal components of T. This op-
eration is done off-line, and can be efficiently per-
formed on large corpora. SVD decomposes T into
three matrixes T — VEk0UT where Ek0 is the di-
agonal k x k matrix containing the highest k0 « k
eigenvalues of T on the diagonal, and all the re-
maining elements are 0. The parameter k0 is the
dimensionality of the domain and can be fixed in
advance1. Under this setting we define the domain
matrix DLSA2 as
</bodyText>
<equation confidence="0.872417">
&apos;✓
DLSA = INV Ek0 (1)
</equation>
<bodyText confidence="0.948804">
where IN is a diagonal matrix such that iNi�i =
�w0i is the ith row of the matrix V&apos;\/Ek0.
</bodyText>
<footnote confidence="0.990506666666667">
1It is not clear how to choose the right dimensionality. In
our experiments we used 100 dimensions.
2Details of this operation can be found in (Gliozzo, 2005).
</footnote>
<figureCaption confidence="0.970544">
Figure 1: Probability of finding paradigmatic rela-
tions
</figureCaption>
<bodyText confidence="0.999896176470588">
The main advantage of adopting semantic do-
mains for relation extraction is that they allow us to
impose a domain restriction on the set of candidate
pairs of related terms. In fact, semantic relations can
be established mainly among terms in the same Se-
mantic Domain, while concepts belonging to differ-
ent fields are mostly unrelated.
To show the validity of the domain restriction we
conducted a preliminary experiment, contrasting the
probability for two words to be related in Word-
Net (Magnini and Cavagli`a, 2000) with their domain
similarity, measured in the Domain Space induced
from the British National Corpus. In particular, for
each couple of words, we estimated the domain sim-
ilarity, and we collected word pairs in sets charac-
terized by different ranges of similarity (e.g. all the
pairs between 0.8 and 0.9). Then we estimated the
</bodyText>
<equation confidence="0.4073495">
1 and
�h ��0 ��0
</equation>
<page confidence="0.988876">
133
</page>
<bodyText confidence="0.999968">
probability of each couple of words in different sets
to be linked by a semantic relation in WordNet, such
as synonymy, hyperonymy, co-hyponymy and do-
main in WordNet Domains (Magnini et al., 2002).
Results in Figure 1 show a monotonic crescent rela-
tion between these two quantities. In particular the
probability for two words to be related tends to 0
when their similarity is negative (i.e., they are not
domain related), supporting the basic hypothesis of
this work. In Section 4 we will show that this prop-
erty can be used to improve the overall performances
of the relation extraction algorithm.
</bodyText>
<sectionHeader confidence="0.993506" genericHeader="method">
3 The pattern-based Espresso system
</sectionHeader>
<bodyText confidence="0.999914791666667">
Espresso (Pantel and Pennacchiotti, 2006) is a
corpus-based general purpose, broad, and accurate
relation extraction algorithm requiring minimal su-
pervision, whose core is based on the framework
adopted in (Hearst, 1992). Espresso introduces two
main innovations that guarantee high performance:
(i) a principled measure for estimating the reliabil-
ity of relational patterns and instances; (ii) an algo-
rithm for exploiting generic patterns. Generic pat-
terns are broad coverage noisy patterns (high recall
and low precision), e.g. “X of Y” for the part-of re-
lation. As underlined in the introduction, previous
algorithms either required significant manual work
to make use of generic patterns, or simply ignore
them. Espresso exploits an unsupervised Web-based
filtering method to detect generic patterns and to dis-
tinguish their correct and incorrect instances.
Given a specific relation (e.g. is-a) and a POS-
tagged corpus, Espresso takes as input few seed
instances (e.g. nitrogen is-a element) or seed surface
patterns (e.g. X/NN such/JJ as/IN Y/NN). It then
incrementally learns new patterns and instances
by iterating on the following three phases, until a
specific stop condition is met (i.e., new patterns are
below a pre-defined threshold of reliability).
Pattern Induction. Given an input set of seed
instances I, Espresso infers new patterns connecting
as many instances as possible in the given corpus.
To do so, Espresso uses a slight modification of the
state of the art algorithm described in (Ravichandran
and Hovy, 2002). For each instance in input, the
sentences containing it are first retrieved and then
generalized, by replacing term expressions with a
terminological label using regular expressions on
the POS-tags. This generalization allows to ease
the problem of data sparseness in small corpora.
Unfortunately, as patterns become more generic,
they are more prone to low precision.
Pattern Ranking and Selection. Espresso ranks
all extracted patterns using a reliability measure rπ
and discards all but the top-k P patterns, where k is
set to the number of patterns from the previous iter-
ation plus one. rπ captures the intuition that a reli-
able pattern is one that is both highly precise and one
that extracts many instances. rπ is formally defined
as the average strength of association between a pat-
tern p and each input instance i in I, weighted by the
reliability rι of the instance i (described later):
</bodyText>
<equation confidence="0.98280675">
Cpmi(i,p) ∗ r
EiEI maxpmi ιl (2
)
|I|
</equation>
<bodyText confidence="0.9681795">
where pmi(i, p) is the pointwise mutual information
(pmi) between i and p (estimated with Maximum
Likelihood Estimation), and maxpmi is the maxi-
mum pmi between all patterns and all instances.
Instance Extraction, Ranking, Selection.
Espresso extracts from the corpus the set of in-
stances I matching the patterns in P. In this phase
generic patterns are detected, and their instances
are filtered, using a technique described in detail in
(Pantel and Pennacchiotti, 2006). Instances are then
ranked using a reliability measure rι, similar to that
adopted for patterns. A reliable instance should be
highly associated with as many reliable patterns as
possible:
</bodyText>
<equation confidence="0.998942">
EpEP 1 maxpmi ) rπ(i)
rι(i) = |P|
</equation>
<bodyText confidence="0.999558166666667">
Finally, the best scoring instances are selected for
the following iteration. If the number of extracted
instances is too low (as often happens in small
corpora) Espresso enters an expansion phase, in
which instances are expanded by using web based
and syntactic techniques.
</bodyText>
<equation confidence="0.980899">
rπ(p) =
</equation>
<page confidence="0.909071">
134
</page>
<bodyText confidence="0.990280913043478">
The output Espresso is a list of instances
i = (X, Y ) E I, ranked according to r,,(i). This
score accounts for the syntagmatic similarity be-
tween X and Y , i.e., how strong is the co-occurrence
of X and Y in texts with a given pattern p.
A key role in the Espresso algorithm is played
by the reliability measures. The accuracy of the
whole extraction process is in fact highly sensitive
to the ranking of patterns and instances because, at
each iteration, only the best scoring entities are re-
tained. For instance, if an erroneous instance is se-
lected after the first iteration, it could in theory af-
fect the following pattern extraction phase and cause
drift in consequent iterations. This issue is criti-
cal for generic patterns (where precision is still a
problem, even with Web-based filtering), and could
sometimes also affect non-generic patterns.
It would be then useful to integrate Espresso with
a technique able to retain only very precise in-
stances, without compromising recall. As syntag-
matic strategies are already in place, another strategy
is needed. In the next Section, we show how this can
be achieved using instance domain information.
</bodyText>
<sectionHeader confidence="0.99221" genericHeader="method">
4 Integrating syntagmatic and domain
information
</sectionHeader>
<bodyText confidence="0.999978444444445">
The strategy of integrating syntagmatic and do-
main information has demonstrated to be fruitful in
many NLP tasks, such as Word Sense Disambigua-
tion and open domain Ontology Learning (Gliozzo,
2006). According to the structural view (de Saus-
sure, 1922), both aspects contribute to determine
the linguistic value (i.e. the meaning) of words:
the meaning of lexical constituents is determined
by a complex network of semantic relations among
words. This suggests that relation extraction can
benefit from accounting for both syntagmatic and
domain aspects at the same time.
To demonstrate the validity of this claim we can
explore many different integration schemata. For ex-
ample we can restrict the search space (i.e. the set of
candidate instances) to the set of all those terms be-
longing to the same domain. Another possibility is
to exploit a similarity metric for domain relatedness
to re-rank the output instances I of Espresso, hoping
that the top ranked ones will mostly be those which
are correct. One advantage of this latter method-
ology is that it can be applied to the output of any
relation extraction system without any modification
to the system itself. In addition, this methodology
can be evaluated by adopting standard Information
Retrieval (IR) measures, such as mean average pre-
cision (see Section 5). Because of these advantages,
we decided to adopt the re-ranking procedure.
The procedure is defined as follows: each in-
stance extracted by Espresso is assigned a Domain
Similarity score Od(X, Y ) estimated in the domain
space according to Equation 2; a higher score is
then assigned to the instances that tend to co-occur
in the same documents in the corpus. For exam-
ple, the candidate instances ethanol is-a nonaro-
matic alcohol has a higher score than ethanol is-a
something, as ethanol and alcohol are both from the
chemistry domain, while something is a generic term
and is thus not associated to any domain.
Instances are then re-ranked according to
Od(X,Y), which is used as the new index of
reliability instead of the original reliability scores
of Espresso. In Subsection 5.2 we will show that
the re-ranking technique improves the original
reliability scores of Espresso.
</bodyText>
<sectionHeader confidence="0.996743" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.99984375">
In this Section we evaluate the benefits of applying
the domain information to relation extraction (ESP-
LSA), by measuring the improvements of Espresso
due to domain based re-ranking.
</bodyText>
<subsectionHeader confidence="0.991549">
5.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.9999242">
As a baseline system, we used the ESP- implemen-
tation of Espresso described in (Pantel and Pennac-
chiotti, 2006). ESP- is a fully functioning Espresso
system, without the generic pattern filtering module
(ESP+). We decided to use ESP- for two main rea-
sons. First, the manual evaluation process would
have been too time consuming, as ESP+ extracts
thousands of relations. Also, the small scale experi-
ment for EXP- allows us to better analyse and com-
pare the results.
To perform the re-ranking operation, we acquired
a Domain Model from the input corpus itself. To this
aim we performed a SVD of the term by document
matrix T describing the input corpus, indexing all
the candidate terms recognized by Espresso.
</bodyText>
<page confidence="0.997581">
135
</page>
<bodyText confidence="0.999784916666667">
As an evaluation benchmark, we adopted the
same instance sets extracted by ESP- in the ex-
periment described in (Pantel and Pennacchiotti,
2006). We used an input corpus of 313,590 words,
a college chemistry textbook (Brown et al. 2003),
pre-processed using the Alembic Workbench POS-
tagger (Day et al. 1997). We considered the fol-
lowing relations: is-a, part-of, reaction (a relation
of chemical reaction among chemical entities) and
production (a process or chemical element/object
producing a result). ESP- extracted 200 is-a, 111
part-of, 40 reaction and 196 production instances.
</bodyText>
<subsectionHeader confidence="0.999783">
5.2 Quantitative Analysis
</subsectionHeader>
<bodyText confidence="0.999961909090909">
The experimental evaluation compared the accuracy
of the ranked set of instances extracted by ESP- with
the re-ranking produced on these instances by ESP-
LSA. By analogy to IR, we are interested in ex-
tracting positive instances (i.e. semantically related
words). Accordingly, we utilize the standard defi-
nitions of precision and recall typically used in IR .
Table 2 reports the Mean Average Precision obtained
by both ESP- and ESP-LSA on the extracted rela-
tions, showing the substantial improvements on all
the relations due to domain based re-ranking.
</bodyText>
<table confidence="0.6582726">
ESP- ESP-LSA
is-a 0.54 0.75 (+0.21)
part-of 0.65 0.82 (+0.17)
react 0.75 0.82 (+0.07)
produce 0.55 0.62 (+0.07)
</table>
<tableCaption confidence="0.6153975">
Table 2: Mean Average Precision reported by ESP-
and ESP-LSA
</tableCaption>
<bodyText confidence="0.9915">
Figures 2, 3, 4 and 5 report the precision/recall
curves obtained for each relation, estimated by mea-
suring the precision / recall at each point of the
ranked list. Results show that precision is very high
especially for the top ranked relations extracted by
ESP-LSA. Precision reaches the upper bound for the
top ranked part of the part-of relation, while it is
close to 0.9 for the is-a relation. In all cases, the
precision reported by the ESP-LSA system surpass
those of the ESP- system at all recall points.
</bodyText>
<subsectionHeader confidence="0.999283">
5.3 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.994361">
Table 3 shows the best scoring instances for ESP-
and ESP-LSA on the evaluated relations. Results
</bodyText>
<figureCaption confidence="0.95319">
Figure 2: Syntagmatic vs. Domain ranking for the
is-a relation
Figure 3: Syntagmatic vs. Domain ranking for the
produce relation
</figureCaption>
<bodyText confidence="0.9992309">
show that ESP-LSA tends to assign a much lower
score to erroneous instances, as compared to the
original Espresso reliability ranking. For exam-
ple for the part-of relation, the ESP- ranks the er-
roneous instance geometry part-of ion in 23th po-
sition, while ESP-LSA re-ranks it in 92nd. In
this case, a lower score is assigned because ge-
ometry is not particularly tied to the domain of
chemistry. Also, ESP-LSA tends to penalize in-
stances derived from parsing/tokenization errors:
</bodyText>
<page confidence="0.997551">
136
</page>
<figureCaption confidence="0.90762825">
Figure 4: Syntagmatic vs. Domain ranking for the
part-of relation
Figure 5: Syntagmatic vs. Domain ranking for the
react relation
</figureCaption>
<bodyText confidence="0.99962465">
] binary hydrogen compounds hydrogen react ele-
ments is 16th for ESP-, while in the last tenth of
the ESP-LSA. In addition, out-of-domain relations
are successfully interpreted by ESP-LSA. For ex-
ample, the instance sentences part-of exceptions is
a possibly correct relation, but unrelated to the do-
main, as an exception in chemistry has nothing to
do with sentences. This instance lies at the bottom
of the ESP-LSA ranking, while is in the middle of
ESP- list. Also, low ranked and correct relations ex-
tracted by ESP- emerge with ESP-LSA. For exam-
ple, magnesium metal react elemental oxygen lies at
the end of ESP- rank, as there are not enough syntag-
matic evidence (co-occurrences) that let the instance
emerge. The domain analysis of ESP-LSA promotes
this instance to the 2nd rank position. However, in
few cases, the strategy adopted by ESP-LSA tends
to promote erroneous instances (e.g. high voltage
produce voltage). Yet, results show that these are
isolated cases.
</bodyText>
<sectionHeader confidence="0.994722" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999986735294118">
In this paper, we propose the domain restriction hy-
pothesis, claiming that semantically related terms
extracted from a corpus tend to be semantically co-
herent. Applying this hypothesis, we presented a
new method to improve the precision of pattern-
based relation extraction algorithms, where the inte-
gration of domain information allows the system to
filter out many irrelevant relations, erroneous can-
didate pairs and metaphorical language relational
expressions, while capturing the assumed knowl-
edge required to discover paradigmatic associations
among terms. Experimental evidences supports this
claim both qualitatively and quantitatively, opening
a promising research direction, that we plan to ex-
plore much more in depth. In the future, we plan
to compare LSA to other term similarity measures,
to train the LSA model on large open domain cor-
pora and to apply our technique to both generic and
specific corpora in different domains. We want also
to increase the level of integration of the LSA tech-
nique in the Espresso algorithm, by using LSA as an
alternative reliability measure at each iteration. We
will also explore the domain restriction property of
semantic domains to develop open domain ontology
learning systems, as proposed in (Gliozzo, 2006).
The domain restriction hypothesis has potential
to greatly impact many applications where match-
ing textual expressions is a primary component. It is
our hope that by combining existing ranking strate-
gies in applications such as information retrieval,
question answering, information extraction and doc-
ument classification, with knowledge of the coher-
ence of the underlying text, one will see significant
improvements in matching accuracy.
</bodyText>
<page confidence="0.990317">
137
</page>
<table confidence="0.999544705882353">
Relation ESP- ESP - LSA
X is-a Y Aluminum; metal F ; electronegative atoms
nitride ion ; strong Br O ; electronegative atoms
heat flow ; calorimeter NaCN ; cyanide salt
complete ionic equation; spectator NaCN ; cyanide salts
X part-of Y elements ; compound amino acid building blocks ; tripeptide
composition; substance acid building blocks ; tripeptide
blocks ; tripeptide powdered zinc metal ; battery
elements ; sodium chloride building blocks ; tripeptide
X react Y hydrazine ; water magnesium metal ; elemental oxygen
magnesium metal; hydrochloric acid nitrogen; ammonia
magnesium; oxygen sodium metal ; chloride
magnesium metal ; acid carbon dioxide; methane
X produce Y bromine ; bromide high voltage ; voltage
oxygen; oxide reactions ; reactions
common fuels ; dioxide dr jekyll ; hyde
kidneys ; stones yellow pigments ; green pigment
</table>
<tableCaption confidence="0.999823">
Table 3: Top scoring relations extracted by ESP- and ESP-LSA.
</tableCaption>
<sectionHeader confidence="0.999457" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9988998">
Thanks to Roberto Basili for his precious comments,
suggestions and support. Alfio Gliozzo was sup-
ported by the OntoText project, funded by the Au-
tonomous Province of Trento under the FUP-2004,
and the FIRB founded project N.RBIN045PXH.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999821407407408">
P. Buitelaar, P. Cimiano, and B. Magnini. 2005. On-
tology learning from texts: methods, evaluation and
applications. IOS Press.
F. de Saussure. 1922. Cours de linguistique g´en´erale.
Payot, Paris.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
U. Eco. 1979. Lector in fabula. Bompiani.
O. Etzioni, M.J. Cafarella, D. Downey, A.-M
A.M. Popescu, T. Shaked, S. Soderland, D.S.
Weld, and A. Yates. 2002. Unsupervised named-
entity extraction from the web: An experimental
study. Artificial Intelligence, 165(1):91–143.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Learn-
ing semantic constraints for the automatic discovery of
part-whole relations. In Proceedings of HLT/NAACL-
03, pages 80–87, Edmonton, Canada, July.
A. Gliozzo. 2005. Semantic Domains in Computational
Linguistics. Ph.D. thesis, University of Trento.
A. Gliozzo. 2006. The god model. In Proceedings of
EACL.
M.A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics.
Nantes, France.
B. Magnini and G. Cavagli`a. 2000. Integrating subject
field codes into WordNet. In Proceedings of LREC-
2000, pages 1413–1418, Athens, Greece, June.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359–373.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proceedings of ACM Conference on
Knowledge Discovery and Data Mining, pages 613–
619.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In ACL-COLING-06, pages 113–
120, Sydney, Australia.
M. Pasca and S. Harabagiu. 2001. The informative role
of wordnet in open-domain question answering. In
Proceedings of NAACL-01 Workshop on WordNet and
Other Lexical Resources, pages 138–143, Pittsburgh,
PA.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of ACL-02, pages 41–47, Philadelphia, PA.
R. Snow, D. Jurafsky, and A.Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
Proceedings of the ACL/COLING-06, pages 801–808,
Sydney, Australia.
</reference>
<page confidence="0.997343">
138
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.908258">
<title confidence="0.9964845">The Domain Restriction Relating Term Similarity and Semantic Consistency</title>
<author confidence="0.999426">Alfio Massimiliano Gliozzo Marco Pennacchiotti Patrick Pantel</author>
<affiliation confidence="0.99999">ITC-irst University of Rome Tor Vergata USC, Information Sciences Institute</affiliation>
<address confidence="0.996837">Trento, Italy Rome, Italy Marina del Rey, CA</address>
<email confidence="0.924935">gliozzo@itc.itpennacchiotti@info.uniroma2.itpantel@isi.edu</email>
<abstract confidence="0.999456266666667">In this paper, we empirically demonstrate what we call the domain restriction hypothesis, claiming that semantically related terms extracted from a corpus tend to be semantically coherent. We apply this hypothesis to define a post-processing for the output of a state of the art relation extraction system, showing that irrelevant and erroneous relations can be filtered out by our module, increasing the precision of the final output. Results are confirmed by both quantitative and qualitative analyses, showing that very high precision can be reached.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
<author>P Cimiano</author>
<author>B Magnini</author>
</authors>
<title>Ontology learning from texts: methods, evaluation and applications.</title>
<date>2005</date>
<publisher>IOS Press.</publisher>
<contexts>
<context position="1078" citStr="Buitelaar et al., 2005" startWordPosition="150" endWordPosition="153">related terms extracted from a corpus tend to be semantically coherent. We apply this hypothesis to define a post-processing module for the output of Espresso, a state of the art relation extraction system, showing that irrelevant and erroneous relations can be filtered out by our module, increasing the precision of the final output. Results are confirmed by both quantitative and qualitative analyses, showing that very high precision can be reached. 1 Introduction Relation extraction is a fundamental step in many natural language processing applications such as learning ontologies from texts (Buitelaar et al., 2005) and Question Answering (Pasca and Harabagiu, 2001). The most common approach for acquiring concepts, instances and relations is to harvest semantic knowledge from texts. These techniques have been largely explored and today they achieve reasonable accuracy. Harvested lexical resources, such as concept lists (Pantel and Lin, 2002), facts (Etzioni et al., 2002) and semantic relations (Pantel and Pennacchiotti, 2006) could be then successfully used in different frameworks and applications. The state of the art technology for relation extraction primarily relies on pattern-based approaches (Snow </context>
</contexts>
<marker>Buitelaar, Cimiano, Magnini, 2005</marker>
<rawString>P. Buitelaar, P. Cimiano, and B. Magnini. 2005. Ontology learning from texts: methods, evaluation and applications. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F de Saussure</author>
</authors>
<title>Cours de linguistique g´en´erale.</title>
<date>1922</date>
<location>Payot, Paris.</location>
<marker>de Saussure, 1922</marker>
<rawString>F. de Saussure. 1922. Cours de linguistique g´en´erale. Payot, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S Dumais</author>
<author>G Furnas</author>
<author>T Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science.</journal>
<contexts>
<context position="9124" citStr="Deerwester et al., 1990" startWordPosition="1413" endWordPosition="1416">he domain similarity Od(ti, tj) among terms is then estimated by the cosine among their corresponding DVs in the Domain Space, defined as follows: Od(ti, tj) = (�ti, Cj) (2) ✓(�ti, �ti)(�tj, 6) DMs can be acquired from texts in a completely unsupervised way by exploiting a lexical coherence assumption. To this end, term clustering algorithms can be used with each cluster representing a Semantic Domain. The degree of association among terms and clusters, estimated by the learning algorithm, provides a domain relevance function. For our experiments we adopted a clustering strategy based on LSA (Deerwester et al., 1990), following the methodology described in (Gliozzo, 2005). The input of the LSA process is a term-by-document matrix T reporting the term frequencies in the whole corpus for each term. The matrix is decomposed by means of a Singular Value Decomposition (SVD), identifying the principal components of T. This operation is done off-line, and can be efficiently performed on large corpora. SVD decomposes T into three matrixes T — VEk0UT where Ek0 is the diagonal k x k matrix containing the highest k0 « k eigenvalues of T on the diagonal, and all the remaining elements are 0. The parameter k0 is the d</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Eco</author>
</authors>
<title>Unsupervised namedentity extraction from the web: An experimental study.</title>
<date>1979</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<note>Lector in</note>
<contexts>
<context position="3672" citStr="Eco, 1979" startWordPosition="537" endWordPosition="538">etely unsupervised Web-filtering methods (Pantel and Pennacchiotti, 2006). Yet, these methods still do not sufficiently mitigate the problem of erroneous relations. Background knowledge. Another aspect that makes relation harvesting difficult is related to the 131 Proceedings of NAACL HLT 2007, pages 131–138, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics nature of semantic relations: relations among entities are mostly paradigmatic (de Saussure, 1922), and are usually established in absentia (i.e., they are not made explicit in text). According to Eco’s position (Eco, 1979), the background knowledge (e.g. “persons are humans”) is often assumed by the writer, and thus is not explicitly mentioned in text. In some cases, such widely-known relations can be captured by distributional similarity techniques but not by pattern-based approaches. Metaphorical language. Even when paradigmatic relations are explicitly expressed in texts, it can be very difficult to distinguish between facts and metaphoric usage (e.g. the expression “My mind is a pearl” occurs 17 times on the Web, but it is clear that mind is not a pearl, at least from an ontological perspective). The consid</context>
</contexts>
<marker>Eco, 1979</marker>
<rawString>U. Eco. 1979. Lector in fabula. Bompiani. O. Etzioni, M.J. Cafarella, D. Downey, A.-M A.M. Popescu, T. Shaked, S. Soderland, D.S. Weld, and A. Yates. 2002. Unsupervised namedentity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>A Badulescu</author>
<author>D Moldovan</author>
</authors>
<title>Learning semantic constraints for the automatic discovery of part-whole relations.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL03,</booktitle>
<pages>80--87</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="3049" citStr="Girju et al., 2006" startWordPosition="447" endWordPosition="450">relation drives the wrong type of ontological knowledge. Erroneous or false relations. These are particularly harmful, since they directly affect algorithm precision. A pattern-based relation extraction algorithm is particularly likely to extract erroneous relations if it uses generic patterns, which are defined in (Pantel and Pennacchiotti, 2006) as broad coverage, noisy patterns with high recall and low precision (e.g. “X of Y” for part-of relation). Harvesting algorithms either ignore generic patterns (Hearst, 1992) (affecting system recall) or use manually supervised filtering approaches (Girju et al., 2006) or use completely unsupervised Web-filtering methods (Pantel and Pennacchiotti, 2006). Yet, these methods still do not sufficiently mitigate the problem of erroneous relations. Background knowledge. Another aspect that makes relation harvesting difficult is related to the 131 Proceedings of NAACL HLT 2007, pages 131–138, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics nature of semantic relations: relations among entities are mostly paradigmatic (de Saussure, 1922), and are usually established in absentia (i.e., they are not made explicit in text). According to Eco</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2006</marker>
<rawString>R. Girju, A. Badulescu, and D. Moldovan. 2006. Learning semantic constraints for the automatic discovery of part-whole relations. In Proceedings of HLT/NAACL03, pages 80–87, Edmonton, Canada, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
</authors>
<date>2005</date>
<booktitle>Semantic Domains in Computational Linguistics. Ph.D. thesis,</booktitle>
<institution>University of Trento.</institution>
<contexts>
<context position="5443" citStr="Gliozzo, 2005" startWordPosition="800" endWordPosition="801">s (Magnini et al., 2002). These are defined as common discourse topics which demonstrate lexical coherence, such as ECONOMICS or POLITICS. We explore whether semantic domains can provide the needed additional constraints to mitigate the acceptance of erroneous relations. At the lexical level, semantic domains identify clusters of (domain) paradigmatically related terms. We believe that the main advantage of adopting semantic domains in relation extraction is that relations are established mainly among terms in the same Domain, while concepts belonging to different fields are mostly unrelated (Gliozzo, 2005), as described in Section 2. For example, in a chemistry domain, an is-a will tend to relate only terms of that domain (e.g., nitrogen is-a element), while outof-domain relations are likely to be erroneous e.g., driver is-a element. By integrating pattern-based and distributional approaches we aim to capture the two characteristic properties of semantic relations: • Syntagmatic properties: if two terms X and Y are in a given relation, they tend to cooccur in texts, and are mostly connected by specific lexical-syntactic patterns (e.g., the patter “X is a Y&amp;quot; connects terms in is-a relations). Th</context>
<context position="7442" citStr="Gliozzo, 2005" startWordPosition="1115" endWordPosition="1116">mproving a state of the art relation extraction system. In Section 6 we draw some interesting research directions opened by our work. 2 Semantic Domains Semantic domains are common areas of human discussion, which demonstrate lexical coherence, such as ECONOMICS, POLITICS, LAW, SCIENCE, (Magnini et al., 2002). At the lexical level, semantic domains identify clusters of (domain) related lexical-concepts, i.e. sets of highly paradigmatically related words also known as Semantic Fields. In the literature, semantic domains have been inferred from corpora by adopting term clustering methodologies (Gliozzo, 2005), and have been used for several NLP tasks, such as Text Categorization and Ontology Learning (Gliozzo, 2006). Semantic domains can be described by Domain Models (DMs) (Gliozzo, 2005). A DM is a com132 putational model for semantic domains, that represents domain information at the term level, by defining a set of term clusters. Each cluster represents a Semantic Domain, i.e. a set of terms that often cooccur in texts having similar topics. A DM is represented by a k x k0 rectangular matrix D, containing the domain relevance for each term with respect to each domain, as illustrated in Table 1.</context>
<context position="9180" citStr="Gliozzo, 2005" startWordPosition="1422" endWordPosition="1423">he cosine among their corresponding DVs in the Domain Space, defined as follows: Od(ti, tj) = (�ti, Cj) (2) ✓(�ti, �ti)(�tj, 6) DMs can be acquired from texts in a completely unsupervised way by exploiting a lexical coherence assumption. To this end, term clustering algorithms can be used with each cluster representing a Semantic Domain. The degree of association among terms and clusters, estimated by the learning algorithm, provides a domain relevance function. For our experiments we adopted a clustering strategy based on LSA (Deerwester et al., 1990), following the methodology described in (Gliozzo, 2005). The input of the LSA process is a term-by-document matrix T reporting the term frequencies in the whole corpus for each term. The matrix is decomposed by means of a Singular Value Decomposition (SVD), identifying the principal components of T. This operation is done off-line, and can be efficiently performed on large corpora. SVD decomposes T into three matrixes T — VEk0UT where Ek0 is the diagonal k x k matrix containing the highest k0 « k eigenvalues of T on the diagonal, and all the remaining elements are 0. The parameter k0 is the dimensionality of the domain and can be fixed in advance1</context>
</contexts>
<marker>Gliozzo, 2005</marker>
<rawString>A. Gliozzo. 2005. Semantic Domains in Computational Linguistics. Ph.D. thesis, University of Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
</authors>
<title>The god model.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="7551" citStr="Gliozzo, 2006" startWordPosition="1132" endWordPosition="1133">ions opened by our work. 2 Semantic Domains Semantic domains are common areas of human discussion, which demonstrate lexical coherence, such as ECONOMICS, POLITICS, LAW, SCIENCE, (Magnini et al., 2002). At the lexical level, semantic domains identify clusters of (domain) related lexical-concepts, i.e. sets of highly paradigmatically related words also known as Semantic Fields. In the literature, semantic domains have been inferred from corpora by adopting term clustering methodologies (Gliozzo, 2005), and have been used for several NLP tasks, such as Text Categorization and Ontology Learning (Gliozzo, 2006). Semantic domains can be described by Domain Models (DMs) (Gliozzo, 2005). A DM is a com132 putational model for semantic domains, that represents domain information at the term level, by defining a set of term clusters. Each cluster represents a Semantic Domain, i.e. a set of terms that often cooccur in texts having similar topics. A DM is represented by a k x k0 rectangular matrix D, containing the domain relevance for each term with respect to each domain, as illustrated in Table 1. MEDICINE COMPUTER SCIENCE HIV 1 0 AIDS 1 0 virus 0.5 0.5 laptop 0 1 Table 1: Example of a Domain Model Once </context>
<context position="16521" citStr="Gliozzo, 2006" startWordPosition="2635" endWordPosition="2636"> filtering), and could sometimes also affect non-generic patterns. It would be then useful to integrate Espresso with a technique able to retain only very precise instances, without compromising recall. As syntagmatic strategies are already in place, another strategy is needed. In the next Section, we show how this can be achieved using instance domain information. 4 Integrating syntagmatic and domain information The strategy of integrating syntagmatic and domain information has demonstrated to be fruitful in many NLP tasks, such as Word Sense Disambiguation and open domain Ontology Learning (Gliozzo, 2006). According to the structural view (de Saussure, 1922), both aspects contribute to determine the linguistic value (i.e. the meaning) of words: the meaning of lexical constituents is determined by a complex network of semantic relations among words. This suggests that relation extraction can benefit from accounting for both syntagmatic and domain aspects at the same time. To demonstrate the validity of this claim we can explore many different integration schemata. For example we can restrict the search space (i.e. the set of candidate instances) to the set of all those terms belonging to the sa</context>
<context position="24427" citStr="Gliozzo, 2006" startWordPosition="3910" endWordPosition="3911">ening a promising research direction, that we plan to explore much more in depth. In the future, we plan to compare LSA to other term similarity measures, to train the LSA model on large open domain corpora and to apply our technique to both generic and specific corpora in different domains. We want also to increase the level of integration of the LSA technique in the Espresso algorithm, by using LSA as an alternative reliability measure at each iteration. We will also explore the domain restriction property of semantic domains to develop open domain ontology learning systems, as proposed in (Gliozzo, 2006). The domain restriction hypothesis has potential to greatly impact many applications where matching textual expressions is a primary component. It is our hope that by combining existing ranking strategies in applications such as information retrieval, question answering, information extraction and document classification, with knowledge of the coherence of the underlying text, one will see significant improvements in matching accuracy. 137 Relation ESP- ESP - LSA X is-a Y Aluminum; metal F ; electronegative atoms nitride ion ; strong Br O ; electronegative atoms heat flow ; calorimeter NaCN ;</context>
</contexts>
<marker>Gliozzo, 2006</marker>
<rawString>A. Gliozzo. 2006. The god model. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics.</booktitle>
<location>Nantes, France.</location>
<contexts>
<context position="2954" citStr="Hearst, 1992" startWordPosition="435" endWordPosition="436">elations are ubiquitous, and affect ontology reliability, if used to populate it, as the relation drives the wrong type of ontological knowledge. Erroneous or false relations. These are particularly harmful, since they directly affect algorithm precision. A pattern-based relation extraction algorithm is particularly likely to extract erroneous relations if it uses generic patterns, which are defined in (Pantel and Pennacchiotti, 2006) as broad coverage, noisy patterns with high recall and low precision (e.g. “X of Y” for part-of relation). Harvesting algorithms either ignore generic patterns (Hearst, 1992) (affecting system recall) or use manually supervised filtering approaches (Girju et al., 2006) or use completely unsupervised Web-filtering methods (Pantel and Pennacchiotti, 2006). Yet, these methods still do not sufficiently mitigate the problem of erroneous relations. Background knowledge. Another aspect that makes relation harvesting difficult is related to the 131 Proceedings of NAACL HLT 2007, pages 131–138, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics nature of semantic relations: relations among entities are mostly paradigmatic (de Saussure, 1922), and a</context>
<context position="11888" citStr="Hearst, 1992" startWordPosition="1884" endWordPosition="1885"> relation between these two quantities. In particular the probability for two words to be related tends to 0 when their similarity is negative (i.e., they are not domain related), supporting the basic hypothesis of this work. In Section 4 we will show that this property can be used to improve the overall performances of the relation extraction algorithm. 3 The pattern-based Espresso system Espresso (Pantel and Pennacchiotti, 2006) is a corpus-based general purpose, broad, and accurate relation extraction algorithm requiring minimal supervision, whose core is based on the framework adopted in (Hearst, 1992). Espresso introduces two main innovations that guarantee high performance: (i) a principled measure for estimating the reliability of relational patterns and instances; (ii) an algorithm for exploiting generic patterns. Generic patterns are broad coverage noisy patterns (high recall and low precision), e.g. “X of Y” for the part-of relation. As underlined in the introduction, previous algorithms either required significant manual work to make use of generic patterns, or simply ignore them. Espresso exploits an unsupervised Web-based filtering method to detect generic patterns and to distingui</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M.A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th International Conference on Computational Linguistics. Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>G Cavagli`a</author>
</authors>
<title>Integrating subject field codes into WordNet.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC2000,</booktitle>
<pages>1413--1418</pages>
<location>Athens, Greece,</location>
<marker>Magnini, Cavagli`a, 2000</marker>
<rawString>B. Magnini and G. Cavagli`a. 2000. Integrating subject field codes into WordNet. In Proceedings of LREC2000, pages 1413–1418, Athens, Greece, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>C Strapparava</author>
<author>G Pezzulo</author>
<author>A Gliozzo</author>
</authors>
<title>The role of domain information in word sense disambiguation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="4853" citStr="Magnini et al., 2002" startWordPosition="711" endWordPosition="714"> an ontological perspective). The considerations above outline some of the difficulties of taking a purely lexico-syntactic approach to relation extraction. Pragmatic issues (background knowledge and metaphorical language) and ontological issues (irrelevant relation) can not be solved at the syntactic level. Also, erroneous relations can always arise. These considerations lead us to the intuition that extraction can benefit from imposing some additional constraints. In this paper, we integrate Espresso with a lexical distribution technique modeling semantic coherence through semantic domains (Magnini et al., 2002). These are defined as common discourse topics which demonstrate lexical coherence, such as ECONOMICS or POLITICS. We explore whether semantic domains can provide the needed additional constraints to mitigate the acceptance of erroneous relations. At the lexical level, semantic domains identify clusters of (domain) paradigmatically related terms. We believe that the main advantage of adopting semantic domains in relation extraction is that relations are established mainly among terms in the same Domain, while concepts belonging to different fields are mostly unrelated (Gliozzo, 2005), as descr</context>
<context position="7138" citStr="Magnini et al., 2002" startWordPosition="1071" endWordPosition="1074">antic Analysis (LSA) and we provide empirical evidence of the connection between relation extraction and domain modelling. Section 3 describes the Espresso system. Section 4 concerns our integration of semantic domains and Espresso. In Section 5, we evaluate the impact of our LSA domain restriction module on improving a state of the art relation extraction system. In Section 6 we draw some interesting research directions opened by our work. 2 Semantic Domains Semantic domains are common areas of human discussion, which demonstrate lexical coherence, such as ECONOMICS, POLITICS, LAW, SCIENCE, (Magnini et al., 2002). At the lexical level, semantic domains identify clusters of (domain) related lexical-concepts, i.e. sets of highly paradigmatically related words also known as Semantic Fields. In the literature, semantic domains have been inferred from corpora by adopting term clustering methodologies (Gliozzo, 2005), and have been used for several NLP tasks, such as Text Categorization and Ontology Learning (Gliozzo, 2006). Semantic domains can be described by Domain Models (DMs) (Gliozzo, 2005). A DM is a com132 putational model for semantic domains, that represents domain information at the term level, b</context>
<context position="11228" citStr="Magnini et al., 2002" startWordPosition="1778" endWordPosition="1781">bability for two words to be related in WordNet (Magnini and Cavagli`a, 2000) with their domain similarity, measured in the Domain Space induced from the British National Corpus. In particular, for each couple of words, we estimated the domain similarity, and we collected word pairs in sets characterized by different ranges of similarity (e.g. all the pairs between 0.8 and 0.9). Then we estimated the 1 and �h ��0 ��0 133 probability of each couple of words in different sets to be linked by a semantic relation in WordNet, such as synonymy, hyperonymy, co-hyponymy and domain in WordNet Domains (Magnini et al., 2002). Results in Figure 1 show a monotonic crescent relation between these two quantities. In particular the probability for two words to be related tends to 0 when their similarity is negative (i.e., they are not domain related), supporting the basic hypothesis of this work. In Section 4 we will show that this property can be used to improve the overall performances of the relation extraction algorithm. 3 The pattern-based Espresso system Espresso (Pantel and Pennacchiotti, 2006) is a corpus-based general purpose, broad, and accurate relation extraction algorithm requiring minimal supervision, wh</context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2002</marker>
<rawString>B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo. 2002. The role of domain information in word sense disambiguation. Natural Language Engineering, 8(4):359–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>613--619</pages>
<contexts>
<context position="1410" citStr="Pantel and Lin, 2002" startWordPosition="199" endWordPosition="202">. Results are confirmed by both quantitative and qualitative analyses, showing that very high precision can be reached. 1 Introduction Relation extraction is a fundamental step in many natural language processing applications such as learning ontologies from texts (Buitelaar et al., 2005) and Question Answering (Pasca and Harabagiu, 2001). The most common approach for acquiring concepts, instances and relations is to harvest semantic knowledge from texts. These techniques have been largely explored and today they achieve reasonable accuracy. Harvested lexical resources, such as concept lists (Pantel and Lin, 2002), facts (Etzioni et al., 2002) and semantic relations (Pantel and Pennacchiotti, 2006) could be then successfully used in different frameworks and applications. The state of the art technology for relation extraction primarily relies on pattern-based approaches (Snow et al., 2006). These techniques are based on the recognition of the typical patterns that express a particular relation in text (e.g. “X such as Y” usually expresses an is-a relation). Yet, text-based algorithms for relation extraction, in particular pattern-based algorithms, still suffer from a number of limitations due to comple</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>P. Pantel and D. Lin. 2002. Discovering word senses from text. In Proceedings of ACM Conference on Knowledge Discovery and Data Mining, pages 613– 619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>M Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In ACL-COLING-06,</booktitle>
<pages>113--120</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1496" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="211" endWordPosition="215">ng that very high precision can be reached. 1 Introduction Relation extraction is a fundamental step in many natural language processing applications such as learning ontologies from texts (Buitelaar et al., 2005) and Question Answering (Pasca and Harabagiu, 2001). The most common approach for acquiring concepts, instances and relations is to harvest semantic knowledge from texts. These techniques have been largely explored and today they achieve reasonable accuracy. Harvested lexical resources, such as concept lists (Pantel and Lin, 2002), facts (Etzioni et al., 2002) and semantic relations (Pantel and Pennacchiotti, 2006) could be then successfully used in different frameworks and applications. The state of the art technology for relation extraction primarily relies on pattern-based approaches (Snow et al., 2006). These techniques are based on the recognition of the typical patterns that express a particular relation in text (e.g. “X such as Y” usually expresses an is-a relation). Yet, text-based algorithms for relation extraction, in particular pattern-based algorithms, still suffer from a number of limitations due to complexities of natural language, some of which we describe below. Irrelevant relations. The</context>
<context position="2779" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="407" endWordPosition="410">he domain at hand. For example, in a political domain, “Condoleezza Rice is a football fan” is not as relevant as “Condoleezza Rice is the Secretary of State of the United States”. Irrelevant relations are ubiquitous, and affect ontology reliability, if used to populate it, as the relation drives the wrong type of ontological knowledge. Erroneous or false relations. These are particularly harmful, since they directly affect algorithm precision. A pattern-based relation extraction algorithm is particularly likely to extract erroneous relations if it uses generic patterns, which are defined in (Pantel and Pennacchiotti, 2006) as broad coverage, noisy patterns with high recall and low precision (e.g. “X of Y” for part-of relation). Harvesting algorithms either ignore generic patterns (Hearst, 1992) (affecting system recall) or use manually supervised filtering approaches (Girju et al., 2006) or use completely unsupervised Web-filtering methods (Pantel and Pennacchiotti, 2006). Yet, these methods still do not sufficiently mitigate the problem of erroneous relations. Background knowledge. Another aspect that makes relation harvesting difficult is related to the 131 Proceedings of NAACL HLT 2007, pages 131–138, Roches</context>
<context position="11709" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="1856" endWordPosition="1859">nt sets to be linked by a semantic relation in WordNet, such as synonymy, hyperonymy, co-hyponymy and domain in WordNet Domains (Magnini et al., 2002). Results in Figure 1 show a monotonic crescent relation between these two quantities. In particular the probability for two words to be related tends to 0 when their similarity is negative (i.e., they are not domain related), supporting the basic hypothesis of this work. In Section 4 we will show that this property can be used to improve the overall performances of the relation extraction algorithm. 3 The pattern-based Espresso system Espresso (Pantel and Pennacchiotti, 2006) is a corpus-based general purpose, broad, and accurate relation extraction algorithm requiring minimal supervision, whose core is based on the framework adopted in (Hearst, 1992). Espresso introduces two main innovations that guarantee high performance: (i) a principled measure for estimating the reliability of relational patterns and instances; (ii) an algorithm for exploiting generic patterns. Generic patterns are broad coverage noisy patterns (high recall and low precision), e.g. “X of Y” for the part-of relation. As underlined in the introduction, previous algorithms either required signi</context>
<context position="14609" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="2316" endWordPosition="2319">on between a pattern p and each input instance i in I, weighted by the reliability rι of the instance i (described later): Cpmi(i,p) ∗ r EiEI maxpmi ιl (2 ) |I| where pmi(i, p) is the pointwise mutual information (pmi) between i and p (estimated with Maximum Likelihood Estimation), and maxpmi is the maximum pmi between all patterns and all instances. Instance Extraction, Ranking, Selection. Espresso extracts from the corpus the set of instances I matching the patterns in P. In this phase generic patterns are detected, and their instances are filtered, using a technique described in detail in (Pantel and Pennacchiotti, 2006). Instances are then ranked using a reliability measure rι, similar to that adopted for patterns. A reliable instance should be highly associated with as many reliable patterns as possible: EpEP 1 maxpmi ) rπ(i) rι(i) = |P| Finally, the best scoring instances are selected for the following iteration. If the number of extracted instances is too low (as often happens in small corpora) Espresso enters an expansion phase, in which instances are expanded by using web based and syntactic techniques. rπ(p) = 134 The output Espresso is a list of instances i = (X, Y ) E I, ranked according to r,,(i). T</context>
<context position="18856" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="3011" endWordPosition="3015">not associated to any domain. Instances are then re-ranked according to Od(X,Y), which is used as the new index of reliability instead of the original reliability scores of Espresso. In Subsection 5.2 we will show that the re-ranking technique improves the original reliability scores of Espresso. 5 Evaluation In this Section we evaluate the benefits of applying the domain information to relation extraction (ESPLSA), by measuring the improvements of Espresso due to domain based re-ranking. 5.1 Experimental Settings As a baseline system, we used the ESP- implementation of Espresso described in (Pantel and Pennacchiotti, 2006). ESP- is a fully functioning Espresso system, without the generic pattern filtering module (ESP+). We decided to use ESP- for two main reasons. First, the manual evaluation process would have been too time consuming, as ESP+ extracts thousands of relations. Also, the small scale experiment for EXP- allows us to better analyse and compare the results. To perform the re-ranking operation, we acquired a Domain Model from the input corpus itself. To this aim we performed a SVD of the term by document matrix T describing the input corpus, indexing all the candidate terms recognized by Espresso. 13</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In ACL-COLING-06, pages 113– 120, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>S Harabagiu</author>
</authors>
<title>The informative role of wordnet in open-domain question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL-01 Workshop on WordNet and Other Lexical Resources,</booktitle>
<pages>138--143</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1129" citStr="Pasca and Harabagiu, 2001" startWordPosition="157" endWordPosition="160"> semantically coherent. We apply this hypothesis to define a post-processing module for the output of Espresso, a state of the art relation extraction system, showing that irrelevant and erroneous relations can be filtered out by our module, increasing the precision of the final output. Results are confirmed by both quantitative and qualitative analyses, showing that very high precision can be reached. 1 Introduction Relation extraction is a fundamental step in many natural language processing applications such as learning ontologies from texts (Buitelaar et al., 2005) and Question Answering (Pasca and Harabagiu, 2001). The most common approach for acquiring concepts, instances and relations is to harvest semantic knowledge from texts. These techniques have been largely explored and today they achieve reasonable accuracy. Harvested lexical resources, such as concept lists (Pantel and Lin, 2002), facts (Etzioni et al., 2002) and semantic relations (Pantel and Pennacchiotti, 2006) could be then successfully used in different frameworks and applications. The state of the art technology for relation extraction primarily relies on pattern-based approaches (Snow et al., 2006). These techniques are based on the re</context>
</contexts>
<marker>Pasca, Harabagiu, 2001</marker>
<rawString>M. Pasca and S. Harabagiu. 2001. The informative role of wordnet in open-domain question answering. In Proceedings of NAACL-01 Workshop on WordNet and Other Lexical Resources, pages 138–143, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-02,</booktitle>
<pages>41--47</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="13203" citStr="Ravichandran and Hovy, 2002" startWordPosition="2085" endWordPosition="2088">OStagged corpus, Espresso takes as input few seed instances (e.g. nitrogen is-a element) or seed surface patterns (e.g. X/NN such/JJ as/IN Y/NN). It then incrementally learns new patterns and instances by iterating on the following three phases, until a specific stop condition is met (i.e., new patterns are below a pre-defined threshold of reliability). Pattern Induction. Given an input set of seed instances I, Espresso infers new patterns connecting as many instances as possible in the given corpus. To do so, Espresso uses a slight modification of the state of the art algorithm described in (Ravichandran and Hovy, 2002). For each instance in input, the sentences containing it are first retrieved and then generalized, by replacing term expressions with a terminological label using regular expressions on the POS-tags. This generalization allows to ease the problem of data sparseness in small corpora. Unfortunately, as patterns become more generic, they are more prone to low precision. Pattern Ranking and Selection. Espresso ranks all extracted patterns using a reliability measure rπ and discards all but the top-k P patterns, where k is set to the number of patterns from the previous iteration plus one. rπ capt</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>D. Ravichandran and E. Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of ACL-02, pages 41–47, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/COLING-06,</booktitle>
<pages>801--808</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1691" citStr="Snow et al., 2006" startWordPosition="241" endWordPosition="244">2005) and Question Answering (Pasca and Harabagiu, 2001). The most common approach for acquiring concepts, instances and relations is to harvest semantic knowledge from texts. These techniques have been largely explored and today they achieve reasonable accuracy. Harvested lexical resources, such as concept lists (Pantel and Lin, 2002), facts (Etzioni et al., 2002) and semantic relations (Pantel and Pennacchiotti, 2006) could be then successfully used in different frameworks and applications. The state of the art technology for relation extraction primarily relies on pattern-based approaches (Snow et al., 2006). These techniques are based on the recognition of the typical patterns that express a particular relation in text (e.g. “X such as Y” usually expresses an is-a relation). Yet, text-based algorithms for relation extraction, in particular pattern-based algorithms, still suffer from a number of limitations due to complexities of natural language, some of which we describe below. Irrelevant relations. These are valid relations that are not of interest in the domain at hand. For example, in a political domain, “Condoleezza Rice is a football fan” is not as relevant as “Condoleezza Rice is the Secr</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>R. Snow, D. Jurafsky, and A.Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of the ACL/COLING-06, pages 801–808, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>