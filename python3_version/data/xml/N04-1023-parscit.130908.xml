<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.997559">
Discriminative Reranking for Machine Translation
</title>
<author confidence="0.996715">
Libin Shen
</author>
<affiliation confidence="0.994418">
Dept. of Comp. &amp; Info. Science
Univ. of Pennsylvania
</affiliation>
<address confidence="0.790176">
Philadelphia, PA 19104
</address>
<email confidence="0.998598">
libin@seas.upenn.edu
</email>
<author confidence="0.97844">
Anoop Sarkar
</author>
<affiliation confidence="0.7946165">
School of Comp. Science
Simon Fraser Univ.
</affiliation>
<address confidence="0.865016">
Burnaby, BC V5A 1S6
</address>
<email confidence="0.995865">
anoop@cs.sfu.ca
</email>
<author confidence="0.99511">
Franz Josef Och
</author>
<affiliation confidence="0.9979805">
Info. Science Institute
Univ. of Southern California
</affiliation>
<address confidence="0.491066">
Marina del Rey, CA 90292
</address>
<email confidence="0.998884">
och@isi.edu
</email>
<sectionHeader confidence="0.993903" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999499375">
This paper describes the application of discrim-
inative reranking techniques to the problem of
machine translation. For each sentence in the
source language, we obtain from a baseline sta-
tistical machine translation system, a ranked -
best list of candidate translations in the target
language. We introduce two novel perceptron-
inspired reranking algorithms that improve on
the quality of machine translation over the
baseline system based on evaluation using the
BLEU metric. We provide experimental results
on the NIST 2003 Chinese-English large data
track evaluation. We also provide theoretical
analysis of our algorithms and experiments that
verify that our algorithms provide state-of-the-
art performance in machine translation.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999888413793103">
The noisy-channel model (Brown et al., 1990) has been
the foundation for statistical machine translation (SMT)
for over ten years. Recently so-called reranking tech-
niques, such as maximum entropy models (Och and Ney,
2002) and gradient methods (Och, 2003), have been ap-
plied to machine translation (MT), and have provided
significant improvements. In this paper, we introduce
two novel machine learning algorithms specialized for
the MT task.
Discriminative reranking algorithms have also con-
tributed to improvements in natural language parsing
and tagging performance. Discriminative reranking al-
gorithms used for these applications include Perceptron,
Boosting and Support Vector Machines (SVMs). In the
machine learning community, some novel discriminative
ranking (also called ordinal regression) algorithms have
been proposed in recent years. Based on this work, in
this paper, we will present some novel discriminative
reranking techniques applied to machine translation. The
reranking problem for natural language is neither a clas-
sification problem nor a regression problem, and under
certain conditions MT reranking turns out to be quite dif-
ferent from parse reranking.
In this paper, we consider the special issues of apply-
ing reranking techniques to the MT task and introduce
two perceptron-like reranking algorithms for MT rerank-
ing. We provide experimental results that show that the
proposed algorithms achieve start-of-the-art results on the
NIST 2003 Chinese-English large data track evaluation.
</bodyText>
<subsectionHeader confidence="0.754867">
1.1 Generative Models for MT
</subsectionHeader>
<bodyText confidence="0.99996564">
The seminal IBM models (Brown et al., 1990) were
the first to introduce generative models to the MT task.
The IBM models applied the sequence learning paradigm
well-known from Hidden Markov Models in speech
recognition to the problem of MT. The source and tar-
get sentences were treated as the observations, but the
alignments were treated as hidden information learned
from parallel texts using the EM algorithm. This source-
channel model treated the task of finding the probability
, where is the translation in the target (English)
language for a given source (foreign) sentence , as two
generative probability models: the language model
which is a generative probability over candidate transla-
tions and the translation model which is a gener-
ative conditional probability of the source sentence given
a candidate translation .
The lexicon of the single-word based IBM models does
not take word context into account. This means unlikely
alignments are being considered while training the model
and this also results in additional decoding complexity.
Several MT models were proposed as extensions of the
IBM models which used this intuition to add additional
linguistic constraints to decrease the decoding perplexity
and increase the translation quality.
Wang and Waibel (1998) proposed an SMT model
based on phrase-based alignments. Since their transla-
tion model reordered phrases directly, it achieved higher
accuracy for translation between languages with differ-
ent word orders. In (Och and Weber, 1998; Och et al.,
1999), a two-level alignment model was employed to uti-
lize shallow phrase structures: alignment between tem-
plates was used to handle phrase reordering, and word
alignments within a template were used to handle phrase
to phrase translation.
However, phrase level alignment cannot handle long
distance reordering effectively. Parse trees have also
been used in alignment models. Wu (1997) introduced
constraints on alignments using a probabilistic syn-
chronous context-free grammar restricted to Chomsky-
normal form. (Wu, 1997) was an implicit or self-
organizing syntax model as it did not use a Treebank. Ya-
mada and Knight (2001) used a statistical parser trained
using a Treebank in the source language to produce parse
trees and proposed a tree to string model for alignment.
Gildea (2003) proposed a tree to tree alignment model us-
ing output from a statistical parser in both source and tar-
get languages. The translation model involved tree align-
ments in which subtree cloning was used to handle cases
of reordering that were not possible in earlier tree-based
alignment models.
</bodyText>
<subsectionHeader confidence="0.863493">
1.2 Discriminative Models for MT
</subsectionHeader>
<bodyText confidence="0.999857846153846">
Och and Ney (2002) proposed a framework for MT based
on direct translation, using the conditional model
estimated using a maximum entropy model. A small
number of feature functions defined on the source and
target sentence were used to rerank the translations gen-
erated by a baseline MT system. While the total num-
ber of feature functions was small, each feature function
was a complex statistical model by itself, as for exam-
ple, the alignment template feature functions used in this
approach.
Och (2003) described the use of minimum error train-
ing directly optimizing the error rate on automatic MT
evaluation metrics such as BLEU. The experiments
showed that this approach obtains significantly better re-
sults than using the maximum mutual information cri-
terion on parameter estimation. This approach used the
same set of features as the alignment template approach
in (Och and Ney, 2002).
SMT Team (2003) also used minimum error training
as in Och (2003), but used a large number of feature func-
tions. More than 450 different feature functions were
used in order to improve the syntactic well-formedness
of MT output. By reranking a 1000-best list generated by
the baseline MT system from Och (2003), the BLEU (Pa-
pineni et al., 2001) score on the test dataset was improved
from 31.6% to 32.9%.
</bodyText>
<sectionHeader confidence="0.89654" genericHeader="introduction">
2 Ranking and Reranking
</sectionHeader>
<subsectionHeader confidence="0.87047">
2.1 Reranking for NLP tasks
</subsectionHeader>
<bodyText confidence="0.999285653846154">
Like machine translation, parsing is another field of natu-
ral language processing in which generative models have
been widely used. In recent years, reranking techniques,
especially discriminative reranking, have resulted in sig-
nificant improvements in parsing. Various machine learn-
ing algorithms have been employed in parse reranking,
such as Boosting (Collins, 2000), Perceptron (Collins and
Duffy, 2002) and Support Vector Machines (Shen and
Joshi, 2003). The reranking techniques have resulted in a
13.5% error reduction in labeled recall/precision over the
previous best generative parsing models. Discriminative
reranking methods for parsing typically use the notion of
a margin as the distance between the best candidate parse
and the rest of the parses. The reranking problem is re-
duced to a classification problem by using pairwise sam-
ples.
In (Shen and Joshi, 2004), we have introduced a new
perceptron-like ordinal regression algorithm for parse
reranking. In that algorithm, pairwise samples are used
for training and margins are defined as the distance be-
tween parses of different ranks. In addition, the uneven
margin technique has been used for the purpose of adapt-
ing ordinal regression to reranking tasks. In this paper,
we apply this algorithm to MT reranking, and we also
introduce a new perceptron-like reranking algorithm for
MT.
</bodyText>
<subsectionHeader confidence="0.999502">
2.2 Ranking and Ordinal Regression
</subsectionHeader>
<bodyText confidence="0.999967045454545">
In the field of machine learning, a class of tasks (called
ranking or ordinal regression) are similar to the rerank-
ing tasks in NLP. One of the motivations of this paper
is to apply ranking or ordinal regression algorithms to
MT reranking. In the previous works on ranking or or-
dinal regression, the margin is defined as the distance
between two consecutive ranks. Two large margin ap-
proaches have been used. One is the PRank algorithm,
a variant of the perceptron algorithm, that uses multi-
ple biases to represent the boundaries between every two
consecutive ranks (Crammer and Singer, 2001; Harring-
ton, 2003). However, as we will show in section 3.7, the
PRank algorithm does not work on the reranking tasks
due to the introduction of global ranks. The other ap-
proach is to reduce the ranking problem to a classification
problem by using the method of pairwise samples (Her-
brich et al., 2000). The underlying assumption is that the
samples of consecutive ranks are separable. This may
become a problem in the case that ranks are unreliable
when ranking does not strongly distinguish between can-
didates. This is just what happens in reranking for ma-
chine translation.
</bodyText>
<sectionHeader confidence="0.990805" genericHeader="method">
3 Discriminative Reranking for MT
</sectionHeader>
<bodyText confidence="0.999925714285714">
The reranking approach for MT is defined as follows:
First, a baseline system generates -best candidates. Fea-
tures that can potentially discriminate between good vs.
bad translations are extracted from these -best candi-
dates. These features are then used to determine a new
ranking for the -best list. The new top ranked candidate
in this -best list is our new best candidate translation.
</bodyText>
<subsectionHeader confidence="0.999844">
3.1 Advantages of Discriminative Reranking
</subsectionHeader>
<bodyText confidence="0.99991">
Discriminative reranking allows us to use global features
which are unavailable for the baseline system. Second,
we can use features of various kinds and need not worry
about fine-grained smoothing issues. Finally, the statis-
tical machine learning approach has been shown to be
effective in many NLP tasks. Reranking enables rapid
experimentation with complex feature functions, because
the complex decoding steps in SMT are done once to gen-
erate the N-best list of translations.
</bodyText>
<subsectionHeader confidence="0.999613">
3.2 Problems applying reranking to MT
</subsectionHeader>
<bodyText confidence="0.999690363636364">
First, we consider how to apply discriminative reranking
to machine translation. We may directly use those algo-
rithms that have been successfully used in parse rerank-
ing. However, we immediately find that those algorithms
are not as appropriate for machine translation. Let
be the candidate ranked at the th position for the source
sentence, where ranking is defined on the quality of the
candidates. In parse reranking, we look for parallel hy-
perplanes successfully separating and for all the
source sentences, but in MT, for each source sentence,
we have a set of reference translations instead of a single
gold standard. For this reason, it is hard to define which
candidate translation is the best. Suppose we have two
translations, one of which is close to reference transla-
tion ref while the other is close to reference translation
ref . It is difficult to say that one candidate is better than
the other.
Although we might invent metrics to define the qual-
ity of a translation, standard reranking algorithms can-
not be directly applied to MT. In parse reranking, each
training sentence has a ranked list of 27 candidates on
average (Collins, 2000), but for machine translation, the
number of candidate translations in the -best list is much
higher. (SMT Team, 2003) show that to get a reasonable
improvement in the BLEU score at least 1000 candidates
need to be considered in the -best list.
In addition, the parallel hyperplanes separating and
actually are unable to distinguish good translations
from bad translations, since they are not trained to distin-
guish any translations in . Furthermore, many good
translations in may differ greatly from , since
there are multiple references. These facts cause problems
for the applicability of reranking algorithms.
</bodyText>
<subsectionHeader confidence="0.998977">
3.3 Splitting
</subsectionHeader>
<bodyText confidence="0.999951857142857">
Our first attempt to handle this problem is to redefine the
notion of good translations versus bad translations. In-
stead of separating and , we say the top of the
-best translations are good translations, and the bottom
of the -best translations are bad translations, where
. Then we look for parallel hyperplanes split-
ting the top translations and bottom translations for
</bodyText>
<figureCaption confidence="0.997284">
Figure 1: Splitting for MT Reranking
</figureCaption>
<bodyText confidence="0.994692">
each sentence. Figure 1 illustrates this situation, where
and .
</bodyText>
<subsectionHeader confidence="0.978031">
3.4 Ordinal Regression
</subsectionHeader>
<bodyText confidence="0.9999646">
Furthermore, if we only look for the hyperplanes to sepa-
rate the good and the bad translations, we, in fact, discard
the order information of translations of the same class.
Maybe knowing that is better than may be use-
less for training to some extent, but knowing is better
than is useful, if . Although we cannot give
an affirmative answer at this time, it is at least reasonable
to use the ordering information. The problem is how to
use the ordering information. In addition, we only want
to maintain the order of two candidates if their ranks are
far away from each other. On the other hand, we do not
care the order of two translations whose ranks are very
close, e.g. 100 and 101. Thus insensitive ordinal regres-
sion is more desirable and is the approach we follow in
this paper.
</bodyText>
<subsectionHeader confidence="0.97292">
3.5 Uneven Margins
</subsectionHeader>
<bodyText confidence="0.9945545">
However, reranking is not an ordinal regression prob-
lem. In reranking evaluation, we are only interested in the
quality of the translation with the highest score, and we
do not care the order of bad translations. Therefore we
cannot simply regard a reranking problem as an ordinal
regression problem, since they have different definitions
for the loss function.
As far as linear classifiers are concerned, we want to
maintain a larger margin in translations of high ranks and
a smaller margin in translations of low ranks. For exam-
ple,
The reason is that the scoring function will be penalized
</bodyText>
<figure confidence="0.883452">
X2
W
score−metric
X1
good translations
bad translations
others
margin
margin margin margin
if it can not separate from , but not for the case of
versus .
</figure>
<subsectionHeader confidence="0.987271">
3.6 Large Margin Classifiers
</subsectionHeader>
<bodyText confidence="0.999953933333333">
There are quite a few linear classifiers1 that can sepa-
rate samples with large margin, such as SVMs (Vapnik,
1998), Boosting (Schapire et al., 1997), Winnow (Zhang,
2000) and Perceptron (Krauth and Mezard, 1987). The
performance of SVMs is superior to other linear classi-
fiers because of their ability to margin maximization.
However, SVMs are extremely slow in training since
they need to solve a quadratic programming search. For
example, SVMs even cannot be used to train on the whole
Penn Treebank in parse reranking (Shen and Joshi, 2003).
Taking this into account, we use perceptron-like algo-
rithms, since the perceptron algorithm is fast in training
which allow us to do experiments on real-world data. Its
large margin version is able to provide relatively good re-
sults in general.
</bodyText>
<subsectionHeader confidence="0.981455">
3.7 Pairwise Samples
</subsectionHeader>
<bodyText confidence="0.99993425">
In previous work on the PRank algorithm, ranks are de-
fined on the entire training and test data. Thus we can
define boundaries between consecutive ranks on the en-
tire data. But in MT reranking, ranks are defined over ev-
ery single source sentence. For example, in our data set,
the rank of a translation is only the rank among all the
translations for the same sentence. The training data in-
cludes about 1000 sentences, each of which normally has
1000 candidate translations with the exception of short
sentences that have a smaller number of candidate trans-
lations. As a result, we cannot use the PRank algorithm
in the reranking task, since there are no global ranks or
boundaries for all the samples.
However, the approach of using pairwise samples does
work. By pairing up two samples, we compute the rel-
ative distance between these two samples in the scoring
metric. In the training phase, we are only interested in
whether the relative distance is positive or negative.
However, the size of generated training samples will
be very large. For samples, the total number of pair-
wise samples in (Herbrich et al., 2000) is roughly . In
the next section, we will introduce two perceptron-like al-
gorithms that utilize pairwise samples while keeping the
complexity of data space unchanged.
</bodyText>
<sectionHeader confidence="0.995542" genericHeader="method">
4 Reranking Algorithms
</sectionHeader>
<bodyText confidence="0.999833">
Considering the desiderata discussed in the last sec-
tion, we present two perceptron-like algorithms for MT
reranking. The first one is a splitting algorithm specially
designed for MT reranking, which has similarities to a
</bodyText>
<footnote confidence="0.94464">
1Here we only consider linear kernels such as polynomial
kernels.
</footnote>
<bodyText confidence="0.99977075">
classification algorithm. We also experimented with an
ordinal regression algorithm proposed in (Shen and Joshi,
2004). For the sake of completeness, we will briefly de-
scribe the algorithm here.
</bodyText>
<subsectionHeader confidence="0.992658">
4.1 Splitting
</subsectionHeader>
<bodyText confidence="0.999428962962963">
In this section, we will propose a splitting algorithm
which separates translations of each sentence into two
parts, the top translations and the bottom translations.
All the separating hyperplanes are parallel by sharing the
same weight vector . The margin is defined on the dis-
tance between the top items and the bottom items in
each cluster, as shown in Figure 1.
Let be the feature vector of the translation of
the sentence, and be the rank for this translation
among all the translations for the sentence. Then the
set of training samples is:
where is the number of clusters and is the length of
ranks for each cluster.
Let be a linear function, where is the
feature vector of a translation, and is a weight vector.
We construct a hypothesis function with
as follows.
where is a function that takes a list of scores for the
candidate translations computed according to the evalua-
tion metric and returns the rank in that list. For example
.
The splitting algorithm searches a linear function
that successfully splits the top -ranked
and bottom -ranked translations for each sentence,
where . Formally, let
for any linear function . We look for the
function such that
</bodyText>
<equation confidence="0.5353375">
if (1)
if (2)
</equation>
<bodyText confidence="0.973261454545455">
which means that can successfully separate the good
translations and the bad translations.
Suppose there exists a linear function satisfying (1)
and (2), we say
is by given
and . Furthermore, we can define the splitting mar-
gin for the translations of the sentence as follows.
The minimal splitting margin, , for given
and is defined as follows.
Algorithm 1 splitting
Require: , and a positive learning margin .
</bodyText>
<listItem confidence="0.967008909090909">
4: compute
5: for (
7:
8: else if and and
) then
9: ; ;
10: end if
11: end for
12: ; ;
13: end for
14: until no updates made in the outer for loop
</listItem>
<bodyText confidence="0.979588423076923">
Algorithm 1 is a perceptron-like algorithm that looks
for a function that splits the training data. The idea of the
algorithm is as follows. For every two translations
and , if
the rank of is higher than or equal to , ,
the rank of is lower than ,,
the weight vector can not successfully separate
and with a learning margin ,
,
then we need to update with the addition of .
However, the updating is not executed until all the in-
consistent pairs in a sentence are found for the purpose
of speeding up the algorithm. When sentence is se-
lected, we first compute and store for all . Thus
we do not need to recompute again in the in-
ner loop. Now the complexity of a repeat iteration is
, where is the average number of active
features in vector . If we updated the weight vector
whenever an inconsistent pair was found, the complexity
of a loop would be .
The following theorem will show that Algorithm 1 will
stop in finite steps, outputting a function that splits the
training data with a large margin, if the training data is
splittable. Due to lack of space, we omit the proof for
Theorem 1 in this paper.
Theorem 1 Suppose the training samples
</bodyText>
<footnote confidence="0.824148">
are by a linear function defined on the weight
vector with a splitting margin , where .
</footnote>
<bodyText confidence="0.827796">
Let . Then Algorithm 1 makes at most
mistakes on the pairwise samples during the
Algorithm 2 ordinal regression with uneven margin
Require: a positive learning margin .
</bodyText>
<listItem confidence="0.987132727272727">
1: , initialize ;
2: repeat
3: for (sentence
9: else if and
and
then
12: end if
13: end for
14: ; ;
15: end for
16: until no updates made in the outer for loop
</listItem>
<subsectionHeader confidence="0.978635">
4.2 Ordinal Regression
</subsectionHeader>
<bodyText confidence="0.999951916666667">
The second algorithm that we will use for MT reranking
is the -insensitive ordinal regression with uneven mar-
gin, which was proposed in (Shen and Joshi, 2004), as
shown in Algorithm 2.
In Algorithm 2, the function is used to control the
level of insensitivity, and the function is used to con-
trol the learning margin between pairs of translations with
different ranks as described in Section 3.5. There are
many candidates for . The following definition for
is one of the simplest solutions.
We will use this function in our experiments on MT
reranking.
</bodyText>
<sectionHeader confidence="0.995067" genericHeader="evaluation">
5 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.99997725">
We provide experimental results on the NIST 2003
Chinese-English large data track evaluation. We use the
data set used in (SMT Team, 2003). The training data
consists of about 170M English words, on which the
baseline translation system is trained. The training data is
also used to build language models which are used to de-
fine feature functions on various syntactic levels. The de-
velopment data consists of 993 Chinese sentences. Each
Chinese sentence is associated with 1000-best English
translations generated by the baseline MT system. The
development data set is used to estimate the parameters
for the feature functions for the purpose of reranking. The
</bodyText>
<figure confidence="0.989672066666667">
1: ,initialize ;
2: repeat
3: for (
) do
) do
6: if and and
) then
, for all ;
; ;
training.
) do
4: compute and for all ;
5: for ( ) do
6: if and and
then
</figure>
<tableCaption confidence="0.6325266">
Table 1: BLEU scores reported in (SMT Team, 2003).
Every single feature was combined with the 6 baseline
features for the training and test. The minimum error
training (Och, 2003) was used on the development data
for parameter estimation.
</tableCaption>
<table confidence="0.9997316">
Feature BLEU%
Baseline 31.6
POS Language Model 31.7
Supertag Language Model 31.7
Wrong NN Position 31.7
Word Popularity 31.8
Aligned Template Models 31.9
Count of Missing Word 31.9
Template Right Continuity 32.0
IBM Model 1 32.5
</table>
<bodyText confidence="0.971437804597701">
test data consists of 878 Chinese sentences. Each Chinese
sentence is associated with 1000-best English translations
too. The test set is used to assess the quality of the rerank-
ing output.
In (SMT Team, 2003), 450 features were generated.
Six features from (Och, 2003) were used as baseline fea-
tures. Each of the 450 features was evaluated indepen-
dently by combining it with 6 baseline features and as-
sessing on the test data with the minimum error training.
The baseline BLEU score on the test set is 31.6%. Table
1 shows some of the best performing features.
In (SMT Team, 2003), aggressive search was used to
combine features. After combining about a dozen fea-
tures, the BLEU score did not improve any more, and
the score was 32.9%. It was also noticed that the major
improvement came from the Model 1 feature. By com-
bining the four features, Model 1, matched parentheses,
matched quotation marks and POS language model, the
system achieved a BLEU score of 32.6%.
In our experiments, we will use 4 different kinds of
feature combinations:
Baseline: The 6 baseline features used in (Och,
2003), such as cost of word penalty, cost of aligned
template penalty.
Best Feature: Baseline + IBM Model 1 + matched
parentheses + matched quotation marks + POS lan-
guage model.
Top Twenty: Baseline + 14 features with individual
BLEU score no less than 31.9% with the minimum
error training.
Large Set: Baseline + 50 features with individual
BLEU score no less than 31.7% with the minimum
error training. Since the baseline is 31.6% and the
95% confidence range is 0.9%, most of the fea-
tures in this set are not individually discriminative
with respect to the BLEU metric.
We apply Algorithm 1 and 2 to the four feature sets.
For algorithm 1, the splitting algorithm, we set
in the 1000-best translations given by the baseline MT
system. For algorithm 2, the ordinal regression algo-
rithm, we set the updating condition as
and , which means one’s rank number is
at most half of the other’s and there are at least 20 ranks
in between. Figures 2-9 show the results of using Al-
gorithm 1 and 2 with the four feature sets. The -axis
represents the number of iterations in the training. The
left -axis stands for the BLEU% score on the test data,
and the right -axis stands for log of the loss function on
the development data.
Algorithm 1, the splitting algorithm, converges on the
first three feature sets. The smaller the feature set is, the
faster the algorithm converges. It achieves a BLEU score
of 31.7% on the Baseline, 32.8% on the Best Feature, but
only 32.6% on the Top Twenty features. However it is
within the range of 95% confidence. Unfortunately on
the Large Set, Algorithm 1 converges very slowly.
In the Top Twenty set there are a fewer number of in-
dividually non-discriminative feature making the pool of
features “better”. In addition, generalization performance
in the Top Twenty set is better than the Large Set due to
the smaller set of “better” features, cf. (Shen and Joshi,
2004). If the number of the non-discriminative features
is large enough, the data set becomes unsplittable. We
have tried using the trick as in (Li et al., 2002) to make
data separable artificially, but the performance could not
be improved with such features.
We achieve similar results with Algorithm 2, the or-
dinal regression with uneven margin. It converges on
the first 3 feature sets too. On the Baseline, it achieves
31.4%. We notice that the model is over-trained on the
development data according to the learning curve. In the
Best Feature category, it achieves 32.7%, and on the Top
Twenty features, it achieves 32.9%. This algorithm does
not converge on the Large Set in 10000 iterations.
We compare our perceptron-like algorithms with the
minimum error training used in (SMT Team, 2003) as
shown in Table 2. The splitting algorithm achieves
slightly better results on the Baseline and the Best Fea-
ture set, while the minimum error training and the regres-
sion algorithm tie for first place on feature combinations.
However, the differences are not significant.
We notice in those separable feature sets the perfor-
mance on the development data and the test data are
tightly consistent. Whenever the log-loss on the devel-
opment set is decreased, and BLEU score on the test set
goes up, and vice versa. This tells us the merit of these
two algorithms; By optimizing on the loss function for
</bodyText>
<tableCaption confidence="0.667925666666667">
Table 2: Comparison between the minimum error
training with discriminative reranking on the test data
(BLEU%)
</tableCaption>
<table confidence="0.9997825">
Algorithm Baseline Best Feat Feat Comb
Minimum Error 31.6 32.6 32.9
Splitting 31.7 32.8 32.6
Regression 31.4 32.7 32.9
</table>
<bodyText confidence="0.972389333333333">
the development data, we can improve performance on
the test data. This property is guaranteed by the theoreti-
cal analysis and is borne out in the experimental results.
</bodyText>
<sectionHeader confidence="0.994828" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999803222222222">
In this paper, we have successfully applied the discrim-
inative reranking to machine translation. We applied a
new perceptron-like splitting algorithm and ordinal re-
gression algorithm with uneven margin to reranking in
MT. We provide a theoretical justification for the perfor-
mance of the splitting algorithms. Experimental results
provided in this paper show that the proposed algorithms
provide state-of-the-art performance in the NIST 2003
Chinese-English large data track evaluation.
</bodyText>
<sectionHeader confidence="0.997641" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99988225">
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 0121285.
The first author was partially supported by JHU post-
workshop fellowship and NSF Grant ITR-0205456. The
second author is partially supported by NSERC, Canada
(RGPIN: 264905). We thank the members of the SMT
team of JHU Workshop 2003 for help on the dataset and
three anonymous reviewers for useful comments.
</bodyText>
<sectionHeader confidence="0.995855" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99281">
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Je-
linek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. 1990.
A statistical approach to machine translation. Computational
Linguistics, 16(2):79–85.
M. Collins and N. Duffy. 2002. New ranking algorithms for
parsing and tagging: Kernels over discrete structures, and
the voted perceptron. In Proceedings ofACL 2002.
M. Collins. 2000. Discriminative reranking for natural lan-
guage parsing. In Proceedings of the 7th ICML.
K. Crammer and Y. Singer. 2001. PRanking with Ranking. In
NIPS 2001.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. In ACL 2003.
E. F. Harrington. 2003. Online Ranking/Collaborative Filtering
Using the Perceptron Algorithm. In ICML.
R. Herbrich, T. Graepel, and K. Obermayer. 2000. Large mar-
gin rank boundaries for ordinal regression. In A.J. Smola,
P. Bartlett, B. Sch¨olkopf, and D. Schuurmans, editors, Ad-
vances in Large Margin Classifiers, pages 115–132. MIT
Press.
W. Krauth and M. Mezard. 1987. Learning algorithms with
optimal stability in neural networks. Journal of Physics A,
20:745–752.
Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and J. Kan-
dola. 2002. The perceptron algorithm with uneven margins.
In Proceedings ofICML 2002.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
ACL 2002.
F. J. Och and H. Weber. 1998. Improving statistical natural
language translation with categories and rules. In COLING-
ACL 1998.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved alignment
models for statistical machine. In EMNLP-WVLC 1999.
F. J. Och. 2003. Minimum error rate training for statistical
machine translation. In ACL 2003.
K. Papineni, S. Roukos, and T. Ward. 2001. Bleu: a method for
automatic evaluation of machine translation. IBM Research
Report, RC22176.
R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. 1997.
Boosting the margin: a new explanation for the effectiveness
of voting methods. In Proc. 14th ICML.
L. Shen and A. K. Joshi. 2003. An SVM based voting algo-
rithm with application to parse reranking. In Proc. of CoNLL
2003.
L. Shen and A. K. Joshi. 2004. Flexible margin selection for
reranking with full pairwise samples. In Proc. of 1st IJC-
NLP.
SMT Team. 2003. Final report: Syntax for statisti-
cal machine translation. JHU Summer Workshop 2003,
http://www.clsp.jhu.edu/ws2003/groups/translate.
V. N. Vapnik. 1998. Statistical Learning Theory. John Wiley
and Sons, Inc.
Y. Wang and A. Waibel. 1998. Modeling with structures in
statistical machine translation. In COLING-ACL 1998.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377–400.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In ACL 2001.
T. Zhang. 2000. Large Margin Winnow Methods for Text Cat-
egorization. In KDD-2000 Workshop on Text Mining.
</reference>
<figureCaption confidence="0.99997675">
Figure 2: Splitting on Baseline
Figure 6: Ordinal Regression on Baseline
Figure 3: Splitting on Best Feature
Figure 7: Ordinal Regression on Best Feature
</figureCaption>
<figure confidence="0.997095666666667">
0 100 200 300 400 500 600
0 2000 4000 6000 8000 10000
# iteration
</figure>
<figureCaption confidence="0.958551">
Figure 8: Ordinal Regression on Top Twenty
# iteration
Figure 4: Splitting on Top Twenty
</figureCaption>
<figure confidence="0.987610333333333">
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
0 2000 4000 6000 8000 10000
# iteration
</figure>
<figureCaption confidence="0.987408">
Figure 9: Ordinal Regression on Large Set
# iteration
Figure 5: Splitting on Large Set
</figureCaption>
<figure confidence="0.993731569767442">
34
bleu% on test
log-loss on dev
33
32
log-loss on dev
➸
ss
31
30
29
34
bleu% on test
log-loss on dev
33
32
log-loss on dev
➸
ss
31
30
29
bleu% on test
�
�
34
bleu% on test
log-loss on dev
33
�
bleu% on test
�
32
log-loss on dev
ss
➸
31
30
29
34
bleu% on test
log-loss on dev
33
32
log-loss on dev
➸
ss
31
30
29
bleu% on test
bleu% on test
34
bleu% on test
log-loss on dev
33
�
bleu% on test
32
log-loss on dev
s
➸
31
30
29
34
bleu% on test
log-loss on dev
33
�
bleu% on test
32
log-loss on dev
➸
s
31
30
29
34
33
�
bleu% on test
32
log-loss on dev
ss
➸
</figure>
<page confidence="0.909849">
31
30
</page>
<figure confidence="0.742505363636364">
bleu% on test
log-loss on dev
29
34
33
�
bleu% on test
32
log-loss on dev
ss
➸
</figure>
<page confidence="0.9367905">
31
30
</page>
<note confidence="0.317665">
bleu% on test
log-loss on dev
</note>
<page confidence="0.991179">
29
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.073946">
<title confidence="0.929549">Discriminative Reranking for Machine Translation</title>
<author confidence="0.440644">Libin</author>
<affiliation confidence="0.9993445">Dept. of Comp. &amp; Info. Univ. of</affiliation>
<address confidence="0.767019">Philadelphia, PA</address>
<email confidence="0.999224">libin@seas.upenn.edu</email>
<author confidence="0.670817">Anoop</author>
<affiliation confidence="0.979179">School of Comp.</affiliation>
<address confidence="0.735067">Simon Fraser Burnaby, BC V5A</address>
<email confidence="0.976247">anoop@cs.sfu.ca</email>
<author confidence="0.999671">Franz Josef</author>
<affiliation confidence="0.999631">Info. Science Univ. of Southern</affiliation>
<author confidence="0.693945">Marina del Rey</author>
<author confidence="0.693945">CA</author>
<email confidence="0.999008">och@isi.edu</email>
<abstract confidence="0.998985470588235">This paper describes the application of discriminative reranking techniques to the problem of machine translation. For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked best list of candidate translations in the target language. We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J Cocke</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1156" citStr="Brown et al., 1990" startWordPosition="163" endWordPosition="166"> baseline statistical machine translation system, a ranked - best list of candidate translations in the target language. We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation. 1 Introduction The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years. Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements. In this paper, we introduce two novel machine learning algorithms specialized for the MT task. Discriminative reranking algorithms have also contributed to improvements in natural language parsing and tagging performance. Discriminative reranking algorithms used for these applications include Pe</context>
<context position="2696" citStr="Brown et al., 1990" startWordPosition="388" endWordPosition="391">translation. The reranking problem for natural language is neither a classification problem nor a regression problem, and under certain conditions MT reranking turns out to be quite different from parse reranking. In this paper, we consider the special issues of applying reranking techniques to the MT task and introduce two perceptron-like reranking algorithms for MT reranking. We provide experimental results that show that the proposed algorithms achieve start-of-the-art results on the NIST 2003 Chinese-English large data track evaluation. 1.1 Generative Models for MT The seminal IBM models (Brown et al., 1990) were the first to introduce generative models to the MT task. The IBM models applied the sequence learning paradigm well-known from Hidden Markov Models in speech recognition to the problem of MT. The source and target sentences were treated as the observations, but the alignments were treated as hidden information learned from parallel texts using the EM algorithm. This sourcechannel model treated the task of finding the probability , where is the translation in the target (English) language for a given source (foreign) sentence , as two generative probability models: the language model whic</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="7041" citStr="Collins and Duffy, 2002" startWordPosition="1075" endWordPosition="1078">g a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%. 2 Ranking and Reranking 2.1 Reranking for NLP tasks Like machine translation, parsing is another field of natural language processing in which generative models have been widely used. In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing. Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003). The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models. Discriminative reranking methods for parsing typically use the notion of a margin as the distance between the best candidate parse and the rest of the parses. The reranking problem is reduced to a classification problem by using pairwise samples. In (Shen and Joshi, 2004), we have introduced a new perceptron-like ordinal regression algorithm for parse reranking. In that algorithm, pairwise samples ar</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>M. Collins and N. Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings ofACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the 7th ICML.</booktitle>
<contexts>
<context position="7003" citStr="Collins, 2000" startWordPosition="1072" endWordPosition="1073">ss of MT output. By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%. 2 Ranking and Reranking 2.1 Reranking for NLP tasks Like machine translation, parsing is another field of natural language processing in which generative models have been widely used. In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing. Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003). The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models. Discriminative reranking methods for parsing typically use the notion of a margin as the distance between the best candidate parse and the rest of the parses. The reranking problem is reduced to a classification problem by using pairwise samples. In (Shen and Joshi, 2004), we have introduced a new perceptron-like ordinal regression algorithm for parse reranking. </context>
<context position="11327" citStr="Collins, 2000" startWordPosition="1775" endWordPosition="1776">sentence, we have a set of reference translations instead of a single gold standard. For this reason, it is hard to define which candidate translation is the best. Suppose we have two translations, one of which is close to reference translation ref while the other is close to reference translation ref . It is difficult to say that one candidate is better than the other. Although we might invent metrics to define the quality of a translation, standard reranking algorithms cannot be directly applied to MT. In parse reranking, each training sentence has a ranked list of 27 candidates on average (Collins, 2000), but for machine translation, the number of candidate translations in the -best list is much higher. (SMT Team, 2003) show that to get a reasonable improvement in the BLEU score at least 1000 candidates need to be considered in the -best list. In addition, the parallel hyperplanes separating and actually are unable to distinguish good translations from bad translations, since they are not trained to distinguish any translations in . Furthermore, many good translations in may differ greatly from , since there are multiple references. These facts cause problems for the applicability of rerankin</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>M. Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of the 7th ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>PRanking with Ranking.</title>
<date>2001</date>
<booktitle>In NIPS</booktitle>
<contexts>
<context position="8612" citStr="Crammer and Singer, 2001" startWordPosition="1329" endWordPosition="1332">2 Ranking and Ordinal Regression In the field of machine learning, a class of tasks (called ranking or ordinal regression) are similar to the reranking tasks in NLP. One of the motivations of this paper is to apply ranking or ordinal regression algorithms to MT reranking. In the previous works on ranking or ordinal regression, the margin is defined as the distance between two consecutive ranks. Two large margin approaches have been used. One is the PRank algorithm, a variant of the perceptron algorithm, that uses multiple biases to represent the boundaries between every two consecutive ranks (Crammer and Singer, 2001; Harrington, 2003). However, as we will show in section 3.7, the PRank algorithm does not work on the reranking tasks due to the introduction of global ranks. The other approach is to reduce the ranking problem to a classification problem by using the method of pairwise samples (Herbrich et al., 2000). The underlying assumption is that the samples of consecutive ranks are separable. This may become a problem in the case that ranks are unreliable when ranking does not strongly distinguish between candidates. This is just what happens in reranking for machine translation. 3 Discriminative Reran</context>
</contexts>
<marker>Crammer, Singer, 2001</marker>
<rawString>K. Crammer and Y. Singer. 2001. PRanking with Ranking. In NIPS 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Loosely tree-based alignment for machine translation.</title>
<date>2003</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="4960" citStr="Gildea (2003)" startWordPosition="742" endWordPosition="743">thin a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. The translation model involved tree alignments in which subtree cloning was used to handle cases of reordering that were not possible in earlier tree-based alignment models. 1.2 Discriminative Models for MT Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model estimated using a maximum entropy model. A small number of feature functions defined on the source and target sentence were used to rerank the translations generated by a b</context>
</contexts>
<marker>Gildea, 2003</marker>
<rawString>D. Gildea. 2003. Loosely tree-based alignment for machine translation. In ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Harrington</author>
</authors>
<title>Online Ranking/Collaborative Filtering Using the Perceptron Algorithm.</title>
<date>2003</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="8631" citStr="Harrington, 2003" startWordPosition="1333" endWordPosition="1335">ession In the field of machine learning, a class of tasks (called ranking or ordinal regression) are similar to the reranking tasks in NLP. One of the motivations of this paper is to apply ranking or ordinal regression algorithms to MT reranking. In the previous works on ranking or ordinal regression, the margin is defined as the distance between two consecutive ranks. Two large margin approaches have been used. One is the PRank algorithm, a variant of the perceptron algorithm, that uses multiple biases to represent the boundaries between every two consecutive ranks (Crammer and Singer, 2001; Harrington, 2003). However, as we will show in section 3.7, the PRank algorithm does not work on the reranking tasks due to the introduction of global ranks. The other approach is to reduce the ranking problem to a classification problem by using the method of pairwise samples (Herbrich et al., 2000). The underlying assumption is that the samples of consecutive ranks are separable. This may become a problem in the case that ranks are unreliable when ranking does not strongly distinguish between candidates. This is just what happens in reranking for machine translation. 3 Discriminative Reranking for MT The rer</context>
</contexts>
<marker>Harrington, 2003</marker>
<rawString>E. F. Harrington. 2003. Online Ranking/Collaborative Filtering Using the Perceptron Algorithm. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Herbrich</author>
<author>T Graepel</author>
<author>K Obermayer</author>
</authors>
<title>Large margin rank boundaries for ordinal regression.</title>
<date>2000</date>
<booktitle>Advances in Large Margin Classifiers,</booktitle>
<pages>115--132</pages>
<editor>In A.J. Smola, P. Bartlett, B. Sch¨olkopf, and D. Schuurmans, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8915" citStr="Herbrich et al., 2000" startWordPosition="1382" endWordPosition="1386"> or ordinal regression, the margin is defined as the distance between two consecutive ranks. Two large margin approaches have been used. One is the PRank algorithm, a variant of the perceptron algorithm, that uses multiple biases to represent the boundaries between every two consecutive ranks (Crammer and Singer, 2001; Harrington, 2003). However, as we will show in section 3.7, the PRank algorithm does not work on the reranking tasks due to the introduction of global ranks. The other approach is to reduce the ranking problem to a classification problem by using the method of pairwise samples (Herbrich et al., 2000). The underlying assumption is that the samples of consecutive ranks are separable. This may become a problem in the case that ranks are unreliable when ranking does not strongly distinguish between candidates. This is just what happens in reranking for machine translation. 3 Discriminative Reranking for MT The reranking approach for MT is defined as follows: First, a baseline system generates -best candidates. Features that can potentially discriminate between good vs. bad translations are extracted from these -best candidates. These features are then used to determine a new ranking for the -</context>
<context position="15964" citStr="Herbrich et al., 2000" startWordPosition="2554" endWordPosition="2557">hort sentences that have a smaller number of candidate translations. As a result, we cannot use the PRank algorithm in the reranking task, since there are no global ranks or boundaries for all the samples. However, the approach of using pairwise samples does work. By pairing up two samples, we compute the relative distance between these two samples in the scoring metric. In the training phase, we are only interested in whether the relative distance is positive or negative. However, the size of generated training samples will be very large. For samples, the total number of pairwise samples in (Herbrich et al., 2000) is roughly . In the next section, we will introduce two perceptron-like algorithms that utilize pairwise samples while keeping the complexity of data space unchanged. 4 Reranking Algorithms Considering the desiderata discussed in the last section, we present two perceptron-like algorithms for MT reranking. The first one is a splitting algorithm specially designed for MT reranking, which has similarities to a 1Here we only consider linear kernels such as polynomial kernels. classification algorithm. We also experimented with an ordinal regression algorithm proposed in (Shen and Joshi, 2004). F</context>
</contexts>
<marker>Herbrich, Graepel, Obermayer, 2000</marker>
<rawString>R. Herbrich, T. Graepel, and K. Obermayer. 2000. Large margin rank boundaries for ordinal regression. In A.J. Smola, P. Bartlett, B. Sch¨olkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 115–132. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Krauth</author>
<author>M Mezard</author>
</authors>
<title>Learning algorithms with optimal stability in neural networks.</title>
<date>1987</date>
<journal>Journal of Physics A,</journal>
<pages>20--745</pages>
<contexts>
<context position="14245" citStr="Krauth and Mezard, 1987" startWordPosition="2263" endWordPosition="2266"> far as linear classifiers are concerned, we want to maintain a larger margin in translations of high ranks and a smaller margin in translations of low ranks. For example, The reason is that the scoring function will be penalized X2 W score−metric X1 good translations bad translations others margin margin margin margin if it can not separate from , but not for the case of versus . 3.6 Large Margin Classifiers There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987). The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization. However, SVMs are extremely slow in training since they need to solve a quadratic programming search. For example, SVMs even cannot be used to train on the whole Penn Treebank in parse reranking (Shen and Joshi, 2003). Taking this into account, we use perceptron-like algorithms, since the perceptron algorithm is fast in training which allow us to do experiments on real-world data. Its large margin version is able to provide relatively good results in general. 3.7 Pairwise Samples </context>
</contexts>
<marker>Krauth, Mezard, 1987</marker>
<rawString>W. Krauth and M. Mezard. 1987. Learning algorithms with optimal stability in neural networks. Journal of Physics A, 20:745–752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
<author>H Zaragoza</author>
<author>R Herbrich</author>
<author>J Shawe-Taylor</author>
<author>J Kandola</author>
</authors>
<title>The perceptron algorithm with uneven margins.</title>
<date>2002</date>
<booktitle>In Proceedings ofICML</booktitle>
<contexts>
<context position="24991" citStr="Li et al., 2002" startWordPosition="4142" endWordPosition="4145"> Feature, but only 32.6% on the Top Twenty features. However it is within the range of 95% confidence. Unfortunately on the Large Set, Algorithm 1 converges very slowly. In the Top Twenty set there are a fewer number of individually non-discriminative feature making the pool of features “better”. In addition, generalization performance in the Top Twenty set is better than the Large Set due to the smaller set of “better” features, cf. (Shen and Joshi, 2004). If the number of the non-discriminative features is large enough, the data set becomes unsplittable. We have tried using the trick as in (Li et al., 2002) to make data separable artificially, but the performance could not be improved with such features. We achieve similar results with Algorithm 2, the ordinal regression with uneven margin. It converges on the first 3 feature sets too. On the Baseline, it achieves 31.4%. We notice that the model is over-trained on the development data according to the learning curve. In the Best Feature category, it achieves 32.7%, and on the Top Twenty features, it achieves 32.9%. This algorithm does not converge on the Large Set in 10000 iterations. We compare our perceptron-like algorithms with the minimum er</context>
</contexts>
<marker>Li, Zaragoza, Herbrich, Shawe-Taylor, Kandola, 2002</marker>
<rawString>Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and J. Kandola. 2002. The perceptron algorithm with uneven margins. In Proceedings ofICML 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="1334" citStr="Och and Ney, 2002" startWordPosition="190" endWordPosition="193">thms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation. 1 Introduction The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years. Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements. In this paper, we introduce two novel machine learning algorithms specialized for the MT task. Discriminative reranking algorithms have also contributed to improvements in natural language parsing and tagging performance. Discriminative reranking algorithms used for these applications include Perceptron, Boosting and Support Vector Machines (SVMs). In the machine learning community, some novel discriminative ranking (also called ordinal regression) algorithms have been </context>
<context position="5302" citStr="Och and Ney (2002)" startWordPosition="797" endWordPosition="800">. (Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. The translation model involved tree alignments in which subtree cloning was used to handle cases of reordering that were not possible in earlier tree-based alignment models. 1.2 Discriminative Models for MT Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model estimated using a maximum entropy model. A small number of feature functions defined on the source and target sentence were used to rerank the translations generated by a baseline MT system. While the total number of feature functions was small, each feature function was a complex statistical model by itself, as for example, the alignment template feature functions used in this approach. Och (2003) described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metric</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Weber</author>
</authors>
<title>Improving statistical natural language translation with categories and rules.</title>
<date>1998</date>
<booktitle>In COLINGACL</booktitle>
<contexts>
<context position="4159" citStr="Och and Weber, 1998" startWordPosition="614" endWordPosition="617">ke word context into account. This means unlikely alignments are being considered while training the model and this also results in additional decoding complexity. Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality. Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments. Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders. In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not u</context>
</contexts>
<marker>Och, Weber, 1998</marker>
<rawString>F. J. Och and H. Weber. 1998. Improving statistical natural language translation with categories and rules. In COLINGACL 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Improved alignment models for statistical machine.</title>
<date>1999</date>
<booktitle>In EMNLP-WVLC</booktitle>
<contexts>
<context position="4178" citStr="Och et al., 1999" startWordPosition="618" endWordPosition="621">account. This means unlikely alignments are being considered while training the model and this also results in additional decoding complexity. Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality. Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments. Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders. In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank. Yama</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>F. J. Och, C. Tillmann, and H. Ney. 1999. Improved alignment models for statistical machine. In EMNLP-WVLC 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="1367" citStr="Och, 2003" startWordPosition="197" endWordPosition="198">ne translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation. 1 Introduction The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years. Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements. In this paper, we introduce two novel machine learning algorithms specialized for the MT task. Discriminative reranking algorithms have also contributed to improvements in natural language parsing and tagging performance. Discriminative reranking algorithms used for these applications include Perceptron, Boosting and Support Vector Machines (SVMs). In the machine learning community, some novel discriminative ranking (also called ordinal regression) algorithms have been proposed in recent years. Based o</context>
<context position="5789" citStr="Och (2003)" startWordPosition="879" endWordPosition="880">ordering that were not possible in earlier tree-based alignment models. 1.2 Discriminative Models for MT Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model estimated using a maximum entropy model. A small number of feature functions defined on the source and target sentence were used to rerank the translations generated by a baseline MT system. While the total number of feature functions was small, each feature function was a complex statistical model by itself, as for example, the alignment template feature functions used in this approach. Och (2003) described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU. The experiments showed that this approach obtains significantly better results than using the maximum mutual information criterion on parameter estimation. This approach used the same set of features as the alignment template approach in (Och and Ney, 2002). SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions. More than 450 different feature functions were used in order to improve the syntactic well-formedne</context>
<context position="21557" citStr="Och, 2003" startWordPosition="3552" endWordPosition="3553">pment data consists of 993 Chinese sentences. Each Chinese sentence is associated with 1000-best English translations generated by the baseline MT system. The development data set is used to estimate the parameters for the feature functions for the purpose of reranking. The 1: ,initialize ; 2: repeat 3: for ( ) do ) do 6: if and and ) then , for all ; ; ; training. ) do 4: compute and for all ; 5: for ( ) do 6: if and and then Table 1: BLEU scores reported in (SMT Team, 2003). Every single feature was combined with the 6 baseline features for the training and test. The minimum error training (Och, 2003) was used on the development data for parameter estimation. Feature BLEU% Baseline 31.6 POS Language Model 31.7 Supertag Language Model 31.7 Wrong NN Position 31.7 Word Popularity 31.8 Aligned Template Models 31.9 Count of Missing Word 31.9 Template Right Continuity 32.0 IBM Model 1 32.5 test data consists of 878 Chinese sentences. Each Chinese sentence is associated with 1000-best English translations too. The test set is used to assess the quality of the reranking output. In (SMT Team, 2003), 450 features were generated. Six features from (Och, 2003) were used as baseline features. Each of t</context>
<context position="22942" citStr="Och, 2003" startWordPosition="3785" endWordPosition="3786"> the test set is 31.6%. Table 1 shows some of the best performing features. In (SMT Team, 2003), aggressive search was used to combine features. After combining about a dozen features, the BLEU score did not improve any more, and the score was 32.9%. It was also noticed that the major improvement came from the Model 1 feature. By combining the four features, Model 1, matched parentheses, matched quotation marks and POS language model, the system achieved a BLEU score of 32.6%. In our experiments, we will use 4 different kinds of feature combinations: Baseline: The 6 baseline features used in (Och, 2003), such as cost of word penalty, cost of aligned template penalty. Best Feature: Baseline + IBM Model 1 + matched parentheses + matched quotation marks + POS language model. Top Twenty: Baseline + 14 features with individual BLEU score no less than 31.9% with the minimum error training. Large Set: Baseline + 50 features with individual BLEU score no less than 31.7% with the minimum error training. Since the baseline is 31.6% and the 95% confidence range is 0.9%, most of the features in this set are not individually discriminative with respect to the BLEU metric. We apply Algorithm 1 and 2 to th</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training for statistical machine translation. In ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research</journal>
<tech>Report, RC22176.</tech>
<contexts>
<context position="6521" citStr="Papineni et al., 2001" startWordPosition="997" endWordPosition="1001">rics such as BLEU. The experiments showed that this approach obtains significantly better results than using the maximum mutual information criterion on parameter estimation. This approach used the same set of features as the alignment template approach in (Och and Ney, 2002). SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions. More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output. By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%. 2 Ranking and Reranking 2.1 Reranking for NLP tasks Like machine translation, parsing is another field of natural language processing in which generative models have been widely used. In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing. Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003). The reranking techniques ha</context>
</contexts>
<marker>Papineni, Roukos, Ward, 2001</marker>
<rawString>K. Papineni, S. Roukos, and T. Ward. 2001. Bleu: a method for automatic evaluation of machine translation. IBM Research Report, RC22176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Freund</author>
<author>P Bartlett</author>
<author>W S Lee</author>
</authors>
<title>Boosting the margin: a new explanation for the effectiveness of voting methods.</title>
<date>1997</date>
<booktitle>In Proc. 14th ICML.</booktitle>
<contexts>
<context position="14182" citStr="Schapire et al., 1997" startWordPosition="2254" endWordPosition="2257">nce they have different definitions for the loss function. As far as linear classifiers are concerned, we want to maintain a larger margin in translations of high ranks and a smaller margin in translations of low ranks. For example, The reason is that the scoring function will be penalized X2 W score−metric X1 good translations bad translations others margin margin margin margin if it can not separate from , but not for the case of versus . 3.6 Large Margin Classifiers There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987). The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization. However, SVMs are extremely slow in training since they need to solve a quadratic programming search. For example, SVMs even cannot be used to train on the whole Penn Treebank in parse reranking (Shen and Joshi, 2003). Taking this into account, we use perceptron-like algorithms, since the perceptron algorithm is fast in training which allow us to do experiments on real-world data. Its large margin version is able to pr</context>
</contexts>
<marker>Schapire, Freund, Bartlett, Lee, 1997</marker>
<rawString>R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. 1997. Boosting the margin: a new explanation for the effectiveness of voting methods. In Proc. 14th ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A K Joshi</author>
</authors>
<title>An SVM based voting algorithm with application to parse reranking.</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL</booktitle>
<contexts>
<context position="7092" citStr="Shen and Joshi, 2003" startWordPosition="1083" endWordPosition="1086"> from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%. 2 Ranking and Reranking 2.1 Reranking for NLP tasks Like machine translation, parsing is another field of natural language processing in which generative models have been widely used. In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing. Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003). The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models. Discriminative reranking methods for parsing typically use the notion of a margin as the distance between the best candidate parse and the rest of the parses. The reranking problem is reduced to a classification problem by using pairwise samples. In (Shen and Joshi, 2004), we have introduced a new perceptron-like ordinal regression algorithm for parse reranking. In that algorithm, pairwise samples are used for training and margins are defined as the </context>
<context position="14577" citStr="Shen and Joshi, 2003" startWordPosition="2318" endWordPosition="2321">separate from , but not for the case of versus . 3.6 Large Margin Classifiers There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987). The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization. However, SVMs are extremely slow in training since they need to solve a quadratic programming search. For example, SVMs even cannot be used to train on the whole Penn Treebank in parse reranking (Shen and Joshi, 2003). Taking this into account, we use perceptron-like algorithms, since the perceptron algorithm is fast in training which allow us to do experiments on real-world data. Its large margin version is able to provide relatively good results in general. 3.7 Pairwise Samples In previous work on the PRank algorithm, ranks are defined on the entire training and test data. Thus we can define boundaries between consecutive ranks on the entire data. But in MT reranking, ranks are defined over every single source sentence. For example, in our data set, the rank of a translation is only the rank among all th</context>
</contexts>
<marker>Shen, Joshi, 2003</marker>
<rawString>L. Shen and A. K. Joshi. 2003. An SVM based voting algorithm with application to parse reranking. In Proc. of CoNLL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A K Joshi</author>
</authors>
<title>Flexible margin selection for reranking with full pairwise samples.</title>
<date>2004</date>
<booktitle>In Proc. of 1st IJCNLP.</booktitle>
<contexts>
<context position="7510" citStr="Shen and Joshi, 2004" startWordPosition="1149" endWordPosition="1152">rsing. Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003). The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models. Discriminative reranking methods for parsing typically use the notion of a margin as the distance between the best candidate parse and the rest of the parses. The reranking problem is reduced to a classification problem by using pairwise samples. In (Shen and Joshi, 2004), we have introduced a new perceptron-like ordinal regression algorithm for parse reranking. In that algorithm, pairwise samples are used for training and margins are defined as the distance between parses of different ranks. In addition, the uneven margin technique has been used for the purpose of adapting ordinal regression to reranking tasks. In this paper, we apply this algorithm to MT reranking, and we also introduce a new perceptron-like reranking algorithm for MT. 2.2 Ranking and Ordinal Regression In the field of machine learning, a class of tasks (called ranking or ordinal regression)</context>
<context position="16561" citStr="Shen and Joshi, 2004" startWordPosition="2643" endWordPosition="2646"> (Herbrich et al., 2000) is roughly . In the next section, we will introduce two perceptron-like algorithms that utilize pairwise samples while keeping the complexity of data space unchanged. 4 Reranking Algorithms Considering the desiderata discussed in the last section, we present two perceptron-like algorithms for MT reranking. The first one is a splitting algorithm specially designed for MT reranking, which has similarities to a 1Here we only consider linear kernels such as polynomial kernels. classification algorithm. We also experimented with an ordinal regression algorithm proposed in (Shen and Joshi, 2004). For the sake of completeness, we will briefly describe the algorithm here. 4.1 Splitting In this section, we will propose a splitting algorithm which separates translations of each sentence into two parts, the top translations and the bottom translations. All the separating hyperplanes are parallel by sharing the same weight vector . The margin is defined on the distance between the top items and the bottom items in each cluster, as shown in Figure 1. Let be the feature vector of the translation of the sentence, and be the rank for this translation among all the translations for the sentence</context>
<context position="20137" citStr="Shen and Joshi, 2004" startWordPosition="3299" endWordPosition="3302">e the training samples are by a linear function defined on the weight vector with a splitting margin , where . Let . Then Algorithm 1 makes at most mistakes on the pairwise samples during the Algorithm 2 ordinal regression with uneven margin Require: a positive learning margin . 1: , initialize ; 2: repeat 3: for (sentence 9: else if and and then 12: end if 13: end for 14: ; ; 15: end for 16: until no updates made in the outer for loop 4.2 Ordinal Regression The second algorithm that we will use for MT reranking is the -insensitive ordinal regression with uneven margin, which was proposed in (Shen and Joshi, 2004), as shown in Algorithm 2. In Algorithm 2, the function is used to control the level of insensitivity, and the function is used to control the learning margin between pairs of translations with different ranks as described in Section 3.5. There are many candidates for . The following definition for is one of the simplest solutions. We will use this function in our experiments on MT reranking. 5 Experiments and Analysis We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We use the data set used in (SMT Team, 2003). The training data consists of about 1</context>
<context position="24835" citStr="Shen and Joshi, 2004" startWordPosition="4115" endWordPosition="4118">irst three feature sets. The smaller the feature set is, the faster the algorithm converges. It achieves a BLEU score of 31.7% on the Baseline, 32.8% on the Best Feature, but only 32.6% on the Top Twenty features. However it is within the range of 95% confidence. Unfortunately on the Large Set, Algorithm 1 converges very slowly. In the Top Twenty set there are a fewer number of individually non-discriminative feature making the pool of features “better”. In addition, generalization performance in the Top Twenty set is better than the Large Set due to the smaller set of “better” features, cf. (Shen and Joshi, 2004). If the number of the non-discriminative features is large enough, the data set becomes unsplittable. We have tried using the trick as in (Li et al., 2002) to make data separable artificially, but the performance could not be improved with such features. We achieve similar results with Algorithm 2, the ordinal regression with uneven margin. It converges on the first 3 feature sets too. On the Baseline, it achieves 31.4%. We notice that the model is over-trained on the development data according to the learning curve. In the Best Feature category, it achieves 32.7%, and on the Top Twenty featu</context>
</contexts>
<marker>Shen, Joshi, 2004</marker>
<rawString>L. Shen and A. K. Joshi. 2004. Flexible margin selection for reranking with full pairwise samples. In Proc. of 1st IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SMT Team</author>
</authors>
<date>2003</date>
<note>Final report: Syntax for statisti-</note>
<contexts>
<context position="6192" citStr="Team (2003)" startWordPosition="943" endWordPosition="944">ile the total number of feature functions was small, each feature function was a complex statistical model by itself, as for example, the alignment template feature functions used in this approach. Och (2003) described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU. The experiments showed that this approach obtains significantly better results than using the maximum mutual information criterion on parameter estimation. This approach used the same set of features as the alignment template approach in (Och and Ney, 2002). SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions. More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output. By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%. 2 Ranking and Reranking 2.1 Reranking for NLP tasks Like machine translation, parsing is another field of natural language processing in which generative models have been widely used. In recent years, reranking</context>
<context position="11445" citStr="Team, 2003" startWordPosition="1794" endWordPosition="1795">e which candidate translation is the best. Suppose we have two translations, one of which is close to reference translation ref while the other is close to reference translation ref . It is difficult to say that one candidate is better than the other. Although we might invent metrics to define the quality of a translation, standard reranking algorithms cannot be directly applied to MT. In parse reranking, each training sentence has a ranked list of 27 candidates on average (Collins, 2000), but for machine translation, the number of candidate translations in the -best list is much higher. (SMT Team, 2003) show that to get a reasonable improvement in the BLEU score at least 1000 candidates need to be considered in the -best list. In addition, the parallel hyperplanes separating and actually are unable to distinguish good translations from bad translations, since they are not trained to distinguish any translations in . Furthermore, many good translations in may differ greatly from , since there are multiple references. These facts cause problems for the applicability of reranking algorithms. 3.3 Splitting Our first attempt to handle this problem is to redefine the notion of good translations ve</context>
<context position="20698" citStr="Team, 2003" startWordPosition="3396" endWordPosition="3397">rgin, which was proposed in (Shen and Joshi, 2004), as shown in Algorithm 2. In Algorithm 2, the function is used to control the level of insensitivity, and the function is used to control the learning margin between pairs of translations with different ranks as described in Section 3.5. There are many candidates for . The following definition for is one of the simplest solutions. We will use this function in our experiments on MT reranking. 5 Experiments and Analysis We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We use the data set used in (SMT Team, 2003). The training data consists of about 170M English words, on which the baseline translation system is trained. The training data is also used to build language models which are used to define feature functions on various syntactic levels. The development data consists of 993 Chinese sentences. Each Chinese sentence is associated with 1000-best English translations generated by the baseline MT system. The development data set is used to estimate the parameters for the feature functions for the purpose of reranking. The 1: ,initialize ; 2: repeat 3: for ( ) do ) do 6: if and and ) then , for all</context>
<context position="22055" citStr="Team, 2003" startWordPosition="3632" endWordPosition="3633">ature was combined with the 6 baseline features for the training and test. The minimum error training (Och, 2003) was used on the development data for parameter estimation. Feature BLEU% Baseline 31.6 POS Language Model 31.7 Supertag Language Model 31.7 Wrong NN Position 31.7 Word Popularity 31.8 Aligned Template Models 31.9 Count of Missing Word 31.9 Template Right Continuity 32.0 IBM Model 1 32.5 test data consists of 878 Chinese sentences. Each Chinese sentence is associated with 1000-best English translations too. The test set is used to assess the quality of the reranking output. In (SMT Team, 2003), 450 features were generated. Six features from (Och, 2003) were used as baseline features. Each of the 450 features was evaluated independently by combining it with 6 baseline features and assessing on the test data with the minimum error training. The baseline BLEU score on the test set is 31.6%. Table 1 shows some of the best performing features. In (SMT Team, 2003), aggressive search was used to combine features. After combining about a dozen features, the BLEU score did not improve any more, and the score was 32.9%. It was also noticed that the major improvement came from the Model 1 fea</context>
<context position="25628" citStr="Team, 2003" startWordPosition="4248" endWordPosition="4249">tificially, but the performance could not be improved with such features. We achieve similar results with Algorithm 2, the ordinal regression with uneven margin. It converges on the first 3 feature sets too. On the Baseline, it achieves 31.4%. We notice that the model is over-trained on the development data according to the learning curve. In the Best Feature category, it achieves 32.7%, and on the Top Twenty features, it achieves 32.9%. This algorithm does not converge on the Large Set in 10000 iterations. We compare our perceptron-like algorithms with the minimum error training used in (SMT Team, 2003) as shown in Table 2. The splitting algorithm achieves slightly better results on the Baseline and the Best Feature set, while the minimum error training and the regression algorithm tie for first place on feature combinations. However, the differences are not significant. We notice in those separable feature sets the performance on the development data and the test data are tightly consistent. Whenever the log-loss on the development set is decreased, and BLEU score on the test set goes up, and vice versa. This tells us the merit of these two algorithms; By optimizing on the loss function for</context>
</contexts>
<marker>Team, 2003</marker>
<rawString>SMT Team. 2003. Final report: Syntax for statisti-</rawString>
</citation>
<citation valid="true">
<title>cal machine translation. JHU Summer Workshop</title>
<date>2003</date>
<pages>2003</pages>
<contexts>
<context position="4960" citStr="(2003)" startWordPosition="743" endWordPosition="743">template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. The translation model involved tree alignments in which subtree cloning was used to handle cases of reordering that were not possible in earlier tree-based alignment models. 1.2 Discriminative Models for MT Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model estimated using a maximum entropy model. A small number of feature functions defined on the source and target sentence were used to rerank the translations generated by a b</context>
<context position="6192" citStr="(2003)" startWordPosition="944" endWordPosition="944">he total number of feature functions was small, each feature function was a complex statistical model by itself, as for example, the alignment template feature functions used in this approach. Och (2003) described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU. The experiments showed that this approach obtains significantly better results than using the maximum mutual information criterion on parameter estimation. This approach used the same set of features as the alignment template approach in (Och and Ney, 2002). SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions. More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output. By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%. 2 Ranking and Reranking 2.1 Reranking for NLP tasks Like machine translation, parsing is another field of natural language processing in which generative models have been widely used. In recent years, reranking</context>
</contexts>
<marker>2003</marker>
<rawString>cal machine translation. JHU Summer Workshop 2003, http://www.clsp.jhu.edu/ws2003/groups/translate.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>John Wiley and Sons, Inc.</publisher>
<contexts>
<context position="14148" citStr="Vapnik, 1998" startWordPosition="2251" endWordPosition="2252">al regression problem, since they have different definitions for the loss function. As far as linear classifiers are concerned, we want to maintain a larger margin in translations of high ranks and a smaller margin in translations of low ranks. For example, The reason is that the scoring function will be penalized X2 W score−metric X1 good translations bad translations others margin margin margin margin if it can not separate from , but not for the case of versus . 3.6 Large Margin Classifiers There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987). The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization. However, SVMs are extremely slow in training since they need to solve a quadratic programming search. For example, SVMs even cannot be used to train on the whole Penn Treebank in parse reranking (Shen and Joshi, 2003). Taking this into account, we use perceptron-like algorithms, since the perceptron algorithm is fast in training which allow us to do experiments on real-world data. Its </context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>V. N. Vapnik. 1998. Statistical Learning Theory. John Wiley and Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>A Waibel</author>
</authors>
<title>Modeling with structures in statistical machine translation.</title>
<date>1998</date>
<booktitle>In COLING-ACL</booktitle>
<contexts>
<context position="3931" citStr="Wang and Waibel (1998)" startWordPosition="580" endWordPosition="583">tive probability over candidate translations and the translation model which is a generative conditional probability of the source sentence given a candidate translation . The lexicon of the single-word based IBM models does not take word context into account. This means unlikely alignments are being considered while training the model and this also results in additional decoding complexity. Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality. Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments. Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders. In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in</context>
</contexts>
<marker>Wang, Waibel, 1998</marker>
<rawString>Y. Wang and A. Waibel. 1998. Modeling with structures in statistical machine translation. In COLING-ACL 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="4559" citStr="Wu (1997)" startWordPosition="677" endWordPosition="678">del based on phrase-based alignments. Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders. In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. The translation model involved tree alignments in which subtree cloning was used t</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="4798" citStr="Yamada and Knight (2001)" startWordPosition="712" endWordPosition="716">999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. The translation model involved tree alignments in which subtree cloning was used to handle cases of reordering that were not possible in earlier tree-based alignment models. 1.2 Discriminative Models for MT Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model estimated </context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In ACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
</authors>
<title>Large Margin Winnow Methods for Text Categorization.</title>
<date>2000</date>
<booktitle>In KDD-2000 Workshop on Text Mining.</booktitle>
<contexts>
<context position="14204" citStr="Zhang, 2000" startWordPosition="2259" endWordPosition="2260">ons for the loss function. As far as linear classifiers are concerned, we want to maintain a larger margin in translations of high ranks and a smaller margin in translations of low ranks. For example, The reason is that the scoring function will be penalized X2 W score−metric X1 good translations bad translations others margin margin margin margin if it can not separate from , but not for the case of versus . 3.6 Large Margin Classifiers There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987). The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization. However, SVMs are extremely slow in training since they need to solve a quadratic programming search. For example, SVMs even cannot be used to train on the whole Penn Treebank in parse reranking (Shen and Joshi, 2003). Taking this into account, we use perceptron-like algorithms, since the perceptron algorithm is fast in training which allow us to do experiments on real-world data. Its large margin version is able to provide relatively good </context>
</contexts>
<marker>Zhang, 2000</marker>
<rawString>T. Zhang. 2000. Large Margin Winnow Methods for Text Categorization. In KDD-2000 Workshop on Text Mining.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>