<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.991857">
Bilingual Sense Similarity for Statistical Machine Translation
</title>
<author confidence="0.976796">
Boxing Chen, George Foster and Roland Kuhn
</author>
<affiliation confidence="0.981102">
National Research Council Canada
</affiliation>
<address confidence="0.981119">
283 Alexandre-Taché Boulevard, Gatineau (Québec), Canada J8X 3X7
</address>
<email confidence="0.994401">
{Boxing.Chen, George.Foster, Roland.Kuhn}@nrc.ca
</email>
<sectionHeader confidence="0.99462" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999039214285714">
This paper proposes new algorithms to com-
pute the sense similarity between two units
(words, phrases, rules, etc.) from parallel cor-
pora. The sense similarity scores are computed
by using the vector space model. We then ap-
ply the algorithms to statistical machine trans-
lation by computing the sense similarity be-
tween the source and target side of translation
rule pairs. Similarity scores are used as addi-
tional features of the translation model to im-
prove translation performance. Significant im-
provements are obtained over a state-of-the-art
hierarchical phrase-based machine translation
system.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996880952381">
The sense of a term can generally be inferred
from its context. The underlying idea is that a
term is characterized by the contexts it co-occurs
with. This is also well known as the Distribu-
tional Hypothesis (Harris, 1954): terms occurring
in similar contexts tend to have similar mean-
ings. There has been a lot of work to compute the
sense similarity between terms based on their
distribution in a corpus, such as (Hindle, 1990;
Lund and Burgess, 1996; Landauer and Dumais,
1997; Lin, 1998; Turney, 2001; Pantel and Lin,
2002; Pado and Lapata, 2007).
In the work just cited, a common procedure is
followed. Given two terms to be compared, one
first extracts various features for each term from
their contexts in a corpus and forms a vector
space model (VSM); then, one computes their
similarity by using similarity functions. The fea-
tures include words within a surface window of a
fixed size (Lund and Burgess, 1996), grammati-
cal dependencies (Lin, 1998; Pantel and Lin
2002; Pado and Lapata, 2007), etc. The similari-
ty function which has been most widely used is
cosine distance (Salton and McGill, 1983); other
similarity functions include Euclidean distance,
City Block distance (Bullinaria and Levy; 2007),
and Dice and Jaccard coefficients (Frakes and
Baeza-Yates, 1992), etc. Measures of monolin-
gual sense similarity have been widely used in
many applications, such as synonym recognizing
(Landauer and Dumais, 1997), word clustering
(Pantel and Lin 2002), word sense disambigua-
tion (Yuret and Yatbaz 2009), etc.
Use of the vector space model to compute
sense similarity has also been adapted to the mul-
tilingual condition, based on the assumption that
two terms with similar meanings often occur in
comparable contexts across languages. Fung
(1998) and Rapp (1999) adopted VSM for the
application of extracting translation pairs from
comparable or even unrelated corpora. The vec-
tors in different languages are first mapped to a
common space using an initial bilingual dictio-
nary, and then compared.
However, there is no previous work that uses
the VSM to compute sense similarity for terms
from parallel corpora. The sense similarities, i.e.
the translation probabilities in a translation mod-
el, for units from parallel corpora are mainly
based on the co-occurrence counts of the two
units. Therefore, questions emerge: how good is
the sense similarity computed via VSM for two
units from parallel corpora? Is it useful for multi-
lingual applications, such as statistical machine
translation (SMT)?
In this paper, we try to answer these questions,
focusing on sense similarity applied to the SMT
task. For this task, translation rules are heuristi-
cally extracted from automatically word-aligned
sentence pairs. Due to noise in the training cor-
pus or wrong word alignment, the source and
target sides of some rules are not semantically
equivalent, as can be seen from the following
</bodyText>
<page confidence="0.97684">
834
</page>
<note confidence="0.943686">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 834–843,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999432923076923">
real examples which are taken from the rule table
built on our training data (Section 5.1):
The source and target sides of the rules with (*)
at the end are not semantically equivalent; it
seems likely that measuring the semantic similar-
ity from their context between the source and
target sides of rules might be helpful to machine
translation.
In this work, we first propose new algorithms
to compute the sense similarity between two
units (unit here includes word, phrase, rule, etc.)
in different languages by using their contexts.
Second, we use the sense similarities between the
source and target sides of a translation rule to
improve statistical machine translation perfor-
mance.
This work attempts to measure directly the
sense similarity for units from different languag-
es by comparing their contexts1. Our contribution
includes proposing new bilingual sense similarity
algorithms and applying them to machine trans-
lation.
We chose a hierarchical phrase-based SMT
system as our baseline; thus, the units involved
in computation of sense similarities are hierar-
chical rules.
</bodyText>
<sectionHeader confidence="0.933494" genericHeader="method">
2 Hierarchical phrase-based MT system
</sectionHeader>
<bodyText confidence="0.999495">
The hierarchical phrase-based translation method
(Chiang, 2005; Chiang, 2007) is a formal syntax-
based translation modeling method; its transla-
tion model is a weighted synchronous context
free grammar (SCFG). No explicit linguistic syn-
tactic information appears in the model. An
SCFG rule has the following form:
where X is a non-terminal symbol shared by all
the rules; each rule has at most two non-
terminals. a (y ) is a source (target) string con-
sisting of terminal and non-terminal symbols. ~
defines a one-to-one correspondence between
non-terminals in a and y .
</bodyText>
<footnote confidence="0.9950452">
1 There has been a lot of work (more details in Section 7) on
applying word sense disambiguation (WSD) techniques in
SMT for translation selection. However, WSD techniques
for SMT do so indirectly, using source-side context to help
select a particular translation for a source rule.
</footnote>
<table confidence="0.9701723">
source target
Ini. phr. 他 出席 了 会议 he attended the meeting
Rule 1 他 出席 了 X1 he attended X1
Context 1 会议 the, meeting
Rule 2 会议 the meeting
Context 2 他, 出席, 了 he, attended
Rule 3 他 X1会议出席, 了 he X1 the meeting
Context 3 attended
Rule 4 出席 了他,会议 attended
Context 4 he, the, meeting
</table>
<figureCaption confidence="0.998381">
Figure 1: example of hierarchical rule pairs and their
context features.
</figureCaption>
<bodyText confidence="0.9731408">
Rule frequencies are counted during rule ex-
traction over word-aligned sentence pairs, and
they are normalized to estimate features on rules.
Following (Chiang, 2005; Chiang, 2007), 4 fea-
tures are computed for each rule:
</bodyText>
<listItem confidence="0.9067495">
• P(y  |a) and P(a  |y) are direct and in-
verse rule-based conditional probabilities;
• Pw (y  |a) and Pw (a  |y) are direct and in-
verse lexical weights (Koehn et al., 2003).
</listItem>
<bodyText confidence="0.99994475">
Empirically, this method has yielded better
performance on language pairs such as Chinese-
English than the phrase-based method because it
permits phrases with gaps; it generalizes the
normal phrase-based models in a way that allows
long-distance reordering (Chiang, 2005; Chiang,
2007). We use the Joshua implementation of the
method for decoding (Li et al., 2009).
</bodyText>
<sectionHeader confidence="0.947519" genericHeader="method">
3 Bag-of-Words Vector Space Model
</sectionHeader>
<bodyText confidence="0.999944">
To compute the sense similarity via VSM, we
follow the previous work (Lin, 1998) and
represent the source and target side of a rule by
feature vectors. In our work, each feature corres-
ponds to a context word which co-occurs with
the translation rule.
</bodyText>
<subsectionHeader confidence="0.999773">
3.1 Context Features
</subsectionHeader>
<bodyText confidence="0.999996">
In the hierarchical phrase-based translation me-
thod, the translation rules are extracted by ab-
stracting some words from an initial phrase pair
(Chiang, 2005). Consider a rule with non-
terminals on the source and target side; for a giv-
en instance of the rule (a particular phrase pair in
the training corpus), the context will be the
words instantiating the non-terminals. In turn, the
context for the sub-phrases that instantiate the
non-terminals will be the words in the remainder
of the phrase pair. For example in Figure 1, if we
</bodyText>
<equation confidence="0.988278333333333">
~
X →
a, y,
</equation>
<page confidence="0.970119">
835
</page>
<bodyText confidence="0.99958075">
attended the meeting, and we extract four rules
fore, the and meeting are context features of tar-
get pattern he attended X1; he and attended are
the context features of the meeting; attended is
the context feature of he X1 the meeting; also he,
the and meeting are the context feature of at-
tended (in each case, there are also source-side
context features).
</bodyText>
<subsectionHeader confidence="0.992255">
3.2 Bag-of-Words Model
</subsectionHeader>
<bodyText confidence="0.999090384615385">
For each side of a translation rule pair, its context
words are all collected from the training data,
and two “bags-of-words” which consist of col-
lections of source and target context words co-
occurring with the rule’s source and target sides
are created.
where fi(1 &lt;— i &lt;— I) are source context words
which co-occur with the source side of rule a ,
and ej (1 &lt;— j &lt;— J) are target context words
which co-occur with the target side of rule y .
Therefore, we can represent source and target
sides of the rule by vectors v� f and ve as in Eq-
uation (2):
</bodyText>
<equation confidence="0.6390795">
vf ={wf1,wf2,...,wfI
{we1 , we2 ,..., w,
</equation>
<bodyText confidence="0.99952275">
where w fi and wej are values for each source
and target context feature; normally, these values
are based on the counts of the words in the cor-
responding bags.
</bodyText>
<subsectionHeader confidence="0.999888">
3.3 Feature Weighting Schemes
</subsectionHeader>
<bodyText confidence="0.9991295">
We use pointwise mutual information (Church et
al., 1990) to compute the feature values. Let c
</bodyText>
<equation confidence="0.582026">
( c E Bf or c E Be ) be a context word and
</equation>
<bodyText confidence="0.985575666666667">
F(r, c) be the frequency count of a rule r (a or
y ) co-occurring with the context word c. The
pointwise mutual information MI(r,c) is de-
</bodyText>
<equation confidence="0.971897125">
fined as:
F r c
( , )
w r c MI r c
( , ) ( , )
= = log F(r) ´ log F(c)
N N
N (3)
</equation>
<bodyText confidence="0.999839">
where N is the total frequency counts of all rules
and their context words. Since we are using this
value as a weight, following (Turney, 2001), we
drop log, N and F(r) . Thus (3) simplifies to:
</bodyText>
<equation confidence="0.99786475">
F r c
( , )
w r c = (4)
( , ) F c ( )
</equation>
<bodyText confidence="0.9568168">
It can be seen as an estimate of P(r  |c) , the em-
pirical probability of observing r given c.
A problem with P(r  |c) is that it is biased
towards infrequent words/features. We therefore
smooth w(r, c) with add-k smoothing:
</bodyText>
<equation confidence="0.940666">
F(r , c)+k F(r,c)+k
∑ (F(r,c)+k) F(c)+
i
=1
</equation>
<bodyText confidence="0.996528">
where k is a tunable global smoothing constant,
and R is the number of rules.
</bodyText>
<sectionHeader confidence="0.989309" genericHeader="method">
4 Similarity Functions
</sectionHeader>
<bodyText confidence="0.99991775">
There are many possibilities for calculating simi-
larities between bags-of-words in different lan-
guages. We consider IBM model 1 probabilities
and cosine distance similarity functions.
</bodyText>
<subsectionHeader confidence="0.584031">
4.1 IBM Model 1 Probabilities
</subsectionHeader>
<bodyText confidence="0.99905225">
For the IBM model 1 similarity function, we take
the geometric mean of symmetrized conditional
IBM model 1 (Brown et al., 1993) bag probabili-
ties, as in Equation (6).
</bodyText>
<equation confidence="0.8589415">
sim a y = sqrt P Bf Be × P Be Bf
( , ) ( (  |) (  |)) (6)
</equation>
<bodyText confidence="0.617304666666667">
To compute P(B f  |Be), IBM model 1 as-
sumes that all source words are conditionally
independent, so that:
</bodyText>
<equation confidence="0.9990598">
I
P Bf Be
(  |) = Õ p f i Be
(  |) (7)
i=1
</equation>
<bodyText confidence="0.99960575">
To compute, we use a “Noisy-OR” combina-
tion which has shown better performance than
standard IBM model 1 probability, as described
in (Zens and Ney, 2004):
</bodyText>
<equation confidence="0.9952802">
p(fi  |Be)=1−p(fi  |Be) (8)
J
p f i Be
(  |) Õ(1−p(fi  |ej)) (9)
j=1
</equation>
<bodyText confidence="0.994297333333333">
where p(fi  |Be) is the probability that fi is not
in the translation of Be, and is the IBM model 1
probability.
</bodyText>
<subsectionHeader confidence="0.985205">
4.2 Vector Space Mapping
</subsectionHeader>
<bodyText confidence="0.999015">
A common way to calculate semantic similarity
is by vector space cosine distance; we will also
</bodyText>
<figure confidence="0.960344857142857">
{ ,
f f
1 2
{,,..., }
e e e
1 2 J
Bf
Be
(1)
}
,...,
fI
ve
}
}
(2)
log
w r c R
( , ) =
kR
(5)
</figure>
<page confidence="0.993795">
836
</page>
<bodyText confidence="0.999470928571429">
use this similarity function in our algorithm.
However, the two vectors in Equation (2) cannot
be directly compared because the axes of their
spaces represent different words in different lan-
guages, and also their dimensions I and J are not
assured to be the same. Therefore, we need to
first map a vector into the space of the other vec-
tor, so that the similarity can be calculated. Fung
(1998) and Rapp (1999) map the vector one-
dimension-to-one-dimension (a context word is a
dimension in each vector space) from one lan-
guage to another language via an initial bilingual
dictionary. We follow (Zhao et al., 2004) to do
vector space mapping.
Our goal is – given a source pattern – to dis-
tinguish between the senses of its associated tar-
get patterns. Therefore, we map all vectors in
target language into the vector space in the
source language. What we want is a representa-
tion v�a in the source language space of the target
vector v�e . To get v�a, we can let fi
wa , the weight
of the ith source feature, be a linear combination
over target features. That is to say, given a
source feature weight for fi, each target feature
weight is linked to it with some probability. So
that we can calculate a transformed vector from
the target vectors by calculating weights fi
</bodyText>
<equation confidence="0.8748982">
wa us-
ing a translation lexicon:
J
wai=EPr(f  |ej )we (10)
j=1
</equation>
<bodyText confidence="0.880525">
where p(fi  |e j) is a lexical probability (we use
IBM model 1 probability). Now the source vec-
tor and the mapped vector va have the same di-
mensions as shown in (11):
v � w w
={ , ,..., w
</bodyText>
<figure confidence="0.980328888888889">
f f f f
1 2 I
~ f
{ , ,...,
f f
1 2 I
v w w
= w
a a a a
</figure>
<subsectionHeader confidence="0.902173">
4.3 Naïve Cosine Distance Similarity
</subsectionHeader>
<bodyText confidence="0.9672735">
The standard cosine distance is defined as the
inner product of the two vectors v—f and va nor-
malized by their norms. Based on Equation (10)
and (11), it is easy to derive the similarity as fol-
</bodyText>
<equation confidence="0.497134269230769">
lows:
� �
}
(11)
}
EE
wfi
Pr(fi  |ej
)we j
(12)
= i =1 j=1
I IEwf&apos;)sqrt(Ew 2)
I = 1 i = 1
)
� �
=
v f ⋅ va
=
cos(
v f,va
α γ
sim ( , )
|vf |⋅|va |
I J
(
sqrt
</equation>
<bodyText confidence="0.9973485">
where I and J are the number of the words in
source and target bag-of-words; w fi and wej are
values of source and target features; fi
wa is the
transformed weight mapped from all target fea-
tures to the source dimension at word fi.
</bodyText>
<subsectionHeader confidence="0.997082">
4.4 Improved Similarity Function
</subsectionHeader>
<bodyText confidence="0.997969375">
To incorporate more information than the origi-
nal similarity functions – IBM model 1 proba-
bilities in Equation (6) and naïve cosine distance
similarity function in Equation (12) – we refine
the similarity function and propose a new algo-
rithm.
As shown in Figure 2, suppose that we have a
rule pair (α, γ) . full
</bodyText>
<subsubsectionHeader confidence="0.462649">
Cf and full
</subsubsectionHeader>
<bodyText confidence="0.928893285714286">
Ce are the contexts
extracted according to the definition in section 3
from the full training data for α and for γ , re-
spectively. cooc
Cf and cooc
Ce are the contexts for
α and γ when α and γ co-occur. Obviously,
</bodyText>
<figure confidence="0.5740004">
full
they satisfy the constraints: C fcooc ⊆ C and
C e ⊆ C . Therefore, the original similarity
cooc full
e
</figure>
<bodyText confidence="0.882134">
functions are to compare the two context vectors
built on full training data directly, as shown in
Equation (13).
</bodyText>
<equation confidence="0.863423">
sim(α, γ) =sim(CfullCfull) e(13)
</equation>
<bodyText confidence="0.9168798">
Then, we propose a new similarity function as
follows:
(, )
α γ
sim
</bodyText>
<table confidence="0.98873075">
sim C C λ coocfull λ3 (14)
( , ) 1 2
full cooc ( , )
f f cooc λ ( , )
cooc
sim C C
⋅ ⋅sim C C
f e e e
</table>
<bodyText confidence="0.999183333333333">
where the parameters λi (i=1,2,3) can be tuned
via minimal error rate training (MERT) (Och,
2003).
</bodyText>
<figureCaption confidence="0.980029">
Figure 2: contexts for rule α and γ .
</figureCaption>
<bodyText confidence="0.9999196">
A unit’s sense is defined by all its contexts in
the whole training data; it may have a lot of dif-
ferent senses in the whole training data. Howev-
er, when it is linked with another unit in the other
language, its sense pool is constrained and is just
</bodyText>
<figure confidence="0.9794194375">
α
γ
Cfull
f
Cfull
e
Ccooc
f
Ccooc
e
=
837
a subset of the whole sense set. ( , cooc)
sim Cf C
full
f
</figure>
<bodyText confidence="0.971565333333333">
is the metric which evaluates the similarity be-
tween the whole sense pool of a and the sense
pool when a co-occurs with y ;
</bodyText>
<equation confidence="0.92468375">
sim Ce C is the analogous similarity me-
( , cooc)
full
e
</equation>
<bodyText confidence="0.962971125">
tric for y . They range from 0 to 1. These two
metrics both evaluate the similarity for two vec-
tors in the same language, so using cosine dis-
tance to compute the similarity is straightfor-
ward. And we can set a relatively large size for
the vector, since it is not necessary to do vector
mapping as the vectors are in the same language.
sim C f C computes the similarity between
</bodyText>
<figure confidence="0.8430965">
( , cooc )
cooc
e
the context vectors when a and y co-occur. We
may compute ( , cooc )
sim C f C by using IBM
cooc
e
</figure>
<bodyText confidence="0.998432777777778">
model 1 probability and cosine distance similari-
ty functions as Equation (6) and (12). Therefore,
on top of the degree of bilingual semantic simi-
larity between a source and a target translation
unit, we have also incorporated the monolingual
semantic similarity between all occurrences of a
source or target unit, and that unit’s occurrence
as part of the given rule, into the sense similarity
measure.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999829">
We evaluate the algorithm of bilingual sense si-
milarity via machine translation. The sense simi-
larity scores are used as feature functions in the
translation model.
</bodyText>
<subsectionHeader confidence="0.965403">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999265">
We evaluated with different language pairs: Chi-
nese-to-English, and German-to-English. For
Chinese-to-English tasks, we carried out the ex-
periments in two data conditions. The first one is
the large data condition, based on training data
for the NIST 2 2009 evaluation Chinese-to-
English track. In particular, all the allowed bilin-
gual corpora except the UN corpus and Hong
Kong Hansard corpus have been used for esti-
mating the translation model. The second one is
the small data condition where only the FBIS3
corpus is used to train the translation model. We
trained two language models: the first one is a 4-
gram LM which is estimated on the target side of
the texts used in the large data condition. The
second LM is a 5-gram LM trained on the so-
</bodyText>
<footnote confidence="0.9316505">
2 http://www.nist.gov/speech/tests/mt
3 LDC2003E14
</footnote>
<bodyText confidence="0.979612214285714">
called English Gigaword corpus. Both language
models are used for both tasks.
We carried out experiments for translating
Chinese to English. We use the same develop-
ment and test sets for the two data conditions.
We first created a development set which used
mainly data from the NIST 2005 test set, and
also some balanced-genre web-text from the
NIST training material. Evaluation was per-
formed on the NIST 2006 and 2008 test sets. Ta-
ble 1 gives figures for training, development and
test corpora; |S |is the number of the sentences,
and |W |is the number of running words. Four
references are provided for all dev and test sets.
</bodyText>
<table confidence="0.999731181818182">
Chi Eng
Parallel Large |S |3,322K
Train Data
|W |64.2M 62.6M
Small |S |245K
Data
|W |9.0M 10.5M
Dev |S |1,506 1,506x4
Test NIST06 |S |1,664 1,664x4
NIST08 |S |1,357 1,357x4
Gigaword |S |- 11.7M
</table>
<tableCaption confidence="0.995565">
Table 1: Statistics of training, dev, and test sets for
Chinese-to-English task.
</tableCaption>
<bodyText confidence="0.99031575">
For German-to-English tasks, we used WMT
20064 data sets. The parallel training data con-
tains 21 million target words; both the dev set
and test set contain 2000 sentences; one refer-
ence is provided for each source input sentence.
Only the target-language half of the parallel
training data are used to train the language model
in this task.
</bodyText>
<subsectionHeader confidence="0.882535">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999884357142857">
For the baseline, we train the translation model
by following (Chiang, 2005; Chiang, 2007) and
our decoder is Joshua5, an open-source hierar-
chical phrase-based machine translation system
written in Java. Our evaluation metric is IBM
BLEU (Papineni et al., 2002), which performs
case-insensitive matching of n-grams up to n = 4.
Following (Koehn, 2004), we use the bootstrap-
resampling test to do significance testing.
By observing the results on dev set in the addi-
tional experiments, we first set the smoothing
constant k in Equation (5) to 0.5.
Then, we need to set the sizes of the vectors to
balance the computing time and translation accu-
</bodyText>
<footnote confidence="0.9981385">
4 http://www.statmt.org/wmt06/
5 http://www.cs.jhu.edu/~ccb/joshua/index.html
</footnote>
<page confidence="0.997187">
838
</page>
<bodyText confidence="0.980597777777778">
racy, i.e., we keep only the top N context words
with the highest feature value for each side of a
rule 6 . In the following, we use “Alg1” to
represent the original similarity functions which
compare the two context vectors built on full
training data, as in Equation (13); while we use
“Alg2” to represent the improved similarity as in
Equation (14). “IBM” represents IBM model 1
probabilities, and “COS” represents cosine dis-
tance similarity function.
After carrying out a series of additional expe-
riments on the small data condition and observ-
ing the results on the dev set, we set the size of
the vector to 500 for Alg1; while for Alg2, we
set the sizes of full
Cf
sizes of cooc
Cf and cooc
Ce N2 to 100.
The sizes of the vectors in Alg2 are set in the
following process: first, we set N2 to 500 and let
N1 range from 500 to 3,000, we observed that the
dev set got best performance when N1 was 1000;
then we set N1 to 1000 and let N1 range from 50
to 1000, we got best performance when N1 =100.
We use this setting as the default setting in all
remaining experiments.
</bodyText>
<table confidence="0.999447666666667">
Algorithm NIST’06 NIST’08
Baseline 27.4 21.2
Alg1 IBM 27.8* 21.5
Alg1 COS 27.8* 21.5
Alg2 IBM 27.9* 21.6*
Alg2 COS 28.1** 21.7*
</table>
<tableCaption confidence="0.79326325">
Table 2: Results (BLEU%) of small data Chinese-to-
English NIST task. Alg1 represents the original simi-
larity functions as in Equation (13); while Alg2
represents the improved similarity as in Equation
(14). IBM represents IBM model 1 probability, and
COS represents cosine distance similarity function. *
or ** means result is significantly better than the
baseline (p &lt; 0.05 or p &lt; 0.01, respectively).
</tableCaption>
<table confidence="0.99982">
Ch-En De-En
Algorithm NIST’06 NIST’08 Test’06
Baseline 31.0 23.8 26.9
Alg2 IBM 31.5* 24.5** 27.2*
Alg2 COS 31.6** 24.5** 27.3*
</table>
<tableCaption confidence="0.967920333333333">
Table 3: Results (BLEU%) of large data Chinese-to-
English NIST task and German-to-English WMT
task.
</tableCaption>
<footnote confidence="0.77088925">
6 We have also conducted additional experiments by remov-
ing the stop words from the context vectors; however, we
did not observe any consistent improvement. So we filter
the context vectors by only considering the feature values.
</footnote>
<bodyText confidence="0.999588818181818">
Table 2 compares the performance of Alg1
and Alg2 on the Chinese-to-English small data
condition. Both Alg1 and Alg2 improved the
performance over the baseline, and Alg2 ob-
tained slight and consistent improvements over
Alg1. The improved similarity function Alg2
makes it possible to incorporate monolingual
semantic similarity on top of the bilingual se-
mantic similarity, thus it may improve the accu-
racy of the similarity estimate. Alg2 significantly
improved the performance over the baseline. The
Alg2 cosine similarity function got 0.7 BLEU-
score (p&lt;0.01) improvement over the baseline
for NIST 2006 test set, and a 0.5 BLEU-score
(p&lt;0.05) for NIST 2008 test set.
Table 3 reports the performance of Alg2 on
Chinese-to-English NIST large data condition
and German-to-English WMT task. We can see
that IBM model 1 and cosine distance similarity
function both obtained significant improvement
on all test sets of the two tasks. The two similari-
ty functions obtained comparable results.
</bodyText>
<sectionHeader confidence="0.989881" genericHeader="method">
6 Analysis and Discussion
</sectionHeader>
<subsectionHeader confidence="0.999977">
6.1 Effect of Single Features
</subsectionHeader>
<bodyText confidence="0.997527">
In Alg2, the similarity score consists of three
</bodyText>
<equation confidence="0.948056666666666">
parts as in Equation (14): ( , cooc)
sim C f C
full ,
f
( , cooc)
full sim C f C
cooc
sim C e C , and ( , cooc) ; where
e e
sim Cf C could be computed by IBM mod-
( , cooc)
cooc
e
el 1 probabilities ( , cooc)
simIBM C f C or cosine dis-
cooc
e
tance similarity function simCOS Cf C
cooc
( , cooc) .
e
</equation>
<bodyText confidence="0.99985">
Therefore, our first study is to determine which
one of the above four features has the most im-
pact on the result. Table 4 shows the results ob-
tained by using each of the 4 features. First, we
</bodyText>
<equation confidence="0.9173375">
can see that ( , cooc)
simIBM C f C always gives a
cooc
e
better improvement than ( ,
simCOS Cf cooc
is because ( , cooc)
simIBM C f C scores are more
cooc
e
</equation>
<bodyText confidence="0.999656857142857">
diverse than the latter when the number of con-
text features is small (there are many rules that
have only a few contexts.) For an extreme exam-
ple, suppose that there is only one context word
in each vector of source and target context fea-
tures, and the translation probability of the two
context words is not 0. In this case,
</bodyText>
<figure confidence="0.54699055">
simIBM C f C reflects the translation proba-
( , cooc )
cooc
e
bility of the context word pair, while
simCOS C f C is always 1.
( , cooc)
cooc
e
Second, ( , cooc)
sim Cf C and ( , cooc)
full sim Ce C
full
f e
also give some improvements even when used
and full N1 to 1000, and the
Ce
C . This
cooc)
e
</figure>
<page confidence="0.996674">
839
</page>
<bodyText confidence="0.943458285714286">
independently. For a possible explanation, con-
sider the following example. The Chinese word
“ 红 ” can translate to “red”, “communist”, or
“hong” (the transliteration of 红, when it is used
in a person’s name). Since these translations are
likely to be associated with very different source
contexts, each will have a low ( , cooc)
</bodyText>
<figure confidence="0.6802039">
sim Cf C
full
f
score. Another Chinese word 小溪 may translate
into synonymous words, such as “brook”,
“stream”, and “rivulet”, each of which will have
a high ( , cooc)
sim Cf C score. Clearly, 红 is a
full
f
</figure>
<figureCaption confidence="0.818867">
more “dangerous” word than 小溪, since choos-
ing the wrong translation for it would be a bad
mistake. But if the two words have similar trans-
lation distributions, the system cannot distinguish
between them. The monolingual similarity scores
give it the ability to avoid “dangerous” words,
and choose alternatives (such as larger phrase
translations) when available.
</figureCaption>
<bodyText confidence="0.8159298">
Third, the similarity function of Alg2 consis-
tently achieved further improvement by incorpo-
rating the monolingual similarities computed for
the source and target side. This confirms the ef-
fectiveness of our algorithm.
</bodyText>
<table confidence="0.999545470588236">
CE.-LD CE.-SD
testset (NIST) ’06 ’08 ’06 ’08
Baseline 31.0 23.8 27.4 21.2
full 31.1 24.3 27.5 21.3
( , cooc)
sim Cf C f
full 31.1 23.9 27.9 21.5
( , cooc)
sim Ce C e
cooc 31.4 24.3 27.9 21.5
( , cooc)
simIBM Cf C e
cooc 31.2 23.9 27.7 21.4
( , cooc)
simCOS Cf C e
Alg2 IBM 31.5 24.5 27.9 21.6
Alg2 COS 31.6 24.5 28.1 21.7
</table>
<tableCaption confidence="0.99621">
Table 4: Results (BLEU%) of Chinese-to-English
large data (CE.-LD) and small data (CE.-SD) NIST
task by applying one feature.
</tableCaption>
<subsectionHeader confidence="0.992342">
6.2 Effect of Combining the Two Similari-
ties
</subsectionHeader>
<bodyText confidence="0.9977583">
We then combine the two similarity scores by
using both of them as features to see if we could
obtain further improvement. In practice, we use
the four features in Table 4 together.
Table 5 reports the results on the small data
condition. We observed further improvement on
dev set, but failed to get the same improvements
on test sets or even lost performance. Since the
IBM+COS configuration has one extra feature, it
is possible that it overfits the dev set.
</bodyText>
<table confidence="0.9997666">
Algorithm Dev NIST’06 NIST’08
Baseline 20.2 27.4 21.2
Alg2 IBM 20.5 27.9 21.6
Alg2 COS 20.6 28.1 21.7
Alg2 IBM+COS 20.8 27.9 21.5
</table>
<tableCaption confidence="0.998555333333333">
Table 5: Results (BLEU%) for combination of two
similarity scores. Further improvement was only ob-
tained on dev set but not on test sets.
</tableCaption>
<subsectionHeader confidence="0.9992605">
6.3 Comparison with Simple Contextual
Features
</subsectionHeader>
<bodyText confidence="0.99978">
Now, we try to answer the question: can the si-
milarity features computed by the function in
Equation (14) be replaced with some other sim-
ple features? We did additional experiments on
small data Chinese-to-English task to test the
following features: (15) and (16) represent the
sum of the counts of the context words in Cfull,
while (17) represents the proportion of words in
the context of α that appeared in the context of
the rule ( α, γ ); similarly, (18) is related to the
properties of the words in the context of γ .
</bodyText>
<equation confidence="0.965757">
N f (α) = Efi∈Cf.ll F(α, fi) (15)
Ne (γ) = Eej∈01l F(γ,ej) (16)
E f C i
( , )
α
F f
i ∈ f cooc
</equation>
<bodyText confidence="0.3031865">
( )
α
</bodyText>
<table confidence="0.999713142857143">
Feature Dev NIST’06 NIST’08
Baseline 20.2 27.4 21.2
+Nf 20.5 27.6 21.4
+Ne 20.5 27.5 21.3
+Ef 20.4 27.5 21.2
+Ee 20.4 27.3 21.2
+Nf+Ne 20.5 27.5 21.3
</table>
<tableCaption confidence="0.743184666666667">
t effect on the test sets.
Table 6 shows results obtained by adding the
above features to the system for the small data
</tableCaption>
<figure confidence="0.97567237037037">
E=
eN ( γ )
e
α γ
, )
E
ej∈CcoocF(γ,ej)
e
(
Ef(α,γ)= N
f
(17)
(18)
where
and
ej) are the frequency
counts of rule
or
co-occurringwith the
context word
or ej
F(α,fi)
F(γ,
α
γ
fi
respectively.
</figure>
<tableCaption confidence="0.689163">
Table 6: Results (BLEU%) of using simple features
based on context on small data NIST task. Some im-
provements are obtained on dev set, but there was no
significan
</tableCaption>
<page confidence="0.976823">
840
</page>
<bodyText confidence="0.999769285714286">
condition. Although all these features have ob-
tained some improvements on dev set, there was
no significant effect on the test sets. This means
simple features based on context, such as the
sum of the counts of the context features, are not
as helpful as the sense similarity computed by
Equation (14).
</bodyText>
<subsectionHeader confidence="0.946323">
6.4 Null Context Feature
</subsectionHeader>
<bodyText confidence="0.999879904761905">
There are two cases where no context word can
be extracted according to the definition of con-
text in Section 3.1. The first case is when a rule
pair is always a full sentence-pair in the training
data. The second case is when for some rule
pairs, either their source or target contexts are
out of the span limit of the initial phrase, so that
we cannot extract contexts for those rule-pairs.
For Chinese-to-English NIST task, there are
about 1% of the rules that do not have contexts;
for German-to-English task, this number is about
0.4%. We assign a uniform number as their bi-
lingual sense similarity score, and this number is
tuned through MERT. We call it the null context
feature. It is included in all the results reported
from Table 2 to Table 6. In Table 7, we show the
weight of the null context feature tuned by run-
ning MERT in the experiments reported in Sec-
tion 5.2. We can learn that penalties always dis-
courage using those rules which have no context
to be extracted.
</bodyText>
<table confidence="0.98190675">
Alg. Task
CE_SD CE_LD DE
Alg2 IBM -0.09 -0.37 -0.15
Alg2 COS -0.59 -0.42 -0.36
</table>
<tableCaption confidence="0.970018">
Table 7: Weight learned for employing the null con-
text feature. CE_SD, CE_LD and DE are Chinese-to-
English small data task, large data task and German-
to-English task respectively.
</tableCaption>
<subsectionHeader confidence="0.852235">
6.5 Discussion
</subsectionHeader>
<bodyText confidence="0.99574075862069">
Our aim in this paper is to characterize the se-
mantic similarity of bilingual hierarchical rules.
We can make several observations concerning
our features:
1) Rules that are largely syntactic in nature,
“meanings” and therefore lower similarity
scores. It could be that the gains we obtained
come simply from biasing the system against
such rules. However, the results in table 6 show
that this is unlikely to be the case: features that
just count context words help very little.
2) In addition to bilingual similarity, Alg2 re-
lies on the degree of monolingual similarity be-
tween the sense of a source or target unit within a
rule, and the sense of the unit in general. This has
a bias in favor of less ambiguous rules, i.e. rules
involving only units with closely related mean-
ings. Although this bias is helpful on its own,
possibly due to the mechanism we outline in sec-
tion 6.1, it appears to have a synergistic effect
when used along with the bilingual similarity
feature.
3) Finally, we note that many of the features
we use for capturing similarity, such as the con-
text “the, of” for instantiations of X in the unit
the X of, are arguably more syntactic than seman-
tic. Thus, like other “semantic” approaches, ours
can be seen as blending syntactic and semantic
information.
</bodyText>
<sectionHeader confidence="0.99998" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999989133333333">
There has been extensive work on incorporating
semantics into SMT. Key papers by Carpuat and
Wu (2007) and Chan et al (2007) showed that
word-sense disambiguation (WSD) techniques
relying on source-language context can be effec-
tive in selecting translations in phrase-based and
hierarchical SMT. More recent work has aimed
at incorporating richer disambiguating features
into the SMT log-linear model (Gimpel and
Smith, 2008; Chiang et al, 2009); predicting co-
herent sets of target words rather than individual
phrase translations (Bangalore et al, 2009; Maus-
er et al, 2009); and selecting applicable rules in
hierarchical (He et al, 2008) and syntactic (Liu et
al, 2008) translation, relying on source as well as
target context. Work by Wu and Fung (2009)
breaks new ground in attempting to match se-
mantic roles derived from a semantic parser
across source and target languages.
Our work is different from all the above ap-
proaches in that we attempt to discriminate
among hierarchical rules based on: 1) the degree
of bilingual semantic similarity between source
and target translation units; and 2) the monolin-
gual semantic similarity between occurrences of
source or target units as part of the given rule,
and in general. In another words, WSD explicitly
tries to choose a translation given the current
source context, while our work rates rule pairs
independent of the current context.
</bodyText>
<sectionHeader confidence="0.993759" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9574925">
In this paper, we have proposed an approach that
uses the vector space model to compute the sense
</bodyText>
<page confidence="0.991742">
841
</page>
<bodyText confidence="0.999861857142857">
similarity for terms from parallel corpora and
applied it to statistical machine translation. We
saw that the bilingual sense similarity computed
by our algorithm led to significant improve-
ments. Therefore, we can answer the questions
proposed in Section 1. We have shown that the
sense similarity computed between units from
parallel corpora by means of our algorithm is
helpful for at least one multilingual application:
statistical machine translation.
Finally, although we described and evaluated
bilingual sense similarity algorithms applied to a
hierarchical phrase-based system, this method is
also suitable for syntax-based MT systems and
phrase-based MT systems. The only difference is
the definition of the context. For a syntax-based
system, the context of a rule could be defined
similarly to the way it was defined in the work
described above. For a phrase-based system, the
context of a phrase could be defined as its sur-
rounding words in a given size window. In our
future work, we may try this algorithm on syn-
tax-based MT systems and phrase-based MT sys-
tems with different context features. It would
also be possible to use this technique during
training of an SMT system – for instance, to im-
prove the bilingual word alignment or reduce the
training data noise.
</bodyText>
<sectionHeader confidence="0.998696" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999909135135135">
S. Bangalore, S. Kanthak, and P. Haffner. 2009. Sta-
tistical Machine Translation through Global Lexi-
cal Selection and Sentence Reconstruction. In:
Goutte et al (ed.), Learning Machine Translation.
MIT Press.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra &amp;
R. L. Mercer. 1993. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19(2) 263-312.
J. Bullinaria and J. Levy. 2007. Extracting semantic
representations from word co-occurrence statistics:
A computational study. Behavior Research Me-
thods, 39 (3), 510–526.
M. Carpuat and D. Wu. 2007. Improving Statistical
Machine Translation using Word Sense Disambig-
uation. In: Proceedings of EMNLP, Prague.
M. Carpuat. 2009. One Translation per Discourse. In:
Proceedings of NAACL HLT Workshop on Se-
mantic Evaluations, Boulder, CO.
Y. Chan, H. Ng and D. Chiang. 2007. Word Sense
Disambiguation Improves Statistical Machine
Translation. In: Proceedings of ACL, Prague.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In: Proceedings
of ACL, pp. 263–270.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics. 33(2):201–228.
D. Chiang, W. Wang and K. Knight. 2009. 11,001
new features for statistical machine translation. In:
Proc. NAACL HLT, pp. 218–226.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22–29.
W. B. Frakes and R. Baeza-Yates, editors. 1992. In-
formation Retrieval, Data Structure and Algo-
rithms. Prentice Hall.
P. Fung. 1998. A statistical view on bilingual lexicon
extraction: From parallel corpora to non-parallel
corpora. In: Proceedings of AMTA, pp. 1–17. Oct.
Langhorne, PA, USA.
J. Gimenez and L. Marquez. 2009. Discriminative
Phrase Selection for SMT. In: Goutte et al (ed.),
Learning Machine Translation. MIT Press.
K. Gimpel and N. A. Smith. 2008. Rich Source-Side
Context for Statistical Machine Translation. In:
Proceedings of WMT, Columbus, OH.
Z. Harris. 1954. Distributional structure. Word,
10(23): 146-162.
Z. He, Q. Liu, and S. Lin. 2008. Improving Statistical
Machine Translation using Lexicalized Rule Selec-
tion. In: Proceedings of COLING, Manchester,
UK.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In: Proceedings of ACL. pp.
268-275. Pittsburgh, PA.
P. Koehn, F. Och, D. Marcu. 2003. Statistical Phrase-
Based Translation. In: Proceedings of HLT-
NAACL. pp. 127-133, Edmonton, Canada
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In: Proceedings of
EMNLP, pp. 388–395. July, Barcelona, Spain.
T. Landauer and S. T. Dumais. 1997. A solution to
Plato’s problem: The Latent Semantic Analysis
theory of the acquisition, induction, and representa-
tion of knowledge. Psychological Review. 104:211-
240.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S.
Khudanpur, L. Schwartz, W. Thornton, J. Weese
and O. Zaidan, 2009. Joshua: An Open Source
Toolkit for Parsing-based Machine Translation. In:
Proceedings of the WMT. March. Athens, Greece.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In: Proceedings of COLING/ACL-
98. pp. 768-774. Montreal, Canada.
</reference>
<page confidence="0.977307">
842
</page>
<reference confidence="0.99971362">
Q. Liu, Z. He, Y. Liu and S. Lin. 2008. Maximum
Entropy based Rule Selection Model for Syntax-
based Statistical Machine Translation. In: Proceed-
ings of EMNLP, Honolulu, Hawaii.
K. Lund, and C. Burgess. 1996. Producing high-
dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28 (2), 203–208.
A. Mauser, S. Hasan and H. Ney. 2009. Extending
Statistical Machine Translation with Discrimina-
tive and Trigger-Based Lexicon Models. In: Pro-
ceedings of EMNLP, Singapore.
F. Och. 2003. Minimum error rate training in statistic-
al machine translation. In: Proceedings of ACL.
Sapporo, Japan.
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33 (2), 161–199.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In: Proceedings of ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining,
pp. 613–619. Edmonton, Canada.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL, pp. 311–
318. July. Philadelphia, PA, USA.
R. Rapp. 1999. Automatic Identification of Word
Translations from Unrelated English and German
Corpora. In: Proceedings of ACL, pp. 519–526.
June. Maryland.
G. Salton and M. J. McGill. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill, New
York.
P. Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In: Proceedings of
the Twelfth European Conference on Machine
Learning, pp. 491–502, Berlin, Germany.
D. Wu and P. Fung. 2009. Semantic Roles for SMT:
A Hybrid Two-Pass Model. In: Proceedings of
NAACL/HLT, Boulder, CO.
D. Yuret and M. A. Yatbaz. 2009. The Noisy Channel
Model for Unsupervised Word Sense Disambigua-
tion. In: Computational Linguistics. Vol. 1(1) 1-18.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In: Proceed-
ings of NAACL-HLT. Boston, MA.
B. Zhao, S. Vogel, M. Eck, and A. Waibel. 2004.
Phrase pair rescoring with term weighting for sta-
tistical machine translation. In Proceedings of
EMNLP, pp. 206–213. July. Barcelona, Spain.
</reference>
<page confidence="0.999181">
843
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.710218">
<title confidence="0.999517">Bilingual Sense Similarity for Statistical Machine Translation</title>
<author confidence="0.999192">George Foster Kuhn Chen</author>
<affiliation confidence="0.99948">National Research Council Canada</affiliation>
<address confidence="0.978368">283 Alexandre-Taché Boulevard, Gatineau (Québec), Canada J8X 3X7</address>
<email confidence="0.741966">Boxing.Chen@nrc.ca</email>
<email confidence="0.741966">George.Foster@nrc.ca</email>
<email confidence="0.741966">Roland.Kuhn@nrc.ca</email>
<abstract confidence="0.998491">This paper proposes new algorithms to compute the sense similarity between two units phrases, rules, from parallel corpora. The sense similarity scores are computed by using the vector space model. We then apply the algorithms to statistical machine translation by computing the sense similarity between the source and target side of translation rule pairs. Similarity scores are used as additional features of the translation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>rounding words in a given size window. In our future work, we may try this algorithm on syntax-based MT systems and phrase-based MT systems with different context features. It would also be possible to use this technique during training of an SMT system – for instance, to improve the bilingual word alignment or reduce the training data noise.</title>
<marker></marker>
<rawString>rounding words in a given size window. In our future work, we may try this algorithm on syntax-based MT systems and phrase-based MT systems with different context features. It would also be possible to use this technique during training of an SMT system – for instance, to improve the bilingual word alignment or reduce the training data noise.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>S Kanthak</author>
<author>P Haffner</author>
</authors>
<date>2009</date>
<booktitle>Statistical Machine Translation through Global Lexical Selection and Sentence Reconstruction. In: Goutte</booktitle>
<editor>et al (ed.),</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="30370" citStr="Bangalore et al, 2009" startWordPosition="5442" endWordPosition="5445">seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurren</context>
</contexts>
<marker>Bangalore, Kanthak, Haffner, 2009</marker>
<rawString>S. Bangalore, S. Kanthak, and P. Haffner. 2009. Statistical Machine Translation through Global Lexical Selection and Sentence Reconstruction. In: Goutte et al (ed.), Learning Machine Translation. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--312</pages>
<contexts>
<context position="10351" citStr="Brown et al., 1993" startWordPosition="1741" endWordPosition="1744"> c. A problem with P(r |c) is that it is biased towards infrequent words/features. We therefore smooth w(r, c) with add-k smoothing: F(r , c)+k F(r,c)+k ∑ (F(r,c)+k) F(c)+ i =1 where k is a tunable global smoothing constant, and R is the number of rules. 4 Similarity Functions There are many possibilities for calculating similarities between bags-of-words in different languages. We consider IBM model 1 probabilities and cosine distance similarity functions. 4.1 IBM Model 1 Probabilities For the IBM model 1 similarity function, we take the geometric mean of symmetrized conditional IBM model 1 (Brown et al., 1993) bag probabilities, as in Equation (6). sim a y = sqrt P Bf Be × P Be Bf ( , ) ( ( |) ( |)) (6) To compute P(B f |Be), IBM model 1 assumes that all source words are conditionally independent, so that: I P Bf Be ( |) = Õ p f i Be ( |) (7) i=1 To compute, we use a “Noisy-OR” combination which has shown better performance than standard IBM model 1 probability, as described in (Zens and Ney, 2004): p(fi |Be)=1−p(fi |Be) (8) J p f i Be ( |) Õ(1−p(fi |ej)) (9) j=1 where p(fi |Be) is the probability that fi is not in the translation of Be, and is the IBM model 1 probability. 4.2 Vector Space Mapping </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, V. J. Della Pietra, S. A. Della Pietra &amp; R. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2) 263-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bullinaria</author>
<author>J Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<volume>39</volume>
<issue>3</issue>
<pages>510--526</pages>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>J. Bullinaria and J. Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39 (3), 510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Improving Statistical Machine Translation using Word Sense Disambiguation. In:</title>
<date>2007</date>
<booktitle>Proceedings of EMNLP,</booktitle>
<location>Prague.</location>
<contexts>
<context position="29919" citStr="Carpuat and Wu (2007)" startWordPosition="5376" endWordPosition="5379">nings. Although this bias is helpful on its own, possibly due to the mechanism we outline in section 6.1, it appears to have a synergistic effect when used along with the bilingual similarity feature. 3) Finally, we note that many of the features we use for capturing similarity, such as the context “the, of” for instantiations of X in the unit the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>M. Carpuat and D. Wu. 2007. Improving Statistical Machine Translation using Word Sense Disambiguation. In: Proceedings of EMNLP, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
</authors>
<title>One Translation per Discourse. In:</title>
<date>2009</date>
<booktitle>Proceedings of NAACL HLT Workshop on Semantic Evaluations,</booktitle>
<location>Boulder, CO.</location>
<marker>Carpuat, 2009</marker>
<rawString>M. Carpuat. 2009. One Translation per Discourse. In: Proceedings of NAACL HLT Workshop on Semantic Evaluations, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chan</author>
<author>H Ng</author>
<author>D Chiang</author>
</authors>
<title>Word Sense Disambiguation Improves Statistical Machine Translation. In:</title>
<date>2007</date>
<booktitle>Proceedings of ACL,</booktitle>
<location>Prague.</location>
<contexts>
<context position="29941" citStr="Chan et al (2007)" startWordPosition="5381" endWordPosition="5384">is helpful on its own, possibly due to the mechanism we outline in section 6.1, it appears to have a synergistic effect when used along with the bilingual similarity feature. 3) Finally, we note that many of the features we use for capturing similarity, such as the context “the, of” for instantiations of X in the unit the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target con</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Y. Chan, H. Ng and D. Chiang. 2007. Word Sense Disambiguation Improves Statistical Machine Translation. In: Proceedings of ACL, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation. In:</title>
<date>2005</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="5141" citStr="Chiang, 2005" startWordPosition="795" endWordPosition="796">s between the source and target sides of a translation rule to improve statistical machine translation performance. This work attempts to measure directly the sense similarity for units from different languages by comparing their contexts1. Our contribution includes proposing new bilingual sense similarity algorithms and applying them to machine translation. We chose a hierarchical phrase-based SMT system as our baseline; thus, the units involved in computation of sense similarities are hierarchical rules. 2 Hierarchical phrase-based MT system The hierarchical phrase-based translation method (Chiang, 2005; Chiang, 2007) is a formal syntaxbased translation modeling method; its translation model is a weighted synchronous context free grammar (SCFG). No explicit linguistic syntactic information appears in the model. An SCFG rule has the following form: where X is a non-terminal symbol shared by all the rules; each rule has at most two nonterminals. a (y ) is a source (target) string consisting of terminal and non-terminal symbols. ~ defines a one-to-one correspondence between non-terminals in a and y . 1 There has been a lot of work (more details in Section 7) on applying word sense disambiguatio</context>
<context position="6444" citStr="Chiang, 2005" startWordPosition="1021" endWordPosition="1022">indirectly, using source-side context to help select a particular translation for a source rule. source target Ini. phr. 他 出席 了 会议 he attended the meeting Rule 1 他 出席 了 X1 he attended X1 Context 1 会议 the, meeting Rule 2 会议 the meeting Context 2 他, 出席, 了 he, attended Rule 3 他 X1会议出席, 了 he X1 the meeting Context 3 attended Rule 4 出席 了他,会议 attended Context 4 he, the, meeting Figure 1: example of hierarchical rule pairs and their context features. Rule frequencies are counted during rule extraction over word-aligned sentence pairs, and they are normalized to estimate features on rules. Following (Chiang, 2005; Chiang, 2007), 4 features are computed for each rule: • P(y |a) and P(a |y) are direct and inverse rule-based conditional probabilities; • Pw (y |a) and Pw (a |y) are direct and inverse lexical weights (Koehn et al., 2003). Empirically, this method has yielded better performance on language pairs such as ChineseEnglish than the phrase-based method because it permits phrases with gaps; it generalizes the normal phrase-based models in a way that allows long-distance reordering (Chiang, 2005; Chiang, 2007). We use the Joshua implementation of the method for decoding (Li et al., 2009). 3 Bag-of-</context>
<context position="18221" citStr="Chiang, 2005" startWordPosition="3270" endWordPosition="3271">.5M Dev |S |1,506 1,506x4 Test NIST06 |S |1,664 1,664x4 NIST08 |S |1,357 1,357x4 Gigaword |S |- 11.7M Table 1: Statistics of training, dev, and test sets for Chinese-to-English task. For German-to-English tasks, we used WMT 20064 data sets. The parallel training data contains 21 million target words; both the dev set and test set contain 2000 sentences; one reference is provided for each source input sentence. Only the target-language half of the parallel training data are used to train the language model in this task. 5.2 Results For the baseline, we train the translation model by following (Chiang, 2005; Chiang, 2007) and our decoder is Joshua5, an open-source hierarchical phrase-based machine translation system written in Java. Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. By observing the results on dev set in the additional experiments, we first set the smoothing constant k in Equation (5) to 0.5. Then, we need to set the sizes of the vectors to balance the computing time and translation accu4 http://www.statmt.org/wmt06/ 5 </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In: Proceedings of ACL, pp. 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics.</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="5156" citStr="Chiang, 2007" startWordPosition="797" endWordPosition="798">source and target sides of a translation rule to improve statistical machine translation performance. This work attempts to measure directly the sense similarity for units from different languages by comparing their contexts1. Our contribution includes proposing new bilingual sense similarity algorithms and applying them to machine translation. We chose a hierarchical phrase-based SMT system as our baseline; thus, the units involved in computation of sense similarities are hierarchical rules. 2 Hierarchical phrase-based MT system The hierarchical phrase-based translation method (Chiang, 2005; Chiang, 2007) is a formal syntaxbased translation modeling method; its translation model is a weighted synchronous context free grammar (SCFG). No explicit linguistic syntactic information appears in the model. An SCFG rule has the following form: where X is a non-terminal symbol shared by all the rules; each rule has at most two nonterminals. a (y ) is a source (target) string consisting of terminal and non-terminal symbols. ~ defines a one-to-one correspondence between non-terminals in a and y . 1 There has been a lot of work (more details in Section 7) on applying word sense disambiguation (WSD) techniq</context>
<context position="6459" citStr="Chiang, 2007" startWordPosition="1023" endWordPosition="1024">ing source-side context to help select a particular translation for a source rule. source target Ini. phr. 他 出席 了 会议 he attended the meeting Rule 1 他 出席 了 X1 he attended X1 Context 1 会议 the, meeting Rule 2 会议 the meeting Context 2 他, 出席, 了 he, attended Rule 3 他 X1会议出席, 了 he X1 the meeting Context 3 attended Rule 4 出席 了他,会议 attended Context 4 he, the, meeting Figure 1: example of hierarchical rule pairs and their context features. Rule frequencies are counted during rule extraction over word-aligned sentence pairs, and they are normalized to estimate features on rules. Following (Chiang, 2005; Chiang, 2007), 4 features are computed for each rule: • P(y |a) and P(a |y) are direct and inverse rule-based conditional probabilities; • Pw (y |a) and Pw (a |y) are direct and inverse lexical weights (Koehn et al., 2003). Empirically, this method has yielded better performance on language pairs such as ChineseEnglish than the phrase-based method because it permits phrases with gaps; it generalizes the normal phrase-based models in a way that allows long-distance reordering (Chiang, 2005; Chiang, 2007). We use the Joshua implementation of the method for decoding (Li et al., 2009). 3 Bag-of-Words Vector Sp</context>
<context position="18236" citStr="Chiang, 2007" startWordPosition="3272" endWordPosition="3273">506 1,506x4 Test NIST06 |S |1,664 1,664x4 NIST08 |S |1,357 1,357x4 Gigaword |S |- 11.7M Table 1: Statistics of training, dev, and test sets for Chinese-to-English task. For German-to-English tasks, we used WMT 20064 data sets. The parallel training data contains 21 million target words; both the dev set and test set contain 2000 sentences; one reference is provided for each source input sentence. Only the target-language half of the parallel training data are used to train the language model in this task. 5.2 Results For the baseline, we train the translation model by following (Chiang, 2005; Chiang, 2007) and our decoder is Joshua5, an open-source hierarchical phrase-based machine translation system written in Java. Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. By observing the results on dev set in the additional experiments, we first set the smoothing constant k in Equation (5) to 0.5. Then, we need to set the sizes of the vectors to balance the computing time and translation accu4 http://www.statmt.org/wmt06/ 5 http://www.cs.j</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics. 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>W Wang</author>
<author>K Knight</author>
</authors>
<title>11,001 new features for statistical machine translation. In:</title>
<date>2009</date>
<booktitle>Proc. NAACL HLT,</booktitle>
<pages>218--226</pages>
<contexts>
<context position="30262" citStr="Chiang et al, 2009" startWordPosition="5426" endWordPosition="5429"> the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similar</context>
</contexts>
<marker>Chiang, Wang, Knight, 2009</marker>
<rawString>D. Chiang, W. Wang and K. Knight. 2009. 11,001 new features for statistical machine translation. In: Proc. NAACL HLT, pp. 218–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<marker>Church, Hanks, 1990</marker>
<rawString>K. W. Church and P. Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Frakes</author>
<author>R Baeza-Yates</author>
<author>editors</author>
</authors>
<date>1992</date>
<booktitle>Information Retrieval, Data Structure and Algorithms.</booktitle>
<publisher>Prentice Hall.</publisher>
<marker>Frakes, Baeza-Yates, editors, 1992</marker>
<rawString>W. B. Frakes and R. Baeza-Yates, editors. 1992. Information Retrieval, Data Structure and Algorithms. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
</authors>
<title>A statistical view on bilingual lexicon extraction: From parallel corpora to non-parallel corpora. In:</title>
<date>1998</date>
<booktitle>Proceedings of AMTA,</booktitle>
<pages>1--17</pages>
<location>Langhorne, PA, USA.</location>
<contexts>
<context position="2633" citStr="Fung (1998)" startWordPosition="407" endWordPosition="408">clidean distance, City Block distance (Bullinaria and Levy; 2007), and Dice and Jaccard coefficients (Frakes and Baeza-Yates, 1992), etc. Measures of monolingual sense similarity have been widely used in many applications, such as synonym recognizing (Landauer and Dumais, 1997), word clustering (Pantel and Lin 2002), word sense disambiguation (Yuret and Yatbaz 2009), etc. Use of the vector space model to compute sense similarity has also been adapted to the multilingual condition, based on the assumption that two terms with similar meanings often occur in comparable contexts across languages. Fung (1998) and Rapp (1999) adopted VSM for the application of extracting translation pairs from comparable or even unrelated corpora. The vectors in different languages are first mapped to a common space using an initial bilingual dictionary, and then compared. However, there is no previous work that uses the VSM to compute sense similarity for terms from parallel corpora. The sense similarities, i.e. the translation probabilities in a translation model, for units from parallel corpora are mainly based on the co-occurrence counts of the two units. Therefore, questions emerge: how good is the sense simil</context>
<context position="11538" citStr="Fung (1998)" startWordPosition="1995" endWordPosition="1996">.2 Vector Space Mapping A common way to calculate semantic similarity is by vector space cosine distance; we will also { , f f 1 2 {,,..., } e e e 1 2 J Bf Be (1) } ,..., fI ve } } (2) log w r c R ( , ) = kR (5) 836 use this similarity function in our algorithm. However, the two vectors in Equation (2) cannot be directly compared because the axes of their spaces represent different words in different languages, and also their dimensions I and J are not assured to be the same. Therefore, we need to first map a vector into the space of the other vector, so that the similarity can be calculated. Fung (1998) and Rapp (1999) map the vector onedimension-to-one-dimension (a context word is a dimension in each vector space) from one language to another language via an initial bilingual dictionary. We follow (Zhao et al., 2004) to do vector space mapping. Our goal is – given a source pattern – to distinguish between the senses of its associated target patterns. Therefore, we map all vectors in target language into the vector space in the source language. What we want is a representation v�a in the source language space of the target vector v�e . To get v�a, we can let fi wa , the weight of the ith sou</context>
</contexts>
<marker>Fung, 1998</marker>
<rawString>P. Fung. 1998. A statistical view on bilingual lexicon extraction: From parallel corpora to non-parallel corpora. In: Proceedings of AMTA, pp. 1–17. Oct. Langhorne, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gimenez</author>
<author>L Marquez</author>
</authors>
<title>Discriminative Phrase Selection for SMT.</title>
<date>2009</date>
<booktitle>Learning Machine Translation.</booktitle>
<editor>In: Goutte et al (ed.),</editor>
<publisher>MIT Press.</publisher>
<marker>Gimenez, Marquez, 2009</marker>
<rawString>J. Gimenez and L. Marquez. 2009. Discriminative Phrase Selection for SMT. In: Goutte et al (ed.), Learning Machine Translation. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Rich Source-Side Context for Statistical Machine Translation. In:</title>
<date>2008</date>
<booktitle>Proceedings of WMT,</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="30241" citStr="Gimpel and Smith, 2008" startWordPosition="5422" endWordPosition="5425">iations of X in the unit the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilin</context>
</contexts>
<marker>Gimpel, Smith, 2008</marker>
<rawString>K. Gimpel and N. A. Smith. 2008. Rich Source-Side Context for Statistical Machine Translation. In: Proceedings of WMT, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1954</date>
<journal>Word,</journal>
<volume>10</volume>
<issue>23</issue>
<pages>146--162</pages>
<contexts>
<context position="1098" citStr="Harris, 1954" startWordPosition="160" endWordPosition="161">ly the algorithms to statistical machine translation by computing the sense similarity between the source and target side of translation rule pairs. Similarity scores are used as additional features of the translation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functi</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Z. Harris. 1954. Distributional structure. Word, 10(23): 146-162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z He</author>
<author>Q Liu</author>
<author>S Lin</author>
</authors>
<title>Improving Statistical Machine Translation using Lexicalized Rule Selection. In:</title>
<date>2008</date>
<booktitle>Proceedings of COLING,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="30456" citStr="He et al, 2008" startWordPosition="5457" endWordPosition="5460"> work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or target units as part of the given rule, and in general. In another wo</context>
</contexts>
<marker>He, Liu, Lin, 2008</marker>
<rawString>Z. He, Q. Liu, and S. Lin. 2008. Improving Statistical Machine Translation using Lexicalized Rule Selection. In: Proceedings of COLING, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicateargument structures. In:</title>
<date>1990</date>
<booktitle>Proceedings of ACL.</booktitle>
<pages>268--275</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1304" citStr="Hindle, 1990" startWordPosition="196" endWordPosition="197">ranslation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity functi</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>D. Hindle. 1990. Noun classification from predicateargument structures. In: Proceedings of ACL. pp. 268-275. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical PhraseBased Translation. In:</title>
<date>2003</date>
<booktitle>Proceedings of HLTNAACL.</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada</location>
<contexts>
<context position="6668" citStr="Koehn et al., 2003" startWordPosition="1062" endWordPosition="1065"> 2 会议 the meeting Context 2 他, 出席, 了 he, attended Rule 3 他 X1会议出席, 了 he X1 the meeting Context 3 attended Rule 4 出席 了他,会议 attended Context 4 he, the, meeting Figure 1: example of hierarchical rule pairs and their context features. Rule frequencies are counted during rule extraction over word-aligned sentence pairs, and they are normalized to estimate features on rules. Following (Chiang, 2005; Chiang, 2007), 4 features are computed for each rule: • P(y |a) and P(a |y) are direct and inverse rule-based conditional probabilities; • Pw (y |a) and Pw (a |y) are direct and inverse lexical weights (Koehn et al., 2003). Empirically, this method has yielded better performance on language pairs such as ChineseEnglish than the phrase-based method because it permits phrases with gaps; it generalizes the normal phrase-based models in a way that allows long-distance reordering (Chiang, 2005; Chiang, 2007). We use the Joshua implementation of the method for decoding (Li et al., 2009). 3 Bag-of-Words Vector Space Model To compute the sense similarity via VSM, we follow the previous work (Lin, 1998) and represent the source and target side of a rule by feature vectors. In our work, each feature corresponds to a cont</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. Och, D. Marcu. 2003. Statistical PhraseBased Translation. In: Proceedings of HLTNAACL. pp. 127-133, Edmonton, Canada</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation. In:</title>
<date>2004</date>
<booktitle>Proceedings of EMNLP,</booktitle>
<pages>388--395</pages>
<location>Barcelona,</location>
<contexts>
<context position="18497" citStr="Koehn, 2004" startWordPosition="3312" endWordPosition="3313"> 21 million target words; both the dev set and test set contain 2000 sentences; one reference is provided for each source input sentence. Only the target-language half of the parallel training data are used to train the language model in this task. 5.2 Results For the baseline, we train the translation model by following (Chiang, 2005; Chiang, 2007) and our decoder is Joshua5, an open-source hierarchical phrase-based machine translation system written in Java. Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. By observing the results on dev set in the additional experiments, we first set the smoothing constant k in Equation (5) to 0.5. Then, we need to set the sizes of the vectors to balance the computing time and translation accu4 http://www.statmt.org/wmt06/ 5 http://www.cs.jhu.edu/~ccb/joshua/index.html 838 racy, i.e., we keep only the top N context words with the highest feature value for each side of a rule 6 . In the following, we use “Alg1” to represent the original similarity functions which compare the two context vectors bu</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In: Proceedings of EMNLP, pp. 388–395. July, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review.</title>
<date>1997</date>
<pages>104--211</pages>
<contexts>
<context position="1355" citStr="Landauer and Dumais, 1997" startWordPosition="202" endWordPosition="205">n performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most widely used is cosine distan</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. Landauer and S. T. Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review. 104:211-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>C Callison-Burch</author>
<author>C Dyer</author>
<author>J Ganitkevitch</author>
<author>S Khudanpur</author>
<author>L Schwartz</author>
<author>W Thornton</author>
<author>J Weese</author>
<author>O Zaidan</author>
</authors>
<title>Joshua: An Open Source Toolkit for Parsing-based Machine Translation. In:</title>
<date>2009</date>
<booktitle>Proceedings of the WMT. March.</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="7033" citStr="Li et al., 2009" startWordPosition="1117" endWordPosition="1120">es. Following (Chiang, 2005; Chiang, 2007), 4 features are computed for each rule: • P(y |a) and P(a |y) are direct and inverse rule-based conditional probabilities; • Pw (y |a) and Pw (a |y) are direct and inverse lexical weights (Koehn et al., 2003). Empirically, this method has yielded better performance on language pairs such as ChineseEnglish than the phrase-based method because it permits phrases with gaps; it generalizes the normal phrase-based models in a way that allows long-distance reordering (Chiang, 2005; Chiang, 2007). We use the Joshua implementation of the method for decoding (Li et al., 2009). 3 Bag-of-Words Vector Space Model To compute the sense similarity via VSM, we follow the previous work (Lin, 1998) and represent the source and target side of a rule by feature vectors. In our work, each feature corresponds to a context word which co-occurs with the translation rule. 3.1 Context Features In the hierarchical phrase-based translation method, the translation rules are extracted by abstracting some words from an initial phrase pair (Chiang, 2005). Consider a rule with nonterminals on the source and target side; for a given instance of the rule (a particular phrase pair in the tr</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. Khudanpur, L. Schwartz, W. Thornton, J. Weese and O. Zaidan, 2009. Joshua: An Open Source Toolkit for Parsing-based Machine Translation. In: Proceedings of the WMT. March. Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words. In:</title>
<date>1998</date>
<booktitle>Proceedings of COLING/ACL98.</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="1366" citStr="Lin, 1998" startWordPosition="206" endWordPosition="207">improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most widely used is cosine distance (Salton </context>
<context position="7149" citStr="Lin, 1998" startWordPosition="1138" endWordPosition="1139">verse rule-based conditional probabilities; • Pw (y |a) and Pw (a |y) are direct and inverse lexical weights (Koehn et al., 2003). Empirically, this method has yielded better performance on language pairs such as ChineseEnglish than the phrase-based method because it permits phrases with gaps; it generalizes the normal phrase-based models in a way that allows long-distance reordering (Chiang, 2005; Chiang, 2007). We use the Joshua implementation of the method for decoding (Li et al., 2009). 3 Bag-of-Words Vector Space Model To compute the sense similarity via VSM, we follow the previous work (Lin, 1998) and represent the source and target side of a rule by feature vectors. In our work, each feature corresponds to a context word which co-occurs with the translation rule. 3.1 Context Features In the hierarchical phrase-based translation method, the translation rules are extracted by abstracting some words from an initial phrase pair (Chiang, 2005). Consider a rule with nonterminals on the source and target side; for a given instance of the rule (a particular phrase pair in the training corpus), the context will be the words instantiating the non-terminals. In turn, the context for the sub-phra</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In: Proceedings of COLING/ACL98. pp. 768-774. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Liu</author>
<author>Z He</author>
<author>Y Liu</author>
<author>S Lin</author>
</authors>
<title>Maximum Entropy based Rule Selection Model for Syntaxbased Statistical Machine Translation. In:</title>
<date>2008</date>
<booktitle>Proceedings of EMNLP,</booktitle>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="30488" citStr="Liu et al, 2008" startWordPosition="5463" endWordPosition="5466">s into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or target units as part of the given rule, and in general. In another words, WSD explicitly tries to cho</context>
</contexts>
<marker>Liu, He, Liu, Lin, 2008</marker>
<rawString>Q. Liu, Z. He, Y. Liu and S. Lin. 2008. Maximum Entropy based Rule Selection Model for Syntaxbased Statistical Machine Translation. In: Proceedings of EMNLP, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
</authors>
<title>Producing highdimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, and Computers,</journal>
<volume>28</volume>
<issue>2</issue>
<pages>203--208</pages>
<contexts>
<context position="1328" citStr="Lund and Burgess, 1996" startWordPosition="198" endWordPosition="201">el to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most w</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>K. Lund, and C. Burgess. 1996. Producing highdimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, and Computers, 28 (2), 203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mauser</author>
<author>S Hasan</author>
<author>H Ney</author>
</authors>
<title>Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models. In:</title>
<date>2009</date>
<booktitle>Proceedings of EMNLP,</booktitle>
<contexts>
<context position="30391" citStr="Mauser et al, 2009" startWordPosition="5446" endWordPosition="5450">tic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or targ</context>
</contexts>
<marker>Mauser, Hasan, Ney, 2009</marker>
<rawString>A. Mauser, S. Hasan and H. Ney. 2009. Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models. In: Proceedings of EMNLP, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation. In:</title>
<date>2003</date>
<booktitle>Proceedings of ACL.</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="14429" citStr="Och, 2003" startWordPosition="2587" endWordPosition="2588">ely. cooc Cf and cooc Ce are the contexts for α and γ when α and γ co-occur. Obviously, full they satisfy the constraints: C fcooc ⊆ C and C e ⊆ C . Therefore, the original similarity cooc full e functions are to compare the two context vectors built on full training data directly, as shown in Equation (13). sim(α, γ) =sim(CfullCfull) e(13) Then, we propose a new similarity function as follows: (, ) α γ sim sim C C λ coocfull λ3 (14) ( , ) 1 2 full cooc ( , ) f f cooc λ ( , ) cooc sim C C ⋅ ⋅sim C C f e e e where the parameters λi (i=1,2,3) can be tuned via minimal error rate training (MERT) (Och, 2003). Figure 2: contexts for rule α and γ . A unit’s sense is defined by all its contexts in the whole training data; it may have a lot of different senses in the whole training data. However, when it is linked with another unit in the other language, its sense pool is constrained and is just α γ Cfull f Cfull e Ccooc f Ccooc e = 837 a subset of the whole sense set. ( , cooc) sim Cf C full f is the metric which evaluates the similarity between the whole sense pool of a and the sense pool when a co-occurs with y ; sim Ce C is the analogous similarity me( , cooc) full e tric for y . They range from </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. In: Proceedings of ACL. Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pado</author>
<author>M Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<pages>161--199</pages>
<contexts>
<context position="1426" citStr="Pado and Lapata, 2007" startWordPosition="214" endWordPosition="217">rt hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most widely used is cosine distance (Salton and McGill, 1983); other similarity functions include Euclid</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>S. Pado and M. Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33 (2), 161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering word senses from text. In:</title>
<date>2002</date>
<booktitle>Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>613--619</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1402" citStr="Pantel and Lin, 2002" startWordPosition="210" endWordPosition="213"> over a state-of-the-art hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most widely used is cosine distance (Salton and McGill, 1983); other similarity </context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>P. Pantel and D. Lin. 2002. Discovering word senses from text. In: Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 613–619. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="18407" citStr="Papineni et al., 2002" startWordPosition="3296" endWordPosition="3299"> task. For German-to-English tasks, we used WMT 20064 data sets. The parallel training data contains 21 million target words; both the dev set and test set contain 2000 sentences; one reference is provided for each source input sentence. Only the target-language half of the parallel training data are used to train the language model in this task. 5.2 Results For the baseline, we train the translation model by following (Chiang, 2005; Chiang, 2007) and our decoder is Joshua5, an open-source hierarchical phrase-based machine translation system written in Java. Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. By observing the results on dev set in the additional experiments, we first set the smoothing constant k in Equation (5) to 0.5. Then, we need to set the sizes of the vectors to balance the computing time and translation accu4 http://www.statmt.org/wmt06/ 5 http://www.cs.jhu.edu/~ccb/joshua/index.html 838 racy, i.e., we keep only the top N context words with the highest feature value for each side of a rule 6 . In the following, we use “Alg</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pp. 311– 318. July. Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Automatic Identification of Word Translations from Unrelated English and German Corpora. In:</title>
<date>1999</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>519--526</pages>
<location>Maryland.</location>
<contexts>
<context position="2649" citStr="Rapp (1999)" startWordPosition="410" endWordPosition="411">, City Block distance (Bullinaria and Levy; 2007), and Dice and Jaccard coefficients (Frakes and Baeza-Yates, 1992), etc. Measures of monolingual sense similarity have been widely used in many applications, such as synonym recognizing (Landauer and Dumais, 1997), word clustering (Pantel and Lin 2002), word sense disambiguation (Yuret and Yatbaz 2009), etc. Use of the vector space model to compute sense similarity has also been adapted to the multilingual condition, based on the assumption that two terms with similar meanings often occur in comparable contexts across languages. Fung (1998) and Rapp (1999) adopted VSM for the application of extracting translation pairs from comparable or even unrelated corpora. The vectors in different languages are first mapped to a common space using an initial bilingual dictionary, and then compared. However, there is no previous work that uses the VSM to compute sense similarity for terms from parallel corpora. The sense similarities, i.e. the translation probabilities in a translation model, for units from parallel corpora are mainly based on the co-occurrence counts of the two units. Therefore, questions emerge: how good is the sense similarity computed v</context>
<context position="11554" citStr="Rapp (1999)" startWordPosition="1998" endWordPosition="1999">Mapping A common way to calculate semantic similarity is by vector space cosine distance; we will also { , f f 1 2 {,,..., } e e e 1 2 J Bf Be (1) } ,..., fI ve } } (2) log w r c R ( , ) = kR (5) 836 use this similarity function in our algorithm. However, the two vectors in Equation (2) cannot be directly compared because the axes of their spaces represent different words in different languages, and also their dimensions I and J are not assured to be the same. Therefore, we need to first map a vector into the space of the other vector, so that the similarity can be calculated. Fung (1998) and Rapp (1999) map the vector onedimension-to-one-dimension (a context word is a dimension in each vector space) from one language to another language via an initial bilingual dictionary. We follow (Zhao et al., 2004) to do vector space mapping. Our goal is – given a source pattern – to distinguish between the senses of its associated target patterns. Therefore, we map all vectors in target language into the vector space in the source language. What we want is a representation v�a in the source language space of the target vector v�e . To get v�a, we can let fi wa , the weight of the ith source feature, be </context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>R. Rapp. 1999. Automatic Identification of Word Translations from Unrelated English and German Corpora. In: Proceedings of ACL, pp. 519–526. June. Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="1983" citStr="Salton and McGill, 1983" startWordPosition="308" endWordPosition="311">n, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most widely used is cosine distance (Salton and McGill, 1983); other similarity functions include Euclidean distance, City Block distance (Bullinaria and Levy; 2007), and Dice and Jaccard coefficients (Frakes and Baeza-Yates, 1992), etc. Measures of monolingual sense similarity have been widely used in many applications, such as synonym recognizing (Landauer and Dumais, 1997), word clustering (Pantel and Lin 2002), word sense disambiguation (Yuret and Yatbaz 2009), etc. Use of the vector space model to compute sense similarity has also been adapted to the multilingual condition, based on the assumption that two terms with similar meanings often occur in</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M. J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Mining the Web for synonyms: PMI-IR versus LSA on TOEFL. In:</title>
<date>2001</date>
<booktitle>Proceedings of the Twelfth European Conference on Machine Learning,</booktitle>
<pages>491--502</pages>
<location>Berlin, Germany.</location>
<contexts>
<context position="1380" citStr="Turney, 2001" startWordPosition="208" endWordPosition="209">s are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most widely used is cosine distance (Salton and McGill, 19</context>
<context position="9553" citStr="Turney, 2001" startWordPosition="1594" endWordPosition="1595">mally, these values are based on the counts of the words in the corresponding bags. 3.3 Feature Weighting Schemes We use pointwise mutual information (Church et al., 1990) to compute the feature values. Let c ( c E Bf or c E Be ) be a context word and F(r, c) be the frequency count of a rule r (a or y ) co-occurring with the context word c. The pointwise mutual information MI(r,c) is defined as: F r c ( , ) w r c MI r c ( , ) ( , ) = = log F(r) ´ log F(c) N N N (3) where N is the total frequency counts of all rules and their context words. Since we are using this value as a weight, following (Turney, 2001), we drop log, N and F(r) . Thus (3) simplifies to: F r c ( , ) w r c = (4) ( , ) F c ( ) It can be seen as an estimate of P(r |c) , the empirical probability of observing r given c. A problem with P(r |c) is that it is biased towards infrequent words/features. We therefore smooth w(r, c) with add-k smoothing: F(r , c)+k F(r,c)+k ∑ (F(r,c)+k) F(c)+ i =1 where k is a tunable global smoothing constant, and R is the number of rules. 4 Similarity Functions There are many possibilities for calculating similarities between bags-of-words in different languages. We consider IBM model 1 probabilities a</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>P. Turney. 2001. Mining the Web for synonyms: PMI-IR versus LSA on TOEFL. In: Proceedings of the Twelfth European Conference on Machine Learning, pp. 491–502, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
<author>P Fung</author>
</authors>
<title>Semantic Roles for SMT: A Hybrid Two-Pass Model. In:</title>
<date>2009</date>
<booktitle>Proceedings of NAACL/HLT,</booktitle>
<location>Boulder, CO.</location>
<contexts>
<context position="30573" citStr="Wu and Fung (2009)" startWordPosition="5478" endWordPosition="5481">ord-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or target units as part of the given rule, and in general. In another words, WSD explicitly tries to choose a translation given the current source context, while our work rates rule pairs i</context>
</contexts>
<marker>Wu, Fung, 2009</marker>
<rawString>D. Wu and P. Fung. 2009. Semantic Roles for SMT: A Hybrid Two-Pass Model. In: Proceedings of NAACL/HLT, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yuret</author>
<author>M A Yatbaz</author>
</authors>
<date>2009</date>
<publisher>The Noisy Channel</publisher>
<contexts>
<context position="2390" citStr="Yuret and Yatbaz 2009" startWordPosition="366" endWordPosition="369">e (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most widely used is cosine distance (Salton and McGill, 1983); other similarity functions include Euclidean distance, City Block distance (Bullinaria and Levy; 2007), and Dice and Jaccard coefficients (Frakes and Baeza-Yates, 1992), etc. Measures of monolingual sense similarity have been widely used in many applications, such as synonym recognizing (Landauer and Dumais, 1997), word clustering (Pantel and Lin 2002), word sense disambiguation (Yuret and Yatbaz 2009), etc. Use of the vector space model to compute sense similarity has also been adapted to the multilingual condition, based on the assumption that two terms with similar meanings often occur in comparable contexts across languages. Fung (1998) and Rapp (1999) adopted VSM for the application of extracting translation pairs from comparable or even unrelated corpora. The vectors in different languages are first mapped to a common space using an initial bilingual dictionary, and then compared. However, there is no previous work that uses the VSM to compute sense similarity for terms from parallel </context>
</contexts>
<marker>Yuret, Yatbaz, 2009</marker>
<rawString>D. Yuret and M. A. Yatbaz. 2009. The Noisy Channel</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>