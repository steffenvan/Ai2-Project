<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000619">
<title confidence="0.9895435">
Automatically generating annotator rationales
to improve sentiment classification
</title>
<author confidence="0.988135">
Ainur Yessenalina Yejin Choi Claire Cardie
</author>
<affiliation confidence="0.998962">
Department of Computer Science, Cornell University, Ithaca NY, 14853 USA
</affiliation>
<email confidence="0.978753">
{ainur, ychoi, cardie}@cs.cornell.edu
</email>
<sectionHeader confidence="0.997296" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999963733333333">
One of the central challenges in sentiment-
based text categorization is that not ev-
ery portion of a document is equally in-
formative for inferring the overall senti-
ment of the document. Previous research
has shown that enriching the sentiment la-
bels with human annotators’ “rationales”
can produce substantial improvements in
categorization performance (Zaidan et al.,
2007). We explore methods to auto-
matically generate annotator rationales for
document-level sentiment classification.
Rather unexpectedly, we find the automat-
ically generated rationales just as helpful
as human rationales.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999750981132076">
One of the central challenges in sentiment-based
text categorization is that not every portion of
a given document is equally informative for in-
ferring its overall sentiment (e.g., Pang and Lee
(2004)). Zaidan et al. (2007) address this prob-
lem by asking human annotators to mark (at least
some of) the relevant text spans that support each
document-level sentiment decision. The text spans
of these “rationales” are then used to construct ad-
ditional training examples that can guide the learn-
ing algorithm toward better categorization models.
But could we perhaps enjoy the performance
gains of rationale-enhanced learning models with-
out any additional human effort whatsoever (be-
yond the document-level sentiment label)? We hy-
pothesize that in the area of sentiment analysis,
where there has been a great deal of recent re-
search attention given to various aspects of the task
(Pang and Lee, 2008), this might be possible: us-
ing existing resources for sentiment analysis, we
might be able to construct annotator rationales au-
tomatically.
In this paper, we explore a number of methods
to automatically generate rationales for document-
level sentiment classification. In particular, we in-
vestigate the use of off-the-shelf sentiment analy-
sis components and lexicons for this purpose. Our
approaches for generating annotator rationales can
be viewed as mostly unsupervised in that we do not
require manually annotated rationales for training.
Rather unexpectedly, our empirical results show
that automatically generated rationales (91.78%)
are just as good as human rationales (91.61%) for
document-level sentiment classification of movie
reviews. In addition, complementing the hu-
man annotator rationales with automatic rationales
boosts the performance even further for this do-
main, achieving 92.5% accuracy. We further eval-
uate our rationale-generation approaches on prod-
uct review data for which human rationales are not
available: here we find that even randomly gener-
ated rationales can improve the classification accu-
racy although rationales generated from sentiment
resources are not as effective as for movie reviews.
The rest of the paper is organized as follows.
We first briefly summarize the SVM-based learn-
ing approach of Zaidan et al. (2007) that allows the
incorporation of rationales (Section 2). We next
introduce three methods for the automatic gener-
ation of rationales (Section 3). The experimental
results are presented in Section 4, followed by re-
lated work (Section 5) and conclusions (Section
6).
</bodyText>
<sectionHeader confidence="0.987693" genericHeader="method">
2 Contrastive Learning with SVMs
</sectionHeader>
<bodyText confidence="0.999737375">
Zaidan et al. (2007) first introduced the notion of
annotator rationales — text spans highlighted by
human annotators as support or evidence for each
document-level sentiment decision. These ratio-
nales, of course, are only useful if the sentiment
categorization algorithm can be extended to ex-
ploit the rationales effectively. With this in mind,
Zaidan et al. (2007) propose the following con-
</bodyText>
<page confidence="0.991122">
336
</page>
<note confidence="0.5077585">
Proceedings of the ACL 2010 Conference Short Papers, pages 336–341,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99986052631579">
trastive learning extension to the standard SVM
learning algorithm.
Let ~xi be movie review i, and let {~rij} be the
set of annotator rationales that support the posi-
tive or negative sentiment decision for ~xi. For each
such rationale ~rij in the set, construct a contrastive
training example ~vij, by removing the text span
associated with the rationale ~rij from the original
review ~xi. Intuitively, the contrastive example ~vij
should not be as informative to the learning algo-
rithm as the original review ~xi, since one of the
supporting regions identified by the human anno-
tator has been deleted. That is, the correct learned
model should be less confident of its classifica-
tion of a contrastive example vs. the corresponding
original example, and the classification boundary
of the model should be modified accordingly.
Zaidan et al. (2007) formulate exactly this intu-
ition as SVM constraints as follows:
</bodyText>
<equation confidence="0.750625">
(∀i, j) : yi (~w~xi − ~w~vij) ≥ µ(1 − ξij)
</equation>
<bodyText confidence="0.999859625">
where yi ∈ {−1, +1} is the negative/positive sen-
timent label of document i, w~ is the weight vector,
µ ≥ 0 controls the size of the margin between the
original examples and the contrastive examples,
and ξij are the associated slack variables. After
some re-writing of the equations, the resulting ob-
jective function and constraints for the SVM are as
follows:
</bodyText>
<equation confidence="0.99561">
1 �
2 ||~w||2 + C
i
subject to constraints:
(∀i) : yi w~ · ~xi ≥ 1 − ξi, ξi ≥ 0
(∀i, j) : yi w~ · ~xij ≥ 1 − ξij ξij ≥ 0
</equation>
<bodyText confidence="0.981195235294118">
where ξi and ξij are the slack variables for ~xi
(the original examples) and ~xij (~xij are named as
�xi−�vi&apos;
pseudo examples and defined as ~xij = µ ), re-
spectively. Intuitively, the pseudo examples (~xij)
represent the difference between the original ex-
amples (~xi) and the contrastive examples (~vij),
weighted by a parameter µ. C and Ccontrast are
parameters to control the trade-offs between train-
ing errors and margins for the original examples ~xi
and pseudo examples ~xij respectively. As noted in
Zaidan et al. (2007), Ccontrast values are generally
smaller than C for noisy rationales.
In the work described below, we similarly em-
ploy Zaidan et al.’s (2007) contrastive learning
method to incorporate rationales for document-
level sentiment categorization.
</bodyText>
<sectionHeader confidence="0.898884" genericHeader="method">
3 Automatically Generating Rationales
</sectionHeader>
<bodyText confidence="0.999822666666667">
Our goal in the current work, is to generate anno-
tator rationales automatically. For this, we rely on
the following two assumptions:
</bodyText>
<listItem confidence="0.831717">
(1) Regions marked as annotator rationales are
</listItem>
<bodyText confidence="0.988583931818182">
more subjective than unmarked regions.
(2) The sentiment of each annotator rationale co-
incides with the document-level sentiment.
Note that assumption 1 was not observed in the
Zaidan et al. (2007) work: annotators were asked
only to mark a few rationales, leaving other (also
subjective) rationale sections unmarked.
And at first glance, assumption (2) might seem
too obvious. But it is important to include as there
can be subjective regions with seemingly conflict-
ing sentiment in the same document (Pang et al.,
2002). For instance, an author for a movie re-
view might express a positive sentiment toward
the movie, while also discussing a negative sen-
timent toward one of the fictional characters ap-
pearing in the movie. This implies that not all sub-
jective regions will be relevant for the document-
level sentiment classification — rather only those
regions whose polarity matches that of the docu-
ment should be considered.
In order to extract regions that satisfy the above
assumptions, we first look for subjective regions
in each document, then filter out those regions that
exhibit a sentiment value (i.e., polarity) that con-
flicts with polarity of the document. Assumption
2 is important as there can be subjective regions
with seemingly conflicting sentiment in the same
document (Pang et al., 2002).
Because our ultimate goal is to reduce human
annotation effort as much as possible, we do not
employ supervised learning methods to directly
learn to identify good rationales from human-
annotated rationales. Instead, we opt for methods
that make use of only the document-level senti-
ment and off-the-shelf utilities that were trained
for slightly different sentiment classification tasks
using a corpus from a different domain and of a
different genre. Although such utilities might not
be optimal for our task, we hoped that these ba-
sic resources from the research community would
constitute an adequate source of sentiment infor-
mation for our purposes.
We next describe three methods for the auto-
matic acquisition of rationales.
</bodyText>
<equation confidence="0.867654">
�ξi + Ccontrast ξij (1)
ij
</equation>
<page confidence="0.984997">
337
</page>
<subsectionHeader confidence="0.998333">
3.1 Contextual Polarity Classification
</subsectionHeader>
<bodyText confidence="0.99998788">
The first approach employs OpinionFinder (Wil-
son et al., 2005a), an off-the-shelf opinion anal-
ysis utility.1 In particular, OpinionFinder identi-
fies phrases expressing positive or negative opin-
ions. Because OpinionFinder models the task as
a word-based classification problem rather than a
sequence tagging task, most of the identified opin-
ion phrases consist of a single word. In general,
such short text spans cannot fully incorporate the
contextual information relevant to the detection of
subjective language (Wilson et al., 2005a). There-
fore, we conjecture that good rationales should ex-
tend beyond short phrases.2 For simplicity, we
choose to extend OpinionFinder phrases to sen-
tence boundaries.
In addition, to be consistent with our second op-
erating assumption, we keep only those sentences
whose polarity coincides with the document-level
polarity. In sentences where OpinionFinder marks
multiple opinion words with opposite polarities
we perform a simple voting — if words with pos-
itive (or negative) polarity dominate, then we con-
sider the entire sentence as positive (or negative).
We ignore sentences with a tie. Each selected sen-
tence is considered as a separate rationale.
</bodyText>
<subsectionHeader confidence="0.999204">
3.2 Polarity Lexicons
</subsectionHeader>
<bodyText confidence="0.999996375">
Unfortunately, domain shift as well as task mis-
match could be a problem with any opinion util-
ity based on supervised learning.3 Therefore, we
next consider an approach that does not rely on su-
pervised learning techniques but instead explores
the use of a manually constructed polarity lexicon.
In particular, we use the lexicon constructed for
Wilson et al. (2005b), which contains about 8000
words. Each entry is assigned one of three polarity
values: positive, negative, neutral. We construct
rationales from the polarity lexicon for every in-
stance of positive and negative words in the lexi-
con that appear in the training corpus.
As in the OpinionFinder rationales, we extend
the words found by the PolarityLexicon approach
to sentence boundaries to incorporate potentially
</bodyText>
<footnote confidence="0.989127">
1Available at www.cs.pitt.edu/mpqa/opinionfinderrelease/.
2This conjecture is indirectly confirmed by the fact that
human-annotated rationales are rarely a single word.
3It is worthwhile to note that OpinionFinder is trained on a
</footnote>
<bodyText confidence="0.751758222222222">
newswire corpus whose prevailing sentiment is known to be
negative (Wiebe et al., 2005). Furthermore, OpinionFinder
is trained for a task (word-level sentiment classification) that
is different from marking annotator rationales (sequence tag-
ging or text segmentation).
relevant contextual information. We retain as ra-
tionales only those sentences whose polarity co-
incides with the document-level polarity as deter-
mined via the voting scheme of Section 3.1.
</bodyText>
<subsectionHeader confidence="0.996642">
3.3 Random Selection
</subsectionHeader>
<bodyText confidence="0.999493">
Finally, we generate annotator rationales ran-
domly, selecting 25% of the sentences from each
document4 and treating each as a separate ratio-
nale.
</bodyText>
<subsectionHeader confidence="0.990598">
3.4 Comparison of Automatic vs.
Human-annotated Rationales
</subsectionHeader>
<bodyText confidence="0.999966233333333">
Before evaluating the performance of the au-
tomatically generated rationales, we summarize
in Table 1 the differences between automatic
vs. human-generated rationales. All computa-
tions were performed on the same movie review
dataset of Pang and Lee (2004) used in Zaidan et
al. (2007). Note, that the Zaidan et al. (2007) an-
notation guidelines did not insist that annotators
mark all rationales, only that some were marked
for each document. Nevertheless, we report pre-
cision, recall, and F-score based on overlap with
the human-annotated rationales of Zaidan et al.
(2007), so as to demonstrate the degree to which
the proposed approaches align with human intu-
ition. Overlap measures were also employed by
Zaidan et al. (2007).
As shown in Table 1, the annotator rationales
found by OpinionFinder (F-score 49.5%) and the
PolarityLexicon approach (F-score 52.6%) match
the human rationales much better than those found
by random selection (F-score 27.3%).
As expected, OpinionFinder’s positive ratio-
nales match the human rationales at a significantly
lower level (F-score 31.9%) than negative ratio-
nales (59.5%). This is due to the fact that Opinion-
Finder is trained on a dataset biased toward nega-
tive sentiment (see Section 3.1 - 3.2). In contrast,
all other approaches show a balanced performance
for positive and negative rationales vs. human ra-
tionales.
</bodyText>
<sectionHeader confidence="0.999819" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.996433333333333">
For our contrastive learning experiments we use
SV Mlight (Joachims, 1999). We evaluate the use-
fulness of automatically generated rationales on
</bodyText>
<footnote confidence="0.676231333333333">
4We chose the value of 25% to match the percentage of
sentences per document, on average, that contain human-
annotated rationales in our dataset (24.7%).
</footnote>
<page confidence="0.992839">
338
</page>
<table confidence="0.999875166666667">
% of sentences Precision Recall F-Score
Method selected
ALL POS NEG ALL POS NEG ALL POS NEG
OPINIONFINDER 22.8% 54.9 56.1 54.6 45.1 22.3 65.3 49.5 31.9 59.5
POLARITYLEXICON 38.7% 45.2 42.7 48.5 63.0 71.8 55.0 52.6 53.5 51.6
RANDOM 25.0% 28.9 26.0 31.8 25.9 24.9 26.7 27.3 25.5 29.0
</table>
<tableCaption confidence="0.999982">
Table 1: Comparison of Automatic vs. Human-annotated Rationales.
</tableCaption>
<bodyText confidence="0.999869375">
five different datasets. The first is the movie re-
view data of Pang and Lee (2004), which was
manually annotated with rationales by Zaidan et
al. (2007)5; the remaining are four product re-
view datasets from Blitzer et al. (2007).6 Only
the movie review dataset contains human annota-
tor rationales. We replicate the same feature set
and experimental set-up as in Zaidan et al. (2007)
to facilitate comparison with their work.7
The contrastive learning method introduced in
Zaidan et al. (2007) requires three parameters: (C,
µ, Ccontrast). To set the parameters, we use a grid
search with step 0.1 for the range of values of each
parameter around the point (1,1,1). In total, we try
around 3000 different parameter triplets for each
type of rationales.
</bodyText>
<subsectionHeader confidence="0.991854">
4.1 Experiments with the Movie Review Data
</subsectionHeader>
<bodyText confidence="0.999878307692308">
We follow Zaidan et al. (2007) for the training/test
data splits. The top half of Table 2 shows the
performance of a system trained with no anno-
tator rationales vs. two variations of human an-
notator rationales. HUMANR treats each rationale
in the same way as Zaidan et al. (2007). HU-
MANR@SENTENCE extends the human annotator
rationales to sentence boundaries, and then treats
each such sentence as a separate rationale. As
shown in Table 2, we get almost the same per-
formance from these two variations (91.33% and
91.61%).8 This result demonstrates that locking
rationales to sentence boundaries was a reasonable
</bodyText>
<footnote confidence="0.994653">
5Available at http://www.cs.jhu.edu/∼ozaidan/rationales/.
6http://www.cs.jhu.edu/∼mdredze/datasets/sentiment/.
</footnote>
<bodyText confidence="0.452351">
7We use binary unigram features corresponding to the un-
stemmed words or punctuation marks with count greater or
equal to 4 in the full 2000 documents, then we normalize the
examples to the unit length. When computing the pseudo ex-
</bodyText>
<equation confidence="0.367374666666667">
�{-vij
amples xij = we first compute (�xi − �vij) using the
µ
</equation>
<tableCaption confidence="0.813961625">
binary representation. As a result, features (unigrams) that
appeared in both vectors will be zeroed out in the resulting
vector. We then normalize the resulting vector to a unit vec-
tor.
8The performance of HUMANR reported by Zaidan et al.
(2007) is 92.2% which lies between the performance we get
(91.61%) and the oracle accuracy we get if we knew the best
parameters for the test set (92.67%).
</tableCaption>
<table confidence="0.999383375">
Method Accuracy
NORATIONALES 88.56
HUMANR 91.61•
HUMANR@SENTENCE 91.33• †
OPINIONFINDER 91.78• †
POLARITYLEXICON 91.39• †
RANDOM 90.00∗
OPINIONFINDER+HUMANR@SENTENCE 92.50• 4
</table>
<tableCaption confidence="0.9911155">
Table 2: Experimental results for the movie
review data.
</tableCaption>
<bodyText confidence="0.992594181818182">
– The numbers marked with • (or ∗) are statistically
significantly better than NORATIONALES according to a
paired t-test with P &lt; 0.001 (or P &lt; 0.01).
– The numbers marked with 4 are statistically significantly
better than HUMANR according to a paired t-test with
P &lt; 0.01.
– The numbers marked with † are not statistically signifi-
cantly worse than HUMANR according to a paired t-test with
P &gt; 0.1.
choice.
Among the approaches that make use of only
automatic rationales (bottom half of Table 2), the
best is OPINIONFINDER, reaching 91.78% accu-
racy. This result is slightly better than results
exploiting human rationales (91.33-91.61%), al-
though the difference is not statistically signifi-
cant. This result demonstrates that automatically
generated rationales are just as good as human
rationales in improving document-level sentiment
classification. Similarly strong results are ob-
tained from the POLARITYLEXICON as well.
Rather unexpectedly, RANDOM also achieves
statistically significant improvement over NORA-
TIONALES (90.0% vs. 88.56%). However, notice
that the performance of RANDOM is statistically
significantly lower than those based on human ra-
tionales (91.33-91.61%).
In our experiments so far, we observed that
some of the automatic rationales are just as
good as human rationales in improving the
document-level sentiment classification. Could
we perhaps achieve an even better result if we
combine the automatic rationales with human
</bodyText>
<page confidence="0.998032">
339
</page>
<bodyText confidence="0.998709875">
rationales? The answer is yes! The accuracy
of OPINIONFINDER+HUMANR@SENTENCE
reaches 92.50%, which is statistically signifi-
cantly better than HUMANR (91.61%). In other
words, not only can our automatically generated
rationales replace human rationales, but they can
also improve upon human rationales when they
are available.
</bodyText>
<subsectionHeader confidence="0.996694">
4.2 Experiments with the Product Reviews
</subsectionHeader>
<bodyText confidence="0.999993814814815">
We next evaluate our approaches on datasets for
which human annotator rationales do not exist.
For this, we use some of the product review data
from Blitzer et al. (2007): reviews for Books,
DVDs, Videos and Kitchen appliances. Each
dataset contains 1000 positive and 1000 negative
reviews. The reviews, however, are substantially
shorter than those in the movie review dataset:
the average number of sentences in each review
is 9.20/9.13/8.12/6.37 respectively vs. 30.86 for
the movie reviews. We perform 10-fold cross-
validation, where 8 folds are used for training, 1
fold for tuning parameters, and 1 fold for testing.
Table 3 shows the results. Rationale-based
methods perform statistically significantly bet-
ter than NORATIONALES for all but the Kitchen
dataset. An interesting trend in product re-
view datasets is that RANDOM rationales are just
as good as other more sophisticated rationales.
We suspect that this is because product reviews
are generally shorter and more focused than the
movie reviews, thereby any randomly selected
sentence is likely to be a good rationale. Quantita-
tively, subjective sentences in the product reviews
amount to 78% (McDonald et al., 2007), while
subjective sentences in the movie review dataset
are only about 25% (Mao and Lebanon, 2006).
</bodyText>
<subsectionHeader confidence="0.999743">
4.3 Examples of Annotator Rationales
</subsectionHeader>
<bodyText confidence="0.976726538461539">
In this section, we examine an example to com-
pare the automatically generated rationales (using
OPINIONFINDER) with human annotator ratio-
nales for the movie review data. In the following
positive document snippet, automatic rationales
are underlined, while human-annotated ratio-
nales are in bold face.
...But a little niceness goes a long way these days, and
there’s no denying the entertainment value of that thing
you do! It’s just about impossible to hate. It’s an
inoffensive, enjoyable piece ofnostalgia that is sure to leave
audiences smiling and humming, if not singing, “that thing
you do!” –quite possibly for days...
</bodyText>
<table confidence="0.9885838">
Method Books DVDs Videos Kitchen
NORATIONALES 80.20 80.95 82.40 87.40
OPINIONFINDER 81.65* 82.35* 84.00* 88.40
POLARITYLEXICON 82.75&apos; 82.85&apos; 84.55&apos; 87.90
RANDOM 82.05&apos; 82.10&apos; 84.15&apos; 88.00
</table>
<tableCaption confidence="0.891423">
Table 3: Experimental results for subset of
Product Review data
</tableCaption>
<bodyText confidence="0.9754195">
– The numbers marked with &apos; (or *) are statistically
significantly better than NORATIONALES according to a
paired t-test with P &lt; 0.05 (or P &lt; 0.08).
Notice that, although OPINIONFINDER misses
some human rationales, it avoids the inclusion of
“impossible to hate”, which contains only negative
terms and is likely to be confusing for the con-
trastive learner.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999964076923077">
In broad terms, constructing annotator rationales
automatically and using them to formulate con-
trastive examples can be viewed as learning with
prior knowledge (e.g., Schapire et al. (2002), Wu
and Srihari (2004)). In our task, the prior knowl-
edge corresponds to our operating assumptions
given in Section 3. Those assumptions can be
loosely connected to recognizing and exploiting
discourse structure (e.g., Pang and Lee (2004),
Taboada et al. (2009)). Our automatically gener-
ated rationales can be potentially combined with
other learning frameworks that can exploit anno-
tator rationales, such as Zaidan and Eisner (2008).
</bodyText>
<sectionHeader confidence="0.999682" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999619363636364">
In this paper, we explore methods to automatically
generate annotator rationales for document-level
sentiment classification. Our study is motivated
by the desire to retain the performance gains of
rationale-enhanced learning models while elimi-
nating the need for additional human annotation
effort. By employing existing resources for sen-
timent analysis, we can create automatic annota-
tor rationales that are as good as human annotator
rationales in improving document-level sentiment
classification.
</bodyText>
<sectionHeader confidence="0.998809" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9985378">
We thank anonymous reviewers for their comments. This
work was supported in part by National Science Founda-
tion Grants BCS-0904822, BCS-0624277, IIS-0535099 and
by the Department of Homeland Security under ONR Grant
N0014-07-1-0152.
</bodyText>
<page confidence="0.996243">
340
</page>
<sectionHeader confidence="0.996318" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999466337662338">
John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Bi-
ographies, bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classification. In Proceedings of
the 45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 440–447, Prague, Czech Repub-
lic, June. Association for Computational Linguistics.
Thorsten Joachims. 1999. Making large-scale support vector
machine learning practical. pages 169–184.
Yi Mao and Guy Lebanon. 2006. Sequential models for sen-
timent prediction. In Proceedings of the ICML Workshop:
Learning in Structured Output Spaces Open Problems in
Statistical Relational Learning Statistical Network Analy-
sis: Models, Issues and New Directions.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells,
and Jeff Reynar. 2007. Structured models for fine-to-
coarse sentiment analysis. In Proceedings of the 45th
Annual Meeting of the Association of Computational Lin-
guistics, pages 432–439, Prague, Czech Republic, June.
Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2004. A sentimental education:
sentiment analysis using subjectivity summarization based
on minimum cuts. In ACL ’04: Proceedings of the 42nd
Annual Meeting on Association for Computational Lin-
guistics, page 271, Morristown, NJ, USA. Association for
Computational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and senti-
ment analysis. Found. Trends Inf. Retr., 2(1-2):1–135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up?: sentiment classification using machine
learning techniques. In EMNLP ’02: Proceedings of the
ACL-02 conference on Empirical methods in natural lan-
guage processing, pages 79–86, Morristown, NJ, USA.
Association for Computational Linguistics.
Robert E. Schapire, Marie Rochery, Mazin G. Rahim, and
Narendra Gupta. 2002. Incorporating prior knowledge
into boosting. In ICML ’02: Proceedings of the Nine-
teenth International Conference on Machine Learning,
pages 538–545, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Maite Taboada, Julian Brooke, and Manfred Stede. 2009.
Genre-based paragraph classification for sentiment anal-
ysis. In Proceedings of the SIGDIAL 2009 Conference,
pages 62–70, London, UK, September. Association for
Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation, 1(2):0.
Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Ja-
son Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie,
Ellen Riloff, and Siddharth Patwardhan. 2005a. Opinion-
finder: a system for subjectivity analysis. In Proceedings
of HLT/EMNLP on Interactive Demonstrations, pages 34–
35, Morristown, NJ, USA. Association for Computational
Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005b.
Recognizing contextual polarity in phrase-level sentiment
analysis. In HLT-EMNLP ’05: Proceedings of the con-
ference on Human Language Technology and Empirical
Methods in Natural Language Processing, pages 347–
354, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Xiaoyun Wu and Rohini Srihari. 2004. Incorporating
prior knowledge with weighted margin support vector ma-
chines. In KDD ’04: Proceedings of the tenth ACM
SIGKDD international conference on Knowledge discov-
ery and data mining, pages 326–333, New York, NY,
USA. ACM.
Omar F. Zaidan and Jason Eisner. 2008. Modeling anno-
tators: a generative approach to learning from annotator
rationales. In EMNLP ’08: Proceedings of the Confer-
ence on Empirical Methods in Natural Language Process-
ing, pages 31–40, Morristown, NJ, USA. Association for
Computational Linguistics.
Omar F. Zaidan, Jason Eisner, and Christine Piatko. 2007.
Using “annotator rationales” to improve machine learning
for text categorization. In NAACL HLT 2007; Proceedings
of the Main Conference, pages 260–267, April.
</reference>
<page confidence="0.998714">
341
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938451">
<title confidence="0.9930605">Automatically generating annotator rationales to improve sentiment classification</title>
<author confidence="0.993679">Ainur Yessenalina Yejin Choi Claire Cardie</author>
<affiliation confidence="0.980956">Department of Computer Science, Cornell University, Ithaca NY, 14853 USA</affiliation>
<email confidence="0.977449">ychoi,</email>
<abstract confidence="0.9996758125">One of the central challenges in sentimentbased text categorization is that not every portion of a document is equally informative for inferring the overall sentiment of the document. Previous research has shown that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al., We explore methods to autoannotator rationales for document-level sentiment classification. Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>440--447</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="13626" citStr="Blitzer et al. (2007)" startWordPosition="2127" endWordPosition="2130">otated rationales in our dataset (24.7%). 338 % of sentences Precision Recall F-Score Method selected ALL POS NEG ALL POS NEG ALL POS NEG OPINIONFINDER 22.8% 54.9 56.1 54.6 45.1 22.3 65.3 49.5 31.9 59.5 POLARITYLEXICON 38.7% 45.2 42.7 48.5 63.0 71.8 55.0 52.6 53.5 51.6 RANDOM 25.0% 28.9 26.0 31.8 25.9 24.9 26.7 27.3 25.5 29.0 Table 1: Comparison of Automatic vs. Human-annotated Rationales. five different datasets. The first is the movie review data of Pang and Lee (2004), which was manually annotated with rationales by Zaidan et al. (2007)5; the remaining are four product review datasets from Blitzer et al. (2007).6 Only the movie review dataset contains human annotator rationales. We replicate the same feature set and experimental set-up as in Zaidan et al. (2007) to facilitate comparison with their work.7 The contrastive learning method introduced in Zaidan et al. (2007) requires three parameters: (C, µ, Ccontrast). To set the parameters, we use a grid search with step 0.1 for the range of values of each parameter around the point (1,1,1). In total, we try around 3000 different parameter triplets for each type of rationales. 4.1 Experiments with the Movie Review Data We follow Zaidan et al. (2007) fo</context>
<context position="17826" citStr="Blitzer et al. (2007)" startWordPosition="2776" endWordPosition="2779">achieve an even better result if we combine the automatic rationales with human 339 rationales? The answer is yes! The accuracy of OPINIONFINDER+HUMANR@SENTENCE reaches 92.50%, which is statistically significantly better than HUMANR (91.61%). In other words, not only can our automatically generated rationales replace human rationales, but they can also improve upon human rationales when they are available. 4.2 Experiments with the Product Reviews We next evaluate our approaches on datasets for which human annotator rationales do not exist. For this, we use some of the product review data from Blitzer et al. (2007): reviews for Books, DVDs, Videos and Kitchen appliances. Each dataset contains 1000 positive and 1000 negative reviews. The reviews, however, are substantially shorter than those in the movie review dataset: the average number of sentences in each review is 9.20/9.13/8.12/6.37 respectively vs. 30.86 for the movie reviews. We perform 10-fold crossvalidation, where 8 folds are used for training, 1 fold for tuning parameters, and 1 fold for testing. Table 3 shows the results. Rationale-based methods perform statistically significantly better than NORATIONALES for all but the Kitchen dataset. An </context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<date>1999</date>
<pages>169--184</pages>
<contexts>
<context position="12824" citStr="Joachims, 1999" startWordPosition="1995" endWordPosition="1996">xicon approach (F-score 52.6%) match the human rationales much better than those found by random selection (F-score 27.3%). As expected, OpinionFinder’s positive rationales match the human rationales at a significantly lower level (F-score 31.9%) than negative rationales (59.5%). This is due to the fact that OpinionFinder is trained on a dataset biased toward negative sentiment (see Section 3.1 - 3.2). In contrast, all other approaches show a balanced performance for positive and negative rationales vs. human rationales. 4 Experiments For our contrastive learning experiments we use SV Mlight (Joachims, 1999). We evaluate the usefulness of automatically generated rationales on 4We chose the value of 25% to match the percentage of sentences per document, on average, that contain humanannotated rationales in our dataset (24.7%). 338 % of sentences Precision Recall F-Score Method selected ALL POS NEG ALL POS NEG ALL POS NEG OPINIONFINDER 22.8% 54.9 56.1 54.6 45.1 22.3 65.3 49.5 31.9 59.5 POLARITYLEXICON 38.7% 45.2 42.7 48.5 63.0 71.8 55.0 52.6 53.5 51.6 RANDOM 25.0% 28.9 26.0 31.8 25.9 24.9 26.7 27.3 25.5 29.0 Table 1: Comparison of Automatic vs. Human-annotated Rationales. five different datasets. T</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale support vector machine learning practical. pages 169–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Mao</author>
<author>Guy Lebanon</author>
</authors>
<title>Sequential models for sentiment prediction.</title>
<date>2006</date>
<booktitle>In Proceedings of the ICML Workshop: Learning in Structured Output Spaces Open Problems in Statistical Relational Learning Statistical Network Analysis: Models, Issues and</booktitle>
<location>New Directions.</location>
<contexts>
<context position="18934" citStr="Mao and Lebanon, 2006" startWordPosition="2947" endWordPosition="2950">le-based methods perform statistically significantly better than NORATIONALES for all but the Kitchen dataset. An interesting trend in product review datasets is that RANDOM rationales are just as good as other more sophisticated rationales. We suspect that this is because product reviews are generally shorter and more focused than the movie reviews, thereby any randomly selected sentence is likely to be a good rationale. Quantitatively, subjective sentences in the product reviews amount to 78% (McDonald et al., 2007), while subjective sentences in the movie review dataset are only about 25% (Mao and Lebanon, 2006). 4.3 Examples of Annotator Rationales In this section, we examine an example to compare the automatically generated rationales (using OPINIONFINDER) with human annotator rationales for the movie review data. In the following positive document snippet, automatic rationales are underlined, while human-annotated rationales are in bold face. ...But a little niceness goes a long way these days, and there’s no denying the entertainment value of that thing you do! It’s just about impossible to hate. It’s an inoffensive, enjoyable piece ofnostalgia that is sure to leave audiences smiling and humming,</context>
</contexts>
<marker>Mao, Lebanon, 2006</marker>
<rawString>Yi Mao and Guy Lebanon. 2006. Sequential models for sentiment prediction. In Proceedings of the ICML Workshop: Learning in Structured Output Spaces Open Problems in Statistical Relational Learning Statistical Network Analysis: Models, Issues and New Directions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kerry Hannan</author>
<author>Tyler Neylon</author>
<author>Mike Wells</author>
<author>Jeff Reynar</author>
</authors>
<title>Structured models for fine-tocoarse sentiment analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>432--439</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="18835" citStr="McDonald et al., 2007" startWordPosition="2931" endWordPosition="2934"> training, 1 fold for tuning parameters, and 1 fold for testing. Table 3 shows the results. Rationale-based methods perform statistically significantly better than NORATIONALES for all but the Kitchen dataset. An interesting trend in product review datasets is that RANDOM rationales are just as good as other more sophisticated rationales. We suspect that this is because product reviews are generally shorter and more focused than the movie reviews, thereby any randomly selected sentence is likely to be a good rationale. Quantitatively, subjective sentences in the product reviews amount to 78% (McDonald et al., 2007), while subjective sentences in the movie review dataset are only about 25% (Mao and Lebanon, 2006). 4.3 Examples of Annotator Rationales In this section, we examine an example to compare the automatically generated rationales (using OPINIONFINDER) with human annotator rationales for the movie review data. In the following positive document snippet, automatic rationales are underlined, while human-annotated rationales are in bold face. ...But a little niceness goes a long way these days, and there’s no denying the entertainment value of that thing you do! It’s just about impossible to hate. It</context>
</contexts>
<marker>McDonald, Hannan, Neylon, Wells, Reynar, 2007</marker>
<rawString>Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells, and Jeff Reynar. 2007. Structured models for fine-tocoarse sentiment analysis. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 432–439, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1050" citStr="Pang and Lee (2004)" startWordPosition="143" endWordPosition="146">evious research has shown that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al., 2007). We explore methods to automatically generate annotator rationales for document-level sentiment classification. Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales. 1 Introduction One of the central challenges in sentiment-based text categorization is that not every portion of a given document is equally informative for inferring its overall sentiment (e.g., Pang and Lee (2004)). Zaidan et al. (2007) address this problem by asking human annotators to mark (at least some of) the relevant text spans that support each document-level sentiment decision. The text spans of these “rationales” are then used to construct additional training examples that can guide the learning algorithm toward better categorization models. But could we perhaps enjoy the performance gains of rationale-enhanced learning models without any additional human effort whatsoever (beyond the document-level sentiment label)? We hypothesize that in the area of sentiment analysis, where there has been a</context>
<context position="11632" citStr="Pang and Lee (2004)" startWordPosition="1805" endWordPosition="1808">ionales only those sentences whose polarity coincides with the document-level polarity as determined via the voting scheme of Section 3.1. 3.3 Random Selection Finally, we generate annotator rationales randomly, selecting 25% of the sentences from each document4 and treating each as a separate rationale. 3.4 Comparison of Automatic vs. Human-annotated Rationales Before evaluating the performance of the automatically generated rationales, we summarize in Table 1 the differences between automatic vs. human-generated rationales. All computations were performed on the same movie review dataset of Pang and Lee (2004) used in Zaidan et al. (2007). Note, that the Zaidan et al. (2007) annotation guidelines did not insist that annotators mark all rationales, only that some were marked for each document. Nevertheless, we report precision, recall, and F-score based on overlap with the human-annotated rationales of Zaidan et al. (2007), so as to demonstrate the degree to which the proposed approaches align with human intuition. Overlap measures were also employed by Zaidan et al. (2007). As shown in Table 1, the annotator rationales found by OpinionFinder (F-score 49.5%) and the PolarityLexicon approach (F-score</context>
<context position="13480" citStr="Pang and Lee (2004)" startWordPosition="2103" endWordPosition="2106">atically generated rationales on 4We chose the value of 25% to match the percentage of sentences per document, on average, that contain humanannotated rationales in our dataset (24.7%). 338 % of sentences Precision Recall F-Score Method selected ALL POS NEG ALL POS NEG ALL POS NEG OPINIONFINDER 22.8% 54.9 56.1 54.6 45.1 22.3 65.3 49.5 31.9 59.5 POLARITYLEXICON 38.7% 45.2 42.7 48.5 63.0 71.8 55.0 52.6 53.5 51.6 RANDOM 25.0% 28.9 26.0 31.8 25.9 24.9 26.7 27.3 25.5 29.0 Table 1: Comparison of Automatic vs. Human-annotated Rationales. five different datasets. The first is the movie review data of Pang and Lee (2004), which was manually annotated with rationales by Zaidan et al. (2007)5; the remaining are four product review datasets from Blitzer et al. (2007).6 Only the movie review dataset contains human annotator rationales. We replicate the same feature set and experimental set-up as in Zaidan et al. (2007) to facilitate comparison with their work.7 The contrastive learning method introduced in Zaidan et al. (2007) requires three parameters: (C, µ, Ccontrast). To set the parameters, we use a grid search with step 0.1 for the range of values of each parameter around the point (1,1,1). In total, we try </context>
<context position="20654" citStr="Pang and Lee (2004)" startWordPosition="3212" endWordPosition="3215">ER misses some human rationales, it avoids the inclusion of “impossible to hate”, which contains only negative terms and is likely to be confusing for the contrastive learner. 5 Related Work In broad terms, constructing annotator rationales automatically and using them to formulate contrastive examples can be viewed as learning with prior knowledge (e.g., Schapire et al. (2002), Wu and Srihari (2004)). In our task, the prior knowledge corresponds to our operating assumptions given in Section 3. Those assumptions can be loosely connected to recognizing and exploiting discourse structure (e.g., Pang and Lee (2004), Taboada et al. (2009)). Our automatically generated rationales can be potentially combined with other learning frameworks that can exploit annotator rationales, such as Zaidan and Eisner (2008). 6 Conclusions In this paper, we explore methods to automatically generate annotator rationales for document-level sentiment classification. Our study is motivated by the desire to retain the performance gains of rationale-enhanced learning models while eliminating the need for additional human annotation effort. By employing existing resources for sentiment analysis, we can create automatic annotator</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 271, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<journal>Found. Trends Inf. Retr.,</journal>
<pages>2--1</pages>
<contexts>
<context position="1748" citStr="Pang and Lee, 2008" startWordPosition="255" endWordPosition="258"> (at least some of) the relevant text spans that support each document-level sentiment decision. The text spans of these “rationales” are then used to construct additional training examples that can guide the learning algorithm toward better categorization models. But could we perhaps enjoy the performance gains of rationale-enhanced learning models without any additional human effort whatsoever (beyond the document-level sentiment label)? We hypothesize that in the area of sentiment analysis, where there has been a great deal of recent research attention given to various aspects of the task (Pang and Lee, 2008), this might be possible: using existing resources for sentiment analysis, we might be able to construct annotator rationales automatically. In this paper, we explore a number of methods to automatically generate rationales for documentlevel sentiment classification. In particular, we investigate the use of off-the-shelf sentiment analysis components and lexicons for this purpose. Our approaches for generating annotator rationales can be viewed as mostly unsupervised in that we do not require manually annotated rationales for training. Rather unexpectedly, our empirical results show that autom</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In EMNLP ’02: Proceedings of the ACL-02 conference on Empirical methods in natural language processing,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6889" citStr="Pang et al., 2002" startWordPosition="1077" endWordPosition="1080">r this, we rely on the following two assumptions: (1) Regions marked as annotator rationales are more subjective than unmarked regions. (2) The sentiment of each annotator rationale coincides with the document-level sentiment. Note that assumption 1 was not observed in the Zaidan et al. (2007) work: annotators were asked only to mark a few rationales, leaving other (also subjective) rationale sections unmarked. And at first glance, assumption (2) might seem too obvious. But it is important to include as there can be subjective regions with seemingly conflicting sentiment in the same document (Pang et al., 2002). For instance, an author for a movie review might express a positive sentiment toward the movie, while also discussing a negative sentiment toward one of the fictional characters appearing in the movie. This implies that not all subjective regions will be relevant for the documentlevel sentiment classification — rather only those regions whose polarity matches that of the document should be considered. In order to extract regions that satisfy the above assumptions, we first look for subjective regions in each document, then filter out those regions that exhibit a sentiment value (i.e., polari</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In EMNLP ’02: Proceedings of the ACL-02 conference on Empirical methods in natural language processing, pages 79–86, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Marie Rochery</author>
<author>Mazin G Rahim</author>
<author>Narendra Gupta</author>
</authors>
<title>Incorporating prior knowledge into boosting.</title>
<date>2002</date>
<booktitle>In ICML ’02: Proceedings of the Nineteenth International Conference on Machine Learning,</booktitle>
<pages>538--545</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="20415" citStr="Schapire et al. (2002)" startWordPosition="3175" endWordPosition="3178"> 3: Experimental results for subset of Product Review data – The numbers marked with &apos; (or *) are statistically significantly better than NORATIONALES according to a paired t-test with P &lt; 0.05 (or P &lt; 0.08). Notice that, although OPINIONFINDER misses some human rationales, it avoids the inclusion of “impossible to hate”, which contains only negative terms and is likely to be confusing for the contrastive learner. 5 Related Work In broad terms, constructing annotator rationales automatically and using them to formulate contrastive examples can be viewed as learning with prior knowledge (e.g., Schapire et al. (2002), Wu and Srihari (2004)). In our task, the prior knowledge corresponds to our operating assumptions given in Section 3. Those assumptions can be loosely connected to recognizing and exploiting discourse structure (e.g., Pang and Lee (2004), Taboada et al. (2009)). Our automatically generated rationales can be potentially combined with other learning frameworks that can exploit annotator rationales, such as Zaidan and Eisner (2008). 6 Conclusions In this paper, we explore methods to automatically generate annotator rationales for document-level sentiment classification. Our study is motivated b</context>
</contexts>
<marker>Schapire, Rochery, Rahim, Gupta, 2002</marker>
<rawString>Robert E. Schapire, Marie Rochery, Mazin G. Rahim, and Narendra Gupta. 2002. Incorporating prior knowledge into boosting. In ICML ’02: Proceedings of the Nineteenth International Conference on Machine Learning, pages 538–545, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Manfred Stede</author>
</authors>
<title>Genre-based paragraph classification for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL 2009 Conference,</booktitle>
<pages>62--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>London, UK,</location>
<contexts>
<context position="20677" citStr="Taboada et al. (2009)" startWordPosition="3216" endWordPosition="3219">rationales, it avoids the inclusion of “impossible to hate”, which contains only negative terms and is likely to be confusing for the contrastive learner. 5 Related Work In broad terms, constructing annotator rationales automatically and using them to formulate contrastive examples can be viewed as learning with prior knowledge (e.g., Schapire et al. (2002), Wu and Srihari (2004)). In our task, the prior knowledge corresponds to our operating assumptions given in Section 3. Those assumptions can be loosely connected to recognizing and exploiting discourse structure (e.g., Pang and Lee (2004), Taboada et al. (2009)). Our automatically generated rationales can be potentially combined with other learning frameworks that can exploit annotator rationales, such as Zaidan and Eisner (2008). 6 Conclusions In this paper, we explore methods to automatically generate annotator rationales for document-level sentiment classification. Our study is motivated by the desire to retain the performance gains of rationale-enhanced learning models while eliminating the need for additional human annotation effort. By employing existing resources for sentiment analysis, we can create automatic annotator rationales that are as</context>
</contexts>
<marker>Taboada, Brooke, Stede, 2009</marker>
<rawString>Maite Taboada, Julian Brooke, and Manfred Stede. 2009. Genre-based paragraph classification for sentiment analysis. In Proceedings of the SIGDIAL 2009 Conference, pages 62–70, London, UK, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="10782" citStr="Wiebe et al., 2005" startWordPosition="1682" endWordPosition="1685">utral. We construct rationales from the polarity lexicon for every instance of positive and negative words in the lexicon that appear in the training corpus. As in the OpinionFinder rationales, we extend the words found by the PolarityLexicon approach to sentence boundaries to incorporate potentially 1Available at www.cs.pitt.edu/mpqa/opinionfinderrelease/. 2This conjecture is indirectly confirmed by the fact that human-annotated rationales are rarely a single word. 3It is worthwhile to note that OpinionFinder is trained on a newswire corpus whose prevailing sentiment is known to be negative (Wiebe et al., 2005). Furthermore, OpinionFinder is trained for a task (word-level sentiment classification) that is different from marking annotator rationales (sequence tagging or text segmentation). relevant contextual information. We retain as rationales only those sentences whose polarity coincides with the document-level polarity as determined via the voting scheme of Section 3.1. 3.3 Random Selection Finally, we generate annotator rationales randomly, selecting 25% of the sentences from each document4 and treating each as a separate rationale. 3.4 Comparison of Automatic vs. Human-annotated Rationales Befo</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 1(2):0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Opinionfinder: a system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP on Interactive Demonstrations,</booktitle>
<pages>34--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8539" citStr="Wilson et al., 2005" startWordPosition="1341" endWordPosition="1345">ake use of only the document-level sentiment and off-the-shelf utilities that were trained for slightly different sentiment classification tasks using a corpus from a different domain and of a different genre. Although such utilities might not be optimal for our task, we hoped that these basic resources from the research community would constitute an adequate source of sentiment information for our purposes. We next describe three methods for the automatic acquisition of rationales. �ξi + Ccontrast ξij (1) ij 337 3.1 Contextual Polarity Classification The first approach employs OpinionFinder (Wilson et al., 2005a), an off-the-shelf opinion analysis utility.1 In particular, OpinionFinder identifies phrases expressing positive or negative opinions. Because OpinionFinder models the task as a word-based classification problem rather than a sequence tagging task, most of the identified opinion phrases consist of a single word. In general, such short text spans cannot fully incorporate the contextual information relevant to the detection of subjective language (Wilson et al., 2005a). Therefore, we conjecture that good rationales should extend beyond short phrases.2 For simplicity, we choose to extend Opini</context>
<context position="10051" citStr="Wilson et al. (2005" startWordPosition="1575" endWordPosition="1578">m a simple voting — if words with positive (or negative) polarity dominate, then we consider the entire sentence as positive (or negative). We ignore sentences with a tie. Each selected sentence is considered as a separate rationale. 3.2 Polarity Lexicons Unfortunately, domain shift as well as task mismatch could be a problem with any opinion utility based on supervised learning.3 Therefore, we next consider an approach that does not rely on supervised learning techniques but instead explores the use of a manually constructed polarity lexicon. In particular, we use the lexicon constructed for Wilson et al. (2005b), which contains about 8000 words. Each entry is assigned one of three polarity values: positive, negative, neutral. We construct rationales from the polarity lexicon for every instance of positive and negative words in the lexicon that appear in the training corpus. As in the OpinionFinder rationales, we extend the words found by the PolarityLexicon approach to sentence boundaries to incorporate potentially 1Available at www.cs.pitt.edu/mpqa/opinionfinderrelease/. 2This conjecture is indirectly confirmed by the fact that human-annotated rationales are rarely a single word. 3It is worthwhile</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005a. Opinionfinder: a system for subjectivity analysis. In Proceedings of HLT/EMNLP on Interactive Demonstrations, pages 34– 35, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8539" citStr="Wilson et al., 2005" startWordPosition="1341" endWordPosition="1345">ake use of only the document-level sentiment and off-the-shelf utilities that were trained for slightly different sentiment classification tasks using a corpus from a different domain and of a different genre. Although such utilities might not be optimal for our task, we hoped that these basic resources from the research community would constitute an adequate source of sentiment information for our purposes. We next describe three methods for the automatic acquisition of rationales. �ξi + Ccontrast ξij (1) ij 337 3.1 Contextual Polarity Classification The first approach employs OpinionFinder (Wilson et al., 2005a), an off-the-shelf opinion analysis utility.1 In particular, OpinionFinder identifies phrases expressing positive or negative opinions. Because OpinionFinder models the task as a word-based classification problem rather than a sequence tagging task, most of the identified opinion phrases consist of a single word. In general, such short text spans cannot fully incorporate the contextual information relevant to the detection of subjective language (Wilson et al., 2005a). Therefore, we conjecture that good rationales should extend beyond short phrases.2 For simplicity, we choose to extend Opini</context>
<context position="10051" citStr="Wilson et al. (2005" startWordPosition="1575" endWordPosition="1578">m a simple voting — if words with positive (or negative) polarity dominate, then we consider the entire sentence as positive (or negative). We ignore sentences with a tie. Each selected sentence is considered as a separate rationale. 3.2 Polarity Lexicons Unfortunately, domain shift as well as task mismatch could be a problem with any opinion utility based on supervised learning.3 Therefore, we next consider an approach that does not rely on supervised learning techniques but instead explores the use of a manually constructed polarity lexicon. In particular, we use the lexicon constructed for Wilson et al. (2005b), which contains about 8000 words. Each entry is assigned one of three polarity values: positive, negative, neutral. We construct rationales from the polarity lexicon for every instance of positive and negative words in the lexicon that appear in the training corpus. As in the OpinionFinder rationales, we extend the words found by the PolarityLexicon approach to sentence boundaries to incorporate potentially 1Available at www.cs.pitt.edu/mpqa/opinionfinderrelease/. 2This conjecture is indirectly confirmed by the fact that human-annotated rationales are rarely a single word. 3It is worthwhile</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005b. Recognizing contextual polarity in phrase-level sentiment analysis. In HLT-EMNLP ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347– 354, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyun Wu</author>
<author>Rohini Srihari</author>
</authors>
<title>Incorporating prior knowledge with weighted margin support vector machines.</title>
<date>2004</date>
<booktitle>In KDD ’04: Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>326--333</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="20438" citStr="Wu and Srihari (2004)" startWordPosition="3179" endWordPosition="3182"> for subset of Product Review data – The numbers marked with &apos; (or *) are statistically significantly better than NORATIONALES according to a paired t-test with P &lt; 0.05 (or P &lt; 0.08). Notice that, although OPINIONFINDER misses some human rationales, it avoids the inclusion of “impossible to hate”, which contains only negative terms and is likely to be confusing for the contrastive learner. 5 Related Work In broad terms, constructing annotator rationales automatically and using them to formulate contrastive examples can be viewed as learning with prior knowledge (e.g., Schapire et al. (2002), Wu and Srihari (2004)). In our task, the prior knowledge corresponds to our operating assumptions given in Section 3. Those assumptions can be loosely connected to recognizing and exploiting discourse structure (e.g., Pang and Lee (2004), Taboada et al. (2009)). Our automatically generated rationales can be potentially combined with other learning frameworks that can exploit annotator rationales, such as Zaidan and Eisner (2008). 6 Conclusions In this paper, we explore methods to automatically generate annotator rationales for document-level sentiment classification. Our study is motivated by the desire to retain </context>
</contexts>
<marker>Wu, Srihari, 2004</marker>
<rawString>Xiaoyun Wu and Rohini Srihari. 2004. Incorporating prior knowledge with weighted margin support vector machines. In KDD ’04: Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 326–333, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Jason Eisner</author>
</authors>
<title>Modeling annotators: a generative approach to learning from annotator rationales.</title>
<date>2008</date>
<booktitle>In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>31--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="20849" citStr="Zaidan and Eisner (2008)" startWordPosition="3241" endWordPosition="3244"> In broad terms, constructing annotator rationales automatically and using them to formulate contrastive examples can be viewed as learning with prior knowledge (e.g., Schapire et al. (2002), Wu and Srihari (2004)). In our task, the prior knowledge corresponds to our operating assumptions given in Section 3. Those assumptions can be loosely connected to recognizing and exploiting discourse structure (e.g., Pang and Lee (2004), Taboada et al. (2009)). Our automatically generated rationales can be potentially combined with other learning frameworks that can exploit annotator rationales, such as Zaidan and Eisner (2008). 6 Conclusions In this paper, we explore methods to automatically generate annotator rationales for document-level sentiment classification. Our study is motivated by the desire to retain the performance gains of rationale-enhanced learning models while eliminating the need for additional human annotation effort. By employing existing resources for sentiment analysis, we can create automatic annotator rationales that are as good as human annotator rationales in improving document-level sentiment classification. Acknowledgments We thank anonymous reviewers for their comments. This work was sup</context>
</contexts>
<marker>Zaidan, Eisner, 2008</marker>
<rawString>Omar F. Zaidan and Jason Eisner. 2008. Modeling annotators: a generative approach to learning from annotator rationales. In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 31–40, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Jason Eisner</author>
<author>Christine Piatko</author>
</authors>
<title>Using “annotator rationales” to improve machine learning for text categorization.</title>
<date>2007</date>
<booktitle>In NAACL HLT 2007; Proceedings of the Main Conference,</booktitle>
<pages>260--267</pages>
<contexts>
<context position="1073" citStr="Zaidan et al. (2007)" startWordPosition="147" endWordPosition="150">own that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al., 2007). We explore methods to automatically generate annotator rationales for document-level sentiment classification. Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales. 1 Introduction One of the central challenges in sentiment-based text categorization is that not every portion of a given document is equally informative for inferring its overall sentiment (e.g., Pang and Lee (2004)). Zaidan et al. (2007) address this problem by asking human annotators to mark (at least some of) the relevant text spans that support each document-level sentiment decision. The text spans of these “rationales” are then used to construct additional training examples that can guide the learning algorithm toward better categorization models. But could we perhaps enjoy the performance gains of rationale-enhanced learning models without any additional human effort whatsoever (beyond the document-level sentiment label)? We hypothesize that in the area of sentiment analysis, where there has been a great deal of recent r</context>
<context position="3103" citStr="Zaidan et al. (2007)" startWordPosition="455" endWordPosition="458">ie reviews. In addition, complementing the human annotator rationales with automatic rationales boosts the performance even further for this domain, achieving 92.5% accuracy. We further evaluate our rationale-generation approaches on product review data for which human rationales are not available: here we find that even randomly generated rationales can improve the classification accuracy although rationales generated from sentiment resources are not as effective as for movie reviews. The rest of the paper is organized as follows. We first briefly summarize the SVM-based learning approach of Zaidan et al. (2007) that allows the incorporation of rationales (Section 2). We next introduce three methods for the automatic generation of rationales (Section 3). The experimental results are presented in Section 4, followed by related work (Section 5) and conclusions (Section 6). 2 Contrastive Learning with SVMs Zaidan et al. (2007) first introduced the notion of annotator rationales — text spans highlighted by human annotators as support or evidence for each document-level sentiment decision. These rationales, of course, are only useful if the sentiment categorization algorithm can be extended to exploit the</context>
<context position="4795" citStr="Zaidan et al. (2007)" startWordPosition="720" endWordPosition="723">tionale ~rij in the set, construct a contrastive training example ~vij, by removing the text span associated with the rationale ~rij from the original review ~xi. Intuitively, the contrastive example ~vij should not be as informative to the learning algorithm as the original review ~xi, since one of the supporting regions identified by the human annotator has been deleted. That is, the correct learned model should be less confident of its classification of a contrastive example vs. the corresponding original example, and the classification boundary of the model should be modified accordingly. Zaidan et al. (2007) formulate exactly this intuition as SVM constraints as follows: (∀i, j) : yi (~w~xi − ~w~vij) ≥ µ(1 − ξij) where yi ∈ {−1, +1} is the negative/positive sentiment label of document i, w~ is the weight vector, µ ≥ 0 controls the size of the margin between the original examples and the contrastive examples, and ξij are the associated slack variables. After some re-writing of the equations, the resulting objective function and constraints for the SVM are as follows: 1 � 2 ||~w||2 + C i subject to constraints: (∀i) : yi w~ · ~xi ≥ 1 − ξi, ξi ≥ 0 (∀i, j) : yi w~ · ~xij ≥ 1 − ξij ξij ≥ 0 where ξi an</context>
<context position="6565" citStr="Zaidan et al. (2007)" startWordPosition="1025" endWordPosition="1028">aller than C for noisy rationales. In the work described below, we similarly employ Zaidan et al.’s (2007) contrastive learning method to incorporate rationales for documentlevel sentiment categorization. 3 Automatically Generating Rationales Our goal in the current work, is to generate annotator rationales automatically. For this, we rely on the following two assumptions: (1) Regions marked as annotator rationales are more subjective than unmarked regions. (2) The sentiment of each annotator rationale coincides with the document-level sentiment. Note that assumption 1 was not observed in the Zaidan et al. (2007) work: annotators were asked only to mark a few rationales, leaving other (also subjective) rationale sections unmarked. And at first glance, assumption (2) might seem too obvious. But it is important to include as there can be subjective regions with seemingly conflicting sentiment in the same document (Pang et al., 2002). For instance, an author for a movie review might express a positive sentiment toward the movie, while also discussing a negative sentiment toward one of the fictional characters appearing in the movie. This implies that not all subjective regions will be relevant for the do</context>
<context position="11661" citStr="Zaidan et al. (2007)" startWordPosition="1811" endWordPosition="1814"> whose polarity coincides with the document-level polarity as determined via the voting scheme of Section 3.1. 3.3 Random Selection Finally, we generate annotator rationales randomly, selecting 25% of the sentences from each document4 and treating each as a separate rationale. 3.4 Comparison of Automatic vs. Human-annotated Rationales Before evaluating the performance of the automatically generated rationales, we summarize in Table 1 the differences between automatic vs. human-generated rationales. All computations were performed on the same movie review dataset of Pang and Lee (2004) used in Zaidan et al. (2007). Note, that the Zaidan et al. (2007) annotation guidelines did not insist that annotators mark all rationales, only that some were marked for each document. Nevertheless, we report precision, recall, and F-score based on overlap with the human-annotated rationales of Zaidan et al. (2007), so as to demonstrate the degree to which the proposed approaches align with human intuition. Overlap measures were also employed by Zaidan et al. (2007). As shown in Table 1, the annotator rationales found by OpinionFinder (F-score 49.5%) and the PolarityLexicon approach (F-score 52.6%) match the human ratio</context>
<context position="13550" citStr="Zaidan et al. (2007)" startWordPosition="2114" endWordPosition="2117">the percentage of sentences per document, on average, that contain humanannotated rationales in our dataset (24.7%). 338 % of sentences Precision Recall F-Score Method selected ALL POS NEG ALL POS NEG ALL POS NEG OPINIONFINDER 22.8% 54.9 56.1 54.6 45.1 22.3 65.3 49.5 31.9 59.5 POLARITYLEXICON 38.7% 45.2 42.7 48.5 63.0 71.8 55.0 52.6 53.5 51.6 RANDOM 25.0% 28.9 26.0 31.8 25.9 24.9 26.7 27.3 25.5 29.0 Table 1: Comparison of Automatic vs. Human-annotated Rationales. five different datasets. The first is the movie review data of Pang and Lee (2004), which was manually annotated with rationales by Zaidan et al. (2007)5; the remaining are four product review datasets from Blitzer et al. (2007).6 Only the movie review dataset contains human annotator rationales. We replicate the same feature set and experimental set-up as in Zaidan et al. (2007) to facilitate comparison with their work.7 The contrastive learning method introduced in Zaidan et al. (2007) requires three parameters: (C, µ, Ccontrast). To set the parameters, we use a grid search with step 0.1 for the range of values of each parameter around the point (1,1,1). In total, we try around 3000 different parameter triplets for each type of rationales. </context>
<context position="15455" citStr="Zaidan et al. (2007)" startWordPosition="2418" endWordPosition="2421">hu.edu/∼ozaidan/rationales/. 6http://www.cs.jhu.edu/∼mdredze/datasets/sentiment/. 7We use binary unigram features corresponding to the unstemmed words or punctuation marks with count greater or equal to 4 in the full 2000 documents, then we normalize the examples to the unit length. When computing the pseudo ex�{-vij amples xij = we first compute (�xi − �vij) using the µ binary representation. As a result, features (unigrams) that appeared in both vectors will be zeroed out in the resulting vector. We then normalize the resulting vector to a unit vector. 8The performance of HUMANR reported by Zaidan et al. (2007) is 92.2% which lies between the performance we get (91.61%) and the oracle accuracy we get if we knew the best parameters for the test set (92.67%). Method Accuracy NORATIONALES 88.56 HUMANR 91.61• HUMANR@SENTENCE 91.33• † OPINIONFINDER 91.78• † POLARITYLEXICON 91.39• † RANDOM 90.00∗ OPINIONFINDER+HUMANR@SENTENCE 92.50• 4 Table 2: Experimental results for the movie review data. – The numbers marked with • (or ∗) are statistically significantly better than NORATIONALES according to a paired t-test with P &lt; 0.001 (or P &lt; 0.01). – The numbers marked with 4 are statistically significantly better </context>
</contexts>
<marker>Zaidan, Eisner, Piatko, 2007</marker>
<rawString>Omar F. Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “annotator rationales” to improve machine learning for text categorization. In NAACL HLT 2007; Proceedings of the Main Conference, pages 260–267, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>