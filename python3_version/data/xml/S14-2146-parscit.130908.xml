<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004032">
<title confidence="0.9965935">
UWM: Applying an Existing Trainable Semantic Parser to Parse Robotic
Spatial Commands
</title>
<author confidence="0.990695">
Rohit J. Kate
</author>
<affiliation confidence="0.993581">
University of Wisconsin-Milwaukee
</affiliation>
<address confidence="0.732024">
Milwaukee, WI
</address>
<email confidence="0.99901">
katerj@uwm.edu
</email>
<sectionHeader confidence="0.993902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999963111111111">
This paper describes Team UWM’s sys-
tem for the Task 6 of SemEval 2014
for doing supervised semantic parsing of
robotic spatial commands. An existing
semantic parser, KRISP, was trained us-
ing the provided training data of natural
language robotic spatial commands paired
with their meaning representations in the
formal robot command language. The en-
tire process required very little manual ef-
fort. Without using the additional annota-
tions of word-aligned semantic trees, the
trained parser was able to exactly parse
new commands into their meaning repre-
sentations with 51.18% best F-measure at
72.67% precision and 39.49% recall. Re-
sults show that the parser was particularly
accurate for short sentences.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.959264916666667">
Semantic parsing is the task of converting natu-
ral language utterances into their complete formal
meaning representations which are executable for
some application. Example applications of seman-
tic parsing include giving natural language com-
mands to robots and querying databases in natu-
ral language. Some old semantic parsers were de-
veloped manually to work for specific applications
(Woods, 1977; Warren and Pereira, 1982). How-
ever, such semantic parsers were generally brittle
and building them required a lot of manual effort.
In addition, these parsers could not be ported to
any other application without again putting signif-
icant manual effort.
More recently, several semantic parsers have
been developed using machine learning (Zelle and
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
Mooney, 1996; Ge and Mooney, 2005; Zettle-
moyer and Collins, 2005; Wong and Mooney,
2006; Kate and Mooney, 2006; Lu et al., 2008;
Kwiatkowski et al., 2011). In this approach, train-
ing data is first created for the domain of inter-
est. Then using one of the many machine learn-
ing methods and semantic parsing frameworks, a
semantic parser is automatically learned from the
training data (Mooney, 2007). The trained seman-
tic parser is then capable of parsing new natu-
ral language utterances into their meaning repre-
sentations. Semantic parsers built using machine
learning tend to be more robust and can be easily
ported to other application domains with appropri-
ate domain-specific training data.
The Task 6 of SemEval 2014 provided a new ap-
plication domain for semantic parsing along with
training and test data. The domain involved giv-
ing natural language commands to a robotic arm
which would then move blocks on a board (Dukes,
2013). The domain was inspired from the classic
AI system SHRDLU (Winograd, 1972). The train-
ing data contained 2500 examples of sentences
paired with their meaning representations in the
Robot Command Language (RCL) which was de-
signed for this domain (Dukes, 2013). The test
data contained 909 such example pairs.
We trained an existing and freely available1 se-
mantic parser KRISP (Kate and Mooney, 2006)
using the training data for this domain. Besides
changing the format of the data for running KRISP
and writing a context-free grammar for the mean-
ing representation language RCL, the entire pro-
cess required minimal manual effort. The author
spent less than a week’s time for participating in
the Task 6, and most of it was spent in running
the experiments. This demonstrates that train-
able semantic parsers like KRISP can be rapidly
adopted to new domains. In the Results section
we show different precisions and recalls it ob-
</bodyText>
<footnote confidence="0.992884">
1http://www.cs.utexas.edu/users/ml/krisp/
</footnote>
<page confidence="0.95762">
823
</page>
<note confidence="0.7338895">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 823–827,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.9997509">
tained at different confidence levels in the form of
a precision-recall curve. The results also show that
the parser was particularly accurate on shorter sen-
tences. Two major reasons that prevented KRISP
from performing better on this domain were - its
high computational demand for memory which
prevented it from being trained beyond 1500 train-
ing examples, and some variability in the mean-
ing representation language RCL that negatively
affected training as well as evaluation.
</bodyText>
<sectionHeader confidence="0.961167" genericHeader="method">
2 Background: KRISP Semantic Parser
</sectionHeader>
<bodyText confidence="0.998347666666667">
KRISP (Kernel-based Robust Interpretation for Se-
mantic Parsing) is a trainable semantic parser
(Kate and Mooney, 2006) that uses Support Vector
Machines (SVMs) (Cristianini and Shawe-Taylor,
2000) as the machine learning method with string-
subsequence kernel (Lodhi et al., 2002). It takes
natural language utterances and their correspond-
ing formal meaning representation as the training
data along with the context-free grammar of the
meaning representation language (MRL). The key
idea in KRISP is that every production of the MRL
is treated as a semantic concept. For every MRL
production, an SVM classifier is trained so that it
can give for any input natural language substring
of words the probability that it expresses the corre-
sponding semantic concept. Once these classifiers
are trained, parsing a sentence reduces to finding
the most probable semantic derivation of the sen-
tence in which different productions cover differ-
ent parts of the sentence and together form a com-
plete meaning representation. Figure 1 shows an
example semantic derivation of a robotic spatial
command. Productions of RCL grammar (Table 1)
are shown at tree nodes depicting different parts of
the sentence they cover.
Since the training data is not in the form of such
semantic derivations, an EM-like iterative algo-
rithm is used to collect appropriate positive and
negative examples in order to train the classifiers
(Kate and Mooney, 2006). Positive examples are
collected from correct semantic derivations de-
rived by the parser learned in the previous itera-
tion, and negative examples are collected from the
incorrect semantic derivations.
KRISP was shown to work well on the US geog-
raphy database query domain (Tang and Mooney,
2001) as well as on the RoboCup Coach Lan-
guage (CLang) domain (Kate et al., 2005). It was
also shown to be particularly robust to noise in
</bodyText>
<figureCaption confidence="0.9964185">
Figure 1: Semantic derivation of the robotic spatial com-
mand “pick up the turquoise pyramid” obtained by KRISP
during testing which gives the correct RCL representation
(event: (action: take) (entity: (color: cyan) (type: prism))).
</figureCaption>
<bodyText confidence="0.999959555555555">
the natural language utterances (Kate and Mooney,
2006). KRISP was later extended to do semi-
supervised semantic parsing (Kate and Mooney,
2007b), to learn from ambiguous supervision in
which multiple sentences could be paired with a
single meaning representation in the training data
(Kate and Mooney, 2007a), and to transform the
MRL grammar to improve semantic parsing (Kate,
2008).
</bodyText>
<sectionHeader confidence="0.997843" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.99998125">
In order to apply KRISP to the Task 6 of SemEval
2014, the format of the provided data was first
changed to the XML-type format that KRISP ac-
cepts. The data contained several instances of
co-references which was also part of RCL, but
KRISP was not designed to handle co-references
and expects them to be pre-resolved. We ob-
served that almost all co-references in the mean-
ing representations, indicated by “reference-id”
token, resolved to the first occurrence of an “en-
tity” element in the meaning representation. This
was found to be true for more than 99% of the
cases. We used this observation to resolve co-
references during semantic parsing in the follow-
ing way. As a pre-processing step, we first remove
from the meaning representations all the “id:” to-
kens (these resolve the references) but keep the
“reference-id:” tokens (these encode presence of
co-references). The natural language sentences
are not modified in any way and the parser learns
from the training data to relate words like “it”
and “one” to the RCL token “reference-id”. After
KRISP generates a meaning representation during
testing, as a post-processing step, “id: 1” is added
to the first “entity” element in the meaning repre-
sentation if it contains the “reference-id:” token.
The context-free grammar for RCL was not pro-
vided by the Task organizers. There are multi-
</bodyText>
<page confidence="0.990436">
824
</page>
<bodyText confidence="0.999925215686275">
ple ways to write a context-free grammar for a
meaning representation language and those that
conform better to natural language work better
for semantic parsing (Kate, 2008). We manu-
ally wrote grammar for RCL which mostly fol-
lowed the structure of the meaning representa-
tions as they already conformed highly to natural
language commands and hence writing the gram-
mar was straightforward. KRISP runs faster if
there are fewer non-terminals on the right-hand-
side (RHS) of the grammar because that makes
the search for the most probable semantic deriva-
tion faster. Hence we kept non-terminals on RHS
as few as possible while writing the grammar.
Table 1 shows the entire grammar for RCL that
we wrote which was given to KRISP. The non-
terminals are indicated with a “*” in their front.
We point out that KRISP needs grammar only for
the meaning representation language (an applica-
tion will need it anyway if the statements are to be
executed) and not for the natural language.
KRISP’s training algorithm could be aided by
providing it with information about which natu-
ral language words are usually used to express the
concept of a production. For example, word “red”
usually expresses “*color: → ( color: red )”. The
data provided with the Task 6 came with the word-
aligned semantic trees which indicated which nat-
ural language words corresponded to which mean-
ing representation components. This information
could have been used to aid KRISP, however, we
found many inconsistencies and errors in the pro-
vided word-aligned semantic trees and chose not
to use them. In addition, KRISP seemed to learn
most of that information on its own anyway.
The Task 6 also included integrating semantic
parsing with spatial planning. This meant that if
the semantic parser generates an RCL representa-
tion that does not make sense for the given block
configuration on the board, then it could be dis-
missed and the next best RCL representation could
be considered. Besides generating the best mean-
ing representation for a natural language utterance,
KRISP is also capable of generating multiple pos-
sible meaning representations sorted by their prob-
abilities. We could have used this capability to
output only the best RCL representation that is
valid for the given board configuration. Unfortu-
nately, unfamiliarity with the provided Java API
for the spatial planner and lack of time prevented
us from doing this.
</bodyText>
<table confidence="0.926445323529412">
*action: → ( action: move)
*action: → ( action: drop )
*action: → ( action: take )
*cardinal: → ( cardinal: 1 )
*cardinal: → ( cardinal: 2 )
*cardinal: → ( cardinal: 3 )
*cardinal: → ( cardinal: 4 )
*color: → ( color: magenta)
*color: → ( color: red )
*color: → ( color: white)
*color: → ( color: cyan )
*color: → ( color: green)
*color: → ( color: yellow )
*color: → ( color: blue)
*color: → ( color: gray)
*indicator: → ( indicator: rightmost)
*indicator: → ( indicator: back)
*indicator: → ( indicator: center)
*indicator: → ( indicator: right)
*indicator: → ( indicator: leftmost)
*indicator: → ( indicator: individual )
*indicator: → ( indicator: nearest )
*indicator: → ( indicator: front)
*indicator: → ( indicator: left )
*reference-id: → ( reference-id: 1 )
*relation: → ( relation: right)
*relation: → ( relation: forward)
*relation: → ( relation: within )
*relation: → ( relation: above )
*relation: → ( relation: nearest )
*relation: → ( relation: adjacent)
*relation: → ( relation: front )
*relation: → ( relation: left)
*relation: → ( relation: backward)
</table>
<figure confidence="0.631858463414634">
*type: → ( type: type-reference-group )
*type: → ( type: board)
*type: → ( type: prism)
*type: → ( type: cube)
*type: → ( type: type-reference )
*type: → ( type: cube-group )
*type: → ( type: corner )
*type: → ( type: robot)
*type: → ( type: stack )
*type: → ( type: edge)
*type: → ( type: region)
*type: → ( type: tile )
*type: → ( type: reference )
*indicator: → *indicator: *indicator:
*color: → *color: *color:
*ct: → *color: *type:
*ict: → *indicator: *ct:
*ctr: → *ct: *reference-id:
*cct: → *cardinal: *ct:
*ed: → *entity: ( destination: *spatial-relation: )
*entity: → ( entity: *type: )
*entity: → ( entity: *type: *reference-id: )
*entity: → ( entity: *type: *spatial-relation: )
*entity: → ( entity: *ct: )
*entity: → ( entity: *indicator: *type: )
*entity: → ( entity: *ict: )
*entity: → ( entity: *ict: *spatial-relation: )
*entity: → ( entity: *cardinal: *type: )
*entity: → ( entity: *cct: )
*entity: → ( entity: *cct: *spatial-relation: )
*entity: → ( entity: *ctr: )
*entity: → ( entity: *ct: *spatial-relation: )
*entity: → ( entity: *ctr: *spatial-relation: )
*measure: → ( measure: *entity: )
*mr: → *measure: *relation:
*spatial-relation: → ( spatial-relation: *relation: *entity: )
*spatial-relation: → ( spatial-relation: *mr: )
*spatial-relation: → ( spatial-relation: *mr: *entity: )
*S → ( sequence: *S *S )
*S → ( event: *action: *ed: )
*S → ( event: *action: *entity: )
</figure>
<tableCaption confidence="0.67729">
Table 1: Grammar for the Robot Command Lan-
</tableCaption>
<bodyText confidence="0.802109666666667">
guage (RCL) given to KRISP for semantic parsing.
The non-terminals are indicated with a “*” in their
front. The start symbol is *S.
</bodyText>
<page confidence="0.989855">
825
</page>
<figure confidence="0.998391714285714">
Precision (%)
100
90
80
70
60
50
40
30
20
10
0
0 5 10 15 20 25 30 35 40 45 50
Recall (%)
</figure>
<figureCaption confidence="0.9858685">
Figure 2: Precision-recall curve for the semantic
parsing output on test sentences.
</figureCaption>
<sectionHeader confidence="0.999758" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999900314285714">
We found that KRISP could not be trained beyond
1500 examples in this domain because the num-
ber of negative examples that are generated during
the training process would become too large for
the available memory size. This is something that
could be fixed in the future by suitably sampling
negative examples. Using the first 1500 train-
ing examples, we evaluated KRISP’s performance
on the provided 909 test examples. A generated
RCL representation is considered correct only if
it exactly matches the correct answer; no partial
credit is given. In order to avoid generating incor-
rect meaning representations when it is not confi-
dent, KRISP uses a threshold and if the confidence
(probability) of the best semantic derivation is be-
low this threshold, it does not generate any mean-
ing representation. This threshold was set to 0.05
as was previously done for other domains.
Performance was measured in terms of preci-
sion (the percentage of generated meaning repre-
sentations that were correct) and recall (the per-
centage of all sentences for which correct meaning
representations were obtained). Given that KRISP
also gives confidences with its output meaning
representations, we can compute precisions and
recalls at various confidence levels. Figure 2
shows the entire precision-recall curve thus ob-
tained. The best F-measure (harmonic mean of
precision and recall) on this curve is 51.18% pdf
at 72.67% precision and 39.49% recall. The pre-
cision at the highest recall was 45.98% which we
had reported as our official evaluation result for
the SemEval Task 6.
We further analyzed the results according to the
lengths of the sentences and found that KRISP was
</bodyText>
<table confidence="0.9980329">
Sentence length Accuracy (Correct/Total)
1-3 100.00% (15/15)
4-7 71.20% (136/191)
8-11 51.76% (147/284)
12-15 41.80% (79/189)
16-19 22.22% (28/126)
20-23 15.71% (11/70)
24-27 3.23% (1/31)
28-31 33.33% (1/3)
All 45.98% (418/909)
</table>
<tableCaption confidence="0.9916015">
Table 2: Accuracy of semantic parsing across test
sentences of varying lengths.
</tableCaption>
<bodyText confidence="0.999945708333333">
very accurate with shorter sentences and became
progressively less accurate as the lengths of the
sentences increase. Table 2 shows these results.
This could be simply because the longer the sen-
tence, the more the likelihood of making an error,
and since no partial credit is given, the entire out-
put meaning representation is deemed incorrect.
On further error analysis we observed that there
was some variability in the meaning representa-
tions. The “move” and “drop” actions seemed
to mean the same thing and were used alterna-
tively. For example in the training data, the ut-
terance “place the red block on single blue block”
had “(action: drop)” in the corresponding mean-
ing representation, while “place red cube on grey
cube” had “(action: move)”, but there is no ap-
parent difference between the two cases. There
were many such instances. This was confusing
KRISP’s training algorithm because it would col-
lect the same phrase sometimes as a positive ex-
ample and sometimes as a negative example. This
also affected the evaluation, because KRISP would
generate “move” which won’t match “drop”, or
vice-versa, and the evaluator will call it an error.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999623">
We participated in the SemEval 2014 Task 6 of su-
pervised semantic parsing of robotic spatial com-
mands. We used an existing semantic parser
learner, KRISP, and trained it on this domain
which required minimum time and effort from our
side. The trained parser was able to map natu-
ral language robotic spatial commands into their
formal robotic command language representations
with good accuracy, particularly for shorter sen-
tences.
</bodyText>
<page confidence="0.99381">
826
</page>
<note confidence="0.969826363636364">
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
08), Honolulu, HI, October.
References
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Uni-
versity Press.
</note>
<reference confidence="0.998530433333333">
Kais Dukes. 2013. Semantic annotation of robotic
spatial commands. In Proceedings of the Language
and Technology Conference (LTC-2013), Poznan,
Poland.
Ruifang Ge and Raymond J. Mooney. 2005. A sta-
tistical semantic parser that integrates syntax and
semantics. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 9–16, Ann Arbor, MI, July.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING/ACL-06), pages 913–920, Sydney, Aus-
tralia, July.
Rohit J. Kate and Raymond J. Mooney. 2007a. Learn-
ing language semantics from ambiguous supervi-
sion. In Proceedings of the Twenty-Second Con-
ference on Artificial Intelligence (AAAI-07), pages
895–900, Vancouver, Canada, July.
Rohit J. Kate and Raymond J. Mooney. 2007b.
Semi-supervised learning for semantic parsing us-
ing support vector machines. In Proceedings of
Human Language Technologies: The Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT-07), pages
81–84, Rochester, NY, April.
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to for-
mal languages. In Proceedings of the Twentieth Na-
tional Conference on Artificial Intelligence (AAAI-
05), pages 1062–1068, Pittsburgh, PA, July.
Rohit J. Kate. 2008. Transforming meaning represen-
tation grammars to improve semantic parsing. In
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning (CoNLL-2008),
pages 33–40. Association for Computational Lin-
guistics.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2011), pages 1512–1523. Association for Computa-
tional Linguistics.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. 2:419–444.
Raymond J. Mooney. 2007. Learning for semantic
parsing. In A. Gelbukh, editor, Computational Lin-
guistics and Intelligent Text Processing: Proceed-
ings of the 8th International Conference (CICLing-
2007), Mexico City, pages 311–324. Springer Ver-
lag, Berlin.
Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Proceedings
of the 12th European Conference on Machine Learn-
ing (ECML-2001), pages 466–477, Freiburg, Ger-
many.
David H. D. Warren and Fernando C. N. Pereira. 1982.
An efficient easily adaptable system for interpret-
ing natural language queries. American Journal of
Computational Linguistics, 8(3-4):110–122.
Terry Winograd. 1972. Understanding Natural Lan-
guage. Academic Press, Orlando, FL.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of Human Lan-
guage Technology Conference / North American
Chapter of the Association for Computational Lin-
guistics Annual Meeting (HLT-NAACL-06), pages
439–446, New York City, NY.
William A. Woods. 1977. Lunar rocks in natural
English: Explorations in natural language question
answering. In Antonio Zampoli, editor, Linguistic
Structures Processing. Elsevier North-Holland, New
York.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence (AAAI-
96), pages 1050–1055, Portland, OR, August.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of 21st Conference on
Uncertainty in Artificial Intelligence (UAI-2005),
Edinburgh, Scotland, July.
</reference>
<page confidence="0.997987">
827
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.828927">
<title confidence="0.9971295">UWM: Applying an Existing Trainable Semantic Parser to Parse Robotic Spatial Commands</title>
<author confidence="0.998326">J Rohit</author>
<affiliation confidence="0.920113">University of Milwaukee,</affiliation>
<email confidence="0.99991">katerj@uwm.edu</email>
<abstract confidence="0.999644263157895">This paper describes Team UWM’s system for the Task 6 of SemEval 2014 for doing supervised semantic parsing of robotic spatial commands. An existing parser, was trained using the provided training data of natural language robotic spatial commands paired with their meaning representations in the formal robot command language. The entire process required very little manual effort. Without using the additional annotations of word-aligned semantic trees, the trained parser was able to exactly parse new commands into their meaning reprewith best F-measure at precision and recall. Results show that the parser was particularly accurate for short sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kais Dukes</author>
</authors>
<title>Semantic annotation of robotic spatial commands.</title>
<date>2013</date>
<booktitle>In Proceedings of the Language and Technology Conference (LTC-2013),</booktitle>
<location>Poznan, Poland.</location>
<contexts>
<context position="2785" citStr="Dukes, 2013" startWordPosition="428" endWordPosition="429">mantic parser is automatically learned from the training data (Mooney, 2007). The trained semantic parser is then capable of parsing new natural language utterances into their meaning representations. Semantic parsers built using machine learning tend to be more robust and can be easily ported to other application domains with appropriate domain-specific training data. The Task 6 of SemEval 2014 provided a new application domain for semantic parsing along with training and test data. The domain involved giving natural language commands to a robotic arm which would then move blocks on a board (Dukes, 2013). The domain was inspired from the classic AI system SHRDLU (Winograd, 1972). The training data contained 2500 examples of sentences paired with their meaning representations in the Robot Command Language (RCL) which was designed for this domain (Dukes, 2013). The test data contained 909 such example pairs. We trained an existing and freely available1 semantic parser KRISP (Kate and Mooney, 2006) using the training data for this domain. Besides changing the format of the data for running KRISP and writing a context-free grammar for the meaning representation language RCL, the entire process re</context>
</contexts>
<marker>Dukes, 2013</marker>
<rawString>Kais Dukes. 2013. Semantic annotation of robotic spatial commands. In Proceedings of the Language and Technology Conference (LTC-2013), Poznan, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005),</booktitle>
<pages>9--16</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="1884" citStr="Ge and Mooney, 2005" startWordPosition="275" endWordPosition="278">applications (Woods, 1977; Warren and Pereira, 1982). However, such semantic parsers were generally brittle and building them required a lot of manual effort. In addition, these parsers could not be ported to any other application without again putting significant manual effort. More recently, several semantic parsers have been developed using machine learning (Zelle and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Mooney, 1996; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2011). In this approach, training data is first created for the domain of interest. Then using one of the many machine learning methods and semantic parsing frameworks, a semantic parser is automatically learned from the training data (Mooney, 2007). The trained semantic parser is then capable of parsing new natural language utterances into their meaning representations. Semantic parsers built using machine learning tend to be more robust and can be easily ported to other applica</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages 9–16, Ann Arbor, MI, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using string-kernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<pages>913--920</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1961" citStr="Kate and Mooney, 2006" startWordPosition="288" endWordPosition="291">c parsers were generally brittle and building them required a lot of manual effort. In addition, these parsers could not be ported to any other application without again putting significant manual effort. More recently, several semantic parsers have been developed using machine learning (Zelle and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Mooney, 1996; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2011). In this approach, training data is first created for the domain of interest. Then using one of the many machine learning methods and semantic parsing frameworks, a semantic parser is automatically learned from the training data (Mooney, 2007). The trained semantic parser is then capable of parsing new natural language utterances into their meaning representations. Semantic parsers built using machine learning tend to be more robust and can be easily ported to other application domains with appropriate domain-specific training data. The Task 6 of Se</context>
<context position="3184" citStr="Kate and Mooney, 2006" startWordPosition="490" endWordPosition="493">Eval 2014 provided a new application domain for semantic parsing along with training and test data. The domain involved giving natural language commands to a robotic arm which would then move blocks on a board (Dukes, 2013). The domain was inspired from the classic AI system SHRDLU (Winograd, 1972). The training data contained 2500 examples of sentences paired with their meaning representations in the Robot Command Language (RCL) which was designed for this domain (Dukes, 2013). The test data contained 909 such example pairs. We trained an existing and freely available1 semantic parser KRISP (Kate and Mooney, 2006) using the training data for this domain. Besides changing the format of the data for running KRISP and writing a context-free grammar for the meaning representation language RCL, the entire process required minimal manual effort. The author spent less than a week’s time for participating in the Task 6, and most of it was spent in running the experiments. This demonstrates that trainable semantic parsers like KRISP can be rapidly adopted to new domains. In the Results section we show different precisions and recalls it ob1http://www.cs.utexas.edu/users/ml/krisp/ 823 Proceedings of the 8th Inte</context>
<context position="4528" citStr="Kate and Mooney, 2006" startWordPosition="696" endWordPosition="699">fferent confidence levels in the form of a precision-recall curve. The results also show that the parser was particularly accurate on shorter sentences. Two major reasons that prevented KRISP from performing better on this domain were - its high computational demand for memory which prevented it from being trained beyond 1500 training examples, and some variability in the meaning representation language RCL that negatively affected training as well as evaluation. 2 Background: KRISP Semantic Parser KRISP (Kernel-based Robust Interpretation for Semantic Parsing) is a trainable semantic parser (Kate and Mooney, 2006) that uses Support Vector Machines (SVMs) (Cristianini and Shawe-Taylor, 2000) as the machine learning method with stringsubsequence kernel (Lodhi et al., 2002). It takes natural language utterances and their corresponding formal meaning representation as the training data along with the context-free grammar of the meaning representation language (MRL). The key idea in KRISP is that every production of the MRL is treated as a semantic concept. For every MRL production, an SVM classifier is trained so that it can give for any input natural language substring of words the probability that it exp</context>
<context position="5835" citStr="Kate and Mooney, 2006" startWordPosition="903" endWordPosition="906">g a sentence reduces to finding the most probable semantic derivation of the sentence in which different productions cover different parts of the sentence and together form a complete meaning representation. Figure 1 shows an example semantic derivation of a robotic spatial command. Productions of RCL grammar (Table 1) are shown at tree nodes depicting different parts of the sentence they cover. Since the training data is not in the form of such semantic derivations, an EM-like iterative algorithm is used to collect appropriate positive and negative examples in order to train the classifiers (Kate and Mooney, 2006). Positive examples are collected from correct semantic derivations derived by the parser learned in the previous iteration, and negative examples are collected from the incorrect semantic derivations. KRISP was shown to work well on the US geography database query domain (Tang and Mooney, 2001) as well as on the RoboCup Coach Language (CLang) domain (Kate et al., 2005). It was also shown to be particularly robust to noise in Figure 1: Semantic derivation of the robotic spatial command “pick up the turquoise pyramid” obtained by KRISP during testing which gives the correct RCL representation (</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-06), pages 913–920, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning language semantics from ambiguous supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twenty-Second Conference on Artificial Intelligence (AAAI-07),</booktitle>
<pages>895--900</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="6639" citStr="Kate and Mooney, 2007" startWordPosition="1032" endWordPosition="1035">antic derivations. KRISP was shown to work well on the US geography database query domain (Tang and Mooney, 2001) as well as on the RoboCup Coach Language (CLang) domain (Kate et al., 2005). It was also shown to be particularly robust to noise in Figure 1: Semantic derivation of the robotic spatial command “pick up the turquoise pyramid” obtained by KRISP during testing which gives the correct RCL representation (event: (action: take) (entity: (color: cyan) (type: prism))). the natural language utterances (Kate and Mooney, 2006). KRISP was later extended to do semisupervised semantic parsing (Kate and Mooney, 2007b), to learn from ambiguous supervision in which multiple sentences could be paired with a single meaning representation in the training data (Kate and Mooney, 2007a), and to transform the MRL grammar to improve semantic parsing (Kate, 2008). 3 Methods In order to apply KRISP to the Task 6 of SemEval 2014, the format of the provided data was first changed to the XML-type format that KRISP accepts. The data contained several instances of co-references which was also part of RCL, but KRISP was not designed to handle co-references and expects them to be pre-resolved. We observed that almost all c</context>
</contexts>
<marker>Kate, Mooney, 2007</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2007a. Learning language semantics from ambiguous supervision. In Proceedings of the Twenty-Second Conference on Artificial Intelligence (AAAI-07), pages 895–900, Vancouver, Canada, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Semi-supervised learning for semantic parsing using support vector machines.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT-07),</booktitle>
<pages>81--84</pages>
<location>Rochester, NY,</location>
<contexts>
<context position="6639" citStr="Kate and Mooney, 2007" startWordPosition="1032" endWordPosition="1035">antic derivations. KRISP was shown to work well on the US geography database query domain (Tang and Mooney, 2001) as well as on the RoboCup Coach Language (CLang) domain (Kate et al., 2005). It was also shown to be particularly robust to noise in Figure 1: Semantic derivation of the robotic spatial command “pick up the turquoise pyramid” obtained by KRISP during testing which gives the correct RCL representation (event: (action: take) (entity: (color: cyan) (type: prism))). the natural language utterances (Kate and Mooney, 2006). KRISP was later extended to do semisupervised semantic parsing (Kate and Mooney, 2007b), to learn from ambiguous supervision in which multiple sentences could be paired with a single meaning representation in the training data (Kate and Mooney, 2007a), and to transform the MRL grammar to improve semantic parsing (Kate, 2008). 3 Methods In order to apply KRISP to the Task 6 of SemEval 2014, the format of the provided data was first changed to the XML-type format that KRISP accepts. The data contained several instances of co-references which was also part of RCL, but KRISP was not designed to handle co-references and expects them to be pre-resolved. We observed that almost all c</context>
</contexts>
<marker>Kate, Mooney, 2007</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2007b. Semi-supervised learning for semantic parsing using support vector machines. In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT-07), pages 81–84, Rochester, NY, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to transform natural to formal languages.</title>
<date>2005</date>
<booktitle>In Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI05),</booktitle>
<pages>1062--1068</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="6207" citStr="Kate et al., 2005" startWordPosition="965" endWordPosition="968">e sentence they cover. Since the training data is not in the form of such semantic derivations, an EM-like iterative algorithm is used to collect appropriate positive and negative examples in order to train the classifiers (Kate and Mooney, 2006). Positive examples are collected from correct semantic derivations derived by the parser learned in the previous iteration, and negative examples are collected from the incorrect semantic derivations. KRISP was shown to work well on the US geography database query domain (Tang and Mooney, 2001) as well as on the RoboCup Coach Language (CLang) domain (Kate et al., 2005). It was also shown to be particularly robust to noise in Figure 1: Semantic derivation of the robotic spatial command “pick up the turquoise pyramid” obtained by KRISP during testing which gives the correct RCL representation (event: (action: take) (entity: (color: cyan) (type: prism))). the natural language utterances (Kate and Mooney, 2006). KRISP was later extended to do semisupervised semantic parsing (Kate and Mooney, 2007b), to learn from ambiguous supervision in which multiple sentences could be paired with a single meaning representation in the training data (Kate and Mooney, 2007a), </context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005. Learning to transform natural to formal languages. In Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI05), pages 1062–1068, Pittsburgh, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
</authors>
<title>Transforming meaning representation grammars to improve semantic parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning (CoNLL-2008),</booktitle>
<pages>33--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6880" citStr="Kate, 2008" startWordPosition="1071" endWordPosition="1072">re 1: Semantic derivation of the robotic spatial command “pick up the turquoise pyramid” obtained by KRISP during testing which gives the correct RCL representation (event: (action: take) (entity: (color: cyan) (type: prism))). the natural language utterances (Kate and Mooney, 2006). KRISP was later extended to do semisupervised semantic parsing (Kate and Mooney, 2007b), to learn from ambiguous supervision in which multiple sentences could be paired with a single meaning representation in the training data (Kate and Mooney, 2007a), and to transform the MRL grammar to improve semantic parsing (Kate, 2008). 3 Methods In order to apply KRISP to the Task 6 of SemEval 2014, the format of the provided data was first changed to the XML-type format that KRISP accepts. The data contained several instances of co-references which was also part of RCL, but KRISP was not designed to handle co-references and expects them to be pre-resolved. We observed that almost all co-references in the meaning representations, indicated by “reference-id” token, resolved to the first occurrence of an “entity” element in the meaning representation. This was found to be true for more than 99% of the cases. We used this obs</context>
<context position="8413" citStr="Kate, 2008" startWordPosition="1324" endWordPosition="1325"> modified in any way and the parser learns from the training data to relate words like “it” and “one” to the RCL token “reference-id”. After KRISP generates a meaning representation during testing, as a post-processing step, “id: 1” is added to the first “entity” element in the meaning representation if it contains the “reference-id:” token. The context-free grammar for RCL was not provided by the Task organizers. There are multi824 ple ways to write a context-free grammar for a meaning representation language and those that conform better to natural language work better for semantic parsing (Kate, 2008). We manually wrote grammar for RCL which mostly followed the structure of the meaning representations as they already conformed highly to natural language commands and hence writing the grammar was straightforward. KRISP runs faster if there are fewer non-terminals on the right-handside (RHS) of the grammar because that makes the search for the most probable semantic derivation faster. Hence we kept non-terminals on RHS as few as possible while writing the grammar. Table 1 shows the entire grammar for RCL that we wrote which was given to KRISP. The nonterminals are indicated with a “*” in the</context>
</contexts>
<marker>Kate, 2008</marker>
<rawString>Rohit J. Kate. 2008. Transforming meaning representation grammars to improve semantic parsing. In Proceedings of the Twelfth Conference on Computational Natural Language Learning (CoNLL-2008), pages 33–40. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Lexical generalization in CCG grammar induction for semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP2011),</booktitle>
<pages>1512--1523</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2005" citStr="Kwiatkowski et al., 2011" startWordPosition="296" endWordPosition="299">lding them required a lot of manual effort. In addition, these parsers could not be ported to any other application without again putting significant manual effort. More recently, several semantic parsers have been developed using machine learning (Zelle and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Mooney, 1996; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2011). In this approach, training data is first created for the domain of interest. Then using one of the many machine learning methods and semantic parsing frameworks, a semantic parser is automatically learned from the training data (Mooney, 2007). The trained semantic parser is then capable of parsing new natural language utterances into their meaning representations. Semantic parsers built using machine learning tend to be more robust and can be easily ported to other application domains with appropriate domain-specific training data. The Task 6 of SemEval 2014 provided a new application domain</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011. Lexical generalization in CCG grammar induction for semantic parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP2011), pages 1512–1523. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<pages>2--419</pages>
<contexts>
<context position="4688" citStr="Lodhi et al., 2002" startWordPosition="719" endWordPosition="722">reasons that prevented KRISP from performing better on this domain were - its high computational demand for memory which prevented it from being trained beyond 1500 training examples, and some variability in the meaning representation language RCL that negatively affected training as well as evaluation. 2 Background: KRISP Semantic Parser KRISP (Kernel-based Robust Interpretation for Semantic Parsing) is a trainable semantic parser (Kate and Mooney, 2006) that uses Support Vector Machines (SVMs) (Cristianini and Shawe-Taylor, 2000) as the machine learning method with stringsubsequence kernel (Lodhi et al., 2002). It takes natural language utterances and their corresponding formal meaning representation as the training data along with the context-free grammar of the meaning representation language (MRL). The key idea in KRISP is that every production of the MRL is treated as a semantic concept. For every MRL production, an SVM classifier is trained so that it can give for any input natural language substring of words the probability that it expresses the corresponding semantic concept. Once these classifiers are trained, parsing a sentence reduces to finding the most probable semantic derivation of th</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing.</title>
<date>2007</date>
<booktitle>Computational Linguistics and Intelligent Text Processing: Proceedings of the 8th International Conference (CICLing2007), Mexico City,</booktitle>
<pages>311--324</pages>
<editor>In A. Gelbukh, editor,</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="2249" citStr="Mooney, 2007" startWordPosition="339" endWordPosition="340">nd This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Mooney, 1996; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2011). In this approach, training data is first created for the domain of interest. Then using one of the many machine learning methods and semantic parsing frameworks, a semantic parser is automatically learned from the training data (Mooney, 2007). The trained semantic parser is then capable of parsing new natural language utterances into their meaning representations. Semantic parsers built using machine learning tend to be more robust and can be easily ported to other application domains with appropriate domain-specific training data. The Task 6 of SemEval 2014 provided a new application domain for semantic parsing along with training and test data. The domain involved giving natural language commands to a robotic arm which would then move blocks on a board (Dukes, 2013). The domain was inspired from the classic AI system SHRDLU (Win</context>
<context position="6639" citStr="Mooney, 2007" startWordPosition="1034" endWordPosition="1035">ivations. KRISP was shown to work well on the US geography database query domain (Tang and Mooney, 2001) as well as on the RoboCup Coach Language (CLang) domain (Kate et al., 2005). It was also shown to be particularly robust to noise in Figure 1: Semantic derivation of the robotic spatial command “pick up the turquoise pyramid” obtained by KRISP during testing which gives the correct RCL representation (event: (action: take) (entity: (color: cyan) (type: prism))). the natural language utterances (Kate and Mooney, 2006). KRISP was later extended to do semisupervised semantic parsing (Kate and Mooney, 2007b), to learn from ambiguous supervision in which multiple sentences could be paired with a single meaning representation in the training data (Kate and Mooney, 2007a), and to transform the MRL grammar to improve semantic parsing (Kate, 2008). 3 Methods In order to apply KRISP to the Task 6 of SemEval 2014, the format of the provided data was first changed to the XML-type format that KRISP accepts. The data contained several instances of co-references which was also part of RCL, but KRISP was not designed to handle co-references and expects them to be pre-resolved. We observed that almost all c</context>
</contexts>
<marker>Mooney, 2007</marker>
<rawString>Raymond J. Mooney. 2007. Learning for semantic parsing. In A. Gelbukh, editor, Computational Linguistics and Intelligent Text Processing: Proceedings of the 8th International Conference (CICLing2007), Mexico City, pages 311–324. Springer Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lappoon R Tang</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using multiple clause constructors in inductive logic programming for semantic parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning (ECML-2001),</booktitle>
<pages>466--477</pages>
<location>Freiburg, Germany.</location>
<contexts>
<context position="6131" citStr="Tang and Mooney, 2001" startWordPosition="950" endWordPosition="953">of RCL grammar (Table 1) are shown at tree nodes depicting different parts of the sentence they cover. Since the training data is not in the form of such semantic derivations, an EM-like iterative algorithm is used to collect appropriate positive and negative examples in order to train the classifiers (Kate and Mooney, 2006). Positive examples are collected from correct semantic derivations derived by the parser learned in the previous iteration, and negative examples are collected from the incorrect semantic derivations. KRISP was shown to work well on the US geography database query domain (Tang and Mooney, 2001) as well as on the RoboCup Coach Language (CLang) domain (Kate et al., 2005). It was also shown to be particularly robust to noise in Figure 1: Semantic derivation of the robotic spatial command “pick up the turquoise pyramid” obtained by KRISP during testing which gives the correct RCL representation (event: (action: take) (entity: (color: cyan) (type: prism))). the natural language utterances (Kate and Mooney, 2006). KRISP was later extended to do semisupervised semantic parsing (Kate and Mooney, 2007b), to learn from ambiguous supervision in which multiple sentences could be paired with a s</context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>Lappoon R. Tang and Raymond J. Mooney. 2001. Using multiple clause constructors in inductive logic programming for semantic parsing. In Proceedings of the 12th European Conference on Machine Learning (ECML-2001), pages 466–477, Freiburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H D Warren</author>
<author>Fernando C N Pereira</author>
</authors>
<title>An efficient easily adaptable system for interpreting natural language queries.</title>
<date>1982</date>
<journal>American Journal of Computational Linguistics,</journal>
<pages>8--3</pages>
<contexts>
<context position="1317" citStr="Warren and Pereira, 1982" startWordPosition="193" endWordPosition="196">commands into their meaning representations with 51.18% best F-measure at 72.67% precision and 39.49% recall. Results show that the parser was particularly accurate for short sentences. 1 Introduction Semantic parsing is the task of converting natural language utterances into their complete formal meaning representations which are executable for some application. Example applications of semantic parsing include giving natural language commands to robots and querying databases in natural language. Some old semantic parsers were developed manually to work for specific applications (Woods, 1977; Warren and Pereira, 1982). However, such semantic parsers were generally brittle and building them required a lot of manual effort. In addition, these parsers could not be ported to any other application without again putting significant manual effort. More recently, several semantic parsers have been developed using machine learning (Zelle and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Mooney, 1996; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; </context>
</contexts>
<marker>Warren, Pereira, 1982</marker>
<rawString>David H. D. Warren and Fernando C. N. Pereira. 1982. An efficient easily adaptable system for interpreting natural language queries. American Journal of Computational Linguistics, 8(3-4):110–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>Academic Press,</publisher>
<location>Orlando, FL.</location>
<contexts>
<context position="2861" citStr="Winograd, 1972" startWordPosition="440" endWordPosition="441">07). The trained semantic parser is then capable of parsing new natural language utterances into their meaning representations. Semantic parsers built using machine learning tend to be more robust and can be easily ported to other application domains with appropriate domain-specific training data. The Task 6 of SemEval 2014 provided a new application domain for semantic parsing along with training and test data. The domain involved giving natural language commands to a robotic arm which would then move blocks on a board (Dukes, 2013). The domain was inspired from the classic AI system SHRDLU (Winograd, 1972). The training data contained 2500 examples of sentences paired with their meaning representations in the Robot Command Language (RCL) which was designed for this domain (Dukes, 2013). The test data contained 909 such example pairs. We trained an existing and freely available1 semantic parser KRISP (Kate and Mooney, 2006) using the training data for this domain. Besides changing the format of the data for running KRISP and writing a context-free grammar for the meaning representation language RCL, the entire process required minimal manual effort. The author spent less than a week’s time for p</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Terry Winograd. 1972. Understanding Natural Language. Academic Press, Orlando, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of Human Language Technology Conference / North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL-06),</booktitle>
<pages>439--446</pages>
<location>New York City, NY.</location>
<contexts>
<context position="1938" citStr="Wong and Mooney, 2006" startWordPosition="284" endWordPosition="287">. However, such semantic parsers were generally brittle and building them required a lot of manual effort. In addition, these parsers could not be ported to any other application without again putting significant manual effort. More recently, several semantic parsers have been developed using machine learning (Zelle and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Mooney, 1996; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2011). In this approach, training data is first created for the domain of interest. Then using one of the many machine learning methods and semantic parsing frameworks, a semantic parser is automatically learned from the training data (Mooney, 2007). The trained semantic parser is then capable of parsing new natural language utterances into their meaning representations. Semantic parsers built using machine learning tend to be more robust and can be easily ported to other application domains with appropriate domain-specific training</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of Human Language Technology Conference / North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL-06), pages 439–446, New York City, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Woods</author>
</authors>
<title>Lunar rocks in natural English: Explorations in natural language question answering.</title>
<date>1977</date>
<booktitle>Linguistic Structures Processing.</booktitle>
<editor>In Antonio Zampoli, editor,</editor>
<publisher>Elsevier North-Holland,</publisher>
<location>New York.</location>
<contexts>
<context position="1290" citStr="Woods, 1977" startWordPosition="191" endWordPosition="192">ly parse new commands into their meaning representations with 51.18% best F-measure at 72.67% precision and 39.49% recall. Results show that the parser was particularly accurate for short sentences. 1 Introduction Semantic parsing is the task of converting natural language utterances into their complete formal meaning representations which are executable for some application. Example applications of semantic parsing include giving natural language commands to robots and querying databases in natural language. Some old semantic parsers were developed manually to work for specific applications (Woods, 1977; Warren and Pereira, 1982). However, such semantic parsers were generally brittle and building them required a lot of manual effort. In addition, these parsers could not be ported to any other application without again putting significant manual effort. More recently, several semantic parsers have been developed using machine learning (Zelle and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Mooney, 1996; Ge and Mooney, 2005; Zett</context>
</contexts>
<marker>Woods, 1977</marker>
<rawString>William A. Woods. 1977. Lunar rocks in natural English: Explorations in natural language question answering. In Antonio Zampoli, editor, Linguistic Structures Processing. Elsevier North-Holland, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI96),</booktitle>
<pages>1050--1055</pages>
<location>Portland, OR,</location>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI96), pages 1050–1055, Portland, OR, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of 21st Conference on Uncertainty in Artificial Intelligence (UAI-2005),</booktitle>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="1915" citStr="Zettlemoyer and Collins, 2005" startWordPosition="279" endWordPosition="283">1977; Warren and Pereira, 1982). However, such semantic parsers were generally brittle and building them required a lot of manual effort. In addition, these parsers could not be ported to any other application without again putting significant manual effort. More recently, several semantic parsers have been developed using machine learning (Zelle and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Mooney, 1996; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2011). In this approach, training data is first created for the domain of interest. Then using one of the many machine learning methods and semantic parsing frameworks, a semantic parser is automatically learned from the training data (Mooney, 2007). The trained semantic parser is then capable of parsing new natural language utterances into their meaning representations. Semantic parsers built using machine learning tend to be more robust and can be easily ported to other application domains with appropriate d</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of 21st Conference on Uncertainty in Artificial Intelligence (UAI-2005), Edinburgh, Scotland, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>