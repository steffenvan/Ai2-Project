<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.8562815">
Viterbi Training for PCFGs:
Hardness Results and Competitiveness of Uniform Initialization
</title>
<author confidence="0.883487">
Shay B. Cohen and Noah A. Smith
</author>
<affiliation confidence="0.885410333333333">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.999127">
{scohen,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.994806" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979461538462">
We consider the search for a maximum
likelihood assignment of hidden deriva-
tions and grammar weights for a proba-
bilistic context-free grammar, the problem
approximately solved by “Viterbi train-
ing.” We show that solving and even ap-
proximating Viterbi training for PCFGs is
NP-hard. We motivate the use of uniform-
at-random initialization for Viterbi EM as
an optimal initializer in absence of further
information about the correct model pa-
rameters, providing an approximate bound
on the log-likelihood.
</bodyText>
<sectionHeader confidence="0.99841" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99966885106383">
Probabilistic context-free grammars are an essen-
tial ingredient in many natural language process-
ing models (Charniak, 1997; Collins, 2003; John-
son et al., 2006; Cohen and Smith, 2009, inter
alia). Various algorithms for training such models
have been proposed, including unsupervised meth-
ods. Many of these are based on the expectation-
maximization (EM) algorithm.
There are alternatives to EM, and one such al-
ternative is Viterbi EM, also called “hard” EM or
“sparse” EM (Neal and Hinton, 1998). Instead
of using the parameters (which are maintained in
the algorithm’s current state) to find the true pos-
terior over the derivations, Viterbi EM algorithm
uses a posterior focused on the Viterbi parse of
those parameters. Viterbi EM and variants have
been used in various settings in natural language
processing (Yejin and Cardie, 2007; Wang et al.,
2007; Goldwater and Johnson, 2005; DeNero and
Klein, 2008; Spitkovsky et al., 2010).
Viterbi EM can be understood as a coordinate
ascent procedure that locally optimizes a function;
we call this optimization goal “Viterbi training.”
In this paper, we explore Viterbi training for
probabilistic context-free grammars. We first
show that under the assumption that P =� NP, solv-
ing and even approximating the Viterbi training
problem is hard. This result holds even for hid-
den Markov models. We extend the main hardness
result to the EM algorithm (giving an alternative
proof to this known result), as well as the problem
of conditional Viterbi training. We then describe
a “competitiveness” result for uniform initializa-
tion of Viterbi EM: we show that initialization of
the trees in an E-step which uses uniform distri-
butions over the trees is optimal with respect to a
certain approximate bound.
The rest of this paper is organized as follows. §2
gives background on PCFGs and introduces some
notation. §3 explains Viterbi training, the declar-
ative form of Viterbi EM. §4 describes a hardness
result for Viterbi training. §5 extends this result to
a hardness result of approximation and §6 further
extends these results for other cases. §7 describes
the advantages in using uniform-at-random initial-
ization for Viterbi training. We relate these results
to work on the k-means problem in §8.
</bodyText>
<sectionHeader confidence="0.862398" genericHeader="introduction">
2 Background and Notation
</sectionHeader>
<bodyText confidence="0.999648">
We assume familiarity with probabilistic context-
free grammars (PCFGs). A PCFG G consists of:
</bodyText>
<listItem confidence="0.993905625">
• A finite set of nonterminal symbols N;
• A finite set of terminal symbols E;
• For each A E N, a set of rewrite rules R(A) of
the form A —* α, where α E (N U E)*, and
9Z = UAENR(A);
• For each rule A —* α, a probability BA→α. The
collection of probabilities is denoted 0, and they
are constrained such that:
</listItem>
<equation confidence="0.981741666666667">
b(A —* α) E R(A), BA→α &gt; 0
bA E N, E BA→α = 1
α:(A→α)ER(A)
</equation>
<bodyText confidence="0.820448">
That is, 0 is grouped into |N |multinomial dis-
tributions.
</bodyText>
<page confidence="0.966209">
1502
</page>
<note confidence="0.943582">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1502–1511,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.958002">
Under the PCFG, the joint probability of a string
x E E* and a grammatical derivation z is1
</bodyText>
<equation confidence="0.970591">
p(x, z  |θ) = Y (0A,α)&apos;A__(z) (1)
(A,α)ET,
X=exp fA,α(z) log 9A,α
(A,α)ET,
</equation>
<bodyText confidence="0.994171">
where fA,α(z) is a function that “counts” the
number of times the rule A —* α appears in
the derivation z. fA(z) will similarly denote the
number of times that nonterminal A appears in z.
Given a sample of derivations z = (z1, ... , zn),
let:
</bodyText>
<equation confidence="0.9994565">
FA,α(z) = Xn fA,α(zi) (2)
i=1
FA(z) = Xn fA(zi) (3)
i=1
</equation>
<bodyText confidence="0.998436">
We use the following notation for G:
</bodyText>
<listItem confidence="0.993335625">
• L(G) is the set of all strings (sentences) x that
can be generated using the grammar G (the
“language of G”).
• D(G) is the set of all possible derivations z that
can be generated using the grammar G.
• D(G, x) is the set of all possible derivations z
that can be generated using the grammar G and
have the yield x.
</listItem>
<sectionHeader confidence="0.9756" genericHeader="method">
3 Viterbi Training
</sectionHeader>
<bodyText confidence="0.999500875">
Viterbi EM, or “hard” EM, is an unsupervised
learning algorithm, used in NLP in various set-
tings (Yejin and Cardie, 2007; Wang et al., 2007;
Goldwater and Johnson, 2005; DeNero and Klein,
2008; Spitkovsky et al., 2010). In the context of
PCFGs, it aims to select parameters θ and phrase-
structure trees z jointly. It does so by iteratively
updating a state consisting of (θ, z). The state
is initialized with some value, then the algorithm
alternates between (i) a “hard” E-step, where the
strings x1, ... , xn are parsed according to a cur-
rent, fixed θ, giving new values for z, and (ii) an
M-step, where the θ are selected to maximize like-
lihood, with z fixed.
With PCFGs, the E-step requires running an al-
gorithm such as (probabilistic) CKY or Earley’s
</bodyText>
<footnote confidence="0.522317333333333">
1Note that x = yield(z); if the derivation is known, the
string is also known. On the other hand, there may be many
derivations with the same yield, perhaps even infinitely many.
</footnote>
<bodyText confidence="0.999847833333333">
algorithm, while the M-step normalizes frequency
counts FA,α(z) to obtain the maximum likeli-
hood estimate’s closed-form solution.
We can understand Viterbi EM as a coordinate
ascent procedure that approximates the solution to
the following declarative problem:
</bodyText>
<equation confidence="0.382554833333333">
Problem 1. ViterbiTrain
Input: G context-free grammar, x1, ... , xn train-
ing instances from L(G)
Output: θ and z1, ... , zn such that
(θ, z1, ... , zn) = argmax
B,z
</equation>
<bodyText confidence="0.9998588">
The optimization problem in Eq. 4 is non-
convex and, as we will show in §4, hard to op-
timize. Therefore it is necessary to resort to ap-
proximate algorithms like Viterbi EM.
Neal and Hinton (1998) use the term “sparse
EM” to refer to a version of the EM algorithm
where the E-step finds the modes of hidden vari-
ables (rather than marginals as in standard EM).
Viterbi EM is a variant of this, where the E-
step finds the mode for each xi’s derivation,
</bodyText>
<equation confidence="0.83490875">
argmaxzED(G,xi) p(xi, z  |θ).
We will refer to
,G(θ, z) = Yn p(xi, zi  |θ) (5)
i=1
</equation>
<bodyText confidence="0.999933333333333">
as “the objective function of ViterbiTrain.”
Viterbi training and Viterbi EM are closely re-
lated to self-training, an important concept in
semi-supervised NLP (Charniak, 1997; McClosky
et al., 2006a; McClosky et al., 2006b). With self-
training, the model is learned with some seed an-
notated data, and then iterates by labeling new,
unannotated data and adding it to the original an-
notated training set. McClosky et al. consider self-
training to be “one round of Viterbi EM” with su-
pervised initialization using labeled seed data. We
refer the reader to Abney (2007) for more details.
</bodyText>
<sectionHeader confidence="0.804652" genericHeader="method">
4 Hardness of Viterbi Training
</sectionHeader>
<bodyText confidence="0.999227">
We now describe hardness results for Problem 1.
We first note that the following problem is known
to be NP-hard, and in fact, NP-complete (Sipser,
2006):
</bodyText>
<equation confidence="0.949806333333333">
Problem 2. 3-SAT
Input: A formula 0 = Vm1 (ai V bi V ci) in con-
junctive normal form, such that each clause has 3
Yn p(xi, zi  |θ) (4)
i=1
1503
Sφ2
Sφ1
A1 A2
������������������� ������������������� ������������������� �������������������
UY1,0
������� �������
UY2,1
������� �������
UY4,0
������� �������
UY1,0
������� �������
UY2,1
������� �������
UY3,1
������� �������
V Y1 VY1 VY2 V Y2 V v4 VY4 V Y1 VY1 VY2 V V2 VY3 V Y3
1 0 1 0 1 0 1 0 1 0 1 0
</equation>
<figureCaption confidence="0.9315485">
Figure 1: An example of a Viterbi parse tree which represents a satisfying assignment for 0 = (Y1 V Y2 V 174) n (�Y1 V 172 V Y3).
In 0φ, all rules appearing in the parse tree have probability 1. The extracted assignment would be Y1 = 0, Y2 = 1, Y3 =
</figureCaption>
<equation confidence="0.732523">
1, Y4 = 0. Note that there is no usage of two different rules for a single nonterminal.
literals.
</equation>
<bodyText confidence="0.9949296875">
Output: 1 if there is a satisfying assignment for φ
and 0 otherwise.
We now describe a reduction of 3-SAT to Prob-
lem 1. Given an instance of the 3-SAT problem,
the reduction will, in polynomial time, create a
grammar and a single string such that solving the
ViterbiTrain problem for this grammar and string
will yield a solution for the instance of the 3-SAT
problem.
Let φ = Ami_1 (ai ∨ bi ∨ ci) be an instance of
the 3-SAT problem, where ai, bi and ci are liter-
als over the set of variables {Y1, ... , YN} (a literal
refers to a variable Yj or its negation, �Yj). Let Cj
be the jth clause in φ, such that Cj = aj ∨ bj ∨ cj.
We define the following context-free grammar Gφ
and string to parse sφ:
</bodyText>
<listItem confidence="0.9957305625">
1. The terminals of Gφ are the binary digits E =
{0,1}.
2. We create N nonterminals VYr, r ∈
{1, ... , N} and rules VYr → 0 and VYr → 1.
3. We create N nonterminals V Yr, r ∈
{1, . . . ,N} and rules V Yr → 0 and V Yr → 1.
4. We create UYr,1 → VYrV Yr and UYr,0 →
V YrVYr.
5. We create the rule Sφ1 → A1. For each j ∈
{2,.. . , m}, we create a rule Sφj → Sφj_1Aj
where Sφj is a new nonterminal indexed by
φj Aji_1 Ci and Aj is also a new nonterminal
indexed by j ∈ {1, . . . , m}.
6. Let Cj = aj ∨ bj ∨ cj be clause j in φ. Let
Y (aj) be the variable that aj mentions. Let
(y1, y2, y3) be a satisfying assignment for Cj
</listItem>
<bodyText confidence="0.99607825">
where yk ∈ {0,1} and is the value of Y (aj),
Y (bj) and Y (cj) respectively for k ∈ {1, 2, 3}.
For each such clause-satisfying assignment, we
add the rule:
</bodyText>
<equation confidence="0.906658">
Aj → UY (aj),y1UY (bj),y2UY (cj),y3 (6)
</equation>
<bodyText confidence="0.952169071428572">
For each Aj, we would have at most 7 rules of
that form, since one rule will be logically incon-
sistent with aj ∨ bj ∨ cj.
7. The grammar’s start symbol is Sφn.
8. The string to parse is sφ = (10)3m, i.e. 3m
consecutive occurrences of the string 10.
A parse of the string sφ using Gφ will be used
to get an assignment by setting Yr = 0 if the rule
VYr → 0 or V Yr → 1 are used in the derivation of
the parse tree, and 1 otherwise. Notice that at this
point we do not exclude “contradictions” coming
from the parse tree, such as VY3 → 0 used in the
tree together with VY3 → 1 or V Y3 → 0. The fol-
lowing lemma gives a condition under which the
assignment is consistent (so contradictions do not
occur in the parse tree):
Lemma 1. Let φ be an instance of the 3-SAT
problem, and let Gφ be a probabilistic CFG based
on the above grammar with weights 0φ. If the
(multiplicative) weight of the Viterbi parse of sφ
is 1, then the assignment extracted from the parse
tree is consistent.
Proof. Since the probability of the Viterbi parse
is 1, all rules of the form {VYr, V Yr} → {0, 1}
which appear in the parse tree have probability 1
as well. There are two possible types of inconsis-
tencies. We show that neither exists in the Viterbi
parse:
</bodyText>
<page confidence="0.971264">
1504
</page>
<listItem confidence="0.9909642">
1. For any r, an appearance of both rules of the
form VYr —* 0 and VYr —* 1 cannot occur be-
cause all rules that appear in the Viterbi parse
tree have probability 1.
2. For any r, an appearance of rules of the form
</listItem>
<bodyText confidence="0.855981636363636">
VYr —* 1 and V Yr —* 1 cannot occur, because
whenever we have an appearance of the rule
VYr —* 0, we have an adjacent appearance of
the rule V Yr —* 1 (because we parse substrings
of the form 10), and then again we use the fact
that all rules in the parse tree have probability 1.
The case of VYr —* 0 and V Yr —* 0 is handled
analogously.
Thus, both possible inconsistencies are ruled out,
resulting in a consistent assignment.
Figure 1 gives an example of an application of
the reduction.
Lemma 2. Define 0, Gφ as before. There exists
0φ such that the Viterbi parse of sφ is 1 if and only
if 0 is satisfiable. Moreover, the satisfying assign-
ment is the one extracted from the parse tree with
weight 1 of sφ under 0φ.
Proof. (==�-) Assume that there is a satisfying as-
signment. Each clause Cj = aj V bj V cj is satis-
fied using a tuple (y1, y2, y3) which assigns value
for Y (aj), Y (bj) and Y (cj). This assignment cor-
responds the following rule
</bodyText>
<equation confidence="0.93546">
Aj —* UY (aj),y1UY (bj),y2UY (cj),y3 (7)
</equation>
<bodyText confidence="0.96860625">
Set its probability to 1, and set all other rules of
Aj to 0. In addition, for each r, if Yr = y, set the
probabilities of the rules VYr —* y and V Yr —* 1—y
to 1 and V Yr —* y and VYr —* 1 — y to 0. The rest
of the weights for Sφj —* Sφj_1Aj are set to 1.
This assignment of rule probabilities results in a
Viterbi parse of weight 1.
(�) Assume that the Viterbi parse has prob-
ability 1. From Lemma 1, we know that we can
extract a consistent assignment from the Viterbi
parse. In addition, for each clause Cj we have a
rule
</bodyText>
<equation confidence="0.947778">
Aj —* UY (aj),y1UY (bj),y2UY (cj),y3 (8)
</equation>
<bodyText confidence="0.970962222222222">
that is assigned probability 1, for some
(y1, y2, y3). One can verify that (y1, y2, y3)
are the values of the assignment for the corre-
sponding variables in clause Cj, and that they
satisfy this clause. This means that each clause is
satisfied by the assignment we extracted.
In order to show an NP-hardness result, we need
to “convert” ViterbiTrain to a decision problem.
The natural way to do it, following Lemmas 1
and 2, is to state the decision problem for Viter-
biTrain as “given G and x1, ... , xn and a &gt; 0,
is the optimized value of the objective function
Z(0, z) &gt; a?” and use a = 1 together with Lem-
mas 1 and 2. (Naturally, an algorithm for solving
ViterbiTrain can easily be used to solve its deci-
sion problem.)
Theorem 3. The decision version of the Viterbi-
Train problem is NP-hard.
</bodyText>
<sectionHeader confidence="0.591641" genericHeader="method">
5 Hardness of Approximation
</sectionHeader>
<bodyText confidence="0.973722848484848">
A natural path of exploration following the hard-
ness result we showed is determining whether an
approximation of ViterbiTrain is also hard. Per-
haps there is an efficient approximation algorithm
for ViterbiTrain we could use instead of coordi-
nate ascent algorithms such as Viterbi EM. Recall
that such algorithms’ main guarantee is identify-
ing a local maximum; we know nothing about how
far it will be from the global maximum.
We next show that approximating the objective
function of ViterbiTrain with a constant factor of p
is hard for any p E (12, 11 (i.e., 1/2 + c approxima-
tion is hard for any c &lt; 1/2). This means that, un-
der the P =� NP assumption, there is no efficient al-
gorithm that, given a grammar G and a sample of
sentences x1, ... , xn, returns 0&apos; and z&apos; such that:
Z(0&apos;, z&apos;) &gt; p · max
B,z
We will continue to use the same reduction from
§4. Let sφ be the string from that reduction, and
let (0, z) be the optimal solution for ViterbiTrain
given Gφ and sφ. We first note that if p(sφ, z |
0) &lt; 1 (implying that there is no satisfying as-
signment), then there must be a nonterminal which
appears along with two different rules in z.
This means that we have a nonterminal B E N
with some rule B —* a that appears k times,
while the nonterminal appears in the parse r &gt;
k + 1 times. Given the tree z, the 0 that maxi-
mizes the objective function is the maximum like-
lihood estimate (MLE) for z (counting and nor-
malizing the rules).2 We therefore know that
the ViterbiTrain objective function, Z(0, z), is at
</bodyText>
<footnote confidence="0.899335">
2Note that we can only make p(z 10, x) greater by using
9 to be the MLE for the derivation z.
</footnote>
<equation confidence="0.981587">
n
p(xi, zi  |0) (9)
i=1
1505
(k )k
, because it includes a factor equal
r
to ( fB(z) ) , where fB (z) is the num-
ber of times nonterminal B appears in z (hence
fB(z) = r) and fB—,α(z) is the number of times
B —* α appears in z (hence fB—,α(z) = k). For
any k &gt; 1, r &gt; k + 1:
(rk ) k
- (k+ 1)k 2 (10)
</equation>
<bodyText confidence="0.9998248">
This means that if the value of the objective func-
tion of ViterbiTrain is not 1 using the reduction
from §4, then it is at most 1�. If we had an efficient
approximate algorithm with approximation coeffi-
cient ρ &gt; 21 (Eq. 9 holds), then in order to solve
3-SAT for formula φ, we could run the algorithm
on Gφ and sφ and check whether the assignment
to (0, z) that the algorithm returns satisfies φ or
not, and return our response accordingly.
If φ were satisfiable, then the true maximal
value of Z would be 1, and the approximation al-
gorithm would return (0, z) such that Z(0, z) &gt;
ρ &gt; 1�. z would have to correspond to a satisfy-
ing assignment, and in fact p(z  |0) = 1, because
in any other case, the probability of a derivation
which does not represent a satisfying assignment
is smaller than 1�. If φ were not satisfiable, then
the approximation algorithm would never return a
(0, z) that results in a satisfying assignment (be-
cause such a (0, z) does not exist).
The conclusion is that an efficient algorithm for
approximating the objective function of Viterbi-
Train (Eq. 4) within a factor of 21 + E is unlikely
to exist. If there were such an algorithm, we could
use it to solve 3-SAT using the reduction from §4.
</bodyText>
<sectionHeader confidence="0.861006" genericHeader="method">
6 Extensions of the Hardness Result
</sectionHeader>
<bodyText confidence="0.999780333333333">
An alternative problem to Problem 1, a variant of
Viterbi-training, is the following (see, for exam-
ple, Klein and Manning, 2001):
</bodyText>
<subsubsectionHeader confidence="0.686009">
Problem 3. ConditionalViterbiTrain
</subsubsectionHeader>
<bodyText confidence="0.97978997826087">
Input: G context-free grammar, x1, ... , xn train-
ing instances from L(G)
Output: 0 and z1, ... , zn such that
(0,z1, ... , zn) = argmax
B,z
Here, instead of maximizing the likelihood, we
maximize the conditional likelihood. Note that
there is a hidden assumption in this problem def-
inition, that xi can be parsed using the grammar
G. Otherwise, the quantity p(zi  |0, xi) is not
well-defined. We can extend ConditionalViterbi-
Train to return + in the case of not having a parse
for one of the xi—this can be efficiently checked
using a run of a cubic-time parser on each of the
strings xi with the grammar G.
An approximate technique for this problem is
similar to Viterbi EM, only modifying the M-
step to maximize the conditional, rather than joint,
likelihood. This new M-step will not have a closed
form and may require auxiliary optimization tech-
niques like gradient ascent.
Our hardness result for ViterbiTrain applies to
ConditionalViterbiTrain as well. The reason is
that if p(z, sφ  |0φ) = 1 for a φ with a satisfying
assignment, then L(G) = {sφ} and D(G) = {z}.
This implies that p(z  |0φ, sφ) = 1. If φ is unsat-
isfiable, then for the optimal 0 of ViterbiTrain we
have z and z&apos; such that 0 &lt; p(z, sφ  |0φ) &lt; 1
and 0 &lt; p(z&apos;, sφ  |0φ) &lt; 1, and therefore p(z |
0φ, sφ) &lt; 1, which means the conditional objec-
tive function will not obtain the value 1. (Note
that there always exist some parameters 0φ that
generate sφ.) So, again, given an algorithm for
ConditionalViterbiTrain, we can discern between
a satisfiable formula and an unsatisfiable formula,
using the reduction from §4 with the given algo-
rithm, and identify whether the value of the objec-
tive function is 1 or strictly less than 1. We get the
result that:
Theorem 4. The decision problem of Condition-
alViterbiTrain problem is NP-hard.
where the decision problem of ConditionalViter-
biTrain is defined analogously to the decision
problem of ViterbiTrain.
We can similarly show that finding the global
maximum of the marginalized likelihood:
</bodyText>
<equation confidence="0.969776">
p(xi, z  |0) (12)
</equation>
<bodyText confidence="0.999739333333333">
is NP-hard. The reasoning follows. Using the
reduction from before, if φ is satisfiable, then
Eq. 12 gets value 0. If φ is unsatisfiable, then we
would still get value 0 only if L(G) = {sφ}. If
Gφ generates a single derivation for (10)3m, then
we actually do have a satisfying assignment from
</bodyText>
<equation confidence="0.869263833333333">
most
Yn p(zi  |0, xi) (11)
i=1
X
log
z
1
n
max
e
Xn
i=1
</equation>
<page confidence="0.915396">
1506
</page>
<bodyText confidence="0.9970945625">
Lemma 1. Otherwise (more than a single deriva-
tion), the optimal O would have to give fractional
probabilities to rules of the form VY� → {0, 1} (or
V y� → {0,1}). In that case, it is no longer true
that (10)3m is the only generated sentence, which
is a contradiction.
The quantity in Eq. 12 can be maximized ap-
proximately using algorithms like EM, so this
gives a hardness result for optimizing the objec-
tive function of EM for PCFGs. Day (1983) pre-
viously showed that maximizing the marginalized
likelihood for hidden Markov models is NP-hard.
We note that the grammar we use for all of our
results is not recursive. Therefore, we can encode
this grammar as a hidden Markov model, strength-
ening our result from PCFGs to HMMs.3
</bodyText>
<sectionHeader confidence="0.991048" genericHeader="method">
7 Uniform-at-Random Initialization
</sectionHeader>
<bodyText confidence="0.988589366666667">
In the previous sections, we showed that solving
Viterbi training is hard, and therefore requires an
approximation algorithm. Viterbi EM, which is an
example of such algorithm, is dependent on an ini-
tialization of either O to start with an E-step or z
to start with an M-step. In the absence of a better-
informed initializer, it is reasonable to initialize
z using a uniform distribution over D(G, xi) for
each i. If D(G, xi) is finite, it can be done effi-
ciently by setting O = 1 (ignoring the normaliza-
tion constraint), running the inside algorithm, and
sampling from the (unnormalized) posterior given
by the chart (Johnson et al., 2007). We turn next
to an analysis of this initialization technique that
suggests it is well-motivated.
The sketch of our result is as follows: we
first give an asymptotic upper bound for the log-
likelihood of derivations and sentences. This
bound, which has an information-theoretic inter-
pretation, depends on a parameter A, which de-
pends on the distribution from which the deriva-
tions were chosen. We then show that this bound
is minimized when we pick A such that this distri-
bution is (conditioned on the sentence) a uniform
distribution over derivations.
Let q(x) be any distribution over L(G) and O
some parameters for G. Let f(z) be some feature
function (such as the one that counts the number
of appearances of a certain rule in a derivation),
and then:
</bodyText>
<equation confidence="0.966741333333333">
1: 1:
Eq,e[f] q(x)
xEL(G) zED(G,x)
</equation>
<footnote confidence="0.411536">
3We thank an anonymous reviewer for pointing this out.
</footnote>
<bodyText confidence="0.9931046">
which gives the expected value of the feature func-
tion f(z) under the distribution q(x)×p(z  |O, x).
We will make the following assumption about G:
Condition 1. There exists some OI such that
∀x ∈ L(G),∀z ∈ D(G, x), p(z  |OI, x) =
1/|D(G,x)|.
This condition is satisfied, for example, when G
is in Chomsky normal form and for all A, A&apos; ∈ N,
|R(A) |= |R(A&apos;)|. Then, if we set 0A,α =
1/|R(A)|, we get that all derivations of x will
have the same number of rules and hence the same
probability. This condition does not hold for gram-
mars with unary cycles because |D(G, x) |may be
infinite for some derivations. Such grammars are
not commonly used in NLP.
Let us assume that some “correct” parameters
O* exist, and that our data were drawn from a dis-
tribution parametrized by O*. The goal of this sec-
tion is to motivate the following initialization for
O, which we call UniformInit:
</bodyText>
<listItem confidence="0.98333925">
1. Initialize z by sampling from the uniform dis-
tribution over D(G, xi) for each xi.
2. Update the grammar parameters using maxi-
mum likelihood estimation.
</listItem>
<subsectionHeader confidence="0.999383">
7.1 Bounding the Objective
</subsectionHeader>
<bodyText confidence="0.999607">
To show our result, we require first the following
definition due to Freund et al. (1997):
</bodyText>
<construct confidence="0.3404715">
Definition 5. A distribution p1 is within A ≥ 1 of
a distribution p2 iffor every event A, we have
</construct>
<equation confidence="0.8805725">
1 p1(A)
A ≤p2(A) ≤ a (13)
</equation>
<bodyText confidence="0.984194333333333">
For any feature function f(z) and any two
sets of parameters O2 and O1 for G and for any
marginal q(x), if p(z  |O1, x) is within A of
</bodyText>
<equation confidence="0.8948485">
p(z  |O2, x) for all x then:
Eq,e1[f] �≤ Eq,e2[f] ≤ AEq,e1[f] (14)
</equation>
<bodyText confidence="0.9992864">
Let O0 be a set of parameters such that we perform
the following procedure in initializing Viterbi EM:
first, we sample from the posterior distribution
p(z  |O0, x), and then update the parameters with
maximum likelihood estimate, in a regular M-step.
Let A be such that p(z  |O0, x) is within A of
p(z  |O*, x) (for all x ∈ L(G)). (Later we will
show that UniformInit is a wise choice for making
A small. Note that UniformInit is equivalent to the
procedure mentioned above with O0 = OI.)
</bodyText>
<equation confidence="0.60088">
p(z  |O, x)f(z)
</equation>
<page confidence="0.943593">
1507
</page>
<bodyText confidence="0.998719666666667">
Consider ˜pn(x), the empirical distribution over
x1, ... , xn. As n → ∞, we have that ˜pn(x) →
p∗(x), almost surely, where p∗ is:
</bodyText>
<equation confidence="0.9806415">
Xp∗(x) = p∗(x, z  |θ∗) (15)
z
</equation>
<bodyText confidence="0.9991954">
This means that as n → ∞ we have E˜pn,θ[f] →
Ep∗,θ[f]. Now, let z0 = (z0,1, ... , z0,n) be sam-
ples from p(z  |θ0, xi) for i ∈ {1,. . . , n}. Then,
from simple MLE computation, we know that the
value
</bodyText>
<equation confidence="0.9973466">
p(xi, z0,i  |θ0) (16)
�
Y
=
(A→α)∈R
</equation>
<bodyText confidence="0.9188095">
We also know that for θ0, from the consistency of
MLE, for large enough samples:
</bodyText>
<equation confidence="0.994238666666667">
FA→α(z0)
FA(z0) ≈ E˜pn,θ0[fA→α] (17)
E˜pn ,θ0 [fA]
</equation>
<bodyText confidence="0.836189333333333">
which means that we have the following as n
grows (starting from the ViterbiTrain objective
with initial state z = z0):
</bodyText>
<equation confidence="0.9973395">
p(xi, z0,i  |θ0) (18)
Y �FA→α(z0)
(Eq. 17) �E˜pn,θ0[fA→α]
≈ (20)
E˜pn,θ0[fA]
(A→α)∈R
</equation>
<bodyText confidence="0.967079375">
We next use the fact that ˜pn(x) ≈ p∗(x) for large
n, and apply Eq. 14, noting again our assumption
that p(z  |θ0, x) is within λ of p(z  |θ∗, x). We
Xalso let B = |zi|, where |zi |is the number of
i
nodes in the derivation zi. Note that FA(zi) ≤
B. The above quantity (Eq. 20) is approximately
bounded above by
</bodyText>
<equation confidence="0.994239857142857">
(A→α)∈R
Y λ2B (Ep∗,θ∗[fA→α])Ep∗,θ∗ [fA] (21)
1 Y λ2|R|B
(A→α)∈R (θ∗ A→α)FA→α(z0) (22)
Eq. 22 follows from:
Ep∗,θ∗[fA→α]
θ∗ A→α = (23) Ep∗,θ∗[fA]
</equation>
<bodyText confidence="0.9939195">
If we continue to develop Eq. 22 and apply
Eq. 17 and Eq. 23 again, we get that:
</bodyText>
<equation confidence="0.99846405882353">
1 Y λ2|R|B (θ∗A→α)FA→α(z0)
(A→α)∈R
1 Y λ2|R|B
(A→α)∈R
1 Y λ2|R|B
(A→α)∈R
1
Yλ2|R|B
(A→α)∈R
1 ⎛ ⎞(θ∗A→α)nθ∗A→α ⎠ Bλ2/n
⎝Y (24)
(A→α)∈R
λ2|R|B
 |{z }
T(θ∗,n)
(λ2|R|B ) T ∗,n)Bλ2/n (25)
o d(λ; θ∗, |x|, B) (26)
</equation>
<bodyText confidence="0.99974375">
where Eq. 24 is the result of FA(z0) ≤ B.
For two series {an} and {bn}, let “an ? bn”
denote that limn→∞ an ≥ limn→∞ bn. In other
words, an is asymptotically larger than bn. Then,
if we changed the representation of the objec-
tive function of the ViterbiTrain problem to log-
likelihood, for θ0 that maximizes Eq. 18 (with
some simple algebra) we have:
</bodyText>
<equation confidence="0.9974669">
log2 p(xi, z0,i  |θ0) (27)
� 1 �
Bλ2
log2 λ + n log2 T(θ∗, n)
n
Bλ 2
log2 λ − |N |X H(θ∗, A)
|N|n
A∈N
(28)
</equation>
<bodyText confidence="0.755849">
where
</bodyText>
<equation confidence="0.920696">
H(θ∗, A) = − X θ∗A→α log2 θ∗A→α
(A→α)∈R(A)
</equation>
<bodyText confidence="0.954639833333333">
(29)
is the entropy of the multinomial for nonter-
minal A. H(θ∗, A) can be thought of as the
minimal number of bits required to encode a
choice of a rule from A, if chosen independently
from the other rules. All together, the quantity
</bodyText>
<page confidence="0.273254">
�P
</page>
<figure confidence="0.888525769230769">
B A∈N H(θ∗, A)) is the average number of
|N|n
bits required to encode a tree in our sample using
Yn
i=1
max
θ0
� FA→α(z0)
FA→α(z0)
FA(z0)
n
Y
i=1
max
θ0
(Eq. 16)
=
Y �FA→α(z0)�FA→α(z0)
FA(z0)
(A→α)∈R
(19)
≈
≥
≥
(θ∗A→α)FA→α(z0)· FA(z0)
FA(z0)
Ep∗,B0 [fA→α] ·FA(z0)
(θ∗ Ep∗,B0[fA]
A→α)
∗ λ2θ∗A→αFA(z0)
(θA→α )
1
n
Xn
i=1
&gt; − 2|x|B
n
2|x|B
n
</figure>
<page confidence="0.974015">
1508
</page>
<bodyText confidence="0.999943294117647">
0*, while removing dependence among all rules
and assuming that each node at the tree is chosen
uniformly.4 This means that the log-likelihood, for
large n, is bounded from above by a linear func-
tion of the (average) number of bits required to
optimally encode n trees of total size B, while as-
suming independence among the rules in a tree.
We note that the quantity B/n will tend toward the
average size of a tree, which, under Condition 1,
must be finite.
Our final approximate bound from Eq. 28 re-
lates the choice of distribution, from which sample
z0, to A. The lower bound in Eq. 28 is a monotone-
decreasing function of A. We seek to make A as
small as possible to make the bound tight. We next
show that the uniform distribution optimizes A in
that sense.
</bodyText>
<subsectionHeader confidence="0.988842">
7.2 Optimizing A
</subsectionHeader>
<bodyText confidence="0.999972">
Note that the optimal choice of A, for a single x
and for candidate initializer 0&apos;, is
</bodyText>
<equation confidence="0.9901855">
p(z  |00, x)(30)
p(z  |0*, x)
</equation>
<bodyText confidence="0.996969">
In order to avoid degenerate cases, we will add an-
other condition on the true model, 0*:
</bodyText>
<construct confidence="0.888155333333333">
Condition 2. There exists T &gt; 0 such that, for
any x E L(G) and for any z E D(G, x), p(z |
0*, x) &gt; T.
</construct>
<bodyText confidence="0.999954125">
This is a strong condition, forcing the cardinal-
ity of D(G) to be finite, but it is not unreason-
able if natural language sentences are effectively
bounded in length.
Without further information about 0* (other
than that it satisfies Condition 2), we may want
to consider the worst-case scenario of possible A,
hence we seek initializer 00 such that
</bodyText>
<equation confidence="0.844180857142857">
A(x; 00)°— sup Aopt(x, 0; 00) (31)
e
is minimized. If 00 = 0I, then we have that
p(z  |0I, x) = |D(G, x)|−1 = µx. Together with
Condition 2, this implies that
p(z  |0I, x) &lt; µx (32)
p(z  |0*, x) T
</equation>
<footnote confidence="0.906941428571429">
4We note that Grenander (1967) describes a (lin-
ear) relationship between the derivational entropy and
H(9*, A). The derivational entropy is defined as h(9*, A) =
− E.,, p(x, z  |9*) log p(x, z  |9*), where z ranges over
trees that have nonterminal A as the root. It follows im-
mediately from Grenander’s result that EA H(9*, A) &lt;
EA h(9*, A).
</footnote>
<bodyText confidence="0.985068">
and hence Aopt(x, 0*) &lt; µx/T for any 0*, hence
A(x; 0I) &lt; µx/T. However, if we choose 00 =�
0I, we have that p(z&apos;  |00, x) &gt; µx for some z&apos;,
hence, for 0* such that it assigns probability T on
z&apos;, we have that
</bodyText>
<equation confidence="0.997199">
p(z  |00, x) µx
p(z  |0*, x) &gt; (33)
T
</equation>
<bodyText confidence="0.977789789473684">
and hence Aopt(x, 0*; 0&apos;) &gt; µx/T, so A(x; 0&apos;) &gt;
µx/T. So, to optimize for the worst-case scenario
over true distributions with respect to A, we are
motivated to choose 00 = 0I as defined in Con-
dition 1. Indeed, UniformInit uses 0I to initialize
the state of Viterbi EM.
We note that if 0I was known for a specific
grammar, then we could have used it as a direct
initializer. However, Condition 1 only guarantees
its existence, and does not give a practical way to
identify it. In general, as mentioned above, 0 = 1
can be used to obtain a weighted CFG that sat-
isfies p(z  |0, x) = 1/|D(G, x)|. Since we re-
quire a uniform posterior distribution, the num-
ber of derivations of a fixed length is finite. This
means that we can converted the weighted CFG
with 0 = 1 to a PCFG with the same posterior
(Smith and Johnson, 2007), and identify the ap-
propriate 0I.
</bodyText>
<sectionHeader confidence="0.999899" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999944">
Viterbi training is closely related to the k-means
clustering problem, where the objective is to find
k centroids for a given set of d-dimensional points
such that the sum of distances between the points
and the closest centroid is minimized. The ana-
log for Viterbi EM for the k-means problem is the
k-means clustering algorithm (Lloyd, 1982), a co-
ordinate ascent algorithm for solving the k-means
problem. It works by iterating between an E-like-
step, in which each point is assigned the closest
centroid, and an M-like-step, in which the cen-
troids are set to be the center of each cluster.
“k” in k-means corresponds, in a sense, to the
size of our grammar. k-means has been shown to
be NP-hard both when k varies and d is fixed and
when d varies and k is fixed (Aloise et al., 2009;
Mahajan et al., 2009). An open problem relating to
our hardness result would be whether ViterbiTrain
(or ConditionalViterbiTrain) is hard even if we do
not permit grammars of arbitrarily large size, or
at least, constrain the number of rules that do not
rewrite to terminals (in our current reduction, the
</bodyText>
<equation confidence="0.5856445">
Aopt(x, 0*; 00) = sup
zED(G,x)
sup
zED(G,x)
</equation>
<page confidence="0.975493">
1509
</page>
<bodyText confidence="0.999973733333334">
size of the grammar grows as the size of the 3-SAT
formula grows).
On a related note to §7, Arthur and Vassilvit-
skii (2007) described a greedy initialization al-
gorithm for initializing the centroids of k-means,
called k-means++. They show that their ini-
tialization is O(log k)-competitive; i.e., it ap-
proximates the optimal clusters assignment by a
factor of O(log k). In §7.1, we showed that
uniform-at-random initialization is approximately
O(|N|LA2/n)-competitive (modulo an additive
constant) for CNF grammars, where n is the num-
ber of sentences, L is the total length of sentences
and A is a measure for distance between the true
distribution and the uniform distribution.5
Many combinatorial problems in NLP involv-
ing phrase-structure trees, alignments, and depen-
dency graphs are hard (Sima’an, 1996; Good-
man, 1998; Knight, 1999; Casacuberta and de la
Higuera, 2000; Lyngsø and Pederson, 2002;
Udupa and Maji, 2006; McDonald and Satta,
2007; DeNero and Klein, 2008, inter alia). Of
special relevance to this paper is Abe and Warmuth
(1992), who showed that the problem of finding
maximum likelihood model of probabilistic au-
tomata is hard even for a single string and an au-
tomaton with two states. Understanding the com-
plexity of NLP problems, we believe, is crucial as
we seek effective practical approximations when
necessary.
</bodyText>
<sectionHeader confidence="0.997402" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999992333333333">
We described some properties of Viterbi train-
ing for probabilistic context-free grammars. We
showed that Viterbi training is NP-hard and, in
fact, NP-hard to approximate. We gave motivation
for uniform-at-random initialization for deriva-
tions in the Viterbi EM algorithm.
</bodyText>
<sectionHeader confidence="0.998237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999867">
We acknowledge helpful comments by the anony-
mous reviewers. This research was supported by
NSF grant 0915187.
</bodyText>
<sectionHeader confidence="0.999191" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994526923076923">
N. Abe and M. Warmuth. 1992. On the computational
complexity of approximating distributions by prob-
5Making the assumption that the grammar is in CNF per-
mits us to use L instead of B, since there is a linear relation-
ship between them in that case.
abilistic automata. Machine Learning, 9(2–3):205–
260.
S. Abney. 2007. Semisupervised Learning for Compu-
tational Linguistics. CRC Press.
D. Aloise, A. Deshpande, P. Hansen, and P. Popat.
2009. NP-hardness of Euclidean sum-of-squares
clustering. Machine Learning, 75(2):245–248.
D. Arthur and S. Vassilvitskii. 2007. k-means++: The
advantages of careful seeding. In Proc. of ACM-
SIAM symposium on Discrete Algorithms.
F. Casacuberta and C. de la Higuera. 2000. Com-
putational complexity of problems on probabilistic
grammars and transducers. In Proc. of ICGI.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. of AAAI.
S. B. Cohen and N. A. Smith. 2009. Shared logis-
tic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proc. of HLT-
NAACL.
M. Collins. 2003. Head-driven statistical models for
natural language processing. Computational Lin-
guistics, 29(4):589–637.
W. H. E. Day. 1983. Computationally difficult parsi-
mony problems in phylogenetic systematics. Jour-
nal of Theoretical Biology, 103.
J. DeNero and D. Klein. 2008. The complexity of
phrase alignment problems. In Proc. of ACL.
Y. Freund, H. Seung, E. Shamir, and N. Tishby. 1997.
Selective sampling using the query by committee al-
gorithm. Machine Learning, 28(2–3):133–168.
S. Goldwater and M. Johnson. 2005. Bias in learning
syllable structure. In Proc. of CoNLL.
J. Goodman. 1998. Parsing Inside-Out. Ph.D. thesis,
Harvard University.
U. Grenander. 1967. Syntax-controlled probabilities.
Technical report, Brown University, Division of Ap-
plied Mathematics.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2006.
Adaptor grammars: A framework for specifying
compositional nonparameteric Bayesian models. In
Advances in NIPS.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL.
D. Klein and C. Manning. 2001. Natural lan-
guage grammar induction using a constituent-
context model. In Advances in NIPS.
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25(4):607–615.
S. P. Lloyd. 1982. Least squares quantization in PCM.
In IEEE Transactions on Information Theory.
R. B. Lyngsø and C. N. S. Pederson. 2002. The con-
sensus string problem and the complexity of com-
paring hidden Markov models. Journal of Comput-
ing and System Science, 65(3):545–569.
M. Mahajan, P. Nimbhorkar, and K. Varadarajan. 2009.
The planar k-means problem is NP-hard. In Proc. of
International Workshop on Algorithms and Compu-
tation.
</reference>
<page confidence="0.745146">
1510
</page>
<reference confidence="0.999524055555555">
D. McClosky, E. Charniak, and M. Johnson. 2006a.
Effective self-training for parsing. In Proc. of HLT-
NAACL.
D. McClosky, E. Charniak, and M. Johnson. 2006b.
Reranking and self-training for parser adaptation. In
Proc. of COLING-ACL.
R. McDonald and G. Satta. 2007. On the complex-
ity of non-projective data-driven dependency pars-
ing. In Proc. of IWPT.
R. M. Neal and G. E. Hinton. 1998. A view of the
EM algorithm that justifies incremental, sparse, and
other variants. In Learning and Graphical Models,
pages 355–368. Kluwer Academic Publishers.
K. Sima’an. 1996. Computational complexity of prob-
abilistic disambiguation by means of tree-grammars.
In In Proc. of COLING.
M. Sipser. 2006. Introduction to the Theory of Com-
putation, Second Edition. Thomson Course Tech-
nology.
N. A. Smith and M. Johnson. 2007. Weighted and
probabilistic context-free grammars are equally ex-
pressive. Computational Linguistics, 33(4):477–
491.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D.
Manning. 2010. Viterbi training improves unsuper-
vised dependency parsing. In Proc. of CoNLL.
R. Udupa and K. Maji. 2006. Computational com-
plexity of statistical machine translation. In Proc. of
EACL.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? a quasi-synchronous gram-
mar for question answering. In Proc. of EMNLP.
C. Yejin and C. Cardie. 2007. Structured local training
and biased potential functions for conditional ran-
dom fields with application to coreference resolu-
tion. In Proc. of HLT-NAACL.
</reference>
<page confidence="0.992569">
1511
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.991046">
<title confidence="0.999615">Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</title>
<author confidence="0.999969">B Cohen A Smith</author>
<affiliation confidence="0.9999575">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.999497">Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.999453857142857">We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Abe</author>
<author>M Warmuth</author>
</authors>
<title>On the computational complexity of approximating distributions by prob5Making the assumption that the grammar is in CNF permits us to use L instead of B, since there is a linear relationship between them in that case.</title>
<date>1992</date>
<contexts>
<context position="31116" citStr="Abe and Warmuth (1992)" startWordPosition="5779" endWordPosition="5782">on is approximately O(|N|LA2/n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and A is a measure for distance between the true distribution and the uniform distribution.5 Many combinatorial problems in NLP involving phrase-structure trees, alignments, and dependency graphs are hard (Sima’an, 1996; Goodman, 1998; Knight, 1999; Casacuberta and de la Higuera, 2000; Lyngsø and Pederson, 2002; Udupa and Maji, 2006; McDonald and Satta, 2007; DeNero and Klein, 2008, inter alia). Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. Understanding the complexity of NLP problems, we believe, is crucial as we seek effective practical approximations when necessary. 9 Conclusion We described some properties of Viterbi training for probabilistic context-free grammars. We showed that Viterbi training is NP-hard and, in fact, NP-hard to approximate. We gave motivation for uniform-at-random initialization for derivations in the Viterbi EM algorithm. Acknowledgments We acknow</context>
</contexts>
<marker>Abe, Warmuth, 1992</marker>
<rawString>N. Abe and M. Warmuth. 1992. On the computational complexity of approximating distributions by prob5Making the assumption that the grammar is in CNF permits us to use L instead of B, since there is a linear relationship between them in that case.</rawString>
</citation>
<citation valid="false">
<authors>
<author>abilistic automata</author>
</authors>
<booktitle>Machine Learning,</booktitle>
<pages>9--2</pages>
<marker>automata, </marker>
<rawString>abilistic automata. Machine Learning, 9(2–3):205– 260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Semisupervised Learning for Computational Linguistics.</title>
<date>2007</date>
<publisher>CRC Press.</publisher>
<contexts>
<context position="7028" citStr="Abney (2007)" startWordPosition="1197" endWordPosition="1198">will refer to ,G(θ, z) = Yn p(xi, zi |θ) (5) i=1 as “the objective function of ViterbiTrain.” Viterbi training and Viterbi EM are closely related to self-training, an important concept in semi-supervised NLP (Charniak, 1997; McClosky et al., 2006a; McClosky et al., 2006b). With selftraining, the model is learned with some seed annotated data, and then iterates by labeling new, unannotated data and adding it to the original annotated training set. McClosky et al. consider selftraining to be “one round of Viterbi EM” with supervised initialization using labeled seed data. We refer the reader to Abney (2007) for more details. 4 Hardness of Viterbi Training We now describe hardness results for Problem 1. We first note that the following problem is known to be NP-hard, and in fact, NP-complete (Sipser, 2006): Problem 2. 3-SAT Input: A formula 0 = Vm1 (ai V bi V ci) in conjunctive normal form, such that each clause has 3 Yn p(xi, zi |θ) (4) i=1 1503 Sφ2 Sφ1 A1 A2 ������������������� ������������������� ������������������� ������������������� UY1,0 ������� ������� UY2,1 ������� ������� UY4,0 ������� ������� UY1,0 ������� ������� UY2,1 ������� ������� UY3,1 ������� ������� V Y1 VY1 VY2 V Y2 V v4 VY4 V</context>
</contexts>
<marker>Abney, 2007</marker>
<rawString>S. Abney. 2007. Semisupervised Learning for Computational Linguistics. CRC Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Aloise</author>
<author>A Deshpande</author>
<author>P Hansen</author>
<author>P Popat</author>
</authors>
<title>NP-hardness of Euclidean sum-of-squares clustering.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<volume>75</volume>
<issue>2</issue>
<contexts>
<context position="29714" citStr="Aloise et al., 2009" startWordPosition="5554" endWordPosition="5557">the sum of distances between the points and the closest centroid is minimized. The analog for Viterbi EM for the k-means problem is the k-means clustering algorithm (Lloyd, 1982), a coordinate ascent algorithm for solving the k-means problem. It works by iterating between an E-likestep, in which each point is assigned the closest centroid, and an M-like-step, in which the centroids are set to be the center of each cluster. “k” in k-means corresponds, in a sense, to the size of our grammar. k-means has been shown to be NP-hard both when k varies and d is fixed and when d varies and k is fixed (Aloise et al., 2009; Mahajan et al., 2009). An open problem relating to our hardness result would be whether ViterbiTrain (or ConditionalViterbiTrain) is hard even if we do not permit grammars of arbitrarily large size, or at least, constrain the number of rules that do not rewrite to terminals (in our current reduction, the Aopt(x, 0*; 00) = sup zED(G,x) sup zED(G,x) 1509 size of the grammar grows as the size of the 3-SAT formula grows). On a related note to §7, Arthur and Vassilvitskii (2007) described a greedy initialization algorithm for initializing the centroids of k-means, called k-means++. They show that</context>
</contexts>
<marker>Aloise, Deshpande, Hansen, Popat, 2009</marker>
<rawString>D. Aloise, A. Deshpande, P. Hansen, and P. Popat. 2009. NP-hardness of Euclidean sum-of-squares clustering. Machine Learning, 75(2):245–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Arthur</author>
<author>S Vassilvitskii</author>
</authors>
<title>k-means++: The advantages of careful seeding.</title>
<date>2007</date>
<booktitle>In Proc. of ACMSIAM symposium on Discrete Algorithms.</booktitle>
<contexts>
<context position="30194" citStr="Arthur and Vassilvitskii (2007)" startWordPosition="5636" endWordPosition="5640">o the size of our grammar. k-means has been shown to be NP-hard both when k varies and d is fixed and when d varies and k is fixed (Aloise et al., 2009; Mahajan et al., 2009). An open problem relating to our hardness result would be whether ViterbiTrain (or ConditionalViterbiTrain) is hard even if we do not permit grammars of arbitrarily large size, or at least, constrain the number of rules that do not rewrite to terminals (in our current reduction, the Aopt(x, 0*; 00) = sup zED(G,x) sup zED(G,x) 1509 size of the grammar grows as the size of the 3-SAT formula grows). On a related note to §7, Arthur and Vassilvitskii (2007) described a greedy initialization algorithm for initializing the centroids of k-means, called k-means++. They show that their initialization is O(log k)-competitive; i.e., it approximates the optimal clusters assignment by a factor of O(log k). In §7.1, we showed that uniform-at-random initialization is approximately O(|N|LA2/n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and A is a measure for distance between the true distribution and the uniform distribution.5 Many combinatorial problems in NLP involving</context>
</contexts>
<marker>Arthur, Vassilvitskii, 2007</marker>
<rawString>D. Arthur and S. Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proc. of ACMSIAM symposium on Discrete Algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Casacuberta</author>
<author>C de la Higuera</author>
</authors>
<title>Computational complexity of problems on probabilistic grammars and transducers.</title>
<date>2000</date>
<booktitle>In Proc. of ICGI.</booktitle>
<marker>Casacuberta, Higuera, 2000</marker>
<rawString>F. Casacuberta and C. de la Higuera. 2000. Computational complexity of problems on probabilistic grammars and transducers. In Proc. of ICGI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical parsing with a contextfree grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proc. of AAAI.</booktitle>
<contexts>
<context position="879" citStr="Charniak, 1997" startWordPosition="125" endWordPosition="126">r a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood. 1 Introduction Probabilistic context-free grammars are an essential ingredient in many natural language processing models (Charniak, 1997; Collins, 2003; Johnson et al., 2006; Cohen and Smith, 2009, inter alia). Various algorithms for training such models have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. V</context>
<context position="6639" citStr="Charniak, 1997" startWordPosition="1130" endWordPosition="1131">ze. Therefore it is necessary to resort to approximate algorithms like Viterbi EM. Neal and Hinton (1998) use the term “sparse EM” to refer to a version of the EM algorithm where the E-step finds the modes of hidden variables (rather than marginals as in standard EM). Viterbi EM is a variant of this, where the Estep finds the mode for each xi’s derivation, argmaxzED(G,xi) p(xi, z |θ). We will refer to ,G(θ, z) = Yn p(xi, zi |θ) (5) i=1 as “the objective function of ViterbiTrain.” Viterbi training and Viterbi EM are closely related to self-training, an important concept in semi-supervised NLP (Charniak, 1997; McClosky et al., 2006a; McClosky et al., 2006b). With selftraining, the model is learned with some seed annotated data, and then iterates by labeling new, unannotated data and adding it to the original annotated training set. McClosky et al. consider selftraining to be “one round of Viterbi EM” with supervised initialization using labeled seed data. We refer the reader to Abney (2007) for more details. 4 Hardness of Viterbi Training We now describe hardness results for Problem 1. We first note that the following problem is known to be NP-hard, and in fact, NP-complete (Sipser, 2006): Problem</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997. Statistical parsing with a contextfree grammar and word statistics. In Proc. of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. of HLTNAACL.</booktitle>
<contexts>
<context position="939" citStr="Cohen and Smith, 2009" startWordPosition="134" endWordPosition="137">ns and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood. 1 Introduction Probabilistic context-free grammars are an essential ingredient in many natural language processing models (Charniak, 1997; Collins, 2003; Johnson et al., 2006; Cohen and Smith, 2009, inter alia). Various algorithms for training such models have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proc. of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language processing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="894" citStr="Collins, 2003" startWordPosition="127" endWordPosition="128">lihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood. 1 Introduction Probabilistic context-free grammars are an essential ingredient in many natural language processing models (Charniak, 1997; Collins, 2003; Johnson et al., 2006; Cohen and Smith, 2009, inter alia). Various algorithms for training such models have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and v</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>M. Collins. 2003. Head-driven statistical models for natural language processing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H E Day</author>
</authors>
<title>Computationally difficult parsimony problems in phylogenetic systematics.</title>
<date>1983</date>
<journal>Journal of Theoretical Biology,</journal>
<volume>103</volume>
<contexts>
<context position="19509" citStr="Day (1983)" startWordPosition="3639" endWordPosition="3640">}. If Gφ generates a single derivation for (10)3m, then we actually do have a satisfying assignment from most Yn p(zi |0, xi) (11) i=1 X log z 1 n max e Xn i=1 1506 Lemma 1. Otherwise (more than a single derivation), the optimal O would have to give fractional probabilities to rules of the form VY� → {0, 1} (or V y� → {0,1}). In that case, it is no longer true that (10)3m is the only generated sentence, which is a contradiction. The quantity in Eq. 12 can be maximized approximately using algorithms like EM, so this gives a hardness result for optimizing the objective function of EM for PCFGs. Day (1983) previously showed that maximizing the marginalized likelihood for hidden Markov models is NP-hard. We note that the grammar we use for all of our results is not recursive. Therefore, we can encode this grammar as a hidden Markov model, strengthening our result from PCFGs to HMMs.3 7 Uniform-at-Random Initialization In the previous sections, we showed that solving Viterbi training is hard, and therefore requires an approximation algorithm. Viterbi EM, which is an example of such algorithm, is dependent on an initialization of either O to start with an E-step or z to start with an M-step. In th</context>
</contexts>
<marker>Day, 1983</marker>
<rawString>W. H. E. Day. 1983. Computationally difficult parsimony problems in phylogenetic systematics. Journal of Theoretical Biology, 103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1663" citStr="DeNero and Klein, 2008" startWordPosition="250" endWordPosition="253">methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.” In this paper, we explore Viterbi training for probabilistic context-free grammars. We first show that under the assumption that P =� NP, solving and even approximating the Viterbi training problem is hard. This result holds even for hidden Markov models. We extend the main hardness result to the EM algorithm (giving an alternative proof to this known result), as well as the problem of conditional Viterbi training. We th</context>
<context position="4768" citStr="DeNero and Klein, 2008" startWordPosition="798" endWordPosition="801">FA,α(z) = Xn fA,α(zi) (2) i=1 FA(z) = Xn fA(zi) (3) i=1 We use the following notation for G: • L(G) is the set of all strings (sentences) x that can be generated using the grammar G (the “language of G”). • D(G) is the set of all possible derivations z that can be generated using the grammar G. • D(G, x) is the set of all possible derivations z that can be generated using the grammar G and have the yield x. 3 Viterbi Training Viterbi EM, or “hard” EM, is an unsupervised learning algorithm, used in NLP in various settings (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). In the context of PCFGs, it aims to select parameters θ and phrasestructure trees z jointly. It does so by iteratively updating a state consisting of (θ, z). The state is initialized with some value, then the algorithm alternates between (i) a “hard” E-step, where the strings x1, ... , xn are parsed according to a current, fixed θ, giving new values for z, and (ii) an M-step, where the θ are selected to maximize likelihood, with z fixed. With PCFGs, the E-step requires running an algorithm such as (probabilistic) CKY or Earley’s 1Note that x = yield(z); if the deriv</context>
<context position="31041" citStr="DeNero and Klein, 2008" startWordPosition="5766" endWordPosition="5769"> factor of O(log k). In §7.1, we showed that uniform-at-random initialization is approximately O(|N|LA2/n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and A is a measure for distance between the true distribution and the uniform distribution.5 Many combinatorial problems in NLP involving phrase-structure trees, alignments, and dependency graphs are hard (Sima’an, 1996; Goodman, 1998; Knight, 1999; Casacuberta and de la Higuera, 2000; Lyngsø and Pederson, 2002; Udupa and Maji, 2006; McDonald and Satta, 2007; DeNero and Klein, 2008, inter alia). Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. Understanding the complexity of NLP problems, we believe, is crucial as we seek effective practical approximations when necessary. 9 Conclusion We described some properties of Viterbi training for probabilistic context-free grammars. We showed that Viterbi training is NP-hard and, in fact, NP-hard to approximate. We gave motivation for uniform-at-random initializa</context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>J. DeNero and D. Klein. 2008. The complexity of phrase alignment problems. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>H Seung</author>
<author>E Shamir</author>
<author>N Tishby</author>
</authors>
<title>Selective sampling using the query by committee algorithm.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>28--2</pages>
<contexts>
<context position="22460" citStr="Freund et al. (1997)" startWordPosition="4150" endWordPosition="4153">ars with unary cycles because |D(G, x) |may be infinite for some derivations. Such grammars are not commonly used in NLP. Let us assume that some “correct” parameters O* exist, and that our data were drawn from a distribution parametrized by O*. The goal of this section is to motivate the following initialization for O, which we call UniformInit: 1. Initialize z by sampling from the uniform distribution over D(G, xi) for each xi. 2. Update the grammar parameters using maximum likelihood estimation. 7.1 Bounding the Objective To show our result, we require first the following definition due to Freund et al. (1997): Definition 5. A distribution p1 is within A ≥ 1 of a distribution p2 iffor every event A, we have 1 p1(A) A ≤p2(A) ≤ a (13) For any feature function f(z) and any two sets of parameters O2 and O1 for G and for any marginal q(x), if p(z |O1, x) is within A of p(z |O2, x) for all x then: Eq,e1[f] �≤ Eq,e2[f] ≤ AEq,e1[f] (14) Let O0 be a set of parameters such that we perform the following procedure in initializing Viterbi EM: first, we sample from the posterior distribution p(z |O0, x), and then update the parameters with maximum likelihood estimate, in a regular M-step. Let A be such that p(z </context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1997</marker>
<rawString>Y. Freund, H. Seung, E. Shamir, and N. Tishby. 1997. Selective sampling using the query by committee algorithm. Machine Learning, 28(2–3):133–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>M Johnson</author>
</authors>
<title>Bias in learning syllable structure.</title>
<date>2005</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="1639" citStr="Goldwater and Johnson, 2005" startWordPosition="246" endWordPosition="249">osed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.” In this paper, we explore Viterbi training for probabilistic context-free grammars. We first show that under the assumption that P =� NP, solving and even approximating the Viterbi training problem is hard. This result holds even for hidden Markov models. We extend the main hardness result to the EM algorithm (giving an alternative proof to this known result), as well as the problem of conditional</context>
<context position="4744" citStr="Goldwater and Johnson, 2005" startWordPosition="794" endWordPosition="797">ons z = (z1, ... , zn), let: FA,α(z) = Xn fA,α(zi) (2) i=1 FA(z) = Xn fA(zi) (3) i=1 We use the following notation for G: • L(G) is the set of all strings (sentences) x that can be generated using the grammar G (the “language of G”). • D(G) is the set of all possible derivations z that can be generated using the grammar G. • D(G, x) is the set of all possible derivations z that can be generated using the grammar G and have the yield x. 3 Viterbi Training Viterbi EM, or “hard” EM, is an unsupervised learning algorithm, used in NLP in various settings (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). In the context of PCFGs, it aims to select parameters θ and phrasestructure trees z jointly. It does so by iteratively updating a state consisting of (θ, z). The state is initialized with some value, then the algorithm alternates between (i) a “hard” E-step, where the strings x1, ... , xn are parsed according to a current, fixed θ, giving new values for z, and (ii) an M-step, where the θ are selected to maximize likelihood, with z fixed. With PCFGs, the E-step requires running an algorithm such as (probabilistic) CKY or Earley’s 1Note that x </context>
</contexts>
<marker>Goldwater, Johnson, 2005</marker>
<rawString>S. Goldwater and M. Johnson. 2005. Bias in learning syllable structure. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Parsing Inside-Out.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="30891" citStr="Goodman, 1998" startWordPosition="5743" endWordPosition="5745">ns, called k-means++. They show that their initialization is O(log k)-competitive; i.e., it approximates the optimal clusters assignment by a factor of O(log k). In §7.1, we showed that uniform-at-random initialization is approximately O(|N|LA2/n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and A is a measure for distance between the true distribution and the uniform distribution.5 Many combinatorial problems in NLP involving phrase-structure trees, alignments, and dependency graphs are hard (Sima’an, 1996; Goodman, 1998; Knight, 1999; Casacuberta and de la Higuera, 2000; Lyngsø and Pederson, 2002; Udupa and Maji, 2006; McDonald and Satta, 2007; DeNero and Klein, 2008, inter alia). Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. Understanding the complexity of NLP problems, we believe, is crucial as we seek effective practical approximations when necessary. 9 Conclusion We described some properties of Viterbi training for probabilistic conte</context>
</contexts>
<marker>Goodman, 1998</marker>
<rawString>J. Goodman. 1998. Parsing Inside-Out. Ph.D. thesis, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Grenander</author>
</authors>
<title>Syntax-controlled probabilities.</title>
<date>1967</date>
<tech>Technical report,</tech>
<institution>Brown University, Division of Applied Mathematics.</institution>
<contexts>
<context position="27507" citStr="Grenander (1967)" startWordPosition="5136" endWordPosition="5137">and for any z E D(G, x), p(z | 0*, x) &gt; T. This is a strong condition, forcing the cardinality of D(G) to be finite, but it is not unreasonable if natural language sentences are effectively bounded in length. Without further information about 0* (other than that it satisfies Condition 2), we may want to consider the worst-case scenario of possible A, hence we seek initializer 00 such that A(x; 00)°— sup Aopt(x, 0; 00) (31) e is minimized. If 00 = 0I, then we have that p(z |0I, x) = |D(G, x)|−1 = µx. Together with Condition 2, this implies that p(z |0I, x) &lt; µx (32) p(z |0*, x) T 4We note that Grenander (1967) describes a (linear) relationship between the derivational entropy and H(9*, A). The derivational entropy is defined as h(9*, A) = − E.,, p(x, z |9*) log p(x, z |9*), where z ranges over trees that have nonterminal A as the root. It follows immediately from Grenander’s result that EA H(9*, A) &lt; EA h(9*, A). and hence Aopt(x, 0*) &lt; µx/T for any 0*, hence A(x; 0I) &lt; µx/T. However, if we choose 00 =� 0I, we have that p(z&apos; |00, x) &gt; µx for some z&apos;, hence, for 0* such that it assigns probability T on z&apos;, we have that p(z |00, x) µx p(z |0*, x) &gt; (33) T and hence Aopt(x, 0*; 0&apos;) &gt; µx/T, so A(x; 0&apos;)</context>
</contexts>
<marker>Grenander, 1967</marker>
<rawString>U. Grenander. 1967. Syntax-controlled probabilities. Technical report, Brown University, Division of Applied Mathematics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>T L Griffiths</author>
<author>S Goldwater</author>
</authors>
<title>Adaptor grammars: A framework for specifying compositional nonparameteric Bayesian models.</title>
<date>2006</date>
<booktitle>In Advances in NIPS.</booktitle>
<contexts>
<context position="916" citStr="Johnson et al., 2006" startWordPosition="129" endWordPosition="133">nt of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood. 1 Introduction Probabilistic context-free grammars are an essential ingredient in many natural language processing models (Charniak, 1997; Collins, 2003; Johnson et al., 2006; Cohen and Smith, 2009, inter alia). Various algorithms for training such models have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2006</marker>
<rawString>M. Johnson, T. L. Griffiths, and S. Goldwater. 2006. Adaptor grammars: A framework for specifying compositional nonparameteric Bayesian models. In Advances in NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>T L Griffiths</author>
<author>S Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="20466" citStr="Johnson et al., 2007" startWordPosition="3799" endWordPosition="3802">ctions, we showed that solving Viterbi training is hard, and therefore requires an approximation algorithm. Viterbi EM, which is an example of such algorithm, is dependent on an initialization of either O to start with an E-step or z to start with an M-step. In the absence of a betterinformed initializer, it is reasonable to initialize z using a uniform distribution over D(G, xi) for each i. If D(G, xi) is finite, it can be done efficiently by setting O = 1 (ignoring the normalization constraint), running the inside algorithm, and sampling from the (unnormalized) posterior given by the chart (Johnson et al., 2007). We turn next to an analysis of this initialization technique that suggests it is well-motivated. The sketch of our result is as follows: we first give an asymptotic upper bound for the loglikelihood of derivations and sentences. This bound, which has an information-theoretic interpretation, depends on a parameter A, which depends on the distribution from which the derivations were chosen. We then show that this bound is minimized when we pick A such that this distribution is (conditioned on the sentence) a uniform distribution over derivations. Let q(x) be any distribution over L(G) and O so</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>M. Johnson, T. L. Griffiths, and S. Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Natural language grammar induction using a constituentcontext model.</title>
<date>2001</date>
<booktitle>In Advances in NIPS.</booktitle>
<contexts>
<context position="16658" citStr="Klein and Manning, 2001" startWordPosition="3123" endWordPosition="3126">tisfying assignment is smaller than 1�. If φ were not satisfiable, then the approximation algorithm would never return a (0, z) that results in a satisfying assignment (because such a (0, z) does not exist). The conclusion is that an efficient algorithm for approximating the objective function of ViterbiTrain (Eq. 4) within a factor of 21 + E is unlikely to exist. If there were such an algorithm, we could use it to solve 3-SAT using the reduction from §4. 6 Extensions of the Hardness Result An alternative problem to Problem 1, a variant of Viterbi-training, is the following (see, for example, Klein and Manning, 2001): Problem 3. ConditionalViterbiTrain Input: G context-free grammar, x1, ... , xn training instances from L(G) Output: 0 and z1, ... , zn such that (0,z1, ... , zn) = argmax B,z Here, instead of maximizing the likelihood, we maximize the conditional likelihood. Note that there is a hidden assumption in this problem definition, that xi can be parsed using the grammar G. Otherwise, the quantity p(zi |0, xi) is not well-defined. We can extend ConditionalViterbiTrain to return + in the case of not having a parse for one of the xi—this can be efficiently checked using a run of a cubic-time parser on</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>D. Klein and C. Manning. 2001. Natural language grammar induction using a constituentcontext model. In Advances in NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="30905" citStr="Knight, 1999" startWordPosition="5746" endWordPosition="5747">ans++. They show that their initialization is O(log k)-competitive; i.e., it approximates the optimal clusters assignment by a factor of O(log k). In §7.1, we showed that uniform-at-random initialization is approximately O(|N|LA2/n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and A is a measure for distance between the true distribution and the uniform distribution.5 Many combinatorial problems in NLP involving phrase-structure trees, alignments, and dependency graphs are hard (Sima’an, 1996; Goodman, 1998; Knight, 1999; Casacuberta and de la Higuera, 2000; Lyngsø and Pederson, 2002; Udupa and Maji, 2006; McDonald and Satta, 2007; DeNero and Klein, 2008, inter alia). Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. Understanding the complexity of NLP problems, we believe, is crucial as we seek effective practical approximations when necessary. 9 Conclusion We described some properties of Viterbi training for probabilistic context-free gramma</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>K. Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4):607–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Lloyd</author>
</authors>
<title>Least squares quantization in PCM.</title>
<date>1982</date>
<booktitle>In IEEE Transactions on Information Theory.</booktitle>
<contexts>
<context position="29273" citStr="Lloyd, 1982" startWordPosition="5472" endWordPosition="5473">we require a uniform posterior distribution, the number of derivations of a fixed length is finite. This means that we can converted the weighted CFG with 0 = 1 to a PCFG with the same posterior (Smith and Johnson, 2007), and identify the appropriate 0I. 8 Related Work Viterbi training is closely related to the k-means clustering problem, where the objective is to find k centroids for a given set of d-dimensional points such that the sum of distances between the points and the closest centroid is minimized. The analog for Viterbi EM for the k-means problem is the k-means clustering algorithm (Lloyd, 1982), a coordinate ascent algorithm for solving the k-means problem. It works by iterating between an E-likestep, in which each point is assigned the closest centroid, and an M-like-step, in which the centroids are set to be the center of each cluster. “k” in k-means corresponds, in a sense, to the size of our grammar. k-means has been shown to be NP-hard both when k varies and d is fixed and when d varies and k is fixed (Aloise et al., 2009; Mahajan et al., 2009). An open problem relating to our hardness result would be whether ViterbiTrain (or ConditionalViterbiTrain) is hard even if we do not p</context>
</contexts>
<marker>Lloyd, 1982</marker>
<rawString>S. P. Lloyd. 1982. Least squares quantization in PCM. In IEEE Transactions on Information Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R B Lyngsø</author>
<author>C N S Pederson</author>
</authors>
<title>The consensus string problem and the complexity of comparing hidden Markov models.</title>
<date>2002</date>
<journal>Journal of Computing and System Science,</journal>
<volume>65</volume>
<issue>3</issue>
<contexts>
<context position="30969" citStr="Lyngsø and Pederson, 2002" startWordPosition="5754" endWordPosition="5757"> k)-competitive; i.e., it approximates the optimal clusters assignment by a factor of O(log k). In §7.1, we showed that uniform-at-random initialization is approximately O(|N|LA2/n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and A is a measure for distance between the true distribution and the uniform distribution.5 Many combinatorial problems in NLP involving phrase-structure trees, alignments, and dependency graphs are hard (Sima’an, 1996; Goodman, 1998; Knight, 1999; Casacuberta and de la Higuera, 2000; Lyngsø and Pederson, 2002; Udupa and Maji, 2006; McDonald and Satta, 2007; DeNero and Klein, 2008, inter alia). Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. Understanding the complexity of NLP problems, we believe, is crucial as we seek effective practical approximations when necessary. 9 Conclusion We described some properties of Viterbi training for probabilistic context-free grammars. We showed that Viterbi training is NP-hard and, in fact, NP-</context>
</contexts>
<marker>Lyngsø, Pederson, 2002</marker>
<rawString>R. B. Lyngsø and C. N. S. Pederson. 2002. The consensus string problem and the complexity of comparing hidden Markov models. Journal of Computing and System Science, 65(3):545–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mahajan</author>
<author>P Nimbhorkar</author>
<author>K Varadarajan</author>
</authors>
<title>The planar k-means problem is NP-hard.</title>
<date>2009</date>
<booktitle>In Proc. of International Workshop on Algorithms and Computation.</booktitle>
<contexts>
<context position="29737" citStr="Mahajan et al., 2009" startWordPosition="5558" endWordPosition="5561">between the points and the closest centroid is minimized. The analog for Viterbi EM for the k-means problem is the k-means clustering algorithm (Lloyd, 1982), a coordinate ascent algorithm for solving the k-means problem. It works by iterating between an E-likestep, in which each point is assigned the closest centroid, and an M-like-step, in which the centroids are set to be the center of each cluster. “k” in k-means corresponds, in a sense, to the size of our grammar. k-means has been shown to be NP-hard both when k varies and d is fixed and when d varies and k is fixed (Aloise et al., 2009; Mahajan et al., 2009). An open problem relating to our hardness result would be whether ViterbiTrain (or ConditionalViterbiTrain) is hard even if we do not permit grammars of arbitrarily large size, or at least, constrain the number of rules that do not rewrite to terminals (in our current reduction, the Aopt(x, 0*; 00) = sup zED(G,x) sup zED(G,x) 1509 size of the grammar grows as the size of the 3-SAT formula grows). On a related note to §7, Arthur and Vassilvitskii (2007) described a greedy initialization algorithm for initializing the centroids of k-means, called k-means++. They show that their initialization i</context>
</contexts>
<marker>Mahajan, Nimbhorkar, Varadarajan, 2009</marker>
<rawString>M. Mahajan, P. Nimbhorkar, and K. Varadarajan. 2009. The planar k-means problem is NP-hard. In Proc. of International Workshop on Algorithms and Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proc. of HLTNAACL.</booktitle>
<contexts>
<context position="6662" citStr="McClosky et al., 2006" startWordPosition="1132" endWordPosition="1135"> is necessary to resort to approximate algorithms like Viterbi EM. Neal and Hinton (1998) use the term “sparse EM” to refer to a version of the EM algorithm where the E-step finds the modes of hidden variables (rather than marginals as in standard EM). Viterbi EM is a variant of this, where the Estep finds the mode for each xi’s derivation, argmaxzED(G,xi) p(xi, z |θ). We will refer to ,G(θ, z) = Yn p(xi, zi |θ) (5) i=1 as “the objective function of ViterbiTrain.” Viterbi training and Viterbi EM are closely related to self-training, an important concept in semi-supervised NLP (Charniak, 1997; McClosky et al., 2006a; McClosky et al., 2006b). With selftraining, the model is learned with some seed annotated data, and then iterates by labeling new, unannotated data and adding it to the original annotated training set. McClosky et al. consider selftraining to be “one round of Viterbi EM” with supervised initialization using labeled seed data. We refer the reader to Abney (2007) for more details. 4 Hardness of Viterbi Training We now describe hardness results for Problem 1. We first note that the following problem is known to be NP-hard, and in fact, NP-complete (Sipser, 2006): Problem 2. 3-SAT Input: A form</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006a. Effective self-training for parsing. In Proc. of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="6662" citStr="McClosky et al., 2006" startWordPosition="1132" endWordPosition="1135"> is necessary to resort to approximate algorithms like Viterbi EM. Neal and Hinton (1998) use the term “sparse EM” to refer to a version of the EM algorithm where the E-step finds the modes of hidden variables (rather than marginals as in standard EM). Viterbi EM is a variant of this, where the Estep finds the mode for each xi’s derivation, argmaxzED(G,xi) p(xi, z |θ). We will refer to ,G(θ, z) = Yn p(xi, zi |θ) (5) i=1 as “the objective function of ViterbiTrain.” Viterbi training and Viterbi EM are closely related to self-training, an important concept in semi-supervised NLP (Charniak, 1997; McClosky et al., 2006a; McClosky et al., 2006b). With selftraining, the model is learned with some seed annotated data, and then iterates by labeling new, unannotated data and adding it to the original annotated training set. McClosky et al. consider selftraining to be “one round of Viterbi EM” with supervised initialization using labeled seed data. We refer the reader to Abney (2007) for more details. 4 Hardness of Viterbi Training We now describe hardness results for Problem 1. We first note that the following problem is known to be NP-hard, and in fact, NP-complete (Sipser, 2006): Problem 2. 3-SAT Input: A form</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006b. Reranking and self-training for parser adaptation. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>G Satta</author>
</authors>
<title>On the complexity of non-projective data-driven dependency parsing.</title>
<date>2007</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="31017" citStr="McDonald and Satta, 2007" startWordPosition="5762" endWordPosition="5765">l clusters assignment by a factor of O(log k). In §7.1, we showed that uniform-at-random initialization is approximately O(|N|LA2/n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and A is a measure for distance between the true distribution and the uniform distribution.5 Many combinatorial problems in NLP involving phrase-structure trees, alignments, and dependency graphs are hard (Sima’an, 1996; Goodman, 1998; Knight, 1999; Casacuberta and de la Higuera, 2000; Lyngsø and Pederson, 2002; Udupa and Maji, 2006; McDonald and Satta, 2007; DeNero and Klein, 2008, inter alia). Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. Understanding the complexity of NLP problems, we believe, is crucial as we seek effective practical approximations when necessary. 9 Conclusion We described some properties of Viterbi training for probabilistic context-free grammars. We showed that Viterbi training is NP-hard and, in fact, NP-hard to approximate. We gave motivation for unif</context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>R. McDonald and G. Satta. 2007. On the complexity of non-projective data-driven dependency parsing. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Neal</author>
<author>G E Hinton</author>
</authors>
<title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
<date>1998</date>
<booktitle>In Learning and Graphical Models,</booktitle>
<pages>355--368</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1251" citStr="Neal and Hinton, 1998" startWordPosition="184" endWordPosition="187">of further information about the correct model parameters, providing an approximate bound on the log-likelihood. 1 Introduction Probabilistic context-free grammars are an essential ingredient in many natural language processing models (Charniak, 1997; Collins, 2003; Johnson et al., 2006; Cohen and Smith, 2009, inter alia). Various algorithms for training such models have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.” In this pape</context>
<context position="6130" citStr="Neal and Hinton (1998)" startWordPosition="1037" endWordPosition="1040">. algorithm, while the M-step normalizes frequency counts FA,α(z) to obtain the maximum likelihood estimate’s closed-form solution. We can understand Viterbi EM as a coordinate ascent procedure that approximates the solution to the following declarative problem: Problem 1. ViterbiTrain Input: G context-free grammar, x1, ... , xn training instances from L(G) Output: θ and z1, ... , zn such that (θ, z1, ... , zn) = argmax B,z The optimization problem in Eq. 4 is nonconvex and, as we will show in §4, hard to optimize. Therefore it is necessary to resort to approximate algorithms like Viterbi EM. Neal and Hinton (1998) use the term “sparse EM” to refer to a version of the EM algorithm where the E-step finds the modes of hidden variables (rather than marginals as in standard EM). Viterbi EM is a variant of this, where the Estep finds the mode for each xi’s derivation, argmaxzED(G,xi) p(xi, z |θ). We will refer to ,G(θ, z) = Yn p(xi, zi |θ) (5) i=1 as “the objective function of ViterbiTrain.” Viterbi training and Viterbi EM are closely related to self-training, an important concept in semi-supervised NLP (Charniak, 1997; McClosky et al., 2006a; McClosky et al., 2006b). With selftraining, the model is learned </context>
</contexts>
<marker>Neal, Hinton, 1998</marker>
<rawString>R. M. Neal and G. E. Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In Learning and Graphical Models, pages 355–368. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima’an</author>
</authors>
<title>Computational complexity of probabilistic disambiguation by means of tree-grammars. In</title>
<date>1996</date>
<booktitle>In Proc. of COLING.</booktitle>
<marker>Sima’an, 1996</marker>
<rawString>K. Sima’an. 1996. Computational complexity of probabilistic disambiguation by means of tree-grammars. In In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sipser</author>
</authors>
<title>Introduction to the Theory of Computation, Second Edition.</title>
<date>2006</date>
<publisher>Thomson Course Technology.</publisher>
<contexts>
<context position="7230" citStr="Sipser, 2006" startWordPosition="1231" endWordPosition="1232">ed NLP (Charniak, 1997; McClosky et al., 2006a; McClosky et al., 2006b). With selftraining, the model is learned with some seed annotated data, and then iterates by labeling new, unannotated data and adding it to the original annotated training set. McClosky et al. consider selftraining to be “one round of Viterbi EM” with supervised initialization using labeled seed data. We refer the reader to Abney (2007) for more details. 4 Hardness of Viterbi Training We now describe hardness results for Problem 1. We first note that the following problem is known to be NP-hard, and in fact, NP-complete (Sipser, 2006): Problem 2. 3-SAT Input: A formula 0 = Vm1 (ai V bi V ci) in conjunctive normal form, such that each clause has 3 Yn p(xi, zi |θ) (4) i=1 1503 Sφ2 Sφ1 A1 A2 ������������������� ������������������� ������������������� ������������������� UY1,0 ������� ������� UY2,1 ������� ������� UY4,0 ������� ������� UY1,0 ������� ������� UY2,1 ������� ������� UY3,1 ������� ������� V Y1 VY1 VY2 V Y2 V v4 VY4 V Y1 VY1 VY2 V V2 VY3 V Y3 1 0 1 0 1 0 1 0 1 0 1 0 Figure 1: An example of a Viterbi parse tree which represents a satisfying assignment for 0 = (Y1 V Y2 V 174) n (�Y1 V 172 V Y3). In 0φ, all rules appea</context>
</contexts>
<marker>Sipser, 2006</marker>
<rawString>M. Sipser. 2006. Introduction to the Theory of Computation, Second Edition. Thomson Course Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>M Johnson</author>
</authors>
<title>Weighted and probabilistic context-free grammars are equally expressive.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<pages>491</pages>
<contexts>
<context position="28881" citStr="Smith and Johnson, 2007" startWordPosition="5404" endWordPosition="5407">tion 1. Indeed, UniformInit uses 0I to initialize the state of Viterbi EM. We note that if 0I was known for a specific grammar, then we could have used it as a direct initializer. However, Condition 1 only guarantees its existence, and does not give a practical way to identify it. In general, as mentioned above, 0 = 1 can be used to obtain a weighted CFG that satisfies p(z |0, x) = 1/|D(G, x)|. Since we require a uniform posterior distribution, the number of derivations of a fixed length is finite. This means that we can converted the weighted CFG with 0 = 1 to a PCFG with the same posterior (Smith and Johnson, 2007), and identify the appropriate 0I. 8 Related Work Viterbi training is closely related to the k-means clustering problem, where the objective is to find k centroids for a given set of d-dimensional points such that the sum of distances between the points and the closest centroid is minimized. The analog for Viterbi EM for the k-means problem is the k-means clustering algorithm (Lloyd, 1982), a coordinate ascent algorithm for solving the k-means problem. It works by iterating between an E-likestep, in which each point is assigned the closest centroid, and an M-like-step, in which the centroids a</context>
</contexts>
<marker>Smith, Johnson, 2007</marker>
<rawString>N. A. Smith and M. Johnson. 2007. Weighted and probabilistic context-free grammars are equally expressive. Computational Linguistics, 33(4):477– 491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Viterbi training improves unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="1689" citStr="Spitkovsky et al., 2010" startWordPosition="254" endWordPosition="257">re based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.” In this paper, we explore Viterbi training for probabilistic context-free grammars. We first show that under the assumption that P =� NP, solving and even approximating the Viterbi training problem is hard. This result holds even for hidden Markov models. We extend the main hardness result to the EM algorithm (giving an alternative proof to this known result), as well as the problem of conditional Viterbi training. We then describe a “competitive</context>
<context position="4794" citStr="Spitkovsky et al., 2010" startWordPosition="802" endWordPosition="805">) i=1 FA(z) = Xn fA(zi) (3) i=1 We use the following notation for G: • L(G) is the set of all strings (sentences) x that can be generated using the grammar G (the “language of G”). • D(G) is the set of all possible derivations z that can be generated using the grammar G. • D(G, x) is the set of all possible derivations z that can be generated using the grammar G and have the yield x. 3 Viterbi Training Viterbi EM, or “hard” EM, is an unsupervised learning algorithm, used in NLP in various settings (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). In the context of PCFGs, it aims to select parameters θ and phrasestructure trees z jointly. It does so by iteratively updating a state consisting of (θ, z). The state is initialized with some value, then the algorithm alternates between (i) a “hard” E-step, where the strings x1, ... , xn are parsed according to a current, fixed θ, giving new values for z, and (ii) an M-step, where the θ are selected to maximize likelihood, with z fixed. With PCFGs, the E-step requires running an algorithm such as (probabilistic) CKY or Earley’s 1Note that x = yield(z); if the derivation is known, the string</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, Manning, 2010</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning. 2010. Viterbi training improves unsupervised dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>K Maji</author>
</authors>
<title>Computational complexity of statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="30991" citStr="Udupa and Maji, 2006" startWordPosition="5758" endWordPosition="5761">pproximates the optimal clusters assignment by a factor of O(log k). In §7.1, we showed that uniform-at-random initialization is approximately O(|N|LA2/n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and A is a measure for distance between the true distribution and the uniform distribution.5 Many combinatorial problems in NLP involving phrase-structure trees, alignments, and dependency graphs are hard (Sima’an, 1996; Goodman, 1998; Knight, 1999; Casacuberta and de la Higuera, 2000; Lyngsø and Pederson, 2002; Udupa and Maji, 2006; McDonald and Satta, 2007; DeNero and Klein, 2008, inter alia). Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. Understanding the complexity of NLP problems, we believe, is crucial as we seek effective practical approximations when necessary. 9 Conclusion We described some properties of Viterbi training for probabilistic context-free grammars. We showed that Viterbi training is NP-hard and, in fact, NP-hard to approximate. W</context>
</contexts>
<marker>Udupa, Maji, 2006</marker>
<rawString>R. Udupa and K. Maji. 2006. Computational complexity of statistical machine translation. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>N A Smith</author>
<author>T Mitamura</author>
</authors>
<title>What is the Jeopardy model? a quasi-synchronous grammar for question answering.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1610" citStr="Wang et al., 2007" startWordPosition="242" endWordPosition="245">dels have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.” In this paper, we explore Viterbi training for probabilistic context-free grammars. We first show that under the assumption that P =� NP, solving and even approximating the Viterbi training problem is hard. This result holds even for hidden Markov models. We extend the main hardness result to the EM algorithm (giving an alternative proof to this known result), as well </context>
<context position="4715" citStr="Wang et al., 2007" startWordPosition="790" endWordPosition="793"> sample of derivations z = (z1, ... , zn), let: FA,α(z) = Xn fA,α(zi) (2) i=1 FA(z) = Xn fA(zi) (3) i=1 We use the following notation for G: • L(G) is the set of all strings (sentences) x that can be generated using the grammar G (the “language of G”). • D(G) is the set of all possible derivations z that can be generated using the grammar G. • D(G, x) is the set of all possible derivations z that can be generated using the grammar G and have the yield x. 3 Viterbi Training Viterbi EM, or “hard” EM, is an unsupervised learning algorithm, used in NLP in various settings (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). In the context of PCFGs, it aims to select parameters θ and phrasestructure trees z jointly. It does so by iteratively updating a state consisting of (θ, z). The state is initialized with some value, then the algorithm alternates between (i) a “hard” E-step, where the strings x1, ... , xn are parsed according to a current, fixed θ, giving new values for z, and (ii) an M-step, where the θ are selected to maximize likelihood, with z fixed. With PCFGs, the E-step requires running an algorithm such as (probabilistic) </context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>M. Wang, N. A. Smith, and T. Mitamura. 2007. What is the Jeopardy model? a quasi-synchronous grammar for question answering. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yejin</author>
<author>C Cardie</author>
</authors>
<title>Structured local training and biased potential functions for conditional random fields with application to coreference resolution.</title>
<date>2007</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="1591" citStr="Yejin and Cardie, 2007" startWordPosition="238" endWordPosition="241">hms for training such models have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.” In this paper, we explore Viterbi training for probabilistic context-free grammars. We first show that under the assumption that P =� NP, solving and even approximating the Viterbi training problem is hard. This result holds even for hidden Markov models. We extend the main hardness result to the EM algorithm (giving an alternative proof to this know</context>
<context position="4696" citStr="Yejin and Cardie, 2007" startWordPosition="786" endWordPosition="789"> A appears in z. Given a sample of derivations z = (z1, ... , zn), let: FA,α(z) = Xn fA,α(zi) (2) i=1 FA(z) = Xn fA(zi) (3) i=1 We use the following notation for G: • L(G) is the set of all strings (sentences) x that can be generated using the grammar G (the “language of G”). • D(G) is the set of all possible derivations z that can be generated using the grammar G. • D(G, x) is the set of all possible derivations z that can be generated using the grammar G and have the yield x. 3 Viterbi Training Viterbi EM, or “hard” EM, is an unsupervised learning algorithm, used in NLP in various settings (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). In the context of PCFGs, it aims to select parameters θ and phrasestructure trees z jointly. It does so by iteratively updating a state consisting of (θ, z). The state is initialized with some value, then the algorithm alternates between (i) a “hard” E-step, where the strings x1, ... , xn are parsed according to a current, fixed θ, giving new values for z, and (ii) an M-step, where the θ are selected to maximize likelihood, with z fixed. With PCFGs, the E-step requires running an algorithm such </context>
</contexts>
<marker>Yejin, Cardie, 2007</marker>
<rawString>C. Yejin and C. Cardie. 2007. Structured local training and biased potential functions for conditional random fields with application to coreference resolution. In Proc. of HLT-NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>