<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012791">
<title confidence="0.999543">
Pointwise Prediction for Robust, Adaptable
Japanese Morphological Analysis
</title>
<author confidence="0.99847">
Graham Neubig, Yosuke Nakata, Shinsuke Mori
</author>
<affiliation confidence="0.743416">
Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
</affiliation>
<sectionHeader confidence="0.978327" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99930625">
We present a pointwise approach to Japanese
morphological analysis (MA) that ignores
structure information during learning and tag-
ging. Despite the lack of structure, it is able to
outperform the current state-of-the-art struc-
tured approach for Japanese MA, and achieves
accuracy similar to that of structured predic-
tors using the same feature set. We also
find that the method is both robust to out-
of-domain data, and can be easily adapted
through the use of a combination of partial an-
notation and active learning.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999795555555556">
Japanese morphological analysis (MA) takes an un-
segmented string of Japanese text as input, and out-
puts a string of morphemes annotated with parts of
speech (POSs). As MA is the first step in Japanese
NLP, its accuracy directly affects the accuracy of
NLP systems as a whole. In addition, with the prolif-
eration of text in various domains, there is increasing
need for methods that are both robust and adaptable
to out-of-domain data (Escudero et al., 2000).
Previous approaches have used structured predic-
tors such as hidden Markov models (HMMs) or con-
ditional random fields (CRFs), which consider the
interactions between neighboring words and parts
of speech (Nagata, 1994; Asahara and Matsumoto,
2000; Kudo et al., 2004). However, while struc-
ture does provide valuable information, Liang et al.
(2008) have shown that gains provided by struc-
tured prediction can be largely recovered by using a
richer feature set. This approach has also been called
“pointwise” prediction, as it makes a single indepen-
dent decision at each point (Neubig and Mori, 2010).
While Liang et al. (2008) focus on the speed ben-
efits of pointwise prediction, we demonstrate that it
also allows for more robust and adaptable MA. We
find experimental evidence that pointwise MA can
exceed the accuracy of a state-of-the-art structured
approach (Kudo et al., 2004) on in-domain data, and
is significantly more robust to out-of-domain data.
We also show that pointwise MA can be adapted
to new domains with minimal effort through the
combination of active learning and partial annota-
tion (Tsuboi et al., 2008), where only informative
parts of a particular sentence are annotated. In a
realistic domain adaptation scenario, we find that a
combination of pointwise prediction, partial annota-
tion, and active learning allows for easy adaptation.
</bodyText>
<sectionHeader confidence="0.949031" genericHeader="method">
2 Japanese Morphological Analysis
</sectionHeader>
<bodyText confidence="0.999958363636364">
Japanese MA takes an unsegmented string of char-
acters xi as input, segments it into morphemes wi ,
and annotates each morpheme with a part of speech
ti . This can be formulated as a two-step process of
first segmenting words, then estimating POSs (Ng
and Low, 2004), or as a single joint process of find-
ing a morpheme/POS string from unsegmented text
(Kudo et al., 2004; Nakagawa, 2004; Kruengkrai et
al., 2009). In this section we describe an existing
joint sequence-based method for Japanese MA, as
well as our proposed two-step pointwise method.
</bodyText>
<subsectionHeader confidence="0.99729">
2.1 Joint Sequence-Based MA
</subsectionHeader>
<bodyText confidence="0.8692465">
Japanese MA has traditionally used sequence based
models, finding a maximal POS sequence for en-
</bodyText>
<figureCaption confidence="0.986706">
Figure 1: Joint MA (a) performs maximization over the
entire sequence, while two-step MA (b) maximizes the 4
boundary and 4 POS tags independently.
</figureCaption>
<table confidence="0.94554075">
Type Feature Strings
Unigram tj, tjwj, c(wj), tjc(wj)
Bigram tj−1tj, tj−1tjwj−1,
tj−1tjwj, tj−1tjwj−1wj
</table>
<tableCaption confidence="0.926591">
Table 1: Features for the joint model using tags t and
words w. c(·) is a mapping function onto character types
(kanji, katakana, etc.).
</tableCaption>
<bodyText confidence="0.981591952380952">
tire sentences as in Figure 1 (a). The CRF-based
method presented by Kudo et al. (2004) is gener-
ally accepted as the state-of-the-art in this paradigm.
CRFs are trained over segmentation lattices, which
allows for the handling of variable length sequences
that occur due to multiple segmentations. The model
is able to take into account arbitrary features, as well
as the context between neighboring tags.
We follow Kudo et al. (2004) in defining our fea-
ture set, as summarized in Table 11. Lexical features
were trained for the top 5000 most frequent words in
the corpus. It should be noted that these are word-
based features, and information about transitions be-
tween POS tags is included. When creating training
data, the use of word-based features indicates that
word boundaries must be annotated, while the use
of POS transition information further indicates that
all of these words must be annotated with POSs.
&apos;More fine-grained POS tags have provided small boosts in
accuracy in previous research (Kudo et al., 2004), but these in-
crease the annotation burden, which is contrary to our goal.
</bodyText>
<table confidence="0.999294333333333">
Type Feature Strings
Character xl, xr, xl−1xl, xlxr,
n-gram xrxr+1, xl−1xlxr, xlxrxr+1
Char. Type c(xl), c(xr)
n-gram c(xl−1xl), c(xlxr), c(xrxr+1)
c(xl−2xl−1xl), c(xl−1xlxr)
c(xlxrxr+1), c(xrxr+1xr+2)
WS Only ls, rs, is
POS Only wj, c(wj), djk
</table>
<tableCaption confidence="0.739433">
Table 2: Features for the two-step model. xl and xr indi-
</tableCaption>
<bodyText confidence="0.93578175">
cate the characters to the left and right of the word bound-
ary or word wj in question. ls, rs, and is represent the
left, right, and inside dictionary features, while djk indi-
cates that tag k exists in the dictionary for word j.
</bodyText>
<subsectionHeader confidence="0.999114">
2.2 2-Step Pointwise MA
</subsectionHeader>
<bodyText confidence="0.999850555555555">
In our research, we take a two-step approach, first
segmenting character sequence xI1 into the word se-
quence wJ1 with the highest probability, then tagging
each word with parts of speech tJ1. This approach is
shown in Figure 1 (b).
We follow Sassano (2002) in formulating word
segmentation as a binary classification problem, es-
timating boundary tags bI−1
1 . Tag bi = 1 indi-
cates that a word boundary exists between charac-
ters xi and xi+1, while bi = 0 indicates that a word
boundary does not exist. POS estimation can also
be formulated as a multi-class classification prob-
lem, where we choose one tag tj for each word wj.
These two classification problems can be solved by
tools in the standard machine learning toolbox such
as logistic regression (LR), support vector machines
(SVMs), or conditional random fields (CRFs).
We use information about the surrounding charac-
ters (character and character-type n-grams), as well
as the presence or absence of words in the dictio-
nary as features (Table 2). Specifically dictionary
features for word segmentation ls and rs are active
if a string of length s included in the dictionary is
present directly to the left or right of the present
word boundary, and is is active if the present word
boundary is included in a dictionary word of length
s. Dictionary feature djk for POS estimation indi-
cates whether the current word wj occurs as a dic-
tionary entry with tag tk.
Previous work using this two-stage approach has
used sequence-based prediction methods, such as
maximum entropy Markov models (MEMMs) or
CRFs (Ng and Low, 2004; Peng et al., 2004). How-
ever, as Liang et al. (2008) note, and we confirm,
sequence-based predictors are often not necessary
when an appropriately rich feature set is used. One
important difference between our formulation and
that of Liang et al. (2008) and all other previous
methods is that we rely only on features that are di-
rectly calculable from the surface string, without us-
ing estimated information such as word boundaries
or neighboring POS tags2. This allows for training
from sentences that are partially annotated as de-
scribed in the following section.
</bodyText>
<sectionHeader confidence="0.977017" genericHeader="method">
3 Domain Adaptation for Morphological
Analysis
</sectionHeader>
<bodyText confidence="0.999741888888889">
NLP is now being used in domains such as medi-
cal text and legal documents, and it is necessary that
MA be easily adaptable to these areas. In a domain
adaptation situation, we have at our disposal both
annotated general domain data, and unannotated tar-
get domain data. We would like to annotate the
target domain data efficiently to achieve a maximal
gain in accuracy for a minimal amount of work.
Active learning has been used as a way to pick
data that is useful to annotate in this scenario for
several applications (Chan and Ng, 2007; Rai et
al., 2010) so we adopt an active-learning-based ap-
proach here. When adapting sequence-based predic-
tion methods, most active learning approaches have
focused on picking full sentences that are valuable to
annotate (Ringger et al., 2007; Settles and Craven,
2008). However, even within sentences, there are
generally a few points of interest surrounded by
large segments that are well covered by already an-
notated data.
Partial annotation provides a solution to this prob-
lem (Tsuboi et al., 2008; Sassano and Kurohashi,
2010). In partial annotation, data that will not con-
tribute to the improvement of the classifier is left
untagged. For example, if there is a single difficult
word in a long sentence, only the word boundaries
and POS of the difficult word will be tagged. “Dif-
</bodyText>
<footnote confidence="0.781619333333333">
2Dictionary features are active if the string exists, regardless
of whether itis treated as a single word in wJ1 , and thus can be
calculated without the word segmentation result.
</footnote>
<table confidence="0.684121666666667">
Type Train Test
782k 87.5k
153k 17.3k
</table>
<tableCaption confidence="0.99875">
Table 3: General and target domain corpus sizes in words.
</tableCaption>
<bodyText confidence="0.999958619047619">
ficult” words can be selected using active learning
approaches, choosing words with the lowest classi-
fier accuracy to annotate. In addition, corpora that
are tagged with word boundaries but not POS tags
are often available; this is another type of partial an-
notation.
When using sequence-based prediction, learning
on partially annotated data is not straightforward,
as the data that must be used to train context-based
transition probabilities may be left unannotated. In
contrast, in the pointwise prediction framework,
training using this data is both simple and efficient;
unannotated points are simply ignored. A method
for learning CRFs from partially annotated data has
been presented by Tsuboi et al. (2008). However,
when using partial annotation, CRFs’ already slow
training time becomes slower still, as they must be
trained over every sequence that has at least one an-
notated point. Training time is important in an active
learning situation, as an annotator must wait while
the model is being re-trained.
</bodyText>
<sectionHeader confidence="0.99965" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99978">
In order to test the effectiveness of pointwise MA,
we did an experiment measuring accuracy both on
in-domain data, and in a domain-adaptation situa-
tion. We used the Balanced Corpus of Contempo-
rary Written Japanese (BCCWJ) (Maekawa, 2008),
specifying the whitepaper, news, and books sections
as our general domain corpus, and the web text sec-
tion as our target domain corpus (Table 3).
As a representative of joint sequence-based MA
described in 2.1, we used MeCab (Kudo, 2006), an
open source implementation of Kudo et al. (2004)’s
CRF-based method (we will call this JOINT). For the
pointwise two-step method, we trained logistic re-
gression models with the LIBLINEAR toolkit (Fan
et al., 2008) using the features described in Section
2.2 (2-LR). In addition, we trained a CRF-based
model with the CRFSuite toolkit (Okazaki, 2007)
using the same features and set-up (for both word
</bodyText>
<table confidence="0.975523166666667">
General
Target
Train Test JOINT 2-CRF 2-LR
GEN GEN 97.31% 98.08% 98.03%
GEN TAR 94.57% 95.39% 95.13%
GEN+TAR TAR 96.45% 96.91% 96.82%
</table>
<tableCaption confidence="0.997099333333333">
Table 4: Word/POS F-measure for each method when
trained and tested on general (GEN) or target (TAR) do-
main corpora.
</tableCaption>
<bodyText confidence="0.996506222222222">
segmentation and POS tagging) to examine the con-
tribution of context information (2-CRF).
To create the dictionary, we added all of the words
in the corpus, but left out a small portion of single-
tons to prevent overfitting on the training data3. As
an evaluation measure, we follow Nagata (1994) and
Kudo et al. (2004) and use Word/POS tag pair F-
measure, so that both word boundaries and POS tags
must be correct for a word to be considered correct.
</bodyText>
<subsectionHeader confidence="0.999315">
4.1 Analysis Results
</subsectionHeader>
<bodyText confidence="0.999942875">
In our first experiment we compared the accuracy of
the three methods on both the in-domain and out-
of-domain test sets (Table 4). It can be seen that
2-LR outperforms JOINT, and achieves similar but
slightly inferior results to 2-CRF. The reason for
accuracy gains over JOINT lies largely in the fact
that while JOINT is more reliant on the dictionary,
and thus tends to mis-segment unknown words, the
two-step methods are significantly more robust. The
small difference between 2-LR and 2-CRF indicates
that given a significantly rich feature set, context-
based features provide little advantage, although the
advantage is larger on out-of-domain data. In addi-
tion, training of 2-LR is significantly faster than 2-
CRF. 2-LR took 16m44s to train, while 2-CRF took
51m19s to train on a 3.33GHz Intel Xeon CPU.
</bodyText>
<subsectionHeader confidence="0.994171">
4.2 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.9999212">
Our second experiment focused on the domain
adaptability of each method. Using the target do-
main training corpus as a pool of unannotated data,
we performed active learning-based domain adapta-
tion using two techniques.
</bodyText>
<listItem confidence="0.874264">
• Sentence-based annotation (SENT), where sen-
tences with the lowest total POS and word
</listItem>
<footnote confidence="0.993645">
3For JOINT we removed singletons randomly until coverage
was 99.99%, and for 2-LR and 2-CRF coverage was set to 99%,
which gave the best results on held-out data.
</footnote>
<figureCaption confidence="0.999885">
Figure 2: Domain adaptation results for three approaches
and two annotation methods.
</figureCaption>
<bodyText confidence="0.992744">
boundary probabilities were annotated first.
</bodyText>
<listItem confidence="0.844074">
• Word-based partial annotation (PART), where
</listItem>
<bodyText confidence="0.954659333333333">
the word or word boundary with the smallest
probability margin between the first and second
candidates was chosen. This can only be used
with the pointwise 2-LR approach4 .
For both methods, 100 words (or for SENT until
the end of the sentence in which the 100th word
is reached) are annotated, then the classifier is re-
trained and new probability scores are generated.
Each set of 100 words is a single iteration, and 100
iterations were performed for each method.
From the results in Figure 2, it can be seen that
the combination of PART and 2-LR allows for sig-
nificantly faster adaptation than other approaches,
achieving accuracy gains in 15 iterations that are
achieved in 100 iterations with SENT, and surpassing
2-CRF after 15 iterations. Finally, it can be seen that
JOINT improves at a pace similar to PART, likely due
to the fact that its pre-adaptation accuracy is lower
than the other methods. It can be seen from Table 4
that even after adaptation with the full corpus, it will
still lag behind the two-step methods.
</bodyText>
<sectionHeader confidence="0.99555" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.913823545454545">
This paper proposed a pointwise approach to
Japanese morphological analysis. It showed that de-
spite the lack of structure, it was able to achieve re-
4In order to prevent wasteful annotation, each unique word
was only annotated once per iteration.
sults that meet or exceed structured prediction meth-
ods. We also demonstrated that it is both robust and
adaptable to out-of-domain text through the use of
partial annotation and active learning. Future work
in this area will include examination of performance
on other tasks and languages.
</bodyText>
<sectionHeader confidence="0.9983" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999013606382979">
Masayuki Asahara and Yuji Matsumoto. 2000. Extended
models and tools for high-performance part-of-speech
tagger. In Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 21–27.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics.
Gerard Escudero, Llu´ıs M`arquez, and German Rigau.
2000. An empirical study of the domain dependence
of supervised word sense disambiguation systems. In
Proceedings of the 2000 Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of the 47th Annual Meeting of
the Association for Computational Linguistics.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 230–237.
Taku Kudo. 2006. MeCab: yet another
part-of-speech and morphological analyzer.
http://mecab.sourceforge.net.
Percy Liang, Hal Daum´e III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th International Conference
on Machine Learning, pages 592–599.
Kikuo Maekawa. 2008. Balanced corpus of contempo-
rary written Japanese. In Proceedings of the 6th Work-
shop on Asian Language Resources, pages 101–102.
Masaaki Nagata. 1994. A stochastic Japanese morpho-
logical analyzer using a forward-DP backward-A* N-
best search algorithm. In Proceedings of the 15th In-
ternational Conference on Computational Linguistics,
pages 201–207.
Tetsuji Nakagawa. 2004. Chinese and Japanese word
segmentation using word-level and character-level in-
formation. In Proceedings of the 20th International
Conference on Computational Linguistics.
Graham Neubig and Shinsuke Mori. 2010. Word-based
partial annotation for efficient corpus construction. In
Proceedings of the 7th International Conference on
Language Resources and Evaluation.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: one-at-a-time or all-at-once? word-
based or character-based. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Naoaki Okazaki. 2007. CRFsuite: a fast im-
plementation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of the
20th International Conference on Computational Lin-
guistics.
Piyush Rai, Avishek Saha, Hal Daum´e III, and Suresh
Venkatasubramanian. 2010. Domain Adaptation
meets Active Learning. In Workshop on Active Learn-
ing for Natural Language Processing (ALNLP-10).
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop,
pages 101–108.
Manabu. Sassano and Sadao Kurohashi. 2010. Us-
ing smaller constituents rather than sentences in ac-
tive learning for Japanese dependency parsing. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 356–365.
Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for Japanese
word segmentation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 505–512.
Burr Settles and Mark Craven. 2008. An analysis of
active learning strategies for sequence labeling tasks.
In Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1070–1079.
Yuta Tsuboi, Hisashi Kashima, Hiroki Oda, Shinsuke
Mori, and Yuji Matsumoto. 2008. Training condi-
tional random fields using incomplete annotations. In
Proceedings of the 22th International Conference on
Computational Linguistics, pages 897–904.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.709575">
<title confidence="0.998563">Pointwise Prediction for Robust, Japanese Morphological Analysis</title>
<author confidence="0.995369">Graham Neubig</author>
<author confidence="0.995369">Yosuke Nakata</author>
<author confidence="0.995369">Shinsuke</author>
<affiliation confidence="0.930904">Graduate School of Informatics, Kyoto</affiliation>
<address confidence="0.757827">Yoshida Honmachi, Sakyo-ku, Kyoto, Japan</address>
<abstract confidence="0.995811230769231">We present a pointwise approach to Japanese morphological analysis (MA) that ignores structure information during learning and tagging. Despite the lack of structure, it is able to outperform the current state-of-the-art structured approach for Japanese MA, and achieves accuracy similar to that of structured predictors using the same feature set. We also find that the method is both robust to outof-domain data, and can be easily adapted through the use of a combination of partial annotation and active learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extended models and tools for high-performance part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>21--27</pages>
<contexts>
<context position="1454" citStr="Asahara and Matsumoto, 2000" startWordPosition="222" endWordPosition="225">text as input, and outputs a string of morphemes annotated with parts of speech (POSs). As MA is the first step in Japanese NLP, its accuracy directly affects the accuracy of NLP systems as a whole. In addition, with the proliferation of text in various domains, there is increasing need for methods that are both robust and adaptable to out-of-domain data (Escudero et al., 2000). Previous approaches have used structured predictors such as hidden Markov models (HMMs) or conditional random fields (CRFs), which consider the interactions between neighboring words and parts of speech (Nagata, 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). However, while structure does provide valuable information, Liang et al. (2008) have shown that gains provided by structured prediction can be largely recovered by using a richer feature set. This approach has also been called “pointwise” prediction, as it makes a single independent decision at each point (Neubig and Mori, 2010). While Liang et al. (2008) focus on the speed benefits of pointwise prediction, we demonstrate that it also allows for more robust and adaptable MA. We find experimental evidence that pointwise MA can exceed the accuracy of a state-of-the-art stru</context>
</contexts>
<marker>Asahara, Matsumoto, 2000</marker>
<rawString>Masayuki Asahara and Yuji Matsumoto. 2000. Extended models and tools for high-performance part-of-speech tagger. In Proceedings of the 18th International Conference on Computational Linguistics, pages 21–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Domain adaptation with active learning for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8039" citStr="Chan and Ng, 2007" startWordPosition="1315" endWordPosition="1318">cribed in the following section. 3 Domain Adaptation for Morphological Analysis NLP is now being used in domains such as medical text and legal documents, and it is necessary that MA be easily adaptable to these areas. In a domain adaptation situation, we have at our disposal both annotated general domain data, and unannotated target domain data. We would like to annotate the target domain data efficiently to achieve a maximal gain in accuracy for a minimal amount of work. Active learning has been used as a way to pick data that is useful to annotate in this scenario for several applications (Chan and Ng, 2007; Rai et al., 2010) so we adopt an active-learning-based approach here. When adapting sequence-based prediction methods, most active learning approaches have focused on picking full sentences that are valuable to annotate (Ringger et al., 2007; Settles and Craven, 2008). However, even within sentences, there are generally a few points of interest surrounded by large segments that are well covered by already annotated data. Partial annotation provides a solution to this problem (Tsuboi et al., 2008; Sassano and Kurohashi, 2010). In partial annotation, data that will not contribute to the improv</context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation with active learning for word sense disambiguation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Escudero</author>
<author>Llu´ıs M`arquez</author>
<author>German Rigau</author>
</authors>
<title>An empirical study of the domain dependence of supervised word sense disambiguation systems.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<marker>Escudero, M`arquez, Rigau, 2000</marker>
<rawString>Gerard Escudero, Llu´ıs M`arquez, and German Rigau. 2000. An empirical study of the domain dependence of supervised word sense disambiguation systems. In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="10828" citStr="Fan et al., 2008" startWordPosition="1763" endWordPosition="1766">ring accuracy both on in-domain data, and in a domain-adaptation situation. We used the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa, 2008), specifying the whitepaper, news, and books sections as our general domain corpus, and the web text section as our target domain corpus (Table 3). As a representative of joint sequence-based MA described in 2.1, we used MeCab (Kudo, 2006), an open source implementation of Kudo et al. (2004)’s CRF-based method (we will call this JOINT). For the pointwise two-step method, we trained logistic regression models with the LIBLINEAR toolkit (Fan et al., 2008) using the features described in Section 2.2 (2-LR). In addition, we trained a CRF-based model with the CRFSuite toolkit (Okazaki, 2007) using the same features and set-up (for both word General Target Train Test JOINT 2-CRF 2-LR GEN GEN 97.31% 98.08% 98.03% GEN TAR 94.57% 95.39% 95.13% GEN+TAR TAR 96.45% 96.91% 96.82% Table 4: Word/POS F-measure for each method when trained and tested on general (GEN) or target (TAR) domain corpora. segmentation and POS tagging) to examine the contribution of context information (2-CRF). To create the dictionary, we added all of the words in the corpus, but l</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3012" citStr="Kruengkrai et al., 2009" startWordPosition="477" endWordPosition="480"> are annotated. In a realistic domain adaptation scenario, we find that a combination of pointwise prediction, partial annotation, and active learning allows for easy adaptation. 2 Japanese Morphological Analysis Japanese MA takes an unsegmented string of characters xi as input, segments it into morphemes wi , and annotates each morpheme with a part of speech ti . This can be formulated as a two-step process of first segmenting words, then estimating POSs (Ng and Low, 2004), or as a single joint process of finding a morpheme/POS string from unsegmented text (Kudo et al., 2004; Nakagawa, 2004; Kruengkrai et al., 2009). In this section we describe an existing joint sequence-based method for Japanese MA, as well as our proposed two-step pointwise method. 2.1 Joint Sequence-Based MA Japanese MA has traditionally used sequence based models, finding a maximal POS sequence for enFigure 1: Joint MA (a) performs maximization over the entire sequence, while two-step MA (b) maximizes the 4 boundary and 4 POS tags independently. Type Feature Strings Unigram tj, tjwj, c(wj), tjc(wj) Bigram tj−1tj, tj−1tjwj−1, tj−1tjwj, tj−1tjwj−1wj Table 1: Features for the joint model using tags t and words w. c(·) is a mapping funct</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying conditional random fields to Japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>230--237</pages>
<contexts>
<context position="1474" citStr="Kudo et al., 2004" startWordPosition="226" endWordPosition="229">string of morphemes annotated with parts of speech (POSs). As MA is the first step in Japanese NLP, its accuracy directly affects the accuracy of NLP systems as a whole. In addition, with the proliferation of text in various domains, there is increasing need for methods that are both robust and adaptable to out-of-domain data (Escudero et al., 2000). Previous approaches have used structured predictors such as hidden Markov models (HMMs) or conditional random fields (CRFs), which consider the interactions between neighboring words and parts of speech (Nagata, 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). However, while structure does provide valuable information, Liang et al. (2008) have shown that gains provided by structured prediction can be largely recovered by using a richer feature set. This approach has also been called “pointwise” prediction, as it makes a single independent decision at each point (Neubig and Mori, 2010). While Liang et al. (2008) focus on the speed benefits of pointwise prediction, we demonstrate that it also allows for more robust and adaptable MA. We find experimental evidence that pointwise MA can exceed the accuracy of a state-of-the-art structured approach (Kud</context>
<context position="2970" citStr="Kudo et al., 2004" startWordPosition="471" endWordPosition="474">tive parts of a particular sentence are annotated. In a realistic domain adaptation scenario, we find that a combination of pointwise prediction, partial annotation, and active learning allows for easy adaptation. 2 Japanese Morphological Analysis Japanese MA takes an unsegmented string of characters xi as input, segments it into morphemes wi , and annotates each morpheme with a part of speech ti . This can be formulated as a two-step process of first segmenting words, then estimating POSs (Ng and Low, 2004), or as a single joint process of finding a morpheme/POS string from unsegmented text (Kudo et al., 2004; Nakagawa, 2004; Kruengkrai et al., 2009). In this section we describe an existing joint sequence-based method for Japanese MA, as well as our proposed two-step pointwise method. 2.1 Joint Sequence-Based MA Japanese MA has traditionally used sequence based models, finding a maximal POS sequence for enFigure 1: Joint MA (a) performs maximization over the entire sequence, while two-step MA (b) maximizes the 4 boundary and 4 POS tags independently. Type Feature Strings Unigram tj, tjwj, c(wj), tjc(wj) Bigram tj−1tj, tj−1tjwj−1, tj−1tjwj, tj−1tjwj−1wj Table 1: Features for the joint model using t</context>
<context position="4685" citStr="Kudo et al., 2004" startWordPosition="746" endWordPosition="749"> We follow Kudo et al. (2004) in defining our feature set, as summarized in Table 11. Lexical features were trained for the top 5000 most frequent words in the corpus. It should be noted that these are wordbased features, and information about transitions between POS tags is included. When creating training data, the use of word-based features indicates that word boundaries must be annotated, while the use of POS transition information further indicates that all of these words must be annotated with POSs. &apos;More fine-grained POS tags have provided small boosts in accuracy in previous research (Kudo et al., 2004), but these increase the annotation burden, which is contrary to our goal. Type Feature Strings Character xl, xr, xl−1xl, xlxr, n-gram xrxr+1, xl−1xlxr, xlxrxr+1 Char. Type c(xl), c(xr) n-gram c(xl−1xl), c(xlxr), c(xrxr+1) c(xl−2xl−1xl), c(xl−1xlxr) c(xlxrxr+1), c(xrxr+1xr+2) WS Only ls, rs, is POS Only wj, c(wj), djk Table 2: Features for the two-step model. xl and xr indicate the characters to the left and right of the word boundary or word wj in question. ls, rs, and is represent the left, right, and inside dictionary features, while djk indicates that tag k exists in the dictionary for wor</context>
<context position="10663" citStr="Kudo et al. (2004)" startWordPosition="1737" endWordPosition="1740">situation, as an annotator must wait while the model is being re-trained. 4 Experiments In order to test the effectiveness of pointwise MA, we did an experiment measuring accuracy both on in-domain data, and in a domain-adaptation situation. We used the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa, 2008), specifying the whitepaper, news, and books sections as our general domain corpus, and the web text section as our target domain corpus (Table 3). As a representative of joint sequence-based MA described in 2.1, we used MeCab (Kudo, 2006), an open source implementation of Kudo et al. (2004)’s CRF-based method (we will call this JOINT). For the pointwise two-step method, we trained logistic regression models with the LIBLINEAR toolkit (Fan et al., 2008) using the features described in Section 2.2 (2-LR). In addition, we trained a CRF-based model with the CRFSuite toolkit (Okazaki, 2007) using the same features and set-up (for both word General Target Train Test JOINT 2-CRF 2-LR GEN GEN 97.31% 98.08% 98.03% GEN TAR 94.57% 95.39% 95.13% GEN+TAR TAR 96.45% 96.91% 96.82% Table 4: Word/POS F-measure for each method when trained and tested on general (GEN) or target (TAR) domain corpor</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
</authors>
<title>MeCab: yet another part-of-speech and morphological analyzer.</title>
<date>2006</date>
<note>http://mecab.sourceforge.net.</note>
<contexts>
<context position="10610" citStr="Kudo, 2006" startWordPosition="1730" endWordPosition="1731">ining time is important in an active learning situation, as an annotator must wait while the model is being re-trained. 4 Experiments In order to test the effectiveness of pointwise MA, we did an experiment measuring accuracy both on in-domain data, and in a domain-adaptation situation. We used the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa, 2008), specifying the whitepaper, news, and books sections as our general domain corpus, and the web text section as our target domain corpus (Table 3). As a representative of joint sequence-based MA described in 2.1, we used MeCab (Kudo, 2006), an open source implementation of Kudo et al. (2004)’s CRF-based method (we will call this JOINT). For the pointwise two-step method, we trained logistic regression models with the LIBLINEAR toolkit (Fan et al., 2008) using the features described in Section 2.2 (2-LR). In addition, we trained a CRF-based model with the CRFSuite toolkit (Okazaki, 2007) using the same features and set-up (for both word General Target Train Test JOINT 2-CRF 2-LR GEN GEN 97.31% 98.08% 98.03% GEN TAR 94.57% 95.39% 95.13% GEN+TAR TAR 96.45% 96.91% 96.82% Table 4: Word/POS F-measure for each method when trained and </context>
</contexts>
<marker>Kudo, 2006</marker>
<rawString>Taku Kudo. 2006. MeCab: yet another part-of-speech and morphological analyzer. http://mecab.sourceforge.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Hal Daum´e</author>
<author>Dan Klein</author>
</authors>
<title>Structure compilation: trading structure for features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>592--599</pages>
<marker>Liang, Daum´e, Klein, 2008</marker>
<rawString>Percy Liang, Hal Daum´e III, and Dan Klein. 2008. Structure compilation: trading structure for features. In Proceedings of the 25th International Conference on Machine Learning, pages 592–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kikuo Maekawa</author>
</authors>
<title>Balanced corpus of contemporary written Japanese.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th Workshop on Asian Language Resources,</booktitle>
<pages>101--102</pages>
<contexts>
<context position="10371" citStr="Maekawa, 2008" startWordPosition="1690" endWordPosition="1691">y annotated data has been presented by Tsuboi et al. (2008). However, when using partial annotation, CRFs’ already slow training time becomes slower still, as they must be trained over every sequence that has at least one annotated point. Training time is important in an active learning situation, as an annotator must wait while the model is being re-trained. 4 Experiments In order to test the effectiveness of pointwise MA, we did an experiment measuring accuracy both on in-domain data, and in a domain-adaptation situation. We used the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa, 2008), specifying the whitepaper, news, and books sections as our general domain corpus, and the web text section as our target domain corpus (Table 3). As a representative of joint sequence-based MA described in 2.1, we used MeCab (Kudo, 2006), an open source implementation of Kudo et al. (2004)’s CRF-based method (we will call this JOINT). For the pointwise two-step method, we trained logistic regression models with the LIBLINEAR toolkit (Fan et al., 2008) using the features described in Section 2.2 (2-LR). In addition, we trained a CRF-based model with the CRFSuite toolkit (Okazaki, 2007) using </context>
</contexts>
<marker>Maekawa, 2008</marker>
<rawString>Kikuo Maekawa. 2008. Balanced corpus of contemporary written Japanese. In Proceedings of the 6th Workshop on Asian Language Resources, pages 101–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A stochastic Japanese morphological analyzer using a forward-DP backward-A* Nbest search algorithm.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>201--207</pages>
<contexts>
<context position="1425" citStr="Nagata, 1994" startWordPosition="220" endWordPosition="221">g of Japanese text as input, and outputs a string of morphemes annotated with parts of speech (POSs). As MA is the first step in Japanese NLP, its accuracy directly affects the accuracy of NLP systems as a whole. In addition, with the proliferation of text in various domains, there is increasing need for methods that are both robust and adaptable to out-of-domain data (Escudero et al., 2000). Previous approaches have used structured predictors such as hidden Markov models (HMMs) or conditional random fields (CRFs), which consider the interactions between neighboring words and parts of speech (Nagata, 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). However, while structure does provide valuable information, Liang et al. (2008) have shown that gains provided by structured prediction can be largely recovered by using a richer feature set. This approach has also been called “pointwise” prediction, as it makes a single independent decision at each point (Neubig and Mori, 2010). While Liang et al. (2008) focus on the speed benefits of pointwise prediction, we demonstrate that it also allows for more robust and adaptable MA. We find experimental evidence that pointwise MA can exceed the accura</context>
<context position="11561" citStr="Nagata (1994)" startWordPosition="1887" endWordPosition="1888"> (Okazaki, 2007) using the same features and set-up (for both word General Target Train Test JOINT 2-CRF 2-LR GEN GEN 97.31% 98.08% 98.03% GEN TAR 94.57% 95.39% 95.13% GEN+TAR TAR 96.45% 96.91% 96.82% Table 4: Word/POS F-measure for each method when trained and tested on general (GEN) or target (TAR) domain corpora. segmentation and POS tagging) to examine the contribution of context information (2-CRF). To create the dictionary, we added all of the words in the corpus, but left out a small portion of singletons to prevent overfitting on the training data3. As an evaluation measure, we follow Nagata (1994) and Kudo et al. (2004) and use Word/POS tag pair Fmeasure, so that both word boundaries and POS tags must be correct for a word to be considered correct. 4.1 Analysis Results In our first experiment we compared the accuracy of the three methods on both the in-domain and outof-domain test sets (Table 4). It can be seen that 2-LR outperforms JOINT, and achieves similar but slightly inferior results to 2-CRF. The reason for accuracy gains over JOINT lies largely in the fact that while JOINT is more reliant on the dictionary, and thus tends to mis-segment unknown words, the two-step methods are s</context>
</contexts>
<marker>Nagata, 1994</marker>
<rawString>Masaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-DP backward-A* Nbest search algorithm. In Proceedings of the 15th International Conference on Computational Linguistics, pages 201–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
</authors>
<title>Chinese and Japanese word segmentation using word-level and character-level information.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="2986" citStr="Nakagawa, 2004" startWordPosition="475" endWordPosition="476">ticular sentence are annotated. In a realistic domain adaptation scenario, we find that a combination of pointwise prediction, partial annotation, and active learning allows for easy adaptation. 2 Japanese Morphological Analysis Japanese MA takes an unsegmented string of characters xi as input, segments it into morphemes wi , and annotates each morpheme with a part of speech ti . This can be formulated as a two-step process of first segmenting words, then estimating POSs (Ng and Low, 2004), or as a single joint process of finding a morpheme/POS string from unsegmented text (Kudo et al., 2004; Nakagawa, 2004; Kruengkrai et al., 2009). In this section we describe an existing joint sequence-based method for Japanese MA, as well as our proposed two-step pointwise method. 2.1 Joint Sequence-Based MA Japanese MA has traditionally used sequence based models, finding a maximal POS sequence for enFigure 1: Joint MA (a) performs maximization over the entire sequence, while two-step MA (b) maximizes the 4 boundary and 4 POS tags independently. Type Feature Strings Unigram tj, tjwj, c(wj), tjc(wj) Bigram tj−1tj, tj−1tjwj−1, tj−1tjwj, tj−1tjwj−1wj Table 1: Features for the joint model using tags t and words </context>
</contexts>
<marker>Nakagawa, 2004</marker>
<rawString>Tetsuji Nakagawa. 2004. Chinese and Japanese word segmentation using word-level and character-level information. In Proceedings of the 20th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Shinsuke Mori</author>
</authors>
<title>Word-based partial annotation for efficient corpus construction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="1806" citStr="Neubig and Mori, 2010" startWordPosition="280" endWordPosition="283">(Escudero et al., 2000). Previous approaches have used structured predictors such as hidden Markov models (HMMs) or conditional random fields (CRFs), which consider the interactions between neighboring words and parts of speech (Nagata, 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). However, while structure does provide valuable information, Liang et al. (2008) have shown that gains provided by structured prediction can be largely recovered by using a richer feature set. This approach has also been called “pointwise” prediction, as it makes a single independent decision at each point (Neubig and Mori, 2010). While Liang et al. (2008) focus on the speed benefits of pointwise prediction, we demonstrate that it also allows for more robust and adaptable MA. We find experimental evidence that pointwise MA can exceed the accuracy of a state-of-the-art structured approach (Kudo et al., 2004) on in-domain data, and is significantly more robust to out-of-domain data. We also show that pointwise MA can be adapted to new domains with minimal effort through the combination of active learning and partial annotation (Tsuboi et al., 2008), where only informative parts of a particular sentence are annotated. In</context>
</contexts>
<marker>Neubig, Mori, 2010</marker>
<rawString>Graham Neubig and Shinsuke Mori. 2010. Word-based partial annotation for efficient corpus construction. In Proceedings of the 7th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese part-ofspeech tagging: one-at-a-time or all-at-once? wordbased or character-based.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2866" citStr="Ng and Low, 2004" startWordPosition="452" endWordPosition="455">ough the combination of active learning and partial annotation (Tsuboi et al., 2008), where only informative parts of a particular sentence are annotated. In a realistic domain adaptation scenario, we find that a combination of pointwise prediction, partial annotation, and active learning allows for easy adaptation. 2 Japanese Morphological Analysis Japanese MA takes an unsegmented string of characters xi as input, segments it into morphemes wi , and annotates each morpheme with a part of speech ti . This can be formulated as a two-step process of first segmenting words, then estimating POSs (Ng and Low, 2004), or as a single joint process of finding a morpheme/POS string from unsegmented text (Kudo et al., 2004; Nakagawa, 2004; Kruengkrai et al., 2009). In this section we describe an existing joint sequence-based method for Japanese MA, as well as our proposed two-step pointwise method. 2.1 Joint Sequence-Based MA Japanese MA has traditionally used sequence based models, finding a maximal POS sequence for enFigure 1: Joint MA (a) performs maximization over the entire sequence, while two-step MA (b) maximizes the 4 boundary and 4 POS tags independently. Type Feature Strings Unigram tj, tjwj, c(wj),</context>
<context position="6888" citStr="Ng and Low, 2004" startWordPosition="1119" endWordPosition="1122">rds in the dictionary as features (Table 2). Specifically dictionary features for word segmentation ls and rs are active if a string of length s included in the dictionary is present directly to the left or right of the present word boundary, and is is active if the present word boundary is included in a dictionary word of length s. Dictionary feature djk for POS estimation indicates whether the current word wj occurs as a dictionary entry with tag tk. Previous work using this two-stage approach has used sequence-based prediction methods, such as maximum entropy Markov models (MEMMs) or CRFs (Ng and Low, 2004; Peng et al., 2004). However, as Liang et al. (2008) note, and we confirm, sequence-based predictors are often not necessary when an appropriately rich feature set is used. One important difference between our formulation and that of Liang et al. (2008) and all other previous methods is that we rely only on features that are directly calculable from the surface string, without using estimated information such as word boundaries or neighboring POS tags2. This allows for training from sentences that are partially annotated as described in the following section. 3 Domain Adaptation for Morpholog</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-ofspeech tagging: one-at-a-time or all-at-once? wordbased or character-based. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>CRFsuite: a fast implementation of conditional random fields</title>
<date>2007</date>
<contexts>
<context position="10964" citStr="Okazaki, 2007" startWordPosition="1786" endWordPosition="1787">CCWJ) (Maekawa, 2008), specifying the whitepaper, news, and books sections as our general domain corpus, and the web text section as our target domain corpus (Table 3). As a representative of joint sequence-based MA described in 2.1, we used MeCab (Kudo, 2006), an open source implementation of Kudo et al. (2004)’s CRF-based method (we will call this JOINT). For the pointwise two-step method, we trained logistic regression models with the LIBLINEAR toolkit (Fan et al., 2008) using the features described in Section 2.2 (2-LR). In addition, we trained a CRF-based model with the CRFSuite toolkit (Okazaki, 2007) using the same features and set-up (for both word General Target Train Test JOINT 2-CRF 2-LR GEN GEN 97.31% 98.08% 98.03% GEN TAR 94.57% 95.39% 95.13% GEN+TAR TAR 96.45% 96.91% 96.82% Table 4: Word/POS F-measure for each method when trained and tested on general (GEN) or target (TAR) domain corpora. segmentation and POS tagging) to examine the contribution of context information (2-CRF). To create the dictionary, we added all of the words in the corpus, but left out a small portion of singletons to prevent overfitting on the training data3. As an evaluation measure, we follow Nagata (1994) an</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>Naoaki Okazaki. 2007. CRFsuite: a fast implementation of conditional random fields (CRFs). http://www.chokkan.org/software/crfsuite/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="6908" citStr="Peng et al., 2004" startWordPosition="1123" endWordPosition="1126">ary as features (Table 2). Specifically dictionary features for word segmentation ls and rs are active if a string of length s included in the dictionary is present directly to the left or right of the present word boundary, and is is active if the present word boundary is included in a dictionary word of length s. Dictionary feature djk for POS estimation indicates whether the current word wj occurs as a dictionary entry with tag tk. Previous work using this two-stage approach has used sequence-based prediction methods, such as maximum entropy Markov models (MEMMs) or CRFs (Ng and Low, 2004; Peng et al., 2004). However, as Liang et al. (2008) note, and we confirm, sequence-based predictors are often not necessary when an appropriately rich feature set is used. One important difference between our formulation and that of Liang et al. (2008) and all other previous methods is that we rely only on features that are directly calculable from the surface string, without using estimated information such as word boundaries or neighboring POS tags2. This allows for training from sentences that are partially annotated as described in the following section. 3 Domain Adaptation for Morphological Analysis NLP is</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proceedings of the 20th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piyush Rai</author>
<author>Avishek Saha</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Domain Adaptation meets Active Learning.</title>
<date>2010</date>
<booktitle>In Workshop on Active Learning for Natural Language Processing (ALNLP-10).</booktitle>
<marker>Rai, Saha, Daum´e, Venkatasubramanian, 2010</marker>
<rawString>Piyush Rai, Avishek Saha, Hal Daum´e III, and Suresh Venkatasubramanian. 2010. Domain Adaptation meets Active Learning. In Workshop on Active Learning for Natural Language Processing (ALNLP-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Ringger</author>
<author>Peter McClanahan</author>
<author>Robbie Haertel</author>
<author>George Busby</author>
<author>Marc Carmen</author>
<author>James Carroll</author>
<author>Kevin Seppi</author>
<author>Deryle Lonsdale</author>
</authors>
<title>Active learning for part-ofspeech tagging: Accelerating corpus annotation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop,</booktitle>
<pages>101--108</pages>
<contexts>
<context position="8282" citStr="Ringger et al., 2007" startWordPosition="1352" endWordPosition="1355">n situation, we have at our disposal both annotated general domain data, and unannotated target domain data. We would like to annotate the target domain data efficiently to achieve a maximal gain in accuracy for a minimal amount of work. Active learning has been used as a way to pick data that is useful to annotate in this scenario for several applications (Chan and Ng, 2007; Rai et al., 2010) so we adopt an active-learning-based approach here. When adapting sequence-based prediction methods, most active learning approaches have focused on picking full sentences that are valuable to annotate (Ringger et al., 2007; Settles and Craven, 2008). However, even within sentences, there are generally a few points of interest surrounded by large segments that are well covered by already annotated data. Partial annotation provides a solution to this problem (Tsuboi et al., 2008; Sassano and Kurohashi, 2010). In partial annotation, data that will not contribute to the improvement of the classifier is left untagged. For example, if there is a single difficult word in a long sentence, only the word boundaries and POS of the difficult word will be tagged. “Dif2Dictionary features are active if the string exists, reg</context>
</contexts>
<marker>Ringger, McClanahan, Haertel, Busby, Carmen, Carroll, Seppi, Lonsdale, 2007</marker>
<rawString>Eric Ringger, Peter McClanahan, Robbie Haertel, George Busby, Marc Carmen, James Carroll, Kevin Seppi, and Deryle Lonsdale. 2007. Active learning for part-ofspeech tagging: Accelerating corpus annotation. In Proceedings of the Linguistic Annotation Workshop, pages 101–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sassano</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Using smaller constituents rather than sentences in active learning for Japanese dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>356--365</pages>
<contexts>
<context position="8571" citStr="Sassano and Kurohashi, 2010" startWordPosition="1398" endWordPosition="1401">ick data that is useful to annotate in this scenario for several applications (Chan and Ng, 2007; Rai et al., 2010) so we adopt an active-learning-based approach here. When adapting sequence-based prediction methods, most active learning approaches have focused on picking full sentences that are valuable to annotate (Ringger et al., 2007; Settles and Craven, 2008). However, even within sentences, there are generally a few points of interest surrounded by large segments that are well covered by already annotated data. Partial annotation provides a solution to this problem (Tsuboi et al., 2008; Sassano and Kurohashi, 2010). In partial annotation, data that will not contribute to the improvement of the classifier is left untagged. For example, if there is a single difficult word in a long sentence, only the word boundaries and POS of the difficult word will be tagged. “Dif2Dictionary features are active if the string exists, regardless of whether itis treated as a single word in wJ1 , and thus can be calculated without the word segmentation result. Type Train Test 782k 87.5k 153k 17.3k Table 3: General and target domain corpus sizes in words. ficult” words can be selected using active learning approaches, choosi</context>
</contexts>
<marker>Sassano, Kurohashi, 2010</marker>
<rawString>Manabu. Sassano and Sadao Kurohashi. 2010. Using smaller constituents rather than sentences in active learning for Japanese dependency parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 356–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Sassano</author>
</authors>
<title>An empirical study of active learning with support vector machines for Japanese word segmentation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="5570" citStr="Sassano (2002)" startWordPosition="898" endWordPosition="899">xr+2) WS Only ls, rs, is POS Only wj, c(wj), djk Table 2: Features for the two-step model. xl and xr indicate the characters to the left and right of the word boundary or word wj in question. ls, rs, and is represent the left, right, and inside dictionary features, while djk indicates that tag k exists in the dictionary for word j. 2.2 2-Step Pointwise MA In our research, we take a two-step approach, first segmenting character sequence xI1 into the word sequence wJ1 with the highest probability, then tagging each word with parts of speech tJ1. This approach is shown in Figure 1 (b). We follow Sassano (2002) in formulating word segmentation as a binary classification problem, estimating boundary tags bI−1 1 . Tag bi = 1 indicates that a word boundary exists between characters xi and xi+1, while bi = 0 indicates that a word boundary does not exist. POS estimation can also be formulated as a multi-class classification problem, where we choose one tag tj for each word wj. These two classification problems can be solved by tools in the standard machine learning toolbox such as logistic regression (LR), support vector machines (SVMs), or conditional random fields (CRFs). We use information about the s</context>
</contexts>
<marker>Sassano, 2002</marker>
<rawString>Manabu Sassano. 2002. An empirical study of active learning with support vector machines for Japanese word segmentation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
<author>Mark Craven</author>
</authors>
<title>An analysis of active learning strategies for sequence labeling tasks.</title>
<date>2008</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1070--1079</pages>
<contexts>
<context position="8309" citStr="Settles and Craven, 2008" startWordPosition="1356" endWordPosition="1359">t our disposal both annotated general domain data, and unannotated target domain data. We would like to annotate the target domain data efficiently to achieve a maximal gain in accuracy for a minimal amount of work. Active learning has been used as a way to pick data that is useful to annotate in this scenario for several applications (Chan and Ng, 2007; Rai et al., 2010) so we adopt an active-learning-based approach here. When adapting sequence-based prediction methods, most active learning approaches have focused on picking full sentences that are valuable to annotate (Ringger et al., 2007; Settles and Craven, 2008). However, even within sentences, there are generally a few points of interest surrounded by large segments that are well covered by already annotated data. Partial annotation provides a solution to this problem (Tsuboi et al., 2008; Sassano and Kurohashi, 2010). In partial annotation, data that will not contribute to the improvement of the classifier is left untagged. For example, if there is a single difficult word in a long sentence, only the word boundaries and POS of the difficult word will be tagged. “Dif2Dictionary features are active if the string exists, regardless of whether itis tre</context>
</contexts>
<marker>Settles, Craven, 2008</marker>
<rawString>Burr Settles and Mark Craven. 2008. An analysis of active learning strategies for sequence labeling tasks. In Conference on Empirical Methods in Natural Language Processing, pages 1070–1079.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuta Tsuboi</author>
<author>Hisashi Kashima</author>
<author>Hiroki Oda</author>
<author>Shinsuke Mori</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Training conditional random fields using incomplete annotations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22th International Conference on Computational Linguistics,</booktitle>
<pages>897--904</pages>
<contexts>
<context position="2333" citStr="Tsuboi et al., 2008" startWordPosition="366" endWordPosition="369">” prediction, as it makes a single independent decision at each point (Neubig and Mori, 2010). While Liang et al. (2008) focus on the speed benefits of pointwise prediction, we demonstrate that it also allows for more robust and adaptable MA. We find experimental evidence that pointwise MA can exceed the accuracy of a state-of-the-art structured approach (Kudo et al., 2004) on in-domain data, and is significantly more robust to out-of-domain data. We also show that pointwise MA can be adapted to new domains with minimal effort through the combination of active learning and partial annotation (Tsuboi et al., 2008), where only informative parts of a particular sentence are annotated. In a realistic domain adaptation scenario, we find that a combination of pointwise prediction, partial annotation, and active learning allows for easy adaptation. 2 Japanese Morphological Analysis Japanese MA takes an unsegmented string of characters xi as input, segments it into morphemes wi , and annotates each morpheme with a part of speech ti . This can be formulated as a two-step process of first segmenting words, then estimating POSs (Ng and Low, 2004), or as a single joint process of finding a morpheme/POS string fro</context>
<context position="8541" citStr="Tsuboi et al., 2008" startWordPosition="1394" endWordPosition="1397">en used as a way to pick data that is useful to annotate in this scenario for several applications (Chan and Ng, 2007; Rai et al., 2010) so we adopt an active-learning-based approach here. When adapting sequence-based prediction methods, most active learning approaches have focused on picking full sentences that are valuable to annotate (Ringger et al., 2007; Settles and Craven, 2008). However, even within sentences, there are generally a few points of interest surrounded by large segments that are well covered by already annotated data. Partial annotation provides a solution to this problem (Tsuboi et al., 2008; Sassano and Kurohashi, 2010). In partial annotation, data that will not contribute to the improvement of the classifier is left untagged. For example, if there is a single difficult word in a long sentence, only the word boundaries and POS of the difficult word will be tagged. “Dif2Dictionary features are active if the string exists, regardless of whether itis treated as a single word in wJ1 , and thus can be calculated without the word segmentation result. Type Train Test 782k 87.5k 153k 17.3k Table 3: General and target domain corpus sizes in words. ficult” words can be selected using acti</context>
<context position="9816" citStr="Tsuboi et al. (2008)" startWordPosition="1599" endWordPosition="1602">classifier accuracy to annotate. In addition, corpora that are tagged with word boundaries but not POS tags are often available; this is another type of partial annotation. When using sequence-based prediction, learning on partially annotated data is not straightforward, as the data that must be used to train context-based transition probabilities may be left unannotated. In contrast, in the pointwise prediction framework, training using this data is both simple and efficient; unannotated points are simply ignored. A method for learning CRFs from partially annotated data has been presented by Tsuboi et al. (2008). However, when using partial annotation, CRFs’ already slow training time becomes slower still, as they must be trained over every sequence that has at least one annotated point. Training time is important in an active learning situation, as an annotator must wait while the model is being re-trained. 4 Experiments In order to test the effectiveness of pointwise MA, we did an experiment measuring accuracy both on in-domain data, and in a domain-adaptation situation. We used the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa, 2008), specifying the whitepaper, news, and books </context>
</contexts>
<marker>Tsuboi, Kashima, Oda, Mori, Matsumoto, 2008</marker>
<rawString>Yuta Tsuboi, Hisashi Kashima, Hiroki Oda, Shinsuke Mori, and Yuji Matsumoto. 2008. Training conditional random fields using incomplete annotations. In Proceedings of the 22th International Conference on Computational Linguistics, pages 897–904.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>