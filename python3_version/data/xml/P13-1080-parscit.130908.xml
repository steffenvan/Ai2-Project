<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.7447175">
Shallow Local Multi Bottom-up Tree Transducers
in Statistical Machine Translation
</title>
<author confidence="0.405325">
Fabienne Braune and Nina Seemann and Daniel Quernheim and Andreas Maletti
</author>
<affiliation confidence="0.373474">
Institute for Natural Language Processing, University of Stuttgart
</affiliation>
<address confidence="0.375376">
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
</address>
<email confidence="0.991469">
{braunefe,seemanna,daniel,maletti}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.994581" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999904363636364">
We present a new translation model in-
tegrating the shallow local multi bottom-
up tree transducer. We perform a large-
scale empirical evaluation of our obtained
system, which demonstrates that we sig-
nificantly beat a realistic tree-to-tree base-
line on the WMT 2009 English → German
translation task. As an additional contribu-
tion we make the developed software and
complete tool-chain publicly available for
further experimentation.
</bodyText>
<sectionHeader confidence="0.998417" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950415384615">
Besides phrase-based machine translation sys-
tems (Koehn et al., 2003), syntax-based systems
have become widely used because of their abil-
ity to handle non-local reordering. Those systems
use synchronous context-free grammars (Chi-
ang, 2007), synchronous tree substitution gram-
mars (Eisner, 2003) or even more powerful for-
malisms like synchronous tree-sequence substitu-
tion grammars (Sun et al., 2009). However, those
systems use linguistic syntactic annotation at dif-
ferent levels. For example, the systems proposed
by Wu (1997) and Chiang (2007) use no linguis-
tic information and are syntactic in a structural
sense only. Huang et al. (2006) and Liu et al.
(2006) use syntactic annotations on the source lan-
guage side and show significant improvements in
translation quality. Using syntax exclusively on
the target language side has also been success-
fully tried by Galley et al. (2004) and Galley et
al. (2006). Nowadays, open-source toolkits such
as Moses (Koehn et al., 2007) offer syntax-based
components (Hoang et al., 2009), which allow
experiments without expert knowledge. The im-
provements observed for systems using syntactic
annotation on either the source or the target lan-
guage side naturally led to experiments with mod-
els that use syntactic annotations on both sides.
However, as noted by Lavie et al. (2008), Liu et
al. (2009), and Chiang (2010), the integration of
syntactic information on both sides tends to de-
crease translation quality because the systems be-
come too restrictive. Several strategies such as
(i) using parse forests instead of single parses (Liu
et al., 2009) or (ii) soft syntactic constraints (Chi-
ang, 2010) have been developed to alleviate this
problem. Another successful approach has been
to switch to more powerful formalisms, which al-
low the extraction of more general rules. A par-
ticularly powerful model is the non-contiguous
version of synchronous tree-sequence substitu-
tion grammars (STSSG) of Zhang et al. (2008a),
Zhang et al. (2008b), and Sun et al. (2009),
which allows sequences of trees on both sides of
the rules [see also (Raoult, 1997)]. The multi
bottom-up tree transducer (MBOT) of Arnold and
Dauchet (1982) and Lilin (1978) offers a middle
ground between traditional syntax-based models
and STSSG. Roughly speaking, an MBOT is an
STSSG, in which all the discontinuities must oc-
cur on the target language side (Maletti, 2011).
This restriction yields many algorithmic advan-
tages over both the traditional models as well as
STSSG as demonstrated by Maletti (2010). For-
mally, they are expressive enough to express all
sensible translations (Maletti, 2012)1. Figure 2
displays sample rules of the MBOT variant, called
eMBOT, that we use (in a graphical representation
of the trees and the alignment).
In this contribution, we report on our novel sta-
tistical machine translation system that uses an
eMBOT-based translation model. The theoreti-
cal foundations of eMBOT and their integration
into our translation model are presented in Sec-
tions 2 and 3. In order to empirically evaluate the
eMBOT model, we implemented a machine trans-
</bodyText>
<footnote confidence="0.989387">
1A translation is sensible if it is of linear size increase
and can be computed by some (potentially copying) top-down
tree transducer.
</footnote>
<page confidence="0.923659">
811
</page>
<note confidence="0.921232">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 811–821,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.995545103448276">
RB2211
CD2212
%2221
just22111
322121
VP2
NP1
Sε
7711
Official111
NNS12
forecasts121
VBD21
predicted211
NP22
QP221
NN222
S
NP VBD NP
�→
NP VAFIN PP VVPP
S
�
NP
QP NN
( PP
` �
→
von AP NN
</figure>
<figureCaption confidence="0.99727">
Figure 2: Sample eMBOT rules.
Figure 1: Example tree t with indicated positions.
</figureCaption>
<bodyText confidence="0.9995708">
We have t(21) = VBD and t|221 is the subtree
marked in red.
lation system that we are going to make available
to the public. We implemented eMBOT inside
the syntax-based component of the Moses open
source toolkit. Section 4 presents the most im-
portant algorithms of our eMBOT decoder. We
evaluate our new system on the WMT 2009 shared
translation task English → German. The trans-
lation quality is automatically measured using
BLEU scores, and we confirm the findings by pro-
viding linguistic evidence (see Section 5). Note
that in contrast to several previous approaches, we
perform large scale experiments by training sys-
tems with approx. 1.5 million parallel sentences.
</bodyText>
<sectionHeader confidence="0.996822" genericHeader="method">
2 Theoretical Model
</sectionHeader>
<bodyText confidence="0.999395485714285">
In this section, we present the theoretical genera-
tive model used in our approach to syntax-based
machine translation. Essentially, it is the local
multi bottom-up tree transducer of Maletti (2011)
with the restriction that all rules must be shallow,
which means that the left-hand side of each rule
has height at most 2 (see Figure 2 for shallow
rules and Figure 4 for rules including non-shallow
rules). The rules extracted from the training exam-
ple of Figure 3 are displayed in Figure 4. Those
extracted rules are forcibly made shallow by re-
moving internal nodes. The application of those
rules is illustrated in Figures 5 and 6.
For those that want to understand the inner
workings, we recall the principal model in full de-
tail in the rest of this section. Since we utilize syn-
tactic parse trees, let us introduce trees first. Given
an alphabet Σ of labels, the set TE of all Σ-trees is
the smallest set T such that a(t1, ... , tk) ∈ T for
all a ∈ Σ, integer k ≥ 0, and t1, ... , tk ∈ T. In-
tuitively, a tree t consists of a labeled root node a
followed by a sequence t1, ... , tk of its children.
A tree t ∈ TE is shallow if t = a(t1, ... , tk) with
a ∈ Σ and t1, ...,tk ∈ Σ.
To address a node inside a tree, we use its po-
sition, which is a word consisting of positive in-
tegers. Roughly speaking, the root of a tree is
addressed with the position E (the empty word).
The position iw with i ∈ N addresses the po-
sition w in the ith direct child of the root. In
this way, each node in the tree is assigned a
unique position. We illustrate this notion in Fig-
ure 1. Formally, the positions pos(t) ⊆ N. of
a tree t = a(t1, ... , tk) are inductively defined
by pos(t) = {E} ∪ pos(k)(t1, ... , tk), where
</bodyText>
<equation confidence="0.993359">
Upos(k)(t1,...,tk) = {iw  |w ∈ pos(ti)} .
1&lt;i&lt;k
</equation>
<bodyText confidence="0.593266166666667">
Let t ∈ TE and w ∈ pos(t). The label of t at
position w is t(w), and the subtree rooted at posi-
tion w is t|w. These notions are also illustrated in
Figure 1. A position w ∈ pos(t) is a leaf (in t) if
w1 ∈/ pos(t). In other words, leaves do not have
any children. Given a subset N ⊆ Σ, we let
</bodyText>
<equation confidence="0.935654">
leafN(t) = {w ∈ pos(t)  |t(w) ∈ N, w leaf in t}
</equation>
<bodyText confidence="0.87213325">
be the set of all leaves labeled by elements of N.
When N is the set of nonterminals, we call them
leaf nonterminals. We extend this notion to se-
quences t1,...,tk ∈ TE by
</bodyText>
<equation confidence="0.777167">
leaf(,) (t1, ... , tk) = U {iw  |w ∈ leafN(ti)}.
1&lt;i&lt;k
</equation>
<bodyText confidence="0.999711083333333">
Let w1, ... , wn ∈ pos(t) be (pairwise prefix-
incomparable) positions and t1, ... , tn ∈ TE.
Then t[wi ← ti]1&lt;i&lt;n denotes the tree that is ob-
tained from t by replacing (in parallel) the subtrees
at wi by ti for every 1 ≤ i ≤ n.
Now we are ready to introduce our model,
which is a minor variation of the local multi
bottom-up tree transducer of Maletti (2011). Let
Σ and Δ be the input and output symbols, respec-
tively, and let N ⊆ Σ ∪ Δ be the set of nontermi-
nal symbols. Essentially, the model works on pairs
ht, (u1, ... , uk)i consisting of an input tree t ∈ TE
</bodyText>
<page confidence="0.995782">
812
</page>
<bodyText confidence="0.94235532">
and a sequence u1, ... , uk E TA of output trees.
Such pairs are pre-translations of rank k. The pre-
translation (t, (u1, ... , uk)) is shallow if all trees
t, u1, ... , uk in it are shallow.
Together with a pre-translation we typically
have to store an alignment. Given a pre-translation
(t, (u1, ... , uk)) of rank k and 1 G i G k,
we call ui the ith translation of t. An align-
ment for this pre-translation is an injective map-
ping 0: leaf(k)
N (u1, ... , uk) -+ leafN(t)xN such
that if (w, j) E ran(0), then also (w, i) E ran(0)
for all 1 G j G i.2 In other words, if an alignment
requests the ith translation, then it should also re-
quest all previous translations.
Definition 1 A shallow local multi bottom-up tree
transducer (EMBOT) is a finite set R of rules to-
gether with a mapping c: R -+ R such that every
rule, written t -+ψ (u1, ... , uk), contains a shal-
low pre-translation (t, (u1, ... , uk)) and an align-
ment 0 for it.
The components t, (u1,... , uk), 0, and c(p)
are called the left-hand side, the right-hand
side, the alignment, and the weight of the
rule p = t -+ψ (u1, ... , uk). Figure 2 shows two
example QMBOT rules (without weights). Overall,
the rules of an QMBOT are similar to the rules of
an SCFG (synchronous context-free grammar), but
our right-hand sides contain a sequence of trees
instead of just a single tree. In addition, the align-
ments in an SCFG rule are bijective between leaf
nonterminals, whereas our model permits multi-
ple alignments to a single leaf nonterminal in the
left-hand side (see Figure 2).
Our eMBOT rules are obtained automatically
from data like that in Figure 3. Thus, we (word)
align the bilingual text and parse it in both the
source and the target language. In this manner
we obtain sentence pairs like the one shown in
Figure 3. To these sentence pairs we apply the
rule extraction method of Maletti (2011). The
rules extracted from the sentence pair of Figure 3
are shown in Figure 4. Note that these rules
are not necessarily shallow (the last two rules are
not). Thus, we post-process the extracted rules
and make them shallow. The shallow rules corre-
sponding to the non-shallow rules of Figure 4 are
shown in Figure 2.
Next, we define how to combine rules to form
derivations. In contrast to most other models, we
</bodyText>
<footnote confidence="0.590519">
2ran(f) for a mapping f : A → B denotes the range off,
which is {f(a)  |a E A}.
</footnote>
<figureCaption confidence="0.999431">
Figure 3: Aligned parsed sentences.
</figureCaption>
<bodyText confidence="0.992652714285714">
only introduce a derivation semantics that does
not collapse multiple derivations for the same
input-output pair.3 We need one final notion.
Let p = t -+ψ (u1, ... , uk) be a rule and
w E leafN(t) be a leaf nonterminal (occurrence)
in the left-hand side. The w-rank rk(p, w) of the
rule p is
</bodyText>
<equation confidence="0.787596">
rk(p, w) = max {i E N  |(w, i) E ran(0)} .
For example, for the lower rule p in Figure 2 we
have rk(p, 1) = 1, rk(p, 2) = 2, and rk(p, 3) = 1.
</equation>
<bodyText confidence="0.71830075">
Definition 2 The set τ(R, c) of weighted pre-
translations of an EMBOT (R, c) is the smallest
set T subject to the following restriction: If there
exist
</bodyText>
<listItem confidence="0.999676">
• a rule p = t -+ψ (u1, ... , uk) E R,
• a weighted pre-translation
</listItem>
<equation confidence="0.904736888888889">
(tw, cw, (uw1 , ... , uwkw)) E T
for every w E leafN(t) with
– rk(p, w) = kw,4
– t(w) = tw(ε),5 and
– for every iw0 E leaf(k)
N (u1, ... , uk),6
ui(w0) = uv j (ε) with 0(iw0) = (v, j),
then (t0, c0, (u01, ... , u0k)) E T is a weighted pre-
translation, where
</equation>
<listItem confidence="0.826315">
• t0 = t[w +-- tw  |w E leafN(t)],
</listItem>
<footnote confidence="0.941115833333333">
3A standard semantics is presented, for example,
in (Maletti, 2011).
4If w has n alignments, then the pre-translation selected
for it has to have suitably many output trees.
5The labels have to coincide for the input tree.
6Also the labels for the output trees have to coincide.
</footnote>
<figure confidence="0.992003014492754">
NP
VP
ADJA
PP
sind
NN
VVPP
NP
VP
VAFIN
S
JJ
Official
NP
QP
CD
3
NNS
forecasts
VBD
predicted
RB
just
NN
%
nur
3
von
ADV
CARD
%
Offizielle
ausgegangen
APPR
AP
NN
S
Prognosen
813
( (
NN VBD ADJA NNS ( WIN VVPP RB r
Offizielle forecasts i ` Prognosen ) predicted → ` sind ausgegangen ) just → \ nur CD → ( CARD) % → (N
\
%N \
NP
JJ NNS
� NP � QP � AP � NP
→ →
ADJA NN RB CD ADV CARD QP NN
�→
APPR
von
PP
S
�AP NN NP VP
VBD NP
�
→
NP
VAFIN VP
PP VVPP
)
JJ
Official
=
c&apos;
c(ρ)·QwEleafN(t)cw, and
• u&apos;i = ui[iw&apos; ← uvj  |ψ(
every
</figure>
<figureCaption confidence="0.826473666666667">
Rules that do not contain any nonterminal
leaves are automatically weighted pre-translations
with their associated rule weight. Otherwise, each
nonterminal leaf w in the left-hand side of a rule
must be replaced by the input tree tw of a pre-
translation
</figureCaption>
<bodyText confidence="0.963936466666667">
, ... ,
whose root is
labeled by the same nonterminal. In addition, the
w) of the replaced nonterminal should
match the number kw of components in the se-
lected weighted pre-translation. Finally, the non-
terminals in the right-hand side that are aligned
to w should be replaced by the translation that the
alignment requests, provided that the nontermi-
nal matches with the root symbol of the requested
translation. The weight of the new pre-translation
is obtained simply by multiplying the rule weight
and the weights of the selected weighted pre-
translations. The overall process is illustrated in
Figures 5 an
</bodyText>
<equation confidence="0.9614666">
1 ≤ i ≤ k.
ρ
htw,cw,(uw1
uwkw)i,
rk(ρ,
</equation>
<bodyText confidence="0.940915">
d 6.
814 (1) The forward translation weight using the rule
</bodyText>
<sectionHeader confidence="0.996972" genericHeader="method">
3 Translation Model
</sectionHeader>
<bodyText confidence="0.858505666666667">
weights as described in Section 2
weights as described in Section 2
target
source
the number of sequences used in the pre-
translations that constructed
(gap penalty)
The rule weights required for (1) are relative
frequencies normalized over all rules with the
same left-hand side. In the same fashion the rule
The lexical weights
[respectively,
ej) accross (possi-
average
Y=
{w(g|e) |g aligned to e}
lexical item
e occurs in t
</bodyText>
<equation confidence="0.806122333333333">
7
p(g|e) ∝ Y hm �ht, (u)i�λm
m=1
</equation>
<bodyText confidence="0.92938625">
Our model uses the following features
hm(ht, (u1, ... , uk)i) for a general pre-translation
τ = ht, (u1, . . . , uk)i:
main translation direction is English to German.
</bodyText>
<footnote confidence="0.481371">
t must embed in the parse tre
7Our
8Actually,
e of e; see Sec-
tion 4.
</footnote>
<listItem confidence="0.998628">
(2) The indirect translation weight using the rule
(3) Lexical translation weight source
(4) Lexical translation weight target
(5) Target side language model
(6) Number of words in the target sentences
(7) Number of rules used in the pre-translation
(8) Number of target side sequences; here k times
</listItem>
<bodyText confidence="0.962794833333333">
weights required for (2) are relative frequencies
normalized over all rules with the same right-
hand side. Additionally, rules that were extracted
at most 10 times are discounted by multiplying
the rule weight by
for (2) and (3) are obtained by multiplying the
word translations
of lexically aligned words
bly discontiguous) target side sequences.9 When-
ever asource word ej is aligned to multiple target
words, we average over the word
as a score
</bodyText>
<figure confidence="0.9632062">
This discourages rules with
man
1001−c.
y target sequences.
S
</figure>
<figureCaption confidence="0.8229035">
Figure 4: Extracted (even non-shall
ow) rules. We obtain our rules by making those rules shallow.
</figureCaption>
<equation confidence="0.4624745">
iw&apos;) = (v, j)] for
rank
</equation>
<bodyText confidence="0.665533727272727">
Given a source language sentence e, our transla-
tion model aims to find the best corresponding tar-
get language translation
ˆg;7 i.e.,
= arg
We estimate the probability
through a log-
linear combination of component models with pa-
rameters
scored on the pre-translations
such that the leaves of t concatenated read e.8
</bodyText>
<equation confidence="0.420979428571429">
gˆ
maxgp(g|e)
p(g|e)
λm
ht,(u)i
→
→
τ
10−2.
w(gi|ej)
w(ej|gi)]
(gi,
translations.10
h3(ht,(u1, ... , uk)i)
</equation>
<bodyText confidence="0.9342632">
The computation of the language model esti-
mates for (6) is adapted to score partial transla-
tions consisting of discontiguous units. We ex-
plain the details in Section 4. Finally, the count c
of target sequences obtained in (7) is actually used
</bodyText>
<footnote confidence="0.889347333333333">
9The lexical alignments are different from the alignments
used with a pre-translation.
10If the word ej has no alignment to a target word, then
it is assumed to be aligned to a special NULL word and this
ali
gnment is scored.
</footnote>
<figure confidence="0.993211652173913">
Combining a rule with pre-translations:
NP
JJ NNS
( NP �
→
ADJA NN
JJ
Official
� ADJA �NNS �NN �
→ →
Offizielle forecasts Prognosen
Obtained new pre-translation:
NP NP
ADJA
Offizielle
NN
Prognosen
NNS
forecasts
JJ
Official
�→
�
</figure>
<figureCaption confidence="0.987908">
Figure 5: Simple rule application.
</figureCaption>
<figure confidence="0.999600985507247">
Combining a rule with pre-translations:
Obtained new pre-translation:
S
S
NP VBD NP
S
NP VAFIN PP VVPP
�→
�
NP
NNS
forecasts
ADJA
Offizielle
NP
NN
Prognosen
PP
von AP
ADV
nur
JJ
Official
�→
� VBD
predicted
/ VAFIN
→
sind
,
VVPP �
ausgegangen
RB
just
NP
QP
CD
3
NN �→
%
NN �
%
CARD
3
JJ
Official
NP
NNS
forecasts
S
VBD
predicted
RB
just
NP
CD
3
NN
%
QP
→
ADJA NN sind von AP NN
Offizielle Prognosen ADV CARD %
NP
VAFIN
PP
VVPP
!ausgegangen
nur 3
</figure>
<figureCaption confidence="0.993518">
Figure 6: Complex rule application.
</figureCaption>
<figure confidence="0.99930725">
S
NP VAFIN PP VVPP
� �
Offizielle Prognosen sind , ausgegangen von nur 3 %
</figure>
<figureCaption confidence="0.999976">
Figure 7: Illustration of LM scoring.
</figureCaption>
<page confidence="0.998148">
815
</page>
<sectionHeader confidence="0.997231" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.997949692307692">
We implemented our model in the syntax-based
component of the Moses open-source toolkit
by Koehn et al. (2007) and Hoang et al. (2009).
The standard Moses syntax-based decoder only
handles SCFG rules; i.e, rules with contiguous
components on the source and the target lan-
guage side. Roughly speaking, SCFG rules are
eMBOT rules with exactly one output tree. We
thus had to extend the system to support our
eMBOT rules, in which arbitrarily many output
trees are allowed.
The standard Moses syntax-based decoder uses
a CYK+ chart parsing algorithm, in which each
source sentence is parsed and contiguous spans are
processed in a bottom-up fashion. A rule is appli-
cable11 if the left-hand side of it matches the non-
terminal assigned to the full span by the parser and
the (non-)terminal assigned to each subspan.12 In
order to speed up the decoding, cube pruning (Chi-
ang, 2007) is applied to each chart cell in order
to select the most likely hypotheses for subspans.
The language model (LM) scoring is directly in-
tegrated into the cube pruning algorithm. Thus,
LM estimates are available for all considered hy-
potheses. To accommodate eMBOT rules, we had
to modify the Moses syntax-based decoder in sev-
eral ways. First, the rule representation itself is ad-
justed to allow sequences of shallow output trees
on the target side. Naturally, we also had to ad-
just hypothesis expansion and, most importantly,
language model scoring inside the cube pruning
algorithm. An overview of the modified pruning
procedure is given in Algorithm 1.
The most important modifications are hidden
in lines 5 and 8. The expansion in Line 5 in-
volves matching all nonterminal leaves in the rule
as defined in Definition 2, which includes match-
ing all leaf nonterminals in all (discontiguous) out-
put trees. Because the output trees can remain
discontiguous after hypothesis creation, LM scor-
ing has to be done individually over all output
trees. Algorithm 2 describes our LM scoring in
detail. In it we use k strings wl, ... , wk to col-
lect the lexical information from the k output com-
11Note that our notion of applicable rules differs from the
default in Moses.
12Theoretically, this allows that the decoder ignores unary
parser nonterminals, which could also disappear when we
make our rules shallow; e.g., the parse tree left in the pre-
translation of Figure 5 can be matched by a rule with left-
hand side NP(Official, forecasts).
Algorithm 1 Cube pruning with eMBOT rules
</bodyText>
<subsectionHeader confidence="0.826347">
Data structures:
</subsectionHeader>
<bodyText confidence="0.6291975">
- r[i, j]: list of rules matching span e[i ... j]
- h[i, j]: hypotheses covering span e[i ... j]
</bodyText>
<listItem confidence="0.976452846153846">
- c[i, j]: cube of hypotheses covering span e[i ... j]
1: for all eMBOT rules p covering span e[i ... j] do
2: Insert p into r[i, j]
3: Sort r[i, j]
4: for all (l →ψ r) ∈ r[i, j] do
5: Create h[i, j] by expanding all nonterminals in l with
best scoring hypotheses for subspans
6: Add h[i, j] to c[i, j]
7: for all hypotheses h ∈ c[i, j] do
8: Estimate LM score for h // see Algorithm 2
9: Estimate remaining feature scores
10: Sort c[i, j]
11: Retrieve first α elements from c[i, j] // we use α = 103
</listItem>
<bodyText confidence="0.995105388888889">
ponents (ul, ... , uk) of a rule. These strings can
later be rearranged in any order, so we LM-score
all of them separately. Roughly speaking, we ob-
tain wi by traversing ui depth-first left-to-right.
If we meet a lexical element (terminal), then we
add it to the end of wi. On the other hand, if we
meet a nonterminal, then we have to consult the
best pre-translation T0 = (t0, (u01, ... , u0k,)), which
will contribute the subtree at this position. Sup-
pose that u0j will be substituted into the nontermi-
nal in question. Then we first LM-score the pre-
translation T0 to obtain the string w0j correspond-
ing to u0j. This string w0j is then appended to wi.
Once all the strings are built, we score them using
our 4-gram LM. The overall LM score for the pre-
translation is obtained by multiplying the scores
for wi, ... , wk. Clearly, this treats wl, ... , wk as
k separate strings, although they eventually will
be combined into a single string. Whenever such
a concatenation happens, our LM scoring will au-
tomatically compute n-gram LM scores based on
the concatenation, which in particular means that
the LM scores get more accurate for larger spans.
Finally, in the final rule only one component is al-
lowed, which yields that the LM indeed scores the
complete output sentence.
Figure 7 illustrates our LM scoring for a pre-
translation involving a rule with two (discontigu-
ous) target sequences (the construction of the pre-
translation is illustrated in Figure 6). When pro-
cessing the rule rooted at S, an LM estimate is
computed by expanding all nonterminal leaves. In
our case, these are NP, VAFIN, PP, and VVPP.
However, the nodes VAFIN and VVPP are assem-
bled from a (discontiguous) tree sequence. This
means that those units have been considered as in-
</bodyText>
<page confidence="0.996328">
816
</page>
<construct confidence="0.309441">
Algorithm 2 LM scoring
</construct>
<subsectionHeader confidence="0.774007">
Data structures:
</subsectionHeader>
<bodyText confidence="0.748287">
- (ul, ... , uk): right-hand side of a rule
</bodyText>
<listItem confidence="0.9850589">
- (wl, ... ,wk): k strings all initially empty
1: score = 1
2: for all 1 &lt; i &lt; k do
3: for all leaves f in ui (in lexicographic order) do
4: if f is a terminal then
5: Append f to wi
6: else
7: LM score the best hypothesis for the subspan
8: Expand wi by the corresponding wj&apos;
9: score = score · LM(wi)
</listItem>
<bodyText confidence="0.614047166666667">
dependent until now. So far, the LM scorer could
only score their associated unigrams. However,
we also have their associated strings wi and w2,
which can now be used. Since VAFIN and VVPP
now become parts of a single tree, we can perform
LM scoring normally. Assembling the string we
obtain
Offizielle Prognosen sind von nur 3 %
ausgegangen
which is scored by the LM. Thus, we first score
the 4-grams “Offizielle Prognosen sind von”, then
“Prognosen sind von nur”, etc.
</bodyText>
<sectionHeader confidence="0.997117" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.980064">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.969861695652174">
The baseline system for our experiments is the
syntax-based component of the Moses open-
source toolkit of Koehn et al. (2007) and Hoang
et al. (2009). We use linguistic syntactic anno-
tation on both the source and the target language
side (tree-to-tree). Our contrastive system is the
eMBOT-based translation system presented here.
We provide the system with a set of SCFG as well
as eMBOT rules. We do not impose any maximal
span restriction on either system.
The compared systems are evaluated on the
English-to-German13 news translation task of
WMT 2009 (Callison-Burch et al., 2009). For
both systems, the used training data is from the
4th version of the Europarl Corpus (Koehn, 2005)
and the News Commentary corpus. Both trans-
lation models were trained with approximately
1.5 million bilingual sentences after length-ratio
filtering. The word alignments were generated
by GIZA++ (Och and Ney, 2003) with the grow-
diag-final-and heuristic (Koehn et al., 2005). The
13Note that our fMBOT-based system can be applied to any
language pair as it involves no language-specific engineering.
</bodyText>
<table confidence="0.98501025">
System BLEU
Baseline 12.60
eMBOT ∗13.06
Moses t-to-s 12.72
</table>
<tableCaption confidence="0.999537">
Table 1: Evaluation results. The starred results
</tableCaption>
<bodyText confidence="0.978493470588235">
are statistically significant improvements over the
Baseline (at confidence p &lt; 0.05).
English side of the bilingual data was parsed us-
ing the Charniak parser of Charniak and John-
son (2005), and the German side was parsed us-
ing BitPar (Schmid, 2004) without the function
and morphological annotations. Our German 4-
gram language model was trained on the Ger-
man sentences in the training data augmented
by the Stuttgart SdeWaC corpus (Web-as-Corpus
Consortium, 2008), whose generation is detailed
in (Baroni et al., 2009). The weights am in the
log-linear model were trained using minimum er-
ror rate training (Och, 2003) with the News 2009
development set. Both systems use glue-rules,
which allow them to concatenate partial transla-
tions without performing any reordering.
</bodyText>
<subsectionHeader confidence="0.911123">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.99974828">
We measured the overall translation quality with
the help of 4-gram BLEU (Papineni et al., 2002),
which was computed on tokenized and lower-
cased data for both systems. The results of our
evaluation are reported in Table 1. For com-
parison, we also report the results obtained by
a system that utilizes parses only on the source
side (Moses tree-to-string) with its standard fea-
tures.
We can observe from Table 1 that our eMBOT-
based system outperforms the baseline. We ob-
tain a BLEU score of 13.06, which is a gain of
0.46 BLEU points over the baseline. This im-
provement is statistically significant at confidence
p &lt; 0.05, which we computed using the pairwise
bootstrap resampling technique of Koehn (2004).
Our system is also better than the Moses tree-to-
string system. However this improvement (0.34)
is not statistically significant. In the next section,
we confirm the result of the automatic evaluation
through a manual examination of some transla-
tions generated by our system and the baseline.
In Table 2, we report the number of eMBOT
rules used by our system when decoding the test
set. By lex we denote rules containing only lexical
</bodyText>
<page confidence="0.992087">
817
</page>
<table confidence="0.957854307692308">
lex non-term total
contiguous 23,175 18,355 41,530
discontiguous 315 2,516 2,831
VP
VB NP PP
~ NP ,PP ,VVINF �
→
NP PP VVINF
S
TO VP
( VP �
→
NP PP PTKZU VVINF
</table>
<tableCaption confidence="0.963">
Table 2: Number of rules used in decoding test
</tableCaption>
<bodyText confidence="0.84527225">
(lex: only lexical items; non-term: at least one
nonterminal).
Figure 10: Used QMBOT rules for verbal reorder-
ing
</bodyText>
<figure confidence="0.899453954545454">
VP
,
�
ADV commented on NP
~ NP ,ADV VPP
→
NP ADV kommentiert
2-dis 3-dis 4-dis
2,480 323 28
Table 3: Number of k-discontiguous rules.
VP
VBZ VP
NP VAFIN ADV VPP )
, , ,
NP VAFIN ADV VPP
�→
TOP
NP VP
TOP
NP VAFIN NP ADV VVPP
�→
�
</figure>
<bodyText confidence="0.796357333333333">
items. The label non-term stands for rules contain-
ing at least one leaf nonterminal. The results show
that approx. 6% of all rules used by our QMBOT-
system have discontiguous target sides. Further-
more, the reported numbers show that the system
also uses rules in which lexical items are com-
bined with nonterminals. Finally, Table 3 presents
the number of rules with k target side components
used during decoding.
</bodyText>
<subsectionHeader confidence="0.99838">
5.3 Linguistic Analysis
</subsectionHeader>
<bodyText confidence="0.999676851851852">
In this section we present linguistic evidence sup-
porting the fact that the QMBOT-based system sig-
nificantly outperforms the baseline. All exam-
ples are taken from the translation of the test set
used for automatic evaluation. We show that when
our system generates better translations, this is di-
rectly related to the use of QMBOT rules.
Figures 8 and 9 show the ability of our system to
correctly reorder multiple segments in the source
sentence where the baseline translates those seg-
ments sequentially. An analysis of the generated
derivations shows that our system produces the
correct translation by taking advantage of rules
with discontiguous units on target language side.
The rules used in the presented derivations are dis-
played in Figures 10 and 11. In the first example
(Figure 8), we begin by translating “((smuggle)VB
(eight projectiles)NP (into the kingdom)PP)VP” into
the discontiguous sequence composed of (i) “(acht
geschosse)NP” ; (ii) “(in das k¨onigreich)PP” and
(iii) “(schmuggeln)VP”. In a second step we as-
semble all sequences in a rule with contiguous tar-
get language side and, at the same time, insert the
word “(zu)PTKZU” between “(in das k¨onigreich)PP”
and “(schmuggeln)VP”.
The second example (Figure 9) illustrates a
more complex reordering. First, we trans-
</bodyText>
<figureCaption confidence="0.564422">
Figure 11: Used QMBOT rules for verbal reorder-
ing
</figureCaption>
<bodyText confidence="0.999916">
late “((again)ADV commented on (the problem
of global warming)NP)VP” into the discontigu-
ous sequence composed of (i) “(das problem
der globalen erw¨armung)NP”; (ii) “(wieder)ADV”
and (iii) “(kommentiert)VPP”. In a second step,
we translate the auxiliary “(has)VBZ” by in-
serting “(hat)VAFIN” into the sequence. We
thus obtain, for the input segment “((has)VBZ
(again)ADV commented on (the problem of global
warming)NP)VP”, the sequence (i) “(das problem
der globalen erw¨armung)NP”; (ii) “(hat)VAFIN”;
(iii) “(wieder)ADV”; (iv) “(kommentiert)VVPP”. In
a last step, the constituent “(president v´aclav
klaus)NP” is inserted between the discontiguous
units “(hat)VAFIN” and “(wieder)ADV” to form the
contiguous sequence “((das problem der glob-
alen erw¨armung)NP (hat)VAFIN (pr¨asident v´aclav
klaus)NP (wieder)ADV (kommentiert)VVPP)TOP”.
Figures 12 and 13 show examples where our
system generates complex words in the target
language out of a simple source language word.
Again, an analysis of the generated derivation
shows that QMBOT takes advantage of rules hav-
ing several target side components. Examples of
such rules are given in Figure 14. Through its
ability to use these discontiguous rules, our sys-
tem correctly translates into reflexive or particle
verbs such as “konzentriert sich” (for the English
“focuses”) or “besteht darauf” (for the English
“insist”). Another phenomenon well handled by
our system are relative pronouns. Pronouns such
as “that” or “whose” are systematically translated
</bodyText>
<page confidence="0.984892">
818
</page>
<figure confidence="0.966756666666667">
... geplant hatten 8 geschosse in das k¨onigreich zu schmuggeln
... had planned to smuggle 8 projectiles into the kingdom
... vorhatten zu schmuggeln 8 geschosse in das k¨onigreich
</figure>
<figureCaption confidence="0.998993">
Figure 8: Verbal Reordering (top: our system, bottom: baseline)
</figureCaption>
<bodyText confidence="0.724989">
das problem der globalen erw¨armung hat pr¨asident v´aclav klaus wieder kommentiert
president v´aclav klaus has again commented on the problem of global warming
pr¨asident v´aclav klaus hat wieder kommentiert das problem der globalen erw¨armung
</bodyText>
<figureCaption confidence="0.995311">
Figure 9: Verbal Reordering (top: our system, bottom: baseline)
</figureCaption>
<figure confidence="0.996767714285714">
... die serbische delegation bestand darauf , dass jede entscheidung ...
i / $ KOUS) VBZ i / VVFIN PRF )
ldass focuses l konzentriert sich
IN
that
... the serbian delegation insisted that every decision ...
... die serbische delegation bestand , jede entscheidung ...
</figure>
<figureCaption confidence="0.8792765">
Figure 12: Relative Clause (top: our system, bot-
tom: baseline)
</figureCaption>
<figure confidence="0.921798666666667">
... die roadmap von bali , konzentriert sich auf die bem¨uhungen ...
... the bali roadmap that focuses on efforts ...
... die bali roadmap , konzentriert auf bem¨uhungen ...
</figure>
<figureCaption confidence="0.9941525">
Figure 13: Reflexive Pronoun (top: our system,
bottom: baseline)
</figureCaption>
<bodyText confidence="0.6220825">
into both both, “,” and “dass” or “,” and “deren”
(Figure 12).
</bodyText>
<sectionHeader confidence="0.994976" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9979563125">
We demonstrated that our `MBOT-based machine
translation system beats a standard tree-to-tree
system (Moses tree-to-tree) on the WMT 2009
translation task English i German. To achieve
this we implemented the formal model as de-
scribed in Section 2 inside the Moses machine
translation toolkit. Several modifications were
necessary to obtain a working system. We publicly
release all our developed software and our com-
plete tool-chain to allow independent experiments
and evaluation. This includes our `MBOT decoder
Figure 14: `MBOT rules generating a relative
clause/reflexive pronoun
presented in Section 4 and a separate C++ module
that we use for rule extraction (see Section 3).
Besides the automatic evaluation, we also per-
formed a small manual analysis of obtained trans-
lations and show-cased some examples (see Sec-
tion 5.3). We argue that our `MBOT approach can
adequately handle discontiguous phrases, which
occur frequently in German. Other languages that
exhibit such phenomena include Czech, Dutch,
Russian, and Polish. Thus, we hope that our sys-
tem can also successfully be applied for other lan-
guage pairs, which we plan to pursue as well.
In other future work, we want to investigate
full backwards application of `MBOT rules, which
would be more suitable for the converse transla-
tion direction German i English. The current in-
dependent LM scoring of components has some
negative side-effects that we plan to circumvent
with the use of lazy LM scoring.
</bodyText>
<sectionHeader confidence="0.97738" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99961225">
The authors thank Alexander Fraser for his ongo-
ing support and advice. All authors were finan-
cially supported by the German Research Founda-
tion (DFG) grant MA 4959/ 1-1.
</bodyText>
<page confidence="0.997944">
819
</page>
<sectionHeader confidence="0.993892" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999911450980392">
Andr´e Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d’arbres. Theoret. Comput. Sci.,
20(1):33–93.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide
Web: A collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209–226.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proc. 4th Workshop on Statistical Machine Trans-
lation, pages 1–28.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. 43rd ACL, pages 173–180.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computat. Linguist., 33(2):201–228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. 48th ACL, pages 1443–
1452.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. 41st
ACL, pages 205–208.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. HLT-NAACL, pages 273–280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve Deneefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
44th ACL, pages 961–968.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proc. 6th Int. Workshop Spoken Language Transla-
tion, pages 152–159.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. 7th Conf. Association
for Machine Translation of the Americas, pages 66–
73.
Philip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL, pages 127–133.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT Speech Translation Evaluation.
In Proc. 2nd Int. Workshop Spoken Language Trans-
lation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL, pages 177–180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP,
pages 388–395.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. 10th Ma-
chine Translation Summit, pages 79–86.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proc. 2nd ACL Workshop on Syntax and
Structure in Statistical Translation, pages 87–95.
Eric Lilin. 1978. Une g´en´eralisation des transducteurs
d’´etats finis d’arbres: les S-transducteurs. Th`ese
3`eme cycle, Universit´e de Lille.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. 44th ACL, pages 609–616.
Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
47th ACL, pages 558–566.
Andreas Maletti. 2010. Why synchronous tree sub-
stitution grammars? In Proc. HLT-NAACL, pages
876–884.
Andreas Maletti. 2011. How to train your multi
bottom-up tree transducer. In Proc. 49th ACL, pages
825–834.
Andreas Maletti. 2012. Every sensible extended top-
down tree transducer is a multi bottom-up tree trans-
ducer. In Proc. HLT-NAACL, pages 263–273.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computat. Linguist., 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. 41st ACL,
pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. 40th
ACL, pages 311–318.
Jean-Claude Raoult. 1997. Rational tree relations.
Bull. Belg. Math. Soc. Simon Stevin, 4(1):149–176.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
Proc. 20th COLING, pages 162–168.
</reference>
<page confidence="0.971508">
820
</page>
<reference confidence="0.9980317">
Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A non-
contiguous tree sequence alignment-based model for
statistical machine translation. In Proc. 47th ACL,
pages 914–922.
Web-as-Corpus Consortium. 2008. SDeWaC — a 0.88
billion word corpus for german. Website: http:
//wacky.sslmit.unibo.it/doku.php.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computat. Linguist., 23(3):377–403.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008a. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. 46th ACL, pages 559–567.
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and
Sheng Li. 2008b. Grammar comparison study
for translational equivalence modeling and statis-
tical machine translation. In Proc. 22nd Inter-
national Conference on Computational Linguistics,
pages 1097–1104.
</reference>
<page confidence="0.998399">
821
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.365872">
<title confidence="0.998152">Shallow Local Multi Bottom-up Tree in Statistical Machine Translation</title>
<author confidence="0.971374">Braune Seemann Quernheim</author>
<affiliation confidence="0.82165">Institute for Natural Language Processing, University of</affiliation>
<address confidence="0.388826">Pfaffenwaldring 5b, 70569 Stuttgart,</address>
<abstract confidence="0.995904416666667">We present a new translation model integrating the shallow local multi bottomup tree transducer. We perform a largescale empirical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree baseon the WMT 2009 English translation task. As an additional contribution we make the developed software and complete tool-chain publicly available for further experimentation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andr´e Arnold</author>
<author>Max Dauchet</author>
</authors>
<title>Morphismes et bimorphismes d’arbres.</title>
<date>1982</date>
<journal>Theoret. Comput. Sci.,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="2940" citStr="Arnold and Dauchet (1982)" startWordPosition="440" endWordPosition="443">rse forests instead of single parses (Liu et al., 2009) or (ii) soft syntactic constraints (Chiang, 2010) have been developed to alleviate this problem. Another successful approach has been to switch to more powerful formalisms, which allow the extraction of more general rules. A particularly powerful model is the non-contiguous version of synchronous tree-sequence substitution grammars (STSSG) of Zhang et al. (2008a), Zhang et al. (2008b), and Sun et al. (2009), which allows sequences of trees on both sides of the rules [see also (Raoult, 1997)]. The multi bottom-up tree transducer (MBOT) of Arnold and Dauchet (1982) and Lilin (1978) offers a middle ground between traditional syntax-based models and STSSG. Roughly speaking, an MBOT is an STSSG, in which all the discontinuities must occur on the target language side (Maletti, 2011). This restriction yields many algorithmic advantages over both the traditional models as well as STSSG as demonstrated by Maletti (2010). Formally, they are expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called eMBOT, that we use (in a graphical representation of the trees and the alignment). In this c</context>
</contexts>
<marker>Arnold, Dauchet, 1982</marker>
<rawString>Andr´e Arnold and Max Dauchet. 1982. Morphismes et bimorphismes d’arbres. Theoret. Comput. Sci., 20(1):33–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky Wide Web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="23940" citStr="Baroni et al., 2009" startWordPosition="4284" endWordPosition="4287">EU Baseline 12.60 eMBOT ∗13.06 Moses t-to-s 12.72 Table 1: Evaluation results. The starred results are statistically significant improvements over the Baseline (at confidence p &lt; 0.05). English side of the bilingual data was parsed using the Charniak parser of Charniak and Johnson (2005), and the German side was parsed using BitPar (Schmid, 2004) without the function and morphological annotations. Our German 4- gram language model was trained on the German sentences in the training data augmented by the Stuttgart SdeWaC corpus (Web-as-Corpus Consortium, 2008), whose generation is detailed in (Baroni et al., 2009). The weights am in the log-linear model were trained using minimum error rate training (Och, 2003) with the News 2009 development set. Both systems use glue-rules, which allow them to concatenate partial translations without performing any reordering. 5.2 Results We measured the overall translation quality with the help of 4-gram BLEU (Papineni et al., 2002), which was computed on tokenized and lowercased data for both systems. The results of our evaluation are reported in Table 1. For comparison, we also report the results obtained by a system that utilizes parses only on the source side (Mo</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky Wide Web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2009 Workshop on Statistical Machine Translation. In Proc. 4th Workshop on Statistical Machine Translation,</booktitle>
<pages>1--28</pages>
<contexts>
<context position="22808" citStr="Callison-Burch et al., 2009" startWordPosition="4108" endWordPosition="4111">sind von nur”, etc. 5 Experiments 5.1 Setup The baseline system for our experiments is the syntax-based component of the Moses opensource toolkit of Koehn et al. (2007) and Hoang et al. (2009). We use linguistic syntactic annotation on both the source and the target language side (tree-to-tree). Our contrastive system is the eMBOT-based translation system presented here. We provide the system with a set of SCFG as well as eMBOT rules. We do not impose any maximal span restriction on either system. The compared systems are evaluated on the English-to-German13 news translation task of WMT 2009 (Callison-Burch et al., 2009). For both systems, the used training data is from the 4th version of the Europarl Corpus (Koehn, 2005) and the News Commentary corpus. Both translation models were trained with approximately 1.5 million bilingual sentences after length-ratio filtering. The word alignments were generated by GIZA++ (Och and Ney, 2003) with the growdiag-final-and heuristic (Koehn et al., 2005). The 13Note that our fMBOT-based system can be applied to any language pair as it involves no language-specific engineering. System BLEU Baseline 12.60 eMBOT ∗13.06 Moses t-to-s 12.72 Table 1: Evaluation results. The starr</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proc. 4th Workshop on Statistical Machine Translation, pages 1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. 43rd ACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="23608" citStr="Charniak and Johnson (2005)" startWordPosition="4231" endWordPosition="4235">with approximately 1.5 million bilingual sentences after length-ratio filtering. The word alignments were generated by GIZA++ (Och and Ney, 2003) with the growdiag-final-and heuristic (Koehn et al., 2005). The 13Note that our fMBOT-based system can be applied to any language pair as it involves no language-specific engineering. System BLEU Baseline 12.60 eMBOT ∗13.06 Moses t-to-s 12.72 Table 1: Evaluation results. The starred results are statistically significant improvements over the Baseline (at confidence p &lt; 0.05). English side of the bilingual data was parsed using the Charniak parser of Charniak and Johnson (2005), and the German side was parsed using BitPar (Schmid, 2004) without the function and morphological annotations. Our German 4- gram language model was trained on the German sentences in the training data augmented by the Stuttgart SdeWaC corpus (Web-as-Corpus Consortium, 2008), whose generation is detailed in (Baroni et al., 2009). The weights am in the log-linear model were trained using minimum error rate training (Och, 2003) with the News 2009 development set. Both systems use glue-rules, which allow them to concatenate partial translations without performing any reordering. 5.2 Results We </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and MaxEnt discriminative reranking. In Proc. 43rd ACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computat. Linguist.,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1017" citStr="Chiang, 2007" startWordPosition="135" endWordPosition="137">bottomup tree transducer. We perform a largescale empirical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree baseline on the WMT 2009 English → German translation task. As an additional contribution we make the developed software and complete tool-chain publicly available for further experimentation. 1 Introduction Besides phrase-based machine translation systems (Koehn et al., 2003), syntax-based systems have become widely used because of their ability to handle non-local reordering. Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al., 2009). However, those systems use linguistic syntactic annotation at different levels. For example, the systems proposed by Wu (1997) and Chiang (2007) use no linguistic information and are syntactic in a structural sense only. Huang et al. (2006) and Liu et al. (2006) use syntactic annotations on the source language side and show significant improvements in translation quality. Using syntax exclusively on the target language side has also b</context>
<context position="17400" citStr="Chiang, 2007" startWordPosition="3137" endWordPosition="3139">arget language side. Roughly speaking, SCFG rules are eMBOT rules with exactly one output tree. We thus had to extend the system to support our eMBOT rules, in which arbitrarily many output trees are allowed. The standard Moses syntax-based decoder uses a CYK+ chart parsing algorithm, in which each source sentence is parsed and contiguous spans are processed in a bottom-up fashion. A rule is applicable11 if the left-hand side of it matches the nonterminal assigned to the full span by the parser and the (non-)terminal assigned to each subspan.12 In order to speed up the decoding, cube pruning (Chiang, 2007) is applied to each chart cell in order to select the most likely hypotheses for subspans. The language model (LM) scoring is directly integrated into the cube pruning algorithm. Thus, LM estimates are available for all considered hypotheses. To accommodate eMBOT rules, we had to modify the Moses syntax-based decoder in several ways. First, the rule representation itself is adjusted to allow sequences of shallow output trees on the target side. Naturally, we also had to adjust hypothesis expansion and, most importantly, language model scoring inside the cube pruning algorithm. An overview of t</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computat. Linguist., 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proc. 48th ACL,</booktitle>
<pages>1443--1452</pages>
<contexts>
<context position="2137" citStr="Chiang (2010)" startWordPosition="314" endWordPosition="315">ments in translation quality. Using syntax exclusively on the target language side has also been successfully tried by Galley et al. (2004) and Galley et al. (2006). Nowadays, open-source toolkits such as Moses (Koehn et al., 2007) offer syntax-based components (Hoang et al., 2009), which allow experiments without expert knowledge. The improvements observed for systems using syntactic annotation on either the source or the target language side naturally led to experiments with models that use syntactic annotations on both sides. However, as noted by Lavie et al. (2008), Liu et al. (2009), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems become too restrictive. Several strategies such as (i) using parse forests instead of single parses (Liu et al., 2009) or (ii) soft syntactic constraints (Chiang, 2010) have been developed to alleviate this problem. Another successful approach has been to switch to more powerful formalisms, which allow the extraction of more general rules. A particularly powerful model is the non-contiguous version of synchronous tree-sequence substitution grammars (STSSG) of Zhang et al. (2008a),</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In Proc. 48th ACL, pages 1443– 1452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proc. 41st ACL,</booktitle>
<pages>205--208</pages>
<contexts>
<context position="1072" citStr="Eisner, 2003" startWordPosition="143" endWordPosition="144">ical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree baseline on the WMT 2009 English → German translation task. As an additional contribution we make the developed software and complete tool-chain publicly available for further experimentation. 1 Introduction Besides phrase-based machine translation systems (Koehn et al., 2003), syntax-based systems have become widely used because of their ability to handle non-local reordering. Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al., 2009). However, those systems use linguistic syntactic annotation at different levels. For example, the systems proposed by Wu (1997) and Chiang (2007) use no linguistic information and are syntactic in a structural sense only. Huang et al. (2006) and Liu et al. (2006) use syntactic annotations on the source language side and show significant improvements in translation quality. Using syntax exclusively on the target language side has also been successfully tried by Galley et al. (2004) and Gall</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proc. 41st ACL, pages 205–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule? In</title>
<date>2004</date>
<booktitle>Proc. HLT-NAACL,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="1663" citStr="Galley et al. (2004)" startWordPosition="236" endWordPosition="239">ution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al., 2009). However, those systems use linguistic syntactic annotation at different levels. For example, the systems proposed by Wu (1997) and Chiang (2007) use no linguistic information and are syntactic in a structural sense only. Huang et al. (2006) and Liu et al. (2006) use syntactic annotations on the source language side and show significant improvements in translation quality. Using syntax exclusively on the target language side has also been successfully tried by Galley et al. (2004) and Galley et al. (2006). Nowadays, open-source toolkits such as Moses (Koehn et al., 2007) offer syntax-based components (Hoang et al., 2009), which allow experiments without expert knowledge. The improvements observed for systems using syntactic annotation on either the source or the target language side naturally led to experiments with models that use syntactic annotations on both sides. However, as noted by Lavie et al. (2008), Liu et al. (2009), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems become too </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. HLT-NAACL, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve Deneefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. 44th ACL,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="1688" citStr="Galley et al. (2006)" startWordPosition="241" endWordPosition="244">003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al., 2009). However, those systems use linguistic syntactic annotation at different levels. For example, the systems proposed by Wu (1997) and Chiang (2007) use no linguistic information and are syntactic in a structural sense only. Huang et al. (2006) and Liu et al. (2006) use syntactic annotations on the source language side and show significant improvements in translation quality. Using syntax exclusively on the target language side has also been successfully tried by Galley et al. (2004) and Galley et al. (2006). Nowadays, open-source toolkits such as Moses (Koehn et al., 2007) offer syntax-based components (Hoang et al., 2009), which allow experiments without expert knowledge. The improvements observed for systems using syntactic annotation on either the source or the target language side naturally led to experiments with models that use syntactic annotations on both sides. However, as noted by Lavie et al. (2008), Liu et al. (2009), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems become too restrictive. Several stra</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, Deneefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve Deneefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. 44th ACL, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
<author>Adam Lopez</author>
</authors>
<title>A unified framework for phrase-based, hierarchical, and syntax-based statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. 6th Int. Workshop Spoken Language Translation,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="1806" citStr="Hoang et al., 2009" startWordPosition="258" endWordPosition="261"> those systems use linguistic syntactic annotation at different levels. For example, the systems proposed by Wu (1997) and Chiang (2007) use no linguistic information and are syntactic in a structural sense only. Huang et al. (2006) and Liu et al. (2006) use syntactic annotations on the source language side and show significant improvements in translation quality. Using syntax exclusively on the target language side has also been successfully tried by Galley et al. (2004) and Galley et al. (2006). Nowadays, open-source toolkits such as Moses (Koehn et al., 2007) offer syntax-based components (Hoang et al., 2009), which allow experiments without expert knowledge. The improvements observed for systems using syntactic annotation on either the source or the target language side naturally led to experiments with models that use syntactic annotations on both sides. However, as noted by Lavie et al. (2008), Liu et al. (2009), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems become too restrictive. Several strategies such as (i) using parse forests instead of single parses (Liu et al., 2009) or (ii) soft syntactic constraints </context>
<context position="16659" citStr="Hoang et al. (2009)" startWordPosition="3012" endWordPosition="3015"> ADJA Offizielle NP NN Prognosen PP von AP ADV nur JJ Official �→ � VBD predicted / VAFIN → sind , VVPP � ausgegangen RB just NP QP CD 3 NN �→ % NN � % CARD 3 JJ Official NP NNS forecasts S VBD predicted RB just NP CD 3 NN % QP → ADJA NN sind von AP NN Offizielle Prognosen ADV CARD % NP VAFIN PP VVPP !ausgegangen nur 3 Figure 6: Complex rule application. S NP VAFIN PP VVPP � � Offizielle Prognosen sind , ausgegangen von nur 3 % Figure 7: Illustration of LM scoring. 815 4 Decoding We implemented our model in the syntax-based component of the Moses open-source toolkit by Koehn et al. (2007) and Hoang et al. (2009). The standard Moses syntax-based decoder only handles SCFG rules; i.e, rules with contiguous components on the source and the target language side. Roughly speaking, SCFG rules are eMBOT rules with exactly one output tree. We thus had to extend the system to support our eMBOT rules, in which arbitrarily many output trees are allowed. The standard Moses syntax-based decoder uses a CYK+ chart parsing algorithm, in which each source sentence is parsed and contiguous spans are processed in a bottom-up fashion. A rule is applicable11 if the left-hand side of it matches the nonterminal assigned to </context>
<context position="22372" citStr="Hoang et al. (2009)" startWordPosition="4039" endWordPosition="4042">far, the LM scorer could only score their associated unigrams. However, we also have their associated strings wi and w2, which can now be used. Since VAFIN and VVPP now become parts of a single tree, we can perform LM scoring normally. Assembling the string we obtain Offizielle Prognosen sind von nur 3 % ausgegangen which is scored by the LM. Thus, we first score the 4-grams “Offizielle Prognosen sind von”, then “Prognosen sind von nur”, etc. 5 Experiments 5.1 Setup The baseline system for our experiments is the syntax-based component of the Moses opensource toolkit of Koehn et al. (2007) and Hoang et al. (2009). We use linguistic syntactic annotation on both the source and the target language side (tree-to-tree). Our contrastive system is the eMBOT-based translation system presented here. We provide the system with a set of SCFG as well as eMBOT rules. We do not impose any maximal span restriction on either system. The compared systems are evaluated on the English-to-German13 news translation task of WMT 2009 (Callison-Burch et al., 2009). For both systems, the used training data is from the 4th version of the Europarl Corpus (Koehn, 2005) and the News Commentary corpus. Both translation models were</context>
</contexts>
<marker>Hoang, Koehn, Lopez, 2009</marker>
<rawString>Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009. A unified framework for phrase-based, hierarchical, and syntax-based statistical machine translation. In Proc. 6th Int. Workshop Spoken Language Translation, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proc. 7th Conf. Association for Machine Translation of the Americas,</booktitle>
<pages>66--73</pages>
<contexts>
<context position="1419" citStr="Huang et al. (2006)" startWordPosition="196" endWordPosition="199">achine translation systems (Koehn et al., 2003), syntax-based systems have become widely used because of their ability to handle non-local reordering. Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al., 2009). However, those systems use linguistic syntactic annotation at different levels. For example, the systems proposed by Wu (1997) and Chiang (2007) use no linguistic information and are syntactic in a structural sense only. Huang et al. (2006) and Liu et al. (2006) use syntactic annotations on the source language side and show significant improvements in translation quality. Using syntax exclusively on the target language side has also been successfully tried by Galley et al. (2004) and Galley et al. (2006). Nowadays, open-source toolkits such as Moses (Koehn et al., 2007) offer syntax-based components (Hoang et al., 2009), which allow experiments without expert knowledge. The improvements observed for systems using syntactic annotation on either the source or the target language side naturally led to experiments with models that u</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. 7th Conf. Association for Machine Translation of the Americas, pages 66– 73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="847" citStr="Koehn et al., 2003" startWordPosition="110" endWordPosition="113">enwaldring 5b, 70569 Stuttgart, Germany {braunefe,seemanna,daniel,maletti}@ims.uni-stuttgart.de Abstract We present a new translation model integrating the shallow local multi bottomup tree transducer. We perform a largescale empirical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree baseline on the WMT 2009 English → German translation task. As an additional contribution we make the developed software and complete tool-chain publicly available for further experimentation. 1 Introduction Besides phrase-based machine translation systems (Koehn et al., 2003), syntax-based systems have become widely used because of their ability to handle non-local reordering. Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al., 2009). However, those systems use linguistic syntactic annotation at different levels. For example, the systems proposed by Wu (1997) and Chiang (2007) use no linguistic information and are syntactic in a structural sense only. Huang et al. (2006) and Liu et al. (2006) use s</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philip Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. HLT-NAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>In Proc. 2nd Int. Workshop Spoken Language Translation.</booktitle>
<contexts>
<context position="23185" citStr="Koehn et al., 2005" startWordPosition="4166" endWordPosition="4169">he system with a set of SCFG as well as eMBOT rules. We do not impose any maximal span restriction on either system. The compared systems are evaluated on the English-to-German13 news translation task of WMT 2009 (Callison-Burch et al., 2009). For both systems, the used training data is from the 4th version of the Europarl Corpus (Koehn, 2005) and the News Commentary corpus. Both translation models were trained with approximately 1.5 million bilingual sentences after length-ratio filtering. The word alignments were generated by GIZA++ (Och and Ney, 2003) with the growdiag-final-and heuristic (Koehn et al., 2005). The 13Note that our fMBOT-based system can be applied to any language pair as it involves no language-specific engineering. System BLEU Baseline 12.60 eMBOT ∗13.06 Moses t-to-s 12.72 Table 1: Evaluation results. The starred results are statistically significant improvements over the Baseline (at confidence p &lt; 0.05). English side of the bilingual data was parsed using the Charniak parser of Charniak and Johnson (2005), and the German side was parsed using BitPar (Schmid, 2004) without the function and morphological annotations. Our German 4- gram language model was trained on the German sent</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT Speech Translation Evaluation. In Proc. 2nd Int. Workshop Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="1755" citStr="Koehn et al., 2007" startWordPosition="251" endWordPosition="254"> substitution grammars (Sun et al., 2009). However, those systems use linguistic syntactic annotation at different levels. For example, the systems proposed by Wu (1997) and Chiang (2007) use no linguistic information and are syntactic in a structural sense only. Huang et al. (2006) and Liu et al. (2006) use syntactic annotations on the source language side and show significant improvements in translation quality. Using syntax exclusively on the target language side has also been successfully tried by Galley et al. (2004) and Galley et al. (2006). Nowadays, open-source toolkits such as Moses (Koehn et al., 2007) offer syntax-based components (Hoang et al., 2009), which allow experiments without expert knowledge. The improvements observed for systems using syntactic annotation on either the source or the target language side naturally led to experiments with models that use syntactic annotations on both sides. However, as noted by Lavie et al. (2008), Liu et al. (2009), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems become too restrictive. Several strategies such as (i) using parse forests instead of single parses (Li</context>
<context position="16635" citStr="Koehn et al. (2007)" startWordPosition="3007" endWordPosition="3010">PP �→ � NP NNS forecasts ADJA Offizielle NP NN Prognosen PP von AP ADV nur JJ Official �→ � VBD predicted / VAFIN → sind , VVPP � ausgegangen RB just NP QP CD 3 NN �→ % NN � % CARD 3 JJ Official NP NNS forecasts S VBD predicted RB just NP CD 3 NN % QP → ADJA NN sind von AP NN Offizielle Prognosen ADV CARD % NP VAFIN PP VVPP !ausgegangen nur 3 Figure 6: Complex rule application. S NP VAFIN PP VVPP � � Offizielle Prognosen sind , ausgegangen von nur 3 % Figure 7: Illustration of LM scoring. 815 4 Decoding We implemented our model in the syntax-based component of the Moses open-source toolkit by Koehn et al. (2007) and Hoang et al. (2009). The standard Moses syntax-based decoder only handles SCFG rules; i.e, rules with contiguous components on the source and the target language side. Roughly speaking, SCFG rules are eMBOT rules with exactly one output tree. We thus had to extend the system to support our eMBOT rules, in which arbitrarily many output trees are allowed. The standard Moses syntax-based decoder uses a CYK+ chart parsing algorithm, in which each source sentence is parsed and contiguous spans are processed in a bottom-up fashion. A rule is applicable11 if the left-hand side of it matches the </context>
<context position="22348" citStr="Koehn et al. (2007)" startWordPosition="4034" endWordPosition="4037">dependent until now. So far, the LM scorer could only score their associated unigrams. However, we also have their associated strings wi and w2, which can now be used. Since VAFIN and VVPP now become parts of a single tree, we can perform LM scoring normally. Assembling the string we obtain Offizielle Prognosen sind von nur 3 % ausgegangen which is scored by the LM. Thus, we first score the 4-grams “Offizielle Prognosen sind von”, then “Prognosen sind von nur”, etc. 5 Experiments 5.1 Setup The baseline system for our experiments is the syntax-based component of the Moses opensource toolkit of Koehn et al. (2007) and Hoang et al. (2009). We use linguistic syntactic annotation on both the source and the target language side (tree-to-tree). Our contrastive system is the eMBOT-based translation system presented here. We provide the system with a set of SCFG as well as eMBOT rules. We do not impose any maximal span restriction on either system. The compared systems are evaluated on the English-to-German13 news translation task of WMT 2009 (Callison-Burch et al., 2009). For both systems, the used training data is from the 4th version of the Europarl Corpus (Koehn, 2005) and the News Commentary corpus. Both</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="24910" citStr="Koehn (2004)" startWordPosition="4448" endWordPosition="4449">02), which was computed on tokenized and lowercased data for both systems. The results of our evaluation are reported in Table 1. For comparison, we also report the results obtained by a system that utilizes parses only on the source side (Moses tree-to-string) with its standard features. We can observe from Table 1 that our eMBOTbased system outperforms the baseline. We obtain a BLEU score of 13.06, which is a gain of 0.46 BLEU points over the baseline. This improvement is statistically significant at confidence p &lt; 0.05, which we computed using the pairwise bootstrap resampling technique of Koehn (2004). Our system is also better than the Moses tree-tostring system. However this improvement (0.34) is not statistically significant. In the next section, we confirm the result of the automatic evaluation through a manual examination of some translations generated by our system and the baseline. In Table 2, we report the number of eMBOT rules used by our system when decoding the test set. By lex we denote rules containing only lexical 817 lex non-term total contiguous 23,175 18,355 41,530 discontiguous 315 2,516 2,831 VP VB NP PP ~ NP ,PP ,VVINF � → NP PP VVINF S TO VP ( VP � → NP PP PTKZU VVINF </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. EMNLP, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. 10th Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="22911" citStr="Koehn, 2005" startWordPosition="4128" endWordPosition="4129">e Moses opensource toolkit of Koehn et al. (2007) and Hoang et al. (2009). We use linguistic syntactic annotation on both the source and the target language side (tree-to-tree). Our contrastive system is the eMBOT-based translation system presented here. We provide the system with a set of SCFG as well as eMBOT rules. We do not impose any maximal span restriction on either system. The compared systems are evaluated on the English-to-German13 news translation task of WMT 2009 (Callison-Burch et al., 2009). For both systems, the used training data is from the 4th version of the Europarl Corpus (Koehn, 2005) and the News Commentary corpus. Both translation models were trained with approximately 1.5 million bilingual sentences after length-ratio filtering. The word alignments were generated by GIZA++ (Och and Ney, 2003) with the growdiag-final-and heuristic (Koehn et al., 2005). The 13Note that our fMBOT-based system can be applied to any language pair as it involves no language-specific engineering. System BLEU Baseline 12.60 eMBOT ∗13.06 Moses t-to-s 12.72 Table 1: Evaluation results. The starred results are statistically significant improvements over the Baseline (at confidence p &lt; 0.05). Engli</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proc. 10th Machine Translation Summit, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Alok Parlikar</author>
<author>Vamshi Ambati</author>
</authors>
<title>Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora.</title>
<date>2008</date>
<booktitle>In Proc. 2nd ACL Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>87--95</pages>
<contexts>
<context position="2099" citStr="Lavie et al. (2008)" startWordPosition="305" endWordPosition="308">e language side and show significant improvements in translation quality. Using syntax exclusively on the target language side has also been successfully tried by Galley et al. (2004) and Galley et al. (2006). Nowadays, open-source toolkits such as Moses (Koehn et al., 2007) offer syntax-based components (Hoang et al., 2009), which allow experiments without expert knowledge. The improvements observed for systems using syntactic annotation on either the source or the target language side naturally led to experiments with models that use syntactic annotations on both sides. However, as noted by Lavie et al. (2008), Liu et al. (2009), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems become too restrictive. Several strategies such as (i) using parse forests instead of single parses (Liu et al., 2009) or (ii) soft syntactic constraints (Chiang, 2010) have been developed to alleviate this problem. Another successful approach has been to switch to more powerful formalisms, which allow the extraction of more general rules. A particularly powerful model is the non-contiguous version of synchronous tree-sequence substitution gra</context>
</contexts>
<marker>Lavie, Parlikar, Ambati, 2008</marker>
<rawString>Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008. Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora. In Proc. 2nd ACL Workshop on Syntax and Structure in Statistical Translation, pages 87–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Lilin</author>
</authors>
<title>Une g´en´eralisation des transducteurs d’´etats finis d’arbres: les S-transducteurs. Th`ese 3`eme cycle, Universit´e de Lille.</title>
<date>1978</date>
<contexts>
<context position="2957" citStr="Lilin (1978)" startWordPosition="445" endWordPosition="446">parses (Liu et al., 2009) or (ii) soft syntactic constraints (Chiang, 2010) have been developed to alleviate this problem. Another successful approach has been to switch to more powerful formalisms, which allow the extraction of more general rules. A particularly powerful model is the non-contiguous version of synchronous tree-sequence substitution grammars (STSSG) of Zhang et al. (2008a), Zhang et al. (2008b), and Sun et al. (2009), which allows sequences of trees on both sides of the rules [see also (Raoult, 1997)]. The multi bottom-up tree transducer (MBOT) of Arnold and Dauchet (1982) and Lilin (1978) offers a middle ground between traditional syntax-based models and STSSG. Roughly speaking, an MBOT is an STSSG, in which all the discontinuities must occur on the target language side (Maletti, 2011). This restriction yields many algorithmic advantages over both the traditional models as well as STSSG as demonstrated by Maletti (2010). Formally, they are expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called eMBOT, that we use (in a graphical representation of the trees and the alignment). In this contribution, we r</context>
</contexts>
<marker>Lilin, 1978</marker>
<rawString>Eric Lilin. 1978. Une g´en´eralisation des transducteurs d’´etats finis d’arbres: les S-transducteurs. Th`ese 3`eme cycle, Universit´e de Lille.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. 44th ACL,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1441" citStr="Liu et al. (2006)" startWordPosition="201" endWordPosition="204">ms (Koehn et al., 2003), syntax-based systems have become widely used because of their ability to handle non-local reordering. Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al., 2009). However, those systems use linguistic syntactic annotation at different levels. For example, the systems proposed by Wu (1997) and Chiang (2007) use no linguistic information and are syntactic in a structural sense only. Huang et al. (2006) and Liu et al. (2006) use syntactic annotations on the source language side and show significant improvements in translation quality. Using syntax exclusively on the target language side has also been successfully tried by Galley et al. (2004) and Galley et al. (2006). Nowadays, open-source toolkits such as Moses (Koehn et al., 2007) offer syntax-based components (Hoang et al., 2009), which allow experiments without expert knowledge. The improvements observed for systems using syntactic annotation on either the source or the target language side naturally led to experiments with models that use syntactic annotatio</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proc. 44th ACL, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Improving tree-to-tree translation with packed forests.</title>
<date>2009</date>
<booktitle>In Proc. 47th ACL,</booktitle>
<pages>558--566</pages>
<marker>Liu, L¨u, Liu, 2009</marker>
<rawString>Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proc. 47th ACL, pages 558–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>Why synchronous tree substitution grammars?</title>
<date>2010</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>876--884</pages>
<contexts>
<context position="3295" citStr="Maletti (2010)" startWordPosition="498" endWordPosition="499">e substitution grammars (STSSG) of Zhang et al. (2008a), Zhang et al. (2008b), and Sun et al. (2009), which allows sequences of trees on both sides of the rules [see also (Raoult, 1997)]. The multi bottom-up tree transducer (MBOT) of Arnold and Dauchet (1982) and Lilin (1978) offers a middle ground between traditional syntax-based models and STSSG. Roughly speaking, an MBOT is an STSSG, in which all the discontinuities must occur on the target language side (Maletti, 2011). This restriction yields many algorithmic advantages over both the traditional models as well as STSSG as demonstrated by Maletti (2010). Formally, they are expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called eMBOT, that we use (in a graphical representation of the trees and the alignment). In this contribution, we report on our novel statistical machine translation system that uses an eMBOT-based translation model. The theoretical foundations of eMBOT and their integration into our translation model are presented in Sections 2 and 3. In order to empirically evaluate the eMBOT model, we implemented a machine trans1A translation is sensible if it is</context>
</contexts>
<marker>Maletti, 2010</marker>
<rawString>Andreas Maletti. 2010. Why synchronous tree substitution grammars? In Proc. HLT-NAACL, pages 876–884.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>How to train your multi bottom-up tree transducer.</title>
<date>2011</date>
<booktitle>In Proc. 49th ACL,</booktitle>
<pages>825--834</pages>
<contexts>
<context position="3158" citStr="Maletti, 2011" startWordPosition="477" endWordPosition="478">which allow the extraction of more general rules. A particularly powerful model is the non-contiguous version of synchronous tree-sequence substitution grammars (STSSG) of Zhang et al. (2008a), Zhang et al. (2008b), and Sun et al. (2009), which allows sequences of trees on both sides of the rules [see also (Raoult, 1997)]. The multi bottom-up tree transducer (MBOT) of Arnold and Dauchet (1982) and Lilin (1978) offers a middle ground between traditional syntax-based models and STSSG. Roughly speaking, an MBOT is an STSSG, in which all the discontinuities must occur on the target language side (Maletti, 2011). This restriction yields many algorithmic advantages over both the traditional models as well as STSSG as demonstrated by Maletti (2010). Formally, they are expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called eMBOT, that we use (in a graphical representation of the trees and the alignment). In this contribution, we report on our novel statistical machine translation system that uses an eMBOT-based translation model. The theoretical foundations of eMBOT and their integration into our translation model are presented</context>
<context position="5342" citStr="Maletti (2011)" startWordPosition="831" endWordPosition="832">T decoder. We evaluate our new system on the WMT 2009 shared translation task English → German. The translation quality is automatically measured using BLEU scores, and we confirm the findings by providing linguistic evidence (see Section 5). Note that in contrast to several previous approaches, we perform large scale experiments by training systems with approx. 1.5 million parallel sentences. 2 Theoretical Model In this section, we present the theoretical generative model used in our approach to syntax-based machine translation. Essentially, it is the local multi bottom-up tree transducer of Maletti (2011) with the restriction that all rules must be shallow, which means that the left-hand side of each rule has height at most 2 (see Figure 2 for shallow rules and Figure 4 for rules including non-shallow rules). The rules extracted from the training example of Figure 3 are displayed in Figure 4. Those extracted rules are forcibly made shallow by removing internal nodes. The application of those rules is illustrated in Figures 5 and 6. For those that want to understand the inner workings, we recall the principal model in full detail in the rest of this section. Since we utilize syntactic parse tre</context>
<context position="7814" citStr="Maletti (2011)" startWordPosition="1341" endWordPosition="1342"> pos(t) |t(w) ∈ N, w leaf in t} be the set of all leaves labeled by elements of N. When N is the set of nonterminals, we call them leaf nonterminals. We extend this notion to sequences t1,...,tk ∈ TE by leaf(,) (t1, ... , tk) = U {iw |w ∈ leafN(ti)}. 1&lt;i&lt;k Let w1, ... , wn ∈ pos(t) be (pairwise prefixincomparable) positions and t1, ... , tn ∈ TE. Then t[wi ← ti]1&lt;i&lt;n denotes the tree that is obtained from t by replacing (in parallel) the subtrees at wi by ti for every 1 ≤ i ≤ n. Now we are ready to introduce our model, which is a minor variation of the local multi bottom-up tree transducer of Maletti (2011). Let Σ and Δ be the input and output symbols, respectively, and let N ⊆ Σ ∪ Δ be the set of nonterminal symbols. Essentially, the model works on pairs ht, (u1, ... , uk)i consisting of an input tree t ∈ TE 812 and a sequence u1, ... , uk E TA of output trees. Such pairs are pre-translations of rank k. The pretranslation (t, (u1, ... , uk)) is shallow if all trees t, u1, ... , uk in it are shallow. Together with a pre-translation we typically have to store an alignment. Given a pre-translation (t, (u1, ... , uk)) of rank k and 1 G i G k, we call ui the ith translation of t. An alignment for th</context>
<context position="9887" citStr="Maletti (2011)" startWordPosition="1742" endWordPosition="1743">rammar), but our right-hand sides contain a sequence of trees instead of just a single tree. In addition, the alignments in an SCFG rule are bijective between leaf nonterminals, whereas our model permits multiple alignments to a single leaf nonterminal in the left-hand side (see Figure 2). Our eMBOT rules are obtained automatically from data like that in Figure 3. Thus, we (word) align the bilingual text and parse it in both the source and the target language. In this manner we obtain sentence pairs like the one shown in Figure 3. To these sentence pairs we apply the rule extraction method of Maletti (2011). The rules extracted from the sentence pair of Figure 3 are shown in Figure 4. Note that these rules are not necessarily shallow (the last two rules are not). Thus, we post-process the extracted rules and make them shallow. The shallow rules corresponding to the non-shallow rules of Figure 4 are shown in Figure 2. Next, we define how to combine rules to form derivations. In contrast to most other models, we 2ran(f) for a mapping f : A → B denotes the range off, which is {f(a) |a E A}. Figure 3: Aligned parsed sentences. only introduce a derivation semantics that does not collapse multiple der</context>
<context position="11425" citStr="Maletti, 2011" startWordPosition="2056" endWordPosition="2057">= 1, rk(p, 2) = 2, and rk(p, 3) = 1. Definition 2 The set τ(R, c) of weighted pretranslations of an EMBOT (R, c) is the smallest set T subject to the following restriction: If there exist • a rule p = t -+ψ (u1, ... , uk) E R, • a weighted pre-translation (tw, cw, (uw1 , ... , uwkw)) E T for every w E leafN(t) with – rk(p, w) = kw,4 – t(w) = tw(ε),5 and – for every iw0 E leaf(k) N (u1, ... , uk),6 ui(w0) = uv j (ε) with 0(iw0) = (v, j), then (t0, c0, (u01, ... , u0k)) E T is a weighted pretranslation, where • t0 = t[w +-- tw |w E leafN(t)], 3A standard semantics is presented, for example, in (Maletti, 2011). 4If w has n alignments, then the pre-translation selected for it has to have suitably many output trees. 5The labels have to coincide for the input tree. 6Also the labels for the output trees have to coincide. NP VP ADJA PP sind NN VVPP NP VP VAFIN S JJ Official NP QP CD 3 NNS forecasts VBD predicted RB just NN % nur 3 von ADV CARD % Offizielle ausgegangen APPR AP NN S Prognosen 813 ( ( NN VBD ADJA NNS ( WIN VVPP RB r Offizielle forecasts i ` Prognosen ) predicted → ` sind ausgegangen ) just → \ nur CD → ( CARD) % → (N \ %N \ NP JJ NNS � NP � QP � AP � NP → → ADJA NN RB CD ADV CARD QP NN �→ </context>
</contexts>
<marker>Maletti, 2011</marker>
<rawString>Andreas Maletti. 2011. How to train your multi bottom-up tree transducer. In Proc. 49th ACL, pages 825–834.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>Every sensible extended topdown tree transducer is a multi bottom-up tree transducer.</title>
<date>2012</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>263--273</pages>
<contexts>
<context position="3386" citStr="Maletti, 2012" startWordPosition="511" endWordPosition="512">l. (2009), which allows sequences of trees on both sides of the rules [see also (Raoult, 1997)]. The multi bottom-up tree transducer (MBOT) of Arnold and Dauchet (1982) and Lilin (1978) offers a middle ground between traditional syntax-based models and STSSG. Roughly speaking, an MBOT is an STSSG, in which all the discontinuities must occur on the target language side (Maletti, 2011). This restriction yields many algorithmic advantages over both the traditional models as well as STSSG as demonstrated by Maletti (2010). Formally, they are expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called eMBOT, that we use (in a graphical representation of the trees and the alignment). In this contribution, we report on our novel statistical machine translation system that uses an eMBOT-based translation model. The theoretical foundations of eMBOT and their integration into our translation model are presented in Sections 2 and 3. In order to empirically evaluate the eMBOT model, we implemented a machine trans1A translation is sensible if it is of linear size increase and can be computed by some (potentially copying) top-down tree tr</context>
</contexts>
<marker>Maletti, 2012</marker>
<rawString>Andreas Maletti. 2012. Every sensible extended topdown tree transducer is a multi bottom-up tree transducer. In Proc. HLT-NAACL, pages 263–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computat. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="23126" citStr="Och and Ney, 2003" startWordPosition="4157" endWordPosition="4160">MBOT-based translation system presented here. We provide the system with a set of SCFG as well as eMBOT rules. We do not impose any maximal span restriction on either system. The compared systems are evaluated on the English-to-German13 news translation task of WMT 2009 (Callison-Burch et al., 2009). For both systems, the used training data is from the 4th version of the Europarl Corpus (Koehn, 2005) and the News Commentary corpus. Both translation models were trained with approximately 1.5 million bilingual sentences after length-ratio filtering. The word alignments were generated by GIZA++ (Och and Ney, 2003) with the growdiag-final-and heuristic (Koehn et al., 2005). The 13Note that our fMBOT-based system can be applied to any language pair as it involves no language-specific engineering. System BLEU Baseline 12.60 eMBOT ∗13.06 Moses t-to-s 12.72 Table 1: Evaluation results. The starred results are statistically significant improvements over the Baseline (at confidence p &lt; 0.05). English side of the bilingual data was parsed using the Charniak parser of Charniak and Johnson (2005), and the German side was parsed using BitPar (Schmid, 2004) without the function and morphological annotations. Our G</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computat. Linguist., 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. 41st ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="24039" citStr="Och, 2003" startWordPosition="4303" endWordPosition="4304">tically significant improvements over the Baseline (at confidence p &lt; 0.05). English side of the bilingual data was parsed using the Charniak parser of Charniak and Johnson (2005), and the German side was parsed using BitPar (Schmid, 2004) without the function and morphological annotations. Our German 4- gram language model was trained on the German sentences in the training data augmented by the Stuttgart SdeWaC corpus (Web-as-Corpus Consortium, 2008), whose generation is detailed in (Baroni et al., 2009). The weights am in the log-linear model were trained using minimum error rate training (Och, 2003) with the News 2009 development set. Both systems use glue-rules, which allow them to concatenate partial translations without performing any reordering. 5.2 Results We measured the overall translation quality with the help of 4-gram BLEU (Papineni et al., 2002), which was computed on tokenized and lowercased data for both systems. The results of our evaluation are reported in Table 1. For comparison, we also report the results obtained by a system that utilizes parses only on the source side (Moses tree-to-string) with its standard features. We can observe from Table 1 that our eMBOTbased sys</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. 41st ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. 40th ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="24301" citStr="Papineni et al., 2002" startWordPosition="4341" endWordPosition="4344">e function and morphological annotations. Our German 4- gram language model was trained on the German sentences in the training data augmented by the Stuttgart SdeWaC corpus (Web-as-Corpus Consortium, 2008), whose generation is detailed in (Baroni et al., 2009). The weights am in the log-linear model were trained using minimum error rate training (Och, 2003) with the News 2009 development set. Both systems use glue-rules, which allow them to concatenate partial translations without performing any reordering. 5.2 Results We measured the overall translation quality with the help of 4-gram BLEU (Papineni et al., 2002), which was computed on tokenized and lowercased data for both systems. The results of our evaluation are reported in Table 1. For comparison, we also report the results obtained by a system that utilizes parses only on the source side (Moses tree-to-string) with its standard features. We can observe from Table 1 that our eMBOTbased system outperforms the baseline. We obtain a BLEU score of 13.06, which is a gain of 0.46 BLEU points over the baseline. This improvement is statistically significant at confidence p &lt; 0.05, which we computed using the pairwise bootstrap resampling technique of Koe</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. 40th ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Claude Raoult</author>
</authors>
<title>Rational tree relations.</title>
<date>1997</date>
<journal>Bull. Belg. Math. Soc. Simon Stevin,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="2866" citStr="Raoult, 1997" startWordPosition="431" endWordPosition="432">ecome too restrictive. Several strategies such as (i) using parse forests instead of single parses (Liu et al., 2009) or (ii) soft syntactic constraints (Chiang, 2010) have been developed to alleviate this problem. Another successful approach has been to switch to more powerful formalisms, which allow the extraction of more general rules. A particularly powerful model is the non-contiguous version of synchronous tree-sequence substitution grammars (STSSG) of Zhang et al. (2008a), Zhang et al. (2008b), and Sun et al. (2009), which allows sequences of trees on both sides of the rules [see also (Raoult, 1997)]. The multi bottom-up tree transducer (MBOT) of Arnold and Dauchet (1982) and Lilin (1978) offers a middle ground between traditional syntax-based models and STSSG. Roughly speaking, an MBOT is an STSSG, in which all the discontinuities must occur on the target language side (Maletti, 2011). This restriction yields many algorithmic advantages over both the traditional models as well as STSSG as demonstrated by Maletti (2010). Formally, they are expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called eMBOT, that we use</context>
</contexts>
<marker>Raoult, 1997</marker>
<rawString>Jean-Claude Raoult. 1997. Rational tree relations. Bull. Belg. Math. Soc. Simon Stevin, 4(1):149–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient parsing of highly ambiguous context-free grammars with bit vectors.</title>
<date>2004</date>
<booktitle>In Proc. 20th COLING,</booktitle>
<pages>162--168</pages>
<contexts>
<context position="23668" citStr="Schmid, 2004" startWordPosition="4245" endWordPosition="4246">ering. The word alignments were generated by GIZA++ (Och and Ney, 2003) with the growdiag-final-and heuristic (Koehn et al., 2005). The 13Note that our fMBOT-based system can be applied to any language pair as it involves no language-specific engineering. System BLEU Baseline 12.60 eMBOT ∗13.06 Moses t-to-s 12.72 Table 1: Evaluation results. The starred results are statistically significant improvements over the Baseline (at confidence p &lt; 0.05). English side of the bilingual data was parsed using the Charniak parser of Charniak and Johnson (2005), and the German side was parsed using BitPar (Schmid, 2004) without the function and morphological annotations. Our German 4- gram language model was trained on the German sentences in the training data augmented by the Stuttgart SdeWaC corpus (Web-as-Corpus Consortium, 2008), whose generation is detailed in (Baroni et al., 2009). The weights am in the log-linear model were trained using minimum error rate training (Och, 2003) with the News 2009 development set. Both systems use glue-rules, which allow them to concatenate partial translations without performing any reordering. 5.2 Results We measured the overall translation quality with the help of 4-</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient parsing of highly ambiguous context-free grammars with bit vectors. In Proc. 20th COLING, pages 162–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Sun</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
</authors>
<title>A noncontiguous tree sequence alignment-based model for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. 47th ACL,</booktitle>
<pages>914--922</pages>
<contexts>
<context position="1177" citStr="Sun et al., 2009" startWordPosition="157" endWordPosition="160">e-to-tree baseline on the WMT 2009 English → German translation task. As an additional contribution we make the developed software and complete tool-chain publicly available for further experimentation. 1 Introduction Besides phrase-based machine translation systems (Koehn et al., 2003), syntax-based systems have become widely used because of their ability to handle non-local reordering. Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al., 2009). However, those systems use linguistic syntactic annotation at different levels. For example, the systems proposed by Wu (1997) and Chiang (2007) use no linguistic information and are syntactic in a structural sense only. Huang et al. (2006) and Liu et al. (2006) use syntactic annotations on the source language side and show significant improvements in translation quality. Using syntax exclusively on the target language side has also been successfully tried by Galley et al. (2004) and Galley et al. (2006). Nowadays, open-source toolkits such as Moses (Koehn et al., 2007) offer syntax-based co</context>
<context position="2781" citStr="Sun et al. (2009)" startWordPosition="414" endWordPosition="417">tic information on both sides tends to decrease translation quality because the systems become too restrictive. Several strategies such as (i) using parse forests instead of single parses (Liu et al., 2009) or (ii) soft syntactic constraints (Chiang, 2010) have been developed to alleviate this problem. Another successful approach has been to switch to more powerful formalisms, which allow the extraction of more general rules. A particularly powerful model is the non-contiguous version of synchronous tree-sequence substitution grammars (STSSG) of Zhang et al. (2008a), Zhang et al. (2008b), and Sun et al. (2009), which allows sequences of trees on both sides of the rules [see also (Raoult, 1997)]. The multi bottom-up tree transducer (MBOT) of Arnold and Dauchet (1982) and Lilin (1978) offers a middle ground between traditional syntax-based models and STSSG. Roughly speaking, an MBOT is an STSSG, in which all the discontinuities must occur on the target language side (Maletti, 2011). This restriction yields many algorithmic advantages over both the traditional models as well as STSSG as demonstrated by Maletti (2010). Formally, they are expressive enough to express all sensible translations (Maletti, </context>
</contexts>
<marker>Sun, Zhang, Tan, 2009</marker>
<rawString>Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A noncontiguous tree sequence alignment-based model for statistical machine translation. In Proc. 47th ACL, pages 914–922.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Web-as-Corpus Consortium</author>
</authors>
<title>SDeWaC — a 0.88 billion word corpus for german. Website:</title>
<date>2008</date>
<note>http: //wacky.sslmit.unibo.it/doku.php.</note>
<contexts>
<context position="23885" citStr="Consortium, 2008" startWordPosition="4277" endWordPosition="4278">involves no language-specific engineering. System BLEU Baseline 12.60 eMBOT ∗13.06 Moses t-to-s 12.72 Table 1: Evaluation results. The starred results are statistically significant improvements over the Baseline (at confidence p &lt; 0.05). English side of the bilingual data was parsed using the Charniak parser of Charniak and Johnson (2005), and the German side was parsed using BitPar (Schmid, 2004) without the function and morphological annotations. Our German 4- gram language model was trained on the German sentences in the training data augmented by the Stuttgart SdeWaC corpus (Web-as-Corpus Consortium, 2008), whose generation is detailed in (Baroni et al., 2009). The weights am in the log-linear model were trained using minimum error rate training (Och, 2003) with the News 2009 development set. Both systems use glue-rules, which allow them to concatenate partial translations without performing any reordering. 5.2 Results We measured the overall translation quality with the help of 4-gram BLEU (Papineni et al., 2002), which was computed on tokenized and lowercased data for both systems. The results of our evaluation are reported in Table 1. For comparison, we also report the results obtained by a </context>
</contexts>
<marker>Consortium, 2008</marker>
<rawString>Web-as-Corpus Consortium. 2008. SDeWaC — a 0.88 billion word corpus for german. Website: http: //wacky.sslmit.unibo.it/doku.php.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computat. Linguist.,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1305" citStr="Wu (1997)" startWordPosition="178" endWordPosition="179">omplete tool-chain publicly available for further experimentation. 1 Introduction Besides phrase-based machine translation systems (Koehn et al., 2003), syntax-based systems have become widely used because of their ability to handle non-local reordering. Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al., 2009). However, those systems use linguistic syntactic annotation at different levels. For example, the systems proposed by Wu (1997) and Chiang (2007) use no linguistic information and are syntactic in a structural sense only. Huang et al. (2006) and Liu et al. (2006) use syntactic annotations on the source language side and show significant improvements in translation quality. Using syntax exclusively on the target language side has also been successfully tried by Galley et al. (2004) and Galley et al. (2006). Nowadays, open-source toolkits such as Moses (Koehn et al., 2007) offer syntax-based components (Hoang et al., 2009), which allow experiments without expert knowledge. The improvements observed for systems using syn</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computat. Linguist., 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proc. 46th ACL,</booktitle>
<pages>559--567</pages>
<contexts>
<context position="2734" citStr="Zhang et al. (2008" startWordPosition="405" endWordPosition="408">9), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems become too restrictive. Several strategies such as (i) using parse forests instead of single parses (Liu et al., 2009) or (ii) soft syntactic constraints (Chiang, 2010) have been developed to alleviate this problem. Another successful approach has been to switch to more powerful formalisms, which allow the extraction of more general rules. A particularly powerful model is the non-contiguous version of synchronous tree-sequence substitution grammars (STSSG) of Zhang et al. (2008a), Zhang et al. (2008b), and Sun et al. (2009), which allows sequences of trees on both sides of the rules [see also (Raoult, 1997)]. The multi bottom-up tree transducer (MBOT) of Arnold and Dauchet (1982) and Lilin (1978) offers a middle ground between traditional syntax-based models and STSSG. Roughly speaking, an MBOT is an STSSG, in which all the discontinuities must occur on the target language side (Maletti, 2011). This restriction yields many algorithmic advantages over both the traditional models as well as STSSG as demonstrated by Maletti (2010). Formally, they are expressive enough </context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008a. A tree sequence alignment-based tree-to-tree translation model. In Proc. 46th ACL, pages 559–567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Haizhou Li</author>
<author>Aiti Aw</author>
<author>Sheng Li</author>
</authors>
<title>Grammar comparison study for translational equivalence modeling and statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. 22nd International Conference on Computational Linguistics,</booktitle>
<pages>1097--1104</pages>
<contexts>
<context position="2734" citStr="Zhang et al. (2008" startWordPosition="405" endWordPosition="408">9), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems become too restrictive. Several strategies such as (i) using parse forests instead of single parses (Liu et al., 2009) or (ii) soft syntactic constraints (Chiang, 2010) have been developed to alleviate this problem. Another successful approach has been to switch to more powerful formalisms, which allow the extraction of more general rules. A particularly powerful model is the non-contiguous version of synchronous tree-sequence substitution grammars (STSSG) of Zhang et al. (2008a), Zhang et al. (2008b), and Sun et al. (2009), which allows sequences of trees on both sides of the rules [see also (Raoult, 1997)]. The multi bottom-up tree transducer (MBOT) of Arnold and Dauchet (1982) and Lilin (1978) offers a middle ground between traditional syntax-based models and STSSG. Roughly speaking, an MBOT is an STSSG, in which all the discontinuities must occur on the target language side (Maletti, 2011). This restriction yields many algorithmic advantages over both the traditional models as well as STSSG as demonstrated by Maletti (2010). Formally, they are expressive enough </context>
</contexts>
<marker>Zhang, Jiang, Li, Aw, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and Sheng Li. 2008b. Grammar comparison study for translational equivalence modeling and statistical machine translation. In Proc. 22nd International Conference on Computational Linguistics, pages 1097–1104.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>