<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000178">
<title confidence="0.948586">
Putting it Simply: a Context-Aware Approach to Lexical Simplification
</title>
<author confidence="0.646751">
No´emie Elhadad
</author>
<affiliation confidence="0.6970705">
Biomedical Informatics
Columbia University
</affiliation>
<address confidence="0.983366">
New York, NY 10032
</address>
<email confidence="0.995962">
noemie@dbmi.columbia.edu
</email>
<author confidence="0.984578">
Or Biran
</author>
<affiliation confidence="0.992636">
Computer Science
Columbia University
</affiliation>
<address confidence="0.989618">
New York, NY 10027
</address>
<email confidence="0.998443">
ob2008@columbia.edu
</email>
<author confidence="0.9279">
Samuel Brody
</author>
<affiliation confidence="0.9008115">
Communication &amp; Information
Rutgers University
</affiliation>
<address confidence="0.98403">
New Brunswick, NJ 08901
</address>
<email confidence="0.999088">
sdbrody@gmail.com
</email>
<sectionHeader confidence="0.993895" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995214">
We present a method for lexical simplifica-
tion. Simplification rules are learned from a
comparable corpus, and the rules are applied
in a context-aware fashion to input sentences.
Our method is unsupervised. Furthermore, it
does not require any alignment or correspon-
dence among the complex and simple corpora.
We evaluate the simplification according to
three criteria: preservation of grammaticality,
preservation of meaning, and degree of sim-
plification. Results show that our method out-
performs an established simplification base-
line for both meaning preservation and sim-
plification, while maintaining a high level of
grammaticality.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998825857142857">
The task of simplification consists of editing an in-
put text into a version that is less complex linguisti-
cally or more readable. Automated sentence sim-
plification has been investigated mostly as a pre-
processing step with the goal of improving NLP
tasks, such as parsing (Chandrasekar et al., 1996;
Siddharthan, 2004; Jonnalagadda et al., 2009), se-
mantic role labeling (Vickrey and Koller, 2008) and
summarization (Blake et al., 2007). Automated sim-
plification can also be considered as a way to help
end users access relevant information, which would
be too complex to understand if left unedited. As
such, it was proposed as a tool for adults with
aphasia (Carroll et al., 1998; Devlin and Unthank,
2006), hearing-impaired people (Daelemans et al.,
2004), readers with low-literacy skills (Williams and
Reiter, 2005), individuals with intellectual disabil-
ities (Huenerfauth et al., 2009), as well as health
INPUT: In 1900, Omaha was the center of a national
uproar over the kidnapping of Edward Cudahy, Jr., the
son of a local meatpacking magnate.
</bodyText>
<sectionHeader confidence="0.683953" genericHeader="method">
CANDIDATE RULES:
</sectionHeader>
<bodyText confidence="0.951685">
{magnate → king} {magnate → businessman}
OUTPUT: In 1900, Omaha was the center of a national
uproar over the kidnapping of Edward Cudahy, Jr., the
son of a local meatpacking businessman.
</bodyText>
<figureCaption confidence="0.9973745">
Figure 1: Input sentence, candidate simplification rules,
and output sentence.
</figureCaption>
<bodyText confidence="0.998891541666667">
consumers looking for medical information (El-
hadad and Sutaria, 2007; Del´eger and Zweigen-
baum, 2009).
Simplification can take place at different levels of
a text – its overall document structure, the syntax
of its sentences, and the individual phrases or words
in a sentence. In this paper, we present a sentence
simplification approach, which focuses on lexical
simplification.1 The key contributions of our work
are (i) an unsupervised method for learning pairs of
complex and simpler synonyms; and (ii) a context-
aware method for substituting one for the other.
Figure 1 shows an example input sentence. The
word magnate is determined as a candidate for sim-
plification. Two learned rules are available to the
simplification system (substitute magnate with king
or with businessman). In the context of this sen-
tence, the second rule is selected, resulting in the
simpler output sentence.
Our method contributes to research on lexical
simplification (both learning of rules and actual sen-
tence simplification), a topic little investigated thus
far. From a technical perspective, the task of lexi-
cal simplification bears similarity with that of para-
</bodyText>
<footnote confidence="0.9984565">
1Our resulting system is available for download at
http://www.cs.columbia.edu/ ob2008/
</footnote>
<page confidence="0.960438">
496
</page>
<note confidence="0.596306">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 496–501,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.979608446808511">
phrase identification (Androutsopoulos and Malaka- was excluded, leaving only the main body text of
siotis, 2010) and the SemEval-2007 English Lexi- the article. Further preprocessing was carried out
cal Substitution Task (McCarthy and Navigli, 2007). with the Stanford NLP Package5 to tokenize the text,
However, these do not consider issues of readabil- transform all words to lower case, and identify sen-
ity and linguistic complexity. Our methods lever- tence boundaries.
age a large comparable collection of texts: En-
glish Wikipedia2 and Simple English Wikipedia3.
Napoles and Dredze (2010) examined Wikipedia
Simple articles looking for features that characterize
a simple text, with the hope of informing research
in automatic simplification methods. Yatskar et al.
(2010) learn lexical simplification rules from the edit
histories of Wikipedia Simple articles. Our method
differs from theirs, as we rely on the two corpora as a
whole, and do not require any aligned or designated
simple/complex sentences when learning simplifica-
tion rules.4
3 Method
Our sentence simplification system consists of two
main stages: rule extraction and simplification. In
the first stage, simplification rules are extracted from
the corpora. Each rule consists of an ordered word
pair {original —* simplified} along with a score indi-
cating the similarity between the words. In the sec-
ond stage, the system decides whether to apply a rule
(i.e., transform the original word into the simplified
one), based on the contextual information.
3.1 Stage 1: Learning Simplification Rules
3.1.1 Obtaining Word Pairs
All content words in the English Wikipedia Cor-
pus (excluding stop words, numbers, and punctua-
tion) were considered as candidates for simplifica-
tion. For each candidate word w, we constructed a
context vector CV, containing co-occurrence infor-
mation within a 10-token window. Each dimension
i in the vector corresponds to a single word wi in
the vocabulary, and a single dimension was added to
represent any number token. The value in each di-
mension CV�[i] of the vector was the number of oc-
currences of the corresponding word wi within a ten-
token window surrounding an instance of the candi-
date word w. Values below a cutoff (2 in our exper-
iments) were discarded to reduce noise and increase
performance.
Next, we consider candidates for substitution.
From all possible word pairs (the Cartesian product
of all words in the corpus vocabulary), we first re-
move pairs of morphological variants. For this pur-
pose, we use MorphAdorner6 for lemmatization, re-
moving words which share a common lemma. We
also prune pairs where one word is a prefix of the
other and the suffix is in {s, es, ed, ly, er, ing}. This
handles some cases which are not covered by Mor-
phAdorner. We use WordNet (Fellbaum, 1998) as
a primary semantic filter. From all remaining word
pairs, we select those in which the second word, in
2 Data
We rely on two collections – English Wikipedia
(EW) and Simple English Wikipedia (SEW). SEW
is a Wikipedia project providing articles in Sim-
ple English, a version of English which uses fewer
words and easier grammar, and which aims to be
easier to read for children, people who are learning
English and people with learning difficulties. Due to
the labor involved in simplifying Wikipedia articles,
only about 2% of the EW articles have been simpli-
fied.
Our method does not assume any specific align-
ment or correspondance between individual EW and
SEW articles. Rather, we leverage SEW only as
an example of an in-domain simple corpus, in or-
der to extract word frequency estimates. Further-
more, we do not make use of any special properties
of Wikipedia (e.g., edit histories). In practice, this
means that our method is suitable for other cases
where there exists a simplified corpus in the same
domain.
The corpora are a snapshot as of April 23, 2010.
EW contains 3,266,245 articles, and SEW contains
60,100 articles. The articles were preprocessed as
follows: all comments, HTML tags, and Wiki links
were removed. Text contained in tables and figures
2http://en.wikipedia.org
3http://simple.wikipedia.org
4Aligning sentences in monolingual comparable corpora has
been investigated (Barzilay and Elhadad, 2003; Nelken and
Shieber, 2006), but is not a focus for this work.
5http://nlp.stanford.edu/software/index.shtml
6http://morphadorner.northwestern.edu
497
its first sense (as listed in WordNet)7 is a synonym
or hypernym of the first.
Finally, we compute the cosine similarity scores
for the remaining pairs using their context vectors.
</bodyText>
<subsectionHeader confidence="0.564759">
3.1.2 Ensuring Simplification
</subsectionHeader>
<bodyText confidence="0.999872285714286">
From among our remaining candidate word pairs,
we want to identify those that represent a complex
word which can be replaced by a simpler one. Our
definition of the complexity of a word is based on
two measures: the corpus complexity and the lexical
complexity. Specifically, we define the corpus com-
plexity of a word as
</bodyText>
<equation confidence="0.8958185">
Cw = fw,English
fw,Simple
</equation>
<bodyText confidence="0.9999055">
where fw,e is the frequency of word w in corpus c,
and the lexical complexity as Lw = jwj, the length
of the word. The final complexity Xw for the word
is given by the product of the two.
</bodyText>
<equation confidence="0.986533">
Xw = Cw x Lw
</equation>
<bodyText confidence="0.9999498">
After calculating the complexity of all words par-
ticipating in the word pairs, we discard the pairs for
which the first word’s complexity is lower than that
of the second. The remaining pairs constitute the
final list of substitution candidates.
</bodyText>
<subsectionHeader confidence="0.734706">
3.1.3 Ensuring Grammaticality
</subsectionHeader>
<bodyText confidence="0.859184625">
To ensure that our simplification substitutions
maintain the grammaticality of the original sentence,
we generate grammatically consistent rules from
the substitution candidate list. For each candidate
pair (original, simplified), we generate all consis-
tent forms (fi(original), fi(substitute)) of the two
words using MorphAdorner. For verbs, we create
the forms for all possible combinations of tenses and
persons, and for nouns we create forms for both sin-
gular and plural.
For example, the word pair (stride, walk) will gen-
erate the form pairs (stride, walk), (striding, walk-
ing), (strode, walked) and (strides, walks). Signifi-
cantly, the word pair (stride, walked) will generate
7Senses in WordNet are listed in order of frequency. Rather
than attempting explicit disambiguation and adding complex-
ity to the model, we rely on the first sense heuristic, which is
know to be very strong, along with contextual information, as
described in Section 3.2.
exactly the same list of form pairs, eliminating the
original ungrammatical pair.
Finally, each pair (fi(original), fi(substitute)) be-
comes a rule {fi(original) —* fi(substitute)},
with weight Similarity(original, substitute).
</bodyText>
<subsectionHeader confidence="0.989984">
3.2 Stage 2: Sentence Simplification
</subsectionHeader>
<bodyText confidence="0.999911314285714">
Given an input sentence and the set of rules learned
in the first stage, this stage determines which words
in the sentence should be simplified, and applies
the corresponding rules. The rules are not applied
blindly, however; the context of the input sentence
influences the simplification in two ways:
Word-Sentence Similarity First, we want to en-
sure that the more complex word, which we are at-
tempting to simplify, was not used precisely because
of its complexity - to emphasize a nuance or for its
specific shade of meaning. For example, suppose we
have a rule {Han —* Chinese}. We would want to
apply it to a sentence such as “In 1368 Han rebels
drove out the Mongols”, but to avoid applying it to
a sentence like “The history of the Han ethnic group
is closely tied to that of China”. The existence of
related words like ethnic and China are clues that
the latter sentence is in a specific, rather than gen-
eral, context and therefore a more general and sim-
pler hypernym is unsuitable. To identify such cases,
we calculate the similarity between the target word
(the candidate for replacement) and the input sen-
tence as a whole. If this similarity is too high, it
might be better not to simplify the original word.
Context Similarity The second factor has to do
with ambiguity. We wish to detect and avoid cases
where a word appears in the sentence with a differ-
ent sense than the one originally considered when
creating the simplification rule. For this purpose, we
examine the similarity between the rule as a whole
(including both the original and the substitute words,
and their associated context vectors) and the context
of the input sentence. If the similarity is high, it is
likely the original word in the sentence and the rule
are about the same sense.
</bodyText>
<subsectionHeader confidence="0.617518">
3.2.1 Simplification Procedure
</subsectionHeader>
<bodyText confidence="0.999273">
Both factors described above require sufficient
context in the input sentence. Therefore, our sys-
tem does not attempt to simplify sentences with less
than seven content words.
</bodyText>
<page confidence="0.997268">
498
</page>
<table confidence="0.998593666666667">
Type Gram. Mean. Simp.
Baseline 70.23(+13.10)% 55.95% 46.43%
System 77.91(+8.14)% 62.79% 75.58%
</table>
<tableCaption confidence="0.9937155">
Table 1: Average scores in three categories: grammatical-
ity (Gram.), meaning preservation (Mean.) and simplifi-
cation (Simp.). For grammaticality, we show percent of
examples judged as good, with ok percent in parentheses.
</tableCaption>
<bodyText confidence="0.991871">
For all other sentences, each content word is ex-
amined in order, ignoring words inside quotation
marks or parentheses. For each word w, the set of
relevant simplification rules {w —* x} is retrieved.
For each rule {w _* x}, unless the replacement
word x already appears in the sentence, our system
does the following:
</bodyText>
<listItem confidence="0.850314066666667">
• Build the vector of sentence context 5CV3,,,, in a
similar manner to that described in Section 3.1,
using the words in a 10-token window surround-
ing w in the input sentence.
• Calculate the cosine similarity of CV� and
5CV3,,,,. If this value is larger than a manually
specified threshold (0.1 in our experiments), do
not use this rule.
• Create a common context vector CCV,,,x for the
rule {w —* x}. The vector contains all fea-
tures common to both words, with the feature
values that are the minimum between them. In
other words, CCV�,x[i] = min(CV„[i], CVx[i]).
We calculate the cosine similarity of the common
context vector and the sentence context vector:
</listItem>
<equation confidence="0.939212">
Context5im = cosine(CCV�,x, 5CV3,,,)
</equation>
<bodyText confidence="0.99983225">
If the context similarity is larger than a threshold
(0.01), we use this rule to simplify.
If multiple rules apply for the same word, we use
the one with the highest context similarity.
</bodyText>
<sectionHeader confidence="0.995529" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999636777777778">
Baseline We employ the method of Devlin and
Unthank (2006) which replaces a word with its most
frequent synonym (presumed to be the simplest) as
our baseline. To provide a fairer comparison to our
system, we add the restriction that the synonyms
should not share a prefix of four or more letters
(a baseline version of lemmatization) and use Mor-
phAdorner to produce a form that agrees with that
of the original word.
</bodyText>
<table confidence="0.996494857142857">
Type Freq. Gram. Mean. Simp.
Base High 63.33(+20)% 46.67% 50%
Sys. High 76.67(+6.66)% 63.33% 73.33%
Base Med 75(+7.14)% 67.86% 42.86%
Sys. Med 72.41(+17.25)% 75.86% 82.76%
Base Low 73.08(+11.54)% 53.85% 46.15%
Sys. Low 85.19(+0)% 48.15% 70.37%
</table>
<tableCaption confidence="0.999106">
Table 2: Average scores by frequency band
</tableCaption>
<bodyText confidence="0.981605358974359">
Evaluation Dataset We sampled simplification
examples for manual evaluation with the following
criteria. Among all sentences in English Wikipedia,
we first extracted those where our system chose to
simplify exactly one word, to provide a straightfor-
ward example for the human judges. Of these, we
chose the sentences where the baseline could also
be used to simplify the target word (i.e., the word
had a more frequent synonym), and the baseline re-
placement was different from the system choice. We
included only a single example (simplified sentence)
for each rule.
The evaluation dataset contained 65 sentences.
Each was simplified by our system and the baseline,
resulting in 130 simplification examples (consisting
of an original and a simplified sentence).
Frequency Bands Although we included only a
single example of each rule, some rules could be
applied much more frequently than others, as the
words and associated contexts were common in the
dataset. Since this factor strongly influences the
utility of the system, we examined the performance
along different frequency bands. We split the eval-
uation dataset into three frequency bands of roughly
equal size, resulting in 46 high, 44 med and 40 low.
Judgment Guidelines We divided the simplifica-
tion examples among three annotators 8 and ensured
that no annotator saw both the system and baseline
examples for the same sentence. Each simplification
example was rated on three scales: Grammaticality
- bad, ok, or good; Meaning - did the transforma-
tion preserve the original meaning of the sentence;
and Simplification - did the transformation result in
8The annotators were native English speakers and were not
the authors of this paper. A small portion of the sentence pairs
were duplicated among annotators to calculate pairwise inter-
annotator agreement. Agreement was moderate in all categories
(Cohen’s Kappa = .350 − .455 for Simplicity, .475 − .530 for
Meaning and .415 − .425 for Grammaticality).
</bodyText>
<page confidence="0.997645">
499
</page>
<figure confidence="0.756329333333333">
a simpler sentence. our method to larger spans of texts, beyond individ-
ual words.
5 Results and Discussion
</figure>
<sectionHeader confidence="0.893022" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.96435476">
Table 1 shows the overall results for the experiment.
Our method is quantitatively better than the base-
line at both grammaticality and meaning preserva-
tion, although the difference is not statistically sig-
nificant. For our main goal of simplification, our
method significantly (p &lt; 0.001) outperforms the
baseline, which represents the established simplifi-
cation strategy of substituting a word with its most
frequent WordNet synonym. The results demon-
strate the value of correctly representing and ad-
dressing content when attempting automatic simpli-
fication.
Table 2 contains the results for each of the fre-
quency bands. Grammaticality is not strongly influ-
enced by frequency, and remains between 80-85%
for both the baseline and our system (considering
the ok judgment as positive). This is not surpris-
ing, since the method for ensuring grammaticality is
largely independent of context, and relies mostly on
a morphological engine. Simplification varies some-
what with frequency, with the best results for the
medium frequency band. In all bands, our system is
significantly better than the baseline. The most no-
ticeable effect is for preservation of meaning. Here,
the performance of the system (and the baseline) is
the best for the medium frequency group. However,
the performance drops significantly for the low fre-
quency band. This is most likely due to sparsity of
data. Since there are few examples from which to
learn, the system is unable to effectively distinguish
between different contexts and meanings of the word
being simplified, and applies the simplification rule
incorrectly.
These results indicate our system can be effec-
tively used for simplification of words that occur
frequently in the domain. In many scenarios, these
are precisely the cases where simplification is most
desirable. For rare words, it may be advisable to
maintain the more complex form, to ensure that the
meaning is preserved.
Future Work Because the method does not place
any restrictions on the complex and simple corpora,
we plan to validate it on different domains and ex-
pect it to be easily portable. We also plan to extend
Androutsopoulos, Ion and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. Journal of Artificial Intelligence
Research 38:135–187.
Barzilay, Regina and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable cor-
</bodyText>
<reference confidence="0.996515771428571">
pora. In Proc. EMNLP. pages 25–32.
Blake, Catherine, Julia Kampov, Andreas Or-
phanides, David West, and Cory Lown. 2007.
Query expansion, lexical simplification, and sen-
tence selection strategies for multi-document
summarization. In Proc. DUC.
Carroll, John, Guido Minnen, Yvonne Canning,
Siobhan Devlin, and John Tait. 1998. Practical
simplication of english newspaper text to assist
aphasic readers. In Proc. AAAI Workshop on Inte-
grating Artificial Intelligence and Assistive Tech-
nology.
Chandrasekar, R., Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simplifi-
cation. In Proc. COLING.
Daelemans, Walter, Anja Hthker, and Erik
Tjong Kim Sang. 2004. Automatic sentence
simplification for subtitling in Dutch and English.
In Proc. LREC. pages 1045–1048.
Del´eger, Louise and Pierre Zweigenbaum. 2009.
Extracting lay paraphrases of specialized expres-
sions from monolingual comparable medical cor-
pora. In Proc. Workshop on Building and Using
Comparable Corpora. pages 2–10.
Devlin, Siobhan and Gary Unthank. 2006. Help-
ing aphasic people process online information. In
Proc. ASSETS. pages 225–226.
Elhadad, Noemie and Komal Sutaria. 2007. Mining
a lexicon of technical terms and lay equivalents.
In Proc. ACL BioNLP Workshop. pages 49–56.
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Database. MIT Press, Cambridge,
MA.
Huenerfauth, Matt, Lijun Feng, and No´emie El-
hadad. 2009. Comparing evaluation techniques
</reference>
<page confidence="0.954918">
500
</page>
<reference confidence="0.976565">
for text readability software for adults with intel-
lectual disabilities. In Proc. ASSETS. pages 3–10.
Jonnalagadda, Siddhartha, Luis Tari, J¨org Haken-
berg, Chitta Baral, and Graciela Gonzalez. 2009.
Towards effective sentence simplification for au-
tomatic processing of biomedical text. In Proc.
NAACL-HLT. pages 177–180.
McCarthy, Diana and Roberto Navigli. 2007.
Semeval-2007 task 10: English lexical substitu-
tion task. In Proc. SemEval. pages 48–53.
Napoles, Courtney and Mark Dredze. 2010. Learn-
ing simple wikipedia: a cogitation in ascertaining
abecedarian language. In Proc. of the NAACL-
HLT Workshop on Computational Linguistics and
Writing. pages 42–50.
Nelken, Rani and Stuart Shieber. 2006. Towards
robust context-sensitive sentence alignment for
monolingual corpora. In Proc. EACL. pages 161–
166.
Siddharthan, Advaith. 2004. Syntactic simplifica-
tion and text cohesion. Technical Report UCAM-
CL-TR-597, University of Cambridge, Computer
Laboratory.
Vickrey, David and Daphne Koller. 2008. Apply-
ing sentence simplification to the CoNLL-2008
shared task. In Proc. CoNLL. pages 268–272.
Williams, Sandra and Ehud Reiter. 2005. Generating
readable texts for readers with low basic skills. In
Proc. ENLG. pages 127–132.
Yatskar, Mark, Bo Pang, Cristian Danescu-
Niculescu-Mizil, and Lillian Lee. 2010. For the
sake of simplicity: Unsupervised extraction of
lexical simplifications from wikipedia. In Proc.
NAACL-HLT. pages 365–368.
</reference>
<page confidence="0.997673">
501
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.176675">
<title confidence="0.999894">Putting it Simply: a Context-Aware Approach to Lexical Simplification</title>
<author confidence="0.556919">No´emie Elhadad</author>
<affiliation confidence="0.9464665">Biomedical Informatics Columbia University</affiliation>
<address confidence="0.999467">New York, NY 10032</address>
<email confidence="0.998613">noemie@dbmi.columbia.edu</email>
<author confidence="0.874297">Or</author>
<affiliation confidence="0.972607">Computer</affiliation>
<address confidence="0.787455">Columbia New York, NY</address>
<email confidence="0.996958">ob2008@columbia.edu</email>
<author confidence="0.934817">Samuel</author>
<affiliation confidence="0.8823935">Communication &amp; Rutgers</affiliation>
<address confidence="0.922246">New Brunswick, NJ</address>
<email confidence="0.999487">sdbrody@gmail.com</email>
<abstract confidence="0.99023525">We present a method for lexical simplification. Simplification rules are learned from a comparable corpus, and the rules are applied in a context-aware fashion to input sentences. Our method is unsupervised. Furthermore, it does not require any alignment or correspondence among the complex and simple corpora. We evaluate the simplification according to three criteria: preservation of grammaticality, preservation of meaning, and degree of simplification. Results show that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>pora</author>
</authors>
<booktitle>In Proc. EMNLP.</booktitle>
<pages>25--32</pages>
<marker>pora, </marker>
<rawString>pora. In Proc. EMNLP. pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Blake</author>
<author>Julia Kampov</author>
<author>Andreas Orphanides</author>
<author>David West</author>
<author>Cory Lown</author>
</authors>
<title>Query expansion, lexical simplification, and sentence selection strategies for multi-document summarization. In</title>
<date>2007</date>
<booktitle>Proc. DUC.</booktitle>
<contexts>
<context position="1456" citStr="Blake et al., 2007" startWordPosition="205" endWordPosition="208">ow that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality. 1 Introduction The task of simplification consists of editing an input text into a version that is less complex linguistically or more readable. Automated sentence simplification has been investigated mostly as a preprocessing step with the goal of improving NLP tasks, such as parsing (Chandrasekar et al., 1996; Siddharthan, 2004; Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) and summarization (Blake et al., 2007). Automated simplification can also be considered as a way to help end users access relevant information, which would be too complex to understand if left unedited. As such, it was proposed as a tool for adults with aphasia (Carroll et al., 1998; Devlin and Unthank, 2006), hearing-impaired people (Daelemans et al., 2004), readers with low-literacy skills (Williams and Reiter, 2005), individuals with intellectual disabilities (Huenerfauth et al., 2009), as well as health INPUT: In 1900, Omaha was the center of a national uproar over the kidnapping of Edward Cudahy, Jr., the son of a local meatp</context>
</contexts>
<marker>Blake, Kampov, Orphanides, West, Lown, 2007</marker>
<rawString>Blake, Catherine, Julia Kampov, Andreas Orphanides, David West, and Cory Lown. 2007. Query expansion, lexical simplification, and sentence selection strategies for multi-document summarization. In Proc. DUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Yvonne Canning</author>
<author>Siobhan Devlin</author>
<author>John Tait</author>
</authors>
<title>Practical simplication of english newspaper text to assist aphasic readers.</title>
<date>1998</date>
<booktitle>In Proc. AAAI Workshop on Integrating Artificial Intelligence and Assistive Technology.</booktitle>
<contexts>
<context position="1701" citStr="Carroll et al., 1998" startWordPosition="248" endWordPosition="251">t into a version that is less complex linguistically or more readable. Automated sentence simplification has been investigated mostly as a preprocessing step with the goal of improving NLP tasks, such as parsing (Chandrasekar et al., 1996; Siddharthan, 2004; Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) and summarization (Blake et al., 2007). Automated simplification can also be considered as a way to help end users access relevant information, which would be too complex to understand if left unedited. As such, it was proposed as a tool for adults with aphasia (Carroll et al., 1998; Devlin and Unthank, 2006), hearing-impaired people (Daelemans et al., 2004), readers with low-literacy skills (Williams and Reiter, 2005), individuals with intellectual disabilities (Huenerfauth et al., 2009), as well as health INPUT: In 1900, Omaha was the center of a national uproar over the kidnapping of Edward Cudahy, Jr., the son of a local meatpacking magnate. CANDIDATE RULES: {magnate → king} {magnate → businessman} OUTPUT: In 1900, Omaha was the center of a national uproar over the kidnapping of Edward Cudahy, Jr., the son of a local meatpacking businessman. Figure 1: Input sentence,</context>
</contexts>
<marker>Carroll, Minnen, Canning, Devlin, Tait, 1998</marker>
<rawString>Carroll, John, Guido Minnen, Yvonne Canning, Siobhan Devlin, and John Tait. 1998. Practical simplication of english newspaper text to assist aphasic readers. In Proc. AAAI Workshop on Integrating Artificial Intelligence and Assistive Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chandrasekar</author>
<author>Christine Doran</author>
<author>B Srinivas</author>
</authors>
<title>Motivations and methods for text simplification.</title>
<date>1996</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="1319" citStr="Chandrasekar et al., 1996" startWordPosition="185" endWordPosition="188">e simplification according to three criteria: preservation of grammaticality, preservation of meaning, and degree of simplification. Results show that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality. 1 Introduction The task of simplification consists of editing an input text into a version that is less complex linguistically or more readable. Automated sentence simplification has been investigated mostly as a preprocessing step with the goal of improving NLP tasks, such as parsing (Chandrasekar et al., 1996; Siddharthan, 2004; Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) and summarization (Blake et al., 2007). Automated simplification can also be considered as a way to help end users access relevant information, which would be too complex to understand if left unedited. As such, it was proposed as a tool for adults with aphasia (Carroll et al., 1998; Devlin and Unthank, 2006), hearing-impaired people (Daelemans et al., 2004), readers with low-literacy skills (Williams and Reiter, 2005), individuals with intellectual disabilities (Huenerfauth et al., 2009), as wel</context>
</contexts>
<marker>Chandrasekar, Doran, Srinivas, 1996</marker>
<rawString>Chandrasekar, R., Christine Doran, and B. Srinivas. 1996. Motivations and methods for text simplification. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
</authors>
<title>Anja Hthker, and Erik Tjong Kim Sang.</title>
<date>2004</date>
<booktitle>In Proc. LREC.</booktitle>
<pages>1045--1048</pages>
<marker>Daelemans, 2004</marker>
<rawString>Daelemans, Walter, Anja Hthker, and Erik Tjong Kim Sang. 2004. Automatic sentence simplification for subtitling in Dutch and English. In Proc. LREC. pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louise Del´eger</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Extracting lay paraphrases of specialized expressions from monolingual comparable medical corpora.</title>
<date>2009</date>
<booktitle>In Proc. Workshop on Building and Using Comparable Corpora.</booktitle>
<pages>2--10</pages>
<marker>Del´eger, Zweigenbaum, 2009</marker>
<rawString>Del´eger, Louise and Pierre Zweigenbaum. 2009. Extracting lay paraphrases of specialized expressions from monolingual comparable medical corpora. In Proc. Workshop on Building and Using Comparable Corpora. pages 2–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siobhan Devlin</author>
<author>Gary Unthank</author>
</authors>
<title>Helping aphasic people process online information.</title>
<date>2006</date>
<booktitle>In Proc. ASSETS.</booktitle>
<pages>225--226</pages>
<contexts>
<context position="1728" citStr="Devlin and Unthank, 2006" startWordPosition="252" endWordPosition="255">is less complex linguistically or more readable. Automated sentence simplification has been investigated mostly as a preprocessing step with the goal of improving NLP tasks, such as parsing (Chandrasekar et al., 1996; Siddharthan, 2004; Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) and summarization (Blake et al., 2007). Automated simplification can also be considered as a way to help end users access relevant information, which would be too complex to understand if left unedited. As such, it was proposed as a tool for adults with aphasia (Carroll et al., 1998; Devlin and Unthank, 2006), hearing-impaired people (Daelemans et al., 2004), readers with low-literacy skills (Williams and Reiter, 2005), individuals with intellectual disabilities (Huenerfauth et al., 2009), as well as health INPUT: In 1900, Omaha was the center of a national uproar over the kidnapping of Edward Cudahy, Jr., the son of a local meatpacking magnate. CANDIDATE RULES: {magnate → king} {magnate → businessman} OUTPUT: In 1900, Omaha was the center of a national uproar over the kidnapping of Edward Cudahy, Jr., the son of a local meatpacking businessman. Figure 1: Input sentence, candidate simplification r</context>
<context position="13994" citStr="Devlin and Unthank (2006)" startWordPosition="2215" endWordPosition="2218"> Create a common context vector CCV,,,x for the rule {w —* x}. The vector contains all features common to both words, with the feature values that are the minimum between them. In other words, CCV�,x[i] = min(CV„[i], CVx[i]). We calculate the cosine similarity of the common context vector and the sentence context vector: Context5im = cosine(CCV�,x, 5CV3,,,) If the context similarity is larger than a threshold (0.01), we use this rule to simplify. If multiple rules apply for the same word, we use the one with the highest context similarity. 4 Experimental Setup Baseline We employ the method of Devlin and Unthank (2006) which replaces a word with its most frequent synonym (presumed to be the simplest) as our baseline. To provide a fairer comparison to our system, we add the restriction that the synonyms should not share a prefix of four or more letters (a baseline version of lemmatization) and use MorphAdorner to produce a form that agrees with that of the original word. Type Freq. Gram. Mean. Simp. Base High 63.33(+20)% 46.67% 50% Sys. High 76.67(+6.66)% 63.33% 73.33% Base Med 75(+7.14)% 67.86% 42.86% Sys. Med 72.41(+17.25)% 75.86% 82.76% Base Low 73.08(+11.54)% 53.85% 46.15% Sys. Low 85.19(+0)% 48.15% 70.3</context>
</contexts>
<marker>Devlin, Unthank, 2006</marker>
<rawString>Devlin, Siobhan and Gary Unthank. 2006. Helping aphasic people process online information. In Proc. ASSETS. pages 225–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noemie Elhadad</author>
<author>Komal Sutaria</author>
</authors>
<title>Mining a lexicon of technical terms and lay equivalents.</title>
<date>2007</date>
<booktitle>In Proc. ACL BioNLP Workshop.</booktitle>
<pages>49--56</pages>
<contexts>
<context position="2423" citStr="Elhadad and Sutaria, 2007" startWordPosition="356" endWordPosition="360">iteracy skills (Williams and Reiter, 2005), individuals with intellectual disabilities (Huenerfauth et al., 2009), as well as health INPUT: In 1900, Omaha was the center of a national uproar over the kidnapping of Edward Cudahy, Jr., the son of a local meatpacking magnate. CANDIDATE RULES: {magnate → king} {magnate → businessman} OUTPUT: In 1900, Omaha was the center of a national uproar over the kidnapping of Edward Cudahy, Jr., the son of a local meatpacking businessman. Figure 1: Input sentence, candidate simplification rules, and output sentence. consumers looking for medical information (Elhadad and Sutaria, 2007; Del´eger and Zweigenbaum, 2009). Simplification can take place at different levels of a text – its overall document structure, the syntax of its sentences, and the individual phrases or words in a sentence. In this paper, we present a sentence simplification approach, which focuses on lexical simplification.1 The key contributions of our work are (i) an unsupervised method for learning pairs of complex and simpler synonyms; and (ii) a contextaware method for substituting one for the other. Figure 1 shows an example input sentence. The word magnate is determined as a candidate for simplificat</context>
</contexts>
<marker>Elhadad, Sutaria, 2007</marker>
<rawString>Elhadad, Noemie and Komal Sutaria. 2007. Mining a lexicon of technical terms and lay equivalents. In Proc. ACL BioNLP Workshop. pages 49–56.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>Fellbaum, Christiane, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Fellbaum, Christiane, editor. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Huenerfauth</author>
<author>Lijun Feng</author>
<author>No´emie Elhadad</author>
</authors>
<title>Comparing evaluation techniques for text readability software for adults with intellectual disabilities.</title>
<date>2009</date>
<booktitle>In Proc. ASSETS.</booktitle>
<pages>3--10</pages>
<contexts>
<context position="1911" citStr="Huenerfauth et al., 2009" startWordPosition="275" endWordPosition="278">arsing (Chandrasekar et al., 1996; Siddharthan, 2004; Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) and summarization (Blake et al., 2007). Automated simplification can also be considered as a way to help end users access relevant information, which would be too complex to understand if left unedited. As such, it was proposed as a tool for adults with aphasia (Carroll et al., 1998; Devlin and Unthank, 2006), hearing-impaired people (Daelemans et al., 2004), readers with low-literacy skills (Williams and Reiter, 2005), individuals with intellectual disabilities (Huenerfauth et al., 2009), as well as health INPUT: In 1900, Omaha was the center of a national uproar over the kidnapping of Edward Cudahy, Jr., the son of a local meatpacking magnate. CANDIDATE RULES: {magnate → king} {magnate → businessman} OUTPUT: In 1900, Omaha was the center of a national uproar over the kidnapping of Edward Cudahy, Jr., the son of a local meatpacking businessman. Figure 1: Input sentence, candidate simplification rules, and output sentence. consumers looking for medical information (Elhadad and Sutaria, 2007; Del´eger and Zweigenbaum, 2009). Simplification can take place at different levels of </context>
</contexts>
<marker>Huenerfauth, Feng, Elhadad, 2009</marker>
<rawString>Huenerfauth, Matt, Lijun Feng, and No´emie Elhadad. 2009. Comparing evaluation techniques for text readability software for adults with intellectual disabilities. In Proc. ASSETS. pages 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddhartha Jonnalagadda</author>
<author>Luis Tari</author>
<author>J¨org Hakenberg</author>
<author>Chitta Baral</author>
<author>Graciela Gonzalez</author>
</authors>
<title>Towards effective sentence simplification for automatic processing of biomedical text.</title>
<date>2009</date>
<booktitle>In Proc. NAACL-HLT.</booktitle>
<pages>177--180</pages>
<contexts>
<context position="1366" citStr="Jonnalagadda et al., 2009" startWordPosition="191" endWordPosition="194">preservation of grammaticality, preservation of meaning, and degree of simplification. Results show that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality. 1 Introduction The task of simplification consists of editing an input text into a version that is less complex linguistically or more readable. Automated sentence simplification has been investigated mostly as a preprocessing step with the goal of improving NLP tasks, such as parsing (Chandrasekar et al., 1996; Siddharthan, 2004; Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) and summarization (Blake et al., 2007). Automated simplification can also be considered as a way to help end users access relevant information, which would be too complex to understand if left unedited. As such, it was proposed as a tool for adults with aphasia (Carroll et al., 1998; Devlin and Unthank, 2006), hearing-impaired people (Daelemans et al., 2004), readers with low-literacy skills (Williams and Reiter, 2005), individuals with intellectual disabilities (Huenerfauth et al., 2009), as well as health INPUT: In 1900, Omaha was the cente</context>
</contexts>
<marker>Jonnalagadda, Tari, Hakenberg, Baral, Gonzalez, 2009</marker>
<rawString>Jonnalagadda, Siddhartha, Luis Tari, J¨org Hakenberg, Chitta Baral, and Graciela Gonzalez. 2009. Towards effective sentence simplification for automatic processing of biomedical text. In Proc. NAACL-HLT. pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval-2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proc. SemEval.</booktitle>
<pages>48--53</pages>
<contexts>
<context position="4046" citStr="McCarthy and Navigli, 2007" startWordPosition="597" endWordPosition="600">al perspective, the task of lexical simplification bears similarity with that of para1Our resulting system is available for download at http://www.cs.columbia.edu/ ob2008/ 496 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 496–501, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics phrase identification (Androutsopoulos and Malaka- was excluded, leaving only the main body text of siotis, 2010) and the SemEval-2007 English Lexi- the article. Further preprocessing was carried out cal Substitution Task (McCarthy and Navigli, 2007). with the Stanford NLP Package5 to tokenize the text, However, these do not consider issues of readabil- transform all words to lower case, and identify senity and linguistic complexity. Our methods lever- tence boundaries. age a large comparable collection of texts: English Wikipedia2 and Simple English Wikipedia3. Napoles and Dredze (2010) examined Wikipedia Simple articles looking for features that characterize a simple text, with the hope of informing research in automatic simplification methods. Yatskar et al. (2010) learn lexical simplification rules from the edit histories of Wikipedia</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>McCarthy, Diana and Roberto Navigli. 2007. Semeval-2007 task 10: English lexical substitution task. In Proc. SemEval. pages 48–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Mark Dredze</author>
</authors>
<title>Learning simple wikipedia: a cogitation in ascertaining abecedarian language.</title>
<date>2010</date>
<booktitle>In Proc. of the NAACLHLT Workshop on Computational Linguistics and Writing.</booktitle>
<pages>42--50</pages>
<contexts>
<context position="4390" citStr="Napoles and Dredze (2010)" startWordPosition="650" endWordPosition="653"> for Computational Linguistics phrase identification (Androutsopoulos and Malaka- was excluded, leaving only the main body text of siotis, 2010) and the SemEval-2007 English Lexi- the article. Further preprocessing was carried out cal Substitution Task (McCarthy and Navigli, 2007). with the Stanford NLP Package5 to tokenize the text, However, these do not consider issues of readabil- transform all words to lower case, and identify senity and linguistic complexity. Our methods lever- tence boundaries. age a large comparable collection of texts: English Wikipedia2 and Simple English Wikipedia3. Napoles and Dredze (2010) examined Wikipedia Simple articles looking for features that characterize a simple text, with the hope of informing research in automatic simplification methods. Yatskar et al. (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles. Our method differs from theirs, as we rely on the two corpora as a whole, and do not require any aligned or designated simple/complex sentences when learning simplification rules.4 3 Method Our sentence simplification system consists of two main stages: rule extraction and simplification. In the first stage, simplification r</context>
</contexts>
<marker>Napoles, Dredze, 2010</marker>
<rawString>Napoles, Courtney and Mark Dredze. 2010. Learning simple wikipedia: a cogitation in ascertaining abecedarian language. In Proc. of the NAACLHLT Workshop on Computational Linguistics and Writing. pages 42–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rani Nelken</author>
<author>Stuart Shieber</author>
</authors>
<title>Towards robust context-sensitive sentence alignment for monolingual corpora.</title>
<date>2006</date>
<booktitle>In Proc. EACL.</booktitle>
<pages>161--166</pages>
<contexts>
<context position="8037" citStr="Nelken and Shieber, 2006" startWordPosition="1241" endWordPosition="1244">e of any special properties of Wikipedia (e.g., edit histories). In practice, this means that our method is suitable for other cases where there exists a simplified corpus in the same domain. The corpora are a snapshot as of April 23, 2010. EW contains 3,266,245 articles, and SEW contains 60,100 articles. The articles were preprocessed as follows: all comments, HTML tags, and Wiki links were removed. Text contained in tables and figures 2http://en.wikipedia.org 3http://simple.wikipedia.org 4Aligning sentences in monolingual comparable corpora has been investigated (Barzilay and Elhadad, 2003; Nelken and Shieber, 2006), but is not a focus for this work. 5http://nlp.stanford.edu/software/index.shtml 6http://morphadorner.northwestern.edu 497 its first sense (as listed in WordNet)7 is a synonym or hypernym of the first. Finally, we compute the cosine similarity scores for the remaining pairs using their context vectors. 3.1.2 Ensuring Simplification From among our remaining candidate word pairs, we want to identify those that represent a complex word which can be replaced by a simpler one. Our definition of the complexity of a word is based on two measures: the corpus complexity and the lexical complexity. Spe</context>
</contexts>
<marker>Nelken, Shieber, 2006</marker>
<rawString>Nelken, Rani and Stuart Shieber. 2006. Towards robust context-sensitive sentence alignment for monolingual corpora. In Proc. EACL. pages 161– 166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Syntactic simplification and text cohesion.</title>
<date>2004</date>
<tech>Technical Report UCAMCL-TR-597,</tech>
<institution>University of Cambridge, Computer Laboratory.</institution>
<contexts>
<context position="1338" citStr="Siddharthan, 2004" startWordPosition="189" endWordPosition="190">to three criteria: preservation of grammaticality, preservation of meaning, and degree of simplification. Results show that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality. 1 Introduction The task of simplification consists of editing an input text into a version that is less complex linguistically or more readable. Automated sentence simplification has been investigated mostly as a preprocessing step with the goal of improving NLP tasks, such as parsing (Chandrasekar et al., 1996; Siddharthan, 2004; Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) and summarization (Blake et al., 2007). Automated simplification can also be considered as a way to help end users access relevant information, which would be too complex to understand if left unedited. As such, it was proposed as a tool for adults with aphasia (Carroll et al., 1998; Devlin and Unthank, 2006), hearing-impaired people (Daelemans et al., 2004), readers with low-literacy skills (Williams and Reiter, 2005), individuals with intellectual disabilities (Huenerfauth et al., 2009), as well as health INPUT: </context>
</contexts>
<marker>Siddharthan, 2004</marker>
<rawString>Siddharthan, Advaith. 2004. Syntactic simplification and text cohesion. Technical Report UCAMCL-TR-597, University of Cambridge, Computer Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vickrey</author>
<author>Daphne Koller</author>
</authors>
<title>Applying sentence simplification to the CoNLL-2008 shared task.</title>
<date>2008</date>
<booktitle>In Proc. CoNLL.</booktitle>
<pages>268--272</pages>
<contexts>
<context position="1417" citStr="Vickrey and Koller, 2008" startWordPosition="199" endWordPosition="202">ing, and degree of simplification. Results show that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality. 1 Introduction The task of simplification consists of editing an input text into a version that is less complex linguistically or more readable. Automated sentence simplification has been investigated mostly as a preprocessing step with the goal of improving NLP tasks, such as parsing (Chandrasekar et al., 1996; Siddharthan, 2004; Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) and summarization (Blake et al., 2007). Automated simplification can also be considered as a way to help end users access relevant information, which would be too complex to understand if left unedited. As such, it was proposed as a tool for adults with aphasia (Carroll et al., 1998; Devlin and Unthank, 2006), hearing-impaired people (Daelemans et al., 2004), readers with low-literacy skills (Williams and Reiter, 2005), individuals with intellectual disabilities (Huenerfauth et al., 2009), as well as health INPUT: In 1900, Omaha was the center of a national uproar over the kidnapping of Edwar</context>
</contexts>
<marker>Vickrey, Koller, 2008</marker>
<rawString>Vickrey, David and Daphne Koller. 2008. Applying sentence simplification to the CoNLL-2008 shared task. In Proc. CoNLL. pages 268–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Williams</author>
<author>Ehud Reiter</author>
</authors>
<title>Generating readable texts for readers with low basic skills.</title>
<date>2005</date>
<booktitle>In Proc. ENLG.</booktitle>
<pages>127--132</pages>
<contexts>
<context position="1840" citStr="Williams and Reiter, 2005" startWordPosition="266" endWordPosition="269"> as a preprocessing step with the goal of improving NLP tasks, such as parsing (Chandrasekar et al., 1996; Siddharthan, 2004; Jonnalagadda et al., 2009), semantic role labeling (Vickrey and Koller, 2008) and summarization (Blake et al., 2007). Automated simplification can also be considered as a way to help end users access relevant information, which would be too complex to understand if left unedited. As such, it was proposed as a tool for adults with aphasia (Carroll et al., 1998; Devlin and Unthank, 2006), hearing-impaired people (Daelemans et al., 2004), readers with low-literacy skills (Williams and Reiter, 2005), individuals with intellectual disabilities (Huenerfauth et al., 2009), as well as health INPUT: In 1900, Omaha was the center of a national uproar over the kidnapping of Edward Cudahy, Jr., the son of a local meatpacking magnate. CANDIDATE RULES: {magnate → king} {magnate → businessman} OUTPUT: In 1900, Omaha was the center of a national uproar over the kidnapping of Edward Cudahy, Jr., the son of a local meatpacking businessman. Figure 1: Input sentence, candidate simplification rules, and output sentence. consumers looking for medical information (Elhadad and Sutaria, 2007; Del´eger and Zw</context>
</contexts>
<marker>Williams, Reiter, 2005</marker>
<rawString>Williams, Sandra and Ehud Reiter. 2005. Generating readable texts for readers with low basic skills. In Proc. ENLG. pages 127–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Bo Pang</author>
<author>Cristian DanescuNiculescu-Mizil</author>
<author>Lillian Lee</author>
</authors>
<title>For the sake of simplicity: Unsupervised extraction of lexical simplifications from wikipedia.</title>
<date>2010</date>
<booktitle>In Proc. NAACL-HLT.</booktitle>
<pages>365--368</pages>
<contexts>
<context position="4574" citStr="Yatskar et al. (2010)" startWordPosition="676" endWordPosition="679">cle. Further preprocessing was carried out cal Substitution Task (McCarthy and Navigli, 2007). with the Stanford NLP Package5 to tokenize the text, However, these do not consider issues of readabil- transform all words to lower case, and identify senity and linguistic complexity. Our methods lever- tence boundaries. age a large comparable collection of texts: English Wikipedia2 and Simple English Wikipedia3. Napoles and Dredze (2010) examined Wikipedia Simple articles looking for features that characterize a simple text, with the hope of informing research in automatic simplification methods. Yatskar et al. (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles. Our method differs from theirs, as we rely on the two corpora as a whole, and do not require any aligned or designated simple/complex sentences when learning simplification rules.4 3 Method Our sentence simplification system consists of two main stages: rule extraction and simplification. In the first stage, simplification rules are extracted from the corpora. Each rule consists of an ordered word pair {original —* simplified} along with a score indicating the similarity between the words. In the second s</context>
</contexts>
<marker>Yatskar, Pang, DanescuNiculescu-Mizil, Lee, 2010</marker>
<rawString>Yatskar, Mark, Bo Pang, Cristian DanescuNiculescu-Mizil, and Lillian Lee. 2010. For the sake of simplicity: Unsupervised extraction of lexical simplifications from wikipedia. In Proc. NAACL-HLT. pages 365–368.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>