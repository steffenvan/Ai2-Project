<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002444">
<title confidence="0.969934">
A Maximum Entropy Approach to FrameNet Tagging
</title>
<note confidence="0.5419375">
Michael Fleischman and Eduard Hovy
USC Information Science Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
</note>
<email confidence="0.994855">
{fleisch, hovy }@ISI.edu
</email>
<sectionHeader confidence="0.995615" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999887642857143">
The development of FrameNet, a large
database of semantically annotated sentences,
has primed research into statistical methods
for semantic tagging. We advance previous
work by adopting a Maximum Entropy
approach and by using Viterbi search to find
the highest probability tag sequence for a
given sentence. Further we examine the use
of syntactic pattern based re-ranking to further
increase performance. We analyze our
strategy using both extracted and human
generated syntactic features. Experiments
indicate 85.7% accuracy using human
annotations on a held out test set.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999770108108108">
The ability to develop automatic methods for semantic
classification has been hampered by the lack of large
semantically annotated corpora. Recent work in the
development of FrameNet, a large database of
semantically annotated sentences, has laid the
foundation for the use of statistical approaches to
automatic semantic classification.
The FrameNet project seeks to annotate a large
subset of the British National Corpus with semantic
information. Annotations are based on Frame
Semantics (Fillmore, 1976), in which frames are defined
as schematic representations of situations involving
various Frame Elements such as participants, props, and
other conceptual roles.
In each FrameNet sentence, a single target
predicate is identified and all of its relevant Frame
Elements are tagged with their element-type (e.g.,
Agent, Judge), their syntactic Phrase Type (e.g., NP,
PP), and their Grammatical Function (e.g., External
Argument, Object Argument). Figure 1 shows an
example of an annotated sentence and its appropriate
semantic frame.
To our knowledge, Gildea and Jurafsky (2000) is
the only work that uses FrameNet to build a statistical
semantic classifier. They split the problem into two
distinct sub-tasks: Frame Element identification and
Frame Element classification. In the identification
phase, they use syntactic information extracted from a
parse tree to learn the boundaries of Frame Elements in
sentences. The work presented here, focuses only on
the second phase: classification.
Gildea and Jurafsky (2000) describe a system that
uses completely syntactic features to classify the Frame
Elements in a sentence. They extract features from a
parse tree and model the conditional probability of a
semantic role given those features. They report an
accuracy of 76.9% on a held out test set.
</bodyText>
<figure confidence="0.404085666666667">
Frame: Body-Movement
Frame Elements:
Agent Body Part Cause
She clapped her hands in inspiration.
-NP -NP -PP
-Ext. -Obj. -Comp.
</figure>
<figureCaption confidence="0.939808666666667">
Figure 1. Frame for lemma “clap” shown with three core Frame
Elements and a sentence annotated with element type, phrase
type, and grammatical function.
</figureCaption>
<bodyText confidence="0.999976666666667">
We extend Gildea and Jurafsky (2000)’s initial
effort in three ways. First, we adopt a Maximum
Entropy (ME) framework to better learn the feature
weights associated with the classification model.
Second, we recast the classification task as a tagging
problem in which an n-gram model of Frame Elements
is applied to find the most probable tag sequence (as
opposed to the most probable individual tags). Finally,
we implement a re-ranking system that takes advantage
of the sentence-level syntactic patterns of each
sequence. We analyze our results using syntactic
features extracted from a parse tree generated by Collins
parser (Collins, 1997) and compare those to models
built using features extracted from FrameNet’s human
annotations.
</bodyText>
<sectionHeader confidence="0.866163" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999712363636364">
Training (32,251 sentences), development (3,491
sentences), and held out test sets (3,398 sentences) were
generated from the June 2002 FrameNet release
following the divisions used in Gildea and Jurafsky
(2000) 1 . Because human-annotated syntactic
information could only be obtained for a subset of their
data, the training, development, and test sets used here
are approximately 10% smaller than those used in
Gildea and Jurafsky (2000).2 There are on average 2.2
Frame Elements per sentence, falling into one of 126
unique classes.
</bodyText>
<subsectionHeader confidence="0.994038">
2.1 Maximum Entropy
</subsectionHeader>
<bodyText confidence="0.981330619047619">
ME models implement the intuition that the best model
will be the one that is consistent with all the evidence,
but otherwise, is as uniform as possible. (Berger et al.,
1996). Following recent successes using it for many
NLP tasks (Och and Ney, 2002; Koeling, 2000), we use
ME to implement a Frame Element classifier.
We use the YASMET ME package (Och,
2002) to train an approximation of the model below:
P(r |pt, voice, position, target, gf, h)
Here r indicates the element type, pt the phrase type, gf
the grammatical function, h the head word, and target
the target predicate. Due to data sparsity issues, we do
not calculate this model directly, but rather, model
various feature combinations as described in Gildea and
Jurafsky (2000).
The classifier was trained, using only features that
had a frequency in training of one or more, and until
performance on the development set ceased to improve.
Feature weights were smoothed using a Bayesian
method, such that weight limits are Gaussian distributed
with mean 0 and standard deviation 1.
</bodyText>
<subsectionHeader confidence="0.99771">
2.2 Tagging
</subsectionHeader>
<bodyText confidence="0.870497631578947">
Frame Elements do not occur in isolation, but rather,
depend very much on what other Elements occur in a
sentence. For example, if a Frame Element is tagged as
an Agent it is highly unlikely that the next Element will
also be an Agent. We exploit this dependency by
treating the Frame Element classification task as a
tagging problem.
The YASMET MEtagger was used to apply an n-
gram tag model to the classification task (Bender et al.,
2003). The feature set for the training data was
1 Divisions given by Dan Gildea via personal communication.
2 Gildea and Jurafsky (2000) use 36995 training, 4000
development, and 3865 test sentences. They do not report
results using hand annotated syntactic information.
augmented to include information about the tags of the
previous one and two Frame Elements in the sentence:
P(r |pt, voice, position, target, gf, h, r -1,r -1+r -2)
Viterbi search was then used to find the most probable
tag sequence through all possible sequences.
</bodyText>
<subsectionHeader confidence="0.996204">
2.3 Pattern Features
</subsectionHeader>
<bodyText confidence="0.997018615384615">
A great deal of information useful for classification can
be found in the syntactic patterns associated with each
sequence of Frame Elements. A typical syntactic
pattern is exhibited by the sentence “Alexandra bent her
head.” Here “Alexandra” is an external argument Noun
Phrase, “bent” is the target, and “her head” is an object
argument Noun Phrase. In the training data, a syntactic
pattern of NP-ext, target, NP-obj, given the predicate
bend, was associated 100% of the time with the Frame
Element pattern: “Agent target BodyPart“, thus,
providing powerful evidence as to the classification of
those Frame Elements.
We exploit these sentence-level patterns by
implementing a re-ranking system that chooses among
the n-best tagger outputs. The re-ranker was trained on
a development corpus, which was first tagged using the
MEtagger described above. For each sentence in the
development corpus, the 10 best tag sequences are
output by the classifier and described by three
probabilities: 3 1) the sequence’s probability given by
the ME classifier (ME); 2) the conditional probability of
that sequence given the syntactic pattern and the target
predicate (pat+target); 3) a back off conditional
probability of the tag sequence given just the syntactic
pattern (pat). A ME model is then used to combine the
log of these probabilities to give a model of the form:
</bodyText>
<equation confidence="0.848953">
P(tag-seq |ME, pat+target, pat)
</equation>
<sectionHeader confidence="0.999817" genericHeader="background">
3 Results
</sectionHeader>
<bodyText confidence="0.991073785714286">
Figure 2 shows the performance of the base ME model,
the base model within a tagging framework, and the
base model within a tagging framework plus the re-
ranker. Results are shown for data sets trained and
tested using human annotated syntactic features and
trained and tested using automatically extracted
syntactic features. In both cases the training and test
sets are identical.
For both the extracted and human conditions,
adopting a tagging framework improves results by over
1%. However, while the syntactic pattern based re-
ranker increases performance using human annotations
by nearly 2%, the effect when using automatically
extracted information is only 0.5%. This is reasonable
</bodyText>
<footnote confidence="0.6754055">
3 Using n-best lists of 50 and 100 showed no significant
difference in performance.
</footnote>
<bodyText confidence="0.9999324">
considering that the re-ranker’s effectiveness is
correlated with the level of noise in the syntactic
patterns upon which it is based.
The difference in performance between the models
under both human and extracted conditions was
relatively consistent: averaging 8.7% with a standard
deviation of 0.7.
As a further analysis, we have examined the
performance of our base ME model on the same test set
as that used in Gildea and Jurafsky (2000). Using only
extracted information, we achieve an accuracy of
74.9%, two percent lower than their reported results.
This result is not unreasonable, however, because, due
to limited time, very little effort was spent tuning the
parameters of the model.
</bodyText>
<figureCaption confidence="0.998067833333333">
Figure 2. Performance of models on held out test data. ME refers
to results of the base Maximum Entropy model, Tagger to a
combined ME and Viterbi search model, Re-Rank to the Tagger
augmented with a re-ranker. Extracted refers to models trained
using features extracted from parse trees, Human to models using
features from FrameNet’s human annotations.
</figureCaption>
<sectionHeader confidence="0.993201" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999926583333333">
It is clear that using a tagging framework and syntactic
patterns improves performance of the semantic classifier
when features are extracted from either automatically
generated parse trees or human annotations. The most
striking result of these experiments, however, is the
dramatic decrease in performance associated with using
features extracted from a parse tree.
This decrease in performance can be traced to at
least two aspects of the automatic extraction process:
noisy parser output and limited grammatical
information.
To compensate for noisy parser output, our current
work is focusing on two strategies. First, we are
looking at using shallower but more reliable methods
for syntactic feature generation, such as part of speech
tagging and text chunking, to either replace or augment
the syntactic parser. Second, we are using ontological
information, such as word classes and synonyms, in the
hopes that semantic information may supplement the
noisy syntactic information.
The models trained on features extracted from parse
trees do not have access to rich grammatical
information. Following Gildea and Jurafsky (2000),
automatic extraction of grammatical information here is
limited to the governing category of a Noun Phrase.
The FrameNet annotations, however, are much richer
and include information about complements, modifiers,
etc. We are looking at ways to include such information
either by using alternative parsers (Hermjakob, 1997) or
as a post processing task (Blaheta and Charniak, 2000).
In future work, we will extend the strategies
outlined here to incorporate Frame Element
identification into our model. By treating semantic
classification as a single tagging problem, we hope to
create a unified, practical, and high performance system
for Frame Element tagging.
</bodyText>
<sectionHeader confidence="0.998562" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999724">
The authors would like to thank Dan Gildea who
generously allowed us access to his data files and Oliver
Bender for making the MEtagger software publicly
available. Finally, we thank Franz Och whose help and
expertise were invaluable.
</bodyText>
<sectionHeader confidence="0.998189" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999425642857143">
O. Bender, K. Macherey, F. J. Och, and H. Ney. 2003.
Comparison of Alignment Templates and Maximum
Entropy Models for Natural Language Processing. EACL-
2003. Budapest, Hungary.
A. Berger, S. Della Pietra and V. Della Pietra, 1996. A
Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics, vol. 22, no. 1.
D. Blaheta and E. Charniak. 2000. Assigning Function Tags to
Parsed Text, In Proc. of the 1st NAACL, Seattle, WA.
M. Collins. 1997. Three generative, lexicalized models for
statistical parsing. In Proc. of the 35th Annual Meeting of
the ACL.
C. Fillmore 1976. Frame semantics and the nature of
language. In Annals of the New York Academy of Sciences:
Conference on the Origin and Development of Language
and Speech, Volume 280 (pp. 20-32).
D. Gildea and D. Jurafsky. 2000. Automatic Labeling of
Semantic Roles, ACL-2000, Hong Kong.
U. Hermjakob, 1997. Learning Parse and Translation
Decisions from Examples with Rich Context. Ph.D.
Dissertation, University of Texas at Austin, Austin, TX.
R. Koeling. 2000. Chunking with maximum entropy models.
CoNLL-2000. Lisbon, Portugal.
F.J. Och, H. Ney. 2002. Discriminative Training and
Maximum Entropy Models for Statistical Machine
Translation. ACL-2002. Philadelphia, PA.
F.J. Och. 2002. Yet another maxent toolkit: YASMET. www-
i6.informatik.rwth-aachen.de/Colleagues/och/.
</reference>
<figure confidence="0.9970255">
% Correct
88
86
84
82
80
78
76
74
72
70
68
74
ME Tagger Re-Rank
82.6
75.8
83.8
Extracted Human
76.3
85.7
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958589">
<title confidence="0.999946">A Maximum Entropy Approach to FrameNet Tagging</title>
<author confidence="0.998886">Michael Fleischman</author>
<author confidence="0.998886">Eduard</author>
<affiliation confidence="0.999568">USC Information Science</affiliation>
<address confidence="0.9882495">4676 Admiralty Marina del Rey, CA 90292-6695</address>
<email confidence="0.996814">fleisch@ISI.edu</email>
<email confidence="0.996814">hovy@ISI.edu</email>
<abstract confidence="0.999065733333333">The development of FrameNet, a large database of semantically annotated sentences, has primed research into statistical methods for semantic tagging. We advance previous work by adopting a Maximum Entropy approach and by using Viterbi search to find the highest probability tag sequence for a given sentence. Further we examine the use of syntactic pattern based re-ranking to further increase performance. We analyze strategy using both extracted and human generated syntactic features. Experiments indicate 85.7% accuracy using human annotations on a held out test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>O Bender</author>
<author>K Macherey</author>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<date>2003</date>
<booktitle>Comparison of Alignment Templates and Maximum Entropy Models for Natural Language Processing. EACL2003.</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="5676" citStr="Bender et al., 2003" startWordPosition="880" endWordPosition="883">nt set ceased to improve. Feature weights were smoothed using a Bayesian method, such that weight limits are Gaussian distributed with mean 0 and standard deviation 1. 2.2 Tagging Frame Elements do not occur in isolation, but rather, depend very much on what other Elements occur in a sentence. For example, if a Frame Element is tagged as an Agent it is highly unlikely that the next Element will also be an Agent. We exploit this dependency by treating the Frame Element classification task as a tagging problem. The YASMET MEtagger was used to apply an ngram tag model to the classification task (Bender et al., 2003). The feature set for the training data was 1 Divisions given by Dan Gildea via personal communication. 2 Gildea and Jurafsky (2000) use 36995 training, 4000 development, and 3865 test sentences. They do not report results using hand annotated syntactic information. augmented to include information about the tags of the previous one and two Frame Elements in the sentence: P(r |pt, voice, position, target, gf, h, r -1,r -1+r -2) Viterbi search was then used to find the most probable tag sequence through all possible sequences. 2.3 Pattern Features A great deal of information useful for classifi</context>
</contexts>
<marker>Bender, Macherey, Och, Ney, 2003</marker>
<rawString>O. Bender, K. Macherey, F. J. Och, and H. Ney. 2003. Comparison of Alignment Templates and Maximum Entropy Models for Natural Language Processing. EACL2003. Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<contexts>
<context position="4354" citStr="Berger et al., 1996" startWordPosition="655" endWordPosition="658">rated from the June 2002 FrameNet release following the divisions used in Gildea and Jurafsky (2000) 1 . Because human-annotated syntactic information could only be obtained for a subset of their data, the training, development, and test sets used here are approximately 10% smaller than those used in Gildea and Jurafsky (2000).2 There are on average 2.2 Frame Elements per sentence, falling into one of 126 unique classes. 2.1 Maximum Entropy ME models implement the intuition that the best model will be the one that is consistent with all the evidence, but otherwise, is as uniform as possible. (Berger et al., 1996). Following recent successes using it for many NLP tasks (Och and Ney, 2002; Koeling, 2000), we use ME to implement a Frame Element classifier. We use the YASMET ME package (Och, 2002) to train an approximation of the model below: P(r |pt, voice, position, target, gf, h) Here r indicates the element type, pt the phrase type, gf the grammatical function, h the head word, and target the target predicate. Due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in Gildea and Jurafsky (2000). The classifier was trained, using</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Della Pietra and V. Della Pietra, 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, vol. 22, no. 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blaheta</author>
<author>E Charniak</author>
</authors>
<title>Assigning Function Tags to Parsed Text,</title>
<date>2000</date>
<booktitle>In Proc. of the 1st NAACL,</booktitle>
<location>Seattle, WA.</location>
<marker>Blaheta, Charniak, 2000</marker>
<rawString>D. Blaheta and E. Charniak. 2000. Assigning Function Tags to Parsed Text, In Proc. of the 1st NAACL, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="3520" citStr="Collins, 1997" startWordPosition="526" endWordPosition="527">(2000)’s initial effort in three ways. First, we adopt a Maximum Entropy (ME) framework to better learn the feature weights associated with the classification model. Second, we recast the classification task as a tagging problem in which an n-gram model of Frame Elements is applied to find the most probable tag sequence (as opposed to the most probable individual tags). Finally, we implement a re-ranking system that takes advantage of the sentence-level syntactic patterns of each sequence. We analyze our results using syntactic features extracted from a parse tree generated by Collins parser (Collins, 1997) and compare those to models built using features extracted from FrameNet’s human annotations. 2 Method Training (32,251 sentences), development (3,491 sentences), and held out test sets (3,398 sentences) were generated from the June 2002 FrameNet release following the divisions used in Gildea and Jurafsky (2000) 1 . Because human-annotated syntactic information could only be obtained for a subset of their data, the training, development, and test sets used here are approximately 10% smaller than those used in Gildea and Jurafsky (2000).2 There are on average 2.2 Frame Elements per sentence, f</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalized models for statistical parsing. In Proc. of the 35th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fillmore</author>
</authors>
<title>Frame semantics and the nature of language.</title>
<date>1976</date>
<booktitle>In Annals of the New York Academy of Sciences: Conference on the Origin and Development of Language and Speech,</booktitle>
<volume>280</volume>
<pages>(pp.</pages>
<contexts>
<context position="1297" citStr="Fillmore, 1976" startWordPosition="186" endWordPosition="187">eatures. Experiments indicate 85.7% accuracy using human annotations on a held out test set. 1 Introduction The ability to develop automatic methods for semantic classification has been hampered by the lack of large semantically annotated corpora. Recent work in the development of FrameNet, a large database of semantically annotated sentences, has laid the foundation for the use of statistical approaches to automatic semantic classification. The FrameNet project seeks to annotate a large subset of the British National Corpus with semantic information. Annotations are based on Frame Semantics (Fillmore, 1976), in which frames are defined as schematic representations of situations involving various Frame Elements such as participants, props, and other conceptual roles. In each FrameNet sentence, a single target predicate is identified and all of its relevant Frame Elements are tagged with their element-type (e.g., Agent, Judge), their syntactic Phrase Type (e.g., NP, PP), and their Grammatical Function (e.g., External Argument, Object Argument). Figure 1 shows an example of an annotated sentence and its appropriate semantic frame. To our knowledge, Gildea and Jurafsky (2000) is the only work that u</context>
</contexts>
<marker>Fillmore, 1976</marker>
<rawString>C. Fillmore 1976. Frame semantics and the nature of language. In Annals of the New York Academy of Sciences: Conference on the Origin and Development of Language and Speech, Volume 280 (pp. 20-32).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles,</title>
<date>2000</date>
<location>ACL-2000, Hong Kong.</location>
<contexts>
<context position="1873" citStr="Gildea and Jurafsky (2000)" startWordPosition="268" endWordPosition="271">ions are based on Frame Semantics (Fillmore, 1976), in which frames are defined as schematic representations of situations involving various Frame Elements such as participants, props, and other conceptual roles. In each FrameNet sentence, a single target predicate is identified and all of its relevant Frame Elements are tagged with their element-type (e.g., Agent, Judge), their syntactic Phrase Type (e.g., NP, PP), and their Grammatical Function (e.g., External Argument, Object Argument). Figure 1 shows an example of an annotated sentence and its appropriate semantic frame. To our knowledge, Gildea and Jurafsky (2000) is the only work that uses FrameNet to build a statistical semantic classifier. They split the problem into two distinct sub-tasks: Frame Element identification and Frame Element classification. In the identification phase, they use syntactic information extracted from a parse tree to learn the boundaries of Frame Elements in sentences. The work presented here, focuses only on the second phase: classification. Gildea and Jurafsky (2000) describe a system that uses completely syntactic features to classify the Frame Elements in a sentence. They extract features from a parse tree and model the </context>
<context position="3834" citStr="Gildea and Jurafsky (2000)" startWordPosition="569" endWordPosition="572">obable tag sequence (as opposed to the most probable individual tags). Finally, we implement a re-ranking system that takes advantage of the sentence-level syntactic patterns of each sequence. We analyze our results using syntactic features extracted from a parse tree generated by Collins parser (Collins, 1997) and compare those to models built using features extracted from FrameNet’s human annotations. 2 Method Training (32,251 sentences), development (3,491 sentences), and held out test sets (3,398 sentences) were generated from the June 2002 FrameNet release following the divisions used in Gildea and Jurafsky (2000) 1 . Because human-annotated syntactic information could only be obtained for a subset of their data, the training, development, and test sets used here are approximately 10% smaller than those used in Gildea and Jurafsky (2000).2 There are on average 2.2 Frame Elements per sentence, falling into one of 126 unique classes. 2.1 Maximum Entropy ME models implement the intuition that the best model will be the one that is consistent with all the evidence, but otherwise, is as uniform as possible. (Berger et al., 1996). Following recent successes using it for many NLP tasks (Och and Ney, 2002; Koe</context>
<context position="5808" citStr="Gildea and Jurafsky (2000)" startWordPosition="902" endWordPosition="905">ted with mean 0 and standard deviation 1. 2.2 Tagging Frame Elements do not occur in isolation, but rather, depend very much on what other Elements occur in a sentence. For example, if a Frame Element is tagged as an Agent it is highly unlikely that the next Element will also be an Agent. We exploit this dependency by treating the Frame Element classification task as a tagging problem. The YASMET MEtagger was used to apply an ngram tag model to the classification task (Bender et al., 2003). The feature set for the training data was 1 Divisions given by Dan Gildea via personal communication. 2 Gildea and Jurafsky (2000) use 36995 training, 4000 development, and 3865 test sentences. They do not report results using hand annotated syntactic information. augmented to include information about the tags of the previous one and two Frame Elements in the sentence: P(r |pt, voice, position, target, gf, h, r -1,r -1+r -2) Viterbi search was then used to find the most probable tag sequence through all possible sequences. 2.3 Pattern Features A great deal of information useful for classification can be found in the syntactic patterns associated with each sequence of Frame Elements. A typical syntactic pattern is exhibi</context>
<context position="8849" citStr="Gildea and Jurafsky (2000)" startWordPosition="1384" endWordPosition="1387">arly 2%, the effect when using automatically extracted information is only 0.5%. This is reasonable 3 Using n-best lists of 50 and 100 showed no significant difference in performance. considering that the re-ranker’s effectiveness is correlated with the level of noise in the syntactic patterns upon which it is based. The difference in performance between the models under both human and extracted conditions was relatively consistent: averaging 8.7% with a standard deviation of 0.7. As a further analysis, we have examined the performance of our base ME model on the same test set as that used in Gildea and Jurafsky (2000). Using only extracted information, we achieve an accuracy of 74.9%, two percent lower than their reported results. This result is not unreasonable, however, because, due to limited time, very little effort was spent tuning the parameters of the model. Figure 2. Performance of models on held out test data. ME refers to results of the base Maximum Entropy model, Tagger to a combined ME and Viterbi search model, Re-Rank to the Tagger augmented with a re-ranker. Extracted refers to models trained using features extracted from parse trees, Human to models using features from FrameNet’s human annot</context>
<context position="10602" citStr="Gildea and Jurafsky (2000)" startWordPosition="1651" endWordPosition="1654">ammatical information. To compensate for noisy parser output, our current work is focusing on two strategies. First, we are looking at using shallower but more reliable methods for syntactic feature generation, such as part of speech tagging and text chunking, to either replace or augment the syntactic parser. Second, we are using ontological information, such as word classes and synonyms, in the hopes that semantic information may supplement the noisy syntactic information. The models trained on features extracted from parse trees do not have access to rich grammatical information. Following Gildea and Jurafsky (2000), automatic extraction of grammatical information here is limited to the governing category of a Noun Phrase. The FrameNet annotations, however, are much richer and include information about complements, modifiers, etc. We are looking at ways to include such information either by using alternative parsers (Hermjakob, 1997) or as a post processing task (Blaheta and Charniak, 2000). In future work, we will extend the strategies outlined here to incorporate Frame Element identification into our model. By treating semantic classification as a single tagging problem, we hope to create a unified, pr</context>
</contexts>
<marker>Gildea, Jurafsky, 2000</marker>
<rawString>D. Gildea and D. Jurafsky. 2000. Automatic Labeling of Semantic Roles, ACL-2000, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hermjakob</author>
</authors>
<title>Learning Parse and Translation Decisions from Examples with Rich Context.</title>
<date>1997</date>
<booktitle>CoNLL-2000.</booktitle>
<tech>Ph.D. Dissertation,</tech>
<institution>University of Texas at</institution>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="10926" citStr="Hermjakob, 1997" startWordPosition="1699" endWordPosition="1700">ntological information, such as word classes and synonyms, in the hopes that semantic information may supplement the noisy syntactic information. The models trained on features extracted from parse trees do not have access to rich grammatical information. Following Gildea and Jurafsky (2000), automatic extraction of grammatical information here is limited to the governing category of a Noun Phrase. The FrameNet annotations, however, are much richer and include information about complements, modifiers, etc. We are looking at ways to include such information either by using alternative parsers (Hermjakob, 1997) or as a post processing task (Blaheta and Charniak, 2000). In future work, we will extend the strategies outlined here to incorporate Frame Element identification into our model. By treating semantic classification as a single tagging problem, we hope to create a unified, practical, and high performance system for Frame Element tagging. Acknowledgments The authors would like to thank Dan Gildea who generously allowed us access to his data files and Oliver Bender for making the MEtagger software publicly available. Finally, we thank Franz Och whose help and expertise were invaluable. Reference</context>
</contexts>
<marker>Hermjakob, 1997</marker>
<rawString>U. Hermjakob, 1997. Learning Parse and Translation Decisions from Examples with Rich Context. Ph.D. Dissertation, University of Texas at Austin, Austin, TX. R. Koeling. 2000. Chunking with maximum entropy models. CoNLL-2000. Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>ACL-2002.</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4429" citStr="Och and Ney, 2002" startWordPosition="668" endWordPosition="671">ea and Jurafsky (2000) 1 . Because human-annotated syntactic information could only be obtained for a subset of their data, the training, development, and test sets used here are approximately 10% smaller than those used in Gildea and Jurafsky (2000).2 There are on average 2.2 Frame Elements per sentence, falling into one of 126 unique classes. 2.1 Maximum Entropy ME models implement the intuition that the best model will be the one that is consistent with all the evidence, but otherwise, is as uniform as possible. (Berger et al., 1996). Following recent successes using it for many NLP tasks (Och and Ney, 2002; Koeling, 2000), we use ME to implement a Frame Element classifier. We use the YASMET ME package (Och, 2002) to train an approximation of the model below: P(r |pt, voice, position, target, gf, h) Here r indicates the element type, pt the phrase type, gf the grammatical function, h the head word, and target the target predicate. Due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in Gildea and Jurafsky (2000). The classifier was trained, using only features that had a frequency in training of one or more, and until p</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F.J. Och, H. Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. ACL-2002. Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<date>2002</date>
<note>Yet another maxent toolkit: YASMET. wwwi6.informatik.rwth-aachen.de/Colleagues/och/.</note>
<contexts>
<context position="4538" citStr="Och, 2002" startWordPosition="689" endWordPosition="690">r data, the training, development, and test sets used here are approximately 10% smaller than those used in Gildea and Jurafsky (2000).2 There are on average 2.2 Frame Elements per sentence, falling into one of 126 unique classes. 2.1 Maximum Entropy ME models implement the intuition that the best model will be the one that is consistent with all the evidence, but otherwise, is as uniform as possible. (Berger et al., 1996). Following recent successes using it for many NLP tasks (Och and Ney, 2002; Koeling, 2000), we use ME to implement a Frame Element classifier. We use the YASMET ME package (Och, 2002) to train an approximation of the model below: P(r |pt, voice, position, target, gf, h) Here r indicates the element type, pt the phrase type, gf the grammatical function, h the head word, and target the target predicate. Due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in Gildea and Jurafsky (2000). The classifier was trained, using only features that had a frequency in training of one or more, and until performance on the development set ceased to improve. Feature weights were smoothed using a Bayesian method, s</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>F.J. Och. 2002. Yet another maxent toolkit: YASMET. wwwi6.informatik.rwth-aachen.de/Colleagues/och/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>