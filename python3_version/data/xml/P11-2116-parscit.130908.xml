<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015845">
<title confidence="0.70945">
Does Size Matter – How Much Data is Required to Train a REG Algorithm?
</title>
<note confidence="0.784695">
Mari¨et Theune Ruud Koolen Emiel Krahmer Sander Wubben
University of Twente Tilburg University Tilburg University Tilburg University
P.O. Box 217 P.O. Box 90135 P.O. Box 90135 P.O. Box 90135
7500 AE Enschede 5000 LE Tilburg 5000 LE Tilburg 5000 LE Tilburg
The Netherlands The Netherlands The Netherlands The Netherlands
</note>
<email confidence="0.754714">
m.theune@utwente.nl r.m.f.koolen@uvt.nl e.j.krahmer@uvt.nl s.wubben@uvt.nl
</email>
<sectionHeader confidence="0.993417" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.992225222222222">
In this paper we investigate how much data
is required to train an algorithm for attribute
selection, a subtask of Referring Expressions
Generation (REG). To enable comparison be-
tween different-sized training sets, a system-
atic training method was developed. The re-
sults show that depending on the complexity
of the domain, training on 10 to 20 items may
already lead to a good performance.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999907545454545">
There are many ways in which we can refer to ob-
jects and people in the real world. A chair, for ex-
ample, can be referred to as red, large, or seen from
the front, while men may be singled out in terms
of their pogonotrophy (facial hairstyle), clothing and
many other attributes. This poses a problem for al-
gorithms that automatically generate referring ex-
pressions: how to determine which attributes to use?
One solution is to assume that some attributes
are preferred over others, and this is indeed what
many Referring Expressions Generation (REG) al-
gorithms do. A classic example is the Incremental
Algorithm (IA), which postulates the existence of
a complete ranking of relevant attributes (Dale and
Reiter, 1995). The IA essentially iterates through
this list of preferred attributes, selecting an attribute
for inclusion in a referring expression if it helps sin-
gling out the target from the other objects in the
scene (the distractors). Crucially, Dale and Reiter do
not specify how the ranking of attributes should be
determined. They refer to psycholinguistic research
suggesting that, in general, absolute attributes (such
as color) are preferred over relative ones (such as
size), but stress that constructing a preference order
is essentially an empirical question, which will dif-
fer from one domain to another.
Many other REG algorithms similarly rely on
preferences. The graph-based based REG algorithm
(Krahmer et al., 2003), for example, models prefer-
ences in terms of costs, with cheaper properties be-
ing more preferred. Various ways to compute costs
are possible; they can be defined, for instance, in
terms of log probabilities, which makes frequently
encountered properties cheap, and infrequent ones
more expensive. Krahmer et al. (2008) argue that
a less fine-grained cost function might generalize
better, and propose to use frequency information
to, somewhat ad hoc, define three costs: 0 (free),
1 (cheap) and 2 (expensive). This approach was
shown to work well: the graph-based algorithm was
the best performing system in the most recent REG
Challenge (Gatt et al., 2009).
Many other attribute selection algorithms also
rely on training data to determine preferences in one
form or another (Fabbrizio et al., 2008; Gerv´as et
al., 2008; Kelleher, 2007; Spanger et al., 2008; Vi-
ethen and Dale, 2010). Unfortunately, suitable data
is hard to come by. It has been argued that determin-
ing which properties to include in a referring expres-
sion requires a “semantically transparent” corpus
(van Deemter et al., 2006): a corpus that contains
the actual properties of all domain objects as well
as the properties that were selected for inclusion in
a given reference to the target. Obviously, text cor-
pora tend not to meet this requirement, which is why
</bodyText>
<page confidence="0.955215">
660
</page>
<note confidence="0.5830385">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 660–664,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999857454545455">
semantically transparent corpora are often collected
using human participants who are asked to produce
referring expressions for targets in controlled visual
scenes for a given domain. Since this is a time con-
suming exercise, it will not be surprising that such
corpora are thin on the ground (and are often only
available for English). An important question there-
fore is how many human-produced references are
needed to achieve a certain level of performance. Do
we really need hundreds of instances, or can we al-
ready make informed decisions about preferences on
a few or even one training instance?
In this paper, we address this question by sys-
tematically training the graph-based REG algorithm
on a number of “semantically transparent” data sets
of various sizes and evaluating on a held-out test
set. The graph-based algorithm seems a good can-
didate for this exercise, in view of its performance
in the REG challenges. For the sake of compari-
son, we also follow the evaluation methodology of
the REG challenges, training and testing on two do-
mains (a furniture and a people domain), and using
two automatic metrics (Dice and accuracy) to mea-
sure human-likeness. One hurdle needs to be taken
beforehand. Krahmer et al. (2008) manually as-
signed one of three costs to properties, loosely based
on corpus frequencies. For our current evaluation
experiments, this would hamper comparison across
data sets, because it is difficult to do it in a manner
that is both consistent and meaningful. Therefore we
first experiment with a more systematic way of as-
signing a limited number of frequency-based costs
to properties using k-means clustering.
</bodyText>
<sectionHeader confidence="0.902189" genericHeader="method">
2 Experiment I: k-means clustering costs
</sectionHeader>
<bodyText confidence="0.9999722">
In this section we describe our experiment with k-
means clustering to derive property costs from En-
glish and Dutch corpus data. For this experiment we
looked at both English and Dutch, to make sure the
chosen method does not only work well for English.
</bodyText>
<subsectionHeader confidence="0.917004">
2.1 Materials
</subsectionHeader>
<bodyText confidence="0.999993206896552">
Our English training and test data were taken from
the TUNA corpus (Gatt et al., 2007). This semanti-
cally transparent corpus contains referring expres-
sions in two domains (furniture and people), col-
lected in one of two conditions: in the -LOC con-
dition, participants were discouraged from mention-
ing the location of the target in the visual scene,
whereas in the +LOC condition they could mention
any properties they wanted. The TUNA corpus was
used for comparative evaluation in the REG Chal-
lenges (2007-2009). For training in our current ex-
periment, we used the -LOC data from the training
set of the REG Challenge 2009 (Gatt et al., 2009):
165 furniture descriptions and 136 people descrip-
tions. For testing, we used the -LOC data from the
TUNA 2009 development set: 38 furniture descrip-
tions and 38 people descriptions.
Dutch data were taken from the D-TUNA corpus
(Koolen and Krahmer, 2010). This corpus uses the
same visual scenes and annotation scheme as the
TUNA corpus, but with Dutch instead of English
descriptions. D-TUNA does not include locations as
object properties at all, hence our restriction to -LOC
data for English (to make the Dutch and English data
more comparable). As Dutch test data, we used 40
furniture items and 40 people items, randomly se-
lected from the textual descriptions in the D-TUNA
corpus. The remaining furniture and people descrip-
tions (160 items each) were used for training.
</bodyText>
<subsectionHeader confidence="0.999293">
2.2 Method
</subsectionHeader>
<bodyText confidence="0.999349">
We first determined the frequency with which each
property was mentioned in our training data, relative
to the number of target objects with this property.
Then we created different cost functions (mapping
properties to costs) by means of k-means clustering,
using the Weka toolkit. The k-means clustering al-
gorithm assigns n points in a vector space to k clus-
ters (S1 to Sk) by assigning each point to the clus-
ter with the nearest centroid. The total intra-cluster
variance V is minimized by the function
</bodyText>
<equation confidence="0.9102115">
2
(xj − µi)
</equation>
<bodyText confidence="0.999653833333333">
where µi is the centroid of all the points xj E Si.
In our case, the points n are properties, the vector
space is one-dimensional (frequency being the only
dimension) and µi is the average frequency of the
properties in Si. The cluster-based costs are defined
as follows:
</bodyText>
<equation confidence="0.990224">
bxj E Si, cost(xj) = i − 1
k
i=1
V =
�
xj∈Si
</equation>
<page confidence="0.972572">
661
</page>
<table confidence="0.999845333333333">
Furniture People
Language Costs Dice Acc. Dice Acc.
English k-means 0.810 0.50 0.733 0.29
FN 0.829 0.55 0.733 0.29
Dutch k-means 0.929 0.68 0.812 0.33
FN 0.929 0.68 0.812 0.33
</table>
<tableCaption confidence="0.959696">
Table 1: Results for k-means costs with k = 2 and the
FN costs of Theune et al. (2010) on Dutch and English.
</tableCaption>
<bodyText confidence="0.98610425">
mance, in the second experiment we derived cost
functions and property orders from different sized
training sets, and evaluated them on our test data.
For this experiment, we only used English data.
</bodyText>
<subsectionHeader confidence="0.913938">
3.1 Materials
</subsectionHeader>
<bodyText confidence="0.99992154054054">
As training sets, we used randomly selected subsets
of the full English training set from Experiment I,
with set sizes of 1, 5, 10, 20 and 30 items. Be-
cause the accidental composition of a training set
may strongly influence the results, we created 5 dif-
ferent sets of each size. The training sets were built
up in a cumulative fashion: we started with five sets
of size 1, then added 4 items to each of them to cre-
ate five sets of size 5, etc. This resulted in five series
of increasingly sized training sets. As test data, we
used the same English test set as in Experiment I.
where S1 is the cluster with the most frequent
properties, S2 is the cluster with the next most fre-
quent properties, and so on. Using this approach,
properties from cluster S1 get cost 0 and thus can be
added “for free” to a description. Free properties are
always included, provided they help distinguish the
target. This may lead to overspecified descriptions,
mimicking the human tendency to mention redun-
dant properties (Dale and Reiter, 1995).
We ran the clustering algorithm on our English
and Dutch training data for up to six clusters (k = 2
to k = 6). Then we evaluated the performance of
the resulting cost functions on the test data from
the same language, using Dice (overlap between at-
tribute sets) and Accuracy (perfect match between
sets) as evaluation metrics. For comparison, we also
evaluated the best scoring cost functions from Theu-
ne et al. (2010) on our test data. These “Free-Naive”
(FN) functions were created using the manual ap-
proach sketched in the introduction.
The order in which the graph-based algorithm
tries to add attributes to a description is explicitly
controlled to ensure that “free” distinguishing prop-
erties are included (Viethen et al., 2008). In our
tests, we used an order of decreasing frequency; i.e.,
always examining more frequent properties first.1
</bodyText>
<sectionHeader confidence="0.790421" genericHeader="method">
2.3 Results
</sectionHeader>
<subsectionHeader confidence="0.999885">
3.2 Method
</subsectionHeader>
<bodyText confidence="0.999954153846154">
For the cluster-based cost functions, the best perfor-
mance was achieved with k = 2, for both domains
and both languages. Interestingly, this is the coarsest
possible k-means function: with only two costs (0
and 1) it is even less fine-grained than the FN func-
tions advocated by Krahmer et al. (2008). The re-
sults for the k-means costs with k = 2 and the FN
costs of Theune et al. (2010) are shown in Table 1.
No significant differences were found, which sug-
gests that k-means clustering, with k = 2, can be
used as a more systematic alternative for the manual
assignment of frequency-based costs. We therefore
applied this method in the next experiment.
</bodyText>
<sectionHeader confidence="0.993933" genericHeader="method">
3 Experiment II: varying training set size
</sectionHeader>
<bodyText confidence="0.9997825">
To find out how much training data is required
to achieve an acceptable attribute selection perfor-
</bodyText>
<footnote confidence="0.9039125">
1We used slightly different property orders than Theune et
al. (2010), leading to minor differences in our FN results.
</footnote>
<bodyText confidence="0.9999883">
We derived cost functions (using k-means clustering
with k = 2) and orders from each of the training
sets, following the method described in Section 2.2.
In doing so, we had to deal with missing data: not all
properties were present in all data sets.2 For the cost
functions, we simply assigned the highest cost (1)
to the missing properties. For the order, we listed
properties with the same frequency (0 for missing
properties) in alphabetical order. This was done for
the sake of comparability between training sets.
</bodyText>
<subsectionHeader confidence="0.703767">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.99989075">
To determine significance, we calculated the means
of the scores of the five training sets for each set
size, so that we could compare them with the scores
of the entire set. We applied repeated measures of
</bodyText>
<footnote confidence="0.674005333333333">
2This problem mostly affected the smaller training sets. By
set size 10 only a few properties were missing, while by set size
20, all properties were present in all sets.
</footnote>
<page confidence="0.996357">
662
</page>
<bodyText confidence="0.999357571428572">
variance (ANOVA) to the Dice and Accuracy scores,
using set size (1, 5, 10, 20, 30, entire set) as a within
variable. The mean results for each training set size
are shown in Table 2.3 The general pattern is that
the scores increase with the size of the training set,
but the increase gets smaller as the set sizes become
larger.
</bodyText>
<table confidence="0.99970725">
Furniture People
Set size Dice Acc. Dice Acc.
1 0.693 0.25 0.560 0.13
5 0.756 0.34 0.620 0.15
10 0.777 0.40 0.686 0.20
20 0.788 0.41 0.719 0.25
30 0.782 0.41 0.718 0.27
Entire set 0.810 0.50 0.733 0.29
</table>
<tableCaption confidence="0.999154">
Table 2: Mean results for the different set sizes.
</tableCaption>
<bodyText confidence="0.999558791666667">
In the furniture domain, we found a main effect
of set size (Dice: F(5,185) = 7.209, p &lt; .001; Ac-
curacy: F(5,185) = 6.140, p &lt; .001). To see which
set sizes performed significantly different as com-
pared to the entire set, we conducted Tukey’s HSD
post hoc comparisons. For Dice, the scores of set
size 10 (p = .141), set size 20 (p = .353), and set
size 30 (p = .197) did not significantly differ from
the scores of the entire set of 165 items. The Accu-
racy scores in the furniture domain show a slightly
different pattern: the scores of the entire training set
were still significantly higher than those of set size
30 (p &lt; .05). This better performance when trained
on the entire set may be caused by the fact that not
all of the five training sets that were used for set sizes
1, 5, 10, 20 and 30 performed equally well.
In the people domain we also found a main effect
of set size (Dice: F(5,185) = 21.359, p &lt; .001; Accu-
racy: F(5,185) = 8.074, p &lt; .001). Post hoc pairwise
comparisons showed that the scores of set size 20
(Dice: p = .416; Accuracy: p = .146) and set size
30 (Dice: p = .238; Accuracy: p = .324) did not
significantly differ from those of the full set of 136
items.
</bodyText>
<footnote confidence="0.9802175">
3For comparison: in the REG Challenge 2008, (which in-
volved a different test set, but the same type of data), the best
systems obtained overall Dice and accuracy scores of around
0.80 and 0.55 respectively (Gatt et al., 2008). These scores may
well represent the performance ceiling for speaker and context
independent algorithms on this task.
</footnote>
<sectionHeader confidence="0.989998" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999984129032258">
Experiment II has shown that when using small data
sets to train an attribute selection algorithm, results
can be achieved that are not significantly different
from those obtained using a much larger training
set. Domain complexity appears to be a factor in
how much training data is needed: using Dice as an
evaluation metric, training sets of 10 sufficed in the
simple furniture domain, while in the more complex
people domain it took a set size of 20 to achieve re-
sults that do not significantly differ from those ob-
tained using the full training set.
The accidental composition of the training sets
may strongly influence the attribute selection per-
formance. In the furniture domain, we found clear
differences between the results of specific training
sets, with “bad sets” pulling the overall performance
down. This affected Accuracy but not Dice, possibly
because the latter is a less strict metric.
Whether the encouraging results found for the
graph-based algorithm generalize to other REG ap-
proaches is still an open question. We also need
to investigate how the use of small training sets af-
fects effectiveness and efficiency of target identifica-
tion by human subjects; as shown by Belz and Gatt
(2008), task-performance measures do not necessar-
ily correlate with similarity measures such as Dice.
Finally, it will be interesting to repeat Experiment II
with Dutch data. The D-TUNA data are cleaner than
the TUNA data (Theune et al., 2010), so the risk of
“bad” training data will be smaller, which may lead
to more consistent results across training sets.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999437375">
Our experiment has shown that with 20 or less train-
ing instances, acceptable attribute selection results
can be achieved; that is, results that do not signif-
icantly differ from those obtained using the entire
training set. This is good news, because collecting
such small amounts of training data should not take
too much time and effort, making it relatively easy
to do REG for new domains and languages.
</bodyText>
<sectionHeader confidence="0.998307" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994170666666667">
Krahmer and Koolen received financial support from
The Netherlands Organization for Scientific Re-
search (Vici grant 27770007).
</bodyText>
<page confidence="0.998652">
663
</page>
<sectionHeader confidence="0.99011" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99940932">
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic
evaluation measures for referring expression genera-
tion. In Proceedings of ACL-08: HLT, Short Papers,
pages 197–200.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretation of the Gricean maxims in the generation of
referring expressions. Cognitive Science, 19(2):233–
263.
Giuseppe Di Fabbrizio, Amanda Stent, and Srinivas
Bangalore. 2008. Trainable speaker-based refer-
ring expression generation. In Twelfth Conference on
Computational Natural Language Learning (CoNLL-
2008), pages 151–158.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. In Pro-
ceedings of the 11th European Workshop on Natural
Language Generation (ENLG 2007), pages 49–56.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The
TUNA Challenge 2008: Overview and evaluation re-
sults. In Proceedings of the 5th International Natural
Language Generation Conference (INLG 2008), pages
198–206.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA-
REG Challenge 2009: Overview and evaluation re-
sults. In Proceedings of the 12th European Workshop
on Natural Language Generation (ENLG 2009), pages
174–182.
Pablo Gerv´as, Raquel Herv´as, and Carlos L´eon. 2008.
NIL-UCM: Most-frequent-value-first attribute selec-
tion and best-scoring-choice realization. In Proceed-
ings of the 5th International Natural Language Gener-
ation Conference (INLG 2008), pages 215–218.
John Kelleher. 2007. DIT - frequency based incremen-
tal attribute selection for GRE. In Proceedings of the
MT Summit XI Workshop Using Corpora for Natural
Language Generation: Language Generation and Ma-
chine Translation (UCNLG+MT), pages 90–92.
Ruud Koolen and Emiel Krahmer. 2010. The D-TUNA
corpus: A Dutch dataset for the evaluation of refer-
ring expression generation algorithms. In Proceedings
of the 7th international conference on Language Re-
sources and Evaluation (LREC 2010).
Emiel Krahmer, Sebastiaan van Erk, and Andr´e Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53–72.
Emiel Krahmer, Mari¨et Theune, Jette Viethen, and Iris
Hendrickx. 2008. GRAPH: The costs of redundancy
in referring expressions. In Proceedings of the 5th In-
ternational Natural Language Generation Conference
(INLG 2008), pages 227–229.
Philipp Spanger, Takehiro Kurosawa, and Takenobu
Tokunaga. 2008. On “redundancy” in selecting at-
tributes for generating referring expressions. In COL-
ING 2008: Companion volume: Posters, pages 115–
118.
Mari¨et Theune, Ruud Koolen, and Emiel Krahmer. 2010.
Cross-linguistic attribute selection for REG: Compar-
ing Dutch and English. In Proceedings of the 6th In-
ternational Natural Language Generation Conference
(INLG 2010), pages 174–182.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus for
the generation of referring expressions. In Proceed-
ings of the 4th International Natural Language Gener-
ation Conference (INLG 2006), pages 130–132.
Jette Viethen and Robert Dale. 2010. Speaker-dependent
variation in content selection for referring expression
generation. In Proceedings of the 8th Australasian
Language Technology Workshop, pages 81–89.
Jette Viethen, Robert Dale, Emiel Krahmer, Mari¨et Theu-
ne, and Pascal Touset. 2008. Controlling redundancy
in referring expressions. In Proceedings of the Sixth
International Conference on Language Resources and
Evaluation (LREC 2008), pages 239–246.
</reference>
<page confidence="0.998066">
664
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.882163">
<title confidence="0.996453">Does Size Matter – How Much Data is Required to Train a REG Algorithm?</title>
<author confidence="0.999773">Mari¨et Theune Ruud Koolen Emiel Krahmer Sander Wubben</author>
<affiliation confidence="0.999942">University of Twente Tilburg University Tilburg University Tilburg University</affiliation>
<address confidence="0.978648666666667">P.O. Box 217 P.O. Box 90135 P.O. Box 90135 P.O. Box 90135 7500 AE Enschede 5000 LE Tilburg 5000 LE Tilburg 5000 LE Tilburg The Netherlands The Netherlands The Netherlands The Netherlands</address>
<email confidence="0.957948">m.theune@utwente.nlr.m.f.koolen@uvt.nle.j.krahmer@uvt.nls.wubben@uvt.nl</email>
<abstract confidence="0.9974296">In this paper we investigate how much data is required to train an algorithm for attribute selection, a subtask of Referring Expressions Generation (REG). To enable comparison between different-sized training sets, a systematic training method was developed. The results show that depending on the complexity of the domain, training on 10 to 20 items may already lead to a good performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Albert Gatt</author>
</authors>
<title>Intrinsic vs. extrinsic evaluation measures for referring expression generation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>197--200</pages>
<contexts>
<context position="15643" citStr="Belz and Gatt (2008)" startWordPosition="2643" endWordPosition="2646">ining sets may strongly influence the attribute selection performance. In the furniture domain, we found clear differences between the results of specific training sets, with “bad sets” pulling the overall performance down. This affected Accuracy but not Dice, possibly because the latter is a less strict metric. Whether the encouraging results found for the graph-based algorithm generalize to other REG approaches is still an open question. We also need to investigate how the use of small training sets affects effectiveness and efficiency of target identification by human subjects; as shown by Belz and Gatt (2008), task-performance measures do not necessarily correlate with similarity measures such as Dice. Finally, it will be interesting to repeat Experiment II with Dutch data. The D-TUNA data are cleaner than the TUNA data (Theune et al., 2010), so the risk of “bad” training data will be smaller, which may lead to more consistent results across training sets. 5 Conclusion Our experiment has shown that with 20 or less training instances, acceptable attribute selection results can be achieved; that is, results that do not significantly differ from those obtained using the entire training set. This is g</context>
</contexts>
<marker>Belz, Gatt, 2008</marker>
<rawString>Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic evaluation measures for referring expression generation. In Proceedings of ACL-08: HLT, Short Papers, pages 197–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretation of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263</pages>
<contexts>
<context position="1598" citStr="Dale and Reiter, 1995" startWordPosition="251" endWordPosition="254"> example, can be referred to as red, large, or seen from the front, while men may be singled out in terms of their pogonotrophy (facial hairstyle), clothing and many other attributes. This poses a problem for algorithms that automatically generate referring expressions: how to determine which attributes to use? One solution is to assume that some attributes are preferred over others, and this is indeed what many Referring Expressions Generation (REG) algorithms do. A classic example is the Incremental Algorithm (IA), which postulates the existence of a complete ranking of relevant attributes (Dale and Reiter, 1995). The IA essentially iterates through this list of preferred attributes, selecting an attribute for inclusion in a referring expression if it helps singling out the target from the other objects in the scene (the distractors). Crucially, Dale and Reiter do not specify how the ranking of attributes should be determined. They refer to psycholinguistic research suggesting that, in general, absolute attributes (such as color) are preferred over relative ones (such as size), but stress that constructing a preference order is essentially an empirical question, which will differ from one domain to an</context>
<context position="9601" citStr="Dale and Reiter, 1995" startWordPosition="1585" endWordPosition="1588">them to create five sets of size 5, etc. This resulted in five series of increasingly sized training sets. As test data, we used the same English test set as in Experiment I. where S1 is the cluster with the most frequent properties, S2 is the cluster with the next most frequent properties, and so on. Using this approach, properties from cluster S1 get cost 0 and thus can be added “for free” to a description. Free properties are always included, provided they help distinguish the target. This may lead to overspecified descriptions, mimicking the human tendency to mention redundant properties (Dale and Reiter, 1995). We ran the clustering algorithm on our English and Dutch training data for up to six clusters (k = 2 to k = 6). Then we evaluated the performance of the resulting cost functions on the test data from the same language, using Dice (overlap between attribute sets) and Accuracy (perfect match between sets) as evaluation metrics. For comparison, we also evaluated the best scoring cost functions from Theune et al. (2010) on our test data. These “Free-Naive” (FN) functions were created using the manual approach sketched in the introduction. The order in which the graph-based algorithm tries to add</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational interpretation of the Gricean maxims in the generation of referring expressions. Cognitive Science, 19(2):233– 263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Di Fabbrizio</author>
<author>Amanda Stent</author>
<author>Srinivas Bangalore</author>
</authors>
<title>Trainable speaker-based referring expression generation.</title>
<date>2008</date>
<booktitle>In Twelfth Conference on Computational Natural Language Learning (CoNLL2008),</booktitle>
<pages>151--158</pages>
<marker>Di Fabbrizio, Stent, Bangalore, 2008</marker>
<rawString>Giuseppe Di Fabbrizio, Amanda Stent, and Srinivas Bangalore. 2008. Trainable speaker-based referring expression generation. In Twelfth Conference on Computational Natural Language Learning (CoNLL2008), pages 151–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Ielka van der Sluis</author>
<author>Kees van Deemter</author>
</authors>
<title>Evaluating algorithms for the generation of referring expressions using a balanced corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th European Workshop on Natural Language Generation (ENLG</booktitle>
<pages>49--56</pages>
<marker>Gatt, van der Sluis, van Deemter, 2007</marker>
<rawString>Albert Gatt, Ielka van der Sluis, and Kees van Deemter. 2007. Evaluating algorithms for the generation of referring expressions using a balanced corpus. In Proceedings of the 11th European Workshop on Natural Language Generation (ENLG 2007), pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Anja Belz</author>
<author>Eric Kow</author>
</authors>
<title>The TUNA Challenge 2008: Overview and evaluation results.</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th International Natural Language Generation Conference (INLG</booktitle>
<pages>198--206</pages>
<contexts>
<context position="14299" citStr="Gatt et al., 2008" startWordPosition="2424" endWordPosition="2427">d 30 performed equally well. In the people domain we also found a main effect of set size (Dice: F(5,185) = 21.359, p &lt; .001; Accuracy: F(5,185) = 8.074, p &lt; .001). Post hoc pairwise comparisons showed that the scores of set size 20 (Dice: p = .416; Accuracy: p = .146) and set size 30 (Dice: p = .238; Accuracy: p = .324) did not significantly differ from those of the full set of 136 items. 3For comparison: in the REG Challenge 2008, (which involved a different test set, but the same type of data), the best systems obtained overall Dice and accuracy scores of around 0.80 and 0.55 respectively (Gatt et al., 2008). These scores may well represent the performance ceiling for speaker and context independent algorithms on this task. 4 Discussion Experiment II has shown that when using small data sets to train an attribute selection algorithm, results can be achieved that are not significantly different from those obtained using a much larger training set. Domain complexity appears to be a factor in how much training data is needed: using Dice as an evaluation metric, training sets of 10 sufficed in the simple furniture domain, while in the more complex people domain it took a set size of 20 to achieve res</context>
</contexts>
<marker>Gatt, Belz, Kow, 2008</marker>
<rawString>Albert Gatt, Anja Belz, and Eric Kow. 2008. The TUNA Challenge 2008: Overview and evaluation results. In Proceedings of the 5th International Natural Language Generation Conference (INLG 2008), pages 198–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Anja Belz</author>
<author>Eric Kow</author>
</authors>
<title>The TUNAREG Challenge 2009: Overview and evaluation results.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG</booktitle>
<pages>174--182</pages>
<contexts>
<context position="2978" citStr="Gatt et al., 2009" startWordPosition="466" endWordPosition="469">ms of costs, with cheaper properties being more preferred. Various ways to compute costs are possible; they can be defined, for instance, in terms of log probabilities, which makes frequently encountered properties cheap, and infrequent ones more expensive. Krahmer et al. (2008) argue that a less fine-grained cost function might generalize better, and propose to use frequency information to, somewhat ad hoc, define three costs: 0 (free), 1 (cheap) and 2 (expensive). This approach was shown to work well: the graph-based algorithm was the best performing system in the most recent REG Challenge (Gatt et al., 2009). Many other attribute selection algorithms also rely on training data to determine preferences in one form or another (Fabbrizio et al., 2008; Gerv´as et al., 2008; Kelleher, 2007; Spanger et al., 2008; Viethen and Dale, 2010). Unfortunately, suitable data is hard to come by. It has been argued that determining which properties to include in a referring expression requires a “semantically transparent” corpus (van Deemter et al., 2006): a corpus that contains the actual properties of all domain objects as well as the properties that were selected for inclusion in a given reference to the targe</context>
<context position="6450" citStr="Gatt et al., 2009" startWordPosition="1033" endWordPosition="1036">d test data were taken from the TUNA corpus (Gatt et al., 2007). This semantically transparent corpus contains referring expressions in two domains (furniture and people), collected in one of two conditions: in the -LOC condition, participants were discouraged from mentioning the location of the target in the visual scene, whereas in the +LOC condition they could mention any properties they wanted. The TUNA corpus was used for comparative evaluation in the REG Challenges (2007-2009). For training in our current experiment, we used the -LOC data from the training set of the REG Challenge 2009 (Gatt et al., 2009): 165 furniture descriptions and 136 people descriptions. For testing, we used the -LOC data from the TUNA 2009 development set: 38 furniture descriptions and 38 people descriptions. Dutch data were taken from the D-TUNA corpus (Koolen and Krahmer, 2010). This corpus uses the same visual scenes and annotation scheme as the TUNA corpus, but with Dutch instead of English descriptions. D-TUNA does not include locations as object properties at all, hence our restriction to -LOC data for English (to make the Dutch and English data more comparable). As Dutch test data, we used 40 furniture items and</context>
</contexts>
<marker>Gatt, Belz, Kow, 2009</marker>
<rawString>Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNAREG Challenge 2009: Overview and evaluation results. In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG 2009), pages 174–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Gerv´as</author>
<author>Raquel Herv´as</author>
<author>Carlos L´eon</author>
</authors>
<title>NIL-UCM: Most-frequent-value-first attribute selection and best-scoring-choice realization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th International Natural Language Generation Conference (INLG</booktitle>
<pages>215--218</pages>
<marker>Gerv´as, Herv´as, L´eon, 2008</marker>
<rawString>Pablo Gerv´as, Raquel Herv´as, and Carlos L´eon. 2008. NIL-UCM: Most-frequent-value-first attribute selection and best-scoring-choice realization. In Proceedings of the 5th International Natural Language Generation Conference (INLG 2008), pages 215–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Kelleher</author>
</authors>
<title>DIT - frequency based incremental attribute selection for GRE.</title>
<date>2007</date>
<booktitle>In Proceedings of the MT Summit XI Workshop Using Corpora for Natural Language Generation: Language Generation and Machine Translation (UCNLG+MT),</booktitle>
<pages>90--92</pages>
<contexts>
<context position="3158" citStr="Kelleher, 2007" startWordPosition="496" endWordPosition="497">quently encountered properties cheap, and infrequent ones more expensive. Krahmer et al. (2008) argue that a less fine-grained cost function might generalize better, and propose to use frequency information to, somewhat ad hoc, define three costs: 0 (free), 1 (cheap) and 2 (expensive). This approach was shown to work well: the graph-based algorithm was the best performing system in the most recent REG Challenge (Gatt et al., 2009). Many other attribute selection algorithms also rely on training data to determine preferences in one form or another (Fabbrizio et al., 2008; Gerv´as et al., 2008; Kelleher, 2007; Spanger et al., 2008; Viethen and Dale, 2010). Unfortunately, suitable data is hard to come by. It has been argued that determining which properties to include in a referring expression requires a “semantically transparent” corpus (van Deemter et al., 2006): a corpus that contains the actual properties of all domain objects as well as the properties that were selected for inclusion in a given reference to the target. Obviously, text corpora tend not to meet this requirement, which is why 660 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, </context>
</contexts>
<marker>Kelleher, 2007</marker>
<rawString>John Kelleher. 2007. DIT - frequency based incremental attribute selection for GRE. In Proceedings of the MT Summit XI Workshop Using Corpora for Natural Language Generation: Language Generation and Machine Translation (UCNLG+MT), pages 90–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruud Koolen</author>
<author>Emiel Krahmer</author>
</authors>
<title>The D-TUNA corpus: A Dutch dataset for the evaluation of referring expression generation algorithms.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th international conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="6704" citStr="Koolen and Krahmer, 2010" startWordPosition="1074" endWordPosition="1077">e discouraged from mentioning the location of the target in the visual scene, whereas in the +LOC condition they could mention any properties they wanted. The TUNA corpus was used for comparative evaluation in the REG Challenges (2007-2009). For training in our current experiment, we used the -LOC data from the training set of the REG Challenge 2009 (Gatt et al., 2009): 165 furniture descriptions and 136 people descriptions. For testing, we used the -LOC data from the TUNA 2009 development set: 38 furniture descriptions and 38 people descriptions. Dutch data were taken from the D-TUNA corpus (Koolen and Krahmer, 2010). This corpus uses the same visual scenes and annotation scheme as the TUNA corpus, but with Dutch instead of English descriptions. D-TUNA does not include locations as object properties at all, hence our restriction to -LOC data for English (to make the Dutch and English data more comparable). As Dutch test data, we used 40 furniture items and 40 people items, randomly selected from the textual descriptions in the D-TUNA corpus. The remaining furniture and people descriptions (160 items each) were used for training. 2.2 Method We first determined the frequency with which each property was men</context>
</contexts>
<marker>Koolen, Krahmer, 2010</marker>
<rawString>Ruud Koolen and Emiel Krahmer. 2010. The D-TUNA corpus: A Dutch dataset for the evaluation of referring expression generation algorithms. In Proceedings of the 7th international conference on Language Resources and Evaluation (LREC 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Sebastiaan van Erk</author>
<author>Andr´e Verleg</author>
</authors>
<title>Graph-based generation of referring expressions.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Krahmer, van Erk, Verleg, 2003</marker>
<rawString>Emiel Krahmer, Sebastiaan van Erk, and Andr´e Verleg. 2003. Graph-based generation of referring expressions. Computational Linguistics, 29(1):53–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Mari¨et Theune</author>
<author>Jette Viethen</author>
<author>Iris Hendrickx</author>
</authors>
<title>GRAPH: The costs of redundancy in referring expressions.</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th International Natural Language Generation Conference (INLG</booktitle>
<pages>227--229</pages>
<contexts>
<context position="2639" citStr="Krahmer et al. (2008)" startWordPosition="411" endWordPosition="414">olor) are preferred over relative ones (such as size), but stress that constructing a preference order is essentially an empirical question, which will differ from one domain to another. Many other REG algorithms similarly rely on preferences. The graph-based based REG algorithm (Krahmer et al., 2003), for example, models preferences in terms of costs, with cheaper properties being more preferred. Various ways to compute costs are possible; they can be defined, for instance, in terms of log probabilities, which makes frequently encountered properties cheap, and infrequent ones more expensive. Krahmer et al. (2008) argue that a less fine-grained cost function might generalize better, and propose to use frequency information to, somewhat ad hoc, define three costs: 0 (free), 1 (cheap) and 2 (expensive). This approach was shown to work well: the graph-based algorithm was the best performing system in the most recent REG Challenge (Gatt et al., 2009). Many other attribute selection algorithms also rely on training data to determine preferences in one form or another (Fabbrizio et al., 2008; Gerv´as et al., 2008; Kelleher, 2007; Spanger et al., 2008; Viethen and Dale, 2010). Unfortunately, suitable data is </context>
<context position="5088" citStr="Krahmer et al. (2008)" startWordPosition="806" endWordPosition="809"> paper, we address this question by systematically training the graph-based REG algorithm on a number of “semantically transparent” data sets of various sizes and evaluating on a held-out test set. The graph-based algorithm seems a good candidate for this exercise, in view of its performance in the REG challenges. For the sake of comparison, we also follow the evaluation methodology of the REG challenges, training and testing on two domains (a furniture and a people domain), and using two automatic metrics (Dice and accuracy) to measure human-likeness. One hurdle needs to be taken beforehand. Krahmer et al. (2008) manually assigned one of three costs to properties, loosely based on corpus frequencies. For our current evaluation experiments, this would hamper comparison across data sets, because it is difficult to do it in a manner that is both consistent and meaningful. Therefore we first experiment with a more systematic way of assigning a limited number of frequency-based costs to properties using k-means clustering. 2 Experiment I: k-means clustering costs In this section we describe our experiment with kmeans clustering to derive property costs from English and Dutch corpus data. For this experimen</context>
<context position="10774" citStr="Krahmer et al. (2008)" startWordPosition="1781" endWordPosition="1784"> in which the graph-based algorithm tries to add attributes to a description is explicitly controlled to ensure that “free” distinguishing properties are included (Viethen et al., 2008). In our tests, we used an order of decreasing frequency; i.e., always examining more frequent properties first.1 2.3 Results 3.2 Method For the cluster-based cost functions, the best performance was achieved with k = 2, for both domains and both languages. Interestingly, this is the coarsest possible k-means function: with only two costs (0 and 1) it is even less fine-grained than the FN functions advocated by Krahmer et al. (2008). The results for the k-means costs with k = 2 and the FN costs of Theune et al. (2010) are shown in Table 1. No significant differences were found, which suggests that k-means clustering, with k = 2, can be used as a more systematic alternative for the manual assignment of frequency-based costs. We therefore applied this method in the next experiment. 3 Experiment II: varying training set size To find out how much training data is required to achieve an acceptable attribute selection perfor1We used slightly different property orders than Theune et al. (2010), leading to minor differences in o</context>
</contexts>
<marker>Krahmer, Theune, Viethen, Hendrickx, 2008</marker>
<rawString>Emiel Krahmer, Mari¨et Theune, Jette Viethen, and Iris Hendrickx. 2008. GRAPH: The costs of redundancy in referring expressions. In Proceedings of the 5th International Natural Language Generation Conference (INLG 2008), pages 227–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Spanger</author>
<author>Takehiro Kurosawa</author>
<author>Takenobu Tokunaga</author>
</authors>
<title>On “redundancy” in selecting attributes for generating referring expressions.</title>
<date>2008</date>
<booktitle>In COLING 2008: Companion volume: Posters,</booktitle>
<pages>115--118</pages>
<contexts>
<context position="3180" citStr="Spanger et al., 2008" startWordPosition="498" endWordPosition="501">red properties cheap, and infrequent ones more expensive. Krahmer et al. (2008) argue that a less fine-grained cost function might generalize better, and propose to use frequency information to, somewhat ad hoc, define three costs: 0 (free), 1 (cheap) and 2 (expensive). This approach was shown to work well: the graph-based algorithm was the best performing system in the most recent REG Challenge (Gatt et al., 2009). Many other attribute selection algorithms also rely on training data to determine preferences in one form or another (Fabbrizio et al., 2008; Gerv´as et al., 2008; Kelleher, 2007; Spanger et al., 2008; Viethen and Dale, 2010). Unfortunately, suitable data is hard to come by. It has been argued that determining which properties to include in a referring expression requires a “semantically transparent” corpus (van Deemter et al., 2006): a corpus that contains the actual properties of all domain objects as well as the properties that were selected for inclusion in a given reference to the target. Obviously, text corpora tend not to meet this requirement, which is why 660 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 660–664, Portlan</context>
</contexts>
<marker>Spanger, Kurosawa, Tokunaga, 2008</marker>
<rawString>Philipp Spanger, Takehiro Kurosawa, and Takenobu Tokunaga. 2008. On “redundancy” in selecting attributes for generating referring expressions. In COLING 2008: Companion volume: Posters, pages 115– 118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari¨et Theune</author>
<author>Ruud Koolen</author>
<author>Emiel Krahmer</author>
</authors>
<title>Cross-linguistic attribute selection for REG: Comparing Dutch and English.</title>
<date>2010</date>
<booktitle>In Proceedings of the 6th International Natural Language Generation Conference (INLG</booktitle>
<pages>174--182</pages>
<contexts>
<context position="8339" citStr="Theune et al. (2010)" startWordPosition="1363" endWordPosition="1366">inimized by the function 2 (xj − µi) where µi is the centroid of all the points xj E Si. In our case, the points n are properties, the vector space is one-dimensional (frequency being the only dimension) and µi is the average frequency of the properties in Si. The cluster-based costs are defined as follows: bxj E Si, cost(xj) = i − 1 k i=1 V = � xj∈Si 661 Furniture People Language Costs Dice Acc. Dice Acc. English k-means 0.810 0.50 0.733 0.29 FN 0.829 0.55 0.733 0.29 Dutch k-means 0.929 0.68 0.812 0.33 FN 0.929 0.68 0.812 0.33 Table 1: Results for k-means costs with k = 2 and the FN costs of Theune et al. (2010) on Dutch and English. mance, in the second experiment we derived cost functions and property orders from different sized training sets, and evaluated them on our test data. For this experiment, we only used English data. 3.1 Materials As training sets, we used randomly selected subsets of the full English training set from Experiment I, with set sizes of 1, 5, 10, 20 and 30 items. Because the accidental composition of a training set may strongly influence the results, we created 5 different sets of each size. The training sets were built up in a cumulative fashion: we started with five sets o</context>
<context position="10022" citStr="Theune et al. (2010)" startWordPosition="1658" endWordPosition="1662">rties are always included, provided they help distinguish the target. This may lead to overspecified descriptions, mimicking the human tendency to mention redundant properties (Dale and Reiter, 1995). We ran the clustering algorithm on our English and Dutch training data for up to six clusters (k = 2 to k = 6). Then we evaluated the performance of the resulting cost functions on the test data from the same language, using Dice (overlap between attribute sets) and Accuracy (perfect match between sets) as evaluation metrics. For comparison, we also evaluated the best scoring cost functions from Theune et al. (2010) on our test data. These “Free-Naive” (FN) functions were created using the manual approach sketched in the introduction. The order in which the graph-based algorithm tries to add attributes to a description is explicitly controlled to ensure that “free” distinguishing properties are included (Viethen et al., 2008). In our tests, we used an order of decreasing frequency; i.e., always examining more frequent properties first.1 2.3 Results 3.2 Method For the cluster-based cost functions, the best performance was achieved with k = 2, for both domains and both languages. Interestingly, this is the</context>
<context position="11339" citStr="Theune et al. (2010)" startWordPosition="1879" endWordPosition="1882">han the FN functions advocated by Krahmer et al. (2008). The results for the k-means costs with k = 2 and the FN costs of Theune et al. (2010) are shown in Table 1. No significant differences were found, which suggests that k-means clustering, with k = 2, can be used as a more systematic alternative for the manual assignment of frequency-based costs. We therefore applied this method in the next experiment. 3 Experiment II: varying training set size To find out how much training data is required to achieve an acceptable attribute selection perfor1We used slightly different property orders than Theune et al. (2010), leading to minor differences in our FN results. We derived cost functions (using k-means clustering with k = 2) and orders from each of the training sets, following the method described in Section 2.2. In doing so, we had to deal with missing data: not all properties were present in all data sets.2 For the cost functions, we simply assigned the highest cost (1) to the missing properties. For the order, we listed properties with the same frequency (0 for missing properties) in alphabetical order. This was done for the sake of comparability between training sets. 3.3 Results To determine signi</context>
<context position="15880" citStr="Theune et al., 2010" startWordPosition="2681" endWordPosition="2684"> Accuracy but not Dice, possibly because the latter is a less strict metric. Whether the encouraging results found for the graph-based algorithm generalize to other REG approaches is still an open question. We also need to investigate how the use of small training sets affects effectiveness and efficiency of target identification by human subjects; as shown by Belz and Gatt (2008), task-performance measures do not necessarily correlate with similarity measures such as Dice. Finally, it will be interesting to repeat Experiment II with Dutch data. The D-TUNA data are cleaner than the TUNA data (Theune et al., 2010), so the risk of “bad” training data will be smaller, which may lead to more consistent results across training sets. 5 Conclusion Our experiment has shown that with 20 or less training instances, acceptable attribute selection results can be achieved; that is, results that do not significantly differ from those obtained using the entire training set. This is good news, because collecting such small amounts of training data should not take too much time and effort, making it relatively easy to do REG for new domains and languages. Acknowledgments Krahmer and Koolen received financial support f</context>
</contexts>
<marker>Theune, Koolen, Krahmer, 2010</marker>
<rawString>Mari¨et Theune, Ruud Koolen, and Emiel Krahmer. 2010. Cross-linguistic attribute selection for REG: Comparing Dutch and English. In Proceedings of the 6th International Natural Language Generation Conference (INLG 2010), pages 174–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
<author>Ielka van der Sluis</author>
<author>Albert Gatt</author>
</authors>
<title>Building a semantically transparent corpus for the generation of referring expressions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 4th International Natural Language Generation Conference (INLG</booktitle>
<pages>130--132</pages>
<marker>van Deemter, van der Sluis, Gatt, 2006</marker>
<rawString>Kees van Deemter, Ielka van der Sluis, and Albert Gatt. 2006. Building a semantically transparent corpus for the generation of referring expressions. In Proceedings of the 4th International Natural Language Generation Conference (INLG 2006), pages 130–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Robert Dale</author>
</authors>
<title>Speaker-dependent variation in content selection for referring expression generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 8th Australasian Language Technology Workshop,</booktitle>
<pages>81--89</pages>
<contexts>
<context position="3205" citStr="Viethen and Dale, 2010" startWordPosition="502" endWordPosition="506">and infrequent ones more expensive. Krahmer et al. (2008) argue that a less fine-grained cost function might generalize better, and propose to use frequency information to, somewhat ad hoc, define three costs: 0 (free), 1 (cheap) and 2 (expensive). This approach was shown to work well: the graph-based algorithm was the best performing system in the most recent REG Challenge (Gatt et al., 2009). Many other attribute selection algorithms also rely on training data to determine preferences in one form or another (Fabbrizio et al., 2008; Gerv´as et al., 2008; Kelleher, 2007; Spanger et al., 2008; Viethen and Dale, 2010). Unfortunately, suitable data is hard to come by. It has been argued that determining which properties to include in a referring expression requires a “semantically transparent” corpus (van Deemter et al., 2006): a corpus that contains the actual properties of all domain objects as well as the properties that were selected for inclusion in a given reference to the target. Obviously, text corpora tend not to meet this requirement, which is why 660 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 660–664, Portland, Oregon, June 19-24, 20</context>
</contexts>
<marker>Viethen, Dale, 2010</marker>
<rawString>Jette Viethen and Robert Dale. 2010. Speaker-dependent variation in content selection for referring expression generation. In Proceedings of the 8th Australasian Language Technology Workshop, pages 81–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Robert Dale</author>
<author>Emiel Krahmer</author>
<author>Mari¨et Theune</author>
<author>Pascal Touset</author>
</authors>
<title>Controlling redundancy in referring expressions.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>239--246</pages>
<contexts>
<context position="10338" citStr="Viethen et al., 2008" startWordPosition="1708" endWordPosition="1711">en we evaluated the performance of the resulting cost functions on the test data from the same language, using Dice (overlap between attribute sets) and Accuracy (perfect match between sets) as evaluation metrics. For comparison, we also evaluated the best scoring cost functions from Theune et al. (2010) on our test data. These “Free-Naive” (FN) functions were created using the manual approach sketched in the introduction. The order in which the graph-based algorithm tries to add attributes to a description is explicitly controlled to ensure that “free” distinguishing properties are included (Viethen et al., 2008). In our tests, we used an order of decreasing frequency; i.e., always examining more frequent properties first.1 2.3 Results 3.2 Method For the cluster-based cost functions, the best performance was achieved with k = 2, for both domains and both languages. Interestingly, this is the coarsest possible k-means function: with only two costs (0 and 1) it is even less fine-grained than the FN functions advocated by Krahmer et al. (2008). The results for the k-means costs with k = 2 and the FN costs of Theune et al. (2010) are shown in Table 1. No significant differences were found, which suggests </context>
</contexts>
<marker>Viethen, Dale, Krahmer, Theune, Touset, 2008</marker>
<rawString>Jette Viethen, Robert Dale, Emiel Krahmer, Mari¨et Theune, and Pascal Touset. 2008. Controlling redundancy in referring expressions. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC 2008), pages 239–246.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>