<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9980755">
Generating Aspect-oriented Multi-Document Summarization with
Event-aspect model
</title>
<author confidence="0.996978">
Peng Li1 and Yinglin Wang1 and Wei Gao2and Jing Jiang3
</author>
<affiliation confidence="0.962164333333333">
1 Department of Computer Science and Engineering, Shanghai Jiao Tong University
2 Department of Systems Engineering and Engineering Management, Chinese University of Hong Kong
3 School of Information Systems, Singapore Management University
</affiliation>
<email confidence="0.992426">
{lipeng, ylwang@sjtu.edu.cn} {wgao@se.cuhk.edu.hk} {jingjiang@smu.edu.sg}
</email>
<sectionHeader confidence="0.997312" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997580444444444">
In this paper, we propose a novel approach to
automatic generation of aspect-oriented sum-
maries from multiple documents. We first de-
velop an event-aspect LDA model to cluster
sentences into aspects. We then use extend-
ed LexRank algorithm to rank the sentences
in each cluster. We use Integer Linear Pro-
gramming for sentence selection. Key features
of our method include automatic grouping of
semantically related sentences and sentence
ranking based on extension of random walk
model. Also, we implement a new sentence
compression algorithm which use dependency
tree instead of parser tree. We compare our
method with four baseline methods. Quantita-
tive evaluation based on Rouge metric demon-
strates the effectiveness and advantages of our
method.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999721764705883">
In recent years, there has been much interest in
the task of multi-document summarization. In this
paper, we study the task of automatically generat-
ing aspect-oriented summaries from multiple docu-
ments. The goal of aspect-oriented summarization
is to present the most important content to the us-
er in a condensed form and a well-organized struc-
ture to satisfy the user’s needs. A summary should
follow a readable structure and cover all the aspect-
s users are interested in. For example, a summary
about natural disasters should include aspects about
what happened, when/where it happened, reasons,
damages, rescue efforts, etc. and these aspects may
be scattered in multiple articles written by different
news agencies. Our goal is to automatically collect
aspects and construct summaries from multiple doc-
uments.
Aspect-oriented summarization can be used in
many scenarios. First of all, it can be used to gener-
ate Wikipedia-like summary articles, especially used
to generate introduction sections that summarizes
the subject of articles before the table of contents
and other elaborate sections. Second, opinionat-
ed text often contains multiple viewpoints about an
issue generated by different people. Summarizing
these multiple opinions can help people easily di-
gest them. Furthermore, combined with search en-
gines and question&amp;answering systems, we can bet-
ter organize the summary content based on aspects
to improve user experience.
Despite its usefulness, the problem of modeling
domain specific aspects for multi-document summa-
rization has not been well studied. The most relevant
work is by (Haghighi and Vanderwende, 2009) on
exploring content models for multi-document sum-
marization. They proposed a HIERSUM model for
finding the subtopics or aspects which are combined
by using KL-divergence criterion for selecting rel-
evant sentences. They introduced a general con-
tent distribution and several specific content distri-
butions to discover the topic and aspects for a s-
ingle document collection. However, the aspects
may be shared not only across documents in a sin-
gle collection, but also across documents in different
topic-related collections. Their model is conceptual-
ly inadequate for simultaneously summarizing mul-
tiple topic-related document collections. Further-
more, their sentence selection method based on KL-
divergence cannot prevent redundancy across differ-
ent aspects.
In this paper, we study how to overcome these
</bodyText>
<page confidence="0.954355">
1137
</page>
<note confidence="0.9577305">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1137–1146,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.966837558823529">
limitations. We hypothesize that comparatively
summarizing topics across similar collections can
improve the effectiveness of aspect-oriented multi-
document summarization. We propose a novel
extraction-based approach which consists of four
main steps listed below:
Sentence Clustering: Our goal in this step is to
automatically identify the different aspects and clus-
ter sentences into aspects (See Section 2). We sub-
stantially extend the entity-aspect model in (Li et al.,
2010) for generating general sentence clusters.
Sentence Ranking: In this step, we use an exten-
sion of LexRank algorithm proposed by (Paul et al.,
2010) to score representative sentences in each clus-
ter (See Section 3).
Sentence Compression: In this step, we aim to
improve the linguistic quality of the summaries by
simplifying the sentence expressions. We prune sen-
tences using grammatical relations defined on de-
pendency trees for recognizing important clauses
and removing redundant subtrees (See Section 4).
Sentence Selection: Finally, we select one com-
pressed version of the sentences from each aspec-
t cluster. We use Integer Linear Programming
(ILP) algorithm, which optimizes a global objective
function, for sentence selection (McDonald, 2007;
Gillick and Favre, 2009; Sauper and Barzilay, 2009)
(See Section 5).
We evaluate our method using TAC2010 Guided
Summarization task data sets1 (Section 6). Our eval-
uation shows that our method obtains better ROUGE
recall score compared with four baseline methods,
and it also achieve reasonably high-quality aspec-
t clusters in terms of purity.
</bodyText>
<sectionHeader confidence="0.919697" genericHeader="introduction">
2 Sentence Clustering
</sectionHeader>
<bodyText confidence="0.999667375">
In this step, our goal is to discover event aspects con-
tained in a document set and cluster sentences in-
to aspects. Here we substantially extend the entity-
aspect model in Li et al. (2010) and refer to it as
event-aspect model. The main difference between
our event-aspect model and entity-aspect model is
that we introduce an additional layer of event topics
and the separation of general and specific aspects.
</bodyText>
<footnote confidence="0.6453345">
lhttp://www.nist.gov/tac/2010/
Summarization/
</footnote>
<bodyText confidence="0.984053225806452">
Our extension is based upon the following ob-
servations. For example, specific events like
“Columbine Massacre” and “Malaysia Resort Ab-
duction” can be related to the “Attack” topic. Each
event consists of multiple articles written by dif-
ferent news agencies. Interesting aspects may in-
clude “what happened, when, where, perpetrators,
reasons, who affected, damages and countermea-
sures,” etc2. We compared the “Columbine Mas-
sacre” and “Malaysia Resort Abduction” data set-
s and found 5 different kinds of words in the text:
(1) stop words that occur frequently in any docu-
ment collection; (2) general content words describ-
ing “damages” or “countermeasures” aspect of at-
tacks; (3) specific content words describing “what
happened”, “who affected” or “where” aspect of the
concrete event; (4) background words describing the
general topic of “Attack”; (5) words that are local to
a single document and do not appear across different
documents. Table 1 shows four sentences related to
two major aspects. We found that the entity-aspect
model does not have enough capacity to cluster sen-
tences into aspects (See Section 6). So we introduce
additional layer to improve the effectiveness of sen-
tence clustering. We also found that their one aspect
per sentence assumption is not very strong in this
scenario. Although a sentence may belong to a sin-
gle general aspect, it still contains multiple specific
aspect words like second sentence in Table 1. There-
fore, We assume that each sentence belongs to both
a general aspect and a specific aspect.
</bodyText>
<subsectionHeader confidence="0.637847">
2.1 Event-Aspect Model
</subsectionHeader>
<bodyText confidence="0.999449">
Stop words can be ignored by LDA model because
they can be easily identified using a standard stop
word list. Suppose that for a given event topic, there
are in total C specific events for which we need to
simultaneously generate summaries. We can assume
four kinds of unigram language models (i.e. multi-
nomial word distributions). For each event topic,
there is a background model ϕB that generates words
commonly used in all documents, and there are AG
general aspect models ϕ9&apos; (1 &lt; ga &lt; AG), where
AG is the number of general aspects. For each spe-
cific event in a topic, there are AS specific aspect
</bodyText>
<footnote confidence="0.850968333333333">
zhttp://www.nist.gov/tac/2010/
Summarization/Guided-Summ.2010.guidelines.
html
</footnote>
<page confidence="0.855334">
1138
</page>
<equation confidence="0.863842416666667">
countermeasures
Police/GA are/S close/B to/S identifying/GA someone/B responsible/GA
for/S the/S attack/B .
Investigators/GA do/S not/S know/B how/S many/S suspects/SA
they/S are/S looking/B for/S, but/S reported/B progress/B toward/S
identifying/GA one/S of/S the/S bombers/SA .
what happened, when, where
During/S the/S morning/SA rush/D hour/D on/S July/SA 7/SA terrorists/B
exploded/SA bombs/SA on/S three/D London/SA subway/D trains/SA and/S a/S
double-decker/D bus/SA .
Four/D coordinated/B bombings/SA struck/B central/B London/SA on/SA
July/SA 7/SA, three/D in/S subway/D cars/SA and/S one/D on/S a/S bus/SA .
</equation>
<tableCaption confidence="0.993085">
Table 1: Four sentences on “COUNTERMEASURES” and “What, When, Where” aspects from the “Attack” topic. S:
stop word. B: background word. GA: general aspect word. SA: specific aspect word. D: document word.
</tableCaption>
<bodyText confidence="0.99079459375">
models ϕsa (1 &lt; sa &lt; AS), where AS is the num-
ber of specific aspects, and also there are D doc-
ument models ϕd (1 &lt; d &lt; D), where D is the
number of documents in this collection. We assume
that these word distributions have a uniform Dirich-
let prior with parameter β.
We introduce a level distribution σ that control-
s whether we choose a word from ϕga or ϕsa. σ
is sampled from Beta(S0, S1) distribution. We also
introduce an aspect distribution θ that controls how
often a general or a specific aspect occurs in the col-
lection, where θ is sampled from another Dirichlet
prior with parameter α. There is also a multinomi-
al distribution 7r that controls in each sentence how
often we encounter a background word, a document
word, or an aspect word. 7r has a Dirichlet prior with
parameter γ.
Let Sd denote the number of sentences in docu-
ment d, Nd,s denote the number of words (after stop
word removal) in sentence s of document d, and
wd,s,n denote the n’th word in this sentence. We
introduce hidden variables zga
d,s and zsa
d,s to indicate
that a sentence s of document d belongs to which
general or specific aspects . We introduce hidden
variables yd,s,n for each word to indicate whether a
word is generated from the background model, the
document model, or the aspect model. We also intro-
duce hidden variables ld,s,n to indicate whether the
n’th word in sentence s of document d is generated
from the general aspect model. Figure 1 describes
the process of generating the whole document col-
lection. The plate notation of the model is shown in
Figure 2. Note that the values of S0, S1, α1, α2, β
and γ are fixed. The number of general and specific
aspects AG and AS are also empirically set.
Given a document collection, i.e. the set of all
wd,s,n, our goal is to find the most likely assignmen-
t of zga
d,s, zsa
d,s, yd,s,n and ld,s,n that maximizes dis-
tribution p(z, y,l|w; α, β, y, S), where z, y, l and w
represent the set of all z, y, l and w variables, respec-
tively. With the assignment, sentences are naturally
clustered into aspects, and words are labeled as ei-
ther a background word, a document word, a general
aspect word or a specific aspect word.
Inference can be done with Gibbs sampling,
which is commonly used in LDA models (Griffiths
and Steyvers, 2004).
In our experiments, we set α1 = 5, α2 = 3,
β = 0.01, γ = 20, S1 = 10 and S2 = 10. We
run 100 burn-in iterations through all documents in
a collection to stabilize the distribution of z and y
before collecting samples. We take 10 samples with
a gap of 10 iterations between two samples, and av-
erage over these 10 samples to get the estimation for
the parameters.
After estimating all the distributions, we can find
the values of each zga
d,s and zsa
d,s that gives us sen-
tences clustered into general and specific aspects.
</bodyText>
<sectionHeader confidence="0.962245" genericHeader="method">
3 Sentence Ranking
</sectionHeader>
<bodyText confidence="0.9996825">
In this step, we want to order the clustered sen-
tences so that the representative sentences can be
ranked higher in each aspect. Inspired by Paul et
al. (2010), we use an extended LexRank algorithm
to obtain top ranked sentences. LexRank (Erkan and
Radev, 2004) algorithm defines a random walk mod-
</bodyText>
<page confidence="0.98302">
1139
</page>
<figure confidence="0.935521714285714">
1. Draw θ1 ∼ Dir(α1), θ2 ∼ Dir(α2), π ∼ Dir(γ)
Draw σ ∼ Beta(δ0, δ1)
2. For each event topic, there is a background model
Og, and there are general aspect ga, where 1 ≤
ga ≤ AG
(a) draw Og ∼ Dir(β)
(b) draw Oga ∼Dir(β)
3. For each document collection, there are specific
aspect sa, where 1 ≤ sa ≤ AS
(a) draw Osa ∼ Dir(β)
4. For each document d = 1, ... , D,
(a) draw Od ∼ Dir(β)
(b) for each sentence s = 1, ... , Sd
i. draw zga ∼ Multi(θ1)
ii. draw zsa ∼ Multi(θ2)
iii. for each word n = 1, ... , Nd,s
A. draw ld,s,n ∼ Binomial(σ)
B. draw yd,s,n ∼ Multi(π)
C. draw wd,s,n ∼ Multi(Og) if yd,s,n =
1, wd,s,n ∼ Multi(Od) if yd,s,n = 2,
wd,s,n ∼ Multi(Ozsa
</figure>
<equation confidence="0.8465352">
d,s) if yd,s,n =
3 and ld,s,n = 1 or wd,s,n ∼
ga
Multi(Ozd,s )
ld,s,n = 0
</equation>
<figureCaption confidence="0.846452">
Figure 2: The event-aspect model.
</figureCaption>
<equation confidence="0.997153">
K
zEadj[v] sim(z, v)
</equation>
<bodyText confidence="0.999763">
The major extension is to modify this jumping
probability so as to favor visiting representative sen-
tences. More specifically, we scale sim(u, v) by the
likelihood that the two sentences represent the same
general aspect ga or specific aspect sa:
</bodyText>
<equation confidence="0.62344275">
AG
sim′(u,v) = sim(u,v)[ E P(ga|u)P(ga|v)
ga=1
T
</equation>
<figure confidence="0.991149038461539">
�
0
B
A G
C
�
A
S
D
�
Q
d
�d s
w
a, a2
zga zsa
0, 0,
l
y
Ir 6
Y
Sd
S
if yd,s,n = 3 and
sim(u, v)
p(u|v) =
</figure>
<figureCaption confidence="0.672858666666667">
Figure 1: The document generation process. AS P(sa|u)P(sa|v)]
+ E
sa=1
</figureCaption>
<bodyText confidence="0.999335333333333">
el on top of a graph that represents sentences to be
summarized as nodes and their similarities as edges.
The LexRank score of a sentence gives the expected
probability that a random walk will visit that sen-
tence in the long run. A variant is called continu-
ous LexRank improved LexRank by making use of
the strength of the similarity links. The continuous
LexRank score can be computed using the following
formula:
</bodyText>
<equation confidence="0.883442">
d E
L(u) = N + (1 − d)
vEadj[u]
</equation>
<bodyText confidence="0.999968125">
where L(u) is the LexRank value of sentence u, N is
the total number of nodes in the graph, d is a damp-
ing factor for the convergence of the method, and
p(u|v) is the jumping probability between sentence
u and its neighboring sentence v. p(u|v) is defined
using content similarity function sim(u, v) between
two sentences:
where the value P(ga|u) and P(sa|u) can be
computed by our event-aspect model. We define
sim(u, v) as the tf * idf weighted cosine similar-
ity between two sentences.
We found that sentence ranking is better con-
ducted before the compression because the pre-
compressed sentences are more informative and the
similarity function in LexRank can be better off with
the complete information.
</bodyText>
<sectionHeader confidence="0.995085" genericHeader="method">
4 Sentence Compression
</sectionHeader>
<bodyText confidence="0.999984222222222">
It has been shown that sentence compression can
improve linguistic quality of summaries (Zajic et
al., 2007; Gillick et al., 2010). Commonly used
“Syntactic parse and trim” approach may produce
poor compression results. For example, given the
sentence “We have friends whose children go to
Columbine, the freshman said”, the procedure tries
to remove the clause “the freshman said” from the
parse tree by using the “SBAR” label to locate the
</bodyText>
<equation confidence="0.817515">
p(u|v)L(v)
</equation>
<page confidence="0.852378">
1140
</page>
<bodyText confidence="0.9993375">
clause, and will result in “whose children go to
Columbine”, which is not adequate. Furthermore,
some important temporal modifier, numeric modifier
and clausal complement need to be retained because
they reflect content aspects of the summary. There-
fore, we propose the “dependency parse and trim”
approach, which prunes sentences based on depen-
dency tree representations, using English grammati-
cal relations to recognize clauses and remove redun-
dant structures. Table 2 shows two examples by re-
moving redundant auxiliary clauses. Below is the
sentence compression procedure:
</bodyText>
<listItem confidence="0.9976319375">
1. Select possible subtree root nodes using gram-
matical relations, such as clausal complement,
complementizer, or parataxis 3.
2. Decide which subtree root node can be the root
of clause. If this root contains maximum num-
ber of child nodes and the collection of all child
edges include object or auxiliary relations, it is
selected as the root node.
3. Remove redundant modifiers such as adverbial-
s, relative clause modifiers and abbreviations,
participials and infinitive modifiers.
4. Traverse the subtrees and generate all possible
compression alternatives using the subtree root
node, then keep the top two longest sub sen-
tences.
5. Drop the sub sentences shorter than 5 words.
</listItem>
<sectionHeader confidence="0.765988" genericHeader="method">
5 Sentence Selection
</sectionHeader>
<bodyText confidence="0.999864777777778">
After sentence pruning, we prepare for the final
event summary generation process. In this step, we
select one compressed version of the sentence from
each aspect cluster. To avoid redundancy between
aspects, we use Integer Linear Programming to opti-
mize a global objective function for sentence selec-
tion. Inspired by (Sauper and Barzilay, 2009), we
formulate the optimization problem based on sen-
tence ranking information. More specifically, we
</bodyText>
<footnote confidence="0.703547666666667">
3The parataxis relation is a relation between the main verb
of a clause and other sentential elements, such as a sentential
parenthetical, colon, or semicolon
</footnote>
<table confidence="0.999418166666667">
Original Compressed
When rescue workers When rescue workers
arrived, they said, on- arrived, only one of his
ly one of his limbs was limbs was visible.
visible.
Two days earlier, a Two days earlier, a
massacre by two s- massacre by two stu-
tudents at Columbine dents at Columbine
High, whose teams are High, left 15 peo-
called the Rebels, left ple dead and dozens
15 people dead and wounded.
dozens wounded.
</table>
<tableCaption confidence="0.999582">
Table 2: Example compressed sentences.
</tableCaption>
<bodyText confidence="0.999504571428571">
would like to select exactly one compressed sen-
tence which receives the highest possible ranking s-
core from each aspect cluster subject to a series of
constraints, such as redundancy and length. We em-
ployed lp solver 4, an efficient mixed integer pro-
gramming solver using the Branch-and-Bound algo-
rithm to select sentences.
Assume that there are in total K aspects in an
event topic. For each aspect j, there are in total R
ranked sentences. The variables Sjl is a binary indi-
cator of the sentence. That is, Sjl= 1 if the sentence
is included in the final summary, and Sjl = 0 other-
wise. l is the ranked position of the sentence in this
aspect cluster.
</bodyText>
<subsectionHeader confidence="0.958501">
Objective Function
</subsectionHeader>
<bodyText confidence="0.9995504">
Top ranked sentences are the most relevant corre-
sponding to the related aspects which we want to in-
clude in the final summary. Thus we try to minimize
the ranks of the sentences to improve the overall re-
sponsiveness.
</bodyText>
<equation confidence="0.997856">
K Rj
min( l · Sjl)
j=1 l=1
</equation>
<subsectionHeader confidence="0.939615">
Exclusivity Constraints
</subsectionHeader>
<bodyText confidence="0.99483775">
To prevent redundancy in each aspect, we just
choose one sentence from each general or specific
aspect cluster. The constraint is formulated as fol-
lows:
</bodyText>
<equation confidence="0.971610333333333">
Rj
S;l = 1 dj E {1... K}
l=1
</equation>
<footnote confidence="0.60442">
4http://lpsolve.sourceforge.net/5.5/
</footnote>
<page confidence="0.981735">
1141
</page>
<subsectionHeader confidence="0.81311">
Redundancy Constraints
</subsectionHeader>
<bodyText confidence="0.999896333333333">
We also want to prevent redundancy across differ-
ent aspects. If sentence-similarity sim(sjl, sj′l′) be-
tween sentence sjl and sj′l′ is above 0.5, then we
drop the pair and choose one sentence ranked higher
from the pair otherwise. This constraint is formulat-
ed as follows:
</bodyText>
<equation confidence="0.918967">
(Sjl + Sj′l′) · sim(sjl, sj′l′) &lt; 0.5
dj, j′ E {1... K}dl E {1... Rj}dl′ E {1... Rj′}
</equation>
<subsectionHeader confidence="0.957094">
Length Constraints
</subsectionHeader>
<bodyText confidence="0.9993225">
We add this constraint to ensure that the length of
the final summary is limited to L words.
</bodyText>
<equation confidence="0.570476">
lenjl · Sjl &lt; L
</equation>
<bodyText confidence="0.993149">
where lenjl is the length of Sjl.
</bodyText>
<sectionHeader confidence="0.999252" genericHeader="method">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.9999015">
In order to systematically evaluate our method, we
want to check (1) whether the whole system is effec-
tive, which means to quantitatively evaluate summa-
ry quality, and (2) whether individual components
like clustering and compression algorithms are use-
ful.
</bodyText>
<subsectionHeader confidence="0.983679">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.999986294117647">
We use TAC2010 Summarization task data set for
the summary content evaluation. This data set pro-
vides 46 events. Each event falls into a predefined
event topic. Each specific event includes an even-
t statement and 20 relevant newswire articles which
have been divided into 2 sets: Document Set A and
Document Set B. Each document set has 10 docu-
ments, and all the documents in Set A chronologi-
cally precede the documents in Set B. We just use
document Set A for our task. Assessors wrote mod-
el summaries for each event, so we can compare
our automatic generated summaries with the model
summaries. We combine topic related data sets to-
gether, then these data sets simultaneously annotated
by our Event-aspect model. After labeling process,
we run sentence ranking, compression and selection
module to get final aspect-oriented summarizations.
</bodyText>
<subsectionHeader confidence="0.9997">
6.2 Quality of summary
</subsectionHeader>
<bodyText confidence="0.985032333333333">
We use the ROUGE (Lin and Hovy, 2003) metric for
measuring the summarization system performance.
Ideally, a summarization criterion should be more
recall oriented. So the average recall of ROUGE-
1, ROUGE-2, ROUGE-SU4, ROUGE-W-1.2 and
ROUGE-L were computed by running ROUGE-
1.5.5 with stemming but no removal of stop word-
s. We compare our method with the following four
baseline methods.
Baseline 1
In this baseline, we try to compare different sen-
tence clustering algorithms in the multi-document
summarization scenario. First, we use CLUTO 5 to
do K-means clustering. Then we try entity-aspect
model proposed by Li et al. (2010) to do sentence
clustering. Entity-aspect model is similar with “HI-
ERSUM” content model proposed by Haghighi and
Vanderwende (2009). We use the same ranking,
compression, and selection components to generate
aspect-oriented summaries for comparison.
Baseline 2
In this baseline, we compare our method with
traditional ranking and selection summary genera-
tion framework (Erkan and Radev, 2004; Nenkova
and Vanderwende, 2005) to show that our sentence
clustering component is necessary in aspect-oriented
summarization system. Also we want check whether
sentence ranking combined with greedy based sen-
tence selection can prevent redundancy effective-
ly. We follow LexRank based sentence ranking
combined with greedy sentence selection methods.
We implement two greedy algorithms (Zhang et al.,
2008; Paul et al., 2010). One is to select the top
ranked sentence simultaneously by removing 10 re-
dundant neighbor sentences from the sentence sim-
ilarity graph if the summary length is less then 100
words. This is repeated until the graph cannot be
partitioned. The similarity graph building threshold
is 0.3, damping factor is 0.2 and error tolerance for
Power Method in LexRank is 0.1. The other is to se-
lect top ranked sentences as long as the redundancy
score (similarity) between a candidate sentence and
</bodyText>
<footnote confidence="0.925661">
5http://glaros.dtc.umn.edu/gkhome/cluto/
cluto/overview
</footnote>
<equation confidence="0.9463194">
Rj
l=1
K
E
j=1
</equation>
<page confidence="0.942526">
1142
</page>
<bodyText confidence="0.994796">
current summary is under 0.5. This is repeated until
the summary reaches a 100 word length limit.
</bodyText>
<subsectionHeader confidence="0.554357">
Baseline 3
</subsectionHeader>
<bodyText confidence="0.99788875">
In this baseline, we compare our ILP based sen-
tence selection with KL-divergence based sentence
selection. The KL-divergence formula we use is be-
low,
</bodyText>
<equation confidence="0.999084">
P (w)
P(w) log Q(w)
</equation>
<bodyText confidence="0.999929133333334">
where P(S) is the empirical unigram distribution of
the candidate summary S, and Q(D) is the unigram
distribution of document collection D. We only re-
placed our selection method with the KL-divergence
selection method. Other parts are the same. After
ranking sentences for each aspect, we add the sen-
tence with the highest ranking score from each as-
pect sentence cluster as long as the KL-divergence
between candidate and current summary does not
decrease. This is repeated until the summary reach-
es a 100 word length limit. To our knowledge, this
is the first work to directly compare Integer Lin-
ear Programming based sentence selection with KL-
divergence based sentence selection in summariza-
tion generation framework.
</bodyText>
<subsectionHeader confidence="0.634467">
Baseline 4
</subsectionHeader>
<bodyText confidence="0.99996754">
In this baseline, we directly compare our method
with “HIERSUM” proposed by (Haghighi and Van-
derwende, 2009). As in Baseline 1, we use entity-
aspect model to approximate “HIERSUM” mod-
el. We replace unigram distribution of P(w) in
KL-divergence with learned distribution estimated
by “HIERSUM” model. The KL-divergence based
greedy sentence selection algorithm is similar to
Baseline 3.
For fair comparison, Baselines 1, 2, 3 and 4 use
the same sentence compression algorithm and have
the summary length no more then 100 words. In
Table 3, we show the average ROUGE recall of 46
summaries generated by our method and four base-
line methods. We can see that our method gives
better Rouge recall measures then the four baseline
methods. For BL-1, we can see that LDA-based sen-
tence clustering is better then k-means. For BL-2,
we can see that traditional ranking plus greedy selec-
tion summary generation framework is not suitable
for the aspect-oriented summarization task. More
specifically, greedy-based sentence selection can not
prevent redundancy effectively. BL-3 evaluation re-
sults showed that ILP-based sentence selection is
better then KL-divergence selection in terms of pre-
venting redundancy across different aspects. The
measurement performance between BL-3 and BL-
4 is close. They use the same KL-divergence based
sentence selection, but topic model they use are d-
ifferent, and also BL-3 has a sentence ranking pro-
cess. The Rouge recall of our method is better than
BL-4. It is expected because our event-aspect mod-
el can better find the aspects and also prove that
our LexRank based sentence ranking combined with
ILP-based sentence selection can prevent redundan-
cy.
Due to TAC2010 summarization community just
compute ROUGE-2 and ROUGE-SU4 metrics for
participants, our ROUGE-2 metric ranked 11 out
of 23, ROUGE-SU4 metric ranked 12 out of 23.
They use MEAD6 as their baseline approach. The
ROUGE-2 score of our approach achieve 0.06508
higher than MEAD’s 0.05929. The ROUGE-SU4 s-
core of our approach achieve 0.10146 higher than
MEAD’s 0.09112. Many systems that get high-
er performances leverage domain knowledge bases
like Wikipedia or training data, but we didn’t. The
advantage of our method is that we generate sum-
maries with totally unsupervised framework and this
approach is domain adaptive.
</bodyText>
<subsectionHeader confidence="0.999951">
6.3 Quality of aspect-oriented sentence clusters
</subsectionHeader>
<bodyText confidence="0.999638909090909">
To judge the quality of the aspect-oriented sentence
clusters, we ask the human judges to group the
ground truth sentences based on the aspect related-
ness in each event topic. We then compute the pu-
rity of the automatically generated clusters against
the human judged clusters. The results are shown
in Table 4. In our experiments, we set the number
of general aspect clusters AG is 5 and specific as-
pect clusters AS is 3. We can see from Table 4 that
our generated aspect clusters can achieve reasonably
good performance.
</bodyText>
<footnote confidence="0.622432">
6http://www.summarization.com/mead/
</footnote>
<figure confidence="0.41622">
�
KL(PS||QD) �
W
</figure>
<page confidence="0.893513">
1143
</page>
<table confidence="0.9998694">
Method Rouge Average Recall
ROUGE-1 ROUGE-2 ROUGE-SU4 ROUGE-W-1.2 ROUGE-L
BL-1 k-means 0.21895 0.03689 0.06644 0.06683 0.19208
entity-aspect 0.26082 0.05082 0.08286 0.08055 0.22976
BL-2 greedy 1 0.27802 0.04872 0.08302 0.08488 0.24426
greedy 2 0.27898 0.04723 0.08275 0.08500 0.24430
BL-3 KL-Div 0.29286 0.05369 0.09117 0.08827 0.25100
BL-4 HIERSUM 0.28736 0.05502 0.08932 0.08923 0.25285
Without compression 0.30563 0.05983 0.09513 0.09468 0.25487
Our Method 0.32641 0.06508 0.10146 0.09998 0.28610
</table>
<tableCaption confidence="0.987912">
Table 3: ROUGE evaluation results on TAC2010 Summarization data sets
</tableCaption>
<table confidence="0.997422333333333">
Category A Purity
Accidents and Natural Disasters 7 0.613
Attacks 8 0.658
Health and Safety 5 0.724
Endangered Resources 4 0.716
Investigations and Trials 6 0.669
</table>
<tableCaption confidence="0.982033">
Table 4: The true numbers of aspects as judged by the
human annotator (A), and the purity of the clusters.
</tableCaption>
<table confidence="0.999271833333333">
Category Average Score
Accidents and Natural Disasters 2.4
Attacks 2.3
Health and Safety 2.6
Endangered Resources 2.5
Investigations and Trials 2.4
</table>
<tableCaption confidence="0.999707">
Table 5: The average score of each event topic.
</tableCaption>
<subsectionHeader confidence="0.998904">
6.4 Quality of sentence compression
</subsectionHeader>
<bodyText confidence="0.999919076923077">
To judge the quality of the dependency tree based
sentence compression algorithm, we ask the human
judges to choose 20 sentences from each event top-
ic then score them. The judges follow 3-point scale
to score each compressed sentence: 1 means poor,
2 means barely acceptable, and 3 means good. We
then compute the average scores. The results are
shown in Table 5. To evaluate the effectiveness of
sentence compression component, we conduct the
system without sentence compression component,
then compare it with our system. In Table 3, we
can see that sentence compression can improve the
system performance.
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.998739727272727">
Our event-aspect model is related to a number of
previous extensions of LDA models. Chemudugun-
ta et al. (2007) proposed to introduce a background
topic and document-specific topics. Our background
and document language models are similar to theirs.
However, they still treat documents as bags of words
rather then sets of sentences as in our models. Titov
and McDonald (2008) exploited the idea that a short
paragraph within a document is likely to be about
the same aspect. The way we separate words in-
to stop words, background words, document word-
s and aspect words bears similarity to that used
in (Daum´e III and Marcu, 2006; Haghighi and Van-
derwende, 2009). Paul and Girju (2010) proposed a
topic-aspect model for simultaneously finding topic-
s and aspects. The most related extension is entity-
aspect model proposed by Li et al. (2010). The main
difference between event-aspect model and entity-
aspect model is our model further consider aspect
granularity and add a layer to model topic-related
events.
Filippova and Strube (2008) proposed a depen-
dency tree based sentence compression algorithm.
Their approach need a large corpus to build language
model for compression, whereas we prune depen-
dency tree using grammatical rules.
Paul et al. (2010) proposed to modify LexRank
algorithm using their topic-aspect model. But their
task is to summarize contrastive viewpoints in opin-
ionated text. Furthermore, they use a simple greedy
approach for constructing summary.
McDonald (2007) proposed to use Integer Linear
Programming framework in multi-document sum-
</bodyText>
<page confidence="0.984324">
1144
</page>
<bodyText confidence="0.999302888888889">
marization. And Sauper and Barzilay (2009) use in-
teger linear programming framework to automatical-
ly generate Wikipedia articles. There is a fundamen-
tal difference between their method and ours. They
used trained perceptron algorithm for ranking ex-
cerpts, whereas we give an extended LexRank with
integer linear programming to optimize sentence se-
lection for our aspect-oriented multi-document sum-
marization.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999979970588235">
In this paper, we study the task of automatically
generating aspect-oriented summary from multiple
documents. We proposed an event-aspect model
that can automatically cluster sentences into aspect-
s. We then use an extension of the LexRank algo-
rithm to rank sentences. We took advantage of the
output generated by the event-aspect model to mod-
ify jumping probabilities so as to favor visiting rep-
resentative sentence. We also proposed dependen-
cy tree compression algorithm to prune sentence for
improving linguistic quality of the summaries. Fi-
nally we use Integer Linear Programming Frame-
work to select aspect relevant sentences. We con-
ducted quantitative evaluation using standard test
data sets. We found that our method gave overal-
l better ROUGE scores than four baseline methods,
and the new sentence clustering and compression al-
gorithm are robust.
There are a number of directions we plan to pur-
sue in the future in order to improve our method.
First, we can possibly apply more linguistic knowl-
edge to improve the quality of sentence compres-
sion. Currently the sentence compression algorith-
m may generate meaningless subtrees. It is rela-
tively hard to decide which clause is redundant in
terms of summarization. Second, we may explore
more domain knowledge to improve the quality of
aspect-oriented summaries. For example, we know
that the “who-affected” aspect is related to person,
and “when, where” are related to Time and Location.
we can import Name Entity Recognition to anno-
tate these phrases and then help locate relevant sen-
tences. Third, we want to extend our event-aspect
model to simultaneously find topics and aspects.
</bodyText>
<sectionHeader confidence="0.999386" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.6318925">
This work was supported by the National Nat-
ural Science Foundation of China (NSFC No.
60773088), the National High-tech R&amp;D Program
of China (863 Program No. 2009AA04Z106), and
the Key Program of Basic Research of Shanghai
Municipal S&amp;T Commission (No. 08JC1411700).
</reference>
<sectionHeader confidence="0.975773" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999756625">
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling general and specific aspects
of documents with a probabilistic topic model. In Ad-
vances in Neural Information Processing Systems 19,
pages 241–248.
Hal. Daum´e III and Daniel. Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 305–312.
Association for Computational Linguistics.
G¨unes. Erkan and Dragomir Radev. 2004. LexRank:
Graph-based lexical centrality as salience in text sum-
marization. Journal of Artificial Intelligence Re-
search, 22(1):457–479.
K. Filippova and M. Strube. 2008. Dependency
tree based sentence compression. In Proceedings of
the Fifth International Natural Language Generation
Conference, pages 25–32. Association for Computa-
tional Linguistics.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
gauge Processing, pages 10–18.
Dan Gillick, Benoit Favre, D. Hakkani-Tur, B. Bohnet,
Y. Liu, and S. Xie. 2010. The icsi/utd summarization
system at tac 2009. In Proceedings of the Second Text
Analysis Conference, Gaithersburg, Maryland, USA:
National Institute of Standards and Technology.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National A-
cademy of Sciences of the United States of America,
101(Suppl. 1):5228–5235.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics on
7.7.7., pages 362–370. Association for Computational
Linguistics.
</reference>
<page confidence="0.849117">
1145
</page>
<reference confidence="0.997905816326531">
Peng Li, Jing Jiang, and Yinglin Wang. 2010. Gen-
erating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
the Joint Conference of the 48th Annual Meeting of the
ACL. Association for Computational Linguistics.
C.Y. Lin and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 71–78. Association for Computation-
al Linguistics.
Ryan McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. Advances in
Information Retrieval, pages 557–564.
A. Nenkova and L. Vanderwende. 2005. The impact
of frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-101.
Michael J. Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering multi-
faceted topics. In In AAAI-2010: Twenty-Fourth Con-
ference on Artificial Intelligence.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ’10, pages 66–76, Morristown, NJ, USA.
Association for Computational Linguistics.
Christina Sauper and Regina Barzilay. 2009. Automati-
cally generating wikipedia articles: A structure-aware
approach. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 208–216, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceeding
of the 17th International Conference on World Wide
Web, pages 111–120.
D. Zajic, B.J. Dorr, J. Lin, and R. Schwartz. 2007. Multi-
candidate reduction: Sentence compression as a tool
for document summarization tasks. Information Pro-
cessing &amp; Management, 43(6):1549–1570.
Jin. Zhang, Xueqi. Cheng, and Hongbo. Xu. 2008. GSP-
Summary: a graph-based sub-topic partition algorith-
m for summarization. In Proceedings of the 4th Asi-
a information retrieval conference on Information re-
trieval technology, pages 321–334. Springer-Verlag.
</reference>
<page confidence="0.993911">
1146
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.354621">
<title confidence="0.999872">Generating Aspect-oriented Multi-Document Summarization</title>
<abstract confidence="0.933192958333333">Event-aspect model and and of Computer Science and Engineering, Shanghai Jiao Tong of Systems Engineering and Engineering Management, Chinese University of Hong of Information Systems, Singapore Management Abstract In this paper, we propose a novel approach to automatic generation of aspect-oriented summaries from multiple documents. We first develop an event-aspect LDA model to cluster sentences into aspects. We then use extended LexRank algorithm to rank the sentences in each cluster. We use Integer Linear Programming for sentence selection. Key features of our method include automatic grouping of semantically related sentences and sentence ranking based on extension of random walk model. Also, we implement a new sentence compression algorithm which use dependency tree instead of parser tree. We compare our method with four baseline methods. Quantitative evaluation based on Rouge metric demonstrates the effectiveness and advantages of our method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This work was supported by the National Natural Science Foundation of China</title>
<booktitle>(NSFC No. 60773088), the National High-tech R&amp;D Program of China (863 Program No. 2009AA04Z106), and the Key Program of Basic Research of Shanghai Municipal S&amp;T Commission (No. 08JC1411700).</booktitle>
<marker></marker>
<rawString>This work was supported by the National Natural Science Foundation of China (NSFC No. 60773088), the National High-tech R&amp;D Program of China (863 Program No. 2009AA04Z106), and the Key Program of Basic Research of Shanghai Municipal S&amp;T Commission (No. 08JC1411700).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chaitanya Chemudugunta</author>
<author>Padhraic Smyth</author>
<author>Mark Steyvers</author>
</authors>
<title>Modeling general and specific aspects of documents with a probabilistic topic model.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems 19,</booktitle>
<pages>241--248</pages>
<contexts>
<context position="27929" citStr="Chemudugunta et al. (2007)" startWordPosition="4537" endWordPosition="4541"> choose 20 sentences from each event topic then score them. The judges follow 3-point scale to score each compressed sentence: 1 means poor, 2 means barely acceptable, and 3 means good. We then compute the average scores. The results are shown in Table 5. To evaluate the effectiveness of sentence compression component, we conduct the system without sentence compression component, then compare it with our system. In Table 3, we can see that sentence compression can improve the system performance. 7 Related Work Our event-aspect model is related to a number of previous extensions of LDA models. Chemudugunta et al. (2007) proposed to introduce a background topic and document-specific topics. Our background and document language models are similar to theirs. However, they still treat documents as bags of words rather then sets of sentences as in our models. Titov and McDonald (2008) exploited the idea that a short paragraph within a document is likely to be about the same aspect. The way we separate words into stop words, background words, document words and aspect words bears similarity to that used in (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009). Paul and Girju (2010) proposed a topic-aspect m</context>
</contexts>
<marker>Chemudugunta, Smyth, Steyvers, 2007</marker>
<rawString>Chaitanya Chemudugunta, Padhraic Smyth, and Mark Steyvers. 2007. Modeling general and specific aspects of documents with a probabilistic topic model. In Advances in Neural Information Processing Systems 19, pages 241–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Bayesian query-focused summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>305--312</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal. Daum´e III and Daniel. Marcu. 2006. Bayesian query-focused summarization. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 305–312. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erkan</author>
<author>Dragomir Radev</author>
</authors>
<title>LexRank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="12057" citStr="Erkan and Radev, 2004" startWordPosition="1910" endWordPosition="1913">of z and y before collecting samples. We take 10 samples with a gap of 10 iterations between two samples, and average over these 10 samples to get the estimation for the parameters. After estimating all the distributions, we can find the values of each zga d,s and zsa d,s that gives us sentences clustered into general and specific aspects. 3 Sentence Ranking In this step, we want to order the clustered sentences so that the representative sentences can be ranked higher in each aspect. Inspired by Paul et al. (2010), we use an extended LexRank algorithm to obtain top ranked sentences. LexRank (Erkan and Radev, 2004) algorithm defines a random walk mod1139 1. Draw θ1 ∼ Dir(α1), θ2 ∼ Dir(α2), π ∼ Dir(γ) Draw σ ∼ Beta(δ0, δ1) 2. For each event topic, there is a background model Og, and there are general aspect ga, where 1 ≤ ga ≤ AG (a) draw Og ∼ Dir(β) (b) draw Oga ∼Dir(β) 3. For each document collection, there are specific aspect sa, where 1 ≤ sa ≤ AS (a) draw Osa ∼ Dir(β) 4. For each document d = 1, ... , D, (a) draw Od ∼ Dir(β) (b) for each sentence s = 1, ... , Sd i. draw zga ∼ Multi(θ1) ii. draw zsa ∼ Multi(θ2) iii. for each word n = 1, ... , Nd,s A. draw ld,s,n ∼ Binomial(σ) B. draw yd,s,n ∼ Multi(π) </context>
<context position="21189" citStr="Erkan and Radev, 2004" startWordPosition="3475" endWordPosition="3478">n this baseline, we try to compare different sentence clustering algorithms in the multi-document summarization scenario. First, we use CLUTO 5 to do K-means clustering. Then we try entity-aspect model proposed by Li et al. (2010) to do sentence clustering. Entity-aspect model is similar with “HIERSUM” content model proposed by Haghighi and Vanderwende (2009). We use the same ranking, compression, and selection components to generate aspect-oriented summaries for comparison. Baseline 2 In this baseline, we compare our method with traditional ranking and selection summary generation framework (Erkan and Radev, 2004; Nenkova and Vanderwende, 2005) to show that our sentence clustering component is necessary in aspect-oriented summarization system. Also we want check whether sentence ranking combined with greedy based sentence selection can prevent redundancy effectively. We follow LexRank based sentence ranking combined with greedy sentence selection methods. We implement two greedy algorithms (Zhang et al., 2008; Paul et al., 2010). One is to select the top ranked sentence simultaneously by removing 10 redundant neighbor sentences from the sentence similarity graph if the summary length is less then 100 </context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes. Erkan and Dragomir Radev. 2004. LexRank: Graph-based lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research, 22(1):457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Filippova</author>
<author>M Strube</author>
</authors>
<title>Dependency tree based sentence compression.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28853" citStr="Filippova and Strube (2008)" startWordPosition="4688" endWordPosition="4691"> paragraph within a document is likely to be about the same aspect. The way we separate words into stop words, background words, document words and aspect words bears similarity to that used in (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009). Paul and Girju (2010) proposed a topic-aspect model for simultaneously finding topics and aspects. The most related extension is entityaspect model proposed by Li et al. (2010). The main difference between event-aspect model and entityaspect model is our model further consider aspect granularity and add a layer to model topic-related events. Filippova and Strube (2008) proposed a dependency tree based sentence compression algorithm. Their approach need a large corpus to build language model for compression, whereas we prune dependency tree using grammatical rules. Paul et al. (2010) proposed to modify LexRank algorithm using their topic-aspect model. But their task is to summarize contrastive viewpoints in opinionated text. Furthermore, they use a simple greedy approach for constructing summary. McDonald (2007) proposed to use Integer Linear Programming framework in multi-document sum1144 marization. And Sauper and Barzilay (2009) use integer linear program</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>K. Filippova and M. Strube. 2008. Dependency tree based sentence compression. In Proceedings of the Fifth International Natural Language Generation Conference, pages 25–32. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
</authors>
<title>A scalable global model for summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="5110" citStr="Gillick and Favre, 2009" startWordPosition="746" endWordPosition="749">o score representative sentences in each cluster (See Section 3). Sentence Compression: In this step, we aim to improve the linguistic quality of the summaries by simplifying the sentence expressions. We prune sentences using grammatical relations defined on dependency trees for recognizing important clauses and removing redundant subtrees (See Section 4). Sentence Selection: Finally, we select one compressed version of the sentences from each aspect cluster. We use Integer Linear Programming (ILP) algorithm, which optimizes a global objective function, for sentence selection (McDonald, 2007; Gillick and Favre, 2009; Sauper and Barzilay, 2009) (See Section 5). We evaluate our method using TAC2010 Guided Summarization task data sets1 (Section 6). Our evaluation shows that our method obtains better ROUGE recall score compared with four baseline methods, and it also achieve reasonably high-quality aspect clusters in terms of purity. 2 Sentence Clustering In this step, our goal is to discover event aspects contained in a document set and cluster sentences into aspects. Here we substantially extend the entityaspect model in Li et al. (2010) and refer to it as event-aspect model. The main difference between ou</context>
</contexts>
<marker>Gillick, Favre, 2009</marker>
<rawString>Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
<author>D Hakkani-Tur</author>
<author>B Bohnet</author>
<author>Y Liu</author>
<author>S Xie</author>
</authors>
<title>The icsi/utd summarization system at tac</title>
<date>2010</date>
<booktitle>In Proceedings of the Second Text Analysis Conference,</booktitle>
<institution>National Institute of Standards and Technology.</institution>
<location>Gaithersburg, Maryland, USA:</location>
<contexts>
<context position="14662" citStr="Gillick et al., 2010" startWordPosition="2410" endWordPosition="2413">(u|v) is defined using content similarity function sim(u, v) between two sentences: where the value P(ga|u) and P(sa|u) can be computed by our event-aspect model. We define sim(u, v) as the tf * idf weighted cosine similarity between two sentences. We found that sentence ranking is better conducted before the compression because the precompressed sentences are more informative and the similarity function in LexRank can be better off with the complete information. 4 Sentence Compression It has been shown that sentence compression can improve linguistic quality of summaries (Zajic et al., 2007; Gillick et al., 2010). Commonly used “Syntactic parse and trim” approach may produce poor compression results. For example, given the sentence “We have friends whose children go to Columbine, the freshman said”, the procedure tries to remove the clause “the freshman said” from the parse tree by using the “SBAR” label to locate the p(u|v)L(v) 1140 clause, and will result in “whose children go to Columbine”, which is not adequate. Furthermore, some important temporal modifier, numeric modifier and clausal complement need to be retained because they reflect content aspects of the summary. Therefore, we propose the “d</context>
</contexts>
<marker>Gillick, Favre, Hakkani-Tur, Bohnet, Liu, Xie, 2010</marker>
<rawString>Dan Gillick, Benoit Favre, D. Hakkani-Tur, B. Bohnet, Y. Liu, and S. Xie. 2010. The icsi/utd summarization system at tac 2009. In Proceedings of the Second Text Analysis Conference, Gaithersburg, Maryland, USA: National Institute of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="11253" citStr="Griffiths and Steyvers, 2004" startWordPosition="1761" endWordPosition="1764">specific aspects AG and AS are also empirically set. Given a document collection, i.e. the set of all wd,s,n, our goal is to find the most likely assignment of zga d,s, zsa d,s, yd,s,n and ld,s,n that maximizes distribution p(z, y,l|w; α, β, y, S), where z, y, l and w represent the set of all z, y, l and w variables, respectively. With the assignment, sentences are naturally clustered into aspects, and words are labeled as either a background word, a document word, a general aspect word or a specific aspect word. Inference can be done with Gibbs sampling, which is commonly used in LDA models (Griffiths and Steyvers, 2004). In our experiments, we set α1 = 5, α2 = 3, β = 0.01, γ = 20, S1 = 10 and S2 = 10. We run 100 burn-in iterations through all documents in a collection to stabilize the distribution of z and y before collecting samples. We take 10 samples with a gap of 10 iterations between two samples, and average over these 10 samples to get the estimation for the parameters. After estimating all the distributions, we can find the values of each zga d,s and zsa d,s that gives us sentences clustered into general and specific aspects. 3 Sentence Ranking In this step, we want to order the clustered sentences so</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl. 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>L Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics on 7.7.7.,</booktitle>
<pages>362--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2853" citStr="Haghighi and Vanderwende, 2009" startWordPosition="416" endWordPosition="419">hat summarizes the subject of articles before the table of contents and other elaborate sections. Second, opinionated text often contains multiple viewpoints about an issue generated by different people. Summarizing these multiple opinions can help people easily digest them. Furthermore, combined with search engines and question&amp;answering systems, we can better organize the summary content based on aspects to improve user experience. Despite its usefulness, the problem of modeling domain specific aspects for multi-document summarization has not been well studied. The most relevant work is by (Haghighi and Vanderwende, 2009) on exploring content models for multi-document summarization. They proposed a HIERSUM model for finding the subtopics or aspects which are combined by using KL-divergence criterion for selecting relevant sentences. They introduced a general content distribution and several specific content distributions to discover the topic and aspects for a single document collection. However, the aspects may be shared not only across documents in a single collection, but also across documents in different topic-related collections. Their model is conceptually inadequate for simultaneously summarizing multi</context>
<context position="20929" citStr="Haghighi and Vanderwende (2009)" startWordPosition="3438" endWordPosition="3441">rion should be more recall oriented. So the average recall of ROUGE1, ROUGE-2, ROUGE-SU4, ROUGE-W-1.2 and ROUGE-L were computed by running ROUGE1.5.5 with stemming but no removal of stop words. We compare our method with the following four baseline methods. Baseline 1 In this baseline, we try to compare different sentence clustering algorithms in the multi-document summarization scenario. First, we use CLUTO 5 to do K-means clustering. Then we try entity-aspect model proposed by Li et al. (2010) to do sentence clustering. Entity-aspect model is similar with “HIERSUM” content model proposed by Haghighi and Vanderwende (2009). We use the same ranking, compression, and selection components to generate aspect-oriented summaries for comparison. Baseline 2 In this baseline, we compare our method with traditional ranking and selection summary generation framework (Erkan and Radev, 2004; Nenkova and Vanderwende, 2005) to show that our sentence clustering component is necessary in aspect-oriented summarization system. Also we want check whether sentence ranking combined with greedy based sentence selection can prevent redundancy effectively. We follow LexRank based sentence ranking combined with greedy sentence selection</context>
<context position="23294" citStr="Haghighi and Vanderwende, 2009" startWordPosition="3808" endWordPosition="3812">on method. Other parts are the same. After ranking sentences for each aspect, we add the sentence with the highest ranking score from each aspect sentence cluster as long as the KL-divergence between candidate and current summary does not decrease. This is repeated until the summary reaches a 100 word length limit. To our knowledge, this is the first work to directly compare Integer Linear Programming based sentence selection with KLdivergence based sentence selection in summarization generation framework. Baseline 4 In this baseline, we directly compare our method with “HIERSUM” proposed by (Haghighi and Vanderwende, 2009). As in Baseline 1, we use entityaspect model to approximate “HIERSUM” model. We replace unigram distribution of P(w) in KL-divergence with learned distribution estimated by “HIERSUM” model. The KL-divergence based greedy sentence selection algorithm is similar to Baseline 3. For fair comparison, Baselines 1, 2, 3 and 4 use the same sentence compression algorithm and have the summary length no more then 100 words. In Table 3, we show the average ROUGE recall of 46 summaries generated by our method and four baseline methods. We can see that our method gives better Rouge recall measures then the</context>
<context position="28480" citStr="Haghighi and Vanderwende, 2009" startWordPosition="4629" endWordPosition="4633">ted to a number of previous extensions of LDA models. Chemudugunta et al. (2007) proposed to introduce a background topic and document-specific topics. Our background and document language models are similar to theirs. However, they still treat documents as bags of words rather then sets of sentences as in our models. Titov and McDonald (2008) exploited the idea that a short paragraph within a document is likely to be about the same aspect. The way we separate words into stop words, background words, document words and aspect words bears similarity to that used in (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009). Paul and Girju (2010) proposed a topic-aspect model for simultaneously finding topics and aspects. The most related extension is entityaspect model proposed by Li et al. (2010). The main difference between event-aspect model and entityaspect model is our model further consider aspect granularity and add a layer to model topic-related events. Filippova and Strube (2008) proposed a dependency tree based sentence compression algorithm. Their approach need a large corpus to build language model for compression, whereas we prune dependency tree using grammatical rules. Paul et al. (2010) proposed</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>A. Haghighi and L. Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics on 7.7.7., pages 362–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Jing Jiang</author>
<author>Yinglin Wang</author>
</authors>
<title>Generating templates of entity summaries with an entityaspect model and pattern mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Conference of the 48th Annual Meeting of the ACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4338" citStr="Li et al., 2010" startWordPosition="629" endWordPosition="632">Natural Language Processing, pages 1137–1146, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics limitations. We hypothesize that comparatively summarizing topics across similar collections can improve the effectiveness of aspect-oriented multidocument summarization. We propose a novel extraction-based approach which consists of four main steps listed below: Sentence Clustering: Our goal in this step is to automatically identify the different aspects and cluster sentences into aspects (See Section 2). We substantially extend the entity-aspect model in (Li et al., 2010) for generating general sentence clusters. Sentence Ranking: In this step, we use an extension of LexRank algorithm proposed by (Paul et al., 2010) to score representative sentences in each cluster (See Section 3). Sentence Compression: In this step, we aim to improve the linguistic quality of the summaries by simplifying the sentence expressions. We prune sentences using grammatical relations defined on dependency trees for recognizing important clauses and removing redundant subtrees (See Section 4). Sentence Selection: Finally, we select one compressed version of the sentences from each asp</context>
<context position="5640" citStr="Li et al. (2010)" startWordPosition="834" endWordPosition="837">al objective function, for sentence selection (McDonald, 2007; Gillick and Favre, 2009; Sauper and Barzilay, 2009) (See Section 5). We evaluate our method using TAC2010 Guided Summarization task data sets1 (Section 6). Our evaluation shows that our method obtains better ROUGE recall score compared with four baseline methods, and it also achieve reasonably high-quality aspect clusters in terms of purity. 2 Sentence Clustering In this step, our goal is to discover event aspects contained in a document set and cluster sentences into aspects. Here we substantially extend the entityaspect model in Li et al. (2010) and refer to it as event-aspect model. The main difference between our event-aspect model and entity-aspect model is that we introduce an additional layer of event topics and the separation of general and specific aspects. lhttp://www.nist.gov/tac/2010/ Summarization/ Our extension is based upon the following observations. For example, specific events like “Columbine Massacre” and “Malaysia Resort Abduction” can be related to the “Attack” topic. Each event consists of multiple articles written by different news agencies. Interesting aspects may include “what happened, when, where, perpetrator</context>
<context position="20798" citStr="Li et al. (2010)" startWordPosition="3419" endWordPosition="3422">ROUGE (Lin and Hovy, 2003) metric for measuring the summarization system performance. Ideally, a summarization criterion should be more recall oriented. So the average recall of ROUGE1, ROUGE-2, ROUGE-SU4, ROUGE-W-1.2 and ROUGE-L were computed by running ROUGE1.5.5 with stemming but no removal of stop words. We compare our method with the following four baseline methods. Baseline 1 In this baseline, we try to compare different sentence clustering algorithms in the multi-document summarization scenario. First, we use CLUTO 5 to do K-means clustering. Then we try entity-aspect model proposed by Li et al. (2010) to do sentence clustering. Entity-aspect model is similar with “HIERSUM” content model proposed by Haghighi and Vanderwende (2009). We use the same ranking, compression, and selection components to generate aspect-oriented summaries for comparison. Baseline 2 In this baseline, we compare our method with traditional ranking and selection summary generation framework (Erkan and Radev, 2004; Nenkova and Vanderwende, 2005) to show that our sentence clustering component is necessary in aspect-oriented summarization system. Also we want check whether sentence ranking combined with greedy based sent</context>
<context position="28658" citStr="Li et al. (2010)" startWordPosition="4659" endWordPosition="4662">ls are similar to theirs. However, they still treat documents as bags of words rather then sets of sentences as in our models. Titov and McDonald (2008) exploited the idea that a short paragraph within a document is likely to be about the same aspect. The way we separate words into stop words, background words, document words and aspect words bears similarity to that used in (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009). Paul and Girju (2010) proposed a topic-aspect model for simultaneously finding topics and aspects. The most related extension is entityaspect model proposed by Li et al. (2010). The main difference between event-aspect model and entityaspect model is our model further consider aspect granularity and add a layer to model topic-related events. Filippova and Strube (2008) proposed a dependency tree based sentence compression algorithm. Their approach need a large corpus to build language model for compression, whereas we prune dependency tree using grammatical rules. Paul et al. (2010) proposed to modify LexRank algorithm using their topic-aspect model. But their task is to summarize contrastive viewpoints in opinionated text. Furthermore, they use a simple greedy appr</context>
</contexts>
<marker>Li, Jiang, Wang, 2010</marker>
<rawString>Peng Li, Jing Jiang, and Yinglin Wang. 2010. Generating templates of entity summaries with an entityaspect model and pattern mining. In Proceedings of the Joint Conference of the 48th Annual Meeting of the ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1,</booktitle>
<pages>71--78</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20208" citStr="Lin and Hovy, 2003" startWordPosition="3327" endWordPosition="3330">ument Set A and Document Set B. Each document set has 10 documents, and all the documents in Set A chronologically precede the documents in Set B. We just use document Set A for our task. Assessors wrote model summaries for each event, so we can compare our automatic generated summaries with the model summaries. We combine topic related data sets together, then these data sets simultaneously annotated by our Event-aspect model. After labeling process, we run sentence ranking, compression and selection module to get final aspect-oriented summarizations. 6.2 Quality of summary We use the ROUGE (Lin and Hovy, 2003) metric for measuring the summarization system performance. Ideally, a summarization criterion should be more recall oriented. So the average recall of ROUGE1, ROUGE-2, ROUGE-SU4, ROUGE-W-1.2 and ROUGE-L were computed by running ROUGE1.5.5 with stemming but no removal of stop words. We compare our method with the following four baseline methods. Baseline 1 In this baseline, we try to compare different sentence clustering algorithms in the multi-document summarization scenario. First, we use CLUTO 5 to do K-means clustering. Then we try entity-aspect model proposed by Li et al. (2010) to do sen</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.Y. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 71–78. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>Advances in Information Retrieval,</booktitle>
<pages>557--564</pages>
<contexts>
<context position="5085" citStr="McDonald, 2007" startWordPosition="744" endWordPosition="745"> et al., 2010) to score representative sentences in each cluster (See Section 3). Sentence Compression: In this step, we aim to improve the linguistic quality of the summaries by simplifying the sentence expressions. We prune sentences using grammatical relations defined on dependency trees for recognizing important clauses and removing redundant subtrees (See Section 4). Sentence Selection: Finally, we select one compressed version of the sentences from each aspect cluster. We use Integer Linear Programming (ILP) algorithm, which optimizes a global objective function, for sentence selection (McDonald, 2007; Gillick and Favre, 2009; Sauper and Barzilay, 2009) (See Section 5). We evaluate our method using TAC2010 Guided Summarization task data sets1 (Section 6). Our evaluation shows that our method obtains better ROUGE recall score compared with four baseline methods, and it also achieve reasonably high-quality aspect clusters in terms of purity. 2 Sentence Clustering In this step, our goal is to discover event aspects contained in a document set and cluster sentences into aspects. Here we substantially extend the entityaspect model in Li et al. (2010) and refer to it as event-aspect model. The m</context>
<context position="29304" citStr="McDonald (2007)" startWordPosition="4757" endWordPosition="4758">vent-aspect model and entityaspect model is our model further consider aspect granularity and add a layer to model topic-related events. Filippova and Strube (2008) proposed a dependency tree based sentence compression algorithm. Their approach need a large corpus to build language model for compression, whereas we prune dependency tree using grammatical rules. Paul et al. (2010) proposed to modify LexRank algorithm using their topic-aspect model. But their task is to summarize contrastive viewpoints in opinionated text. Furthermore, they use a simple greedy approach for constructing summary. McDonald (2007) proposed to use Integer Linear Programming framework in multi-document sum1144 marization. And Sauper and Barzilay (2009) use integer linear programming framework to automatically generate Wikipedia articles. There is a fundamental difference between their method and ours. They used trained perceptron algorithm for ranking excerpts, whereas we give an extended LexRank with integer linear programming to optimize sentence selection for our aspect-oriented multi-document summarization. 8 Conclusions and Future Work In this paper, we study the task of automatically generating aspect-oriented summ</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. Advances in Information Retrieval, pages 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>L Vanderwende</author>
</authors>
<title>The impact of frequency on summarization. Microsoft Research,</title>
<date>2005</date>
<pages>2005--101</pages>
<location>Redmond, Washington, Tech. Rep.</location>
<contexts>
<context position="21221" citStr="Nenkova and Vanderwende, 2005" startWordPosition="3479" endWordPosition="3482"> to compare different sentence clustering algorithms in the multi-document summarization scenario. First, we use CLUTO 5 to do K-means clustering. Then we try entity-aspect model proposed by Li et al. (2010) to do sentence clustering. Entity-aspect model is similar with “HIERSUM” content model proposed by Haghighi and Vanderwende (2009). We use the same ranking, compression, and selection components to generate aspect-oriented summaries for comparison. Baseline 2 In this baseline, we compare our method with traditional ranking and selection summary generation framework (Erkan and Radev, 2004; Nenkova and Vanderwende, 2005) to show that our sentence clustering component is necessary in aspect-oriented summarization system. Also we want check whether sentence ranking combined with greedy based sentence selection can prevent redundancy effectively. We follow LexRank based sentence ranking combined with greedy sentence selection methods. We implement two greedy algorithms (Zhang et al., 2008; Paul et al., 2010). One is to select the top ranked sentence simultaneously by removing 10 redundant neighbor sentences from the sentence similarity graph if the summary length is less then 100 words. This is repeated until th</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>A. Nenkova and L. Vanderwende. 2005. The impact of frequency on summarization. Microsoft Research, Redmond, Washington, Tech. Rep. MSR-TR-2005-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>Roxana Girju</author>
</authors>
<title>A twodimensional topic-aspect model for discovering multifaceted topics.</title>
<date>2010</date>
<booktitle>In In AAAI-2010: Twenty-Fourth Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="28503" citStr="Paul and Girju (2010)" startWordPosition="4634" endWordPosition="4637">sions of LDA models. Chemudugunta et al. (2007) proposed to introduce a background topic and document-specific topics. Our background and document language models are similar to theirs. However, they still treat documents as bags of words rather then sets of sentences as in our models. Titov and McDonald (2008) exploited the idea that a short paragraph within a document is likely to be about the same aspect. The way we separate words into stop words, background words, document words and aspect words bears similarity to that used in (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009). Paul and Girju (2010) proposed a topic-aspect model for simultaneously finding topics and aspects. The most related extension is entityaspect model proposed by Li et al. (2010). The main difference between event-aspect model and entityaspect model is our model further consider aspect granularity and add a layer to model topic-related events. Filippova and Strube (2008) proposed a dependency tree based sentence compression algorithm. Their approach need a large corpus to build language model for compression, whereas we prune dependency tree using grammatical rules. Paul et al. (2010) proposed to modify LexRank algo</context>
</contexts>
<marker>Paul, Girju, 2010</marker>
<rawString>Michael J. Paul and Roxana Girju. 2010. A twodimensional topic-aspect model for discovering multifaceted topics. In In AAAI-2010: Twenty-Fourth Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>ChengXiang Zhai</author>
<author>Roxana Girju</author>
</authors>
<title>Summarizing contrastive viewpoints in opinionated text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>66--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4485" citStr="Paul et al., 2010" startWordPosition="653" endWordPosition="656">ions. We hypothesize that comparatively summarizing topics across similar collections can improve the effectiveness of aspect-oriented multidocument summarization. We propose a novel extraction-based approach which consists of four main steps listed below: Sentence Clustering: Our goal in this step is to automatically identify the different aspects and cluster sentences into aspects (See Section 2). We substantially extend the entity-aspect model in (Li et al., 2010) for generating general sentence clusters. Sentence Ranking: In this step, we use an extension of LexRank algorithm proposed by (Paul et al., 2010) to score representative sentences in each cluster (See Section 3). Sentence Compression: In this step, we aim to improve the linguistic quality of the summaries by simplifying the sentence expressions. We prune sentences using grammatical relations defined on dependency trees for recognizing important clauses and removing redundant subtrees (See Section 4). Sentence Selection: Finally, we select one compressed version of the sentences from each aspect cluster. We use Integer Linear Programming (ILP) algorithm, which optimizes a global objective function, for sentence selection (McDonald, 2007</context>
<context position="11955" citStr="Paul et al. (2010)" startWordPosition="1894" endWordPosition="1897">We run 100 burn-in iterations through all documents in a collection to stabilize the distribution of z and y before collecting samples. We take 10 samples with a gap of 10 iterations between two samples, and average over these 10 samples to get the estimation for the parameters. After estimating all the distributions, we can find the values of each zga d,s and zsa d,s that gives us sentences clustered into general and specific aspects. 3 Sentence Ranking In this step, we want to order the clustered sentences so that the representative sentences can be ranked higher in each aspect. Inspired by Paul et al. (2010), we use an extended LexRank algorithm to obtain top ranked sentences. LexRank (Erkan and Radev, 2004) algorithm defines a random walk mod1139 1. Draw θ1 ∼ Dir(α1), θ2 ∼ Dir(α2), π ∼ Dir(γ) Draw σ ∼ Beta(δ0, δ1) 2. For each event topic, there is a background model Og, and there are general aspect ga, where 1 ≤ ga ≤ AG (a) draw Og ∼ Dir(β) (b) draw Oga ∼Dir(β) 3. For each document collection, there are specific aspect sa, where 1 ≤ sa ≤ AS (a) draw Osa ∼ Dir(β) 4. For each document d = 1, ... , D, (a) draw Od ∼ Dir(β) (b) for each sentence s = 1, ... , Sd i. draw zga ∼ Multi(θ1) ii. draw zsa ∼ </context>
<context position="21613" citStr="Paul et al., 2010" startWordPosition="3536" endWordPosition="3539">o generate aspect-oriented summaries for comparison. Baseline 2 In this baseline, we compare our method with traditional ranking and selection summary generation framework (Erkan and Radev, 2004; Nenkova and Vanderwende, 2005) to show that our sentence clustering component is necessary in aspect-oriented summarization system. Also we want check whether sentence ranking combined with greedy based sentence selection can prevent redundancy effectively. We follow LexRank based sentence ranking combined with greedy sentence selection methods. We implement two greedy algorithms (Zhang et al., 2008; Paul et al., 2010). One is to select the top ranked sentence simultaneously by removing 10 redundant neighbor sentences from the sentence similarity graph if the summary length is less then 100 words. This is repeated until the graph cannot be partitioned. The similarity graph building threshold is 0.3, damping factor is 0.2 and error tolerance for Power Method in LexRank is 0.1. The other is to select top ranked sentences as long as the redundancy score (similarity) between a candidate sentence and 5http://glaros.dtc.umn.edu/gkhome/cluto/ cluto/overview Rj l=1 K E j=1 1142 current summary is under 0.5. This is</context>
<context position="29071" citStr="Paul et al. (2010)" startWordPosition="4722" endWordPosition="4725">ighi and Vanderwende, 2009). Paul and Girju (2010) proposed a topic-aspect model for simultaneously finding topics and aspects. The most related extension is entityaspect model proposed by Li et al. (2010). The main difference between event-aspect model and entityaspect model is our model further consider aspect granularity and add a layer to model topic-related events. Filippova and Strube (2008) proposed a dependency tree based sentence compression algorithm. Their approach need a large corpus to build language model for compression, whereas we prune dependency tree using grammatical rules. Paul et al. (2010) proposed to modify LexRank algorithm using their topic-aspect model. But their task is to summarize contrastive viewpoints in opinionated text. Furthermore, they use a simple greedy approach for constructing summary. McDonald (2007) proposed to use Integer Linear Programming framework in multi-document sum1144 marization. And Sauper and Barzilay (2009) use integer linear programming framework to automatically generate Wikipedia articles. There is a fundamental difference between their method and ours. They used trained perceptron algorithm for ranking excerpts, whereas we give an extended Lex</context>
</contexts>
<marker>Paul, Zhai, Girju, 2010</marker>
<rawString>Michael J. Paul, ChengXiang Zhai, and Roxana Girju. 2010. Summarizing contrastive viewpoints in opinionated text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 66–76, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Sauper</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatically generating wikipedia articles: A structure-aware approach.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>208--216</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="5138" citStr="Sauper and Barzilay, 2009" startWordPosition="750" endWordPosition="753">ntences in each cluster (See Section 3). Sentence Compression: In this step, we aim to improve the linguistic quality of the summaries by simplifying the sentence expressions. We prune sentences using grammatical relations defined on dependency trees for recognizing important clauses and removing redundant subtrees (See Section 4). Sentence Selection: Finally, we select one compressed version of the sentences from each aspect cluster. We use Integer Linear Programming (ILP) algorithm, which optimizes a global objective function, for sentence selection (McDonald, 2007; Gillick and Favre, 2009; Sauper and Barzilay, 2009) (See Section 5). We evaluate our method using TAC2010 Guided Summarization task data sets1 (Section 6). Our evaluation shows that our method obtains better ROUGE recall score compared with four baseline methods, and it also achieve reasonably high-quality aspect clusters in terms of purity. 2 Sentence Clustering In this step, our goal is to discover event aspects contained in a document set and cluster sentences into aspects. Here we substantially extend the entityaspect model in Li et al. (2010) and refer to it as event-aspect model. The main difference between our event-aspect model and ent</context>
<context position="16615" citStr="Sauper and Barzilay, 2009" startWordPosition="2713" endWordPosition="2716">rs and abbreviations, participials and infinitive modifiers. 4. Traverse the subtrees and generate all possible compression alternatives using the subtree root node, then keep the top two longest sub sentences. 5. Drop the sub sentences shorter than 5 words. 5 Sentence Selection After sentence pruning, we prepare for the final event summary generation process. In this step, we select one compressed version of the sentence from each aspect cluster. To avoid redundancy between aspects, we use Integer Linear Programming to optimize a global objective function for sentence selection. Inspired by (Sauper and Barzilay, 2009), we formulate the optimization problem based on sentence ranking information. More specifically, we 3The parataxis relation is a relation between the main verb of a clause and other sentential elements, such as a sentential parenthetical, colon, or semicolon Original Compressed When rescue workers When rescue workers arrived, they said, on- arrived, only one of his ly one of his limbs was limbs was visible. visible. Two days earlier, a Two days earlier, a massacre by two s- massacre by two stutudents at Columbine dents at Columbine High, whose teams are High, left 15 peocalled the Rebels, lef</context>
<context position="29426" citStr="Sauper and Barzilay (2009)" startWordPosition="4772" endWordPosition="4775">del topic-related events. Filippova and Strube (2008) proposed a dependency tree based sentence compression algorithm. Their approach need a large corpus to build language model for compression, whereas we prune dependency tree using grammatical rules. Paul et al. (2010) proposed to modify LexRank algorithm using their topic-aspect model. But their task is to summarize contrastive viewpoints in opinionated text. Furthermore, they use a simple greedy approach for constructing summary. McDonald (2007) proposed to use Integer Linear Programming framework in multi-document sum1144 marization. And Sauper and Barzilay (2009) use integer linear programming framework to automatically generate Wikipedia articles. There is a fundamental difference between their method and ours. They used trained perceptron algorithm for ranking excerpts, whereas we give an extended LexRank with integer linear programming to optimize sentence selection for our aspect-oriented multi-document summarization. 8 Conclusions and Future Work In this paper, we study the task of automatically generating aspect-oriented summary from multiple documents. We proposed an event-aspect model that can automatically cluster sentences into aspects. We t</context>
</contexts>
<marker>Sauper, Barzilay, 2009</marker>
<rawString>Christina Sauper and Regina Barzilay. 2009. Automatically generating wikipedia articles: A structure-aware approach. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 208–216, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceeding of the 17th International Conference on World Wide Web,</booktitle>
<pages>111--120</pages>
<contexts>
<context position="28194" citStr="Titov and McDonald (2008)" startWordPosition="4579" endWordPosition="4582">e the effectiveness of sentence compression component, we conduct the system without sentence compression component, then compare it with our system. In Table 3, we can see that sentence compression can improve the system performance. 7 Related Work Our event-aspect model is related to a number of previous extensions of LDA models. Chemudugunta et al. (2007) proposed to introduce a background topic and document-specific topics. Our background and document language models are similar to theirs. However, they still treat documents as bags of words rather then sets of sentences as in our models. Titov and McDonald (2008) exploited the idea that a short paragraph within a document is likely to be about the same aspect. The way we separate words into stop words, background words, document words and aspect words bears similarity to that used in (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009). Paul and Girju (2010) proposed a topic-aspect model for simultaneously finding topics and aspects. The most related extension is entityaspect model proposed by Li et al. (2010). The main difference between event-aspect model and entityaspect model is our model further consider aspect granularity and add a layer</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. Modeling online reviews with multi-grain topic models. In Proceeding of the 17th International Conference on World Wide Web, pages 111–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zajic</author>
<author>B J Dorr</author>
<author>J Lin</author>
<author>R Schwartz</author>
</authors>
<title>Multicandidate reduction: Sentence compression as a tool for document summarization tasks.</title>
<date>2007</date>
<journal>Information Processing &amp; Management,</journal>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="14639" citStr="Zajic et al., 2007" startWordPosition="2406" endWordPosition="2409">boring sentence v. p(u|v) is defined using content similarity function sim(u, v) between two sentences: where the value P(ga|u) and P(sa|u) can be computed by our event-aspect model. We define sim(u, v) as the tf * idf weighted cosine similarity between two sentences. We found that sentence ranking is better conducted before the compression because the precompressed sentences are more informative and the similarity function in LexRank can be better off with the complete information. 4 Sentence Compression It has been shown that sentence compression can improve linguistic quality of summaries (Zajic et al., 2007; Gillick et al., 2010). Commonly used “Syntactic parse and trim” approach may produce poor compression results. For example, given the sentence “We have friends whose children go to Columbine, the freshman said”, the procedure tries to remove the clause “the freshman said” from the parse tree by using the “SBAR” label to locate the p(u|v)L(v) 1140 clause, and will result in “whose children go to Columbine”, which is not adequate. Furthermore, some important temporal modifier, numeric modifier and clausal complement need to be retained because they reflect content aspects of the summary. There</context>
</contexts>
<marker>Zajic, Dorr, Lin, Schwartz, 2007</marker>
<rawString>D. Zajic, B.J. Dorr, J. Lin, and R. Schwartz. 2007. Multicandidate reduction: Sentence compression as a tool for document summarization tasks. Information Processing &amp; Management, 43(6):1549–1570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xueqi Cheng Zhang</author>
<author>Hongbo Xu</author>
</authors>
<title>GSPSummary: a graph-based sub-topic partition algorithm for summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 4th Asia information retrieval conference on Information retrieval technology,</booktitle>
<pages>321--334</pages>
<publisher>Springer-Verlag.</publisher>
<marker>Zhang, Xu, 2008</marker>
<rawString>Jin. Zhang, Xueqi. Cheng, and Hongbo. Xu. 2008. GSPSummary: a graph-based sub-topic partition algorithm for summarization. In Proceedings of the 4th Asia information retrieval conference on Information retrieval technology, pages 321–334. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>