<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001676">
<title confidence="0.997676">
Universal Morphological Analysis using Structured Nearest Neighbor
Prediction
</title>
<author confidence="0.994785">
Young-Bum Kim João V. Graça Benjamin Snyder
</author>
<affiliation confidence="0.994019">
University of Wisconsin-Madison L2F INESC-ID University of Wisconsin-Madison
</affiliation>
<email confidence="0.920376">
ybkim@cs.wisc.edu Lisboa, Portugal bsnyder@cs.wisc.edu
joao.graca@l2f.inesc-id.pt
</email>
<sectionHeader confidence="0.995532" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999774125">
In this paper, we consider the problem of un-
supervised morphological analysis from a new
angle. Past work has endeavored to design un-
supervised learning methods which explicitly
or implicitly encode inductive biases appropri-
ate to the task at hand. We propose instead
to treat morphological analysis as a structured
prediction problem, where languages with la-
beled data serve as training examples for un-
labeled languages, without the assumption of
parallel data. We define a universal morpho-
logical feature space in which every language
and its morphological analysis reside. We de-
velop a novel structured nearest neighbor pre-
diction method which seeks to find the mor-
phological analysis for each unlabeled lan-
guage which lies as close as possible in the
feature space to a training language. We ap-
ply our model to eight inflecting languages,
and induce nominal morphology with substan-
tially higher accuracy than a traditional, MDL-
based approach. Our analysis indicates that
accuracy continues to improve substantially as
the number of training languages increases.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998356642857143">
Over the past several decades, researchers in the nat-
ural language processing community have focused
most of their efforts on developing text processing
tools and techniques for English (Bender, 2009),
a morphologically simple language. Recently, in-
creasing attention has been paid to the wide variety
of other languages of the world. Most of these lan-
guages still pose severe difficulties, due to (i) their
lack of annotated textual data, and (ii) the fact that
they exhibit linguistic structure not found in English,
and are thus not immediately susceptible to many
traditional NLP techniques.
Consider the example of nominal part-of-speech
analysis. The Penn Treebank defines only four En-
glish noun tags (Marcus et al., 1994), and as a re-
sult, it is easy to treat the words bearing these tags
as completely distinct word classes, with no inter-
nal morphological structure. In contrast, a compara-
ble tagset for Hungarian includes 154 distinct noun
tags (Erjavec, 2004), reflecting Hungarian’s rich in-
flectional morphology. When dealing with such lan-
guages, treating words as atoms leads to severe data
sparsity problems.
Because annotated resources do not exist for most
morphologically rich languages, prior research has
focused on unsupervised methods, with a focus on
developing appropriate inductive biases. However,
inductive biases and declarative knowledge are no-
toriously difficult to encode in well-founded models.
Even putting aside this practical matter, a universally
correct inductive bias, if there is one, is unlikely to
be be discovered by a priori reasoning alone.
In this paper, we argue that languages for which
we have gold-standard morphological analyses can
be used as effective guides for languages lacking
such resources. In other words, instead of treating
each language’s morphological analysis as a de novo
induction problem to be solved with a purely hand-
coded bias, we instead learn from our labeled lan-
guages what linguistically plausible morphological
analyses looks like, and guide our analysis in this
direction.
</bodyText>
<page confidence="0.962925">
322
</page>
<note confidence="0.957951">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 322–332,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999960170731707">
More formally, we recast morphological induc-
tion as a new kind of supervised structured predic-
tion problem, where each annotated language serves
as a single training example. Each language’s noun
lexicon serves as a single input x, and the analysis
of the nouns into stems and suffixes serves as a com-
plex structured label y.
Our first step is to define a universal morpholog-
ical feature space, into which each language and its
morphological analysis can be mapped. We opt for
a simple and intuitive mapping, which measures the
sizes of the stem and suffix lexicons, the entropy of
these lexicons, and the fraction of word forms which
appear without any inflection.
Because languages tend to cluster into well de-
fined morphological groups, we cast our learn-
ing and prediction problem in the nearest neighbor
framework (Cover and Hart, 1967). In contrast to
its typical use in classification problems, where one
can simply pick the label of the nearest training ex-
ample, we are here faced with a structured predic-
tion problem, where locations in feature space de-
pend jointly on the input-label pair (x, y). Finding a
nearest neighbor thus consists of searching over the
space of morphological analyses, until a point in fea-
ture space is reached which lies closest to one of the
labeled languages. See Figure 1 for an illustration.
To provide a measure of empirical validation, we
applied our approach to eight languages with inflec-
tional nominal morphology, ranging in complexity
from very simple (English) to very complex (Hun-
garian). In all but one case, our approach yields
substantial improvements over a comparable mono-
lingual baseline (Goldsmith, 2005), which uses the
minimum description length principle (MDL) as its
inductive bias. On average, our method increases
accuracy by 11.8 percentage points, corresponding
to a 42% decrease in error relative to a supervised
upper bound. Further analysis indicates that accu-
racy improves as the number of training languages
increases.
</bodyText>
<sectionHeader confidence="0.999664" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9924086">
In this section, we briefly review prior work on un-
supervised morphological induction, as well as mul-
tilingual analysis in NLP.
Unsupervised Morphological Induction: Unsu-
pervised morphology remains an active area of re-
search (Schone and Jurafsky, 2001; Goldsmith,
2005; Adler and Elhadad, 2006; Creutz and La-
gus, 2005; Dasgupta and Ng, 2007; Creutz and La-
gus, 2007; Poon et al., 2009). Many existing algo-
rithms derive morpheme lexicons by identifying re-
curring patterns in words. The goal is to optimize the
compactness of the data representation by finding a
small lexicon of highly frequent strings, resulting in
a minimum description length (MDL) lexicon and
corpus (Goldsmith, 2001; Goldsmith, 2005). Later
work cast this idea in a probabilistic framework in
which the the MDL solution is equivalent to a MAP
estimate in a suitable Bayesian model (Creutz and
Lagus, 2005). In all these approaches, a locally op-
timal segmentation is identified using a task-specific
greedy search.
Multilingual Analysis: An influential line of prior
multilingual work starts with the observation that
rich linguistic resources exist for some languages
but not others. The idea then is to project linguis-
tic information from one language onto others via
parallel data. Yarowsky and his collaborators first
developed this idea and applied it to the problems of
part-of-speech tagging, noun-phrase bracketing, and
morphology induction (Yarowsky and Wicentowski,
2000; Yarowsky et al., 2000; Yarowsky and Ngai,
2001), and other researchers have applied the idea
to syntactic and semantic analysis (Hwa et al., 2005;
Padó and Lapata, 2006) In these cases, the existence
of a bilingual parallel text along with highly accurate
predictions for one of the languages was assumed.
Another line of work assumes the existence of
bilingual parallel texts without the use of any super-
vision (Dagan et al., 1991; Resnik and Yarowsky,
1997). This idea has been developed and applied to
a wide variety tasks, including morphological anal-
ysis (Snyder and Barzilay, 2008b; Snyder and Barzi-
lay, 2008a), part-of-speech induction (Snyder et al.,
2008; Snyder et al., 2009b; Naseem et al., 2009),
and grammar induction (Snyder et al., 2009a; Blun-
som et al., 2009; Burkett et al., 2010). An even
more recent line of work does away with the as-
sumption of parallel texts and performs joint unsu-
pervised induction for various languages through the
use of coupled priors in the context of grammar in-
</bodyText>
<page confidence="0.99875">
323
</page>
<bodyText confidence="0.997084">
duction (Cohen and Smith, 2009; Berg-Kirkpatrick
and Klein, 2010).
In contrast to these previous approaches, the
method proposed in this paper does not assume the
existence of any parallel text, but does assume that
labeled data exists for a wide variety of languages, to
be used as training examples for our test language.
</bodyText>
<sectionHeader confidence="0.992152" genericHeader="method">
3 Structured Nearest Neighbor
</sectionHeader>
<bodyText confidence="0.99974105">
We reformulate morphological induction as a super-
vised learning task, where each annotated language
serves as a single training example for our language-
independent model. Each such example consists
of an input-label pair (x, y), both of which contain
complex internal structure: The input x E X con-
sists of a vocabulary list of all words observed in a
particular monolingual corpus, and the label y E Y
consists of the correct morphological analysis of all
the vocabulary items in x.1 Because our goal is
to generalize across languages, we define a feature
function which maps each (x, y) pair to a universal
feature space: f : X x Y -+ Rd.
For each unlabeled input language x, our goal is
to predict a complete morphological analysis y E Y
which maximizes a scoring function on the fea-
ture space, score : Rd -+ R. This scoring func-
tion is trained using the n labeled-language exam-
ples: (x, y)1, ... , (x, y)n, and the resulting predic-
tion rule for unlabeled input x is given by:
</bodyText>
<equation confidence="0.874942">
score(f(x, y))
</equation>
<bodyText confidence="0.959255789473684">
Languages can be typologically categorized by
the type and richness of their morphology. On the
assumption that for each test language, at least one
typologically similar language will be present in the
training set, we employ a nearest neighbor scoring
function. In the standard nearest neighbor classifi-
cation setting, one simply predicts the label of the
closest training example in the input space.2 In our
structured prediction setting, the mapping to the uni-
versal feature space depends crucially on the struc-
ture of the proposed label y, not simply the input
1Technically, the label space of each input, Y, should be
thought of as a function of the input x. We suppress this depen-
dence for notational clarity.
2More generally the majority label of the k-nearest neigh-
bors.
x. We thus generalize nearest-neighbor prediction
to the structured scenario and propose the following
prediction rule:
</bodyText>
<equation confidence="0.8982625">
y* = argmin min 11 f(x, y) − f(xe, ye) 11, (1)
yEY e
</equation>
<bodyText confidence="0.999710666666667">
where the index E ranges over the training languages.
In words, we predict the morphological analysis y
for our test language which places it as close as pos-
sible in the universal feature space to one of the
training languages E.
Morphological Analysis: In this paper we focus
on nominal inflectional suffix morphology. Consider
the word utiskom in Serbian, meaning impression
with the instrumental case marking. A correct analy-
sis of this word would divide it into a stem (utisak =
impression), a suffix (-om = instrumental case), and
a phonological deletion rule on the stem’s penulti-
mate vowel (..ak# -+ ..k#).
More generally, as we define it, a morphological
analysis of a word type w consists of (i) a stem t, (ii),
a suffix f, and (iii) a deletion rule d. Either or both
of the suffix and deletion rule can be NULL. We al-
low three types of deletion rules on stems: deletion
of final vowels (..V # -+ ..#), deletion of penulti-
mate vowels (..V C# -+ ..C#), and removals and
additions of final accent marks (e.g. ..˜a# -+ ..a#).
We require that stems be at least three characters
long and that suffixes be no more than four. And,
of course, we require that after (1) applying deletion
rule d to stem t, and (2) adding suffix f to the result,
we obtain word w.
Universal Feature Space: We employ a fairly
simple and minimal set of features, all of which
could plausibly generalize across a wide range of
languages. Consider the set of stems T, suffixes F,
and deletion rules D, induced by the morphological
analyses y of the words x. Our first three features
simply count the sizes of these three sets.
These counting features consider only the raw
number of unique morphemes (and phonological
rules) being used, but not their individual frequency
or distribution. Our next set of features considers
the empirical entropy of these occurrences as dis-
tributed across the lexicon of words x by analysis y.
</bodyText>
<equation confidence="0.9829735">
y* = argmax
yEY
</equation>
<page confidence="0.995296">
324
</page>
<figureCaption confidence="0.9388995">
Figure 1: Structured Nearest Neighbor Search: The inference procedure for unlabeled test language x, when trained
with three labeled languages, (x1, y1), (x2, y2), (x3, y3). Our search procedure iteratively attempts to find labels for x
which are as close as possible in feature space to each of the training languages. After convergence, the label which is
closest in distance to a training language is predicted, in this case being the label near training language (x3, y3).
</figureCaption>
<equation confidence="0.998772833333333">
t 1 ,1
Y
t 1 ,3
Y
t�1
Y
f(x1, yl)
,∗
f x, y
Initialization
t,3
Y
t,2
Y
t 1 ,2
Y
f(x2, y2)
f(x3, y3)
</equation>
<bodyText confidence="0.99625225">
For example, if the (x, y) pair consists of the ana-
lyzed words {kiss, kiss-es, hug}, then the empirical
distributions over stems, suffixes, and deletion rules
would be:
</bodyText>
<listItem confidence="0.9999918">
• P(t = kiss) = 2/3
• P(t = hug) = 1/3
• P(f = NULL) = 2/3
• P(f = −es) = 1/3
• P(d = NULL) = 1
</listItem>
<bodyText confidence="0.936243642857143">
The three entropy features are defined as the shan-
non entropies of these stem, suffix, and deletion rule
probabilities: H(t), H(f), H(d).3
Finally, we consider two simple percentage fea-
tures: the percentage of words in x which according
to y are left unsegmented (i.e. have the null suf-
fix, 2/3 in the example above), and the percentage of
segmented words which employ a deletion rule (0 in
the example above). Thus, in total, our model em-
ploys 8 universal morphological features. All fea-
tures are scaled to the unit interval and are assumed
to have equal weight.
3Note that here and throughout the paper, we operate over
word types, ignoring their corpus frequencies.
</bodyText>
<subsectionHeader confidence="0.999563">
3.1 Search Algorithm
</subsectionHeader>
<bodyText confidence="0.998527095238095">
The main algorithmic challenge for our model lies in
efficiently computing the best morphological analy-
sis y for each language-specific word set x, accord-
ing to Equation 1. Exhaustive search through the
set of all possible morphological analyses is impos-
sible, as the number of such analyses grows expo-
nentially in the size of the vocabulary. Instead, we
develop a greedy search algorithm in the following
fashion (the search procedure is visually depicted in
Figure 1).
At each time-step t, we maintain a set of frontier
analyses { y(t,`) h, where E ranges over the training
languages. The goal is to iteratively modify each of
these frontier analyses y(t,`) → y(t+1,`) so that the
location of the training language in universal feature
space — f(x, y(t+1,`)) — is as close as possible to
the location of the training language E: f(x`, y`).
After iterating this procedure to convergence, we
are left with a set of analyses {y(`)}`, each of which
approximates the analyses which yield minimal dis-
tances to a particular training language:
</bodyText>
<equation confidence="0.599522">
y(`) Pz argmin II f(x, y) − f(x`, y`) II .
y∈Y
</equation>
<bodyText confidence="0.988311">
We finally select from amongst these analyses and
</bodyText>
<page confidence="0.967631">
325
</page>
<bodyText confidence="0.981730710526316">
make our prediction: suffix pair (t, f), for which f ∈ F, and where ei-
`* = argmin k f(x,y(`)) − f(x`,y`) k ther t ∈ T or some t&apos; ∈ T such that t is obtained
` from t&apos; using a deletion rule d (e.g. by deleting a
y* = y(`∗) final or penultimate vowel). For each such possi-
The main outline of our search algorithm is based ble analysis y&apos;, we compute the resulting location
on the MDL-based greedy search heuristic devel- in feature space f(x, y&apos;), and select the analysis that
oped and studied by (Goldsmith, 2005). At a high brings us closest to our target training language:
level, this search procedure alternates between indi- y = argminy, k f(x, y&apos;) − f(x`, y`) k .
vidual analyses of words (keeping the set of stems Stage 2: Find New Stems
and suffixes fixed), aggregate discoveries of new In this stage, we keep our set of suffixes F and
stems (keeping the suffixes fixed), and aggregate dis- deletion rules D from the previous stage fixed, and
coveries of new suffixes (keeping stems fixed). As attempt to find new stems to add to T through an ag-
input, we consider the test words x in our new lan- gregate analysis of unsegmented words. For every
guage, and we run the search in parallel for each string s, we consider the set of words which are cur-
training language (x`, y`). For each such test-train rently unsegmented, and can be analyzed as a stem-
language pair, the search consists of the following suffix pair (s, f) for some existing suffix f ∈ F,
stages: and some deletion rule d ∈ D. We then consider
Stage 0: Initialization the joint segmentation of these words into a new
We initially analyze each word w ∈ x according stem s, and their respective suffixes. As before, we
to peaks in successor frequency.4 If w’s n-character choose the segmentation if it brings us closer in fea-
prefix w:n has successor frequency &gt; 1 and the sur- ture space to our target training language.
rounding prefixes, w:n_1 and w:n+1 both have suc- Stage 3: Find New Suffixes
cessor frequency = 1, then we analyze w as a stem- This stage is exactly analogous to the previous
suffix pair: (w:n, wn+1:).5 Otherwise, we initialize stage, except we now fix the set of stems T and seek
w as an unsuffixed stem. As this procedure tends to to find new suffixes.
produce an overly large set of suffixes F, we further 3.2 A Monolingual Supervised Model
prune F down to the number of suffixes found in In order to provide a plausible upper bound on per-
the training language, retaining those which appear formance, we also formulate a supervised monolin-
with the largest number of stems. This initialization gual morphological model, using the structured per-
stage is carried out once, and afterwards the follow- ceptron framework (Collins, 2002). Here we as-
ing three stages are repeated until convergence. sume that we are given some training sequence of in-
Stage 1: Reanalyze each word puts and morphological analyses (all within one lan-
In this stage, we reanalyze each word (in random guage): (x1, y1), (x2, y2), ... , (xn, yn). We define
order). We use the set of stems T and suffixes F each input xi to be a noun w, along with a morpho-
obtained from the previous stage, and don’t permit logical tag z, which specifies the gender, case, and
the addition of any new items to these lists. In- number of the noun. The goal is to predict the cor-
stead, we focus on obtaining better analyses of each rect segmentation of w into stem, suffix, and phono-
word, while also building up a set of phonological logical deletion rule: yi = (t, f, d).6
</bodyText>
<table confidence="0.802556913043478">
deletion rules D. For each word w ∈ x, we con- To do so, we define a feature function over input-
sider all possible segmentations of w into a stem- label pairs, (x, y), with the following binary feature
templates: (1) According to label yi, the stem is t
4The successor frequency of a string prefix s is defined as
the number of unique characters that occur immediately after s
in the vocabulary.
5With the restriction that at this stage we only allow suffixes
up to length 5, and stems of at least length 3.
6While the assumption of the correct morphological tag as
input is somewhat unrealistic, this model still gives us a strong
upper bound on how well we can expect our unsupervised
model to perform.
326
Type Counts Entropy Percentage
# words # stems # suffs # dels stem entropy suff entropy del entropy unseg deleted
BG 4833 3112 21 8 11.4 2.7 0.9 .45 .29
CS 5836 3366 28 12 11.5 3.2 1.6 .38 .53
EN 4178 3453 3 1 11.7 1.0 0.1 .73 .06
ET 6371 3742 141 5 11.5 5.0 0.2 .31 .04
HU 8051 3746 231 7 11.3 5.8 0.5 .23 .11
RO 5578 3297 23 8 11.5 2.9 1.4 .48 .51
SL 6111 3172 32 6 11.3 3.2 1.5 .33 .56
SR 5849 3178 28 5 11.4 2.9 1.4 .33 .53
</table>
<tableCaption confidence="0.999799">
Table 1: Corpus statistics for the eight languages. The first four columns give the number of unique word, stem, suffix,
</tableCaption>
<bodyText confidence="0.987761105263158">
and phonological deletion rule types. The next three columns give, respectively, the entropies of the distributions
of stems, suffixes (including NULL), and deletion rules (including NULL) over word types. The final two columns
give, respectively, the percentage of word types occurring with the NULL suffix, and the number of non-NULL suffix
words which use a phonological deletion rule. Note that the final eight columns define the universal feature space used
by our model. BG = Bulgarian, CS = Czech, EN = English, ET = Estonian, HU = Hungarian, RO = Romanian, SL =
Slovene, SR = Serbian
(one feature for each possible stem). (2) Accord-
ing to label yz, the suffix and deletion rule are (f, d)
(one feature for every possible pair of deletion rules
and suffixes). (3) According to label yz and morpho-
logical tag z, the suffix, deletion rule, and gender
are respectively (f, d, G). (4) According to label yz
and morphological tag z, the suffix, deletion rule,
and case are (f, d, C). (5) According to label yz and
morphological tag z, the suffix, deletion rule, and
number are (f, d, N).
We train a set of linear weights on our fea-
tures using the averaged structured perceptron algo-
rithm (Collins, 2002).
</bodyText>
<sectionHeader confidence="0.999743" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999973744186047">
In this section we turn to experimental findings to
provide empirical support for our proposed frame-
work.
Corpus: To test our cross-lingual model, we ap-
ply it to a morphologically analyzed corpus of eight
languages (Erjavec, 2004). The corpus includes a
roughly 100,000 word English text, Orwell’s novel
“Nineteen Eighty Four,” and its translation into
seven languages: Bulgarian, Czech, Estonian, Hun-
garian, Romanian, Slovene, and Serbian. All the
words in the corpus are tagged with morphologi-
cal stems and a detailed morpho-syntactic analysis.
Although the texts are parallel, we note that par-
allelism is nowhere assumed nor exploited by our
model. See Table 1 for a summary of relevant cor-
pus statistics. As indicated in the table, the raw num-
ber of nominal word types varies quite a bit across
the languages, almost doubling from 4,178 (English)
to 8,051 (Hungarian). In contrast, the number of
stems appearing within these words is relatively sta-
ble across languages, ranging from a minimum of
3,112 (Bulgarian) to a maximum of 3,746 (Hungar-
ian), an increase of just 20%.
In contrast, the number of suffixes across the lan-
guages varies quite a bit. Hungarian and Esto-
nian, both Uralic languages with very complex nom-
inal morphology, use 231 and 141 nominal suffixes,
respectively. Besides English, the remaining lan-
guages employ between 21 and 32 suffixes, and En-
glish is the outlier in the other direction, with just
three nominal inflectional suffixes.
Baselines and Results: As our unsupervised
monolingual baseline, we use the Linguistica pro-
gram (Goldsmith, 2001; Goldsmith, 2005). We ap-
ply Linguistica’s default settings, and run the “suffix
prediction” option. Our model’s search procedure
closely mirrors the one used by Linguistica, with
the crucial difference that instead of attempting to
greedily minimize description length, our algorithm
instead tries to find the analysis as close as possi-
ble in the universal feature space to that of another
language.
To apply our model, we treat each of the eight
</bodyText>
<page confidence="0.995779">
327
</page>
<table confidence="0.999846416666667">
Linguistica Our Model Supervised
Nearest Neighbor Self (oracle) Avg.
Accuracy Distance Accuracy Distance Accuracy Distance
BG 68.7 84.0 (RO) 0.13 88.7 0.03 68.6 3.90 94.7
CS 60.4 82.8 (BG) 0.40 84.5 0.03 66.3 4.05 93.5
EN 81.1 75.8 (BG) 1.29 89.3 0.10 58.3 4.30 93.4
ET 51.2 66.6 (HU) 0.35 80.9 0.03 52.8 4.57 86.5
HU 64.5 69.3 (ET) 0.81 66.5 1.10 68.0 4.94 94.9
RO 65.6 71.0 (CS) 0.11 71.2 0.15 62.3 3.95 89.1
SL 61.1 82.8 (SR) 0.07 85.5 0.04 61.7 3.69 95.4
SR 64.2 79.1 (SL) 0.06 82.2 0.04 63.0 3.71 94.8
avg. 64.6 76.4 0.40 81.1 0.19 62.6 4.14 92.8
</table>
<tableCaption confidence="0.795832">
Table 2: Prediction accuracy over word types for the Linguistica baseline, our cross-lingual model, and the monolin-
gual supervised perceptron model. For our model, we provide both prediction accuracy and resulting distance to the
training language in three different scenarios: (i) Nearest Neighbor: The training languages include all seven other
languages in our data set, and the predictions with minimal distance to a training language are chosen (the nearest
neighbor is indicated in parentheses). (ii) Self (oracle): Each language is trained to minimize the distance to its own
gold-standard analysis. (iii) Average: The feature values of all seven training languages are averaged together to
create a single objective.
</tableCaption>
<bodyText confidence="0.999892314814815">
languages in turn as the test language, with the other
seven serving as training examples. For each test
language, we iterate the search procedure for each
training language (performed in parallel), until con-
vergence. The number of required iterations varies
from 6 to 36 (depending on the test-training lan-
guage pair), and each iteration takes no more than 30
seconds of run-time on a 2.4GHz Intel Xeon E5620
processor. We also consider two variants of our
method. In the first (Self (oracle)), we train each
test language to minimize the distance to its own
gold standard feature values. In the second variant
(Avg.), we average the feature values of all seven
training languages into a single objective. As a plau-
sible upper bound on performance, we implemented
the structured perceptron described in Section 3.2.
For each language, we train the perceptron on a ran-
domly selected set of 80% of the nouns, and test on
the remaining 20%.
The prediction accuracy for all models is calcu-
lated as the fraction of word types with correctly
predicted suffixes. See Table 2 for the results. For
all languages other than English (which is a mor-
phological loner in our group of languages), our
model improves over the baseline by a substantial
margin, yielding an average increase of 11.8 abso-
lute percentage points, and a reduction in error rela-
tive to the supervised upper bound of 42%. Some of
the most striking improvements are seen on Serbian
and Slovene. These languages are closely related
to one another, and indeed our model discovers that
they are each others’ nearest neighbors. By guiding
their morphological analyses towards one another,
our model achieves a 21 percentage point increase
in the case of Slovene and a 15 percentage point in-
crease in the case of Slovene.
Perhaps unsurprisingly, when each language’s
gold standard feature values are used as its own
target (Self (oracle) in Table 2), performance in-
creases even further, to an average of 81.1%. By the
same token, the resulting distance in universal fea-
ture space between training and test analyses is cut
in half under this variant, when compared to the non-
oracular nearest neighbor method. The remaining
errors may be due to limitations of the search proce-
dure (i.e. getting caught in local minima), or to the
coarseness of the feature space (i.e. incorrect analy-
ses might map to the same feature values as the cor-
rect analysis). Finally, we note that minimizing the
distance to the average feature values of the seven
training languages (Avg. in Table 2) yields subpar
performance and very large distances between be-
tween predicted analyses and target feature values
(4.14 compared to 0.40 for nearest neighbor). This
</bodyText>
<page confidence="0.99767">
328
</page>
<figureCaption confidence="0.974234333333333">
Figure 2: Locations in Feature Space of Linguistica predictions (green squares), gold standard analyses (red tri-
angles), and our model’s nearest neighbor predictions (blue circles). The original 8-dimensional feature space was
reduced to two dimensions using Multidimensional Scaling.
</figureCaption>
<figure confidence="0.99832488">
ET
RO
ET HU
HU
HU
Linguistica
Gold Standard
Our Method
SL
RO
ET
SR
SL
CS
CS
BG
SL SR
SR
RO
EN
BG
CS
BG
EN
EN
</figure>
<bodyText confidence="0.99050585106383">
result may indicate that the average feature point be-
tween training languages is simply unattainable as
an analysis of a real lexicon of nouns.
Visualizing Locations in Feature Space: Besides
assessing our method quantitatively, we can also vi-
sualize the the eight languages in universal feature
space according to (i) their gold standard analyses,
(ii) the predictions of our model and (iii) the pre-
dictions of Linguistica. To do so, we reduce the 8-
dimensional features space down to two dimensions
while preserving the distances between the predicted
and gold standard feature vectors, using Multidi-
mensional Scaling (MDS). The results of this anal-
ysis are shown in Figure 2. With the exception of
English, our model’s analyses lie closer in feature
space to their gold standard counterparts than those
of the baseline. It is interesting to note that Serbian
and Slovene, which are very similar languages, have
essentially swapped places under our model’s anal-
ysis, as have Estonian and Hungarian (both highly
inflected Uralic languages). English has (unfortu-
nately) been pulled towards Bulgarian, the second
least inflecting language in our set.
Learning Curves: We also measured the perfor-
mance of our method as a function of the number
of languages in the training set. For each target lan-
guage, we consider all possible training sets of sizes
ranging from 1 to 7 and select the predictions which
bring our test language closest in distance to one of
the languages in the set. We then average the result-
ing accuracy over all training sets of each size. Fig-
ure 3 shows the resulting learning curves averaged
over all test languages (left), as well as broken down
by test language (right). The overall trend is clear:
as additional languages are added to the training set,
test performance improves. In fact, with only one
training language, our method performs worse (on
average) than the Linguistica baseline. However,
with two or more training languages available, our
method achieves superior results.
Accuracy vs. Distance: We can gain some in-
sight into these learning curves if we consider the
relationship between accuracy (of the test language
analysis) and distance to the training language (of
the same predicted analysis). The more training lan-
guages available, the greater the chance that we can
guide our test language into very close proximity to
</bodyText>
<page confidence="0.993897">
329
</page>
<figure confidence="0.999424034482759">
0.76
0.74
0.72
0.68
0.66
0.64
0.62
0.7
Our Model
Linguistica
0.85
0.8
0.75
0.7
0.65
0.6
0.55
BG
CS
SL
SR
EN
RO
HU
ET
1 2 3 4 5 6 7
Number of training languages
1 2 3 4 5 6 7
Number of training languages
</figure>
<figureCaption confidence="0.96652275">
Figure 3: Learning curves for our model as the number of training languages increases. The figure on the left shows
the average accuracy of all eight languages for increasingly larger training sets (results are averaged over all training
sets of size 1,2,3,...). The dotted line indicates the average performance of the baseline. The figure on the right shows
similar learning curves, broken down individually for each test language (see Figure 1 for language abbreviations).
</figureCaption>
<bodyText confidence="0.980236066666667">
Accuracy (normalized)
one of them. It thus stands to reason that a strong
(negative) correlation between distance and accu-
racy would lead to increased accuracy with larger
training sets. In order to assess this correlation, we
considered all 56 test-train language pairs and col-
lected the resulting accuracy and distance for each
pair. We separately scaled accuracy and distance to
the unit interval for each test language (as some test
languages are inherently more difficult than others).
The resulting plot, shown in Figure 4, shows the ex-
pected correlation: When our test language can be
guided very closely to the training language, the re-
sulting predictions are likely to be good. If not, the
predictions are likely to be bad.
</bodyText>
<sectionHeader confidence="0.998268" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999964272727273">
The approach presented in this paper recasts mor-
phological induction as a structured prediction task.
We assume the presence of morphologically labeled
languages as training examples which guide the in-
duction process for unlabeled test languages. We
developed a novel structured nearest neighbor ap-
proach for this task, in which all languages and their
morphological analyses lie in a universal feature
space. The task of the learner is to search through
the space of morphological analyses for the test lan-
guage and return the result which lies closest to one
</bodyText>
<figure confidence="0.996919">
1
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1
Distance (normalized)
</figure>
<figureCaption confidence="0.790437142857143">
Figure 4: Accuracy vs. Distance: For all 56 possi-
ble test-train language pairs, we computed test accuracy
along with resulting distance in universal feature space
to the training language. Distance and accuracy are sep-
arately normalized to the unit interval for each test lan-
guage, and all resulting points are plotted together. A
line is fit to the points using least-squares regression.
</figureCaption>
<page confidence="0.989239">
330
</page>
<bodyText confidence="0.99994625">
of the training languages. Our empirical findings
validate this approach: On a set of eight different
languages, our method yields substantial accuracy
gains over a traditional MDL-based approach in the
task of nominal morphological induction.
One possible shortcoming of our approach is that
it assumes a uniform weighting of the cross-lingual
feature space. In fact, some features may be far more
relevant than others in guiding our test language to
an accurate analysis. In future work, we plan to in-
tegrate distance metric learning into our approach,
allowing some features to be weighted more heavily
than others. Besides potential gains in prediction ac-
curacy, this approach may shed light on deeper rela-
tionships between languages than are otherwise ap-
parent.
</bodyText>
<sectionHeader confidence="0.998585" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998407880952381">
Meni Adler and Michael Elhadad. 2006. An un-
supervised morpheme-based hmm for hebrew mor-
phological disambiguation. In Proceedings of the
ACL/CONLL, pages 665–672.
Emily M. Bender. 2009. Linguistically naïve != lan-
guage independent: why NLP needs linguistic typol-
ogy. In Proceedings of the EACL 2009 Workshop
on the Interaction between Linguistics and Compu-
tational Linguistics, pages 26–32, Morristown, NJ,
USA. Association for Computational Linguistics.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In Proceedings of the ACL,
pages 1288–1297, Uppsala, Sweden, July. Association
for Computational Linguistics.
P. Blunsom, T. Cohn, and M. Osborne. 2009. Bayesian
synchronous grammar induction. Advances in Neural
Information Processing Systems, 21:161–168.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proceedings of CoNLL.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
the NAACL/HLT.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1–8.
T. Cover and P. Hart. 1967. Nearest neighbor pattern
classification. Information Theory, IEEE Transactions
on, 13(1):21–27.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. Publications
in Computer and Information Science Report A81,
Helsinki University of Technology.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1).
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Proceed-
ings of the ACL, pages 130–137.
Sajib Dasgupta and Vincent Ng. 2007. Unsuper-
vised part-of-speech acquisition for resource-scarce
languages. In Proceedings of the EMNLP-CoNLL,
pages 218–227.
T. Erjavec. 2004. MULTEXT-East version 3: Multi-
lingual morphosyntactic specifications, lexicons and
corpora. In Fourth International Conference on Lan-
guage Resources and Evaluation, LREC, volume 4,
pages 1535–1538.
John Goldsmith. 2001. Unsupervised Learning of the
Morphology of a Natural Language. Computational
Linguistics, 27(2):153–198.
John Goldsmith. 2005. An algorithm for the unsuper-
vised learning of morphology. Technical report, Uni-
versity of Chicago.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-
lak. 2005. Bootstrapping parsers via syntactic projec-
tion across parallel texts. Journal of Natural Language
Engineering, 11(3):311–325.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguistics,
19(2):313–330.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: two unsupervised approaches. Journal of Ar-
tificial Intelligence Research, 36(1):341–385.
Sebastian Padó and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings ofACL, pages 1161 – 1168.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, NAACL ’09, pages 209–
217, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Philip Resnik and David Yarowsky. 1997. A perspective
on word sense disambiguation methods and their eval-
uation. In Proceedings of the ACL SIGLEX Workshop
</reference>
<page confidence="0.982953">
331
</page>
<reference confidence="0.995277341463415">
on Tagging Text with Lexical Semantics: Why, What,
and How?, pages 79–86.
Patrick Schone and Daniel Jurafsky. 2001. Knowledge-
free induction of inflectional morphologies. In NAACL
’01: Second meeting of the North American Chapter of
the Association for Computational Linguistics on Lan-
guage technologies 2001, pages 1–9, Morristown, NJ,
USA. Association for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008a. Cross-
lingual propagation for morphological analysis. In
Proceedings of the AAAI, pages 848–854.
Benjamin Snyder and Regina Barzilay. 2008b. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proceedings of the ACL/HLT, pages 737–
745.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2008. Unsupervised multilingual
learning for POS tagging. In Proceedings of EMNLP,
pages 1041–1050.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009a. Unsupervised multilingual grammar induction.
In Proceedings of the ACL, pages 73–81.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2009b. Adding more languages im-
proves unsupervised multilingual part-of-speech tag-
ging: a Bayesian non-parametric approach. In Pro-
ceedings of the NAACL, pages 83–91.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust pro-
jection across aligned corpora. In Proceedings of the
NAACL, pages 1–8.
David Yarowsky and Richard Wicentowski. 2000. Min-
imally supervised morphological analysis by multi-
modal alignment. In ACL ’00: Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 207–216, Morristown, NJ,
USA. Association for Computational Linguistics.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2000. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT, pages 161–168.
</reference>
<page confidence="0.998212">
332
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.582561">
<title confidence="0.998874">Universal Morphological Analysis using Structured Nearest Neighbor Prediction</title>
<author confidence="0.999049">Young-Bum Kim João V Graça Benjamin Snyder</author>
<affiliation confidence="0.896762">of Wisconsin-Madison University of Wisconsin-Madison</affiliation>
<address confidence="0.6164">Portugal</address>
<email confidence="0.955842">joao.graca@l2f.inesc-id.pt</email>
<abstract confidence="0.99970256">In this paper, we consider the problem of unsupervised morphological analysis from a new angle. Past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand. We propose instead to treat morphological analysis as a structured prediction problem, where languages with labeled data serve as training examples for unlabeled languages, without the assumption of parallel data. We define a universal morphological feature space in which every language and its morphological analysis reside. We develop a novel structured nearest neighbor prediction method which seeks to find the morphological analysis for each unlabeled language which lies as close as possible in the feature space to a training language. We apply our model to eight inflecting languages, and induce nominal morphology with substantially higher accuracy than a traditional, MDLbased approach. Our analysis indicates that accuracy continues to improve substantially as the number of training languages increases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>An unsupervised morpheme-based hmm for hebrew morphological disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/CONLL,</booktitle>
<pages>665--672</pages>
<contexts>
<context position="5910" citStr="Adler and Elhadad, 2006" startWordPosition="905" endWordPosition="908">the minimum description length principle (MDL) as its inductive bias. On average, our method increases accuracy by 11.8 percentage points, corresponding to a 42% decrease in error relative to a supervised upper bound. Further analysis indicates that accuracy improves as the number of training languages increases. 2 Related Work In this section, we briefly review prior work on unsupervised morphological induction, as well as multilingual analysis in NLP. Unsupervised Morphological Induction: Unsupervised morphology remains an active area of research (Schone and Jurafsky, 2001; Goldsmith, 2005; Adler and Elhadad, 2006; Creutz and Lagus, 2005; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Poon et al., 2009). Many existing algorithms derive morpheme lexicons by identifying recurring patterns in words. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings, resulting in a minimum description length (MDL) lexicon and corpus (Goldsmith, 2001; Goldsmith, 2005). Later work cast this idea in a probabilistic framework in which the the MDL solution is equivalent to a MAP estimate in a suitable Bayesian model (Creutz and Lagus, 2005). In all these app</context>
</contexts>
<marker>Adler, Elhadad, 2006</marker>
<rawString>Meni Adler and Michael Elhadad. 2006. An unsupervised morpheme-based hmm for hebrew morphological disambiguation. In Proceedings of the ACL/CONLL, pages 665–672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
</authors>
<title>Linguistically naïve != language independent: why NLP needs linguistic typology.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics,</booktitle>
<pages>26--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1571" citStr="Bender, 2009" startWordPosition="226" endWordPosition="227">phological analysis for each unlabeled language which lies as close as possible in the feature space to a training language. We apply our model to eight inflecting languages, and induce nominal morphology with substantially higher accuracy than a traditional, MDLbased approach. Our analysis indicates that accuracy continues to improve substantially as the number of training languages increases. 1 Introduction Over the past several decades, researchers in the natural language processing community have focused most of their efforts on developing text processing tools and techniques for English (Bender, 2009), a morphologically simple language. Recently, increasing attention has been paid to the wide variety of other languages of the world. Most of these languages still pose severe difficulties, due to (i) their lack of annotated textual data, and (ii) the fact that they exhibit linguistic structure not found in English, and are thus not immediately susceptible to many traditional NLP techniques. Consider the example of nominal part-of-speech analysis. The Penn Treebank defines only four English noun tags (Marcus et al., 1994), and as a result, it is easy to treat the words bearing these tags as c</context>
</contexts>
<marker>Bender, 2009</marker>
<rawString>Emily M. Bender. 2009. Linguistically naïve != language independent: why NLP needs linguistic typology. In Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 26–32, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Phylogenetic grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>1288--1297</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="8143" citStr="Berg-Kirkpatrick and Klein, 2010" startWordPosition="1262" endWordPosition="1265">d Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar in323 duction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any parallel text, but does assume that labeled data exists for a wide variety of languages, to be used as training examples for our test language. 3 Structured Nearest Neighbor We reformulate morphological induction as a supervised learning task, where each annotated language serves as a single training example for our languageindependent model. Each such example consists of an input-label pair (x, y), both of which contain complex internal structure: The input x E X consists of a vo</context>
</contexts>
<marker>Berg-Kirkpatrick, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylogenetic grammar induction. In Proceedings of the ACL, pages 1288–1297, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>Bayesian synchronous grammar induction.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>21--161</pages>
<contexts>
<context position="7842" citStr="Blunsom et al., 2009" startWordPosition="1210" endWordPosition="1214">d Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar in323 duction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any parallel text, but does assume that labeled data exists for a wide variety of languages, to be used as training examples for our test language. 3 Structured Nearest Neighbor We reformu</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2009</marker>
<rawString>P. Blunsom, T. Cohn, and M. Osborne. 2009. Bayesian synchronous grammar induction. Advances in Neural Information Processing Systems, 21:161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Slav Petrov</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Learning better monolingual models with unannotated bilingual text.</title>
<date>2010</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="7865" citStr="Burkett et al., 2010" startWordPosition="1215" endWordPosition="1218">se cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar in323 duction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any parallel text, but does assume that labeled data exists for a wide variety of languages, to be used as training examples for our test language. 3 Structured Nearest Neighbor We reformulate morphological indu</context>
</contexts>
<marker>Burkett, Petrov, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, Slav Petrov, John Blitzer, and Dan Klein. 2010. Learning better monolingual models with unannotated bilingual text. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL/HLT.</booktitle>
<contexts>
<context position="8108" citStr="Cohen and Smith, 2009" startWordPosition="1258" endWordPosition="1261">et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar in323 duction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any parallel text, but does assume that labeled data exists for a wide variety of languages, to be used as training examples for our test language. 3 Structured Nearest Neighbor We reformulate morphological induction as a supervised learning task, where each annotated language serves as a single training example for our languageindependent model. Each such example consists of an input-label pair (x, y), both of which contain complex internal structur</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proceedings of the NAACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="17700" citStr="Collins, 2002" startWordPosition="2952" endWordPosition="2953">itialize stage, except we now fix the set of stems T and seek w as an unsuffixed stem. As this procedure tends to to find new suffixes. produce an overly large set of suffixes F, we further 3.2 A Monolingual Supervised Model prune F down to the number of suffixes found in In order to provide a plausible upper bound on perthe training language, retaining those which appear formance, we also formulate a supervised monolinwith the largest number of stems. This initialization gual morphological model, using the structured perstage is carried out once, and afterwards the follow- ceptron framework (Collins, 2002). Here we asing three stages are repeated until convergence. sume that we are given some training sequence of inStage 1: Reanalyze each word puts and morphological analyses (all within one lanIn this stage, we reanalyze each word (in random guage): (x1, y1), (x2, y2), ... , (xn, yn). We define order). We use the set of stems T and suffixes F each input xi to be a noun w, along with a morphoobtained from the previous stage, and don’t permit logical tag z, which specifies the gender, case, and the addition of any new items to these lists. In- number of the noun. The goal is to predict the corste</context>
<context position="20955" citStr="Collins, 2002" startWordPosition="3557" endWordPosition="3558"> (one feature for each possible stem). (2) According to label yz, the suffix and deletion rule are (f, d) (one feature for every possible pair of deletion rules and suffixes). (3) According to label yz and morphological tag z, the suffix, deletion rule, and gender are respectively (f, d, G). (4) According to label yz and morphological tag z, the suffix, deletion rule, and case are (f, d, C). (5) According to label yz and morphological tag z, the suffix, deletion rule, and number are (f, d, N). We train a set of linear weights on our features using the averaged structured perceptron algorithm (Collins, 2002). 4 Experiments In this section we turn to experimental findings to provide empirical support for our proposed framework. Corpus: To test our cross-lingual model, we apply it to a morphologically analyzed corpus of eight languages (Erjavec, 2004). The corpus includes a roughly 100,000 word English text, Orwell’s novel “Nineteen Eighty Four,” and its translation into seven languages: Bulgarian, Czech, Estonian, Hungarian, Romanian, Slovene, and Serbian. All the words in the corpus are tagged with morphological stems and a detailed morpho-syntactic analysis. Although the texts are parallel, we n</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>P Hart</author>
</authors>
<title>Nearest neighbor pattern classification. Information Theory,</title>
<date>1967</date>
<journal>IEEE Transactions on,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="4457" citStr="Cover and Hart, 1967" startWordPosition="676" endWordPosition="679">gle input x, and the analysis of the nouns into stems and suffixes serves as a complex structured label y. Our first step is to define a universal morphological feature space, into which each language and its morphological analysis can be mapped. We opt for a simple and intuitive mapping, which measures the sizes of the stem and suffix lexicons, the entropy of these lexicons, and the fraction of word forms which appear without any inflection. Because languages tend to cluster into well defined morphological groups, we cast our learning and prediction problem in the nearest neighbor framework (Cover and Hart, 1967). In contrast to its typical use in classification problems, where one can simply pick the label of the nearest training example, we are here faced with a structured prediction problem, where locations in feature space depend jointly on the input-label pair (x, y). Finding a nearest neighbor thus consists of searching over the space of morphological analyses, until a point in feature space is reached which lies closest to one of the labeled languages. See Figure 1 for an illustration. To provide a measure of empirical validation, we applied our approach to eight languages with inflectional nom</context>
</contexts>
<marker>Cover, Hart, 1967</marker>
<rawString>T. Cover and P. Hart. 1967. Nearest neighbor pattern classification. Information Theory, IEEE Transactions on, 13(1):21–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised morpheme segmentation and morphology induction from text corpora using morfessor 1.0.</title>
<date>2005</date>
<booktitle>Publications in Computer and Information Science Report A81,</booktitle>
<institution>Helsinki University of Technology.</institution>
<contexts>
<context position="5934" citStr="Creutz and Lagus, 2005" startWordPosition="909" endWordPosition="913">ength principle (MDL) as its inductive bias. On average, our method increases accuracy by 11.8 percentage points, corresponding to a 42% decrease in error relative to a supervised upper bound. Further analysis indicates that accuracy improves as the number of training languages increases. 2 Related Work In this section, we briefly review prior work on unsupervised morphological induction, as well as multilingual analysis in NLP. Unsupervised Morphological Induction: Unsupervised morphology remains an active area of research (Schone and Jurafsky, 2001; Goldsmith, 2005; Adler and Elhadad, 2006; Creutz and Lagus, 2005; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Poon et al., 2009). Many existing algorithms derive morpheme lexicons by identifying recurring patterns in words. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings, resulting in a minimum description length (MDL) lexicon and corpus (Goldsmith, 2001; Goldsmith, 2005). Later work cast this idea in a probabilistic framework in which the the MDL solution is equivalent to a MAP estimate in a suitable Bayesian model (Creutz and Lagus, 2005). In all these approaches, a locally optim</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>Mathias Creutz and Krista Lagus. 2005. Unsupervised morpheme segmentation and morphology induction from text corpora using morfessor 1.0. Publications in Computer and Information Science Report A81, Helsinki University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="5981" citStr="Creutz and Lagus, 2007" startWordPosition="918" endWordPosition="922"> average, our method increases accuracy by 11.8 percentage points, corresponding to a 42% decrease in error relative to a supervised upper bound. Further analysis indicates that accuracy improves as the number of training languages increases. 2 Related Work In this section, we briefly review prior work on unsupervised morphological induction, as well as multilingual analysis in NLP. Unsupervised Morphological Induction: Unsupervised morphology remains an active area of research (Schone and Jurafsky, 2001; Goldsmith, 2005; Adler and Elhadad, 2006; Creutz and Lagus, 2005; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Poon et al., 2009). Many existing algorithms derive morpheme lexicons by identifying recurring patterns in words. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings, resulting in a minimum description length (MDL) lexicon and corpus (Goldsmith, 2001; Goldsmith, 2005). Later work cast this idea in a probabilistic framework in which the the MDL solution is equivalent to a MAP estimate in a suitable Bayesian model (Creutz and Lagus, 2005). In all these approaches, a locally optimal segmentation is identified using a task-spec</context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and Language Processing, 4(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
<author>Ulrike Schwall</author>
</authors>
<title>Two languages are more informative than one.</title>
<date>1991</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>130--137</pages>
<contexts>
<context position="7499" citStr="Dagan et al., 1991" startWordPosition="1156" endWordPosition="1159">nd his collaborators first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar in323 duction (Cohen and Sm</context>
</contexts>
<marker>Dagan, Itai, Schwall, 1991</marker>
<rawString>Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two languages are more informative than one. In Proceedings of the ACL, pages 130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajib Dasgupta</author>
<author>Vincent Ng</author>
</authors>
<title>Unsupervised part-of-speech acquisition for resource-scarce languages.</title>
<date>2007</date>
<booktitle>In Proceedings of the EMNLP-CoNLL,</booktitle>
<pages>218--227</pages>
<contexts>
<context position="5957" citStr="Dasgupta and Ng, 2007" startWordPosition="914" endWordPosition="917"> its inductive bias. On average, our method increases accuracy by 11.8 percentage points, corresponding to a 42% decrease in error relative to a supervised upper bound. Further analysis indicates that accuracy improves as the number of training languages increases. 2 Related Work In this section, we briefly review prior work on unsupervised morphological induction, as well as multilingual analysis in NLP. Unsupervised Morphological Induction: Unsupervised morphology remains an active area of research (Schone and Jurafsky, 2001; Goldsmith, 2005; Adler and Elhadad, 2006; Creutz and Lagus, 2005; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Poon et al., 2009). Many existing algorithms derive morpheme lexicons by identifying recurring patterns in words. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings, resulting in a minimum description length (MDL) lexicon and corpus (Goldsmith, 2001; Goldsmith, 2005). Later work cast this idea in a probabilistic framework in which the the MDL solution is equivalent to a MAP estimate in a suitable Bayesian model (Creutz and Lagus, 2005). In all these approaches, a locally optimal segmentation is iden</context>
</contexts>
<marker>Dasgupta, Ng, 2007</marker>
<rawString>Sajib Dasgupta and Vincent Ng. 2007. Unsupervised part-of-speech acquisition for resource-scarce languages. In Proceedings of the EMNLP-CoNLL, pages 218–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Erjavec</author>
</authors>
<title>MULTEXT-East version 3: Multilingual morphosyntactic specifications, lexicons and corpora.</title>
<date>2004</date>
<booktitle>In Fourth International Conference on Language Resources and Evaluation, LREC,</booktitle>
<volume>4</volume>
<pages>1535--1538</pages>
<contexts>
<context position="2340" citStr="Erjavec, 2004" startWordPosition="351" endWordPosition="352">ges still pose severe difficulties, due to (i) their lack of annotated textual data, and (ii) the fact that they exhibit linguistic structure not found in English, and are thus not immediately susceptible to many traditional NLP techniques. Consider the example of nominal part-of-speech analysis. The Penn Treebank defines only four English noun tags (Marcus et al., 1994), and as a result, it is easy to treat the words bearing these tags as completely distinct word classes, with no internal morphological structure. In contrast, a comparable tagset for Hungarian includes 154 distinct noun tags (Erjavec, 2004), reflecting Hungarian’s rich inflectional morphology. When dealing with such languages, treating words as atoms leads to severe data sparsity problems. Because annotated resources do not exist for most morphologically rich languages, prior research has focused on unsupervised methods, with a focus on developing appropriate inductive biases. However, inductive biases and declarative knowledge are notoriously difficult to encode in well-founded models. Even putting aside this practical matter, a universally correct inductive bias, if there is one, is unlikely to be be discovered by a priori rea</context>
<context position="21201" citStr="Erjavec, 2004" startWordPosition="3596" endWordPosition="3597">n rule, and gender are respectively (f, d, G). (4) According to label yz and morphological tag z, the suffix, deletion rule, and case are (f, d, C). (5) According to label yz and morphological tag z, the suffix, deletion rule, and number are (f, d, N). We train a set of linear weights on our features using the averaged structured perceptron algorithm (Collins, 2002). 4 Experiments In this section we turn to experimental findings to provide empirical support for our proposed framework. Corpus: To test our cross-lingual model, we apply it to a morphologically analyzed corpus of eight languages (Erjavec, 2004). The corpus includes a roughly 100,000 word English text, Orwell’s novel “Nineteen Eighty Four,” and its translation into seven languages: Bulgarian, Czech, Estonian, Hungarian, Romanian, Slovene, and Serbian. All the words in the corpus are tagged with morphological stems and a detailed morpho-syntactic analysis. Although the texts are parallel, we note that parallelism is nowhere assumed nor exploited by our model. See Table 1 for a summary of relevant corpus statistics. As indicated in the table, the raw number of nominal word types varies quite a bit across the languages, almost doubling </context>
</contexts>
<marker>Erjavec, 2004</marker>
<rawString>T. Erjavec. 2004. MULTEXT-East version 3: Multilingual morphosyntactic specifications, lexicons and corpora. In Fourth International Conference on Language Resources and Evaluation, LREC, volume 4, pages 1535–1538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised Learning of the Morphology of a Natural Language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="6302" citStr="Goldsmith, 2001" startWordPosition="971" endWordPosition="972">gical induction, as well as multilingual analysis in NLP. Unsupervised Morphological Induction: Unsupervised morphology remains an active area of research (Schone and Jurafsky, 2001; Goldsmith, 2005; Adler and Elhadad, 2006; Creutz and Lagus, 2005; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Poon et al., 2009). Many existing algorithms derive morpheme lexicons by identifying recurring patterns in words. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings, resulting in a minimum description length (MDL) lexicon and corpus (Goldsmith, 2001; Goldsmith, 2005). Later work cast this idea in a probabilistic framework in which the the MDL solution is equivalent to a MAP estimate in a suitable Bayesian model (Creutz and Lagus, 2005). In all these approaches, a locally optimal segmentation is identified using a task-specific greedy search. Multilingual Analysis: An influential line of prior multilingual work starts with the observation that rich linguistic resources exist for some languages but not others. The idea then is to project linguistic information from one language onto others via parallel data. Yarowsky and his collaborators </context>
<context position="22544" citStr="Goldsmith, 2001" startWordPosition="3812" endWordPosition="3813">ross languages, ranging from a minimum of 3,112 (Bulgarian) to a maximum of 3,746 (Hungarian), an increase of just 20%. In contrast, the number of suffixes across the languages varies quite a bit. Hungarian and Estonian, both Uralic languages with very complex nominal morphology, use 231 and 141 nominal suffixes, respectively. Besides English, the remaining languages employ between 21 and 32 suffixes, and English is the outlier in the other direction, with just three nominal inflectional suffixes. Baselines and Results: As our unsupervised monolingual baseline, we use the Linguistica program (Goldsmith, 2001; Goldsmith, 2005). We apply Linguistica’s default settings, and run the “suffix prediction” option. Our model’s search procedure closely mirrors the one used by Linguistica, with the crucial difference that instead of attempting to greedily minimize description length, our algorithm instead tries to find the analysis as close as possible in the universal feature space to that of another language. To apply our model, we treat each of the eight 327 Linguistica Our Model Supervised Nearest Neighbor Self (oracle) Avg. Accuracy Distance Accuracy Distance Accuracy Distance BG 68.7 84.0 (RO) 0.13 88</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John Goldsmith. 2001. Unsupervised Learning of the Morphology of a Natural Language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>An algorithm for the unsupervised learning of morphology.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>University of Chicago.</institution>
<contexts>
<context position="5274" citStr="Goldsmith, 2005" startWordPosition="811" endWordPosition="812"> in feature space depend jointly on the input-label pair (x, y). Finding a nearest neighbor thus consists of searching over the space of morphological analyses, until a point in feature space is reached which lies closest to one of the labeled languages. See Figure 1 for an illustration. To provide a measure of empirical validation, we applied our approach to eight languages with inflectional nominal morphology, ranging in complexity from very simple (English) to very complex (Hungarian). In all but one case, our approach yields substantial improvements over a comparable monolingual baseline (Goldsmith, 2005), which uses the minimum description length principle (MDL) as its inductive bias. On average, our method increases accuracy by 11.8 percentage points, corresponding to a 42% decrease in error relative to a supervised upper bound. Further analysis indicates that accuracy improves as the number of training languages increases. 2 Related Work In this section, we briefly review prior work on unsupervised morphological induction, as well as multilingual analysis in NLP. Unsupervised Morphological Induction: Unsupervised morphology remains an active area of research (Schone and Jurafsky, 2001; Gold</context>
<context position="15488" citStr="Goldsmith, 2005" startWordPosition="2561" endWordPosition="2562">e: y(`) Pz argmin II f(x, y) − f(x`, y`) II . y∈Y We finally select from amongst these analyses and 325 make our prediction: suffix pair (t, f), for which f ∈ F, and where ei`* = argmin k f(x,y(`)) − f(x`,y`) k ther t ∈ T or some t&apos; ∈ T such that t is obtained ` from t&apos; using a deletion rule d (e.g. by deleting a y* = y(`∗) final or penultimate vowel). For each such possiThe main outline of our search algorithm is based ble analysis y&apos;, we compute the resulting location on the MDL-based greedy search heuristic devel- in feature space f(x, y&apos;), and select the analysis that oped and studied by (Goldsmith, 2005). At a high brings us closest to our target training language: level, this search procedure alternates between indi- y = argminy, k f(x, y&apos;) − f(x`, y`) k . vidual analyses of words (keeping the set of stems Stage 2: Find New Stems and suffixes fixed), aggregate discoveries of new In this stage, we keep our set of suffixes F and stems (keeping the suffixes fixed), and aggregate dis- deletion rules D from the previous stage fixed, and coveries of new suffixes (keeping stems fixed). As attempt to find new stems to add to T through an aginput, we consider the test words x in our new lan- gregate </context>
<context position="22562" citStr="Goldsmith, 2005" startWordPosition="3814" endWordPosition="3815">anging from a minimum of 3,112 (Bulgarian) to a maximum of 3,746 (Hungarian), an increase of just 20%. In contrast, the number of suffixes across the languages varies quite a bit. Hungarian and Estonian, both Uralic languages with very complex nominal morphology, use 231 and 141 nominal suffixes, respectively. Besides English, the remaining languages employ between 21 and 32 suffixes, and English is the outlier in the other direction, with just three nominal inflectional suffixes. Baselines and Results: As our unsupervised monolingual baseline, we use the Linguistica program (Goldsmith, 2001; Goldsmith, 2005). We apply Linguistica’s default settings, and run the “suffix prediction” option. Our model’s search procedure closely mirrors the one used by Linguistica, with the crucial difference that instead of attempting to greedily minimize description length, our algorithm instead tries to find the analysis as close as possible in the universal feature space to that of another language. To apply our model, we treat each of the eight 327 Linguistica Our Model Supervised Nearest Neighbor Self (oracle) Avg. Accuracy Distance Accuracy Distance Accuracy Distance BG 68.7 84.0 (RO) 0.13 88.7 0.03 68.6 3.90 </context>
</contexts>
<marker>Goldsmith, 2005</marker>
<rawString>John Goldsmith. 2005. An algorithm for the unsupervised learning of morphology. Technical report, University of Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
<author>P Resnik</author>
<author>A Weinberg</author>
<author>C Cabezas</author>
<author>O Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="7213" citStr="Hwa et al., 2005" startWordPosition="1108" endWordPosition="1111">ch. Multilingual Analysis: An influential line of prior multilingual work starts with the observation that rich linguistic resources exist for some languages but not others. The idea then is to project linguistic information from one language onto others via parallel data. Yarowsky and his collaborators first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al.</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Journal of Natural Language Engineering, 11(3):311–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational linguistics,</title>
<date>1994</date>
<contexts>
<context position="2099" citStr="Marcus et al., 1994" startWordPosition="308" endWordPosition="311"> their efforts on developing text processing tools and techniques for English (Bender, 2009), a morphologically simple language. Recently, increasing attention has been paid to the wide variety of other languages of the world. Most of these languages still pose severe difficulties, due to (i) their lack of annotated textual data, and (ii) the fact that they exhibit linguistic structure not found in English, and are thus not immediately susceptible to many traditional NLP techniques. Consider the example of nominal part-of-speech analysis. The Penn Treebank defines only four English noun tags (Marcus et al., 1994), and as a result, it is easy to treat the words bearing these tags as completely distinct word classes, with no internal morphological structure. In contrast, a comparable tagset for Hungarian includes 154 distinct noun tags (Erjavec, 2004), reflecting Hungarian’s rich inflectional morphology. When dealing with such languages, treating words as atoms leads to severe data sparsity problems. Because annotated resources do not exist for most morphologically rich languages, prior research has focused on unsupervised methods, with a focus on developing appropriate inductive biases. However, induct</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Benjamin Snyder</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Multilingual part-of-speech tagging: two unsupervised approaches.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="7775" citStr="Naseem et al., 2009" startWordPosition="1199" endWordPosition="1202"> idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar in323 duction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any parallel text, but does assume that labeled data exists for a wide variety of languages, to be used as training examp</context>
</contexts>
<marker>Naseem, Snyder, Eisenstein, Barzilay, 2009</marker>
<rawString>Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and Regina Barzilay. 2009. Multilingual part-of-speech tagging: two unsupervised approaches. Journal of Artificial Intelligence Research, 36(1):341–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Padó</author>
<author>Mirella Lapata</author>
</authors>
<title>Optimal constituent alignment with edge covers for semantic projection.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>1161--1168</pages>
<contexts>
<context position="7237" citStr="Padó and Lapata, 2006" startWordPosition="1112" endWordPosition="1115">nalysis: An influential line of prior multilingual work starts with the observation that rich linguistic resources exist for some languages but not others. The idea then is to project linguistic information from one language onto others via parallel data. Yarowsky and his collaborators first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al.,</context>
</contexts>
<marker>Padó, Lapata, 2006</marker>
<rawString>Sebastian Padó and Mirella Lapata. 2006. Optimal constituent alignment with edge covers for semantic projection. In Proceedings ofACL, pages 1161 – 1168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>209--217</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6001" citStr="Poon et al., 2009" startWordPosition="923" endWordPosition="926">reases accuracy by 11.8 percentage points, corresponding to a 42% decrease in error relative to a supervised upper bound. Further analysis indicates that accuracy improves as the number of training languages increases. 2 Related Work In this section, we briefly review prior work on unsupervised morphological induction, as well as multilingual analysis in NLP. Unsupervised Morphological Induction: Unsupervised morphology remains an active area of research (Schone and Jurafsky, 2001; Goldsmith, 2005; Adler and Elhadad, 2006; Creutz and Lagus, 2005; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Poon et al., 2009). Many existing algorithms derive morpheme lexicons by identifying recurring patterns in words. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings, resulting in a minimum description length (MDL) lexicon and corpus (Goldsmith, 2001; Goldsmith, 2005). Later work cast this idea in a probabilistic framework in which the the MDL solution is equivalent to a MAP estimate in a suitable Bayesian model (Creutz and Lagus, 2005). In all these approaches, a locally optimal segmentation is identified using a task-specific greedy search. </context>
</contexts>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>Hoifung Poon, Colin Cherry, and Kristina Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 209– 217, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>David Yarowsky</author>
</authors>
<title>A perspective on word sense disambiguation methods and their evaluation.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How?,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="7527" citStr="Resnik and Yarowsky, 1997" startWordPosition="1160" endWordPosition="1163"> first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar in323 duction (Cohen and Smith, 2009; Berg-Kirkpatrick </context>
</contexts>
<marker>Resnik, Yarowsky, 1997</marker>
<rawString>Philip Resnik and David Yarowsky. 1997. A perspective on word sense disambiguation methods and their evaluation. In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How?, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Knowledgefree induction of inflectional morphologies.</title>
<date>2001</date>
<booktitle>In NAACL ’01: Second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5868" citStr="Schone and Jurafsky, 2001" startWordPosition="899" endWordPosition="902">gual baseline (Goldsmith, 2005), which uses the minimum description length principle (MDL) as its inductive bias. On average, our method increases accuracy by 11.8 percentage points, corresponding to a 42% decrease in error relative to a supervised upper bound. Further analysis indicates that accuracy improves as the number of training languages increases. 2 Related Work In this section, we briefly review prior work on unsupervised morphological induction, as well as multilingual analysis in NLP. Unsupervised Morphological Induction: Unsupervised morphology remains an active area of research (Schone and Jurafsky, 2001; Goldsmith, 2005; Adler and Elhadad, 2006; Creutz and Lagus, 2005; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Poon et al., 2009). Many existing algorithms derive morpheme lexicons by identifying recurring patterns in words. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings, resulting in a minimum description length (MDL) lexicon and corpus (Goldsmith, 2001; Goldsmith, 2005). Later work cast this idea in a probabilistic framework in which the the MDL solution is equivalent to a MAP estimate in a suitable Bayesian model </context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>Patrick Schone and Daniel Jurafsky. 2001. Knowledgefree induction of inflectional morphologies. In NAACL ’01: Second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies 2001, pages 1–9, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Crosslingual propagation for morphological analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the AAAI,</booktitle>
<pages>848--854</pages>
<contexts>
<context position="7654" citStr="Snyder and Barzilay, 2008" startWordPosition="1180" endWordPosition="1183">ction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar in323 duction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of </context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008a. Crosslingual propagation for morphological analysis. In Proceedings of the AAAI, pages 848–854.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL/HLT,</booktitle>
<pages>737--745</pages>
<contexts>
<context position="7654" citStr="Snyder and Barzilay, 2008" startWordPosition="1180" endWordPosition="1183">ction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar in323 duction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of </context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008b. Unsupervised multilingual learning for morphological segmentation. In Proceedings of the ACL/HLT, pages 737– 745.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for POS tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1041--1050</pages>
<contexts>
<context position="7731" citStr="Snyder et al., 2008" startWordPosition="1191" endWordPosition="1194">01), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar in323 duction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any parallel text, but does assume that labeled data exists for a wide variet</context>
</contexts>
<marker>Snyder, Naseem, Eisenstein, Barzilay, 2008</marker>
<rawString>Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and Regina Barzilay. 2008. Unsupervised multilingual learning for POS tagging. In Proceedings of EMNLP, pages 1041–1050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>73--81</pages>
<contexts>
<context position="7752" citStr="Snyder et al., 2009" startWordPosition="1195" endWordPosition="1198">chers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar in323 duction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any parallel text, but does assume that labeled data exists for a wide variety of languages, to be</context>
</contexts>
<marker>Snyder, Naseem, Barzilay, 2009</marker>
<rawString>Benjamin Snyder, Tahira Naseem, and Regina Barzilay. 2009a. Unsupervised multilingual grammar induction. In Proceedings of the ACL, pages 73–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Adding more languages improves unsupervised multilingual part-of-speech tagging: a Bayesian non-parametric approach.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL,</booktitle>
<pages>83--91</pages>
<contexts>
<context position="7752" citStr="Snyder et al., 2009" startWordPosition="1195" endWordPosition="1198">chers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar in323 duction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any parallel text, but does assume that labeled data exists for a wide variety of languages, to be</context>
</contexts>
<marker>Snyder, Naseem, Eisenstein, Barzilay, 2009</marker>
<rawString>Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and Regina Barzilay. 2009b. Adding more languages improves unsupervised multilingual part-of-speech tagging: a Bayesian non-parametric approach. In Proceedings of the NAACL, pages 83–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="7115" citStr="Yarowsky and Ngai, 2001" startWordPosition="1092" endWordPosition="1095">). In all these approaches, a locally optimal segmentation is identified using a task-specific greedy search. Multilingual Analysis: An influential line of prior multilingual work starts with the observation that rich linguistic resources exist for some languages but not others. The idea then is to project linguistic information from one language onto others via parallel data. Yarowsky and his collaborators first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Sny</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora. In Proceedings of the NAACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Richard Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>In ACL ’00: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>207--216</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7066" citStr="Yarowsky and Wicentowski, 2000" startWordPosition="1084" endWordPosition="1087">te in a suitable Bayesian model (Creutz and Lagus, 2005). In all these approaches, a locally optimal segmentation is identified using a task-specific greedy search. Multilingual Analysis: An influential line of prior multilingual work starts with the observation that rich linguistic resources exist for some languages but not others. The idea then is to project linguistic information from one language onto others via parallel data. Yarowsky and his collaborators first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder an</context>
</contexts>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>David Yarowsky and Richard Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In ACL ’00: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 207–216, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of HLT,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="7089" citStr="Yarowsky et al., 2000" startWordPosition="1088" endWordPosition="1091">(Creutz and Lagus, 2005). In all these approaches, a locally optimal segmentation is identified using a task-specific greedy search. Multilingual Analysis: An influential line of prior multilingual work starts with the observation that rich linguistic resources exist for some languages but not others. The idea then is to project linguistic information from one language onto others via parallel data. Yarowsky and his collaborators first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), par</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2000</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2000. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of HLT, pages 161–168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>