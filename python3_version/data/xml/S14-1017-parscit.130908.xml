<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000119">
<title confidence="0.9974695">
Using Text Segmentation Algorithms for the Automatic Generation of
E-Learning Courses
</title>
<author confidence="0.913028">
Can Beck, Alexander Streicher and Andrea Zielinski
</author>
<affiliation confidence="0.614586">
Fraunhofer IOSB
Karlsruhe, Germany
</affiliation>
<email confidence="0.7329175">
{can.beck, alexander.streicher,
andrea.zielinski}@iosb.fraunhofer.de
</email>
<sectionHeader confidence="0.994641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999991347826087">
With the advent of e-learning, there is a
strong demand for tools that help to cre-
ate e-learning courses in an automatic or
semi-automatic way. While resources for
new courses are often freely available,
they are generally not properly structured
into easy to handle units. In this paper,
we investigate how state of the art text
segmentation algorithms can be applied
to automatically transform unstructured
text into coherent pieces appropriate for
e-learning courses. The feasibility to
course generation is validated on a test
corpus specifically tailored to this scenar-
io. We also introduce a more generic
training and testing method for text seg-
mentation algorithms based on a Latent
Dirichlet Allocation (LDA) topic model.
In addition we introduce a scalable ran-
dom text segmentation algorithm, in or-
der to establish lower and upper bounds
to be able to evaluate segmentation re-
sults on a common basis.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.983643767441861">
The creation of e-learning courses is generally a
time consuming effort. However, separating text
into topically cohesive segments can help to re-
duce this effort whenever textual content is al-
ready available but not properly structured ac-
cording to e-learning standards. Since these seg-
ments textually describe the content of learning
units, automatic pedagogical annotation algo-
rithms could be applied to categorize them into
introductions, descriptions, explanations, exam-
ples and other pedagogical meaningful concepts
(K.Sathiyamurthy &amp; T.V.Geetha, 2011).
Course designers generally assume that learn-
ing content is composed of small inseparable
learning objects at the micro level which in turn
are wrapped into Concept Containers (CCs) at
the macro level. This approach is followed, e.g.,
in the Web-Didactic approach by Swertz et al.
(2013) where CCs correspond to chapters in a
book and Knowledge Objects (KOs) correspond
to course pages. To automate the partition of an
unstructured text source into appropriate seg-
ments for the macro and micro level we applied
different text segmentation algorithms (segment-
ers) on each level.
To evaluate the segmenters in the described
scenario, we created a test corpus based on fea-
tured Wikipedia articles. For the macro level we
exploit sections from different articles and the
corresponding micro structure consists of subse-
quent paragraphs from these sections. On the
macro level the segmenter TopicTiling (TT) by
Riedl and Biemann (2012) is used. It is based on
a LDA topic model which we train based on the
articles from Wikipedia to extract a predefined
number of different topics. On the micro level,
the segmenter BayesSeg (BS) is applied
(Eisenstein &amp; Barzilay, 2008).
We achieved overall good results measured in
three different metrics over a baseline approach,
i.e., a scalable random segmenter, that indicate
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.934261">
132
</page>
<note confidence="0.9810715">
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 132â€“140,
Dublin, Ireland, August 23-24 2014.
</note>
<bodyText confidence="0.999563583333333">
text segmentation algorithms are ready to be ap-
plied to facilitate the creation of e-learning
courses.
This paper is organized as follows: Section 2
gives an overview of related work on automatic
course generation as well as text segmentation
applications. In the main sections 3 and 4 we de-
scribe our approach and evaluation results on our
corpus. In the last section we summarize the pre-
sented findings and give an outlook on further
research needed for the automatic generation of
e-learning courses.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999960267605634">
Automatic course generation can roughly be di-
vided into two different areas. One is concerned
with generation from existing courses and is
mainly focused on adaption to the learner or in-
structional plans see Lin et al. (2009), Capuno et
al. (2009) and Tan et al. (2010). The other area is
the course creation itself on which we focus on
in this paper.
Since the publication of the segmenter Text-
Tiling by Hearst (1997) at least a dozen different
segmenters have been developed. They can be
divided into linear and hierarchical segmenters.
Linear segmenters process the text sequentially
sentence by sentence. Hierarchical segmenters
first process the whole text and extract topics
with varying granularities. These topics are then
agglomerated based on a predefined criterion.
Linear segmenters have been developed by
Kan et al. (1998) and Galley et al. (2003). One of
the first probabilistic algorithms has been intro-
duced by Utiyama and Isahara (2001). LDA
based approaches were first described by Sun et
al. (2008) and improved by Misra et al. (2009).
The newest LDA based segmenter is TT. It per-
forms linear text segmentation based on a pre-
trained LDA topic model and calculates the simi-
larity between segments (adjacent sentences) to
measure text coherence on the basis of a topic
vector representation using cosine similarity. For
reasons of efficiency, only the most frequent top-
ic ID is assigned to each word in the sentence,
using Gibbs sampling.
Hierarchical text segmentation algorithms
were first introduced by Yaari (1997). The latest
approach by Eisenstein (2008) uses a generative
Bayesian model BS for text segmentation, as-
suming that a) topic shifts are likely to occur at
points marked by cue phrases and b) a linear dis-
course structure. Each sentence in the document
is modeled by a language model associated with
a segment. The algorithm then calculates the
maximum likelihood estimates of observing the
whole sequence of sentences at selected topic
boundaries.
The applications of text segmentation algo-
rithms range from information retrieval (Huang,
et al., 2002) to topic tracking and segmentation
of multi-party conversations (Galley, et al.,
2003).
Similar to our work Sathiyamurthy and Geetha
(2011) showed how LDA based text segmenta-
tion algorithms combined with hierarchical do-
main ontology and pedagogical ontology can be
applied to content generation for e-learning
courses. They focussed on the segmentation of
existing e-learning material in the domain of
computer science and introduced new metrics to
measure the segmentation results with respect to
concepts from the ontologies. Our work focusses
on the appropriate segmentation of unstructured
text instead of existing e-learning material. Alt-
hough the usage of domain models is an interest-
ing approach the availability of such models is
very domain dependent. We rely on the LDA
model parameters and training to accomplish a
word to topic assignment.
Rather than introducing new aspects such as
pedagogical concepts we investigated the general
usability of segmentation algorithms with focus
on the macro and micro structure which is char-
acteristic for most e-learning content.
</bodyText>
<sectionHeader confidence="0.9358945" genericHeader="method">
3 Automatic Generation of E-Learning
Courses
</sectionHeader>
<bodyText confidence="0.999985842105263">
The main objective is to provide e-learning
course designers with a tool to efficiently organ-
ize existing textual content for new e-learning
courses. This can be done by the application of
text segmenters that automatically generate the
basic structure of the course. The intended web-
didactic conform two-level structure differenti-
ates between macro and micro levels. The levels
have different requirements with respect to the-
matic coherence: the CCs are thematically rather
independent and the KOs within each CC need to
be intrinsically coherent but still separable.
We chose the linear LDA-based segmenter TT
to find the boundaries between CCs. The LDA-
based topic model can be trained on content
which is topically related to the target course.
This approach gives the course creator flexibility
in the generation of the macro level structure by
either adjusting the training documents or by
</bodyText>
<page confidence="0.998513">
133
</page>
<bodyText confidence="0.999886769230769">
changing the number and size of topics that
should be extracted for the topic model.
On the micro level we did not use TT. The
training of an appropriate LDA model would
have to be done for every CC separately since
they are thematically relatively unrelated. Apart
from that the boundaries between the KOs
should be an optimal division for a given number
of expected boundaries. The reason for this is
that the length of KOs should be adapted to the
intended skill and background of the learners.
This is why we decided to use the hierarchical
segmenter BS.
</bodyText>
<subsectionHeader confidence="0.999979">
3.1 Application Setting and Corpus
</subsectionHeader>
<bodyText confidence="0.999996486486487">
To evaluate segmenters many different corpora
have been created. The most commonly used
corpus was introduced by Choi (2000). It is
based on the Brown Corpus and contains 700
samples, each containing a fixed number of sen-
tences from 10 different news texts, which are
randomly chosen from the Brown Corpus. Two
other widely tested corpora were introduced by
Galley et al. (2003). Both contain 500 samples,
one with concatenated texts from the Wall Street
Journal (WSJ) and the other with concatenated
texts from the Topic Detection and Tracking
(TDT) corpus (Strassel, et al., 2000). A standard
for the segmentation of speech is the corpus from
the International Computer Science Institute
(ICSI) by Janin et al. (2003). A medical text
book has been used by Eisenstein and Barzilay
(2008). The approaches to evaluate segmenters
are always similar: they have to find the bounda-
ries in artificially concatenated texts.
We developed our own dataset because we
wanted to use text that potentially could be used
as a basis for creating e-learning courses. We
therefore need samples which, on the one hand,
have relatively clear topic boundaries on the
macro level and, on the other hand resemble the
differences in number of topics and inter-topic
cohesion on the micro level.
We based our corpus on 530 featured1 articles
from 6 different categories of the English Wik-
ipedia. It can be assumed that Wikipedia articles
are often the source for learning courses. We
used featured articles because the content struc-
ture is very consistent and clear, i.e., sections and
paragraphs are well defined.
The corpus is divided into a macro and micro
dataset in the following manner: The macro da-
</bodyText>
<footnote confidence="0.9386285">
1http://en.wikipedia.org/wiki/Wikipedia:Featured_arti
cles
</footnote>
<bodyText confidence="0.999591578947368">
taset contains 1200 samples. Each sample is a
concatenation of paragraphs from 6-8 different
sections from featured articles. Each topic in a
sample consists of 3-6 subsequent paragraphs
from a randomly selected section. We propose
that one paragraph describes one KO. One CC
contains all KOs which are from the same sec-
tion in the article. Thus, one sample from the
macro dataset contains 6-8 CCs, each containing
3-6 KOs. The segmentation task is to find the
topic boundaries between the CCs. The macro
dataset is quite similar in structure to the Choi-
Corpus.
The micro dataset is extracted from the macro
dataset. It contains 8231 samples, where each
sample contains all KOs from one CC of the
macro dataset. The segmentation task is to find
the topic boundaries between the KOs, i.e, sub-
sequent paragraphs of one section, see Figure 1.
</bodyText>
<figureCaption confidence="0.70364">
Figure 1: Schema for corpus samples: left and
</figureCaption>
<bodyText confidence="0.950100380952381">
right Wikipedia articles with sections and para-
graphs, in the middle three samples, dashed rec-
tangle is a macro sample and dashed circles are
micro samples. Filled squares indicate topic
boundaries in the macro sample and filled circles
in the micro samples.
All texts in our corpus are stemmed and stop-
words are removed with the NLP-Toolkit for
Python (Bird, et al., 2009) using an adapted vari-
ant2 of the keyword extraction method by Kim et
al. (2013).
The macro and micro dataset themselves are
divided into multiple subsets to evaluate the sta-
bility of the segmenters when the number of sen-
tences per topic or the number of topics per sam-
ple have changed. The detailed configuration is
shown in Table 1 and 2. Each subset is identified
by the number of CCs per sample and the num-
ber of KOs per CC (the subset is denoted as
#CC_#KO). Subsets of the micro dataset are
identified by a single value which is the number
</bodyText>
<footnote confidence="0.930773">
2 https://gist.github.com/alexbowe/879414
</footnote>
<page confidence="0.997073">
134
</page>
<bodyText confidence="0.99980425">
of KOs per sample (#KO). In Table 1 the identi-
fier R means that the number of CCs or KOs is
not the same for all samples, it is chosen random-
ly from the set depicted by curly brackets.
</bodyText>
<table confidence="0.777771222222222">
ID CCs per KOs per mean sen-
sample CC tences per
CC
7_3 7 3 20
7_4 7 4 27
7_5 7 5 33
7_6 7 6 40
7_R 7 {3,4,5,6} 30
R_R {6,7,8} {3,4,5,6} 30
</table>
<tableCaption confidence="0.910576">
Table 1: Macro dataset and its subsets each with
200 samples.
</tableCaption>
<figure confidence="0.996231166666667">
ID KOs per sam- mean sentences per
ple KO
3 3 9
4 4 8
5 5 7
6 6 7
</figure>
<tableCaption confidence="0.968416">
Table 2: Micro dataset and its subsets.
</tableCaption>
<bodyText confidence="0.9999426875">
The important difference between the macro and
micro dataset is that every subset of the macro
dataset contains a constant number of topics
which differ in number of sentences per topic
between 20 and 40, except the subset R_R which
contains a random number of topics between 6
and 8. In contrast, each micro-level subset differs
in number of topics but not significantly in the
number of sentences per topic.
This difference between the datasets allows us
to focus on the different level-specific aspects.
On the macro dataset we can evaluate the stabil-
ity of TT over topics with highly varying lengths
and on the micro dataset we can evaluate BS
when the number of strongly coherent topics
changes.
</bodyText>
<subsectionHeader confidence="0.998516">
3.2 Text Segmentation Metrics
</subsectionHeader>
<bodyText confidence="0.999955027027027">
The performance of a segmenter cannot simply
be measured by false positive and false negative
boundaries compared to the true boundaries be-
cause, if the predicted boundary is only one sen-
tence away from the true boundary this could
still be very close, e.g., if the next true topic
boundary is 30 sentences away. Thus, the rela-
tive proximity to true boundaries should also be
considered. There is an ongoing discussion about
what kind of metric is appropriate to measure the
performance of segmenters (Fournier &amp; Inkpen,
2012). Most prominent and widely used are
WindowDiff wd (Pevzner &amp; Hearst, 2002) and
the probabilistic metric pk (Beeferman, et al.,
1999). The basic principle is to slide a window of
fixed size over the segmented text, i.e., fixed
number of words or sentences, and assess wheth-
er the sentences on the edges are correctly seg-
mented with respect to each other. Both metrics
wd and pk are penalty metrics, therefore lower
values indicate better segmentations. The prob-
lem with these metrics is that they strongly de-
pend on the arbitrarily defined window size pa-
rameter and do not penalize all error types equal-
ly, e.g., pk penalizes false negatives more than
false positives and wd penalizes false positive
and negative boundaries more at the beginning
and end of the text (Lamprier, et al., 2007). Be-
cause of that we also used a rather new metric
called BoundarySimilarity b. This metric is pa-
rameter independent and has been developed by
Fournier and Inkpen (2013) to solve the men-
tioned deficiencies. Since b measures the similar-
ity between the boundaries, higher values indi-
cate better segmentations. We used the imple-
mentations of wd, pk and b by Fournier3 (wd and
pk with default parameters).
</bodyText>
<subsectionHeader confidence="0.984566">
3.3 LDA Topic Model Training
</subsectionHeader>
<bodyText confidence="0.999972952380952">
Riedl and Biemann evaluated TT on the Choi-
Corpus based on a 10-fold cross validation.
Thus, the LDA topic model was generated with
90% of the samples and TT then tested on the
remaining 10% of the samples. The 700 samples
in the Choi-Corpus are only concatenations of
1111 different excerpts from the Brown Corpus
and each sample contains 10 of these excepts it is
clear that there are just not enough excerpts to
make sure that the samples in the training set do
not contain any excerpt that is also part of some
samples in the testing set.
That is one reason why we do not use the
same approach since we want to make sure that
training and testing sets are truly disjoint to eval-
uate TT on the macro dataset. The other reason is
that the topic structure generated by TT should
be based on an LDA topic model with topics ex-
tracted from documents which are thematically
related to certain parts of the course that is to be
created without using its text source.
</bodyText>
<footnote confidence="0.898957">
3 https://github.com/cfournie/segmentation.evaluation
</footnote>
<page confidence="0.996799">
135
</page>
<bodyText confidence="0.9959976875">
We train the LDA topic model to extract top-
ics from the real Wikipedia articles. This model
is then used to evaluate TT on the macro dataset
and not the Wikipedia articles. This approach has
consequences for the LDA topic model training
and respective TT testing sets, since the LDA
training set contains real articles and the TT test
set contains the samples from the macro dataset.
Because training and testing set should truly be
disjoint we cannot train with any article that is
part of a sample from the test set. Because each
test sample from the macro dataset contains parts
of 6 to 8 articles, the training set is reduced by a
large factor, even with little test set size, which is
shown for different number of folds (k) for cross
validation in Table 3.
</bodyText>
<table confidence="0.996773">
k Test Set Size Training Set Size
10 120Â±0 Samples 139Â±7 featured
(10% of the Articles
macro dataset) (26% of all arti-
cles)
20 60Â±0 Samples 267Â±8 featured
(5% of the macro Articles
dataset) (51% of all arti-
cles)
30 40Â±0 Samples 338Â±7
(3% of the macro featured Articles
dataset) (64% of all arti-
cles)
</table>
<tableCaption confidence="0.834300333333333">
Table 3: Mean size and standard deviation of
truly disjunctive LDA training and respective TT
testing set.
</tableCaption>
<bodyText confidence="0.99994">
If we truly separate training and testing sets and
train the LDA topic model with real articles a 10-
fold cross validation leads to very small training
sets (only 26% of all articles are used), which is
why we also used higher folds to evaluate the
results of TT on the macro dataset.
</bodyText>
<sectionHeader confidence="0.993211" genericHeader="method">
4 Evaluation Results
</sectionHeader>
<bodyText confidence="0.988945038461538">
We evaluated TT on the macro dataset without
providing the number of boundaries. On the mi-
cro dataset we evaluated BS with the expected
number of boundaries provided. We also imple-
mented a scalable random segmenter (RS) to
compare TT and BS against some algorithm with
interpretable performance. The interpretation of
the values in any metric even with respect to dif-
ferent metrics is very difficult without compari-
son to another segmenter. For every true bounda-
ry in a document, RS predicts a boundary drawn
from a normally distributed set around the true
boundary with scalable standard deviation Ïƒ.
Thus smaller values for Ïƒ result in better seg-
mentations because the probability of selecting
the true boundary increases, e.g., for Ïƒ = 2, more
than 68% of all predicted boundaries are at most
2 sentences away from the true boundary and
more than 99% of all predicted boundaries are
located within a range of 6 sentences from it. But
whether 6 sentences is a large or small distance
should depend on the average topic size. We
therefore relate the performance of RS to the
mean number of sentence per topic by defining Ïƒ
in percentages of that number as shown in the
table below.
</bodyText>
<table confidence="0.5703876">
Distance from True Standard Deviation
Boundary:
very close Ïƒ = 0% - 5%
close Ïƒ = 5% -15%
large Ïƒ = 15% - 30%
</table>
<tableCaption confidence="0.807515333333333">
Table 4: Defined performance of RS for different
standard deviations Ïƒ, given in percentage of
mean sentences per topic.
</tableCaption>
<bodyText confidence="0.999929863636364">
To give an example, the subset 7_6 of the macro
dataset has an average of 40 sentences per topic,
therefore RS with Ïƒ=15% means that it is set to 6
which is 15% of 40. This is defined as a medium
performance in Table 4 because 68% of the
boundaries predicted are within a range of 6 sen-
tences from the true boundaries and 99% within
18 sentences.
One important difference between the macro
and micro dataset is that all subsets of the macro
dataset have 7 topics, differing in length, except
for subset R_R where this number is only slightly
varied (Table 1). In contrast, all topics subsets of
the micro dataset have roughly the same number
of sentences but highly differ in the number of
topics (Table 2). We therefore do not compare
the performance of BS and TT since they are
evaluated on quite different datasets designed for
testing different types of segmentation tasks rel-
evant to course generation, as explained earlier.
We compare both to RS for different standard
deviations Ïƒ.
</bodyText>
<sectionHeader confidence="0.979097" genericHeader="method">
4.1 Results for TopicTiling on the Macro
Dataset
</sectionHeader>
<bodyText confidence="0.998712666666667">
For the LDA topic model training we used the
following default parameters: alpha=0.5,
beta=0.1,ntopics=100,niters=1000,
</bodyText>
<page confidence="0.997738">
136
</page>
<bodyText confidence="0.999767133333333">
twords=20,savestep=100, for details we
refer to (Griffiths &amp; Steyvers, 2004). To compare
TTâ€™s performance for different folds of the mac-
ro dataset we optimized the window parameter
which has to be set for TT, it specifies the num-
ber of sentences to the left and to the right of the
current position p between two sentences that are
used to calculate the coherence score between
these sentences (Riedl &amp; Biemann, 2012). The
performance for TT has been best with window
sizes between 9 and 11 for all metrics as shown
in Figure 2. As expected, higher folds increase
TTâ€™s overall performance especially with respect
to metric b (Figure 3). This is due to the larger
training set sizes of the LDA topic model.
</bodyText>
<figure confidence="0.9954255">
0.5
0.4
0.1
0.0
5 6 7 8 9 10 11 12 13 14 15
window
</figure>
<figureCaption confidence="0.9989075">
Figure 2: TT performance for different window
sizes with 30-fold cross validation.
</figureCaption>
<figure confidence="0.996361">
0.4
0.1
0.0
10 20 30
folds
</figure>
<figureCaption confidence="0.965077">
Figure 3: TT performance for different folds and
window size set to 9.
</figureCaption>
<bodyText confidence="0.999943307692308">
In general smaller window sizes increase the
number of predicted boundaries. The optimal
window size is between 9 and 11 and we would
expect the measures for 5 and 15 to be similar
(Figure 2). This is only the case for metric b, the
metrics wd and pk seem to penalize false posi-
tives more than false negatives. This would be a
contradiction to the findings of Lamprier et al.
(2007) since they actually found the opposite to
be true. This behaviour is explained by the non-
linear relation between the window parameter
and number of predicted boundaries by TT as
shown in Figure 4.
</bodyText>
<figure confidence="0.9830862">
mean boundaries per sample 12.5
10.0
7.5
5 6 7 8 9 10 11 12 13 14 15
window size
</figure>
<figureCaption confidence="0.961535">
Figure 4: Mean number of predicted boundaries
by TT for different window sizes and an LDA
topic model trained with 30 folds.
</figureCaption>
<bodyText confidence="0.999858846153846">
Another important finding is the stability of TTâ€™s
performance over different window sizes (from 9
to 11). This is important since a very sensitive
behaviour would be very difficult to handle for
course creators because they would have to esti-
mate this parameter in advance.
For the following detailed evaluation TT win-
dow size is set to 9 because of the best overall
results with respect to metric b and 30-fold cross
validation. The detailed performance with re-
spect to metric wd, pk and b of TT compared to
RS with different standard deviations Ïƒ is shown
in Figure 5 i), ii) and iii).
</bodyText>
<figure confidence="0.994328782608696">
0.8
0.6
0.4
0.2
0.0
7_3 7_4 7_5 7_6 7_R R_R
Subset
i. TT measured with metric b.
mean
0.3
0.2
metric
wd
pk
b
0.3
mean
0.2
metric
wd
pk
b
mean
</figure>
<page confidence="0.864633">
137
138
</page>
<figure confidence="0.999318866666667">
0.8
0.6
0.4
0.2
0.0
7_3 7_4 7_5 7_6 7_R R_R
Subset
ii. TT measured with metric wd.
0.8
0.6
0.4
0.2
0.0
7_3 7_4 7_5 7_6 7_R R_R
Subset
</figure>
<figureCaption confidence="0.721383666666667">
iii. TT measured with metric pk.
Figure 5: Performance of TT on the macro da-
taset.
</figureCaption>
<bodyText confidence="0.9803835">
First of all we want to point out that the graphs
of RS for different values of Ïƒ are ordered as ex-
pected by all metrics. Lower percentages indicate
better results. And with respect to metric wd and
pk the performance for each Ïƒ is nearly constant
over all subsets, which indicates that the metrics
correctly consider the relative distance of a pre-
dicted boundary from the true boundary by using
the mean number of sentences per topic. In met-
ric b only the RS with Ïƒ=30%, 15% and 5% are
constant. For Ïƒ=5% there is a strong decrease in
performance for subsets with more sentences per
topic.
The overall performance of TT is between that
of RS for Ïƒ=1% and Ïƒ=15%, except for subset
7_6 with respect to metric wd. With respect to
metric b TT even predicts very close boundaries.
In all metrics TT has the worst results on subset
7_6, which has the largest number of sentences
per topic (see Table 1). This is due to TTâ€™s win-
dow parameter which influences the number of
predicted boundaries as shown in Figure 4.
</bodyText>
<figure confidence="0.886473137931034">
4.2 Results for BayesSeg on the Micro Da-
taset
BS does not need any training or parameter fit-
ting, since it is provided with the number of ex-
pected segments. We therefore used the default
parameter settings.
0.8
0.6
0.4
0.2
0.0
3 4 5 6
Subset
i. BS measured with metric b.
0.8
0.6
0.4
0.2
0.0
3 4 5 6
Subset
ii. BS measured with metric wd.
0.8
0.6
0.4
0.2
0.0
3 4 5 6
Subset
</figure>
<figureCaption confidence="0.7819045">
iii. BS measured with metric pk.
Figure 6: Performance of BS on the micro da-
</figureCaption>
<bodyText confidence="0.98318635483871">
taset.
As expected, the performance of RS is decreas-
ing for higher values of Ïƒ in all metrics (Figure 6
i), ii), iii)). For metric wd and pk the increasing
mean
mean
mean
mean
mean
number of topics leads to slightly increasing
penalties for constant values of 6, which clearly
indicates that the metrics do not treat all errors
equally, as repeatedly pointed out. Metric b treats
errors equally over increasing number of topics
for RS. BS predicts with respect to all metrics
close boundaries since it is better than RS with
6=15% except on subset 6 (Table 4). With an
increasing number of topics BS is getting worse
in all metrics.
Comparing the measures of metric b for macro
and micro dataset it seems that it handles increas-
ing numbers of topics better than increasing size
of topics. On the micro dataset the results with
respect to all metrics are far more similar than
the once on the macro dataset, where the differ-
ences are very large. Since we are only interested
in comparative measures of the performance of
the segmenters and RS, which has shown to be a
very useful approach to interpret segmentation
results, we leave detailed explanations of the
metrics behaviours itself to further research.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999992957446809">
We demonstrated that text segmentation algo-
rithms can be applied to the generation of e-
learning courses. We use a web-didactic ap-
proach that is based on a flat two-level hierar-
chical structure. A new corpus has been com-
piled based on featured articles from the English
Wikipedia that reflects this kind of course struc-
ture. On the broader macro level we applied the
linear LDA-based text segmentation algorithm
TopicTiling without providing the expected
number of boundaries. The LDA topic model is
usually trained with concatenated texts from the
very same dataset TopicTiling is tested on. We
showed that it is very difficult to ensure that the
two sets are always truly disjoint. The reason is
that concatenated texts normally always have
identical parts. This problem is solved by apply-
ing a different training and testing method.
The more fine grained micro level was seg-
mented using BayesSeg, a hierarchical algorithm
which we provided with the expected number of
boundaries.
We used three different evaluation metrics and
presented a scalable random segmentation algo-
rithm to establish upper and lower bounds for
baseline comparison. The results, especially on
the macro level, demonstrate that text segmenta-
tion algorithms have evolved enough to be used
for the automatic generation of e-learning cours-
es.
An interesting aspect of future research would
be the application and creation of real e-learning
content. Based on the textual segments, summa-
rization and question generation algorithms as
well as automatic replacement with appropriate
pictures and videos instead of text could be used
to finally evaluate an automatically generated e-
learning course with real learners.
Regarding text segmentation in general, future
research especially needs to address the difficult
task of transparently and equally measuring the
performance of segmentation algorithms. Our
results, i.e., the ones from the random segmenta-
tion algorithm, indicate that there are still un-
solved issues regarding the penalization of false
positives and false negatives when the number of
topics or sentences per topic is changed.
</bodyText>
<sectionHeader confidence="0.99301" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999872939393939">
Beeferman, D., Berger, A. &amp; Lafferty, J., 1999.
Statistical Models for Text Segmentation. Mach.
Learn., #feb#, 34(1-3), pp. 177-210.
Bird, S., Klein, E. &amp; Loper, E., 2009. Natural
Language Processing with Python. s.l.:O&apos;Reilly
Media.
Capuano, N. et al., 2009. LIA: an intelligent advisor
for e-learning. Interactive Learning Environments,
17(3), pp. 221-239.
Choi, F. Y. Y., 2000. Advances in Domain
Independent Linear Text Segmentation.
Stroudsburg, PA, USA, Association for
Computational Linguistics, pp. 26-33.
Eisenstein, J. &amp; Barzilay, R., 2008. Bayesian
Unsupervised Topic Segmentation. Honolulu,
Hawaii, Association for Computational Linguistics,
pp. 334-343.
Fournier, C., 2013. Evaluating Text Segmentation
using Boundary Edit Distance. Stroudsburg, PA,
USA, Association for Computational Linguistics,
p. To appear.
Fournier, C. &amp; Inkpen, D., 2012. Segmentation
Similarity and Agreement. Montreal, Canada,
Association for Computational Linguistics, pp.
152-161.
Galley, M., McKeown, K., Fosler-Lussier, E. &amp; Jing,
H., 2003. Discourse Segmentation of Multi-party
Conversation. Stroudsburg, PA, USA, Association
for Computational Linguistics, pp. 562-569.
Griffiths, T. L. &amp; Steyvers, M., 2004. Finding
scientific topics. Proceedings of the National
Academy of Sciences, April, 101(Suppl. 1), pp.
5228-5235.
</reference>
<page confidence="0.986919">
139
</page>
<reference confidence="0.999849017241379">
Hearst, M. A., 1997. TextTiling: Segmenting Text
into Multi-paragraph Subtopic Passages. Comput.
Linguist., #mar#, 23(1), pp. 33-64.
Huang, X. et al., 2002. Applying Machine Learning to
Text Segmentation for Information Retrieval.
s.l.:s.n.
Janin, A. et al., 2003. The ICSI Meeting Corpus. s.l.,
s.n., pp. I-364--I-367 vol.1.
Kan, M.-Y., Klavans, J. L. &amp; McKeown, K. R., 1998.
Linear Segmentation and Segment Significance.
s.l., s.n., pp. 197-205.
Kim, S., Medelyan, O., Kan, M.-Y. &amp; Baldwin, T.,
2013. Automatic keyphrase extraction from
scientific articles. Language Resources and
Evaluation, 47(3), pp. 723-742.
Lamprier, S., Amghar, T., Levrat, B. &amp; Saubion, F.,
2007. On Evaluation Methodologies for Text
Segmentation Algorithms. s.l., s.n., pp. 19-26.
Lin, Y.-T., Cheng, S.-C., Yang, J.-T. &amp; Huang, Y.-
M., 2009. An Automatic Course Generation
System for Organizing Existent Learning Objects
Using Particle Swarm Optimization. In: M. Chang,
et al. Hrsg. Learning by Playing. Game-based
Education System Design and Development.
s.l.:Springer Berlin Heidelberg, pp. 565-570.
Misra, H., Yvon, F., Jose, J. M. &amp; Cappe, O., 2009.
Text Segmentation via Topic Modeling: An
Analytical Study. New York, NY, USA, ACM, pp.
1553-1556.
Pevzner, L. &amp; Hearst, M. A., 2002. A Critique and
Improvement of an Evaluation Metric for Text
Segmentation. Comput. Linguist., #mar#, 28(1), pp.
19-36.
Riedl, M. &amp; Biemann, C., 2012. TopicTiling: A Text
Segmentation Algorithm Based on LDA.
Stroudsburg, PA, USA, Association for
Computational Linguistics, pp. 37-42.
Strassel, S., Graff, D., Martey, N. &amp; Cieri, C., 2000.
Quality Control in Large Annotation Projects
Involving Multiple Judges: The Case of the TDT
Corpora. s.l., s.n.
Sun, Q., Li, R., Luo, D. &amp; Wu, X., 2008. Text
Segmentation with LDA-based Fisher Kernel.
Stroudsburg, PA, USA, Association for
Computational Linguistics, pp. 269-272.
Swertz, C. et al., 2013. A Pedagogical Ontology as a
Playground in Adaptive Elearning Environments..
s.l., GI, pp. 1955-1960.
Tan, X., Ullrich, C., Wang, Y. &amp; Shen, R., 2010. The
Design and Application of an Automatic Course
Generation System for Large-Scale Education. s.l.,
s.n., pp. 607-609.
Utiyama, M. &amp; Isahara, H., 2001. A Statistical Model
for Domain-independent Text Segmentation.
Stroudsburg, PA, USA, Association for
Computational Linguistics, pp. 499-506.
Yaari, Y., 1997. Segmentation of Expository Texts by
Hierarchical Agglomerative Clustering. s.l.:s.n.
</reference>
<page confidence="0.997525">
140
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.382978">
<title confidence="0.999637">Using Text Segmentation Algorithms for the Automatic Generation E-Learning Courses</title>
<author confidence="0.945464">Can Beck</author>
<author confidence="0.945464">Alexander Streicher</author>
<author confidence="0.945464">Andrea</author>
<affiliation confidence="0.407522">Fraunhofer</affiliation>
<address confidence="0.989132">Karlsruhe, Germany</address>
<email confidence="0.9808875">can.beck@iosb.fraunhofer.de</email>
<email confidence="0.9808875">andrea.zielinski@iosb.fraunhofer.de</email>
<abstract confidence="0.999291333333334">With the advent of e-learning, there is a strong demand for tools that help to create e-learning courses in an automatic or semi-automatic way. While resources for new courses are often freely available, they are generally not properly structured into easy to handle units. In this paper, we investigate how state of the art text segmentation algorithms can be applied to automatically transform unstructured text into coherent pieces appropriate for e-learning courses. The feasibility to course generation is validated on a test corpus specifically tailored to this scenario. We also introduce a more generic training and testing method for text segmentation algorithms based on a Latent Dirichlet Allocation (LDA) topic model. In addition we introduce a scalable random text segmentation algorithm, in order to establish lower and upper bounds to be able to evaluate segmentation results on a common basis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Statistical Models for Text Segmentation.</title>
<date>1999</date>
<pages>34--1</pages>
<location>Mach. Learn.,</location>
<contexts>
<context position="14099" citStr="Beeferman, et al., 1999" startWordPosition="2282" endWordPosition="2285">ot simply be measured by false positive and false negative boundaries compared to the true boundaries because, if the predicted boundary is only one sentence away from the true boundary this could still be very close, e.g., if the next true topic boundary is 30 sentences away. Thus, the relative proximity to true boundaries should also be considered. There is an ongoing discussion about what kind of metric is appropriate to measure the performance of segmenters (Fournier &amp; Inkpen, 2012). Most prominent and widely used are WindowDiff wd (Pevzner &amp; Hearst, 2002) and the probabilistic metric pk (Beeferman, et al., 1999). The basic principle is to slide a window of fixed size over the segmented text, i.e., fixed number of words or sentences, and assess whether the sentences on the edges are correctly segmented with respect to each other. Both metrics wd and pk are penalty metrics, therefore lower values indicate better segmentations. The problem with these metrics is that they strongly depend on the arbitrarily defined window size parameter and do not penalize all error types equally, e.g., pk penalizes false negatives more than false positives and wd penalizes false positive and negative boundaries more at t</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Beeferman, D., Berger, A. &amp; Lafferty, J., 1999. Statistical Models for Text Segmentation. Mach. Learn., #feb#, 34(1-3), pp. 177-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bird</author>
<author>E Klein</author>
<author>E Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. s.l.:O&apos;Reilly Media.</booktitle>
<contexts>
<context position="11625" citStr="Bird, et al., 2009" startWordPosition="1831" endWordPosition="1834">s, where each sample contains all KOs from one CC of the macro dataset. The segmentation task is to find the topic boundaries between the KOs, i.e, subsequent paragraphs of one section, see Figure 1. Figure 1: Schema for corpus samples: left and right Wikipedia articles with sections and paragraphs, in the middle three samples, dashed rectangle is a macro sample and dashed circles are micro samples. Filled squares indicate topic boundaries in the macro sample and filled circles in the micro samples. All texts in our corpus are stemmed and stopwords are removed with the NLP-Toolkit for Python (Bird, et al., 2009) using an adapted variant2 of the keyword extraction method by Kim et al. (2013). The macro and micro dataset themselves are divided into multiple subsets to evaluate the stability of the segmenters when the number of sentences per topic or the number of topics per sample have changed. The detailed configuration is shown in Table 1 and 2. Each subset is identified by the number of CCs per sample and the number of KOs per CC (the subset is denoted as #CC_#KO). Subsets of the micro dataset are identified by a single value which is the number 2 https://gist.github.com/alexbowe/879414 134 of KOs p</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Bird, S., Klein, E. &amp; Loper, E., 2009. Natural Language Processing with Python. s.l.:O&apos;Reilly Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Capuano</author>
</authors>
<title>LIA: an intelligent advisor for e-learning.</title>
<date>2009</date>
<journal>Interactive Learning Environments,</journal>
<volume>17</volume>
<issue>3</issue>
<pages>221--239</pages>
<marker>Capuano, 2009</marker>
<rawString>Capuano, N. et al., 2009. LIA: an intelligent advisor for e-learning. Interactive Learning Environments, 17(3), pp. 221-239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Y Y Choi</author>
</authors>
<title>Advances in Domain Independent Linear Text Segmentation.</title>
<date>2000</date>
<journal>Association for Computational Linguistics,</journal>
<pages>26--33</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="8750" citStr="Choi (2000)" startWordPosition="1360" endWordPosition="1361">did not use TT. The training of an appropriate LDA model would have to be done for every CC separately since they are thematically relatively unrelated. Apart from that the boundaries between the KOs should be an optimal division for a given number of expected boundaries. The reason for this is that the length of KOs should be adapted to the intended skill and background of the learners. This is why we decided to use the hierarchical segmenter BS. 3.1 Application Setting and Corpus To evaluate segmenters many different corpora have been created. The most commonly used corpus was introduced by Choi (2000). It is based on the Brown Corpus and contains 700 samples, each containing a fixed number of sentences from 10 different news texts, which are randomly chosen from the Brown Corpus. Two other widely tested corpora were introduced by Galley et al. (2003). Both contain 500 samples, one with concatenated texts from the Wall Street Journal (WSJ) and the other with concatenated texts from the Topic Detection and Tracking (TDT) corpus (Strassel, et al., 2000). A standard for the segmentation of speech is the corpus from the International Computer Science Institute (ICSI) by Janin et al. (2003). A m</context>
</contexts>
<marker>Choi, 2000</marker>
<rawString>Choi, F. Y. Y., 2000. Advances in Domain Independent Linear Text Segmentation. Stroudsburg, PA, USA, Association for Computational Linguistics, pp. 26-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>R Barzilay</author>
</authors>
<title>Bayesian Unsupervised Topic Segmentation.</title>
<date>2008</date>
<journal>Association for Computational Linguistics,</journal>
<pages>334--343</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="9414" citStr="Eisenstein and Barzilay (2008)" startWordPosition="1468" endWordPosition="1471">and contains 700 samples, each containing a fixed number of sentences from 10 different news texts, which are randomly chosen from the Brown Corpus. Two other widely tested corpora were introduced by Galley et al. (2003). Both contain 500 samples, one with concatenated texts from the Wall Street Journal (WSJ) and the other with concatenated texts from the Topic Detection and Tracking (TDT) corpus (Strassel, et al., 2000). A standard for the segmentation of speech is the corpus from the International Computer Science Institute (ICSI) by Janin et al. (2003). A medical text book has been used by Eisenstein and Barzilay (2008). The approaches to evaluate segmenters are always similar: they have to find the boundaries in artificially concatenated texts. We developed our own dataset because we wanted to use text that potentially could be used as a basis for creating e-learning courses. We therefore need samples which, on the one hand, have relatively clear topic boundaries on the macro level and, on the other hand resemble the differences in number of topics and inter-topic cohesion on the micro level. We based our corpus on 530 featured1 articles from 6 different categories of the English Wikipedia. It can be assume</context>
<context position="2897" citStr="Eisenstein &amp; Barzilay, 2008" startWordPosition="436" endWordPosition="439">on algorithms (segmenters) on each level. To evaluate the segmenters in the described scenario, we created a test corpus based on featured Wikipedia articles. For the macro level we exploit sections from different articles and the corresponding micro structure consists of subsequent paragraphs from these sections. On the macro level the segmenter TopicTiling (TT) by Riedl and Biemann (2012) is used. It is based on a LDA topic model which we train based on the articles from Wikipedia to extract a predefined number of different topics. On the micro level, the segmenter BayesSeg (BS) is applied (Eisenstein &amp; Barzilay, 2008). We achieved overall good results measured in three different metrics over a baseline approach, i.e., a scalable random segmenter, that indicate This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 132 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 132â€“140, Dublin, Ireland, August 23-24 2014. text segmentation algorithms are ready to be applied to facilitate the creation of e-learning</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Eisenstein, J. &amp; Barzilay, R., 2008. Bayesian Unsupervised Topic Segmentation. Honolulu, Hawaii, Association for Computational Linguistics, pp. 334-343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fournier</author>
</authors>
<title>Evaluating Text Segmentation using Boundary Edit Distance.</title>
<date>2013</date>
<journal>Association for Computational Linguistics,</journal>
<pages>p.</pages>
<location>Stroudsburg, PA, USA,</location>
<note>To appear.</note>
<marker>Fournier, 2013</marker>
<rawString>Fournier, C., 2013. Evaluating Text Segmentation using Boundary Edit Distance. Stroudsburg, PA, USA, Association for Computational Linguistics, p. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fournier</author>
<author>D Inkpen</author>
</authors>
<title>Segmentation Similarity and Agreement.</title>
<date>2012</date>
<journal>Association for Computational Linguistics,</journal>
<pages>152--161</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="13966" citStr="Fournier &amp; Inkpen, 2012" startWordPosition="2261" endWordPosition="2264">an evaluate BS when the number of strongly coherent topics changes. 3.2 Text Segmentation Metrics The performance of a segmenter cannot simply be measured by false positive and false negative boundaries compared to the true boundaries because, if the predicted boundary is only one sentence away from the true boundary this could still be very close, e.g., if the next true topic boundary is 30 sentences away. Thus, the relative proximity to true boundaries should also be considered. There is an ongoing discussion about what kind of metric is appropriate to measure the performance of segmenters (Fournier &amp; Inkpen, 2012). Most prominent and widely used are WindowDiff wd (Pevzner &amp; Hearst, 2002) and the probabilistic metric pk (Beeferman, et al., 1999). The basic principle is to slide a window of fixed size over the segmented text, i.e., fixed number of words or sentences, and assess whether the sentences on the edges are correctly segmented with respect to each other. Both metrics wd and pk are penalty metrics, therefore lower values indicate better segmentations. The problem with these metrics is that they strongly depend on the arbitrarily defined window size parameter and do not penalize all error types eq</context>
</contexts>
<marker>Fournier, Inkpen, 2012</marker>
<rawString>Fournier, C. &amp; Inkpen, D., 2012. Segmentation Similarity and Agreement. Montreal, Canada, Association for Computational Linguistics, pp. 152-161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>E Fosler-Lussier</author>
<author>H Jing</author>
</authors>
<date>2003</date>
<journal>Association for Computational Linguistics,</journal>
<booktitle>Discourse Segmentation of Multi-party Conversation.</booktitle>
<pages>562--569</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="4783" citStr="Galley et al. (2003)" startWordPosition="728" endWordPosition="731"> (2009) and Tan et al. (2010). The other area is the course creation itself on which we focus on in this paper. Since the publication of the segmenter TextTiling by Hearst (1997) at least a dozen different segmenters have been developed. They can be divided into linear and hierarchical segmenters. Linear segmenters process the text sequentially sentence by sentence. Hierarchical segmenters first process the whole text and extract topics with varying granularities. These topics are then agglomerated based on a predefined criterion. Linear segmenters have been developed by Kan et al. (1998) and Galley et al. (2003). One of the first probabilistic algorithms has been introduced by Utiyama and Isahara (2001). LDA based approaches were first described by Sun et al. (2008) and improved by Misra et al. (2009). The newest LDA based segmenter is TT. It performs linear text segmentation based on a pretrained LDA topic model and calculates the similarity between segments (adjacent sentences) to measure text coherence on the basis of a topic vector representation using cosine similarity. For reasons of efficiency, only the most frequent topic ID is assigned to each word in the sentence, using Gibbs sampling. Hier</context>
<context position="6095" citStr="Galley, et al., 2003" startWordPosition="937" endWordPosition="940">roach by Eisenstein (2008) uses a generative Bayesian model BS for text segmentation, assuming that a) topic shifts are likely to occur at points marked by cue phrases and b) a linear discourse structure. Each sentence in the document is modeled by a language model associated with a segment. The algorithm then calculates the maximum likelihood estimates of observing the whole sequence of sentences at selected topic boundaries. The applications of text segmentation algorithms range from information retrieval (Huang, et al., 2002) to topic tracking and segmentation of multi-party conversations (Galley, et al., 2003). Similar to our work Sathiyamurthy and Geetha (2011) showed how LDA based text segmentation algorithms combined with hierarchical domain ontology and pedagogical ontology can be applied to content generation for e-learning courses. They focussed on the segmentation of existing e-learning material in the domain of computer science and introduced new metrics to measure the segmentation results with respect to concepts from the ontologies. Our work focusses on the appropriate segmentation of unstructured text instead of existing e-learning material. Although the usage of domain models is an inte</context>
<context position="9004" citStr="Galley et al. (2003)" startWordPosition="1402" endWordPosition="1405">ber of expected boundaries. The reason for this is that the length of KOs should be adapted to the intended skill and background of the learners. This is why we decided to use the hierarchical segmenter BS. 3.1 Application Setting and Corpus To evaluate segmenters many different corpora have been created. The most commonly used corpus was introduced by Choi (2000). It is based on the Brown Corpus and contains 700 samples, each containing a fixed number of sentences from 10 different news texts, which are randomly chosen from the Brown Corpus. Two other widely tested corpora were introduced by Galley et al. (2003). Both contain 500 samples, one with concatenated texts from the Wall Street Journal (WSJ) and the other with concatenated texts from the Topic Detection and Tracking (TDT) corpus (Strassel, et al., 2000). A standard for the segmentation of speech is the corpus from the International Computer Science Institute (ICSI) by Janin et al. (2003). A medical text book has been used by Eisenstein and Barzilay (2008). The approaches to evaluate segmenters are always similar: they have to find the boundaries in artificially concatenated texts. We developed our own dataset because we wanted to use text th</context>
</contexts>
<marker>Galley, McKeown, Fosler-Lussier, Jing, 2003</marker>
<rawString>Galley, M., McKeown, K., Fosler-Lussier, E. &amp; Jing, H., 2003. Discourse Segmentation of Multi-party Conversation. Stroudsburg, PA, USA, Association for Computational Linguistics, pp. 562-569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>5228--5235</pages>
<contexts>
<context position="20335" citStr="Griffiths &amp; Steyvers, 2004" startWordPosition="3369" endWordPosition="3372">t have roughly the same number of sentences but highly differ in the number of topics (Table 2). We therefore do not compare the performance of BS and TT since they are evaluated on quite different datasets designed for testing different types of segmentation tasks relevant to course generation, as explained earlier. We compare both to RS for different standard deviations Ïƒ. 4.1 Results for TopicTiling on the Macro Dataset For the LDA topic model training we used the following default parameters: alpha=0.5, beta=0.1,ntopics=100,niters=1000, 136 twords=20,savestep=100, for details we refer to (Griffiths &amp; Steyvers, 2004). To compare TTâ€™s performance for different folds of the macro dataset we optimized the window parameter which has to be set for TT, it specifies the number of sentences to the left and to the right of the current position p between two sentences that are used to calculate the coherence score between these sentences (Riedl &amp; Biemann, 2012). The performance for TT has been best with window sizes between 9 and 11 for all metrics as shown in Figure 2. As expected, higher folds increase TTâ€™s overall performance especially with respect to metric b (Figure 3). This is due to the larger training set </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Griffiths, T. L. &amp; Steyvers, M., 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, April, 101(Suppl. 1), pp. 5228-5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>TextTiling: Segmenting Text into Multi-paragraph Subtopic Passages.</title>
<date>1997</date>
<journal>Comput. Linguist.,</journal>
<volume>23</volume>
<issue>1</issue>
<pages>33--64</pages>
<contexts>
<context position="4341" citStr="Hearst (1997)" startWordPosition="665" endWordPosition="666">n results on our corpus. In the last section we summarize the presented findings and give an outlook on further research needed for the automatic generation of e-learning courses. 2 Related Work Automatic course generation can roughly be divided into two different areas. One is concerned with generation from existing courses and is mainly focused on adaption to the learner or instructional plans see Lin et al. (2009), Capuno et al. (2009) and Tan et al. (2010). The other area is the course creation itself on which we focus on in this paper. Since the publication of the segmenter TextTiling by Hearst (1997) at least a dozen different segmenters have been developed. They can be divided into linear and hierarchical segmenters. Linear segmenters process the text sequentially sentence by sentence. Hierarchical segmenters first process the whole text and extract topics with varying granularities. These topics are then agglomerated based on a predefined criterion. Linear segmenters have been developed by Kan et al. (1998) and Galley et al. (2003). One of the first probabilistic algorithms has been introduced by Utiyama and Isahara (2001). LDA based approaches were first described by Sun et al. (2008) </context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Hearst, M. A., 1997. TextTiling: Segmenting Text into Multi-paragraph Subtopic Passages. Comput. Linguist., #mar#, 23(1), pp. 33-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Huang</author>
</authors>
<title>Applying Machine Learning to Text Segmentation for Information Retrieval.</title>
<date>2002</date>
<tech>s.l.:s.n.</tech>
<marker>Huang, 2002</marker>
<rawString>Huang, X. et al., 2002. Applying Machine Learning to Text Segmentation for Information Retrieval. s.l.:s.n.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
</authors>
<date>2003</date>
<booktitle>The ICSI Meeting Corpus. s.l., s.n.,</booktitle>
<pages>364--367</pages>
<marker>Janin, 2003</marker>
<rawString>Janin, A. et al., 2003. The ICSI Meeting Corpus. s.l., s.n., pp. I-364--I-367 vol.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-Y Kan</author>
<author>J L Klavans</author>
<author>K R McKeown</author>
</authors>
<date>1998</date>
<booktitle>Linear Segmentation and Segment Significance. s.l., s.n.,</booktitle>
<pages>197--205</pages>
<contexts>
<context position="4758" citStr="Kan et al. (1998)" startWordPosition="723" endWordPosition="726"> (2009), Capuno et al. (2009) and Tan et al. (2010). The other area is the course creation itself on which we focus on in this paper. Since the publication of the segmenter TextTiling by Hearst (1997) at least a dozen different segmenters have been developed. They can be divided into linear and hierarchical segmenters. Linear segmenters process the text sequentially sentence by sentence. Hierarchical segmenters first process the whole text and extract topics with varying granularities. These topics are then agglomerated based on a predefined criterion. Linear segmenters have been developed by Kan et al. (1998) and Galley et al. (2003). One of the first probabilistic algorithms has been introduced by Utiyama and Isahara (2001). LDA based approaches were first described by Sun et al. (2008) and improved by Misra et al. (2009). The newest LDA based segmenter is TT. It performs linear text segmentation based on a pretrained LDA topic model and calculates the similarity between segments (adjacent sentences) to measure text coherence on the basis of a topic vector representation using cosine similarity. For reasons of efficiency, only the most frequent topic ID is assigned to each word in the sentence, u</context>
</contexts>
<marker>Kan, Klavans, McKeown, 1998</marker>
<rawString>Kan, M.-Y., Klavans, J. L. &amp; McKeown, K. R., 1998. Linear Segmentation and Segment Significance. s.l., s.n., pp. 197-205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>O Medelyan</author>
<author>M-Y Kan</author>
<author>T Baldwin</author>
</authors>
<title>Automatic keyphrase extraction from scientific articles.</title>
<date>2013</date>
<journal>Language Resources and Evaluation,</journal>
<volume>47</volume>
<issue>3</issue>
<pages>723--742</pages>
<contexts>
<context position="11705" citStr="Kim et al. (2013)" startWordPosition="1846" endWordPosition="1849">tation task is to find the topic boundaries between the KOs, i.e, subsequent paragraphs of one section, see Figure 1. Figure 1: Schema for corpus samples: left and right Wikipedia articles with sections and paragraphs, in the middle three samples, dashed rectangle is a macro sample and dashed circles are micro samples. Filled squares indicate topic boundaries in the macro sample and filled circles in the micro samples. All texts in our corpus are stemmed and stopwords are removed with the NLP-Toolkit for Python (Bird, et al., 2009) using an adapted variant2 of the keyword extraction method by Kim et al. (2013). The macro and micro dataset themselves are divided into multiple subsets to evaluate the stability of the segmenters when the number of sentences per topic or the number of topics per sample have changed. The detailed configuration is shown in Table 1 and 2. Each subset is identified by the number of CCs per sample and the number of KOs per CC (the subset is denoted as #CC_#KO). Subsets of the micro dataset are identified by a single value which is the number 2 https://gist.github.com/alexbowe/879414 134 of KOs per sample (#KO). In Table 1 the identifier R means that the number of CCs or KOs</context>
</contexts>
<marker>Kim, Medelyan, Kan, Baldwin, 2013</marker>
<rawString>Kim, S., Medelyan, O., Kan, M.-Y. &amp; Baldwin, T., 2013. Automatic keyphrase extraction from scientific articles. Language Resources and Evaluation, 47(3), pp. 723-742.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lamprier</author>
<author>T Amghar</author>
<author>B Levrat</author>
<author>F Saubion</author>
</authors>
<title>On Evaluation Methodologies for Text Segmentation Algorithms. s.l., s.n.,</title>
<date>2007</date>
<pages>pp.</pages>
<contexts>
<context position="14756" citStr="Lamprier, et al., 2007" startWordPosition="2395" endWordPosition="2398">a window of fixed size over the segmented text, i.e., fixed number of words or sentences, and assess whether the sentences on the edges are correctly segmented with respect to each other. Both metrics wd and pk are penalty metrics, therefore lower values indicate better segmentations. The problem with these metrics is that they strongly depend on the arbitrarily defined window size parameter and do not penalize all error types equally, e.g., pk penalizes false negatives more than false positives and wd penalizes false positive and negative boundaries more at the beginning and end of the text (Lamprier, et al., 2007). Because of that we also used a rather new metric called BoundarySimilarity b. This metric is parameter independent and has been developed by Fournier and Inkpen (2013) to solve the mentioned deficiencies. Since b measures the similarity between the boundaries, higher values indicate better segmentations. We used the implementations of wd, pk and b by Fournier3 (wd and pk with default parameters). 3.3 LDA Topic Model Training Riedl and Biemann evaluated TT on the ChoiCorpus based on a 10-fold cross validation. Thus, the LDA topic model was generated with 90% of the samples and TT then tested </context>
<context position="21579" citStr="Lamprier et al. (2007)" startWordPosition="3601" endWordPosition="3604"> model. 0.5 0.4 0.1 0.0 5 6 7 8 9 10 11 12 13 14 15 window Figure 2: TT performance for different window sizes with 30-fold cross validation. 0.4 0.1 0.0 10 20 30 folds Figure 3: TT performance for different folds and window size set to 9. In general smaller window sizes increase the number of predicted boundaries. The optimal window size is between 9 and 11 and we would expect the measures for 5 and 15 to be similar (Figure 2). This is only the case for metric b, the metrics wd and pk seem to penalize false positives more than false negatives. This would be a contradiction to the findings of Lamprier et al. (2007) since they actually found the opposite to be true. This behaviour is explained by the nonlinear relation between the window parameter and number of predicted boundaries by TT as shown in Figure 4. mean boundaries per sample 12.5 10.0 7.5 5 6 7 8 9 10 11 12 13 14 15 window size Figure 4: Mean number of predicted boundaries by TT for different window sizes and an LDA topic model trained with 30 folds. Another important finding is the stability of TTâ€™s performance over different window sizes (from 9 to 11). This is important since a very sensitive behaviour would be very difficult to handle for </context>
</contexts>
<marker>Lamprier, Amghar, Levrat, Saubion, 2007</marker>
<rawString>Lamprier, S., Amghar, T., Levrat, B. &amp; Saubion, F., 2007. On Evaluation Methodologies for Text Segmentation Algorithms. s.l., s.n., pp. 19-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-T Lin</author>
<author>S-C Cheng</author>
<author>J-T Yang</author>
<author>Y-M Huang</author>
</authors>
<title>An Automatic Course Generation System for Organizing Existent Learning Objects Using Particle Swarm Optimization. In:</title>
<date>2009</date>
<booktitle>Hrsg. Learning by Playing. Game-based Education System Design and Development. s.l.:Springer</booktitle>
<pages>565--570</pages>
<location>Berlin Heidelberg,</location>
<contexts>
<context position="4148" citStr="Lin et al. (2009)" startWordPosition="627" endWordPosition="630">as follows: Section 2 gives an overview of related work on automatic course generation as well as text segmentation applications. In the main sections 3 and 4 we describe our approach and evaluation results on our corpus. In the last section we summarize the presented findings and give an outlook on further research needed for the automatic generation of e-learning courses. 2 Related Work Automatic course generation can roughly be divided into two different areas. One is concerned with generation from existing courses and is mainly focused on adaption to the learner or instructional plans see Lin et al. (2009), Capuno et al. (2009) and Tan et al. (2010). The other area is the course creation itself on which we focus on in this paper. Since the publication of the segmenter TextTiling by Hearst (1997) at least a dozen different segmenters have been developed. They can be divided into linear and hierarchical segmenters. Linear segmenters process the text sequentially sentence by sentence. Hierarchical segmenters first process the whole text and extract topics with varying granularities. These topics are then agglomerated based on a predefined criterion. Linear segmenters have been developed by Kan et </context>
</contexts>
<marker>Lin, Cheng, Yang, Huang, 2009</marker>
<rawString>Lin, Y.-T., Cheng, S.-C., Yang, J.-T. &amp; Huang, Y.-M., 2009. An Automatic Course Generation System for Organizing Existent Learning Objects Using Particle Swarm Optimization. In: M. Chang, et al. Hrsg. Learning by Playing. Game-based Education System Design and Development. s.l.:Springer Berlin Heidelberg, pp. 565-570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Misra</author>
<author>F Yvon</author>
<author>J M Jose</author>
<author>O Cappe</author>
</authors>
<title>Text Segmentation via Topic Modeling: An Analytical Study.</title>
<date>2009</date>
<pages>1553--1556</pages>
<publisher>ACM,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="4976" citStr="Misra et al. (2009)" startWordPosition="761" endWordPosition="764">n different segmenters have been developed. They can be divided into linear and hierarchical segmenters. Linear segmenters process the text sequentially sentence by sentence. Hierarchical segmenters first process the whole text and extract topics with varying granularities. These topics are then agglomerated based on a predefined criterion. Linear segmenters have been developed by Kan et al. (1998) and Galley et al. (2003). One of the first probabilistic algorithms has been introduced by Utiyama and Isahara (2001). LDA based approaches were first described by Sun et al. (2008) and improved by Misra et al. (2009). The newest LDA based segmenter is TT. It performs linear text segmentation based on a pretrained LDA topic model and calculates the similarity between segments (adjacent sentences) to measure text coherence on the basis of a topic vector representation using cosine similarity. For reasons of efficiency, only the most frequent topic ID is assigned to each word in the sentence, using Gibbs sampling. Hierarchical text segmentation algorithms were first introduced by Yaari (1997). The latest approach by Eisenstein (2008) uses a generative Bayesian model BS for text segmentation, assuming that a)</context>
</contexts>
<marker>Misra, Yvon, Jose, Cappe, 2009</marker>
<rawString>Misra, H., Yvon, F., Jose, J. M. &amp; Cappe, O., 2009. Text Segmentation via Topic Modeling: An Analytical Study. New York, NY, USA, ACM, pp. 1553-1556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Pevzner</author>
<author>M A Hearst</author>
</authors>
<title>A Critique and Improvement of an Evaluation Metric for Text Segmentation.</title>
<date>2002</date>
<journal>Comput. Linguist.,</journal>
<volume>28</volume>
<issue>1</issue>
<pages>pp.</pages>
<contexts>
<context position="14041" citStr="Pevzner &amp; Hearst, 2002" startWordPosition="2273" endWordPosition="2276"> Segmentation Metrics The performance of a segmenter cannot simply be measured by false positive and false negative boundaries compared to the true boundaries because, if the predicted boundary is only one sentence away from the true boundary this could still be very close, e.g., if the next true topic boundary is 30 sentences away. Thus, the relative proximity to true boundaries should also be considered. There is an ongoing discussion about what kind of metric is appropriate to measure the performance of segmenters (Fournier &amp; Inkpen, 2012). Most prominent and widely used are WindowDiff wd (Pevzner &amp; Hearst, 2002) and the probabilistic metric pk (Beeferman, et al., 1999). The basic principle is to slide a window of fixed size over the segmented text, i.e., fixed number of words or sentences, and assess whether the sentences on the edges are correctly segmented with respect to each other. Both metrics wd and pk are penalty metrics, therefore lower values indicate better segmentations. The problem with these metrics is that they strongly depend on the arbitrarily defined window size parameter and do not penalize all error types equally, e.g., pk penalizes false negatives more than false positives and wd </context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Pevzner, L. &amp; Hearst, M. A., 2002. A Critique and Improvement of an Evaluation Metric for Text Segmentation. Comput. Linguist., #mar#, 28(1), pp. 19-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Riedl</author>
<author>C Biemann</author>
</authors>
<title>TopicTiling: A Text Segmentation Algorithm Based on LDA.</title>
<date>2012</date>
<journal>Association for Computational Linguistics,</journal>
<pages>37--42</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="2662" citStr="Riedl and Biemann (2012)" startWordPosition="395" endWordPosition="398">ond to chapters in a book and Knowledge Objects (KOs) correspond to course pages. To automate the partition of an unstructured text source into appropriate segments for the macro and micro level we applied different text segmentation algorithms (segmenters) on each level. To evaluate the segmenters in the described scenario, we created a test corpus based on featured Wikipedia articles. For the macro level we exploit sections from different articles and the corresponding micro structure consists of subsequent paragraphs from these sections. On the macro level the segmenter TopicTiling (TT) by Riedl and Biemann (2012) is used. It is based on a LDA topic model which we train based on the articles from Wikipedia to extract a predefined number of different topics. On the micro level, the segmenter BayesSeg (BS) is applied (Eisenstein &amp; Barzilay, 2008). We achieved overall good results measured in three different metrics over a baseline approach, i.e., a scalable random segmenter, that indicate This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 132 Pro</context>
<context position="20676" citStr="Riedl &amp; Biemann, 2012" startWordPosition="3430" endWordPosition="3433">ifferent standard deviations Ïƒ. 4.1 Results for TopicTiling on the Macro Dataset For the LDA topic model training we used the following default parameters: alpha=0.5, beta=0.1,ntopics=100,niters=1000, 136 twords=20,savestep=100, for details we refer to (Griffiths &amp; Steyvers, 2004). To compare TTâ€™s performance for different folds of the macro dataset we optimized the window parameter which has to be set for TT, it specifies the number of sentences to the left and to the right of the current position p between two sentences that are used to calculate the coherence score between these sentences (Riedl &amp; Biemann, 2012). The performance for TT has been best with window sizes between 9 and 11 for all metrics as shown in Figure 2. As expected, higher folds increase TTâ€™s overall performance especially with respect to metric b (Figure 3). This is due to the larger training set sizes of the LDA topic model. 0.5 0.4 0.1 0.0 5 6 7 8 9 10 11 12 13 14 15 window Figure 2: TT performance for different window sizes with 30-fold cross validation. 0.4 0.1 0.0 10 20 30 folds Figure 3: TT performance for different folds and window size set to 9. In general smaller window sizes increase the number of predicted boundaries. Th</context>
</contexts>
<marker>Riedl, Biemann, 2012</marker>
<rawString>Riedl, M. &amp; Biemann, C., 2012. TopicTiling: A Text Segmentation Algorithm Based on LDA. Stroudsburg, PA, USA, Association for Computational Linguistics, pp. 37-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Strassel</author>
<author>D Graff</author>
<author>N Martey</author>
<author>C Cieri</author>
</authors>
<date>2000</date>
<booktitle>Quality Control in Large Annotation Projects Involving Multiple Judges: The Case of the TDT Corpora. s.l.,</booktitle>
<pages>s.n.</pages>
<contexts>
<context position="9208" citStr="Strassel, et al., 2000" startWordPosition="1434" endWordPosition="1437">r BS. 3.1 Application Setting and Corpus To evaluate segmenters many different corpora have been created. The most commonly used corpus was introduced by Choi (2000). It is based on the Brown Corpus and contains 700 samples, each containing a fixed number of sentences from 10 different news texts, which are randomly chosen from the Brown Corpus. Two other widely tested corpora were introduced by Galley et al. (2003). Both contain 500 samples, one with concatenated texts from the Wall Street Journal (WSJ) and the other with concatenated texts from the Topic Detection and Tracking (TDT) corpus (Strassel, et al., 2000). A standard for the segmentation of speech is the corpus from the International Computer Science Institute (ICSI) by Janin et al. (2003). A medical text book has been used by Eisenstein and Barzilay (2008). The approaches to evaluate segmenters are always similar: they have to find the boundaries in artificially concatenated texts. We developed our own dataset because we wanted to use text that potentially could be used as a basis for creating e-learning courses. We therefore need samples which, on the one hand, have relatively clear topic boundaries on the macro level and, on the other hand </context>
</contexts>
<marker>Strassel, Graff, Martey, Cieri, 2000</marker>
<rawString>Strassel, S., Graff, D., Martey, N. &amp; Cieri, C., 2000. Quality Control in Large Annotation Projects Involving Multiple Judges: The Case of the TDT Corpora. s.l., s.n.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Sun</author>
<author>R Li</author>
<author>D Luo</author>
<author>X Wu</author>
</authors>
<title>Text Segmentation with LDA-based Fisher Kernel.</title>
<date>2008</date>
<journal>Association for Computational Linguistics,</journal>
<pages>269--272</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="4940" citStr="Sun et al. (2008)" startWordPosition="754" endWordPosition="757">g by Hearst (1997) at least a dozen different segmenters have been developed. They can be divided into linear and hierarchical segmenters. Linear segmenters process the text sequentially sentence by sentence. Hierarchical segmenters first process the whole text and extract topics with varying granularities. These topics are then agglomerated based on a predefined criterion. Linear segmenters have been developed by Kan et al. (1998) and Galley et al. (2003). One of the first probabilistic algorithms has been introduced by Utiyama and Isahara (2001). LDA based approaches were first described by Sun et al. (2008) and improved by Misra et al. (2009). The newest LDA based segmenter is TT. It performs linear text segmentation based on a pretrained LDA topic model and calculates the similarity between segments (adjacent sentences) to measure text coherence on the basis of a topic vector representation using cosine similarity. For reasons of efficiency, only the most frequent topic ID is assigned to each word in the sentence, using Gibbs sampling. Hierarchical text segmentation algorithms were first introduced by Yaari (1997). The latest approach by Eisenstein (2008) uses a generative Bayesian model BS for</context>
</contexts>
<marker>Sun, Li, Luo, Wu, 2008</marker>
<rawString>Sun, Q., Li, R., Luo, D. &amp; Wu, X., 2008. Text Segmentation with LDA-based Fisher Kernel. Stroudsburg, PA, USA, Association for Computational Linguistics, pp. 269-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Swertz</author>
</authors>
<title>A Pedagogical Ontology as a Playground in Adaptive Elearning Environments.. s.l., GI,</title>
<date>2013</date>
<pages>1955--1960</pages>
<marker>Swertz, 2013</marker>
<rawString>Swertz, C. et al., 2013. A Pedagogical Ontology as a Playground in Adaptive Elearning Environments.. s.l., GI, pp. 1955-1960.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Tan</author>
<author>C Ullrich</author>
<author>Y Wang</author>
<author>R Shen</author>
</authors>
<title>The Design and Application of an Automatic Course Generation System for Large-Scale Education. s.l., s.n.,</title>
<date>2010</date>
<pages>607--609</pages>
<contexts>
<context position="4192" citStr="Tan et al. (2010)" startWordPosition="636" endWordPosition="639">elated work on automatic course generation as well as text segmentation applications. In the main sections 3 and 4 we describe our approach and evaluation results on our corpus. In the last section we summarize the presented findings and give an outlook on further research needed for the automatic generation of e-learning courses. 2 Related Work Automatic course generation can roughly be divided into two different areas. One is concerned with generation from existing courses and is mainly focused on adaption to the learner or instructional plans see Lin et al. (2009), Capuno et al. (2009) and Tan et al. (2010). The other area is the course creation itself on which we focus on in this paper. Since the publication of the segmenter TextTiling by Hearst (1997) at least a dozen different segmenters have been developed. They can be divided into linear and hierarchical segmenters. Linear segmenters process the text sequentially sentence by sentence. Hierarchical segmenters first process the whole text and extract topics with varying granularities. These topics are then agglomerated based on a predefined criterion. Linear segmenters have been developed by Kan et al. (1998) and Galley et al. (2003). One of </context>
</contexts>
<marker>Tan, Ullrich, Wang, Shen, 2010</marker>
<rawString>Tan, X., Ullrich, C., Wang, Y. &amp; Shen, R., 2010. The Design and Application of an Automatic Course Generation System for Large-Scale Education. s.l., s.n., pp. 607-609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Utiyama</author>
<author>H Isahara</author>
</authors>
<title>A Statistical Model for Domain-independent Text Segmentation.</title>
<date>2001</date>
<journal>Association for Computational Linguistics,</journal>
<pages>499--506</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="4876" citStr="Utiyama and Isahara (2001)" startWordPosition="743" endWordPosition="746"> focus on in this paper. Since the publication of the segmenter TextTiling by Hearst (1997) at least a dozen different segmenters have been developed. They can be divided into linear and hierarchical segmenters. Linear segmenters process the text sequentially sentence by sentence. Hierarchical segmenters first process the whole text and extract topics with varying granularities. These topics are then agglomerated based on a predefined criterion. Linear segmenters have been developed by Kan et al. (1998) and Galley et al. (2003). One of the first probabilistic algorithms has been introduced by Utiyama and Isahara (2001). LDA based approaches were first described by Sun et al. (2008) and improved by Misra et al. (2009). The newest LDA based segmenter is TT. It performs linear text segmentation based on a pretrained LDA topic model and calculates the similarity between segments (adjacent sentences) to measure text coherence on the basis of a topic vector representation using cosine similarity. For reasons of efficiency, only the most frequent topic ID is assigned to each word in the sentence, using Gibbs sampling. Hierarchical text segmentation algorithms were first introduced by Yaari (1997). The latest appro</context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>Utiyama, M. &amp; Isahara, H., 2001. A Statistical Model for Domain-independent Text Segmentation. Stroudsburg, PA, USA, Association for Computational Linguistics, pp. 499-506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yaari</author>
</authors>
<title>Segmentation of Expository Texts by Hierarchical Agglomerative Clustering.</title>
<date>1997</date>
<tech>s.l.:s.n.</tech>
<contexts>
<context position="5458" citStr="Yaari (1997)" startWordPosition="840" endWordPosition="841"> by Utiyama and Isahara (2001). LDA based approaches were first described by Sun et al. (2008) and improved by Misra et al. (2009). The newest LDA based segmenter is TT. It performs linear text segmentation based on a pretrained LDA topic model and calculates the similarity between segments (adjacent sentences) to measure text coherence on the basis of a topic vector representation using cosine similarity. For reasons of efficiency, only the most frequent topic ID is assigned to each word in the sentence, using Gibbs sampling. Hierarchical text segmentation algorithms were first introduced by Yaari (1997). The latest approach by Eisenstein (2008) uses a generative Bayesian model BS for text segmentation, assuming that a) topic shifts are likely to occur at points marked by cue phrases and b) a linear discourse structure. Each sentence in the document is modeled by a language model associated with a segment. The algorithm then calculates the maximum likelihood estimates of observing the whole sequence of sentences at selected topic boundaries. The applications of text segmentation algorithms range from information retrieval (Huang, et al., 2002) to topic tracking and segmentation of multi-party</context>
</contexts>
<marker>Yaari, 1997</marker>
<rawString>Yaari, Y., 1997. Segmentation of Expository Texts by Hierarchical Agglomerative Clustering. s.l.:s.n.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>