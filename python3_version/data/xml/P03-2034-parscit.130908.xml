<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010516">
<title confidence="0.998212">
A speech interface for open-domain question-answering
</title>
<author confidence="0.998299">
Edward Schofield
</author>
<affiliation confidence="0.98458425">
ftw. Telecommunications Research Center
Vienna, Austria
Department of Computing
Imperial College London, U.K.
</affiliation>
<email confidence="0.989634">
schofield@ftw.at
</email>
<author confidence="0.997983">
Zhiping Zheng
</author>
<affiliation confidence="0.9973705">
Dept. of Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.50226">
Saarbr¨ucken, Germany
</address>
<email confidence="0.995116">
zheng@coli.uni-sb.de
</email>
<sectionHeader confidence="0.995588" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920833333333">
Speech interfaces to question-answering
systems offer significant potential for find-
ing information with phones and mo-
bile networked devices. We describe a
demonstration of spoken question answer-
ing using a commercial dictation engine
whose language models we have cus-
tomized to questions, a Web-based text-
prediction interface allowing quick cor-
rection of errors, and an open-domain
question-answering system, AnswerBus,
which is freely available on the Web. We
describe a small evaluation of the effect
of recognition errors on the precision of
the answers returned and make some con-
crete recommendations for modifying a
question-answering system for improving
robustness to spoken input.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943388888889">
This paper demonstrates a multimodal interface for
asking questions and retrieving a set of likely an-
swers. Such an interface is particularly appropri-
ate for mobile networked devices with screens that
are too small to display general Web pages and
documents. Palm and Pocket PC devices, whose
screens commonly display 10–15 lines, are candi-
dates. Schofield and Kubin (2002) argue that for
such devices question-answering is more appropri-
ate than traditional document retrieval. But until
recently no method has existed for inputting ques-
tions in a reasonable amount of time. The study
of Schofield (2003) concludes that questions tend
to have a limited lexical structure that can be ex-
ploited for accurate speech recognition or text pre-
diction. In this demonstration we test whether this
result can endow a real spoken question answering
system with acceptable precision.
</bodyText>
<sectionHeader confidence="0.988948" genericHeader="introduction">
2 Related research
</sectionHeader>
<bodyText confidence="0.999890224489796">
Kupiec and others (1994) at Xerox labs built one
of the earliest spoken information retrieval systems,
with a speaker-dependent isolated-word speech rec-
ognizer and an electronic encyclopedia. One rea-
son they reported for the success of their system
was their use of simple language models to exploit
the observation that pairs of words co-occurring in
a document source are likely to be spoken together
as keywords in a query. Later research at CMU
built upon similar intuition by deriving the language-
model of their Sphinx-II speech recognizer from
the searched document source. Colineau and others
(1999) developed a system as a part of the THISL
project for retrieval from broadcast news to respond
to news-related queries such as What do you have on
... ? and I am doing a report on ... — can you help
me? The queries the authors addressed had a sim-
ple structure, and they successfully modelled them
in two parts: a question-frame, for which they hand-
wrote grammar rules; and a content-bearing string
of keywords, for which they fitted standard lexical
language-models from the news collection.
Extensive research (Garofolo et al., 2000; Allan,
2001) has concluded that spoken documents can be
effectively indexed and searched with word-error
rates as high as 30–40%. One might expect a much
higher sensitivity to recognition errors with a short
query or natural-language question. Two studies (et
al., 1997; Crestani, 2002) have measured the detri-
mental effect of speech recognition errors on the pre-
cision of document retrieval and found that this task
can be somewhat robust to 25% word-error rates for
queries of 2–8 words.
Two recent systems are worthy of special men-
tion. First, Google Labs deployed a speaker-in-
dependent system in late 2001 as a demo of a
telephone-interface to its popular search engine. (It
is still live as of April 2003.) Second, Chang and
others (2002a; 2002b) have implemented systems
for the Pocket PC that interpret queries spoken in
English or Chinese. This last group appears to be at
the forefront of current research in spoken interfaces
for document retrieval.
None of the above are question-answering sys-
tems; they boil utterances down to strings of key-
words, discarding any other information, and return
only lists of matching documents. To our knowledge
automatic answering of spoken natural-language
questions has not previously been attempted.
</bodyText>
<sectionHeader confidence="0.959912" genericHeader="method">
3 System overview
</sectionHeader>
<bodyText confidence="0.999897">
Our demonstration system has three components: a
commercial speaker-dependent dictation system, a
predictive interface for typing or correcting natural-
language questions, and a Web-based open-domain
question-answering engine. We describe these in
turn.
</bodyText>
<subsectionHeader confidence="0.9999">
3.1 Speech recognizer
</subsectionHeader>
<bodyText confidence="0.999972045454545">
The dictation system is Dragon NaturallySpeaking
6.1, whose language models we have customized
to a large corpus of questions. We performed tests
with a head-mounted microphone in a relatively
quiet acoustic environment. (The Dragon Audio
Setup Wizard identified the signal-to-noise ratio as
22 dBs.) We tested a male native speaker of En-
glish and a female non-native speaker, requesting
each first to train the acoustic models with 5–10 min-
utes of software-prompted dictation.
We also trained the language models by present-
ing the Vocabulary Wizard the corpus of 280,000
questions described in (Schofield, 2003), of which
Table 1 contains a random sample. The primary
function of this training feature in NaturallySpeak-
ing is to add new words to the lexicon; the nature
of the other adaptations is not clearly documented.
New 2-grams and 3-grams also appear to be iden-
tified, which one would expect to reduce the word-
error rate by increasing the ‘hit rate’ over the 30–
50% of 3-grams in a new text for which a language
model typically has explicit frequency estimates.
</bodyText>
<subsectionHeader confidence="0.999294">
3.2 Predictive typing interface
</subsectionHeader>
<bodyText confidence="0.999931933333333">
We have designed a predictive typing interface
whose purpose is to save keystrokes and time in edit-
ing misrecognitions. Such an interface is particu-
larly applicable in a mobile context, in which text
entry is slow and circumstances may prohibit speech
altogether.
We fitted a 3-gram language model to the same
corpus as above using the CMU–Cambridge SLM
Toolkit (Clarkson and Rosenfeld, 1997). The inter-
face in our demo is a thin JavaScript client accessible
from a Web browser that intercepts each keystroke
and performs a CGI request for an updated list of
predictions. The predictions themselves appear as
hyperlinks that modify the question when clicked.
Figure 1 shows a screen-shot.
</bodyText>
<subsectionHeader confidence="0.998631">
3.3 Question-answering system
</subsectionHeader>
<bodyText confidence="0.9770609">
The AnswerBus system (Zheng, 2002) has been run-
ning on the Web since November 2001. It serves
thousands of users every day. The original engine
was not designed for a spoken interface, and we have
recently made modifications in two respects. We de-
scribe these in turn. Later we propose other modifi-
cations that we believe would increase robustness to
a speech interface.
Speed
The original engine took several seconds to an-
swer each question, which may be too slow in a spo-
ken interface or on a mobile device after factoring
in the additional computational overhead of decod-
ing the speech and the longer latency in mobile data
networks. We have now implemented a multi-level
caching system to increase speed.
Our cache system currently contains two levels.
The first is a cache of recently asked questions. If
a question has been asked within a certain period
of time the system will fetch the answers directly
</bodyText>
<tableCaption confidence="0.8625275">
Table 1: A random sample of questions from the cor-
pus.
</tableCaption>
<bodyText confidence="0.975912322580645">
How many people take ibuprofen
What are some work rules
Does GE sell auto insurance
The roxana video diaz
What is the shortest day of the year
Where Can I find Frog T-Shirts
Where can I find cheats for Soul Reaver
for the PC
How can I plug my electric blanket in to
my car cigarette lighter
How can I take home videos and put them
on my computer
What are squamous epithelial cells
from the cache. The second level is a cache of semi-
structured Web documents. If a Web document is in
the cache and has not expired the system will use it
instead of connecting to the remote site. By ‘semi-
structured’ we mean that we cache semi-parsed sen-
tences rather than the original HTML document. We
will discuss some technical issues, like how and how
often to update the cache and how to use hash tables
for fast access, in another paper.
Output
The original engine provided a list of sentences as
hyperlinks to the source documents. This is conve-
nient for Web users but should be transformed for
spoken output. It now offers plain text as an alterna-
tive to HTML for output. 1
We have also made some cosmetic modifications
for small-screen devices like shrinking the large
logo.
</bodyText>
<sectionHeader confidence="0.998917" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9262102">
We evaluated the accuracy of the system subject
to spoken input using 200 test questions from the
TREC 2002 QA track (Voorhees, 2002). AnswerBus
returns snippets from Web pages containing pos-
sible answers; we compared these with the refer-
</bodyText>
<footnote confidence="0.998009">
1See http://www.answerbus.com/voice/
</footnote>
<figureCaption confidence="0.988566">
Figure 1: The interface for rapidly typing
questions and correcting mistranscriptions from
speech. Available at speech.ftw.at/˜ejs/
answerbus
</figureCaption>
<tableCaption confidence="0.691626">
Table 2: % of questions answered correctly from
perfect text versus misrecognized speech.
</tableCaption>
<bodyText confidence="0.990855">
Speaker 1 Speaker 2
Misrecognized speech 39% 26%
Verbatim typing 58% 60%
ence answers used in the TREC competition, over-
riding about 5 negative judgments when we felt
the answers were satisfactory but absent from the
TREC scorecard. For each of these 200 questions
we passed two strings to the AnswerBus engine,
one typed verbatim, the other transcribed from the
speech of one of the people described above. The
results are in Tables 2 and 3.
</bodyText>
<sectionHeader confidence="0.998705" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.992452">
We currently perform no automatic checking or cor-
rection of spelling and no morphological stemming
</bodyText>
<tableCaption confidence="0.895661">
Table 3: # of answers degraded or improved by the
dodgy input.
</tableCaption>
<bodyText confidence="0.8717869">
Speaker 1 Speaker 2
Degraded 12 34
Improved 5 0
of words in the questions. Table 3 indicates that
these features would improve robustness to errors
in speech recognition. We now make some specific
points regarding homographs, which are typically
troublesome for speech recognizers. QA systems
could relatively easily compensate for confusion in
two common classes of homograph:
</bodyText>
<listItem confidence="0.8432565">
• plural nouns ending –s versus possessive nouns
ending –’s or –s’. Our system answered Q39
Where is Devil’s tower?, but not the transcribed
question Where is Devils tower?
• written numbers versus numerals. Our system
could not answer What is slang for a 5 dol-
lar bill? although it could answer Q92 What
is slang for a five dollar bill?.
</listItem>
<bodyText confidence="0.9998709">
More extensive ‘query expansion’ using syn-
onyms or other orthographic forms would be trickier
to implement but could also improve recall. For ex-
ample, Q245 What city in Australia has rainforests?
it answered correctly, but the transcription What city
in Australia has rainforests (without a space), got no
answers. Another example: Q35 Who won the No-
bel Peace Prize in 1992? got no answers, whereas
Who was the winner... ? would have found the right
answer.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999775">
This paper has described a multimodal interface to a
question-answering system designed for rapid input
of questions and correction of speech recognition
errors. The interface for this demo is Web-based,
but should scale to mobile devices. We described a
small evaluation of the system’s accuracy given raw
(uncorrected) transcribed questions from two speak-
ers, which indicates that speech can be used for au-
tomatic question-answering, but that an interface for
correcting misrecognitions is probably necessary for
acceptable accuracy.
In the future we will continue tightening the inte-
gration of the components of the system and port the
interface to phones and Palm or Pocket PC devices.
</bodyText>
<sectionHeader confidence="0.997037" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99932475">
The authors would like to thank Stefan R¨uger for
his suggestions and moral support. Ed Schofield’s
research is supported by a Marie Curie Fellowship
of the European Commission.
</bodyText>
<sectionHeader confidence="0.98989" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999880418604651">
J. Allan. 2001. Perspectives on information retrieval and
speech. Lect. Notes in Comp. Sci., 2273:1.
E. Chang, Helen Meng, Yuk-chi Li, and Tien-ying Fung.
2002a. Efficient web search on mobile devices with
multi-modal input and intelligent text summarization.
In The 11th Int. WWW Conference, May.
E. Chang, F. Seide, H.M. Meng, Z. Chen, S. Yu, and Y.C.
Li. 2002b. A system for spoken query information
retrieval on mobile devices. IEEE Trans. Speech and
Audio Processing, 10(8):531–541, nov.
P. R. Clarkson and R. Rosenfeld. 1997. Statistical lan-
guage modeling using the CMU–Cambridge toolkit.
In Proc. ESCA Eurospeech 1997.
N. Colineau and A. Halber. 1999. A hybrid approach to
spoken query processing in document retrieval system.
In Proc. ESCA Workshop on Accessing Information In
Spoken Audio, pages 31–36.
F. Crestani. 2002. Spoken query processing for interac-
tive information retrieval. Data &amp; Knowledge Engi-
neering, 41(1):105–124, apr.
J. Barnett et al. 1997. Experiments in spoken queries for
document retrieval. In Proc. Eurospeech ’97, pages
1323–1326, Rhodes, Greece.
J. S. Garofolo, C. G. P. Auzanne, and E. M. Voorhees.
2000. The TREC spoken document retrieval track: A
success story. In Proc. Content-Based Multimedia In-
formation Access Conf., apr.
J. Kupiec, D. Kimber, and V. Balasubramanian. 1994.
Speech-based retrieval using semantic co-occurrence
filtering. In Proc. ARPA Human Lang. Tech. Work-
shop, Plainsboro, NJ, mar.
E. Schofield and G. Kubin. 2002. On interfaces for mo-
bile information retrieval. In Proc. 4th Int. Symp. Hu-
man Computer Interaction with Mobile Devices, pages
383–387, sep.
E. Schofield. 2003. Language models for questions. In
Proc. EACL Workshop on Language Modelingfor Text
Entry Methods, apr.
E.M. Voorhees. 2002. Overview of the trec 2002 ques-
tion answering track. In The 11th Text Retrieval Conf.
(TREC 2002). NIST Special Publication: SP 500-251.
Z. Zheng. 2002. AnswerBus question answering system.
In Human Lang. Tech. Conf., San Diego, CA., mar.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.658793">
<title confidence="0.999586">A speech interface for open-domain question-answering</title>
<author confidence="0.999924">Edward Schofield</author>
<affiliation confidence="0.999498">ftw. Telecommunications Research Center</affiliation>
<address confidence="0.989133">Vienna, Austria</address>
<affiliation confidence="0.8815645">Department of Computing Imperial College London, U.K.</affiliation>
<email confidence="0.940723">schofield@ftw.at</email>
<author confidence="0.980961">Zhiping Zheng</author>
<affiliation confidence="0.999955">Dept. of Computational Linguistics Saarland University</affiliation>
<address confidence="0.973698">Saarbr¨ucken, Germany</address>
<email confidence="0.999292">zheng@coli.uni-sb.de</email>
<abstract confidence="0.999059736842105">Speech interfaces to question-answering systems offer significant potential for finding information with phones and mobile networked devices. We describe a demonstration of spoken question answering using a commercial dictation engine whose language models we have customized to questions, a Web-based textprediction interface allowing quick correction of errors, and an open-domain question-answering system, AnswerBus, which is freely available on the Web. We describe a small evaluation of the effect of recognition errors on the precision of the answers returned and make some concrete recommendations for modifying a question-answering system for improving robustness to spoken input.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
</authors>
<title>Perspectives on information retrieval and speech.</title>
<date>2001</date>
<booktitle>Lect. Notes in Comp. Sci.,</booktitle>
<pages>2273--1</pages>
<contexts>
<context position="3069" citStr="Allan, 2001" startWordPosition="465" endWordPosition="466">er from the searched document source. Colineau and others (1999) developed a system as a part of the THISL project for retrieval from broadcast news to respond to news-related queries such as What do you have on ... ? and I am doing a report on ... — can you help me? The queries the authors addressed had a simple structure, and they successfully modelled them in two parts: a question-frame, for which they handwrote grammar rules; and a content-bearing string of keywords, for which they fitted standard lexical language-models from the news collection. Extensive research (Garofolo et al., 2000; Allan, 2001) has concluded that spoken documents can be effectively indexed and searched with word-error rates as high as 30–40%. One might expect a much higher sensitivity to recognition errors with a short query or natural-language question. Two studies (et al., 1997; Crestani, 2002) have measured the detrimental effect of speech recognition errors on the precision of document retrieval and found that this task can be somewhat robust to 25% word-error rates for queries of 2–8 words. Two recent systems are worthy of special mention. First, Google Labs deployed a speaker-independent system in late 2001 as</context>
</contexts>
<marker>Allan, 2001</marker>
<rawString>J. Allan. 2001. Perspectives on information retrieval and speech. Lect. Notes in Comp. Sci., 2273:1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Chang</author>
<author>Helen Meng</author>
<author>Yuk-chi Li</author>
<author>Tien-ying Fung</author>
</authors>
<title>Efficient web search on mobile devices with multi-modal input and intelligent text summarization.</title>
<date>2002</date>
<booktitle>In The 11th Int. WWW Conference,</booktitle>
<marker>Chang, Meng, Li, Fung, 2002</marker>
<rawString>E. Chang, Helen Meng, Yuk-chi Li, and Tien-ying Fung. 2002a. Efficient web search on mobile devices with multi-modal input and intelligent text summarization. In The 11th Int. WWW Conference, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Chang</author>
<author>F Seide</author>
<author>H M Meng</author>
<author>Z Chen</author>
<author>S Yu</author>
<author>Y C Li</author>
</authors>
<title>A system for spoken query information retrieval on mobile devices.</title>
<date>2002</date>
<journal>IEEE Trans. Speech and Audio Processing,</journal>
<volume>10</volume>
<issue>8</issue>
<marker>Chang, Seide, Meng, Chen, Yu, Li, 2002</marker>
<rawString>E. Chang, F. Seide, H.M. Meng, Z. Chen, S. Yu, and Y.C. Li. 2002b. A system for spoken query information retrieval on mobile devices. IEEE Trans. Speech and Audio Processing, 10(8):531–541, nov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Clarkson</author>
<author>R Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU–Cambridge toolkit.</title>
<date>1997</date>
<booktitle>In Proc. ESCA Eurospeech</booktitle>
<contexts>
<context position="6093" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="940" endWordPosition="943">dentified, which one would expect to reduce the worderror rate by increasing the ‘hit rate’ over the 30– 50% of 3-grams in a new text for which a language model typically has explicit frequency estimates. 3.2 Predictive typing interface We have designed a predictive typing interface whose purpose is to save keystrokes and time in editing misrecognitions. Such an interface is particularly applicable in a mobile context, in which text entry is slow and circumstances may prohibit speech altogether. We fitted a 3-gram language model to the same corpus as above using the CMU–Cambridge SLM Toolkit (Clarkson and Rosenfeld, 1997). The interface in our demo is a thin JavaScript client accessible from a Web browser that intercepts each keystroke and performs a CGI request for an updated list of predictions. The predictions themselves appear as hyperlinks that modify the question when clicked. Figure 1 shows a screen-shot. 3.3 Question-answering system The AnswerBus system (Zheng, 2002) has been running on the Web since November 2001. It serves thousands of users every day. The original engine was not designed for a spoken interface, and we have recently made modifications in two respects. We describe these in turn. Late</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>P. R. Clarkson and R. Rosenfeld. 1997. Statistical language modeling using the CMU–Cambridge toolkit. In Proc. ESCA Eurospeech 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Colineau</author>
<author>A Halber</author>
</authors>
<title>A hybrid approach to spoken query processing in document retrieval system.</title>
<date>1999</date>
<booktitle>In Proc. ESCA Workshop on Accessing Information In Spoken Audio,</booktitle>
<pages>31--36</pages>
<marker>Colineau, Halber, 1999</marker>
<rawString>N. Colineau and A. Halber. 1999. A hybrid approach to spoken query processing in document retrieval system. In Proc. ESCA Workshop on Accessing Information In Spoken Audio, pages 31–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Crestani</author>
</authors>
<title>Spoken query processing for interactive information retrieval.</title>
<date>2002</date>
<journal>Data &amp; Knowledge Engineering,</journal>
<volume>41</volume>
<issue>1</issue>
<contexts>
<context position="3343" citStr="Crestani, 2002" startWordPosition="507" endWordPosition="508"> queries the authors addressed had a simple structure, and they successfully modelled them in two parts: a question-frame, for which they handwrote grammar rules; and a content-bearing string of keywords, for which they fitted standard lexical language-models from the news collection. Extensive research (Garofolo et al., 2000; Allan, 2001) has concluded that spoken documents can be effectively indexed and searched with word-error rates as high as 30–40%. One might expect a much higher sensitivity to recognition errors with a short query or natural-language question. Two studies (et al., 1997; Crestani, 2002) have measured the detrimental effect of speech recognition errors on the precision of document retrieval and found that this task can be somewhat robust to 25% word-error rates for queries of 2–8 words. Two recent systems are worthy of special mention. First, Google Labs deployed a speaker-independent system in late 2001 as a demo of a telephone-interface to its popular search engine. (It is still live as of April 2003.) Second, Chang and others (2002a; 2002b) have implemented systems for the Pocket PC that interpret queries spoken in English or Chinese. This last group appears to be at the f</context>
</contexts>
<marker>Crestani, 2002</marker>
<rawString>F. Crestani. 2002. Spoken query processing for interactive information retrieval. Data &amp; Knowledge Engineering, 41(1):105–124, apr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Barnett</author>
</authors>
<title>Experiments in spoken queries for document retrieval.</title>
<date>1997</date>
<booktitle>In Proc. Eurospeech ’97,</booktitle>
<pages>1323--1326</pages>
<location>Rhodes, Greece.</location>
<marker>Barnett, 1997</marker>
<rawString>J. Barnett et al. 1997. Experiments in spoken queries for document retrieval. In Proc. Eurospeech ’97, pages 1323–1326, Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Garofolo</author>
<author>C G P Auzanne</author>
<author>E M Voorhees</author>
</authors>
<title>The TREC spoken document retrieval track: A success story.</title>
<date>2000</date>
<booktitle>In Proc. Content-Based Multimedia Information Access Conf.,</booktitle>
<contexts>
<context position="3055" citStr="Garofolo et al., 2000" startWordPosition="461" endWordPosition="464">hinx-II speech recognizer from the searched document source. Colineau and others (1999) developed a system as a part of the THISL project for retrieval from broadcast news to respond to news-related queries such as What do you have on ... ? and I am doing a report on ... — can you help me? The queries the authors addressed had a simple structure, and they successfully modelled them in two parts: a question-frame, for which they handwrote grammar rules; and a content-bearing string of keywords, for which they fitted standard lexical language-models from the news collection. Extensive research (Garofolo et al., 2000; Allan, 2001) has concluded that spoken documents can be effectively indexed and searched with word-error rates as high as 30–40%. One might expect a much higher sensitivity to recognition errors with a short query or natural-language question. Two studies (et al., 1997; Crestani, 2002) have measured the detrimental effect of speech recognition errors on the precision of document retrieval and found that this task can be somewhat robust to 25% word-error rates for queries of 2–8 words. Two recent systems are worthy of special mention. First, Google Labs deployed a speaker-independent system i</context>
</contexts>
<marker>Garofolo, Auzanne, Voorhees, 2000</marker>
<rawString>J. S. Garofolo, C. G. P. Auzanne, and E. M. Voorhees. 2000. The TREC spoken document retrieval track: A success story. In Proc. Content-Based Multimedia Information Access Conf., apr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>D Kimber</author>
<author>V Balasubramanian</author>
</authors>
<title>Speech-based retrieval using semantic co-occurrence filtering.</title>
<date>1994</date>
<booktitle>In Proc. ARPA Human Lang. Tech. Workshop,</booktitle>
<location>Plainsboro, NJ,</location>
<marker>Kupiec, Kimber, Balasubramanian, 1994</marker>
<rawString>J. Kupiec, D. Kimber, and V. Balasubramanian. 1994. Speech-based retrieval using semantic co-occurrence filtering. In Proc. ARPA Human Lang. Tech. Workshop, Plainsboro, NJ, mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Schofield</author>
<author>G Kubin</author>
</authors>
<title>On interfaces for mobile information retrieval.</title>
<date>2002</date>
<booktitle>In Proc. 4th Int. Symp. Human Computer Interaction with Mobile Devices,</booktitle>
<pages>383--387</pages>
<contexts>
<context position="1397" citStr="Schofield and Kubin (2002)" startWordPosition="192" endWordPosition="195">e Web. We describe a small evaluation of the effect of recognition errors on the precision of the answers returned and make some concrete recommendations for modifying a question-answering system for improving robustness to spoken input. 1 Introduction This paper demonstrates a multimodal interface for asking questions and retrieving a set of likely answers. Such an interface is particularly appropriate for mobile networked devices with screens that are too small to display general Web pages and documents. Palm and Pocket PC devices, whose screens commonly display 10–15 lines, are candidates. Schofield and Kubin (2002) argue that for such devices question-answering is more appropriate than traditional document retrieval. But until recently no method has existed for inputting questions in a reasonable amount of time. The study of Schofield (2003) concludes that questions tend to have a limited lexical structure that can be exploited for accurate speech recognition or text prediction. In this demonstration we test whether this result can endow a real spoken question answering system with acceptable precision. 2 Related research Kupiec and others (1994) at Xerox labs built one of the earliest spoken informatio</context>
</contexts>
<marker>Schofield, Kubin, 2002</marker>
<rawString>E. Schofield and G. Kubin. 2002. On interfaces for mobile information retrieval. In Proc. 4th Int. Symp. Human Computer Interaction with Mobile Devices, pages 383–387, sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Schofield</author>
</authors>
<title>Language models for questions.</title>
<date>2003</date>
<booktitle>In Proc. EACL Workshop on Language Modelingfor Text Entry Methods,</booktitle>
<contexts>
<context position="1628" citStr="Schofield (2003)" startWordPosition="230" endWordPosition="231">ntroduction This paper demonstrates a multimodal interface for asking questions and retrieving a set of likely answers. Such an interface is particularly appropriate for mobile networked devices with screens that are too small to display general Web pages and documents. Palm and Pocket PC devices, whose screens commonly display 10–15 lines, are candidates. Schofield and Kubin (2002) argue that for such devices question-answering is more appropriate than traditional document retrieval. But until recently no method has existed for inputting questions in a reasonable amount of time. The study of Schofield (2003) concludes that questions tend to have a limited lexical structure that can be exploited for accurate speech recognition or text prediction. In this demonstration we test whether this result can endow a real spoken question answering system with acceptable precision. 2 Related research Kupiec and others (1994) at Xerox labs built one of the earliest spoken information retrieval systems, with a speaker-dependent isolated-word speech recognizer and an electronic encyclopedia. One reason they reported for the success of their system was their use of simple language models to exploit the observati</context>
<context position="5210" citStr="Schofield, 2003" startWordPosition="794" endWordPosition="795">zer The dictation system is Dragon NaturallySpeaking 6.1, whose language models we have customized to a large corpus of questions. We performed tests with a head-mounted microphone in a relatively quiet acoustic environment. (The Dragon Audio Setup Wizard identified the signal-to-noise ratio as 22 dBs.) We tested a male native speaker of English and a female non-native speaker, requesting each first to train the acoustic models with 5–10 minutes of software-prompted dictation. We also trained the language models by presenting the Vocabulary Wizard the corpus of 280,000 questions described in (Schofield, 2003), of which Table 1 contains a random sample. The primary function of this training feature in NaturallySpeaking is to add new words to the lexicon; the nature of the other adaptations is not clearly documented. New 2-grams and 3-grams also appear to be identified, which one would expect to reduce the worderror rate by increasing the ‘hit rate’ over the 30– 50% of 3-grams in a new text for which a language model typically has explicit frequency estimates. 3.2 Predictive typing interface We have designed a predictive typing interface whose purpose is to save keystrokes and time in editing misrec</context>
</contexts>
<marker>Schofield, 2003</marker>
<rawString>E. Schofield. 2003. Language models for questions. In Proc. EACL Workshop on Language Modelingfor Text Entry Methods, apr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the trec 2002 question answering track.</title>
<date>2002</date>
<booktitle>In The 11th Text Retrieval Conf. (TREC 2002). NIST Special Publication: SP</booktitle>
<pages>500--251</pages>
<contexts>
<context position="8697" citStr="Voorhees, 2002" startWordPosition="1399" endWordPosition="1400"> some technical issues, like how and how often to update the cache and how to use hash tables for fast access, in another paper. Output The original engine provided a list of sentences as hyperlinks to the source documents. This is convenient for Web users but should be transformed for spoken output. It now offers plain text as an alternative to HTML for output. 1 We have also made some cosmetic modifications for small-screen devices like shrinking the large logo. 4 Evaluation We evaluated the accuracy of the system subject to spoken input using 200 test questions from the TREC 2002 QA track (Voorhees, 2002). AnswerBus returns snippets from Web pages containing possible answers; we compared these with the refer1See http://www.answerbus.com/voice/ Figure 1: The interface for rapidly typing questions and correcting mistranscriptions from speech. Available at speech.ftw.at/˜ejs/ answerbus Table 2: % of questions answered correctly from perfect text versus misrecognized speech. Speaker 1 Speaker 2 Misrecognized speech 39% 26% Verbatim typing 58% 60% ence answers used in the TREC competition, overriding about 5 negative judgments when we felt the answers were satisfactory but absent from the TREC scor</context>
</contexts>
<marker>Voorhees, 2002</marker>
<rawString>E.M. Voorhees. 2002. Overview of the trec 2002 question answering track. In The 11th Text Retrieval Conf. (TREC 2002). NIST Special Publication: SP 500-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zheng</author>
</authors>
<title>AnswerBus question answering system.</title>
<date>2002</date>
<booktitle>In Human Lang. Tech. Conf.,</booktitle>
<location>San Diego, CA.,</location>
<contexts>
<context position="6454" citStr="Zheng, 2002" startWordPosition="998" endWordPosition="999">particularly applicable in a mobile context, in which text entry is slow and circumstances may prohibit speech altogether. We fitted a 3-gram language model to the same corpus as above using the CMU–Cambridge SLM Toolkit (Clarkson and Rosenfeld, 1997). The interface in our demo is a thin JavaScript client accessible from a Web browser that intercepts each keystroke and performs a CGI request for an updated list of predictions. The predictions themselves appear as hyperlinks that modify the question when clicked. Figure 1 shows a screen-shot. 3.3 Question-answering system The AnswerBus system (Zheng, 2002) has been running on the Web since November 2001. It serves thousands of users every day. The original engine was not designed for a spoken interface, and we have recently made modifications in two respects. We describe these in turn. Later we propose other modifications that we believe would increase robustness to a speech interface. Speed The original engine took several seconds to answer each question, which may be too slow in a spoken interface or on a mobile device after factoring in the additional computational overhead of decoding the speech and the longer latency in mobile data network</context>
</contexts>
<marker>Zheng, 2002</marker>
<rawString>Z. Zheng. 2002. AnswerBus question answering system. In Human Lang. Tech. Conf., San Diego, CA., mar.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>