<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.267474">
<title confidence="0.977861">
Acquiring and Using Limited User Models in NLG
</title>
<author confidence="0.995709">
Ehud Reiter, Somayajulu Sripada, and Sandra Williams
</author>
<affiliation confidence="0.998996">
Department of Computer Science
University of Aberdeen
</affiliation>
<email confidence="0.991785">
fereiter,ssripada,swilliaml@csd.abdn.ac.uk
</email>
<sectionHeader confidence="0.998569" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993709">
It is a truism of NLG that good knowl-
edge of the reader can improve the qual-
ity of generated texts, and many NLG
systems have been developed that ex-
ploit detailed user models when gener-
ating texts. Unfortunately, it is very dif-
ficult in practice to obtain detailed in-
formation about users. In this paper
we describe our experiences in acquir-
ing and using limited user models for
NLG in four different systems, each of
which took a different approach to this
issue. One general conclusion is that it
is useful if imperfect user models are
understandable to users or domain ex-
perts, and indeed perhaps can be directly
edited by them; this agrees with re-
cent thinking about user models in other
applications such as intelligent tutoring
systems (Kay, 2001).
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996988372093">
It has long been recognised that NLG systems
should in principle generate texts that are targeted
towards individual readers, and should use de-
tailed models of the readers when doing so. The
content of generated texts should be tailored to the
reader&apos;s tasks and existing knowledge; for exam-
ple, a weather forecast for a pilot landing an air-
plane should focus on wind and visibility at the
destination airport, while a weather forecast for
a farmer planting crops at a farm next to the air-
port should focus on temperature and precipita-
tion. The expression (microplanning) of a gen-
erated texts should be tailored to the user&apos;s lin-
guistic abilities and preferences; for example, a
smoking-cessation letter sent to someone with an
age 10 literacy level should use short sentences
and simple words, while a smoking-cessation let-
ter sent to a doctor with excellent literacy could
use complex sentences and specialised medical
terminology. And the realisation (for example,
grammar) of a text could be tailored to a user&apos;s
dialect, although this is perhaps more debatable.
In other words, people are very different, and texts
intended for individuals will be more effective if
they can be targeted towards that individual.
In accordance with this accepted wisdom, many
NLG systems and models allow detailed user mod-
els to be specified. For example, plan-based
content determination (Appelt, 1985; Moore and
Paris, 1993) is based on detailed models of user
tasks and goals, and dialect or even ideolect gram-
mars can be specified for realisation engines such
as SURGE (Elhadad and Robin, 1997) and KPML
(Bateman, 1997). Zukerman and Litman (2001)
review how user models have been used in a vari-
ety of NLG and NLU systems.
Unfortunately, we are not aware of any NLG
systems which actually use detailed user models
with non-trivial numbers of users, probably be-
cause of the difficulty of acquiring detailed user
models. Such models can of course be hand-
crafted for demonstration purposes for a single
user performing a single task, but we are not aware
</bodyText>
<page confidence="0.998081">
87
</page>
<bodyText confidence="0.999704872727273">
of any successful systems based on detailed user
models which work for a non-trivial number of
real users.
The reality of NLG today is that any system with
a non-trivial number of users has imperfect infor-
mation about its users. It may know something
about them, but its knowledge is far from com-
plete. This raises an important question for NLG -
what is the best way to acquire and use limited and
imperfect information about users? Little has been
published about this topic in the NLG literature;
Zukerman and Litman&apos;s (2001) review, for exam-
ple, says little about this topic, other than suggest-
ing that perhaps user models can be built up during
the course of a dialogue with the user.
We have struggled with this question in the
course of building several NLG systems — IDAS,
STOP, SUMTIME-MOUSAM, and GIRL - and used
a different approach in each of these systems. In
this paper we summarise the approaches we have
taken and how well they seemed to work. We cer-
tainly do not have any definitive answers, but we
hope our paper will at least clarify the issue. Also,
perhaps one general lesson from our work is that it
is helpful if imperfect user models are understand-
able to users or at least to domain experts. Under
some circumstances this could allow people to edit
and thus directly control their model; even if this
is not possible, users are likely to be more helpful
in the model acquisition process if they understand
how the model is going to be used.
These observations fit in with recent thinking
in the general user-modelling community. Fis-
cher (2001) acknowledges that user modelling has
been less successful than originally hoped, and
suggests that in part this could be due to the dif-
ficulty of creating effective user models, and prob-
lems in dealing with models that are incorrect, in-
complete, and/or out-of-date. Kay (2001) suggests
that user models should be scrutable (understand-
able and modifiable) to the user, because (among
other things) this allows users to understand what
the system is doing and to correct mistakes.
Note that while research has been done on plan-
ning under uncertainty, the focus of such work
is contexts where the uncertainty is well under-
stood; for example, an object is in one of two lo-
cations (Collins and Pryor, 1995), or a diagnos-
tic test has a well-understood false positive and
false negative rate (Haddawy et al., 1995). Under
these conditions it is possible to produce plans that
are optimal under some cost or effectiveness crite-
ria. However, in our experience the level of un-
certainty in user models for NLG greatly exceeds
what can be handled by such approaches.
</bodyText>
<sectionHeader confidence="0.921931" genericHeader="introduction">
2 IDAS: User in Control
</sectionHeader>
<bodyText confidence="0.999949384615385">
One approach to the imperfect user knowledge
problem is to generate a text using whatever user
knowledge is available (which may not be much),
and then allow the user to request additional infor-
mation, clarifications, and so forth. This of course
is the approach used by human speakers in dia-
logues, and indeed by many computer dialogue
systems.
It was also used in IDAS (Reiter et al., 1995), an
NLG system developed in the early 1990s which
dynamically generated hypertext technical docu-
mentation from an Al knowledge base. IDAS had
a facility which allowed detailed user models to
be used during the generation process; but in fact
no detailed models of real users were created for
IDAS. Instead, users in experimental trials of IDAS
would use its hypertext links to obtain more in-
formation if they needed it (Levine and Mellish,
1995).
For example, consider the question of how
much detail should be in an instructional text on
how to change a flat tire on a bicycle. Should such
a text use high-level instructions such as remove
the front wheel, or should it use more detailed
instructions such as lift the front wheel&apos;s quick-
release lever? The original IDAS vision was to
make this choice on the basis of a detailed user
model which stated which high-level actions the
user already knew how to perform, and which
needed to be broken down into substeps. However,
in practice it was not possible to acquire such de-
tailed information about IDAS users. Instead, IDAS
produced a guess at an appropriate instruction se-
quence, often based on a coarse expert/novice dis-
tinction between users, and then allowed the user
to click on a text if he or she wanted more infor-
mation. For example, IDAS might initially produce
the high-level text remove the front wheel, and the
user could expand this into substeps (such as lift
</bodyText>
<page confidence="0.998061">
88
</page>
<bodyText confidence="0.9994489">
the front wheel&apos;s quick-release lever) by clicking
on the original instruction.
Of course this strategy only makes sense if the
user understands what questions he can ask. This
requires both a good user interface and also an in-
tuitive and easily understandable &apos;question space&apos;
(using IDAS &apos;s terminology) of what questions the
system can answer.
Letting the user specify what he or she wants to
know is not always possible; for example, it is not
possible in a system which generates paper letters
such as STOP, and may not be realistic in a system
used by people with limited computer confidence
such as GIRL. It also may sometimes be risky, for
example if a user thought he knew how to remove
a wheel but in fact did not. But it certainly seems
to be an effective strategy in many situations, be-
cause users usually have excellent knowledge of
their own goals, tasks, and expertise, much better
than any computer system.
</bodyText>
<sectionHeader confidence="0.815894" genericHeader="method">
3 STOP: Ask User for Key Information
</sectionHeader>
<bodyText confidence="0.999992675675676">
Another response to the difficulty of getting per-
fect user knowledge is to try to determine which
knowledge about the user is most important, and
then design a questionnaire or GUI to explicitly
acquire this knowledge. In such cases we may
need to impose a size or time-to-complete con-
straint on the questionnaire or GUI, based on what
we think is realistic for the target user group.
This approach was used in STOP (Reiter et
al., 2003), which generated personalised smoking-
cessation letters. We devised a 4-page multiple-
choice questionnaire for smokers based on what
previous research suggested would be the most
important information for the letter-tailoring pro-
cess. The STOP software then used this question-
naire (which was completed on paper, and scanned
into a database) as its primary information source
when generating tailored smoking-cessation let-
ters; some information was also obtained from
the smoker&apos;s medical record. The user question-
naire data only effected content decisions in STOP;
in principle it would have been desirable to also
take user information into account when making
microplanning and realisation (expression) deci-
sions, but this was not done.
One problem we encountered was that in ret-
rospect the information elicited by the question-
naire was perhaps not the most important infor-
mation needed by the tailoring process. For exam-
ple, although we asked users about their smoking
habits, beliefs, and concerns, we did not directly
ask them what information they would like to see
in the generated letter (for example, medical in-
formation about the effects of smoking vs. practi-
cal how-to-stop advice); in hindsight we probably
should have asked for this information. But this is
not a flaw with the technique, it is a flaw in how
we applied it.
A perhaps more basic problem is that we
were limited to acquiring small amounts of well-
structured information. We could not acquire
a lot of information (since smokers would not
spend more than 10-15 minutes on a question-
naire), and we could not acquire unstructured in-
formation such as free-text explanations of inter-
ests and goals (since such texts could not be inter-
preted and understood by our software). For ex-
ample, one key issue in smoking advice is why
previous attempts to quit have failed. Our ques-
tionnaire had seven check boxes for standard rea-
sons such as stress or weight gain. It did not elicit
detailed information which turned out to be very
important to individual smokers, such as one per-
son&apos;s frustration with hypnosis techniques, and an-
other&apos;s promising attempt to quit being derailed by
stress caused by the death of a relative.
Also, the questionnaire approach of course only
works if the user understands and can answer the
questions. For example, detailed medical informa-
tion about the smoker&apos;s health, such as the con-
dition of his lungs, would have been useful but
in general we could not expect smokers to have
this information. Another example is whether the
smoker is addicted to nicotine (again important in-
formation for selecting appropriate cessation ad-
vice). Many smokers have incorrect beliefs about
whether they are addicted, so instead of directly
asking this question, STOP inferred addiction sta-
tus from a set of questions devised by Fagerstrom
and colleagues (Heatherton et al., 1991), such as
whether the smoker smoked within 30 minutes of
waking up.
In other words, in IDAS we believed that users
themselves had the best knowledge of relevant in-
</bodyText>
<page confidence="0.99884">
89
</page>
<bodyText confidence="0.999842555555556">
formation such as their tasks and goals. In STOP,
however, we believed that users might not have
good self-knowledge of some important informa-
tion, such as addiction status.
In summary, a questionnaire can work well if
we need a small amount of well-structured infor-
mation, and we believe that users have (and will
provide) this information. Otherwise, we should
consider other approaches.
</bodyText>
<sectionHeader confidence="0.924806" genericHeader="method">
4 SumTime-Mousam: Domain Expert
Creates a Model
</sectionHeader>
<bodyText confidence="0.999985500000001">
Another approach to creating user models that use-
fully approximate reality is to get a domain ex-
pert (or &apos;knowledge engineer&apos;) to build the model.
That is, the domain expert meets with users and
discusses their needs and constraints, and from
this develops a user model for a software system.
This approach was used in SUMTIME-
MOUSAM (Sripada et al., 2002), which generates
weather forecasts for offshore oil rigs; these are
essentially summaries of the output of a numerical
weather simulation, where the summarisation is
controlled by a model of what is important to
the user. Such forecasts are used by oil company
staff to make specific decisions, for example on
how to unload supply boats and when to schedule
diving operations. If we had perfect knowledge
about what decisions needed to be made and
what the constraints on these decisions were (for
example, what sea conditions were too rough for
the particular diving equipment currently at a rig),
then we could generate perfectly tailored forecasts
using plan-based techniques. Unfortunately, this
information is not available.
Instead, we included in SumTimE-MousAm
some parameters which made sense to expert me-
teorologists (such as what changes in wind speed
were significant in different contexts), and let an
expert set the parameters appropriately for differ-
ent users. The expert had previously discussed
with the end user what the user&apos;s tasks and needs
were, and based the parameter settings on these
discussions and on his expertise. These parame-
ters essentially constituted a &apos;user model&apos; which
was intended to apply to an entire rig, and cover
all types of operations.
We have not yet evaluated the usefulness of the
generated forecasts with end users, so we do not
know how effective this strategy is. However, we
can make some observations. First of all, building
such models requires making a tradeoff between
the range of situations they cover and their effec-
tiveness. A model which covered all possible de-
cisions would insist that all data from the numer-
ical simulation be communicated, and no data be
summarised; this would negate the usefulness of
textual summaries. On the other hand, a model
that was tailored just to the most typical decisions
would generate texts that were well suited to those
situations, but not to others.
For example, in general light winds (less than
15 knots) have minimal impact on rig operations,
so when the wind is light there is no need to report
(for instance) a small change in wind direction,
such as N to NNE. However, there are some un-
usual operations, such as flaring gas, when small
changes in wind direction are relevant even with
light winds. If we had perfect user knowledge we
would report such changes if and only if the user
was flaring gas or otherwise doing an operation for
which this information was important. However,
since we do not know what operations are planned
for a particular day, we have to choose between
either never reporting such changes (which some-
what improves forecast quality for most days by
eliminating unnecessary information), or always
reporting such changes (which greatly improves
forecast quality on those rare days when the in-
formation is important).
One solution to this problem would be to al-
low users to explicitly specify information about
their planned tasks and known constraints (such as
whether they planned to flare gas), via a software
package which was installed on the rigs. We have
not seriously investigated this because of the sub-
stantial cost of developing, installing, and main-
taining such a system, plus the cost of training
users so that they entered the correct information
at the correct time. In other words, SumTimE-
MOUSAM as it currently exists fits smoothly into
current operational procedures and does not re-
quire users to install new hardware or software or
change the way they work; this would not be true
of a system which required users to enter informa-
tion about their tasks and constraints.
</bodyText>
<page confidence="0.990493">
90
</page>
<bodyText confidence="0.999863807692307">
Another issue with SumTimE-MousAm&apos;s ap-
proach is that the parameters and model must be
understandable to the domain expert. The first
versions of SumTimE-Mous Am used fairly sim-
ple and hence understandable algorithms for data
analysis and text generation, but the most recent
version of the system uses more sophisticated al-
gorithms which are less easy for someone who is
not a computer scientist to understand. Hence it is
harder for the domain expert to build models for
the most recent version of the system than for pre-
vious versions.
Finally, economics means that domain experts
cannot continually create new models, the models
they creates must be usable for a period of time.
This means that the usage of the texts must be
fairly stable.
In summary, asking domain experts to build
models that approximate how a group of users will
use texts can work if the texts are used in ways
that are predictable, limited, and stable; and if the
user model is understandable to the domain expert.
We expect this may be true in other NLG appli-
cations in addition to weather forecasts (financial
summaries?), and encourage other researchers to
consider this approach when it seems appropriate.
</bodyText>
<sectionHeader confidence="0.967939" genericHeader="method">
5 GIRL: Obtain Model by Testing Users
</sectionHeader>
<bodyText confidence="0.999985784313725">
The final approach we have tried is building a
model of a user&apos;s skills by testing his performance
on a set of tasks, using an independently devel-
oped assessment test.
We are using this approach in a new system,
GIRL (Williams, 2002), which generates reports
on how well a student has done in a computer-
based literacy assessment. From a research per-
spective, GIRL &apos;s focus is on making microplan-
ning choices (aggregation, word choice, etc.) that
are appropriate for the recipient&apos;s reading ability.
For example, an aggregation decision that leads to
a 30-word sentence is acceptable for a good reader
but not for a poor reader. This requires know-
ing how well the recipient can read, and GIRL ob-
tains this information from the literacy assessment
which the student has completed. The assessment
was independently developed by NFER-Nelson as
a skill-based reading, writing and listening test for
adult literacy learners. The results give a hierar-
chical model of derived literacy skills, with results
of individual questions at the leaf nodes, results of
skills tests at the next level, overall reading, overall
writing and overall listening levels at the next and
the overall literacy level at the root of the tree. Part
of the experimental work in GIRL is determining
which information from this model is most use-
ful in guiding expression choices. The ultimate
vision in GIRL is to create user models for read-
ers which include this key information, and then
use the reader model to control microplanning and
perhaps also content and realisation choices in a
text-generation system.
We cannot say much about the success or fail-
ure of this approach yet, as GIRL is still being de-
veloped and has not yet been evaluated. But cer-
tainly it seems likely that we cannot expect to ob-
tain a large amount of information using this strat-
egy; like the STOP strategy of asking a user to
fill out a questionnaire, the strategy of testing the
user is feasible if a small amount of information is
needed, but not otherwise.
It is possible that in the long term, a lot of obser-
vational data about users will be available, which
perhaps can be analysed and &apos;mined&apos; for user in-
formation (Fitzgibbon and Reiter, 2002). For ex-
ample, if we knew what web pages a person had
read, we could probably make a plausible guess
about his or her literacy level. This is an interest-
ing possibility which should be kept in mind for
the future, even if it is not realistic now.
</bodyText>
<sectionHeader confidence="0.999564" genericHeader="method">
6 Discussion
</sectionHeader>
<subsectionHeader confidence="0.999834">
6.1 Understandability of User Models
</subsectionHeader>
<bodyText confidence="0.999921272727273">
One recurring theme in our work is that user mod-
els should be understandable to users or domain
experts. If we had perfect user models, users per-
haps would not care how they worked; but since
we have imperfect user models, it is very helpful
if users can understand them, either to edit them or
to understand what the system is doing and how it
might fail.
Kay (2001, page 118) discusses giving users ac-
cess to and control over models about themselves
in intelligent tutoring systems, and argues that this
</bodyText>
<page confidence="0.990355">
91
</page>
<figure confidence="0.955764909090909">
Precip. Change Twe j Misc. Total Cloud Precip Change
Wind Sig We I Max Wave i Wave Period
1
Blizzard I Floodin1 I Visibility 1 Hurricane 1 Cloud
Precip. Intensity
Switches I Precip. Type
VisRain
reduced to 1-3 in precipitation
reduced to 1-3 in precipitation
reduced to 1-3 in precipitation
VisS now
</figure>
<figureCaption confidence="0.9462976">
reduced to 4-6 in precipitation
reduced to 1-3 in precipitation
reduced to 1-3 in precipitation
reduced to 1-3 in precipitation
reduced to 1-3 in precipitation
</figureCaption>
<figure confidence="0.998506076923077">
Operator i Rain Amount Rain Snow
4
.7
12
20
10-0000,0
. moderate to heaw heaw
heavy heavy
hew/ heaw
0.2 . light . light $
. light to moderate . moderate reduced to 5-8 in precipitation
moderate moderate to heavy&apos; reduced to 3-5 in precipitation
Update I Cancel
</figure>
<figureCaption confidence="0.999946">
Figure 1: Precipitation Intensity Options Box from SumTimE-Mous Am
</figureCaption>
<bodyText confidence="0.725458">
is desirable because:
</bodyText>
<listItem confidence="0.998629307692308">
• Users have a legal and ethical right to access
and control about themselves.
• Users can assess the correctness of the model,
and repair mistakes.
• Users who have access to their user models
will have a better idea of what the system is
doing.
• Developers will be more accountable if mod-
els can be inspected by users, and will place
more importance on making models under-
standable.
• Users may learn useful information about
themselves from their user models.
</listItem>
<bodyText confidence="0.9999454">
Although the last reason may be specific to tu-
toring systems, we believe that the first four ra-
tionales give by Kay for giving users control over
their models are likely to apply to NLG systems
that use user models as well.
Many commercial software packages allow
users to specify their preferences and needs via
an options box, and we have in fact done this to
a limited extent in SumTimE-MousAm. A sim-
ple SumTimE-MousAm options box is shown
in Figure 1; this box allows the domain expert
to specify how numerical precipitation figures (in
mm/hr) should be translated into linguistic de-
scriptors such as heavy. This is user-dependent
because different readers interpret heavy in dif-
ferent ways (Reiter and Sripada, 2002). Among
other things, the interpretation depends on loca-
tion; a precipitation amount that would be consid-
ered heavy in a dry environment such as the Mid-
dle East might not be considered heavy in a wet
environment such as Scotland.
Perhaps the key technical challenge to building
such options boxes or other model-editing facili-
ties is making choices understandable to users and
domain experts. In particular we cannot expect
such people to have expertise in linguistics; this is
why the descriptors in Figure 1 are specified as ac-
tual adjectival phrases instead of as conceptual or
semantic representations. We also cannot expect
such people to have expertise in computer science;
this is an issue in some of the other SumTimE-
MOU SAM options boxes, where the expert is ex-
pected to specify parameters that control a linear
segmentation algorithm (Sripada et al., 2002). Fi-
nally, if the options box is intended to be filled
out by users instead of domain experts, we should
not expect an unrealistic amount of domain knowl-
edge. For example the options box in Figure 1
may not be understandable to end users, because
they may not understand precipitation expressed
as mm/hr.
In short, we need to be able to present user
model choices in an understandable way to people
who do not have expertise in linguistics or com-
puter science; how best to do this is an important
topic for future research. This problem is likely
to become even more acute if we try to learn user
models from large amounts of observational data,
as many learning algorithms do not produce re-
sults which are easy for people to understand.
</bodyText>
<page confidence="0.986397">
92
</page>
<subsectionHeader confidence="0.9422905">
6.2 Knowledge Source: Users or Domain
(Communication) Experts
</subsectionHeader>
<bodyText confidence="0.999956913043478">
An important related issue is who supplies the in-
formation for a user model. In IDAS, STOP and
GIRL the users themselves supply the basic infor-
mation (which the system may make inferences
from), but in SumTimE-MousAm the informa-
tion comes from a forecaster, that is a domain ex-
pert who knows the users. The forecaster is also
a domain communication expert (Kittredge et al.,
1991); that is, he is an expert in how to commu-
nicate information about the weather to users, as
well as an expert in meteorology itself.
The disadvantage of using a domain (commu-
nication) expert is that he is supplying informa-
tion secondhand, he does not know the users as
well as the users know themselves. If we con-
sider Figure 1, for example, presumably the users
themselves have a better idea of what heavy means
to them than the domain expert does. But on
the other hand, the advantage of using a domain
expert is that he will have more domain knowl-
edge (for example, understand precipitation ex-
pressed as mmihr), more understanding of how the
NLG system works and how it is controlled by the
user model, and also more domain communication
knowledge (for example, know how heavy was de-
fined for other users). Because he understands the
domain, the user, and the system, the domain ex-
pert can be a good person to create a model that
&apos;bridges the gap&apos; between the user and the system,
and specifies to the system, in terms it can under-
stand, what is important to the user and how best
to communicate with the user linguistically.
The other reason for acquiring user models from
domain experts in SumTimE-MousAm was that
S umTimE-MousAm weather forecasts are read
by many people in an oil rig, not just a single in-
dividual. Hence the user model needs to be an ap-
proximation to a group of people, not a detailed
description of a single person. In other words,
the user model should record how the recipients
on average interpret heavy, not how one individ-
ual interprets this word. In SumTimE-MousAm
we believed that it might in fact be easiest for an
outside expert to create such a model, especially
if the expert had experience in doing this for other
user groups.
</bodyText>
<subsectionHeader confidence="0.999616">
6.3 Cost of Mistakes
</subsectionHeader>
<bodyText confidence="0.999931738095238">
Another general issue that needs to be considered
is the impact of getting the user model wrong. For
example, what is the impact of careless mistakes
in STOP questionnaires, of users failing to give the
S umTImE-Mous Am domain expert complete in-
formation about all of their tasks and lexical inter-
pretations, or of a GIRL subject doing poorly on a
test because she hadn&apos;t had much sleep the previ-
ous night? Do such mistakes marginally decrease
the utility of generated texts, or do they make gen-
erated texts completely useless? NLG systems that
are not robust in the face of such mistakes may not
be useful in real-world applications.
This may be an especially serious problem for
systems such as STOP and GIRL which make in-
ferences about the user based on the information
he or she explicitly specifies. For example, if a
smoker does not realise that STOP uses the &apos;do you
smoke within 30 minutes of waking up&apos; question
to infer addiction level, and instead believes that
this question is unimportant, then he or she may
ignore it or respond very quickly without thinking
about the question.
This suggests that at minimum we need to
carefully design the user model acquisition tool
(STOP questionnaire, SumTimE-Mous Am option
boxes, GIRL literacy test) to minimise careless er-
rors. It may also be sensible to check user models
for plausibility and consistency (as STOP in fact
does). Finally, this once again emphasises the de-
sirability of users understanding what is in the user
model and how the system uses it.
A related issue is how to update and maintain
user models. People of course change over time,
and user models should change with them. We are
not aware of any previous research on how user
models for NLG systems should be updated to re-
flect changes in the user&apos;s expertise or preferences.
Probably the simplest approach here is simply to
let users or domain experts directly edit and up-
date a user model, which is possible in SUMTIME-
MOUSAM.
</bodyText>
<sectionHeader confidence="0.999277" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9959355">
One of the great promises of NLG is that it
can tailor texts for individual users, but realising
</bodyText>
<page confidence="0.995923">
93
</page>
<bodyText confidence="0.99998525">
this promise requires that we know a lot about
users, and in practice it can be difficult to obtain
detailed information about the expertise, back-
ground, tasks, goals, and so forth of individual
users. We have tried various approaches to ac-
quiring information about users for NLG systems,
including letting users implicitly specify models
(IDAs), explicitly asking users to enter a model
(STOP), explicitly asking a domain expert to con-
struct a model (SumTimE-MousAm), and im-
plicitly inferring a model from a standard assess-
ment test (GIRL). None of these approaches seem
generally applicable, but all probably work better
if users can easily understand their models, so that
they can edit their model or at least better under-
stand what the NLG system is doing.
</bodyText>
<sectionHeader confidence="0.991645" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999841875">
Many thanks to Chris Mellish, Ingrid Zuker-
man and the anonymous reviewers for their help-
ful comments. This work was supported by
the UK Engineering and Physical Sciences Re-
search Council (EPSRC), under a PhD Stu-
dentship and grants GR/F36750/01, GR/L48812
and GR/M76681; and by the Scottish Office De-
partment of Health under grant K/OPR/2/2/D318.
</bodyText>
<sectionHeader confidence="0.998275" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998741805970149">
Doug Appelt. 1985. Planning English Referring Ex-
pressions. Cambridge University Press, New York.
John Bateman. 1997. Enabling technology for multi-
lingual natural language generation: the KPML de-
velopment environment. Natural Language Engi-
neering, 3:15-55.
Gregg Collins and Louise Pryor. 1995. Planning un-
der uncertainty: Some key issues. In Proceedings of
the 14th International Joint Conference on Artificial
Intelligence (IJCAI), pages 1567-1573.
Michael Elhadad and Jacques Robin. 1997. SURGE:
A comprehensive plug-in syntactic realisation com-
ponent for text generation. Technical report, Com-
puter Science Dept, Ben-Gurion University, Beer
Sheva, Israel.
Gerhard Fischer. 2001. User modelling in human-
computer interaction. User Modeling and User-
Adapted Interaction, 11:65-86.
Andrew Fitzgibbon and Ehud Reiter. 2002. Memories
for life: Managing information over a human life-
time. Technical Report AUCS/TR0207, Department
of Computing Science, University of Aberdeen, UK.
Peter Haddawy, AnHai Doan, and Richard Goodwin.
1995. Efficient decision-theoretic planning: Tech-
niques and empirical analysis. In Proc. Eleventh
Conf on Uncertainty in Artificial Intelligence, pages
229-236.
Todd Heatherton, Lynn Kozlowski, Richard Frecker,
and Karl-Olav Fagerstrom. 1991. The Fagerstrom
test for nicotine dependence: A revision of the
Fagerstrom tolerance questionnaire. British Journal
of Addiction, 86:1119-1127.
Judy Kay. 2001. Learner control. User Modeling and
User-Adapted Interaction, 11:111-127.
Richard Kittredge, Tanya Korelsky, and Owen Ram-
bow. 1991. On the need for domain communication
language. Computational Intelligence, 7(4):305-
314.
John Levine and Chris Mellish. 1995. The IDAS user
trials: Quantitative evaluation of an applied natu-
ral language generation system. In Proceedings of
the Fifth European Workshop on Natural Language
Generation, pages 75-93, Leiden, The Netherlands.
Johanna Moore and Cecile Paris. 1993. Planning text
for advisory dialogues. Computational Linguistics,
19:651-694.
Ehud Reiter and Somayajulu Sripada. 2002. Human
variation and lexical choice. Computational Lin-
guistics, 28:545-553.
Ehud Reiter, Chris Mellish, and John Levine. 1995.
Automatic generation of technical documentation.
Applied Artificial Intelligence, 9(3):259-287.
Ehud Reiter, Roma Robertson, and Liesl Osman. 2003.
Lessons from a failure: Generating tailored smoking
cessation letters. Artificial Intelligence, in press.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2002. Segmenting time series for weather fore-
casting. In Applications and Innovations in Intelli-
gent Systems X, pages 105-118. Springer-Verlag.
Sandra Williams. 2002. Natural language generation
of discourse connectives for different reading levels.
In Proceedings of the 5th Annual CLUK Research
Colloquium.
Ingrid Zukerman and Diane Litman. 2001. Natural
language processing and user modeling: Synergies
and limitations. User Modeling and User-Adapted
Interaction, 11:129-158.
</reference>
<page confidence="0.999541">
94
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.450981">
<title confidence="0.999844">Acquiring and Using Limited User Models in NLG</title>
<author confidence="0.992293">Ehud Reiter</author>
<author confidence="0.992293">Somayajulu Sripada</author>
<author confidence="0.992293">Sandra</author>
<affiliation confidence="0.9995575">Department of Computer University of</affiliation>
<email confidence="0.995909">fereiter,ssripada,swilliaml@csd.abdn.ac.uk</email>
<abstract confidence="0.9998491">is a truism of good knowledge of the reader can improve the qualof generated texts, and many systems have been developed that exploit detailed user models when generating texts. Unfortunately, it is very difficult in practice to obtain detailed information about users. In this paper we describe our experiences in acquiring and using limited user models for four different systems, each of which took a different approach to this issue. One general conclusion is that it is useful if imperfect user models are understandable to users or domain experts, and indeed perhaps can be directly edited by them; this agrees with recent thinking about user models in other applications such as intelligent tutoring</abstract>
<note confidence="0.458616">systems (Kay, 2001).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Doug Appelt</author>
</authors>
<title>Planning English Referring Expressions.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="2365" citStr="Appelt, 1985" startWordPosition="381" endWordPosition="382">and simple words, while a smoking-cessation letter sent to a doctor with excellent literacy could use complex sentences and specialised medical terminology. And the realisation (for example, grammar) of a text could be tailored to a user&apos;s dialect, although this is perhaps more debatable. In other words, people are very different, and texts intended for individuals will be more effective if they can be targeted towards that individual. In accordance with this accepted wisdom, many NLG systems and models allow detailed user models to be specified. For example, plan-based content determination (Appelt, 1985; Moore and Paris, 1993) is based on detailed models of user tasks and goals, and dialect or even ideolect grammars can be specified for realisation engines such as SURGE (Elhadad and Robin, 1997) and KPML (Bateman, 1997). Zukerman and Litman (2001) review how user models have been used in a variety of NLG and NLU systems. Unfortunately, we are not aware of any NLG systems which actually use detailed user models with non-trivial numbers of users, probably because of the difficulty of acquiring detailed user models. Such models can of course be handcrafted for demonstration purposes for a singl</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>Doug Appelt. 1985. Planning English Referring Expressions. Cambridge University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bateman</author>
</authors>
<title>Enabling technology for multilingual natural language generation: the KPML development environment.</title>
<date>1997</date>
<journal>Natural Language Engineering,</journal>
<pages>3--15</pages>
<contexts>
<context position="2586" citStr="Bateman, 1997" startWordPosition="419" endWordPosition="420">tailored to a user&apos;s dialect, although this is perhaps more debatable. In other words, people are very different, and texts intended for individuals will be more effective if they can be targeted towards that individual. In accordance with this accepted wisdom, many NLG systems and models allow detailed user models to be specified. For example, plan-based content determination (Appelt, 1985; Moore and Paris, 1993) is based on detailed models of user tasks and goals, and dialect or even ideolect grammars can be specified for realisation engines such as SURGE (Elhadad and Robin, 1997) and KPML (Bateman, 1997). Zukerman and Litman (2001) review how user models have been used in a variety of NLG and NLU systems. Unfortunately, we are not aware of any NLG systems which actually use detailed user models with non-trivial numbers of users, probably because of the difficulty of acquiring detailed user models. Such models can of course be handcrafted for demonstration purposes for a single user performing a single task, but we are not aware 87 of any successful systems based on detailed user models which work for a non-trivial number of real users. The reality of NLG today is that any system with a non-tr</context>
</contexts>
<marker>Bateman, 1997</marker>
<rawString>John Bateman. 1997. Enabling technology for multilingual natural language generation: the KPML development environment. Natural Language Engineering, 3:15-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregg Collins</author>
<author>Louise Pryor</author>
</authors>
<title>Planning under uncertainty: Some key issues.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>1567--1573</pages>
<contexts>
<context position="5292" citStr="Collins and Pryor, 1995" startWordPosition="888" endWordPosition="891">ed, and suggests that in part this could be due to the difficulty of creating effective user models, and problems in dealing with models that are incorrect, incomplete, and/or out-of-date. Kay (2001) suggests that user models should be scrutable (understandable and modifiable) to the user, because (among other things) this allows users to understand what the system is doing and to correct mistakes. Note that while research has been done on planning under uncertainty, the focus of such work is contexts where the uncertainty is well understood; for example, an object is in one of two locations (Collins and Pryor, 1995), or a diagnostic test has a well-understood false positive and false negative rate (Haddawy et al., 1995). Under these conditions it is possible to produce plans that are optimal under some cost or effectiveness criteria. However, in our experience the level of uncertainty in user models for NLG greatly exceeds what can be handled by such approaches. 2 IDAS: User in Control One approach to the imperfect user knowledge problem is to generate a text using whatever user knowledge is available (which may not be much), and then allow the user to request additional information, clarifications, and </context>
</contexts>
<marker>Collins, Pryor, 1995</marker>
<rawString>Gregg Collins and Louise Pryor. 1995. Planning under uncertainty: Some key issues. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI), pages 1567-1573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
<author>Jacques Robin</author>
</authors>
<title>SURGE: A comprehensive plug-in syntactic realisation component for text generation.</title>
<date>1997</date>
<tech>Technical report,</tech>
<institution>Computer Science Dept, Ben-Gurion University, Beer Sheva, Israel.</institution>
<contexts>
<context position="2561" citStr="Elhadad and Robin, 1997" startWordPosition="413" endWordPosition="416">ample, grammar) of a text could be tailored to a user&apos;s dialect, although this is perhaps more debatable. In other words, people are very different, and texts intended for individuals will be more effective if they can be targeted towards that individual. In accordance with this accepted wisdom, many NLG systems and models allow detailed user models to be specified. For example, plan-based content determination (Appelt, 1985; Moore and Paris, 1993) is based on detailed models of user tasks and goals, and dialect or even ideolect grammars can be specified for realisation engines such as SURGE (Elhadad and Robin, 1997) and KPML (Bateman, 1997). Zukerman and Litman (2001) review how user models have been used in a variety of NLG and NLU systems. Unfortunately, we are not aware of any NLG systems which actually use detailed user models with non-trivial numbers of users, probably because of the difficulty of acquiring detailed user models. Such models can of course be handcrafted for demonstration purposes for a single user performing a single task, but we are not aware 87 of any successful systems based on detailed user models which work for a non-trivial number of real users. The reality of NLG today is that</context>
</contexts>
<marker>Elhadad, Robin, 1997</marker>
<rawString>Michael Elhadad and Jacques Robin. 1997. SURGE: A comprehensive plug-in syntactic realisation component for text generation. Technical report, Computer Science Dept, Ben-Gurion University, Beer Sheva, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Fischer</author>
</authors>
<title>User modelling in humancomputer interaction. User Modeling and UserAdapted Interaction,</title>
<date>2001</date>
<pages>11--65</pages>
<contexts>
<context position="4590" citStr="Fischer (2001)" startWordPosition="771" endWordPosition="773">work. We certainly do not have any definitive answers, but we hope our paper will at least clarify the issue. Also, perhaps one general lesson from our work is that it is helpful if imperfect user models are understandable to users or at least to domain experts. Under some circumstances this could allow people to edit and thus directly control their model; even if this is not possible, users are likely to be more helpful in the model acquisition process if they understand how the model is going to be used. These observations fit in with recent thinking in the general user-modelling community. Fischer (2001) acknowledges that user modelling has been less successful than originally hoped, and suggests that in part this could be due to the difficulty of creating effective user models, and problems in dealing with models that are incorrect, incomplete, and/or out-of-date. Kay (2001) suggests that user models should be scrutable (understandable and modifiable) to the user, because (among other things) this allows users to understand what the system is doing and to correct mistakes. Note that while research has been done on planning under uncertainty, the focus of such work is contexts where the uncer</context>
</contexts>
<marker>Fischer, 2001</marker>
<rawString>Gerhard Fischer. 2001. User modelling in humancomputer interaction. User Modeling and UserAdapted Interaction, 11:65-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Fitzgibbon</author>
<author>Ehud Reiter</author>
</authors>
<title>Memories for life: Managing information over a human lifetime.</title>
<date>2002</date>
<tech>Technical Report AUCS/TR0207,</tech>
<institution>Department of Computing Science, University of Aberdeen, UK.</institution>
<contexts>
<context position="19749" citStr="Fitzgibbon and Reiter, 2002" startWordPosition="3318" endWordPosition="3321">ation system. We cannot say much about the success or failure of this approach yet, as GIRL is still being developed and has not yet been evaluated. But certainly it seems likely that we cannot expect to obtain a large amount of information using this strategy; like the STOP strategy of asking a user to fill out a questionnaire, the strategy of testing the user is feasible if a small amount of information is needed, but not otherwise. It is possible that in the long term, a lot of observational data about users will be available, which perhaps can be analysed and &apos;mined&apos; for user information (Fitzgibbon and Reiter, 2002). For example, if we knew what web pages a person had read, we could probably make a plausible guess about his or her literacy level. This is an interesting possibility which should be kept in mind for the future, even if it is not realistic now. 6 Discussion 6.1 Understandability of User Models One recurring theme in our work is that user models should be understandable to users or domain experts. If we had perfect user models, users perhaps would not care how they worked; but since we have imperfect user models, it is very helpful if users can understand them, either to edit them or to under</context>
</contexts>
<marker>Fitzgibbon, Reiter, 2002</marker>
<rawString>Andrew Fitzgibbon and Ehud Reiter. 2002. Memories for life: Managing information over a human lifetime. Technical Report AUCS/TR0207, Department of Computing Science, University of Aberdeen, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Haddawy</author>
<author>AnHai Doan</author>
<author>Richard Goodwin</author>
</authors>
<title>Efficient decision-theoretic planning: Techniques and empirical analysis.</title>
<date>1995</date>
<booktitle>In Proc. Eleventh Conf on Uncertainty in Artificial Intelligence,</booktitle>
<pages>229--236</pages>
<contexts>
<context position="5398" citStr="Haddawy et al., 1995" startWordPosition="906" endWordPosition="909">lems in dealing with models that are incorrect, incomplete, and/or out-of-date. Kay (2001) suggests that user models should be scrutable (understandable and modifiable) to the user, because (among other things) this allows users to understand what the system is doing and to correct mistakes. Note that while research has been done on planning under uncertainty, the focus of such work is contexts where the uncertainty is well understood; for example, an object is in one of two locations (Collins and Pryor, 1995), or a diagnostic test has a well-understood false positive and false negative rate (Haddawy et al., 1995). Under these conditions it is possible to produce plans that are optimal under some cost or effectiveness criteria. However, in our experience the level of uncertainty in user models for NLG greatly exceeds what can be handled by such approaches. 2 IDAS: User in Control One approach to the imperfect user knowledge problem is to generate a text using whatever user knowledge is available (which may not be much), and then allow the user to request additional information, clarifications, and so forth. This of course is the approach used by human speakers in dialogues, and indeed by many computer </context>
</contexts>
<marker>Haddawy, Doan, Goodwin, 1995</marker>
<rawString>Peter Haddawy, AnHai Doan, and Richard Goodwin. 1995. Efficient decision-theoretic planning: Techniques and empirical analysis. In Proc. Eleventh Conf on Uncertainty in Artificial Intelligence, pages 229-236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Todd Heatherton</author>
<author>Lynn Kozlowski</author>
<author>Richard Frecker</author>
<author>Karl-Olav Fagerstrom</author>
</authors>
<title>The Fagerstrom test for nicotine dependence: A revision of the Fagerstrom tolerance questionnaire.</title>
<date>1991</date>
<journal>British Journal of Addiction,</journal>
<pages>86--1119</pages>
<contexts>
<context position="11749" citStr="Heatherton et al., 1991" startWordPosition="1976" endWordPosition="1979">works if the user understands and can answer the questions. For example, detailed medical information about the smoker&apos;s health, such as the condition of his lungs, would have been useful but in general we could not expect smokers to have this information. Another example is whether the smoker is addicted to nicotine (again important information for selecting appropriate cessation advice). Many smokers have incorrect beliefs about whether they are addicted, so instead of directly asking this question, STOP inferred addiction status from a set of questions devised by Fagerstrom and colleagues (Heatherton et al., 1991), such as whether the smoker smoked within 30 minutes of waking up. In other words, in IDAS we believed that users themselves had the best knowledge of relevant in89 formation such as their tasks and goals. In STOP, however, we believed that users might not have good self-knowledge of some important information, such as addiction status. In summary, a questionnaire can work well if we need a small amount of well-structured information, and we believe that users have (and will provide) this information. Otherwise, we should consider other approaches. 4 SumTime-Mousam: Domain Expert Creates a Mo</context>
</contexts>
<marker>Heatherton, Kozlowski, Frecker, Fagerstrom, 1991</marker>
<rawString>Todd Heatherton, Lynn Kozlowski, Richard Frecker, and Karl-Olav Fagerstrom. 1991. The Fagerstrom test for nicotine dependence: A revision of the Fagerstrom tolerance questionnaire. British Journal of Addiction, 86:1119-1127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judy Kay</author>
</authors>
<date>2001</date>
<booktitle>Learner control. User Modeling and User-Adapted Interaction,</booktitle>
<pages>11--111</pages>
<contexts>
<context position="959" citStr="Kay, 2001" startWordPosition="154" endWordPosition="155">at exploit detailed user models when generating texts. Unfortunately, it is very difficult in practice to obtain detailed information about users. In this paper we describe our experiences in acquiring and using limited user models for NLG in four different systems, each of which took a different approach to this issue. One general conclusion is that it is useful if imperfect user models are understandable to users or domain experts, and indeed perhaps can be directly edited by them; this agrees with recent thinking about user models in other applications such as intelligent tutoring systems (Kay, 2001). 1 Introduction It has long been recognised that NLG systems should in principle generate texts that are targeted towards individual readers, and should use detailed models of the readers when doing so. The content of generated texts should be tailored to the reader&apos;s tasks and existing knowledge; for example, a weather forecast for a pilot landing an airplane should focus on wind and visibility at the destination airport, while a weather forecast for a farmer planting crops at a farm next to the airport should focus on temperature and precipitation. The expression (microplanning) of a genera</context>
<context position="4867" citStr="Kay (2001)" startWordPosition="817" endWordPosition="818">tances this could allow people to edit and thus directly control their model; even if this is not possible, users are likely to be more helpful in the model acquisition process if they understand how the model is going to be used. These observations fit in with recent thinking in the general user-modelling community. Fischer (2001) acknowledges that user modelling has been less successful than originally hoped, and suggests that in part this could be due to the difficulty of creating effective user models, and problems in dealing with models that are incorrect, incomplete, and/or out-of-date. Kay (2001) suggests that user models should be scrutable (understandable and modifiable) to the user, because (among other things) this allows users to understand what the system is doing and to correct mistakes. Note that while research has been done on planning under uncertainty, the focus of such work is contexts where the uncertainty is well understood; for example, an object is in one of two locations (Collins and Pryor, 1995), or a diagnostic test has a well-understood false positive and false negative rate (Haddawy et al., 1995). Under these conditions it is possible to produce plans that are opt</context>
<context position="20412" citStr="Kay (2001" startWordPosition="3444" endWordPosition="3445">read, we could probably make a plausible guess about his or her literacy level. This is an interesting possibility which should be kept in mind for the future, even if it is not realistic now. 6 Discussion 6.1 Understandability of User Models One recurring theme in our work is that user models should be understandable to users or domain experts. If we had perfect user models, users perhaps would not care how they worked; but since we have imperfect user models, it is very helpful if users can understand them, either to edit them or to understand what the system is doing and how it might fail. Kay (2001, page 118) discusses giving users access to and control over models about themselves in intelligent tutoring systems, and argues that this 91 Precip. Change Twe j Misc. Total Cloud Precip Change Wind Sig We I Max Wave i Wave Period 1 Blizzard I Floodin1 I Visibility 1 Hurricane 1 Cloud Precip. Intensity Switches I Precip. Type VisRain reduced to 1-3 in precipitation reduced to 1-3 in precipitation reduced to 1-3 in precipitation VisS now reduced to 4-6 in precipitation reduced to 1-3 in precipitation reduced to 1-3 in precipitation reduced to 1-3 in precipitation reduced to 1-3 in precipitati</context>
</contexts>
<marker>Kay, 2001</marker>
<rawString>Judy Kay. 2001. Learner control. User Modeling and User-Adapted Interaction, 11:111-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Kittredge</author>
<author>Tanya Korelsky</author>
<author>Owen Rambow</author>
</authors>
<title>On the need for domain communication language.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<pages>7--4</pages>
<contexts>
<context position="24599" citStr="Kittredge et al., 1991" startWordPosition="4157" endWordPosition="4160">n more acute if we try to learn user models from large amounts of observational data, as many learning algorithms do not produce results which are easy for people to understand. 92 6.2 Knowledge Source: Users or Domain (Communication) Experts An important related issue is who supplies the information for a user model. In IDAS, STOP and GIRL the users themselves supply the basic information (which the system may make inferences from), but in SumTimE-MousAm the information comes from a forecaster, that is a domain expert who knows the users. The forecaster is also a domain communication expert (Kittredge et al., 1991); that is, he is an expert in how to communicate information about the weather to users, as well as an expert in meteorology itself. The disadvantage of using a domain (communication) expert is that he is supplying information secondhand, he does not know the users as well as the users know themselves. If we consider Figure 1, for example, presumably the users themselves have a better idea of what heavy means to them than the domain expert does. But on the other hand, the advantage of using a domain expert is that he will have more domain knowledge (for example, understand precipitation expres</context>
</contexts>
<marker>Kittredge, Korelsky, Rambow, 1991</marker>
<rawString>Richard Kittredge, Tanya Korelsky, and Owen Rambow. 1991. On the need for domain communication language. Computational Intelligence, 7(4):305-314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Levine</author>
<author>Chris Mellish</author>
</authors>
<title>The IDAS user trials: Quantitative evaluation of an applied natural language generation system.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fifth European Workshop on Natural Language Generation,</booktitle>
<pages>75--93</pages>
<location>Leiden, The Netherlands.</location>
<contexts>
<context position="6508" citStr="Levine and Mellish, 1995" startWordPosition="1095" endWordPosition="1098">ons, and so forth. This of course is the approach used by human speakers in dialogues, and indeed by many computer dialogue systems. It was also used in IDAS (Reiter et al., 1995), an NLG system developed in the early 1990s which dynamically generated hypertext technical documentation from an Al knowledge base. IDAS had a facility which allowed detailed user models to be used during the generation process; but in fact no detailed models of real users were created for IDAS. Instead, users in experimental trials of IDAS would use its hypertext links to obtain more information if they needed it (Levine and Mellish, 1995). For example, consider the question of how much detail should be in an instructional text on how to change a flat tire on a bicycle. Should such a text use high-level instructions such as remove the front wheel, or should it use more detailed instructions such as lift the front wheel&apos;s quickrelease lever? The original IDAS vision was to make this choice on the basis of a detailed user model which stated which high-level actions the user already knew how to perform, and which needed to be broken down into substeps. However, in practice it was not possible to acquire such detailed information a</context>
</contexts>
<marker>Levine, Mellish, 1995</marker>
<rawString>John Levine and Chris Mellish. 1995. The IDAS user trials: Quantitative evaluation of an applied natural language generation system. In Proceedings of the Fifth European Workshop on Natural Language Generation, pages 75-93, Leiden, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna Moore</author>
<author>Cecile Paris</author>
</authors>
<title>Planning text for advisory dialogues.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--651</pages>
<contexts>
<context position="2389" citStr="Moore and Paris, 1993" startWordPosition="383" endWordPosition="386">ds, while a smoking-cessation letter sent to a doctor with excellent literacy could use complex sentences and specialised medical terminology. And the realisation (for example, grammar) of a text could be tailored to a user&apos;s dialect, although this is perhaps more debatable. In other words, people are very different, and texts intended for individuals will be more effective if they can be targeted towards that individual. In accordance with this accepted wisdom, many NLG systems and models allow detailed user models to be specified. For example, plan-based content determination (Appelt, 1985; Moore and Paris, 1993) is based on detailed models of user tasks and goals, and dialect or even ideolect grammars can be specified for realisation engines such as SURGE (Elhadad and Robin, 1997) and KPML (Bateman, 1997). Zukerman and Litman (2001) review how user models have been used in a variety of NLG and NLU systems. Unfortunately, we are not aware of any NLG systems which actually use detailed user models with non-trivial numbers of users, probably because of the difficulty of acquiring detailed user models. Such models can of course be handcrafted for demonstration purposes for a single user performing a sing</context>
</contexts>
<marker>Moore, Paris, 1993</marker>
<rawString>Johanna Moore and Cecile Paris. 1993. Planning text for advisory dialogues. Computational Linguistics, 19:651-694.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Somayajulu Sripada</author>
</authors>
<title>Human variation and lexical choice.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--545</pages>
<contexts>
<context position="22550" citStr="Reiter and Sripada, 2002" startWordPosition="3807" endWordPosition="3810">ionales give by Kay for giving users control over their models are likely to apply to NLG systems that use user models as well. Many commercial software packages allow users to specify their preferences and needs via an options box, and we have in fact done this to a limited extent in SumTimE-MousAm. A simple SumTimE-MousAm options box is shown in Figure 1; this box allows the domain expert to specify how numerical precipitation figures (in mm/hr) should be translated into linguistic descriptors such as heavy. This is user-dependent because different readers interpret heavy in different ways (Reiter and Sripada, 2002). Among other things, the interpretation depends on location; a precipitation amount that would be considered heavy in a dry environment such as the Middle East might not be considered heavy in a wet environment such as Scotland. Perhaps the key technical challenge to building such options boxes or other model-editing facilities is making choices understandable to users and domain experts. In particular we cannot expect such people to have expertise in linguistics; this is why the descriptors in Figure 1 are specified as actual adjectival phrases instead of as conceptual or semantic representa</context>
</contexts>
<marker>Reiter, Sripada, 2002</marker>
<rawString>Ehud Reiter and Somayajulu Sripada. 2002. Human variation and lexical choice. Computational Linguistics, 28:545-553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Chris Mellish</author>
<author>John Levine</author>
</authors>
<title>Automatic generation of technical documentation.</title>
<date>1995</date>
<journal>Applied Artificial Intelligence,</journal>
<pages>9--3</pages>
<contexts>
<context position="6062" citStr="Reiter et al., 1995" startWordPosition="1021" endWordPosition="1024">roduce plans that are optimal under some cost or effectiveness criteria. However, in our experience the level of uncertainty in user models for NLG greatly exceeds what can be handled by such approaches. 2 IDAS: User in Control One approach to the imperfect user knowledge problem is to generate a text using whatever user knowledge is available (which may not be much), and then allow the user to request additional information, clarifications, and so forth. This of course is the approach used by human speakers in dialogues, and indeed by many computer dialogue systems. It was also used in IDAS (Reiter et al., 1995), an NLG system developed in the early 1990s which dynamically generated hypertext technical documentation from an Al knowledge base. IDAS had a facility which allowed detailed user models to be used during the generation process; but in fact no detailed models of real users were created for IDAS. Instead, users in experimental trials of IDAS would use its hypertext links to obtain more information if they needed it (Levine and Mellish, 1995). For example, consider the question of how much detail should be in an instructional text on how to change a flat tire on a bicycle. Should such a text u</context>
</contexts>
<marker>Reiter, Mellish, Levine, 1995</marker>
<rawString>Ehud Reiter, Chris Mellish, and John Levine. 1995. Automatic generation of technical documentation. Applied Artificial Intelligence, 9(3):259-287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Roma Robertson</author>
<author>Liesl Osman</author>
</authors>
<title>Lessons from a failure: Generating tailored smoking cessation letters. Artificial Intelligence,</title>
<date>2003</date>
<note>in press.</note>
<contexts>
<context position="8889" citStr="Reiter et al., 2003" startWordPosition="1511" endWordPosition="1514">n many situations, because users usually have excellent knowledge of their own goals, tasks, and expertise, much better than any computer system. 3 STOP: Ask User for Key Information Another response to the difficulty of getting perfect user knowledge is to try to determine which knowledge about the user is most important, and then design a questionnaire or GUI to explicitly acquire this knowledge. In such cases we may need to impose a size or time-to-complete constraint on the questionnaire or GUI, based on what we think is realistic for the target user group. This approach was used in STOP (Reiter et al., 2003), which generated personalised smokingcessation letters. We devised a 4-page multiplechoice questionnaire for smokers based on what previous research suggested would be the most important information for the letter-tailoring process. The STOP software then used this questionnaire (which was completed on paper, and scanned into a database) as its primary information source when generating tailored smoking-cessation letters; some information was also obtained from the smoker&apos;s medical record. The user questionnaire data only effected content decisions in STOP; in principle it would have been des</context>
</contexts>
<marker>Reiter, Robertson, Osman, 2003</marker>
<rawString>Ehud Reiter, Roma Robertson, and Liesl Osman. 2003. Lessons from a failure: Generating tailored smoking cessation letters. Artificial Intelligence, in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Somayajulu Sripada</author>
<author>Ehud Reiter</author>
<author>Jim Hunter</author>
<author>Jin Yu</author>
</authors>
<title>Segmenting time series for weather forecasting.</title>
<date>2002</date>
<booktitle>In Applications and Innovations in Intelligent Systems X,</booktitle>
<pages>105--118</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="12708" citStr="Sripada et al., 2002" startWordPosition="2136" endWordPosition="2139">. In summary, a questionnaire can work well if we need a small amount of well-structured information, and we believe that users have (and will provide) this information. Otherwise, we should consider other approaches. 4 SumTime-Mousam: Domain Expert Creates a Model Another approach to creating user models that usefully approximate reality is to get a domain expert (or &apos;knowledge engineer&apos;) to build the model. That is, the domain expert meets with users and discusses their needs and constraints, and from this develops a user model for a software system. This approach was used in SUMTIMEMOUSAM (Sripada et al., 2002), which generates weather forecasts for offshore oil rigs; these are essentially summaries of the output of a numerical weather simulation, where the summarisation is controlled by a model of what is important to the user. Such forecasts are used by oil company staff to make specific decisions, for example on how to unload supply boats and when to schedule diving operations. If we had perfect knowledge about what decisions needed to be made and what the constraints on these decisions were (for example, what sea conditions were too rough for the particular diving equipment currently at a rig), </context>
<context position="23416" citStr="Sripada et al., 2002" startWordPosition="3950" endWordPosition="3953"> technical challenge to building such options boxes or other model-editing facilities is making choices understandable to users and domain experts. In particular we cannot expect such people to have expertise in linguistics; this is why the descriptors in Figure 1 are specified as actual adjectival phrases instead of as conceptual or semantic representations. We also cannot expect such people to have expertise in computer science; this is an issue in some of the other SumTimEMOU SAM options boxes, where the expert is expected to specify parameters that control a linear segmentation algorithm (Sripada et al., 2002). Finally, if the options box is intended to be filled out by users instead of domain experts, we should not expect an unrealistic amount of domain knowledge. For example the options box in Figure 1 may not be understandable to end users, because they may not understand precipitation expressed as mm/hr. In short, we need to be able to present user model choices in an understandable way to people who do not have expertise in linguistics or computer science; how best to do this is an important topic for future research. This problem is likely to become even more acute if we try to learn user mod</context>
</contexts>
<marker>Sripada, Reiter, Hunter, Yu, 2002</marker>
<rawString>Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin Yu. 2002. Segmenting time series for weather forecasting. In Applications and Innovations in Intelligent Systems X, pages 105-118. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Williams</author>
</authors>
<title>Natural language generation of discourse connectives for different reading levels.</title>
<date>2002</date>
<booktitle>In Proceedings of the 5th Annual CLUK Research Colloquium.</booktitle>
<contexts>
<context position="17787" citStr="Williams, 2002" startWordPosition="2986" endWordPosition="2987">exts can work if the texts are used in ways that are predictable, limited, and stable; and if the user model is understandable to the domain expert. We expect this may be true in other NLG applications in addition to weather forecasts (financial summaries?), and encourage other researchers to consider this approach when it seems appropriate. 5 GIRL: Obtain Model by Testing Users The final approach we have tried is building a model of a user&apos;s skills by testing his performance on a set of tasks, using an independently developed assessment test. We are using this approach in a new system, GIRL (Williams, 2002), which generates reports on how well a student has done in a computerbased literacy assessment. From a research perspective, GIRL &apos;s focus is on making microplanning choices (aggregation, word choice, etc.) that are appropriate for the recipient&apos;s reading ability. For example, an aggregation decision that leads to a 30-word sentence is acceptable for a good reader but not for a poor reader. This requires knowing how well the recipient can read, and GIRL obtains this information from the literacy assessment which the student has completed. The assessment was independently developed by NFER-Nel</context>
</contexts>
<marker>Williams, 2002</marker>
<rawString>Sandra Williams. 2002. Natural language generation of discourse connectives for different reading levels. In Proceedings of the 5th Annual CLUK Research Colloquium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Zukerman</author>
<author>Diane Litman</author>
</authors>
<title>Natural language processing and user modeling: Synergies and limitations. User Modeling and User-Adapted Interaction,</title>
<date>2001</date>
<pages>11--129</pages>
<contexts>
<context position="2614" citStr="Zukerman and Litman (2001)" startWordPosition="421" endWordPosition="424">er&apos;s dialect, although this is perhaps more debatable. In other words, people are very different, and texts intended for individuals will be more effective if they can be targeted towards that individual. In accordance with this accepted wisdom, many NLG systems and models allow detailed user models to be specified. For example, plan-based content determination (Appelt, 1985; Moore and Paris, 1993) is based on detailed models of user tasks and goals, and dialect or even ideolect grammars can be specified for realisation engines such as SURGE (Elhadad and Robin, 1997) and KPML (Bateman, 1997). Zukerman and Litman (2001) review how user models have been used in a variety of NLG and NLU systems. Unfortunately, we are not aware of any NLG systems which actually use detailed user models with non-trivial numbers of users, probably because of the difficulty of acquiring detailed user models. Such models can of course be handcrafted for demonstration purposes for a single user performing a single task, but we are not aware 87 of any successful systems based on detailed user models which work for a non-trivial number of real users. The reality of NLG today is that any system with a non-trivial number of users has im</context>
</contexts>
<marker>Zukerman, Litman, 2001</marker>
<rawString>Ingrid Zukerman and Diane Litman. 2001. Natural language processing and user modeling: Synergies and limitations. User Modeling and User-Adapted Interaction, 11:129-158.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>