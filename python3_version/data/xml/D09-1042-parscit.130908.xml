<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000084">
<title confidence="0.904049">
Natural Language Generation with Tree Conditional Random Fields
</title>
<author confidence="0.94079">
Wei Lu&apos;, Hwee Tou Ng&apos; 2, Wee Sun Lee&apos; ,2
</author>
<affiliation confidence="0.909921666666667">
&apos;Singapore-MIT Alliance
2Department of Computer Science
National University of Singapore
</affiliation>
<email confidence="0.955646">
luwei@nus.edu.sg
{nght,leews}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.994647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999569625">
This paper presents an effective method
for generating natural language sentences
from their underlying meaning represen-
tations. The method is built on top of
a hybrid tree representation that jointly
encodes both the meaning representation
as well as the natural language in a tree
structure. By using a tree conditional
random field on top of the hybrid tree
representation, we are able to explicitly
model phrase-level dependencies amongst
neighboring natural language phrases and
meaning representation components in a
simple and natural way. We show that
the additional dependencies captured by
the tree conditional random field allows it
to perform better than directly inverting a
previously developed hybrid tree semantic
parser. Furthermore, we demonstrate that
the model performs better than a previ-
ous state-of-the-art natural language gen-
eration model. Experiments are performed
on two benchmark corpora with standard
automatic evaluation metrics.
</bodyText>
<sectionHeader confidence="0.998889" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970944444445">
One of the ultimate goals in the field of natural lan-
guage processing (NLP) is to enable computers to
converse with humans through human languages.
To achieve this goal, two important issues need
to be studied. First, it is important for comput-
ers to capture the meaning of a natural language
sentence in a meaning representation. Second,
computers should be able to produce a human-
understandable natural language sentence from its
meaning representation. These two tasks are re-
ferred to as semantic parsing and natural language
generation (NLG), respectively.
In this paper, we use corpus-based statistical
methods for constructing a natural language gener-
ation system. Given a set of pairs, where each pair
consists of a natural language (NL) sentence and
its formal meaning representation (MR), a learn-
ing method induces an algorithm that can be used
for performing language generation from other
previously unseen meaning representations.
A crucial question in any natural language pro-
cessing system is the representation used. Mean-
ing representations can be in the form of a tree
structure. In Lu et al. (2008), we introduced a
hybrid tree framework together with a probabilis-
tic generative model to tackle semantic parsing,
where tree structured meaning representations are
used. The hybrid tree gives a natural joint tree rep-
resentation of a natural language sentence and its
meaning representation.
A joint generative model for natural language
and its meaning representation, such as that used
in Lu et al. (2008) has several advantages over var-
ious previous approaches designed for semantic
parsing. First, unlike most previous approaches,
the generative approach models a simultaneous
generation process for both NL and MR. One el-
egant property of such a joint generative model
is that it allows the modeling of both semantic
parsing and natural language generation within the
same process. Second, the generative process pro-
ceeds as a recursive top-down Markov process in
a way that takes advantage of the tree structure
of the MR. The hybrid tree generative model pro-
posed in Lu et al. (2008) was shown to give state-
of-the-art accuracy in semantic parsing on bench-
mark corpora.
While semantic parsing with hybrid trees has
been studied in Lu et al. (2008), its inverse task
– NLG with hybrid trees – has not yet been ex-
plored. We believe that the properties that make
the hybrid trees effective for semantic parsing also
make them effective for NLG. In this paper, we de-
velop systems for the generation task by building
</bodyText>
<page confidence="0.961823">
400
</page>
<note confidence="0.9966175">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 400–409,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.998962413793103">
on top of the generative model introduced in Lu et
al. (2008) (referred to as the LNLZ08 system).
We first present a baseline model by directly
“inverting” the LNLZ08 system, where an NL sen-
tence is generated word by word. We call this
model the direct inversion model. This model is
unable to model some long range global depen-
dencies over the entire NL sentence to be gener-
ated. To tackle several weaknesses exhibited by
the baseline model, we next introduce an alterna-
tive, novel model that performs generation at the
phrase level. Motivated by conditional random
fields (CRF) (Lafferty et al., 2001), a different pa-
rameterization of the conditional probability of the
hybrid tree that enables the model to encode some
longer range dependencies amongst phrases and
MRs is used. This novel model is referred to as
the tree CRF-based model.
Evaluation results for both models are pre-
sented, through which we demonstrate that the tree
CRF-based model performs better than the direct
inversion model. We also compare the tree CRF-
based model against the previous state-of-the-art
model of Wong and Mooney (2007). Further-
more, we evaluate our model on a dataset anno-
tated with several natural languages other than En-
glish (Japanese, Spanish, and Turkish). Evalua-
tion results show that our proposed tree CRF-based
model outperforms the previous model.
</bodyText>
<sectionHeader confidence="0.999732" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999822351351351">
There have been substantial earlier research ef-
forts on investigating methods for transforming
MR to their corresponding NL sentences. Most
of the recent systems tackled the problem through
the architecture of chart generation introduced by
Kay (1996). Examples of such systems include
the chart generator for Head-Driven Phrase Struc-
ture Grammar (HPSG) (Carroll et al., 1999; Car-
roll and Oepen, 2005; Nakanishi et al., 2005), and
more recently for Combinatory Categorial Gram-
mar (CCG) (White and Baldridge, 2003; White,
2004). However, most of these systems only fo-
cused on surface realization (inflection and order-
ing of NL words) and ignored lexical selection
(learning the mappings from MR domain concepts
to NL words).
The recent work by Wong and Mooney (2007)
explored methods for generation by inverting a
system originally designed for semantic pars-
ing. They introduced a system named WASP−1
that employed techniques from statistical ma-
chine translation using Synchronous Context-Free
Grammar (SCFG) (Aho and Ullman, 1972). The
system took in a linearized MR tree as input, and
translated it into a natural language sentence as
output. Unlike most previous systems, their sys-
tem integrated both lexical selection and surface
realization in a single framework. The perfor-
mance of the system was enhanced by incorpo-
rating models borrowed from PHARAOH (Koehn,
2004). Experiments show that this new hybrid
system named WASP−1++ gives state-of-the-art
accuracies and outperforms the direct translation
model obtained from PHARAOH, when evaluated
on two corpora. We will compare our system’s
performance against that of WASP−1++ in Sec-
tion 5.
</bodyText>
<sectionHeader confidence="0.9598865" genericHeader="method">
3 The Hybrid Tree Framework and the
LNLZ08 System
</sectionHeader>
<figure confidence="0.3466275">
QUERY: answer(RIVER)
RIVER: longest(RIVER)
RIVER: exclude(RIVER1 RIVER2)
RIVER: river(all) RIVER: traverse(STATE)
STATE: stateid(STATENAME)
STATENAME : texas
</figure>
<figureCaption confidence="0.88824475">
what is the longest river that
does not run through texas
Figure 1: An example MR paired with its NL sen-
tence.
</figureCaption>
<bodyText confidence="0.999871692307692">
Following most previous works in this
area (Kate et al., 2005; Ge and Mooney, 2005;
Kate and Mooney, 2006; Wong and Mooney,
2006; Lu et al., 2008), we consider MRs in the
form of tree structures. An example MR and
its corresponding natural language sentence are
shown in Figure 1. The MR is a tree consisting
of nodes called MR productions. For example,
the node “QUERY : answer(RIVER)” is one MR
production. Each MR production consists of a
semantic category (“QUERY”), a function symbol
(“answer”) which can be optionally omitted, as
well as an argument list which possibly contains
</bodyText>
<page confidence="0.995878">
401
</page>
<figure confidence="0.979991875">
what is
QUERY: answer(RIVER)
RIVER: longest(RIVER)
RIVER: exclude(RIVER1 RIVER2)
RIVER: river(all) that does not RIVER :traverse(STATE)
river run through STATE: stateid(STATENAME)
STATENAME : texas
texas
</figure>
<figureCaption confidence="0.991248">
Figure 2: One possible hybrid tree T1
</figureCaption>
<bodyText confidence="0.992891611111111">
the longest
child semantic categories (“RIVER”).
Now we give a brief overview of the hybrid tree
framework and the LNLZ08 system that was pre-
sented in Lu et al. (2008). The training corpus re-
quired by the LNLZ08 system contains example
pairs d(i) = (m(i), w(i)) for i = 1... N, where
each m(i) is an MR, and each w(i) is an NL sen-
tence. The system makes the assumption that the
entire training corpus is generated from an under-
lying generative model, which is specified by the
parameter set Q.
The parameter set Q includes the following: the
MR model parameter ρ(mj|mi, argk) which mod-
els the generation of an MR production mj from
its parent MR production mi as its k-th child, the
emission parameter θ(t|mi, A) that is responsible
for generation of an NL word or a semantic cate-
gory t from the MR production mi (the parent of
t) under the context A (such as the token to the left
of the current token), and the pattern parameter
φ(r|mi), which models the selection of a hybrid
pattern r that defines globally how the NL words
and semantic categories are interleaved given a
parent MR production mi. All these parameters
are estimated from the corpus during the training
phase. The list of possible hybrid patterns is given
in Table 1 (at most two child semantic categories
are allowed – MR productions with more child se-
mantic categories are transformed into those with
two).
In the table, m refers to the MR production, the
symbol w denotes an NL word sequence and is
optional if it appears inside []. The symbol Y and
Z refer to the first and second semantic category
under the MR production m respectively.
</bodyText>
<table confidence="0.6855712">
# RHS Hybrid Pattern # Patterns
0 m → w 1
1 m → [w]Y[w] 4
2 m → [w]Y[w]Z[w] 8
m → [w]Z[w]Y[w] 8
</table>
<tableCaption confidence="0.963122">
Table 1: The list of possible hybrid patterns, [] de-
notes optional
</tableCaption>
<bodyText confidence="0.99988326923077">
The generative process recursively creates MR
productions as well as NL words at each gen-
eration step in a top-down manner. This pro-
cess results in a hybrid tree for each MR-NL
pair. The list of children under each MR pro-
duction in the hybrid tree forms a hybrid se-
quence. One example hybrid tree for the MR-
NL pair given in Figure 1 is shown in Figure 2.
In this hybrid tree T1, the list of children under
the production RIVER : longest(RIVER) forms
the hybrid sequence “the longest RIVER :
exclude(RIVER1 RIVER2)”. The yield of the hy-
brid tree is exactly the NL sentence. The MR can
also be recovered from the hybrid tree by record-
ing all the internal nodes of the tree, subject to the
reordering operation required by the hybrid pat-
tern.
To illustrate, consider the generation of the hy-
brid tree T1 shown in Figure 2. The model first
generates an MR production from its parent MR
production (empty as the MR production is the
root in the MR). Next, it selects a hybrid pattern
m → wY from the predefined list of hybrid pat-
terns, which puts a constraint on the set of all al-
lowable hybrid sequences that can be generated:
the hybrid sequence must be an NL word sequence
</bodyText>
<page confidence="0.99359">
402
</page>
<bodyText confidence="0.999953692307692">
followed by a semantic category. Finally, actual
NL words and semantic categories are generated
from the parent MR production. Now the genera-
tion for one level is complete, and the above pro-
cess repeats at the newly generated MR produc-
tions, until the complete NL sentence and MR are
both generated.
Mathematically, the above generative process
yields the following formula that models the joint
probability for the MR-NL pair, assuming the con-
text A for the emission parameter is the preceding
word or semantic category (i.e., the bigram model
is assumed, as discussed in Lu et al. (2008)):
</bodyText>
<sectionHeader confidence="0.915761" genericHeader="method">
4 Generation with Hybrid Trees
</sectionHeader>
<bodyText confidence="0.99996825">
The task of generating NL sentences from MRs
can be defined as follows. Given a training cor-
pus consisting of MRs paired with their NL sen-
tences, one needs to develop algorithms that learn
how to effectively “paraphrase” MRs with natu-
ral language sentences. During testing, the sys-
tem should be able to output the most probable NL
“paraphrase” for a given new MR.
The LNLZ08 system models p(T (w, m)), the
joint generative process for the hybrid tree con-
taining both NL and MR. This term can be rewrit-
ten in the following way:
</bodyText>
<equation confidence="0.999206777777778">
p(T1(�w, m))
= ρ(QUERY : answer(RIVER)|−, arg1)
xφ(m → wY|QUERY : answer(RIVER))
xO(what|QUERY : answer(RIVER), BEGIN)
xO(is|QUERY : answer(RIVER), what)
xO(RIVER|QUERY : answer(RIVER), is)
xO(END|QUERY : answer(RIVER), RIVER)
xρ(RIVER : longest(RIVER)|
QUERY: answer(RIVER), arg1) x ... (1)
</equation>
<bodyText confidence="0.989277918918919">
where T1(w, m) denotes the hybrid tree T1 which
contains the NL sentence w� and MR m.
For each MR-NL pair in the training set, there
can be potentially many possible hybrid trees asso-
ciated with the pair. However, the correct hybrid
tree is completely unknown during training. The
correct hybrid tree is therefore treated as a hidden
variable. An efficient inside-outside style algo-
rithm (Baker, 1979) coupled with further dynamic
programming techniques is used for efficient pa-
rameter estimation.
During the testing phase, the system makes use
of the learned model parameters to determine the
most probable hybrid tree given a new natural lan-
guage sentence. The MR contained in that hybrid
tree is the output of the system. Dynamic pro-
gramming techniques similar to those of training
are also employed for efficient decoding.
The generative model used in the LNLZ08 sys-
tem has a natural symmetry, allowing for easy
transformation from NL to MR, as well as from
MR to NL. This provides the starting point for our
work in “inverting” the LNLZ08 system to gener-
ate natural language sentences from the underly-
ing meaning representations.
p(T (�w, m)) = p(m) x p (T (�w, �m)|�m) (2)
In other words, we reach an alternative view of
the joint generative process as follows. We choose
to generate the complete MR m� first. Given m, we
generate hybrid sequences below each of its MR
production, which gives us a complete hybrid tree
T (w, m). The NL sentence w� can be constructed
from this hybrid tree exactly.
We define an operation yield(T) which returns
the NL sentence as the yield of the hybrid tree T .
Given an MR m, we find the most probable NL
* as follows:
</bodyText>
<equation confidence="0.9924655">
yield (argmaxp(T |m)) (3)
T
</equation>
<bodyText confidence="0.999887923076923">
In other words, we first find the most probable
hybrid tree T that contains the provided MR m.
Next we return the yield of T as the most probable
NL sentence.
Different assumptions can be made in the pro-
cess of finding the most probable hybrid tree. We
first describe a simple model which is a direct in-
version of the LNLZ08 system. This model, as a
baseline model, generates a complete NL sentence
word by word. Next, a more sophisticated model
that exploits NL phrase-level dependencies is built
that tackles some weaknesses of the simple base-
line model.
</bodyText>
<subsectionHeader confidence="0.783762">
4.1 Direct Inversion Model
</subsectionHeader>
<bodyText confidence="0.999693333333333">
Assume that a pre-order traversal of the
MR m� gives us the list of MR productions
m1, m2, ... , ms, where S is the number of MR
productions in m. Based on the independence
assumption made by the LNLZ08 system, each
MR production independently generates a hybrid
</bodyText>
<equation confidence="0.816293666666667">
w
sentence
w* =
</equation>
<page confidence="0.98384">
403
</page>
<bodyText confidence="0.851644857142857">
sequence. Denote the hybrid sequence gener-
ated under the MR production ms as hs, for
s = 1, ... , S. We call the list of hybrid sequences
h = (h1, h2, ... , hS) a hybrid sequence list
associated with this particular MR. Thus, our goal
is to find the optimal hybrid sequence list h∗ for
the given MR m, which is formulated as follows:
</bodyText>
<equation confidence="0.9317845">
h∗ =(h∗1, ... , h∗S) = argmax
h1,...,hS
</equation>
<bodyText confidence="0.9980705">
The optimal hybrid sequence list defines the op-
timal hybrid tree whose yield gives the optimal NL
sentence.
Due to the strong independence assumption in-
troduced by the LNLZ08 system, the hybrid tree
generation process is in fact highly decompos-
able. Optimization of the hybrid sequence list
(h1, ... , hS) can be performed individually since
they are independent of one another. Thus, math-
ematically, for s = 1, ... , S, we have:
</bodyText>
<equation confidence="0.9953635">
h∗ s = argmax p(hs|ms) (5)
h3
</equation>
<bodyText confidence="0.999895">
The LNLZ08 system presented three models for
the task of transforming NL to MR. In this in-
verse task, for generation of a hybrid sequence,
we choose to use the bigram model (model II). We
choose this model mainly due to its stronger abil-
ity in modeling dependencies between adjacent
NL words, which we believe to be quite important
in this NL generation task. With the bigram model
assumption, the optimal hybrid sequence that can
be generated from each MR production is defined
as follows:
</bodyText>
<equation confidence="0.998074375">
h∗ s = argmax p(hs|ms)
h3
= argmax
h3 � �
φ(r|ms) x θ(tj|ms, tj−1) (6)
|h3|+1
j=1
H
</equation>
<bodyText confidence="0.987668166666667">
where ti is either an NL word or a semantic cat-
egory with t0 - BEGIN and t|h3|+1 - END, and
r is the hybrid pattern that matches the hybrid se-
quence hs, which is equivalent to t1, ... , t|h3|.
Equivalently, we can view the problem in the
log-space:
</bodyText>
<equation confidence="0.997906333333333">
h∗ s = argmin
h3 �− log φ(r|ms)
�− log θ(tj|ms,tj−1) (7)
</equation>
<bodyText confidence="0.999944409090909">
Note the term − log φ(r|ms) is a constant for
a particular MR production ms and a particu-
lar hybrid pattern r. This search problem can
be equivalently cast as the shortest path problem
which can be solved efficiently with Dijkstra’s al-
gorithm (Cormen et al., 2001). We define a set
of states. Each state represents a single NL word
or a semantic category, including the special sym-
bols BEGIN and END. A directed path between
two different states tu and t„ is associated with
a distance measure − log θ(t„|ms, tu), which is
non-negative. The task now is to find the short-
est path between BEGIN and END1. The sequence
of words appearing in this path is simply the most
probable hybrid sequence under this MR produc-
tion ms. We build this model by directly inverting
the LNLZ08 system, and this model is therefore
referred to as the direct inversion model.
A major weakness of this baseline model is that
it encodes strong independence assumptions dur-
ing the hybrid tree generation process. Though
shown to be effective in the task of transform-
ing NL to MR, such independence assumptions
may introduce difficulties in this NLG task. For
example, consider the MR shown in Figure 1.
The generation steps of the hybrid sequences
from the two adjacent MR productions QUERY :
answer(RIVER) and RIVER : longest(RIVER)
are completely independent of each other. This
may harm the fluency of the generated NL sen-
tence, especially when a transition from one hy-
brid sequence to another is required. In fact, due
to such an independence assumption, the model
always generates the same hybrid sequence from
the same MR production, regardless of its context
such as parent or child MR productions. Such a
limitation points to the importance of better uti-
lizing the tree structure of the MR for this NLG
task. Furthermore, due to the bigram assumption,
the model is unable to capture longer range depen-
dencies amongst the words or semantic categories
in each hybrid sequence.
To tackle the above issues, we explore ways of
relaxing various assumptions, which leads to an
</bodyText>
<footnote confidence="0.899790555555556">
1In addition, we should make sure that the generated hy-
brid sequence t0 ... t1hg+1 is a valid hybrid sequence that
comply with the hybrid pattern r. For example, the MR
production STATE : loc 1(RIVER) can generate the follow-
ing hybrid sequence “BEGIN have RIVER END” but not
this hybrid sequence “BEGIN have END”. This can be
achieved by finding the shortest path from BEGIN to RIVER,
which then gets concatenated to the shortest path from RIVER
to END.
</footnote>
<equation confidence="0.4756395">
S
H p(hs|ms) (4)
s=1
+ |h3|+1
</equation>
<page confidence="0.782152333333333">
�
j=1
404
</page>
<figure confidence="0.898237666666667">
QUERY: answer(RIVER)
RIVER: longest(RIVER)
RIVER: exclude(RIVER1 RIVER2)
RIVER: river(all) RIVER: traverse(STATE)
STATE: stateid(STATENAME)
STATENAME : texas
what is RIVER1
the longest RIVER1
RIVER1 that does not RIVER2
river run through STATE1
STATENAME1
texas
</figure>
<figureCaption confidence="0.997394">
Figure 3: An MR (left) and its associated hybrid sequences (right)
</figureCaption>
<bodyText confidence="0.824605">
alternative model as discussed next.
</bodyText>
<subsectionHeader confidence="0.959305">
4.2 Tree CRF-Based Model
</subsectionHeader>
<bodyText confidence="0.999989588235294">
Based on the belief that using known phrases usu-
ally leads to better fluency in the NLG task (Wong
and Mooney, 2007), we explore methods for gen-
erating an NL sentence at phrase level rather than
at word level. This is done by generating hybrid
sequences as complete objects, rather than sequen-
tially one word or semantic category at a time,
from MR productions.
We assume that each MR production can gen-
erate a complete hybrid sequence below it from a
finite set of possible hybrid sequences. Each such
hybrid sequence is called a candidate hybrid se-
quence associated with that particular MR produc-
tion. Given a set of candidate hybrid sequences as-
sociated with each MR production, the generation
task is to find the optimal hybrid sequence list h*
for a given MR m:
</bodyText>
<equation confidence="0.984902">
h* = argmax p(hIm) (8)
h
</equation>
<bodyText confidence="0.996165055555556">
Figure 3 shows a complete MR, as well as a
possible tree that contains hybrid sequences as-
sociated with the MR productions. For exam-
ple, in the figure the MR production RIVER :
traverse(STATE) is associated with the hybrid se-
quence run through STATE1. Each MR pro-
duction can be associated with potentially many
different hybrid sequences. The task is to deter-
mine the most probable list of hybrid sequences as
the ones appearing on the right of Figure 3, one for
each MR production.
To make better use of the tree structure of MR,
we take the approach of modeling the conditional
distribution using a log-linear model. Following
the conditional random fields (CRF) framework
(Lafferty et al., 2001), we can define the probabil-
ity of the hybrid sequence list given the complete
MR m, as follows:
</bodyText>
<equation confidence="0.998768">
p(h|m) = Z(m) exp \ iEV� µkgk (hi, m, i)
�Akfk(hi, hj, m, i, j) (9)
</equation>
<bodyText confidence="0.999945321428571">
where V is the set of all the vertices in the tree, and
E is the set of the edges in the tree, consisting of
parent-child pairs. The function Z(m) is the nor-
malization function. Note that the dependencies
among the features here form a tree, unlike the se-
quence models used in Lafferty et al. (2001). The
function fk(hi, hj, m, i, j) is a feature function of
the entire MR tree m� and the hybrid sequences at
vertex i and j. These features are usually referred
to as the edge features in the CRF framework. The
function gk(hi, m, i) is a feature function of the
hybrid sequence at vertex i and the entire MR tree.
These features are usually referred to as the vertex
features. The parameters Ak and µk are learned
from the training data.
In this task, we are given only MR-NL pairs
and do not have the hybrid tree corresponding to
each MR as training data. Now we describe how
the set of candidate hybrid sequences for each MR
production is obtained as well as how the train-
ing data for this model is constructed. After the
joint generative model is learned as done in Lu et
al. (2008), we first use a Viterbi algorithm to find
the optimal hybrid tree for each MR-NL pair in
the training set. From each optimal hybrid tree,
we extract the hybrid sequence hi below each MR
production mi. Using this process on the train-
ing MR-NL pairs, we can obtain a set of candidate
</bodyText>
<equation confidence="0.994141">
+ � �
(i,j)EE k
</equation>
<page confidence="0.987729">
405
</page>
<bodyText confidence="0.999786225806452">
hybrid sequences that can be associated with each
MR production. The optimal hybrid tree generated
by the Viterbi algorithm in this way is considered
the “correct” hybrid tree for the MR-NL pair and is
used as training data. While this does not provide
hand-labeled training data, we believe the hybrid
trees generated this way form a high quality train-
ing set as both the MR and NL are available when
Viterbi decoding is performed, guaranteeing that
the generated hybrid tree has the correct yield.
There exist several advantages of such a model
over the simple generative model. First, this model
allows features that specifically model the depen-
dencies between neighboring hybrid sequences in
the tree to be used. In addition, the model can effi-
ciently capture long range dependencies between
MR productions and hybrid sequences since each
hybrid sequence is allowed to depend on the entire
MR tree.
For features, we employ four types of simple
features, as presented below. Note that the first
three types of features are vertex features, and the
last are edge features. Examples are given based
on Figure 3. All the features are indicator func-
tions, i.e., a feature takes value 1 if a certain com-
bination is present, and 0 otherwise. The last three
features explicitly encode information from the
tree structure of MR.
Hybrid sequence features : one hybrid sequence
together with the associated MR production.
For example:
</bodyText>
<equation confidence="0.5864025">
g1 : hrun through STATE1,
RIVER: traverse(STATE)i;
</equation>
<bodyText confidence="0.999973333333333">
Two-level hybrid sequence features : one hy-
brid sequence, its associated MR production,
and the parent MR production. For example:
</bodyText>
<equation confidence="0.985933666666667">
g2 : hrun through STATE1,
RIVER: traverse(STATE),
RIVER: exclude(RIVER1, RIVER2)i ;
</equation>
<bodyText confidence="0.9996595">
Three-level hybrid sequence features : one hy-
brid sequence, its associated MR production,
the parent MR production, and the grandpar-
ent MR production. For example:
</bodyText>
<footnote confidence="0.89857525">
g3 : hrun through STATE1,
RIVER: traverse(STATE),
RIVER: exclude(RIVER1, RIVER2),
RIVER: longest(RIVER)i;
</footnote>
<bodyText confidence="0.917073">
Adjacent hybrid sequence features : two adja-
cent hybrid sequences, together with their as-
sociated MR productions. For example:
</bodyText>
<note confidence="0.56184175">
f1 : hrun through STATE1,
RIVER1 that does not RIVER2,
RIVER: traverse(STATE),
RIVER: exclude(RIVER1, RIVER2)i .
</note>
<bodyText confidence="0.999961357142857">
For training, we use the feature forest model
(Miyao and Tsujii, 2008), which was originally
designed as an efficient algorithm for solving max-
imum entropy models for data with complex struc-
tures. The model enables efficient training over
packed trees that potentially represent exponen-
tial number of trees. The tree conditional random
fields model can be effectively represented using
the feature forest model. The model has also been
successfully applied to the HPSG parsing task.
To train the model, we run the Viterbi algorithm
on the trained LNLZ08 model and perform convex
optimization using the feature forest model. The
LNLZ08 model is trained using an EM algorithm
with time complexity O(MN3D) per EM itera-
tion, where M and N are respectively the maxi-
mum number of MR productions and NL words
for each MR-NL pair, and D is the number of
training instances. The time complexity of the
Viterbi algorithm is also O(MN3D). For training
the feature forest, we use the Amis toolkit (Miyao
and Tsujii, 2002) which utilizes the GIS algorithm.
The time complexity for each iteration of the GIS
algorithm is O(MK2D), where K is the maxi-
mum number of candidate hybrid sequences asso-
ciated with each MR production. Finally, the time
complexity for generating a natural language sen-
tence from a particular MR is O(MK2).
</bodyText>
<sectionHeader confidence="0.998176" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9998325">
In this section, we present the results of our sys-
tems when evaluated on two standard benchmark
corpora. The first corpus is GEOQUERY, which
contains Prolog-based MRs that can be used to
query a US geographic database (Kate et al.,
2005). Our task for this domain is to generate
NL sentences from the formal queries. The second
corpus is ROBOCUP. This domain contains MRs
which are instructions written in a formal language
called CLANG. Our task for this domain is to gen-
erate NL sentences from the coaching advice writ-
ten in CLANG.
</bodyText>
<page confidence="0.997097">
406
</page>
<table confidence="0.99993775">
GEOQUERY (880) ROBOCUP (300)
BLEU NIST BLEU NIST
Direct inversion model 0.3973 5.5466 0.5468 6.6738
Tree CRF-based model 0.5733 6.7459 0.6220 6.9845
</table>
<tableCaption confidence="0.979932">
Table 2: Results of automatic evaluation of both models (bold type indicates the best performing system).
</tableCaption>
<table confidence="0.9999375">
GEOQUERY (880) ROBOCUP (300)
BLEU NIST BLEU NIST
WASP−1++ 0.5370 6.4808 0.6022 6.8976
Tree CRF-based model 0.5733 6.7459 0.6220 6.9845
</table>
<tableCaption confidence="0.997546">
Table 3: Results of automatic evaluation of our tree CRF-based model and WASP−1++.
</tableCaption>
<table confidence="0.99985125">
English Japanese Spanish Turkish
BLEU NIST BLEU NIST BLEU NIST BLEU NIST
WASP−1++ 0.6035 5.7133 0.6585 4.6648 0.6175 5.7293 0.4824 4.3283
Tree CRF-based model 0.6265 5.8907 0.6788 4.8486 0.6382 5.8488 0.5096 4.5033
</table>
<tableCaption confidence="0.999924">
Table 4: Results on the GEOQUERY-250 corpus with 4 natural languages.
</tableCaption>
<bodyText confidence="0.999944103448276">
The GEOQUERY domain contains 880 in-
stances, while the ROBOCUP domain contains 300
instances. The average NL sentence length for the
two corpora are 7.57 and 22.52 respectively. Fol-
lowing the evaluation methodology of Wong and
Mooney (2007), we performed 4 runs of the stan-
dard 10-fold cross validation and report the aver-
aged performance in this section using the stan-
dard automatic evaluation metric BLEU (Papineni
et al., 2002) and NIST (Doddington, 2002)2. The
BLEU and NIST scores of the WASP−1++ sys-
tem reported in this section are obtained from
the published paper of Wong and Mooney (2007).
Note that to make our experimental results directly
comparable to Wong and Mooney (2007), we used
the identical training and test data splits for the 4
runs of 10-fold cross validation used by Wong and
Mooney (2007) on both corpora.
Our system has the advantage of always pro-
ducing an NL sentence given any input MR, even
if there exist unseen MR productions in the input
MR. We can achieve this by simply skipping those
unseen MR productions during the generation pro-
cess. However, in order to make a fair comparison
against WASP−1++, which can only generate NL
sentences for 97% of the input MRs, we also do
not generate any NL sentence in the case of ob-
serving an unseen MR production. All the evalu-
ations discussed in this section follow this evalu-
</bodyText>
<footnote confidence="0.995068">
2We used the official evaluation script (version 11b) pro-
vided by http://www.nist.gov/.
</footnote>
<bodyText confidence="0.99034125">
ation methodology, but we notice that empirically
our system is able to achieve higher BLEU/NIST
scores if we allow generation for those MRs that
include unseen MR productions.
</bodyText>
<subsectionHeader confidence="0.981894">
5.1 Comparison between the two models
</subsectionHeader>
<bodyText confidence="0.999990416666667">
We compare the performance of our two models
in Table 2. From the table, we observe that the
tree CRF-based model outperforms the direct in-
version model on both domains. This validates
our earlier belief that some long range dependen-
cies are important for the generation task. In ad-
dition, while the direct inversion model performs
reasonably well on the ROBOCUP domain, it per-
forms substantially worse on the GEOQUERY do-
main where the sentence length is shorter. We note
that the evaluation metrics are strongly correlated
with the cumulative matching n-grams between
the output and the reference sentence (n ranges
from 1 to 4 for BLEU, and 1 to 5 for NIST). The
direct inversion model fails to capture the transi-
tional behavior from one phrase to another, which
makes it more vulnerable to n-gram mismatch, es-
pecially when evaluated on the GEOQUERY cor-
pus where phrase-to-phrase transitions are more
frequent. On the other hand, the tree CRF-based
model does not suffer from this problem, mainly
due to its ability to model such dependencies be-
tween neighboring phrases. Sample outputs from
the two models are shown in Figure 4.
</bodyText>
<page confidence="0.994723">
407
</page>
<table confidence="0.657326444444444">
Reference: what is the largest state bordering texas
Direct inversion model: what the largest states border texas
Tree CRF-based model: what is the largest state that borders texas
Reference: if DR2C7 is true then players 2 , 3 , 7 and 8
should pass to player 4
Direct inversion model: if DR2C7 , then players 2 , 3 7 and 8 should
ball to player 4
Tree CRF-based model: if the condition DR2C7 is true then players 2 ,
3 , 7 and 8 should pass to player 4
</table>
<figureCaption confidence="0.99587">
Figure 4: Sample outputs from the two models, for GEOQUERY domain (top) and ROBOCUP domain
(bottom) respectively.
</figureCaption>
<subsectionHeader confidence="0.999493">
5.2 Comparison with previous model
</subsectionHeader>
<bodyText confidence="0.99999275">
We also compare the performance of our tree CRF-
based model against the previous state-of-the-art
system WASP−1++ in Table 3. Our tree CRF-based
model achieves better performance on both cor-
pora. We are unable to carry out statistical sig-
nificance tests since the detailed BLEU and NIST
scores of the cross validation runs of WASP−1++
as reported in the published paper of Wong and
Mooney (2007) are not available.
The results confirm our earlier discussions: the
dependencies between the generated NL words
are important and need to be properly modeled.
The WASP−1++ system uses a log-linear model
which incorporates two major techniques to at-
tempt to model such dependencies. First, a back-
off language model is used to capture dependen-
cies at adjacent word level. Second, a technique
that merges smaller translation rules into a single
rigid rule is used to capture dependencies at phrase
level (Wong, 2007). In contrast, the proposed tree
CRF-based model is able to explicitly and flexibly
exploit phrase-level features that model dependen-
cies between adjacent phrases. In fact, with the
hybrid tree framework, the better treatment of the
tree structure of MR enables us to model some cru-
cial dependencies between the complete MR tree
and generated NL phrases. We believe that this
property plays an important role in improving the
quality of the generated sentences in terms of flu-
ency, which is assessed by the evaluation metrics.
Furthermore, WASP−1++ employs minimum
error rate training (Och, 2003) to directly optimize
the evaluation metrics. We have not done so but
still obtain better performance. In future, we plan
to explore ways to directly optimize the evaluation
metrics in our system.
</bodyText>
<subsectionHeader confidence="0.977059">
5.3 Experiments on different languages
</subsectionHeader>
<bodyText confidence="0.9999928">
Following the work of Wong and Mooney (2007),
we also evaluated our system’s performance on
a subset of the GEOQUERY corpus with 250 in-
stances, where sentences of 4 natural languages
(English, Japanese, Spanish, and Turkish) are
available. The evaluation results are shown in Ta-
ble 4. Our tree CRF-based model achieves better
performance on this task compared to WASP−1++.
We are again unable to conduct statistical signifi-
cance tests for the same reason reported earlier.
</bodyText>
<sectionHeader confidence="0.999357" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999660214285714">
In this paper, we presented two novel models for
the task of generating natural language sentences
from given meaning representations, under a hy-
brid tree framework. We first built a simple di-
rect inversion model as a baseline. Next, to ad-
dress the limitations associated with the direct in-
version model, a tree CRF-based model was in-
troduced. We evaluated both models on standard
benchmark corpora. Evaluation results show that
the tree CRF-based model performs better than the
direct inversion model, and that the tree CRF-based
model also outperforms WASP−1++, which was a
previous state-of-the-art system reported in the lit-
erature.
</bodyText>
<sectionHeader confidence="0.998024" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999899">
The authors would like to thank Seung-Hoon Na
for his suggestions on the presentation of this pa-
per, Yuk Wah Wong for answering various ques-
tions related to the WASP−1++ system, and the
anonymous reviewers for their thoughtful com-
ments on this work.
</bodyText>
<page confidence="0.99832">
408
</page>
<sectionHeader confidence="0.99355" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99923676146789">
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation and Compiling.
Prentice-Hall, Englewood Clis, NJ.
James K. Baker. 1979. Trainable grammars for speech
recognition. In Proceedings of the Spring Confer-
ence of the Acoustical Society of America, pages
547–550, Boston, MA, June.
John Carroll and Stephan Oepen. 2005. High ef-
ficiency realization for a wide-coverage unification
grammar. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP 2005), pages 165–176.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznanski. 1999. An efficient chart generator
for (semi-) lexicalist grammars. In Proceedings of
the 7th European Workshop on Natural Language
Generation (EWNLG 1999), pages 86–95.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to
Algorithms (Second Edition). MIT Press.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the 2nd In-
ternational Conference on Human Language Tech-
nology Research (HLT 2002), pages 138–145.
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proceedings of the 9th Conference on
Computational Natural Language Learning (CoNLL
2005), pages 9–16.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Lin-
guistics (COLING/ACL 2006), pages 913–920.
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to for-
mal languages. In Proceedings of the 20th National
Conference on Artificial Intelligence (AAAI 2005),
pages 1062–1068.
Martin Kay. 1996. Chart generation. In Proceedings
of the 34th Annual Meeting of the Association for
Computational Linguistics (ACL 1996), pages 200–
204.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of the 6th Conference
of the Association for Machine Translation in the
Americas (AMTA 2004), pages 115–124.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 18th Inter-
national Conference on Machine Learning (ICML
2001), pages 282–289.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2008), pages 783–792.
Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the 2nd International Conference on Human
Language Technology Research (HLT 2002), pages
292–297.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35–80.
Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic models for disambiguation of an
HPSG-based chart generator. In Proceedings of the
9th International Workshop on Parsing Technologies
(IWPT 2005), volume 5, pages 93–102.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2003), pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2002), pages 311–318.
Michael White and Jason Baldridge. 2003. Adapting
chart realization to CCG. In Proceedings of the 9th
European Workshop on Natural Language Genera-
tion (EWNLG 2003), pages 119–126.
Michael White. 2004. Reining in CCG chart realiza-
tion. In Proceeding of the 3rd International Confer-
ence on Natural Language Generation (INLG 2004),
pages 182–191.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT/NAACL 2006), pages 439–446.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Generation by inverting a semantic parser that uses
statistical machine translation. In Proceedings of
the Human Language Technology Conference of
the North American Chapter of the Association
for Computational Linguistics (NAACL/HLT 2007),
pages 172–179.
Yuk Wah Wong. 2007. Learning for Semantic Parsing
and Natural Language Generation Using Statistical
Machine Translation Techniques. Ph.D. thesis, The
University of Texas at Austin.
</reference>
<page confidence="0.999138">
409
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986001">
<title confidence="0.999477">Natural Language Generation with Tree Conditional Random Fields</title>
<author confidence="0.995327">Hwee Tou Wee Sun</author>
<affiliation confidence="0.997807">of Computer National University of</affiliation>
<abstract confidence="0.9997602">This paper presents an effective method for generating natural language sentences from their underlying meaning representations. The method is built on top of tree that jointly encodes both the meaning representation as well as the natural language in a tree structure. By using a tree conditional random field on top of the hybrid tree representation, we are able to explicitly model phrase-level dependencies amongst neighboring natural language phrases and meaning representation components in a simple and natural way. We show that the additional dependencies captured by the tree conditional random field allows it to perform better than directly inverting a previously developed hybrid tree semantic parser. Furthermore, we demonstrate that the model performs better than a previous state-of-the-art natural language generation model. Experiments are performed on two benchmark corpora with standard automatic evaluation metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation and Compiling. Prentice-Hall,</booktitle>
<location>Englewood Clis, NJ.</location>
<contexts>
<context position="6306" citStr="Aho and Ullman, 1972" startWordPosition="985" endWordPosition="988">hi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 that employed techniques from statistical machine translation using Synchronous Context-Free Grammar (SCFG) (Aho and Ullman, 1972). The system took in a linearized MR tree as input, and translated it into a natural language sentence as output. Unlike most previous systems, their system integrated both lexical selection and surface realization in a single framework. The performance of the system was enhanced by incorporating models borrowed from PHARAOH (Koehn, 2004). Experiments show that this new hybrid system named WASP−1++ gives state-of-the-art accuracies and outperforms the direct translation model obtained from PHARAOH, when evaluated on two corpora. We will compare our system’s performance against that of WASP−1++</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation and Compiling. Prentice-Hall, Englewood Clis, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Proceedings of the Spring Conference of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<location>Boston, MA,</location>
<contexts>
<context position="12864" citStr="Baker, 1979" startWordPosition="2109" endWordPosition="2110">xO(what|QUERY : answer(RIVER), BEGIN) xO(is|QUERY : answer(RIVER), what) xO(RIVER|QUERY : answer(RIVER), is) xO(END|QUERY : answer(RIVER), RIVER) xρ(RIVER : longest(RIVER)| QUERY: answer(RIVER), arg1) x ... (1) where T1(w, m) denotes the hybrid tree T1 which contains the NL sentence w� and MR m. For each MR-NL pair in the training set, there can be potentially many possible hybrid trees associated with the pair. However, the correct hybrid tree is completely unknown during training. The correct hybrid tree is therefore treated as a hidden variable. An efficient inside-outside style algorithm (Baker, 1979) coupled with further dynamic programming techniques is used for efficient parameter estimation. During the testing phase, the system makes use of the learned model parameters to determine the most probable hybrid tree given a new natural language sentence. The MR contained in that hybrid tree is the output of the system. Dynamic programming techniques similar to those of training are also employed for efficient decoding. The generative model used in the LNLZ08 system has a natural symmetry, allowing for easy transformation from NL to MR, as well as from MR to NL. This provides the starting po</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>James K. Baker. 1979. Trainable grammars for speech recognition. In Proceedings of the Spring Conference of the Acoustical Society of America, pages 547–550, Boston, MA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Stephan Oepen</author>
</authors>
<title>High efficiency realization for a wide-coverage unification grammar.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<pages>165--176</pages>
<contexts>
<context position="5676" citStr="Carroll and Oepen, 2005" startWordPosition="888" endWordPosition="892">model on a dataset annotated with several natural languages other than English (Japanese, Spanish, and Turkish). Evaluation results show that our proposed tree CRF-based model outperforms the previous model. 2 Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 that employed techniques from statistical machine translation using Synchronous Context-Free Grammar</context>
</contexts>
<marker>Carroll, Oepen, 2005</marker>
<rawString>John Carroll and Stephan Oepen. 2005. High efficiency realization for a wide-coverage unification grammar. In Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP 2005), pages 165–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Victor Poznanski</author>
</authors>
<title>An efficient chart generator for (semi-) lexicalist grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 7th European Workshop on Natural Language Generation (EWNLG</booktitle>
<pages>86--95</pages>
<contexts>
<context position="5651" citStr="Carroll et al., 1999" startWordPosition="884" endWordPosition="887">more, we evaluate our model on a dataset annotated with several natural languages other than English (Japanese, Spanish, and Turkish). Evaluation results show that our proposed tree CRF-based model outperforms the previous model. 2 Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 that employed techniques from statistical machine translation using Synchro</context>
</contexts>
<marker>Carroll, Copestake, Flickinger, Poznanski, 1999</marker>
<rawString>John Carroll, Ann Copestake, Dan Flickinger, and Victor Poznanski. 1999. An efficient chart generator for (semi-) lexicalist grammars. In Proceedings of the 7th European Workshop on Natural Language Generation (EWNLG 1999), pages 86–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
<author>Clifford Stein</author>
</authors>
<title>Introduction to Algorithms (Second Edition).</title>
<date>2001</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16997" citStr="Cormen et al., 2001" startWordPosition="2858" endWordPosition="2861"> h3 � � φ(r|ms) x θ(tj|ms, tj−1) (6) |h3|+1 j=1 H where ti is either an NL word or a semantic category with t0 - BEGIN and t|h3|+1 - END, and r is the hybrid pattern that matches the hybrid sequence hs, which is equivalent to t1, ... , t|h3|. Equivalently, we can view the problem in the log-space: h∗ s = argmin h3 �− log φ(r|ms) �− log θ(tj|ms,tj−1) (7) Note the term − log φ(r|ms) is a constant for a particular MR production ms and a particular hybrid pattern r. This search problem can be equivalently cast as the shortest path problem which can be solved efficiently with Dijkstra’s algorithm (Cormen et al., 2001). We define a set of states. Each state represents a single NL word or a semantic category, including the special symbols BEGIN and END. A directed path between two different states tu and t„ is associated with a distance measure − log θ(t„|ms, tu), which is non-negative. The task now is to find the shortest path between BEGIN and END1. The sequence of words appearing in this path is simply the most probable hybrid sequence under this MR production ms. We build this model by directly inverting the LNLZ08 system, and this model is therefore referred to as the direct inversion model. A major wea</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, Stein, 2001</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2001. Introduction to Algorithms (Second Edition). MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Conference on Human Language Technology Research (HLT</booktitle>
<pages>138--145</pages>
<contexts>
<context position="28000" citStr="Doddington, 2002" startWordPosition="4711" endWordPosition="4712">.4824 4.3283 Tree CRF-based model 0.6265 5.8907 0.6788 4.8486 0.6382 5.8488 0.5096 4.5033 Table 4: Results on the GEOQUERY-250 corpus with 4 natural languages. The GEOQUERY domain contains 880 instances, while the ROBOCUP domain contains 300 instances. The average NL sentence length for the two corpora are 7.57 and 22.52 respectively. Following the evaluation methodology of Wong and Mooney (2007), we performed 4 runs of the standard 10-fold cross validation and report the averaged performance in this section using the standard automatic evaluation metric BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)2. The BLEU and NIST scores of the WASP−1++ system reported in this section are obtained from the published paper of Wong and Mooney (2007). Note that to make our experimental results directly comparable to Wong and Mooney (2007), we used the identical training and test data splits for the 4 runs of 10-fold cross validation used by Wong and Mooney (2007) on both corpora. Our system has the advantage of always producing an NL sentence given any input MR, even if there exist unseen MR productions in the input MR. We can achieve this by simply skipping those unseen MR productions during the gener</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the 2nd International Conference on Human Language Technology Research (HLT 2002), pages 138–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL</booktitle>
<pages>9--16</pages>
<contexts>
<context position="7322" citStr="Ge and Mooney, 2005" startWordPosition="1142" endWordPosition="1145">−1++ gives state-of-the-art accuracies and outperforms the direct translation model obtained from PHARAOH, when evaluated on two corpora. We will compare our system’s performance against that of WASP−1++ in Section 5. 3 The Hybrid Tree Framework and the LNLZ08 System QUERY: answer(RIVER) RIVER: longest(RIVER) RIVER: exclude(RIVER1 RIVER2) RIVER: river(all) RIVER: traverse(STATE) STATE: stateid(STATENAME) STATENAME : texas what is the longest river that does not run through texas Figure 1: An example MR paired with its NL sentence. Following most previous works in this area (Kate et al., 2005; Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008), we consider MRs in the form of tree structures. An example MR and its corresponding natural language sentence are shown in Figure 1. The MR is a tree consisting of nodes called MR productions. For example, the node “QUERY : answer(RIVER)” is one MR production. Each MR production consists of a semantic category (“QUERY”), a function symbol (“answer”) which can be optionally omitted, as well as an argument list which possibly contains 401 what is QUERY: answer(RIVER) RIVER: longest(RIVER) RIVER: exclude(RIVER1 RIVER2) RIVER: river</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL 2005), pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using string-kernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL</booktitle>
<pages>913--920</pages>
<contexts>
<context position="7345" citStr="Kate and Mooney, 2006" startWordPosition="1146" endWordPosition="1149">he-art accuracies and outperforms the direct translation model obtained from PHARAOH, when evaluated on two corpora. We will compare our system’s performance against that of WASP−1++ in Section 5. 3 The Hybrid Tree Framework and the LNLZ08 System QUERY: answer(RIVER) RIVER: longest(RIVER) RIVER: exclude(RIVER1 RIVER2) RIVER: river(all) RIVER: traverse(STATE) STATE: stateid(STATENAME) STATENAME : texas what is the longest river that does not run through texas Figure 1: An example MR paired with its NL sentence. Following most previous works in this area (Kate et al., 2005; Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008), we consider MRs in the form of tree structures. An example MR and its corresponding natural language sentence are shown in Figure 1. The MR is a tree consisting of nodes called MR productions. For example, the node “QUERY : answer(RIVER)” is one MR production. Each MR production consists of a semantic category (“QUERY”), a function symbol (“answer”) which can be optionally omitted, as well as an argument list which possibly contains 401 what is QUERY: answer(RIVER) RIVER: longest(RIVER) RIVER: exclude(RIVER1 RIVER2) RIVER: river(all) that does not RIV</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL 2006), pages 913–920.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to transform natural to formal languages.</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th National Conference on Artificial Intelligence (AAAI</booktitle>
<pages>1062--1068</pages>
<contexts>
<context position="7301" citStr="Kate et al., 2005" startWordPosition="1138" endWordPosition="1141">d system named WASP−1++ gives state-of-the-art accuracies and outperforms the direct translation model obtained from PHARAOH, when evaluated on two corpora. We will compare our system’s performance against that of WASP−1++ in Section 5. 3 The Hybrid Tree Framework and the LNLZ08 System QUERY: answer(RIVER) RIVER: longest(RIVER) RIVER: exclude(RIVER1 RIVER2) RIVER: river(all) RIVER: traverse(STATE) STATE: stateid(STATENAME) STATENAME : texas what is the longest river that does not run through texas Figure 1: An example MR paired with its NL sentence. Following most previous works in this area (Kate et al., 2005; Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008), we consider MRs in the form of tree structures. An example MR and its corresponding natural language sentence are shown in Figure 1. The MR is a tree consisting of nodes called MR productions. For example, the node “QUERY : answer(RIVER)” is one MR production. Each MR production consists of a semantic category (“QUERY”), a function symbol (“answer”) which can be optionally omitted, as well as an argument list which possibly contains 401 what is QUERY: answer(RIVER) RIVER: longest(RIVER) RIVER: exclude(RIVER1</context>
<context position="26484" citStr="Kate et al., 2005" startWordPosition="4468" endWordPosition="4471">ure forest, we use the Amis toolkit (Miyao and Tsujii, 2002) which utilizes the GIS algorithm. The time complexity for each iteration of the GIS algorithm is O(MK2D), where K is the maximum number of candidate hybrid sequences associated with each MR production. Finally, the time complexity for generating a natural language sentence from a particular MR is O(MK2). 5 Experiments In this section, we present the results of our systems when evaluated on two standard benchmark corpora. The first corpus is GEOQUERY, which contains Prolog-based MRs that can be used to query a US geographic database (Kate et al., 2005). Our task for this domain is to generate NL sentences from the formal queries. The second corpus is ROBOCUP. This domain contains MRs which are instructions written in a formal language called CLANG. Our task for this domain is to generate NL sentences from the coaching advice written in CLANG. 406 GEOQUERY (880) ROBOCUP (300) BLEU NIST BLEU NIST Direct inversion model 0.3973 5.5466 0.5468 6.6738 Tree CRF-based model 0.5733 6.7459 0.6220 6.9845 Table 2: Results of automatic evaluation of both models (bold type indicates the best performing system). GEOQUERY (880) ROBOCUP (300) BLEU NIST BLEU </context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005. Learning to transform natural to formal languages. In Proceedings of the 20th National Conference on Artificial Intelligence (AAAI 2005), pages 1062–1068.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Chart generation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>200--204</pages>
<contexts>
<context position="5527" citStr="Kay (1996)" startWordPosition="867" endWordPosition="868">lso compare the tree CRFbased model against the previous state-of-the-art model of Wong and Mooney (2007). Furthermore, we evaluate our model on a dataset annotated with several natural languages other than English (Japanese, Spanish, and Turkish). Evaluation results show that our proposed tree CRF-based model outperforms the previous model. 2 Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semanti</context>
</contexts>
<marker>Kay, 1996</marker>
<rawString>Martin Kay. 1996. Chart generation. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL 1996), pages 200– 204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA 2004),</booktitle>
<pages>115--124</pages>
<contexts>
<context position="6646" citStr="Koehn, 2004" startWordPosition="1042" endWordPosition="1043"> (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 that employed techniques from statistical machine translation using Synchronous Context-Free Grammar (SCFG) (Aho and Ullman, 1972). The system took in a linearized MR tree as input, and translated it into a natural language sentence as output. Unlike most previous systems, their system integrated both lexical selection and surface realization in a single framework. The performance of the system was enhanced by incorporating models borrowed from PHARAOH (Koehn, 2004). Experiments show that this new hybrid system named WASP−1++ gives state-of-the-art accuracies and outperforms the direct translation model obtained from PHARAOH, when evaluated on two corpora. We will compare our system’s performance against that of WASP−1++ in Section 5. 3 The Hybrid Tree Framework and the LNLZ08 System QUERY: answer(RIVER) RIVER: longest(RIVER) RIVER: exclude(RIVER1 RIVER2) RIVER: river(all) RIVER: traverse(STATE) STATE: stateid(STATENAME) STATENAME : texas what is the longest river that does not run through texas Figure 1: An example MR paired with its NL sentence. Follow</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA 2004), pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning (ICML</booktitle>
<pages>282--289</pages>
<contexts>
<context position="4516" citStr="Lafferty et al., 2001" startWordPosition="707" endWordPosition="710">2009 ACL and AFNLP on top of the generative model introduced in Lu et al. (2008) (referred to as the LNLZ08 system). We first present a baseline model by directly “inverting” the LNLZ08 system, where an NL sentence is generated word by word. We call this model the direct inversion model. This model is unable to model some long range global dependencies over the entire NL sentence to be generated. To tackle several weaknesses exhibited by the baseline model, we next introduce an alternative, novel model that performs generation at the phrase level. Motivated by conditional random fields (CRF) (Lafferty et al., 2001), a different parameterization of the conditional probability of the hybrid tree that enables the model to encode some longer range dependencies amongst phrases and MRs is used. This novel model is referred to as the tree CRF-based model. Evaluation results for both models are presented, through which we demonstrate that the tree CRF-based model performs better than the direct inversion model. We also compare the tree CRFbased model against the previous state-of-the-art model of Wong and Mooney (2007). Furthermore, we evaluate our model on a dataset annotated with several natural languages oth</context>
<context position="21160" citStr="Lafferty et al., 2001" startWordPosition="3565" endWordPosition="3568">d sequences associated with the MR productions. For example, in the figure the MR production RIVER : traverse(STATE) is associated with the hybrid sequence run through STATE1. Each MR production can be associated with potentially many different hybrid sequences. The task is to determine the most probable list of hybrid sequences as the ones appearing on the right of Figure 3, one for each MR production. To make better use of the tree structure of MR, we take the approach of modeling the conditional distribution using a log-linear model. Following the conditional random fields (CRF) framework (Lafferty et al., 2001), we can define the probability of the hybrid sequence list given the complete MR m, as follows: p(h|m) = Z(m) exp \ iEV� µkgk (hi, m, i) �Akfk(hi, hj, m, i, j) (9) where V is the set of all the vertices in the tree, and E is the set of the edges in the tree, consisting of parent-child pairs. The function Z(m) is the normalization function. Note that the dependencies among the features here form a tree, unlike the sequence models used in Lafferty et al. (2001). The function fk(hi, hj, m, i, j) is a feature function of the entire MR tree m� and the hybrid sequences at vertex i and j. These feat</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning (ICML 2001), pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>A generative model for parsing natural language to meaning representations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>783--792</pages>
<contexts>
<context position="2339" citStr="Lu et al. (2008)" startWordPosition="347" endWordPosition="350"> parsing and natural language generation (NLG), respectively. In this paper, we use corpus-based statistical methods for constructing a natural language generation system. Given a set of pairs, where each pair consists of a natural language (NL) sentence and its formal meaning representation (MR), a learning method induces an algorithm that can be used for performing language generation from other previously unseen meaning representations. A crucial question in any natural language processing system is the representation used. Meaning representations can be in the form of a tree structure. In Lu et al. (2008), we introduced a hybrid tree framework together with a probabilistic generative model to tackle semantic parsing, where tree structured meaning representations are used. The hybrid tree gives a natural joint tree representation of a natural language sentence and its meaning representation. A joint generative model for natural language and its meaning representation, such as that used in Lu et al. (2008) has several advantages over various previous approaches designed for semantic parsing. First, unlike most previous approaches, the generative approach models a simultaneous generation process </context>
<context position="3974" citStr="Lu et al. (2008)" startWordPosition="616" endWordPosition="619">e-art accuracy in semantic parsing on benchmark corpora. While semantic parsing with hybrid trees has been studied in Lu et al. (2008), its inverse task – NLG with hybrid trees – has not yet been explored. We believe that the properties that make the hybrid trees effective for semantic parsing also make them effective for NLG. In this paper, we develop systems for the generation task by building 400 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 400–409, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP on top of the generative model introduced in Lu et al. (2008) (referred to as the LNLZ08 system). We first present a baseline model by directly “inverting” the LNLZ08 system, where an NL sentence is generated word by word. We call this model the direct inversion model. This model is unable to model some long range global dependencies over the entire NL sentence to be generated. To tackle several weaknesses exhibited by the baseline model, we next introduce an alternative, novel model that performs generation at the phrase level. Motivated by conditional random fields (CRF) (Lafferty et al., 2001), a different parameterization of the conditional probabil</context>
<context position="7386" citStr="Lu et al., 2008" startWordPosition="1154" endWordPosition="1157">ranslation model obtained from PHARAOH, when evaluated on two corpora. We will compare our system’s performance against that of WASP−1++ in Section 5. 3 The Hybrid Tree Framework and the LNLZ08 System QUERY: answer(RIVER) RIVER: longest(RIVER) RIVER: exclude(RIVER1 RIVER2) RIVER: river(all) RIVER: traverse(STATE) STATE: stateid(STATENAME) STATENAME : texas what is the longest river that does not run through texas Figure 1: An example MR paired with its NL sentence. Following most previous works in this area (Kate et al., 2005; Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008), we consider MRs in the form of tree structures. An example MR and its corresponding natural language sentence are shown in Figure 1. The MR is a tree consisting of nodes called MR productions. For example, the node “QUERY : answer(RIVER)” is one MR production. Each MR production consists of a semantic category (“QUERY”), a function symbol (“answer”) which can be optionally omitted, as well as an argument list which possibly contains 401 what is QUERY: answer(RIVER) RIVER: longest(RIVER) RIVER: exclude(RIVER1 RIVER2) RIVER: river(all) that does not RIVER :traverse(STATE) river run through STA</context>
<context position="11610" citStr="Lu et al. (2008)" startWordPosition="1903" endWordPosition="1906">be an NL word sequence 402 followed by a semantic category. Finally, actual NL words and semantic categories are generated from the parent MR production. Now the generation for one level is complete, and the above process repeats at the newly generated MR productions, until the complete NL sentence and MR are both generated. Mathematically, the above generative process yields the following formula that models the joint probability for the MR-NL pair, assuming the context A for the emission parameter is the preceding word or semantic category (i.e., the bigram model is assumed, as discussed in Lu et al. (2008)): 4 Generation with Hybrid Trees The task of generating NL sentences from MRs can be defined as follows. Given a training corpus consisting of MRs paired with their NL sentences, one needs to develop algorithms that learn how to effectively “paraphrase” MRs with natural language sentences. During testing, the system should be able to output the most probable NL “paraphrase” for a given new MR. The LNLZ08 system models p(T (w, m)), the joint generative process for the hybrid tree containing both NL and MR. This term can be rewritten in the following way: p(T1(�w, m)) = ρ(QUERY : answer(RIVER)|</context>
<context position="22410" citStr="Lu et al. (2008)" startWordPosition="3804" endWordPosition="3807">e edge features in the CRF framework. The function gk(hi, m, i) is a feature function of the hybrid sequence at vertex i and the entire MR tree. These features are usually referred to as the vertex features. The parameters Ak and µk are learned from the training data. In this task, we are given only MR-NL pairs and do not have the hybrid tree corresponding to each MR as training data. Now we describe how the set of candidate hybrid sequences for each MR production is obtained as well as how the training data for this model is constructed. After the joint generative model is learned as done in Lu et al. (2008), we first use a Viterbi algorithm to find the optimal hybrid tree for each MR-NL pair in the training set. From each optimal hybrid tree, we extract the hybrid sequence hi below each MR production mi. Using this process on the training MR-NL pairs, we can obtain a set of candidate + � � (i,j)EE k 405 hybrid sequences that can be associated with each MR production. The optimal hybrid tree generated by the Viterbi algorithm in this way is considered the “correct” hybrid tree for the MR-NL pair and is used as training data. While this does not provide hand-labeled training data, we believe the h</context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer. 2008. A generative model for parsing natural language to meaning representations. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP 2008), pages 783–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Conference on Human Language Technology Research (HLT</booktitle>
<pages>292--297</pages>
<contexts>
<context position="25926" citStr="Miyao and Tsujii, 2002" startWordPosition="4374" endWordPosition="4377">the feature forest model. The model has also been successfully applied to the HPSG parsing task. To train the model, we run the Viterbi algorithm on the trained LNLZ08 model and perform convex optimization using the feature forest model. The LNLZ08 model is trained using an EM algorithm with time complexity O(MN3D) per EM iteration, where M and N are respectively the maximum number of MR productions and NL words for each MR-NL pair, and D is the number of training instances. The time complexity of the Viterbi algorithm is also O(MN3D). For training the feature forest, we use the Amis toolkit (Miyao and Tsujii, 2002) which utilizes the GIS algorithm. The time complexity for each iteration of the GIS algorithm is O(MK2D), where K is the maximum number of candidate hybrid sequences associated with each MR production. Finally, the time complexity for generating a natural language sentence from a particular MR is O(MK2). 5 Experiments In this section, we present the results of our systems when evaluated on two standard benchmark corpora. The first corpus is GEOQUERY, which contains Prolog-based MRs that can be used to query a US geographic database (Kate et al., 2005). Our task for this domain is to generate </context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum entropy estimation for feature forests. In Proceedings of the 2nd International Conference on Human Language Technology Research (HLT 2002), pages 292–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature forest models for probabilistic HPSG parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="24987" citStr="Miyao and Tsujii, 2008" startWordPosition="4219" endWordPosition="4222">(STATE), RIVER: exclude(RIVER1, RIVER2)i ; Three-level hybrid sequence features : one hybrid sequence, its associated MR production, the parent MR production, and the grandparent MR production. For example: g3 : hrun through STATE1, RIVER: traverse(STATE), RIVER: exclude(RIVER1, RIVER2), RIVER: longest(RIVER)i; Adjacent hybrid sequence features : two adjacent hybrid sequences, together with their associated MR productions. For example: f1 : hrun through STATE1, RIVER1 that does not RIVER2, RIVER: traverse(STATE), RIVER: exclude(RIVER1, RIVER2)i . For training, we use the feature forest model (Miyao and Tsujii, 2008), which was originally designed as an efficient algorithm for solving maximum entropy models for data with complex structures. The model enables efficient training over packed trees that potentially represent exponential number of trees. The tree conditional random fields model can be effectively represented using the feature forest model. The model has also been successfully applied to the HPSG parsing task. To train the model, we run the Viterbi algorithm on the trained LNLZ08 model and perform convex optimization using the feature forest model. The LNLZ08 model is trained using an EM algori</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational Linguistics, 34(1):35–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic models for disambiguation of an HPSG-based chart generator.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technologies (IWPT</booktitle>
<volume>5</volume>
<pages>93--102</pages>
<contexts>
<context position="5701" citStr="Nakanishi et al., 2005" startWordPosition="893" endWordPosition="896">ted with several natural languages other than English (Japanese, Spanish, and Turkish). Evaluation results show that our proposed tree CRF-based model outperforms the previous model. 2 Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 that employed techniques from statistical machine translation using Synchronous Context-Free Grammar (SCFG) (Aho and Ullman, </context>
</contexts>
<marker>Nakanishi, Miyao, Tsujii, 2005</marker>
<rawString>Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic models for disambiguation of an HPSG-based chart generator. In Proceedings of the 9th International Workshop on Parsing Technologies (IWPT 2005), volume 5, pages 93–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="32439" citStr="Och, 2003" startWordPosition="5460" endWordPosition="5461">l (Wong, 2007). In contrast, the proposed tree CRF-based model is able to explicitly and flexibly exploit phrase-level features that model dependencies between adjacent phrases. In fact, with the hybrid tree framework, the better treatment of the tree structure of MR enables us to model some crucial dependencies between the complete MR tree and generated NL phrases. We believe that this property plays an important role in improving the quality of the generated sentences in terms of fluency, which is assessed by the evaluation metrics. Furthermore, WASP−1++ employs minimum error rate training (Och, 2003) to directly optimize the evaluation metrics. We have not done so but still obtain better performance. In future, we plan to explore ways to directly optimize the evaluation metrics in our system. 5.3 Experiments on different languages Following the work of Wong and Mooney (2007), we also evaluated our system’s performance on a subset of the GEOQUERY corpus with 250 instances, where sentences of 4 natural languages (English, Japanese, Spanish, and Turkish) are available. The evaluation results are shown in Table 4. Our tree CRF-based model achieves better performance on this task compared to W</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003), pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>311--318</pages>
<contexts>
<context position="27972" citStr="Papineni et al., 2002" startWordPosition="4705" endWordPosition="4708">133 0.6585 4.6648 0.6175 5.7293 0.4824 4.3283 Tree CRF-based model 0.6265 5.8907 0.6788 4.8486 0.6382 5.8488 0.5096 4.5033 Table 4: Results on the GEOQUERY-250 corpus with 4 natural languages. The GEOQUERY domain contains 880 instances, while the ROBOCUP domain contains 300 instances. The average NL sentence length for the two corpora are 7.57 and 22.52 respectively. Following the evaluation methodology of Wong and Mooney (2007), we performed 4 runs of the standard 10-fold cross validation and report the averaged performance in this section using the standard automatic evaluation metric BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)2. The BLEU and NIST scores of the WASP−1++ system reported in this section are obtained from the published paper of Wong and Mooney (2007). Note that to make our experimental results directly comparable to Wong and Mooney (2007), we used the identical training and test data splits for the 4 runs of 10-fold cross validation used by Wong and Mooney (2007) on both corpora. Our system has the advantage of always producing an NL sentence given any input MR, even if there exist unseen MR productions in the input MR. We can achieve this by simply skipping those unseen MR </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Jason Baldridge</author>
</authors>
<title>Adapting chart realization to CCG.</title>
<date>2003</date>
<booktitle>In Proceedings of the 9th European Workshop on Natural Language Generation (EWNLG</booktitle>
<pages>119--126</pages>
<contexts>
<context position="5788" citStr="White and Baldridge, 2003" startWordPosition="906" endWordPosition="909">). Evaluation results show that our proposed tree CRF-based model outperforms the previous model. 2 Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 that employed techniques from statistical machine translation using Synchronous Context-Free Grammar (SCFG) (Aho and Ullman, 1972). The system took in a linearized MR tree as input, and translated it into a natur</context>
</contexts>
<marker>White, Baldridge, 2003</marker>
<rawString>Michael White and Jason Baldridge. 2003. Adapting chart realization to CCG. In Proceedings of the 9th European Workshop on Natural Language Generation (EWNLG 2003), pages 119–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>Reining in CCG chart realization.</title>
<date>2004</date>
<booktitle>In Proceeding of the 3rd International Conference on Natural Language Generation (INLG</booktitle>
<pages>182--191</pages>
<contexts>
<context position="5802" citStr="White, 2004" startWordPosition="910" endWordPosition="911">that our proposed tree CRF-based model outperforms the previous model. 2 Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 that employed techniques from statistical machine translation using Synchronous Context-Free Grammar (SCFG) (Aho and Ullman, 1972). The system took in a linearized MR tree as input, and translated it into a natural language se</context>
</contexts>
<marker>White, 2004</marker>
<rawString>Michael White. 2004. Reining in CCG chart realization. In Proceeding of the 3rd International Conference on Natural Language Generation (INLG 2004), pages 182–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL</booktitle>
<pages>439--446</pages>
<contexts>
<context position="7368" citStr="Wong and Mooney, 2006" startWordPosition="1150" endWordPosition="1153">utperforms the direct translation model obtained from PHARAOH, when evaluated on two corpora. We will compare our system’s performance against that of WASP−1++ in Section 5. 3 The Hybrid Tree Framework and the LNLZ08 System QUERY: answer(RIVER) RIVER: longest(RIVER) RIVER: exclude(RIVER1 RIVER2) RIVER: river(all) RIVER: traverse(STATE) STATE: stateid(STATENAME) STATENAME : texas what is the longest river that does not run through texas Figure 1: An example MR paired with its NL sentence. Following most previous works in this area (Kate et al., 2005; Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008), we consider MRs in the form of tree structures. An example MR and its corresponding natural language sentence are shown in Figure 1. The MR is a tree consisting of nodes called MR productions. For example, the node “QUERY : answer(RIVER)” is one MR production. Each MR production consists of a semantic category (“QUERY”), a function symbol (“answer”) which can be optionally omitted, as well as an argument list which possibly contains 401 what is QUERY: answer(RIVER) RIVER: longest(RIVER) RIVER: exclude(RIVER1 RIVER2) RIVER: river(all) that does not RIVER :traverse(STATE) riv</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2006), pages 439–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Generation by inverting a semantic parser that uses statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL/HLT</booktitle>
<pages>172--179</pages>
<contexts>
<context position="5022" citStr="Wong and Mooney (2007)" startWordPosition="788" endWordPosition="791">l that performs generation at the phrase level. Motivated by conditional random fields (CRF) (Lafferty et al., 2001), a different parameterization of the conditional probability of the hybrid tree that enables the model to encode some longer range dependencies amongst phrases and MRs is used. This novel model is referred to as the tree CRF-based model. Evaluation results for both models are presented, through which we demonstrate that the tree CRF-based model performs better than the direct inversion model. We also compare the tree CRFbased model against the previous state-of-the-art model of Wong and Mooney (2007). Furthermore, we evaluate our model on a dataset annotated with several natural languages other than English (Japanese, Spanish, and Turkish). Evaluation results show that our proposed tree CRF-based model outperforms the previous model. 2 Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar</context>
<context position="19786" citStr="Wong and Mooney, 2007" startWordPosition="3327" endWordPosition="3330">IVER, which then gets concatenated to the shortest path from RIVER to END. S H p(hs|ms) (4) s=1 + |h3|+1 � j=1 404 QUERY: answer(RIVER) RIVER: longest(RIVER) RIVER: exclude(RIVER1 RIVER2) RIVER: river(all) RIVER: traverse(STATE) STATE: stateid(STATENAME) STATENAME : texas what is RIVER1 the longest RIVER1 RIVER1 that does not RIVER2 river run through STATE1 STATENAME1 texas Figure 3: An MR (left) and its associated hybrid sequences (right) alternative model as discussed next. 4.2 Tree CRF-Based Model Based on the belief that using known phrases usually leads to better fluency in the NLG task (Wong and Mooney, 2007), we explore methods for generating an NL sentence at phrase level rather than at word level. This is done by generating hybrid sequences as complete objects, rather than sequentially one word or semantic category at a time, from MR productions. We assume that each MR production can generate a complete hybrid sequence below it from a finite set of possible hybrid sequences. Each such hybrid sequence is called a candidate hybrid sequence associated with that particular MR production. Given a set of candidate hybrid sequences associated with each MR production, the generation task is to find the</context>
<context position="27782" citStr="Wong and Mooney (2007)" startWordPosition="4673" endWordPosition="4676"> 0.6220 6.9845 Table 3: Results of automatic evaluation of our tree CRF-based model and WASP−1++. English Japanese Spanish Turkish BLEU NIST BLEU NIST BLEU NIST BLEU NIST WASP−1++ 0.6035 5.7133 0.6585 4.6648 0.6175 5.7293 0.4824 4.3283 Tree CRF-based model 0.6265 5.8907 0.6788 4.8486 0.6382 5.8488 0.5096 4.5033 Table 4: Results on the GEOQUERY-250 corpus with 4 natural languages. The GEOQUERY domain contains 880 instances, while the ROBOCUP domain contains 300 instances. The average NL sentence length for the two corpora are 7.57 and 22.52 respectively. Following the evaluation methodology of Wong and Mooney (2007), we performed 4 runs of the standard 10-fold cross validation and report the averaged performance in this section using the standard automatic evaluation metric BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)2. The BLEU and NIST scores of the WASP−1++ system reported in this section are obtained from the published paper of Wong and Mooney (2007). Note that to make our experimental results directly comparable to Wong and Mooney (2007), we used the identical training and test data splits for the 4 runs of 10-fold cross validation used by Wong and Mooney (2007) on both corpora. Our syst</context>
<context position="31329" citStr="Wong and Mooney (2007)" startWordPosition="5282" endWordPosition="5285">e condition DR2C7 is true then players 2 , 3 , 7 and 8 should pass to player 4 Figure 4: Sample outputs from the two models, for GEOQUERY domain (top) and ROBOCUP domain (bottom) respectively. 5.2 Comparison with previous model We also compare the performance of our tree CRFbased model against the previous state-of-the-art system WASP−1++ in Table 3. Our tree CRF-based model achieves better performance on both corpora. We are unable to carry out statistical significance tests since the detailed BLEU and NIST scores of the cross validation runs of WASP−1++ as reported in the published paper of Wong and Mooney (2007) are not available. The results confirm our earlier discussions: the dependencies between the generated NL words are important and need to be properly modeled. The WASP−1++ system uses a log-linear model which incorporates two major techniques to attempt to model such dependencies. First, a backoff language model is used to capture dependencies at adjacent word level. Second, a technique that merges smaller translation rules into a single rigid rule is used to capture dependencies at phrase level (Wong, 2007). In contrast, the proposed tree CRF-based model is able to explicitly and flexibly ex</context>
<context position="32719" citStr="Wong and Mooney (2007)" startWordPosition="5503" endWordPosition="5506">enables us to model some crucial dependencies between the complete MR tree and generated NL phrases. We believe that this property plays an important role in improving the quality of the generated sentences in terms of fluency, which is assessed by the evaluation metrics. Furthermore, WASP−1++ employs minimum error rate training (Och, 2003) to directly optimize the evaluation metrics. We have not done so but still obtain better performance. In future, we plan to explore ways to directly optimize the evaluation metrics in our system. 5.3 Experiments on different languages Following the work of Wong and Mooney (2007), we also evaluated our system’s performance on a subset of the GEOQUERY corpus with 250 instances, where sentences of 4 natural languages (English, Japanese, Spanish, and Turkish) are available. The evaluation results are shown in Table 4. Our tree CRF-based model achieves better performance on this task compared to WASP−1++. We are again unable to conduct statistical significance tests for the same reason reported earlier. 6 Conclusions In this paper, we presented two novel models for the task of generating natural language sentences from given meaning representations, under a hybrid tree fr</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2007. Generation by inverting a semantic parser that uses statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL/HLT 2007), pages 172–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
</authors>
<title>Learning for Semantic Parsing and Natural Language Generation Using Statistical Machine Translation Techniques.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>The University of Texas at Austin.</institution>
<contexts>
<context position="31843" citStr="Wong, 2007" startWordPosition="5366" endWordPosition="5367">he cross validation runs of WASP−1++ as reported in the published paper of Wong and Mooney (2007) are not available. The results confirm our earlier discussions: the dependencies between the generated NL words are important and need to be properly modeled. The WASP−1++ system uses a log-linear model which incorporates two major techniques to attempt to model such dependencies. First, a backoff language model is used to capture dependencies at adjacent word level. Second, a technique that merges smaller translation rules into a single rigid rule is used to capture dependencies at phrase level (Wong, 2007). In contrast, the proposed tree CRF-based model is able to explicitly and flexibly exploit phrase-level features that model dependencies between adjacent phrases. In fact, with the hybrid tree framework, the better treatment of the tree structure of MR enables us to model some crucial dependencies between the complete MR tree and generated NL phrases. We believe that this property plays an important role in improving the quality of the generated sentences in terms of fluency, which is assessed by the evaluation metrics. Furthermore, WASP−1++ employs minimum error rate training (Och, 2003) to </context>
</contexts>
<marker>Wong, 2007</marker>
<rawString>Yuk Wah Wong. 2007. Learning for Semantic Parsing and Natural Language Generation Using Statistical Machine Translation Techniques. Ph.D. thesis, The University of Texas at Austin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>