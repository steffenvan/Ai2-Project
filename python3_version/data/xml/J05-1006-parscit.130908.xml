<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001093">
<title confidence="0.771584">
Book Reviews
</title>
<author confidence="0.7840925">
Ontological Semantics
Sergei Nirenburg and Victor Raskin
</author>
<affiliation confidence="0.994988">
(University of Maryland, Baltimore County, and Purdue University)
</affiliation>
<address confidence="0.506728666666667">
Cambridge, MA: The MIT Press, 2004,
xxi+420 pp; hardbound, ISBN
0-262-14086-1, $50.00, £32.95
</address>
<subsubsectionHeader confidence="0.231835">
Reviewed by
John F. Sowa
</subsubsectionHeader>
<bodyText confidence="0.999933481481482">
In this book, Nirenburg and Raskin present an important body of work in
computational linguistics that they and their colleagues have been developing over
the past 20 years. For a unifying perspective, they organize their assumptions, theories,
and techniques around the theme of ontological semantics. Along the way, they
critique many alternative views of semantics, which they distinguish from their own.
Their analyses contribute to a much-needed debate about the history and future of
computational linguistics, but to preserve some balance, teachers and students should
keep a few of the alternatives on their reference shelf.
The book is divided into two parts: a philosophical part I and a practical part II. The
first part consists of an introductory chapter 1 and four chapters that survey important
but controversial issues about linguistics, both theoretical and computational. In those
chapters, the authors make a good case for their version of ontological semantics, but the
alternatives are not treated in detail. In part II, the authors present their text-meaning
representation (TMR) and demonstrate how it is used in language analysis. Any dis-
cussion of technical material must use some notation, and TMR is sufficiently flexible to
illustrate a wide range of semantic-based methods that could be adapted to many other
formalisms. For most readers, part II would be the more important.
Chapter 1 is a good 25-page overview of computational linguistics with an
emphasis on semantics. Students and novices, however, need examples, and none are
given until chapter 6. The authors suggest that “a well-prepared and/or uninterested
reader”skip the remainder of part I and go straight to chapter 6, which begins with an
excellent five-page example. The authors follow that advice when they teach courses
from this text.
In Chapter 2, the authors present their “Prolegomena to the Philosophy of
Linguistics.”Their ideas are well taken, and some are as old as Socrates: Examine the
assumptions, challenge conventional wisdom, and test conclusions against experience.
The basis of their approach is what they call the four components of a scientific theory:
</bodyText>
<listItem confidence="0.99012125">
1. The purview of a theory is “the set of phenomena for which the theory
holds itself—and is held—accountable.”
2. The premises are belief statements “taken for granted by the theory and
not addressed in its body.”
</listItem>
<subsectionHeader confidence="0.240735">
Computational Linguistics Volume 31, Number 1
</subsectionHeader>
<bodyText confidence="0.993604068181819">
3. The body of a theory “is a set of its statements, variously referred to as
laws, propositions, regularities, theorems, or rules.”4. Justification “is the component of a theory that deals with considerations
about the quality of descriptions and about the choices a theory makes
in its premises, purview, and body.”
Under various names and with varying definitions, similar components are present
in most theories about theories. The authors’ claims of novelty in proposing them
“surprisingly, for the first time in the philosophy of science”are overstated.
An important point that the authors fail to mention is the purpose of a pro-
ject and the nature of the subject matter: Theories in mathematics, engineering,
and the empirical sciences are very different in kind and methods of justifica-
tion. Since computational linguistics is primarily an engineering discipline, it uses
theories from mathematics and the sciences, and it helps test and develop them.
But the primary justification for an engineering project is the ability to solve
a problem within the limits of budgets and deadlines. The authors spend too
much time arguing against engineering goals that are different from their own.
For some applications, such as machine translation, an analysis of truth condi-
tions may be unnecessary. For other applications, such as translating an English
question into a database query, truth conditions are the focus of the task. Instead
of recognizing that different engineers have different goals, they have tried to
banish truth conditions from linguistics. The following passage indicates a serious
misunderstanding:
—First, we maintain that reference is relevant for the study of coreference and anaphora ...
relations in text. Second, while we agree that truth plays no role in the speaker’s processing
of meaning, we are also aware of the need to “anchor”language in extralinguistic reality.
Formal semanticists use truth values for this purpose. We believe that this task requires a
tool with much more content, and that an ontology can and should serve as such a tool.
(page 109)
First, logicians do not use truth values to anchor language in reality; they use
references, which are resolved to entities (objects, properties, and events) in some
situation. Second, truth values are not primary, but derived from the mapping of
linguistic references to actual entities; a sentence is true if and only if the linguistic
configuration of references and relations conforms to the extralinguistic configura-
tion of entities. Third, every logician from Aristotle to the present has insisted that
an ontology of every general term is essential to determine the correct mapping
from language to reality. Aristotle himself never used the word ontology, even
though he created the subject; logicians are more likely to use the words theory,
axiomatization, and conceptualization as synonyms for what Nirenburg and Raskin call
an ontology.
Chapter 3 is a brief, 11-page history of semantics, but it is distorted by the fact
that the word semantics was not coined until the end of the 19th century. The subject
matter, however, was established by Aristotle in the books Categories, On
Interpretation, Analytics, Rhetoric, and Poetics. Under the name of logic or theory of
signs, the subject was thoroughly developed by the Hellenistic and medieval
philosophers. Most books on logic before the 20th century devoted at least half their
</bodyText>
<page confidence="0.989424">
148
</page>
<subsectionHeader confidence="0.883265">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.9843125">
text to conceptual analysis and ontology. The truncated view of history ignores 2,000
years of research:
</bodyText>
<listItem confidence="0.99273808">
• Frege is credited with the distinction between extension and intension,
but those words are Hamilton’s translation of the 17th century e´tendue
and compre´hension, which were just new names for a distinction that
had been analyzed in detail by Aristotle and the medieval scholastics.
• The citations for the “dawn of metalanguage”are to the 1950s. But
Aristotle’s theory of definition in terms of genus and differentiae is
metalanguage, and so is his theory of syllogisms for analyzing the
components of meaning. The scholastics introduced the terms first
intentions for language about physical objects (e.g., Homo est animal)
and second intentions for language about language (e.g., Homo est species).
• Katz and Fodor are given well-deserved credit for being the first in
the Chomskyan school to integrate a componential analysis of word
meaning with a compositional analysis of sentence meaning. Ockham
(1323), however, combined both componential and compositional
analyses to determine the truth conditions for Latin sentences. He not
only anticipated Frege’s compositionality and Tarski’s model theory,
he went beyond them by applying the techniques to a natural
language instead of an artificial one.
• Kamp did not “add to the agenda of formal semantics a treatment of
coreference and anaphora.”The “donkey sentences”that Kamp
analyzed were English translations of examples used by the scholastics
for analyzing similar phenomena.
• The modern history is just as flawed. The authors claim that first-order
predicate calculus (FOPC) has failed “to have made a historical impact.”In fact, FOPC is the foundation for the SQL databases that run the
world economy. That is certainly an impact.
</listItem>
<bodyText confidence="0.999767875">
A major omission is the early semantic work in AI and MT. One of the pioneers in
ontological semantics for MT was Margaret Masterman (1961), a former student of
Wittgenstein’s. She organized her ontology as a lattice defined in terms of 100
primitive concepts, which Wilks adopted as a basis for preference semantics. Hutchins
(1986) showed that her MT system did a better job of word selection than purely
syntactic systems of that time. Appropriately, her first publication on the subject was
in the Proceedings of the Aristotelian Society. Another pioneer was Silvio Ceccato (1961),
who based his correlational nets on a selection of 56 relation types, which included
case relations, type–subtype, type–instance, part–whole, and miscellaneous logical,
numerical, causal, spatial, and temporal relations. In parsing, Ceccato built
dependency trees, which he “correlated”with predefined nets to resolve ambiguities;
in generation, he used the nets to guide word selection. The single most influential
collection of the early work in these two fields, edited by Minsky (1968), included
classic papers by McCarthy, Quillian, Bobrow, and Raphael, among others.
Chapter 4 summarizes the goals and issues of lexical semantics with numerous
citations of authors who contributed to the field. Unfortunately, it has very few
</bodyText>
<page confidence="0.989535">
149
</page>
<note confidence="0.490449">
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.998740775510204">
examples comparing the ways different authors would analyze similar phenomena.
Nirenburg and Raskin cite seven authors in the Russian meaning–text school but don’t
give a single example to show how a meaning–text analysis would differ from their own
text-meaning analysis. Throughout the chapter, they discuss Pustejovsky’s generative
lexicon but never illustrate the arguments with examples. They consider Pustejovsky
“as a representationalist, antiformalist ally,”but they never explain why they consider
lexical semantics incompatible with formal semantics. That is especially odd, since the
next chapter positions “ontological semantics within the field of formal ontology.”Chapter 5 is a survey of formal ontology, an ancient subject that has become the
latest hope for conferring interoperability on incompatible systems. Most of that work,
however, has not been adapted to natural language processing. Work on lexical
resources, such as WordNet, is only loosely connected to work on formal ontology.
Section 5.3 discusses “the difficult and underexplored part of formal ontology, namely,
the relations between ontology and natural language.”The most difficult problem,
which the proponents of formal ontology fail to address, is the nature of ambiguities in
natural languages. A good parser can enumerate syntactic ambiguities, and selectional
constraints are usually sufficient to resolve most of them. The most serious ambiguities
are subtle variations in word senses (sometimes called microsenses), which change
over time with variations in word usage or in the subject matter to which the words are
applied. Such variations inevitably occur among independently developed systems
and Web sites, and attempts to legislate a single definition will not stop the growth and
shift of meaning. From their long experience with NL processing, Nirenburg and
Raskin probably have a deeper understanding of the nature of ambiguity than the
proponents of the Semantic Web. Section 5.4 is a wish list of features from formal
ontology that NL processors would need. Providing them is still a major research
problem.
After all the preliminaries, chapter 6 introduces text-meaning representation with
examples that illustrate the mapping from English to TMR. Section 6.1 begins with the
sample sentence Dresser Industries said it expects that major capital expenditure for
expansion of U.S. manufacturing capacity will reduce imports from Japan. The next five
pages carry out an informal analysis of that sentence without introducing any special
notation, not even TMR. Then section 6.2 introduces TMR and shows how the results
of the analysis in section 6.1 are mapped into it. The remaining sections of chapter 6
discuss the fine points of using TMR and compare them to other computational and
theoretical techniques.
TMR is essentially a network of frames, each of which has a head and a list of
binary relations that link the head to a frame, a pointer to another frame, a simple
value, or a more complex combination for defaults, semantic types, relaxable types,
etc. Each TMR is a set of six kinds of frames: one or more propositions, zero or more
discourse relations, zero or more modalities, one style, zero or more references, and
one TMR time. The kinds of frames are illustrated with numerous examples
discussed throughout chapters 6, 7, 8, and 9. Unfortunately, there is no appendix or
other reference section that gives a complete grammar or table of all the options for a
well-formed TMR. From the examples, one can surmise that the head of each
proposition frame is a concept instance that represents a state or event, which is
linked by case roles to the participants. In the middle of chapter 7 is a table of nine
case roles; at the end of chapter 8 is a list of five types of discourse relations, each of
which may have several subtypes. The authors acknowledge that TMR has been
evolving over the years, but a complete list of options for one version would be
appreciated.
</bodyText>
<page confidence="0.99403">
150
</page>
<subsectionHeader confidence="0.924195">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.99987774">
Chapter 7 presents the four static knowledge sources: ontology, fact database,
lexicon, and onomasticon. The subdivision into four sections is uneven: The fact
database is described in three pages, and the onomasticon in half a page, but the
ontology and lexicon sections take 36 pages and 15 pages, respectively. The discussion
of inheritance (a description logic with defaults) should be in a separate section, and
some material belongs in an appendix: the table of case roles, the list of 34 axioms that
define constraints on the Mikrokosmos ontology, and the description of the software
for browsing the knowledge sources. The question of what information to put in the
onomasticon, the lexicon, or the ontology raises some troublesome issues: Toyota, for
example, is in the onomasticon because it is the name of an instance of type
corporation, but Toyota Corolla is in the ontology because it is a type of car, which can
have many instances.
Chapter 8, which at 62 pages is the longest in the book, shows how TMR is used in
text analysis. Section 8.1 presents the stages of tokenization, morphology, lexical
lookup, and syntactic analysis. Section 8.2 covers the construction of dependency
structures for propositions, which includes matching selectional restrictions and
relaxing them for sentences such as The gorilla makes tools. Sections 8.3 and 8.4 cover
problems of ambiguity, nonliteral language, and the inevitable exceptions. Section 8.5
treats time, aspect, and modality. Section 8.6 handles discourse: reference and
coreference, discourse relations, and the temporal ordering of the propositions. This is
a good chapter, but one might like to see some discussion of alternative methods of
parsing and semantic interpretation. It would also be interesting to see a step-by-step
processing of the sample sentence that was analyzed by hand in section 6.1.
Chapter 9 addresses knowledge acquisition: the problem of constructing the four
knowledge sources discussed in chapter 7. This is a universal problem that
everybody involved with NL processing has to face, and nobody working in the field
is completely satisfied with the available resources. In this chapter, the authors focus
on the methods they have used in developing the Mikrokosmos ontology and
associated lexicon, but they discuss issues involved in adopting and adapting
resources such as WordNet and machine-readable dictionaries. They try to take an
ideal scientific stance toward the subject, but most readers are likely to adopt a
mixed strategy of adapting whatever resources they are given or are likely to find on
the Internet. As a fact database, many readers are likely to be given, in advance, a
conventional relational database, and the authors should discuss the issues of
incorporating such resources.
Chapter 10 is a three-page conclusion in which the authors apologize for the lack
of detail on applications and processing. Earlier in the book they say that the kinds
of applications for which TMR has been used “include machine translation,
information extraction (IE), question answering (QA), general human-computer
dialog systems, text summarization, and specialized applications combining some or
all of the above.”A couple of more chapters on language generation and reasoning
would have been more useful than most of the five chapters of part I. For students, a
glossary would be especially welcome, since the authors frequently mention a word
such as defeasible and follow it with a parenthetical list of citations instead of a
definition.
An embarrassing lapse shows that even people who design semantic processors
are forced to use less sophisticated tools for routine chores. The index contains five
references to I. J. Good, who had not been working on computational linguistics, but
only one reference leads to Good’s publications and four lead to capitalized
occurrences of the word good. This lapse is even more embarrassing for Good
</bodyText>
<page confidence="0.98896">
151
</page>
<note confidence="0.548261">
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.959847555555555">
(1965), who predicted “It is more probable than not that, within the twentieth century,
an ultraintelligent machine will be built and that it will be the last invention that man
need make.”Despite the historical and philosophical inaccuracies, this is a valuable textbook on
computational linguistics. Its greatest strength is its engineering contribution, and its
greatest weakness is the constant bickering with linguists and logicians who study
different aspects of the rich and complex subject of language. Humans and machines
require both logical and lexical processing for language understanding, and the
authors could better inform students by showing what their approach does best than
by trying to limit the range of topics linguists are allowed to explore.
</bodyText>
<sectionHeader confidence="0.99675" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.999783444444445">
Ceccato, Silvio. 1961. Linguistic Analysis and
Programming for Mechanical Translation.
Gordon and Breach, New York.
Good, Irving John. 1965. Speculations
concerning the first ultraintelligent
machine. In F. L. Alt and M. Rubinoff, eds.,
Advances in Computers, volume 6.
Academic Press, New York, 31–88.
Hutchins, W. John. 1986. Machine Translation:
Past, Present, Future. Ellis Horwood,
Chichester, England. Available at http://
ourworld.compuserve.com/homepages/
WJHutchins/PPF-TOC.htm.
Masterman, Margaret. 1961. Translation.
Proceedings of the Aristotelian Society,
61:169–216.
Minsky, Marvin, ed. 1968. Semantic
Information Processing. MIT Press,
Cambridge, MA.
Ockham, William of. 1323. Summa Logicae.
Part I translated as Ockham’s Theory of
Terms by M. J. Loux, University of Notre
Dame Press, Notre Dame, IN, 1974.
Part II translated as Ockham’s Theory of
Propositions by A. J. Freddoso and
H. Schuurman, University of Notre
Dame Press, Notre Dame, IN, 1980.
</reference>
<bodyText confidence="0.732238166666667">
John F. Sowa worked for 30 years on research and development projects at IBM, and he is a
cofounder of a startup company, VivoMind Intelligence, Inc. He has published and edited
several books and numerous articles on knowledge representation, computational linguistics,
and related areas of artificial intelligence. He is a Fellow of the AAAI and is best known for his
work on the theory of conceptual graphs and their application to natural language semantics.
Sowa’s e-mail address is sowa@bestweb.net.
</bodyText>
<page confidence="0.997085">
152
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.002935">
<title confidence="0.9964435">Book Reviews Ontological Semantics</title>
<author confidence="0.999908">Sergei Nirenburg</author>
<author confidence="0.999908">Victor Raskin</author>
<affiliation confidence="0.999729">(University of Maryland, Baltimore County, and Purdue University)</affiliation>
<address confidence="0.9740335">Cambridge, MA: The MIT Press, 2004, xxi+420 pp; hardbound, ISBN</address>
<email confidence="0.607793">$50.00,</email>
<note confidence="0.751166">Reviewed by</note>
<author confidence="0.974679">John F Sowa</author>
<abstract confidence="0.998037114893618">In this book, Nirenburg and Raskin present an important body of work in computational linguistics that they and their colleagues have been developing over the past 20 years. For a unifying perspective, they organize their assumptions, theories, techniques around the theme of semantics. the way, they critique many alternative views of semantics, which they distinguish from their own. Their analyses contribute to a much-needed debate about the history and future of computational linguistics, but to preserve some balance, teachers and students should keep a few of the alternatives on their reference shelf. The book is divided into two parts: a philosophical part I and a practical part II. The first part consists of an introductory chapter 1 and four chapters that survey important but controversial issues about linguistics, both theoretical and computational. In those chapters, the authors make a good case for their version of ontological semantics, but the alternatives are not treated in detail. In part II, the authors present their text-meaning representation (TMR) and demonstrate how it is used in language analysis. Any discussion of technical material must use some notation, and TMR is sufficiently flexible to illustrate a wide range of semantic-based methods that could be adapted to many other formalisms. For most readers, part II would be the more important. Chapter 1 is a good 25-page overview of computational linguistics with an emphasis on semantics. Students and novices, however, need examples, and none are given until chapter 6. The authors suggest that “a well-prepared and/or uninterested reader”skip the remainder of part I and go straight to chapter 6, which begins with an excellent five-page example. The authors follow that advice when they teach courses from this text. In Chapter 2, the authors present their “Prolegomena to the Philosophy of Linguistics.”Their ideas are well taken, and some are as old as Socrates: Examine the assumptions, challenge conventional wisdom, and test conclusions against experience. The basis of their approach is what they call the four components of a scientific theory: The a theory is “the set of phenomena for which the theory holds itself—and is held—accountable.” The belief statements “taken for granted by the theory and not addressed in its body.” Computational Linguistics Volume 31, Number 1 The a theory “is a set of its statements, variously referred to as propositions, regularities, theorems, or rules.”4. the component of a theory that deals with considerations about the quality of descriptions and about the choices a theory makes its premises, purview, and Under various names and with varying definitions, similar components are present in most theories about theories. The authors’ claims of novelty in proposing them “surprisingly, for the first time in the philosophy of science”are overstated. An important point that the authors fail to mention is the purpose of a project and the nature of the subject matter: Theories in mathematics, engineering, and the empirical sciences are very different in kind and methods of justification. Since computational linguistics is primarily an engineering discipline, it uses theories from mathematics and the sciences, and it helps test and develop them. But the primary justification for an engineering project is the ability to solve a problem within the limits of budgets and deadlines. The authors spend too much time arguing against engineering goals that are different from their own. For some applications, such as machine translation, an analysis of truth conditions may be unnecessary. For other applications, such as translating an English question into a database query, truth conditions are the focus of the task. Instead of recognizing that different engineers have different goals, they have tried to banish truth conditions from linguistics. The following passage indicates a serious misunderstanding: —First, we maintain that reference is relevant for the study of coreference and anaphora ... relations in text. Second, while we agree that truth plays no role in the speaker’s processing of meaning, we are also aware of the need to “anchor”language in extralinguistic reality. Formal semanticists use truth values for this purpose. We believe that this task requires a tool with much more content, and that an ontology can and should serve as such a tool. (page 109) First, logicians do not use truth values to anchor language in reality; they use references, which are resolved to entities (objects, properties, and events) in some situation. Second, truth values are not primary, but derived from the mapping of linguistic references to actual entities; a sentence is true if and only if the linguistic configuration of references and relations conforms to the extralinguistic configuration of entities. Third, every logician from Aristotle to the present has insisted that an ontology of every general term is essential to determine the correct mapping language to reality. Aristotle himself never used the word even he created the subject; logicians are more likely to use the words and synonyms for what Nirenburg and Raskin call an ontology. Chapter 3 is a brief, 11-page history of semantics, but it is distorted by the fact the word not coined until the end of the 19th century. The subject however, was established by Aristotle in the books and Under the name of logic or theory of signs, the subject was thoroughly developed by the Hellenistic and medieval philosophers. Most books on logic before the 20th century devoted at least half their 148 Book Reviews text to conceptual analysis and ontology. The truncated view of history ignores 2,000 years of research: • Frege is credited with the distinction between extension and intension, those words are Hamilton’s translation of the 17th century which were just new names for a distinction that had been analyzed in detail by Aristotle and the medieval scholastics. • The citations for the “dawn of metalanguage”are to the 1950s. But theory of definition in terms of metalanguage, and so is his theory of syllogisms for analyzing the of meaning. The scholastics introduced the terms language about physical objects (e.g., est intentions language about language (e.g., est • Katz and Fodor are given well-deserved credit for being the first in the Chomskyan school to integrate a componential analysis of word meaning with a compositional analysis of sentence meaning. Ockham (1323), however, combined both componential and compositional analyses to determine the truth conditions for Latin sentences. He not only anticipated Frege’s compositionality and Tarski’s model theory, he went beyond them by applying the techniques to a natural language instead of an artificial one. • Kamp did not “add to the agenda of formal semantics a treatment of coreference and anaphora.”The “donkey sentences”that Kamp analyzed were English translations of examples used by the scholastics for analyzing similar phenomena. • The modern history is just as flawed. The authors claim that first-order predicate calculus (FOPC) has failed “to have made a historical impact.”In fact, FOPC is the foundation for the SQL databases that run the world economy. That is certainly an impact. A major omission is the early semantic work in AI and MT. One of the pioneers in ontological semantics for MT was Margaret Masterman (1961), a former student of Wittgenstein’s. She organized her ontology as a lattice defined in terms of 100 primitive concepts, which Wilks adopted as a basis for preference semantics. Hutchins (1986) showed that her MT system did a better job of word selection than purely syntactic systems of that time. Appropriately, her first publication on the subject was the of the Aristotelian Another pioneer was Silvio Ceccato (1961), based his nets a selection of 56 relation types, which included case relations, type–subtype, type–instance, part–whole, and miscellaneous logical, numerical, causal, spatial, and temporal relations. In parsing, Ceccato built dependency trees, which he “correlated”with predefined nets to resolve ambiguities; in generation, he used the nets to guide word selection. The single most influential collection of the early work in these two fields, edited by Minsky (1968), included classic papers by McCarthy, Quillian, Bobrow, and Raphael, among others. Chapter 4 summarizes the goals and issues of lexical semantics with numerous citations of authors who contributed to the field. Unfortunately, it has very few 149 Computational Linguistics Volume 31, Number 1 examples comparing the ways different authors would analyze similar phenomena. Nirenburg and Raskin cite seven authors in the Russian meaning–text school but don’t give a single example to show how a meaning–text analysis would differ from their own text-meaning analysis. Throughout the chapter, they discuss Pustejovsky’s generative lexicon but never illustrate the arguments with examples. They consider Pustejovsky “as a representationalist, antiformalist ally,”but they never explain why they consider lexical semantics incompatible with formal semantics. That is especially odd, since the chapter positions “ontological semantics within the field of formal 5 is a survey of formal ontology, an ancient subject that has become the latest hope for conferring interoperability on incompatible systems. Most of that work, however, has not been adapted to natural language processing. Work on lexical resources, such as WordNet, is only loosely connected to work on formal ontology. Section 5.3 discusses “the difficult and underexplored part of formal ontology, namely, the relations between ontology and natural language.”The most difficult problem, which the proponents of formal ontology fail to address, is the nature of ambiguities in natural languages. A good parser can enumerate syntactic ambiguities, and selectional constraints are usually sufficient to resolve most of them. The most serious ambiguities subtle variations in word senses (sometimes called which change over time with variations in word usage or in the subject matter to which the words are applied. Such variations inevitably occur among independently developed systems and Web sites, and attempts to legislate a single definition will not stop the growth and shift of meaning. From their long experience with NL processing, Nirenburg and Raskin probably have a deeper understanding of the nature of ambiguity than the proponents of the Semantic Web. Section 5.4 is a wish list of features from formal ontology that NL processors would need. Providing them is still a major research problem. After all the preliminaries, chapter 6 introduces text-meaning representation with examples that illustrate the mapping from English to TMR. Section 6.1 begins with the sentence Industries said it expects that major capital expenditure for of U.S. manufacturing capacity will reduce imports from The next five pages carry out an informal analysis of that sentence without introducing any special notation, not even TMR. Then section 6.2 introduces TMR and shows how the results of the analysis in section 6.1 are mapped into it. The remaining sections of chapter 6 discuss the fine points of using TMR and compare them to other computational and theoretical techniques. TMR is essentially a network of frames, each of which has a head and a list of binary relations that link the head to a frame, a pointer to another frame, a simple value, or a more complex combination for defaults, semantic types, relaxable types, Each TMR is a set of six kinds of frames: one or more or more relations, or more or more time. kinds of frames are illustrated with numerous examples discussed throughout chapters 6, 7, 8, and 9. Unfortunately, there is no appendix or other reference section that gives a complete grammar or table of all the options for a well-formed TMR. From the examples, one can surmise that the head of each frame is a instance represents a state or event, which is linked by case roles to the participants. In the middle of chapter 7 is a table of nine case roles; at the end of chapter 8 is a list of five types of discourse relations, each of which may have several subtypes. The authors acknowledge that TMR has been evolving over the years, but a complete list of options for one version would be appreciated. 150 Book Reviews 7 presents the four knowledge ontology, fact database, lexicon, and onomasticon. The subdivision into four sections is uneven: The fact database is described in three pages, and the onomasticon in half a page, but the ontology and lexicon sections take 36 pages and 15 pages, respectively. The discussion of inheritance (a description logic with defaults) should be in a separate section, and some material belongs in an appendix: the table of case roles, the list of 34 axioms that define constraints on the Mikrokosmos ontology, and the description of the software for browsing the knowledge sources. The question of what information to put in the the lexicon, or the ontology raises some troublesome issues: for example, is in the onomasticon because it is the name of an instance of type but Corolla in the ontology because it is a type of car, which can have many instances. Chapter 8, which at 62 pages is the longest in the book, shows how TMR is used in text analysis. Section 8.1 presents the stages of tokenization, morphology, lexical lookup, and syntactic analysis. Section 8.2 covers the construction of dependency structures for propositions, which includes matching selectional restrictions and them for sentences such as gorilla makes Sections 8.3 and 8.4 cover problems of ambiguity, nonliteral language, and the inevitable exceptions. Section 8.5 treats time, aspect, and modality. Section 8.6 handles discourse: reference and coreference, discourse relations, and the temporal ordering of the propositions. This is a good chapter, but one might like to see some discussion of alternative methods of parsing and semantic interpretation. It would also be interesting to see a step-by-step processing of the sample sentence that was analyzed by hand in section 6.1. Chapter 9 addresses knowledge acquisition: the problem of constructing the four knowledge sources discussed in chapter 7. This is a universal problem that everybody involved with NL processing has to face, and nobody working in the field is completely satisfied with the available resources. In this chapter, the authors focus on the methods they have used in developing the Mikrokosmos ontology and associated lexicon, but they discuss issues involved in adopting and adapting resources such as WordNet and machine-readable dictionaries. They try to take an ideal scientific stance toward the subject, but most readers are likely to adopt a mixed strategy of adapting whatever resources they are given or are likely to find on the Internet. As a fact database, many readers are likely to be given, in advance, a conventional relational database, and the authors should discuss the issues of incorporating such resources. Chapter 10 is a three-page conclusion in which the authors apologize for the lack of detail on applications and processing. Earlier in the book they say that the kinds of applications for which TMR has been used “include machine translation, information extraction (IE), question answering (QA), general human-computer dialog systems, text summarization, and specialized applications combining some or all of the above.”A couple of more chapters on language generation and reasoning would have been more useful than most of the five chapters of part I. For students, a glossary would be especially welcome, since the authors frequently mention a word as follow it with a parenthetical list of citations instead of a definition. An embarrassing lapse shows that even people who design semantic processors are forced to use less sophisticated tools for routine chores. The index contains five references to I. J. Good, who had not been working on computational linguistics, but only one reference leads to Good’s publications and four lead to capitalized of the word This lapse is even more embarrassing for Good 151 Computational Linguistics Volume 31, Number 1 (1965), who predicted “It is more probable than not that, within the twentieth century, an ultraintelligent machine will be built and that it will be the last invention that man need make.”Despite the historical and philosophical inaccuracies, this is a valuable textbook on computational linguistics. Its greatest strength is its engineering contribution, and its greatest weakness is the constant bickering with linguists and logicians who study different aspects of the rich and complex subject of language. Humans and machines require both logical and lexical processing for language understanding, and the authors could better inform students by showing what their approach does best than by trying to limit the range of topics linguists are allowed to explore.</abstract>
<note confidence="0.564470444444444">References Silvio. 1961. Analysis and for Mechanical Gordon and Breach, New York. Good, Irving John. 1965. Speculations concerning the first ultraintelligent machine. In F. L. Alt and M. Rubinoff, eds., in volume 6. Academic Press, New York, 31–88. W. John. 1986. Translation: Present, Ellis Horwood, Chichester, England. Available at http:// ourworld.compuserve.com/homepages/ WJHutchins/PPF-TOC.htm. Masterman, Margaret. 1961. Translation. of the Aristotelian 61:169–216. Marvin, ed. 1968.</note>
<affiliation confidence="0.961199">MIT Press,</affiliation>
<address confidence="0.964558">Cambridge, MA.</address>
<note confidence="0.917641444444444">William of. 1323. I translated as Theory of M. J. Loux, University of Notre Dame Press, Notre Dame, IN, 1974. II translated as Theory of A. J. Freddoso and H. Schuurman, University of Notre Dame Press, Notre Dame, IN, 1980. F. Sowa for 30 years on research and development projects at IBM, and he is a</note>
<abstract confidence="0.9827982">cofounder of a startup company, VivoMind Intelligence, Inc. He has published and edited several books and numerous articles on knowledge representation, computational linguistics, and related areas of artificial intelligence. He is a Fellow of the AAAI and is best known for his work on the theory of conceptual graphs and their application to natural language semantics. Sowa’s e-mail address is sowa@bestweb.net.</abstract>
<intro confidence="0.822205">152</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Silvio Ceccato</author>
</authors>
<title>Linguistic Analysis and Programming for Mechanical Translation.</title>
<date>1961</date>
<publisher>Gordon and Breach,</publisher>
<location>New York.</location>
<contexts>
<context position="8519" citStr="Ceccato (1961)" startWordPosition="1326" endWordPosition="1327"> That is certainly an impact. A major omission is the early semantic work in AI and MT. One of the pioneers in ontological semantics for MT was Margaret Masterman (1961), a former student of Wittgenstein’s. She organized her ontology as a lattice defined in terms of 100 primitive concepts, which Wilks adopted as a basis for preference semantics. Hutchins (1986) showed that her MT system did a better job of word selection than purely syntactic systems of that time. Appropriately, her first publication on the subject was in the Proceedings of the Aristotelian Society. Another pioneer was Silvio Ceccato (1961), who based his correlational nets on a selection of 56 relation types, which included case relations, type–subtype, type–instance, part–whole, and miscellaneous logical, numerical, causal, spatial, and temporal relations. In parsing, Ceccato built dependency trees, which he “correlated”with predefined nets to resolve ambiguities; in generation, he used the nets to guide word selection. The single most influential collection of the early work in these two fields, edited by Minsky (1968), included classic papers by McCarthy, Quillian, Bobrow, and Raphael, among others. Chapter 4 summarizes the </context>
</contexts>
<marker>Ceccato, 1961</marker>
<rawString>Ceccato, Silvio. 1961. Linguistic Analysis and Programming for Mechanical Translation. Gordon and Breach, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irving John Good</author>
</authors>
<title>Speculations concerning the first ultraintelligent machine.</title>
<date>1965</date>
<booktitle>Advances in Computers,</booktitle>
<volume>6</volume>
<pages>31--88</pages>
<editor>In F. L. Alt and M. Rubinoff, eds.,</editor>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<marker>Good, 1965</marker>
<rawString>Good, Irving John. 1965. Speculations concerning the first ultraintelligent machine. In F. L. Alt and M. Rubinoff, eds., Advances in Computers, volume 6. Academic Press, New York, 31–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W John Hutchins</author>
</authors>
<title>Machine Translation: Past, Present, Future. Ellis Horwood,</title>
<date>1986</date>
<location>Chichester, England.</location>
<note>Available at http:// ourworld.compuserve.com/homepages/ WJHutchins/PPF-TOC.htm.</note>
<contexts>
<context position="8268" citStr="Hutchins (1986)" startWordPosition="1286" endWordPosition="1287">zing similar phenomena. • The modern history is just as flawed. The authors claim that first-order predicate calculus (FOPC) has failed “to have made a historical impact.”In fact, FOPC is the foundation for the SQL databases that run the world economy. That is certainly an impact. A major omission is the early semantic work in AI and MT. One of the pioneers in ontological semantics for MT was Margaret Masterman (1961), a former student of Wittgenstein’s. She organized her ontology as a lattice defined in terms of 100 primitive concepts, which Wilks adopted as a basis for preference semantics. Hutchins (1986) showed that her MT system did a better job of word selection than purely syntactic systems of that time. Appropriately, her first publication on the subject was in the Proceedings of the Aristotelian Society. Another pioneer was Silvio Ceccato (1961), who based his correlational nets on a selection of 56 relation types, which included case relations, type–subtype, type–instance, part–whole, and miscellaneous logical, numerical, causal, spatial, and temporal relations. In parsing, Ceccato built dependency trees, which he “correlated”with predefined nets to resolve ambiguities; in generation, h</context>
</contexts>
<marker>Hutchins, 1986</marker>
<rawString>Hutchins, W. John. 1986. Machine Translation: Past, Present, Future. Ellis Horwood, Chichester, England. Available at http:// ourworld.compuserve.com/homepages/ WJHutchins/PPF-TOC.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Masterman</author>
</authors>
<date>1961</date>
<booktitle>Translation. Proceedings of the Aristotelian Society,</booktitle>
<pages>61--169</pages>
<contexts>
<context position="8074" citStr="Masterman (1961)" startWordPosition="1256" endWordPosition="1257">ot “add to the agenda of formal semantics a treatment of coreference and anaphora.”The “donkey sentences”that Kamp analyzed were English translations of examples used by the scholastics for analyzing similar phenomena. • The modern history is just as flawed. The authors claim that first-order predicate calculus (FOPC) has failed “to have made a historical impact.”In fact, FOPC is the foundation for the SQL databases that run the world economy. That is certainly an impact. A major omission is the early semantic work in AI and MT. One of the pioneers in ontological semantics for MT was Margaret Masterman (1961), a former student of Wittgenstein’s. She organized her ontology as a lattice defined in terms of 100 primitive concepts, which Wilks adopted as a basis for preference semantics. Hutchins (1986) showed that her MT system did a better job of word selection than purely syntactic systems of that time. Appropriately, her first publication on the subject was in the Proceedings of the Aristotelian Society. Another pioneer was Silvio Ceccato (1961), who based his correlational nets on a selection of 56 relation types, which included case relations, type–subtype, type–instance, part–whole, and miscell</context>
</contexts>
<marker>Masterman, 1961</marker>
<rawString>Masterman, Margaret. 1961. Translation. Proceedings of the Aristotelian Society, 61:169–216.</rawString>
</citation>
<citation valid="true">
<date>1968</date>
<booktitle>Semantic Information Processing.</booktitle>
<editor>Minsky, Marvin, ed.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9010" citStr="(1968)" startWordPosition="1396" endWordPosition="1396">cation on the subject was in the Proceedings of the Aristotelian Society. Another pioneer was Silvio Ceccato (1961), who based his correlational nets on a selection of 56 relation types, which included case relations, type–subtype, type–instance, part–whole, and miscellaneous logical, numerical, causal, spatial, and temporal relations. In parsing, Ceccato built dependency trees, which he “correlated”with predefined nets to resolve ambiguities; in generation, he used the nets to guide word selection. The single most influential collection of the early work in these two fields, edited by Minsky (1968), included classic papers by McCarthy, Quillian, Bobrow, and Raphael, among others. Chapter 4 summarizes the goals and issues of lexical semantics with numerous citations of authors who contributed to the field. Unfortunately, it has very few 149 Computational Linguistics Volume 31, Number 1 examples comparing the ways different authors would analyze similar phenomena. Nirenburg and Raskin cite seven authors in the Russian meaning–text school but don’t give a single example to show how a meaning–text analysis would differ from their own text-meaning analysis. Throughout the chapter, they discu</context>
</contexts>
<marker>1968</marker>
<rawString>Minsky, Marvin, ed. 1968. Semantic Information Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<title>Summa Logicae. Part I translated as Ockham’s Theory of Terms</title>
<date>1974</date>
<booktitle>Part II translated as Ockham’s Theory of Propositions by</booktitle>
<publisher>Dame Press,</publisher>
<institution>University of Notre</institution>
<location>Notre Dame, IN,</location>
<marker>1974</marker>
<rawString>Ockham, William of. 1323. Summa Logicae. Part I translated as Ockham’s Theory of Terms by M. J. Loux, University of Notre Dame Press, Notre Dame, IN, 1974. Part II translated as Ockham’s Theory of Propositions by A. J. Freddoso and H. Schuurman, University of Notre Dame Press, Notre Dame, IN, 1980.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>