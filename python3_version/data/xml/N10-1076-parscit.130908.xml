<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025287">
<title confidence="0.9988375">
Automatic Diacritization for Low-Resource Languages Using a Hybrid
Word and Consonant CMM
</title>
<author confidence="0.997832">
Robbie A. Haertel, Peter McClanahan, and Eric K. Ringger
</author>
<affiliation confidence="0.827102666666667">
Department of Computer Science
Brigham Young University
Provo, Utah 84602, USA
</affiliation>
<email confidence="0.994567">
rah67@cs.byu.edu, petermcclanahan@gmail.com, ringger@cs.byu.edu
</email>
<sectionHeader confidence="0.998592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999681875">
We are interested in diacritizing Semitic lan-
guages, especially Syriac, using only dia-
critized texts. Previous methods have required
the use of tools such as part-of-speech taggers,
segmenters, morphological analyzers, and lin-
guistic rules to produce state-of-the-art results.
We present a low-resource, data-driven, and
language-independent approach that uses a
hybrid word- and consonant-level conditional
Markov model. Our approach rivals the best
previously published results in Arabic (15%
WER with case endings), without the use of
a morphological analyzer. In Syriac, we re-
duce the WER over a strong baseline by 30%
to achieve a WER of 10.5%. We also report
results for Hebrew and English.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973399999999">
Abjad writing systems omit vowels and other di-
acritics. The ability to restore these diacritics is
useful for personal, industrial, and governmental
purposes—especially for Semitic languages. In its
own right, the ability to diacritize can aid language
learning and is necessary for speech-based assis-
tive technologies, including speech recognition and
text-to-speech. Diacritics are also useful for tasks
such as segmentation, morphological disambigua-
tion, and machine translation, making diacritization
important to Natural Language Processing (NLP)
systems and intelligence gathering. In alphabetic
writing systems, similar techniques have been used
to restore accents from plain text (Yarowsky, 1999)
and could be used to recover missing letters in the
compressed writing styles found in email, text, and
instant messages.
We are particularly interested in diacritizing Syr-
iac, a low-resource dialect of Aramaic, which pos-
sesses properties similar to Arabic and Hebrew. This
work employs conditional Markov models (CMMs)
(Klein and Manning, 2002) to diacritize Semitic
(and other) languages and requires only diacritized
texts for training. Such an approach is useful for
languages (like Syriac) in which annotated data and
linguistic tools such as part-of-speech (POS) tag-
gers, segmenters, and morphological analyzers are
not available. Our main contributions are as follows:
(1) we introduce a hybrid word and consonant CMM
that allows access to the diacritized form of the pre-
vious words; (2) we introduce new features avail-
able in the proposed model; and (3) we describe an
efficient, approximate decoder. Our models signifi-
cantly outperform existing low-resource approaches
across multiple related and unrelated languages and
even achieve near state-of-the-art results when com-
pared to resource-rich systems.
In the next section, we review previous work rel-
evant to our approach. Section 3 then motivates and
describes the models and features used in our frame-
work, including a description of the decoder. We
describe our data in Section 4 and detail our exper-
imental setup in Section 5. Section 6 presents our
results. Finally, Section 7 briefly discusses our con-
clusions and offers ideas for future work.
</bodyText>
<sectionHeader confidence="0.997273" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.9959595">
Diacritization has been receiving increased attention
due to the rising interest in Semitic languages, cou-
</bodyText>
<page confidence="0.975203">
519
</page>
<note confidence="0.752669">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 519–527,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999872377777778">
pled with the importance of diacritization to other
NLP-related tasks. The existing approaches can be
categorized based on the amount of resources they
require, their basic unit of analysis, and of course
the language they are targeting. Probabilistic sys-
tems can be further divided into generative and con-
ditional approaches.
Existing methodologies can be placed along a
continuum based on the quantity of resources they
require—a reflection of their cost. Examples of
resources used include morphological analyzers
(Habash and Rambow, 2007; Ananthakrishnan et al.,
2005; Vergyri and Kirchhoff, 2004; El-Sadany and
Hashish, 1989), rules for grapheme-to-sound con-
version (El-Imam, 2008), transcribed speech (Ver-
gyri and Kirchhoff, 2004), POS tags (Zitouni et al.,
2006; Ananthakrishnan et al., 2005), and a list of
prefixes and suffixes (Nelken and Shieber, 2005).
When such resources exist for a particular language,
they typically improve performance. For instance,
Habash and Rambow’s (2007) approach reduces the
error rate of Zitouni et al.’s (2006) by as much as
30% through its use of a morphological analyzer. In
fact, such resources are not always available. Sev-
eral data-driven approaches exist that require only
diacritized texts (e.g., K¨ubler and Mohamed, 2008;
Zitouni et al., 2006; Gal, 2002) which are relatively
inexpensive to obtain: most literate speakers of the
target language could readily provide them.
Apart from the quantity of resources required, di-
acritization systems also differ in their basic unit of
analysis. A consonant-based approach treats each
consonant1 in a word as a potential host for one
or more (possibly null) diacritics; the goal is to
predict the correct diacritic(s) for each consonant
(e.g., K¨ubler and Mohamed, 2008). Zitouni et al.
(2006) extend the problem to a sequence labeling
task wherein they seek the best sequence of diacrit-
ics for the consonants. Consequently, their approach
has access to previously chosen diacritics.
Alternatively, the basic unit of analysis can be the
full, undiacritized word. Since morphological ana-
lyzers produce analyses of undiacritized words, di-
acritization approaches that employ them typically
fall into this category (e.g., Habash and Rambow,
</bodyText>
<footnote confidence="0.888534">
1We refer to all graphemes present in undiacritized texts as
consonants.
</footnote>
<bodyText confidence="0.998540833333333">
2007; Vergyri and Kirchoff, 2004). Word-based,
low-resource solutions tend to treat the problem as
word-level sequence labeling (e.g., Gal, 2002).
Unfortunately, word-based techniques face prob-
lems due to data sparsity: not all words in the
test set are seen during training. In contrast,
consonant-based approaches rarely face the anal-
ogous problem of previously unseen consonants.
Thus, one low-resource solution to data sparsity is to
use consonant-based techniques for unknown words
(Ananthakrishnan et al., 2005; Nelken and Shieber,
2005).
Many of the existing systems, especially recent
ones, are probabilistic or contain probabilistic com-
ponents. Zitouni et al. (2006) show the superior-
ity of their conditional-based approaches over the
best-performing generative approaches. However,
the instance-based learning approach of K¨ubler and
Mohamed (2008) slightly outperforms Zitouni et
al. (2006). In the published literature for Arabic,
the latter two have the best low-resource solutions.
Habash and Rambow (2007) is the state-of-the-art,
high-resource solution for Arabic. To our knowl-
edge, no work has been done in this area for Syriac.
</bodyText>
<sectionHeader confidence="0.993851" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.999827947368421">
In this work, we are concerned with diacritiza-
tion for Syriac for which a POS tagger, segmenter,
and other tools are not readily available, but for
which diacritized text is obtainable.2 Use of a sys-
tem dependent on a morphological analyzer such as
Habash and Rambow’s (2007) is therefore not cost-
effective. Furthermore, we seek a system that is ap-
plicable to a wide variety of languages. Although
K¨ubler and Mohamed’s (2008) approach is compet-
itive to Zitouni et al.’s (2006), instance-based ap-
proaches tend to suffer with the addition of new fea-
tures (their own experiments demonstrate this). We
desire to add linguistically relevant features to im-
prove performance and thus choose to use a condi-
tional model. However, unlike Zitouni et al. (2006),
we use a hybrid word- and consonant-level approach
based on the following observations (statistics taken
from the Syriac training and development sets ex-
plained in Section 4):
</bodyText>
<footnote confidence="0.9833215">
2Kiraz (2000) describes a morphological analyzer for Syriac
that is not publicly available and is costly to reproduce.
</footnote>
<page confidence="0.986628">
520
</page>
<listItem confidence="0.98697">
1. Many undiacritized words are unambiguous: in which the local conditional distribution of a di-
90.8% of the word types and 63.5% of the to- acritized word is an interpolation of a word-level
kens have a single diacritized form. model (ωui) and a consonant-level model (γ):
2. Most undiacritized word types have only a few
possible diacritizations: the average number of
possible diacritizations is 1.11.
3. Low-frequency words have low ambiguity:
Undiacritized types occurring fewer than 5
times have an average of 1.05 possible diacriti-
zations.
4. Diacritized words not seen in the training data
occur infrequently at test time: 10.5% of the
diacritized test tokens were not seen in training.
5. The diacritics of previous words can provide
useful morphological information such as per-
son, number, and gender.
</listItem>
<bodyText confidence="0.99716176">
Contrary to observations 1 and 2, consonant-level
approaches dedicate modeling capacity to an expo-
nential (in the number of consonants) number of
possible diacritizations of a word. In contrast, a
word-level approach directly models the (few) dia-
critized forms seen in training. Furthermore, word-
based approaches naturally have access to the dia-
critics of previous words if used in a sequence la-
beler, as per observation 5. However, without a
“backoff” strategy, word-level models cannot pre-
dict a diacritized form not seen in the training data.
Also, low-frequency words by definition have less
information from which to estimate parameters. In
contrast, abundant information exists for each dia-
critic in a consonant-level system. To the degree
to which they hold, observations 3 and 4 mitigate
these latter two problems. Clearly a hybrid approach
would be advantageous.
To this end we employ a CMM in which we treat
the problem as an instance of sequence labeling at
the word level with less common words being han-
dled by a consonant-level CMM. Let u be the undi-
acriatized words in a sentence. Applying an order o
Markov assumption, the distribution over sequences
of diacritized words d is:
</bodyText>
<equation confidence="0.9998842">
P (d|u) _ � kdk P (di|di−o...i−1, u; ω, γ, α) (1)
i=1
P (di|di−o...i−1, u; ω,γ, α) _
αP (di|di−o...i−1, u; ωui) +
(1 − α)P (di|di−o...i−1, u; γ)
</equation>
<bodyText confidence="0.9999591">
We let the consonant-level model be a standard
CMM, similar to Zitouni et al. (2006), but with ac-
cess to previously diacritized words. Note that the
order of this “inner” CMM need not be the same as
that of the outer CMM.
The parameter α reflects the degree to which we
trust the word-level model. In the most general case,
α can be a function of the undiacritized words and
the previous o diacritized words. Based on our ear-
lier enumerated observations, we use a simple delta
function for α: we let α be 0 when ui is rare and 1
otherwise. We leave discussion for what constitutes
a “rare” undiacritized type for Section 5.2.
Figure 1b presents a graphical model of a sim-
ple example sentence in Syriac. The diacritiza-
tion for non-rare words is predicted for a whole
word, hence the random variable D for each such
word. These diacritized words Di depend on previ-
ous Di−1 as per equation (1) for an order-1 CMM
(note that the capitalized A, I, and O are in fact con-
sonants in this transliteration). Because “NKTA”
and “RGT” are rare, their diacritization is repre-
sented by a consonant-level CMM: one variable for
each possible diacritic in the word. Importantly,
these consonant-level models have access to the pre-
viously diacritized word (D4 and D6, respectively).
We use log-linear models for all local distribu-
tions in our CMMs, i.e., we use maximum entropy
(maxent) Markov models (McCallum et al., 2000;
Berger et al., 1996). Due to the phenomenon known
as d-separation (Pearl and Shafer, 1988), it is possi-
ble to independently learn parameters for each word
model ωui by training only on those instances for
the corresponding word. Similarly, the consonant
model can be learned independent of the word mod-
els. We place a spherical normal prior centered at
zero with a standard deviation of 1 over the weights
of all models and use an L-BFGS minimizer to find
the MAP estimate of the weights for all the models
(words and consonant).
</bodyText>
<page confidence="0.981518">
521
</page>
<figure confidence="0.998598210526316">
γ ω eo γ
Cs.� ...
Cs.� Cs.� Cs.� Cs.� Cs.�
CSIA AO
ω����
ω ����
ω ��
D,
D,
D3
D4
Cs.� Cx qs C 4 D, C7.1 C7. C73
CSIA
AO
DHRA
AO NKTA
LA
RGT
(a) (b)
</figure>
<figureCaption confidence="0.8919665">
Figure 1: Graphical models of Acts 20:33 in Syriac, CSIA AO DHBA AO NKTA LA RGT ‘silver or gold or
garment I have not coveted,’ using Kiraz’s (1994) transliteration for (a) the initial portion of a consonant-
</figureCaption>
<bodyText confidence="0.96498125">
level-only model and (b) a combined word- and consonant-level model. For clarity, both models assume a
consonant-level Markov order of 1; (b) shows a word-level Markov order of 1. For simplicity, the figure
further assumes that additional features come only from the current (undiacritized) word.
Note that Zitouni et al.’s (2006) model is a spe-
cial case of equation (1) where all words are rare, the
word-level Markov order (o) is 0, and the consonant-
level Markov order is 2. A simplified version of Zi-
touni’s model is presented in Figure 1a.
</bodyText>
<subsectionHeader confidence="0.986271">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.985239444444444">
Our features are based on those found in Zitouni et
al. (2006), although we have added a few of our own
which we consider to be one of the contributions of
this paper. Unlike their work, our consonant-level
model has access to previously diacritized words,
allowing us to exploit information noted in obser-
vation 5.
Each of the word-level models shares the same set
of features, defined by the following templates:
</bodyText>
<listItem confidence="0.977347571428571">
• The prefixes and suffixes (up to 4 characters) of
the previously diacritized words.
• The string of the actual diacritics, including the
null diacritic, from each of the previous o dia-
critized words and n-grams of these strings; a
similar set of features is extracted but without
the null diacritics.
• Every possible (overlapping) n-gram of all
sizes from n = 1 to n = 5 of undiacritized
words contained within the window defined by
2 words to the right and 2 to the left. These
templates yield 15 features for each token.
• The count of how far away the current token
is from the beginning/end of the sentence up
</listItem>
<bodyText confidence="0.888438833333333">
to the Markov order; also, their binary equiva-
lents.
The first two templates rely on diacritizations of pre-
vious words, in keeping with observation 5.
The consonant-level model has the following fea-
ture templates:
</bodyText>
<listItem confidence="0.9881486">
• The current consonant.
• Previous diacritics (individually, and n-grams
of diacritics ending in the diacritic prior to the
current consonant, where n is the consonant-
level Markov order).
• Conjunctions of the first two templates.
• Indicators as to whether this is the first or last
consonant.
• The first three templates independently con-
joined with the current consonant.
• Every possible (overlapping) n-gram of all
sizes from n = 1 to n = 11 consisting of con-
sonants contained within the window defined
by 5 words to the right and 5 to the left.
• Same as previous, but available diacritics are
included in the window.
• Prefixes and suffixes (of up to length 4) of pre-
viously diacritized words conjoined with previ-
ous diacritics in the current token, both individ-
ually and n-grams of such.
</listItem>
<page confidence="0.991303">
522
</page>
<bodyText confidence="0.9990725">
This last template is only possible because of our
model’s dependency on previous diacritized words.
</bodyText>
<subsectionHeader confidence="0.990769">
3.2 Decoder
</subsectionHeader>
<bodyText confidence="0.999995230769231">
Given a sentence consisting of undiacritized words,
we seek the most probable sequence of diacritized
words, i.e., arg maxd P(dju...). In sentences con-
taining no rare words, the well-known Viterbi algo-
rithm can be used to find the optimum.
However, as can be seen in Figure 1b, predictions
in the consonant-level model (e.g., C5,1...4) depend
on previously diacritized words (D4), and some dia-
critized words (e.g., DO depend on diacritics in the
previous rare word (C5,1...4). These dependencies
introduce an exponential number of states (in the
length of the word) for rare words, making exact de-
coding intractable. Instead, we apply a non-standard
beam during decoding to limit the number of states
for rare words to the n-best (locally). This is ac-
complished by using an independent “inner” n-best
decoder for the consonant-level CMM to produce
the n-best diacritizations for the rare word given the
previous diacritized words and other features. These
become the only states to and from which transitions
in the “outer” word-level decoder can be made. We
note this is the same type of decoding that is done in
pipeline models that use n-best decoders (Finkel et
al., 2006). Additionally, we use a traditional beam-
search of width 5 to further reduce the search space
both in the outer and inner CMMs.
</bodyText>
<sectionHeader confidence="0.998694" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.9998146">
Although our primary interest is in the Syriac lan-
guage, we also experimented with the Penn Arabic
Treebank (Maamouri et al., 2004) for the sake of
comparison with other approaches. We include He-
brew to provide results for yet another Semitic lan-
guage. We also apply the models to English to show
that our method and features work well outside of
the Semitic languages. A summary of the datasets,
including the number of diacritics, is found in Fig-
ure 2. The number of diacritics shown in the table
is less than the number of possible predictions since
we treat contiguous diacritics between consonants as
a single prediction.
For our experiments in Syriac, we use the New
Testament portion of the Peshitta (Kiraz, 1994) and
</bodyText>
<table confidence="0.9627566">
lang diacs train dev test
Syriac 9 87,874 10,747 11,021
Arabic 8 246,512 42,105 51,664
Hebrew 17 239,615 42,133 49,455
English 5 1,004,073 80,156 89,537
</table>
<figureCaption confidence="0.9631395">
Figure 2: Number of diacritics and size (in tokens)
of each dataset
</figureCaption>
<bodyText confidence="0.998906918918919">
treat each verse as if it were a sentence. The diacrit-
ics we predict are the five short vowels, as well as
S˚ey¯am¯e, Rukk¯akh¯a, Quˇsˇs¯ay¯a, and linea ocultans.
For Arabic, we use the training/test split defined
by Zitouni et al. (2006). We group all words having
the same P index value into a sentence. We build our
own development set by removing the last 15% of
the sentences of the training set. Like Zitouni, when
no solution exists in the treebank, we take the first
solution as the gold tag. Zitouni et al. (2006) report
results on several different conditions, but we focus
on the most challenging of the conditions: we pre-
dict the standard three short vowels, three tanween,
sukuun, shadda, and all case endings. (Preliminary
experiments show that our models perform equally
favorably in the other scenarios as well.)
For Hebrew, we use the Hebrew Bible (Old Tes-
tament) in the Westminster Leningrad Codex (Zefa-
nia XML Project, 2009). As with Syriac, we treat
each verse as a sentence and remove the paragraph
markers (pe and samekh). There is a large number
of diacritics that could be predicted in Hebrew and
no apparent standardization in the literature. For
these reasons, we attempt to predict as many dia-
critics as possible. Specifically, we predict the di-
acritics whose unicode values are 05B0-B9, 05BB-
BD, 05BF, 05C1-C2, and 05C4. We treat the follow-
ing list of punctuation as consonants: maqaf, paseq,
sof pasuq, geresh, and gershayim. The cantillation
marks are removed entirely from the data.
Our English data comes from the Penn Treebank
(Marcus et al., 1994). We used sections 0–20 as
training data, 21–22 as development data, and 23–
24 as our test set. Unlike words in the Semitic lan-
guages, English words can begin with a vowel, re-
quiring us to prepend a prosthetic consonant to every
word; we also convert all English text to lowercase.
</bodyText>
<page confidence="0.998875">
523
</page>
<sectionHeader confidence="0.999461" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999953470588236">
For all feature engineering and tuning, we trained
and tested on training and development test sets, re-
spectively (as specified above). Final results are re-
ported by folding the development test set into the
training data and evaluating on the blind test set. We
retain only those features that occur more than once.
For each approach, we report the Word Error Rate
(WER) (i.e., the percentage of words that were in-
correctly diacritized), along with the Diacritic Er-
ror Rate (DER) (i.e., the percentage of diacritics, in-
cluding the null diacritic, that were incorrectly pre-
dicted). We also report both WER and DER for
only those words that were not seen during training
(UWER and UDER, respectively). We found that
precision, recall, and f-score were nearly perfectly
correlated with DER; hence, we omit this informa-
tion for brevity.
</bodyText>
<subsectionHeader confidence="0.905787">
5.1 Models for Evaluation
</subsectionHeader>
<bodyText confidence="0.999801227272727">
In previous work, K¨ubler et al. (2008) report the
lowest error rates of the low-resource models. Al-
though their results are not directly comparable to
Zitouni et al. (2006), we have independently con-
firmed that the former slightly outperforms the latter
using the same diacritics and on the same dataset
(see Figure 4), thereby providing the strongest pub-
lished baseline for Arabic on a common dataset. We
denote this model as k¨ubler and use it as a strong
baseline for all datasets.
For the Arabic results, we additionally include Zi-
touni et al.’s (2006) lexical model (zitouni-lex)
and their model that uses a segmenter and POS
tagger (zitouni-all), which are not immediately
available to us for Syriac. For yet another point of
reference for Arabic, we provide the results from the
state-of-the-art (resource-rich) approach of Habash
and Rambow (2007) (habash). This model is at an
extreme advantage, having access to a full morpho-
logical analyzer. Note that for these three models
we simply report their published results and do not
attempt to reproduce them.
Since k¨ubler is of a different model class than
ours, we consider an additional baseline that is a
consonant-level CMM with access to the same in-
formation, namely, only those consonants within a
window of 5 to either side (ccmm). This is equiva-
lent to a special case of our hybrid model wherein
both the word-level and the consonant-level Markov
order are 0. The features that we extract from this
window are the windowed n-gram features.
In order to assess the utility of previous diacritics
and how effectively our features leverage them, we
build a model based on the methodology from Sec-
tion 3 but specify that all words are rare, effectively
creating a consonant-only model that has access to
the diacritics of previous words. We call this model
cons-only. We note that the main difference be-
tween this model and zitouni-lex are features
that depend on previous diacritized words.
Finally, we present results using our full hybrid
model (hybrid). We use a Markov order of 2 at
the word and consonant level for both hybrid and
cons-only.
</bodyText>
<subsectionHeader confidence="0.99493">
5.2 Consonant-Level Model and Rare Words
</subsectionHeader>
<bodyText confidence="0.999906892857143">
The hybrid nature of hybrid naturally raises the
question of whether or not the inner consonant
model should be trained only on rare words or on
all of the data. In other words, is the distribution
of diacritics different in rare words? If so, the con-
sonant model should be trained only on rare words.
To answer this question, we trained our consonant-
level model (cons-only) on words occurring fewer
than n times. We swept the value of the threshold n
and compared the results to the same model trained
on a random selection of words. As can be seen in
Figure 3, the performance on unknown words (both
UWER and UDER) using a model trained on rare
words can be much lower than using a model trained
on the same amount of randomly selected data. In
fact, training on rare words can lead to a lower error
rate on unknown words than training on all tokens
in the corpus. This suggests that the distribution of
diacritics in rare words is different from the distri-
bution of diacritics in general. This difference may
come from foreign words, especially in the Arabic
news corpus.
While this phenomenon is more pronounced in
some languages and with some models more than
others, it appears to hold in the cases we tried. We
found the WER for unknown words to be lowest for
a threshold of 8, 16, 32, and 32 for Syriac, Arabic,
Hebrew, and English, respectively.
</bodyText>
<page confidence="0.992266">
524
</page>
<figure confidence="0.999775821428571">
error rate
error rate
0 10000 20000 30000 40000 50000 60000 70000 80000 90000
tokens
(a) Syriac
0 50000 100000 150000 200000 250000
tokens
(b) Arabic
0.8
0.6
0.4
0.2
0
1
UWER (random)
UWER (rare))
UDER ( random
UDER (rare)
0.8
0.6
0.4
0.2
0
1
UWER (random)
UWER (rare))
UDER ( random
UDER (rare)
</figure>
<figureCaption confidence="0.997769333333333">
Figure 3: Learning curves showing impact on consonant-level models when training on rare tokens for
Syriac and Arabic. Series marked “rare” were trained with the least common tokens in the dataset.
Figure 4: Results for all languages and approaches
</figureCaption>
<sectionHeader confidence="0.769409" genericHeader="method">
6 Discussion of Results
</sectionHeader>
<bodyText confidence="0.999039675">
Since Syriac is of primary interest to us, we begin
by examining the results from this dataset. Syriac
appears to be easier to diacritize than Arabic, con-
sidering it has a similar number of diacritics and
only one-third the amount of data. On this dataset,
hybrid has the lowest WER and DER, achieving
nearly 30% and 18% reduction in WER and DER,
respectively, over k¨ubler; it reduces both error
rates over cons-only by more than 14%. These
results attest to the effectiveness of our model in ac-
counting for the observations made in Section 3.
A similar pattern holds for the Hebrew and En-
glish datasets, namely that hybrid reduces the
WER over k¨ubler by 28% to upwards of 50%;
cons-only also consistently and significantly out-
performs k¨ubler and ccmm. However, the reduc-
tion in error rate for our cons-only and hybrid
models tends to be lower for DER than WER in
all languages except for English. In the case of
hybrid, this is probably because it is inherently
word-based. Having access to entire previous dia-
critized words may be a contributing factor as well,
especially in cons-only.
When comparing model classes (k¨ubler and
ccmm), it appears that performance is comparable
across all languages, with the maxent approach en-
joying a slight advantage except in English. Interest-
ingly, the maxent solution usually handles unknown
words better, although it does not specifically target
this case. Both models outperform zitouni-lex
in Arabic, despite the fact that they use a much
simpler feature set, most notably, the lack of pre-
vious diacritics. In the case of ccmm this may be at-
tributable in part to our use of an L-BFGS optimizer,
convergence criteria, feature selection, or other po-
tential differences not noted in Zitouni et al. (2006).
We note that the maxent-based approaches are much
more time and memory intensive.
Using the Arabic data, we are able to com-
pare our methods to several other published results.
</bodyText>
<table confidence="0.942395458333333">
Approach WER DER UWER UDER
k¨ubler 15.04 5.23 64.65 18.21
ccmm 13.99 4.82 54.54 15.18
cons-only 12.31 5.03 55.68 19.09
hybrid 10.54 4.29 55.16 18.86
zitouni-lex 25.1 8.2 NA NA
k¨ubler 23.61 7.25 66.69 20.51
ccmm 22.63 6.61 57.71 16.10
cons-only 15.02 5.15 48.10 15.76
hybrid 17.87 5.67 47.85 15.63
zitouni-all 18.0 5.5 NA NA
habash 14.9 4.8 NA NA
k¨ubler 30.60 12.96 89.52 36.86
ccmm 29.67 12.05 80.02 29.39
cons-only 23.39 10.92 75.70 33.34
hybrid 22.18 10.71 74.38 32.40
k¨ubler 10.54 4.38 54.96 16.31
ccmm 11.60 4.71 58.55 16.34
cons-only 8.71 3.87 58.93 17.85
hybrid 5.39 2.38 57.24 16.51
Syriac
Arabic
Hebrew
English
</table>
<page confidence="0.996499">
525
</page>
<bodyText confidence="0.999988096774193">
The cons-only model significantly outperforms
zitouni-all despite the additional resources to
which the latter has access. This is evidence sup-
porting our hypothesis that the diacritics from pre-
vious words in fact contain useful information for
prediction. This empirically suggests that the inde-
pendence assumptions in consonant-only models are
too strict.
Perhaps even more importantly, our low-resource
method approaches the performance of habash. We
note that the differences may not be statistically sig-
nificant, and also that Habash and Rambow (2007)
omit instances in the data that lack solutions. In fact,
cons-only has a lower WER than all but two of
the seven techniques used by Habash and Rambow
(2007), which use a morphological analyzer.
Interestingly, hybrid does worse than
cons-only on this dataset, although it is still
competitive with zitouni-all. We hypothesize
that the observations from Section 3 do not hold
as strongly for this dataset. For this reason, using
a smooth interpolation function (rather than the
abrupt one we employ) may be advantageous and is
an interesting avenue for future research.
One last observation is that the approaches that
use diacritics from previous words (i.e., cons-only
and hybrid) usually have lower sentence error rates
(not shown in Figure 4). This highlights an advan-
tage of observation 5: that dependencies on previ-
ously diacritized words can help ensure a consistent
tagging within a sentence.
</bodyText>
<sectionHeader confidence="0.998007" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999991685185186">
In this paper, we have presented a low-resource so-
lution for automatic diacritization. Our approach is
motivated by empirical observations of the ambigu-
ity and frequency of undiacritized and diacritized
words as well as by the hypothesis that diacrit-
ics from previous words provide useful informa-
tion. The main contributions of our work, based
on these observations, are (1) a hybrid word-level
CMM combined with a consonant-level model for
rare words, (2) a consonant-level model with depen-
dencies on previous diacritized words, (3) new fea-
tures that leverage these dependencies, and (4) an
efficient, approximate decoder for these models. As
expected, the efficacy of our approach varies across
languages, due to differences in the actual ambigu-
ity and frequency of words in these languages. Nev-
ertheless, our models consistently reduce WER by
15% to nearly 50% over the best performing low-
resource models in the literature. In Arabic, our
models approach state-of-the-art despite not using a
morphological analyzer. Arguably, our results have
brought diacritization very close to being useful for
practical application, especially when considering
that we evaluated our method on the most difficult
task in Arabic, which has been reported to have dou-
ble the WER (Zitouni et al., 2006).
The success of this low-resource solution natu-
rally suggests that where more resources are avail-
able (e.g., in Arabic), they could be used to further
reduce error rates. For instance, it may be fruitful to
incorporate a morphological analyzer or segmenta-
tion and part-of-speech tags.
In future work, we would like to consider using
CRFs in place of MEMMs. Also, other approximate
decoders used in pipeline approaches could be ex-
plored as alternatives to the one we used (e.g., Finkel
et al., 2006). Additionally, we wish to include our
model as a stage in a pipeline that segments, dia-
critizes, and labels morphemes. Since obtaining data
for these tasks is substantially more expensive, we
hope to use active learning to obtain more data.
Our framework is applicable for any sequence la-
beling task that can be done at either a word or a
sub-word (e.g., character) level. Segmentation and
lemmatization are particularly promising tasks to
which our approach could be applied.
Finally, for the sake of completeness, we note that
more recent work has been done based on our base-
line models that has emerged since the preparation
of the current work, particularly Zitouni et al. (2009)
and Mohamed et al. (2009). We wish to address any
improvements captured by this more recent work
such as the use of different data sets and addressing
problems with the hamza to decrease error rates.
</bodyText>
<sectionHeader confidence="0.999338" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.989543333333333">
We thank Imed Zitouni, Nizar Habash, Sandra
K¨ubler, and Emad Mohamed for their assistance in
reconstructing datasets, models, and features.
</bodyText>
<page confidence="0.997462">
526
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999800880434783">
S. Ananthakrishnan, S. Narayanan, and S. Bangalore.
2005. Automatic diacritization of Arabic transcripts
for automatic speech recognition. In Proceedings of
the International Conference on Natural Language
Processing.
A. L. Berger, S. Della Pietra, and V. J. Della Pietra. 1996.
A maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22:39–71.
Y. A. El-Imam. 2008. Synthesis of the intonation of neu-
trally spoken Modern Standard Arabic speech. Signal
Processing, 88(9):2206–2221.
T. A. El-Sadany and M. A. Hashish. 1989. An Ara-
bic morphological system. IBM Systems Journal,
28(4):600–612.
J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
Bayesian inference for linguistic annotation pipelines.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 618–
626.
Y. Gal. 2002. An HMM approach to vowel restoration
in Arabic and Hebrew. In Proceedings of the ACL-
02 Workshop on ComputationalApproaches to Semitic
Languages, pages 1–7.
N. Habash and O. Rambow. 2007. Arabic diacritiza-
tion through full morphological tagging. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short Pa-
pers, pages 53–56.
G. Kiraz. 1994. Automatic concordance generation of
Syriac texts. In R. Lavenant, editor, VI Symposium
Syriacum 1992, pages 461–471, Rome, Italy.
G. A. Kiraz. 2000. Multitiered nonlinear morphology
using multitape finite automata: a case study on Syr-
iac and Arabic. Computational Linguistics, 26(1):77–
105.
D. Klein and C. D. Manning. 2002. Conditional structure
versus conditional estimation in NLP models. In Pro-
ceedings of the 2002 Conference on Empirical Meth-
ods in Natural Language Processing, pages 9–16.
S. K¨ubler and E. Mohamed. 2008. Memory-based vocal-
ization of Arabic. In Proceedings of the LREC Work-
shop on HLT and NLP within the Arabic World.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a large-
scale annotated Arabic corpus. In Proceedings of the
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102–109.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19:313–330.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proceedings of the 17th In-
ternational Conference on Machine Learning, pages
591–598.
E. Mohamed and S. K¨ubler. 2009. Diacritization for
real-world Arabic texts. In Proceedings of Recent Ad-
vances in Natural Language Processing 2009.
R. Nelken and S. M. Shieber. 2005. Arabic diacritiza-
tion using weighted finite-state transducers. In Pro-
ceedings of the ACL Workshop on Computational Ap-
proaches to Semitic Languages, pages 79–86.
J. Pearl and G. Shafer. 1988. Probabilistic reasoning
in intelligent systems: networks ofplausible inference.
Morgan Kaufman, San Mateo, CA.
D. Vergyri and K. Kirchhoff. 2004. Automatic diacritiza-
tion of Arabic for acoustic modeling in speech recog-
nition. In Proceedings of the COLING 2004 Workshop
on Computational Approaches to Arabic Script-based
Languages, pages 66–73.
D. Yarowsky. 1999. A comparison of corpus-based tech-
niques for restoring accents in Spanish and French
text. Natural language processing using very large
corpora, pages 99–120.
Zefania XML Project. 2009. Zefania XML bible:
Leningrad codex. http://sourceforge.
net/projects/zefania-sharp/files/
Zefania\%20XML\%20Bibles\%204\
%20hebraica/Leningrad\%20Codex/sf_
wcl.zip/download.
I. Zitouni and R. Sarikaya. 2009. Arabic diacritic
restoration approach based on maximum entropy mod-
els. Computer Speech &amp; Language, 23(3):257–276.
I. Zitouni, J. S. Sorensen, and R. Sarikaya. 2006. Max-
imum entropy based restoration of Arabic diacritics.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 577–584.
</reference>
<page confidence="0.997286">
527
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.755647">
<title confidence="0.999302">Automatic Diacritization for Low-Resource Languages Using a Word and Consonant CMM</title>
<author confidence="0.999789">Robbie A Haertel</author>
<author confidence="0.999789">Peter McClanahan</author>
<author confidence="0.999789">K Eric</author>
<affiliation confidence="0.999275">Department of Computer</affiliation>
<address confidence="0.8941365">Brigham Young Provo, Utah 84602, USA</address>
<email confidence="0.999335">rah67@cs.byu.edu,petermcclanahan@gmail.com,ringger@cs.byu.edu</email>
<abstract confidence="0.996464117647059">We are interested in diacritizing Semitic languages, especially Syriac, using only diacritized texts. Previous methods have required the use of tools such as part-of-speech taggers, segmenters, morphological analyzers, and linguistic rules to produce state-of-the-art results. We present a low-resource, data-driven, and language-independent approach that uses a hybrid wordand consonant-level conditional Markov model. Our approach rivals the best previously published results in Arabic (15% WER with case endings), without the use of a morphological analyzer. In Syriac, we reduce the WER over a strong baseline by 30% to achieve a WER of 10.5%. We also report results for Hebrew and English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Ananthakrishnan</author>
<author>S Narayanan</author>
<author>S Bangalore</author>
</authors>
<title>Automatic diacritization of Arabic transcripts for automatic speech recognition.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="4122" citStr="Ananthakrishnan et al., 2005" startWordPosition="601" endWordPosition="604">e 2010. c�2010 Association for Computational Linguistics pled with the importance of diacritization to other NLP-related tasks. The existing approaches can be categorized based on the amount of resources they require, their basic unit of analysis, and of course the language they are targeting. Probabilistic systems can be further divided into generative and conditional approaches. Existing methodologies can be placed along a continuum based on the quantity of resources they require—a reflection of their cost. Examples of resources used include morphological analyzers (Habash and Rambow, 2007; Ananthakrishnan et al., 2005; Vergyri and Kirchhoff, 2004; El-Sadany and Hashish, 1989), rules for grapheme-to-sound conversion (El-Imam, 2008), transcribed speech (Vergyri and Kirchhoff, 2004), POS tags (Zitouni et al., 2006; Ananthakrishnan et al., 2005), and a list of prefixes and suffixes (Nelken and Shieber, 2005). When such resources exist for a particular language, they typically improve performance. For instance, Habash and Rambow’s (2007) approach reduces the error rate of Zitouni et al.’s (2006) by as much as 30% through its use of a morphological analyzer. In fact, such resources are not always available. Seve</context>
<context position="6371" citStr="Ananthakrishnan et al., 2005" startWordPosition="938" endWordPosition="941">his category (e.g., Habash and Rambow, 1We refer to all graphemes present in undiacritized texts as consonants. 2007; Vergyri and Kirchoff, 2004). Word-based, low-resource solutions tend to treat the problem as word-level sequence labeling (e.g., Gal, 2002). Unfortunately, word-based techniques face problems due to data sparsity: not all words in the test set are seen during training. In contrast, consonant-based approaches rarely face the analogous problem of previously unseen consonants. Thus, one low-resource solution to data sparsity is to use consonant-based techniques for unknown words (Ananthakrishnan et al., 2005; Nelken and Shieber, 2005). Many of the existing systems, especially recent ones, are probabilistic or contain probabilistic components. Zitouni et al. (2006) show the superiority of their conditional-based approaches over the best-performing generative approaches. However, the instance-based learning approach of K¨ubler and Mohamed (2008) slightly outperforms Zitouni et al. (2006). In the published literature for Arabic, the latter two have the best low-resource solutions. Habash and Rambow (2007) is the state-of-the-art, high-resource solution for Arabic. To our knowledge, no work has been </context>
</contexts>
<marker>Ananthakrishnan, Narayanan, Bangalore, 2005</marker>
<rawString>S. Ananthakrishnan, S. Narayanan, and S. Bangalore. 2005. Automatic diacritization of Arabic transcripts for automatic speech recognition. In Proceedings of the International Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--39</pages>
<contexts>
<context position="11639" citStr="Berger et al., 1996" startWordPosition="1806" endWordPosition="1809">h such word. These diacritized words Di depend on previous Di−1 as per equation (1) for an order-1 CMM (note that the capitalized A, I, and O are in fact consonants in this transliteration). Because “NKTA” and “RGT” are rare, their diacritization is represented by a consonant-level CMM: one variable for each possible diacritic in the word. Importantly, these consonant-level models have access to the previously diacritized word (D4 and D6, respectively). We use log-linear models for all local distributions in our CMMs, i.e., we use maximum entropy (maxent) Markov models (McCallum et al., 2000; Berger et al., 1996). Due to the phenomenon known as d-separation (Pearl and Shafer, 1988), it is possible to independently learn parameters for each word model ωui by training only on those instances for the corresponding word. Similarly, the consonant model can be learned independent of the word models. We place a spherical normal prior centered at zero with a standard deviation of 1 over the weights of all models and use an L-BFGS minimizer to find the MAP estimate of the weights for all the models (words and consonant). 521 γ ω eo γ Cs.� ... Cs.� Cs.� Cs.� Cs.� Cs.� CSIA AO ω���� ω ���� ω �� D, D, D3 D4 Cs.� </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22:39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y A El-Imam</author>
</authors>
<title>Synthesis of the intonation of neutrally spoken Modern Standard Arabic speech.</title>
<date>2008</date>
<journal>Signal Processing,</journal>
<volume>88</volume>
<issue>9</issue>
<contexts>
<context position="4237" citStr="El-Imam, 2008" startWordPosition="618" endWordPosition="619">he existing approaches can be categorized based on the amount of resources they require, their basic unit of analysis, and of course the language they are targeting. Probabilistic systems can be further divided into generative and conditional approaches. Existing methodologies can be placed along a continuum based on the quantity of resources they require—a reflection of their cost. Examples of resources used include morphological analyzers (Habash and Rambow, 2007; Ananthakrishnan et al., 2005; Vergyri and Kirchhoff, 2004; El-Sadany and Hashish, 1989), rules for grapheme-to-sound conversion (El-Imam, 2008), transcribed speech (Vergyri and Kirchhoff, 2004), POS tags (Zitouni et al., 2006; Ananthakrishnan et al., 2005), and a list of prefixes and suffixes (Nelken and Shieber, 2005). When such resources exist for a particular language, they typically improve performance. For instance, Habash and Rambow’s (2007) approach reduces the error rate of Zitouni et al.’s (2006) by as much as 30% through its use of a morphological analyzer. In fact, such resources are not always available. Several data-driven approaches exist that require only diacritized texts (e.g., K¨ubler and Mohamed, 2008; Zitouni et a</context>
</contexts>
<marker>El-Imam, 2008</marker>
<rawString>Y. A. El-Imam. 2008. Synthesis of the intonation of neutrally spoken Modern Standard Arabic speech. Signal Processing, 88(9):2206–2221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A El-Sadany</author>
<author>M A Hashish</author>
</authors>
<title>An Arabic morphological system.</title>
<date>1989</date>
<journal>IBM Systems Journal,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="4181" citStr="El-Sadany and Hashish, 1989" startWordPosition="609" endWordPosition="612">ed with the importance of diacritization to other NLP-related tasks. The existing approaches can be categorized based on the amount of resources they require, their basic unit of analysis, and of course the language they are targeting. Probabilistic systems can be further divided into generative and conditional approaches. Existing methodologies can be placed along a continuum based on the quantity of resources they require—a reflection of their cost. Examples of resources used include morphological analyzers (Habash and Rambow, 2007; Ananthakrishnan et al., 2005; Vergyri and Kirchhoff, 2004; El-Sadany and Hashish, 1989), rules for grapheme-to-sound conversion (El-Imam, 2008), transcribed speech (Vergyri and Kirchhoff, 2004), POS tags (Zitouni et al., 2006; Ananthakrishnan et al., 2005), and a list of prefixes and suffixes (Nelken and Shieber, 2005). When such resources exist for a particular language, they typically improve performance. For instance, Habash and Rambow’s (2007) approach reduces the error rate of Zitouni et al.’s (2006) by as much as 30% through its use of a morphological analyzer. In fact, such resources are not always available. Several data-driven approaches exist that require only diacriti</context>
</contexts>
<marker>El-Sadany, Hashish, 1989</marker>
<rawString>T. A. El-Sadany and M. A. Hashish. 1989. An Arabic morphological system. IBM Systems Journal, 28(4):600–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>618--626</pages>
<contexts>
<context position="16401" citStr="Finkel et al., 2006" startWordPosition="2631" endWordPosition="2634">the word) for rare words, making exact decoding intractable. Instead, we apply a non-standard beam during decoding to limit the number of states for rare words to the n-best (locally). This is accomplished by using an independent “inner” n-best decoder for the consonant-level CMM to produce the n-best diacritizations for the rare word given the previous diacritized words and other features. These become the only states to and from which transitions in the “outer” word-level decoder can be made. We note this is the same type of decoding that is done in pipeline models that use n-best decoders (Finkel et al., 2006). Additionally, we use a traditional beamsearch of width 5 to further reduce the search space both in the outer and inner CMMs. 4 Data Although our primary interest is in the Syriac language, we also experimented with the Penn Arabic Treebank (Maamouri et al., 2004) for the sake of comparison with other approaches. We include Hebrew to provide results for yet another Semitic language. We also apply the models to English to show that our method and features work well outside of the Semitic languages. A summary of the datasets, including the number of diacritics, is found in Figure 2. The number</context>
<context position="30125" citStr="Finkel et al., 2006" startWordPosition="4925" endWordPosition="4928">ed our method on the most difficult task in Arabic, which has been reported to have double the WER (Zitouni et al., 2006). The success of this low-resource solution naturally suggests that where more resources are available (e.g., in Arabic), they could be used to further reduce error rates. For instance, it may be fruitful to incorporate a morphological analyzer or segmentation and part-of-speech tags. In future work, we would like to consider using CRFs in place of MEMMs. Also, other approximate decoders used in pipeline approaches could be explored as alternatives to the one we used (e.g., Finkel et al., 2006). Additionally, we wish to include our model as a stage in a pipeline that segments, diacritizes, and labels morphemes. Since obtaining data for these tasks is substantially more expensive, we hope to use active learning to obtain more data. Our framework is applicable for any sequence labeling task that can be done at either a word or a sub-word (e.g., character) level. Segmentation and lemmatization are particularly promising tasks to which our approach could be applied. Finally, for the sake of completeness, we note that more recent work has been done based on our baseline models that has e</context>
</contexts>
<marker>Finkel, Manning, Ng, 2006</marker>
<rawString>J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 618– 626.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gal</author>
</authors>
<title>An HMM approach to vowel restoration in Arabic and Hebrew.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL02 Workshop on ComputationalApproaches to Semitic Languages,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="4857" citStr="Gal, 2002" startWordPosition="715" endWordPosition="716">bed speech (Vergyri and Kirchhoff, 2004), POS tags (Zitouni et al., 2006; Ananthakrishnan et al., 2005), and a list of prefixes and suffixes (Nelken and Shieber, 2005). When such resources exist for a particular language, they typically improve performance. For instance, Habash and Rambow’s (2007) approach reduces the error rate of Zitouni et al.’s (2006) by as much as 30% through its use of a morphological analyzer. In fact, such resources are not always available. Several data-driven approaches exist that require only diacritized texts (e.g., K¨ubler and Mohamed, 2008; Zitouni et al., 2006; Gal, 2002) which are relatively inexpensive to obtain: most literate speakers of the target language could readily provide them. Apart from the quantity of resources required, diacritization systems also differ in their basic unit of analysis. A consonant-based approach treats each consonant1 in a word as a potential host for one or more (possibly null) diacritics; the goal is to predict the correct diacritic(s) for each consonant (e.g., K¨ubler and Mohamed, 2008). Zitouni et al. (2006) extend the problem to a sequence labeling task wherein they seek the best sequence of diacritics for the consonants. C</context>
</contexts>
<marker>Gal, 2002</marker>
<rawString>Y. Gal. 2002. An HMM approach to vowel restoration in Arabic and Hebrew. In Proceedings of the ACL02 Workshop on ComputationalApproaches to Semitic Languages, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>O Rambow</author>
</authors>
<title>Arabic diacritization through full morphological tagging.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers,</booktitle>
<pages>53--56</pages>
<contexts>
<context position="4092" citStr="Habash and Rambow, 2007" startWordPosition="597" endWordPosition="600"> Angeles, California, June 2010. c�2010 Association for Computational Linguistics pled with the importance of diacritization to other NLP-related tasks. The existing approaches can be categorized based on the amount of resources they require, their basic unit of analysis, and of course the language they are targeting. Probabilistic systems can be further divided into generative and conditional approaches. Existing methodologies can be placed along a continuum based on the quantity of resources they require—a reflection of their cost. Examples of resources used include morphological analyzers (Habash and Rambow, 2007; Ananthakrishnan et al., 2005; Vergyri and Kirchhoff, 2004; El-Sadany and Hashish, 1989), rules for grapheme-to-sound conversion (El-Imam, 2008), transcribed speech (Vergyri and Kirchhoff, 2004), POS tags (Zitouni et al., 2006; Ananthakrishnan et al., 2005), and a list of prefixes and suffixes (Nelken and Shieber, 2005). When such resources exist for a particular language, they typically improve performance. For instance, Habash and Rambow’s (2007) approach reduces the error rate of Zitouni et al.’s (2006) by as much as 30% through its use of a morphological analyzer. In fact, such resources </context>
<context position="6875" citStr="Habash and Rambow (2007)" startWordPosition="1008" endWordPosition="1011">-resource solution to data sparsity is to use consonant-based techniques for unknown words (Ananthakrishnan et al., 2005; Nelken and Shieber, 2005). Many of the existing systems, especially recent ones, are probabilistic or contain probabilistic components. Zitouni et al. (2006) show the superiority of their conditional-based approaches over the best-performing generative approaches. However, the instance-based learning approach of K¨ubler and Mohamed (2008) slightly outperforms Zitouni et al. (2006). In the published literature for Arabic, the latter two have the best low-resource solutions. Habash and Rambow (2007) is the state-of-the-art, high-resource solution for Arabic. To our knowledge, no work has been done in this area for Syriac. 3 Models In this work, we are concerned with diacritization for Syriac for which a POS tagger, segmenter, and other tools are not readily available, but for which diacritized text is obtainable.2 Use of a system dependent on a morphological analyzer such as Habash and Rambow’s (2007) is therefore not costeffective. Furthermore, we seek a system that is applicable to a wide variety of languages. Although K¨ubler and Mohamed’s (2008) approach is competitive to Zitouni et </context>
<context position="21079" citStr="Habash and Rambow (2007)" startWordPosition="3419" endWordPosition="3422">outperforms the latter using the same diacritics and on the same dataset (see Figure 4), thereby providing the strongest published baseline for Arabic on a common dataset. We denote this model as k¨ubler and use it as a strong baseline for all datasets. For the Arabic results, we additionally include Zitouni et al.’s (2006) lexical model (zitouni-lex) and their model that uses a segmenter and POS tagger (zitouni-all), which are not immediately available to us for Syriac. For yet another point of reference for Arabic, we provide the results from the state-of-the-art (resource-rich) approach of Habash and Rambow (2007) (habash). This model is at an extreme advantage, having access to a full morphological analyzer. Note that for these three models we simply report their published results and do not attempt to reproduce them. Since k¨ubler is of a different model class than ours, we consider an additional baseline that is a consonant-level CMM with access to the same information, namely, only those consonants within a window of 5 to either side (ccmm). This is equivalent to a special case of our hybrid model wherein both the word-level and the consonant-level Markov order are 0. The features that we extract f</context>
<context position="27409" citStr="Habash and Rambow (2007)" startWordPosition="4492" endWordPosition="4495">17.85 hybrid 5.39 2.38 57.24 16.51 Syriac Arabic Hebrew English 525 The cons-only model significantly outperforms zitouni-all despite the additional resources to which the latter has access. This is evidence supporting our hypothesis that the diacritics from previous words in fact contain useful information for prediction. This empirically suggests that the independence assumptions in consonant-only models are too strict. Perhaps even more importantly, our low-resource method approaches the performance of habash. We note that the differences may not be statistically significant, and also that Habash and Rambow (2007) omit instances in the data that lack solutions. In fact, cons-only has a lower WER than all but two of the seven techniques used by Habash and Rambow (2007), which use a morphological analyzer. Interestingly, hybrid does worse than cons-only on this dataset, although it is still competitive with zitouni-all. We hypothesize that the observations from Section 3 do not hold as strongly for this dataset. For this reason, using a smooth interpolation function (rather than the abrupt one we employ) may be advantageous and is an interesting avenue for future research. One last observation is that th</context>
</contexts>
<marker>Habash, Rambow, 2007</marker>
<rawString>N. Habash and O. Rambow. 2007. Arabic diacritization through full morphological tagging. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, pages 53–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kiraz</author>
</authors>
<title>Automatic concordance generation of Syriac texts.</title>
<date>1994</date>
<booktitle>VI Symposium Syriacum 1992,</booktitle>
<pages>461--471</pages>
<editor>In R. Lavenant, editor,</editor>
<location>Rome, Italy.</location>
<contexts>
<context position="17256" citStr="Kiraz, 1994" startWordPosition="2781" endWordPosition="2782">(Maamouri et al., 2004) for the sake of comparison with other approaches. We include Hebrew to provide results for yet another Semitic language. We also apply the models to English to show that our method and features work well outside of the Semitic languages. A summary of the datasets, including the number of diacritics, is found in Figure 2. The number of diacritics shown in the table is less than the number of possible predictions since we treat contiguous diacritics between consonants as a single prediction. For our experiments in Syriac, we use the New Testament portion of the Peshitta (Kiraz, 1994) and lang diacs train dev test Syriac 9 87,874 10,747 11,021 Arabic 8 246,512 42,105 51,664 Hebrew 17 239,615 42,133 49,455 English 5 1,004,073 80,156 89,537 Figure 2: Number of diacritics and size (in tokens) of each dataset treat each verse as if it were a sentence. The diacritics we predict are the five short vowels, as well as S˚ey¯am¯e, Rukk¯akh¯a, Quˇsˇs¯ay¯a, and linea ocultans. For Arabic, we use the training/test split defined by Zitouni et al. (2006). We group all words having the same P index value into a sentence. We build our own development set by removing the last 15% of the sen</context>
</contexts>
<marker>Kiraz, 1994</marker>
<rawString>G. Kiraz. 1994. Automatic concordance generation of Syriac texts. In R. Lavenant, editor, VI Symposium Syriacum 1992, pages 461–471, Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Kiraz</author>
</authors>
<title>Multitiered nonlinear morphology using multitape finite automata: a case study on Syriac and Arabic.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<pages>105</pages>
<contexts>
<context position="7951" citStr="Kiraz (2000)" startWordPosition="1187" endWordPosition="1188">a system that is applicable to a wide variety of languages. Although K¨ubler and Mohamed’s (2008) approach is competitive to Zitouni et al.’s (2006), instance-based approaches tend to suffer with the addition of new features (their own experiments demonstrate this). We desire to add linguistically relevant features to improve performance and thus choose to use a conditional model. However, unlike Zitouni et al. (2006), we use a hybrid word- and consonant-level approach based on the following observations (statistics taken from the Syriac training and development sets explained in Section 4): 2Kiraz (2000) describes a morphological analyzer for Syriac that is not publicly available and is costly to reproduce. 520 1. Many undiacritized words are unambiguous: in which the local conditional distribution of a di90.8% of the word types and 63.5% of the to- acritized word is an interpolation of a word-level kens have a single diacritized form. model (ωui) and a consonant-level model (γ): 2. Most undiacritized word types have only a few possible diacritizations: the average number of possible diacritizations is 1.11. 3. Low-frequency words have low ambiguity: Undiacritized types occurring fewer than 5</context>
</contexts>
<marker>Kiraz, 2000</marker>
<rawString>G. A. Kiraz. 2000. Multitiered nonlinear morphology using multitape finite automata: a case study on Syriac and Arabic. Computational Linguistics, 26(1):77– 105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Conditional structure versus conditional estimation in NLP models.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2061" citStr="Klein and Manning, 2002" startWordPosition="287" endWordPosition="290">, morphological disambiguation, and machine translation, making diacritization important to Natural Language Processing (NLP) systems and intelligence gathering. In alphabetic writing systems, similar techniques have been used to restore accents from plain text (Yarowsky, 1999) and could be used to recover missing letters in the compressed writing styles found in email, text, and instant messages. We are particularly interested in diacritizing Syriac, a low-resource dialect of Aramaic, which possesses properties similar to Arabic and Hebrew. This work employs conditional Markov models (CMMs) (Klein and Manning, 2002) to diacritize Semitic (and other) languages and requires only diacritized texts for training. Such an approach is useful for languages (like Syriac) in which annotated data and linguistic tools such as part-of-speech (POS) taggers, segmenters, and morphological analyzers are not available. Our main contributions are as follows: (1) we introduce a hybrid word and consonant CMM that allows access to the diacritized form of the previous words; (2) we introduce new features available in the proposed model; and (3) we describe an efficient, approximate decoder. Our models significantly outperform </context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>D. Klein and C. D. Manning. 2002. Conditional structure versus conditional estimation in NLP models. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K¨ubler</author>
<author>E Mohamed</author>
</authors>
<title>Memory-based vocalization of Arabic.</title>
<date>2008</date>
<booktitle>In Proceedings of the LREC Workshop on HLT and NLP within the Arabic World.</booktitle>
<marker>K¨ubler, Mohamed, 2008</marker>
<rawString>S. K¨ubler and E. Mohamed. 2008. Memory-based vocalization of Arabic. In Proceedings of the LREC Workshop on HLT and NLP within the Arabic World.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
<author>A Bies</author>
<author>T Buckwalter</author>
<author>W Mekki</author>
</authors>
<title>The Penn Arabic Treebank: Building a largescale annotated Arabic corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the NEMLAR Conference on Arabic Language Resources and Tools,</booktitle>
<pages>102--109</pages>
<contexts>
<context position="16667" citStr="Maamouri et al., 2004" startWordPosition="2678" endWordPosition="2681">sonant-level CMM to produce the n-best diacritizations for the rare word given the previous diacritized words and other features. These become the only states to and from which transitions in the “outer” word-level decoder can be made. We note this is the same type of decoding that is done in pipeline models that use n-best decoders (Finkel et al., 2006). Additionally, we use a traditional beamsearch of width 5 to further reduce the search space both in the outer and inner CMMs. 4 Data Although our primary interest is in the Syriac language, we also experimented with the Penn Arabic Treebank (Maamouri et al., 2004) for the sake of comparison with other approaches. We include Hebrew to provide results for yet another Semitic language. We also apply the models to English to show that our method and features work well outside of the Semitic languages. A summary of the datasets, including the number of diacritics, is found in Figure 2. The number of diacritics shown in the table is less than the number of possible predictions since we treat contiguous diacritics between consonants as a single prediction. For our experiments in Syriac, we use the New Testament portion of the Peshitta (Kiraz, 1994) and lang d</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, Mekki, 2004</marker>
<rawString>M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki. 2004. The Penn Arabic Treebank: Building a largescale annotated Arabic corpus. In Proceedings of the NEMLAR Conference on Arabic Language Resources and Tools, pages 102–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="19062" citStr="Marcus et al., 1994" startWordPosition="3084" endWordPosition="3087"> we treat each verse as a sentence and remove the paragraph markers (pe and samekh). There is a large number of diacritics that could be predicted in Hebrew and no apparent standardization in the literature. For these reasons, we attempt to predict as many diacritics as possible. Specifically, we predict the diacritics whose unicode values are 05B0-B9, 05BBBD, 05BF, 05C1-C2, and 05C4. We treat the following list of punctuation as consonants: maqaf, paseq, sof pasuq, geresh, and gershayim. The cantillation marks are removed entirely from the data. Our English data comes from the Penn Treebank (Marcus et al., 1994). We used sections 0–20 as training data, 21–22 as development data, and 23– 24 as our test set. Unlike words in the Semitic languages, English words can begin with a vowel, requiring us to prepend a prosthetic consonant to every word; we also convert all English text to lowercase. 523 5 Experiments For all feature engineering and tuning, we trained and tested on training and development test sets, respectively (as specified above). Final results are reported by folding the development test set into the training data and evaluating on the blind test set. We retain only those features that occu</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th International Conference on Machine Learning,</booktitle>
<pages>591--598</pages>
<contexts>
<context position="11617" citStr="McCallum et al., 2000" startWordPosition="1802" endWordPosition="1805">ndom variable D for each such word. These diacritized words Di depend on previous Di−1 as per equation (1) for an order-1 CMM (note that the capitalized A, I, and O are in fact consonants in this transliteration). Because “NKTA” and “RGT” are rare, their diacritization is represented by a consonant-level CMM: one variable for each possible diacritic in the word. Importantly, these consonant-level models have access to the previously diacritized word (D4 and D6, respectively). We use log-linear models for all local distributions in our CMMs, i.e., we use maximum entropy (maxent) Markov models (McCallum et al., 2000; Berger et al., 1996). Due to the phenomenon known as d-separation (Pearl and Shafer, 1988), it is possible to independently learn parameters for each word model ωui by training only on those instances for the corresponding word. Similarly, the consonant model can be learned independent of the word models. We place a spherical normal prior centered at zero with a standard deviation of 1 over the weights of all models and use an L-BFGS minimizer to find the MAP estimate of the weights for all the models (words and consonant). 521 γ ω eo γ Cs.� ... Cs.� Cs.� Cs.� Cs.� Cs.� CSIA AO ω���� ω ���� </context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Proceedings of the 17th International Conference on Machine Learning, pages 591–598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mohamed</author>
<author>S K¨ubler</author>
</authors>
<title>Diacritization for real-world Arabic texts.</title>
<date>2009</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing</booktitle>
<marker>Mohamed, K¨ubler, 2009</marker>
<rawString>E. Mohamed and S. K¨ubler. 2009. Diacritization for real-world Arabic texts. In Proceedings of Recent Advances in Natural Language Processing 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nelken</author>
<author>S M Shieber</author>
</authors>
<title>Arabic diacritization using weighted finite-state transducers.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="4414" citStr="Nelken and Shieber, 2005" startWordPosition="644" endWordPosition="647">Probabilistic systems can be further divided into generative and conditional approaches. Existing methodologies can be placed along a continuum based on the quantity of resources they require—a reflection of their cost. Examples of resources used include morphological analyzers (Habash and Rambow, 2007; Ananthakrishnan et al., 2005; Vergyri and Kirchhoff, 2004; El-Sadany and Hashish, 1989), rules for grapheme-to-sound conversion (El-Imam, 2008), transcribed speech (Vergyri and Kirchhoff, 2004), POS tags (Zitouni et al., 2006; Ananthakrishnan et al., 2005), and a list of prefixes and suffixes (Nelken and Shieber, 2005). When such resources exist for a particular language, they typically improve performance. For instance, Habash and Rambow’s (2007) approach reduces the error rate of Zitouni et al.’s (2006) by as much as 30% through its use of a morphological analyzer. In fact, such resources are not always available. Several data-driven approaches exist that require only diacritized texts (e.g., K¨ubler and Mohamed, 2008; Zitouni et al., 2006; Gal, 2002) which are relatively inexpensive to obtain: most literate speakers of the target language could readily provide them. Apart from the quantity of resources r</context>
<context position="6398" citStr="Nelken and Shieber, 2005" startWordPosition="942" endWordPosition="945"> Rambow, 1We refer to all graphemes present in undiacritized texts as consonants. 2007; Vergyri and Kirchoff, 2004). Word-based, low-resource solutions tend to treat the problem as word-level sequence labeling (e.g., Gal, 2002). Unfortunately, word-based techniques face problems due to data sparsity: not all words in the test set are seen during training. In contrast, consonant-based approaches rarely face the analogous problem of previously unseen consonants. Thus, one low-resource solution to data sparsity is to use consonant-based techniques for unknown words (Ananthakrishnan et al., 2005; Nelken and Shieber, 2005). Many of the existing systems, especially recent ones, are probabilistic or contain probabilistic components. Zitouni et al. (2006) show the superiority of their conditional-based approaches over the best-performing generative approaches. However, the instance-based learning approach of K¨ubler and Mohamed (2008) slightly outperforms Zitouni et al. (2006). In the published literature for Arabic, the latter two have the best low-resource solutions. Habash and Rambow (2007) is the state-of-the-art, high-resource solution for Arabic. To our knowledge, no work has been done in this area for Syria</context>
</contexts>
<marker>Nelken, Shieber, 2005</marker>
<rawString>R. Nelken and S. M. Shieber. 2005. Arabic diacritization using weighted finite-state transducers. In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pearl</author>
<author>G Shafer</author>
</authors>
<title>Probabilistic reasoning in intelligent systems: networks ofplausible inference.</title>
<date>1988</date>
<publisher>Morgan</publisher>
<location>Kaufman, San Mateo, CA.</location>
<contexts>
<context position="11709" citStr="Pearl and Shafer, 1988" startWordPosition="1817" endWordPosition="1820">per equation (1) for an order-1 CMM (note that the capitalized A, I, and O are in fact consonants in this transliteration). Because “NKTA” and “RGT” are rare, their diacritization is represented by a consonant-level CMM: one variable for each possible diacritic in the word. Importantly, these consonant-level models have access to the previously diacritized word (D4 and D6, respectively). We use log-linear models for all local distributions in our CMMs, i.e., we use maximum entropy (maxent) Markov models (McCallum et al., 2000; Berger et al., 1996). Due to the phenomenon known as d-separation (Pearl and Shafer, 1988), it is possible to independently learn parameters for each word model ωui by training only on those instances for the corresponding word. Similarly, the consonant model can be learned independent of the word models. We place a spherical normal prior centered at zero with a standard deviation of 1 over the weights of all models and use an L-BFGS minimizer to find the MAP estimate of the weights for all the models (words and consonant). 521 γ ω eo γ Cs.� ... Cs.� Cs.� Cs.� Cs.� Cs.� CSIA AO ω���� ω ���� ω �� D, D, D3 D4 Cs.� Cx qs C 4 D, C7.1 C7. C73 CSIA AO DHRA AO NKTA LA RGT (a) (b) Figure 1</context>
</contexts>
<marker>Pearl, Shafer, 1988</marker>
<rawString>J. Pearl and G. Shafer. 1988. Probabilistic reasoning in intelligent systems: networks ofplausible inference. Morgan Kaufman, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vergyri</author>
<author>K Kirchhoff</author>
</authors>
<title>Automatic diacritization of Arabic for acoustic modeling in speech recognition.</title>
<date>2004</date>
<booktitle>In Proceedings of the COLING 2004 Workshop on Computational Approaches to Arabic Script-based Languages,</booktitle>
<pages>66--73</pages>
<contexts>
<context position="4151" citStr="Vergyri and Kirchhoff, 2004" startWordPosition="605" endWordPosition="608"> Computational Linguistics pled with the importance of diacritization to other NLP-related tasks. The existing approaches can be categorized based on the amount of resources they require, their basic unit of analysis, and of course the language they are targeting. Probabilistic systems can be further divided into generative and conditional approaches. Existing methodologies can be placed along a continuum based on the quantity of resources they require—a reflection of their cost. Examples of resources used include morphological analyzers (Habash and Rambow, 2007; Ananthakrishnan et al., 2005; Vergyri and Kirchhoff, 2004; El-Sadany and Hashish, 1989), rules for grapheme-to-sound conversion (El-Imam, 2008), transcribed speech (Vergyri and Kirchhoff, 2004), POS tags (Zitouni et al., 2006; Ananthakrishnan et al., 2005), and a list of prefixes and suffixes (Nelken and Shieber, 2005). When such resources exist for a particular language, they typically improve performance. For instance, Habash and Rambow’s (2007) approach reduces the error rate of Zitouni et al.’s (2006) by as much as 30% through its use of a morphological analyzer. In fact, such resources are not always available. Several data-driven approaches ex</context>
</contexts>
<marker>Vergyri, Kirchhoff, 2004</marker>
<rawString>D. Vergyri and K. Kirchhoff. 2004. Automatic diacritization of Arabic for acoustic modeling in speech recognition. In Proceedings of the COLING 2004 Workshop on Computational Approaches to Arabic Script-based Languages, pages 66–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>A comparison of corpus-based techniques for restoring accents in Spanish and French text. Natural language processing using very large corpora,</title>
<date>1999</date>
<pages>99--120</pages>
<contexts>
<context position="1715" citStr="Yarowsky, 1999" startWordPosition="236" endWordPosition="237">ritics is useful for personal, industrial, and governmental purposes—especially for Semitic languages. In its own right, the ability to diacritize can aid language learning and is necessary for speech-based assistive technologies, including speech recognition and text-to-speech. Diacritics are also useful for tasks such as segmentation, morphological disambiguation, and machine translation, making diacritization important to Natural Language Processing (NLP) systems and intelligence gathering. In alphabetic writing systems, similar techniques have been used to restore accents from plain text (Yarowsky, 1999) and could be used to recover missing letters in the compressed writing styles found in email, text, and instant messages. We are particularly interested in diacritizing Syriac, a low-resource dialect of Aramaic, which possesses properties similar to Arabic and Hebrew. This work employs conditional Markov models (CMMs) (Klein and Manning, 2002) to diacritize Semitic (and other) languages and requires only diacritized texts for training. Such an approach is useful for languages (like Syriac) in which annotated data and linguistic tools such as part-of-speech (POS) taggers, segmenters, and morph</context>
</contexts>
<marker>Yarowsky, 1999</marker>
<rawString>D. Yarowsky. 1999. A comparison of corpus-based techniques for restoring accents in Spanish and French text. Natural language processing using very large corpora, pages 99–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zefania XML Project</author>
</authors>
<date>2009</date>
<booktitle>Zefania XML bible: Leningrad codex. http://sourceforge. net/projects/zefania-sharp/files/ Zefania\%20XML\%20Bibles\%204\ %20hebraica/Leningrad\%20Codex/sf_ wcl.zip/download.</booktitle>
<contexts>
<context position="18425" citStr="Project, 2009" startWordPosition="2980" endWordPosition="2981">pment set by removing the last 15% of the sentences of the training set. Like Zitouni, when no solution exists in the treebank, we take the first solution as the gold tag. Zitouni et al. (2006) report results on several different conditions, but we focus on the most challenging of the conditions: we predict the standard three short vowels, three tanween, sukuun, shadda, and all case endings. (Preliminary experiments show that our models perform equally favorably in the other scenarios as well.) For Hebrew, we use the Hebrew Bible (Old Testament) in the Westminster Leningrad Codex (Zefania XML Project, 2009). As with Syriac, we treat each verse as a sentence and remove the paragraph markers (pe and samekh). There is a large number of diacritics that could be predicted in Hebrew and no apparent standardization in the literature. For these reasons, we attempt to predict as many diacritics as possible. Specifically, we predict the diacritics whose unicode values are 05B0-B9, 05BBBD, 05BF, 05C1-C2, and 05C4. We treat the following list of punctuation as consonants: maqaf, paseq, sof pasuq, geresh, and gershayim. The cantillation marks are removed entirely from the data. Our English data comes from th</context>
</contexts>
<marker>Project, 2009</marker>
<rawString>Zefania XML Project. 2009. Zefania XML bible: Leningrad codex. http://sourceforge. net/projects/zefania-sharp/files/ Zefania\%20XML\%20Bibles\%204\ %20hebraica/Leningrad\%20Codex/sf_ wcl.zip/download.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Zitouni</author>
<author>R Sarikaya</author>
</authors>
<title>Arabic diacritic restoration approach based on maximum entropy models.</title>
<date>2009</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>23</volume>
<issue>3</issue>
<marker>Zitouni, Sarikaya, 2009</marker>
<rawString>I. Zitouni and R. Sarikaya. 2009. Arabic diacritic restoration approach based on maximum entropy models. Computer Speech &amp; Language, 23(3):257–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Zitouni</author>
<author>J S Sorensen</author>
<author>R Sarikaya</author>
</authors>
<title>Maximum entropy based restoration of Arabic diacritics.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>577--584</pages>
<contexts>
<context position="4319" citStr="Zitouni et al., 2006" startWordPosition="629" endWordPosition="632">hey require, their basic unit of analysis, and of course the language they are targeting. Probabilistic systems can be further divided into generative and conditional approaches. Existing methodologies can be placed along a continuum based on the quantity of resources they require—a reflection of their cost. Examples of resources used include morphological analyzers (Habash and Rambow, 2007; Ananthakrishnan et al., 2005; Vergyri and Kirchhoff, 2004; El-Sadany and Hashish, 1989), rules for grapheme-to-sound conversion (El-Imam, 2008), transcribed speech (Vergyri and Kirchhoff, 2004), POS tags (Zitouni et al., 2006; Ananthakrishnan et al., 2005), and a list of prefixes and suffixes (Nelken and Shieber, 2005). When such resources exist for a particular language, they typically improve performance. For instance, Habash and Rambow’s (2007) approach reduces the error rate of Zitouni et al.’s (2006) by as much as 30% through its use of a morphological analyzer. In fact, such resources are not always available. Several data-driven approaches exist that require only diacritized texts (e.g., K¨ubler and Mohamed, 2008; Zitouni et al., 2006; Gal, 2002) which are relatively inexpensive to obtain: most literate spe</context>
<context position="6530" citStr="Zitouni et al. (2006)" startWordPosition="961" endWordPosition="964">ource solutions tend to treat the problem as word-level sequence labeling (e.g., Gal, 2002). Unfortunately, word-based techniques face problems due to data sparsity: not all words in the test set are seen during training. In contrast, consonant-based approaches rarely face the analogous problem of previously unseen consonants. Thus, one low-resource solution to data sparsity is to use consonant-based techniques for unknown words (Ananthakrishnan et al., 2005; Nelken and Shieber, 2005). Many of the existing systems, especially recent ones, are probabilistic or contain probabilistic components. Zitouni et al. (2006) show the superiority of their conditional-based approaches over the best-performing generative approaches. However, the instance-based learning approach of K¨ubler and Mohamed (2008) slightly outperforms Zitouni et al. (2006). In the published literature for Arabic, the latter two have the best low-resource solutions. Habash and Rambow (2007) is the state-of-the-art, high-resource solution for Arabic. To our knowledge, no work has been done in this area for Syriac. 3 Models In this work, we are concerned with diacritization for Syriac for which a POS tagger, segmenter, and other tools are not</context>
<context position="7760" citStr="Zitouni et al. (2006)" startWordPosition="1156" endWordPosition="1159">available, but for which diacritized text is obtainable.2 Use of a system dependent on a morphological analyzer such as Habash and Rambow’s (2007) is therefore not costeffective. Furthermore, we seek a system that is applicable to a wide variety of languages. Although K¨ubler and Mohamed’s (2008) approach is competitive to Zitouni et al.’s (2006), instance-based approaches tend to suffer with the addition of new features (their own experiments demonstrate this). We desire to add linguistically relevant features to improve performance and thus choose to use a conditional model. However, unlike Zitouni et al. (2006), we use a hybrid word- and consonant-level approach based on the following observations (statistics taken from the Syriac training and development sets explained in Section 4): 2Kiraz (2000) describes a morphological analyzer for Syriac that is not publicly available and is costly to reproduce. 520 1. Many undiacritized words are unambiguous: in which the local conditional distribution of a di90.8% of the word types and 63.5% of the to- acritized word is an interpolation of a word-level kens have a single diacritized form. model (ωui) and a consonant-level model (γ): 2. Most undiacritized wor</context>
<context position="10296" citStr="Zitouni et al. (2006)" startWordPosition="1570" endWordPosition="1573">latter two problems. Clearly a hybrid approach would be advantageous. To this end we employ a CMM in which we treat the problem as an instance of sequence labeling at the word level with less common words being handled by a consonant-level CMM. Let u be the undiacriatized words in a sentence. Applying an order o Markov assumption, the distribution over sequences of diacritized words d is: P (d|u) _ � kdk P (di|di−o...i−1, u; ω, γ, α) (1) i=1 P (di|di−o...i−1, u; ω,γ, α) _ αP (di|di−o...i−1, u; ωui) + (1 − α)P (di|di−o...i−1, u; γ) We let the consonant-level model be a standard CMM, similar to Zitouni et al. (2006), but with access to previously diacritized words. Note that the order of this “inner” CMM need not be the same as that of the outer CMM. The parameter α reflects the degree to which we trust the word-level model. In the most general case, α can be a function of the undiacritized words and the previous o diacritized words. Based on our earlier enumerated observations, we use a simple delta function for α: we let α be 0 when ui is rare and 1 otherwise. We leave discussion for what constitutes a “rare” undiacritized type for Section 5.2. Figure 1b presents a graphical model of a simple example s</context>
<context position="13127" citStr="Zitouni et al. (2006)" startWordPosition="2078" endWordPosition="2081">ntlevel-only model and (b) a combined word- and consonant-level model. For clarity, both models assume a consonant-level Markov order of 1; (b) shows a word-level Markov order of 1. For simplicity, the figure further assumes that additional features come only from the current (undiacritized) word. Note that Zitouni et al.’s (2006) model is a special case of equation (1) where all words are rare, the word-level Markov order (o) is 0, and the consonantlevel Markov order is 2. A simplified version of Zitouni’s model is presented in Figure 1a. 3.1 Features Our features are based on those found in Zitouni et al. (2006), although we have added a few of our own which we consider to be one of the contributions of this paper. Unlike their work, our consonant-level model has access to previously diacritized words, allowing us to exploit information noted in observation 5. Each of the word-level models shares the same set of features, defined by the following templates: • The prefixes and suffixes (up to 4 characters) of the previously diacritized words. • The string of the actual diacritics, including the null diacritic, from each of the previous o diacritized words and n-grams of these strings; a similar set of</context>
<context position="17720" citStr="Zitouni et al. (2006)" startWordPosition="2858" endWordPosition="2861">ontiguous diacritics between consonants as a single prediction. For our experiments in Syriac, we use the New Testament portion of the Peshitta (Kiraz, 1994) and lang diacs train dev test Syriac 9 87,874 10,747 11,021 Arabic 8 246,512 42,105 51,664 Hebrew 17 239,615 42,133 49,455 English 5 1,004,073 80,156 89,537 Figure 2: Number of diacritics and size (in tokens) of each dataset treat each verse as if it were a sentence. The diacritics we predict are the five short vowels, as well as S˚ey¯am¯e, Rukk¯akh¯a, Quˇsˇs¯ay¯a, and linea ocultans. For Arabic, we use the training/test split defined by Zitouni et al. (2006). We group all words having the same P index value into a sentence. We build our own development set by removing the last 15% of the sentences of the training set. Like Zitouni, when no solution exists in the treebank, we take the first solution as the gold tag. Zitouni et al. (2006) report results on several different conditions, but we focus on the most challenging of the conditions: we predict the standard three short vowels, three tanween, sukuun, shadda, and all case endings. (Preliminary experiments show that our models perform equally favorably in the other scenarios as well.) For Hebre</context>
<context position="20396" citStr="Zitouni et al. (2006)" startWordPosition="3309" endWordPosition="3312"> incorrectly diacritized), along with the Diacritic Error Rate (DER) (i.e., the percentage of diacritics, including the null diacritic, that were incorrectly predicted). We also report both WER and DER for only those words that were not seen during training (UWER and UDER, respectively). We found that precision, recall, and f-score were nearly perfectly correlated with DER; hence, we omit this information for brevity. 5.1 Models for Evaluation In previous work, K¨ubler et al. (2008) report the lowest error rates of the low-resource models. Although their results are not directly comparable to Zitouni et al. (2006), we have independently confirmed that the former slightly outperforms the latter using the same diacritics and on the same dataset (see Figure 4), thereby providing the strongest published baseline for Arabic on a common dataset. We denote this model as k¨ubler and use it as a strong baseline for all datasets. For the Arabic results, we additionally include Zitouni et al.’s (2006) lexical model (zitouni-lex) and their model that uses a segmenter and POS tagger (zitouni-all), which are not immediately available to us for Syriac. For yet another point of reference for Arabic, we provide the res</context>
<context position="26049" citStr="Zitouni et al. (2006)" startWordPosition="4276" endWordPosition="4279">er and ccmm), it appears that performance is comparable across all languages, with the maxent approach enjoying a slight advantage except in English. Interestingly, the maxent solution usually handles unknown words better, although it does not specifically target this case. Both models outperform zitouni-lex in Arabic, despite the fact that they use a much simpler feature set, most notably, the lack of previous diacritics. In the case of ccmm this may be attributable in part to our use of an L-BFGS optimizer, convergence criteria, feature selection, or other potential differences not noted in Zitouni et al. (2006). We note that the maxent-based approaches are much more time and memory intensive. Using the Arabic data, we are able to compare our methods to several other published results. Approach WER DER UWER UDER k¨ubler 15.04 5.23 64.65 18.21 ccmm 13.99 4.82 54.54 15.18 cons-only 12.31 5.03 55.68 19.09 hybrid 10.54 4.29 55.16 18.86 zitouni-lex 25.1 8.2 NA NA k¨ubler 23.61 7.25 66.69 20.51 ccmm 22.63 6.61 57.71 16.10 cons-only 15.02 5.15 48.10 15.76 hybrid 17.87 5.67 47.85 15.63 zitouni-all 18.0 5.5 NA NA habash 14.9 4.8 NA NA k¨ubler 30.60 12.96 89.52 36.86 ccmm 29.67 12.05 80.02 29.39 cons-only 23.3</context>
<context position="29626" citStr="Zitouni et al., 2006" startWordPosition="4842" endWordPosition="4845"> of our approach varies across languages, due to differences in the actual ambiguity and frequency of words in these languages. Nevertheless, our models consistently reduce WER by 15% to nearly 50% over the best performing lowresource models in the literature. In Arabic, our models approach state-of-the-art despite not using a morphological analyzer. Arguably, our results have brought diacritization very close to being useful for practical application, especially when considering that we evaluated our method on the most difficult task in Arabic, which has been reported to have double the WER (Zitouni et al., 2006). The success of this low-resource solution naturally suggests that where more resources are available (e.g., in Arabic), they could be used to further reduce error rates. For instance, it may be fruitful to incorporate a morphological analyzer or segmentation and part-of-speech tags. In future work, we would like to consider using CRFs in place of MEMMs. Also, other approximate decoders used in pipeline approaches could be explored as alternatives to the one we used (e.g., Finkel et al., 2006). Additionally, we wish to include our model as a stage in a pipeline that segments, diacritizes, and</context>
</contexts>
<marker>Zitouni, Sorensen, Sarikaya, 2006</marker>
<rawString>I. Zitouni, J. S. Sorensen, and R. Sarikaya. 2006. Maximum entropy based restoration of Arabic diacritics. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 577–584.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>