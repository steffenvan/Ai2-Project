<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030316">
<title confidence="0.995422">
Towards Robust Semantic Role Labeling
</title>
<author confidence="0.99828">
Sameer Pradhan Wayne Ward, James H. Martin
</author>
<affiliation confidence="0.992449">
BBN Technologies University of Colorado
</affiliation>
<address confidence="0.897039">
Cambridge, MA 02138 Boulder, CO 80303
</address>
<email confidence="0.997269">
pradhan@bbn.com {whw,martin}@colorado.edu
</email>
<sectionHeader confidence="0.941821" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999790448275862">
Most research on semantic role labeling
(SRL) has been focused on training and
evaluating on the same corpus in order
to develop the technology. This strategy,
while appropriate for initiating research,
can lead to over-training to the particular
corpus. The work presented in this pa-
per focuses on analyzing the robustness
of an SRL system when trained on one
genre of data and used to label a different
genre. Our state-of-the-art semantic role
labeling system, while performing well on
WSJ test data, shows significant perfor-
mance degradation when applied to data
from the Brown corpus. We present a se-
ries of experiments designed to investigate
the source of this lack of portability. These
experiments are based on comparisons of
performance using PropBanked WSJ data
and PropBanked Brown corpus data. Our
results indicate that while syntactic parses
and argument identification port relatively
well to a new genre, argument classifica-
tion does not. Our analysis of the reasons
for this is presented and generally point
to the nature of the more lexical/semantic
features dominating the classification task
and general structural features dominating
the argument identification task.
</bodyText>
<sectionHeader confidence="0.999248" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99800853125">
Automatic, accurate and wide-coverage techniques
that can annotate naturally occurring text with se-
mantic argument structure play a key role in NLP
applications such as Information Extraction (Sur-
deanu et al., 2003; Harabagiu et al., 2005), Question
Answering (Narayanan and Harabagiu, 2004) and
Machine Translation (Boas, 2002; Chen and Fung,
2004). Semantic Role Labeling (SRL) is the pro-
cess of producing such a markup. When presented
with a sentence, a parser should, for each predicate
in the sentence, identify and label the predicate’s se-
mantic arguments. In recent work, a number of re-
searchers have cast this problem as a tagging prob-
lem and have applied various supervised machine
learning techniques to it. On the Wall Street Jour-
nal (WSJ) data, using correct syntactic parses, it is
possible to achieve accuracies rivaling human inter-
annotator agreement. However, the performance gap
widens when information derived from automatic
syntactic parses is used.
So far, most of the work on SRL systems has been
focused on improving the labeling performance on a
test set belonging to the same genre of text as the
training set. Both the Treebank on which the syntac-
tic parser is trained and the PropBank on which the
SRL systems are trained represent articles from the
year 1989 of the WSJ. While all these systems per-
form quite well on the WSJ test data, they show sig-
nificant performance degradation (approximately 10
point drop in F-score) when applied to label test data
that is different than the genre that WSJ represents
(Pradhan et al., 2004; Carreras and M`arquez, 2005).
</bodyText>
<page confidence="0.99072">
556
</page>
<note confidence="0.7976395">
Proceedings of NAACL HLT 2007, pages 556–563,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99980075">
Surprisingly, it does not matter much whether the
data is from another newswire, or a completely dif-
ferent type of text – as in the Brown corpus. These
results indicate that the systems are being over-fit to
the specific genre of text. Many performance im-
provements on the WSJ PropBank corpus may re-
flect tuning to the corpus. For the technology to
be widely accepted and useful, it must be robust
to change in genre of the data. Until recently, data
tagged with similar semantic argument structure was
not available for multiple genres of text. Recently,
Palmer et al., (2005), have PropBanked a significant
portion of the Treebanked Brown corpus which en-
ables us to perform experiments to analyze the rea-
sons behind the performance degradation, and sug-
gest potential solutions.
</bodyText>
<sectionHeader confidence="0.830178" genericHeader="method">
2 Semantic Annotation and Corpora
</sectionHeader>
<bodyText confidence="0.999931571428571">
In the PropBank1 corpus (Palmer et al., 2005), pred-
icate argument relations are marked for the verbs
in the text. PropBank was constructed by assign-
ing semantic arguments to constituents of the hand-
corrected Treebank parses. The arguments of a verb
are labeled ARG0 to ARG5, where ARG0 is the
PROTO-AGENT (usually the subject of a transitive
verb) ARG1 is the PROTO-PATIENT (usually its di-
rect object), etc. In addition to these CORE ARGU-
MENTS, 16 additional ADJUNCTIVE ARGUMENTS,
referred to as ARGMs are also marked.
More recently the PropBanking effort has been
extended to encompass multiple corpora. In this
study we use PropBanked versions of the Wall Street
Journal (WSJ) part of the Penn Treebank (Marcus et
al., 1994) and part of the Brown portion of the Penn
Treebank.
The WSJ PropBank data comprise 24 sections
of the WSJ, each section representing about 100
documents. PropBank release 1.0 contains about
114,000 predicates instantiating about 250,000 argu-
ments and covering about 3,200 verb lemmas. Sec-
tion 23, which is a standard test set and a test set
in some of our experiments, comprises 5,400 predi-
cates instantiating about 12,000 arguments.
The Brown corpus is a Standard Corpus of Ameri-
can English that consists of about one million words
of English text printed in the calendar year 1961
</bodyText>
<footnote confidence="0.941041">
1http://www.cis.upenn.edu/˜ace/
</footnote>
<bodyText confidence="0.999790666666667">
(Kuˇcera and Francis, 1967). The corpus contains
about 500 samples of 2000+ words each. The idea
behind creating this corpus was to create a hetero-
geneous sample of English text so that it would be
useful for comparative language studies.
The Release 3 of the Penn Treebank contains the
hand parsed syntactic trees of a subset of the Brown
Corpus – sections F, G, K, L, M, N, P and R. Palmer
et al., (2005) have recently PropBanked a signifi-
cant portion of this Treebanked Brown corpus. In
all, about 17,500 predicates are tagged with their se-
mantic arguments. For these experiments we used a
limited release of PropBank dated September 2005.
A small portion of the predicates – about 8,000 have
also been tagged with frame sense information.
</bodyText>
<sectionHeader confidence="0.996886" genericHeader="method">
3 SRL System Description
</sectionHeader>
<bodyText confidence="0.999843769230769">
We formulate the labeling task as a classification
problem as initiated by Gildea and Jurafsky (2002)
and use Support Vector Machine (SVM) classi-
fiers (2005). We use TinySVM2 along with Yam-
Cha3 (Kudo and Matsumoto, 2000) (Kudo and Mat-
sumoto, 2001) as the SVM training and classifica-
tion software. The system uses a polynomial kernel
with degree 2; the cost per unit violation of the mar-
gin, C=1; and, tolerance of the termination criterion,
e=0.001. More details of this system can be found
in Pradhan et al., (2005). The performance of this
system on section 23 of the WSJ when trained on
sections 02-21 is shown in Table 1
</bodyText>
<table confidence="0.99782425">
ALL ARDs Task P R F A
(%) (%) (%)
TREESANx Id. 97.5 96.1 96.8
Class. - - - 93.0
Id. + Class. 91.8 90.5 91.2
AUTOMATIC Id. 86.9 84.2 85.5
Class. - - - 92.0
Id. + Class. 82.1 77.9 79.9
</table>
<tableCaption confidence="0.999938">
Table 1: Performance of the SRL system on WSJ
</tableCaption>
<bodyText confidence="0.999853714285714">
The performance of the SRL system is reported
on three different tasks, all of which are with respect
to a particular predicate: i) argument identification
(ID), is the task of identifying the set of words (here,
parse constituents) that represent a semantic role; ii)
argument classification (Class.), is the task of clas-
sifying parse constituents known to represent some
</bodyText>
<footnote confidence="0.999947">
2http://cl.aist-nara.ac.jp/˜talus-Au/software/TinySVM/
3http://cl.aist-nara.ac.jp/˜taku-Au/software/yamcha/
</footnote>
<page confidence="0.99682">
557
</page>
<bodyText confidence="0.999003866666667">
semantic role into one of the many semantic role
types; and iii) argument identification and classifi-
cation (ID + Class.), which involves both the iden-
tification of the parse constituents that represent se-
mantic roles of the predicate and their classification
into the respective semantic roles. As usual, argu-
ment classification is measured as percent accuracy
(A), whereas ID and ID + Class. are measured in
terms of precision (P), recall (R) and F-score (F)
– the harmonic mean of P and R. The first three
rows of Table 1 report performance for the system
that uses hand-corrected Treebank parses, and the
next three report performance for the SRL system
that uses automatically generated – Charniak parser
– parses, both during training and testing.
</bodyText>
<sectionHeader confidence="0.992045" genericHeader="method">
4 Robustness Experiments
</sectionHeader>
<bodyText confidence="0.994659121212121">
This section describes experiments that we per-
formed using the PropBanked Brown corpus in an
attempt to analyze the factors affecting the portabil-
ity of SRL systems.
4.1 How does the SRL system trained on WSJ
perform on Brown?
In order to test the robustness of the SRL system,
we used a system trained on the PropBanked WSJ
corpus to label data from the Brown corpus. We use
the entire PropBanked Brown corpus (about 17,500
predicates) as a test set for this experiment and use
the SRL system trained on WSJ sections 02-21 to
tag its arguments.
Table 2 shows the performance for training and
testing on WSJ, and for training on WSJ and testing
on Brown. There is a significant reduction in per-
formance when the system trained on WSJ is used
to label data from the Brown corpus. The degrada-
tion in the Identification task is small compared to
that of the combined Identification and Classifica-
tion task. A number of factors could be responsible
for the loss of performance. It is possible that the
SRL models are tuned to the particular vocabulary
and sense structure associated with the training data.
Also, since the syntactic parser that is used for gen-
erating the syntax parse trees (Charniak) is heavily
lexicalized and is trained on WSJ, it could have de-
creased accuracy on the Brown data resulting in re-
duced accuracy for Semantic Role Labeling. Since
the SRL algorithm walks the syntax tree classifying
each node, if no constituent node is present that cor-
responds to the correct argument, the system cannot
produce a correct labeling for the argument.
</bodyText>
<table confidence="0.997688">
Train Test Id. Id. + Class
F F
WSJ WSJ 85.5 79.9
WSJ Brown 82.4 65.1
</table>
<tableCaption confidence="0.740549125">
Table 2: Performance of the SRL system on Brown.
In order to check the extent to which constituent
nodes representing semantic arguments were deleted
from the syntax tree due to parser error, we gener-
ated the performance numbers which are shown in
Table 3. These numbers are for top one parse for the
Charniak parser, and represent not all parser errors,
but deletion of argument bearing constituent nodes.
</tableCaption>
<table confidence="0.975317333333333">
Total Misses %
PropBank 12000 800 6.7
Brown 45880 3692 8.1
</table>
<tableCaption confidence="0.999825">
Table 3: Constituent deletions in WSJ and Brown.
</tableCaption>
<bodyText confidence="0.9999674">
The parser misses 6.7% of the argument-bearing
nodes in the PropBank test set and about 8.1% in
the Brown corpus. This indicates that the errors in
syntactic parsing account for a fairly small amount
of the argument deletions and probably do not con-
tributing significantly to the increased SRL error
rate. Obviously, just the presence of a argument-
bearing constituent does not necessarily guarantee
the correctness of the structural connections be-
tween itself and the predicate.
</bodyText>
<subsectionHeader confidence="0.996363">
4.2 Identification vs Classification Performance
</subsectionHeader>
<bodyText confidence="0.999927785714286">
Different features tend to dominate in the identifi-
cation task vs the classification task. For example,
the path feature (representing the path in the syntax
tree from the argument to the predicate) is the sin-
gle most salient feature for the ID task and is not
very important in the classification task. In the next
experiment we look at cross genre performance of
the ID and Classification tasks. We used gold stan-
dard syntactic trees from the Treebank so there are
no errors in generating the syntactic structure. In
addition to training on the WSJ and testing on WSJ
and Brown, we trained the SRL system on a Brown
training set and tested it on a test set also from the
Brown corpus. In generating the Brown training and
</bodyText>
<page confidence="0.981222">
558
</page>
<table confidence="0.9994835">
SRL SRL Task P R F A
Train Test (%) (%) (%)
WSJ WSJ Id. 97.5 96.1 96.8
(104k) (5k) Class. 93.0
Id. + Class. 91.8 90.5 91.2
WSJ WSJ Id. 96.3 94.4 95.3
(14k) (5k) Class. 86.1
Id. + Class. 84.4 79.8 82.0
BROWN BROWN Id. 95.7 94.9 95.2
(14k) (1.6k) Class. 80.1
Id. + Class. 79.9 77.0 78.4
WSJ BROWN Id. 94.2 91.4 92.7
(14k) (1.6k) Class. 72.0
Id. + Class. 71.8 65.8 68.6
</table>
<tableCaption confidence="0.99984">
Table 4: Performance of the SRL system using correct Treebank parses.
</tableCaption>
<bodyText confidence="0.999856411764706">
test sets, we used stratified sampling, which is often
used by the syntactic parsing community (Gildea,
2001). The test set was generated by selecting ev-
ery 101h sentence in the Brown Corpus. We also
held out the development set used by Bacchiani et
al., (2006) to tune system parameters in the future.
This procedure resulted in a training set of approxi-
mately 14,000 predicates and a test set of about 1600
predicates. We did not perform any parameter tun-
ing for any of the following experiments, and used
the parameter settings from the best performing ver-
sion of the SRL system as reported in Table1. We
compare the performance on this test set with that
obtained when the SRL system is trained using WSJ
sections 02-21 and use section 23 for testing. For
a more balanced comparison, we retrained the SRL
system on the same amount of data as used for train-
ing on Brown, and tested it on section 23. As usual,
trace information, and function tag information from
the Treebank is stripped out.
Table 4 shows the results. There is a fairly small
difference in argument Identification performance
when the SRL system is trained on 14,000 predi-
cates vs 104,000 predicates from the WSJ (F-score
95.3 vs 96.8). However, there is a considerable drop
in Classification accuracy (86.1% vs 93.0%). When
the SRL system is trained and tested on Brown data,
the argument Identification performance is not sig-
nificantly different than that for the system trained
and tested on WSJ data (F-score 95.2 vs 95.3). The
drop in argument Classification accuracy is much
more severe (86.1% vs 80.1%).
This same trend between ID and Classification is
even more pronounced when training on WSJ and
testing on Brown. For a system trained on WSJ,
there is a fairly small drop in performance of the
ID task when tested on Brown vs tested on WSJ (F-
score 92.7 vs 95.3). However, in this same condi-
tion, the Classification task has a very large drop in
performance (72.0% vs 86.1%).
So argument ID is not very sensitive to amount
of training data in a corpus, or to the genre of the
corpus, and ports well from WSJ to Brown. This ex-
periment supports the belief that there is no signifi-
cant drop in the task of identifying the right syntactic
constituents that are arguments – and this is intuitive
since previous experiments have shown that the task
of argument identification is more dependent on the
structural features – one such feature being the path
in the syntax tree.
Argument Classification seems to be the problem.
It requires more training data within the WSJ corpus,
does not perform as well when trained and tested on
Brown as it does for WSJ and does not port well
from WSJ to Brown. This suggests that the features
it uses are being over-fit to the training data and are
more idiosyncratic to a given dataset. In particular,
the predicate whose arguments are being identified,
and the head word of the syntactic constituent being
classified are both important features in the task of
argument classification.
As a generalization, the features used by the Iden-
tification task reflect structure and port well. The
features used by the Classification task reflect spe-
cific lexical usage and semantics, and tend to require
more training data and are more subject to over-
fitting. Even when training and testing on Brown,
Classification accuracy is considerably worse than
</bodyText>
<page confidence="0.992757">
559
</page>
<bodyText confidence="0.999871">
training and testing on WSJ (with comparable train-
ing set size). It is probably the case that the predi-
cates and head words in a homogeneous corpus such
as the WSJ are used more consistently, and tend to
have single dominant word senses. The Brown cor-
pus probably has much more variety in its lexical
usage and word senses.
</bodyText>
<subsectionHeader confidence="0.869969">
4.3 How sensitive is semantic argument
</subsectionHeader>
<bodyText confidence="0.997950230769231">
prediction to the syntactic correctness
across genre?
This experiment examines the same cross-genre ef-
fects as the last experiment, but uses automatically
generated syntactic parses rather than gold standard
ones.
For this experiment, we used the same amount of
training data from WSJ as available in the Brown
training set – that is about 14,000 predicates. The
examples from WSJ were selected randomly. The
Brown test set is the same as used in the previous
experiment, and the WSJ test set is the entire section
23.
Recently there have been some improvements to
the Charniak parser, use n-best re-ranking as re-
ported in (Charniak and Johnson, 2005) and self-
training and re-ranking using data from the North
American News corpus (NANC) and adapts much
better to the Brown corpus (McClosky et al., 2006a;
McClosky et al., 2006b). The performance of these
parsers as reported in the respective literature are
shown in Table 6 shows the performance (as re-
ported in the literature) of the Charniak parser: when
trained and tested on WSJ, when trained on WSJ and
tested on Brown, When trained and tested on Brown,
and when trained on WSJ and adapted with NANC.
</bodyText>
<table confidence="0.995242">
Train Test F
WSJ WSJ 91.0
WSJ Brown 85.2
Brown Brown 88.4
WSJ+NANC Brown 87.9
</table>
<tableCaption confidence="0.998698">
Table 6: Charniak parser performance.
</tableCaption>
<bodyText confidence="0.9997095">
We describe the results of Semantic Role Label-
ing under the following five conditions:
</bodyText>
<listItem confidence="0.924022454545454">
1. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked WSJ sentences. The syntactic
parser – Charniak parser – is itself trained on
the WSJ training sections of the Treebank. This
is used for Semantic Role Labeling of section-
23 of WSJ.
2. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked WSJ sentences. The syntac-
tic parser – Charniak parser – is itself trained
on the WSJ training sections of the Treebank.
This is used to classify the Brown test set.
3. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked Brown corpus sentences. The
syntactic parser is trained using the WSJ por-
tion of the Treebank. This is used to classify
the Brown test set.
4. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked Brown corpus sentences. The
syntactic parser is trained using the Brown
training portion of the Treebank. This is used
to classify the Brown test set.
5. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked Brown corpus sentences. The
syntactic parser is the version that is self-
trained using 2,500,000 sentences from NANC,
and where the starting version is trained only
on WSJ data (McClosky et al., 2006b). This is
used to classify the Brown test set.
</listItem>
<bodyText confidence="0.999931888888889">
Table 5 shows the results. For simplicity of dis-
cussion we have tagged the five conditions as 1.,
2., 3., 4., and 5. Comparing conditions 2. and 3.
shows that when the features used to train the SRL
system are extracted using a syntactic parser that is
trained on WSJ it performs at almost the same level
on the task of Identification, regardless of whether
it is trained on the PropBanked Brown corpus or
the PropBanked WSJ corpus. This, however, is sig-
nificantly lower than when all the three – the syn-
tactic parser training set, the SRL system training
set, and the SRL system test set, are from the same
genre (6 F-score points lower than condition 1, and
5 points lower than conditions 4 and 5). In case of
the combined task, the gap between the performance
for conditions 2 and 3 is about 10 points in F-score
(59.1 vs 69.8). Looking at the argument classifica-
tion accuracies, we see that using the SRL system
</bodyText>
<page confidence="0.988724">
560
</page>
<table confidence="0.999892941176471">
Setup Parser SRL SRL Task P R F A
Train Train Test (%) (%) (%)
WSJ WSJ WSJ Id. 87.3 84.8 86.0
(40k – sec:00-21) (14k) (5k) Class. 84.1
Id. + Class. 77.5 69.7 73.4
WSJ WSJ Brown Id. 81.7 78.3 79.9
(40k – sec:00-21) (14k) (1.6k) Class. 72.1
Id. + Class. 63.7 55.1 59.1
WSJ Brown Brown Id. 81.7 78.3 80.0
(40k – sec:00-21) (14k) (1.6k) Class. 79.2
Id. + Class. 78.2 63.2 69.8
Brown Brown Brown Id. 87.6 82.3 84.8
(20k) (14k) (1.6k) Class. 78.9
Id. + Class. 77.4 62.1 68.9
WSJ+NANC Brown Brown Id. 87.7 82.5 85.0
(2,500k) (14k) (1.6k) Class. 79.9
Id. + Class. 77.2 64.4 70.0
</table>
<tableCaption confidence="0.999869">
Table 5: Performance on WSJ and Brown using automatic syntactic parses
</tableCaption>
<bodyText confidence="0.999775090909091">
trained on WSJ to test Brown sentences give a 12
point drop in F-score (84.1 vs 72.1). Using the SRL
system trained on Brown using WSJ trained syntac-
tic parser shows a drop in accuracy by about 5 F-
score points (84.1 to 79.2). When the SRL system is
trained on Brown using syntactic parser also trained
on Brown, we get a quite similar classification per-
formance, which is again about 5 points lower than
what we get using all WSJ data. This shows lexical
semantic features might be very important to get a
better argument classification on Brown corpus.
</bodyText>
<subsectionHeader confidence="0.9719315">
4.4 How much data is required to adapt to a
new genre?
</subsectionHeader>
<bodyText confidence="0.999977057142857">
We would like to know how much data from a new
genre we need to annotate and add to the training
data of an existing corpus to adapt the system such
that it gives the same level of performance as when
it is trained on the new genre.
One section of the Brown corpus – section CK
has about 8,200 predicates annotated. We use six
different conditions – two in which we use correct
Treebank parses, and the four others in which we
use automatically generated parses using the varia-
tions described before. All training sets start with
the same number of examples as in the Brown train-
ing set. The part of this section used as a test set for
the CoNLL 2005 shared task is used as the test set
here. It contains a total of about 800 predicates.
Table 7 shows a comparison of these conditions.
In all the six conditions, the performance on the task
of Identification and Classification improves gradu-
ally until about 5625 examples of section CK which
is about 75% of the total added, above which they
improve very little. In fact, even 50% of the new
data accounts for 90% of the performance differ-
ence. Even when the syntactic parser is trained on
WSJ and the SRL is trained on WSJ, adding 7,500
instances of the new genres allows it to achieve al-
most the same performance as when all three are
from the same genre (67.2 vs 69.9). Numbers for ar-
gument identification aren’t shown because adding
more data does not have any statistically signifi-
cant impact on its performance. The system that
uses self-trained syntactic parser seems to perform
slightly better than the rest of the versions that use
automatically generated syntactic parses. The preci-
sion numbers are almost unaffected – except when
the labeler is trained on WSJ PropBank data.
</bodyText>
<subsectionHeader confidence="0.841967">
4.5 How much does verb sense information
contribute?
</subsectionHeader>
<bodyText confidence="0.999944">
In order to find out how important the verb sense
information is in the process of genre transfer, we
used the subset of PropBanked Brown corpus that
was tagged with verb sense information, ran an ex-
periment similar to that of Experiment 1. We used
the oracle sense information and correct syntactic in-
formation for this experiment.
Table 8 shows the results of this experiment.
There is about 1 point F-score increase on using
oracle sense information on the overall data. We
looked at predicates that had high perplexity in both
the training and test sets, and whose sense distribu-
</bodyText>
<page confidence="0.991295">
561
</page>
<table confidence="0.921861967741935">
SRL P Id. + Class F Parser SRL P Id. + Class F
R R
Train (%) (%) (%) (%)
WSJ (14k) WSJ Brown (14k)
(Treebank parses)
+0 ex. from CK 74.1 66.5 70.1 (40k) +0 ex. from CK 74.4 57.0 64.5
+1875 ex. from CK 77.6 71.3 74.3 +1875 ex. from CK 75.1 58.7 65.9
+3750 ex. from CK 79.1 74.1 76.5 +3750 ex. from CK 76.1 59.6 66.9
+5625 ex. from CK 80.4 76.1 78.1 +5625 ex. from CK 76.9 60.5 67.7
+7500 ex. from CK 80.2 76.1 78.1 +7500 ex. from CK 76.8 59.8 67.2
Brown (14k) Brown Brown (14k)
(Treebank parses)
+0 ex. from CK 77.1 73.0 75.0 (20k) +0 ex. from CK 76.0 59.2 66.5
+1875 ex. from CK 78.8 75.1 76.9 +1875 ex. from CK 76.1 60.0 67.1
+3750 ex. from CK 80.4 76.9 78.6 +3750 ex. from CK 77.7 62.4 69.2
+5625 ex. from CK 80.4 77.2 78.7 +5625 ex. from CK 78.2 63.5 70.1
+7500 ex. from CK 81.2 78.1 79.6 +7500 ex. from CK 78.2 63.2 69.9
WSJ (14k) WSJ+NANC Brown (14k)
+0 ex. from CK 65.2 55.7 60.1 (2,500k) +0 ex. from CK 74.4 60.1 66.5
+1875 ex. from CK 68.9 57.5 62.7 +1875 ex. from CK 76.2 62.3 68.5
+3750 ex. from CK 71.8 59.3 64.9 +3750 ex. from CK 76.8 63.6 69.6
+5625 ex. from CK 74.3 61.3 67.2 +5625 ex. from CK 77.7 63.8 70.0
+7500 ex. from CK 74.8 61.0 67.2 +7500 ex. from CK 78.2 64.9 70.9
Parser
Train
WSJ
(Treebank parses)
Brown
(Treebank parses)
WSJ
(40k)
</table>
<tableCaption confidence="0.990168">
Table 7: Effect of incrementally adding data from a new genre
</tableCaption>
<table confidence="0.9877336">
Train Test Without Sense With Sense
Id. Id.
F F
WSJ Brown (All) 69.1 69.9
WSJ Brown (predicate: go) 46.9 48.9
</table>
<tableCaption confidence="0.999061">
Table 8: Influence of verb sense feature.
</tableCaption>
<bodyText confidence="0.999902">
tion was different. One such predicate is “go”. The
improvement on classifying the arguments of this
predicate was about 2 points (46.9 to 48.9), which
suggests that verb sense is more important when the
sense structure of the test corpus is more ambiguous
and is different from the training. Here we used ora-
cle verb sense information, but one can train a clas-
sifier as done by Girju et al., (2005) which achieves
a disambiguation accuracy in the 80s for within the
WSJ corpus.
</bodyText>
<sectionHeader confidence="0.99944" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9986955">
Our experimental results on robustness to change in
genre can be summarized as follows:
</bodyText>
<listItem confidence="0.997290333333333">
• There is a significant drop in performance when
training and testing on different corpora – for
both Treebank and Charniak parses
• In this process the classification task is more
disrupted than the identification task.
• There is a performance drop in classification
</listItem>
<bodyText confidence="0.992325952380952">
even when training and testing on Brown (com-
pared to training and testing on WSJ)
• The syntactic parser error is not a large part of
the degradation for the case of automatically
generated parses.
An error analysis leads us to believe that some
reasons for this behavior could be: i) lexical us-
ages that are specific to WSJ, ii) variation in sub-
categorization across corpora, iii) variation in word
sense distribution and iv) changes in topics and enti-
ties. Training and testing on the same corpora tends
to give a high weight to very specific semantic fea-
tures. Two possibilities remedies could be: i) using
less homogeneous corpora and ii) less specific fea-
tures, for eg., proper names are replaced with the
name entities that they represent. This way the sys-
tem could be forced to use the more general features.
Both of these manipulations would most likely re-
duce performance on the training set, and on test
sets of the same genre as the training data. But they
would be likely to generalize better.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996125">
We are extremely grateful to Martha Palmer for pro-
viding us with the PropBanked Brown corpus, and
to David McClosky for providing us with hypothe-
ses on the Brown test set as well as a cross-validated
</bodyText>
<page confidence="0.9886">
562
</page>
<bodyText confidence="0.999755166666667">
version of the Brown training data for the various
models reported in his work reported at HLT 2006.
This research was partially supported by
the ARDA AQUAINT program via contract
OCG4423B and by the NSF via grants IS-9978025
and ITR/HCI 0086132.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999929375">
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41–68.
Hans Boas. 2002. Bilingual framenet dictionaries for
machine translation. In Proceedings ofLREC-2002.
Xavier Carreras and Lluis M`arquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2005, pages 152–164,
Ann Arbor, MI.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL-2005, pages 173–180,
Ann Arbor, MI.
Benfeng Chen and Pascale Fung. 2004. Automatic con-
struction of an english-chinese bilingual framenet. In
Proceedings of the HLT/NAACL-2004, Boston, MA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245–288.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In In Proceedings ofEMNLP-2001.
R. Girju, D. Roth, and M. Sammons. 2005. Token-
level disambiguation of verbnet classes. In Proceed-
ings of the Interdisciplinary Workshop on the Identifi-
cation and Representation of Verb Features and Verb
Classes, K. Erk, A. Melinger, and S. Schulte im Walde
(eds.).
Sanda Harabagiu, Cosmin Adrian Bejan, and
Paul Morarescu. 2005. Shallow semantics for relation
extraction. In IJCAI-2005, pages 1061–1067, Edin-
burgh, Scotland.
Henry Kuˇcera and W. Nelson Francis. 1967. Com-
putational analysis ofpresent-day American English.
Brown University Press, Providence, RI.
Taku Kudo and Yuji Matsumoto. 2000. Use of support
vector learning for chunk identification. In Proceed-
ings of CoNLL-2000 and LLL-2000, pages 142–144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of the
NAACL-2001.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure.
David McClosky, Eugene Charniak, and Mark Johnson.
2006a. Effective self-training for parsing. In Proceed-
ings ofHLT/NAACL-2006, pages 152–159, New York
City, USA. Association for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2006b. Rerankinng and self-training for parser adapta-
tion. In Proceedings of COLING/ACL-2006, Sydney,
Australia.
Srini Narayanan and Sanda Harabagiu. 2004. Question
answering based on semantic structures. In Proceed-
ings of COLING-2004), Geneva, Switzerland.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71–
106.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-
ings ofHLT/NAACL-2004, Boston, MA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2005. Semantic role label-
ing using different syntactic views. In Proceedings of
ACL-2005, Ann Arbor, MI.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL-2003, Sapporo, Japan.
</reference>
<page confidence="0.998942">
563
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.988735">
<title confidence="0.999855">Towards Robust Semantic Role Labeling</title>
<author confidence="0.999373">Sameer Pradhan Wayne Ward</author>
<author confidence="0.999373">James H Martin</author>
<affiliation confidence="0.995341">BBN Technologies University of Colorado</affiliation>
<address confidence="0.999954">Cambridge, MA 02138 Boulder, CO 80303</address>
<abstract confidence="0.9997963">Most research on semantic role labeling (SRL) has been focused on training and evaluating on the same corpus in order to develop the technology. This strategy, while appropriate for initiating research, can lead to over-training to the particular corpus. The work presented in this paper focuses on analyzing the robustness of an SRL system when trained on one genre of data and used to label a different genre. Our state-of-the-art semantic role labeling system, while performing well on WSJ test data, shows significant performance degradation when applied to data from the Brown corpus. We present a series of experiments designed to investigate the source of this lack of portability. These experiments are based on comparisons of performance using PropBanked WSJ data and PropBanked Brown corpus data. Our results indicate that while syntactic parses and argument identification port relatively well to a new genre, argument classification does not. Our analysis of the reasons for this is presented and generally point to the nature of the more lexical/semantic features dominating the classification task and general structural features dominating the argument identification task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Michael Riley</author>
<author>Brian Roark</author>
<author>Richard Sproat</author>
</authors>
<title>MAP adaptation of stochastic grammars.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="12269" citStr="Bacchiani et al., (2006)" startWordPosition="2037" endWordPosition="2040">4k) (5k) Class. 93.0 Id. + Class. 91.8 90.5 91.2 WSJ WSJ Id. 96.3 94.4 95.3 (14k) (5k) Class. 86.1 Id. + Class. 84.4 79.8 82.0 BROWN BROWN Id. 95.7 94.9 95.2 (14k) (1.6k) Class. 80.1 Id. + Class. 79.9 77.0 78.4 WSJ BROWN Id. 94.2 91.4 92.7 (14k) (1.6k) Class. 72.0 Id. + Class. 71.8 65.8 68.6 Table 4: Performance of the SRL system using correct Treebank parses. test sets, we used stratified sampling, which is often used by the syntactic parsing community (Gildea, 2001). The test set was generated by selecting every 101h sentence in the Brown Corpus. We also held out the development set used by Bacchiani et al., (2006) to tune system parameters in the future. This procedure resulted in a training set of approximately 14,000 predicates and a test set of about 1600 predicates. We did not perform any parameter tuning for any of the following experiments, and used the parameter settings from the best performing version of the SRL system as reported in Table1. We compare the performance on this test set with that obtained when the SRL system is trained using WSJ sections 02-21 and use section 23 for testing. For a more balanced comparison, we retrained the SRL system on the same amount of data as used for traini</context>
</contexts>
<marker>Bacchiani, Riley, Roark, Sproat, 2006</marker>
<rawString>Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat. 2006. MAP adaptation of stochastic grammars. Computer Speech and Language, 20(1):41–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Boas</author>
</authors>
<title>Bilingual framenet dictionaries for machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofLREC-2002.</booktitle>
<contexts>
<context position="1741" citStr="Boas, 2002" startWordPosition="257" endWordPosition="258">argument classification does not. Our analysis of the reasons for this is presented and generally point to the nature of the more lexical/semantic features dominating the classification task and general structural features dominating the argument identification task. 1 Introduction Automatic, accurate and wide-coverage techniques that can annotate naturally occurring text with semantic argument structure play a key role in NLP applications such as Information Extraction (Surdeanu et al., 2003; Harabagiu et al., 2005), Question Answering (Narayanan and Harabagiu, 2004) and Machine Translation (Boas, 2002; Chen and Fung, 2004). Semantic Role Labeling (SRL) is the process of producing such a markup. When presented with a sentence, a parser should, for each predicate in the sentence, identify and label the predicate’s semantic arguments. In recent work, a number of researchers have cast this problem as a tagging problem and have applied various supervised machine learning techniques to it. On the Wall Street Journal (WSJ) data, using correct syntactic parses, it is possible to achieve accuracies rivaling human interannotator agreement. However, the performance gap widens when information derived</context>
</contexts>
<marker>Boas, 2002</marker>
<rawString>Hans Boas. 2002. Bilingual framenet dictionaries for machine translation. In Proceedings ofLREC-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<pages>152--164</pages>
<location>Ann Arbor, MI.</location>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Lluis M`arquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of CoNLL-2005, pages 152–164, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="16381" citStr="Charniak and Johnson, 2005" startWordPosition="2741" endWordPosition="2744">tic correctness across genre? This experiment examines the same cross-genre effects as the last experiment, but uses automatically generated syntactic parses rather than gold standard ones. For this experiment, we used the same amount of training data from WSJ as available in the Brown training set – that is about 14,000 predicates. The examples from WSJ were selected randomly. The Brown test set is the same as used in the previous experiment, and the WSJ test set is the entire section 23. Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and selftraining and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al., 2006a; McClosky et al., 2006b). The performance of these parsers as reported in the respective literature are shown in Table 6 shows the performance (as reported in the literature) of the Charniak parser: when trained and tested on WSJ, when trained on WSJ and tested on Brown, When trained and tested on Brown, and when trained on WSJ and adapted with NANC. Train Test F WSJ WSJ 91.0 WSJ Brown 85.2 Brown Brown 88.4 WSJ+NANC Brown 87.9 Table 6: Charni</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of ACL-2005, pages 173–180, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benfeng Chen</author>
<author>Pascale Fung</author>
</authors>
<title>Automatic construction of an english-chinese bilingual framenet.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT/NAACL-2004,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="1763" citStr="Chen and Fung, 2004" startWordPosition="259" endWordPosition="262">ssification does not. Our analysis of the reasons for this is presented and generally point to the nature of the more lexical/semantic features dominating the classification task and general structural features dominating the argument identification task. 1 Introduction Automatic, accurate and wide-coverage techniques that can annotate naturally occurring text with semantic argument structure play a key role in NLP applications such as Information Extraction (Surdeanu et al., 2003; Harabagiu et al., 2005), Question Answering (Narayanan and Harabagiu, 2004) and Machine Translation (Boas, 2002; Chen and Fung, 2004). Semantic Role Labeling (SRL) is the process of producing such a markup. When presented with a sentence, a parser should, for each predicate in the sentence, identify and label the predicate’s semantic arguments. In recent work, a number of researchers have cast this problem as a tagging problem and have applied various supervised machine learning techniques to it. On the Wall Street Journal (WSJ) data, using correct syntactic parses, it is possible to achieve accuracies rivaling human interannotator agreement. However, the performance gap widens when information derived from automatic syntac</context>
</contexts>
<marker>Chen, Fung, 2004</marker>
<rawString>Benfeng Chen and Pascale Fung. 2004. Automatic construction of an english-chinese bilingual framenet. In Proceedings of the HLT/NAACL-2004, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="6153" citStr="Gildea and Jurafsky (2002)" startWordPosition="988" endWordPosition="991">Release 3 of the Penn Treebank contains the hand parsed syntactic trees of a subset of the Brown Corpus – sections F, G, K, L, M, N, P and R. Palmer et al., (2005) have recently PropBanked a significant portion of this Treebanked Brown corpus. In all, about 17,500 predicates are tagged with their semantic arguments. For these experiments we used a limited release of PropBank dated September 2005. A small portion of the predicates – about 8,000 have also been tagged with frame sense information. 3 SRL System Description We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software. The system uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and, tolerance of the termination criterion, e=0.001. More details of this system can be found in Pradhan et al., (2005). The performance of this system on section 23 of the WSJ when trained on sections 02-21 is shown in Table 1 ALL ARDs Task P R F A (%) (%) (%) TREESANx Id. 97.5 96.1 96.8 Class. - - - 9</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance. In</title>
<date>2001</date>
<booktitle>In Proceedings ofEMNLP-2001.</booktitle>
<contexts>
<context position="12117" citStr="Gildea, 2001" startWordPosition="2011" endWordPosition="2012">so from the Brown corpus. In generating the Brown training and 558 SRL SRL Task P R F A Train Test (%) (%) (%) WSJ WSJ Id. 97.5 96.1 96.8 (104k) (5k) Class. 93.0 Id. + Class. 91.8 90.5 91.2 WSJ WSJ Id. 96.3 94.4 95.3 (14k) (5k) Class. 86.1 Id. + Class. 84.4 79.8 82.0 BROWN BROWN Id. 95.7 94.9 95.2 (14k) (1.6k) Class. 80.1 Id. + Class. 79.9 77.0 78.4 WSJ BROWN Id. 94.2 91.4 92.7 (14k) (1.6k) Class. 72.0 Id. + Class. 71.8 65.8 68.6 Table 4: Performance of the SRL system using correct Treebank parses. test sets, we used stratified sampling, which is often used by the syntactic parsing community (Gildea, 2001). The test set was generated by selecting every 101h sentence in the Brown Corpus. We also held out the development set used by Bacchiani et al., (2006) to tune system parameters in the future. This procedure resulted in a training set of approximately 14,000 predicates and a test set of about 1600 predicates. We did not perform any parameter tuning for any of the following experiments, and used the parameter settings from the best performing version of the SRL system as reported in Table1. We compare the performance on this test set with that obtained when the SRL system is trained using WSJ </context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In In Proceedings ofEMNLP-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>D Roth</author>
<author>M Sammons</author>
</authors>
<title>Tokenlevel disambiguation of verbnet classes.</title>
<date>2005</date>
<booktitle>In Proceedings of the Interdisciplinary Workshop on the Identification and Representation of Verb Features and Verb Classes,</booktitle>
<editor>K. Erk, A. Melinger, and S. Schulte im Walde (eds.).</editor>
<contexts>
<context position="24919" citStr="Girju et al., (2005)" startWordPosition="4296" endWordPosition="4299"> (40k) Table 7: Effect of incrementally adding data from a new genre Train Test Without Sense With Sense Id. Id. F F WSJ Brown (All) 69.1 69.9 WSJ Brown (predicate: go) 46.9 48.9 Table 8: Influence of verb sense feature. tion was different. One such predicate is “go”. The improvement on classifying the arguments of this predicate was about 2 points (46.9 to 48.9), which suggests that verb sense is more important when the sense structure of the test corpus is more ambiguous and is different from the training. Here we used oracle verb sense information, but one can train a classifier as done by Girju et al., (2005) which achieves a disambiguation accuracy in the 80s for within the WSJ corpus. 5 Conclusions Our experimental results on robustness to change in genre can be summarized as follows: • There is a significant drop in performance when training and testing on different corpora – for both Treebank and Charniak parses • In this process the classification task is more disrupted than the identification task. • There is a performance drop in classification even when training and testing on Brown (compared to training and testing on WSJ) • The syntactic parser error is not a large part of the degradatio</context>
</contexts>
<marker>Girju, Roth, Sammons, 2005</marker>
<rawString>R. Girju, D. Roth, and M. Sammons. 2005. Tokenlevel disambiguation of verbnet classes. In Proceedings of the Interdisciplinary Workshop on the Identification and Representation of Verb Features and Verb Classes, K. Erk, A. Melinger, and S. Schulte im Walde (eds.).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Cosmin Adrian Bejan</author>
<author>Paul Morarescu</author>
</authors>
<title>Shallow semantics for relation extraction. In</title>
<date>2005</date>
<booktitle>IJCAI-2005,</booktitle>
<pages>1061--1067</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="1653" citStr="Harabagiu et al., 2005" startWordPosition="244" endWordPosition="247">ndicate that while syntactic parses and argument identification port relatively well to a new genre, argument classification does not. Our analysis of the reasons for this is presented and generally point to the nature of the more lexical/semantic features dominating the classification task and general structural features dominating the argument identification task. 1 Introduction Automatic, accurate and wide-coverage techniques that can annotate naturally occurring text with semantic argument structure play a key role in NLP applications such as Information Extraction (Surdeanu et al., 2003; Harabagiu et al., 2005), Question Answering (Narayanan and Harabagiu, 2004) and Machine Translation (Boas, 2002; Chen and Fung, 2004). Semantic Role Labeling (SRL) is the process of producing such a markup. When presented with a sentence, a parser should, for each predicate in the sentence, identify and label the predicate’s semantic arguments. In recent work, a number of researchers have cast this problem as a tagging problem and have applied various supervised machine learning techniques to it. On the Wall Street Journal (WSJ) data, using correct syntactic parses, it is possible to achieve accuracies rivaling huma</context>
</contexts>
<marker>Harabagiu, Bejan, Morarescu, 2005</marker>
<rawString>Sanda Harabagiu, Cosmin Adrian Bejan, and Paul Morarescu. 2005. Shallow semantics for relation extraction. In IJCAI-2005, pages 1061–1067, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Kuˇcera</author>
<author>W Nelson Francis</author>
</authors>
<title>Computational analysis ofpresent-day American English.</title>
<date>1967</date>
<publisher>Brown University Press,</publisher>
<location>Providence, RI.</location>
<marker>Kuˇcera, Francis, 1967</marker>
<rawString>Henry Kuˇcera and W. Nelson Francis. 1967. Computational analysis ofpresent-day American English. Brown University Press, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Use of support vector learning for chunk identification.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL-2000 and LLL-2000,</booktitle>
<pages>142--144</pages>
<contexts>
<context position="6272" citStr="Kudo and Matsumoto, 2000" startWordPosition="1008" endWordPosition="1011"> K, L, M, N, P and R. Palmer et al., (2005) have recently PropBanked a significant portion of this Treebanked Brown corpus. In all, about 17,500 predicates are tagged with their semantic arguments. For these experiments we used a limited release of PropBank dated September 2005. A small portion of the predicates – about 8,000 have also been tagged with frame sense information. 3 SRL System Description We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software. The system uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and, tolerance of the termination criterion, e=0.001. More details of this system can be found in Pradhan et al., (2005). The performance of this system on section 23 of the WSJ when trained on sections 02-21 is shown in Table 1 ALL ARDs Task P R F A (%) (%) (%) TREESANx Id. 97.5 96.1 96.8 Class. - - - 93.0 Id. + Class. 91.8 90.5 91.2 AUTOMATIC Id. 86.9 84.2 85.5 Class. - - - 92.0 Id. + Class. 82.1 77.9 79.9 Table 1: Per</context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk identification. In Proceedings of CoNLL-2000 and LLL-2000, pages 142–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL-2001.</booktitle>
<contexts>
<context position="6299" citStr="Kudo and Matsumoto, 2001" startWordPosition="1012" endWordPosition="1016">r et al., (2005) have recently PropBanked a significant portion of this Treebanked Brown corpus. In all, about 17,500 predicates are tagged with their semantic arguments. For these experiments we used a limited release of PropBank dated September 2005. A small portion of the predicates – about 8,000 have also been tagged with frame sense information. 3 SRL System Description We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software. The system uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and, tolerance of the termination criterion, e=0.001. More details of this system can be found in Pradhan et al., (2005). The performance of this system on section 23 of the WSJ when trained on sections 02-21 is shown in Table 1 ALL ARDs Task P R F A (%) (%) (%) TREESANx Id. 97.5 96.1 96.8 Class. - - - 93.0 Id. + Class. 91.8 90.5 91.2 AUTOMATIC Id. 86.9 84.2 85.5 Class. - - - 92.0 Id. + Class. 82.1 77.9 79.9 Table 1: Performance of the SRL system </context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proceedings of the NAACL-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<contexts>
<context position="4668" citStr="Marcus et al., 1994" startWordPosition="741" endWordPosition="744">e text. PropBank was constructed by assigning semantic arguments to constituents of the handcorrected Treebank parses. The arguments of a verb are labeled ARG0 to ARG5, where ARG0 is the PROTO-AGENT (usually the subject of a transitive verb) ARG1 is the PROTO-PATIENT (usually its direct object), etc. In addition to these CORE ARGUMENTS, 16 additional ADJUNCTIVE ARGUMENTS, referred to as ARGMs are also marked. More recently the PropBanking effort has been extended to encompass multiple corpora. In this study we use PropBanked versions of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1994) and part of the Brown portion of the Penn Treebank. The WSJ PropBank data comprise 24 sections of the WSJ, each section representing about 100 documents. PropBank release 1.0 contains about 114,000 predicates instantiating about 250,000 arguments and covering about 3,200 verb lemmas. Section 23, which is a standard test set and a test set in some of our experiments, comprises 5,400 predicates instantiating about 12,000 arguments. The Brown corpus is a Standard Corpus of American English that consists of about one million words of English text printed in the calendar year 1961 1http://www.cis.</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings ofHLT/NAACL-2006,</booktitle>
<pages>152--159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA.</location>
<contexts>
<context position="16533" citStr="McClosky et al., 2006" startWordPosition="2767" endWordPosition="2770">rather than gold standard ones. For this experiment, we used the same amount of training data from WSJ as available in the Brown training set – that is about 14,000 predicates. The examples from WSJ were selected randomly. The Brown test set is the same as used in the previous experiment, and the WSJ test set is the entire section 23. Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and selftraining and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al., 2006a; McClosky et al., 2006b). The performance of these parsers as reported in the respective literature are shown in Table 6 shows the performance (as reported in the literature) of the Charniak parser: when trained and tested on WSJ, when trained on WSJ and tested on Brown, When trained and tested on Brown, and when trained on WSJ and adapted with NANC. Train Test F WSJ WSJ 91.0 WSJ Brown 85.2 Brown Brown 88.4 WSJ+NANC Brown 87.9 Table 6: Charniak parser performance. We describe the results of Semantic Role Labeling under the following five conditions: 1. The SRL system is trained on features e</context>
<context position="18451" citStr="McClosky et al., 2006" startWordPosition="3093" endWordPosition="3096">of the Treebank. This is used to classify the Brown test set. 4. The SRL system is trained on features extracted from automatically generated parses of the PropBanked Brown corpus sentences. The syntactic parser is trained using the Brown training portion of the Treebank. This is used to classify the Brown test set. 5. The SRL system is trained on features extracted from automatically generated parses of the PropBanked Brown corpus sentences. The syntactic parser is the version that is selftrained using 2,500,000 sentences from NANC, and where the starting version is trained only on WSJ data (McClosky et al., 2006b). This is used to classify the Brown test set. Table 5 shows the results. For simplicity of discussion we have tagged the five conditions as 1., 2., 3., 4., and 5. Comparing conditions 2. and 3. shows that when the features used to train the SRL system are extracted using a syntactic parser that is trained on WSJ it performs at almost the same level on the task of Identification, regardless of whether it is trained on the PropBanked Brown corpus or the PropBanked WSJ corpus. This, however, is significantly lower than when all the three – the syntactic parser training set, the SRL system trai</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006a. Effective self-training for parsing. In Proceedings ofHLT/NAACL-2006, pages 152–159, New York City, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Rerankinng and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL-2006,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="16533" citStr="McClosky et al., 2006" startWordPosition="2767" endWordPosition="2770">rather than gold standard ones. For this experiment, we used the same amount of training data from WSJ as available in the Brown training set – that is about 14,000 predicates. The examples from WSJ were selected randomly. The Brown test set is the same as used in the previous experiment, and the WSJ test set is the entire section 23. Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and selftraining and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al., 2006a; McClosky et al., 2006b). The performance of these parsers as reported in the respective literature are shown in Table 6 shows the performance (as reported in the literature) of the Charniak parser: when trained and tested on WSJ, when trained on WSJ and tested on Brown, When trained and tested on Brown, and when trained on WSJ and adapted with NANC. Train Test F WSJ WSJ 91.0 WSJ Brown 85.2 Brown Brown 88.4 WSJ+NANC Brown 87.9 Table 6: Charniak parser performance. We describe the results of Semantic Role Labeling under the following five conditions: 1. The SRL system is trained on features e</context>
<context position="18451" citStr="McClosky et al., 2006" startWordPosition="3093" endWordPosition="3096">of the Treebank. This is used to classify the Brown test set. 4. The SRL system is trained on features extracted from automatically generated parses of the PropBanked Brown corpus sentences. The syntactic parser is trained using the Brown training portion of the Treebank. This is used to classify the Brown test set. 5. The SRL system is trained on features extracted from automatically generated parses of the PropBanked Brown corpus sentences. The syntactic parser is the version that is selftrained using 2,500,000 sentences from NANC, and where the starting version is trained only on WSJ data (McClosky et al., 2006b). This is used to classify the Brown test set. Table 5 shows the results. For simplicity of discussion we have tagged the five conditions as 1., 2., 3., 4., and 5. Comparing conditions 2. and 3. shows that when the features used to train the SRL system are extracted using a syntactic parser that is trained on WSJ it performs at almost the same level on the task of Identification, regardless of whether it is trained on the PropBanked Brown corpus or the PropBanked WSJ corpus. This, however, is significantly lower than when all the three – the syntactic parser training set, the SRL system trai</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006b. Rerankinng and self-training for parser adaptation. In Proceedings of COLING/ACL-2006, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Question answering based on semantic structures.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-2004),</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1705" citStr="Narayanan and Harabagiu, 2004" startWordPosition="250" endWordPosition="253">ent identification port relatively well to a new genre, argument classification does not. Our analysis of the reasons for this is presented and generally point to the nature of the more lexical/semantic features dominating the classification task and general structural features dominating the argument identification task. 1 Introduction Automatic, accurate and wide-coverage techniques that can annotate naturally occurring text with semantic argument structure play a key role in NLP applications such as Information Extraction (Surdeanu et al., 2003; Harabagiu et al., 2005), Question Answering (Narayanan and Harabagiu, 2004) and Machine Translation (Boas, 2002; Chen and Fung, 2004). Semantic Role Labeling (SRL) is the process of producing such a markup. When presented with a sentence, a parser should, for each predicate in the sentence, identify and label the predicate’s semantic arguments. In recent work, a number of researchers have cast this problem as a tagging problem and have applied various supervised machine learning techniques to it. On the Wall Street Journal (WSJ) data, using correct syntactic parses, it is possible to achieve accuracies rivaling human interannotator agreement. However, the performance</context>
</contexts>
<marker>Narayanan, Harabagiu, 2004</marker>
<rawString>Srini Narayanan and Sanda Harabagiu. 2004. Question answering based on semantic structures. In Proceedings of COLING-2004), Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>106</pages>
<contexts>
<context position="3705" citStr="Palmer et al., (2005)" startWordPosition="585" endWordPosition="588">7 Association for Computational Linguistics Surprisingly, it does not matter much whether the data is from another newswire, or a completely different type of text – as in the Brown corpus. These results indicate that the systems are being over-fit to the specific genre of text. Many performance improvements on the WSJ PropBank corpus may reflect tuning to the corpus. For the technology to be widely accepted and useful, it must be robust to change in genre of the data. Until recently, data tagged with similar semantic argument structure was not available for multiple genres of text. Recently, Palmer et al., (2005), have PropBanked a significant portion of the Treebanked Brown corpus which enables us to perform experiments to analyze the reasons behind the performance degradation, and suggest potential solutions. 2 Semantic Annotation and Corpora In the PropBank1 corpus (Palmer et al., 2005), predicate argument relations are marked for the verbs in the text. PropBank was constructed by assigning semantic arguments to constituents of the handcorrected Treebank parses. The arguments of a verb are labeled ARG0 to ARG5, where ARG0 is the PROTO-AGENT (usually the subject of a transitive verb) ARG1 is the PRO</context>
<context position="5690" citStr="Palmer et al., (2005)" startWordPosition="914" endWordPosition="917">antiating about 12,000 arguments. The Brown corpus is a Standard Corpus of American English that consists of about one million words of English text printed in the calendar year 1961 1http://www.cis.upenn.edu/˜ace/ (Kuˇcera and Francis, 1967). The corpus contains about 500 samples of 2000+ words each. The idea behind creating this corpus was to create a heterogeneous sample of English text so that it would be useful for comparative language studies. The Release 3 of the Penn Treebank contains the hand parsed syntactic trees of a subset of the Brown Corpus – sections F, G, K, L, M, N, P and R. Palmer et al., (2005) have recently PropBanked a significant portion of this Treebanked Brown corpus. In all, about 17,500 predicates are tagged with their semantic arguments. For these experiments we used a limited release of PropBank dated September 2005. A small portion of the predicates – about 8,000 have also been tagged with frame sense information. 3 SRL System Description We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumo</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71– 106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT/NAACL-2004,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="2970" citStr="Pradhan et al., 2004" startWordPosition="464" endWordPosition="467">omatic syntactic parses is used. So far, most of the work on SRL systems has been focused on improving the labeling performance on a test set belonging to the same genre of text as the training set. Both the Treebank on which the syntactic parser is trained and the PropBank on which the SRL systems are trained represent articles from the year 1989 of the WSJ. While all these systems perform quite well on the WSJ test data, they show significant performance degradation (approximately 10 point drop in F-score) when applied to label test data that is different than the genre that WSJ represents (Pradhan et al., 2004; Carreras and M`arquez, 2005). 556 Proceedings of NAACL HLT 2007, pages 556–563, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics Surprisingly, it does not matter much whether the data is from another newswire, or a completely different type of text – as in the Brown corpus. These results indicate that the systems are being over-fit to the specific genre of text. Many performance improvements on the WSJ PropBank corpus may reflect tuning to the corpus. For the technology to be widely accepted and useful, it must be robust to change in genre of the data. Until recent</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky. 2004. Shallow semantic parsing using support vector machines. In Proceedings ofHLT/NAACL-2004, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Semantic role labeling using different syntactic views.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="6568" citStr="Pradhan et al., (2005)" startWordPosition="1060" endWordPosition="1063">the predicates – about 8,000 have also been tagged with frame sense information. 3 SRL System Description We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software. The system uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and, tolerance of the termination criterion, e=0.001. More details of this system can be found in Pradhan et al., (2005). The performance of this system on section 23 of the WSJ when trained on sections 02-21 is shown in Table 1 ALL ARDs Task P R F A (%) (%) (%) TREESANx Id. 97.5 96.1 96.8 Class. - - - 93.0 Id. + Class. 91.8 90.5 91.2 AUTOMATIC Id. 86.9 84.2 85.5 Class. - - - 92.0 Id. + Class. 82.1 77.9 79.9 Table 1: Performance of the SRL system on WSJ The performance of the SRL system is reported on three different tasks, all of which are with respect to a particular predicate: i) argument identification (ID), is the task of identifying the set of words (here, parse constituents) that represent a semantic rol</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky. 2005. Semantic role labeling using different syntactic views. In Proceedings of ACL-2005, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL-2003,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1628" citStr="Surdeanu et al., 2003" startWordPosition="239" endWordPosition="243">pus data. Our results indicate that while syntactic parses and argument identification port relatively well to a new genre, argument classification does not. Our analysis of the reasons for this is presented and generally point to the nature of the more lexical/semantic features dominating the classification task and general structural features dominating the argument identification task. 1 Introduction Automatic, accurate and wide-coverage techniques that can annotate naturally occurring text with semantic argument structure play a key role in NLP applications such as Information Extraction (Surdeanu et al., 2003; Harabagiu et al., 2005), Question Answering (Narayanan and Harabagiu, 2004) and Machine Translation (Boas, 2002; Chen and Fung, 2004). Semantic Role Labeling (SRL) is the process of producing such a markup. When presented with a sentence, a parser should, for each predicate in the sentence, identify and label the predicate’s semantic arguments. In recent work, a number of researchers have cast this problem as a tagging problem and have applied various supervised machine learning techniques to it. On the Wall Street Journal (WSJ) data, using correct syntactic parses, it is possible to achieve</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate-argument structures for information extraction. In Proceedings of ACL-2003, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>