<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000024">
<title confidence="0.975468">
Analyzing Dependencies of Japanese Subordinate Clauses
based on Statistics of Scope Embedding Preference
</title>
<author confidence="0.988375">
Takehito Utsuro, Shigeyuki Nishiokayama, Masakazu Fujio, Yuji Matsumoto
</author>
<affiliation confidence="0.974108">
Graduate School of Information Science, Nara Institute of Science and Technology
</affiliation>
<address confidence="0.856247">
8916-5, Takayama-cho, Ikoma-shi, Nara, 630-0101, JAPAN
</address>
<email confidence="0.999226">
E-mail: utsuro @is. aist-nara. ac.jp, URL: http://cl.aist-nara.ac.jprutsuro/
</email>
<sectionHeader confidence="0.995659" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999736444444445">
This paper proposes a statistical method for
learning dependency preference of Japanese
subordinate clauses, in which scope embedding
preference of subordinate clauses is exploited
as a useful information source for disambiguat-
ing dependencies between subordinate clauses.
Estimated dependencies of subordinate clauses
successfully increase the precision of an existing
statistical dependency analyzer.
</bodyText>
<sectionHeader confidence="0.998431" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99168540625">
In the Japanese language, since word order in a
sentence is relatively free compared with Euro-
pean languages, dependency analysis has been
shown to be practical and effective in both rule-
based and stochastic approaches to syntactic
analysis. In dependency analysis of a Japanese
sentence, among various source of ambiguities
in a sentence, dependency ambiguities of sub-
ordinate clauses are one of the most problem-
atic ones, partly because word order in a sen-
tence is relatively free. In general, dependency
ambiguities of subordinate clauses cause scope
ambiguities of subordinate clauses, which result
in enormous number of syntactic ambiguities of
other types of phrases such as noun phrases.1
1In our preliminary corpus analysis using the stochas-
tic dependency analyzer of Fujio and Matsumoto (1998),
about 30% of the 210,000 sentences in EDR bracketed
corpus (EDR, 1995) have dependency ambiguities of sub-
ordinate clauses, for which the precision of chunk (bun-
setsu) level dependencies is about 85.3% and that of sen-
tence level is about 25.4% (for best one) — 35.8% (for
best five), while for the rest 70% of EDR bracketed cor-
pus, the precision of chunk (bunsetsu) level dependencies
is about 86.7% and that of sentence level is about 47.5%
(for best one) 60.2% (for best five). In addition to
that, when assuming that those ambiguities of subor-
dinate clause dependencies are initially resolved in some
way, the chunk level precision increases to 90.4%, and the
sentence level precision to 40.6% (for best one) 67.7%
(for best five). This result of our preliminary analysis
In the Japanese linguistics, a theory of Mi-
nami (1974) regarding scope embedding pref-
erence of subordinate clauses is well-known.
Minami (1974) classifies Japanese subordinate
clauses according to the breadths of their scopes
and claim that subordinate clauses which inher-
ently have narrower scopes are embedded within
the scopes of subordinate clauses which inher-
ently have broader scopes (details are in sec-
tion 2). By manually analyzing several raw cor-
pora, Minami (1974) classifies various types of
Japanese subordinate clauses into three cate-
gories, which are totally ordered by the embed-
ding relation of their scopes. In the Japanese
computational linguistics community, Shirai et
al. (1995) employed Minami (1974)&apos;s theory on
scope embedding preference of Japanese sub-
ordinate clauses and applied it to rule-based
Japanese dependency analysis. However, in
their approach, since categories of subordinate
clauses are obtained by manually analyzing
a small number of sentences, their coverage
against a large corpus such as EDR bracketed
corpus (EDR, 1995) is quite low.2
In order to realize a broad coverage and high
performance dependency analysis of Japanese
sentences which exploits scope embedding pref-
erence of subordinate clauses, we propose a
corpus-based and statistical alternative to the
rule-based manual approach (section 3).3
clearly shows that dependency ambiguities of subordi-
nate clauses are among the most problematic source of
syntactic ambiguities in a Japanese sentence.
</bodyText>
<footnote confidence="0.993565777777778">
2In our implementation, the coverage of the categories
of Shirai et al. (1995) is only 30% for all the subordinate
clauses included in the whole EDR corpus.
3Previous works on statistical dependency analysis in-
clude Fujio and Matsumoto (1998) and Haruno et al.
(1998) in Japanese analysis as well as Lafferty et al.
(1992), Eisner (1996), and Collins (1996) in English anal-
ysis. In later sections, we discuss the advantages of our
approach over several closely related previous works.
</footnote>
<page confidence="0.998897">
110
</page>
<tableCaption confidence="0.999706">
Table 1: Word Segmentation, POS tagging, and Bunsetsu Segmentation of A Japanese Sentence
</tableCaption>
<table confidence="0.947045714285714">
Word Segmentation Tenki ga yoi kara dekakeyou
POS (+ conjugation form) noun case- adjective predicate- verb
Tagging particle (base) conjunctive-particle (volitional)
Bunsetsu Segmentation Tenki-ga yoi-kara dekakeyou
(Chunking)
English Translation weather subject fine because let&apos;s go out
(Because the weather is fine, let&apos;s go out.)
</table>
<bodyText confidence="0.9999205625">
First, we formalize the problem of decid-
ing scope embedding preference as a classifi-
cation problem, in which various types of lin-
guistic information of each subordinate clause
are encoded as features and used for deciding
which one of given two subordinate clauses has
a broader scope than the other. As in the case of
Shirai et al. (1995), we formalize the problem of
deciding dependency preference of subordinate
clauses by utilizing the correlation of scope em-
bedding preference and dependency preference
of Japanese subordinate clauses. Then, as a sta-
tistical learning method, we employ the decision
list learning method of Yarowsky (1994), where
optimal combination of those features are se-
lected and sorted in the form of decision rules,
according to the strength of correlation between
those features and the dependency preference
of the two subordinate clauses. We evaluate
the proposed method through the experiment
on learning dependency preference of Japanese
subordinate clauses from the EDR bracketed
corpus (section 4). We show that the pro-
posed method outperforms other related meth-
ods/models. We also evaluate the estimated de-
pendencies of subordinate clauses in Fujio and
Matsumoto (1998)&apos;s framework of the statisti-
cal dependency analysis of a whole sentence, in
which we successfully increase the precisions of
both chunk level and sentence level dependen-
cies thanks to the estimated dependencies of
sub ordinate clauses.
</bodyText>
<sectionHeader confidence="0.5102675" genericHeader="method">
2 Analyzing Dependencies between
Japanese Subordinate Clauses
based on Scope Embedding
Preference
</sectionHeader>
<subsectionHeader confidence="0.999257">
2.1 Dependency Analysis of A
Japanese Sentence
</subsectionHeader>
<bodyText confidence="0.9987515">
First, we overview dependency analysis of a
Japanese sentence. Since words in a Japanese
sentence are not segmented by explicit delim-
iters, input sentences are first word segmented,
</bodyText>
<figure confidence="0.6252495">
Phrase Structure
Scope of
Subordinate Clause
( (Tenki-ga) (yoi-kara) ) (dekakeyou) )
I t t
Dependency (modification) Relation
</figure>
<figureCaption confidence="0.970623">
Figure 1: An Example of Japanese Subordinate
Clause (taken from the Sentence of Table 1)
</figureCaption>
<bodyText confidence="0.99967">
part-of-speech tagged, and then chunked into a
sequence of segments called bunsetsus.4 Each
chunk (bunsetsu) generally consists of a set of
content words and function words. Then, de-
pendency relations among those chunks are es-
timated, where most practical dependency ana-
lyzers for the Japanese language usually assume
the following two constraints:
</bodyText>
<listItem confidence="0.955079">
1. Every chunk (bunsetsu) except the last one
modifies only one posterior chunk (bun-
setsu).
2. No modification crosses to other modifica-
tions in a sentence.
</listItem>
<bodyText confidence="0.998586777777778">
Table 1 gives an example of word segmenta-
tion, part-of-speech tagging, and bunsetsu seg-
mentation (chunking) of a Japanese sentence,
where the verb and the adjective are tagged
with their parts-of-speech as well as conjuga-
tion forms. Figure 1 shows the phrase structure,
the bracketing,5 and the dependency (modifica-
tion) relation of the chunks (bunsetsus) within
the sentence.
</bodyText>
<footnote confidence="0.999681125">
4Word segmentation and part-of-speech tagging are
performed by the Japanese morphological analyzer
Chasen (Matsumoto et al., 1997), and chunking is done
by the preprocessor used in Fujio and Matsumoto (1998).
&apos;The phrase structure and the bracketing are shown
just for explanation, and we do not consider them
but consider only dependency relations in the analysis
throughout this paper.
</footnote>
<page confidence="0.99454">
111
</page>
<figure confidence="0.994064923076923">
A Japanese subordinate clause is a clause whose head chunk satisfies the following properties.
1. The content words part of the chunk (bunsetsu) is one of the following types:
(a) A predicate (i.e., a verb or an adjective).
(b) nouns and a copula like &amp;quot;Nouni dearu&amp;quot; (in English, &amp;quot;be Nouni&amp;quot;)•
2. The function words part of the chunk (bunsetsu) is one of the following types:
(a) Null.
(b) Adverb type such as &amp;quot;Verbi ippou-de&amp;quot; (in English, &amp;quot;(subject) Verbi ..., on the other hand,&amp;quot;).
(c) Adverbial noun type such as &amp;quot;Verbi tame&amp;quot; (in English, &amp;quot;in order to Verbi&amp;quot;).
(d) Formal noun type such as &amp;quot;Verbi koto&amp;quot; (in English, gerund &amp;quot;Verbi-ing&amp;quot;).
(e) Temporal noun type such as &amp;quot;Verb&apos; mae&amp;quot; (in English, &amp;quot;before (subject) Verbi ...&amp;quot;).
(f) A predicate conjunctive particle such as &amp;quot;Verbi go&amp;quot; (in English, &amp;quot;although (subject) Verbi ...,11).
(g) A quoting particle such as &amp;quot;Verb&apos; to (iu)&amp;quot; (in English, &amp;quot;(say) that (subject) Verbi • • •&amp;quot;)•
(h) (a)—(g) followed by topic marking particles and/or sentence-final particles.
</figure>
<figureCaption confidence="0.999892">
Figure 2: Definition of Japanese Subordinate Clause
</figureCaption>
<subsectionHeader confidence="0.998875">
2.2 Japanese Subordinate Clause
</subsectionHeader>
<bodyText confidence="0.999952636363636">
The following gives the definition of what we call
a &amp;quot;Japanese subordinate clause&amp;quot; throughout this
paper. A clause in a sentence is represented as
a sequence of chunks. Since the Japanese lan-
guage is a head-final language, the clause head
is the final chunk in the sequence. A grammati-
cal definition of a Japanese subordinate clause is
given in Figure 2.6 For example, the Japanese
sentence in Table 1 has one subordinate clause,
whose scope is indicated as the shaded rectangle
in Figure 1.
</bodyText>
<subsectionHeader confidence="0.99981">
2.3 Scope Embedding Preference of
Subordinate Clauses
</subsectionHeader>
<bodyText confidence="0.982350055555556">
We introduce the concept of Minami (1974)&apos;s
classification of Japanese subordinate clauses
by describing the more specific classification by
Shirai et al. (1995). From 972 newspaper
summary sentences, Shirai et al. (1995) man-
ually extracted 54 clause final function words
of Japanese subordinate clauses and classified
them into the following three categories accord-
ing to the embedding relation of their scopes.
Category A: Seven expressions representing
simultaneous occurrences such as &amp;quot;Verbi
6This definition includes adnominal or noun phrase
modifying clauses &amp;quot;Clausei (N Pi)&amp;quot; (in English, rela-
tive clauses &amp;quot;(N.F).) that Clausei&amp;quot;). Since an adnom-
inal clause does not modify any posterior subordinate
clauses, but modifies a posterior noun phrase, we regard
adnominal clauses only as modifees when considering de-
pendencies between subordinate clauses.
</bodyText>
<construct confidence="0.9266614">
to-tomoni (Clause2)&amp;quot; and &amp;quot;Verb]. nagara
(Clause2)&amp;quot;.
Category B: 46 expressions representing
cause and discontinuity such as &amp;quot;Verb].
te (Clause2)&amp;quot; (in English &amp;quot;Verb]. and
(Clause2)&amp;quot;) and &amp;quot;V erbi node&amp;quot; (in English
&amp;quot;because (subject) Verb&apos; • • •,&amp;quot;)•
Category C: One expression representing in-
dependence, &amp;quot;Verb]. ga&amp;quot; (in English, &amp;quot;al-
though (subject) Verbi ...,&amp;quot;).
</construct>
<bodyText confidence="0.6247195">
The category A has the narrowest scope, while
the category C has the broadest scope, i.e.,
</bodyText>
<subsectionHeader confidence="0.602387">
Category A -‹ Category B -‹ Category C
</subsectionHeader>
<bodyText confidence="0.999931">
where the relation `-e denotes the embedding
relation of scopes of subordinate clauses. Then,
scope embedding preference of Japanese subor-
dinate clauses can be stated as below:
</bodyText>
<subsectionHeader confidence="0.9805985">
Scope Embedding Preference of
Japanese Subordinate Clauses
</subsectionHeader>
<listItem confidence="0.974258285714286">
1. A subordinate clause can be embedded within
the scope of another subordinate clause which
inherently has a scope of the same or a broader
breadth.
2. A subordinate clause can not be embedded
within the scope of another subordinate clause
which inherently has a narrower scope.
</listItem>
<bodyText confidence="0.929779">
For example, a subordinate clause of &apos;Category
B&apos; can be embedded within the scope of another
subordinate clause of &apos;Category B&apos; or &apos;Category
C&apos;, but not within that of &apos;Category A&apos;. Figure 3
</bodyText>
<page confidence="0.976786">
112
</page>
<figure confidence="0.9985335">
(a) Category A Category C
(b) Category C Category A
</figure>
<figureCaption confidence="0.996494">
Figure 3: Examples of Scope Embedding of Japanese Subordinate Clauses
</figureCaption>
<figure confidence="0.984829333333333">
Scopes of Subordinate Clauses
Category C
(kogete-shimai-mashita-.) )
Category A
( (kakimaze-nagara
(ni-mashita-ga-,)
I.
boil- polite/past-
although- comma
( Although I boiled it with stirring it up, it had got scorched.)
stir_up-with
scorch- perfect-polite/past-period
Scopes of Subordinate Clauses
Category C Category A
( ( (kogeru) (osore-ga) ) (ari-masu-ga-,) ) I( (tsuyobi-de) (kakimaze-nagara) (ni-m
hou-.) ) )
J• it
scorch fear- sbj exist- polite- hot_fire-over stir_up-with boil- polite
although- comma (volitional)-period
( Although there LI some fear of its getting scorched let&apos;s boil it with stirring it up over a hot fire. )
(a) gives an example of an anterior Japanese
</figure>
<bodyText confidence="0.9876584375">
subordinate clause ( &amp;quot;kakimaze-nagara&amp;quot;, Cate-
gory A), which is embedded within the scope
of a posterior one with a broader scope (&amp;quot;ni-
mashita-ga-,&amp;quot;, Category C). Since the poste-
rior subordinate clause inherently has a broader
scope than the anterior, the anterior is embed-
ded within the scope of the posterior. On the
other hand, Figure 3 (b) gives an example of
an anterior Japanese subordinate clause ( &amp;quot;ari-
masu-ga-,&amp;quot;, Category C), which is not embed-
ded within the scope of a posterior one with a
narrower scope (&amp;quot;kakimaze-nagara&amp;quot;, Category
A). Since the posterior subordinate clause in-
herently has a narrower scope than the anterior,
the anterior is not embedded within the scope
of the posterior.
</bodyText>
<subsectionHeader confidence="0.991996333333333">
2.4 Preference of Dependencies
between Subordinate Clauses based
on Scope Embedding Preference
</subsectionHeader>
<bodyText confidence="0.999949888888889">
Following the scope embedding preference of
Japanese subordinate clauses proposed by Mi-
nami (1974), Shirai et al. (1995) applied it
to rule-based Japanese dependency analysis,
and proposed the following preference of decid-
ing dependencies between subordinate clauses.
Suppose that a sentence has two subordinate
clauses Clausei and Clause2, where the head
vp chunk of Clausei precedes that of Clause2.
</bodyText>
<subsectionHeader confidence="0.963817">
Dependency Preference of Japanese
Subordinate Clauses
</subsectionHeader>
<listItem confidence="0.909029142857143">
1. The head vp chunk of Clause&apos; can modify that
of Clause2 if Clause2 inherently has a scope of
the same or a broader breadth compared with
that of Clause&apos;.
2. The head vp chunk of Clausei can not mod-
ify that of Clause2 if Clause2 inherently has a
narrower scope compared with that of Clausei.
</listItem>
<sectionHeader confidence="0.757424" genericHeader="method">
3 Learning Dependency Preference
</sectionHeader>
<subsectionHeader confidence="0.69799">
of Japanese Subordinate Clauses
</subsectionHeader>
<bodyText confidence="0.996884944444445">
As we mentioned in section 1, the rule-based
approach of Shirai et al. (1995) to analyz-
ing dependencies of subordinate clauses using
scope embedding preference has serious limi-
tation in its coverage against corpora of large
size for practical use. In order to overcome
the limitation of the rule-based approach, in
this section, we propose a method of learning
dependency preference of Japanese subordinate
clauses from a bracketed corpus. We formalize
the problem of deciding scope embedding pref-
erence as a classification problem, in which var-
ious types of linguistic information of each sub-
ordinate clause are encoded as features and used
for deciding which one of given two subordinate
clauses has a broader scope than the other. As
a statistical learning method, we employ the de-
cision list learning method of Yarowsky (1994).
</bodyText>
<page confidence="0.990934">
113
</page>
<table confidence="0.990887">
Table 2: Features of Japanese Subordinate Clauses
Feature Type # of Features Each Binary Feature
Punctuation 2 with-comma, without-comma
Grammatical 17 adverb, adverbial-noun, formal-noun, temporal-noun,
(some features have distinction quoting-particle, copula, predicate-conjunctive-particle,
of chunk-final/middle) topic-marking-particle, sentence-final-particle
Conjugation form of 12 stem, base, mizen, ren&apos;you, rental, conditional,
chunk-final conjugative word imperative, ta, tan, te, conjecture, volitional
Lexical (lexicalized forms of 235 adverb (e.g., ippou-de, irai), adverbial-noun (e.g., tame, baai)
&apos;Grammatical&apos; features, topic-marking-particle (e.g., ha, mo), quoting-particle (to),
with more than predicate-conjunctive-particle (e.g., ga, kora),
9 occurrences temporal-noun (e.g., ima, shunkan), formal-noun (e.g., koto),
in EDR corpus) copula (dearu), sentence-final-particle (e.g., ka, yo)
</table>
<subsectionHeader confidence="0.996065">
3.1 The Task Definition
</subsectionHeader>
<bodyText confidence="0.9976491">
Considering the dependency preference of
Japanese subordinate clauses described in sec-
tion 2.4, the following gives the definition of our
task of deciding the dependency of Japanese
subordinate clauses. Suppose that a sen-
tence has two subordinate clauses Clausei and
Clause2, where the head vp chunk of Clausei
precedes that of Clause2. Then, our task of de-
ciding the dependency of Japanese subordinate
clauses is to distinguish the following two cases:
</bodyText>
<listItem confidence="0.963577333333333">
1. The head vp chunk of Clausei modifies that of
Clause2.
2. The head vp chunk of Clause&apos; does not modify
that of Clause2, but modifies that of another
subordinate clause or the matrix clause which
follows Clause2.
</listItem>
<bodyText confidence="0.998303833333333">
Roughly speaking, the first corresponds to the
case where Clause2 inherently has a scope of the
same or a broader breadth compared with that
of Clausei, while the second corresponds to the
case where Clause2 inherently has a narrower
scope compared with that of Clause1.7
</bodyText>
<subsectionHeader confidence="0.999217">
3.2 Decision List Learning
</subsectionHeader>
<bodyText confidence="0.935111605263158">
A decision list (Yarowsky, 1994) is a sorted list
of the decision rules each of which decides the
value of a decision D given some evidence E.
Each decision rule in a decision list is sorted
7Our modeling is slightly different from those of other
standard approaches to statistical dependency analy-
sis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno
et al., 1998) which simply distinguish the two cases: the
case where dependency relation holds between the given
two vp chunks or clauses, and the case where dependency
relation does not hold. In contrast to those standard ap-
proaches, we ignore the case where the head vp chunk
of Clausei modifies that of another subordinate clause
which precedes Clause2. This is because we assume that
this case is more loosely related to the scope embedding
preference of subordinate clauses.
in descending order with respect to some pref-
erence value, and rules with higher preference
values are applied first when applying the deci-
sion list to some new test data.
First, let the random variable D represent-
ing a decision varies over several possible values,
and the random variable E representing some
evidence varies over &apos;1&apos; and &apos;0&apos; (where &apos;1&apos; de-
notes the presence of the corresponding piece
of evidence, &apos;0&apos; its absence). Then, given some
training data in which the correct value of the
decision D is annotated to each instance, the
conditional probabilities P(D =x I E =1) of ob-
serving the decision D = x under the condition
of the presence of the evidence E (E =1) are
calculated and the decision list is constructed
by the following procedure.
1. For each piece of evidence, calculate the likeli-
hood ratio of the conditional probability of a de-
cision D =x1 (given the presence of that piece
of evidence) to the conditional probability of
the rest of the decisions
</bodyText>
<equation confidence="0.999465666666667">
P(D=x1 I E=1)
log2
E=1)
</equation>
<bodyText confidence="0.990049857142857">
Then, a decision list is constructed with pieces
of evidence sorted in descending order with re-
spect to their likelihood ratios.8
2. The final line of a decision list is defined as &apos;a
default&apos;, where the likelihood ratio is calculated
as the ratio of the largest marginal probability
of the decision D = x1 to the marginal proba-
</bodyText>
<footnote confidence="0.9741955">
8Yarowsky (1994) discusses several techniques for
avoiding the problems which arise when an observed
count is 0. Among those techniques, we employ the sim-
plest one, i.e., adding a small constant a (0.1 &lt; a &lt;
0.25) to the numerator and denominator. With this
modification, more frequent evidence is preferred when
there exist several evidences for each of which the con-
ditional probability P(D= x I E1) equals to 1.
</footnote>
<page confidence="0.994834">
114
</page>
<table confidence="0.95468275">
(a) An Example Sentence with Chunking, Bracketing, and Dependency Relations
Subordinate Clauses (keesu-ga) ) (dete-kuru-darou-.) ) )
Clause 1 Clause2 f I &apos;ft
-
d r
(10%-nara) (neage-suru-ga-,) (3%-na-n e-,) I ( ( ( (tsui) (gyousha-hutan-toiu) )
I. Segl I Seg2
I
raise-price 3%- emphatic_auxiliay
10%-if -although in voluntary dealer-charge-of case- sbj happen-will/may- period
- comma _verb(te-form) -comma
(lithe tax rate is 10%, the dealers will raise price, but, because it is 3%, there will happen to be the cases that the dealers pay the tax.)
(b) Feature Expression of Head VP Chunk of Subordinate Clauses
Head VP Chunk of Subordinate Clause Feature Set
Segi : &amp;quot;neage-suru-ga-,&amp;quot; ..7-1 = 1 with-comma, predicate-conjunctive-particle(chunk-final),
Seg2 : &amp;quot;3%-na-node-,&amp;quot; predicate-conjunctive-particle(chunk-final)-&amp;quot;ga&amp;quot; 1
1.2 = { with-comma, chunk-final-conjugative-word-te-form }
(c) Evidence-Decision Pairs for Decision List Learning
Evidence E (E=1) (feature names are abbreviated) Decision D
F1 F2
with-comma with-comma &amp;quot;beyond&amp;quot;
with-comma te-form &amp;quot;beyond&amp;quot;
with-comma with-comma, te-form &amp;quot;beyond&amp;quot;
pred-conj-particle(final) with-comma &amp;quot;beyond&amp;quot;
... ... ...
with-comma, pred-conj-particle(final) with-comma &amp;quot;beyond&amp;quot;
... ... ...
pred-conj-particle(final)-&amp;quot;ga&amp;quot; with-comma &amp;quot;beyond&amp;quot;
... ... ...
with-comma, pred-conj-particle(final)-&amp;quot;ga&amp;quot; with-comma &amp;quot;beyond&amp;quot;
... ... ...
Fi
</table>
<bodyText confidence="0.747316">
bility of the rest of the decisions
</bodyText>
<equation confidence="0.942461">
log2 P(D=x1)
</equation>
<bodyText confidence="0.9945095">
The &apos;default&apos; decision of this final line is D= xi
with the largest marginal probability.
</bodyText>
<subsectionHeader confidence="0.999949">
3.3 Feature of Subordinate Clauses
</subsectionHeader>
<bodyText confidence="0.94548165">
Japanese subordinate clauses defined in sec-
tion 2.2 are encoded using the following four
types of features: i) Punctuation: represents
whether the head vp chunk of the subordinate
clause is marked with a comma or not, ii) Gram-
matical: represents parts-of-speech of function
words of the head vp chunk of the subordi-
nate clause,9 iii) Conjugation form of chunk-
9Terms of parts-of-speech tags and conjugation forms
are borrowed from those of the Japanese morphological
analysis system Chasen (Matsumoto et al., 1997).
final conjugative word: used when the chunk-
final word is conjugative, iv) Lexical: lexicalized
forms of &apos;Grammatical&apos; features which appear
more than 9 times in EDR corpus. Each fea-
ture of these four types is binary and its value
is &apos;1&apos; or &apos;0&apos; (&apos;1&apos; denotes the presence of the cor-
responding feature, &apos;0&apos; its absence). The whole
feature set shown in Table 2 is designed so as to
cover the 210,000 sentences of EDR corpus.
</bodyText>
<subsectionHeader confidence="0.999204666666667">
3.4 Decision List Learning of
Dependency Preference of
Subordinate Clauses
</subsectionHeader>
<bodyText confidence="0.999993333333333">
First, in the modeling of the evidence, we con-
sider every possible correlation (i.e., depen-
dency) of the features of the subordinate clauses
listed in section 3.3. Furthermore, since it is
necessary to consider the features for both of the
given two subordinate clauses, we consider all
</bodyText>
<page confidence="0.997845">
115
</page>
<bodyText confidence="0.999953058823529">
the possible combination of features of the an-
terior and posterior head vp chunks of the given
two subordinate clauses. More specifically, let
Segi and Seg2 be the head vp chunks of the
given two subordinate clauses (Seg, is the ante-
rior and Seg2 is the posterior). Also let F, and
.T2 be the sets of features which Sem. and Seg2
have, respectively (i.e., the values of these fea-
tures are &apos;1&apos;). We consider every possible subset
and F2 of ..T1 and F2, respectively, and then
model the evidence of the decision list learning
method as any possible pair (F1, F2).1°
Second, in the modeling of the decision, we
distinguish the two cases of dependency rela-
tions described in section 3.1. We name the first
case as the decision &amp;quot;modify&amp;quot;, while the second
as the decision &amp;quot;beyond&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.89083">
3.5 Example
</subsectionHeader>
<bodyText confidence="0.952550294117647">
Figure 4 illustrates an example of transforming
subordinate clauses into feature expression, and
then obtaining training pairs of an evidence and
a decision from a bracketed sentence. Figure 4
(a) shows an example sentence which contains
two subordinate clauses Clausei and Clause2,
with chunking, bracketing, and dependency re-
lations of chunks. Both of the head vp chunks
Sem. and Seg2 of Clause, and Clause2 modify
the sentence-final vp chunk. As shown in Fig-
ure 4 (b), the head vp chunks Sem. and Seg2
have feature sets .T1 and F2 respectively. Then,
every possible subsets F, and F2 of 1.1 and
F2 are considered,11 respectively, and training
pairs of an evidence and a decision are collected
as in Figure 4 (c). In this case, the value of the
decision D is &amp;quot;beyond&amp;quot;, because Seg, modifies
the sentence-final vp chunk, which follows Seg2.
100ur formalization of the evidence of decision list
learning has an advantage over the decision tree learn-
ing (Quinlan, 1993) approach to feature selection of de-
pendency analysis (Haruno et al., 1998). In the feature
selection procedure of the decision tree learning method,
the utility of each feature is evaluated independently,
and thus the utility of the combination of more than one
features is not evaluated directly. On the other hand, in
our formalization of the evidence of decision list learn-
ing, we consider every possible pair of the subsets F1 and
F2, and thus the utility of the combination of more than
one features is evaluated directly.
11Since the feature &apos;predicate-conjunctive-
particle(chunk-final)&apos; subsumes &apos;predicate-conjunctive-
particle(chunk-final)-&amp;quot;ga&amp;quot;, they are not considered
together as one evidence.
</bodyText>
<figure confidence="0.981365">
100
80
so
40
20
05 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Lower Bound of P(D(E)
</figure>
<figureCaption confidence="0.9556365">
Figure 5: Precisions and Coverages of Deciding
Dependency between Two Subordinate Clauses
</figureCaption>
<figure confidence="0.99892325">
100
95
90
85
80
75
0 20 40 60 80 100
Coverage (%)
</figure>
<figureCaption confidence="0.999969">
Figure 6: Correlation of Coverages and Precisions
</figureCaption>
<sectionHeader confidence="0.996533" genericHeader="evaluation">
4 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.99993785">
We divided the 210,000 sentences of the whole
EDR bracketed Japanese corpus into 95% train-
ing sentences and 5% test sentences. Then,
we extracted 162,443 pairs of subordinate
clauses from the 199,500 training sentences, and
learned a decision list for dependency prefer-
ence of subordinate clauses from those pairs.
The default decision in the decision list is
D =&amp;quot;beyond&amp;quot;, where the marginal probability
P(D = &amp;quot;beyond&amp;quot;) = 0.5378, i.e., the baseline
precision of deciding dependency between two
subordinate clauses is 53.78 %. We limit the fre-
quency of each evidence-decision pair to be more
than 9. The total number of obtained evidence-
decision pairs is 7,812. We evaluate the learned
decision list through several experiments.12
First, we apply the learned decision list to
deciding dependency between two subordinate
clauses of the 5% test sentences. We change
the threshold of the probability P(D E)13 in
</bodyText>
<footnote confidence="0.737032">
12Details of the experimental evaluation will be pre-
sented in Utsuro (2000).
13P(DI E) can be used equivalently to the likelihood
</footnote>
<table confidence="0.907736285714286">
Coverage (Our Model
Coverage (Model a)
Coverage (Model b)
Precision (Our Model
Precision (Model (a)
Precision (Model (b))-- -
...
</table>
<page confidence="0.997649">
116
</page>
<bodyText confidence="0.99998140625">
the decision list and plot the trade-off between
coverage and precision.14 As shown in the plot
of &amp;quot;Our Model&amp;quot; in Figure 5, the precision varies
from 78% to 100% according to the changes of
the threshold of the probability P(D E).
Next, we compare our model with the other
two models: (a) the model learned by apply-
ing the decision tree learning method of Haruno
et al. (1998) to our task of deciding depen-
dency between two subordinate clauses, and (b)
a decision list whose decisions are the following
two cases, i.e., the case where dependency rela-
tion holds between the given two vp chunks or
clauses, and the case where dependency relation
does not hold. The model (b) corresponds to a
model in which standard approaches to statis-
tical dependency analysis (Collins, 1996; Fujio
and Matsumoto, 1998; Haruno et al., 1998) are
applied to our task of deciding dependency be-
tween two subordinate clauses. Their results
are also in Figures 5 and 6. Figure 5 shows that
&amp;quot;Our Model&amp;quot; outperforms the other two mod-
els in coverage. Figure 6 shows that our model
outperforms both of the models (a) and (b) in
coverage and precision.
Finally, we examine whether the estimated
dependencies of subordinate clauses improve
the precision of Fujio and Matsumoto (1998)&apos;s
statistical dependency analyzer.15 Depending
on the threshold of P(D I E), we achieve
0.8,1.8% improvement in chunk level precision,
and 1.6-4.7% improvement in sentence leve1.16
</bodyText>
<sectionHeader confidence="0.999597" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99908796">
This paper proposed a statistical method for
learning dependency preference of Japanese
ratio.
14Coverage: the rate of the pairs of subordinate clauses
whose dependencies are decided by the decision list,
against the total pairs of subordinate clauses, Precision:
the rate of the pairs of subordinate clauses whose depen-
dencies are correctly decided by the decision list, against
those covered pairs of subordinate clauses.
15Fujio and Matsumoto (1998)&apos;s lexicalized depen-
dency analyzer is similar to that of Collins (1996), where
various features were evaluated through performance
test and an optimal feature set was manually selected.
16The upper bounds of the improvement in chunk level
and sentence level precisions, which are estimated by
providing Fujio and Matsumoto (1998)&apos;s statistical de-
pendency analyzer with correct dependencies of subor-
dinate clauses extracted from the bracketing of the EDR
corpus, are 5.1% and 15%, respectively.
subordinate clauses, in which scope embed-
ding preference of subordinate clauses is ex-
ploited. We evaluated the estimated dependen-
cies of subordinate clauses through several ex-
periments and showed that our model outper-
formed other related models.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996265625">
M. Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In Proceedings of the
34th Annual Meeting of ACL, pages 184-191.
EDR (Japan Electronic Dictionary Research Insti-
tute, Ltd.). 1995. EDR Electronic Dictionary
Technical Guide.
J. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceed-
ings of the 16th COLING, pages 340-345.
M. Fujio and Y. Matsumoto. 1998. Japanese de-
pendency structure analysis based on lexicalized
statistics. In Proceedings of the 3rd Conference on
Empirical Methods in Natural Language Process-
ing, pages 88-96.
M. Haruno, S. Shirai, and Y. Oyama. 1998. Us-
ing decision trees to construct a practical parser.
In Proceedings of the 17th COLING and the 36th
Annual Meeting of ACL, pages 505-511.
J. Lafferty, D. Sleator, and D. Temperley. 1992.
Grammatical trigrams: A probabilistic model of
link grammar. In Proceedings of the AAAI Fall
Symposium: Probabilistic Approaches to Natural
Language, pages 89-97.
Y. Matsumoto, A. Kitauchi, T. Yamashita,
0. Imaichi, and T. Imamura. 1997. Japanese
morphological analyzer ChaSen 1.0 users manual.
Information Science Technical Report NAIST-IS-
TR97007, Nara Institute of Science and Technol-
ogy. (in Japanese).
F. Minami. 1974. Gendai Nihongo no Kouzou.
Taishuukan Shoten. (in Japanese).
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
S. Shirai, S. Ikehara, A. Yokoo, and J. Kimura. 1995.
A new dependency analysis method based on
semantically embedded sentence structures and
its performance on Japanese subordinate clauses.
Transactions of Information Processing Society of
Japan, 36(10):2353-2361. (in Japanese).
T. Utsuro. 2000. Learning preference of depen-
dency between Japanese subordinate clauses and
its evaluation in parsing. In Proceedings of the 2nd
International Conference on Language Resources
and Evaluation. (to appear).
D. Yarowsky. 1994. Decision lists for lexical ambi-
guity resolution: Application to accent restora-
tion in Spanish and French. In Proceedings of the
32nd Annual Meeting of ACL, pages 88-95.
</reference>
<page confidence="0.998104">
117
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.364157">
<title confidence="0.9940225">Analyzing Dependencies of Japanese Subordinate Clauses based on Statistics of Scope Embedding Preference</title>
<author confidence="0.993954">Takehito Utsuro</author>
<author confidence="0.993954">Shigeyuki Nishiokayama</author>
<author confidence="0.993954">Masakazu Fujio</author>
<author confidence="0.993954">Yuji Matsumoto</author>
<affiliation confidence="0.9984">Graduate School of Information Science, Nara Institute of Science and Technology</affiliation>
<address confidence="0.986541">Nara, JAPAN</address>
<email confidence="0.377163">@is.aist-nara.ac.jp,</email>
<abstract confidence="0.9990895">This paper proposes a statistical method for learning dependency preference of Japanese subordinate clauses, in which scope embedding preference of subordinate clauses is exploited as a useful information source for disambiguating dependencies between subordinate clauses. Estimated dependencies of subordinate clauses successfully increase the precision of an existing statistical dependency analyzer.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of ACL,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="4249" citStr="Collins (1996)" startWordPosition="633" endWordPosition="634">s, we propose a corpus-based and statistical alternative to the rule-based manual approach (section 3).3 clearly shows that dependency ambiguities of subordinate clauses are among the most problematic source of syntactic ambiguities in a Japanese sentence. 2In our implementation, the coverage of the categories of Shirai et al. (1995) is only 30% for all the subordinate clauses included in the whole EDR corpus. 3Previous works on statistical dependency analysis include Fujio and Matsumoto (1998) and Haruno et al. (1998) in Japanese analysis as well as Lafferty et al. (1992), Eisner (1996), and Collins (1996) in English analysis. In later sections, we discuss the advantages of our approach over several closely related previous works. 110 Table 1: Word Segmentation, POS tagging, and Bunsetsu Segmentation of A Japanese Sentence Word Segmentation Tenki ga yoi kara dekakeyou POS (+ conjugation form) noun case- adjective predicate- verb Tagging particle (base) conjunctive-particle (volitional) Bunsetsu Segmentation Tenki-ga yoi-kara dekakeyou (Chunking) English Translation weather subject fine because let&apos;s go out (Because the weather is fine, let&apos;s go out.) First, we formalize the problem of deciding </context>
<context position="17222" citStr="Collins, 1996" startWordPosition="2620" endWordPosition="2621">ughly speaking, the first corresponds to the case where Clause2 inherently has a scope of the same or a broader breadth compared with that of Clausei, while the second corresponds to the case where Clause2 inherently has a narrower scope compared with that of Clause1.7 3.2 Decision List Learning A decision list (Yarowsky, 1994) is a sorted list of the decision rules each of which decides the value of a decision D given some evidence E. Each decision rule in a decision list is sorted 7Our modeling is slightly different from those of other standard approaches to statistical dependency analysis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998) which simply distinguish the two cases: the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold. In contrast to those standard approaches, we ignore the case where the head vp chunk of Clausei modifies that of another subordinate clause which precedes Clause2. This is because we assume that this case is more loosely related to the scope embedding preference of subordinate clauses. in descending order with respect to some preference value, and rules with higher pre</context>
<context position="27044" citStr="Collins, 1996" startWordPosition="4211" endWordPosition="4212"> according to the changes of the threshold of the probability P(D E). Next, we compare our model with the other two models: (a) the model learned by applying the decision tree learning method of Haruno et al. (1998) to our task of deciding dependency between two subordinate clauses, and (b) a decision list whose decisions are the following two cases, i.e., the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold. The model (b) corresponds to a model in which standard approaches to statistical dependency analysis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998) are applied to our task of deciding dependency between two subordinate clauses. Their results are also in Figures 5 and 6. Figure 5 shows that &amp;quot;Our Model&amp;quot; outperforms the other two models in coverage. Figure 6 shows that our model outperforms both of the models (a) and (b) in coverage and precision. Finally, we examine whether the estimated dependencies of subordinate clauses improve the precision of Fujio and Matsumoto (1998)&apos;s statistical dependency analyzer.15 Depending on the threshold of P(D I E), we achieve 0.8,1.8% improvement in chunk l</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>M. Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Annual Meeting of ACL, pages 184-191.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>EDR Electronic Dictionary Technical Guide.</booktitle>
<institution>EDR (Japan Electronic Dictionary Research Institute, Ltd.).</institution>
<contexts>
<context position="3092" citStr="(1995)" startWordPosition="459" endWordPosition="459"> of subordinate clauses is well-known. Minami (1974) classifies Japanese subordinate clauses according to the breadths of their scopes and claim that subordinate clauses which inherently have narrower scopes are embedded within the scopes of subordinate clauses which inherently have broader scopes (details are in section 2). By manually analyzing several raw corpora, Minami (1974) classifies various types of Japanese subordinate clauses into three categories, which are totally ordered by the embedding relation of their scopes. In the Japanese computational linguistics community, Shirai et al. (1995) employed Minami (1974)&apos;s theory on scope embedding preference of Japanese subordinate clauses and applied it to rule-based Japanese dependency analysis. However, in their approach, since categories of subordinate clauses are obtained by manually analyzing a small number of sentences, their coverage against a large corpus such as EDR bracketed corpus (EDR, 1995) is quite low.2 In order to realize a broad coverage and high performance dependency analysis of Japanese sentences which exploits scope embedding preference of subordinate clauses, we propose a corpus-based and statistical alternative </context>
<context position="5144" citStr="(1995)" startWordPosition="771" endWordPosition="771">rm) noun case- adjective predicate- verb Tagging particle (base) conjunctive-particle (volitional) Bunsetsu Segmentation Tenki-ga yoi-kara dekakeyou (Chunking) English Translation weather subject fine because let&apos;s go out (Because the weather is fine, let&apos;s go out.) First, we formalize the problem of deciding scope embedding preference as a classification problem, in which various types of linguistic information of each subordinate clause are encoded as features and used for deciding which one of given two subordinate clauses has a broader scope than the other. As in the case of Shirai et al. (1995), we formalize the problem of deciding dependency preference of subordinate clauses by utilizing the correlation of scope embedding preference and dependency preference of Japanese subordinate clauses. Then, as a statistical learning method, we employ the decision list learning method of Yarowsky (1994), where optimal combination of those features are selected and sorted in the form of decision rules, according to the strength of correlation between those features and the dependency preference of the two subordinate clauses. We evaluate the proposed method through the experiment on learning de</context>
<context position="9861" citStr="(1995)" startWordPosition="1502" endWordPosition="1502">per. A clause in a sentence is represented as a sequence of chunks. Since the Japanese language is a head-final language, the clause head is the final chunk in the sequence. A grammatical definition of a Japanese subordinate clause is given in Figure 2.6 For example, the Japanese sentence in Table 1 has one subordinate clause, whose scope is indicated as the shaded rectangle in Figure 1. 2.3 Scope Embedding Preference of Subordinate Clauses We introduce the concept of Minami (1974)&apos;s classification of Japanese subordinate clauses by describing the more specific classification by Shirai et al. (1995). From 972 newspaper summary sentences, Shirai et al. (1995) manually extracted 54 clause final function words of Japanese subordinate clauses and classified them into the following three categories according to the embedding relation of their scopes. Category A: Seven expressions representing simultaneous occurrences such as &amp;quot;Verbi 6This definition includes adnominal or noun phrase modifying clauses &amp;quot;Clausei (N Pi)&amp;quot; (in English, relative clauses &amp;quot;(N.F).) that Clausei&amp;quot;). Since an adnominal clause does not modify any posterior subordinate clauses, but modifies a posterior noun phrase, we regard</context>
<context position="13474" citStr="(1995)" startWordPosition="2058" endWordPosition="2058"> On the other hand, Figure 3 (b) gives an example of an anterior Japanese subordinate clause ( &amp;quot;arimasu-ga-,&amp;quot;, Category C), which is not embedded within the scope of a posterior one with a narrower scope (&amp;quot;kakimaze-nagara&amp;quot;, Category A). Since the posterior subordinate clause inherently has a narrower scope than the anterior, the anterior is not embedded within the scope of the posterior. 2.4 Preference of Dependencies between Subordinate Clauses based on Scope Embedding Preference Following the scope embedding preference of Japanese subordinate clauses proposed by Minami (1974), Shirai et al. (1995) applied it to rule-based Japanese dependency analysis, and proposed the following preference of deciding dependencies between subordinate clauses. Suppose that a sentence has two subordinate clauses Clausei and Clause2, where the head vp chunk of Clausei precedes that of Clause2. Dependency Preference of Japanese Subordinate Clauses 1. The head vp chunk of Clause&apos; can modify that of Clause2 if Clause2 inherently has a scope of the same or a broader breadth compared with that of Clause&apos;. 2. The head vp chunk of Clausei can not modify that of Clause2 if Clause2 inherently has a narrower scope c</context>
</contexts>
<marker>1995</marker>
<rawString>EDR (Japan Electronic Dictionary Research Institute, Ltd.). 1995. EDR Electronic Dictionary Technical Guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th COLING,</booktitle>
<pages>340--345</pages>
<contexts>
<context position="4229" citStr="Eisner (1996)" startWordPosition="630" endWordPosition="631"> subordinate clauses, we propose a corpus-based and statistical alternative to the rule-based manual approach (section 3).3 clearly shows that dependency ambiguities of subordinate clauses are among the most problematic source of syntactic ambiguities in a Japanese sentence. 2In our implementation, the coverage of the categories of Shirai et al. (1995) is only 30% for all the subordinate clauses included in the whole EDR corpus. 3Previous works on statistical dependency analysis include Fujio and Matsumoto (1998) and Haruno et al. (1998) in Japanese analysis as well as Lafferty et al. (1992), Eisner (1996), and Collins (1996) in English analysis. In later sections, we discuss the advantages of our approach over several closely related previous works. 110 Table 1: Word Segmentation, POS tagging, and Bunsetsu Segmentation of A Japanese Sentence Word Segmentation Tenki ga yoi kara dekakeyou POS (+ conjugation form) noun case- adjective predicate- verb Tagging particle (base) conjunctive-particle (volitional) Bunsetsu Segmentation Tenki-ga yoi-kara dekakeyou (Chunking) English Translation weather subject fine because let&apos;s go out (Because the weather is fine, let&apos;s go out.) First, we formalize the </context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th COLING, pages 340-345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fujio</author>
<author>Y Matsumoto</author>
</authors>
<title>Japanese dependency structure analysis based on lexicalized statistics.</title>
<date>1998</date>
<booktitle>In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>88--96</pages>
<contexts>
<context position="1619" citStr="Fujio and Matsumoto (1998)" startWordPosition="219" endWordPosition="222">n both rulebased and stochastic approaches to syntactic analysis. In dependency analysis of a Japanese sentence, among various source of ambiguities in a sentence, dependency ambiguities of subordinate clauses are one of the most problematic ones, partly because word order in a sentence is relatively free. In general, dependency ambiguities of subordinate clauses cause scope ambiguities of subordinate clauses, which result in enormous number of syntactic ambiguities of other types of phrases such as noun phrases.1 1In our preliminary corpus analysis using the stochastic dependency analyzer of Fujio and Matsumoto (1998), about 30% of the 210,000 sentences in EDR bracketed corpus (EDR, 1995) have dependency ambiguities of subordinate clauses, for which the precision of chunk (bunsetsu) level dependencies is about 85.3% and that of sentence level is about 25.4% (for best one) — 35.8% (for best five), while for the rest 70% of EDR bracketed corpus, the precision of chunk (bunsetsu) level dependencies is about 86.7% and that of sentence level is about 47.5% (for best one) 60.2% (for best five). In addition to that, when assuming that those ambiguities of subordinate clause dependencies are initially resolved in </context>
<context position="4134" citStr="Fujio and Matsumoto (1998)" startWordPosition="611" endWordPosition="614"> and high performance dependency analysis of Japanese sentences which exploits scope embedding preference of subordinate clauses, we propose a corpus-based and statistical alternative to the rule-based manual approach (section 3).3 clearly shows that dependency ambiguities of subordinate clauses are among the most problematic source of syntactic ambiguities in a Japanese sentence. 2In our implementation, the coverage of the categories of Shirai et al. (1995) is only 30% for all the subordinate clauses included in the whole EDR corpus. 3Previous works on statistical dependency analysis include Fujio and Matsumoto (1998) and Haruno et al. (1998) in Japanese analysis as well as Lafferty et al. (1992), Eisner (1996), and Collins (1996) in English analysis. In later sections, we discuss the advantages of our approach over several closely related previous works. 110 Table 1: Word Segmentation, POS tagging, and Bunsetsu Segmentation of A Japanese Sentence Word Segmentation Tenki ga yoi kara dekakeyou POS (+ conjugation form) noun case- adjective predicate- verb Tagging particle (base) conjunctive-particle (volitional) Bunsetsu Segmentation Tenki-ga yoi-kara dekakeyou (Chunking) English Translation weather subject </context>
<context position="6010" citStr="Fujio and Matsumoto (1998)" startWordPosition="897" endWordPosition="900">, we employ the decision list learning method of Yarowsky (1994), where optimal combination of those features are selected and sorted in the form of decision rules, according to the strength of correlation between those features and the dependency preference of the two subordinate clauses. We evaluate the proposed method through the experiment on learning dependency preference of Japanese subordinate clauses from the EDR bracketed corpus (section 4). We show that the proposed method outperforms other related methods/models. We also evaluate the estimated dependencies of subordinate clauses in Fujio and Matsumoto (1998)&apos;s framework of the statistical dependency analysis of a whole sentence, in which we successfully increase the precisions of both chunk level and sentence level dependencies thanks to the estimated dependencies of sub ordinate clauses. 2 Analyzing Dependencies between Japanese Subordinate Clauses based on Scope Embedding Preference 2.1 Dependency Analysis of A Japanese Sentence First, we overview dependency analysis of a Japanese sentence. Since words in a Japanese sentence are not segmented by explicit delimiters, input sentences are first word segmented, Phrase Structure Scope of Subordinate</context>
<context position="7878" citStr="Fujio and Matsumoto (1998)" startWordPosition="1179" endWordPosition="1182">ses to other modifications in a sentence. Table 1 gives an example of word segmentation, part-of-speech tagging, and bunsetsu segmentation (chunking) of a Japanese sentence, where the verb and the adjective are tagged with their parts-of-speech as well as conjugation forms. Figure 1 shows the phrase structure, the bracketing,5 and the dependency (modification) relation of the chunks (bunsetsus) within the sentence. 4Word segmentation and part-of-speech tagging are performed by the Japanese morphological analyzer Chasen (Matsumoto et al., 1997), and chunking is done by the preprocessor used in Fujio and Matsumoto (1998). &apos;The phrase structure and the bracketing are shown just for explanation, and we do not consider them but consider only dependency relations in the analysis throughout this paper. 111 A Japanese subordinate clause is a clause whose head chunk satisfies the following properties. 1. The content words part of the chunk (bunsetsu) is one of the following types: (a) A predicate (i.e., a verb or an adjective). (b) nouns and a copula like &amp;quot;Nouni dearu&amp;quot; (in English, &amp;quot;be Nouni&amp;quot;)• 2. The function words part of the chunk (bunsetsu) is one of the following types: (a) Null. (b) Adverb type such as &amp;quot;Verbi </context>
<context position="17249" citStr="Fujio and Matsumoto, 1998" startWordPosition="2622" endWordPosition="2625"> the first corresponds to the case where Clause2 inherently has a scope of the same or a broader breadth compared with that of Clausei, while the second corresponds to the case where Clause2 inherently has a narrower scope compared with that of Clause1.7 3.2 Decision List Learning A decision list (Yarowsky, 1994) is a sorted list of the decision rules each of which decides the value of a decision D given some evidence E. Each decision rule in a decision list is sorted 7Our modeling is slightly different from those of other standard approaches to statistical dependency analysis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998) which simply distinguish the two cases: the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold. In contrast to those standard approaches, we ignore the case where the head vp chunk of Clausei modifies that of another subordinate clause which precedes Clause2. This is because we assume that this case is more loosely related to the scope embedding preference of subordinate clauses. in descending order with respect to some preference value, and rules with higher preference values are applied </context>
<context position="27071" citStr="Fujio and Matsumoto, 1998" startWordPosition="4213" endWordPosition="4216">he changes of the threshold of the probability P(D E). Next, we compare our model with the other two models: (a) the model learned by applying the decision tree learning method of Haruno et al. (1998) to our task of deciding dependency between two subordinate clauses, and (b) a decision list whose decisions are the following two cases, i.e., the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold. The model (b) corresponds to a model in which standard approaches to statistical dependency analysis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998) are applied to our task of deciding dependency between two subordinate clauses. Their results are also in Figures 5 and 6. Figure 5 shows that &amp;quot;Our Model&amp;quot; outperforms the other two models in coverage. Figure 6 shows that our model outperforms both of the models (a) and (b) in coverage and precision. Finally, we examine whether the estimated dependencies of subordinate clauses improve the precision of Fujio and Matsumoto (1998)&apos;s statistical dependency analyzer.15 Depending on the threshold of P(D I E), we achieve 0.8,1.8% improvement in chunk level precision, and 1.6-4.7</context>
</contexts>
<marker>Fujio, Matsumoto, 1998</marker>
<rawString>M. Fujio and Y. Matsumoto. 1998. Japanese dependency structure analysis based on lexicalized statistics. In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing, pages 88-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Haruno</author>
<author>S Shirai</author>
<author>Y Oyama</author>
</authors>
<title>Using decision trees to construct a practical parser.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th COLING and the 36th Annual Meeting of ACL,</booktitle>
<pages>505--511</pages>
<contexts>
<context position="4159" citStr="Haruno et al. (1998)" startWordPosition="616" endWordPosition="619">y analysis of Japanese sentences which exploits scope embedding preference of subordinate clauses, we propose a corpus-based and statistical alternative to the rule-based manual approach (section 3).3 clearly shows that dependency ambiguities of subordinate clauses are among the most problematic source of syntactic ambiguities in a Japanese sentence. 2In our implementation, the coverage of the categories of Shirai et al. (1995) is only 30% for all the subordinate clauses included in the whole EDR corpus. 3Previous works on statistical dependency analysis include Fujio and Matsumoto (1998) and Haruno et al. (1998) in Japanese analysis as well as Lafferty et al. (1992), Eisner (1996), and Collins (1996) in English analysis. In later sections, we discuss the advantages of our approach over several closely related previous works. 110 Table 1: Word Segmentation, POS tagging, and Bunsetsu Segmentation of A Japanese Sentence Word Segmentation Tenki ga yoi kara dekakeyou POS (+ conjugation form) noun case- adjective predicate- verb Tagging particle (base) conjunctive-particle (volitional) Bunsetsu Segmentation Tenki-ga yoi-kara dekakeyou (Chunking) English Translation weather subject fine because let&apos;s go out</context>
<context position="17271" citStr="Haruno et al., 1998" startWordPosition="2626" endWordPosition="2629">he case where Clause2 inherently has a scope of the same or a broader breadth compared with that of Clausei, while the second corresponds to the case where Clause2 inherently has a narrower scope compared with that of Clause1.7 3.2 Decision List Learning A decision list (Yarowsky, 1994) is a sorted list of the decision rules each of which decides the value of a decision D given some evidence E. Each decision rule in a decision list is sorted 7Our modeling is slightly different from those of other standard approaches to statistical dependency analysis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998) which simply distinguish the two cases: the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold. In contrast to those standard approaches, we ignore the case where the head vp chunk of Clausei modifies that of another subordinate clause which precedes Clause2. This is because we assume that this case is more loosely related to the scope embedding preference of subordinate clauses. in descending order with respect to some preference value, and rules with higher preference values are applied first when applying th</context>
<context position="24167" citStr="Haruno et al., 1998" startWordPosition="3740" endWordPosition="3743">ntence-final vp chunk. As shown in Figure 4 (b), the head vp chunks Sem. and Seg2 have feature sets .T1 and F2 respectively. Then, every possible subsets F, and F2 of 1.1 and F2 are considered,11 respectively, and training pairs of an evidence and a decision are collected as in Figure 4 (c). In this case, the value of the decision D is &amp;quot;beyond&amp;quot;, because Seg, modifies the sentence-final vp chunk, which follows Seg2. 100ur formalization of the evidence of decision list learning has an advantage over the decision tree learning (Quinlan, 1993) approach to feature selection of dependency analysis (Haruno et al., 1998). In the feature selection procedure of the decision tree learning method, the utility of each feature is evaluated independently, and thus the utility of the combination of more than one features is not evaluated directly. On the other hand, in our formalization of the evidence of decision list learning, we consider every possible pair of the subsets F1 and F2, and thus the utility of the combination of more than one features is evaluated directly. 11Since the feature &apos;predicate-conjunctiveparticle(chunk-final)&apos; subsumes &apos;predicate-conjunctiveparticle(chunk-final)-&amp;quot;ga&amp;quot;, they are not considere</context>
<context position="26646" citStr="Haruno et al. (1998)" startWordPosition="4143" endWordPosition="4146">erimental evaluation will be presented in Utsuro (2000). 13P(DI E) can be used equivalently to the likelihood Coverage (Our Model Coverage (Model a) Coverage (Model b) Precision (Our Model Precision (Model (a) Precision (Model (b))-- - ... 116 the decision list and plot the trade-off between coverage and precision.14 As shown in the plot of &amp;quot;Our Model&amp;quot; in Figure 5, the precision varies from 78% to 100% according to the changes of the threshold of the probability P(D E). Next, we compare our model with the other two models: (a) the model learned by applying the decision tree learning method of Haruno et al. (1998) to our task of deciding dependency between two subordinate clauses, and (b) a decision list whose decisions are the following two cases, i.e., the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold. The model (b) corresponds to a model in which standard approaches to statistical dependency analysis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998) are applied to our task of deciding dependency between two subordinate clauses. Their results are also in Figures 5 and 6. Figure 5 shows that &amp;quot;Our Mode</context>
</contexts>
<marker>Haruno, Shirai, Oyama, 1998</marker>
<rawString>M. Haruno, S. Shirai, and Y. Oyama. 1998. Using decision trees to construct a practical parser. In Proceedings of the 17th COLING and the 36th Annual Meeting of ACL, pages 505-511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>D Sleator</author>
<author>D Temperley</author>
</authors>
<title>Grammatical trigrams: A probabilistic model of link grammar.</title>
<date>1992</date>
<booktitle>In Proceedings of the AAAI Fall Symposium: Probabilistic Approaches to Natural Language,</booktitle>
<pages>89--97</pages>
<contexts>
<context position="4214" citStr="Lafferty et al. (1992)" startWordPosition="626" endWordPosition="629"> embedding preference of subordinate clauses, we propose a corpus-based and statistical alternative to the rule-based manual approach (section 3).3 clearly shows that dependency ambiguities of subordinate clauses are among the most problematic source of syntactic ambiguities in a Japanese sentence. 2In our implementation, the coverage of the categories of Shirai et al. (1995) is only 30% for all the subordinate clauses included in the whole EDR corpus. 3Previous works on statistical dependency analysis include Fujio and Matsumoto (1998) and Haruno et al. (1998) in Japanese analysis as well as Lafferty et al. (1992), Eisner (1996), and Collins (1996) in English analysis. In later sections, we discuss the advantages of our approach over several closely related previous works. 110 Table 1: Word Segmentation, POS tagging, and Bunsetsu Segmentation of A Japanese Sentence Word Segmentation Tenki ga yoi kara dekakeyou POS (+ conjugation form) noun case- adjective predicate- verb Tagging particle (base) conjunctive-particle (volitional) Bunsetsu Segmentation Tenki-ga yoi-kara dekakeyou (Chunking) English Translation weather subject fine because let&apos;s go out (Because the weather is fine, let&apos;s go out.) First, we</context>
</contexts>
<marker>Lafferty, Sleator, Temperley, 1992</marker>
<rawString>J. Lafferty, D. Sleator, and D. Temperley. 1992. Grammatical trigrams: A probabilistic model of link grammar. In Proceedings of the AAAI Fall Symposium: Probabilistic Approaches to Natural Language, pages 89-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsumoto</author>
<author>A Kitauchi</author>
<author>T Yamashita</author>
</authors>
<title>Japanese morphological analyzer ChaSen 1.0 users manual. Information Science</title>
<date>1997</date>
<tech>Technical Report NAIST-ISTR97007,</tech>
<institution>Nara Institute of Science and Technology.</institution>
<note>(in Japanese).</note>
<contexts>
<context position="7801" citStr="Matsumoto et al., 1997" startWordPosition="1166" endWordPosition="1169"> one modifies only one posterior chunk (bunsetsu). 2. No modification crosses to other modifications in a sentence. Table 1 gives an example of word segmentation, part-of-speech tagging, and bunsetsu segmentation (chunking) of a Japanese sentence, where the verb and the adjective are tagged with their parts-of-speech as well as conjugation forms. Figure 1 shows the phrase structure, the bracketing,5 and the dependency (modification) relation of the chunks (bunsetsus) within the sentence. 4Word segmentation and part-of-speech tagging are performed by the Japanese morphological analyzer Chasen (Matsumoto et al., 1997), and chunking is done by the preprocessor used in Fujio and Matsumoto (1998). &apos;The phrase structure and the bracketing are shown just for explanation, and we do not consider them but consider only dependency relations in the analysis throughout this paper. 111 A Japanese subordinate clause is a clause whose head chunk satisfies the following properties. 1. The content words part of the chunk (bunsetsu) is one of the following types: (a) A predicate (i.e., a verb or an adjective). (b) nouns and a copula like &amp;quot;Nouni dearu&amp;quot; (in English, &amp;quot;be Nouni&amp;quot;)• 2. The function words part of the chunk (bunse</context>
<context position="21547" citStr="Matsumoto et al., 1997" startWordPosition="3295" endWordPosition="3298">ecision of this final line is D= xi with the largest marginal probability. 3.3 Feature of Subordinate Clauses Japanese subordinate clauses defined in section 2.2 are encoded using the following four types of features: i) Punctuation: represents whether the head vp chunk of the subordinate clause is marked with a comma or not, ii) Grammatical: represents parts-of-speech of function words of the head vp chunk of the subordinate clause,9 iii) Conjugation form of chunk9Terms of parts-of-speech tags and conjugation forms are borrowed from those of the Japanese morphological analysis system Chasen (Matsumoto et al., 1997). final conjugative word: used when the chunkfinal word is conjugative, iv) Lexical: lexicalized forms of &apos;Grammatical&apos; features which appear more than 9 times in EDR corpus. Each feature of these four types is binary and its value is &apos;1&apos; or &apos;0&apos; (&apos;1&apos; denotes the presence of the corresponding feature, &apos;0&apos; its absence). The whole feature set shown in Table 2 is designed so as to cover the 210,000 sentences of EDR corpus. 3.4 Decision List Learning of Dependency Preference of Subordinate Clauses First, in the modeling of the evidence, we consider every possible correlation (i.e., dependency) of t</context>
</contexts>
<marker>Matsumoto, Kitauchi, Yamashita, 1997</marker>
<rawString>Y. Matsumoto, A. Kitauchi, T. Yamashita, 0. Imaichi, and T. Imamura. 1997. Japanese morphological analyzer ChaSen 1.0 users manual. Information Science Technical Report NAIST-ISTR97007, Nara Institute of Science and Technology. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Minami</author>
</authors>
<title>Gendai Nihongo no Kouzou. Taishuukan Shoten.</title>
<date>1974</date>
<note>(in Japanese).</note>
<contexts>
<context position="2449" citStr="Minami (1974)" startWordPosition="361" endWordPosition="363">ntence level is about 25.4% (for best one) — 35.8% (for best five), while for the rest 70% of EDR bracketed corpus, the precision of chunk (bunsetsu) level dependencies is about 86.7% and that of sentence level is about 47.5% (for best one) 60.2% (for best five). In addition to that, when assuming that those ambiguities of subordinate clause dependencies are initially resolved in some way, the chunk level precision increases to 90.4%, and the sentence level precision to 40.6% (for best one) 67.7% (for best five). This result of our preliminary analysis In the Japanese linguistics, a theory of Minami (1974) regarding scope embedding preference of subordinate clauses is well-known. Minami (1974) classifies Japanese subordinate clauses according to the breadths of their scopes and claim that subordinate clauses which inherently have narrower scopes are embedded within the scopes of subordinate clauses which inherently have broader scopes (details are in section 2). By manually analyzing several raw corpora, Minami (1974) classifies various types of Japanese subordinate clauses into three categories, which are totally ordered by the embedding relation of their scopes. In the Japanese computational </context>
<context position="9741" citStr="Minami (1974)" startWordPosition="1485" endWordPosition="1486">panese Subordinate Clause The following gives the definition of what we call a &amp;quot;Japanese subordinate clause&amp;quot; throughout this paper. A clause in a sentence is represented as a sequence of chunks. Since the Japanese language is a head-final language, the clause head is the final chunk in the sequence. A grammatical definition of a Japanese subordinate clause is given in Figure 2.6 For example, the Japanese sentence in Table 1 has one subordinate clause, whose scope is indicated as the shaded rectangle in Figure 1. 2.3 Scope Embedding Preference of Subordinate Clauses We introduce the concept of Minami (1974)&apos;s classification of Japanese subordinate clauses by describing the more specific classification by Shirai et al. (1995). From 972 newspaper summary sentences, Shirai et al. (1995) manually extracted 54 clause final function words of Japanese subordinate clauses and classified them into the following three categories according to the embedding relation of their scopes. Category A: Seven expressions representing simultaneous occurrences such as &amp;quot;Verbi 6This definition includes adnominal or noun phrase modifying clauses &amp;quot;Clausei (N Pi)&amp;quot; (in English, relative clauses &amp;quot;(N.F).) that Clausei&amp;quot;). Sinc</context>
<context position="13452" citStr="Minami (1974)" startWordPosition="2052" endWordPosition="2054">n the scope of the posterior. On the other hand, Figure 3 (b) gives an example of an anterior Japanese subordinate clause ( &amp;quot;arimasu-ga-,&amp;quot;, Category C), which is not embedded within the scope of a posterior one with a narrower scope (&amp;quot;kakimaze-nagara&amp;quot;, Category A). Since the posterior subordinate clause inherently has a narrower scope than the anterior, the anterior is not embedded within the scope of the posterior. 2.4 Preference of Dependencies between Subordinate Clauses based on Scope Embedding Preference Following the scope embedding preference of Japanese subordinate clauses proposed by Minami (1974), Shirai et al. (1995) applied it to rule-based Japanese dependency analysis, and proposed the following preference of deciding dependencies between subordinate clauses. Suppose that a sentence has two subordinate clauses Clausei and Clause2, where the head vp chunk of Clausei precedes that of Clause2. Dependency Preference of Japanese Subordinate Clauses 1. The head vp chunk of Clause&apos; can modify that of Clause2 if Clause2 inherently has a scope of the same or a broader breadth compared with that of Clause&apos;. 2. The head vp chunk of Clausei can not modify that of Clause2 if Clause2 inherently </context>
</contexts>
<marker>Minami, 1974</marker>
<rawString>F. Minami. 1974. Gendai Nihongo no Kouzou. Taishuukan Shoten. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="24092" citStr="Quinlan, 1993" startWordPosition="3730" endWordPosition="3731">the head vp chunks Sem. and Seg2 of Clause, and Clause2 modify the sentence-final vp chunk. As shown in Figure 4 (b), the head vp chunks Sem. and Seg2 have feature sets .T1 and F2 respectively. Then, every possible subsets F, and F2 of 1.1 and F2 are considered,11 respectively, and training pairs of an evidence and a decision are collected as in Figure 4 (c). In this case, the value of the decision D is &amp;quot;beyond&amp;quot;, because Seg, modifies the sentence-final vp chunk, which follows Seg2. 100ur formalization of the evidence of decision list learning has an advantage over the decision tree learning (Quinlan, 1993) approach to feature selection of dependency analysis (Haruno et al., 1998). In the feature selection procedure of the decision tree learning method, the utility of each feature is evaluated independently, and thus the utility of the combination of more than one features is not evaluated directly. On the other hand, in our formalization of the evidence of decision list learning, we consider every possible pair of the subsets F1 and F2, and thus the utility of the combination of more than one features is evaluated directly. 11Since the feature &apos;predicate-conjunctiveparticle(chunk-final)&apos; subsum</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shirai</author>
<author>S Ikehara</author>
<author>A Yokoo</author>
<author>J Kimura</author>
</authors>
<title>A new dependency analysis method based on semantically embedded sentence structures and its performance on Japanese subordinate clauses.</title>
<date>1995</date>
<journal>Transactions of Information Processing Society of Japan,</journal>
<pages>36--10</pages>
<note>(in Japanese).</note>
<contexts>
<context position="3092" citStr="Shirai et al. (1995)" startWordPosition="456" endWordPosition="459">ing preference of subordinate clauses is well-known. Minami (1974) classifies Japanese subordinate clauses according to the breadths of their scopes and claim that subordinate clauses which inherently have narrower scopes are embedded within the scopes of subordinate clauses which inherently have broader scopes (details are in section 2). By manually analyzing several raw corpora, Minami (1974) classifies various types of Japanese subordinate clauses into three categories, which are totally ordered by the embedding relation of their scopes. In the Japanese computational linguistics community, Shirai et al. (1995) employed Minami (1974)&apos;s theory on scope embedding preference of Japanese subordinate clauses and applied it to rule-based Japanese dependency analysis. However, in their approach, since categories of subordinate clauses are obtained by manually analyzing a small number of sentences, their coverage against a large corpus such as EDR bracketed corpus (EDR, 1995) is quite low.2 In order to realize a broad coverage and high performance dependency analysis of Japanese sentences which exploits scope embedding preference of subordinate clauses, we propose a corpus-based and statistical alternative </context>
<context position="5144" citStr="Shirai et al. (1995)" startWordPosition="768" endWordPosition="771">conjugation form) noun case- adjective predicate- verb Tagging particle (base) conjunctive-particle (volitional) Bunsetsu Segmentation Tenki-ga yoi-kara dekakeyou (Chunking) English Translation weather subject fine because let&apos;s go out (Because the weather is fine, let&apos;s go out.) First, we formalize the problem of deciding scope embedding preference as a classification problem, in which various types of linguistic information of each subordinate clause are encoded as features and used for deciding which one of given two subordinate clauses has a broader scope than the other. As in the case of Shirai et al. (1995), we formalize the problem of deciding dependency preference of subordinate clauses by utilizing the correlation of scope embedding preference and dependency preference of Japanese subordinate clauses. Then, as a statistical learning method, we employ the decision list learning method of Yarowsky (1994), where optimal combination of those features are selected and sorted in the form of decision rules, according to the strength of correlation between those features and the dependency preference of the two subordinate clauses. We evaluate the proposed method through the experiment on learning de</context>
<context position="9861" citStr="Shirai et al. (1995)" startWordPosition="1499" endWordPosition="1502">ughout this paper. A clause in a sentence is represented as a sequence of chunks. Since the Japanese language is a head-final language, the clause head is the final chunk in the sequence. A grammatical definition of a Japanese subordinate clause is given in Figure 2.6 For example, the Japanese sentence in Table 1 has one subordinate clause, whose scope is indicated as the shaded rectangle in Figure 1. 2.3 Scope Embedding Preference of Subordinate Clauses We introduce the concept of Minami (1974)&apos;s classification of Japanese subordinate clauses by describing the more specific classification by Shirai et al. (1995). From 972 newspaper summary sentences, Shirai et al. (1995) manually extracted 54 clause final function words of Japanese subordinate clauses and classified them into the following three categories according to the embedding relation of their scopes. Category A: Seven expressions representing simultaneous occurrences such as &amp;quot;Verbi 6This definition includes adnominal or noun phrase modifying clauses &amp;quot;Clausei (N Pi)&amp;quot; (in English, relative clauses &amp;quot;(N.F).) that Clausei&amp;quot;). Since an adnominal clause does not modify any posterior subordinate clauses, but modifies a posterior noun phrase, we regard</context>
<context position="13474" citStr="Shirai et al. (1995)" startWordPosition="2055" endWordPosition="2058">the posterior. On the other hand, Figure 3 (b) gives an example of an anterior Japanese subordinate clause ( &amp;quot;arimasu-ga-,&amp;quot;, Category C), which is not embedded within the scope of a posterior one with a narrower scope (&amp;quot;kakimaze-nagara&amp;quot;, Category A). Since the posterior subordinate clause inherently has a narrower scope than the anterior, the anterior is not embedded within the scope of the posterior. 2.4 Preference of Dependencies between Subordinate Clauses based on Scope Embedding Preference Following the scope embedding preference of Japanese subordinate clauses proposed by Minami (1974), Shirai et al. (1995) applied it to rule-based Japanese dependency analysis, and proposed the following preference of deciding dependencies between subordinate clauses. Suppose that a sentence has two subordinate clauses Clausei and Clause2, where the head vp chunk of Clausei precedes that of Clause2. Dependency Preference of Japanese Subordinate Clauses 1. The head vp chunk of Clause&apos; can modify that of Clause2 if Clause2 inherently has a scope of the same or a broader breadth compared with that of Clause&apos;. 2. The head vp chunk of Clausei can not modify that of Clause2 if Clause2 inherently has a narrower scope c</context>
</contexts>
<marker>Shirai, Ikehara, Yokoo, Kimura, 1995</marker>
<rawString>S. Shirai, S. Ikehara, A. Yokoo, and J. Kimura. 1995. A new dependency analysis method based on semantically embedded sentence structures and its performance on Japanese subordinate clauses. Transactions of Information Processing Society of Japan, 36(10):2353-2361. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Utsuro</author>
</authors>
<title>Learning preference of dependency between Japanese subordinate clauses and its evaluation in parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation.</booktitle>
<note>(to appear).</note>
<contexts>
<context position="26081" citStr="Utsuro (2000)" startWordPosition="4047" endWordPosition="4048">&amp;quot;, where the marginal probability P(D = &amp;quot;beyond&amp;quot;) = 0.5378, i.e., the baseline precision of deciding dependency between two subordinate clauses is 53.78 %. We limit the frequency of each evidence-decision pair to be more than 9. The total number of obtained evidencedecision pairs is 7,812. We evaluate the learned decision list through several experiments.12 First, we apply the learned decision list to deciding dependency between two subordinate clauses of the 5% test sentences. We change the threshold of the probability P(D E)13 in 12Details of the experimental evaluation will be presented in Utsuro (2000). 13P(DI E) can be used equivalently to the likelihood Coverage (Our Model Coverage (Model a) Coverage (Model b) Precision (Our Model Precision (Model (a) Precision (Model (b))-- - ... 116 the decision list and plot the trade-off between coverage and precision.14 As shown in the plot of &amp;quot;Our Model&amp;quot; in Figure 5, the precision varies from 78% to 100% according to the changes of the threshold of the probability P(D E). Next, we compare our model with the other two models: (a) the model learned by applying the decision tree learning method of Haruno et al. (1998) to our task of deciding dependency</context>
</contexts>
<marker>Utsuro, 2000</marker>
<rawString>T. Utsuro. 2000. Learning preference of dependency between Japanese subordinate clauses and its evaluation in parsing. In Proceedings of the 2nd International Conference on Language Resources and Evaluation. (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of ACL,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="5448" citStr="Yarowsky (1994)" startWordPosition="814" endWordPosition="815">lem of deciding scope embedding preference as a classification problem, in which various types of linguistic information of each subordinate clause are encoded as features and used for deciding which one of given two subordinate clauses has a broader scope than the other. As in the case of Shirai et al. (1995), we formalize the problem of deciding dependency preference of subordinate clauses by utilizing the correlation of scope embedding preference and dependency preference of Japanese subordinate clauses. Then, as a statistical learning method, we employ the decision list learning method of Yarowsky (1994), where optimal combination of those features are selected and sorted in the form of decision rules, according to the strength of correlation between those features and the dependency preference of the two subordinate clauses. We evaluate the proposed method through the experiment on learning dependency preference of Japanese subordinate clauses from the EDR bracketed corpus (section 4). We show that the proposed method outperforms other related methods/models. We also evaluate the estimated dependencies of subordinate clauses in Fujio and Matsumoto (1998)&apos;s framework of the statistical depend</context>
<context position="14999" citStr="Yarowsky (1994)" startWordPosition="2302" endWordPosition="2303">of large size for practical use. In order to overcome the limitation of the rule-based approach, in this section, we propose a method of learning dependency preference of Japanese subordinate clauses from a bracketed corpus. We formalize the problem of deciding scope embedding preference as a classification problem, in which various types of linguistic information of each subordinate clause are encoded as features and used for deciding which one of given two subordinate clauses has a broader scope than the other. As a statistical learning method, we employ the decision list learning method of Yarowsky (1994). 113 Table 2: Features of Japanese Subordinate Clauses Feature Type # of Features Each Binary Feature Punctuation 2 with-comma, without-comma Grammatical 17 adverb, adverbial-noun, formal-noun, temporal-noun, (some features have distinction quoting-particle, copula, predicate-conjunctive-particle, of chunk-final/middle) topic-marking-particle, sentence-final-particle Conjugation form of 12 stem, base, mizen, ren&apos;you, rental, conditional, chunk-final conjugative word imperative, ta, tan, te, conjecture, volitional Lexical (lexicalized forms of 235 adverb (e.g., ippou-de, irai), adverbial-noun </context>
<context position="16938" citStr="Yarowsky, 1994" startWordPosition="2571" endWordPosition="2572">anese subordinate clauses is to distinguish the following two cases: 1. The head vp chunk of Clausei modifies that of Clause2. 2. The head vp chunk of Clause&apos; does not modify that of Clause2, but modifies that of another subordinate clause or the matrix clause which follows Clause2. Roughly speaking, the first corresponds to the case where Clause2 inherently has a scope of the same or a broader breadth compared with that of Clausei, while the second corresponds to the case where Clause2 inherently has a narrower scope compared with that of Clause1.7 3.2 Decision List Learning A decision list (Yarowsky, 1994) is a sorted list of the decision rules each of which decides the value of a decision D given some evidence E. Each decision rule in a decision list is sorted 7Our modeling is slightly different from those of other standard approaches to statistical dependency analysis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998) which simply distinguish the two cases: the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold. In contrast to those standard approaches, we ignore the case where the head vp chunk of</context>
<context position="19084" citStr="Yarowsky (1994)" startWordPosition="2939" endWordPosition="2941">y the following procedure. 1. For each piece of evidence, calculate the likelihood ratio of the conditional probability of a decision D =x1 (given the presence of that piece of evidence) to the conditional probability of the rest of the decisions P(D=x1 I E=1) log2 E=1) Then, a decision list is constructed with pieces of evidence sorted in descending order with respect to their likelihood ratios.8 2. The final line of a decision list is defined as &apos;a default&apos;, where the likelihood ratio is calculated as the ratio of the largest marginal probability of the decision D = x1 to the marginal proba8Yarowsky (1994) discusses several techniques for avoiding the problems which arise when an observed count is 0. Among those techniques, we employ the simplest one, i.e., adding a small constant a (0.1 &lt; a &lt; 0.25) to the numerator and denominator. With this modification, more frequent evidence is preferred when there exist several evidences for each of which the conditional probability P(D= x I E1) equals to 1. 114 (a) An Example Sentence with Chunking, Bracketing, and Dependency Relations Subordinate Clauses (keesu-ga) ) (dete-kuru-darou-.) ) ) Clause 1 Clause2 f I &apos;ft - d r (10%-nara) (neage-suru-ga-,) (3%-</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>D. Yarowsky. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of ACL, pages 88-95.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>