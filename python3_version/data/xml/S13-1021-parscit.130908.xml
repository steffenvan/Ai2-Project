<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000312">
<title confidence="0.992287">
MayoClinicNLP–CORE: Semantic representations for textual similarity
</title>
<author confidence="0.999181">
Stephen Wu Dongqing Zhu &amp; Ben Carterette Hongfang Liu
</author>
<affiliation confidence="0.99119">
Mayo Clinic University of Delaware Mayo Clinic
</affiliation>
<address confidence="0.808454">
Rochester, MN 55905 Newark, DE 19716 Rochester, MN 55905
</address>
<email confidence="0.999424">
wu.stephen@mayo.edu {zhu,carteret}@cis.udel.edu liu.hongfang@mayo.edu
</email>
<sectionHeader confidence="0.998251" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9331137">
The Semantic Textual Similarity (STS) task
examines semantic similarity at a sentence-
level. We explored three representations of
semantics (implicit or explicit): named enti-
ties, semantic vectors, and structured vectorial
semantics. From a DKPro baseline, we also
performed feature selection and used source-
specific linear regression models to combine
our features. Our systems placed 5th, 6th, and
8th among 90 submitted systems.
</bodyText>
<sectionHeader confidence="0.999283" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999837875">
The Semantic Textual Similarity (STS) task (Agirre
et al., 2012; Agirre et al., 2013) examines semantic
similarity at a sentence-level. While much work has
compared the semantics of terms, concepts, or doc-
uments, this space has been relatively unexplored.
The 2013 STS task provided sentence pairs and a
0–5 human rating of their similarity, with training
data from 5 sources and test data from 4 sources.
We sought to explore and evaluate the usefulness
of several semantic representations that have had
recent significance in research or practice. First,
information extraction (IE) methods often implic-
itly consider named entities as ad hoc semantic rep-
resentations, for example, in the clinical domain.
Therefore, we sought to evaluate similarity based on
named entity-based features. Second, in many appli-
cations, an effective means of incorporating distri-
butional semantics is Random Indexing (RI). Thus
we consider three different representations possi-
ble within Random Indexing (Kanerva et al., 2000;
Sahlgren, 2005). Finally, because compositional
distributional semantics is an important research
topic (Mitchell and Lapata, 2008; Erk and Pad´o,
2008), we sought to evaluate a principled compo-
sition strategy: structured vectorial semantics (Wu
and Schuler, 2011).
The remainder of this paper proceeds as follows.
Section 2 overviews our similarity metrics, and Sec-
tion 3 overviews the systems that were defined on
these metrics. Competition results and additional
analyses are in Section 4. We end with discussion
on the results in Section 5.
</bodyText>
<sectionHeader confidence="0.99077" genericHeader="method">
2 Similarity measures
</sectionHeader>
<bodyText confidence="0.999973818181818">
Because we expect semantic similarity to be multi-
layered, we expect that we will need many similar-
ity measures to approximate human similarity judg-
ments. Rather than reinvent the wheel, we have cho-
sen to introduce features that complement existing
successful feature sets. We utilized 17 features from
DKPro Similarity and 21 features from TakeLab,
i.e., the two top-performing systems in the 2012 STS
task, as a solid baseline.
These are summarized in Table 1. We introduce 3
categories of new similarity metrics, 9 metrics in all.
</bodyText>
<subsectionHeader confidence="0.980235">
2.1 Named entity measures
</subsectionHeader>
<bodyText confidence="0.999077875">
Named entity recognition provides a common ap-
proximation of semantic content for the informa-
tion extraction perspective. We define three simple
similarity metrics based on named entities. First,
we computed the named entity overlap (exact string
matches) between the two sentences, where NEk
was the set of named entities found in sentence
Sk. This is the harmonic mean of how closely S1
</bodyText>
<page confidence="0.961625">
148
</page>
<note confidence="0.969672">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 148–154, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
</note>
<tableCaption confidence="0.82349">
Table 1: Full feature pool in MayoClinicNLP systems. The proposed MayoClinicNLP metrics are meant to comple-
ment DKPro (B¨ar et al., 2012) and TakeLab (ˇSari´c et al., 2012) metrics.
DKPro metrics (17) TakeLab metrics (21) Custom MayoClinicNLP metrics (9)
</tableCaption>
<bodyText confidence="0.447133714285714">
n-grams/WordNGramContainmentMeasure 1 stopword-filtered t ngram/UnigramOverlap
n-grams/WordNGramContainmentMeasure 2 stopword-filtered t ngram/BigramOverlap
n-grams/WordNGramJaccardMeasure 1 t ngram/TrigramOverlap
n-grams/WordNGramJaccardMeasure 2 stopword-filtered t ngram/ContentUnigramOverlap
n-grams/WordNGramJaccardMeasure 3 t ngram/ContentBigramOverlap
n-grams/WordNGramJaccardMeasure 4 t ngram/ContentTrigramOverlap
n-grams/WordNGramJaccardMeasure 4 stopword-filtered
</bodyText>
<equation confidence="0.645848166666667">
t words/WeightedWordOverlap custom/StanfordNerMeasure overlap.txt
t words/GreedyLemmaAligningOverlap custom/StanfordNerMeasure aligngst.txt
t words/WordNetAugmentedWordOverlap custom/StanfordNerMeasure alignlcs.txt
esa/ESA Wiktionary t vec/LSAWordSimilarity NYT custom/SVSePhrSimilarityMeasure.txt
esa/ESA WordNet t vec/LSAWordSimilarity weighted NYT custom/SVSeTopSimilarityMeasure.txt
t vec/LSAWordSimilarity weighted Wiki custom/SemanticVectorsSimilarityMeasure d200 wr0.txt
</equation>
<bodyText confidence="0.409957333333333">
custom/SemanticVectorsSimilarityMeasure d200 wr6b.txt
custom/SemanticVectorsSimilarityMeasure d200 wr6d.txt
custom/SemanticVectorsSimilarityMeasure d200 wr6p.txt
</bodyText>
<footnote confidence="0.480122285714286">
n-grams/CharacterNGramMeasure 2 t other/RelativeLengthDifference
n-grams/CharacterNGramMeasure 3 t other/RelativeInfoContentDifference
n-grams/CharacterNGramMeasure 4 t other/NumbersSize
string/GreedyStringTiling 3 t other/NumbersOverlap
string/LongestCommonSubsequenceComparator t other/NumbersSubset
string/LongestCommonSubsequenceNormComparator t other/SentenceSize
string/LongestCommonSubstringComparator t other/CaseMatches
</footnote>
<equation confidence="0.9807984">
t other/StocksSize
t other/StocksOverlap
matches 52, and how closely 52 matches 51:
simneo(51, 52) = 2 ⋅ INE1 n NE2I (1)
INE1 I + INE2I
</equation>
<bodyText confidence="0.999941692307692">
Additionally, we relax the constraint of requiring
exact string matches between the two sentences by
using the longest common subsequence (Allison and
Dix, 1986) and greedy string tiling (Wise, 1996) al-
gorithms. These metrics give similarities between
two strings, rather than two sets of strings as we
have with NE1 and NE2. Thus, we follow previ-
ous work in greedily aligning these named entities
(Lavie and Denkowski, 2009; ˇSari´c et al., 2012) into
pairs. Namely, we compare each pair (nei,1, nej,2)
of named entity strings in NE1 and NE2. The
highest-scoring pair is entered into a set of pairs, P.
Then, the next highest pair is added to P if neither
named entity is already in P, and discarded other-
wise; this continues until there are no more named
entities in either NE1 or NE2.
We then define two named entity aligning mea-
sures that use the longest common subsequence
(LCS) and greedy string tiling (GST) fuzzy string
matching algorithms:
where f(⋅) is either the LCS or GST algorithm.
In our experiments, we performed named entity
recognition with the Stanford NER tool using the
standard English model (Finkel et al., 2005). Also,
we used UKP’s existing implementation of LCS and
GST (ˇSari´c et al., 2012) for the latter two measures.
</bodyText>
<subsectionHeader confidence="0.997818">
2.2 Random indexing measures
</subsectionHeader>
<bodyText confidence="0.9997993">
Random indexing (Kanerva et al., 2000; Sahlgren,
2005) is another distributional semantics framework
for representing terms as vectors. Similar to LSA
(Deerwester et al., 1990), an index is created that
represents each term as a semantic vector. But
in random indexing, each term is represented by
an elemental vector et with a small number of
randomly-generated non-zero components. The in-
tuition for this means of dimensionality reduction is
that these randomly-generated elemental vectors are
like quasi-orthogonal bases in a traditional geomet-
ric semantic space, rather than, e.g., 300 fully or-
thogonal dimensions from singular value decompo-
sition (Landauer and Dumais, 1997). For a standard
model with random indexing, a contextual term vec-
tor ct,std is the the sum of the elemental vectors cor-
responding to tokens in the document. All contexts
for a particular term are summed and normalized to
produce a final term vector vt,std.
Other notions of context can be incorporated into
</bodyText>
<equation confidence="0.999003333333333">
E f(ne1, ne2)
simnea(51, 52) = (ne1,nee)∈P
max (INE1I, INE2I) (2)
</equation>
<page confidence="0.99053">
149
</page>
<bodyText confidence="0.999831">
this model. Local co-occurrence context can be ac-
counted for in a basic sliding-window model by con-
sidering words within some window radius r (in-
stead of a whole document). Each instance of the
term t will have a contextual vector ct,win = et_r +
... + et_1 + et+1 + ... + et+r; context vectors for each
instance (in a large corpus) would again be added
and normalized to create the overall vector vt,win.
A directional model doubles the dimensionality of
the vector and considers left- and right-context sepa-
rately (half the indices for left-context, half for right-
context), using a permutation to achieve one of the
two contexts. A permutated positional model uses a
position-specific permutation function to encode the
relative word positions (rather than just left- or right-
context) separately. Again, vt would be summed
and normalized over all instances of ct.
Sentence vectors from any of these 4 Random
Indexing-based models (standard, windowed, direc-
tional, positional) are just the sum of the vectors for
each term vS = Et∈S vt. We define 4 separate simi-
larity metrics for STS as:
</bodyText>
<equation confidence="0.961479">
simRI(S1, S2) = cos(vS1, vS2) (3)
</equation>
<bodyText confidence="0.999945888888889">
We used the semantic vectors package (Widdows
and Ferraro, 2008; Widdows and Cohen, 2010) in
the default configuration for the standard model. For
the windowed, directional, and positional models,
we used a 6-word window radius with 200 dimen-
sions and a seed length of 5. All models were
trained on the raw text of the Penn Treebank Wall
Street Journal corpus and a 100,075-article subset of
Wikipedia.
</bodyText>
<subsectionHeader confidence="0.999791">
2.3 Semantic vectorial semantics measures
</subsectionHeader>
<bodyText confidence="0.999436857142857">
Structured vectorial semantics (SVS) composes dis-
tributional semantic representations in syntactic
context (Wu and Schuler, 2011). Similarity met-
rics defined with SVS inherently explore the quali-
ties of a fully interactive syntax–semantics interface.
While previous work evaluated the syntactic contri-
butions of this model, the STS task allows us to eval-
uate the phrase-level semantic validity of the model.
We summarize SVS here as bottom-up vector com-
position and parsing, then continue on to define the
associated similarity metrics.
Each token in a sentence is modeled generatively
as a vector eγ of latent referents iγ in syntactic con-
text cγ; each element in the vector is defined as:
</bodyText>
<equation confidence="0.971497">
eγ[iγ] = P(xγ I lciγ), for preterm -y (4)
</equation>
<bodyText confidence="0.998881333333333">
where lγ is a constant for preterminals.
We write SVS vector composition between two
word (or phrase) vectors in linear algebra form,1 as-
suming that we are composing the semantics of two
children eα and eβ in a binary syntactic tree into
their parent eγ:
</bodyText>
<equation confidence="0.966287">
eγ = M O (Lγxα • eα) O (Lγxβ • eβ) • 1 (5)
</equation>
<bodyText confidence="0.998359368421053">
M is a diagonal matrix that encapsulates probabilis-
tic syntactic information; the L matrices are linear
transformations that capture how semantically rele-
vant child vectors are to the resulting vector (e.g.,
Lγxα defines the the relevance of eα to eγ). These
matrices are defined such that the resulting eγ is a
semantic vector of consistent P(xγ I lciγ) probabil-
ities. Further detail is in our previous work (Wu,
2010; Wu and Schuler, 2011).
Similarity metrics can be defined in the SVS
space by comparing the distributions of the com-
posed eγ vectors — i.e., our similarity metric is
a comparison of the vector semantics at different
phrasal nodes. We define two measures, one cor-
responding to the top node cp (e.g., with a syntactic
constituent cp = ‘S’), and one corresponding to the
left and right largest child nodes (e.g.,, c∠ = ‘NP’
and c= ‘VP’ for a canonical subject–verb–object
sentence in English).
</bodyText>
<equation confidence="0.999958">
simsvs-top(S1,S2) =cos(ep(S1),ep(S2)) (6)
simsvs-phr(S1, S2) = max(
avgsim(e∠(S1), e∠(S2); e(S1), e(S2)),
avgsim(e∠(S1),e(S2);e(S1),e∠(S2))) (7)
</equation>
<bodyText confidence="0.9975916">
where avgsim() is the harmonic mean of the co-
sine similarities between the two pairs of arguments.
Top-level similarity comparisons in (6) amounts to
comparing the semantics of a whole sentence. The
phrasal similarity function simsvs-phr(S1, S2) in (7)
thus seeks to semantically align the two largest sub-
trees, and weight them. Compared to simsvs-top,
1We define the operator ⊙ as point-by-point multiplication
of two diagonal matrices and 1 as a column vector of ones, col-
lapsing a diagonal matrix onto a column vector.
</bodyText>
<page confidence="0.990227">
150
</page>
<bodyText confidence="0.999860375">
the phrasal similarity function simsvs-phr(51, 52) as-
sumes there might be some information captured in
the child nodes that could be lost in the final compo-
sition to the top node.
In our experiments, we used the parser described
in Wu and Schuler (2011) with 1,000 headwords
and 10 relational clusters, trained on the Wall Street
Journal treebank.
</bodyText>
<sectionHeader confidence="0.995426" genericHeader="method">
3 Feature combination framework
</sectionHeader>
<bodyText confidence="0.999583">
The similarity metrics of Section 2 were calculated
for each of the sentence pairs in the training set, and
later the test set. In combining these metrics, we ex-
tended a DKPro Similarity baseline (3.1) with fea-
ture selection (3.2) and source-specific models and
classification (3.3).
</bodyText>
<subsectionHeader confidence="0.992647">
3.1 Linear regression via DKPro Similarity
</subsectionHeader>
<bodyText confidence="0.999980333333333">
For our baseline (MayoClinicNLPr1wtCDT), we
used the UIMA-based DKPro Similarity system
from STS 2012 (B¨ar et al., 2012). Aside from the
large number of sound similarity measures, this pro-
vided linear regression through the WEKA package
(Hall et al., 2009) to combine all of the disparate
similarity metrics into a single one, and some pre-
processing. Regression weights were determined on
the whole training set for each source.
</bodyText>
<subsectionHeader confidence="0.997957">
3.2 Feature selection
</subsectionHeader>
<bodyText confidence="0.999955">
Not every feature was included in the final linear re-
gression models. To determine the best of the 47
(DKPro–17, TakeLab–21, MayoClinicNLP–9) fea-
tures, we performed a full forward-search on the
space of similarity measures. In forward-search, we
perform 10-fold cross-validation on the training set
for each measure, and pick the best one; in the next
round, that best metric is retained, and the remaining
metrics are considered for addition. Rounds con-
tinue until all the features are exhausted, though a
stopping-point is noted when performance no longer
increases.
</bodyText>
<subsectionHeader confidence="0.7697285">
3.3 Subdomain source models and
classification
</subsectionHeader>
<bodyText confidence="0.999981851851852">
There were 5 sources of data in the training set:
paraphrase sentence pairs (MSRpar), sentence pairs
from video descriptions (MSRvid), MT evaluation
sentence pairs (MTnews and MTeuroparl) and gloss
pairs (OnWN). In our submitted runs, we trained
a separate, feature-selected model based on cross-
validation for each of these data sources. In train-
ing data on cross-validation tests, training domain-
specific models outperformed training a single con-
glomerate model.
In the test data, there were 4 sources, with 2
appearing in training data (OnWN, SMT) and 2
that were novel (FrameNet/Wordnet sense defini-
tions (FNWN), European news headlines (head-
lines)). We examined two different strategies for ap-
plying the 5-source trained models on these 4 test
sets. Both of these strategies rely on a multiclass
random forest classifier, which we trained on the 47
similarity metrics.
First, for each sentence pair, we considered the
final similarity score to be a weighted combination
of the similarity score from each of the 5 source-
specific similarity models. The combination weights
were determined by utilizing the classifier’s confi-
dence scores. Second, the final similarity was cho-
sen as the single source-specific similarity score cor-
responding to the classifier’s output class.
</bodyText>
<sectionHeader confidence="0.999218" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9991720625">
The MayoClinicNLP team submitted three systems
to the STS-Core task. We also include here a post-
hoc run that was considered as a possible submis-
sion.
r1wtCDT This run used the 47 metrics from
DKPro, TakeLab, and MayoClinicNLP as a
feature pool for feature selection. Source-
specific similarity metrics were combined with
classifier-confidence-score weights.
r2CDT Same feature pool as run 1. Best-match (as
determined by classifier) source-specific simi-
larity metric was used rather than a weighted
combination.
r3wtCD TakeLab features were removed from the
feature pool (before feature selection). Same
source combination as run 1.
</bodyText>
<footnote confidence="0.929438666666667">
r4ALL Post-hoc run using all 47 metrics, but train-
ing a single linear regression model rather than
source-specific models.
</footnote>
<page confidence="0.998692">
151
</page>
<tableCaption confidence="0.998893">
Table 2: Performance comparison.
</tableCaption>
<table confidence="0.999664833333333">
TEAM NAME headlines rank OnWN rank FNWN rank SMT rank mean rank
UMBC EBIQUITY-ParingWords 0.7642 0.7529 0.5818 0.3804 0.6181 1
UMBC EBIQUITY-galactus 0.7428 0.7053 0.5444 0.3705 0.5927 2
deft-baseline 0.6532 0.8431 0.5083 0.3265 0.5795 3
MayoClinicNLP-r4ALL 0.7275 0.7618 0.4359 0.3048 0.5707
UMBC EBIQUITY-saiyan 0.7838 0.5593 0.5815 0.3563 0.5683 4
MayoClinicNLP-r3wtCD 0.6440 43 0.8295 2 0.3202 47 0.3561 17 0.5671 5
MayoClinicNLP-r1wtCDT 0.6584 33 0.7775 4 0.3735 26 0.3605 13 0.5649 6
CLaC-RUN2 0.6921 0.7366 0.3793 0.3375 0.5587 7
MayoClinicNLP-r2CDT 0.6827 23 0.6612 20 0.396 17 0.3946 5 0.5572 8
NTNU-RUN1 0.7279 0.5952 0.3215 0.4015 0.5519 9
CLaC-RUN1 0.6774 0.7667 0.3793 0.3068 0.5511 10
</table>
<subsectionHeader confidence="0.995465">
4.1 Competition performance
</subsectionHeader>
<bodyText confidence="0.999944307692308">
Table 2 shows the top 10 runs of 90 submitted in
the STS-Core task are shown, with our three sys-
tems placing 5th, 6th, and 8th. Additionally, we can
see that run 4 would have placed 4th. Notice that
there are significant source-specific differences be-
tween the runs. For example, while run 4 is better
overall, runs 1–3 outperform it on all but the head-
lines and FNWN datasets, i.e., the test datasets that
were not present in the training data. Thus, it is
clear that the source-specific models are beneficial
when the training data is in-domain, but a combined
model is more beneficial when no such training data
is available.
</bodyText>
<subsectionHeader confidence="0.337749">
Step
</subsectionHeader>
<figureCaption confidence="0.9969305">
Figure 1: Performance curve of feature selection for
r1wtCDT, r2CDT, and r4ALL
</figureCaption>
<bodyText confidence="0.999971388888889">
Due to the source-specific variability among the
runs, it is important to know whether the forward-
search feature selection performed as expected. For
source specific models (runs 1 and 3) and a com-
bined model (run 4), Figure 1 shows the 10-fold
cross-validation scores on the training set as the next
feature is added to the model. As we would ex-
pect, there is an initial growth region where the first
features truly complement one another and improve
performance significantly. A plateau is reached for
each of the models, and some (e.g., SMTnews) even
decay if too many noisy features are added.
The feature selection curves are as expected. Be-
cause the plateau regions are large, feature selection
could be cut off at about 10 features, with gains in
efficiency and perhaps little effect on accuracy.
The resulting selected features for some of the
trained models are shown in Table 3.
</bodyText>
<subsectionHeader confidence="0.999933">
4.3 Contribution of MayoClinicNLP metrics
</subsectionHeader>
<bodyText confidence="0.999977466666667">
We determined whether including MayoClinicNLP
features was any benefit over a feature-selected
DKPro baseline. Table 4 analyzes this question
by adding each of our measures in turn to a base-
line feature-selected DKPro (dkselected). Note that
this baseline was extremely effective; it would have
ranked 4th in the STS competition, outperforming
our run 4. Thus, metrics that improve this baseline
must truly be complementary metrics. Here, we see
that only the phrasal SVS measure is able to improve
performance overall, largely by its contributions to
the most difficult categories, FNWN and SMT. In
fact, that system (dkselected + SVSePhrSimilari-
tyMeasure) represents the best-performing run of
any that was produced in our framework.
</bodyText>
<figure confidence="0.9842442">
4.2 Feature selection analysis
0 10 20 30 40
Pearson’s Correlation Coefficient
0.60 0.65 0.70 0.75 0.80 0.85 0.90
MSRpar
MSRvid
SMTeuroparl
OnWN
SMTnews
ALL
</figure>
<page confidence="0.99032">
152
</page>
<tableCaption confidence="0.999726">
Table 3: Top retained features for several linear regression models.
</tableCaption>
<table confidence="0.99203203030303">
OnWN - r1wtCDT and r2CDT (15 shown/19 selected) SMTnews - r1wtCDT and r2CDT (15 shown/17 selected) All - r4ALL (29 shown/29 selected)
t ngram/ContentUnigramOverlap t other/RelativeInfoContentDifference t vec/LSAWordSimilarity weighted NYT
t other/RelativeInfoContentDifference n-grams/CharacterNGramMeasure 2 n-grams/CharacterNGramMeasure 2
t vec/LSAWordSimilarity weighted NYT t other/CaseMatches string/LongestCommonSubstringComparator
esa/ESA Wiktionary string/GreedyStringTiling 3 t other/NumbersOverlap
t ngram/ContentBigramOverlap custom/RandomIndexingMeasure d200 wr6p t words/WordNetAugmentedWordOverlap
n-grams/CharacterNGramMeasure 2 custom/StanfordNerMeasure overlap n-grams/WordNGramJaccardMeasure 1
t words/WordNetAugmentedWordOverlap t vec/LSAWordSimilarity weighted NYT n-grams/CharacterNGramMeasure 3
t ngram/BigramOverlap t other/SentenceSize t other/SentenceSize
string/GreedyStringTiling 3 custom/RandomIndexingMeasure d200 wr0 t other/RelativeInfoContentDifference
string/LongestCommonSubsequenceNormComparator custom/SVSePhrSimilarityMeasure t ngram/ContentBigramOverlap
custom/RandomIndexingMeasure d200 wr0 esa/ESA Wiktionary n-grams/WordNGramJaccardMeasure 4
custom/StanfordNerMeasure aligngst string/LongestCommonSubstringComparator t other/NumbersSize
custom/StanfordNerMeasure alignlcs t other/NumbersSize t other/NumbersSubset
custom/StanfordNerMeasure overlap n-grams/WordNGramContainmentMeasure 2 stopword-filtered custom/SVSePhrSimilarityMeasure
custom/SVSePhrSimilarityMeasure custom/SVSeTopSimilarityMeasure custom/SemanticVectorsSimilarityMeasure d200 wr6p
esa/ESA WordNet
OnWN - r3wtCD (7 shown/7 selected) SMTnews - r3wtCD (15 shown/23 selected) esa/ESA Wiktionary
esa/ESA Wiktionary string/GreedyStringTiling 3 string/LongestCommonSubsequenceComparator
string/LongestCommonSubsequenceComparator custom/StanfordNerMeasure overlap string/LongestCommonSubsequenceNormComparator
string/GreedyStringTiling 3 n-grams/CharacterNGramMeasure 2 n-grams/WordNGramContainmentMeasure 1 stopword-filtered
string/LongestCommonSubsequenceNormComparator custom/RandomIndexingMeasure d200 wr6p word-sim/MCS06 Resnik WordNet
string/LongestCommonSubstringComparator n-grams/CharacterNGramMeasure 3 t ngram/ContentUnigramOverlap
word-sim/MCS06 Resnik WordNet string/LongestCommonSubsequenceComparator n-grams/WordNGramContainmentMeasure 2 stopword-filtered
n-grams/WordNGramContainmentMeasure 2 stopword-filtered custom/StanfordNerMeasure aligngst n-grams/WordNGramJaccardMeasure 2 stopword-filtered
custom/SVSePhrSimilarityMeasure t ngram/UnigramOverlap
esa/ESA Wiktionary t ngram/BigramOverlap
esa/ESA WordNet t other/StocksSize
n-grams/WordNGramContainmentMeasure 2 stopword-filtered t words/GreedyLemmaAligningOverlap
n-grams/WordNGramJaccardMeasure 1 t other/StocksOverlap
string/LongestCommonSubstringComparator
custom/RandomIndexingMeasure d200 wr6d
custom/RandomIndexingMeasure d200 wr0
</table>
<tableCaption confidence="0.957312">
Table 4: Adding customized features one at a time into optimized DKPro feature set. Models are trained across all
sources.
</tableCaption>
<figure confidence="0.429459">
dkselected
</figure>
<figureCaption confidence="0.8642908">
dkselected + SVSePhrSimilarityMeasure
dkselected + RandomIndexingMeasure d200 wr0
dkselected + SVSeTopSimilarityMeasure
dkselected + RandomIndexingMeasure d200 wr6d
dkselected + RandomIndexingMeasure d200 wr6b
dkselected + RandomIndexingMeasure d200 wr6p
dkselected + StanfordNerMeasure aligngst
dkselected + StanfordNerMeasure overlap
dkselected + StanfordNerMeasure alignlcs
(dk + all custom) selected
</figureCaption>
<table confidence="0.998498333333333">
headlines OnWN FNWN SMT mean
0.70331 0.79752 0.38358 0.31744 0.571319
0.70178 0.79644 0.38685 0.32332 0.572774
0.70054 0.79752 0.38432 0.31615 0.570028
0.69873 0.79522 0.38815 0.31723 0.569533
0.69944 0.79836 0.38416 0.31397 0.569131
0.69992 0.79788 0.38435 0.31328 0.568957
0.69878 0.79848 0.37876 0.31436 0.568617
0.69446 0.79502 0.38703 0.31497 0.567212
0.69468 0.79509 0.38703 0.31466 0.567200
0.69451 0.79486 0.38657 0.31394 0.566807
0.70311 0.79887 0.37477 0.31665 0.570586
</table>
<bodyText confidence="0.999454333333333">
Also, we see some source-specific behavior. None
of our introduced measures are able to improve the
headlines similarities. However, random indexing
improves OnWN scores, several strategies improve
the FNWN metric, and simsvs-phr is the only viable
performance improvement on the SMT corpus.
</bodyText>
<sectionHeader confidence="0.999861" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999902">
Mayo Clinic’s submissions to Semantic Textual
Similarity 2013 performed well, placing 5th, 6th,
and 8th among 90 submitted systems. We intro-
duced similarity metrics that used different means
to do compositional distributional semantics along
with some named entity-based measures, finding
some improvement especially for phrasal similar-
ity from structured vectorial semantics. Through-
out, we utilized forward-search feature selection,
which enhanced the performance of the models. We
also used source-based linear regression models and
considered unseen sources as mixtures of existing
sources; we found that in-domain data is neces-
sary for smaller, source-based models to outperform
larger, conglomerate models.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999922">
Thanks to the developers of the UKP DKPro sys-
tem and the TakeLab system for making their code
available. Also, thanks to James Masanz for initial
implementations of some similarity measures.
</bodyText>
<page confidence="0.998882">
153
</page>
<sectionHeader confidence="0.995673" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999474129032258">
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, pages 385–393. Association for Computational
Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Lloyd Allison and Trevor I Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23(5):305–310.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics-Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation, pages 435–
440. Association for Computational Linguistics.
Scott Deerwester, Susan Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391–
407.
Katrin Erk and Sebastian Pad´o. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP 2008.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 363–370.
Association for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10–18, November.
Pentti Kanerva, Jan Kristofersson, and Anders Holst.
2000. Random indexing of text samples for latent se-
mantic analysis. In Proceedings of the 22nd annual
conference of the cognitive science society, volume
1036. Citeseer.
T.K. Landauer and S.T. Dumais. 1997. A Solution to
Plato’s Problem: The Latent Semantic Analysis The-
ory of Acquisition, Induction, and Representation of
Knowledge. Psychological Review, 104:211–240.
Alon Lavie and Michael J Denkowski. 2009. The meteor
metric for automatic evaluation of machine translation.
Machine translation, 23(2-3):105–115.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236–244, Columbus, OH.
M. Sahlgren. 2005. An introduction to random index-
ing. In Methods and Applications of Semantic Index-
ing Workshop at the 7th International Conference on
Terminology and Knowledge Engineering, TKE, vol-
ume 5.
Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder,
and Bojana Dalbelo Baˇsi´c. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441–448,
Montr´eal, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Dominic Widdows and Trevor Cohen. 2010. The seman-
tic vectors package: New algorithms and public tools
for distributional semantics. In Semantic Computing
(ICSC), 2010 IEEE Fourth International Conference
on, pages 9–15. IEEE.
D. Widdows and K. Ferraro. 2008. Semantic vec-
tors: a scalable open source package and online tech-
nology management application. Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC’08), pages 1183–1190.
Michael J Wise. 1996. Yap3: Improved detection of sim-
ilarities in computer program and other texts. In ACM
SIGCSE Bulletin, volume 28, pages 130–134. ACM.
Stephen Wu and William Schuler. 2011. Structured com-
position of semantic vectors. In Proceedings of the In-
ternational Conference on Computational Semantics.
Stephen Tze-Inn Wu. 2010. Vectorial Representations
of Meaning for a Computational Model of Language
Comprehension. Ph.D. thesis, Department of Com-
puter Science and Engineering, University of Min-
nesota.
</reference>
<page confidence="0.999771">
154
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.901666">
<title confidence="0.999441">MayoClinicNLP–CORE: Semantic representations for textual similarity</title>
<author confidence="0.994986">Stephen Wu Dongqing Zhu</author>
<author confidence="0.994986">Ben Carterette Hongfang Liu</author>
<affiliation confidence="0.964849">Mayo Clinic University of Delaware Mayo Clinic</affiliation>
<address confidence="0.997275">Rochester, MN 55905 Newark, DE 19716 Rochester, MN 55905</address>
<email confidence="0.999882">liu.hongfang@mayo.edu</email>
<abstract confidence="0.994163727272727">The Semantic Textual Similarity (STS) task examines semantic similarity at a sentencelevel. We explored three representations of semantics (implicit or explicit): named entities, semantic vectors, and structured vectorial semantics. From a DKPro baseline, we also performed feature selection and used sourcespecific linear regression models to combine our features. Our systems placed 5th, 6th, and 8th among 90 submitted systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>385--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="814" citStr="Agirre et al., 2012" startWordPosition="105" endWordPosition="108">, DE 19716 Rochester, MN 55905 wu.stephen@mayo.edu {zhu,carteret}@cis.udel.edu liu.hongfang@mayo.edu Abstract The Semantic Textual Similarity (STS) task examines semantic similarity at a sentencelevel. We explored three representations of semantics (implicit or explicit): named entities, semantic vectors, and structured vectorial semantics. From a DKPro baseline, we also performed feature selection and used sourcespecific linear regression models to combine our features. Our systems placed 5th, 6th, and 8th among 90 submitted systems. 1 Introduction The Semantic Textual Similarity (STS) task (Agirre et al., 2012; Agirre et al., 2013) examines semantic similarity at a sentence-level. While much work has compared the semantics of terms, concepts, or documents, this space has been relatively unexplored. The 2013 STS task provided sentence pairs and a 0–5 human rating of their similarity, with training data from 5 sources and test data from 4 sources. We sought to explore and evaluate the usefulness of several semantic representations that have had recent significance in research or practice. First, information extraction (IE) methods often implicitly consider named entities as ad hoc semantic representa</context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 385–393. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="836" citStr="Agirre et al., 2013" startWordPosition="109" endWordPosition="112"> MN 55905 wu.stephen@mayo.edu {zhu,carteret}@cis.udel.edu liu.hongfang@mayo.edu Abstract The Semantic Textual Similarity (STS) task examines semantic similarity at a sentencelevel. We explored three representations of semantics (implicit or explicit): named entities, semantic vectors, and structured vectorial semantics. From a DKPro baseline, we also performed feature selection and used sourcespecific linear regression models to combine our features. Our systems placed 5th, 6th, and 8th among 90 submitted systems. 1 Introduction The Semantic Textual Similarity (STS) task (Agirre et al., 2012; Agirre et al., 2013) examines semantic similarity at a sentence-level. While much work has compared the semantics of terms, concepts, or documents, this space has been relatively unexplored. The 2013 STS task provided sentence pairs and a 0–5 human rating of their similarity, with training data from 5 sources and test data from 4 sources. We sought to explore and evaluate the usefulness of several semantic representations that have had recent significance in research or practice. First, information extraction (IE) methods often implicitly consider named entities as ad hoc semantic representations, for example, in</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity. In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lloyd Allison</author>
<author>Trevor I Dix</author>
</authors>
<title>A bit-string longest-common-subsequence algorithm.</title>
<date>1986</date>
<journal>Information Processing Letters,</journal>
<volume>23</volume>
<issue>5</issue>
<contexts>
<context position="5611" citStr="Allison and Dix, 1986" startWordPosition="703" endWordPosition="706">/RelativeInfoContentDifference n-grams/CharacterNGramMeasure 4 t other/NumbersSize string/GreedyStringTiling 3 t other/NumbersOverlap string/LongestCommonSubsequenceComparator t other/NumbersSubset string/LongestCommonSubsequenceNormComparator t other/SentenceSize string/LongestCommonSubstringComparator t other/CaseMatches t other/StocksSize t other/StocksOverlap matches 52, and how closely 52 matches 51: simneo(51, 52) = 2 ⋅ INE1 n NE2I (1) INE1 I + INE2I Additionally, we relax the constraint of requiring exact string matches between the two sentences by using the longest common subsequence (Allison and Dix, 1986) and greedy string tiling (Wise, 1996) algorithms. These metrics give similarities between two strings, rather than two sets of strings as we have with NE1 and NE2. Thus, we follow previous work in greedily aligning these named entities (Lavie and Denkowski, 2009; ˇSari´c et al., 2012) into pairs. Namely, we compare each pair (nei,1, nej,2) of named entity strings in NE1 and NE2. The highest-scoring pair is entered into a set of pairs, P. Then, the next highest pair is added to P if neither named entity is already in P, and discarded otherwise; this continues until there are no more named enti</context>
</contexts>
<marker>Allison, Dix, 1986</marker>
<rawString>Lloyd Allison and Trevor I Dix. 1986. A bit-string longest-common-subsequence algorithm. Information Processing Letters, 23(5):305–310.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>435--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 435– 440. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<pages>407</pages>
<contexts>
<context position="6903" citStr="Deerwester et al., 1990" startWordPosition="918" endWordPosition="921">measures that use the longest common subsequence (LCS) and greedy string tiling (GST) fuzzy string matching algorithms: where f(⋅) is either the LCS or GST algorithm. In our experiments, we performed named entity recognition with the Stanford NER tool using the standard English model (Finkel et al., 2005). Also, we used UKP’s existing implementation of LCS and GST (ˇSari´c et al., 2012) for the latter two measures. 2.2 Random indexing measures Random indexing (Kanerva et al., 2000; Sahlgren, 2005) is another distributional semantics framework for representing terms as vectors. Similar to LSA (Deerwester et al., 1990), an index is created that represents each term as a semantic vector. But in random indexing, each term is represented by an elemental vector et with a small number of randomly-generated non-zero components. The intuition for this means of dimensionality reduction is that these randomly-generated elemental vectors are like quasi-orthogonal bases in a traditional geometric semantic space, rather than, e.g., 300 fully orthogonal dimensions from singular value decomposition (Landauer and Dumais, 1997). For a standard model with random indexing, a contextual term vector ct,std is the the sum of th</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391– 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6585" citStr="Finkel et al., 2005" startWordPosition="870" endWordPosition="873">tity strings in NE1 and NE2. The highest-scoring pair is entered into a set of pairs, P. Then, the next highest pair is added to P if neither named entity is already in P, and discarded otherwise; this continues until there are no more named entities in either NE1 or NE2. We then define two named entity aligning measures that use the longest common subsequence (LCS) and greedy string tiling (GST) fuzzy string matching algorithms: where f(⋅) is either the LCS or GST algorithm. In our experiments, we performed named entity recognition with the Stanford NER tool using the standard English model (Finkel et al., 2005). Also, we used UKP’s existing implementation of LCS and GST (ˇSari´c et al., 2012) for the latter two measures. 2.2 Random indexing measures Random indexing (Kanerva et al., 2000; Sahlgren, 2005) is another distributional semantics framework for representing terms as vectors. Similar to LSA (Deerwester et al., 1990), an index is created that represents each term as a semantic vector. But in random indexing, each term is represented by an elemental vector et with a small number of randomly-generated non-zero components. The intuition for this means of dimensionality reduction is that these ran</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="12929" citStr="Hall et al., 2009" startWordPosition="1902" endWordPosition="1905">. 3 Feature combination framework The similarity metrics of Section 2 were calculated for each of the sentence pairs in the training set, and later the test set. In combining these metrics, we extended a DKPro Similarity baseline (3.1) with feature selection (3.2) and source-specific models and classification (3.3). 3.1 Linear regression via DKPro Similarity For our baseline (MayoClinicNLPr1wtCDT), we used the UIMA-based DKPro Similarity system from STS 2012 (B¨ar et al., 2012). Aside from the large number of sound similarity measures, this provided linear regression through the WEKA package (Hall et al., 2009) to combine all of the disparate similarity metrics into a single one, and some preprocessing. Regression weights were determined on the whole training set for each source. 3.2 Feature selection Not every feature was included in the final linear regression models. To determine the best of the 47 (DKPro–17, TakeLab–21, MayoClinicNLP–9) features, we performed a full forward-search on the space of similarity measures. In forward-search, we perform 10-fold cross-validation on the training set for each measure, and pick the best one; in the next round, that best metric is retained, and the remainin</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: an update. SIGKDD Explor. Newsl., 11(1):10–18, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pentti Kanerva</author>
<author>Jan Kristofersson</author>
<author>Anders Holst</author>
</authors>
<title>Random indexing of text samples for latent semantic analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the 22nd annual conference of the cognitive science society,</booktitle>
<volume>volume</volume>
<pages>1036</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="1758" citStr="Kanerva et al., 2000" startWordPosition="248" endWordPosition="251">ta from 4 sources. We sought to explore and evaluate the usefulness of several semantic representations that have had recent significance in research or practice. First, information extraction (IE) methods often implicitly consider named entities as ad hoc semantic representations, for example, in the clinical domain. Therefore, we sought to evaluate similarity based on named entity-based features. Second, in many applications, an effective means of incorporating distributional semantics is Random Indexing (RI). Thus we consider three different representations possible within Random Indexing (Kanerva et al., 2000; Sahlgren, 2005). Finally, because compositional distributional semantics is an important research topic (Mitchell and Lapata, 2008; Erk and Pad´o, 2008), we sought to evaluate a principled composition strategy: structured vectorial semantics (Wu and Schuler, 2011). The remainder of this paper proceeds as follows. Section 2 overviews our similarity metrics, and Section 3 overviews the systems that were defined on these metrics. Competition results and additional analyses are in Section 4. We end with discussion on the results in Section 5. 2 Similarity measures Because we expect semantic simi</context>
<context position="6764" citStr="Kanerva et al., 2000" startWordPosition="899" endWordPosition="902">carded otherwise; this continues until there are no more named entities in either NE1 or NE2. We then define two named entity aligning measures that use the longest common subsequence (LCS) and greedy string tiling (GST) fuzzy string matching algorithms: where f(⋅) is either the LCS or GST algorithm. In our experiments, we performed named entity recognition with the Stanford NER tool using the standard English model (Finkel et al., 2005). Also, we used UKP’s existing implementation of LCS and GST (ˇSari´c et al., 2012) for the latter two measures. 2.2 Random indexing measures Random indexing (Kanerva et al., 2000; Sahlgren, 2005) is another distributional semantics framework for representing terms as vectors. Similar to LSA (Deerwester et al., 1990), an index is created that represents each term as a semantic vector. But in random indexing, each term is represented by an elemental vector et with a small number of randomly-generated non-zero components. The intuition for this means of dimensionality reduction is that these randomly-generated elemental vectors are like quasi-orthogonal bases in a traditional geometric semantic space, rather than, e.g., 300 fully orthogonal dimensions from singular value</context>
</contexts>
<marker>Kanerva, Kristofersson, Holst, 2000</marker>
<rawString>Pentti Kanerva, Jan Kristofersson, and Anders Holst. 2000. Random indexing of text samples for latent semantic analysis. In Proceedings of the 22nd annual conference of the cognitive science society, volume 1036. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A Solution to Plato’s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge. Psychological Review,</title>
<date>1997</date>
<pages>104--211</pages>
<contexts>
<context position="7406" citStr="Landauer and Dumais, 1997" startWordPosition="994" endWordPosition="997">) is another distributional semantics framework for representing terms as vectors. Similar to LSA (Deerwester et al., 1990), an index is created that represents each term as a semantic vector. But in random indexing, each term is represented by an elemental vector et with a small number of randomly-generated non-zero components. The intuition for this means of dimensionality reduction is that these randomly-generated elemental vectors are like quasi-orthogonal bases in a traditional geometric semantic space, rather than, e.g., 300 fully orthogonal dimensions from singular value decomposition (Landauer and Dumais, 1997). For a standard model with random indexing, a contextual term vector ct,std is the the sum of the elemental vectors corresponding to tokens in the document. All contexts for a particular term are summed and normalized to produce a final term vector vt,std. Other notions of context can be incorporated into E f(ne1, ne2) simnea(51, 52) = (ne1,nee)∈P max (INE1I, INE2I) (2) 149 this model. Local co-occurrence context can be accounted for in a basic sliding-window model by considering words within some window radius r (instead of a whole document). Each instance of the term t will have a contextua</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T.K. Landauer and S.T. Dumais. 1997. A Solution to Plato’s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge. Psychological Review, 104:211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Michael J Denkowski</author>
</authors>
<title>The meteor metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<booktitle>Machine translation,</booktitle>
<pages>23--2</pages>
<contexts>
<context position="5874" citStr="Lavie and Denkowski, 2009" startWordPosition="747" endWordPosition="750">Size string/LongestCommonSubstringComparator t other/CaseMatches t other/StocksSize t other/StocksOverlap matches 52, and how closely 52 matches 51: simneo(51, 52) = 2 ⋅ INE1 n NE2I (1) INE1 I + INE2I Additionally, we relax the constraint of requiring exact string matches between the two sentences by using the longest common subsequence (Allison and Dix, 1986) and greedy string tiling (Wise, 1996) algorithms. These metrics give similarities between two strings, rather than two sets of strings as we have with NE1 and NE2. Thus, we follow previous work in greedily aligning these named entities (Lavie and Denkowski, 2009; ˇSari´c et al., 2012) into pairs. Namely, we compare each pair (nei,1, nej,2) of named entity strings in NE1 and NE2. The highest-scoring pair is entered into a set of pairs, P. Then, the next highest pair is added to P if neither named entity is already in P, and discarded otherwise; this continues until there are no more named entities in either NE1 or NE2. We then define two named entity aligning measures that use the longest common subsequence (LCS) and greedy string tiling (GST) fuzzy string matching algorithms: where f(⋅) is either the LCS or GST algorithm. In our experiments, we perfo</context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>Alon Lavie and Michael J Denkowski. 2009. The meteor metric for automatic evaluation of machine translation. Machine translation, 23(2-3):105–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="1890" citStr="Mitchell and Lapata, 2008" startWordPosition="264" endWordPosition="267">gnificance in research or practice. First, information extraction (IE) methods often implicitly consider named entities as ad hoc semantic representations, for example, in the clinical domain. Therefore, we sought to evaluate similarity based on named entity-based features. Second, in many applications, an effective means of incorporating distributional semantics is Random Indexing (RI). Thus we consider three different representations possible within Random Indexing (Kanerva et al., 2000; Sahlgren, 2005). Finally, because compositional distributional semantics is an important research topic (Mitchell and Lapata, 2008; Erk and Pad´o, 2008), we sought to evaluate a principled composition strategy: structured vectorial semantics (Wu and Schuler, 2011). The remainder of this paper proceeds as follows. Section 2 overviews our similarity metrics, and Section 3 overviews the systems that were defined on these metrics. Competition results and additional analyses are in Section 4. We end with discussion on the results in Section 5. 2 Similarity measures Because we expect semantic similarity to be multilayered, we expect that we will need many similarity measures to approximate human similarity judgments. Rather th</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
</authors>
<title>An introduction to random indexing.</title>
<date>2005</date>
<booktitle>In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE,</booktitle>
<volume>5</volume>
<contexts>
<context position="1775" citStr="Sahlgren, 2005" startWordPosition="252" endWordPosition="253">sought to explore and evaluate the usefulness of several semantic representations that have had recent significance in research or practice. First, information extraction (IE) methods often implicitly consider named entities as ad hoc semantic representations, for example, in the clinical domain. Therefore, we sought to evaluate similarity based on named entity-based features. Second, in many applications, an effective means of incorporating distributional semantics is Random Indexing (RI). Thus we consider three different representations possible within Random Indexing (Kanerva et al., 2000; Sahlgren, 2005). Finally, because compositional distributional semantics is an important research topic (Mitchell and Lapata, 2008; Erk and Pad´o, 2008), we sought to evaluate a principled composition strategy: structured vectorial semantics (Wu and Schuler, 2011). The remainder of this paper proceeds as follows. Section 2 overviews our similarity metrics, and Section 3 overviews the systems that were defined on these metrics. Competition results and additional analyses are in Section 4. We end with discussion on the results in Section 5. 2 Similarity measures Because we expect semantic similarity to be mult</context>
<context position="6781" citStr="Sahlgren, 2005" startWordPosition="903" endWordPosition="904"> continues until there are no more named entities in either NE1 or NE2. We then define two named entity aligning measures that use the longest common subsequence (LCS) and greedy string tiling (GST) fuzzy string matching algorithms: where f(⋅) is either the LCS or GST algorithm. In our experiments, we performed named entity recognition with the Stanford NER tool using the standard English model (Finkel et al., 2005). Also, we used UKP’s existing implementation of LCS and GST (ˇSari´c et al., 2012) for the latter two measures. 2.2 Random indexing measures Random indexing (Kanerva et al., 2000; Sahlgren, 2005) is another distributional semantics framework for representing terms as vectors. Similar to LSA (Deerwester et al., 1990), an index is created that represents each term as a semantic vector. But in random indexing, each term is represented by an elemental vector et with a small number of randomly-generated non-zero components. The intuition for this means of dimensionality reduction is that these randomly-generated elemental vectors are like quasi-orthogonal bases in a traditional geometric semantic space, rather than, e.g., 300 fully orthogonal dimensions from singular value decomposition (L</context>
</contexts>
<marker>Sahlgren, 2005</marker>
<rawString>M. Sahlgren. 2005. An introduction to random indexing. In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane ˇSari´c</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
<author>Jan ˇSnajder</author>
<author>Bojana Dalbelo Baˇsi´c</author>
</authors>
<title>Takelab: Systems for measuring semantic text similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>441--448</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<marker>ˇSari´c, Glavaˇs, Karan, ˇSnajder, Baˇsi´c, 2012</marker>
<rawString>Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsi´c. 2012. Takelab: Systems for measuring semantic text similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 441–448, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
<author>Trevor Cohen</author>
</authors>
<title>The semantic vectors package: New algorithms and public tools for distributional semantics.</title>
<date>2010</date>
<booktitle>In Semantic Computing (ICSC), 2010 IEEE Fourth International Conference on,</booktitle>
<pages>9--15</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="8997" citStr="Widdows and Cohen, 2010" startWordPosition="1263" endWordPosition="1266">tation to achieve one of the two contexts. A permutated positional model uses a position-specific permutation function to encode the relative word positions (rather than just left- or rightcontext) separately. Again, vt would be summed and normalized over all instances of ct. Sentence vectors from any of these 4 Random Indexing-based models (standard, windowed, directional, positional) are just the sum of the vectors for each term vS = Et∈S vt. We define 4 separate similarity metrics for STS as: simRI(S1, S2) = cos(vS1, vS2) (3) We used the semantic vectors package (Widdows and Ferraro, 2008; Widdows and Cohen, 2010) in the default configuration for the standard model. For the windowed, directional, and positional models, we used a 6-word window radius with 200 dimensions and a seed length of 5. All models were trained on the raw text of the Penn Treebank Wall Street Journal corpus and a 100,075-article subset of Wikipedia. 2.3 Semantic vectorial semantics measures Structured vectorial semantics (SVS) composes distributional semantic representations in syntactic context (Wu and Schuler, 2011). Similarity metrics defined with SVS inherently explore the qualities of a fully interactive syntax–semantics inte</context>
</contexts>
<marker>Widdows, Cohen, 2010</marker>
<rawString>Dominic Widdows and Trevor Cohen. 2010. The semantic vectors package: New algorithms and public tools for distributional semantics. In Semantic Computing (ICSC), 2010 IEEE Fourth International Conference on, pages 9–15. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
<author>K Ferraro</author>
</authors>
<title>Semantic vectors: a scalable open source package and online technology management application.</title>
<date>2008</date>
<booktitle>Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<pages>1183--1190</pages>
<contexts>
<context position="8971" citStr="Widdows and Ferraro, 2008" startWordPosition="1259" endWordPosition="1262">ightcontext), using a permutation to achieve one of the two contexts. A permutated positional model uses a position-specific permutation function to encode the relative word positions (rather than just left- or rightcontext) separately. Again, vt would be summed and normalized over all instances of ct. Sentence vectors from any of these 4 Random Indexing-based models (standard, windowed, directional, positional) are just the sum of the vectors for each term vS = Et∈S vt. We define 4 separate similarity metrics for STS as: simRI(S1, S2) = cos(vS1, vS2) (3) We used the semantic vectors package (Widdows and Ferraro, 2008; Widdows and Cohen, 2010) in the default configuration for the standard model. For the windowed, directional, and positional models, we used a 6-word window radius with 200 dimensions and a seed length of 5. All models were trained on the raw text of the Penn Treebank Wall Street Journal corpus and a 100,075-article subset of Wikipedia. 2.3 Semantic vectorial semantics measures Structured vectorial semantics (SVS) composes distributional semantic representations in syntactic context (Wu and Schuler, 2011). Similarity metrics defined with SVS inherently explore the qualities of a fully interac</context>
</contexts>
<marker>Widdows, Ferraro, 2008</marker>
<rawString>D. Widdows and K. Ferraro. 2008. Semantic vectors: a scalable open source package and online technology management application. Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), pages 1183–1190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Wise</author>
</authors>
<title>Yap3: Improved detection of similarities in computer program and other texts.</title>
<date>1996</date>
<journal>In ACM SIGCSE Bulletin,</journal>
<volume>28</volume>
<pages>130--134</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5649" citStr="Wise, 1996" startWordPosition="711" endWordPosition="712">GramMeasure 4 t other/NumbersSize string/GreedyStringTiling 3 t other/NumbersOverlap string/LongestCommonSubsequenceComparator t other/NumbersSubset string/LongestCommonSubsequenceNormComparator t other/SentenceSize string/LongestCommonSubstringComparator t other/CaseMatches t other/StocksSize t other/StocksOverlap matches 52, and how closely 52 matches 51: simneo(51, 52) = 2 ⋅ INE1 n NE2I (1) INE1 I + INE2I Additionally, we relax the constraint of requiring exact string matches between the two sentences by using the longest common subsequence (Allison and Dix, 1986) and greedy string tiling (Wise, 1996) algorithms. These metrics give similarities between two strings, rather than two sets of strings as we have with NE1 and NE2. Thus, we follow previous work in greedily aligning these named entities (Lavie and Denkowski, 2009; ˇSari´c et al., 2012) into pairs. Namely, we compare each pair (nei,1, nej,2) of named entity strings in NE1 and NE2. The highest-scoring pair is entered into a set of pairs, P. Then, the next highest pair is added to P if neither named entity is already in P, and discarded otherwise; this continues until there are no more named entities in either NE1 or NE2. We then def</context>
</contexts>
<marker>Wise, 1996</marker>
<rawString>Michael J Wise. 1996. Yap3: Improved detection of similarities in computer program and other texts. In ACM SIGCSE Bulletin, volume 28, pages 130–134. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wu</author>
<author>William Schuler</author>
</authors>
<title>Structured composition of semantic vectors.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Computational Semantics.</booktitle>
<contexts>
<context position="2024" citStr="Wu and Schuler, 2011" startWordPosition="284" endWordPosition="287"> representations, for example, in the clinical domain. Therefore, we sought to evaluate similarity based on named entity-based features. Second, in many applications, an effective means of incorporating distributional semantics is Random Indexing (RI). Thus we consider three different representations possible within Random Indexing (Kanerva et al., 2000; Sahlgren, 2005). Finally, because compositional distributional semantics is an important research topic (Mitchell and Lapata, 2008; Erk and Pad´o, 2008), we sought to evaluate a principled composition strategy: structured vectorial semantics (Wu and Schuler, 2011). The remainder of this paper proceeds as follows. Section 2 overviews our similarity metrics, and Section 3 overviews the systems that were defined on these metrics. Competition results and additional analyses are in Section 4. We end with discussion on the results in Section 5. 2 Similarity measures Because we expect semantic similarity to be multilayered, we expect that we will need many similarity measures to approximate human similarity judgments. Rather than reinvent the wheel, we have chosen to introduce features that complement existing successful feature sets. We utilized 17 features </context>
<context position="9482" citStr="Wu and Schuler, 2011" startWordPosition="1337" endWordPosition="1340"> for STS as: simRI(S1, S2) = cos(vS1, vS2) (3) We used the semantic vectors package (Widdows and Ferraro, 2008; Widdows and Cohen, 2010) in the default configuration for the standard model. For the windowed, directional, and positional models, we used a 6-word window radius with 200 dimensions and a seed length of 5. All models were trained on the raw text of the Penn Treebank Wall Street Journal corpus and a 100,075-article subset of Wikipedia. 2.3 Semantic vectorial semantics measures Structured vectorial semantics (SVS) composes distributional semantic representations in syntactic context (Wu and Schuler, 2011). Similarity metrics defined with SVS inherently explore the qualities of a fully interactive syntax–semantics interface. While previous work evaluated the syntactic contributions of this model, the STS task allows us to evaluate the phrase-level semantic validity of the model. We summarize SVS here as bottom-up vector composition and parsing, then continue on to define the associated similarity metrics. Each token in a sentence is modeled generatively as a vector eγ of latent referents iγ in syntactic context cγ; each element in the vector is defined as: eγ[iγ] = P(xγ I lciγ), for preterm -y </context>
<context position="10825" citStr="Wu and Schuler, 2011" startWordPosition="1570" endWordPosition="1573">r algebra form,1 assuming that we are composing the semantics of two children eα and eβ in a binary syntactic tree into their parent eγ: eγ = M O (Lγxα • eα) O (Lγxβ • eβ) • 1 (5) M is a diagonal matrix that encapsulates probabilistic syntactic information; the L matrices are linear transformations that capture how semantically relevant child vectors are to the resulting vector (e.g., Lγxα defines the the relevance of eα to eγ). These matrices are defined such that the resulting eγ is a semantic vector of consistent P(xγ I lciγ) probabilities. Further detail is in our previous work (Wu, 2010; Wu and Schuler, 2011). Similarity metrics can be defined in the SVS space by comparing the distributions of the composed eγ vectors — i.e., our similarity metric is a comparison of the vector semantics at different phrasal nodes. We define two measures, one corresponding to the top node cp (e.g., with a syntactic constituent cp = ‘S’), and one corresponding to the left and right largest child nodes (e.g.,, c∠ = ‘NP’ and c= ‘VP’ for a canonical subject–verb–object sentence in English). simsvs-top(S1,S2) =cos(ep(S1),ep(S2)) (6) simsvs-phr(S1, S2) = max( avgsim(e∠(S1), e∠(S2); e(S1), e(S2)), avgsim(e∠(S1),e(S2);e(S1)</context>
<context position="12218" citStr="Wu and Schuler (2011)" startWordPosition="1792" endWordPosition="1795">g the semantics of a whole sentence. The phrasal similarity function simsvs-phr(S1, S2) in (7) thus seeks to semantically align the two largest subtrees, and weight them. Compared to simsvs-top, 1We define the operator ⊙ as point-by-point multiplication of two diagonal matrices and 1 as a column vector of ones, collapsing a diagonal matrix onto a column vector. 150 the phrasal similarity function simsvs-phr(51, 52) assumes there might be some information captured in the child nodes that could be lost in the final composition to the top node. In our experiments, we used the parser described in Wu and Schuler (2011) with 1,000 headwords and 10 relational clusters, trained on the Wall Street Journal treebank. 3 Feature combination framework The similarity metrics of Section 2 were calculated for each of the sentence pairs in the training set, and later the test set. In combining these metrics, we extended a DKPro Similarity baseline (3.1) with feature selection (3.2) and source-specific models and classification (3.3). 3.1 Linear regression via DKPro Similarity For our baseline (MayoClinicNLPr1wtCDT), we used the UIMA-based DKPro Similarity system from STS 2012 (B¨ar et al., 2012). Aside from the large nu</context>
</contexts>
<marker>Wu, Schuler, 2011</marker>
<rawString>Stephen Wu and William Schuler. 2011. Structured composition of semantic vectors. In Proceedings of the International Conference on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tze-Inn Wu</author>
</authors>
<title>Vectorial Representations of Meaning for a Computational Model of Language Comprehension.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science and Engineering, University of Minnesota.</institution>
<contexts>
<context position="10802" citStr="Wu, 2010" startWordPosition="1568" endWordPosition="1569">s in linear algebra form,1 assuming that we are composing the semantics of two children eα and eβ in a binary syntactic tree into their parent eγ: eγ = M O (Lγxα • eα) O (Lγxβ • eβ) • 1 (5) M is a diagonal matrix that encapsulates probabilistic syntactic information; the L matrices are linear transformations that capture how semantically relevant child vectors are to the resulting vector (e.g., Lγxα defines the the relevance of eα to eγ). These matrices are defined such that the resulting eγ is a semantic vector of consistent P(xγ I lciγ) probabilities. Further detail is in our previous work (Wu, 2010; Wu and Schuler, 2011). Similarity metrics can be defined in the SVS space by comparing the distributions of the composed eγ vectors — i.e., our similarity metric is a comparison of the vector semantics at different phrasal nodes. We define two measures, one corresponding to the top node cp (e.g., with a syntactic constituent cp = ‘S’), and one corresponding to the left and right largest child nodes (e.g.,, c∠ = ‘NP’ and c= ‘VP’ for a canonical subject–verb–object sentence in English). simsvs-top(S1,S2) =cos(ep(S1),ep(S2)) (6) simsvs-phr(S1, S2) = max( avgsim(e∠(S1), e∠(S2); e(S1), e(S2)), av</context>
</contexts>
<marker>Wu, 2010</marker>
<rawString>Stephen Tze-Inn Wu. 2010. Vectorial Representations of Meaning for a Computational Model of Language Comprehension. Ph.D. thesis, Department of Computer Science and Engineering, University of Minnesota.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>