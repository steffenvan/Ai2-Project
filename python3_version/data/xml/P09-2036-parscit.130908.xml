<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.050650">
<title confidence="0.996606">
Asynchronous Binarization for Synchronous Grammars
</title>
<author confidence="0.999786">
John DeNero, Adam Pauls, and Dan Klein
</author>
<affiliation confidence="0.998886">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.991377">
{denero, adpauls, klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.993869" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999281941176471">
Binarization of n-ary rules is critical for the effi-
ciency of syntactic machine translation decoding.
Because the target side of a rule will generally
reorder the source side, it is complex (and some-
times impossible) to find synchronous rule bina-
rizations. However, we show that synchronous
binarizations are not necessary in a two-stage de-
coder. Instead, the grammar can be binarized one
way for the parsing stage, then rebinarized in a
different way for the reranking stage. Each indi-
vidual binarization considers only one monolin-
gual projection of the grammar, entirely avoid-
ing the constraints of synchronous binarization
and allowing binarizations that are separately op-
timized for each stage. Compared to n-ary for-
est reranking, even simple target-side binariza-
tion schemes improve overall decoding accuracy.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999127">
Syntactic machine translation decoders search
over a space of synchronous derivations, scoring
them according to both a weighted synchronous
grammar and an n-gram language model. The
rewrites of the synchronous translation gram-
mar are typically flat, n-ary rules. Past work
has synchronously binarized such rules for effi-
ciency (Zhang et al., 2006; Huang et al., 2008).
Unfortunately, because source and target orders
differ, synchronous binarizations can be highly
constrained and sometimes impossible to find.
Recent work has explored two-stage decoding,
which explicitly decouples decoding into a source
parsing stage and a target language model inte-
gration stage (Huang and Chiang, 2007). Be-
cause translation grammars continue to increase
in size and complexity, both decoding stages re-
quire efficient approaches (DeNero et al., 2009).
In this paper, we show how two-stage decoding
enables independent binarizations for each stage.
The source-side binarization guarantees cubic-
time construction of a derivation forest, while an
entirely different target-side binarization leads to
efficient forest reranking with a language model.
Binarizing a synchronous grammar twice inde-
pendently has two principal advantages over syn-
chronous binarization. First, each binarization can
be fully tailored to its decoding stage, optimiz-
ing the efficiency of both parsing and language
model reranking. Second, the ITG constraint on
non-terminal reordering patterns is circumvented,
allowing the efficient application of synchronous
rules that do not have a synchronous binarization.
The primary contribution of this paper is to es-
tablish that binarization of synchronous grammars
need not be constrained by cross-lingual reorder-
ing patterns. We also demonstrate that even sim-
ple target-side binarization schemes improve the
search accuracy of forest reranking with a lan-
guage model, relative to n-ary forest reranking.
</bodyText>
<sectionHeader confidence="0.959993" genericHeader="method">
2 Asynchronous Binarization
</sectionHeader>
<bodyText confidence="0.999967642857143">
Two-stage decoding consists of parsing and lan-
guage model integration. The parsing stage builds
a pruned forest of derivations scored by the trans-
lation grammar only. In the second stage, this for-
est is reranked by an n-gram language model. We
rerank derivations with cube growing, a lazy beam
search algorithm (Huang and Chiang, 2007).
In this paper, we focus on syntactic translation
with tree-transducer rules (Galley et al., 2006).
These synchronous rules allow multiple adjacent
non-terminals and place no restrictions on rule size
or lexicalization. Two example unlexicalized rules
appear in Figure 1, along with aligned and parsed
training sentences that would have licensed them.
</bodyText>
<subsectionHeader confidence="0.997466">
2.1 Constructing Translation Forests
</subsectionHeader>
<bodyText confidence="0.999982142857143">
The parsing stage builds a forest of derivations by
parsing with the source-side projection of the syn-
chronous grammar. Each forest node Pik com-
pactly encodes all parse derivations rooted by
grammar symbol P and spanning the source sen-
tence from positions i to j. Each derivation of Pik
is rooted by a rule with non-terminals that each
</bodyText>
<page confidence="0.977021">
141
</page>
<note confidence="0.9802925">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 141–144,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.99844875">
Figure 1: Two unlexicalized transducer rules (top) and
aligned, parsed training sentences from which they could be
extracted (bottom). The internal structure of English parses
has been omitted, as it is irrelevant to our decoding problem.
</figureCaption>
<bodyText confidence="0.999061136363636">
anchor to some child node C(t)
k` , where the symbol
C(t) is the tth child in the source side of the rule,
and i ≤ k &lt; E ≤ j.
We build this forest with a CKY-style algorithm.
For each span (i, j) from small to large, and each
symbol P, we iterate over all ways of building a
node Pij, first considering all grammar rules with
parent symbol P and then, for each rule, consider-
ing all ways of anchoring its non-terminals to ex-
isting forest nodes. Because we do not incorporate
a language model in this stage, we need only oper-
ate over the source-side projection of the grammar.
Of course, the number of possible anchorings
for a rule is exponential in the number of non-
terminals it contains. The purpose of binarization
during the parsing pass is to make this exponential
algorithm polynomial by reducing rule branching
to at most two non-terminals. Binarization reduces
algorithmic complexity by eliminating redundant
work: the shared substructures of n-ary rules are
scored only once, cached, and reused. Caching is
also commonplace in Early-style parsers that im-
plicitly binarize when applying n-ary rules.
While any binarization of the source side will
give a cubic-time algorithm, the particulars of a
grammar transformation can affect parsing speed
substantially. For instance, DeNero et al. (2009)
describe normal forms particularly suited to trans-
ducer grammars, demonstrating that well-chosen
binarizations admit cubic-time parsing algorithms
while introducing very few intermediate grammar
symbols. Binarization choice can also improve
monolingual parsing efficiency (Song et al., 2008).
The parsing stage of our decoder proceeds
by first converting the source-side projection of
the translation grammar into lexical normal form
(DeNero et al., 2009), which allows each rule to
be applied to any span in linear time, then build-
ing a binary-branching translation forest, as shown
in Figure 2(a). The intermediate nodes introduced
during this transformation do not have a target-
side projection or interpretation. They only exist
for the sake of source-side parsing efficiency.
</bodyText>
<subsectionHeader confidence="0.999666">
2.2 Collapsing Binarization
</subsectionHeader>
<bodyText confidence="0.997589814814815">
To facilitate a change in binarization, we transform
the translation forest into n-ary form. In the n-ary
forest, each hyperedge corresponds to an original
grammar rule, and all nodes correspond to original
�
grammar symbols, rather than those introduced
during binarizaiton. Transforming the entire for-
est to n-ary form is intractable, however, because
the number of hyperedges would be exponential in
n. Instead, we include only the top k n-ary back-
traces for each forest node. These backtraces can
be enumerated efficiently from the binary forest.
Figure 2(b) illustrates the result.
For efficiency, we follow DeNero et al. (2009)
in pruning low-scoring nodes in the n-ary for-
est under the weighted translation grammar. We
use a max-marginal threshold to prune unlikely
nodes, which can be computed through a max-
sum semiring variant of inside-outside (Goodman,
1996; Petrov and Klein, 2007).
Forest reranking with a language model can be
performed over this n-ary forest using the cube
growing algorithm of Huang and Chiang (2007).
Cube growing lazily builds k-best lists of deriva-
tions at each node in the forest by filling a node-
specific priority queue upon request from the par-
ent. N-ary forest reranking serves as our baseline.
</bodyText>
<subsectionHeader confidence="0.999632">
2.3 Reranking with Target-Side Binarization
</subsectionHeader>
<bodyText confidence="0.999370166666667">
Zhang et al. (2006) demonstrate that reranking
over binarized derivations improves search accu-
racy by better exploring the space of translations
within the strict confines of beam search. Binariz-
ing the forest during reranking permits pairs of ad-
jacent non-terminals in the target-side projection
of rules to be rescored at intermediate forest nodes.
This target-side binarization can be performed on-
the-fly: when a node Pij is queried for its k-best
list, we binarize its n-ary backtraces.
Suppose Pij can be constructed from a rule r
with target-side projection
</bodyText>
<equation confidence="0.762508">
P → f0 C1 i1 C2 E2 ... Cn in
</equation>
<bodyText confidence="0.9887865">
where C1, ... , Cn are non-terminal symbols that
are each anchored to a node C(i)
kl in the forest, and
i are (possibly empty) sequences of lexical items.
</bodyText>
<figure confidence="0.899758783783784">
yo ayer com’ en casa
yo ayer com’ en casa
PRP1 VBD3 PP4 NN2
S --+
PRP1 NN2 VBD3 PP4
S
PRP VBD PP NN
PRP1 VBD3 NN2 PP4
S --+
PRP1 NN2 VBD3 PP4
S
I ate at home yesterday
I ate yesterday at home
PRP VBD NN PP
142
S
S
PRP
NN
VBD
PP
“I ate”
PRP+VBD
PRP+VBD+NN
S
PRP+VBD+PP
PRP+NN+VBD
PRP+NN
PRP
NN
VBD
PP
PRP
NN
VBD
PP
yo ayer com’ en casa
</figure>
<equation confidence="0.968428055555556">
S — PRP1 VBD3 NN2 PPa
[[PRP1 NN2] VBD3] PPa
S — PRP1 VBD3 PPa NN2
[[PRP1 NN2] VBD3] PPa
yo ayer com’ en casa
PRP1 VBD3 NN2 PPa
PRP1 NN2 VBD3 PPa
PRP1 VBD3 PPa NN2
PRP1 NN2 VBD3 PPa
yo ayer com’ en casa
[[PRP1 VBD3] NN2] PPa
PRP1 NN2 VBD3 PPa
[[PRP1 VBD3] PPa] NN2
PRP1 NN2 VBD3 PPa
S —
S —
S —
S —
</equation>
<figure confidence="0.880872">
(a) Parsing stage binarization (b) Collapsed n-ary forest (c) Reranking stage binarization
</figure>
<figureCaption confidence="0.991717">
Figure 2: A translation forest as it evolves during two-stage decoding, along with two n-ary rules in the forest that are rebi-
narized. (a) A source-binarized forest constructed while parsing the source sentence with the translation grammar. (b) A flat
n-ary forest constructed by collapsing out the source-side binarization. (c) A target-binarized forest containing two derivations
of the root symbol—the second is dashed for clarity. Both derivations share the node PRP+VBD, which will contain a single
k-best list of translations during language model reranking. One such translation of PRP+VBD is shown: “I ate”.
</figureCaption>
<bodyText confidence="0.999994666666667">
We apply a simple left-branching binarization to
r, though in principle any binarization is possible.
We construct a new symbol B and two new rules:
</bodyText>
<equation confidence="0.9997215">
r1 : B → 0 C1 i1 C2 E2
r2 : P → B C3 6 ... Cnin
</equation>
<bodyText confidence="0.999983772727273">
These rules are also anchored to forest nodes. Any
Ci remains anchored to the same node as it was in
the n-ary forest. For the new symbol B, we intro-
duce a new forest node B that does not correspond
to any particular span of the source sentence. We
likewise transform the resulting r2 until all rules
have at most two non-terminal items. The original
rule r from the n-ary forest is replaced by binary
rules. Figure 2(c) illustrates the rebinarized forest.
Language model reranking treats the newly in-
troduced forest node B as any other node: building
a k-best derivation list by combining derivations
from C(1) and C(2) using rule r1. These deriva-
tions are made available to the parent of B, which
may be another introduced node (if more binariza-
tion were required) or the original root Pij.
Crucially, the ordering of non-terminals in the
source-side projection of r does not play a role
in this binarization process. The intermediate
nodes B may comprise translations of discontigu-
ous parts of the source sentence, as long as those
parts are contained within the span (i, j).
</bodyText>
<subsectionHeader confidence="0.999163">
2.4 Reusing Intermediate Nodes
</subsectionHeader>
<bodyText confidence="0.999977666666667">
The binarization we describe transforms the for-
est on a rule-by-rule basis. We must consider in-
dividual rules because they may contain different
lexical items and non-terminal orderings. How-
ever, two different rules that can build a node often
share some substructures. For instance, the two
rules in Figure 2 both begin with PRP followed by
VBD. In addition, these symbols are anchored to
the same source-side spans. Thus, binarizing both
rules yields the same intermediate forest node B.
In the case where two intermediate nodes share
the same intermediate rule anchored to the same
forest nodes, they can be shared. That is, we need
only generate one k-best list of derivations, then
use it in derivations rooted by both rules. Sharing
derivation lists in this way provides an additional
advantage of binarization over n-ary forest rerank-
ing. Not only do we assess language model penal-
ties over smaller partial derivations, but repeated
language model evaluations are cached and reused
across rules with common substructure.
</bodyText>
<sectionHeader confidence="0.999242" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.998936">
The utility of binarization for parsing is well
known, and plays an important role in the effi-
ciency of the parsing stage of decoding (DeNero et
al., 2009). The benefit of binarization for language
</bodyText>
<page confidence="0.998034">
143
</page>
<table confidence="0.997119333333333">
Forest Reranked BLEU Model Score
N-ary baseline 58.2 41,543
Left-branching binary 58.5 41,556
</table>
<tableCaption confidence="0.999099">
Table 1: Reranking a binarized forest improves BLEU by 0.3
</tableCaption>
<bodyText confidence="0.970194785714286">
and model score by 13 relative to an n-ary forest baseline by
reducing search errors during forest rescoring.
model reranking has also been established, both
for synchronous binarization (Zhang et al., 2006)
and for target-only binarization (Huang, 2007). In
our experiment, we evaluate the benefit of target-
side forest re-binarization in the two-stage decoder
of DeNero et al. (2009), relative to reranking n-ary
forests directly.
We translated 300 NIST 2005 Arabic sentences
to English with a large grammar learned from a
220 million word bitext, using rules with up to 6
non-terminals. We used a trigram language model
trained on the English side of this bitext. Model
parameters were tuned with MERT. Beam size was
limited to 200 derivations per forest node.
Table 1 shows a modest increase in model
and BLEU score from left-branching binarization
during language model reranking. We used the
same pruned n-ary forest from an identical parsing
stage in both conditions. Binarization did increase
reranking time by 25% because more k-best lists
are constructed. However, reusing intermediate
edges during reranking binarization reduced bina-
rized reranking time by 37%. We found that on
average, intermediate nodes introduced in the for-
est are used in 4.5 different rules, which accounts
for the speed increase.
</bodyText>
<sectionHeader confidence="0.999791" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.997302516129032">
Asynchronous binarization in two-stage decoding
allows us to select an appropriate grammar trans-
formation for each language. The source trans-
formation can optimize specifically for the parsing
stage of translation, while the target-side binariza-
tion can optimize for the reranking stage.
Synchronous binarization is of course a way to
get the benefits of binarizing both grammar pro-
jections; it is a special case of asynchronous bi-
narization. However, synchronous binarization is
constrained by the non-terminal reordering, lim-
iting the possible binarization options. For in-
stance, none of the binarization choices used in
Figure 2 on either side would be possible in a
synchronous binarization. There are rules, though
rare, that cannot be binarized synchronously at all
(Wu, 1997), but can be incorporated in two-stage
decoding with asynchronous binarization.
On the source side, these limited binarization
options may, for example, prevent a binarization
that minimizes intermediate symbols (DeNero et
al., 2009). On the target side, the speed of for-
est reranking depends upon the degree of reuse
of intermediate k-best lists, which in turn depends
upon the manner in which the target-side grammar
projection is binarized. Limiting options may pre-
vent a binarization that allows intermediate nodes
to be maximally reused. In future work, we look
forward to evaluating the wide array of forest bi-
narization strategies that are enabled by our asyn-
chronous approach.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999801315789474">
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009. Efficient parsing for transducer grammars. In Pro-
ceedings of the Annual Conference of the North American
Association for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich syn-
tactic translation models. In Proceedings of the Annual
Conference of the Association for Computational Linguis-
tics.
Joshua Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the Annual Meeting of the Association for
Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In Pro-
ceedings of the Annual Conference of the Association for
Computational Linguistics.
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin Knight.
2008. Binarization of synchronous context-free gram-
mars. Computational Linguistics.
Liang Huang. 2007. Binarization, synchronous binarization,
and target-side binarization. In Proceedings of the HLT-
NAACL Workshop on Syntax and Structure in Statistical
Translation (SSST).
Slav Petrov and Dan Klein. 2007. Improved inference for un-
lexicalized parsing. In Proceedings of the North American
Chapter of the Association for Computational Linguistics.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008. Better
binarization for the CKY parsing. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23:377–404.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.
2006. Synchronous binarization for machine translation.
In Proceedings of the North American Chapter of the As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.998596">
144
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912649">
<title confidence="0.999101">Asynchronous Binarization for Synchronous Grammars</title>
<author confidence="0.986144">Adam Pauls DeNero</author>
<author confidence="0.986144">Klein</author>
<affiliation confidence="0.993874">Computer Science Division University of California, Berkeley</affiliation>
<email confidence="0.95183">adpauls,</email>
<abstract confidence="0.999144222222222">of rules is critical for the efficiency of syntactic machine translation decoding. Because the target side of a rule will generally reorder the source side, it is complex (and sometimes impossible) to find synchronous rule binarizations. However, we show that synchronous binarizations are not necessary in a two-stage decoder. Instead, the grammar can be binarized one way for the parsing stage, then rebinarized in a different way for the reranking stage. Each individual binarization considers only one monolingual projection of the grammar, entirely avoiding the constraints of synchronous binarization and allowing binarizations that are separately opfor each stage. Compared to forest reranking, even simple target-side binarization schemes improve overall decoding accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Mohit Bansal</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Efficient parsing for transducer grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Conference of the North American Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1869" citStr="DeNero et al., 2009" startWordPosition="269" endWordPosition="272">tion grammar are typically flat, n-ary rules. Past work has synchronously binarized such rules for efficiency (Zhang et al., 2006; Huang et al., 2008). Unfortunately, because source and target orders differ, synchronous binarizations can be highly constrained and sometimes impossible to find. Recent work has explored two-stage decoding, which explicitly decouples decoding into a source parsing stage and a target language model integration stage (Huang and Chiang, 2007). Because translation grammars continue to increase in size and complexity, both decoding stages require efficient approaches (DeNero et al., 2009). In this paper, we show how two-stage decoding enables independent binarizations for each stage. The source-side binarization guarantees cubictime construction of a derivation forest, while an entirely different target-side binarization leads to efficient forest reranking with a language model. Binarizing a synchronous grammar twice independently has two principal advantages over synchronous binarization. First, each binarization can be fully tailored to its decoding stage, optimizing the efficiency of both parsing and language model reranking. Second, the ITG constraint on non-terminal reord</context>
<context position="5702" citStr="DeNero et al. (2009)" startWordPosition="869" endWordPosition="872">ntains. The purpose of binarization during the parsing pass is to make this exponential algorithm polynomial by reducing rule branching to at most two non-terminals. Binarization reduces algorithmic complexity by eliminating redundant work: the shared substructures of n-ary rules are scored only once, cached, and reused. Caching is also commonplace in Early-style parsers that implicitly binarize when applying n-ary rules. While any binarization of the source side will give a cubic-time algorithm, the particulars of a grammar transformation can affect parsing speed substantially. For instance, DeNero et al. (2009) describe normal forms particularly suited to transducer grammars, demonstrating that well-chosen binarizations admit cubic-time parsing algorithms while introducing very few intermediate grammar symbols. Binarization choice can also improve monolingual parsing efficiency (Song et al., 2008). The parsing stage of our decoder proceeds by first converting the source-side projection of the translation grammar into lexical normal form (DeNero et al., 2009), which allows each rule to be applied to any span in linear time, then building a binary-branching translation forest, as shown in Figure 2(a).</context>
<context position="7144" citStr="DeNero et al. (2009)" startWordPosition="1083" endWordPosition="1086"> a change in binarization, we transform the translation forest into n-ary form. In the n-ary forest, each hyperedge corresponds to an original grammar rule, and all nodes correspond to original � grammar symbols, rather than those introduced during binarizaiton. Transforming the entire forest to n-ary form is intractable, however, because the number of hyperedges would be exponential in n. Instead, we include only the top k n-ary backtraces for each forest node. These backtraces can be enumerated efficiently from the binary forest. Figure 2(b) illustrates the result. For efficiency, we follow DeNero et al. (2009) in pruning low-scoring nodes in the n-ary forest under the weighted translation grammar. We use a max-marginal threshold to prune unlikely nodes, which can be computed through a maxsum semiring variant of inside-outside (Goodman, 1996; Petrov and Klein, 2007). Forest reranking with a language model can be performed over this n-ary forest using the cube growing algorithm of Huang and Chiang (2007). Cube growing lazily builds k-best lists of derivations at each node in the forest by filling a nodespecific priority queue upon request from the parent. N-ary forest reranking serves as our baseline</context>
<context position="12382" citStr="DeNero et al., 2009" startWordPosition="1996" endWordPosition="1999">he same forest nodes, they can be shared. That is, we need only generate one k-best list of derivations, then use it in derivations rooted by both rules. Sharing derivation lists in this way provides an additional advantage of binarization over n-ary forest reranking. Not only do we assess language model penalties over smaller partial derivations, but repeated language model evaluations are cached and reused across rules with common substructure. 3 Experiments The utility of binarization for parsing is well known, and plays an important role in the efficiency of the parsing stage of decoding (DeNero et al., 2009). The benefit of binarization for language 143 Forest Reranked BLEU Model Score N-ary baseline 58.2 41,543 Left-branching binary 58.5 41,556 Table 1: Reranking a binarized forest improves BLEU by 0.3 and model score by 13 relative to an n-ary forest baseline by reducing search errors during forest rescoring. model reranking has also been established, both for synchronous binarization (Zhang et al., 2006) and for target-only binarization (Huang, 2007). In our experiment, we evaluate the benefit of targetside forest re-binarization in the two-stage decoder of DeNero et al. (2009), relative to re</context>
</contexts>
<marker>DeNero, Bansal, Pauls, Klein, 2009</marker>
<rawString>John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein. 2009. Efficient parsing for transducer grammars. In Proceedings of the Annual Conference of the North American Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3400" citStr="Galley et al., 2006" startWordPosition="494" endWordPosition="497"> that even simple target-side binarization schemes improve the search accuracy of forest reranking with a language model, relative to n-ary forest reranking. 2 Asynchronous Binarization Two-stage decoding consists of parsing and language model integration. The parsing stage builds a pruned forest of derivations scored by the translation grammar only. In the second stage, this forest is reranked by an n-gram language model. We rerank derivations with cube growing, a lazy beam search algorithm (Huang and Chiang, 2007). In this paper, we focus on syntactic translation with tree-transducer rules (Galley et al., 2006). These synchronous rules allow multiple adjacent non-terminals and place no restrictions on rule size or lexicalization. Two example unlexicalized rules appear in Figure 1, along with aligned and parsed training sentences that would have licensed them. 2.1 Constructing Translation Forests The parsing stage builds a forest of derivations by parsing with the source-side projection of the synchronous grammar. Each forest node Pik compactly encodes all parse derivations rooted by grammar symbol P and spanning the source sentence from positions i to j. Each derivation of Pik is rooted by a rule wi</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7379" citStr="Goodman, 1996" startWordPosition="1122" endWordPosition="1123">during binarizaiton. Transforming the entire forest to n-ary form is intractable, however, because the number of hyperedges would be exponential in n. Instead, we include only the top k n-ary backtraces for each forest node. These backtraces can be enumerated efficiently from the binary forest. Figure 2(b) illustrates the result. For efficiency, we follow DeNero et al. (2009) in pruning low-scoring nodes in the n-ary forest under the weighted translation grammar. We use a max-marginal threshold to prune unlikely nodes, which can be computed through a maxsum semiring variant of inside-outside (Goodman, 1996; Petrov and Klein, 2007). Forest reranking with a language model can be performed over this n-ary forest using the cube growing algorithm of Huang and Chiang (2007). Cube growing lazily builds k-best lists of derivations at each node in the forest by filling a nodespecific priority queue upon request from the parent. N-ary forest reranking serves as our baseline. 2.3 Reranking with Target-Side Binarization Zhang et al. (2006) demonstrate that reranking over binarized derivations improves search accuracy by better exploring the space of translations within the strict confines of beam search. B</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Parsing algorithms and metrics. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1722" citStr="Huang and Chiang, 2007" startWordPosition="247" endWordPosition="250">onous derivations, scoring them according to both a weighted synchronous grammar and an n-gram language model. The rewrites of the synchronous translation grammar are typically flat, n-ary rules. Past work has synchronously binarized such rules for efficiency (Zhang et al., 2006; Huang et al., 2008). Unfortunately, because source and target orders differ, synchronous binarizations can be highly constrained and sometimes impossible to find. Recent work has explored two-stage decoding, which explicitly decouples decoding into a source parsing stage and a target language model integration stage (Huang and Chiang, 2007). Because translation grammars continue to increase in size and complexity, both decoding stages require efficient approaches (DeNero et al., 2009). In this paper, we show how two-stage decoding enables independent binarizations for each stage. The source-side binarization guarantees cubictime construction of a derivation forest, while an entirely different target-side binarization leads to efficient forest reranking with a language model. Binarizing a synchronous grammar twice independently has two principal advantages over synchronous binarization. First, each binarization can be fully tailo</context>
<context position="3301" citStr="Huang and Chiang, 2007" startWordPosition="479" endWordPosition="482">synchronous grammars need not be constrained by cross-lingual reordering patterns. We also demonstrate that even simple target-side binarization schemes improve the search accuracy of forest reranking with a language model, relative to n-ary forest reranking. 2 Asynchronous Binarization Two-stage decoding consists of parsing and language model integration. The parsing stage builds a pruned forest of derivations scored by the translation grammar only. In the second stage, this forest is reranked by an n-gram language model. We rerank derivations with cube growing, a lazy beam search algorithm (Huang and Chiang, 2007). In this paper, we focus on syntactic translation with tree-transducer rules (Galley et al., 2006). These synchronous rules allow multiple adjacent non-terminals and place no restrictions on rule size or lexicalization. Two example unlexicalized rules appear in Figure 1, along with aligned and parsed training sentences that would have licensed them. 2.1 Constructing Translation Forests The parsing stage builds a forest of derivations by parsing with the source-side projection of the synchronous grammar. Each forest node Pik compactly encodes all parse derivations rooted by grammar symbol P an</context>
<context position="7544" citStr="Huang and Chiang (2007)" startWordPosition="1147" endWordPosition="1150">ead, we include only the top k n-ary backtraces for each forest node. These backtraces can be enumerated efficiently from the binary forest. Figure 2(b) illustrates the result. For efficiency, we follow DeNero et al. (2009) in pruning low-scoring nodes in the n-ary forest under the weighted translation grammar. We use a max-marginal threshold to prune unlikely nodes, which can be computed through a maxsum semiring variant of inside-outside (Goodman, 1996; Petrov and Klein, 2007). Forest reranking with a language model can be performed over this n-ary forest using the cube growing algorithm of Huang and Chiang (2007). Cube growing lazily builds k-best lists of derivations at each node in the forest by filling a nodespecific priority queue upon request from the parent. N-ary forest reranking serves as our baseline. 2.3 Reranking with Target-Side Binarization Zhang et al. (2006) demonstrate that reranking over binarized derivations improves search accuracy by better exploring the space of translations within the strict confines of beam search. Binarizing the forest during reranking permits pairs of adjacent non-terminals in the target-side projection of rules to be rescored at intermediate forest nodes. Thi</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Binarization of synchronous context-free grammars. Computational Linguistics.</title>
<date>2008</date>
<contexts>
<context position="1399" citStr="Huang et al., 2008" startWordPosition="202" endWordPosition="205">ng the constraints of synchronous binarization and allowing binarizations that are separately optimized for each stage. Compared to n-ary forest reranking, even simple target-side binarization schemes improve overall decoding accuracy. 1 Introduction Syntactic machine translation decoders search over a space of synchronous derivations, scoring them according to both a weighted synchronous grammar and an n-gram language model. The rewrites of the synchronous translation grammar are typically flat, n-ary rules. Past work has synchronously binarized such rules for efficiency (Zhang et al., 2006; Huang et al., 2008). Unfortunately, because source and target orders differ, synchronous binarizations can be highly constrained and sometimes impossible to find. Recent work has explored two-stage decoding, which explicitly decouples decoding into a source parsing stage and a target language model integration stage (Huang and Chiang, 2007). Because translation grammars continue to increase in size and complexity, both decoding stages require efficient approaches (DeNero et al., 2009). In this paper, we show how two-stage decoding enables independent binarizations for each stage. The source-side binarization gua</context>
</contexts>
<marker>Huang, Zhang, Gildea, Knight, 2008</marker>
<rawString>Liang Huang, Hao Zhang, Daniel Gildea, and Kevin Knight. 2008. Binarization of synchronous context-free grammars. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Binarization, synchronous binarization, and target-side binarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the HLTNAACL Workshop on Syntax and Structure in Statistical Translation (SSST).</booktitle>
<contexts>
<context position="12836" citStr="Huang, 2007" startWordPosition="2066" endWordPosition="2067">ts The utility of binarization for parsing is well known, and plays an important role in the efficiency of the parsing stage of decoding (DeNero et al., 2009). The benefit of binarization for language 143 Forest Reranked BLEU Model Score N-ary baseline 58.2 41,543 Left-branching binary 58.5 41,556 Table 1: Reranking a binarized forest improves BLEU by 0.3 and model score by 13 relative to an n-ary forest baseline by reducing search errors during forest rescoring. model reranking has also been established, both for synchronous binarization (Zhang et al., 2006) and for target-only binarization (Huang, 2007). In our experiment, we evaluate the benefit of targetside forest re-binarization in the two-stage decoder of DeNero et al. (2009), relative to reranking n-ary forests directly. We translated 300 NIST 2005 Arabic sentences to English with a large grammar learned from a 220 million word bitext, using rules with up to 6 non-terminals. We used a trigram language model trained on the English side of this bitext. Model parameters were tuned with MERT. Beam size was limited to 200 derivations per forest node. Table 1 shows a modest increase in model and BLEU score from left-branching binarization du</context>
</contexts>
<marker>Huang, 2007</marker>
<rawString>Liang Huang. 2007. Binarization, synchronous binarization, and target-side binarization. In Proceedings of the HLTNAACL Workshop on Syntax and Structure in Statistical Translation (SSST).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7404" citStr="Petrov and Klein, 2007" startWordPosition="1124" endWordPosition="1127">iton. Transforming the entire forest to n-ary form is intractable, however, because the number of hyperedges would be exponential in n. Instead, we include only the top k n-ary backtraces for each forest node. These backtraces can be enumerated efficiently from the binary forest. Figure 2(b) illustrates the result. For efficiency, we follow DeNero et al. (2009) in pruning low-scoring nodes in the n-ary forest under the weighted translation grammar. We use a max-marginal threshold to prune unlikely nodes, which can be computed through a maxsum semiring variant of inside-outside (Goodman, 1996; Petrov and Klein, 2007). Forest reranking with a language model can be performed over this n-ary forest using the cube growing algorithm of Huang and Chiang (2007). Cube growing lazily builds k-best lists of derivations at each node in the forest by filling a nodespecific priority queue upon request from the parent. N-ary forest reranking serves as our baseline. 2.3 Reranking with Target-Side Binarization Zhang et al. (2006) demonstrate that reranking over binarized derivations improves search accuracy by better exploring the space of translations within the strict confines of beam search. Binarizing the forest duri</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinying Song</author>
<author>Shilin Ding</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Better binarization for the CKY parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5994" citStr="Song et al., 2008" startWordPosition="905" endWordPosition="908">ly once, cached, and reused. Caching is also commonplace in Early-style parsers that implicitly binarize when applying n-ary rules. While any binarization of the source side will give a cubic-time algorithm, the particulars of a grammar transformation can affect parsing speed substantially. For instance, DeNero et al. (2009) describe normal forms particularly suited to transducer grammars, demonstrating that well-chosen binarizations admit cubic-time parsing algorithms while introducing very few intermediate grammar symbols. Binarization choice can also improve monolingual parsing efficiency (Song et al., 2008). The parsing stage of our decoder proceeds by first converting the source-side projection of the translation grammar into lexical normal form (DeNero et al., 2009), which allows each rule to be applied to any span in linear time, then building a binary-branching translation forest, as shown in Figure 2(a). The intermediate nodes introduced during this transformation do not have a targetside projection or interpretation. They only exist for the sake of source-side parsing efficiency. 2.2 Collapsing Binarization To facilitate a change in binarization, we transform the translation forest into n-</context>
</contexts>
<marker>Song, Ding, Lin, 2008</marker>
<rawString>Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008. Better binarization for the CKY parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--377</pages>
<contexts>
<context position="14691" citStr="Wu, 1997" startWordPosition="2354" endWordPosition="2355">lly for the parsing stage of translation, while the target-side binarization can optimize for the reranking stage. Synchronous binarization is of course a way to get the benefits of binarizing both grammar projections; it is a special case of asynchronous binarization. However, synchronous binarization is constrained by the non-terminal reordering, limiting the possible binarization options. For instance, none of the binarization choices used in Figure 2 on either side would be possible in a synchronous binarization. There are rules, though rare, that cannot be binarized synchronously at all (Wu, 1997), but can be incorporated in two-stage decoding with asynchronous binarization. On the source side, these limited binarization options may, for example, prevent a binarization that minimizes intermediate symbols (DeNero et al., 2009). On the target side, the speed of forest reranking depends upon the degree of reuse of intermediate k-best lists, which in turn depends upon the manner in which the target-side grammar projection is binarized. Limiting options may prevent a binarization that allows intermediate nodes to be maximally reused. In future work, we look forward to evaluating the wide ar</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23:377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1378" citStr="Zhang et al., 2006" startWordPosition="198" endWordPosition="201">mar, entirely avoiding the constraints of synchronous binarization and allowing binarizations that are separately optimized for each stage. Compared to n-ary forest reranking, even simple target-side binarization schemes improve overall decoding accuracy. 1 Introduction Syntactic machine translation decoders search over a space of synchronous derivations, scoring them according to both a weighted synchronous grammar and an n-gram language model. The rewrites of the synchronous translation grammar are typically flat, n-ary rules. Past work has synchronously binarized such rules for efficiency (Zhang et al., 2006; Huang et al., 2008). Unfortunately, because source and target orders differ, synchronous binarizations can be highly constrained and sometimes impossible to find. Recent work has explored two-stage decoding, which explicitly decouples decoding into a source parsing stage and a target language model integration stage (Huang and Chiang, 2007). Because translation grammars continue to increase in size and complexity, both decoding stages require efficient approaches (DeNero et al., 2009). In this paper, we show how two-stage decoding enables independent binarizations for each stage. The source-</context>
<context position="7809" citStr="Zhang et al. (2006)" startWordPosition="1191" endWordPosition="1194">est under the weighted translation grammar. We use a max-marginal threshold to prune unlikely nodes, which can be computed through a maxsum semiring variant of inside-outside (Goodman, 1996; Petrov and Klein, 2007). Forest reranking with a language model can be performed over this n-ary forest using the cube growing algorithm of Huang and Chiang (2007). Cube growing lazily builds k-best lists of derivations at each node in the forest by filling a nodespecific priority queue upon request from the parent. N-ary forest reranking serves as our baseline. 2.3 Reranking with Target-Side Binarization Zhang et al. (2006) demonstrate that reranking over binarized derivations improves search accuracy by better exploring the space of translations within the strict confines of beam search. Binarizing the forest during reranking permits pairs of adjacent non-terminals in the target-side projection of rules to be rescored at intermediate forest nodes. This target-side binarization can be performed onthe-fly: when a node Pij is queried for its k-best list, we binarize its n-ary backtraces. Suppose Pij can be constructed from a rule r with target-side projection P → f0 C1 i1 C2 E2 ... Cn in where C1, ... , Cn are non</context>
<context position="12789" citStr="Zhang et al., 2006" startWordPosition="2058" endWordPosition="2061">sed across rules with common substructure. 3 Experiments The utility of binarization for parsing is well known, and plays an important role in the efficiency of the parsing stage of decoding (DeNero et al., 2009). The benefit of binarization for language 143 Forest Reranked BLEU Model Score N-ary baseline 58.2 41,543 Left-branching binary 58.5 41,556 Table 1: Reranking a binarized forest improves BLEU by 0.3 and model score by 13 relative to an n-ary forest baseline by reducing search errors during forest rescoring. model reranking has also been established, both for synchronous binarization (Zhang et al., 2006) and for target-only binarization (Huang, 2007). In our experiment, we evaluate the benefit of targetside forest re-binarization in the two-stage decoder of DeNero et al. (2009), relative to reranking n-ary forests directly. We translated 300 NIST 2005 Arabic sentences to English with a large grammar learned from a 220 million word bitext, using rules with up to 6 non-terminals. We used a trigram language model trained on the English side of this bitext. Model parameters were tuned with MERT. Beam size was limited to 200 derivations per forest node. Table 1 shows a modest increase in model and</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>