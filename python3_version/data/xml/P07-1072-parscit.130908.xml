<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.059426">
<title confidence="0.997552">
Improving the Interpretation of Noun Phrases with Cross-linguistic
Information
</title>
<author confidence="0.995349">
Roxana Girju
</author>
<affiliation confidence="0.998909">
University of Illinois at Urbana-Champaign
</affiliation>
<email confidence="0.998339">
girju@uiuc.edu
</email>
<sectionHeader confidence="0.99564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953266666667">
This paper addresses the automatic classifi-
cation of semantic relations in noun phrases
based on cross-linguistic evidence from a
set of five Romance languages. A set
of novel semantic and contextual English–
Romance NP features is derived based on
empirical observations on the distribution
of the syntax and meaning of noun phrases
on two corpora of different genre (Europarl
and CLUVI). The features were employed
in a Support Vector Machines algorithm
which achieved an accuracy of 77.9% (Eu-
roparl) and 74.31% (CLUVI), an improve-
ment compared with two state-of-the-art
models reported in the literature.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974755102041">
Semantic knowledge is very important for any ap-
plication that requires a deep understanding of natu-
ral language. The automatic acquisition of semantic
information in text has become increasingly impor-
tant in ontology development, information extrac-
tion, question answering, and other advanced natural
language processing applications.
In this paper we present a model for the auto-
matic semantic interpretation of noun phrases (NPs),
which is the task of determining the semantic re-
lation among the noun constituents. For example,
family estate encodes a POSSESSION relation, while
dress of silk refers to PART-WHOLE. The problem,
while simple to state is hard to solve. The rea-
son is that the meaning of these constructions is
most of the time ambiguous or implicit. Interpreting
NPs correctly requires various types of information
from world knowledge to complex context features.
Moreover, the extension of this task to other natu-
ral languages brings forward new issues and prob-
lems. For instance, beer glass translates into tarro
de cerveza in Spanish, bicchiere da birra in Italian,
verre a` bi`ere in French, and pahar de bere in Roma-
nian. Thus, an important research question is how
do the syntactic constructions in the target language
contribute to the preservation of meaning in context.
In this paper we investigate noun phrases based on
cross-linguistic evidence and present a domain inde-
pendent model for their semantic interpretation. We
aim at uncovering the general aspects that govern
the semantics of NPs in English based on a set of
five Romance languages: Spanish, Italian, French,
Portuguese, and Romanian. The focus on Romance
languages is well motivated. It is mostly true that
English noun phrases translate into constructions of
the form N P N in Romance languages where, as
we will show below, the P (preposition) varies in
ways that correlate with the semantics. Thus Ro-
mance languages will give us another source of evi-
dence for disambiguating the semantic relations in
English NPs. We also present empirical observa-
tions on the distribution of the syntax and meaning
of noun phrases on two different corpora based on
two state-of-the-art classification tag sets: Lauer’s
set of 8 prepositions (Lauer, 1995) and our list of 22
semantic relations. We show that various crosslin-
gual cues can help in the NP interpretation task when
employed in an SVM model. The results are com-
pared against two state of the art approaches: a su-
</bodyText>
<page confidence="0.98593">
568
</page>
<note confidence="0.925547">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 568–575,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999508083333334">
pervised machine learning model, Semantic Scatter-
ing (Moldovan and Badulescu, 2005), and a web-
based probabilistic model (Lapata and Keller, 2004).
The paper is organized as follows. In Section 2
we present a summary of the previous work. Sec-
tion 3 lists the syntactic and semantic interpretation
categories used along with observations regarding
their distribution on the two different cross-lingual
corpora. Sections 4 and 5 present a learning model
and results for the interpretation of English noun
phrases. Finally, in Section 6 we offer some dis-
cussion and conclusions.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999929242424242">
Currently, the best-performing NP interpretation
methods in computational linguistics focus mostly
on two consecutive noun instances (noun com-
pounds) and rely either on rather ad-hoc, domain-
specific semantic taxonomies, or on statistical mod-
els on large collections of unlabeled data. Recent
results have shown that symbolic noun compound
interpretation systems using machine learning tech-
niques coupled with a large lexical hierarchy per-
form with very good accuracy, but they are most of
the time tailored to a specific domain (Rosario and
Hearst, 2001). On the other hand, the majority of
corpus statistics approaches to noun compound in-
terpretation collect statistics on the occurrence fre-
quency of the noun constituents and use them in a
probabilistic model (Lauer, 1995). More recently,
(Lapata and Keller, 2004) showed that simple unsu-
pervised models perform significantly better when
the frequencies are obtained from the web, rather
than from a large standard corpus. Other researchers
(Pantel and Pennacchiotti, 2006), (Snow et al., 2006)
use clustering techniques coupled with syntactic de-
pendency features to identify IS-A relations in large
text collections. (Kim and Baldwin, 2006) and (Tur-
ney, 2006) focus on the lexical similarity of unseen
noun compounds with those found in training.
However, although the web-based solution might
overcome the data sparseness problem, the current
probabilistic models are limited by the lack of deep
linguistic information. In this paper we investigate
the role of cross-linguistic information in the task
of English NP semantic interpretation and show the
importance of a set of novel linguistic features.
</bodyText>
<sectionHeader confidence="0.935534" genericHeader="method">
3 Corpus Analysis
</sectionHeader>
<bodyText confidence="0.999891222222222">
For a better understanding of the meaning of the
N N and N P N instances, we analyzed the seman-
tic behavior of these constructions on a large cross-
linguistic corpora of examples. We are interested
in what syntactic constructions are used to trans-
late the English instances to the target Romance lan-
guages and vice-versa, what semantic relations do
these constructions encode, and what is the corpus
distribution of the semantic relations.
</bodyText>
<subsectionHeader confidence="0.999959">
3.1 Lists of semantic classification relations
</subsectionHeader>
<bodyText confidence="0.999981133333333">
Although the NP interpretation problem has been
studied for a long time, researchers haven’t agreed
on the number and the level of abstraction of these
semantic categories. They can vary from a few
prepositions (Lauer, 1995) to hundreds or thousands
specific semantic relations (Finin, 1980). The more
abstract the categories, the more noun phrases are
covered, but also the more room for variation as to
which category a phrase should be assigned.
In this paper we experiment with two state of the
art classification sets used in NP interpretation. The
first is a core set of 22 semantic relations (22 SRs)
identified by us from the computational linguistics
literature. This list, presented in Table 1 along with
examples is general enough to cover a large major-
ity of text semantics while keeping the semantic re-
lations to a manageable number. The second set is
Lauer’s list of 8 prepositions (8 PP) and can be ap-
plied only to noun compounds (of, for, with, in, on,
at, about, and from – e.g., according to this classifi-
cation, love story can be classified as story about
love). We selected these sets as they are of different
size and contain semantic classification categories at
different levels of abstraction. Lauer’s list is more
abstract and, thus capable of encoding a large num-
ber of noun compound instances, while the 22-SR
list contains finer grained semantic categories. We
show below the coverage of these semantic lists on
two different corpora and how well they solve the
interpretation problem of noun phrases.
</bodyText>
<subsectionHeader confidence="0.99929">
3.2 The data
</subsectionHeader>
<bodyText confidence="0.9999525">
The data was collected from two text collections
with different distributions and of different genre,
</bodyText>
<page confidence="0.996564">
569
</page>
<construct confidence="0.7709075">
POSSESSION (family estate); KINSHIP (sister of the boy); PROPERTY (lubricant viscosity); AGENT (return of the natives);
THEME (acquisition of stock); TEMPORAL (morning news); DEPICTION-DEPICTED (a picture of my niece); PART-WHOLE
(brush hut); HYPERNYMY (IS-A) (daisy flower); CAUSE (scream of pain); MAKE/PRODUCE (chocolate factory); INSTRUMENT
(laser treatment); LOCATION (castle in the desert); PURPOSE (cough syrup); SOURCE (grapefruit oil); TOPIC (weather report);
MANNER (performance with passion); beneficiary (rights of citizens); MEANS (bus service); EXPERIENCER (fear of the girl);
MEASURE (cup of sugar); TYPE (framework law);
</construct>
<tableCaption confidence="0.998192">
Table 1: The list of 22 semantic relations (22-SRs).
</tableCaption>
<bodyText confidence="0.99986252">
Europarl1 and CLUVI2. The Europarl data was as-
sembled by combining the Spanish-English, Italian-
English, French-English and Portuguese-English
corpora which were automatically aligned based on
exact matches of English translations. Then, we
considered only the English sentences which ap-
peared verbatim in all four language pairs. The re-
sulting English corpus contained 10,000 sentences
which were syntactically parsed (Charniak, 2000).
From these we extracted the first 3,000 NP instances
(N N: 48.82% and N P N: 51.18%).
CLUVI is an open text repository of parallel cor-
pora of contemporary oral and written texts in some
of the Romance languages. Here, we focused only
on the English-Portuguese and English-Spanish par-
allel texts from the works of John Steinbeck, H. G.
Wells, J. Salinger, and others. Using the CLUVI
search interface we created a sentence-aligned par-
allel corpus of 2,800 English-Spanish and English-
Portuguese sentences. The English versions were
automatically parsed after which each N N and
N P N instance thus identified was manually mapped
to the corresponding translations. The resulting cor-
pus contains 2,200 English instances with a distribu-
tion of 26.77% N N and 73.23% N P N.
</bodyText>
<subsectionHeader confidence="0.999454">
3.3 Corpus Annotation
</subsectionHeader>
<bodyText confidence="0.999978375">
For each corpus, each NP instance was presented
separately to two experienced annotators in a web
interface in context along with the English sentence
and its translations. Since the corpora do not cover
some of the languages (Romanian in Europarl and
CLUVI, and Italian and French in CLUVI), three
other native speakers of these languages and flu-
ent in English provided the translations which were
</bodyText>
<footnote confidence="0.998774333333333">
1http://www.isi.edu/koehn/europarl/. This corpus contains
over 20 million words in eleven official languages of the Euro-
pean Union covering the proceedings of the European Parlia-
ment from 1996 to 2001.
2CLUVI - Linguistic Corpus of the University of Vigo - Par-
allel Corpus 2.1 - http://sli.uvigo.es/CLUVI/
</footnote>
<bodyText confidence="0.999745">
added to the list. The two computational semantics
annotators had to tag each English constituent noun
with its corresponding WordNet sense and each in-
stance with the corresponding semantic category. If
the word was not found in WordNet the instance was
not considered. Whenever the annotators found an
example encoding a semantic category other than
those provided or they didn’t know what interpre-
tation to give, they had to tag it as “OTHER-SR”, and
respectively “OTHER-PP”3. The details of the anno-
tation task and the observations drawn from there are
presented in a companion paper (Girju, 2007).
The corpus instances used in the corpus analy-
sis phase have the following format: &lt;NPEn ;NPEs;
NPIt; NPFr; NPPort; NPRo; target&gt;. The word
target is one of the 23 (22 + OTHER-SR) seman-
tic relations and one of the eight prepositions con-
sidered or OTHER-PP (with the exception of those
N P N instances that already contain a preposi-
tion). For example, &lt;development cooperation;
cooperaci´on para el desarrollo; cooperazione allo
sviluppo; coop´eration au d´eveloppement; cooperare
pentru dezvoltare; PURPOSE / FOR&gt;.
The annotators’ agreement was measured using
</bodyText>
<equation confidence="0.8937445">
Kappa statistics: K = Pr(�)�P r(E)
1�P r(E) , where Pr(A)
</equation>
<bodyText confidence="0.998457692307692">
is the proportion of times the annotators agree and
Pr(E) is the probability of agreement by chance.
The Kappa values were obtained on Europarl (N N:
0.80 for 8-PP and 0.61 for 22-SR; N P N: 0.67 for
22-SR) and CLUVI (N N: 0.77 for 8-PP and 0.56 for
22-SR; N P N: 0.68 for 22-SR). We also computed
the number of pairs that were tagged with OTHER
by both annotators for each semantic relation and
preposition paraphrase, over the number of exam-
ples classified in that category by at least one of the
judges (in Europarl: 91% for 8-PP and 78% for 22-
SR; in CLUVI: 86% for 8-PP and 69% for 22-SR).
The agreement obtained on the Europarl corpus is
</bodyText>
<footnote confidence="0.9995525">
3The annotated corpora resulted in this research is available
at http://apfel.ai.uiuc.edu.
</footnote>
<page confidence="0.974025">
570
</page>
<bodyText confidence="0.945809844262295">
higher than the one on CLUVI on both classification cle}, .. {entity}. This feature helps generalize over
sets. This is partially explained by the distribution of the semantic classes of the two nouns in the corpus.
semantic relations in both corpora, as will be shown F3 and F4. WordNet derivationally related form
in the next subsection. specifies if the head (F3) and the modifier (F4) nouns
3.4 Cross-linguistic distribution of Syntactic are related to a corresponding WordNet verb (e.g.
Constructions statement derived from to state; cry from to cry).
From the sets of 2,954 (Europarl) and 2,168 F5. Prepositional cues that link the two nouns in an
(CLUVI) instances resulted after annotation, the NP. These can be either simple or complex preposi-
data show that over 83% of the translation patterns tions such as “of” or “according to”. In case of N N
for both text corpora on all languages were of the instances, this feature is “–” (e.g., framework law).
type N N and N P N. However, while their distribu- F6 and F7. Type of nominalized noun indicates the
tion is balanced in the Europarl corpus (about 45%, specific class of nouns the head (F6) or modifier (F7)
with a 64% N P N – 26% N N ratio for Romanian), belongs to depending on the verb it derives from.
in CLUVI the N P N constructions occur in more First, we check if the noun is a nominalization. For
than 85% of the cases (again, with the exception of English we used NomLex-Plus (Meyers et al., 2004)
Romanian – 50%). It is interesting to note here that to map nouns to corresponding verbs.5 For exam-
some of the English NPs are translated into both ple, “destruction of the city”, where destruction is
noun–noun and noun–adjective compounds in the a nominalization. F6 and F7 may overlap with fea-
target languages. For example, love affair translates tures F3 and F4 which are used in case the noun to be
in Italian as storia d’amore or the noun–adjective checked does not have an entry in the NomLex-Plus
compound relazione amorosa. There are also in- dictionary. These features are of particular impor-
stances that have just one word correspondent in tance since they impose some constraints on the pos-
the target language (e.g., ankle boot is bottine in sible set of relations the instance can encode. They
French). The rest of the data is encoded by other take the following values (identified based on list of
syntactic paraphrases (e.g., bomb site is luogo dove verbs extracted from VerbNet (Kipper et al., 2000)):
e` esplosa la bomba (It.)). 4. a. Active form nouns which have an intrinsic
From the initial corpus we considered those En- active voice predicate-argument structure. (Giorgi
glish instances that had all the translations encoded and Longobardi, 1991) argue that in English this is a
only by N N and N P N. Out of these, we selected necessary restriction. Most of the time, they rep-
only 1,023 Europarl and 1,008 CLUVI instances en- resent states of emotion, such as fear, desire, etc.
coded by N N and N P N in all languages considered These nouns mark their internal argument through
and resulted after agreement. of and require most of the time prepositions like por
4 Model and not de when translated in Romance. Our obser-
4.1 Feature space vations on the Romanian translations (captured by
We have identified and experimented with 13 NP features F12 and F13 below) show that the possible
features presented below. With the exceptions of cases of ambiguity are solved by the type of syntac-
features F1-F5 (Girju et al., 2005), all the other fea- tic construction used. For example, N N genitive-
tures are novel. marked constructions are used for EXPERIENCER–
A. English Features encoding instances, while N de N or N pentru N (N
F1 and F2. Noun semantic class specifies the Word- for N) are used for other relations. Such examples
Net sense of the head (F1) and modifier noun (F2) are the love of children – THEME (and not the love by
and implicitly points to all its hypernyms. For ex- the children). (Giorgi and Longobardi, 1991) men-
ample, the hypernyms of car#1 are: {motor vehi- tion that with such nouns that resist passivisation,
5NomLex-Plus is a hand-coded database of 5,000 verb nom-
inalizations, de-adjectival, and de-adverbial nouns including the
corresponding subcategorization frames (verb-argument struc-
ture information).
4“the place where the bomb is exploded” (It.)
571
the preposition introducing the internal argument, 4.2 Learning Models
even if it is of, has always a semantic content, and We have experimented with the support vector ma-
is not a bare case-marker realizing the genitive case. chines (SVM) model6 and compared the results
b. Unaccusative (ergative) nouns which are de- against two state-of-the-art models: a supervised
rived from ergative verbs that take only internal ar- model, Semantic Scattering (SS), (Moldovan and
guments (e.g., not agentive ones). For example, the Badulescu, 2005), and a web-based unsupervised
transitive verb to disband allows the subject to be model (Lapata and Keller, 2004). The SVM and SS
deleted as in the following sentences (1) “The lead models were trained and tested on the Europarl and
singer disbanded the group in 1991.” and (2) “The CLUVI corpora using a 8:2 ratio. The test dataset
group disbanded.”. Thus, the corresponding erga- was randomly selected from each corpus and the test
tive nominalization the disbandment of the group en- nouns (only for English) were tagged with the cor-
codes a THEME relation and not AGENT. responding sense in context using a state of the art
c. Unergative (intransitive) nouns are derived WSD tool (Mihalcea and Faruque, 2004).
from intransitive verbs and take only AGENT seman- After the initial NP instances in the training and
tic relations. For example, the departure of the girl. test corpora were expanded with the corresponding
d. Inherently passive nouns such as the cap- features, we had to prepare them for SVM and SS.
ture of the soldier. These nouns, like the verbs they The method consists of a set of automatic iterative
are derived from, assume a default AGENT (subject) procedures of specialization of the English nouns on
and being transitive, associate to their internal argu- the WordNet IS-A hierarchy. Thus, after a set of nec-
ment (introduced by “of” in the example above) the essary specialization iterations, the method produces
THEME relation. specialized examples which through supervised ma-
B. Romance Features chine learning are transformed into sets of seman-
F8, F9, F10, F11 and F12. Prepositional cues that tic rules. This specialization procedure improves
link the two nouns are extracted from each transla- the system’s performance since it efficiently sepa-
tion of the English instance: F8 (Es.), F9 (Fr.), F10 rates the positive and negative noun-noun pairs in
(It.), F11 (Port.), and F12 (Ro.). These can be either the WordNet hierarchy.
simple or complex prepositions (e.g., de, in materia Initially, the training corpus consists of examples
de (Es.)) in all five Romance languages, or the Ro- in the format exemplified by the feature space. Note
manian genitival article a/ai/ale. In Romanian the that for the English NP instances, each noun con-
genitive case is assigned by the definite article of the stituent was expanded with the corresponding Word-
first noun to the second noun, case realized as a suf- Net top semantic class. At this point, the general-
fix if the second noun is preceded by the definite arti- ized training corpus contains two types of examples:
cle or as one of the genitival articles a/ai/ale. For ex- unambiguous and ambiguous. The second situation
ample, the noun phrase the beauty of the girl is trans- occurs when the training corpus classifies the same
lated as frumuset¸ea fetei (beauty-the girl-gen), and noun – noun pair into more than one semantic cat-
the beauty of a girl as frumuset¸ea unei fete (beauty- egory. For example, both relationships “chocolate
the gen girl). For N N instances, this feature is “–”. cake”-PART-WHOLE and “chocolate article”-TOPIC
F13. Noun inflection is defined only for Romanian are mapped into the more general type &lt;entity#1,
and shows if the modifier noun is inflected (indicates entity#1, PART-WHOLE/TOPIC&gt;7. We recursively
the genitive case). This feature is used to help differ- specialize these examples to eliminate the ambigu-
entiate between instances encoding IS-A and other ity. By specialization, the semantic class is replaced
semantic relations in N N compounds in Romanian. with the corresponding hyponym for that particular
It also helps in features F6 and F7, case a) when the sense, i.e. the concept immediately below in the hi-
choice of syntactic construction reflects different se- erarchy. These steps are repeated until there are no
mantic content. For example, iubirea pentru copii
(N P N) (the love for children) and not iubirea copi-
ilor (N N) (love expressed by the children).
572
6We used the package LIBSVM with a radial-based kernel
http://www.csie.ntu.edu.tw/∼cjlin/libsvm/
7The specialization procedure applies only to features 1, 2.
more ambiguous examples. For the example above,
the specialization stops at the first hyponym of en-
tity: physical entity (for cake) and abstract entity
(for article). For the unambiguous examples in the
generalized training corpus (those that are classified
with a single semantic relation), constraints are de-
termined using cross validation on SVM.
A. Semantic Scattering uses a training data set
to establish a boundary G* on WordNet noun hier-
archies such that each feature pair of noun – noun
senses fij on this boundary maps uniquely into one
of a predefined list of semantic relations, and any
feature pair above the boundary maps into more than
one semantic relation. For any new pair of noun–
noun senses, the model finds the closest WordNet
boundary pair.
The authors define with SCm = {fmi } and
SCh = {fhj } the sets of semantic class features
for modifier noun and, respectively head noun. A
pair of &lt;modifier – head&gt; nouns maps uniquely
into a semantic class feature pair &lt; fim, fhj &gt;,
denoted as fij. The probability of a semantic re-
</bodyText>
<equation confidence="0.5807885">
lation r given feature pair fij, P(r|fij) = n(r,fij)
n(fij) ,
</equation>
<bodyText confidence="0.9961855">
is defined as the ratio between the number of oc-
currences of a relation r in the presence of fea-
ture pair fij over the number of occurrences of
feature pair fij in the corpus. The most proba-
ble semantic relation rˆ is arg maxrER P(r|fij) =
arg maxrER P(fij|r)P(r).
B. (Zapata and Keller, 2004)’s web-based un-
supervised model classifies noun - noun instances
based on Lauer’s list of 8 prepositions and uses
the web as training corpus. They show that the
best performance is obtained with the trigram model
f(n1, p, n2). The count used for a given trigram is
the number of pages returned by Altavista on the tri-
gram corresponding queries. For example, for the
test instance war stories, the best number of hits was
obtained with the query stories about war.
For the Europarl and CLUVI test sets, we repli-
cated Lapata &amp; Keller’s experiments using Google8.
We formed inflected queries with the patterns they
proposed and searched the web.
</bodyText>
<footnote confidence="0.9613754">
8As Google limits the number of queries to 1,000 per day,
we repeated the experiment for a number of days. Although
(Lapata and Keller, 2004) used Altavista in their experiments,
they showed there is almost no difference between the correla-
tions achieved using Google and Altavista counts.
</footnote>
<sectionHeader confidence="0.978234" genericHeader="evaluation">
5 Experimental results
</sectionHeader>
<bodyText confidence="0.998060217391304">
Table 2 shows the results obtained against SS and
Lapata &amp; Keller’s model on both corpora and the
contribution the features exemplified in one baseline
and six versions of the SVM model. The baseline is
defined only for the English part of the NP feature
set and measures the the contribution of the Word-
Net IS-A lexical hierarchy specialization. The base-
line does not differentiate between unambiguous and
ambiguous training examples (after just one level
specialization) and thus, does not specialize the am-
biguous ones. Moreover, here we wanted to see what
is the difference between SS and SVM, and what is
the contribution of the other English features, such
as preposition and nominalization (F1–F7).
The table shows that, overall the performance is
better for the Europarl corpus than for CLUVI. For
the Baseline and SV M1, SS [F1 + F2] gives bet-
ter results than SVM. The inclusion of other English
features (SVM [F1–F7]) adds more than 15% (with
a higher increase in Europarl) for SV M1.
The contribution of Romance linguistic features.
Since our intuition is that the more translations are
provided for an English noun phrase instance, the
better the results, we wanted to see what is the im-
pact of each Romance language on the overall per-
formance. Thus, SVM2 shows the results obtained
for English and the Romance language that con-
tributed the least to the performance (F1–F12). Here
we computed the performance on all five English –
Romance language combinations and chose the Ro-
mance language that provided the best result. Thus,
SVM #2, #3, #4, #5, and #6 add Spanish, French,
Italian, Portuguese, and Romanian in this order and
show the contribution of each Romance preposition
and all features for English.
The language ranking in Table 2 shows that Ro-
mance languages considered here have a different
contribution to the overall performance. While the
addition of Italian in Europarl decreases the per-
formance, Portuguese doesn’t add anything. How-
ever, a closer analysis of the data shows that this
is mostly due to the distribution of the corpus in-
stances. For example, French, Italian, Spanish, and
Portuguese are most of the time consistent in the
choice of preposition (e.g. most of the time, if the
preposition ’de’ (’of’) is used in French, then the
</bodyText>
<page confidence="0.996361">
573
</page>
<table confidence="0.999490266666667">
Learning models Results [%]
CLUVI Europarl
8-PP 22-SR 8-PP 22-SR
Baseline (En.) (no specializ.) SS (F1+F2) 44.11 48.03 38.7 38
SVM (F1+F2) 36.37 40.67 31.18 34.81
SVM (F1-F7) – 52.15 – 47.37
SVM1 (En.) SS (F1+F2) 56.22 61.33 53.1 56.81
SVM (F1+F2) 45.08 46.1 40.23 42.2
SVM (F1-F7) – 62.54 – 74.19
SVM2 (En. + Es.) SVM (F1-F8) – 64.18 – 75.74
SVM3 (En.+Es.+Fr.) SVM (F1-F9) – 67.8 – 76.52
SVM4 (En.+Es.+Fr.+It.) SVM (F1-F10) – 66.31 – 75.74
SVM5 (En.+Es.+Fr.+It+Port.) SVM (F1-F11) – 67.12 – 75.74
SVM6 (En.+Romance: F1–F13) – 74.31 – 77.9
Lapata &amp; Keller’s unsupervised model (En.) 44.15 – 45.31 –
</table>
<tableCaption confidence="0.931671">
Table 2: The performance of the cross-linguistic SVM models compared against one baseline, SS model and
Lapata &amp; Keller’s unsupervised model. Accuracy (number of correctly labeled instances over the number of
instances in the test set).
</tableCaption>
<bodyText confidence="0.998711596774194">
corresponding preposition is used in the other four
language translations). A notable exception here
is Romanian which provides two possible construc-
tions: the N P N and the genitive-marked N N. The
table shows (in the increase in performance between
5V M5 and 5V M6) that this choice is not random,
but influenced by the meaning of the instances (fea-
tures F12, F13). This observation is also supported
by the contribution of each feature to the overall per-
formance. For example, in Europarl, the WordNet
verb and nominalization features of the head noun
(F3, F6) have a contribution of 4.08%, while for the
modifier nouns it decreases by about 2%. The prepo-
sition (F5) contributes 4.41% (Europarl) and 5.24%
(CLUVI) to the overall performance.
A closer analysis of the data shows that in Eu-
roparl most of the N N instances were naming noun
compounds such as framework law (TYPE) and,
most of the time, are encoded by N N patterns in
the target languages (e.g., legge quadro (It.)). In
the CLUVI corpus, on the other hand, the N N Ro-
mance translations represented only 1% of the data.
A notable exception here is Romanian where most
NPs are represented as genitive–marked noun com-
pounds. However, there are instances that are en-
coded mostly or only as N P N constructions and this
choice correlates with the meaning of the instance.
For example, the milk glass (PURPOSE) translates
as paharul de lapte (glass-the of milk) and not as
paharul laptelui (glass-the milk-gen), the olive oil
(SOURCE) translates as uleiul de mˇasline (oil-the of
olive) and not as uleiul mˇaslinei (oil-the olive-gen).
Other examples include CAUSE and TOPIC.
Lauer’s set of 8 prepositions represents 94.5%
(Europarl) and 97% (CLUVI) of the N P N in-
stances. From these, the most frequent preposition
is “of” with a coverage of 70.31% (Europarl) and
85.08% (CLUVI). Moreover, in the Europarl cor-
pus, 26.39% of the instances are synthetic phrases
(where one of the nouns is a nominalization) encod-
ing AGENT, EXPERIENCER, THEME, BENEFICIARY.
Out of these instances, 74.81% use the preposition
of. In CLUVI, 11.71% of the examples were ver-
bal, from which the preposition of has a coverage of
82.20%. The many-to-many mappings of the prepo-
sitions (especially of/de) to the semantic classes adds
to the complexity of the interpretation task. Thus,
for the interpretation of these constructions a system
must rely on the semantic information of the prepo-
sition and two constituent nouns in particular, and
on context in general.
In Europarl, the most frequently occurring re-
lations are PURPOSE, TYPE, and THEME that to-
gether represent about 57% of the data followed by
PART-WHOLE, PROPERTY, TOPIC, AGENT, and LO-
CATION with an average coverage of about 6.23%.
Moreover, other relations such as KINSHIP, DE-
PICTION, MANNER, MEANS did not occur in this
corpus and 5.08% represented OTHER-SR relations.
This semantic distribution contrasts with the one
in CLUVI, which uses a more descriptive lan-
guage. Here, the most frequent relation by far
</bodyText>
<page confidence="0.99313">
574
</page>
<bodyText confidence="0.99995712195122">
is PART-WHOLE (32.14%), followed by LOCATION
(12.40%), THEME (9.23%) and OTHER-SR (7.74%).
It is interesting to note here that only 5.70% of the
TYPE relation instances in Europarl were unique.
This is in contrast with the other relations in both
corpora, where instances were mostly unique.
We also report here our observations on Lap-
ata &amp; Keller’s unsupervised model. An analysis
of these results showed that the order of the con-
stituent nouns in the N P N paraphrase plays an im-
portant role. For example, a search for blood ves-
sels generated similar frequency counts for vessels
of blood and blood in vessels. About 30% noun -
noun paraphrasable pairs preserved the order in the
corresponding N P N paraphrases. We also manually
checked the first five entries generated by Google for
each most frequent prepositional paraphrase for 50
instances and noticed that about 35% of them were
wrong due to syntactic and/or semantic ambiguities.
Thus, since we wanted to measure the impact of
these ambiguities of noun compounds on the inter-
pretation performance, we further tested the prob-
abilistic web-based model on four distinct test sets
selected from Europarl, each containing 30 noun -
noun pairs encoding different types of ambiguity:
in set#1 the noun constituents had only one part of
speech and one WordNet sense; in set#2 the nouns
had at least two possible parts of speech and were
semantically unambiguous, in set#3 the nouns were
ambiguous only semantically, and in set#4 they were
ambiguous both syntactically and semantically. For
unambiguous noun-noun pairs (set#1), the model
obtained an accuracy of 35.01%, while for more se-
mantically ambiguous compounds it obtained an ac-
curacy of about 48.8%. This shows that for more
semantically ambiguous noun - noun pairs, the web-
based probabilistic model introduces a significant
number of false positives. Thus, the more abstract
the categories, the more noun compounds are cov-
ered, but also the more room for variation as to
which category a compound should be assigned.
</bodyText>
<sectionHeader confidence="0.999115" genericHeader="conclusions">
6 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.99983585">
In this paper we presented a supervised, knowledge-
intensive interpretation model which takes advan-
tage of new linguistic information from English and
a list of five Romance languages. Our approach to
NP interpretation is novel in several ways. We de-
fined the problem in a cross-linguistic framework
and provided empirical observations on the distribu-
tion of the syntax and meaning of noun phrases on
two different corpora based on two state-of-the-art
classification tag sets.
As future work we consider the inclusion of other
features such as the semantic classes of Romance
nouns from aligned EuroWordNets, and other sen-
tence features. Since the results obtained can be seen
as an upper bound on NP interpretation due to per-
fect English - Romance NP alignment, we will ex-
periment with automatic translations generated for
the test data. Moreover, we like to extend the anal-
ysis to other set of languages whose structures are
very different from English and Romance.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999809657894737">
T. W. Finin. 1980. The Semantic Interpretation of Compound
Nominals. Ph.D. thesis, University of Illinois at Urbana-
Champaign.
A. Giorgi and G. Longobardi. 1991. The syntax of noun
phrases. Cambridge University Press.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On
the semantics of noun compounds. Computer Speech and
Language, 19(4):479–496.
R. Girju. 2007. Experiments with an annotation scheme for a
knowledge-rich noun phrase interpretation system. The Lin-
guistic Annotation Workshop at ACL, Prague.
Su Nam Kim and T. Baldwin. 2006. Interpreting semantic rela-
tions in noun compounds via verb semantics. COLING-ACL.
K. Kipper, H. Dong, and M. Palmer. 2000. Class-based con-
struction of a verb lexicon. AAAI Conference, Austin.
M. Lapata and F. Keller. 2004. The Web as a baseline: Eval-
uating the performance of unsupervised Web-based models
for a range of NLP tasks. HLT-NAACL.
M. Lauer. 1995. Corpus statistics meet the noun compound:
Some empirical results. ACL, Cambridge, Mass.
A. Meyers, R. Reeves, C. Macleod, R. Szekeley V. Zielinska,
and B. Young. 2004. The cross-breeding of dictionaries.
LREC-2004, Lisbon, Portugal.
R. Mihalcea and E. Faruque. 2004. Senselearner: Minimally
supervised word sense disambiguation for all words in open
text. ACL/SIGLEX Senseval-3, Barcelona, Spain.
D. Moldovan and A. Badulescu. 2005. A semantic scat-
tering model for the automatic interpretation of genitives.
HLT/EMNLP Conference, Vancouver, Canada.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging
generic patterns for automatically harvesting semantic rela-
tions. COLING/ACL, Sydney, Australia.
B. Rosario and M. Hearst. 2001. Classifying the semantic rela-
tions in noun compounds. EMNLP Conference.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic taxonomy
induction from heterogenous evidence. COLING-ACL.
P. Turney. 2006. Expressing implicit semantic relations without
supervision. COLING/ACL, Sydney, Australia.
</reference>
<page confidence="0.99847">
575
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.982199">
<title confidence="0.999219">Improving the Interpretation of Noun Phrases with Cross-linguistic Information</title>
<author confidence="0.999925">Roxana Girju</author>
<affiliation confidence="0.999692">University of Illinois at Urbana-Champaign</affiliation>
<email confidence="0.998298">girju@uiuc.edu</email>
<abstract confidence="0.9990961875">This paper addresses the automatic classification of semantic relations in noun phrases based on cross-linguistic evidence from a set of five Romance languages. A set of novel semantic and contextual English– Romance NP features is derived based on empirical observations on the distribution of the syntax and meaning of noun phrases on two corpora of different genre (Europarl and CLUVI). The features were employed in a Support Vector Machines algorithm which achieved an accuracy of 77.9% (Europarl) and 74.31% (CLUVI), an improvement compared with two state-of-the-art models reported in the literature.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T W Finin</author>
</authors>
<title>The Semantic Interpretation of Compound Nominals.</title>
<date>1980</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Illinois at UrbanaChampaign.</institution>
<contexts>
<context position="6469" citStr="Finin, 1980" startWordPosition="1002" endWordPosition="1003"> of examples. We are interested in what syntactic constructions are used to translate the English instances to the target Romance languages and vice-versa, what semantic relations do these constructions encode, and what is the corpus distribution of the semantic relations. 3.1 Lists of semantic classification relations Although the NP interpretation problem has been studied for a long time, researchers haven’t agreed on the number and the level of abstraction of these semantic categories. They can vary from a few prepositions (Lauer, 1995) to hundreds or thousands specific semantic relations (Finin, 1980). The more abstract the categories, the more noun phrases are covered, but also the more room for variation as to which category a phrase should be assigned. In this paper we experiment with two state of the art classification sets used in NP interpretation. The first is a core set of 22 semantic relations (22 SRs) identified by us from the computational linguistics literature. This list, presented in Table 1 along with examples is general enough to cover a large majority of text semantics while keeping the semantic relations to a manageable number. The second set is Lauer’s list of 8 preposit</context>
</contexts>
<marker>Finin, 1980</marker>
<rawString>T. W. Finin. 1980. The Semantic Interpretation of Compound Nominals. Ph.D. thesis, University of Illinois at UrbanaChampaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Giorgi</author>
<author>G Longobardi</author>
</authors>
<title>The syntax of noun phrases.</title>
<date>1991</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="16424" citStr="Giorgi and Longobardi, 1991" startWordPosition="2650" endWordPosition="2653">sented below. With the exceptions of cases of ambiguity are solved by the type of syntacfeatures F1-F5 (Girju et al., 2005), all the other fea- tic construction used. For example, N N genitivetures are novel. marked constructions are used for EXPERIENCER– A. English Features encoding instances, while N de N or N pentru N (N F1 and F2. Noun semantic class specifies the Word- for N) are used for other relations. Such examples Net sense of the head (F1) and modifier noun (F2) are the love of children – THEME (and not the love by and implicitly points to all its hypernyms. For ex- the children). (Giorgi and Longobardi, 1991) menample, the hypernyms of car#1 are: {motor vehi- tion that with such nouns that resist passivisation, 5NomLex-Plus is a hand-coded database of 5,000 verb nominalizations, de-adjectival, and de-adverbial nouns including the corresponding subcategorization frames (verb-argument structure information). 4“the place where the bomb is exploded” (It.) 571 the preposition introducing the internal argument, 4.2 Learning Models even if it is of, has always a semantic content, and We have experimented with the support vector mais not a bare case-marker realizing the genitive case. chines (SVM) model6 </context>
</contexts>
<marker>Giorgi, Longobardi, 1991</marker>
<rawString>A. Giorgi and G. Longobardi. 1991. The syntax of noun phrases. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>D Moldovan</author>
<author>M Tatu</author>
<author>D Antohe</author>
</authors>
<title>On the semantics of noun compounds.</title>
<date>2005</date>
<journal>Computer Speech and Language,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="15919" citStr="Girju et al., 2005" startWordPosition="2560" endWordPosition="2563">,008 CLUVI instances en- resent states of emotion, such as fear, desire, etc. coded by N N and N P N in all languages considered These nouns mark their internal argument through and resulted after agreement. of and require most of the time prepositions like por 4 Model and not de when translated in Romance. Our obser4.1 Feature space vations on the Romanian translations (captured by We have identified and experimented with 13 NP features F12 and F13 below) show that the possible features presented below. With the exceptions of cases of ambiguity are solved by the type of syntacfeatures F1-F5 (Girju et al., 2005), all the other fea- tic construction used. For example, N N genitivetures are novel. marked constructions are used for EXPERIENCER– A. English Features encoding instances, while N de N or N pentru N (N F1 and F2. Noun semantic class specifies the Word- for N) are used for other relations. Such examples Net sense of the head (F1) and modifier noun (F2) are the love of children – THEME (and not the love by and implicitly points to all its hypernyms. For ex- the children). (Giorgi and Longobardi, 1991) menample, the hypernyms of car#1 are: {motor vehi- tion that with such nouns that resist passi</context>
</contexts>
<marker>Girju, Moldovan, Tatu, Antohe, 2005</marker>
<rawString>R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On the semantics of noun compounds. Computer Speech and Language, 19(4):479–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
</authors>
<title>Experiments with an annotation scheme for a knowledge-rich noun phrase interpretation system. The Linguistic Annotation Workshop at ACL,</title>
<date>2007</date>
<location>Prague.</location>
<contexts>
<context position="11048" citStr="Girju, 2007" startWordPosition="1723" endWordPosition="1724">o.es/CLUVI/ added to the list. The two computational semantics annotators had to tag each English constituent noun with its corresponding WordNet sense and each instance with the corresponding semantic category. If the word was not found in WordNet the instance was not considered. Whenever the annotators found an example encoding a semantic category other than those provided or they didn’t know what interpretation to give, they had to tag it as “OTHER-SR”, and respectively “OTHER-PP”3. The details of the annotation task and the observations drawn from there are presented in a companion paper (Girju, 2007). The corpus instances used in the corpus analysis phase have the following format: &lt;NPEn ;NPEs; NPIt; NPFr; NPPort; NPRo; target&gt;. The word target is one of the 23 (22 + OTHER-SR) semantic relations and one of the eight prepositions considered or OTHER-PP (with the exception of those N P N instances that already contain a preposition). For example, &lt;development cooperation; cooperaci´on para el desarrollo; cooperazione allo sviluppo; coop´eration au d´eveloppement; cooperare pentru dezvoltare; PURPOSE / FOR&gt;. The annotators’ agreement was measured using Kappa statistics: K = Pr(�)�P r(E) 1�P </context>
</contexts>
<marker>Girju, 2007</marker>
<rawString>R. Girju. 2007. Experiments with an annotation scheme for a knowledge-rich noun phrase interpretation system. The Linguistic Annotation Workshop at ACL, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>T Baldwin</author>
</authors>
<title>Interpreting semantic relations in noun compounds via verb semantics.</title>
<date>2006</date>
<location>Austin.</location>
<contexts>
<context position="5210" citStr="Kim and Baldwin, 2006" startWordPosition="802" endWordPosition="805">d, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. Other researchers (Pantel and Pennacchiotti, 2006), (Snow et al., 2006) use clustering techniques coupled with syntactic dependency features to identify IS-A relations in large text collections. (Kim and Baldwin, 2006) and (Turney, 2006) focus on the lexical similarity of unseen noun compounds with those found in training. However, although the web-based solution might overcome the data sparseness problem, the current probabilistic models are limited by the lack of deep linguistic information. In this paper we investigate the role of cross-linguistic information in the task of English NP semantic interpretation and show the importance of a set of novel linguistic features. 3 Corpus Analysis For a better understanding of the meaning of the N N and N P N instances, we analyzed the semantic behavior of these c</context>
</contexts>
<marker>Kim, Baldwin, 2006</marker>
<rawString>Su Nam Kim and T. Baldwin. 2006. Interpreting semantic relations in noun compounds via verb semantics. COLING-ACL. K. Kipper, H. Dong, and M. Palmer. 2000. Class-based construction of a verb lexicon. AAAI Conference, Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>F Keller</author>
</authors>
<title>The Web as a baseline: Evaluating the performance of unsupervised Web-based models for a range of NLP tasks.</title>
<date>2004</date>
<publisher>HLT-NAACL.</publisher>
<contexts>
<context position="3571" citStr="Lapata and Keller, 2004" startWordPosition="552" endWordPosition="555"> classification tag sets: Lauer’s set of 8 prepositions (Lauer, 1995) and our list of 22 semantic relations. We show that various crosslingual cues can help in the NP interpretation task when employed in an SVM model. The results are compared against two state of the art approaches: a su568 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 568–575, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics pervised machine learning model, Semantic Scattering (Moldovan and Badulescu, 2005), and a webbased probabilistic model (Lapata and Keller, 2004). The paper is organized as follows. In Section 2 we present a summary of the previous work. Section 3 lists the syntactic and semantic interpretation categories used along with observations regarding their distribution on the two different cross-lingual corpora. Sections 4 and 5 present a learning model and results for the interpretation of English noun phrases. Finally, in Section 6 we offer some discussion and conclusions. 2 Related Work Currently, the best-performing NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and r</context>
<context position="4833" citStr="Lapata and Keller, 2004" startWordPosition="747" endWordPosition="750">cific semantic taxonomies, or on statistical models on large collections of unlabeled data. Recent results have shown that symbolic noun compound interpretation systems using machine learning techniques coupled with a large lexical hierarchy perform with very good accuracy, but they are most of the time tailored to a specific domain (Rosario and Hearst, 2001). On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. Other researchers (Pantel and Pennacchiotti, 2006), (Snow et al., 2006) use clustering techniques coupled with syntactic dependency features to identify IS-A relations in large text collections. (Kim and Baldwin, 2006) and (Turney, 2006) focus on the lexical similarity of unseen noun compounds with those found in training. However, although the web-based solution might overcome the data sparseness problem, the current probabilistic model</context>
<context position="17429" citStr="Lapata and Keller, 2004" startWordPosition="2799" endWordPosition="2802"> the internal argument, 4.2 Learning Models even if it is of, has always a semantic content, and We have experimented with the support vector mais not a bare case-marker realizing the genitive case. chines (SVM) model6 and compared the results b. Unaccusative (ergative) nouns which are de- against two state-of-the-art models: a supervised rived from ergative verbs that take only internal ar- model, Semantic Scattering (SS), (Moldovan and guments (e.g., not agentive ones). For example, the Badulescu, 2005), and a web-based unsupervised transitive verb to disband allows the subject to be model (Lapata and Keller, 2004). The SVM and SS deleted as in the following sentences (1) “The lead models were trained and tested on the Europarl and singer disbanded the group in 1991.” and (2) “The CLUVI corpora using a 8:2 ratio. The test dataset group disbanded.”. Thus, the corresponding erga- was randomly selected from each corpus and the test tive nominalization the disbandment of the group en- nouns (only for English) were tagged with the corcodes a THEME relation and not AGENT. responding sense in context using a state of the art c. Unergative (intransitive) nouns are derived WSD tool (Mihalcea and Faruque, 2004). </context>
<context position="23642" citStr="Lapata and Keller, 2004" startWordPosition="3829" endWordPosition="3832"> best performance is obtained with the trigram model f(n1, p, n2). The count used for a given trigram is the number of pages returned by Altavista on the trigram corresponding queries. For example, for the test instance war stories, the best number of hits was obtained with the query stories about war. For the Europarl and CLUVI test sets, we replicated Lapata &amp; Keller’s experiments using Google8. We formed inflected queries with the patterns they proposed and searched the web. 8As Google limits the number of queries to 1,000 per day, we repeated the experiment for a number of days. Although (Lapata and Keller, 2004) used Altavista in their experiments, they showed there is almost no difference between the correlations achieved using Google and Altavista counts. 5 Experimental results Table 2 shows the results obtained against SS and Lapata &amp; Keller’s model on both corpora and the contribution the features exemplified in one baseline and six versions of the SVM model. The baseline is defined only for the English part of the NP feature set and measures the the contribution of the WordNet IS-A lexical hierarchy specialization. The baseline does not differentiate between unambiguous and ambiguous training ex</context>
</contexts>
<marker>Lapata, Keller, 2004</marker>
<rawString>M. Lapata and F. Keller. 2004. The Web as a baseline: Evaluating the performance of unsupervised Web-based models for a range of NLP tasks. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lauer</author>
</authors>
<title>Corpus statistics meet the noun compound: Some empirical results. ACL,</title>
<date>1995</date>
<location>Cambridge, Mass.</location>
<contexts>
<context position="3016" citStr="Lauer, 1995" startWordPosition="467" endWordPosition="468">ian. The focus on Romance languages is well motivated. It is mostly true that English noun phrases translate into constructions of the form N P N in Romance languages where, as we will show below, the P (preposition) varies in ways that correlate with the semantics. Thus Romance languages will give us another source of evidence for disambiguating the semantic relations in English NPs. We also present empirical observations on the distribution of the syntax and meaning of noun phrases on two different corpora based on two state-of-the-art classification tag sets: Lauer’s set of 8 prepositions (Lauer, 1995) and our list of 22 semantic relations. We show that various crosslingual cues can help in the NP interpretation task when employed in an SVM model. The results are compared against two state of the art approaches: a su568 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 568–575, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics pervised machine learning model, Semantic Scattering (Moldovan and Badulescu, 2005), and a webbased probabilistic model (Lapata and Keller, 2004). The paper is organized as follows. In Secti</context>
<context position="4791" citStr="Lauer, 1995" startWordPosition="743" endWordPosition="744">er on rather ad-hoc, domainspecific semantic taxonomies, or on statistical models on large collections of unlabeled data. Recent results have shown that symbolic noun compound interpretation systems using machine learning techniques coupled with a large lexical hierarchy perform with very good accuracy, but they are most of the time tailored to a specific domain (Rosario and Hearst, 2001). On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. Other researchers (Pantel and Pennacchiotti, 2006), (Snow et al., 2006) use clustering techniques coupled with syntactic dependency features to identify IS-A relations in large text collections. (Kim and Baldwin, 2006) and (Turney, 2006) focus on the lexical similarity of unseen noun compounds with those found in training. However, although the web-based solution might overcome the data sparsenes</context>
<context position="6402" citStr="Lauer, 1995" startWordPosition="993" endWordPosition="994"> behavior of these constructions on a large crosslinguistic corpora of examples. We are interested in what syntactic constructions are used to translate the English instances to the target Romance languages and vice-versa, what semantic relations do these constructions encode, and what is the corpus distribution of the semantic relations. 3.1 Lists of semantic classification relations Although the NP interpretation problem has been studied for a long time, researchers haven’t agreed on the number and the level of abstraction of these semantic categories. They can vary from a few prepositions (Lauer, 1995) to hundreds or thousands specific semantic relations (Finin, 1980). The more abstract the categories, the more noun phrases are covered, but also the more room for variation as to which category a phrase should be assigned. In this paper we experiment with two state of the art classification sets used in NP interpretation. The first is a core set of 22 semantic relations (22 SRs) identified by us from the computational linguistics literature. This list, presented in Table 1 along with examples is general enough to cover a large majority of text semantics while keeping the semantic relations t</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>M. Lauer. 1995. Corpus statistics meet the noun compound: Some empirical results. ACL, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekeley V Zielinska</author>
<author>B Young</author>
</authors>
<title>The cross-breeding of dictionaries. LREC-2004,</title>
<date>2004</date>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="13876" citStr="Meyers et al., 2004" startWordPosition="2212" endWordPosition="2215"> of N N for both text corpora on all languages were of the instances, this feature is “–” (e.g., framework law). type N N and N P N. However, while their distribu- F6 and F7. Type of nominalized noun indicates the tion is balanced in the Europarl corpus (about 45%, specific class of nouns the head (F6) or modifier (F7) with a 64% N P N – 26% N N ratio for Romanian), belongs to depending on the verb it derives from. in CLUVI the N P N constructions occur in more First, we check if the noun is a nominalization. For than 85% of the cases (again, with the exception of English we used NomLex-Plus (Meyers et al., 2004) Romanian – 50%). It is interesting to note here that to map nouns to corresponding verbs.5 For examsome of the English NPs are translated into both ple, “destruction of the city”, where destruction is noun–noun and noun–adjective compounds in the a nominalization. F6 and F7 may overlap with featarget languages. For example, love affair translates tures F3 and F4 which are used in case the noun to be in Italian as storia d’amore or the noun–adjective checked does not have an entry in the NomLex-Plus compound relazione amorosa. There are also in- dictionary. These features are of particular imp</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Zielinska, Young, 2004</marker>
<rawString>A. Meyers, R. Reeves, C. Macleod, R. Szekeley V. Zielinska, and B. Young. 2004. The cross-breeding of dictionaries. LREC-2004, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>E Faruque</author>
</authors>
<title>Senselearner: Minimally supervised word sense disambiguation for all words in open text.</title>
<date>2004</date>
<booktitle>ACL/SIGLEX Senseval-3,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="18027" citStr="Mihalcea and Faruque, 2004" startWordPosition="2900" endWordPosition="2903">del (Lapata and Keller, 2004). The SVM and SS deleted as in the following sentences (1) “The lead models were trained and tested on the Europarl and singer disbanded the group in 1991.” and (2) “The CLUVI corpora using a 8:2 ratio. The test dataset group disbanded.”. Thus, the corresponding erga- was randomly selected from each corpus and the test tive nominalization the disbandment of the group en- nouns (only for English) were tagged with the corcodes a THEME relation and not AGENT. responding sense in context using a state of the art c. Unergative (intransitive) nouns are derived WSD tool (Mihalcea and Faruque, 2004). from intransitive verbs and take only AGENT seman- After the initial NP instances in the training and tic relations. For example, the departure of the girl. test corpora were expanded with the corresponding d. Inherently passive nouns such as the cap- features, we had to prepare them for SVM and SS. ture of the soldier. These nouns, like the verbs they The method consists of a set of automatic iterative are derived from, assume a default AGENT (subject) procedures of specialization of the English nouns on and being transitive, associate to their internal argu- the WordNet IS-A hierarchy. Thu</context>
</contexts>
<marker>Mihalcea, Faruque, 2004</marker>
<rawString>R. Mihalcea and E. Faruque. 2004. Senselearner: Minimally supervised word sense disambiguation for all words in open text. ACL/SIGLEX Senseval-3, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>A Badulescu</author>
</authors>
<title>A semantic scattering model for the automatic interpretation of genitives. HLT/EMNLP Conference,</title>
<date>2005</date>
<location>Vancouver, Canada.</location>
<contexts>
<context position="3509" citStr="Moldovan and Badulescu, 2005" startWordPosition="542" endWordPosition="545">noun phrases on two different corpora based on two state-of-the-art classification tag sets: Lauer’s set of 8 prepositions (Lauer, 1995) and our list of 22 semantic relations. We show that various crosslingual cues can help in the NP interpretation task when employed in an SVM model. The results are compared against two state of the art approaches: a su568 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 568–575, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics pervised machine learning model, Semantic Scattering (Moldovan and Badulescu, 2005), and a webbased probabilistic model (Lapata and Keller, 2004). The paper is organized as follows. In Section 2 we present a summary of the previous work. Section 3 lists the syntactic and semantic interpretation categories used along with observations regarding their distribution on the two different cross-lingual corpora. Sections 4 and 5 present a learning model and results for the interpretation of English noun phrases. Finally, in Section 6 we offer some discussion and conclusions. 2 Related Work Currently, the best-performing NP interpretation methods in computational linguistics focus m</context>
</contexts>
<marker>Moldovan, Badulescu, 2005</marker>
<rawString>D. Moldovan and A. Badulescu. 2005. A semantic scattering model for the automatic interpretation of genitives. HLT/EMNLP Conference, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>M Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations. COLING/ACL,</title>
<date>2006</date>
<location>Sydney, Australia.</location>
<contexts>
<context position="5042" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="777" endWordPosition="780"> coupled with a large lexical hierarchy perform with very good accuracy, but they are most of the time tailored to a specific domain (Rosario and Hearst, 2001). On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. Other researchers (Pantel and Pennacchiotti, 2006), (Snow et al., 2006) use clustering techniques coupled with syntactic dependency features to identify IS-A relations in large text collections. (Kim and Baldwin, 2006) and (Turney, 2006) focus on the lexical similarity of unseen noun compounds with those found in training. However, although the web-based solution might overcome the data sparseness problem, the current probabilistic models are limited by the lack of deep linguistic information. In this paper we investigate the role of cross-linguistic information in the task of English NP semantic interpretation and show the importance of a se</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. COLING/ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Rosario</author>
<author>M Hearst</author>
</authors>
<title>Classifying the semantic relations in noun compounds.</title>
<date>2001</date>
<journal>EMNLP Conference.</journal>
<contexts>
<context position="4570" citStr="Rosario and Hearst, 2001" startWordPosition="706" endWordPosition="709"> in Section 6 we offer some discussion and conclusions. 2 Related Work Currently, the best-performing NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and rely either on rather ad-hoc, domainspecific semantic taxonomies, or on statistical models on large collections of unlabeled data. Recent results have shown that symbolic noun compound interpretation systems using machine learning techniques coupled with a large lexical hierarchy perform with very good accuracy, but they are most of the time tailored to a specific domain (Rosario and Hearst, 2001). On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. Other researchers (Pantel and Pennacchiotti, 2006), (Snow et al., 2006) use clustering techniques coupled with syntactic dependency features to identify IS-A relations in large t</context>
</contexts>
<marker>Rosario, Hearst, 2001</marker>
<rawString>B. Rosario and M. Hearst. 2001. Classifying the semantic relations in noun compounds. EMNLP Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<journal>COLING-ACL. P. Turney.</journal>
<publisher>COLING/ACL, Sydney, Australia.</publisher>
<contexts>
<context position="5063" citStr="Snow et al., 2006" startWordPosition="781" endWordPosition="784">archy perform with very good accuracy, but they are most of the time tailored to a specific domain (Rosario and Hearst, 2001). On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. Other researchers (Pantel and Pennacchiotti, 2006), (Snow et al., 2006) use clustering techniques coupled with syntactic dependency features to identify IS-A relations in large text collections. (Kim and Baldwin, 2006) and (Turney, 2006) focus on the lexical similarity of unseen noun compounds with those found in training. However, although the web-based solution might overcome the data sparseness problem, the current probabilistic models are limited by the lack of deep linguistic information. In this paper we investigate the role of cross-linguistic information in the task of English NP semantic interpretation and show the importance of a set of novel linguistic</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. COLING-ACL. P. Turney. 2006. Expressing implicit semantic relations without supervision. COLING/ACL, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>