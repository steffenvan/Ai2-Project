<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001831">
<title confidence="0.838943">
Minimum Bayes Risk Decoding for BLEU
</title>
<author confidence="0.93921">
Nicola Ehling and Richard Zens and Hermann Ney
</author>
<affiliation confidence="0.904963666666667">
Human Language Technology and Pattern Recognition
Lehrstuhl f¨ur Informatik 6 – Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
</affiliation>
<email confidence="0.996438">
{ehling,zens,ney}@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.996625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995801875">
We present a Minimum Bayes Risk (MBR)
decoder for statistical machine translation.
The approach aims to minimize the expected
loss of translation errors with regard to the
BLEU score. We show that MBR decoding
on N-best lists leads to an improvement of
translation quality.
We report the performance of the MBR
decoder on four different tasks: the TC-
STAR EPPS Spanish-English task 2006, the
NIST Chinese-English task 2005 and the
GALE Arabic-English and Chinese-English
task 2006. The absolute improvement of the
BLEU score is between 0.2% for the TC-
STAR task and 1.1% for the GALE Chinese-
English task.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965173913043">
In recent years, statistical machine translation
(SMT) systems have achieved substantial progress
regarding their perfomance in international transla-
tion tasks (TC-STAR, NIST, GALE).
Statistical approaches to machine translation were
proposed at the beginning of the nineties and found
widespread use in the last years. The ”standard” ver-
sion of the Bayes decision rule, which aims at a min-
imization of the sentence error rate is used in vir-
tually all approaches to statistical machine transla-
tion. However, most translation systems are judged
by their ability to minimize the error rate on the word
level or n-gram level. Common error measures are
the Word Error Rate (WER) and the Position Inde-
pendent Word Error Rate (PER) as well as evalua-
tion metric on the n-gram level like the BLEU and
NIST score that measure precision and fluency of a
given translation hypothesis.
The remaining part of this paper is structured as
follows: after a short overview of related work in
Sec. 2, we describe the MBR decoder in Sec. 3. We
present the experimental results in Sec. 4 and con-
clude in Sec. 5.
</bodyText>
<sectionHeader confidence="0.999785" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999878636363636">
MBR decoder for automatic speech recognition
(ASR) have been reported to yield improvement
over the widely used maximum a-posteriori prob-
ability (MAP) decoder (Goel and Byrne, 2003;
Mangu et al., 2000; Stolcke et al., 1997).
For MT, MBR decoding was introduced in (Ku-
mar and Byrne, 2004). It was shown that MBR is
preferable over MAP decoding for different evalu-
ation criteria. Here, we focus on the performance
of MBR decoding for the BLEU score on various
translation tasks.
</bodyText>
<subsectionHeader confidence="0.693505666666667">
3 Implementation of Minimum Bayes Risk
Decoding for the BLEU Score
3.1 Bayes Decision Rule
</subsectionHeader>
<bodyText confidence="0.999846571428571">
In statistical machine translation, we are given a
source language sentence fJ1 = f1 ... fj ... fJ,
which is to be translated into a target language sen-
tence eI1 = e1 ... ei ... eI. Statistical decision the-
ory tells us that among all possible target language
sentences, we should choose the sentence which
minimizes the Bayes risk:
</bodyText>
<equation confidence="0.9215064">
� Pr(e&apos;I&apos;
1 |fJ 1 ) · L(eI1, e&apos;I&apos;
1 )
I&apos;,e&apos;�&apos;
1
</equation>
<bodyText confidence="0.996971666666667">
Here, L(·, ·) denotes the loss function under con-
sideration. In the following, we will call this deci-
sion rule the MBR rule (Kumar and Byrne, 2004).
</bodyText>
<equation confidence="0.942509666666667">
�e1 = argmin
ˆI
I,ei
</equation>
<page confidence="0.929786">
101
</page>
<bodyText confidence="0.898657571428571">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 101–104,
Prague, June 2007. c�2007 Association for Computational Linguistics
Although it is well known that this decision rule is
optimal, most SMT systems do not use it. The most
common approach is to use the MAP decision rule.
Thus, we select the hypothesis which maximizes the
posterior probability Pr(eI 1|fJ 1 ):
</bodyText>
<equation confidence="0.993302">
� �
Pr(eI1|fJ 1 )
</equation>
<subsectionHeader confidence="0.9935">
3.3 BLEU Score
</subsectionHeader>
<bodyText confidence="0.997965285714286">
The BLEU score (Papineni et al., 2002) measures
the agreement between a hypothesis eI1 generated by
the MT system and a reference translation �e1. It is
ˆI
the geometric mean of n-gram precisions Precn(·, ·)
in combination with a brevity penalty BP(·, ·) for too
short translation hypotheses.
</bodyText>
<figure confidence="0.539704">
e ˆI
1 = argmax
I,ei
This decision rule is equivalent to the MBR crite- BLEU(eI1, �e1) = BP(I, 4 Precn(eI 1, �e1)1/4
rion under a 0-1 loss function: ˆI I) · H ˆI
n=1
0 (el, e V 1 if eI1 = e&apos;i0 BP(I, I) 1 ifI&gt;_I
L-1 ( 1, 1)={ 0 else l = exp (1 − I/�I) if I &lt; I
</figure>
<bodyText confidence="0.999243166666667">
Hence, the MAP decision rule is optimal for the
sentence or string error rate. It is not necessarily
optimal for other evaluation metrics as for example
the BLEU score. One reason for the popularity of
the MAP decision rule might be that, compared to
the MBR rule, its computation is simpler.
</bodyText>
<subsectionHeader confidence="0.999676">
3.2 Baseline System
</subsectionHeader>
<bodyText confidence="0.998981">
The posterior probability Pr(eI1|fJ1 ) is modeled di-
rectly using a log-linear combination of several
models (Och and Ney, 2002):
</bodyText>
<equation confidence="0.9578295">
(EM )
exp m=1 λmhm(eI 1, fJ 1 )
E exp (EM1 λmhm(e�i0, fJ1 ))
I0,e0�0
1
(1)
</equation>
<bodyText confidence="0.99667065">
This approach is a generalization of the source-
channel approach (Brown et al., 1990). It has the
advantage that additional models h(·) can be easily
integrated into the overall system.
The denominator represents a normalization fac-
tor that depends only on the source sentence fJ1 .
Therefore, we can omit it in case of the MAP de-
cision rule during the search process. Note that the
denominator affects the results of the MBR decision
rule and, thus, cannot be omitted in that case.
We use a state-of-the-art phrase-based translation
system similar to (Matusov et al., 2006) including
the following models: an n-gram language model,
a phrase translation model and a word-based lex-
icon model. The latter two models are used for
both directions: p(f|e) and p(e|f). Additionally,
we use a word penalty, phrase penalty and a distor-
tion penalty. The model scaling factors λM1 are opti-
mized with respect to the BLEU score as described
in (Och, 2003).
</bodyText>
<equation confidence="0.485299">
Precn(eI1, �e1)= E min{C(wn1 |eI1), C(wn1 |�e1)}
ˆI wl ˆI
E C(wn1 |eI1)
wi
</equation>
<bodyText confidence="0.9884468">
Here, C(wn1 |eI1) denotes the number of occur-
rences of an n-gram wn1 in a sentence eI1. The de-
nominator of the n-gram precisions evaluate to the
number of n-grams in the hypothesis, i.e. I − n + 1.
As loss function for the MBR decoder, we use:
</bodyText>
<equation confidence="0.994952666666667">
L[eI1, �e1] = 1 − BLEU(eI
ˆI 1, �e1) .
ˆI
</equation>
<bodyText confidence="0.999955714285714">
While the original BLEU score was intended to be
used only for aggregate counts over a whole test set,
we use the BLEU score at the sentence-level during
the selection of the MBR hypotheses. Note that we
will use this sentence-level BLEU score only during
decoding. The translation results that we will report
later are computed using the standard BLEU score.
</bodyText>
<subsectionHeader confidence="0.98867">
3.4 Hypothesis Selection
</subsectionHeader>
<bodyText confidence="0.9998798">
We select the MBR hypothesis among the N best
translation candidates of the MAP system. For each
entry, we have to compute its expected BLEU score,
i.e. the weighted sum over all entries in the N-best
list. Therefore, finding the MBR hypothesis has a
quadratic complexity in the size of the N-best list.
To reduce this large work load, we stop the summa-
tion over the translation candidates as soon as the
risk of the regarded hypothesis exceeds the current
minimum risk, i.e. the risk of the current best hy-
pothesis. Additionally, the hypotheses are processed
according to the posterior probabilities. Thus, we
can hope to find a good candidate soon. This allows
for an early stopping of the computation for each of
the remaining candidates.
</bodyText>
<equation confidence="0.867554">
P r(eI 1|fJ 1 ) =
</equation>
<page confidence="0.994677">
102
</page>
<sectionHeader confidence="0.882286" genericHeader="method">
3.5 Global Model Scaling Factor
</sectionHeader>
<bodyText confidence="0.999937714285714">
During the translation process, the different sub-
models hm() get different weights am. These scal-
ing factors are optimized with regard to a specific
evaluation criteria, here: BLEU. This optimization
describes the relation between the different models
but does not define the absolute values for the scal-
ing factors. Because search is performed using the
maximum approximation, these absolute values are
not needed during the translation process. In con-
trast to this, using the MBR decision rule, we per-
form a summation over all sentence probabilities
contained in the N-best list. Therefore, we use a
global scaling factor a0 &gt; 0 to modify the individ-
ual scaling factors am:
</bodyText>
<equation confidence="0.884697">
a&apos;m = a0 - am ,m = 1,...,M.
</equation>
<bodyText confidence="0.998943625">
For the MBR decision rule the modified scaling fac-
tors a&apos; m are used instead of the original model scal-
ing factors am to compute the sentence probabilities
as in Eq. 1. The global scaling factor a0 is tuned on
the development set. Note that under the MAP deci-
sion rule any global scaling factor a0 &gt; 0 yields the
same result. Similar tests were reported by (Mangu
et al., 2000; Goel and Byrne, 2003) for ASR.
</bodyText>
<sectionHeader confidence="0.995385" genericHeader="method">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.996482">
4.1 Corpus Statistics
</subsectionHeader>
<bodyText confidence="0.9991456">
We tested the MBR decoder on four translation
tasks: the TC-STAR EPPS Spanish-English task of
2006, the NIST Chinese-English evaluation test set
of 2005 and the GALE Arabic-English and Chinese-
English evaluation test set of 2006. The TC-STAR
EPPS corpus is a spoken language translation corpus
containing the verbatim transcriptions of speeches
of the European Parliament. The NIST Chinese-
English test sets consists of news stories. The GALE
project text track consists of two parts: newswire
(“news”) and newsgroups (“ng”). The newswire part
is similar to the NIST task. The newsgroups part
covers posts to electronic bulletin boards, Usenet
newsgroups, discussion groups and similar forums.
The corpus statistics of the training corpora are
shown in Tab. 1 to Tab. 3. To measure the trans-
lation quality, we use the BLEU score. With ex-
ception of the TC-STAR EPPS task, all scores are
computed case-insensitive. As BLEU measures ac-
curacy, higher scores are better.
</bodyText>
<tableCaption confidence="0.997455">
Table 1: NIST Chinese-English: corpus statistics.
</tableCaption>
<table confidence="0.999819857142857">
Chinese English
Train Sentences 9M
Words 232 M 250M
Vocabulary 238 K 412 K
NIST 02 Sentences 878
Words 26 431 24 352
NIST 05 Sentences 1082
Words 34 908 36 027
GALE 06 Sentences 460
news
Words 9 979 11493
GALE 06 Sentences 461
ng
Words 9 606 11689
</table>
<tableCaption confidence="0.939494">
Table 2: TC-Star Spanish-English: corpus statistics.
</tableCaption>
<table confidence="0.999587375">
Spanish English
Train Sentences 1.2 M
Words 35M 33M
Vocabulary 159 K 110K
Dev Sentences 1452
Words 51982 54 857
Test Sentences 1780
Words 56 515 58 295
</table>
<subsectionHeader confidence="0.99663">
4.2 Translation Results
</subsectionHeader>
<bodyText confidence="0.9998961875">
The translation results for all tasks are presented
in Tab. 4. For each translation task, we tested the
decoder on N-best lists of size N=10000, i.e. the
10 000 best translation candidates. Note that in some
cases the list is smaller because the translation sys-
tem did not produce more candidates. To analyze
the improvement that can be gained through rescor-
ing with MBR, we start from a system that has al-
ready been rescored with additional models like an
n-gram language model, HMM, IBM-1 and IBM-4.
It turned out that the use of 1000 best candidates
for the MBR decoding is sufficient, and leads to ex-
actly the same results as the use of 10 000 best lists.
Similar experiences were reported by (Mangu et al.,
2000; Stolcke et al., 1997) for ASR.
We observe that the improvement is larger for
</bodyText>
<tableCaption confidence="0.99854">
Table 3: GALE Arabic-English: corpus statistics.
</tableCaption>
<table confidence="0.998882">
Arabic English
Train Sentences 4M
Words 125 M 124M
Vocabulary 421 K 337 K
news Sentences 566
Words 14160 15 320
ng Sentences 615
Words 11 195 14 493
</table>
<page confidence="0.982949">
103
</page>
<tableCaption confidence="0.902551">
Table 4: Translation results BLEU [%] for the NIST task, GALE task and TC-STAR task (S-E: Spanish-
English; C-E: Chinese-English; A-E: Arabic-English).
</tableCaption>
<table confidence="0.99965325">
TC-STAR S-E NIST C-E GALE A-E GALE C-E
decision rule test 2002 (dev) 2005 news ng news ng
MAP 52.6 32.8 31.2 23.6 12.2 14.6 9.4
MBR 52.8 33.3 31.9 24.2 13.3 15.4 10.5
</table>
<tableCaption confidence="0.996335">
Table 5: Translation examples for the GALE Arabic-English newswire task.
</tableCaption>
<table confidence="0.4612089">
Reference the saudi interior ministry announced in a report the implementation of the death penalty
today, tuesday, in the area of medina (west) of a saudi citizen convicted of murdering a
fellow citizen.
MAP-Hyp saudi interior ministry in a statement to carry out the death sentence today in the area of
medina (west) in saudi citizen found guilty of killing one of its citizens.
MBR-Hyp the saudi interior ministry announced in a statement to carry out the death sentence today
in the area of medina (west) in saudi citizen was killed one of its citizens.
Reference faruq al-shar’a takes the constitutional oath of office before the syrian president
MAP-Hyp farouk al-shara leads sworn in by the syrian president
MBR-Hyp farouk al-shara lead the constitutional oath before the syrian president
</table>
<bodyText confidence="0.999401142857143">
low-scoring translations, as can be seen in the GALE
task. For an ASR task, similar results were reported
by (Stolcke et al., 1997).
Some translation examples for the GALE Arabic-
English newswire task are shown in Tab. 5. The dif-
ferences between the MAP and the MBR hypotheses
are set in italics.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999987846153846">
We have shown that Minimum Bayes Risk decod-
ing on N-best lists improves the BLEU score con-
siderably. The achieved results are promising. The
improvements were consistent among several eval-
uation sets. Even if the improvement is sometimes
small, e.g. TC-STAR, it is statistically significant:
the absolute improvement of the BLEU score is be-
tween 0.2% for the TC-STAR task and 1.1% for the
GALE Chinese-English task. Note, that MBR de-
coding is never worse than MAP decoding, and is
therefore promising for SMT. It is easy to integrate
and can improve even well-trained systems by tun-
ing them for a particular evaluation criterion.
</bodyText>
<sectionHeader confidence="0.998028" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999024428571429">
This material is partly based upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023,
and was partly funded by the European Union un-
der the integrated project TC-STAR (Technology
and Corpora for Speech to Speech Translation, IST-
2002-FP6-506738, http://www.tc-star.org).
</bodyText>
<sectionHeader confidence="0.999187" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999843944444444">
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra,
F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.
1990. A statistical approach to machine translation. Com-
putational Linguistics, 16(2):79–85, June.
V. Goel and W. Byrne. 2003. Minimum bayes-risk automatic
speech recognition. Pattern Recognition in Speech and Lan-
guage Processing.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk decod-
ing for statistical machine translation. In Proc. Human Lan-
guage Technology Conf. / North American Chapter of the
Assoc. for Computational Linguistics Annual Meeting (HLT-
NAACL), pages 169–176, Boston, MA, May.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus
in speech recognition: Word error minimization and other
applications of confusion networks. Computer, Speech and
Language, 14(4):373–400, October.
E. Matusov, R. Zens, D. Vilar, A. Mauser, M. Popovi´c,
S. Hasan, and H. Ney. 2006. The RWTH machine trans-
lation system. In Proc. TC-Star Workshop on Speech-to-
Speech Translation, pages 31–36, Barcelona, Spain, June.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proc. 40th Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 295–302, Philadelphia, PA, July.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. 41st Annual Meeting of the As-
soc. for Computational Linguistics (ACL), pages 160–167,
Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. Bleu: a
method for automatic evaluation of machine translation. In
Proc. 40th Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 311–318, Philadelphia, PA, July.
A. Stolcke, Y. Konig, and M. Weintraub. 1997. Explicit word
error minimization in N-best list rescoring. In Proc. Eu-
ropean Conf. on Speech Communication and Technology,
pages 163–166, Rhodes, Greece, September.
</reference>
<page confidence="0.998782">
104
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.310117">
<title confidence="0.999354">Minimum Bayes Risk Decoding for BLEU</title>
<author confidence="0.996667">Nicola Ehling</author>
<author confidence="0.996667">Richard Zens</author>
<author confidence="0.996667">Hermann Ney</author>
<affiliation confidence="0.7291975">Human Language Technology and Pattern Recognition Lehrstuhl f¨ur Informatik 6 – Computer Science Department</affiliation>
<address confidence="0.673229">RWTH Aachen University, D-52056 Aachen, Germany</address>
<abstract confidence="0.9701273">We present a Minimum Bayes Risk (MBR) decoder for statistical machine translation. The approach aims to minimize the expected loss of translation errors with regard to the BLEU score. We show that MBR decoding lists leads to an improvement of translation quality. We report the performance of the MBR decoder on four different tasks: the TC-</abstract>
<note confidence="0.898645428571428">STAR EPPS Spanish-English task 2006, the NIST Chinese-English task 2005 and the GALE Arabic-English and Chinese-English task 2006. The absolute improvement of the BLEU score is between 0.2% for the TC- STAR task and 1.1% for the GALE Chinese- English task.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J Cocke</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="4697" citStr="Brown et al., 1990" startWordPosition="806" endWordPosition="809"> − I/�I) if I &lt; I Hence, the MAP decision rule is optimal for the sentence or string error rate. It is not necessarily optimal for other evaluation metrics as for example the BLEU score. One reason for the popularity of the MAP decision rule might be that, compared to the MBR rule, its computation is simpler. 3.2 Baseline System The posterior probability Pr(eI1|fJ1 ) is modeled directly using a log-linear combination of several models (Och and Ney, 2002): (EM ) exp m=1 λmhm(eI 1, fJ 1 ) E exp (EM1 λmhm(e�i0, fJ1 )) I0,e0�0 1 (1) This approach is a generalization of the sourcechannel approach (Brown et al., 1990). It has the advantage that additional models h(·) can be easily integrated into the overall system. The denominator represents a normalization factor that depends only on the source sentence fJ1 . Therefore, we can omit it in case of the MAP decision rule during the search process. Note that the denominator affects the results of the MBR decision rule and, thus, cannot be omitted in that case. We use a state-of-the-art phrase-based translation system similar to (Matusov et al., 2006) including the following models: an n-gram language model, a phrase translation model and a word-based lexicon </context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Goel</author>
<author>W Byrne</author>
</authors>
<title>Minimum bayes-risk automatic speech recognition.</title>
<date>2003</date>
<booktitle>Pattern Recognition in Speech and Language Processing.</booktitle>
<contexts>
<context position="2190" citStr="Goel and Byrne, 2003" startWordPosition="346" endWordPosition="349">e (WER) and the Position Independent Word Error Rate (PER) as well as evaluation metric on the n-gram level like the BLEU and NIST score that measure precision and fluency of a given translation hypothesis. The remaining part of this paper is structured as follows: after a short overview of related work in Sec. 2, we describe the MBR decoder in Sec. 3. We present the experimental results in Sec. 4 and conclude in Sec. 5. 2 Related Work MBR decoder for automatic speech recognition (ASR) have been reported to yield improvement over the widely used maximum a-posteriori probability (MAP) decoder (Goel and Byrne, 2003; Mangu et al., 2000; Stolcke et al., 1997). For MT, MBR decoding was introduced in (Kumar and Byrne, 2004). It was shown that MBR is preferable over MAP decoding for different evaluation criteria. Here, we focus on the performance of MBR decoding for the BLEU score on various translation tasks. 3 Implementation of Minimum Bayes Risk Decoding for the BLEU Score 3.1 Bayes Decision Rule In statistical machine translation, we are given a source language sentence fJ1 = f1 ... fj ... fJ, which is to be translated into a target language sentence eI1 = e1 ... ei ... eI. Statistical decision theory te</context>
<context position="8202" citStr="Goel and Byrne, 2003" startWordPosition="1417" endWordPosition="1420">decision rule, we perform a summation over all sentence probabilities contained in the N-best list. Therefore, we use a global scaling factor a0 &gt; 0 to modify the individual scaling factors am: a&apos;m = a0 - am ,m = 1,...,M. For the MBR decision rule the modified scaling factors a&apos; m are used instead of the original model scaling factors am to compute the sentence probabilities as in Eq. 1. The global scaling factor a0 is tuned on the development set. Note that under the MAP decision rule any global scaling factor a0 &gt; 0 yields the same result. Similar tests were reported by (Mangu et al., 2000; Goel and Byrne, 2003) for ASR. 4 Experimental Results 4.1 Corpus Statistics We tested the MBR decoder on four translation tasks: the TC-STAR EPPS Spanish-English task of 2006, the NIST Chinese-English evaluation test set of 2005 and the GALE Arabic-English and ChineseEnglish evaluation test set of 2006. The TC-STAR EPPS corpus is a spoken language translation corpus containing the verbatim transcriptions of speeches of the European Parliament. The NIST ChineseEnglish test sets consists of news stories. The GALE project text track consists of two parts: newswire (“news”) and newsgroups (“ng”). The newswire part is </context>
</contexts>
<marker>Goel, Byrne, 2003</marker>
<rawString>V. Goel and W. Byrne. 2003. Minimum bayes-risk automatic speech recognition. Pattern Recognition in Speech and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics Annual Meeting (HLTNAACL),</booktitle>
<pages>169--176</pages>
<location>Boston, MA,</location>
<contexts>
<context position="2297" citStr="Kumar and Byrne, 2004" startWordPosition="365" endWordPosition="369">el like the BLEU and NIST score that measure precision and fluency of a given translation hypothesis. The remaining part of this paper is structured as follows: after a short overview of related work in Sec. 2, we describe the MBR decoder in Sec. 3. We present the experimental results in Sec. 4 and conclude in Sec. 5. 2 Related Work MBR decoder for automatic speech recognition (ASR) have been reported to yield improvement over the widely used maximum a-posteriori probability (MAP) decoder (Goel and Byrne, 2003; Mangu et al., 2000; Stolcke et al., 1997). For MT, MBR decoding was introduced in (Kumar and Byrne, 2004). It was shown that MBR is preferable over MAP decoding for different evaluation criteria. Here, we focus on the performance of MBR decoding for the BLEU score on various translation tasks. 3 Implementation of Minimum Bayes Risk Decoding for the BLEU Score 3.1 Bayes Decision Rule In statistical machine translation, we are given a source language sentence fJ1 = f1 ... fj ... fJ, which is to be translated into a target language sentence eI1 = e1 ... ei ... eI. Statistical decision theory tells us that among all possible target language sentences, we should choose the sentence which minimizes the</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>S. Kumar and W. Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In Proc. Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics Annual Meeting (HLTNAACL), pages 169–176, Boston, MA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mangu</author>
<author>E Brill</author>
<author>A Stolcke</author>
</authors>
<title>Finding consensus in speech recognition: Word error minimization and other applications of confusion networks.</title>
<date>2000</date>
<journal>Computer, Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="2210" citStr="Mangu et al., 2000" startWordPosition="350" endWordPosition="353">on Independent Word Error Rate (PER) as well as evaluation metric on the n-gram level like the BLEU and NIST score that measure precision and fluency of a given translation hypothesis. The remaining part of this paper is structured as follows: after a short overview of related work in Sec. 2, we describe the MBR decoder in Sec. 3. We present the experimental results in Sec. 4 and conclude in Sec. 5. 2 Related Work MBR decoder for automatic speech recognition (ASR) have been reported to yield improvement over the widely used maximum a-posteriori probability (MAP) decoder (Goel and Byrne, 2003; Mangu et al., 2000; Stolcke et al., 1997). For MT, MBR decoding was introduced in (Kumar and Byrne, 2004). It was shown that MBR is preferable over MAP decoding for different evaluation criteria. Here, we focus on the performance of MBR decoding for the BLEU score on various translation tasks. 3 Implementation of Minimum Bayes Risk Decoding for the BLEU Score 3.1 Bayes Decision Rule In statistical machine translation, we are given a source language sentence fJ1 = f1 ... fj ... fJ, which is to be translated into a target language sentence eI1 = e1 ... ei ... eI. Statistical decision theory tells us that among al</context>
<context position="8179" citStr="Mangu et al., 2000" startWordPosition="1413" endWordPosition="1416">this, using the MBR decision rule, we perform a summation over all sentence probabilities contained in the N-best list. Therefore, we use a global scaling factor a0 &gt; 0 to modify the individual scaling factors am: a&apos;m = a0 - am ,m = 1,...,M. For the MBR decision rule the modified scaling factors a&apos; m are used instead of the original model scaling factors am to compute the sentence probabilities as in Eq. 1. The global scaling factor a0 is tuned on the development set. Note that under the MAP decision rule any global scaling factor a0 &gt; 0 yields the same result. Similar tests were reported by (Mangu et al., 2000; Goel and Byrne, 2003) for ASR. 4 Experimental Results 4.1 Corpus Statistics We tested the MBR decoder on four translation tasks: the TC-STAR EPPS Spanish-English task of 2006, the NIST Chinese-English evaluation test set of 2005 and the GALE Arabic-English and ChineseEnglish evaluation test set of 2006. The TC-STAR EPPS corpus is a spoken language translation corpus containing the verbatim transcriptions of speeches of the European Parliament. The NIST ChineseEnglish test sets consists of news stories. The GALE project text track consists of two parts: newswire (“news”) and newsgroups (“ng”)</context>
<context position="10464" citStr="Mangu et al., 2000" startWordPosition="1799" endWordPosition="1802">d the decoder on N-best lists of size N=10000, i.e. the 10 000 best translation candidates. Note that in some cases the list is smaller because the translation system did not produce more candidates. To analyze the improvement that can be gained through rescoring with MBR, we start from a system that has already been rescored with additional models like an n-gram language model, HMM, IBM-1 and IBM-4. It turned out that the use of 1000 best candidates for the MBR decoding is sufficient, and leads to exactly the same results as the use of 10 000 best lists. Similar experiences were reported by (Mangu et al., 2000; Stolcke et al., 1997) for ASR. We observe that the improvement is larger for Table 3: GALE Arabic-English: corpus statistics. Arabic English Train Sentences 4M Words 125 M 124M Vocabulary 421 K 337 K news Sentences 566 Words 14160 15 320 ng Sentences 615 Words 11 195 14 493 103 Table 4: Translation results BLEU [%] for the NIST task, GALE task and TC-STAR task (S-E: SpanishEnglish; C-E: Chinese-English; A-E: Arabic-English). TC-STAR S-E NIST C-E GALE A-E GALE C-E decision rule test 2002 (dev) 2005 news ng news ng MAP 52.6 32.8 31.2 23.6 12.2 14.6 9.4 MBR 52.8 33.3 31.9 24.2 13.3 15.4 10.5 Ta</context>
</contexts>
<marker>Mangu, Brill, Stolcke, 2000</marker>
<rawString>L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus in speech recognition: Word error minimization and other applications of confusion networks. Computer, Speech and Language, 14(4):373–400, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>R Zens</author>
<author>D Vilar</author>
<author>A Mauser</author>
<author>M Popovi´c</author>
<author>S Hasan</author>
<author>H Ney</author>
</authors>
<title>The RWTH machine translation system.</title>
<date>2006</date>
<booktitle>In Proc. TC-Star Workshop on Speech-toSpeech Translation,</booktitle>
<pages>31--36</pages>
<location>Barcelona, Spain,</location>
<marker>Matusov, Zens, Vilar, Mauser, Popovi´c, Hasan, Ney, 2006</marker>
<rawString>E. Matusov, R. Zens, D. Vilar, A. Mauser, M. Popovi´c, S. Hasan, and H. Ney. 2006. The RWTH machine translation system. In Proc. TC-Star Workshop on Speech-toSpeech Translation, pages 31–36, Barcelona, Spain, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. 40th Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="4536" citStr="Och and Ney, 2002" startWordPosition="774" endWordPosition="777">�e1) = BP(I, 4 Precn(eI 1, �e1)1/4 rion under a 0-1 loss function: ˆI I) · H ˆI n=1 0 (el, e V 1 if eI1 = e&apos;i0 BP(I, I) 1 ifI&gt;_I L-1 ( 1, 1)={ 0 else l = exp (1 − I/�I) if I &lt; I Hence, the MAP decision rule is optimal for the sentence or string error rate. It is not necessarily optimal for other evaluation metrics as for example the BLEU score. One reason for the popularity of the MAP decision rule might be that, compared to the MBR rule, its computation is simpler. 3.2 Baseline System The posterior probability Pr(eI1|fJ1 ) is modeled directly using a log-linear combination of several models (Och and Ney, 2002): (EM ) exp m=1 λmhm(eI 1, fJ 1 ) E exp (EM1 λmhm(e�i0, fJ1 )) I0,e0�0 1 (1) This approach is a generalization of the sourcechannel approach (Brown et al., 1990). It has the advantage that additional models h(·) can be easily integrated into the overall system. The denominator represents a normalization factor that depends only on the source sentence fJ1 . Therefore, we can omit it in case of the MAP decision rule during the search process. Note that the denominator affects the results of the MBR decision rule and, thus, cannot be omitted in that case. We use a state-of-the-art phrase-based tr</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. 40th Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 295–302, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. 41st Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="5555" citStr="Och, 2003" startWordPosition="952" endWordPosition="953">ion rule during the search process. Note that the denominator affects the results of the MBR decision rule and, thus, cannot be omitted in that case. We use a state-of-the-art phrase-based translation system similar to (Matusov et al., 2006) including the following models: an n-gram language model, a phrase translation model and a word-based lexicon model. The latter two models are used for both directions: p(f|e) and p(e|f). Additionally, we use a word penalty, phrase penalty and a distortion penalty. The model scaling factors λM1 are optimized with respect to the BLEU score as described in (Och, 2003). Precn(eI1, �e1)= E min{C(wn1 |eI1), C(wn1 |�e1)} ˆI wl ˆI E C(wn1 |eI1) wi Here, C(wn1 |eI1) denotes the number of occurrences of an n-gram wn1 in a sentence eI1. The denominator of the n-gram precisions evaluate to the number of n-grams in the hypothesis, i.e. I − n + 1. As loss function for the MBR decoder, we use: L[eI1, �e1] = 1 − BLEU(eI ˆI 1, �e1) . ˆI While the original BLEU score was intended to be used only for aggregate counts over a whole test set, we use the BLEU score at the sentence-level during the selection of the MBR hypotheses. Note that we will use this sentence-level BLEU</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. 41st Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. 40th Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="3581" citStr="Papineni et al., 2002" startWordPosition="595" endWordPosition="598">ere, L(·, ·) denotes the loss function under consideration. In the following, we will call this decision rule the MBR rule (Kumar and Byrne, 2004). �e1 = argmin ˆI I,ei 101 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 101–104, Prague, June 2007. c�2007 Association for Computational Linguistics Although it is well known that this decision rule is optimal, most SMT systems do not use it. The most common approach is to use the MAP decision rule. Thus, we select the hypothesis which maximizes the posterior probability Pr(eI 1|fJ 1 ): � � Pr(eI1|fJ 1 ) 3.3 BLEU Score The BLEU score (Papineni et al., 2002) measures the agreement between a hypothesis eI1 generated by the MT system and a reference translation �e1. It is ˆI the geometric mean of n-gram precisions Precn(·, ·) in combination with a brevity penalty BP(·, ·) for too short translation hypotheses. e ˆI 1 = argmax I,ei This decision rule is equivalent to the MBR crite- BLEU(eI1, �e1) = BP(I, 4 Precn(eI 1, �e1)1/4 rion under a 0-1 loss function: ˆI I) · H ˆI n=1 0 (el, e V 1 if eI1 = e&apos;i0 BP(I, I) 1 ifI&gt;_I L-1 ( 1, 1)={ 0 else l = exp (1 − I/�I) if I &lt; I Hence, the MAP decision rule is optimal for the sentence or string error rate. It is </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. 40th Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>Y Konig</author>
<author>M Weintraub</author>
</authors>
<title>Explicit word error minimization in N-best list rescoring. In</title>
<date>1997</date>
<booktitle>Proc. European Conf. on Speech Communication and Technology,</booktitle>
<pages>163--166</pages>
<location>Rhodes, Greece,</location>
<contexts>
<context position="2233" citStr="Stolcke et al., 1997" startWordPosition="354" endWordPosition="357">Error Rate (PER) as well as evaluation metric on the n-gram level like the BLEU and NIST score that measure precision and fluency of a given translation hypothesis. The remaining part of this paper is structured as follows: after a short overview of related work in Sec. 2, we describe the MBR decoder in Sec. 3. We present the experimental results in Sec. 4 and conclude in Sec. 5. 2 Related Work MBR decoder for automatic speech recognition (ASR) have been reported to yield improvement over the widely used maximum a-posteriori probability (MAP) decoder (Goel and Byrne, 2003; Mangu et al., 2000; Stolcke et al., 1997). For MT, MBR decoding was introduced in (Kumar and Byrne, 2004). It was shown that MBR is preferable over MAP decoding for different evaluation criteria. Here, we focus on the performance of MBR decoding for the BLEU score on various translation tasks. 3 Implementation of Minimum Bayes Risk Decoding for the BLEU Score 3.1 Bayes Decision Rule In statistical machine translation, we are given a source language sentence fJ1 = f1 ... fj ... fJ, which is to be translated into a target language sentence eI1 = e1 ... ei ... eI. Statistical decision theory tells us that among all possible target langu</context>
<context position="10487" citStr="Stolcke et al., 1997" startWordPosition="1803" endWordPosition="1806">est lists of size N=10000, i.e. the 10 000 best translation candidates. Note that in some cases the list is smaller because the translation system did not produce more candidates. To analyze the improvement that can be gained through rescoring with MBR, we start from a system that has already been rescored with additional models like an n-gram language model, HMM, IBM-1 and IBM-4. It turned out that the use of 1000 best candidates for the MBR decoding is sufficient, and leads to exactly the same results as the use of 10 000 best lists. Similar experiences were reported by (Mangu et al., 2000; Stolcke et al., 1997) for ASR. We observe that the improvement is larger for Table 3: GALE Arabic-English: corpus statistics. Arabic English Train Sentences 4M Words 125 M 124M Vocabulary 421 K 337 K news Sentences 566 Words 14160 15 320 ng Sentences 615 Words 11 195 14 493 103 Table 4: Translation results BLEU [%] for the NIST task, GALE task and TC-STAR task (S-E: SpanishEnglish; C-E: Chinese-English; A-E: Arabic-English). TC-STAR S-E NIST C-E GALE A-E GALE C-E decision rule test 2002 (dev) 2005 news ng news ng MAP 52.6 32.8 31.2 23.6 12.2 14.6 9.4 MBR 52.8 33.3 31.9 24.2 13.3 15.4 10.5 Table 5: Translation exam</context>
<context position="12062" citStr="Stolcke et al., 1997" startWordPosition="2067" endWordPosition="2070">edina (west) in saudi citizen found guilty of killing one of its citizens. MBR-Hyp the saudi interior ministry announced in a statement to carry out the death sentence today in the area of medina (west) in saudi citizen was killed one of its citizens. Reference faruq al-shar’a takes the constitutional oath of office before the syrian president MAP-Hyp farouk al-shara leads sworn in by the syrian president MBR-Hyp farouk al-shara lead the constitutional oath before the syrian president low-scoring translations, as can be seen in the GALE task. For an ASR task, similar results were reported by (Stolcke et al., 1997). Some translation examples for the GALE ArabicEnglish newswire task are shown in Tab. 5. The differences between the MAP and the MBR hypotheses are set in italics. 5 Conclusions We have shown that Minimum Bayes Risk decoding on N-best lists improves the BLEU score considerably. The achieved results are promising. The improvements were consistent among several evaluation sets. Even if the improvement is sometimes small, e.g. TC-STAR, it is statistically significant: the absolute improvement of the BLEU score is between 0.2% for the TC-STAR task and 1.1% for the GALE Chinese-English task. Note,</context>
</contexts>
<marker>Stolcke, Konig, Weintraub, 1997</marker>
<rawString>A. Stolcke, Y. Konig, and M. Weintraub. 1997. Explicit word error minimization in N-best list rescoring. In Proc. European Conf. on Speech Communication and Technology, pages 163–166, Rhodes, Greece, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>