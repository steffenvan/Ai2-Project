<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.98063">
Learning Dependency-Based Compositional Semantics
</title>
<author confidence="0.941967">
Percy Liang
</author>
<affiliation confidence="0.514055">
UC Berkeley
</affiliation>
<email confidence="0.969761">
pliang@cs.berkeley.edu
</email>
<note confidence="0.7487385">
Michael I. Jordan
UC Berkeley
</note>
<email confidence="0.934996">
jordan@cs.berkeley.edu
</email>
<author confidence="0.740262">
Dan Klein
</author>
<affiliation confidence="0.40784">
UC Berkeley
</affiliation>
<email confidence="0.985366">
klein@cs.berkeley.edu
</email>
<sectionHeader confidence="0.997244" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9974953125">
Compositional question answering begins by
mapping questions to logical forms, but train-
ing a semantic parser to perform this mapping
typically requires the costly annotation of the
target logical forms. In this paper, we learn
to map questions to answers via latent log-
ical forms, which are induced automatically
from question-answer pairs. In tackling this
challenging learning problem, we introduce a
new semantic representation which highlights
a parallel between dependency syntax and effi-
cient evaluation of logical forms. On two stan-
dard semantic parsing benchmarks (GEO and
JOBS), our system obtains the highest pub-
lished accuracies, despite requiring no anno-
tated logical forms.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999322">
What is the total population of the ten largest cap-
itals in the US? Answering these types of complex
questions compositionally involves first mapping the
questions into logical forms (semantic parsing). Su-
pervised semantic parsers (Zelle and Mooney, 1996;
Tang and Mooney, 2001; Ge and Mooney, 2005;
Zettlemoyer and Collins, 2005; Kate and Mooney,
2007; Zettlemoyer and Collins, 2007; Wong and
Mooney, 2007; Kwiatkowski et al., 2010) rely on
manual annotation of logical forms, which is expen-
sive. On the other hand, existing unsupervised se-
mantic parsers (Poon and Domingos, 2009) do not
handle deeper linguistic phenomena such as quan-
tification, negation, and superlatives.
As in Clarke et al. (2010), we obviate the need
for annotated logical forms by considering the end-
to-end problem of mapping questions to answers.
However, we still model the logical form (now as a
latent variable) to capture the complexities of lan-
guage. Figure 1 shows our probabilistic model:
</bodyText>
<figure confidence="0.9013456">
(parameters) (world)
θ w
Semantic Parsing Evaluation
x z y
(question) (logical form) (answer)
</figure>
<figureCaption confidence="0.805263">
Figure 1: Our probabilistic model: a question x is
mapped to a latent logical form z, which is then evaluated
</figureCaption>
<bodyText confidence="0.986119173913043">
with respect to a world w (database of facts), producing
an answer y. We represent logical forms z as labeled
trees, induced automatically from (x, y) pairs.
We want to induce latent logical forms z (and pa-
rameters 0) given only question-answer pairs (x, y),
which is much cheaper to obtain than (x, z) pairs.
The core problem that arises in this setting is pro-
gram induction: finding a logical form z (over an
exponentially large space of possibilities) that pro-
duces the target answer y. Unlike standard semantic
parsing, our end goal is only to generate the correct
y, so we are free to choose the representation for z.
Which one should we use?
The dominant paradigm in compositional se-
mantics is Montague semantics, which constructs
lambda calculus forms in a bottom-up manner. CCG
is one instantiation (Steedman, 2000), which is used
by many semantic parsers, e.g., Zettlemoyer and
Collins (2005). However, the logical forms there
can become quite complex, and in the context of
program induction, this would lead to an unwieldy
search space. At the same time, representations such
as FunQL (Kate et al., 2005), which was used in
</bodyText>
<figure confidence="0.9211899">
state with the
largest area X,
state
1
1
area
argmax
Alaska
z ∼ pθ(z  |x)
y = Hw
</figure>
<page confidence="0.958311">
590
</page>
<note confidence="0.981406">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 590–599,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.984486066666667">
Clarke et al. (2010), are simpler but lack the full ex-
pressive power of lambda calculus.
The main technical contribution of this work is
a new semantic representation, dependency-based
compositional semantics (DCS), which is both sim-
ple and expressive (Section 2). The logical forms in
this framework are trees, which is desirable for two
reasons: (i) they parallel syntactic dependency trees,
which facilitates parsing and learning; and (ii) eval-
uating them to obtain the answer is computationally
efficient.
We trained our model using an EM-like algorithm
(Section 3) on two benchmarks, GEO and JOBS
(Section 4). Our system outperforms all existing
systems despite using no annotated logical forms.
</bodyText>
<sectionHeader confidence="0.993234" genericHeader="method">
2 Semantic Representation
</sectionHeader>
<bodyText confidence="0.974704090909091">
We first present a basic version (Section 2.1) of
dependency-based compositional semantics (DCS),
which captures the core idea of using trees to rep-
resent formal semantics. We then introduce the full
version (Section 2.2), which handles linguistic phe-
nomena such as quantification, where syntactic and
semantic scope diverge.
We start with some definitions, using US geogra-
phy as an example domain. Let V be the set of all
values, which includes primitives (e.g., 3, CA ∈ V)
as well as sets and tuples formed from other values
(e.g., 3, {3, 4, 7}, (CA, {5}) ∈ V). Let P be a set
of predicates (e.g., state, count ∈ P), which are
just symbols.
A world w is mapping from each predicate p ∈
P to a set of tuples; for example, w(state) =
{(CA), (OR),... }. Conceptually, a world is a rela-
tional database where each predicate is a relation
(possibly infinite). Define a special predicate ø with
w(ø) = V. We represent functions by a set of input-
output pairs, e.g., w(count) = {(S, n) : n = |S|}.
As another example, w(average) = {(S, ¯x) :
</bodyText>
<equation confidence="0.510009666666667">
x¯ = |S1|−1 E
Relations R
j0 j E (extract)
Σ (join) Q (quantify)
Xi (aggregate) C (compare)
(execute)
</equation>
<tableCaption confidence="0.9326816">
Table 1: Possible relations appearing on the edges of a
DCS tree. Here, j, j&apos; ∈ {1, 2,... } and i ∈ {1, 2,... }�.
z.p ∈ P and (ii) a sequence of edges z.e1, ... , z.em,
each edge e consisting of a relation e.r ∈ R (see
Table 1) and a child tree e.c ∈ Z.
</tableCaption>
<bodyText confidence="0.99846125">
We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.
Figure 2(a) shows an example of a DCS tree. Al-
though a DCS tree is a logical form, note that it looks
like a syntactic dependency tree with predicates in
place of words. It is this transparency between syn-
tax and semantics provided by DCS which leads to
a simple and streamlined compositional semantics
suitable for program induction.
</bodyText>
<subsectionHeader confidence="0.993381">
2.1 Basic Version
</subsectionHeader>
<bodyText confidence="0.999924157894737">
The basic version of DCS restricts R to join and ag-
gregate relations (see Table 1). Let us start by con-
sidering a DCS tree z with only join relations. Such
a z defines a constraint satisfaction problem (CSP)
with nodes as variables. The CSP has two types of
constraints: (i) x ∈ w(p) for each node x labeled
with predicate p ∈ P; and (ii) xj = yj0 (the j-th
component of x must equal the j&apos;-th component of
y) for each edge (x, y) labeled with j0j ∈ R.
A solution to the CSP is an assignment of nodes
to values that satisfies all the constraints. We say a
value v is consistent for a node x if there exists a
solution that assigns v to x. The denotation JzKw (z
evaluated on w) is the set of consistent values of the
root node (see Figure 2 for an example).
Computation We can compute the denotation
JzKw of a DCS tree z by exploiting dynamic pro-
gramming on trees (Dechter, 2003). The recurrence
is as follows:
</bodyText>
<equation confidence="0.9970685">
J(p;j01:c1; ··· ; j :cm)K (1)
{v : vjz= tj0 z, t ∈ JciKw}.
</equation>
<bodyText confidence="0.9642258">
At each node, we compute the set of tuples v consis-
tent with the predicate at that node (v ∈ w(p)), and
S(x)}, where a set of pairs S
is treated as a set-valued function S(x) = {y :
(x, y) ∈ S} with domain S1 = {x : (x, y) ∈ S}.
The logical forms in DCS are called DCS trees,
where nodes are labeled with predicates, and edges
are labeled with relations. Formally:
Definition 1 (DCS trees) Let Z be the set of DCS
trees, where each z ∈ Z consists of (i) a predicate
</bodyText>
<equation confidence="0.925466142857143">
xES1
m
n
i=1
= w(p) ∩
591
Example: major city in California
z = (city;11:(major) ; 11:(loc; 21:(CA)))
Ac Elm Eli Els.
city(c) ∧ major(m)∧
loc(i) ∧ CA(s)∧
c1 = m1 ∧ c1 = i1 ∧ i2 = s1
(a) DCS tree (b) Lambda calculus formula
(c) Denotation: Ez�.. = {SF, LA, ... }
</equation>
<figureCaption confidence="0.9895545">
Figure 2: (a) An example of a DCS tree (written in both
the mathematical and graphical notation). Each node is
labeled with a predicate, and each edge is labeled with a
relation. (b) A DCS tree z with only join relations en-
codes a constraint satisfaction problem. (c) The denota-
tion of z is the set of consistent values for the root node.
</figureCaption>
<bodyText confidence="0.9996335">
for each child i, the ji-th component of v must equal
the j&apos;i-th component of some t in the child’s deno-
tation (t ∈ JciKw). This algorithm is linear in the
number of nodes times the size of the denotations.1
Now the dual importance of trees in DCS is clear:
We have seen that trees parallel syntactic depen-
dency structure, which will facilitate parsing. In
addition, trees enable efficient computation, thereby
establishing a new connection between dependency
syntax and efficient semantic evaluation.
Aggregate relation DCS trees that only use join
relations can represent arbitrarily complex compo-
sitional structures, but they cannot capture higher-
order phenomena in language. For example, con-
sider the phrase number of major cities, and suppose
that number corresponds to the count predicate.
It is impossible to represent the semantics of this
phrase with just a CSP, so we introduce a new ag-
gregate relation, notated E. Consider a tree hE:ci,
whose root is connected to a child c via E. If the de-
notation of c is a set of values s, the parent’s denota-
tion is then a singleton set containing s. Formally:
</bodyText>
<equation confidence="0.989986">
JhE:ciKw = {JcKw}- (2)
</equation>
<bodyText confidence="0.959194">
Figure 3(a) shows the DCS tree for our running
example. The denotation of the middle node is {s},
</bodyText>
<footnote confidence="0.76196975">
1Infinite denotations (such as Q&lt;Jw) are represented as im-
plicit sets on which we can perform membership queries. The
intersection of two sets can be performed as long as at least one
of the sets is finite.
</footnote>
<figureCaption confidence="0.999060666666667">
Figure 3: Examples of DCS trees that use the aggregate
relation (E) to (a) compute the cardinality of a set and (b)
take the average over a set.
</figureCaption>
<bodyText confidence="0.999727444444444">
where s is all major cities. Having instantiated s as
a value, everything above this node is an ordinary
CSP: s constrains the count node, which in turns
constrains the root node to |s|.
A DCS tree that contains only join and aggre-
gate relations can be viewed as a collection of tree-
structured CSPs connected via aggregate relations.
The tree structure still enables us to compute deno-
tations efficiently based on (1) and (2).
</bodyText>
<subsectionHeader confidence="0.996021">
2.2 Full Version
</subsectionHeader>
<bodyText confidence="0.9996935625">
The basic version of DCS described thus far han-
dles a core subset of language. But consider Fig-
ure 4: (a) is headed by borders, but states needs
to be extracted; in (b), the quantifier no is syntacti-
cally dominated by the head verb borders but needs
to take wider scope. We now present the full ver-
sion of DCS which handles this type of divergence
between syntactic and semantic scope.
The key idea that allows us to give semantically-
scoped denotations to syntactically-scoped trees is
as follows: We mark a node low in the tree with a
mark relation (one of E, Q, or C). Then higher up in
the tree, we invoke it with an execute relation Xi to
create the desired semantic scope.2
This mark-execute construct acts non-locally, so
to maintain compositionality, we must augment the
</bodyText>
<footnote confidence="0.865209333333333">
2Our mark-execute construct is analogous to Montague’s
quantifying in, Cooper storage, and Carpenter’s scoping con-
structor (Carpenter, 1998).
</footnote>
<figure confidence="0.997070779069767">
number of average population of
major cities major cities
1
1
major
(a) Counting (b) Averaging
1
1
Σ
2
count
1
1
Σ
city
1
1
2
average
1
1
population
1
1
city
major
city
1
1
1
1
major
loc
1
CA
2
592
California borders which states?
Alaska borders no states.
X
1
X
1
1
CA
border
1 2
1
state
1
AK
border
1 2
1
state
E
Q
no
X12
X21
Some river traverses every city.
1
river
Q
some
traverse
1 2
1
every
city
Q
river
some
Q
traverse
1 2
1 1
every
city
Q
(narrow) (wide)
city traversed by no rivers
X12
city
E
1
2
traverse
1
1
river
no
Q
(a) Extraction (E) (b) Quantification (Q) (c) Quantifier ambiguity (Q, Q) (d) Quantification (Q, E)
state bordering
the largest state
border
state
state
E
X12
2
1
1
1
1
1
argmaE
size
c
state
E
X12
1
1
border
argmaE
state
size
c
2
1
1
1
(absolute) (relative)
Every state’s
largest city is major.
X1
major
city
X2
1 1
1 1
loc
2
1
size
c
state
argmaE
Q
every
state bordering
the most states
state bordering
more states than Texas
X12
state
E
1
1
border
2
1
state
c
more
3
1
TR
X12
2
1
state
c
argmaE
state
E
1
1
border
(e) Superlative (c) (f) Comparative (c) (g) Superlative ambiguity (c) (h) Quantification+Superlative (Q, c)
</figure>
<figureCaption confidence="0.9912685">
Figure 4: Example DCS trees for utterances in which syntactic and semantic scope diverge. These trees reflect the
syntactic structure, which facilitates parsing, but importantly, these trees also precisely encode the correct semantic
scope. The main mechanism is using a mark relation (E, Q, or C) low in the tree paired with an execute relation (Xi)
higher up at the desired semantic point.
</figureCaption>
<bodyText confidence="0.993116">
denotation d = JzKw to include any information
about the marked nodes in z that can be accessed
by an execute relation later on. In the basic ver-
sion, d was simply the consistent assignments to the
root. Now d contains the consistent joint assign-
ments to the active nodes (which include the root
and all marked nodes), as well as information stored
about each marked node. Think of d as consisting
of n columns, one for each active node according to
a pre-order traversal of z. Column 1 always corre-
sponds to the root node. Formally, a denotation is
defined as follows (see Figure 5 for an example):
Definition 2 (Denotations) Let D be the set of de-
notations, where each d E D consists of
</bodyText>
<listItem confidence="0.99959675">
• a set of arrays d.A, where each array a =
[ai, ... , an] E d.A is a sequence of n tuples
(ai E V*); and
• a list of n stores d.α = (d.ai, ... , d.an),
</listItem>
<bodyText confidence="0.983685777777778">
where each store a contains a mark relation
a.r E {E, Q, C, 0}, a base denotation a.b E
D U{0}, and a child denotation a.c E D U{0}.
We write d as ((A; (ri, bi, ci); ... ; (rn, bn, cn))). We
use d{ri = x} to mean d with d.ri = d.ai.r = x
(similar definitions apply for d{ai = x}, d{bi = x},
and d{ci = x}).
The denotation of a DCS tree can now be defined
recursively:
</bodyText>
<equation confidence="0.997455857142857">
J(p)Kw = (({[v] : v E w(p¶¶)}; ø)), (3)
JCp; e; j,: c&gt;Kw = Jp; eK w mj,j, JcI w , (4)
J(p; e; Σ:c)Kw = Jp; e¶�w m*,* Σ (JcKw), (5)
J(p; e; Xi: c)Kw = Jp; el w m*,* Xi(JcKw), (6)
J(p; e; E:c)Kw = M(Jp; eKw, E, c), (7)
J(p; e; C:c)Kw = M(Jp; eKw, C, c), (8)
J(p; Q:c;e)Kw = M(Jp;eKw, Q, c). (9)
</equation>
<page confidence="0.994627">
593
</page>
<figureCaption confidence="0.7092108">
DCS tree Denotation
Figure 5: Example of the denotation for a DCS tree with
a compare relation C. This denotation has two columns,
one for each active node—the root node state and the
marked node size.
</figureCaption>
<bodyText confidence="0.999242363636364">
The base case is defined in (3): if z is a sin-
gle node with predicate p, then the denotation of z
has one column with the tuples w(p) and an empty
store. The other six cases handle different edge re-
lations. These definitions depend on several opera-
tions (mj,j0, E, Xi, M) which we will define shortly,
but let us first get some intuition.
Let z be a DCS tree. If the last child c of z’s
root is a join (jj0), aggregate (E), or execute (Xi) re-
lation ((4)–(6)), then we simply recurse on z with c
removed and join it with some transformation (iden-
tity, E, or Xi) of c’s denotation. If the last (or first)
child is connected via a mark relation E, C (or Q),
then we strip off that child and put the appropriate
information in the store by invoking M.
We now define the operations mj,j0, E, Xi, M.
Some helpful notation: For a sequence v =
(v1,... , vn) and indices i = (i1, ... , ik), let vi =
(vi1, ... , vik) be the projection of v onto i; we write
v−i to mean v�1 i. Extending this notation to
denotations, let (hA; αii[i] = hh{ai : a ∈ A}; αiii.
Let d[−ø] = d[−i], where i are the columns with
empty stores. For example, for d in Figure 5, d[1]
keeps column 1, d[−ø] keeps column 2, and d[2, −2]
swaps the two columns.
Join The join of two denotations d and d&apos; with re-
spect to components j and j&apos; (∗ means all compo-
nents) is formed by concatenating all arrays a of d
with all compatible arrays a&apos; of d&apos;, where compat-
ibility means a1j = a&apos;1j0. The stores are also con-
catenated (α + α&apos;). Non-initial columns with empty
stores are projected away by applying ·[1,−ø]. The
full definition of join is as follows:
</bodyText>
<equation confidence="0.9901985">
hhA; αii mj,j0 hhA&apos;; α&apos;ii = hhA&apos;&apos;; α + α&apos;ii[1,−ø],
A&apos;&apos; = {a + a&apos; : a ∈ A,a&apos; ∈ A&apos;,a1j = a&apos;1j0}. (10)
</equation>
<bodyText confidence="0.999855666666667">
Aggregate The aggregate operation takes a deno-
tation and forms a set out of the tuples in the first
column for each setting of the rest of the columns:
</bodyText>
<equation confidence="0.9987996">
E (hhA; αii) = hhA&apos; ∪ A&apos;&apos;; αii (11)
A&apos; = {[5(a), a2, ... , an] : a ∈ A}
5(a) = {a&apos;1 : [a&apos;1, a2, ... , an] ∈ A}
A&apos;&apos; = {[∅, a2, ... , an] : ¬∃a1, a ∈ A,
∀2 ≤ i ≤ n, [ai] ∈ d.bi[1].A}.
</equation>
<subsectionHeader confidence="0.424851">
2.2.1 Mark and Execute
</subsectionHeader>
<bodyText confidence="0.961169625">
Now we turn to the mark (M) and execute (Xi)
operations, which handles the divergence between
syntactic and semantic scope. In some sense, this is
the technical core of DCS. Marking is simple: When
a node (e.g., size in Figure 5) is marked (e.g., with
relation C), we simply put the relation r, current de-
notation d and child c’s denotation into the store of
column 1:
</bodyText>
<equation confidence="0.997067">
M(d, r, c) = d{r1 = r, b1 = d, c1 = Qc�w}. (12)
</equation>
<bodyText confidence="0.993772375">
The execute operation Xi(d) processes columns
i in reverse order. It suffices to define Xi(d) for a
single column i. There are three cases:
Extraction (d.ri = E) In the basic version, the
denotation of a tree was always the set of con-
sistent values of the root node. Extraction al-
lows us to return the set of consistent values of a
marked non-root node. Formally, extraction sim-
ply moves the i-th column to the front: Xi(d) =
d[i, −(i, ø)]{α1 = ø}. For example, in Figure 4(a),
before execution, the denotation of the DCS tree
is hh{[(CA, OR), (OR)],... }; ø; (E, Qhstatei�w, ø)ii;
after applying X1, we have hh{[(OR)], ... }; øii.
Generalized Quantification (d.ri = Q) Gener-
alized quantifiers are predicates on two sets, a re-
strictor A and a nuclear scope B. For example,
</bodyText>
<equation confidence="0.7887835">
w(no) = {(A, B) : A ∩ B = ∅} and w(most) =
{(A, B) : |A ∩ B |&gt; 12|A|}.
</equation>
<bodyText confidence="0.9996585">
In a DCS tree, the quantifier appears as the
child of a Q relation, and the restrictor is the par-
ent (see Figure 4(b) for an example). This in-
formation is retrieved from the store when the
</bodyText>
<figure confidence="0.979463913043478">
column 1 column 2
(TX,2.7e5)
(TX,2.7e5)
(CA,1.6e5)
r: 0 C
b: 0 J(size&amp;
c: 0 J(argmaxA.
A:
(OK)
(NM)
(NV)
border
argmax
state
state
size
2
1
1
1
1
1
J·K.
</figure>
<page confidence="0.979906">
594
</page>
<bodyText confidence="0.809997625">
quantifier in column i is executed. In particu- using the exact same machinery as superlatives. Fig-
lar, the restrictor is A = E (d.bi) and the nu- ure 4(g) shows that we can naturally account for
clear scope is B = E (d[i, −(i, 0)]). We then superlative ambiguity based on where the scope-
apply d.ci to these two sets (technically, denota- determining execute relation is placed.
tions) and project away the first column: Xi(d) = 3 Semantic Parsing
((d.ci ./1,1 A) ./2,1 B) [−1]. We now turn to the task of mapping natural language
For the example in Figure 4(b), the de- utterances to DCS trees. Our first question is: given
notation of the DCS tree before execution is an utterance x, what trees z ∈ Z are permissible? To
</bodyText>
<figureCaption confidence="0.9796786">
hh∅; 0; (Q, JhstateiK,,,, JhnoiK,,,)ii. The restrictor define the search space, we first assume a fixed set
set (A) is the set of all states, and the nuclear scope of lexical triggers L. Each trigger is a pair (x, p),
(B) is the empty set. Since (A, B) exists in no, the where x is a sequence of words (usually one) and p
final denotation, which projects away the actual pair, is a predicate (e.g., x = California and p = CA).
is hh{[ ]}ii (our representation of true). We use L(x) to denote the set of predicates p trig-
Figure 4(c) shows an example with two interact- gered by x ((x,p) ∈ L). Let L(E) be the set of
ing quantifiers. The quantifier scope ambiguity is trace predicates, which can be introduced without
resolved by the choice of execute relation; X12 gives an overt lexical trigger.
the narrow reading and X21 gives the wide reading. Given an utterance x = (x1, ... , x,,,), we define
Figure 4(d) shows how extraction and quantification ZL(x) ⊂ Z, the set of permissible DCS trees for
work together. x. The basic approach is reminiscent of projective
Comparatives and Superlatives (d.ri = C) To labeled dependency parsing: For each span i..j, we
compare entities, we use a set S of (x, y) pairs, build a set of trees Ci,j and set ZL(x) = C0,,,,. Each
where x is an entity and y is a number. For su- set Ci,j is constructed recursively by combining the
perlatives, the argmax predicate denotes pairs of trees of its subspans Ci,k and Ck,,j for each pair of
sets and the set’s largest element(s): w(argmax) = split points k, k&apos; (words between k and k&apos; are ig-
{(S, x*) : x* ∈ argmaxxES1 max S(x)}. For com- nored). These combinations are then augmented via
paratives, w(more) contains triples (S, x, y), where a function A and filtered via a function F, to be spec-
x is “more than” y as measured by S; formally: ified later. Formally, Ci,j is defined recursively as
w(more) = {(S,x,y) : maxS(x) &gt; maxS(y)}. follows:
In a superlative/comparative construction, the Ci,j = F A L(xi+1..j) ∪
root x of the DCS tree is the entity to be compared, ( ( U ))T1(a, b)) .
the child c of a C relation is the comparative or su- i&lt;k&lt;k&apos;&lt;j
perlative, and its parent p contains the information aECi,k
used for comparison (see Figure 4(e) for an exam- bECkl,j
ple). If d is the denotation of the root, its i-th column (13)
contains this information. There are two cases: (i) if In (13), L(xi+1..j) is the set of predicates triggered
the i-th column of d contains pairs (e.g., size in by the phrase under span i..j (the base case), and
Figure 5), then let d&apos; = Jh0iK. ./1,2 d[i, −i], which Td(a, b) = ~Td(a, b) ∪Td(b, a), which returns all
reads out the second components of these pairs; (ii) ways of combining trees a and b where b is a de-
otherwise (e.g., state in Figure 4(e)), let d&apos; = ~
Jh0iK� ./1,2 JhcountiK� ./1,1 E (d[i, −i]), which scendant of a ( Td) or vice-versa (Td). The former is
counts the number of things (e.g., states) that occur defined recursively as follows: ~T0(a, b) = ∅, and
with each value of the root x. Given d&apos;, we construct ~Td(a, b) = U {ha;r:bi} ∪ ~Td−1(a, hp; r:bi).
a denotation S by concatenating (+i) the second and rEIZ
first columns of d&apos; (S = E (+2,1 (d&apos;{α2 = 0}))) pEL(e)
and apply the superlative/comparative: Xi(d) = The latter (Tk) is defined similarly. Essentially,
(Jh0iK. ./1,2 (d.ci ./1,1 S)){α1 = d.α1}. ~Td(a, b) allows us to insert up to d trace predi-
Figure 4(f) shows that comparatives are handled cates between the roots of a and b. This is use-
595 ful for modeling relations in noun compounds (e.g.,
</figureCaption>
<bodyText confidence="0.9988562">
California cities), and it also allows us to underspec-
ify L. In particular, our L will not include verbs or
prepositions; rather, we rely on the predicates corre-
sponding to those words to be triggered by traces.
The augmentation function A takes a set of trees
and optionally attaches E and Xi relations to the
root (e.g., A(hcityi) = {hcityi , hcity; E:øi}).
The filtering function F rules out improperly-typed
trees such as hcity; 00:hstateii. To further reduce
the search space, F imposes a few additional con-
straints, e.g., limiting the number of marked nodes
to 2 and only allowing trace predicates between ar-
ity 1 predicates.
Model We now present our discriminative se-
mantic parsing model, which places a log-linear
distribution over z ∈ ZL(x) given an utter-
ance x. Formally, pθ(z  |x) ∝ eφ(x,z)Tθ,
where θ and φ(x, z) are parameter and feature vec-
tors, respectively. As a running example, con-
sider x = city that is in California and z =
hcity; 11:hloc; 21:hCAiii, where city triggers city
and California triggers CA.
To define the features, we technically need to
augment each tree z ∈ ZL(x) with alignment
information—namely, for each predicate in z, the
span in x (if any) that triggered it. This extra infor-
mation is already generated from the recursive defi-
nition in (13).
The feature vector φ(x, z) is defined by sums of
five simple indicator feature templates: (F1) a word
triggers a predicate (e.g., [city, city]); (F2) a word
is under a relation (e.g., [that, 11]); (F3) a word is un-
der a trace predicate (e.g., [in, loc]); (F4) two pred-
icates are linked via a relation in the left or right
direction (e.g., [city,11, loc, RIGHT]); and (F5) a
predicate has a child relation (e.g., [city, 11]).
Learning Given a training dataset D con-
taining (x, y) pairs, we define the regu-
larized marginal log-likelihood objective
O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈
ZL(x)) − λkθk22, which sums over all DCS trees z
that evaluate to the target answer y.
Our model is arc-factored, so we can sum over all
DCS trees in ZL(x) using dynamic programming.
However, in order to learn, we need to sum over
{z ∈ ZL(x) : JzKw = y}, and unfortunately, the
additional constraint JzKw = y does not factorize.
We therefore resort to beam search. Specifically, we
truncate each Ci,j to a maximum of K candidates
sorted by decreasing score based on parameters θ.
Let ˜ZL,θ(x) be this approximation of ZL(x).
Our learning algorithm alternates between (i) us-
ing the current parameters θ to generate the K-best
set ˜ZL,θ(x) for each training example x, and (ii)
optimizing the parameters to put probability mass
on the correct trees in these sets; sets contain-
ing no correct answers are skipped. Formally, let
˜O(θ, θ&apos;) be the objective function O(θ) with ZL(x)
˜ZL,θI(x). We optimize ˜O(θ,θ&apos;) by
setting θ(0) = 0~ and iteratively solving θ(t+1) =
argmaxθ ˜O(θ, θ(t)) using L-BFGS until t = T. In all
experiments, we set λ = 0.01, T = 5, and K = 100.
After training, given a new utterance x, our system
outputs the most likely y, summing out the latent
logical form z: argmaxy pθ(T)(y  |x, z ∈ ˜ZL,θ(T)).
</bodyText>
<sectionHeader confidence="0.999239" genericHeader="conclusions">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999970923076923">
We tested our system on two standard datasets, GEO
and JOBS. In each dataset, each sentence x is an-
notated with a Prolog logical form, which we use
only to evaluate and get an answer y. This evalua-
tion is done with respect to a world w. Recall that
a world w maps each predicate p ∈ P to a set of
tuples w(p). There are three types of predicates in
P: generic (e.g., argmax), data (e.g., city), and
value (e.g., CA). GEO has 48 non-value predicates
and JOBS has 26. For GEO, w is the standard US
geography database that comes with the dataset. For
JOBS, if we use the standard Jobs database, close to
half the y’s are empty, which makes it uninteresting.
We therefore generated a random Jobs database in-
stead as follows: we created 100 job IDs. For each
data predicate p (e.g., language), we add each pos-
sible tuple (e.g., (job37, Java)) to w(p) indepen-
dently with probability 0.8.
We used the same training-test splits as Zettle-
moyer and Collins (2005) (600+280 for GEO and
500+140 for JOBS). During development, we fur-
ther held out a random 30% of the training sets for
validation.
Our lexical triggers L include the following: (i)
predicates for a small set of ≈ 20 function words
(e.g., (most, argmax)), (ii) (x, x) for each value
</bodyText>
<equation confidence="0.315573">
replaced with
</equation>
<page confidence="0.978793">
596
</page>
<table confidence="0.9844366">
System Accuracy
Clarke et al. (2010) w/answers 73.2
Clarke et al. (2010) w/logical forms 80.4
Our system (DCS with L) 78.9
Our system (DCS with L+) 87.2
</table>
<tableCaption confidence="0.856120833333333">
Table 2: Results on GEO with 250 training and 250
test examples. Our results are averaged over 10 random
250+250 splits taken from our 600 training examples. Of
the three systems that do not use logical forms, our two
systems yield significant improvements. Our better sys-
tem even outperforms the system that uses logical forms.
</tableCaption>
<bodyText confidence="0.997532967741935">
predicate x in w (e.g., (Boston, Boston)), and
(iii) predicates for each POS tag in {JJ, NN, NNS}
(e.g., (JJ, size), (JJ, area), etc.).3 Predicates
corresponding to verbs and prepositions (e.g.,
traverse) are not included as overt lexical trig-
gers, but rather in the trace predicates L(E).
We also define an augmented lexicon L+ which
includes a prototype word x for each predicate ap-
pearing in (iii) above (e.g., (large, size)), which
cancels the predicates triggered by x’s POS tag. For
GEO, there are 22 prototype words; for JOBS, there
are 5. Specifying these triggers requires minimal
domain-specific supervision.
Results We first compare our system with Clarke
et al. (2010) (henceforth, SEMRESP), which also
learns a semantic parser from question-answer pairs.
Table 2 shows that our system using lexical triggers
L (henceforth, DCS) outperforms SEMRESP (78.9%
over 73.2%). In fact, although neither DCS nor
SEMRESP uses logical forms, DCS uses even less su-
pervision than SEMRESP. SEMRESP requires a lex-
icon of 1.42 words per non-value predicate, Word-
Net features, and syntactic parse trees; DCS requires
only words for the domain-independent predicates
(overall, around 0.5 words per non-value predicate),
POS tags, and very simple indicator features. In
fact, DCS performs comparably to even the version
of SEMRESP trained using logical forms. If we add
prototype triggers (use L+), the resulting system
(DCS+) outperforms both versions of SEMRESP by
a significant margin (87.2% over 73.2% and 80.4%).
</bodyText>
<footnote confidence="0.870243666666667">
3We used the Berkeley Parser (Petrov et al., 2006) to per-
form POS tagging. The triggers L(x) for a word x thus include
L(t) where t is the POS tag of x.
</footnote>
<table confidence="0.923287555555556">
System GEO JOBS
Tang and Mooney (2001) 79.4 79.8
Wong and Mooney (2007) 86.6 –
Zettlemoyer and Collins (2005) 79.3 79.3
Zettlemoyer and Collins (2007) 81.6 –
Kwiatkowski et al. (2010) 88.2 –
Kwiatkowski et al. (2010) 88.9 –
Our system (DCS with L) 88.6 91.4
Our system (DCS with L+) 91.1 95.0
</table>
<tableCaption confidence="0.811452777777778">
Table 3: Accuracy (recall) of systems on the two bench-
marks. The systems are divided into three groups. Group
1 uses 10-fold cross-validation; groups 2 and 3 use the in-
dependent test set. Groups 1 and 2 measure accuracy of
logical form; group 3 measures accuracy of the answer;
but there is very small difference between the two as seen
from the Kwiatkowski et al. (2010) numbers. Our best
system improves substantially over past work, despite us-
ing no logical forms as training data.
</tableCaption>
<bodyText confidence="0.999987">
Next, we compared our systems (DCS and DCS+)
with the state-of-the-art semantic parsers on the full
dataset for both GEO and JOBS (see Table 3). All
other systems require logical forms as training data,
whereas ours does not. Table 3 shows that even DCS,
which does not use prototypes, is comparable to the
best previous system (Kwiatkowski et al., 2010), and
by adding a few prototypes, DCS+ offers a decisive
edge (91.1% over 88.9% on GEO). Rather than us-
ing lexical triggers, several of the other systems use
IBM word alignment models to produce an initial
word-predicate mapping. This option is not avail-
able to us since we do not have annotated logical
forms, so we must instead rely on lexical triggers
to define the search space. Note that having lexical
triggers is a much weaker requirement than having
a CCG lexicon, and far easier to obtain than logical
forms.
Intuitions How is our system learning? Initially,
the weights are zero, so the beam search is essen-
tially unguided. We find that only for a small frac-
tion of training examples do the K-best sets contain
any trees yielding the correct answer (29% for DCS
on GEO). However, training on just these exam-
ples is enough to improve the parameters, and this
29% increases to 66% and then to 95% over the next
few iterations. This bootstrapping behavior occurs
naturally: The “easy” examples are processed first,
where easy is defined by the ability of the current
</bodyText>
<page confidence="0.920819">
597
</page>
<bodyText confidence="0.998255212765957">
model to generate the correct answer using any tree. with scope variation. Think of DCS as a higher-level
Our system learns lexical associations between programming language tailored to natural language,
words and predicates. For example, area (by virtue which results in programs (DCS trees) which are
of being a noun) triggers many predicates: city, much simpler than the logically-equivalent lambda
state, area, etc. Inspecting the final parameters calculus formulae.
(DCS on GEO), we find that the feature [area, area] The idea of using CSPs to represent semantics is
has a much higher weight than [area, city]. Trace inspired by Discourse Representation Theory (DRT)
predicates can be inserted anywhere, but the fea- (Kamp and Reyle, 1993; Kamp et al., 2005), where
tures favor some insertions depending on the words variables are discourse referents. The restriction to
present (for example, [in, loc] has high weight). trees is similar to economical DRT (Bos, 2009).
The errors that the system makes stem from mul- The other major focus of this work is program
tiple sources, including errors in the POS tags (e.g., induction—inferring logical forms from their deno-
states is sometimes tagged as a verb, which triggers tations. There has been a fair amount of past work on
no predicates), confusion of Washington state with this topic: Liang et al. (2010) induces combinatory
Washington D.C., learning the wrong lexical asso- logic programs in a non-linguistic setting. Eisen-
ciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and
ciently large K. uses them as features in another learning problem.
5 Discussion Piantadosi et al. (2008) induces first-order formu-
A major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed
resentation, DCS, which offers a new perspective lexical semantics. The closest work to ours is Clarke
on compositional semantics. To contrast, consider et al. (2010), which we discussed earlier.
CCG (Steedman, 2000), in which semantic pars- The integration of natural language with denota-
ing is driven from the lexicon. The lexicon en- tions computed against a world (grounding) is be-
codes information about how each word can used in coming increasingly popular. Feedback from the
context; for example, the lexical entry for borders world has been used to guide both syntactic parsing
is S\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et
borders looks right for the first argument and left al., 2003; Clarke et al., 2010). Past work has also fo-
for the second. These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009),
and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al.,
order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others. Our
employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards
are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded
al., 2010). compositional semantics.
In DCS, we start with lexical triggers, which are 6 Conclusion
more basic than CCG lexical entries. A trigger for We built a system that interprets natural language
borders specifies only that border can be used, but utterances much more accurately than existing sys-
not how. The combination rules are encoded in the tems, despite using no annotated logical forms. Our
features as soft preferences. This yields a more system is based on a new semantic representation,
factorized and flexible representation that is easier DCS, which offers a simple and expressive alter-
to search through and parametrize using features. native to lambda calculus. Free from the burden
It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our
without becoming mired in the semantic formalism. techniques in developing even more accurate and
Quantifiers and superlatives significantly compli- broader-coverage language understanding systems.
cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer
ing needs to be employed. In DCS, the mark-execute and Tom Kwiatkowski for providing us with data
construct provides a flexible framework for dealing and answering questions.
598
</bodyText>
<sectionHeader confidence="0.998426" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849504761905">
J. Bos. 2009. A controlled fragment of DRT. In Work-
shop on Controlled Natural Language, pages 1–5.
S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay.
2009. Reinforcement learning for mapping instruc-
tions to actions. In Association for Computational Lin-
guistics and International Joint Conference on Natural
Language Processing (ACL-IJCNLP), Singapore. As-
sociation for Computational Linguistics.
S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010.
Reading between the lines: Learning to map high-level
instructions to commands. In Association for Compu-
tational Linguistics (ACL). Association for Computa-
tional Linguistics.
B. Carpenter. 1998. Type-Logical Semantics. MIT Press.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world’s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL).
R. Dechter. 2003. Constraint Processing. Morgan Kauf-
mann.
J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.
2009. Reading to learn: Constructing features from
semantic abstracts. In Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
R. Ge and R. J. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Compu-
tational Natural Language Learning (CoNLL), pages
9–16, Ann Arbor, Michigan.
H. Kamp and U. Reyle. 1993. From Discourse to Logic:
An Introduction to the Model-theoretic Semantics of
Natural Language, Formal Logic and Discourse Rep-
resentation Theory. Kluwer, Dordrecht.
H. Kamp, J. v. Genabith, and U. Reyle. 2005. Discourse
representation theory. In Handbook of Philosophical
Logic.
R. J. Kate and R. J. Mooney. 2007. Learning lan-
guage semantics from ambiguous supervision. In As-
sociation for the Advancement ofArtificial Intelligence
(AAAI), pages 895–900, Cambridge, MA. MIT Press.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005.
Learning to transform natural to formal languages. In
Association for the Advancement of Artificial Intel-
ligence (AAAI), pages 1062–1068, Cambridge, MA.
MIT Press.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order unifi-
cation. In Empirical Methods in Natural Language
Processing (EMNLP).
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning se-
mantic correspondences with less supervision. In As-
sociation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Process-
ing (ACL-IJCNLP), Singapore. Association for Com-
putational Linguistics.
P. Liang, M. I. Jordan, and D. Klein. 2010. Learning
programs: A hierarchical Bayesian approach. In In-
ternational Conference on Machine Learning (ICML).
Omnipress.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In International Conference on Computa-
tional Linguistics and Association for Computational
Linguistics (COLING/ACL), pages 433–440. Associa-
tion for Computational Linguistics.
S. T. Piantadosi, N. D. Goodman, B. A. Ellis, and J. B.
Tenenbaum. 2008. A Bayesian model of the acquisi-
tion of compositional semantics. In Proceedings of the
Thirtieth Annual Conference of the Cognitive Science
Society.
H. Poon and P. Domingos. 2009. Unsupervised semantic
parsing. In Empirical Methods in Natural Language
Processing (EMNLP), Singapore.
A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards
a theory of natural language interfaces to databases.
In International Conference on Intelligent User Inter-
faces (IUI).
W. Schuler. 2003. Using model-theoretic semantic inter-
pretation to guide statistical parsing and word recog-
nition in a spoken language interface. In Association
for Computational Linguistics (ACL). Association for
Computational Linguistics.
M. Steedman. 2000. The Syntactic Process. MIT Press.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In European Conference on Ma-
chine Learning, pages 466–477.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960–967, Prague, Czech Republic.
Association for Computational Linguistics.
M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming. In
Association for the Advancement of Artificial Intelli-
gence (AAAI), Cambridge, MA. MIT Press.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Uncer-
tainty in Artificial Intelligence (UAI), pages 658–666.
L. S. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP/CoNLL), pages 678–687.
</reference>
<page confidence="0.99873">
599
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.880994">
<title confidence="0.99994">Learning Dependency-Based Compositional Semantics</title>
<author confidence="0.996134">Percy</author>
<affiliation confidence="0.990441">UC</affiliation>
<email confidence="0.998083">pliang@cs.berkeley.edu</email>
<author confidence="0.997562">I Michael</author>
<affiliation confidence="0.987733">UC</affiliation>
<email confidence="0.998563">jordan@cs.berkeley.edu</email>
<author confidence="0.95628">Dan</author>
<affiliation confidence="0.996406">UC</affiliation>
<email confidence="0.999187">klein@cs.berkeley.edu</email>
<abstract confidence="0.997195823529412">Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bos</author>
</authors>
<title>A controlled fragment of DRT.</title>
<date>2009</date>
<booktitle>In Workshop on Controlled Natural Language,</booktitle>
<pages>1--5</pages>
<contexts>
<context position="31869" citStr="Bos, 2009" startWordPosition="5756" endWordPosition="5757">er than the logically-equivalent lambda state, area, etc. Inspecting the final parameters calculus formulae. (DCS on GEO), we find that the feature [area, area] The idea of using CSPs to represent semantics is has a much higher weight than [area, city]. Trace inspired by Discourse Representation Theory (DRT) predicates can be inserted anywhere, but the fea- (Kamp and Reyle, 1993; Kamp et al., 2005), where tures favor some insertions depending on the words variables are discourse referents. The restriction to present (for example, [in, loc] has high weight). trees is similar to economical DRT (Bos, 2009). The errors that the system makes stem from mul- The other major focus of this work is program tiple sources, including errors in the POS tags (e.g., induction—inferring logical forms from their denostates is sometimes tagged as a verb, which triggers tations. There has been a fair amount of past work on no predicates), confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C., learning the wrong lexical asso- logic programs in a non-linguistic setting. Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjuncti</context>
</contexts>
<marker>Bos, 2009</marker>
<rawString>J. Bos. 2009. A controlled fragment of DRT. In Workshop on Controlled Natural Language, pages 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Branavan</author>
<author>H Chen</author>
<author>L S Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), Singapore. Association for Computational Linguistics.</booktitle>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Branavan</author>
<author>L Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reading between the lines: Learning to map high-level instructions to commands.</title>
<date>2010</date>
<booktitle>In Association for Computational Linguistics (ACL). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="33800" citStr="Branavan et al., 2010" startWordPosition="6066" endWordPosition="6069">r. Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010). Past work has also fofor the second. These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others. Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al., 2010). compositional semantics. In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries. A trigger for We built a system that interprets natural language borders specifies only that border can be used, but utterances much more accurately than existing sysnot how. The combination rules are encoded in the tems, despite using no a</context>
</contexts>
<marker>Branavan, Zettlemoyer, Barzilay, 2010</marker>
<rawString>S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010. Reading between the lines: Learning to map high-level instructions to commands. In Association for Computational Linguistics (ACL). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carpenter</author>
</authors>
<title>Type-Logical Semantics.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10995" citStr="Carpenter, 1998" startWordPosition="1917" endWordPosition="1918"> of DCS which handles this type of divergence between syntactic and semantic scope. The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E, Q, or C). Then higher up in the tree, we invoke it with an execute relation Xi to create the desired semantic scope.2 This mark-execute construct acts non-locally, so to maintain compositionality, we must augment the 2Our mark-execute construct is analogous to Montague’s quantifying in, Cooper storage, and Carpenter’s scoping constructor (Carpenter, 1998). number of average population of major cities major cities 1 1 major (a) Counting (b) Averaging 1 1 Σ 2 count 1 1 Σ city 1 1 2 average 1 1 population 1 1 city major city 1 1 1 1 major loc 1 CA 2 592 California borders which states? Alaska borders no states. X 1 X 1 1 CA border 1 2 1 state 1 AK border 1 2 1 state E Q no X12 X21 Some river traverses every city. 1 river Q some traverse 1 2 1 every city Q river some Q traverse 1 2 1 1 every city Q (narrow) (wide) city traversed by no rivers X12 city E 1 2 traverse 1 1 river no Q (a) Extraction (E) (b) Quantification (Q) (c) Quantifier ambiguity (</context>
</contexts>
<marker>Carpenter, 1998</marker>
<rawString>B. Carpenter. 1998. Type-Logical Semantics. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>D Goldwasser</author>
<author>M Chang</author>
<author>D Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<journal>In Computational Natural Language Learning (CoNLL).</journal>
<contexts>
<context position="1608" citStr="Clarke et al. (2010)" startWordPosition="232" endWordPosition="235">e types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: (parameters) (world) θ w Semantic Parsing Evaluation x z y (question) (logical form) (answer) Figure 1: Our probabilistic model: a question x is mapped to a latent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced au</context>
<context position="3509" citStr="Clarke et al. (2010)" startWordPosition="550" endWordPosition="553">Steedman, 2000), which is used by many semantic parsers, e.g., Zettlemoyer and Collins (2005). However, the logical forms there can become quite complex, and in the context of program induction, this would lead to an unwieldy search space. At the same time, representations such as FunQL (Kate et al., 2005), which was used in state with the largest area X, state 1 1 area argmax Alaska z ∼ pθ(z |x) y = Hw 590 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 590–599, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Clarke et al. (2010), are simpler but lack the full expressive power of lambda calculus. The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2). The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees, which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient. We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4). Our s</context>
<context position="26577" citStr="Clarke et al. (2010)" startWordPosition="4875" endWordPosition="4878">enerated a random Jobs database instead as follows: we created 100 job IDs. For each data predicate p (e.g., language), we add each possible tuple (e.g., (job37, Java)) to w(p) independently with probability 0.8. We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS). During development, we further held out a random 30% of the training sets for validation. Our lexical triggers L include the following: (i) predicates for a small set of ≈ 20 function words (e.g., (most, argmax)), (ii) (x, x) for each value replaced with 596 System Accuracy Clarke et al. (2010) w/answers 73.2 Clarke et al. (2010) w/logical forms 80.4 Our system (DCS with L) 78.9 Our system (DCS with L+) 87.2 Table 2: Results on GEO with 250 training and 250 test examples. Our results are averaged over 10 random 250+250 splits taken from our 600 training examples. Of the three systems that do not use logical forms, our two systems yield significant improvements. Our better system even outperforms the system that uses logical forms. predicate x in w (e.g., (Boston, Boston)), and (iii) predicates for each POS tag in {JJ, NN, NNS} (e.g., (JJ, size), (JJ, area), etc.).3 Predicates corres</context>
<context position="33480" citStr="Clarke et al., 2010" startWordPosition="6014" endWordPosition="6017"> consider et al. (2010), which we discussed earlier. CCG (Steedman, 2000), in which semantic pars- The integration of natural language with denotaing is driven from the lexicon. The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular. Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010). Past work has also fofor the second. These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others. Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al., 2010). compositional semantics. In DCS, we start wit</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010. Driving semantic parsing from the world’s response. In Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dechter</author>
</authors>
<title>Constraint Processing.</title>
<date>2003</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="6893" citStr="Dechter, 2003" startWordPosition="1173" endWordPosition="1174">ints: (i) x ∈ w(p) for each node x labeled with predicate p ∈ P; and (ii) xj = yj0 (the j-th component of x must equal the j&apos;-th component of y) for each edge (x, y) labeled with j0j ∈ R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints. We say a value v is consistent for a node x if there exists a solution that assigns v to x. The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example). Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter, 2003). The recurrence is as follows: J(p;j01:c1; ··· ; j :cm)K (1) {v : vjz= tj0 z, t ∈ JciKw}. At each node, we compute the set of tuples v consistent with the predicate at that node (v ∈ w(p)), and S(x)}, where a set of pairs S is treated as a set-valued function S(x) = {y : (x, y) ∈ S} with domain S1 = {x : (x, y) ∈ S}. The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations. Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees, where each z ∈ Z consists of (i) a predicate xES1 m n i=1 = w(p) ∩ 591 Example: major ci</context>
</contexts>
<marker>Dechter, 2003</marker>
<rawString>R. Dechter. 2003. Constraint Processing. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>J Clarke</author>
<author>D Goldwasser</author>
<author>D Roth</author>
</authors>
<title>Reading to learn: Constructing features from semantic abstracts.</title>
<date>2009</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<marker>Eisenstein, Clarke, Goldwasser, Roth, 2009</marker>
<rawString>J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth. 2009. Reading to learn: Constructing features from semantic abstracts. In Empirical Methods in Natural Language Processing (EMNLP), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ge</author>
<author>R J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Computational Natural Language Learning (CoNLL),</booktitle>
<pages>9--16</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1204" citStr="Ge and Mooney, 2005" startWordPosition="169" endWordPosition="172">oblem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms. 1 Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to captu</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>R. Ge and R. J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Computational Natural Language Learning (CoNLL), pages 9–16, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kamp</author>
<author>U Reyle</author>
</authors>
<title>From Discourse to Logic: An Introduction to the Model-theoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory.</title>
<date>1993</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="31640" citStr="Kamp and Reyle, 1993" startWordPosition="5718" endWordPosition="5721"> lexical associations between programming language tailored to natural language, words and predicates. For example, area (by virtue which results in programs (DCS trees) which are of being a noun) triggers many predicates: city, much simpler than the logically-equivalent lambda state, area, etc. Inspecting the final parameters calculus formulae. (DCS on GEO), we find that the feature [area, area] The idea of using CSPs to represent semantics is has a much higher weight than [area, city]. Trace inspired by Discourse Representation Theory (DRT) predicates can be inserted anywhere, but the fea- (Kamp and Reyle, 1993; Kamp et al., 2005), where tures favor some insertions depending on the words variables are discourse referents. The restriction to present (for example, [in, loc] has high weight). trees is similar to economical DRT (Bos, 2009). The errors that the system makes stem from mul- The other major focus of this work is program tiple sources, including errors in the POS tags (e.g., induction—inferring logical forms from their denostates is sometimes tagged as a verb, which triggers tations. There has been a fair amount of past work on no predicates), confusion of Washington state with this topic: L</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>H. Kamp and U. Reyle. 1993. From Discourse to Logic: An Introduction to the Model-theoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kamp</author>
<author>J v Genabith</author>
<author>U Reyle</author>
</authors>
<title>Discourse representation theory.</title>
<date>2005</date>
<booktitle>In Handbook of Philosophical Logic.</booktitle>
<contexts>
<context position="31660" citStr="Kamp et al., 2005" startWordPosition="5722" endWordPosition="5725">between programming language tailored to natural language, words and predicates. For example, area (by virtue which results in programs (DCS trees) which are of being a noun) triggers many predicates: city, much simpler than the logically-equivalent lambda state, area, etc. Inspecting the final parameters calculus formulae. (DCS on GEO), we find that the feature [area, area] The idea of using CSPs to represent semantics is has a much higher weight than [area, city]. Trace inspired by Discourse Representation Theory (DRT) predicates can be inserted anywhere, but the fea- (Kamp and Reyle, 1993; Kamp et al., 2005), where tures favor some insertions depending on the words variables are discourse referents. The restriction to present (for example, [in, loc] has high weight). trees is similar to economical DRT (Bos, 2009). The errors that the system makes stem from mul- The other major focus of this work is program tiple sources, including errors in the POS tags (e.g., induction—inferring logical forms from their denostates is sometimes tagged as a verb, which triggers tations. There has been a fair amount of past work on no predicates), confusion of Washington state with this topic: Liang et al. (2010) i</context>
</contexts>
<marker>Kamp, Genabith, Reyle, 2005</marker>
<rawString>H. Kamp, J. v. Genabith, and U. Reyle. 2005. Discourse representation theory. In Handbook of Philosophical Logic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>R J Mooney</author>
</authors>
<title>Learning language semantics from ambiguous supervision.</title>
<date>2007</date>
<booktitle>In Association for the Advancement ofArtificial Intelligence (AAAI),</booktitle>
<pages>895--900</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1258" citStr="Kate and Mooney, 2007" startWordPosition="177" endWordPosition="180">ich highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms. 1 Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our pr</context>
</contexts>
<marker>Kate, Mooney, 2007</marker>
<rawString>R. J. Kate and R. J. Mooney. 2007. Learning language semantics from ambiguous supervision. In Association for the Advancement ofArtificial Intelligence (AAAI), pages 895–900, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning to transform natural to formal languages.</title>
<date>2005</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI),</booktitle>
<pages>1062--1068</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3196" citStr="Kate et al., 2005" startWordPosition="498" endWordPosition="501">standard semantic parsing, our end goal is only to generate the correct y, so we are free to choose the representation for z. Which one should we use? The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner. CCG is one instantiation (Steedman, 2000), which is used by many semantic parsers, e.g., Zettlemoyer and Collins (2005). However, the logical forms there can become quite complex, and in the context of program induction, this would lead to an unwieldy search space. At the same time, representations such as FunQL (Kate et al., 2005), which was used in state with the largest area X, state 1 1 area argmax Alaska z ∼ pθ(z |x) y = Hw 590 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 590–599, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Clarke et al. (2010), are simpler but lack the full expressive power of lambda calculus. The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2). The logical forms in this framework are tr</context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to transform natural to formal languages. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1062–1068, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Inducing probabilistic CCG grammars from logical form with higher-order unification.</title>
<date>2010</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1339" citStr="Kwiatkowski et al., 2010" startWordPosition="189" endWordPosition="192">of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms. 1 Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: (parameters) (world) θ w Semantic Parsing Evaluation x z y (qu</context>
<context position="28870" citStr="Kwiatkowski et al. (2010)" startWordPosition="5250" endWordPosition="5253">, and very simple indicator features. In fact, DCS performs comparably to even the version of SEMRESP trained using logical forms. If we add prototype triggers (use L+), the resulting system (DCS+) outperforms both versions of SEMRESP by a significant margin (87.2% over 73.2% and 80.4%). 3We used the Berkeley Parser (Petrov et al., 2006) to perform POS tagging. The triggers L(x) for a word x thus include L(t) where t is the POS tag of x. System GEO JOBS Tang and Mooney (2001) 79.4 79.8 Wong and Mooney (2007) 86.6 – Zettlemoyer and Collins (2005) 79.3 79.3 Zettlemoyer and Collins (2007) 81.6 – Kwiatkowski et al. (2010) 88.2 – Kwiatkowski et al. (2010) 88.9 – Our system (DCS with L) 88.6 91.4 Our system (DCS with L+) 91.1 95.0 Table 3: Accuracy (recall) of systems on the two benchmarks. The systems are divided into three groups. Group 1 uses 10-fold cross-validation; groups 2 and 3 use the independent test set. Groups 1 and 2 measure accuracy of logical form; group 3 measures accuracy of the answer; but there is very small difference between the two as seen from the Kwiatkowski et al. (2010) numbers. Our best system improves substantially over past work, despite using no logical forms as training data. Next,</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and M. Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higher-order unification. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), Singapore. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="33610" citStr="Liang et al., 2009" startWordPosition="6039" endWordPosition="6042">age with denotaing is driven from the lexicon. The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular. Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010). Past work has also fofor the second. These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others. Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al., 2010). compositional semantics. In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries. A trigger for We built a system that interprets na</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning programs: A hierarchical Bayesian approach.</title>
<date>2010</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<publisher>Omnipress.</publisher>
<contexts>
<context position="32258" citStr="Liang et al. (2010)" startWordPosition="5820" endWordPosition="5823">3; Kamp et al., 2005), where tures favor some insertions depending on the words variables are discourse referents. The restriction to present (for example, [in, loc] has high weight). trees is similar to economical DRT (Bos, 2009). The errors that the system makes stem from mul- The other major focus of this work is program tiple sources, including errors in the POS tags (e.g., induction—inferring logical forms from their denostates is sometimes tagged as a verb, which triggers tations. There has been a fair amount of past work on no predicates), confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C., learning the wrong lexical asso- logic programs in a non-linguistic setting. Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem. 5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation, DCS, which offers a new perspective lexical semantics. The closest work to ours is Clarke on compositional semantics. To contras</context>
</contexts>
<marker>Liang, Jordan, Klein, 2010</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2010. Learning programs: A hierarchical Bayesian approach. In International Conference on Machine Learning (ICML). Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL),</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28584" citStr="Petrov et al., 2006" startWordPosition="5196" endWordPosition="5199">, DCS uses even less supervision than SEMRESP. SEMRESP requires a lexicon of 1.42 words per non-value predicate, WordNet features, and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall, around 0.5 words per non-value predicate), POS tags, and very simple indicator features. In fact, DCS performs comparably to even the version of SEMRESP trained using logical forms. If we add prototype triggers (use L+), the resulting system (DCS+) outperforms both versions of SEMRESP by a significant margin (87.2% over 73.2% and 80.4%). 3We used the Berkeley Parser (Petrov et al., 2006) to perform POS tagging. The triggers L(x) for a word x thus include L(t) where t is the POS tag of x. System GEO JOBS Tang and Mooney (2001) 79.4 79.8 Wong and Mooney (2007) 86.6 – Zettlemoyer and Collins (2005) 79.3 79.3 Zettlemoyer and Collins (2007) 81.6 – Kwiatkowski et al. (2010) 88.2 – Kwiatkowski et al. (2010) 88.9 – Our system (DCS with L) 88.6 91.4 Our system (DCS with L+) 91.1 95.0 Table 3: Accuracy (recall) of systems on the two benchmarks. The systems are divided into three groups. Group 1 uses 10-fold cross-validation; groups 2 and 3 use the independent test set. Groups 1 and 2 m</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL), pages 433–440. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S T Piantadosi</author>
<author>N D Goodman</author>
<author>B A Ellis</author>
<author>J B Tenenbaum</author>
</authors>
<title>A Bayesian model of the acquisition of compositional semantics.</title>
<date>2008</date>
<booktitle>In Proceedings of the Thirtieth Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="32590" citStr="Piantadosi et al. (2008)" startWordPosition="5870" endWordPosition="5873">iple sources, including errors in the POS tags (e.g., induction—inferring logical forms from their denostates is sometimes tagged as a verb, which triggers tations. There has been a fair amount of past work on no predicates), confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C., learning the wrong lexical asso- logic programs in a non-linguistic setting. Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem. 5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation, DCS, which offers a new perspective lexical semantics. The closest work to ours is Clarke on compositional semantics. To contrast, consider et al. (2010), which we discussed earlier. CCG (Steedman, 2000), in which semantic pars- The integration of natural language with denotaing is driven from the lexicon. The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular. Feedback </context>
</contexts>
<marker>Piantadosi, Goodman, Ellis, Tenenbaum, 2008</marker>
<rawString>S. T. Piantadosi, N. D. Goodman, B. A. Ellis, and J. B. Tenenbaum. 2008. A Bayesian model of the acquisition of compositional semantics. In Proceedings of the Thirtieth Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
<author>P Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="1487" citStr="Poon and Domingos, 2009" startWordPosition="213" endWordPosition="216">no annotated logical forms. 1 Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: (parameters) (world) θ w Semantic Parsing Evaluation x z y (question) (logical form) (answer) Figure 1: Our probabilistic model: a question x is mapped to a latent logical form z, which is then evaluated with r</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>H. Poon and P. Domingos. 2009. Unsupervised semantic parsing. In Empirical Methods in Natural Language Processing (EMNLP), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Popescu</author>
<author>O Etzioni</author>
<author>H Kautz</author>
</authors>
<title>Towards a theory of natural language interfaces to databases.</title>
<date>2003</date>
<booktitle>In International Conference on Intelligent User Interfaces (IUI).</booktitle>
<marker>Popescu, Etzioni, Kautz, 2003</marker>
<rawString>A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards a theory of natural language interfaces to databases. In International Conference on Intelligent User Interfaces (IUI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Schuler</author>
</authors>
<title>Using model-theoretic semantic interpretation to guide statistical parsing and word recognition in a spoken language interface.</title>
<date>2003</date>
<booktitle>In Association for Computational Linguistics (ACL). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="33363" citStr="Schuler, 2003" startWordPosition="5996" endWordPosition="5997"> new perspective lexical semantics. The closest work to ours is Clarke on compositional semantics. To contrast, consider et al. (2010), which we discussed earlier. CCG (Steedman, 2000), in which semantic pars- The integration of natural language with denotaing is driven from the lexicon. The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular. Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010). Past work has also fofor the second. These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others. Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiat</context>
</contexts>
<marker>Schuler, 2003</marker>
<rawString>W. Schuler. 2003. Using model-theoretic semantic interpretation to guide statistical parsing and word recognition in a spoken language interface. In Association for Computational Linguistics (ACL). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2904" citStr="Steedman, 2000" startWordPosition="452" endWordPosition="453">meters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs. The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y. Unlike standard semantic parsing, our end goal is only to generate the correct y, so we are free to choose the representation for z. Which one should we use? The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner. CCG is one instantiation (Steedman, 2000), which is used by many semantic parsers, e.g., Zettlemoyer and Collins (2005). However, the logical forms there can become quite complex, and in the context of program induction, this would lead to an unwieldy search space. At the same time, representations such as FunQL (Kate et al., 2005), which was used in state with the largest area X, state 1 1 area argmax Alaska z ∼ pθ(z |x) y = Hw 590 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 590–599, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Clarke et al. (</context>
<context position="32933" citStr="Steedman, 2000" startWordPosition="5927" endWordPosition="5928">ical asso- logic programs in a non-linguistic setting. Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem. 5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation, DCS, which offers a new perspective lexical semantics. The closest work to ours is Clarke on compositional semantics. To contrast, consider et al. (2010), which we discussed earlier. CCG (Steedman, 2000), in which semantic pars- The integration of natural language with denotaing is driven from the lexicon. The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular. Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010). Past work has also fofor the second. These rules ar</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>M. Steedman. 2000. The Syntactic Process. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Tang</author>
<author>R J Mooney</author>
</authors>
<title>Using multiple clause constructors in inductive logic programming for semantic parsing.</title>
<date>2001</date>
<booktitle>In European Conference on Machine Learning,</booktitle>
<pages>466--477</pages>
<contexts>
<context position="1183" citStr="Tang and Mooney, 2001" startWordPosition="165" endWordPosition="168">challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms. 1 Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a late</context>
<context position="28725" citStr="Tang and Mooney (2001)" startWordPosition="5226" endWordPosition="5229">actic parse trees; DCS requires only words for the domain-independent predicates (overall, around 0.5 words per non-value predicate), POS tags, and very simple indicator features. In fact, DCS performs comparably to even the version of SEMRESP trained using logical forms. If we add prototype triggers (use L+), the resulting system (DCS+) outperforms both versions of SEMRESP by a significant margin (87.2% over 73.2% and 80.4%). 3We used the Berkeley Parser (Petrov et al., 2006) to perform POS tagging. The triggers L(x) for a word x thus include L(t) where t is the POS tag of x. System GEO JOBS Tang and Mooney (2001) 79.4 79.8 Wong and Mooney (2007) 86.6 – Zettlemoyer and Collins (2005) 79.3 79.3 Zettlemoyer and Collins (2007) 81.6 – Kwiatkowski et al. (2010) 88.2 – Kwiatkowski et al. (2010) 88.9 – Our system (DCS with L) 88.6 91.4 Our system (DCS with L+) 91.1 95.0 Table 3: Accuracy (recall) of systems on the two benchmarks. The systems are divided into three groups. Group 1 uses 10-fold cross-validation; groups 2 and 3 use the independent test set. Groups 1 and 2 measure accuracy of logical form; group 3 measures accuracy of the answer; but there is very small difference between the two as seen from the</context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>L. R. Tang and R. J. Mooney. 2001. Using multiple clause constructors in inductive logic programming for semantic parsing. In European Conference on Machine Learning, pages 466–477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>960--967</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1312" citStr="Wong and Mooney, 2007" startWordPosition="185" endWordPosition="188">d efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms. 1 Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: (parameters) (world) θ w Semantic P</context>
<context position="28758" citStr="Wong and Mooney (2007)" startWordPosition="5232" endWordPosition="5235">nly words for the domain-independent predicates (overall, around 0.5 words per non-value predicate), POS tags, and very simple indicator features. In fact, DCS performs comparably to even the version of SEMRESP trained using logical forms. If we add prototype triggers (use L+), the resulting system (DCS+) outperforms both versions of SEMRESP by a significant margin (87.2% over 73.2% and 80.4%). 3We used the Berkeley Parser (Petrov et al., 2006) to perform POS tagging. The triggers L(x) for a word x thus include L(t) where t is the POS tag of x. System GEO JOBS Tang and Mooney (2001) 79.4 79.8 Wong and Mooney (2007) 86.6 – Zettlemoyer and Collins (2005) 79.3 79.3 Zettlemoyer and Collins (2007) 81.6 – Kwiatkowski et al. (2010) 88.2 – Kwiatkowski et al. (2010) 88.9 – Our system (DCS with L) 88.6 91.4 Our system (DCS with L+) 91.1 95.0 Table 3: Accuracy (recall) of systems on the two benchmarks. The systems are divided into three groups. Group 1 uses 10-fold cross-validation; groups 2 and 3 use the independent test set. Groups 1 and 2 measure accuracy of logical form; group 3 measures accuracy of the answer; but there is very small difference between the two as seen from the Kwiatkowski et al. (2010) number</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Y. W. Wong and R. J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Association for Computational Linguistics (ACL), pages 960–967, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic proramming.</title>
<date>1996</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI),</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1160" citStr="Zelle and Mooney, 1996" startWordPosition="161" endWordPosition="164">pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms. 1 Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logi</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>M. Zelle and R. J. Mooney. 1996. Learning to parse database queries using inductive logic proramming. In Association for the Advancement of Artificial Intelligence (AAAI), Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Uncertainty in Artificial Intelligence (UAI),</booktitle>
<pages>658--666</pages>
<contexts>
<context position="1235" citStr="Zettlemoyer and Collins, 2005" startWordPosition="173" endWordPosition="176"> new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms. 1 Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language</context>
<context position="2982" citStr="Zettlemoyer and Collins (2005)" startWordPosition="462" endWordPosition="465">h cheaper to obtain than (x, z) pairs. The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y. Unlike standard semantic parsing, our end goal is only to generate the correct y, so we are free to choose the representation for z. Which one should we use? The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner. CCG is one instantiation (Steedman, 2000), which is used by many semantic parsers, e.g., Zettlemoyer and Collins (2005). However, the logical forms there can become quite complex, and in the context of program induction, this would lead to an unwieldy search space. At the same time, representations such as FunQL (Kate et al., 2005), which was used in state with the largest area X, state 1 1 area argmax Alaska z ∼ pθ(z |x) y = Hw 590 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 590–599, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Clarke et al. (2010), are simpler but lack the full expressive power of lambda calculus. The </context>
<context position="26241" citStr="Zettlemoyer and Collins (2005)" startWordPosition="4815" endWordPosition="4819">ee types of predicates in P: generic (e.g., argmax), data (e.g., city), and value (e.g., CA). GEO has 48 non-value predicates and JOBS has 26. For GEO, w is the standard US geography database that comes with the dataset. For JOBS, if we use the standard Jobs database, close to half the y’s are empty, which makes it uninteresting. We therefore generated a random Jobs database instead as follows: we created 100 job IDs. For each data predicate p (e.g., language), we add each possible tuple (e.g., (job37, Java)) to w(p) independently with probability 0.8. We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS). During development, we further held out a random 30% of the training sets for validation. Our lexical triggers L include the following: (i) predicates for a small set of ≈ 20 function words (e.g., (most, argmax)), (ii) (x, x) for each value replaced with 596 System Accuracy Clarke et al. (2010) w/answers 73.2 Clarke et al. (2010) w/logical forms 80.4 Our system (DCS with L) 78.9 Our system (DCS with L+) 87.2 Table 2: Results on GEO with 250 training and 250 test examples. Our results are averaged over 10 random 250+250 splits taken from our 600 training</context>
<context position="28796" citStr="Zettlemoyer and Collins (2005)" startWordPosition="5238" endWordPosition="5241">endent predicates (overall, around 0.5 words per non-value predicate), POS tags, and very simple indicator features. In fact, DCS performs comparably to even the version of SEMRESP trained using logical forms. If we add prototype triggers (use L+), the resulting system (DCS+) outperforms both versions of SEMRESP by a significant margin (87.2% over 73.2% and 80.4%). 3We used the Berkeley Parser (Petrov et al., 2006) to perform POS tagging. The triggers L(x) for a word x thus include L(t) where t is the POS tag of x. System GEO JOBS Tang and Mooney (2001) 79.4 79.8 Wong and Mooney (2007) 86.6 – Zettlemoyer and Collins (2005) 79.3 79.3 Zettlemoyer and Collins (2007) 81.6 – Kwiatkowski et al. (2010) 88.2 – Kwiatkowski et al. (2010) 88.9 – Our system (DCS with L) 88.6 91.4 Our system (DCS with L+) 91.1 95.0 Table 3: Accuracy (recall) of systems on the two benchmarks. The systems are divided into three groups. Group 1 uses 10-fold cross-validation; groups 2 and 3 use the independent test set. Groups 1 and 2 measure accuracy of logical form; group 3 measures accuracy of the answer; but there is very small difference between the two as seen from the Kwiatkowski et al. (2010) numbers. Our best system improves substantia</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Uncertainty in Artificial Intelligence (UAI), pages 658–666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL),</booktitle>
<pages>678--687</pages>
<contexts>
<context position="1289" citStr="Zettlemoyer and Collins, 2007" startWordPosition="181" endWordPosition="184">el between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms. 1 Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: (parameters)</context>
<context position="28837" citStr="Zettlemoyer and Collins (2007)" startWordPosition="5244" endWordPosition="5247">rds per non-value predicate), POS tags, and very simple indicator features. In fact, DCS performs comparably to even the version of SEMRESP trained using logical forms. If we add prototype triggers (use L+), the resulting system (DCS+) outperforms both versions of SEMRESP by a significant margin (87.2% over 73.2% and 80.4%). 3We used the Berkeley Parser (Petrov et al., 2006) to perform POS tagging. The triggers L(x) for a word x thus include L(t) where t is the POS tag of x. System GEO JOBS Tang and Mooney (2001) 79.4 79.8 Wong and Mooney (2007) 86.6 – Zettlemoyer and Collins (2005) 79.3 79.3 Zettlemoyer and Collins (2007) 81.6 – Kwiatkowski et al. (2010) 88.2 – Kwiatkowski et al. (2010) 88.9 – Our system (DCS with L) 88.6 91.4 Our system (DCS with L+) 91.1 95.0 Table 3: Accuracy (recall) of systems on the two benchmarks. The systems are divided into three groups. Group 1 uses 10-fold cross-validation; groups 2 and 3 use the independent test set. Groups 1 and 2 measure accuracy of logical form; group 3 measures accuracy of the answer; but there is very small difference between the two as seen from the Kwiatkowski et al. (2010) numbers. Our best system improves substantially over past work, despite using no logi</context>
<context position="33863" citStr="Zettlemoyer and Collins, 2007" startWordPosition="6075" endWordPosition="6078">entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010). Past work has also fofor the second. These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others. Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al., 2010). compositional semantics. In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries. A trigger for We built a system that interprets natural language borders specifies only that border can be used, but utterances much more accurately than existing sysnot how. The combination rules are encoded in the tems, despite using no annotated logical forms. Our features as soft preferences. This </context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 678–687.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>