<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002140">
<note confidence="0.961844333333333">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 247-254.
The Descent of Hierarchy, and Selection in Relational Semantics
</note>
<title confidence="0.87998725">
Barbara Rosario
SIMS
UC Berkeley
Berkeley, CA 94720
</title>
<email confidence="0.926216">
rosario@sims.berkeley.edu
</email>
<title confidence="0.7558975">
Marti A. Hearst
SIMS
UC Berkeley
Berkeley, CA 94720
</title>
<email confidence="0.933191">
hearst@sims.berkeley.edu
</email>
<title confidence="0.61108775">
Charles Fillmore
ICSI
UC Berkeley
Berkeley, CA 94720
</title>
<email confidence="0.99081">
fillmore@icsi.berkeley.edu
</email>
<sectionHeader confidence="0.995332" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996947826087">
In many types of technical texts, meaning is
embedded in noun compounds. A language un-
derstanding program needs to be able to inter-
pret these in order to ascertain sentence mean-
ing. We explore the possibility of using an ex-
isting lexical hierarchy for the purpose of plac-
ing words from a noun compound into cate-
gories, and then using this category member-
ship to determine the relation that holds be-
tween the nouns. In this paper we present the
results of an analysis of this method on two-
word noun compounds from the biomedical do-
main, obtaining classification accuracy of ap-
proximately 90%. Since lexical hierarchies are
not necessarily ideally suited for this task, we
also pose the question: how far down the hi-
erarchy must the algorithm descend before all
the terms within the subhierarchy behave uni-
formly with respect to the semantic relation in
question? We find that the topmost levels of the
hierarchy yield an accurate classification, thus
providing an economic way of assigning rela-
tions to noun compounds.
</bodyText>
<sectionHeader confidence="0.999412" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.980939333333333">
A major difficulty for the interpretation of sentences from
technical texts is the complex structure of noun phrases
and noun compounds. Consider, for example, this title,
taken from a biomedical journal abstract:
Open-labeled long-term study of the subcutaneous
sumatriptan efficacy and tolerability in acute mi-
graine treatment.
An important step towards being able to interpret such
technical sentences is to analyze the meaning of noun
compounds, and noun phrases more generally.
With apologies to Charles Darwin.
Interpretation of noun compounds (NCs) is highly de-
pendent on lexical information. Thus we explore the use
of a large corpus (Medline) and a large lexical hierarchy
(MeSH, Medical Subject Headings) to determine the re-
lations that hold between the words in noun compounds.
Surprisingly, we find that we can simply use the juxta-
position of category membership within the lexical hier-
archy to determine the relation that holds between pairs
of nouns. For example, for the NCs leg paresis, skin
numbness, and hip pain, the first word of the NC falls into
the MeSH A01 (Body Regions) category, and the second
word falls into the C10 (Nervous System Diseases) cat-
egory. From these we can declare that the relation that
holds between the words is “located in”. Similarly, for
influenza patients and aids survivors, the first word falls
under C02 (Virus Diseases) and the second is found in
M01.643 (Patients), yielding the “afflicted by” relation.
Using this technique on a subpart of the category space,
we obtain 90% accuracy overall.
In some sense, this is a very old idea, dating back to
the early days of semantic nets and semantic grammars.
The critical difference now is that large lexical resources
and corpora have become available, thus allowing some
of those old techniques to become feasible in terms of
coverage. However, the success of such an approach de-
pends on the structure and coverage of the underlying lex-
ical ontology.
In the following sections we discuss the linguistic mo-
tivations behind this approach, the characteristics of the
lexical ontology MeSH, the use of a corpus to examine
the problem space, the method of determining the rela-
tions, the accuracy of the results, and the problem of am-
biguity. The paper concludes with related work and a
discussion of future work.
</bodyText>
<sectionHeader confidence="0.985038" genericHeader="method">
2 Linguistic Motivation
</sectionHeader>
<bodyText confidence="0.998321666666667">
One way to understand the relations between the words
in a two-word noun compound is to cast the words into
a head-modifier relationship, and assume that the head
noun has an argument structure, much the way verbs do,
as well as a qualia structure in the sense of Pustejovsky
(1995). Then the meaning of the head noun determines
what kinds of things can be done to it, what it is made of,
what it is a part of, and so on.
For example, consider the noun knife. Knives are cre-
ated for particular activities or settings, can be made of
various materials, and can be used for cutting or manip-
ulating various kinds of things. A set of relations for
knives, and example NCs exhibiting these relations is
shown below:
(Used-in): kitchen knife, hunting knife
(Made-of): steel knife, plastic knife
(Instrument-for): carving knife
(Used-on): meat knife, putty knife
(Used-by): chef’s knife, butcher’s knife
Some relationships apply to only certain classes of nouns;
the semantic structure of the head noun determines the
range of possibilities. Thus if we can capture regularities
about the behaviors of the constituent nouns, we should
also be able to predict which relations will hold between
them.
We propose using the categorization provided by a lex-
ical hierarchy for this purpose. Using a large collection
of noun compounds, we assign semantic descriptors from
the lexical hierarchy to the constituent nouns and deter-
mine the relations between them. This approach avoids
the need to enumerate in advance all of the relations that
may hold. Rather, the corpus determines which relations
occur.
</bodyText>
<sectionHeader confidence="0.979369" genericHeader="method">
3 The Lexical Hierarchy: MeSH
</sectionHeader>
<bodyText confidence="0.9795575">
MeSH (Medical Subject Headings)1 is the National Li-
brary of Medicine’s controlled vocabulary thesaurus; it
consists of set of terms arranged in a hierarchical struc-
ture. There are 15 main sub-hierarchies (trees) in MeSH,
each corresponding to a major branch of medical termi-
nology. For example, tree A corresponds to Anatomy,
tree B to Organisms, tree C to Diseases and so on. Every
branch has several sub-branches; Anatomy, for example,
consists ofBody Regions (A01), Musculoskeletal System
(A02), Digestive System (A03) etc. We refer to these as
“level 0” categories.
These nodes have children, for example, Abdomen
(A01.047) and Back (A01.176) are level 1 children
of Body Regions. The longer the ID of the MeSH
term, the longer the path from the root and the more
precise the description. For example migraine is
C10.228.140.546.800.525, that is, C (a disease), C10
(Nervous System Diseases), C10.228 (Central Nervous
</bodyText>
<footnote confidence="0.998552">
1http://www.nlm.nih.gov/mesh/meshhome.html; the work
reported in this paper uses MeSH 2001.
</footnote>
<bodyText confidence="0.9990232">
System Diseases) and so on. There are over 35,000
unique IDs in MeSH 2001. Many words are assigned
more than one MeSH ID and so occur in more than one
location within the hierarchy; thus the structure of MeSH
can be interpreted as a network.
Some of the categories are more homogeneous than
others. The tree A (Anatomy) for example, seems to be
quite homogeneous; at level 0, the nodes are all part of
(meronymic to) Anatomy: the Digestive (A03), Respi-
ratory (A04) and the Urogenital (A05) Systems are all
part of anatomy; at level 1, the Biliary Tract (A03.159)
and the Esophagus (A03.365) are part of the Digestive
System (level 0) and so on. Thus we assume that every
node is a (body) part of the parent node (and all the nodes
above it).
Tree C for Diseases is also homogeneous; the child
nodes are a kind of (hyponym of) the disease at the par-
ent node: Neoplasms (C04) is a kind of Disease C and
Hamartoma (C04.445) is a kind ofNeoplasms.
Other trees are more heterogeneous, in the sense that
the meanings among the nodes are more diverse. Infor-
mation Science (L01), for example, contains, among oth-
ers, Communications Media (L01.178), Computer Secu-
rity (L01.209) and Pattern Recognition (L01.725). An-
other heterogeneous sub-hierarchy is Natural Science
(H01). Among the children of H01 we find Chemistry
(parent of Biochemistry), Electronics (parent of Ampli-
fiers and Robotics), Mathematics (Fractals, Game The-
ory and Fourier Analysis). In other words, we find a wide
range of concepts that are not described by a simple rela-
tionship.
These observations suggest that once an algorithm de-
scends to a homogeneous level, words falling into the
subhierarchy at that level (and below it) behave similarly
with respect to relation assignment.
</bodyText>
<sectionHeader confidence="0.985376" genericHeader="method">
4 Counting Noun Compounds
</sectionHeader>
<bodyText confidence="0.9614641875">
In this and the next section, we describe how we investi-
gated the hypothesis:
For all two-word noun compounds (NCs) that
can be characterized by a category pair (CP), a
particular semantic relationship holds between
the nouns comprising those NCs.
The kinds of relations we found are similar to those
described in Section 2. Note that, in this analysis we fo-
cused on determining which sets ofNCs fall into the same
relation, without explicitly assigning names to the rela-
tions themselves. Furthermore, the same relation may be
described by many different category pairs (see Section
5.5).
First, we extracted two-word noun compounds from
approximately 1M titles and abstracts from the Med-
line collection of biomedical journal articles, resulting
</bodyText>
<figureCaption confidence="0.9978205">
Figure 1: Distribution of Level 0 Category Pairs. Mark size
indicates the number of unique NCs that fall under the CP. Only
</figureCaption>
<bodyText confidence="0.998251790697674">
those for which NCs occur are shown.
in about 1M NCs. The NCs were extracted by finding
adjacent word pairs in which both words are tagged as
nouns by a tagger and appear in the MeSH hierarchy, and
the words preceding and following the pair do not appear
in MeSH2 Of these two-word noun compounds, 79,677
were unique.
Next we used MeSH to characterize the NCs according
to semantic category(ies). For example, the NC fibroblast
growth was categorized into A11.329.228 (Fibroblasts)
and G07.553.481 (Growth).
Note that the same words can be represented at differ-
ent levels of description. For example, fibroblast growth
can be described by the MeSH descriptors A11.329.228
G07.553.481 (original level), but also by A11 G07 (Cell
and Physiological Processes) or A11.329 G07.553 (Con-
nective Tissue Cells and Growth and Embryonic Devel-
opment). If a noun fell under more than one MeSH ID,
we made multiple versions of this categorization. We re-
fer to the result of this renaming as a category pair (CP).
We placed these CPs into a two-dimensional table,
with the MeSH category for the first noun on the X axis,
and the MeSH category for the second noun on the Y
axis. Each intersection indicates the number of NCs that
are classified under the corresponding two MeSH cate-
gories.
A visualization tool (Ahlberg and Shneiderman, 1994)
allowed us to explore the dataset to see which areas of
the category space are most heavily populated, and to get
a feeling for whether the distribution is uniform or not
(see Figure 1). If our hypothesis holds (that NCs that fall
2Clearly, this simple approach results in some erroneous ex-
tractions.
within the same category pairs are assigned the same re-
lation), then if most of the NCs fall within only a few
category pairs then we only need to determine which re-
lations hold between a subset of the possible pairs. Thus,
the more clumped the distribution, the easier (potentially)
our task is. Figure 1 shows that some areas in the CP
space have a higher concentration of unique NCs (the
Anatomy, and the E through N sub-hierarchies, for ex-
ample), especially when we focus on those for which at
least 50 unique NCs are found.
</bodyText>
<sectionHeader confidence="0.969664" genericHeader="method">
5 Labeling NC Relations
</sectionHeader>
<bodyText confidence="0.999990833333333">
Given the promising nature of the NC distributions, the
question remains as to whether or not the hypothesis
holds. To answer this, we examined a subset of the CPs to
see if we could find positions within the sub-hierarchies
for which the relation assignments for the member NCs
are always the same.
</bodyText>
<subsectionHeader confidence="0.996852">
5.1 Method
</subsectionHeader>
<bodyText confidence="0.997099484848485">
We first selected a subset of the CPs to examine in detail.
For each of these we examined, by hand, 20% of the NCs
they cover, paraphrasing the relation between the nouns,
and seeing if that paraphrase was the same for all the NCs
in the group. If it was the same, then the current levels of
the CP were considered to be the correct levels of descrip-
tion. If, on the other hand, several different paraphrases
were found, then the analysis descended one level of the
hierarchy. This repeated until the resulting partition of
the NCs resulted in uniform relation assignments.
For example, all the following NCs were mapped to the
same CP, A01 (Body Regions) and A07 (Cardiovascular
System): scalp arteries, heel capillary, shoulder artery,
ankle artery, leg veins, limb vein, forearm arteries, fin-
ger capillary, eyelid capillary, forearm microcirculation,
hand vein, forearm veins, limb arteries, thigh vein, foot
vein. All these NCs are “similar” in the sense that the
relationships between the two words are the same; there-
fore, we do not need to descend either hierarchy. We call
the pair (A01, A07) a “rule”, where a rule is a CP for
which all the NCs under it have the same relationship. In
the future, when we see an NC mapped to this rule, we
will assign this semantic relationship to it.
On the other hand, the following NCs, having the CP
A01 (Body Regions) and M01 (Persons), do not have
the same relationship between the component words: ab-
domen patients, arm amputees, chest physicians, eye pa-
tients, skin donor. The relationships are different depend-
ing on whether the person is a patient, a physician or a
donor. We therefore descend the M01 sub-hierarchy, ob-
taining the following clusters of NCs:
A01 M01.643 (Patients): abdomen patients, ankle
inpatient, eye outpatient
</bodyText>
<table confidence="0.974726785714286">
A01 H01 (Natural Sciences):
A01 H01 abdomen x-ray, ankle motion
A01 H01.770 (Science): skin observation
A01 H01.548 (Mathematics): breast risk
A01 H01.939 (Weights and Measures): head calibration
A01 H01.181 (Chemistry): skin iontophoresis
A01 H01.671 (Physics)
A01 H01.671.538 (Motion): shoulder rotations
A01 H01.671.100 (Biophysics): shoulder biomechanics
A01 H01.671.691 (Pressure): eye pressures
A01 H01.671.868 (Temp.): forehead temperature
A01 H01.671.768 (Radiation): thorax x-ray
A01 H01.671.252 (Electricity): chest electrode
A01 H01.671.606 (Optics): skin color
</table>
<figureCaption confidence="0.74417125">
Figure 2: Levels of descent needed for NCs classified un-
der A01 H01.
A01 M01.526 (Occupational Groups): chest physician,
eye nurse, eye physician
</figureCaption>
<bodyText confidence="0.997411072727273">
A01, M01.898 (Donors): eye donor, skin donor
A01, M01.150 (Disabled Persons): arm amputees, knee
amputees.
In other words, to correctly assign a relationship to
these NCs, we needed to descend one level for the second
word. The resulting rules in this case are (A01 M01.643),
(A01, M01.150) etc. Figure 2 shows one CP for which we
needed to descend 3 levels.
In our collection, a total of 2627 CPs at level 0 have at
least 10 unique NCs. Of these, 798 (30%) are classified
with A (Anatomy) for either the first or the second noun.
We randomly selected 250 of such CPs for analysis.
We also analyzed 21 of the 90 CPs for which the sec-
ond noun was H01 (Natural Sciences); we decided to ana-
lyze this portion of the MeSH hierarchy because the NCs
with H01 as second noun are frequent in our collection,
and because we wanted to test the hypothesis that we do
indeed need to descend farther for heterogeneous parts of
MeSH.
Finally, we analyzed three CPs in category C (Dis-
eases); the most frequent CP in terms of the total number
of non-unique NCs is C04 (Neoplasms) A11 (Cells), with
30606 NCs; the second CP was A10 C04 (27520 total
NCs) and the fifth most frequent, A01 C04, with 20617
total NCs; we analyzed these CPs.
We started with the CPs at level 0 for both words, de-
scending when the corresponding clusters of NCs were
not homogeneous and stopping when they were. We did
this for 20% of the NCs in each CP. The results were as
follows.
For 187 of 250 (74%) CPs with a noun in the Anatomy
category, the classification remained at level 0 for both
words (for example, A01 A07). For 55 (22%) of the CPs
we had to descend 1 level (e.g., A01 M01: A01 M01.898,
A01 M01.643) and for 7 CPs (2%) we descended two
levels. We descended one level most of the time for the
sub-hierarchies E (Analytical, Diagnostic and Therapeu-
tic Techniques), G (Biological Sciences) and N (Health
Care) (around 50% of the time for these categories com-
bined). We never descended for B (Organisms) and did
so only for A13 (Animal Structures) in A. This was to be
able to distinguish a few non-homogeneous subcategories
(e.g., milk appearing among body parts, thus forcing a
distinction between buffalo milk and cat forelimb).
For CPs with H01 as the second noun, of the 21
CPs analyzed, we observed the following (level number,
count) pairs: (0, 1) (1, 8) (2, 12).
In all but three cases, the descending was done for the
second noun only. This may be because the second noun
usually plays the role of the head noun in two-word noun
compounds in English, thus requiring more specificity.
Alternatively, it may reflect the fact that for the exam-
ples we have examined so far, the more heterogeneous
terms dominate the second noun. Further examination is
needed to answer this decisively.
</bodyText>
<subsectionHeader confidence="0.994256">
5.2 Accuracy
</subsectionHeader>
<bodyText confidence="0.999992566666667">
We tested the resulting classifications by developing a
randomly chosen test set (20% of the NCs for each
CP), entirely distinct from the labeled set, and used the
classifications (rules) found above to automatically pre-
dict which relations should be assigned to the member
NCs. An independent evaluator with biomedical training
checked these results manually, and found high accura-
cies: For the CPs which contained a noun in the Anatomy
domain, the assignments of new NCs were 94.2% accu-
rate computed via intra-category averaging, and 91.3%
accurate with extra-category averaging. For the CPs in
the Natural Sciences (H01) we found 81.6% accuracy via
intra-category averaging, and 78.6% accuracy with extra-
category averaging. For the three CPs in the C04 category
we obtained 100% accuracy.
The total accuracy across the portions of the A, H01
and C04 hierarchies that we analyzed were 89.6% via
intra-category averaging, and 90.8% via extra-category
averaging.
The lower accuracy for the Natural Sciences category
illustrates the dependence of the results on the proper-
ties of the lexical hierarchy. We can generalize well if
the sub-hierarchies are in a well-defined semantic rela-
tion with their ancestors. If they are a list of “unrelated”
topics, we cannot use the generalization of the higher lev-
els; most of the mistakes for the Natural Sciences CPs oc-
curred in fact when we failed to descend for broad terms
such as Physics. Performing this evaluation allowed us
to find such problems and update the rules; the resulting
categorization should now be more accurate.
</bodyText>
<subsectionHeader confidence="0.965795">
5.3 Generalization
</subsectionHeader>
<bodyText confidence="0.999938727272727">
An important issue is whether this method is an economic
way of classifying the NCs. The advantage of the high
level description is, of course, that we need to assign by
hand many fewer relationships than if we used all CPs at
their most specific levels. Our approach provides gener-
alization over the “training” examples in two ways. First,
we find that we can use the juxtaposition of categories
in a lexical hierarchy to identify semantic relationships.
Second, we find we can use the higher levels of these cat-
egories for the assignments of these relationships.
To assess the degree of this generalization we calcu-
lated how many CPs are accounted for by the classifica-
tion rules created above for the Anatomy categories. In
other words, if we know that A01 A07 unequivocally de-
termines a relationship, how many possible (i.e., present
in our collection) CPs are there that are “covered by” A01
A07 and that we do not need to consider explicitly? It
turns out that our 415 classification rules cover 46001
possible CP pairs3.
This, and the fact that we achieve high accuracies with
these classification rules, show that we successfully use
MeSH to generalize over unique NCs.
</bodyText>
<subsectionHeader confidence="0.9714">
5.4 Ambiguity
</subsectionHeader>
<bodyText confidence="0.938014413793103">
A common problem for NLP tasks is ambiguity. In this
work we observe two kinds: lexical and “relationship”
ambiguity. As an example of the former, mortality can
refer to the state of being mortal or to death rate. As an
example of the latter, bacteria mortality can either mean
“death of bacteria” or “death caused by bacteria”.
In some cases, the relationship assignment method de-
scribed here can help disambiguate the meaning of an
ambiguous lexical item. Milk for example, can be both
Animal Structures (A13) and Food and Beverages (J02).
Consider the NCs chocolate milk, coconut milk that fall
under the CPs (B06 -Plants-, J02) and (B06, A13). The
CP (B06, J02) contains 180 NCs (other examples are
berry wines, cocoa beverages) while (B06, A13) has
only 6 NCs (4 of which with milk). Assuming then that
(B06, A13) is “wrong”, we will assign only (B06, J02)
to chocolate milk, coconut milk, therefore disambiguat-
ing the sense for milk in this context (Beverage). Anal-
ogously, for buffalo milk, caprine milk we also have two
CPs (B02, J02) (B02, A13). In this case, however, it is
easy to show that only (B02 -Vertebrates-, A13) is the
correct one (i.e. yielding the correct relationship) and we
then assign the MeSH sense A13 to milk.
Nevertheless, ambiguity may be a problem for this
method. We see five different cases:
3Although we began with 250 CPs in the A category, when a
descend operation is performed, the CP is split into two or more
CPs at the level below. Thus the total number of CPs after all
assignments are made was 415.
</bodyText>
<listItem confidence="0.850488283018868">
1) Single MeSH senses for the nouns in the NC (no lex-
ical ambiguity) and only one possible relationship which
can predicted by the CP; that is, no ambiguity. For in-
stance, in abdomen radiography, abdomen is classified
exclusively under Body Regions and radiography ex-
clusively under Diagnosis, and the relationship between
them is unambiguous. Other examples include aciclovir
treatment (Heterocyclic Compounds, Therapeutics) and
adenocarcinoma treatment (Neoplasms, Therapeutics).
2) Single MeSH senses (no lexical ambiguity) but mul-
tiple readings for the relationships that therefore cannot
be predicted by the CP. It was quite difficult to find exam-
ples of this case; disambiguating this kind of NC requires
looking at the context of use. The examples we did find
include hospital databases which can be databases re-
garding (topic) hospitals, databases found in (location)
or owned by hospitals. Education efforts can be efforts
done through (education) or done to achieve education.
Kidney metabolism can be metabolism happening in (lo-
cation) or done by the kidney. Immunoglobulin stain-
ing, (D12 -Amino Acids, Peptides-, and Proteins, E05 -
Investigative Techniques-) can mean either staining with
immunoglobulin or staining of immunoglobulin.
3) Multiple MeSH mappings but only one possible re-
lation. One example of this case is alcoholism treatment
where treatment is Therapeutics (E02) and alcoholism is
both Disorders of Environmental Origin (C21) and Men-
tal Disorders (F03). For this NC we have therefore 2 CPs:
(C21, E02) as in wound treatments, injury rehabilitation
and (F03, E02) as in delirium treatment, schizophrenia
therapeutics. The multiple mappings reflect the conflict-
ing views on how to classify the condition of alcoholism,
but the relationship does not change.
4) Multiple MeSH mappings and multiple relations
that can be predicted by the different CPs. For exam-
ple, Bread diet can mean either that a person usually eats
bread or that a physician prescribed bread to treat a con-
dition. This difference is reflected by the different map-
pings: diet is both Investigative Techniques (E05) and
Metabolism and Nutrition (G06), bread is Food and Bev-
erages (J02). In these cases, the category can help disam-
biguate the relation (as opposed to in case 5 below); word
sense disambiguation algorithms that use context may be
helpful.
5) Multiple MeSH mappings and multiple relations
that cannot be predicted by the different CPs. As an ex-
ample of this case, bacteria mortality can be both “death
of bacteria” or “death caused by bacteria”. The multiple
mapping for mortality (Public Health, Information Sci-
ence, Population Characteristics and Investigative Tech-
niques) does not account for this ambiguity. Similarly,
for inhibin immunization, the first noun falls under Hor-
mones and Amino Acids, while immunization falls under
</listItem>
<bodyText confidence="0.971937173076923">
Environment and Public Health and Investigative Tech-
niques. The meanings are immunization against inhibin
or immunization using inhibin, and they cannot be dis-
ambiguated using only the MeSH descriptors.
We currently do not have a way to determine how many
instances of each case occur. Cases 2 and 5 are the most
problematic; however, as it was quite difficult to find ex-
amples for these cases, we suspect they are relatively rare.
A question arises as to if representing nouns using the
topmost levels of the hierarchy causes a loss in informa-
tion about lexical ambiguity. In effect, when we represent
the terms at higher levels, we assume that words that have
multiple descriptors under the same level are very similar,
and that retaining the distinction would not be useful for
most computational tasks. For example, osteosarcoma
occurs twice in MeSH, as C04.557.450.565.575.650 and
C04.557.450.795.620. When described at level 0, both
descriptors reduce to C04, at level 1 to C04.557, remov-
ing the ambiguity. By contrast, microscopy also occurs
twice, but under E05.595 and H01.671.606.624. Reduc-
ing these descriptors to level 0 retains the two distinct
senses.
To determine how often different senses are grouped
together, we calculated the number of MeSH senses for
words at different levels of the hierarchy. Table 1 shows
a histogram of the number of senses for the first noun of
all the unique NCs in our collection, the average degree
of ambiguity and the average description lengths.4 The
average number of MeSH senses is always less than two,
and increases with length of description, as is to be ex-
pected.
We observe that 3.6% of the lexical ambiguity is at lev-
els higher that 2, 16% at L2, 21.4% at L1 and 59% at L0.
Level 1 and 2 combined account for more than 80% of the
lexical ambiguity. This means that when a noun has mul-
tiple senses, those senses are more likely to come from
different main subtrees of MeSH (A and B, for exam-
ple), than from different deeper nodes in the same subtree
(H01.671.538 vs. H01.671.252). This fits nicely with our
method of describing the NCs with the higher levels of
the hierarchy: if most of the ambiguity is at the highest
levels (as these results show), information about lexical
ambiguity is not lost when we describe the NCs using the
higher levels of MeSH. Ideally, however, we would like
to reduce the lexical ambiguity for similar senses and to
retain it when the senses are semantically distinct (like,
for example, for diet in case 4). In other words, ideally,
the ambiguity left at the levels of our rules accounts for
only (and for all) the semantically different senses. Fur-
ther analysis is needed, but the high accuracy we obtained
in the classification seems to indicate that this indeed is
what is happening.
</bodyText>
<footnote confidence="0.954562">
4We obtained very similar results for the second noun.
</footnote>
<table confidence="0.999967214285714">
# Senses Original L2 L1 L0
1 (Unambiguous) 51539 51766 54087 58763
2 18637 18611 18677 17373
3 5719 5816 4572 2177
4 2222 2048 1724 1075
5 831 827 418 289
6 223 262 167 0
7 384 254 32 0
8 2 2 0 0
9 61 91 0 0
10 59 0 0 0
Total(Ambiguous) 28138 27911 25590 20914
Avg # Senses 1.56 1.54 1.45 1.33
Avg Desc Len 3.71 2.79 1.97 1
</table>
<tableCaption confidence="0.999637">
Table 1: The number of MeSH senses for N1 when truncated
</tableCaption>
<bodyText confidence="0.966886">
to different levels of MeSH. Original refers to the actual (non-
truncated) MeSH descriptor. Avg # Senses is the average num-
ber of senses computed for all first nouns in the collection. Avg
Desc Len is the average description length; the value for level 1
is less than 2 and for level 2 is less that 3, because some nouns
are always mapped to higher levels (for example, cell is always
mapped to A11).
</bodyText>
<subsectionHeader confidence="0.990927">
5.5 Multiple Occurrences of Semantic Relations
</subsectionHeader>
<bodyText confidence="0.999913290322581">
Because we determine the possible relations in a data-
driven manner, the question arises of how often does the
same semantic relation occur for different category pairs.
To determine the answer, we could (i) look at all the CPs,
give a name to the relations and “merge” the CPs that
have the same relationships; or (ii) draw a sample of NC
examples for a given relation, look at the CPs for those
examples and verify that all the NCs for those CPs are
indeed in the same relationship.
We may not be able to determine the total number of
relations, or how often they repeat across different CPs,
until we examine the full spectrum of CPs. However, we
did a preliminary analysis to attempt to find relation repe-
tition across category pairs. As one example, we hypoth-
esized a relation afflicted by and verified that it applies
to all the CPs of the form (Disease C, Patients M01.643),
e.g.: anorexia (C23) patients, cancer (C04) survivor, in-
fluenza (C02) patients. This relation also applies to some
of the F category (Psychiatry), as in delirium (F03) pa-
tients, anxiety (F01) patient.
It becomes a judgement call whether to also include
NCs such as eye (A01) patient, gallbladder (A03) pa-
tients, and more generally, all the (Anatomy, Patients)
pairs. The question is, is “afflicted-by (unspecified) Dis-
ease in Anatomy Part” equivalent to “afflicted by Dis-
ease?” The answer depends on one’s theory of rela-
tional semantics. Another quandary is illustrated by the
NCs adolescent cancer, child tumors, adult dementia (in
which adolescent, child and adult are Age Groups) and
the heads are Diseases. Should these fall under the af-
flicted by relation, given the references to entire groups?
</bodyText>
<sectionHeader confidence="0.999978" genericHeader="method">
6 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999022">
6.1 Noun Compound Relation Assignment
</subsectionHeader>
<bodyText confidence="0.9999826">
Several approaches have been proposed for empirical
noun compound interpretation. Lauer &amp; Dras (1994)
point out that there are three components to the prob-
lem: identification of the compound from within the text,
syntactic analysis of the compound (left versus right as-
sociation), and the interpretation of the underlying se-
mantics. Several researchers have tackled the syntactic
analysis (Lauer, 1995), (Pustejovsky et al., 1993), (Liber-
man and Church, 1992), usually using a variation of the
idea of finding the subconstituents elsewhere in the cor-
pus and using those to predict how the larger compounds
are structured.
We are interested in the third task, interpretation of the
underlying semantics. Most related work relies on hand-
written rules of one kind or another. Finin (1980) exam-
ines the problem of noun compound interpretation in de-
tail, and constructs a complex set of rules. Uanderwende
(1994) uses a sophisticated system to extract semantic in-
formation automatically from an on-line dictionary, and
then manipulates a set of hand-written rules with hand-
assigned weights to create an interpretation. Rindflesch
et al. (2000) use hand-coded rule-based systems to ex-
tract the factual assertions from biomedical text. Lapata
(2000) classifies nominalizations according to whether
the modifier is the subject or the object of the underly-
ing verb expressed by the head noun.
Barker &amp; Szpakowicz (1998) describe noun com-
pounds as triplets of information: the first constituent, the
second constituent, and a marker that can indicate a num-
ber of syntactic clues. Relations are initially assigned by
hand, and then new ones are classified based on their sim-
ilarity to previously classified NCs. However, similarity
at the lexical level means only that the same word occurs;
no generalization over lexical items is made. The algo-
rithm is assessed in terms of how much it speeds up the
hand-labeling of relations. Barrett et al. (2001) have a
somewhat similar approach, using WordNet and creating
heuristics about how to classify a new NC given its simi-
larity to one that has already been seen.
In previous work (Rosario and Hearst, 2001), we
demonstrated the utility of using a lexical hierarchy for
assigning relations to two-word noun compounds. We
use machine learning algorithms and MeSH to success-
fully generalize from training instances, achieving about
60% accuracy on an 18-way classification problem us-
ing a very small training set. That approach is bottom
up and requires good coverage in the training set; the ap-
proach described in this paper is top-down, characteriz-
ing the lexical hierarchies explicitly rather than implicitly
through machine learning algorithms.
</bodyText>
<subsectionHeader confidence="0.999912">
6.2 Using Lexical Hierarchies
</subsectionHeader>
<bodyText confidence="0.999990444444445">
Many approaches attempt to automatically assign seman-
tic roles (such as case roles) by computing semantic
similarity measures across a large lexical hierarchy; pri-
marily using WordNet (Fellbaum, 1998). Budanitsky &amp;
Hirst (2001) provide a comparative analysis of such algo-
rithms.
However, it is uncommon to simply use the hier-
archy directly for generalization purposes. Many re-
searchers have noted that WordNet’s words are classi-
fied into senses that are too fine-grained for standard NLP
tasks. For example, Buitelaar (1997) notes that the noun
book is assigned to seven different senses, including fact
and section, subdivision. Thus most users of WordNet
must contend with the sense disambiguation issue in or-
der to use the lexicon.
The most closely related use of a lexical hierarchy
that we know of is that of Li &amp; Abe (1998), which uses
an information-theoretic measure to make a cut through
the top levels of the noun portion of WordNet. This is
then used to determine acceptable classes for verb argu-
ment structure, and for the prepositional phrase attach-
ment problem and is found to perform as well as or better
than existing algorithms.
Additionally, Boggess et al. (1991) “tag” veterinary
text using a small set of semantic labels, assigned in much
the same way a parser works, and describe this in the
context of prepositional phrase attachment.
</bodyText>
<sectionHeader confidence="0.997354" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999962520833333">
We have provided evidence that the upper levels of a lex-
ical hierarchy can be used to accurately classify the re-
lations that hold between two-word technical noun com-
pounds. In this paper we focus on biomedical terms us-
ing the biomedical lexical ontology MeSH. It may be that
such technical, domain-specific terminology is better be-
haved than NCs drawn from more general text; we will
have to assess the technique in other domains to fully as-
sess its applicability.
Several issues need to be explored further. First, we
need to ensure that this technique works across the full
spectrum of the lexical hierarchy. We have demonstrated
the likely usefulness of such an exercise, but all of our
analysis was done by hand. It may be useful enough to
simply complete the job manually; however, it would be
preferable to automate some or all of the analysis. There
are several ways to go about this. One approach would be
to use existing statistical similarity measures (Budanitsky
and Hirst, 2001) to attempt to identify which subhierar-
chies are homogeneous. Another approach would be to
see if, after analyzing more CPs, those categories found
to be heterogeneous should be assumed to be heteroge-
neous across classifications, and similarly for those that
seem to be homogeneous.
The second major issue to address is how to extend the
technique to multi-word noun compounds. We will need
to distinguish between NCs such as acute migraine treat-
ment and oral migraine treatment, and handle the case
when the relation must first be found between the left-
most words. Thus additional steps will be needed; one
approach is to compute statistics to indicate likelihood of
the various CPs.
Finding noun compound relations is part of our larger
effort to investigate what we call statistical semantic pars-
ing (as in (Burton and Brown, 1979); see Grishman
(1986) for a nice overview). For example, we would like
to be able to interpret titles in terms of semantic relations,
for example, transforming Congenital anomalies of tra-
cheobronchial branching patterns into a form that allows
questions to be answered such as “What kinds of irreg-
ularities can occur in lung structure?” We hope that by
compositional application of relations to entities, such in-
ferences will be possible.
Acknowledgements We thank Kaichi Sung for her
work on the relation labeling, Steve Maiorano for his
support of this research, and the anonymous reviewers
for their comments on the paper. This research was sup-
ported by a grant from ARDA.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994174617647059">
Christopher Ahlberg and Ben Shneiderman. 1994. Vi-
sual information seeking: Tight coupling of dynamic
query filters with starfield displays. In Proceedings of
ACM CHI’94, pages 313–317.
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Pro-
ceedings of COLING-ACL ’98, Montreal, Canada.
Leslie Barrett, Anthony R. Davis, and Bonnie J. Dorr.
2001. Interpreting noun-noun compounds using word-
net. In Proceedings of 2001 CICLing Conference,
Mexico City.
Lois Boggess, Rajeev Agarwal, and Ron Davis. 1991.
Disambiguation of prepositional phrases in automati-
cally labelled technical text. In AAAI 91, pages 155–
159.
Alexander Budanitsky and Graeme Hirst. 2001. Seman-
tic distance in wordnet: an experimental, application-
oriented evaluation of five measures. In Proceedings
of the NAACL 2001 Workshop on WordNet and Other
Lexical Resources, Pittsburgh, PA, June.
P. Buitelaar. 1997. A lexicon for underspecified semantic
tagging. In Proceedings of ANLP 97, SIGLEX Work-
shop, Washington DC.
R. R. Burton and J. S. Brown. 1979. Toward a natural-
language capability for computer-assisted instruction.
In H. O’Neil, editor, Procedures for Instructional Sys-
tems Development, pages 273–313. Academic Press,
New York.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Timothy W. Finin. 1980. The Semantic Interpretation of
Compound Nominals. Ph.d. dissertation, University of
Illinois, Urbana, Illinois.
Ralph Grishman. 1986. Computational Linguistics.
Cambridge University Press, Cambridge.
Maria Lapata. 2000. The automatic interpretation of
nominalizations. In Proceedings ofAAAI.
Mark Lauer and Mark Dras. 1994. A probabilistic model
of compound nouns. In Proceedings of the 7th Aus-
tralian Joint Conference on AI.
Mark Lauer. 1995. Corpus statistics meet the compound
noun. In Proceedings of the 33rd Meeting of the Asso-
ciation for Computational Linguistics, June.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDI principle. Computa-
tional Linguistics, 24(2):217–244.
Mark Y. Liberman and Kenneth W. Church. 1992. Text
analysis and word pronunciation in text-to-speech syn-
thesis. In Sadaoki Furui and Man Mohan Sondhi, ed-
itors, Advances in Speech Signal Processing, pages
791–831. Marcel Dekker, Inc.
James Pustejovsky, Sabine Bergler, and Peter Anick.
1993. Lexical semantic techniques for corpus analy-
sis. Computational Linguistics, 19(2).
James Pustejovsky, editor. 1995. The Generative Lexi-
con. MIT Press.
Thomas Rindflesch, Lorraine Tanabe, John N. Weinstein,
and Lawrence Hunter. 2000. Extraction of drugs,
genes and relations from the biomedical literature. Pa-
cific Symposium on Biocomputing, 5(5).
Barbara Rosario and Marti A. Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 2001 Conference on Empirical Methods in Natural
Language Processing. ACL.
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings of
COLING-94, pages 782–788.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.697237">
<note confidence="0.9974715">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 247-254.</note>
<title confidence="0.970521">The Descent of Hierarchy, and Selection in Relational Semantics</title>
<author confidence="0.998908">Barbara Rosario</author>
<affiliation confidence="0.967728">SIMS UC Berkeley</affiliation>
<address confidence="0.999738">Berkeley, CA 94720</address>
<email confidence="0.996567">rosario@sims.berkeley.edu</email>
<author confidence="0.99996">Marti A Hearst</author>
<affiliation confidence="0.964866">SIMS UC Berkeley</affiliation>
<address confidence="0.999709">Berkeley, CA 94720</address>
<email confidence="0.998986">hearst@sims.berkeley.edu</email>
<author confidence="0.993615">Charles Fillmore</author>
<affiliation confidence="0.9618805">ICSI UC Berkeley</affiliation>
<address confidence="0.999557">Berkeley, CA 94720</address>
<email confidence="0.999629">fillmore@icsi.berkeley.edu</email>
<abstract confidence="0.996142833333333">In many types of technical texts, meaning is embedded in noun compounds. A language understanding program needs to be able to interpret these in order to ascertain sentence meaning. We explore the possibility of using an existing lexical hierarchy for the purpose of placing words from a noun compound into categories, and then using this category membership to determine the relation that holds between the nouns. In this paper we present the results of an analysis of this method on twoword noun compounds from the biomedical domain, obtaining classification accuracy of approximately 90%. Since lexical hierarchies are not necessarily ideally suited for this task, we also pose the question: how far down the hierarchy must the algorithm descend before all the terms within the subhierarchy behave uniformly with respect to the semantic relation in question? We find that the topmost levels of the hierarchy yield an accurate classification, thus providing an economic way of assigning relations to noun compounds.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christopher Ahlberg</author>
<author>Ben Shneiderman</author>
</authors>
<title>Visual information seeking: Tight coupling of dynamic query filters with starfield displays.</title>
<date>1994</date>
<booktitle>In Proceedings of ACM CHI’94,</booktitle>
<pages>313--317</pages>
<contexts>
<context position="10404" citStr="Ahlberg and Shneiderman, 1994" startWordPosition="1690" endWordPosition="1693">vel), but also by A11 G07 (Cell and Physiological Processes) or A11.329 G07.553 (Connective Tissue Cells and Growth and Embryonic Development). If a noun fell under more than one MeSH ID, we made multiple versions of this categorization. We refer to the result of this renaming as a category pair (CP). We placed these CPs into a two-dimensional table, with the MeSH category for the first noun on the X axis, and the MeSH category for the second noun on the Y axis. Each intersection indicates the number of NCs that are classified under the corresponding two MeSH categories. A visualization tool (Ahlberg and Shneiderman, 1994) allowed us to explore the dataset to see which areas of the category space are most heavily populated, and to get a feeling for whether the distribution is uniform or not (see Figure 1). If our hypothesis holds (that NCs that fall 2Clearly, this simple approach results in some erroneous extractions. within the same category pairs are assigned the same relation), then if most of the NCs fall within only a few category pairs then we only need to determine which relations hold between a subset of the possible pairs. Thus, the more clumped the distribution, the easier (potentially) our task is. F</context>
</contexts>
<marker>Ahlberg, Shneiderman, 1994</marker>
<rawString>Christopher Ahlberg and Ben Shneiderman. 1994. Visual information seeking: Tight coupling of dynamic query filters with starfield displays. In Proceedings of ACM CHI’94, pages 313–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Barker</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semi-automatic recognition of noun modifier relationships.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL ’98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="30818" citStr="Barker &amp; Szpakowicz (1998)" startWordPosition="5116" endWordPosition="5119">) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Uanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules with handassigned weights to create an interpretation. Rindflesch et al. (2000) use hand-coded rule-based systems to extract the factual assertions from biomedical text. Lapata (2000) classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun. Barker &amp; Szpakowicz (1998) describe noun compounds as triplets of information: the first constituent, the second constituent, and a marker that can indicate a number of syntactic clues. Relations are initially assigned by hand, and then new ones are classified based on their similarity to previously classified NCs. However, similarity at the lexical level means only that the same word occurs; no generalization over lexical items is made. The algorithm is assessed in terms of how much it speeds up the hand-labeling of relations. Barrett et al. (2001) have a somewhat similar approach, using WordNet and creating heuristic</context>
</contexts>
<marker>Barker, Szpakowicz, 1998</marker>
<rawString>Ken Barker and Stan Szpakowicz. 1998. Semi-automatic recognition of noun modifier relationships. In Proceedings of COLING-ACL ’98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leslie Barrett</author>
<author>Anthony R Davis</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Interpreting noun-noun compounds using wordnet.</title>
<date>2001</date>
<booktitle>In Proceedings of 2001 CICLing Conference,</booktitle>
<location>Mexico City.</location>
<contexts>
<context position="31347" citStr="Barrett et al. (2001)" startWordPosition="5204" endWordPosition="5207"> or the object of the underlying verb expressed by the head noun. Barker &amp; Szpakowicz (1998) describe noun compounds as triplets of information: the first constituent, the second constituent, and a marker that can indicate a number of syntactic clues. Relations are initially assigned by hand, and then new ones are classified based on their similarity to previously classified NCs. However, similarity at the lexical level means only that the same word occurs; no generalization over lexical items is made. The algorithm is assessed in terms of how much it speeds up the hand-labeling of relations. Barrett et al. (2001) have a somewhat similar approach, using WordNet and creating heuristics about how to classify a new NC given its similarity to one that has already been seen. In previous work (Rosario and Hearst, 2001), we demonstrated the utility of using a lexical hierarchy for assigning relations to two-word noun compounds. We use machine learning algorithms and MeSH to successfully generalize from training instances, achieving about 60% accuracy on an 18-way classification problem using a very small training set. That approach is bottom up and requires good coverage in the training set; the approach desc</context>
</contexts>
<marker>Barrett, Davis, Dorr, 2001</marker>
<rawString>Leslie Barrett, Anthony R. Davis, and Bonnie J. Dorr. 2001. Interpreting noun-noun compounds using wordnet. In Proceedings of 2001 CICLing Conference, Mexico City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lois Boggess</author>
<author>Rajeev Agarwal</author>
<author>Ron Davis</author>
</authors>
<title>Disambiguation of prepositional phrases in automatically labelled technical text.</title>
<date>1991</date>
<booktitle>In AAAI 91,</booktitle>
<pages>155--159</pages>
<contexts>
<context position="33301" citStr="Boggess et al. (1991)" startWordPosition="5517" endWordPosition="5520">d to seven different senses, including fact and section, subdivision. Thus most users of WordNet must contend with the sense disambiguation issue in order to use the lexicon. The most closely related use of a lexical hierarchy that we know of is that of Li &amp; Abe (1998), which uses an information-theoretic measure to make a cut through the top levels of the noun portion of WordNet. This is then used to determine acceptable classes for verb argument structure, and for the prepositional phrase attachment problem and is found to perform as well as or better than existing algorithms. Additionally, Boggess et al. (1991) “tag” veterinary text using a small set of semantic labels, assigned in much the same way a parser works, and describe this in the context of prepositional phrase attachment. 7 Conclusions and Future Work We have provided evidence that the upper levels of a lexical hierarchy can be used to accurately classify the relations that hold between two-word technical noun compounds. In this paper we focus on biomedical terms using the biomedical lexical ontology MeSH. It may be that such technical, domain-specific terminology is better behaved than NCs drawn from more general text; we will have to as</context>
</contexts>
<marker>Boggess, Agarwal, Davis, 1991</marker>
<rawString>Lois Boggess, Rajeev Agarwal, and Ron Davis. 1991. Disambiguation of prepositional phrases in automatically labelled technical text. In AAAI 91, pages 155– 159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Semantic distance in wordnet: an experimental, applicationoriented evaluation of five measures.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL 2001 Workshop on WordNet and Other Lexical Resources,</booktitle>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="34497" citStr="Budanitsky and Hirst, 2001" startWordPosition="5720" endWordPosition="5723">eneral text; we will have to assess the technique in other domains to fully assess its applicability. Several issues need to be explored further. First, we need to ensure that this technique works across the full spectrum of the lexical hierarchy. We have demonstrated the likely usefulness of such an exercise, but all of our analysis was done by hand. It may be useful enough to simply complete the job manually; however, it would be preferable to automate some or all of the analysis. There are several ways to go about this. One approach would be to use existing statistical similarity measures (Budanitsky and Hirst, 2001) to attempt to identify which subhierarchies are homogeneous. Another approach would be to see if, after analyzing more CPs, those categories found to be heterogeneous should be assumed to be heterogeneous across classifications, and similarly for those that seem to be homogeneous. The second major issue to address is how to extend the technique to multi-word noun compounds. We will need to distinguish between NCs such as acute migraine treatment and oral migraine treatment, and handle the case when the relation must first be found between the leftmost words. Thus additional steps will be need</context>
<context position="32347" citStr="Budanitsky &amp; Hirst (2001)" startWordPosition="5357" endWordPosition="5360">eneralize from training instances, achieving about 60% accuracy on an 18-way classification problem using a very small training set. That approach is bottom up and requires good coverage in the training set; the approach described in this paper is top-down, characterizing the lexical hierarchies explicitly rather than implicitly through machine learning algorithms. 6.2 Using Lexical Hierarchies Many approaches attempt to automatically assign semantic roles (such as case roles) by computing semantic similarity measures across a large lexical hierarchy; primarily using WordNet (Fellbaum, 1998). Budanitsky &amp; Hirst (2001) provide a comparative analysis of such algorithms. However, it is uncommon to simply use the hierarchy directly for generalization purposes. Many researchers have noted that WordNet’s words are classified into senses that are too fine-grained for standard NLP tasks. For example, Buitelaar (1997) notes that the noun book is assigned to seven different senses, including fact and section, subdivision. Thus most users of WordNet must contend with the sense disambiguation issue in order to use the lexicon. The most closely related use of a lexical hierarchy that we know of is that of Li &amp; Abe (199</context>
</contexts>
<marker>Budanitsky, Hirst, 2001</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2001. Semantic distance in wordnet: an experimental, applicationoriented evaluation of five measures. In Proceedings of the NAACL 2001 Workshop on WordNet and Other Lexical Resources, Pittsburgh, PA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
</authors>
<title>A lexicon for underspecified semantic tagging.</title>
<date>1997</date>
<booktitle>In Proceedings of ANLP 97, SIGLEX Workshop,</booktitle>
<location>Washington DC.</location>
<contexts>
<context position="32644" citStr="Buitelaar (1997)" startWordPosition="5406" endWordPosition="5407">y rather than implicitly through machine learning algorithms. 6.2 Using Lexical Hierarchies Many approaches attempt to automatically assign semantic roles (such as case roles) by computing semantic similarity measures across a large lexical hierarchy; primarily using WordNet (Fellbaum, 1998). Budanitsky &amp; Hirst (2001) provide a comparative analysis of such algorithms. However, it is uncommon to simply use the hierarchy directly for generalization purposes. Many researchers have noted that WordNet’s words are classified into senses that are too fine-grained for standard NLP tasks. For example, Buitelaar (1997) notes that the noun book is assigned to seven different senses, including fact and section, subdivision. Thus most users of WordNet must contend with the sense disambiguation issue in order to use the lexicon. The most closely related use of a lexical hierarchy that we know of is that of Li &amp; Abe (1998), which uses an information-theoretic measure to make a cut through the top levels of the noun portion of WordNet. This is then used to determine acceptable classes for verb argument structure, and for the prepositional phrase attachment problem and is found to perform as well as or better than</context>
</contexts>
<marker>Buitelaar, 1997</marker>
<rawString>P. Buitelaar. 1997. A lexicon for underspecified semantic tagging. In Proceedings of ANLP 97, SIGLEX Workshop, Washington DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R R Burton</author>
<author>J S Brown</author>
</authors>
<title>Toward a naturallanguage capability for computer-assisted instruction.</title>
<date>1979</date>
<booktitle>Procedures for Instructional Systems Development,</booktitle>
<pages>273--313</pages>
<editor>In H. O’Neil, editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="35331" citStr="Burton and Brown, 1979" startWordPosition="5857" endWordPosition="5860">s classifications, and similarly for those that seem to be homogeneous. The second major issue to address is how to extend the technique to multi-word noun compounds. We will need to distinguish between NCs such as acute migraine treatment and oral migraine treatment, and handle the case when the relation must first be found between the leftmost words. Thus additional steps will be needed; one approach is to compute statistics to indicate likelihood of the various CPs. Finding noun compound relations is part of our larger effort to investigate what we call statistical semantic parsing (as in (Burton and Brown, 1979); see Grishman (1986) for a nice overview). For example, we would like to be able to interpret titles in terms of semantic relations, for example, transforming Congenital anomalies of tracheobronchial branching patterns into a form that allows questions to be answered such as “What kinds of irregularities can occur in lung structure?” We hope that by compositional application of relations to entities, such inferences will be possible. Acknowledgements We thank Kaichi Sung for her work on the relation labeling, Steve Maiorano for his support of this research, and the anonymous reviewers for the</context>
</contexts>
<marker>Burton, Brown, 1979</marker>
<rawString>R. R. Burton and J. S. Brown. 1979. Toward a naturallanguage capability for computer-assisted instruction. In H. O’Neil, editor, Procedures for Instructional Systems Development, pages 273–313. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="30818" citStr="(1998)" startWordPosition="5119" endWordPosition="5119">em of noun compound interpretation in detail, and constructs a complex set of rules. Uanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules with handassigned weights to create an interpretation. Rindflesch et al. (2000) use hand-coded rule-based systems to extract the factual assertions from biomedical text. Lapata (2000) classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun. Barker &amp; Szpakowicz (1998) describe noun compounds as triplets of information: the first constituent, the second constituent, and a marker that can indicate a number of syntactic clues. Relations are initially assigned by hand, and then new ones are classified based on their similarity to previously classified NCs. However, similarity at the lexical level means only that the same word occurs; no generalization over lexical items is made. The algorithm is assessed in terms of how much it speeds up the hand-labeling of relations. Barrett et al. (2001) have a somewhat similar approach, using WordNet and creating heuristic</context>
<context position="32949" citStr="(1998)" startWordPosition="5462" endWordPosition="5462">001) provide a comparative analysis of such algorithms. However, it is uncommon to simply use the hierarchy directly for generalization purposes. Many researchers have noted that WordNet’s words are classified into senses that are too fine-grained for standard NLP tasks. For example, Buitelaar (1997) notes that the noun book is assigned to seven different senses, including fact and section, subdivision. Thus most users of WordNet must contend with the sense disambiguation issue in order to use the lexicon. The most closely related use of a lexical hierarchy that we know of is that of Li &amp; Abe (1998), which uses an information-theoretic measure to make a cut through the top levels of the noun portion of WordNet. This is then used to determine acceptable classes for verb argument structure, and for the prepositional phrase attachment problem and is found to perform as well as or better than existing algorithms. Additionally, Boggess et al. (1991) “tag” veterinary text using a small set of semantic labels, assigned in much the same way a parser works, and describe this in the context of prepositional phrase attachment. 7 Conclusions and Future Work We have provided evidence that the upper l</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy W Finin</author>
</authors>
<title>The Semantic Interpretation of Compound Nominals.</title>
<date>1980</date>
<institution>University of Illinois,</institution>
<location>Urbana, Illinois.</location>
<note>Ph.d. dissertation,</note>
<contexts>
<context position="30193" citStr="Finin (1980)" startWordPosition="5022" endWordPosition="5023">n of the compound from within the text, syntactic analysis of the compound (left versus right association), and the interpretation of the underlying semantics. Several researchers have tackled the syntactic analysis (Lauer, 1995), (Pustejovsky et al., 1993), (Liberman and Church, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics. Most related work relies on handwritten rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Uanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules with handassigned weights to create an interpretation. Rindflesch et al. (2000) use hand-coded rule-based systems to extract the factual assertions from biomedical text. Lapata (2000) classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun. B</context>
</contexts>
<marker>Finin, 1980</marker>
<rawString>Timothy W. Finin. 1980. The Semantic Interpretation of Compound Nominals. Ph.d. dissertation, University of Illinois, Urbana, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>Computational Linguistics.</title>
<date>1986</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="35352" citStr="Grishman (1986)" startWordPosition="5862" endWordPosition="5863">rly for those that seem to be homogeneous. The second major issue to address is how to extend the technique to multi-word noun compounds. We will need to distinguish between NCs such as acute migraine treatment and oral migraine treatment, and handle the case when the relation must first be found between the leftmost words. Thus additional steps will be needed; one approach is to compute statistics to indicate likelihood of the various CPs. Finding noun compound relations is part of our larger effort to investigate what we call statistical semantic parsing (as in (Burton and Brown, 1979); see Grishman (1986) for a nice overview). For example, we would like to be able to interpret titles in terms of semantic relations, for example, transforming Congenital anomalies of tracheobronchial branching patterns into a form that allows questions to be answered such as “What kinds of irregularities can occur in lung structure?” We hope that by compositional application of relations to entities, such inferences will be possible. Acknowledgements We thank Kaichi Sung for her work on the relation labeling, Steve Maiorano for his support of this research, and the anonymous reviewers for their comments on the pa</context>
</contexts>
<marker>Grishman, 1986</marker>
<rawString>Ralph Grishman. 1986. Computational Linguistics. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
</authors>
<title>The automatic interpretation of nominalizations.</title>
<date>2000</date>
<booktitle>In Proceedings ofAAAI.</booktitle>
<contexts>
<context position="30650" citStr="Lapata (2000)" startWordPosition="5091" endWordPosition="5092">interested in the third task, interpretation of the underlying semantics. Most related work relies on handwritten rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Uanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules with handassigned weights to create an interpretation. Rindflesch et al. (2000) use hand-coded rule-based systems to extract the factual assertions from biomedical text. Lapata (2000) classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun. Barker &amp; Szpakowicz (1998) describe noun compounds as triplets of information: the first constituent, the second constituent, and a marker that can indicate a number of syntactic clues. Relations are initially assigned by hand, and then new ones are classified based on their similarity to previously classified NCs. However, similarity at the lexical level means only that the same word occurs; no generalization over lexical items is made. The algorithm is</context>
</contexts>
<marker>Lapata, 2000</marker>
<rawString>Maria Lapata. 2000. The automatic interpretation of nominalizations. In Proceedings ofAAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
<author>Mark Dras</author>
</authors>
<title>A probabilistic model of compound nouns.</title>
<date>1994</date>
<booktitle>In Proceedings of the 7th Australian Joint Conference on AI.</booktitle>
<contexts>
<context position="29509" citStr="Lauer &amp; Dras (1994)" startWordPosition="4911" endWordPosition="4914">the (Anatomy, Patients) pairs. The question is, is “afflicted-by (unspecified) Disease in Anatomy Part” equivalent to “afflicted by Disease?” The answer depends on one’s theory of relational semantics. Another quandary is illustrated by the NCs adolescent cancer, child tumors, adult dementia (in which adolescent, child and adult are Age Groups) and the heads are Diseases. Should these fall under the afflicted by relation, given the references to entire groups? 6 Related Work 6.1 Noun Compound Relation Assignment Several approaches have been proposed for empirical noun compound interpretation. Lauer &amp; Dras (1994) point out that there are three components to the problem: identification of the compound from within the text, syntactic analysis of the compound (left versus right association), and the interpretation of the underlying semantics. Several researchers have tackled the syntactic analysis (Lauer, 1995), (Pustejovsky et al., 1993), (Liberman and Church, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics</context>
</contexts>
<marker>Lauer, Dras, 1994</marker>
<rawString>Mark Lauer and Mark Dras. 1994. A probabilistic model of compound nouns. In Proceedings of the 7th Australian Joint Conference on AI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Corpus statistics meet the compound noun.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="29810" citStr="Lauer, 1995" startWordPosition="4959" endWordPosition="4960">scent, child and adult are Age Groups) and the heads are Diseases. Should these fall under the afflicted by relation, given the references to entire groups? 6 Related Work 6.1 Noun Compound Relation Assignment Several approaches have been proposed for empirical noun compound interpretation. Lauer &amp; Dras (1994) point out that there are three components to the problem: identification of the compound from within the text, syntactic analysis of the compound (left versus right association), and the interpretation of the underlying semantics. Several researchers have tackled the syntactic analysis (Lauer, 1995), (Pustejovsky et al., 1993), (Liberman and Church, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics. Most related work relies on handwritten rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Uanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dict</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Corpus statistics meet the compound noun. In Proceedings of the 33rd Meeting of the Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDI principle.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="32949" citStr="Li &amp; Abe (1998)" startWordPosition="5459" endWordPosition="5462"> Hirst (2001) provide a comparative analysis of such algorithms. However, it is uncommon to simply use the hierarchy directly for generalization purposes. Many researchers have noted that WordNet’s words are classified into senses that are too fine-grained for standard NLP tasks. For example, Buitelaar (1997) notes that the noun book is assigned to seven different senses, including fact and section, subdivision. Thus most users of WordNet must contend with the sense disambiguation issue in order to use the lexicon. The most closely related use of a lexical hierarchy that we know of is that of Li &amp; Abe (1998), which uses an information-theoretic measure to make a cut through the top levels of the noun portion of WordNet. This is then used to determine acceptable classes for verb argument structure, and for the prepositional phrase attachment problem and is found to perform as well as or better than existing algorithms. Additionally, Boggess et al. (1991) “tag” veterinary text using a small set of semantic labels, assigned in much the same way a parser works, and describe this in the context of prepositional phrase attachment. 7 Conclusions and Future Work We have provided evidence that the upper l</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the MDI principle. Computational Linguistics, 24(2):217–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Y Liberman</author>
<author>Kenneth W Church</author>
</authors>
<title>Text analysis and word pronunciation in text-to-speech synthesis.</title>
<date>1992</date>
<booktitle>In Sadaoki Furui and Man Mohan Sondhi, editors, Advances in Speech Signal Processing,</booktitle>
<pages>791--831</pages>
<publisher>Marcel Dekker, Inc.</publisher>
<contexts>
<context position="29867" citStr="Liberman and Church, 1992" startWordPosition="4965" endWordPosition="4969">the heads are Diseases. Should these fall under the afflicted by relation, given the references to entire groups? 6 Related Work 6.1 Noun Compound Relation Assignment Several approaches have been proposed for empirical noun compound interpretation. Lauer &amp; Dras (1994) point out that there are three components to the problem: identification of the compound from within the text, syntactic analysis of the compound (left versus right association), and the interpretation of the underlying semantics. Several researchers have tackled the syntactic analysis (Lauer, 1995), (Pustejovsky et al., 1993), (Liberman and Church, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics. Most related work relies on handwritten rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Uanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules </context>
</contexts>
<marker>Liberman, Church, 1992</marker>
<rawString>Mark Y. Liberman and Kenneth W. Church. 1992. Text analysis and word pronunciation in text-to-speech synthesis. In Sadaoki Furui and Man Mohan Sondhi, editors, Advances in Speech Signal Processing, pages 791–831. Marcel Dekker, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Sabine Bergler</author>
<author>Peter Anick</author>
</authors>
<title>Lexical semantic techniques for corpus analysis.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="29838" citStr="Pustejovsky et al., 1993" startWordPosition="4961" endWordPosition="4964">d adult are Age Groups) and the heads are Diseases. Should these fall under the afflicted by relation, given the references to entire groups? 6 Related Work 6.1 Noun Compound Relation Assignment Several approaches have been proposed for empirical noun compound interpretation. Lauer &amp; Dras (1994) point out that there are three components to the problem: identification of the compound from within the text, syntactic analysis of the compound (left versus right association), and the interpretation of the underlying semantics. Several researchers have tackled the syntactic analysis (Lauer, 1995), (Pustejovsky et al., 1993), (Liberman and Church, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics. Most related work relies on handwritten rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Uanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates</context>
</contexts>
<marker>Pustejovsky, Bergler, Anick, 1993</marker>
<rawString>James Pustejovsky, Sabine Bergler, and Peter Anick. 1993. Lexical semantic techniques for corpus analysis. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<title>The Generative Lexicon.</title>
<date>1995</date>
<editor>James Pustejovsky, editor.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4088" citStr="(1995)" startWordPosition="659" endWordPosition="659">ations behind this approach, the characteristics of the lexical ontology MeSH, the use of a corpus to examine the problem space, the method of determining the relations, the accuracy of the results, and the problem of ambiguity. The paper concludes with related work and a discussion of future work. 2 Linguistic Motivation One way to understand the relations between the words in a two-word noun compound is to cast the words into a head-modifier relationship, and assume that the head noun has an argument structure, much the way verbs do, as well as a qualia structure in the sense of Pustejovsky (1995). Then the meaning of the head noun determines what kinds of things can be done to it, what it is made of, what it is a part of, and so on. For example, consider the noun knife. Knives are created for particular activities or settings, can be made of various materials, and can be used for cutting or manipulating various kinds of things. A set of relations for knives, and example NCs exhibiting these relations is shown below: (Used-in): kitchen knife, hunting knife (Made-of): steel knife, plastic knife (Instrument-for): carving knife (Used-on): meat knife, putty knife (Used-by): chef’s knife, b</context>
</contexts>
<marker>1995</marker>
<rawString>James Pustejovsky, editor. 1995. The Generative Lexicon. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Rindflesch</author>
<author>Lorraine Tanabe</author>
<author>John N Weinstein</author>
<author>Lawrence Hunter</author>
</authors>
<title>Extraction of drugs, genes and relations from the biomedical literature. Pacific Symposium on</title>
<date>2000</date>
<journal>Biocomputing,</journal>
<volume>5</volume>
<issue>5</issue>
<contexts>
<context position="30546" citStr="Rindflesch et al. (2000)" startWordPosition="5074" endWordPosition="5077">subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics. Most related work relies on handwritten rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Uanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules with handassigned weights to create an interpretation. Rindflesch et al. (2000) use hand-coded rule-based systems to extract the factual assertions from biomedical text. Lapata (2000) classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun. Barker &amp; Szpakowicz (1998) describe noun compounds as triplets of information: the first constituent, the second constituent, and a marker that can indicate a number of syntactic clues. Relations are initially assigned by hand, and then new ones are classified based on their similarity to previously classified NCs. However, similarity at the lexical le</context>
</contexts>
<marker>Rindflesch, Tanabe, Weinstein, Hunter, 2000</marker>
<rawString>Thomas Rindflesch, Lorraine Tanabe, John N. Weinstein, and Lawrence Hunter. 2000. Extraction of drugs, genes and relations from the biomedical literature. Pacific Symposium on Biocomputing, 5(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Rosario</author>
<author>Marti A Hearst</author>
</authors>
<title>Classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="31550" citStr="Rosario and Hearst, 2001" startWordPosition="5239" endWordPosition="5242">arker that can indicate a number of syntactic clues. Relations are initially assigned by hand, and then new ones are classified based on their similarity to previously classified NCs. However, similarity at the lexical level means only that the same word occurs; no generalization over lexical items is made. The algorithm is assessed in terms of how much it speeds up the hand-labeling of relations. Barrett et al. (2001) have a somewhat similar approach, using WordNet and creating heuristics about how to classify a new NC given its similarity to one that has already been seen. In previous work (Rosario and Hearst, 2001), we demonstrated the utility of using a lexical hierarchy for assigning relations to two-word noun compounds. We use machine learning algorithms and MeSH to successfully generalize from training instances, achieving about 60% accuracy on an 18-way classification problem using a very small training set. That approach is bottom up and requires good coverage in the training set; the approach described in this paper is top-down, characterizing the lexical hierarchies explicitly rather than implicitly through machine learning algorithms. 6.2 Using Lexical Hierarchies Many approaches attempt to aut</context>
</contexts>
<marker>Rosario, Hearst, 2001</marker>
<rawString>Barbara Rosario and Marti A. Hearst. 2001. Classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
</authors>
<title>Algorithm for automatic interpretation of noun sequences.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING-94,</booktitle>
<pages>782--788</pages>
<marker>Vanderwende, 1994</marker>
<rawString>Lucy Vanderwende. 1994. Algorithm for automatic interpretation of noun sequences. In Proceedings of COLING-94, pages 782–788.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>