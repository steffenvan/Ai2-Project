<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998759">
Ambiguity Resolution in the DMTRANS PLUS
</title>
<author confidence="0.986939">
Hiroaki Kitano; Hideto Tomabechi, and Lori Levin
</author>
<affiliation confidence="0.841451333333333">
Center for Machine Translation
Carnegie Mellon University
Pittsburgh, PA 15213 U.S.A.
</affiliation>
<bodyText confidence="0.9905159">
access (DMA) paradigm of natural language process-
ing. Under the DMA paradigm, the mental state of
the hearer is modelled by a massively parallel network
representing memory. Parsing is performed by pass-
ing markers in the memory network. In our model,
the meaning of a sentence is viewed as modifications
made to the memory network. The meaning of a sen-
tence in our model is definable as the difference in the
memory network before and after understanding the
sentence.
</bodyText>
<sectionHeader confidence="0.926768" genericHeader="abstract">
2 Limitations of Current Methods
</sectionHeader>
<subsectionHeader confidence="0.481016">
of Ambiguity Resolution
</subsectionHeader>
<bodyText confidence="0.900866">
Abstract
We present a cost-based (or energy-based) model of dis-
ambiguation. When a sentence is ambiguous, a parse with
the least cost is chosen from among multiple hypotheses.
Each hypothesis is assigned a cost which is added when:
</bodyText>
<listItem confidence="0.564433">
(1) a new instance is created to satisfy reference success,
</listItem>
<bodyText confidence="0.941758444444444">
(2) links between instances are created or removed to sat-
isfy constraints on concept sequences, and (3) a concept
node with insufficient priming is used for further process-
ing. This method of ambiguity resolution is implemented in
DMTRANS PLUS, which is a second generation bi-directional
English/Japanese machine translation system based on a mas-
sively parallel spreading activation paradigm developed at
the Center for Machine Translation at Carnegie Mellon Uni-
versity.
</bodyText>
<sectionHeader confidence="0.999188" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.977140296875">
One of the central issues in natural language under-
standing research is ambiguity resolution. Since many
sentences are ambiguous out of context, techniques for
ambiguity resolution have been an important topic in
natural language understanding. In this paper, we de-
scribe a model of ambiguity resolution implemented
in DMTRANS PLUS, which is a next generation ma-
chine translation system based on a massively parallel
comuputational paradigm. In our model, ambiguities
are resolved by evaluating the cost of each hypothe-
sis; the hypothesis with the least cost will be selected.
Costs are assigned when (1) a new instance is cre-
ated to satisfy reference success, (2) links between in-
stances are created or removed to satisfy constraints
on concept sequences, and (3) a concept node with
insufficient priming is used for further processing.
The underlying philosophy of the model is to view
parsing as a dynamic physical process in which one
trajectory is taken from among many other possible
paths. Thus our notion of the cost of the hypothesis is
a representation of the workload required to take the
path representing the hypothesis. One other impor-
tant idea is that our model employs the direct memory
*E-mail address is hiroalci@aalcs.cmu.edu. Also with NEC
Corporation.
Traditional syntactic parsers have been using attach-
ment preferences and local syntactic and semantic con-
straints for resolving lexical and structural ambiguities.
([171, [28], [2], [7], [26], [II], [5]) However, these
methods cannot select one interpretation from several
plausible interpretations because they do not incorpo-
rate the discourse context of the sentences being parsed
([8], [4]).
Connectionist-type approaches as seen in [18], [25],
and [8] essentially stick to semantic restrictions and
associations. However, [18], [25], [24] only provide
local interactions, omitting interaction with context.
Moreover, difficulties regarding variable-binding and
embedded sentences should be noticed.
In [8], world knowledge is used through testing ref-
erential success and other sequential tests. However,
this method does not provide a uniform model of pars-
ing: lexical ambiguities are resolved by marker passing
and structural disambiguations are resolved by apply-
ing separate sequential tests.
An approach by [15] is similar to our model in that
both precieve parsing as a physical process. However,
their model, along with most other models, fails to
capture discourse context.
[12] uses marker passing as a method of contex-
tual inference after a parse; however, no contextual in-
formation is feed-backed during the sentential parsing
(marker-passing is performed after a separate parsing
- 72 -
process providing multiple hypotheses of the parse).
[20] is closer to our model in that marker-passing
based contextual inference is used during a sentential
parse (i.e., an integrated processing of syntax, seman-
tics and pragmatics at real-time); however the parsing
(LFG, and case-frame based) and contextual inferences
(marker-passing) are not under an uniform architecture.
Past generations of DMTRANS ([19], [23]) have not
incorporated cost-based structural ambiguity resolution
schemes.
</bodyText>
<sectionHeader confidence="0.92041" genericHeader="introduction">
3 Overview of DMTRANS PLUS
</sectionHeader>
<subsectionHeader confidence="0.998716">
3.1 Memory Access Parsing
</subsectionHeader>
<bodyText confidence="0.9972182">
DMTRANS PLUS is a second generation DMA system
based upon DMTRANS ([19]) with new methods of am-
biguity resolution based on costs.
Unlike most natural language systems, which are
based on the &amp;quot;Build-and-Store&amp;quot; model, our system
employs a &amp;quot;Recognize-and-Record&amp;quot; model ([14],[19],
[21]). Understanding of an input sentence (or speech
input in ODMTRANS PLUS) is defined as changes made
in a memory network. Parsing and natural language
understanding in these systems are considered to be
memory-access processes, identifying existent knowl-
edge in memory with the current input. Sentences
are always parsed in context, i.e., through utilizing
the existing and (currently acquired) knowledge about
the world. In other words, during parsing, relevant
discourse entities in memory are constantly being re-
membered.
The model behind DMTRANS PLUS is a simulation
of such a process. The memory network incorporates
knowledge from morphophonetics to discourse. Each
node represents a concept (Concept Class node; CC)
or a sequence of concepts (Concept Sequence Class
node; CSC).
CCs represent such knowledge as phones (i.e. [k]),
phonemes (i.e. /k0, concepts (i.e. *Hand-Gun,
*Event, *Mtrans-Action), and plans (i.e. *Pick-Up-
Gun). A hierarchy of Concept Class (CC) entities
stores knowledge both declaratively and procedurely
as described in [19] and [21]. Lexical entries are rep-
resented as lexical nodes which are a kind of CC.
Phoneme sequences are used only for DMTRANS
PLUS, the speech-input version of DMTRANS PLUS.
CSCs represent sequences of concepts such as
phoneme sequences (i.e. &lt;/lc//a//i//g//i/&gt;), concept
sequences (i.e. &lt;*Conference *Goal-Role *Attend
*Want&gt;), and plan sequences (i.e. &lt;*Declare-Want-
Attend *Listen-Instruction&gt;). The linguistic knowl-
edge represented as CSCs can be low-level surface
specific patterns such as phrasal lexicon entries [1]
or material at higher levels of abstration such as in
MOP&apos;s [16]. However, CSCs should not be confused
with &apos;discourse segments&apos; [6]. In our model, infor-
mation represented in discourse segments are distribu-
tively incorporated in the memory network.
During sentence processing we create concept in-
stances (CI) correpsonding to CCs and concept se-
quence instances (CSI) corresponding to CSCs. This
is a substantial improvement over past DMA research.
Lack of instance creation and reference in past research
was a major obstacle to seriously modelling discourse
phenomena.
CIs and CSIs are connected through several types of
links. A guided marker passing scheme is employed
for inference on the memory network following meth-
ods adopted in past DMA models.
</bodyText>
<listItem confidence="0.972651117647059">
DMTRANS PLUS uses three markers for parsing:
• An Activation Marker (A-Marker) is created
when a concept is initially activated by a lexical
item or as a result of concept refinement. It indi-
cates which instance of a concept is the source of
activation and contains relevant cost information.
A-Markers are passed upward along is-a links in
the abstraction hierarchy.
• A Prediction marker (P-Marker) is passed along
a concept sequence to identify the linear order
of concepts in the sequence. When an A-Marker
reaches a node that has a P-Marker, the P-Marker
is sent to the next element of the concept se-
quence, thus predicting which node is to be acti-
vated next.
• A Context marker (C-Marker) is placed on a node
which has contextual priming.
</listItem>
<bodyText confidence="0.998167875">
Information about which instances originated acti-
vations is carried by A-Markers. The binding list of
instances and their roles are held in P-Markers&apos;.
The following is the algorithm used in DMTRANS
PLUS parsing:
Let Lex, Con, Elem, and Seq be a set of lexical
nodes, conceptual nodes, elements of concept se-
quences, and concept sequences, respectively.
</bodyText>
<equation confidence="0.914120666666667">
Parse(S)
For each word w in S, do:
Activate(w),
</equation>
<bodyText confidence="0.549500857142857">
For all i and j:
if Active(Ni) A N5 E Con
&apos;Marker parsing spreading activation is our choice over con-
nectionist network precisely because of this reason. Variable bind-
ing (which cannot be easily handled in cotmectionist network) can
be trivially attained through structure (information) passing of A-
Markers and P-Markers.
</bodyText>
<figure confidence="0.931047275862069">
- 73 -
then do concurrently:
Activate(isa(Ni)
if Active(ei.Ni) A Predicted(ej.Ni)A
then Predict(ei.i.Ni)
if Active(ei.Ni) A Predicted(ei.Ni) A Last(ei.Ni)
then Accept(Ni), Activate(isa(Ni))
Predict(N)
for all N, E N do:
if Ali E Con,
then Pmark(Ni),Predict(isainv(Ni))
if Ni E Elem,
then Pmark(Ni),Predict(isainv(Ni))
if Ali E Seq,
then Pmark(eo.Ni), Predict(isainv(eo.NO)
if Is = NIL,
then Stop.
Activate
I &lt;— instanceof(c)
if i = then
createinst(c),Addcost, activate(c)
else
for each i E I
do concurrently:
activate(c)
Accept
if Constraints V T
Assume(Constraints), Addcost
activate(isa(c))
</figure>
<bodyText confidence="0.801461166666667">
where Ni and ei.Ni denote a node in the memory net-
work indexed by i and a j-th element of a node Ni,
respectively.
Active(N) is true iff a node or an element of a node
gets an A-Marker.
Activate(N) sends A-Markers to nodes and elements
given in the argument.
Predict(N) moves a P-Marker to the next element of
the CSC.
Predicted(N) is true if a node or an element of a node
gets a P-Marker.
Pmark(N) puts a P-Marker on a node or an element
given in the argument.
Last (N) is true iff an element is the last element of the
concept sequence.
Accept (N) creates an instance under N with links which
connect the instance to other instances.
isa(IV) returns a list of nodes and elements which are
connected to the node in the argument by abstraction
links.
isainv(N) returns a list of nodes and elements which
are daughters of a node N.
Some explanation would help understanding this al-
gorithm:
</bodyText>
<listItem confidence="0.768138">
1. Prediction.
</listItem>
<bodyText confidence="0.703958666666667">
Initially all the first elements of concept sequences
(CSC - Concept Sequence Class) are predicted by
putting P-Markers on them.
</bodyText>
<listItem confidence="0.540023">
2. Lexical Access.
</listItem>
<bodyText confidence="0.475694">
A lexical node is activated by the input word.
</bodyText>
<listItem confidence="0.580961">
3. Concept Activation.
</listItem>
<bodyText confidence="0.73677925">
An A-Marker is created and sent to the correspond-
ing CC (Concept Class) nodes. A cost is added to the
A-Marker if the CC is not C-Marked (i.e. A C-Marker
is not placed on it.).
</bodyText>
<sectionHeader confidence="0.588889" genericHeader="method">
4. Discourse Entity Identification
</sectionHeader>
<bodyText confidence="0.917051333333333">
A CI (Concept Instance) under the CC is searched
for.
If the CI exists, an A-Marker is propagated to
higher CC nodes.
Else, a CI node is created under the CC, and an
A-Marker is propagated to higher CC nodes.
</bodyText>
<sectionHeader confidence="0.819554" genericHeader="method">
5. Activation Propagation.
</sectionHeader>
<bodyText confidence="0.9940115">
An A-Marker is propagated upward in the abstrac-
tion hierarchy.
</bodyText>
<sectionHeader confidence="0.719016" genericHeader="method">
6. Sequential Prediction.
</sectionHeader>
<bodyText confidence="0.985419333333333">
When an A-Marker reaches any P-Marked node (i.e.
part of CSC), the P-Marker on the node is sent to the
next element of the concept sequence.
</bodyText>
<subsectionHeader confidence="0.455025">
7. Contextual Priming
</subsectionHeader>
<bodyText confidence="0.993707">
When an A-Marker reaches any Contextual Root
node. C-Makers are put on the contexual children
nodes designated by the root node.
</bodyText>
<sectionHeader confidence="0.501589" genericHeader="method">
8. Conceptual Relation Instantiation.
</sectionHeader>
<subsectionHeader confidence="0.627484">
When the last element of a concept sequence re-
</subsectionHeader>
<bodyText confidence="0.998482125">
cieves an A-Marker, Constraints (world and dis-
course knowledge) are checked for.
A CSI is created under the CSC with packaging
links to each CI. This process is called concept refine-
ment. See [19].
The memory network is modified by performing
inferences stored in the root CSC which had the ac-
cepted CSC attached to it.
</bodyText>
<sectionHeader confidence="0.678575" genericHeader="method">
9. Activation Propagation
</sectionHeader>
<bodyText confidence="0.834234">
A-Marker is propagated from the CSC to higher
nodes.
</bodyText>
<subsectionHeader confidence="0.998164">
3.2 Memory Network Modification
</subsectionHeader>
<bodyText confidence="0.998508">
Several different incidents trigger the modification of
the memory network during parsing:
</bodyText>
<listItem confidence="0.84055175">
• An individual concept is instantiated (i.e. an in-
stance is created) under a CC when the CC re-
ceives an A-Marker and a CI (an instance that
- 74 -
</listItem>
<bodyText confidence="0.99977675">
was created by preceding utterances) is not exis-
tent. This instantiation is a creation of a specific
discourse entity which may be used as an existent
instance in the subsequent recognitions.
</bodyText>
<listItem confidence="0.874832545454545">
• A concept sequence instance is created under the
accepted CSC. In other words, if a whole concept
sequence is accepted, we create an instance of
the sequence instantiating it with the specific CIs
that were created by (or identified with) the spe-
cific lexical inputs. This newly created instance
is linked to the accepted CSC with a instance re-
lation link and to the instances of the elements of
the concept sequences by links labelled with their
roles given in the CSC.
• Links are created or removed in the CSI creation
</listItem>
<bodyText confidence="0.923055230769231">
phase as a result of invoking inferences based on
the knowledge attached to CSCs. For example,
when the parser accepts the sentence I went to
the UMIST, an instance of is created under the
CC representing I. Next, a CSI is created under
PTRANS. Since VIRANS entails that the agent
is at the location, a location link must be created
between the discourse entities I and UMIST. Such
revision of the memory network is conducted by
invoking knowledge attached to each CSC.
Since modification of any part of the memory net-
work requires some workload, certain costs are added
to analyses which require such modifications.
</bodyText>
<sectionHeader confidence="0.9907685" genericHeader="method">
4 Cost-based Approach to the
Ambiguity Resolution
</sectionHeader>
<bodyText confidence="0.997501">
Ambiguity resolution in DmTRANs Puis is based on
the calculation of the cost of each parse. Costs are
attached to each parse during the parse process.
Costs are attached when:
</bodyText>
<listItem confidence="0.9861834">
1. A CC with insufficient priming is activated,
2. A CI is created under CC, and
3. Constraints imposed on CSC are not satisfied ini-
tially and links are created or removed to satisfy
the constraint.
</listItem>
<bodyText confidence="0.990435333333333">
Costs are attached to A-Markers when these oper-
ations are taken because these operations modify the
memory network and, hence, workloads are required.
Cost information is then carried upward by A-Markers.
The parse with the least cost will be chosen.
The cost of each hypothesis are calculated by:
</bodyText>
<note confidence="0.417932333333333">
ns
= E Cy .4- E constraintik + biasi
kss0
</note>
<bodyText confidence="0.999969481481482">
where Ci is a cost of the i-th hypothesis, cu is a cost
carried by an A-Marker activating the j-th element of
the CSC for the i-th hypothesis, constraintik is a cost
of assuming k-th constraint of the i-th hypothesis, and
biasi represents lexical preference of the CSC for the
i-th hypothesis. This cost is assigned to each CSC and
the value of Ci is passed up by A-Markers if higher-
level processing is performed. At higher levels, each
Cy may be a result of the sum of costs at lower-levels.
It should be noted that this equation is very simi-
lar to the activation function of most neural networks
except for the fact our equation is a simple linear equa-
tion which does not have threshold value. In fact, if
we only assume the addition of cost by priming at the
lexical-level, our mechanism of ambiguity resolution
would behave much like connectionist models with-
out inhibition among syntactic nodes and excitation
links from syntax to lexicon2. However, the major
difference between our approach and the connectionist
approach is the addition of costs for instance creation
and constraint satisfaction. We will show that these
factors are especially important in resolving structural
ambiguities.
The following subsections describe three mecha-
nisms that play a role in ambiguity resolution. How-
ever, we do not claim that these are the only mecha-
nisms involved in the examples which follow3.
</bodyText>
<subsectionHeader confidence="0.99402">
4.1 Contextual Priming
</subsectionHeader>
<bodyText confidence="0.999971642857143">
In our system, some CC nodes designated as Contex-
tual Root Nodes have a list of thematically relevant
nodes. C-Markers are sent to these nodes as soon as
a Contextual Root Node is activated. Thus each sen-
tence and/or each word might influence the interpre-
tation of following sentences or words. When a node
with C-Marker is activated by receiving an A-Marker,
the activation will be propagated with no cost. Thus, a
parse using such nodes would have no cost. However,
when a node without a C-Marker is activated, a small
cost is attached to the interpretation using that node.
In [19] the discussion of C-Marker propagation con-
centrated on the resolution of word-level ambiguities.
However, C-Markers are also propagated to conceptual
</bodyText>
<footnote confidence="0.6110305">
2We have not incorporated these factors primarily because struc-
tured P-Markers can play the role of top-down priming; however,
we may be incorporating these factors in the future.
3For example, in one implementation of DMTRANS, we are us-
ing time-delayed decaying activations which resolve ambiguity even
when two CI nodes are concurrently active.
</footnote>
<equation confidence="0.251838">
- 75 -
</equation>
<bodyText confidence="0.98141665625">
class nodes, which can represent word-level, phrasal,
or sentential knowledge. Therefore, C-Markers can
be used for resolving phrasal-level and sentential-level
ambiguities such as structural ambiguities. For exam-
ple, atama ga itai literally means, `(my) head hurts.&apos;
This normally is identified with the concept sequences
associated with the *have-a-symptom concept class
node, but if the preceding sentence is asita yakuinkai
da (&amp;quot;There is a board of directors meeting tomorrow&apos;),
the *have-a-problem concept class node must be ac-
tivated instead. Contextual priming attained by C-
Markers can also help resolve structural ambiguity in
sentences like did you read about the problem with
the students? The cost of each parse will be deter-
mined by whether reading with students or problems
with students is contextually activated. (Of course,
many other factors are involved in resolving this type
of ambiguity.)
Our model can incorporate either C-Markers or a
connectionist-type competitive activation and inhibi-
tion scheme for priming. In the current implementa-
tion, we use C-Markers for priming simply because C-
Marker propagation is computationally less-expensive
than connectionist-type competitive activation and in-
hibition schemes4. Although connectionist approaches
can resolve certain types of lexical ambiguity, they
are computationally expensive unless we have mas-
sively parallel computers. C-Markers are a resonable
compromise because they are sent to semantically rel-
evant concept nodes to attain contextual priming with-
out computationally expensive competitive activation
and inhibition methods.
</bodyText>
<subsectionHeader confidence="0.990543">
4.2 Reference to the Discourse Entity
</subsectionHeader>
<bodyText confidence="0.8360158">
When a lexical node activates any CC node, a CI node
under the CC node is searched for ([19], [21]). This
activity models reference to an already established dis-
course entity [27] in the hearer&apos;s mind. If such a CI
node exists, the reference succeeds and this parse will
be attached with no cost. However, if no such instance
is found, reference failure results. If this happens, an
instantiation activity is performed creating a new in-
stance with certain costs. As a result, a parse using
newly created instance node will be attached with some
cost.
For example, if a preceding discourse contained a
reference to a thesis, a CI node such as THEsts005
would have been created. Now if a new input sen-
tence contains the word paper, CC nodes for THE-
411tis does not mean that our model can not incorporate a con-
nectionist model. The choice of C-Markers over the connectionist
approach is mostly due to computational cost. As we will describe
later, our model is capable of incorporating a connectionist approach.
sts and SHEET-OF-PAPER are activated. This causes
search for CI nodes under both CC nodes. Since th
CI node mEsisoo5 will be found, the reading wher
paper means thesis will not acquire a cost. Howevei
assuming that there is not a CI node corresponding t
a sheet of paper, we will need to create a new one fc
this reading, thus incurring a cost.
We can also use reference to discourse entities t
resolve structural ambiguities. In the sentence W
sent her papers, if the preceding discourse mentione
Yoshiko&apos;s papers, a specific CI node such as Yostincc
PAPER003 representing Yoshiko&apos;s papers would hay
been created. Therefore, during the processing of W
sent her papers, the reading which means we sent pa
pers to her needs to create a CI node representing pa
pers that we sent, incurring some cost for creating tha
instance node. On the other hand, the reading whic
means we sent Yoshiko&apos;s papers does not need to cre
ate an instance (because it was already created) so it i
costless. Also, the reading that uses paper as a she
of paper is costly as we have demonstrated above.
</bodyText>
<subsectionHeader confidence="0.997108">
4.3 Constraints
</subsectionHeader>
<bodyText confidence="0.874058675">
Constraints are attached to each CSC. These cos
straints play important roles during disambiguatior
Constraints define relations between instances whe
sentences or sentence fragments are accepted. Whe
a constraint is satisfied, the parse is regarded as plau
sible. On the other hand, the parse is less plausibl
when the constraint is unsatisfied. Whereas traditiont
parsers simply reject a parse which does not satisfy
given constraint, DMTRANS PLUS, builds or remove
links between nodes forcing them to satisfy constraint
A parse with such forced constraints will record a
increased cost and will be less preferred than parse
without attached costs.
The following example illustrates how this schem
resolves an ambiguity. As an initial setting we a;
sume that the memory network has instances of &apos;mar
(MANI.) and &apos;hand-gun&apos; (HAND-GUN1) connecte
with a POSSES relation (i.e. link). The input utteranc
is: &amp;quot;Mary picked up an Uzzi. Mary shot the man wit
the hand-gun.&amp;quot; The second sentence is ambiguous i
isolation and it is also ambiguious if it is not know
that an Uzzi is a machine gun. However, when it
preceedecl by the first sentence and if the hearer know
that Uzzi is a machine gun, the ambiguity is drasticall
reduced. DMTRANS PLUS hypothesizes and model
this disambiguation activity utilizing knowledge abot
world through the cost recording mechanism describe
above.
During the processing of the first sentence, DM
TRANS PLUS creates instances of &apos;Mary&apos; and &apos;Uzzi
- 76 -
and records them as active instances in memory (i.e.,
MARY1 and UZZI1 are created). In addition, a
link between MARY1 and UZZI1 is created with the
POSSES relation label. This link creation is invoked by
triggering side-effects (i.e., inferences) stored in the
CSC representing the action of `MARY1 picking up
the UZZI1&apos;. We omit the details of marker passing
(for A-, P-, and C-Markers) since it is described detail
elsewhere (particulary in [19]).
</bodyText>
<tableCaption confidence="0.754007857142857">
When the second sentence comes in, an instance
MARY I already exists and, therefore, no cost is
charged for parsing `Mary&apos;5. However, we now have
three relevant concept sequences (CSC&apos;s6):
CSC1: (&lt;agent&gt; &lt;shoot&gt; &lt;object&gt;)
CSC2: (&lt;agent&gt; &lt;shoot&gt; &lt;object&gt; &lt;with&gt; &lt;instrument&gt;)
CSC3: (&lt;person&gt; &lt;with&gt; &lt;instrument&gt;)
</tableCaption>
<bodyText confidence="0.993593787878788">
These sequences are activated when concepts in
the sequences are activated in order from below in
the abstraction hierarchy. When the &amp;quot;man&amp;quot; comes in,
recognition of CSC3:(&lt;person&gt; &lt;with&gt; &lt;instrument&gt;)
starts. When the whole sentence is received, we have
two top-level CSCs (i.e., CSC1 and CSC2) accepted
(all elements of the sequences recognized). The ac-
ceptance of CSC1 is performed through first accepting
CSC3 and then substituting CSC3 for &lt;object&gt;.
When the concept sequences are satisfied, their con-
straints are tested. A constraint for CSC2 is (POSSES
&lt;agent&gt; &lt;instrument&gt;) and a constraint for CSC3 (and
CSC1, which uses CSC3) is (POSSES &lt;person&gt; &lt;in-
strument&gt;). Since &apos;MARY! POSSESS HAND-GUN!&apos;
now has to be satisfied and there is no instance of this
in memory, we must create a POSSESS link between
MARY1 and HAND-GUN1. A certain cost, say 10,
is associated with the creation of this link. On the
other hand, MANI POSSESS HAND-GUN1 is known
in memory because of an earlier sentence. As a result,
CSC3 is instantiated with no cost and an A-Marker
from CSC3 is propagated upward to CSC1 with no
cost. Thus, the cost of instantiating CSC1 is 0 and
the cost of instantiating CSC2 is 10. This way, the
inteipretation with CSC1 is favored by our system.
50f course, &apos;Mary&apos; can be &apos;She&apos;. The method for handling this
type of pronoun reference was already reported in [19] and we do
not discuss it here.
6As we can see from this example of CSC&apos;s, a concept sequence
can be normally regarded as a subcategorization list of a VP head.
However, concept sequences are not restricted to such lists and are
actually often at higher levels of abstraction representing MOP-like
sequences.
</bodyText>
<sectionHeader confidence="0.99448" genericHeader="method">
5 Discussion:
</sectionHeader>
<subsectionHeader confidence="0.999539">
5.1 Global Minima
</subsectionHeader>
<bodyText confidence="0.999618782608696">
The correct hypothesis in our model is the hypothe-
sis with the least cost. This corresponds to the notion
of global minima in most connectionist literature. On
other hand, the hypothesis which has the least cost
within a local scope but does not have the least cost
when it is combined with global context is a local
minimum. The goal of our model is to find a global
minimum hypothesis in a given context. This idea is
advantageous for discourse processing because a parse
which may not be preferred in a local context may
yeild a least cost hypothesis in the global context. Sim-
ilarly, the least costing parse may turn out to be costly
at the end of processing due to some contexual infer-
ence triggered by some higher context.
One advantage of our system is that it is possible to
define global and local minima using massively paral-
lel marking passing, which is computationally efficient
and is more powerful in high-level processing involv-
ing variable-binding, structure building, and constraint
propagations7 than neural network models. In addi-
tion, our model is suitable for massively parallel archi-
tectures which are now being researched by hardware
designers as next generation machiness.
</bodyText>
<subsectionHeader confidence="0.99609">
5.2 Psycholinguistic Relevance of the
Model
</subsectionHeader>
<bodyText confidence="0.9789911">
The phenomenon of lexical ambiguity has been studied
by many psycholinguistic researchers including [13],
[3], and [17]. These studies have identified contextual
priming as an important factor in ambiguity resolution.
One psycholinguistic study that is particularly
relevent to DMTRANS PLUS is Crain and Steedman
[4], which argues for the principle of referential suc-
cess. Their experiments demonstrate that people prefer
the interpretation which is most plausible and accesses
previously defined discourse entities. This psycholin-
guistic claim and experimental result was incorporated
in our model by adding costs for instance creation and
constraint satisfaction.
Another study relevent to our model is be the lex-
ical preference theory by Ford, Bresnan and Kaplan
[5]. Lexical preference theory assumes a preference
order among lexical entries of verbs which differ in
subcategorization for prepositional phrases. This type
of preference was incorporated as the bias term in our
cost equation.
</bodyText>
<footnote confidence="0.751909666666667">
7Refer to [22] for details in this direction.
8See [23] and [9] for discussion.
- 77 -
</footnote>
<bodyText confidence="0.9999162">
Although we have presented a basic mechanism to
incorporate these psycholinguistic theories, well con-
trolled psycholinguistic experiments will be necessary
to set values of each constant and to validate our model
psycholinguistically.
</bodyText>
<subsectionHeader confidence="0.986465">
5.3 Reverse Cost
</subsectionHeader>
<bodyText confidence="0.999907571428572">
In our example in the previous section, if the first
sentence was Mary picked an S&amp;W where the hearer
knows that an S&amp;W is a hand-gun, then an instance
of &apos;MARY POSSES HAND-GUN1&apos; is asserted as true
in the first sentence and no cost is incurred in the in-
terpretation of the second sentence using CSC2. This
means that the cost for both PP-attachements in Mary
shot the man with the handgun are the same (no cost
in either cases) and the sentence remains ambiguous.
This seems contrary to the fact that in Mary picked a
S&amp;W. She shot the man with the hand-gun, that natural
interpretation (given that the hearer knows S&amp;W is a
hand-gun) seems to be that it was Mary that had the
hand-gun not the man. Since our costs are only neg-
atively charged, the fact that `MARY1 POSSES S&amp;W&apos;
is recorded in previous sentence does not help the dis-
ambiguation of the second sentence.
In order to resolve ambiguities such as this one
which remain after our cost-assignment procedure has
applies, we are currently working on a reverse cost
charge scheme. This scheme will retroactively in-
crease or decrease the cost of parses based on other
evidence from the discourse context. For example, the
discourse context might contain information that would
make it more plausible or less plausible for Mary to use
a handgun. We also plan to implement time-sensitive
diminishing levels of charges to prefer facts recognized
in later utterances.
</bodyText>
<subsectionHeader confidence="0.996024">
5.4 Incorporation of Connectionist Model
</subsectionHeader>
<bodyText confidence="0.99999128">
As already mentioned, our model can incorporate
connectionist models of ambiguity resolution. In a
connectionist network activation of one node trig-
gers interactive excitation and inhibition among nodes.
Nodes which get more activated will be primed more
than others. When a parse uses these more active
nodes, no cost will be added to the hypothesis. On
the other hand, hypotheses using less activated nodes
should be assigned higher costs. There is nothing
to prevent our model from integrating this idea, es-
pecially for lexical ambiguity resolution. The only
reason that we do not implement a connectionist ap-
proach at present is that the computational cost will
be emonomous on current computers. Readers should
also be aware that DMA is a guided marker passing al-
gorithm in which markers are passed only along certain
links whereas connectionist models allow spreading
of activation and inhibition virtually to any connected
nodes. We hope to integrate DMA and connectionist
models on a real massively parallel computer and wish
to demonstrate real-time translation. One other possi-
bility is to integrate with a connectionist network for
speech recognition9. We expect, by integrating with
connectionist networks, to develop a uniform model
of cost-based processing.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999946">
We have described the ambiguity resolution scheme
in DMTRANS PLUS. Perhaps the central contribution
of this paper to the field is that we have shown a
method of ambiguity resolution in a massively paral-
lel marker passing paradigm. Cost evaluation for each
parse through (1) reference and instance creation, (2)
constraint satisfaction and (3) C-Markers are combined
into the marker passing model. We have also dicussed
on the possibility to merge our model with connec-
tionist models where they are applicable. The guiding
principle of our model, that parsing is a physical pro-
cess of memory modification, was useful in deriving
mechanisms described in this paper. We expect further
investigation along these lines to provide us insights
in many aspects of natural language processing.
</bodyText>
<sectionHeader confidence="0.994385" genericHeader="method">
Acknowldgements
</sectionHeader>
<bodyText confidence="0.99961">
The authors would like to thank members of the Center
for Machine Translation for fruitful discussions. We
would especially like to thank Masaru Tomita, Hitoshi
Iida, Jaime Carbonell, and Jay McClelland for their
encouragement.
</bodyText>
<sectionHeader confidence="0.967121" genericHeader="method">
Appendix: Implementation
</sectionHeader>
<bodyText confidence="0.974629769230769">
DMTRANS PLUS is implemented on IBM-RT&apos;s using
both CMU-COMMONLISP and MULTILISP running on
the Mach distributed operating system at CMU. Algo-
rithms for structural disambiguation using cost attache-
ment were added along with some other house-keeping
functions to the original DMTRANS to implement DM-
TRANS PLUS. All capacities reported in this paper have
been implemented except the schemes mentioned in
the sections 5.3 and 5.4 (i.e., negative costs, integra-
tion of connectionist models).
9Augmentation of the cost-based model to the phonological level
has already been implemented in DM.
- 78 -
</bodyText>
<sectionHeader confidence="0.994104" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995185871794872">
[1] Becker, J.D. The phrasal lexicon. In &apos;Theoretical Issues in
Natural Language Processing&apos;, 1975.
[2] Boguraev, B. K., et al., Three Papers on Parsing, Technical
Report 17, Computer Laboratory, University of Cambridge,
1982.
[3] Cottrell, G., A Model of Lexical Access of Ambiguous Words, in
&apos;Lexical Ambiguity Resolution&apos;, S. Small, et. al. (eds), Morgan
Kaufmann Publishers, 1988.
[4] Crain, S. and Steedman, M., On not being led up with guarden
path: the use of context by the psychological syntax processor,
in &apos;Natural Language Parsing&apos;, 1985.
[5] Ford, M., Bresnan, J. and Kaplan, R., A Competence-Based
Theory of Syntactic Closure, in &apos;The Mental Representation of
Grammatical Relations&apos;, 1981.
[6] Grosz, B. and Sidner, C. L, The Structure of Discourse Struc-
ture, CSLI Report No. CSLI-85-39, 1985.
[7] Hays, P. J., On semantic nets, frames and associations, in
&apos;Proceedings of IJCAI-77, 1977.
[8] Hirst, G., Semantic Interpretation and the Resolution of Am-
biguity, Cambridge University Press, 1987.
[9] Kitano, H., Multilingual Information Retrieval Mechanism us-
ing VLSI, in &apos;Proceedings of RIAO-88&apos;, 1988.
[10] Kitano, H., et. at, Manuscript An Integrated Discourse Under-
standing Model for an Interpreting Telephony under the Direct
Memory Access Paradigm, Carnegie Mellon University, 1989.
[111 Marcus, M. P., A theory of syntactic recognition for natural
language, MIT Press, 1980.
[12] Norvig, P., Unified Theory of Inference for Text Understading,
Ph.D. Dissertation, University of California, Berkeley, 1987.
[13] Prather, P. and Swinney, D., Lexical Processing and Ambigu-
ity Resolution: An Autonomous Processing in an Interactive
Box, in &apos;Lexical Ambiguity Resolution&apos;, S. Small, et. al. (Eds),
Morgan Kaufnuum Publishers, 1988.
[14] Riesbedc, C. and Martin, C., Direct Memory Access Parsing,
YALEU/DCS/RR 354, 1985.
[15] Selman, B. and Hirst, G., Parsing as an Energy Minimiza-
tion Problem, in Genetic Algorithms and Simulated Annealing,
Davis, L (Ed.), Morgan Kaufmann Publishers, CA, 1987.
[16] Schank, R., Dynamic Memory: A theory of learning in com-
puters and people. Cambridge University Press. 1982
[17] Small, S., et. at (Eds.) Lexical Ambiguity Resolution, Morgan
Kaufmann Publishers, Inc., CA, 1988.
[18] Small, S., et. a. Toward Connectionist Parsing, in Proceedings
of AAAI-82, 1982.
[19] Tomabechi, H., Direct Memory Access Translation, in &apos;Pro-
ceedings of the IJCAI-88&apos;, 1987.
[20] Tomabechi, H. and Tombs, M., The Integration of Unification-
based Syntax/Semantics and Memory-based Pragmatics for
Real-Time Understanding of Noisy Continuous Speech Input,
in &apos;Proceedings of the AAAI-88&apos;, 1988.
[21] Tomabechi, H. and Tomita, M., Application of the Direct
Memory Access paradigm to natural language interfaces to
knowledge-based systems, in &apos;Proceedings of the COLING-
88&apos;, 1988.
[22] Tomabechi, H. and Tomita, M., Manuscript. MASSIVELY
PARALLEL CONSTRAIIVT PROPAGATION: Parsing with
Unification-based Grammar without Unification. Carnegie
Mellon University.
[23] Tomabechi, H., Mitamura, T., and Tornita, M., DIRECT MEM-
ORY ACCESS TRANSLATION FOR SPEECH INPUT: A Mas-
sively Parallel Network of EpisodicIThematic and Phonolog-
ical Memory, in &apos;Proceedings of the International Confer-
ence on Fifth Generation Computer Systems 1988&apos; (FGCS&apos;88),
1988.
[24] Touretzlcy, D. S., Connectionism and PP Attachment, in &apos;Pro-
ceedings of the 1988 Cotmectionist Models Summer School,
1988.
[25] Waltz, D. L and Pollack, J. B., Massively Parallel Parsing: A
Strongly Interactive Model of Natural Language Interpretation.
Cognitive Science 9(1): 51-74, 1985.
[26] Wanner, E., The ATN and the Sausage Machine: Which one
is baloney? Cognition, 8(2), June, 1980.
[27] Webber, B. L, So what can we talk about now?, in &apos;Com-
putational Models of Discourse&apos;, (Eds. M. Brady and R.C.
Berwick), MIT Press, 1983.
[28] Wilks, Y. A., Huang, X. and Fess, D., Syntax, preference and
right attachment, in &apos;Proceedings of the IJCAI-85, 1985.
- 79 -
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.195870">
<title confidence="0.999368">Ambiguity Resolution in the DMTRANS PLUS</title>
<author confidence="0.997949">Hiroaki Kitano</author>
<author confidence="0.997949">Hideto Tomabechi</author>
<author confidence="0.997949">Lori Levin</author>
<affiliation confidence="0.9987055">Center for Machine Translation Carnegie Mellon University</affiliation>
<address confidence="0.851682">Pittsburgh, PA 15213 U.S.A.</address>
<abstract confidence="0.939820296296296">access (DMA) paradigm of natural language processing. Under the DMA paradigm, the mental state of the hearer is modelled by a massively parallel network representing memory. Parsing is performed by passing markers in the memory network. In our model, the meaning of a sentence is viewed as modifications made to the memory network. The meaning of a sentence in our model is definable as the difference in the memory network before and after understanding the sentence. 2 Limitations of Current Methods of Ambiguity Resolution Abstract We present a cost-based (or energy-based) model of disambiguation. When a sentence is ambiguous, a parse with the least cost is chosen from among multiple hypotheses. Each hypothesis is assigned a cost which is added when: (1) a new instance is created to satisfy reference success, (2) links between instances are created or removed to satisfy constraints on concept sequences, and (3) a concept node with insufficient priming is used for further processing. This method of ambiguity resolution is implemented in which is a second generation bi-directional English/Japanese machine translation system based on a massively parallel spreading activation paradigm developed at the Center for Machine Translation at Carnegie Mellon University.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J D Becker</author>
</authors>
<title>The phrasal lexicon. In &apos;Theoretical Issues in Natural Language Processing&apos;,</title>
<date>1975</date>
<contexts>
<context position="6557" citStr="[1]" startWordPosition="986" endWordPosition="986">ntities stores knowledge both declaratively and procedurely as described in [19] and [21]. Lexical entries are represented as lexical nodes which are a kind of CC. Phoneme sequences are used only for DMTRANS PLUS, the speech-input version of DMTRANS PLUS. CSCs represent sequences of concepts such as phoneme sequences (i.e. &lt;/lc//a//i//g//i/&gt;), concept sequences (i.e. &lt;*Conference *Goal-Role *Attend *Want&gt;), and plan sequences (i.e. &lt;*Declare-WantAttend *Listen-Instruction&gt;). The linguistic knowledge represented as CSCs can be low-level surface specific patterns such as phrasal lexicon entries [1] or material at higher levels of abstration such as in MOP&apos;s [16]. However, CSCs should not be confused with &apos;discourse segments&apos; [6]. In our model, information represented in discourse segments are distributively incorporated in the memory network. During sentence processing we create concept instances (CI) correpsonding to CCs and concept sequence instances (CSI) corresponding to CSCs. This is a substantial improvement over past DMA research. Lack of instance creation and reference in past research was a major obstacle to seriously modelling discourse phenomena. CIs and CSIs are connected th</context>
</contexts>
<marker>[1]</marker>
<rawString>Becker, J.D. The phrasal lexicon. In &apos;Theoretical Issues in Natural Language Processing&apos;, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B K Boguraev</author>
</authors>
<title>Three Papers on Parsing,</title>
<date>1982</date>
<tech>Technical Report 17,</tech>
<institution>Computer Laboratory, University of Cambridge,</institution>
<contexts>
<context position="2934" citStr="[2]" startWordPosition="458" endWordPosition="458">ng philosophy of the model is to view parsing as a dynamic physical process in which one trajectory is taken from among many other possible paths. Thus our notion of the cost of the hypothesis is a representation of the workload required to take the path representing the hypothesis. One other important idea is that our model employs the direct memory *E-mail address is hiroalci@aalcs.cmu.edu. Also with NEC Corporation. Traditional syntactic parsers have been using attachment preferences and local syntactic and semantic constraints for resolving lexical and structural ambiguities. ([171, [28], [2], [7], [26], [II], [5]) However, these methods cannot select one interpretation from several plausible interpretations because they do not incorporate the discourse context of the sentences being parsed ([8], [4]). Connectionist-type approaches as seen in [18], [25], and [8] essentially stick to semantic restrictions and associations. However, [18], [25], [24] only provide local interactions, omitting interaction with context. Moreover, difficulties regarding variable-binding and embedded sentences should be noticed. In [8], world knowledge is used through testing referential success and other</context>
</contexts>
<marker>[2]</marker>
<rawString>Boguraev, B. K., et al., Three Papers on Parsing, Technical Report 17, Computer Laboratory, University of Cambridge, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Cottrell</author>
</authors>
<title>A Model of Lexical Access of Ambiguous Words, in &apos;Lexical Ambiguity Resolution&apos;,</title>
<date>1988</date>
<publisher>Morgan Kaufmann Publishers,</publisher>
<contexts>
<context position="25733" citStr="[3]" startWordPosition="4142" endWordPosition="4142">ur system is that it is possible to define global and local minima using massively parallel marking passing, which is computationally efficient and is more powerful in high-level processing involving variable-binding, structure building, and constraint propagations7 than neural network models. In addition, our model is suitable for massively parallel architectures which are now being researched by hardware designers as next generation machiness. 5.2 Psycholinguistic Relevance of the Model The phenomenon of lexical ambiguity has been studied by many psycholinguistic researchers including [13], [3], and [17]. These studies have identified contextual priming as an important factor in ambiguity resolution. One psycholinguistic study that is particularly relevent to DMTRANS PLUS is Crain and Steedman [4], which argues for the principle of referential success. Their experiments demonstrate that people prefer the interpretation which is most plausible and accesses previously defined discourse entities. This psycholinguistic claim and experimental result was incorporated in our model by adding costs for instance creation and constraint satisfaction. Another study relevent to our model is be t</context>
</contexts>
<marker>[3]</marker>
<rawString>Cottrell, G., A Model of Lexical Access of Ambiguous Words, in &apos;Lexical Ambiguity Resolution&apos;, S. Small, et. al. (eds), Morgan Kaufmann Publishers, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Crain</author>
<author>M Steedman</author>
</authors>
<title>On not being led up with guarden path: the use of context by the psychological syntax processor, in &apos;Natural Language Parsing&apos;,</title>
<date>1985</date>
<contexts>
<context position="3146" citStr="[4]" startWordPosition="489" endWordPosition="489"> of the workload required to take the path representing the hypothesis. One other important idea is that our model employs the direct memory *E-mail address is hiroalci@aalcs.cmu.edu. Also with NEC Corporation. Traditional syntactic parsers have been using attachment preferences and local syntactic and semantic constraints for resolving lexical and structural ambiguities. ([171, [28], [2], [7], [26], [II], [5]) However, these methods cannot select one interpretation from several plausible interpretations because they do not incorporate the discourse context of the sentences being parsed ([8], [4]). Connectionist-type approaches as seen in [18], [25], and [8] essentially stick to semantic restrictions and associations. However, [18], [25], [24] only provide local interactions, omitting interaction with context. Moreover, difficulties regarding variable-binding and embedded sentences should be noticed. In [8], world knowledge is used through testing referential success and other sequential tests. However, this method does not provide a uniform model of parsing: lexical ambiguities are resolved by marker passing and structural disambiguations are resolved by applying separate sequential </context>
<context position="25940" citStr="[4]" startWordPosition="4172" endWordPosition="4172">e-binding, structure building, and constraint propagations7 than neural network models. In addition, our model is suitable for massively parallel architectures which are now being researched by hardware designers as next generation machiness. 5.2 Psycholinguistic Relevance of the Model The phenomenon of lexical ambiguity has been studied by many psycholinguistic researchers including [13], [3], and [17]. These studies have identified contextual priming as an important factor in ambiguity resolution. One psycholinguistic study that is particularly relevent to DMTRANS PLUS is Crain and Steedman [4], which argues for the principle of referential success. Their experiments demonstrate that people prefer the interpretation which is most plausible and accesses previously defined discourse entities. This psycholinguistic claim and experimental result was incorporated in our model by adding costs for instance creation and constraint satisfaction. Another study relevent to our model is be the lexical preference theory by Ford, Bresnan and Kaplan [5]. Lexical preference theory assumes a preference order among lexical entries of verbs which differ in subcategorization for prepositional phrases. </context>
</contexts>
<marker>[4]</marker>
<rawString>Crain, S. and Steedman, M., On not being led up with guarden path: the use of context by the psychological syntax processor, in &apos;Natural Language Parsing&apos;, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ford</author>
<author>J Bresnan</author>
<author>R Kaplan</author>
</authors>
<title>A Competence-Based Theory of Syntactic Closure, in &apos;The Mental Representation of Grammatical Relations&apos;,</title>
<date>1981</date>
<contexts>
<context position="2956" citStr="[5]" startWordPosition="462" endWordPosition="462">odel is to view parsing as a dynamic physical process in which one trajectory is taken from among many other possible paths. Thus our notion of the cost of the hypothesis is a representation of the workload required to take the path representing the hypothesis. One other important idea is that our model employs the direct memory *E-mail address is hiroalci@aalcs.cmu.edu. Also with NEC Corporation. Traditional syntactic parsers have been using attachment preferences and local syntactic and semantic constraints for resolving lexical and structural ambiguities. ([171, [28], [2], [7], [26], [II], [5]) However, these methods cannot select one interpretation from several plausible interpretations because they do not incorporate the discourse context of the sentences being parsed ([8], [4]). Connectionist-type approaches as seen in [18], [25], and [8] essentially stick to semantic restrictions and associations. However, [18], [25], [24] only provide local interactions, omitting interaction with context. Moreover, difficulties regarding variable-binding and embedded sentences should be noticed. In [8], world knowledge is used through testing referential success and other sequential tests. How</context>
<context position="26393" citStr="[5]" startWordPosition="4239" endWordPosition="4239">ing as an important factor in ambiguity resolution. One psycholinguistic study that is particularly relevent to DMTRANS PLUS is Crain and Steedman [4], which argues for the principle of referential success. Their experiments demonstrate that people prefer the interpretation which is most plausible and accesses previously defined discourse entities. This psycholinguistic claim and experimental result was incorporated in our model by adding costs for instance creation and constraint satisfaction. Another study relevent to our model is be the lexical preference theory by Ford, Bresnan and Kaplan [5]. Lexical preference theory assumes a preference order among lexical entries of verbs which differ in subcategorization for prepositional phrases. This type of preference was incorporated as the bias term in our cost equation. 7Refer to [22] for details in this direction. 8See [23] and [9] for discussion. - 77 - Although we have presented a basic mechanism to incorporate these psycholinguistic theories, well controlled psycholinguistic experiments will be necessary to set values of each constant and to validate our model psycholinguistically. 5.3 Reverse Cost In our example in the previous sec</context>
</contexts>
<marker>[5]</marker>
<rawString>Ford, M., Bresnan, J. and Kaplan, R., A Competence-Based Theory of Syntactic Closure, in &apos;The Mental Representation of Grammatical Relations&apos;, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>C L Sidner</author>
</authors>
<title>The Structure of Discourse Structure,</title>
<date>1985</date>
<tech>CSLI Report No. CSLI-85-39,</tech>
<contexts>
<context position="6690" citStr="[6]" startWordPosition="1008" endWordPosition="1008"> nodes which are a kind of CC. Phoneme sequences are used only for DMTRANS PLUS, the speech-input version of DMTRANS PLUS. CSCs represent sequences of concepts such as phoneme sequences (i.e. &lt;/lc//a//i//g//i/&gt;), concept sequences (i.e. &lt;*Conference *Goal-Role *Attend *Want&gt;), and plan sequences (i.e. &lt;*Declare-WantAttend *Listen-Instruction&gt;). The linguistic knowledge represented as CSCs can be low-level surface specific patterns such as phrasal lexicon entries [1] or material at higher levels of abstration such as in MOP&apos;s [16]. However, CSCs should not be confused with &apos;discourse segments&apos; [6]. In our model, information represented in discourse segments are distributively incorporated in the memory network. During sentence processing we create concept instances (CI) correpsonding to CCs and concept sequence instances (CSI) corresponding to CSCs. This is a substantial improvement over past DMA research. Lack of instance creation and reference in past research was a major obstacle to seriously modelling discourse phenomena. CIs and CSIs are connected through several types of links. A guided marker passing scheme is employed for inference on the memory network following methods adopte</context>
</contexts>
<marker>[6]</marker>
<rawString>Grosz, B. and Sidner, C. L, The Structure of Discourse Structure, CSLI Report No. CSLI-85-39, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hays</author>
</authors>
<title>On semantic nets, frames and associations,</title>
<date>1977</date>
<booktitle>in &apos;Proceedings of IJCAI-77,</booktitle>
<contexts>
<context position="2939" citStr="[7]" startWordPosition="459" endWordPosition="459">ilosophy of the model is to view parsing as a dynamic physical process in which one trajectory is taken from among many other possible paths. Thus our notion of the cost of the hypothesis is a representation of the workload required to take the path representing the hypothesis. One other important idea is that our model employs the direct memory *E-mail address is hiroalci@aalcs.cmu.edu. Also with NEC Corporation. Traditional syntactic parsers have been using attachment preferences and local syntactic and semantic constraints for resolving lexical and structural ambiguities. ([171, [28], [2], [7], [26], [II], [5]) However, these methods cannot select one interpretation from several plausible interpretations because they do not incorporate the discourse context of the sentences being parsed ([8], [4]). Connectionist-type approaches as seen in [18], [25], and [8] essentially stick to semantic restrictions and associations. However, [18], [25], [24] only provide local interactions, omitting interaction with context. Moreover, difficulties regarding variable-binding and embedded sentences should be noticed. In [8], world knowledge is used through testing referential success and other sequ</context>
</contexts>
<marker>[7]</marker>
<rawString>Hays, P. J., On semantic nets, frames and associations, in &apos;Proceedings of IJCAI-77, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
</authors>
<title>Semantic Interpretation and the Resolution of Ambiguity,</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="3141" citStr="[8]" startWordPosition="488" endWordPosition="488">ation of the workload required to take the path representing the hypothesis. One other important idea is that our model employs the direct memory *E-mail address is hiroalci@aalcs.cmu.edu. Also with NEC Corporation. Traditional syntactic parsers have been using attachment preferences and local syntactic and semantic constraints for resolving lexical and structural ambiguities. ([171, [28], [2], [7], [26], [II], [5]) However, these methods cannot select one interpretation from several plausible interpretations because they do not incorporate the discourse context of the sentences being parsed ([8], [4]). Connectionist-type approaches as seen in [18], [25], and [8] essentially stick to semantic restrictions and associations. However, [18], [25], [24] only provide local interactions, omitting interaction with context. Moreover, difficulties regarding variable-binding and embedded sentences should be noticed. In [8], world knowledge is used through testing referential success and other sequential tests. However, this method does not provide a uniform model of parsing: lexical ambiguities are resolved by marker passing and structural disambiguations are resolved by applying separate sequen</context>
</contexts>
<marker>[8]</marker>
<rawString>Hirst, G., Semantic Interpretation and the Resolution of Ambiguity, Cambridge University Press, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kitano</author>
</authors>
<title>Multilingual Information Retrieval Mechanism using VLSI, in &apos;Proceedings of RIAO-88&apos;,</title>
<date>1988</date>
<contexts>
<context position="26683" citStr="[9]" startWordPosition="4284" endWordPosition="4284">lausible and accesses previously defined discourse entities. This psycholinguistic claim and experimental result was incorporated in our model by adding costs for instance creation and constraint satisfaction. Another study relevent to our model is be the lexical preference theory by Ford, Bresnan and Kaplan [5]. Lexical preference theory assumes a preference order among lexical entries of verbs which differ in subcategorization for prepositional phrases. This type of preference was incorporated as the bias term in our cost equation. 7Refer to [22] for details in this direction. 8See [23] and [9] for discussion. - 77 - Although we have presented a basic mechanism to incorporate these psycholinguistic theories, well controlled psycholinguistic experiments will be necessary to set values of each constant and to validate our model psycholinguistically. 5.3 Reverse Cost In our example in the previous section, if the first sentence was Mary picked an S&amp;W where the hearer knows that an S&amp;W is a hand-gun, then an instance of &apos;MARY POSSES HAND-GUN1&apos; is asserted as true in the first sentence and no cost is incurred in the interpretation of the second sentence using CSC2. This means that the co</context>
</contexts>
<marker>[9]</marker>
<rawString>Kitano, H., Multilingual Information Retrieval Mechanism using VLSI, in &apos;Proceedings of RIAO-88&apos;, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>at</author>
</authors>
<title>Manuscript An Integrated Discourse Understanding Model for an Interpreting Telephony under the Direct Memory Access Paradigm,</title>
<date>1989</date>
<pages>111</pages>
<publisher>MIT Press,</publisher>
<institution>Carnegie Mellon University,</institution>
<marker>[10]</marker>
<rawString>Kitano, H., et. at, Manuscript An Integrated Discourse Understanding Model for an Interpreting Telephony under the Direct Memory Access Paradigm, Carnegie Mellon University, 1989. [111 Marcus, M. P., A theory of syntactic recognition for natural language, MIT Press, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Norvig</author>
</authors>
<title>Unified Theory of Inference for</title>
<date>1987</date>
<institution>Text Understading, Ph.D. Dissertation, University of California, Berkeley,</institution>
<contexts>
<context position="3942" citStr="[12]" startWordPosition="604" endWordPosition="604"> interaction with context. Moreover, difficulties regarding variable-binding and embedded sentences should be noticed. In [8], world knowledge is used through testing referential success and other sequential tests. However, this method does not provide a uniform model of parsing: lexical ambiguities are resolved by marker passing and structural disambiguations are resolved by applying separate sequential tests. An approach by [15] is similar to our model in that both precieve parsing as a physical process. However, their model, along with most other models, fails to capture discourse context. [12] uses marker passing as a method of contextual inference after a parse; however, no contextual information is feed-backed during the sentential parsing (marker-passing is performed after a separate parsing - 72 - process providing multiple hypotheses of the parse). [20] is closer to our model in that marker-passing based contextual inference is used during a sentential parse (i.e., an integrated processing of syntax, semantics and pragmatics at real-time); however the parsing (LFG, and case-frame based) and contextual inferences (marker-passing) are not under an uniform architecture. Past gene</context>
</contexts>
<marker>[12]</marker>
<rawString>Norvig, P., Unified Theory of Inference for Text Understading, Ph.D. Dissertation, University of California, Berkeley, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Prather</author>
<author>D Swinney</author>
</authors>
<title>Lexical Processing and Ambiguity Resolution: An Autonomous Processing in an Interactive Box, in &apos;Lexical Ambiguity Resolution&apos;,</title>
<date>1988</date>
<publisher>Morgan Kaufnuum Publishers,</publisher>
<contexts>
<context position="25728" citStr="[13]" startWordPosition="4141" endWordPosition="4141">e of our system is that it is possible to define global and local minima using massively parallel marking passing, which is computationally efficient and is more powerful in high-level processing involving variable-binding, structure building, and constraint propagations7 than neural network models. In addition, our model is suitable for massively parallel architectures which are now being researched by hardware designers as next generation machiness. 5.2 Psycholinguistic Relevance of the Model The phenomenon of lexical ambiguity has been studied by many psycholinguistic researchers including [13], [3], and [17]. These studies have identified contextual priming as an important factor in ambiguity resolution. One psycholinguistic study that is particularly relevent to DMTRANS PLUS is Crain and Steedman [4], which argues for the principle of referential success. Their experiments demonstrate that people prefer the interpretation which is most plausible and accesses previously defined discourse entities. This psycholinguistic claim and experimental result was incorporated in our model by adding costs for instance creation and constraint satisfaction. Another study relevent to our model is</context>
</contexts>
<marker>[13]</marker>
<rawString>Prather, P. and Swinney, D., Lexical Processing and Ambiguity Resolution: An Autonomous Processing in an Interactive Box, in &apos;Lexical Ambiguity Resolution&apos;, S. Small, et. al. (Eds), Morgan Kaufnuum Publishers, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Riesbedc</author>
<author>C Martin</author>
</authors>
<title>Direct Memory Access Parsing,</title>
<date>1985</date>
<journal>YALEU/DCS/RR</journal>
<volume>354</volume>
<contexts>
<context position="4972" citStr="[14]" startWordPosition="756" endWordPosition="756">tics and pragmatics at real-time); however the parsing (LFG, and case-frame based) and contextual inferences (marker-passing) are not under an uniform architecture. Past generations of DMTRANS ([19], [23]) have not incorporated cost-based structural ambiguity resolution schemes. 3 Overview of DMTRANS PLUS 3.1 Memory Access Parsing DMTRANS PLUS is a second generation DMA system based upon DMTRANS ([19]) with new methods of ambiguity resolution based on costs. Unlike most natural language systems, which are based on the &amp;quot;Build-and-Store&amp;quot; model, our system employs a &amp;quot;Recognize-and-Record&amp;quot; model ([14],[19], [21]). Understanding of an input sentence (or speech input in ODMTRANS PLUS) is defined as changes made in a memory network. Parsing and natural language understanding in these systems are considered to be memory-access processes, identifying existent knowledge in memory with the current input. Sentences are always parsed in context, i.e., through utilizing the existing and (currently acquired) knowledge about the world. In other words, during parsing, relevant discourse entities in memory are constantly being remembered. The model behind DMTRANS PLUS is a simulation of such a process. </context>
</contexts>
<marker>[14]</marker>
<rawString>Riesbedc, C. and Martin, C., Direct Memory Access Parsing, YALEU/DCS/RR 354, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Selman</author>
<author>G Hirst</author>
</authors>
<title>Parsing as an Energy Minimization Problem,</title>
<date>1987</date>
<booktitle>in Genetic Algorithms and Simulated Annealing,</booktitle>
<editor>(Ed.),</editor>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>Davis, L</location>
<contexts>
<context position="3772" citStr="[15]" startWordPosition="576" endWordPosition="576">approaches as seen in [18], [25], and [8] essentially stick to semantic restrictions and associations. However, [18], [25], [24] only provide local interactions, omitting interaction with context. Moreover, difficulties regarding variable-binding and embedded sentences should be noticed. In [8], world knowledge is used through testing referential success and other sequential tests. However, this method does not provide a uniform model of parsing: lexical ambiguities are resolved by marker passing and structural disambiguations are resolved by applying separate sequential tests. An approach by [15] is similar to our model in that both precieve parsing as a physical process. However, their model, along with most other models, fails to capture discourse context. [12] uses marker passing as a method of contextual inference after a parse; however, no contextual information is feed-backed during the sentential parsing (marker-passing is performed after a separate parsing - 72 - process providing multiple hypotheses of the parse). [20] is closer to our model in that marker-passing based contextual inference is used during a sentential parse (i.e., an integrated processing of syntax, semantics</context>
</contexts>
<marker>[15]</marker>
<rawString>Selman, B. and Hirst, G., Parsing as an Energy Minimization Problem, in Genetic Algorithms and Simulated Annealing, Davis, L (Ed.), Morgan Kaufmann Publishers, CA, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schank</author>
</authors>
<title>Dynamic Memory: A theory of learning in computers and people.</title>
<date>1982</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6622" citStr="[16]" startWordPosition="998" endWordPosition="998">escribed in [19] and [21]. Lexical entries are represented as lexical nodes which are a kind of CC. Phoneme sequences are used only for DMTRANS PLUS, the speech-input version of DMTRANS PLUS. CSCs represent sequences of concepts such as phoneme sequences (i.e. &lt;/lc//a//i//g//i/&gt;), concept sequences (i.e. &lt;*Conference *Goal-Role *Attend *Want&gt;), and plan sequences (i.e. &lt;*Declare-WantAttend *Listen-Instruction&gt;). The linguistic knowledge represented as CSCs can be low-level surface specific patterns such as phrasal lexicon entries [1] or material at higher levels of abstration such as in MOP&apos;s [16]. However, CSCs should not be confused with &apos;discourse segments&apos; [6]. In our model, information represented in discourse segments are distributively incorporated in the memory network. During sentence processing we create concept instances (CI) correpsonding to CCs and concept sequence instances (CSI) corresponding to CSCs. This is a substantial improvement over past DMA research. Lack of instance creation and reference in past research was a major obstacle to seriously modelling discourse phenomena. CIs and CSIs are connected through several types of links. A guided marker passing scheme is e</context>
</contexts>
<marker>[16]</marker>
<rawString>Schank, R., Dynamic Memory: A theory of learning in computers and people. Cambridge University Press. 1982</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Small</author>
<author>et</author>
</authors>
<title>at (Eds.) Lexical Ambiguity Resolution,</title>
<date>1988</date>
<publisher>Morgan Kaufmann Publishers, Inc.,</publisher>
<location>CA,</location>
<contexts>
<context position="25743" citStr="[17]" startWordPosition="4144" endWordPosition="4144"> is that it is possible to define global and local minima using massively parallel marking passing, which is computationally efficient and is more powerful in high-level processing involving variable-binding, structure building, and constraint propagations7 than neural network models. In addition, our model is suitable for massively parallel architectures which are now being researched by hardware designers as next generation machiness. 5.2 Psycholinguistic Relevance of the Model The phenomenon of lexical ambiguity has been studied by many psycholinguistic researchers including [13], [3], and [17]. These studies have identified contextual priming as an important factor in ambiguity resolution. One psycholinguistic study that is particularly relevent to DMTRANS PLUS is Crain and Steedman [4], which argues for the principle of referential success. Their experiments demonstrate that people prefer the interpretation which is most plausible and accesses previously defined discourse entities. This psycholinguistic claim and experimental result was incorporated in our model by adding costs for instance creation and constraint satisfaction. Another study relevent to our model is be the lexical</context>
</contexts>
<marker>[17]</marker>
<rawString>Small, S., et. at (Eds.) Lexical Ambiguity Resolution, Morgan Kaufmann Publishers, Inc., CA, 1988.</rawString>
</citation>
<citation valid="false">
<authors>
<author>a</author>
</authors>
<title>Toward Connectionist Parsing,</title>
<booktitle>in Proceedings of AAAI-82,</booktitle>
<pages>1982</pages>
<contexts>
<context position="3194" citStr="[18]" startWordPosition="495" endWordPosition="495">esenting the hypothesis. One other important idea is that our model employs the direct memory *E-mail address is hiroalci@aalcs.cmu.edu. Also with NEC Corporation. Traditional syntactic parsers have been using attachment preferences and local syntactic and semantic constraints for resolving lexical and structural ambiguities. ([171, [28], [2], [7], [26], [II], [5]) However, these methods cannot select one interpretation from several plausible interpretations because they do not incorporate the discourse context of the sentences being parsed ([8], [4]). Connectionist-type approaches as seen in [18], [25], and [8] essentially stick to semantic restrictions and associations. However, [18], [25], [24] only provide local interactions, omitting interaction with context. Moreover, difficulties regarding variable-binding and embedded sentences should be noticed. In [8], world knowledge is used through testing referential success and other sequential tests. However, this method does not provide a uniform model of parsing: lexical ambiguities are resolved by marker passing and structural disambiguations are resolved by applying separate sequential tests. An approach by [15] is similar to our mod</context>
</contexts>
<marker>[18]</marker>
<rawString>Small, S., et. a. Toward Connectionist Parsing, in Proceedings of AAAI-82, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tomabechi</author>
</authors>
<title>Direct Memory Access Translation,</title>
<date>1987</date>
<booktitle>in &apos;Proceedings of the IJCAI-88&apos;,</booktitle>
<contexts>
<context position="4566" citStr="[19]" startWordPosition="697" endWordPosition="697"> as a method of contextual inference after a parse; however, no contextual information is feed-backed during the sentential parsing (marker-passing is performed after a separate parsing - 72 - process providing multiple hypotheses of the parse). [20] is closer to our model in that marker-passing based contextual inference is used during a sentential parse (i.e., an integrated processing of syntax, semantics and pragmatics at real-time); however the parsing (LFG, and case-frame based) and contextual inferences (marker-passing) are not under an uniform architecture. Past generations of DMTRANS ([19], [23]) have not incorporated cost-based structural ambiguity resolution schemes. 3 Overview of DMTRANS PLUS 3.1 Memory Access Parsing DMTRANS PLUS is a second generation DMA system based upon DMTRANS ([19]) with new methods of ambiguity resolution based on costs. Unlike most natural language systems, which are based on the &amp;quot;Build-and-Store&amp;quot; model, our system employs a &amp;quot;Recognize-and-Record&amp;quot; model ([14],[19], [21]). Understanding of an input sentence (or speech input in ODMTRANS PLUS) is defined as changes made in a memory network. Parsing and natural language understanding in these systems ar</context>
<context position="6034" citStr="[19]" startWordPosition="912" endWordPosition="912">sing, relevant discourse entities in memory are constantly being remembered. The model behind DMTRANS PLUS is a simulation of such a process. The memory network incorporates knowledge from morphophonetics to discourse. Each node represents a concept (Concept Class node; CC) or a sequence of concepts (Concept Sequence Class node; CSC). CCs represent such knowledge as phones (i.e. [k]), phonemes (i.e. /k0, concepts (i.e. *Hand-Gun, *Event, *Mtrans-Action), and plans (i.e. *Pick-UpGun). A hierarchy of Concept Class (CC) entities stores knowledge both declaratively and procedurely as described in [19] and [21]. Lexical entries are represented as lexical nodes which are a kind of CC. Phoneme sequences are used only for DMTRANS PLUS, the speech-input version of DMTRANS PLUS. CSCs represent sequences of concepts such as phoneme sequences (i.e. &lt;/lc//a//i//g//i/&gt;), concept sequences (i.e. &lt;*Conference *Goal-Role *Attend *Want&gt;), and plan sequences (i.e. &lt;*Declare-WantAttend *Listen-Instruction&gt;). The linguistic knowledge represented as CSCs can be low-level surface specific patterns such as phrasal lexicon entries [1] or material at higher levels of abstration such as in MOP&apos;s [16]. However, C</context>
<context position="11606" citStr="[19]" startWordPosition="1822" endWordPosition="1822">rchy. 6. Sequential Prediction. When an A-Marker reaches any P-Marked node (i.e. part of CSC), the P-Marker on the node is sent to the next element of the concept sequence. 7. Contextual Priming When an A-Marker reaches any Contextual Root node. C-Makers are put on the contexual children nodes designated by the root node. 8. Conceptual Relation Instantiation. When the last element of a concept sequence recieves an A-Marker, Constraints (world and discourse knowledge) are checked for. A CSI is created under the CSC with packaging links to each CI. This process is called concept refinement. See [19]. The memory network is modified by performing inferences stored in the root CSC which had the accepted CSC attached to it. 9. Activation Propagation A-Marker is propagated from the CSC to higher nodes. 3.2 Memory Network Modification Several different incidents trigger the modification of the memory network during parsing: • An individual concept is instantiated (i.e. an instance is created) under a CC when the CC receives an A-Marker and a CI (an instance that - 74 - was created by preceding utterances) is not existent. This instantiation is a creation of a specific discourse entity which ma</context>
<context position="16177" citStr="[19]" startWordPosition="2603" endWordPosition="2603"> Contextual Priming In our system, some CC nodes designated as Contextual Root Nodes have a list of thematically relevant nodes. C-Markers are sent to these nodes as soon as a Contextual Root Node is activated. Thus each sentence and/or each word might influence the interpretation of following sentences or words. When a node with C-Marker is activated by receiving an A-Marker, the activation will be propagated with no cost. Thus, a parse using such nodes would have no cost. However, when a node without a C-Marker is activated, a small cost is attached to the interpretation using that node. In [19] the discussion of C-Marker propagation concentrated on the resolution of word-level ambiguities. However, C-Markers are also propagated to conceptual 2We have not incorporated these factors primarily because structured P-Markers can play the role of top-down priming; however, we may be incorporating these factors in the future. 3For example, in one implementation of DMTRANS, we are using time-delayed decaying activations which resolve ambiguity even when two CI nodes are concurrently active. - 75 - class nodes, which can represent word-level, phrasal, or sentential knowledge. Therefore, C-Mar</context>
<context position="18418" citStr="[19]" startWordPosition="2932" endWordPosition="2932"> computationally less-expensive than connectionist-type competitive activation and inhibition schemes4. Although connectionist approaches can resolve certain types of lexical ambiguity, they are computationally expensive unless we have massively parallel computers. C-Markers are a resonable compromise because they are sent to semantically relevant concept nodes to attain contextual priming without computationally expensive competitive activation and inhibition methods. 4.2 Reference to the Discourse Entity When a lexical node activates any CC node, a CI node under the CC node is searched for ([19], [21]). This activity models reference to an already established discourse entity [27] in the hearer&apos;s mind. If such a CI node exists, the reference succeeds and this parse will be attached with no cost. However, if no such instance is found, reference failure results. If this happens, an instantiation activity is performed creating a new instance with certain costs. As a result, a parse using newly created instance node will be attached with some cost. For example, if a preceding discourse contained a reference to a thesis, a CI node such as THEsts005 would have been created. Now if a new in</context>
<context position="22354" citStr="[19]" startWordPosition="3591" endWordPosition="3591">through the cost recording mechanism describe above. During the processing of the first sentence, DM TRANS PLUS creates instances of &apos;Mary&apos; and &apos;Uzzi - 76 - and records them as active instances in memory (i.e., MARY1 and UZZI1 are created). In addition, a link between MARY1 and UZZI1 is created with the POSSES relation label. This link creation is invoked by triggering side-effects (i.e., inferences) stored in the CSC representing the action of `MARY1 picking up the UZZI1&apos;. We omit the details of marker passing (for A-, P-, and C-Markers) since it is described detail elsewhere (particulary in [19]). When the second sentence comes in, an instance MARY I already exists and, therefore, no cost is charged for parsing `Mary&apos;5. However, we now have three relevant concept sequences (CSC&apos;s6): CSC1: (&lt;agent&gt; &lt;shoot&gt; &lt;object&gt;) CSC2: (&lt;agent&gt; &lt;shoot&gt; &lt;object&gt; &lt;with&gt; &lt;instrument&gt;) CSC3: (&lt;person&gt; &lt;with&gt; &lt;instrument&gt;) These sequences are activated when concepts in the sequences are activated in order from below in the abstraction hierarchy. When the &amp;quot;man&amp;quot; comes in, recognition of CSC3:(&lt;person&gt; &lt;with&gt; &lt;instrument&gt;) starts. When the whole sentence is received, we have two top-level CSCs (i.e., CSC1 </context>
<context position="24042" citStr="[19]" startWordPosition="3864" endWordPosition="3864"> we must create a POSSESS link between MARY1 and HAND-GUN1. A certain cost, say 10, is associated with the creation of this link. On the other hand, MANI POSSESS HAND-GUN1 is known in memory because of an earlier sentence. As a result, CSC3 is instantiated with no cost and an A-Marker from CSC3 is propagated upward to CSC1 with no cost. Thus, the cost of instantiating CSC1 is 0 and the cost of instantiating CSC2 is 10. This way, the inteipretation with CSC1 is favored by our system. 50f course, &apos;Mary&apos; can be &apos;She&apos;. The method for handling this type of pronoun reference was already reported in [19] and we do not discuss it here. 6As we can see from this example of CSC&apos;s, a concept sequence can be normally regarded as a subcategorization list of a VP head. However, concept sequences are not restricted to such lists and are actually often at higher levels of abstraction representing MOP-like sequences. 5 Discussion: 5.1 Global Minima The correct hypothesis in our model is the hypothesis with the least cost. This corresponds to the notion of global minima in most connectionist literature. On other hand, the hypothesis which has the least cost within a local scope but does not have the leas</context>
</contexts>
<marker>[19]</marker>
<rawString>Tomabechi, H., Direct Memory Access Translation, in &apos;Proceedings of the IJCAI-88&apos;, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tomabechi</author>
<author>M Tombs</author>
</authors>
<title>The Integration of Unificationbased Syntax/Semantics and Memory-based Pragmatics for Real-Time Understanding of Noisy Continuous Speech Input,</title>
<date>1988</date>
<booktitle>in &apos;Proceedings of the AAAI-88&apos;,</booktitle>
<contexts>
<context position="4212" citStr="[20]" startWordPosition="646" endWordPosition="646">f parsing: lexical ambiguities are resolved by marker passing and structural disambiguations are resolved by applying separate sequential tests. An approach by [15] is similar to our model in that both precieve parsing as a physical process. However, their model, along with most other models, fails to capture discourse context. [12] uses marker passing as a method of contextual inference after a parse; however, no contextual information is feed-backed during the sentential parsing (marker-passing is performed after a separate parsing - 72 - process providing multiple hypotheses of the parse). [20] is closer to our model in that marker-passing based contextual inference is used during a sentential parse (i.e., an integrated processing of syntax, semantics and pragmatics at real-time); however the parsing (LFG, and case-frame based) and contextual inferences (marker-passing) are not under an uniform architecture. Past generations of DMTRANS ([19], [23]) have not incorporated cost-based structural ambiguity resolution schemes. 3 Overview of DMTRANS PLUS 3.1 Memory Access Parsing DMTRANS PLUS is a second generation DMA system based upon DMTRANS ([19]) with new methods of ambiguity resoluti</context>
</contexts>
<marker>[20]</marker>
<rawString>Tomabechi, H. and Tombs, M., The Integration of Unificationbased Syntax/Semantics and Memory-based Pragmatics for Real-Time Understanding of Noisy Continuous Speech Input, in &apos;Proceedings of the AAAI-88&apos;, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tomabechi</author>
<author>M Tomita</author>
</authors>
<title>Application of the Direct Memory Access paradigm to natural language interfaces to knowledge-based systems, in &apos;Proceedings of the COLING88&apos;,</title>
<date>1988</date>
<contexts>
<context position="4983" citStr="[21]" startWordPosition="757" endWordPosition="757">agmatics at real-time); however the parsing (LFG, and case-frame based) and contextual inferences (marker-passing) are not under an uniform architecture. Past generations of DMTRANS ([19], [23]) have not incorporated cost-based structural ambiguity resolution schemes. 3 Overview of DMTRANS PLUS 3.1 Memory Access Parsing DMTRANS PLUS is a second generation DMA system based upon DMTRANS ([19]) with new methods of ambiguity resolution based on costs. Unlike most natural language systems, which are based on the &amp;quot;Build-and-Store&amp;quot; model, our system employs a &amp;quot;Recognize-and-Record&amp;quot; model ([14],[19], [21]). Understanding of an input sentence (or speech input in ODMTRANS PLUS) is defined as changes made in a memory network. Parsing and natural language understanding in these systems are considered to be memory-access processes, identifying existent knowledge in memory with the current input. Sentences are always parsed in context, i.e., through utilizing the existing and (currently acquired) knowledge about the world. In other words, during parsing, relevant discourse entities in memory are constantly being remembered. The model behind DMTRANS PLUS is a simulation of such a process. The memory </context>
<context position="18424" citStr="[21]" startWordPosition="2933" endWordPosition="2933">tationally less-expensive than connectionist-type competitive activation and inhibition schemes4. Although connectionist approaches can resolve certain types of lexical ambiguity, they are computationally expensive unless we have massively parallel computers. C-Markers are a resonable compromise because they are sent to semantically relevant concept nodes to attain contextual priming without computationally expensive competitive activation and inhibition methods. 4.2 Reference to the Discourse Entity When a lexical node activates any CC node, a CI node under the CC node is searched for ([19], [21]). This activity models reference to an already established discourse entity [27] in the hearer&apos;s mind. If such a CI node exists, the reference succeeds and this parse will be attached with no cost. However, if no such instance is found, reference failure results. If this happens, an instantiation activity is performed creating a new instance with certain costs. As a result, a parse using newly created instance node will be attached with some cost. For example, if a preceding discourse contained a reference to a thesis, a CI node such as THEsts005 would have been created. Now if a new input se</context>
</contexts>
<marker>[21]</marker>
<rawString>Tomabechi, H. and Tomita, M., Application of the Direct Memory Access paradigm to natural language interfaces to knowledge-based systems, in &apos;Proceedings of the COLING88&apos;, 1988.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Tomabechi</author>
<author>M Tomita</author>
<author>Manuscript</author>
</authors>
<title>MASSIVELY PARALLEL CONSTRAIIVT PROPAGATION: Parsing with Unification-based Grammar without Unification.</title>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="26634" citStr="[22]" startWordPosition="4275" endWordPosition="4275">t people prefer the interpretation which is most plausible and accesses previously defined discourse entities. This psycholinguistic claim and experimental result was incorporated in our model by adding costs for instance creation and constraint satisfaction. Another study relevent to our model is be the lexical preference theory by Ford, Bresnan and Kaplan [5]. Lexical preference theory assumes a preference order among lexical entries of verbs which differ in subcategorization for prepositional phrases. This type of preference was incorporated as the bias term in our cost equation. 7Refer to [22] for details in this direction. 8See [23] and [9] for discussion. - 77 - Although we have presented a basic mechanism to incorporate these psycholinguistic theories, well controlled psycholinguistic experiments will be necessary to set values of each constant and to validate our model psycholinguistically. 5.3 Reverse Cost In our example in the previous section, if the first sentence was Mary picked an S&amp;W where the hearer knows that an S&amp;W is a hand-gun, then an instance of &apos;MARY POSSES HAND-GUN1&apos; is asserted as true in the first sentence and no cost is incurred in the interpretation of the s</context>
</contexts>
<marker>[22]</marker>
<rawString>Tomabechi, H. and Tomita, M., Manuscript. MASSIVELY PARALLEL CONSTRAIIVT PROPAGATION: Parsing with Unification-based Grammar without Unification. Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tomabechi</author>
<author>T Mitamura</author>
<author>M Tornita</author>
</authors>
<date>1988</date>
<booktitle>DIRECT MEMORY ACCESS TRANSLATION FOR SPEECH INPUT: A Massively Parallel Network of EpisodicIThematic and Phonological Memory, in &apos;Proceedings of the International Conference on Fifth Generation Computer Systems</booktitle>
<contexts>
<context position="4572" citStr="[23]" startWordPosition="698" endWordPosition="698">method of contextual inference after a parse; however, no contextual information is feed-backed during the sentential parsing (marker-passing is performed after a separate parsing - 72 - process providing multiple hypotheses of the parse). [20] is closer to our model in that marker-passing based contextual inference is used during a sentential parse (i.e., an integrated processing of syntax, semantics and pragmatics at real-time); however the parsing (LFG, and case-frame based) and contextual inferences (marker-passing) are not under an uniform architecture. Past generations of DMTRANS ([19], [23]) have not incorporated cost-based structural ambiguity resolution schemes. 3 Overview of DMTRANS PLUS 3.1 Memory Access Parsing DMTRANS PLUS is a second generation DMA system based upon DMTRANS ([19]) with new methods of ambiguity resolution based on costs. Unlike most natural language systems, which are based on the &amp;quot;Build-and-Store&amp;quot; model, our system employs a &amp;quot;Recognize-and-Record&amp;quot; model ([14],[19], [21]). Understanding of an input sentence (or speech input in ODMTRANS PLUS) is defined as changes made in a memory network. Parsing and natural language understanding in these systems are cons</context>
<context position="26675" citStr="[23]" startWordPosition="4282" endWordPosition="4282">is most plausible and accesses previously defined discourse entities. This psycholinguistic claim and experimental result was incorporated in our model by adding costs for instance creation and constraint satisfaction. Another study relevent to our model is be the lexical preference theory by Ford, Bresnan and Kaplan [5]. Lexical preference theory assumes a preference order among lexical entries of verbs which differ in subcategorization for prepositional phrases. This type of preference was incorporated as the bias term in our cost equation. 7Refer to [22] for details in this direction. 8See [23] and [9] for discussion. - 77 - Although we have presented a basic mechanism to incorporate these psycholinguistic theories, well controlled psycholinguistic experiments will be necessary to set values of each constant and to validate our model psycholinguistically. 5.3 Reverse Cost In our example in the previous section, if the first sentence was Mary picked an S&amp;W where the hearer knows that an S&amp;W is a hand-gun, then an instance of &apos;MARY POSSES HAND-GUN1&apos; is asserted as true in the first sentence and no cost is incurred in the interpretation of the second sentence using CSC2. This means tha</context>
</contexts>
<marker>[23]</marker>
<rawString>Tomabechi, H., Mitamura, T., and Tornita, M., DIRECT MEMORY ACCESS TRANSLATION FOR SPEECH INPUT: A Massively Parallel Network of EpisodicIThematic and Phonological Memory, in &apos;Proceedings of the International Conference on Fifth Generation Computer Systems 1988&apos; (FGCS&apos;88), 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Touretzlcy</author>
</authors>
<title>Connectionism and PP Attachment,</title>
<date>1988</date>
<booktitle>in &apos;Proceedings of the 1988 Cotmectionist Models Summer School,</booktitle>
<contexts>
<context position="3296" citStr="[24]" startWordPosition="509" endWordPosition="509">address is hiroalci@aalcs.cmu.edu. Also with NEC Corporation. Traditional syntactic parsers have been using attachment preferences and local syntactic and semantic constraints for resolving lexical and structural ambiguities. ([171, [28], [2], [7], [26], [II], [5]) However, these methods cannot select one interpretation from several plausible interpretations because they do not incorporate the discourse context of the sentences being parsed ([8], [4]). Connectionist-type approaches as seen in [18], [25], and [8] essentially stick to semantic restrictions and associations. However, [18], [25], [24] only provide local interactions, omitting interaction with context. Moreover, difficulties regarding variable-binding and embedded sentences should be noticed. In [8], world knowledge is used through testing referential success and other sequential tests. However, this method does not provide a uniform model of parsing: lexical ambiguities are resolved by marker passing and structural disambiguations are resolved by applying separate sequential tests. An approach by [15] is similar to our model in that both precieve parsing as a physical process. However, their model, along with most other mo</context>
</contexts>
<marker>[24]</marker>
<rawString>Touretzlcy, D. S., Connectionism and PP Attachment, in &apos;Proceedings of the 1988 Cotmectionist Models Summer School, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Waltz</author>
<author>J B Pollack</author>
</authors>
<title>Massively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation.</title>
<date>1985</date>
<journal>Cognitive Science</journal>
<volume>9</volume>
<issue>1</issue>
<pages>51--74</pages>
<contexts>
<context position="3200" citStr="[25]" startWordPosition="496" endWordPosition="496">ng the hypothesis. One other important idea is that our model employs the direct memory *E-mail address is hiroalci@aalcs.cmu.edu. Also with NEC Corporation. Traditional syntactic parsers have been using attachment preferences and local syntactic and semantic constraints for resolving lexical and structural ambiguities. ([171, [28], [2], [7], [26], [II], [5]) However, these methods cannot select one interpretation from several plausible interpretations because they do not incorporate the discourse context of the sentences being parsed ([8], [4]). Connectionist-type approaches as seen in [18], [25], and [8] essentially stick to semantic restrictions and associations. However, [18], [25], [24] only provide local interactions, omitting interaction with context. Moreover, difficulties regarding variable-binding and embedded sentences should be noticed. In [8], world knowledge is used through testing referential success and other sequential tests. However, this method does not provide a uniform model of parsing: lexical ambiguities are resolved by marker passing and structural disambiguations are resolved by applying separate sequential tests. An approach by [15] is similar to our model in </context>
</contexts>
<marker>[25]</marker>
<rawString>Waltz, D. L and Pollack, J. B., Massively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation. Cognitive Science 9(1): 51-74, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Wanner</author>
</authors>
<title>The ATN and the Sausage Machine: Which one is baloney?</title>
<date>1980</date>
<journal>Cognition,</journal>
<volume>8</volume>
<issue>2</issue>
<contexts>
<context position="2945" citStr="[26]" startWordPosition="460" endWordPosition="460">phy of the model is to view parsing as a dynamic physical process in which one trajectory is taken from among many other possible paths. Thus our notion of the cost of the hypothesis is a representation of the workload required to take the path representing the hypothesis. One other important idea is that our model employs the direct memory *E-mail address is hiroalci@aalcs.cmu.edu. Also with NEC Corporation. Traditional syntactic parsers have been using attachment preferences and local syntactic and semantic constraints for resolving lexical and structural ambiguities. ([171, [28], [2], [7], [26], [II], [5]) However, these methods cannot select one interpretation from several plausible interpretations because they do not incorporate the discourse context of the sentences being parsed ([8], [4]). Connectionist-type approaches as seen in [18], [25], and [8] essentially stick to semantic restrictions and associations. However, [18], [25], [24] only provide local interactions, omitting interaction with context. Moreover, difficulties regarding variable-binding and embedded sentences should be noticed. In [8], world knowledge is used through testing referential success and other sequential</context>
</contexts>
<marker>[26]</marker>
<rawString>Wanner, E., The ATN and the Sausage Machine: Which one is baloney? Cognition, 8(2), June, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B L Webber</author>
</authors>
<title>So what can we talk about now?, in &apos;Computational Models of Discourse&apos;,</title>
<date>1983</date>
<publisher>MIT Press,</publisher>
<contexts>
<context position="18505" citStr="[27]" startWordPosition="2945" endWordPosition="2945">bition schemes4. Although connectionist approaches can resolve certain types of lexical ambiguity, they are computationally expensive unless we have massively parallel computers. C-Markers are a resonable compromise because they are sent to semantically relevant concept nodes to attain contextual priming without computationally expensive competitive activation and inhibition methods. 4.2 Reference to the Discourse Entity When a lexical node activates any CC node, a CI node under the CC node is searched for ([19], [21]). This activity models reference to an already established discourse entity [27] in the hearer&apos;s mind. If such a CI node exists, the reference succeeds and this parse will be attached with no cost. However, if no such instance is found, reference failure results. If this happens, an instantiation activity is performed creating a new instance with certain costs. As a result, a parse using newly created instance node will be attached with some cost. For example, if a preceding discourse contained a reference to a thesis, a CI node such as THEsts005 would have been created. Now if a new input sentence contains the word paper, CC nodes for THE411tis does not mean that our mod</context>
</contexts>
<marker>[27]</marker>
<rawString>Webber, B. L, So what can we talk about now?, in &apos;Computational Models of Discourse&apos;, (Eds. M. Brady and R.C. Berwick), MIT Press, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y A Wilks</author>
<author>X Huang</author>
<author>D Fess</author>
</authors>
<title>Syntax, preference and right attachment,</title>
<date>1985</date>
<booktitle>in &apos;Proceedings of the IJCAI-85,</booktitle>
<note>79 -</note>
<contexts>
<context position="2929" citStr="[28]" startWordPosition="457" endWordPosition="457">derlying philosophy of the model is to view parsing as a dynamic physical process in which one trajectory is taken from among many other possible paths. Thus our notion of the cost of the hypothesis is a representation of the workload required to take the path representing the hypothesis. One other important idea is that our model employs the direct memory *E-mail address is hiroalci@aalcs.cmu.edu. Also with NEC Corporation. Traditional syntactic parsers have been using attachment preferences and local syntactic and semantic constraints for resolving lexical and structural ambiguities. ([171, [28], [2], [7], [26], [II], [5]) However, these methods cannot select one interpretation from several plausible interpretations because they do not incorporate the discourse context of the sentences being parsed ([8], [4]). Connectionist-type approaches as seen in [18], [25], and [8] essentially stick to semantic restrictions and associations. However, [18], [25], [24] only provide local interactions, omitting interaction with context. Moreover, difficulties regarding variable-binding and embedded sentences should be noticed. In [8], world knowledge is used through testing referential success and </context>
</contexts>
<marker>[28]</marker>
<rawString>Wilks, Y. A., Huang, X. and Fess, D., Syntax, preference and right attachment, in &apos;Proceedings of the IJCAI-85, 1985. - 79 -</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>