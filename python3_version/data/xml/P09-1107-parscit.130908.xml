<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.918304">
Incremental HMM Alignment for MT System Combination
</title>
<author confidence="0.992909">
Chi-Ho Li
</author>
<affiliation confidence="0.993741">
Microsoft Research Asia
</affiliation>
<address confidence="0.961392">
49 Zhichun Road, Beijing, China
</address>
<email confidence="0.990125">
chl@microsoft.com
</email>
<author confidence="0.999044">
Yupeng Liu
</author>
<affiliation confidence="0.999268">
Harbin Institute of Technology
</affiliation>
<address confidence="0.85007">
92 Xidazhi Street, Harbin, China
</address>
<email confidence="0.997196">
ypliu@mtlab.hit.edu.cn
</email>
<sectionHeader confidence="0.997363" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999409384615384">
Inspired by the incremental TER align-
ment, we re-designed the Indirect HMM
(IHMM) alignment, which is one of the
best hypothesis alignment methods for
conventional MT system combination, in
an incremental manner. One crucial prob-
lem of incremental alignment is to align a
hypothesis to a confusion network (CN).
Our incremental IHMM alignment is im-
plemented in three different ways: 1) treat
CN spans as HMM states and define state
transition as distortion over covered n-
grams between two spans; 2) treat CN
spans as HMM states and define state tran-
sition as distortion over words in compo-
nent translations in the CN; and 3) use
a consensus decoding algorithm over one
hypothesis and multiple IHMMs, each of
which corresponds to a component trans-
lation in the CN. All these three ap-
proaches of incremental alignment based
on IHMM are shown to be superior to both
incremental TER alignment and conven-
tional IHMM alignment in the setting of
the Chinese-to-English track of the 2008
NIST Open MT evaluation.
</bodyText>
<sectionHeader confidence="0.999617" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999725375">
Word-level combination using confusion network
(Matusov et al. (2006) and Rosti et al. (2007)) is a
widely adopted approach for combining Machine
Translation (MT) systems’ output. Word align-
ment between a backbone (or skeleton) translation
and a hypothesis translation is a key problem in
this approach. Translation Edit Rate (TER, Snover
et al. (2006)) based alignment proposed in Sim
</bodyText>
<author confidence="0.742397">
Xiaodong He
</author>
<affiliation confidence="0.805304">
Microsoft Research
One Microsoft Way, Redmond, USA
</affiliation>
<email confidence="0.825874">
xiaohe@microsoft.com
</email>
<author confidence="0.979003">
Ning Xi
</author>
<affiliation confidence="0.985936">
Nanjing University
</affiliation>
<address confidence="0.818784">
8 Hankou Road, Nanjing, China
</address>
<email confidence="0.978341">
xin@nlp.nju.edu.cn
</email>
<bodyText confidence="0.999978171428571">
et al. (2007) is often taken as the baseline, and
a couple of other approaches, such as the Indi-
rect Hidden Markov Model (IHMM, He et al.
(2008)) and the ITG-based alignment (Karakos et
al. (2008)), were recently proposed with better re-
sults reported. With an alignment method, each
hypothesis is aligned against the backbone and all
the alignments are then used to build a confusion
network (CN) for generating a better translation.
However, as pointed out by Rosti et al. (2008),
such a pair-wise alignment strategy will produce
a low-quality CN if there are errors in the align-
ment of any of the hypotheses, no matter how good
the alignments of other hypotheses are. For ex-
ample, suppose we have the backbone “he buys a
computer” and two hypotheses “he bought a lap-
top computer” and “he buys a laptop”. It will be
natural for most alignment methods to produce the
alignments in Figure 1a. The alignment of hypoth-
esis 2 against the backbone cannot be considered
an error if we consider only these two translations;
nevertheless, when added with the alignment of
another hypothesis, it produces the low-quality
CN in Figure 1b, which may generate poor trans-
lations like “he bought a laptop laptop”. While it
could be argued that such poor translations are un-
likely to be selected due to language model, this
CN does disperse the votes to the word “laptop” to
two distinct arcs.
Rosti et al. (2008) showed that this problem can
be rectified by incremental alignment. If hypoth-
esis 1 is first aligned against the backbone, the
CN thus produced (depicted in Figure 2a) is then
aligned to hypothesis 2, giving rise to the good CN
as depicted in Figure 2b.1 On the other hand, the
</bodyText>
<footnote confidence="0.991077666666667">
1Note that this CN may generate an incomplete sentence
“he bought a”, which is nevertheless unlikely to be selected
as it leads to low language model score.
</footnote>
<page confidence="0.944776">
949
</page>
<note confidence="0.9996745">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 949–957,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.98263525">
Figure 1: An example bad confusion network due
to pair-wise alignment strategy
Figure 2: An example good confusion network
due to incremental alignment strategy
</figureCaption>
<bodyText confidence="0.999967366666667">
correct result depends on the order of hypotheses.
If hypothesis 2 is aligned before hypothesis 1, the
final CN will not be good. Therefore, the obser-
vation in Rosti et al. (2008) that different order
of hypotheses does not affect translation quality is
counter-intuitive.
This paper attempts to answer two questions: 1)
as incremental TER alignment gives better perfor-
mance than pair-wise TER alignment, would the
incremental strategy still be better than the pair-
wise strategy if the TER method is replaced by
another alignment method? 2) how does transla-
tion quality vary for different orders of hypotheses
being incrementally added into a CN? For ques-
tion 1, we will focus on the IHMM alignment
method and propose three different ways of imple-
menting incremental IHMM alignment. Our ex-
periments will also try several orders of hypothe-
ses in response to question 2.
This paper is structured as follows. After set-
ting the notations on CN in section 2, we will
first introduce, in section 3, two variations of the
basic incremental IHMM model (IncIHMM1 and
IncIHMM2). In section 4, a consensus decoding
algorithm (CD-IHMM) is proposed as an alterna-
tive way to search for the optimal alignment. The
issues of alignment normalization and the order of
hypotheses being added into a CN are discussed in
sections 5 and 6 respectively. Experiment results
and analysis are presented in section 7.
</bodyText>
<sectionHeader confidence="0.926308" genericHeader="introduction">
2 Preliminaries: Notation on Confusion
Network
</sectionHeader>
<bodyText confidence="0.999448724137931">
Before the elaboration of the models, let us first
clarify the notation on CN. A CN is usually de-
scribed as a finite state graph with many spans.
Each span corresponds to a word position and con-
tains several arcs, each of which represents an al-
ternative word (could be the empty symbol , E) at
that position. Each arc is also associated with M
weights in an M-way system combination task.
Follow Rosti et al. (2007), the i-th weight of an
arc is Er 11 , where r is the rank of the hypothe-
sis in the i-th system that votes for the word repre-
sented by the arc. This conception of CN is called
the conventional or compact form of CN. The net-
works in Figures 1b and 2b are examples.
On the other hand, as a CN is an integration
of the skeleton and all hypotheses, it can be con-
ceived as a list of the component translations. For
example, the CN in Figure 2b can be converted
to the form in Figure 3. In such an expanded or
tabular form, each row represents a component
translation. Each column, which is equivalent to
a span in the compact form, comprises the alter-
native words at a word position. Thus each cell
represents an alternative word at certain word po-
sition voted by certain translation. Each row is as-
signed the weight 1
1+r, where r is the rank of the
translation of some MT system. It is assumed that
all MT systems are weighted equally and thus the
</bodyText>
<page confidence="0.997446">
950
</page>
<figureCaption confidence="0.921992">
Figure 3: An example of confusion network in tab-
ular form
</figureCaption>
<bodyText confidence="0.976036142857143">
rank-based weights from different system can be
compared to each other without adjustment. The
weight of a cell is the same as the weight of the
corresponding row. In this paper the elaboration
of the incremental IHMM models is based on such
tabular form of CN.
Let EI1 = (E1 ... EI) denote the backbone CN,
and e0J1 = (e01... e0J) denote a hypothesis being
aligned to the backbone. Each e0j is simply a word
in the target language. However, each Ei is a span,
or a column, of the CN. We will also use E(k) to
denote the k-th row of the tabular form CN, and
Ei(k) to denote the cell at the k-th row and the
i-th column. W(k) is the weight for E(k), and
</bodyText>
<equation confidence="0.95761125">
Wi(k) = W (k) is the weight for Ei(k). pi(k)
is the normalized weight for the cell Ei(k), such
that pi(k) = Wi(k)Note that E(k) contains
Ei Wi (k)
</equation>
<bodyText confidence="0.997908833333333">
the same bag-of-words as the k-th original trans-
lation, but may have different word order. Note
also that E(k) represents a word sequence with
inserted empty symbols; the sequence with all in-
serted symbols removed is known as the compact
form of E(k).
</bodyText>
<sectionHeader confidence="0.985787" genericHeader="method">
3 The Basic IncIHMM Model
</sectionHeader>
<bodyText confidence="0.997436333333333">
A naive application of the incremental strategy to
IHMM is to treat a span in the CN as an HMM
state. Like He et al. (2008), the conditional prob-
ability of the hypothesis given the backbone CN
can be decomposed into similarity model and dis-
tortion model in accordance with equation 1
</bodyText>
<equation confidence="0.960889">
[p(aj|aj−1,I)p(e0j|eaj)1 (1)
</equation>
<bodyText confidence="0.9772465">
The similarity between a hypothesis word e0j and
a span Ei is simply a weighted sum of the similar-
ities between e0j and each word contained in Ei as
equation 2:
</bodyText>
<equation confidence="0.982871">
p(e0 j|Ei) = � pi(k) - p(e0j|Ei(k)) (2)
Ei(k)2Ei
</equation>
<bodyText confidence="0.9998645">
The similarity between two words is estimated in
exactly the same way as in conventional IHMM
alignment.
As to the distortion model, the incremental
IHMM model also groups distortion parameters
into a few ‘buckets’:
</bodyText>
<equation confidence="0.998078">
c(d) = (1 + |d − 1|)−K
</equation>
<bodyText confidence="0.999348666666667">
The problem in incremental IHMM is when to ap-
ply a bucket. In conventional IHMM, the transi-
tion from state i to j has probability:
</bodyText>
<equation confidence="0.9848975">
p0(j|i, I) = I (j − i) (3)
EL, c(l − i)
</equation>
<bodyText confidence="0.999992888888889">
It is tempting to apply the same formula to the
transitions in incremental IHMM. However, the
backbone in the incremental IHMM has a special
property that it is gradually expanding due to the
insertion operator. For example, initially the back-
bone CN contains the option ei in the i-th span and
the option ei+1 in the (i+1)-th span. After the first
round alignment, perhaps ei is aligned to the hy-
pothesis word e0j, ei+1 to e0j+2, and the hypothesis
word e0j+1 is left unaligned. Then the consequent
CN have an extra span containing the option e0j+1
inserted between the i-th and (i + 1)-th spans of
the initial CN. If the distortion buckets are applied
as in equation 3, then in the first round alignment,
the transition from the span containing ei to that
containing ei+1 is based on the bucket c(1), but
in the second round alignment, the same transition
will be based on the bucket c(2). It is therefore not
reasonable to apply equation 3 to such gradually
extending backbone as the monotonic alignment
assumption behind the equation no longer holds.
There are two possible ways to tackle this prob-
lem. The first solution estimates the transition
probability as a weighted average of different dis-
tortion probabilities, whereas the second solution
converts the distortion over spans to the distortion
over the words in each hypothesis E(k) in the CN.
</bodyText>
<subsectionHeader confidence="0.4803145">
3.1 Distortion Model 1: simple weighting of
covered n-grams
</subsectionHeader>
<bodyText confidence="0.996458571428572">
Distortion Model 1 shifts the monotonic alignment
assumption from spans of CN to n-grams covered
by state transitions. Let us illustrate this point with
the following examples.
In conventional IHMM, the distortion probabil-
ity p0(i + 1|i, I) is applied to the transition from
state i to i+1 given I states because such transition
</bodyText>
<equation confidence="0.623757714285714">
�
p(e0J1 |EI1) =
ai
1
J
H
j=1
</equation>
<page confidence="0.977516">
951
</page>
<bodyText confidence="0.999907">
jumps across only one word, viz. the i-th word of
the backbone. In incremental IHMM, suppose the
i-th span covers two arcs ea and 2, with probabili-
ties p1 and p2 = 1− p1 respectively, then the tran-
sition from state i to i + 1 jumps across one word
(ea) with probability p1 and jumps across nothing
with probability p2. Thus the transition probabil-
ity should be p1 · p0(i + 1|i, I) + p2 · p0(i|i, I).
Suppose further that the (i + 1)-th span covers
two arcs eb and 2, with probabilities p3 and p4 re-
spectively, then the transition from state i to i + 2
covers 4 possible cases:
</bodyText>
<listItem confidence="0.9989354">
1. nothing (22) with probability p2 · p4;
2. the unigram ea with probability p1 · p4;
3. the unigram eb with probability p2 · p3;
4. the bigram eaeb with probability p1 · p3.
Accordingly the transition probability should be
</listItem>
<equation confidence="0.9983585">
p2p4p0(i|i, I) + p1p3p0(i + 2|i, I) +
(p1p4 + p2p3)p0(i + 1|i, I).
</equation>
<bodyText confidence="0.999909833333334">
The estimation of transition probability can be
generalized to any transition from i to i0 by ex-
panding all possible n-grams covered by the tran-
sition and calculating the corresponding probabil-
ities. We enumerate all possible cell sequences
S(i, i0) covered by the transition from span i to
i0; each sequence is assigned the probability
where the cell at the i0-th span is on some row
E(k). Since a cell may represent an empty word,
a cell sequence may represent an n-gram where
0 ≤ n ≤ i0 − i (or 0 ≤ n ≤ i − i0 in backward
transition). We denote |S(i, i0) |to be the length of
n-gram represented by a particular cell sequence
S(i, i0). All the cell sequences S(i, i0) can be clas-
sified, with respect to the length of corresponding
n-grams, into a set of parameters where each ele-
ment (with a particular value of n) has the proba-
bility
</bodyText>
<equation confidence="0.922525">
Pii0 (n;I) = E
|S(i,i0)|=n
</equation>
<bodyText confidence="0.943633857142857">
The probability of the transition from i to i0 is:
[Pi0
i (n;I) · p0(i + n|i,I)]. (4)
That is, the transition probability of incremental
IHMM is a weighted sum of probabilities of ‘n-
gram jumping’, defined as conventional IHMM
distortion probabilities.
However, in practice it is not feasible to ex-
pand all possible n-grams covered by any transi-
tion since the number of n-grams grows exponen-
tially. Therefore a length limit L is imposed such
that for all state transitions where |i0 − i |≤ L, the
transition probability is calculated as equation 4,
otherwise it is calculated by:
</bodyText>
<equation confidence="0.948885">
p(i0|q,I) · p(q|i,I)
</equation>
<bodyText confidence="0.999738222222222">
for some q between i and i0. In other words, the
probability of longer state transition is estimated
in terms of the probabilities of transitions shorter
or equal to the length limit.2 All the state transi-
tions can be calculated efficiently by dynamic pro-
gramming.
A fixed value P0 is assigned to transitions to
null state, which can be optimized on held-out
data. The overall distortion model is:
</bodyText>
<equation confidence="0.983098333333333">
�
P0 if j is null state
˜p(j|i, I) =(1 − P0)p(j|i,I) otherwise
</equation>
<subsectionHeader confidence="0.4889775">
3.2 Distortion Model 2: weighting of
distortions of component translations
</subsectionHeader>
<bodyText confidence="0.999979789473684">
The cause of the problem of distortion over CN
spans is the gradual extension of CN due to the
inserted empty words. Therefore, the problem
will disappear if the inserted empty words are re-
moved. The rationale of Distortion Model 2 is
that the distortion model is defined over the ac-
tual word sequence in each component translation
E(k).
Distortion Model 2 implements a CN in such a
way that the real position of the i-th word of the k-
th component translation can always be retrieved.
The real position of Ei(k), δ(i, k), refers to the
position of the word represented by Ei(k) in the
compact form of E(k) (i.e. the form without any
inserted empty words), or, if Ei(k) represents an
empty word, the position of the nearest preceding
non-empty word. For convenience, we also denote
by δ2(i, k) the null state associated with the state
of the real word δ(i, k). Similarly, the real length
</bodyText>
<footnote confidence="0.51201175">
2This limit L is also imposed on the parameter I in distor-
tion probability p0(i0Ji, I), because the value of I is growing
larger and larger during the incremental alignment process. I
is defined as L if I &gt; L.
</footnote>
<equation confidence="0.954358583333333">
i0−1�
q=i
Pi0
i =
pq(k).
Pi0
i .
E
p(i0|i, I) =
n
p(i0|i,I) = max
q
</equation>
<page confidence="0.965733">
952
</page>
<bodyText confidence="0.98006875">
of E(k), L(k), refers to the number of non-empty
words of E(k).
The transition from span i0 to i is then defined
as
</bodyText>
<equation confidence="0.943673">
1 p(i |i0) = Ek W (k) E[W (k) - pk(i|i0)] )] (5) k
</equation>
<bodyText confidence="0.991032333333333">
where k is the row index of the tabular form CN.
Depending on Ei(k) and Ei,(k), pk(i|i0) is
computed as follows:
</bodyText>
<listItem confidence="0.728905">
1. if both Ei(k) and Ei,(k) represent real
words, then
</listItem>
<equation confidence="0.992821">
pk(i|i0) = p0(δ(i, k)|δ(i0, k), L(k))
</equation>
<bodyText confidence="0.999555666666667">
where p0 refers to the conventional IHMM
distortion probability as defined by equa-
tion 3.
</bodyText>
<listItem confidence="0.8869975">
2. if Ei(k) represents a real word but Ei,(k) the
empty word, then
</listItem>
<equation confidence="0.989499">
pk(i|i0) = p0(δ(i, k)|δ2(i0, k), L(k))
</equation>
<bodyText confidence="0.989859666666666">
Like conventional HMM-based word align-
ment, the probability of the transition from a
null state to a real word state is the same as
that of the transition from the real word state
associated with that null state to the other real
word state. Therefore,
</bodyText>
<equation confidence="0.6009485">
p0(δ(i, k)|δ2(i0, k), L(k)) =
p0(δ(i, k)|δ(i0, k), L(k))
</equation>
<listItem confidence="0.8212965">
3. if Ei(k) represents the empty word but
Ei,(k) a real word, then
</listItem>
<equation confidence="0.988644666666667">
�
P0 ifδ(i, k) = δ(i0, k)
pk(i|i0) = P0Pδ(i|i0; k) otherwise
</equation>
<bodyText confidence="0.871205909090909">
where Pδ(i|i0; k) = p0(δ(i, k)|δ(i0, k), L(k)).
The second option is due to the constraint that
a null state is accessible only to itself or the
real word state associated with it. Therefore,
the transition from i0 to i is in fact composed
of the first transition from i0 to δ(i, k) and the
second transition from δ(i, k) to the null state
at i.
4. if both Ei(k) and Ei,(k) represent the empty
word, then, with similar logic as cases 2
and 3,
</bodyText>
<equation confidence="0.907832">
pk(i|i0) = 5 P0 P0Pδ(i|i0; k) otherwise δ(i0, k)
</equation>
<sectionHeader confidence="0.951311" genericHeader="method">
4 Incremental Alignment using
Consensus Decoding over Multiple
IHMMs
</sectionHeader>
<bodyText confidence="0.994747">
The previous section describes an incremental
IHMM model in which the state space is based on
the CN taken as a whole. An alternative approach
is to conceive the rows (component translations)
in the CN as individuals, and transforms the align-
ment of a hypothesis against an entire network to
that against the individual translations. Each in-
dividual translation constitutes an IHMM and the
optimal alignment is obtained from consensus de-
coding over these multiple IHMMs.
Alignment over multiple sequential patterns has
been investigated in different contexts. For ex-
ample, Nair and Sreenivas (2007) proposed multi-
pattern dynamic time warping (MPDTW) to align
multiple speech utterances to each other. How-
ever, these methods usually assume that the align-
ment is monotonic. In this section, a consensus
decoding algorithm that searches for the optimal
(non-monotonic) alignment between a hypothesis
and a set of translations in a CN (which are already
aligned to each other) is developed as follows.
A prerequisite of the algorithm is a function
for converting a span index to the corresponding
HMM state index of a component translation. The
two functions δ and δ2 s defined in section 3.2 are
used to define a new function:
δ(i, k) = r δ2(i, k) if Ei (k) is null
l δ(i, k) otherwise
Accordingly, given the alignment aJ1 = a1 ... aJ
of a hypothesis (with J words) against a CN
(where each aj is an index referring to the span
of the CN), we can obtain the alignment ak =
�δ(a1, k) ... �
δ(aJ, k) between the hypothesis and
the k-th row of the tabular CN. The real length
function L(k) is also used to obtain the number of
non-empty words of E(k).
Given the k-th row of a CN, E(k), an IHMM
λ(k) is formed and the cost of the pair-wise align-
ment, ak, between a hypothesis h and λ(k) is de-
fined as:
</bodyText>
<equation confidence="0.972568">
C(ak; h, λ(k)) = −log P(�ak|h, λ(k)) (6)
</equation>
<bodyText confidence="0.998378666666667">
The cost of the alignment of h against a CN is then
defined as the weighted sum of the costs of the K
alignments ak:
</bodyText>
<equation confidence="0.9276802">
C(a; h, A) = � W (k)C(4; h, λ(k))
k
953
X= − W (k) log P(ik|h, A(k))
k
</equation>
<bodyText confidence="0.9981465">
where A = IA(k)I is the set of pair-wise IHMMs,
and W (k) is the weight of the k-th row. The op-
timal alignment a� is the one that minimizes this
cost:
</bodyText>
<equation confidence="0.997769809523809">
a� = arg max
a X W (k) log P(ak|h, A(k))
k
X
=arg max
a
k
log P( S(aj, k) |S(aj−1, k), L(k)) +
log P(ej|EZ(k))]]
X
=arg max
a
j
X W (k) log P( S(aj, k) |S(aj−1, k), L(k)) +
k
X W (k) log P(ej|EZ(k))]
k
=arg max
a X [log P&apos;(aj|aj−1) +
j
log P&apos;(ej|Eat)]
</equation>
<bodyText confidence="0.9997545">
A Viterbi-like dynamic programming algorithm
can be developed to search for a� by treating CN
spans as HMM states, with a pseudo emission
probability as
</bodyText>
<equation confidence="0.999283">
K
P&apos;(ej|Eat) = Y P(ej|Eat(k))W (k)
k=1
</equation>
<bodyText confidence="0.847804">
and a pseudo transition probability as
</bodyText>
<equation confidence="0.999085">
K
P&apos;(j|i) = Y P(�S(j, k)|�S(i, k), L(k))W(k)
k=1
</equation>
<bodyText confidence="0.983986666666667">
Note that P&apos;(ej|Eat) and P&apos;(j|i) are not true
probabilities and do not have the sum-to-one prop-
erty.
</bodyText>
<sectionHeader confidence="0.966522" genericHeader="method">
5 Alignment Normalization
</sectionHeader>
<bodyText confidence="0.999952363636364">
After alignment, the backbone CN and the hypoth-
esis can be combined to form an even larger CN.
The same principles and heuristics for the con-
struction of CN in conventional system combina-
tion approaches can be applied. Our incremen-
tal alignment approaches adopt the same heuris-
tics for alignment normalization stated in He et al.
(2008). There is one exception, though. All 1-
N mappings are not converted to N − 1 2-1 map-
pings since this conversion leads to N − 1 inser-
tion in the CN and therefore extending the net-
work to an unreasonable length. The Viterbi align-
ment is abandoned if it contains an 1-N mapping.
The best alignment which contains no 1-N map-
ping is searched in the N-Best alignments in a way
inspired by Nilsson and Goldberger (2001). For
example, if both hypothesis words e&apos;1 and e&apos;2 are
aligned to the same backbone span E1, then all
alignments aj=11,21 = i (where i =� 1) will be
examined. The alignment leading to the least re-
duction of Viterbi probability when replacing the
alignment aj=11,21 = 1 will be selected.
</bodyText>
<sectionHeader confidence="0.831066" genericHeader="method">
6 Order of Hypotheses
</sectionHeader>
<bodyText confidence="0.999987125">
The default order of hypotheses in Rosti et al.
(2008) is to rank the hypotheses in descending of
their TER scores against the backbone. This pa-
per attempts several other orders. The first one is
system-based order, i.e. assume an arbitrary order
of the MT systems and feeds all the translations
(in their original order) from a system before the
translations from the next system. The rationale
behind the system-based order is that the transla-
tions from the same system are much more similar
to each other than to the translations from other
systems, and it might be better to build CN by
incorporating similar translations first. The sec-
ond one is N-best rank-based order, which means,
rather than keeping the translations from the same
system as a block, we feed the top-1 translations
from all systems in some order of systems, and
then the second best translations from all systems,
and so on. The presumption of the rank-based or-
der is that top-ranked hypotheses are more reliable
and it seemed beneficial to incorporate more reli-
able hypotheses as early as possible. These two
kinds of order of hypotheses involve a certain de-
gree of randomness as the order of systems is arbi-
trary. Such randomness can be removed by impos-
ing a Bayes Risk order on MT systems, i.e. arrange
the MT systems in ascending order of the Bayes
Risk of their top-1 translations. These four orders
of hypotheses are summarized in Table 1. We also
tried some intuitively bad orders of hypotheses, in-
cluding the reversal of these four orders and the
random order.
</bodyText>
<sectionHeader confidence="0.996259" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.998994666666667">
The proposed approaches of incremental IHMM
are evaluated with respect to the constrained
Chinese-to-English track of 2008 NIST Open MT
</bodyText>
<equation confidence="0.840895333333333">
W (k)[ X [
j
[
</equation>
<page confidence="0.985986">
954
</page>
<table confidence="0.999782">
Order Example
System-based 1:1 ...1:N 2:1 ... 2:N ... M:1 ... M:N
N-best Rank-based 1:1 2:1 ... M:1 ...1:2 2:2 ... M:2 ...1:N ... M:N
Bayes Risk + System-based 4:1 4:2 ... 4:N ...1:1 1:2 ...1:N ... 5:1 5:2 ... 5:N
Bayes Risk + Rank-based 4:1 ...1:1 ... 5:1 4:2 ...1:2 ... 5:2 ... 4:N ...1:N ... 5:N
</table>
<tableCaption confidence="0.830151">
Table 1: The list of order of hypothesis and examples. Note that ‘m:n’ refers to the n-th translation from
the m-th system.
</tableCaption>
<bodyText confidence="0.999646414634146">
Evaluation (NIST (2008)). In the following sec-
tions, the incremental IHMM approaches using
distortion model 1 and 2 are named as IncIHMM1
and IncIHMM2 respectively, and the consensus
decoding of multiple IHMMs as CD-IHMM. The
baselines include the TER-based method in Rosti
et al. (2007), the incremental TER method in Rosti
et al. (2008), and the IHMM approach in He et
al. (2008). The development (dev) set comprises
the newswire and newsgroup sections of MT06,
whereas the test set is the entire MT08. The 10-
best translations for every source sentence in the
dev and test sets are collected from eight MT sys-
tems. Case-insensitive BLEU-4, presented in per-
centage, is used as evaluation metric.
The various parameters in the IHMM model are
set as the optimal values found in He et al. (2008).
The lexical translation probabilities used in the
semantic similarity model are estimated from a
small portion (FBIS + GALE) of the constrained
track training data, using standard HMM align-
ment model (Och and Ney (2003)). The back-
bone of CN is selected by MBR. The loss function
used for TER-based approaches is TER and that
for IHMM-based approaches is BLEU. As to the
incremental systems, the default order of hypothe-
ses is the ascending order of TER score against the
backbone, which is the order proposed in Rosti
et al. (2008). The default order of hypotheses
for our three incremental IHMM approaches is
N-best rank order with Bayes Risk system order,
which is empirically found to be giving the high-
est BLEU score. Once the CN is built, the final
system combination output can be obtained by de-
coding it with a set of features and decoding pa-
rameters. The features we used include word con-
fidences, language model score, word penalty and
empty word penalty. The decoding parameters are
trained by maximum BLEU training on the dev
set. The training and decoding processes are the
same as described by Rosti et al. (2007).
</bodyText>
<table confidence="0.994773">
Method dev test
best single system 32.60 27.75
pair-wise TER 37.90 30.96
incremental TER 38.10 31.23
pair-wise IHMM 38.52 31.65
incremental IHMM 39.22 32.63
</table>
<tableCaption confidence="0.9941255">
Table 2: Comparison between IncIHMM2 and the
three baselines
</tableCaption>
<subsectionHeader confidence="0.990865">
7.1 Comparison against Baselines
</subsectionHeader>
<bodyText confidence="0.999976136363636">
Table 2 lists the BLEU scores achieved by
the three baseline combination methods and
IncIHMM2. The comparison between pairwise
and incremental TER methods justifies the supe-
riority of the incremental strategy. However, the
benefit of incremental TER over pair-wise TER is
smaller than that mentioned in Rosti et al. (2008),
which may be because of the difference between
test sets and other experimental conditions. The
comparison between the two pair-wise alignment
methods shows that IHMM gives a 0.7 BLEU
point gain over TER, which is a bit smaller than
the difference reported in He et al. (2008). The
possible causes of such discrepancy include the
different dev set and the smaller training set for
estimating semantic similarity parameters. De-
spite that, the pair-wise IHMM method is still a
strong baseline. Table 2 also shows the perfor-
mance of IncIHMM2, our best incremental IHMM
approach. It is almost one BLEU point higher than
the pair-wise IHMM baseline and much higher
than the two TER baselines.
</bodyText>
<subsectionHeader confidence="0.6662275">
7.2 Comparison among the Incremental
IHMM Models
</subsectionHeader>
<bodyText confidence="0.997598">
Table 3 lists the BLEU scores achieved by
the three incremental IHMM approaches. The
two distortion models for IncIHMM approach
lead to almost the same performance, whereas
CD-IHMM is much less satisfactory.
For IncIHMM, the gist of both distortion mod-
</bodyText>
<page confidence="0.994106">
955
</page>
<table confidence="0.94995575">
Method dev test
IncIHMM1 39.06 32.60
IncIHMM2 39.22 32.63
CD-IHMM 38.64 31.87
</table>
<tableCaption confidence="0.8533285">
Table 3: Comparison between the three incremen-
tal IHMM approaches
</tableCaption>
<bodyText confidence="0.999072411764706">
els is to shift the distortion over spans to the dis-
tortion over word sequences. In distortion model 2
the word sequences are those sequences available
in one of the component translations in the CN.
Distortion model 1 is more encompassing as it also
considers the word sequences which are combined
from subsequences from various component trans-
lations. However, as mentioned in section 3.1,
the number of sequences grows exponentially and
there is therefore a limit L to the length of se-
quences. In general the limit L &gt; 8 would ren-
der the tuning/decoding process intolerably slow.
We tried the values 5 to 8 for L and the variation
of performance is less than 0.1 BLEU point. That
is, distortion model 1 cannot be improved by tun-
ing L. The similar BLEU scores as shown in Ta-
ble 3 implies that the incorporation of more word
sequences in distortion model 1 does not lead to
extra improvement.
Although consensus decoding is conceptually
different from both variations of IncIHMM, it
can indeed be transformed into a form similar to
IncIHMM2. IncIHMM2 calculates the parameters
of the IHMM as a weighted sum of various proba-
bilities of the component translations. In contrast,
the equations in section 4 shows that CD-IHMM
calculates the weighted sum of the logarithm of
those probabilities of the component translations.
In other words, IncIHMM2 makes use of the sum
of probabilities whereas CD-IHMM makes use
of the product of probabilities. The experiment
results indicate that the interaction between the
weights and the probabilities is more fragile in the
product case than in the summation case.
</bodyText>
<subsectionHeader confidence="0.995609">
7.3 Impact of Order of Hypotheses
</subsectionHeader>
<bodyText confidence="0.999955">
Table 4 lists the BLEU scores on the test set
achieved by IncIHMM1 using different orders of
hypotheses. The column ‘reversal’ shows the im-
pact of deliberately bad order, viz. more than one
BLEU point lower than the best order. The ran-
dom order is a baseline for not caring about or-
der of hypotheses at all, which is about 0.7 BLEU
</bodyText>
<table confidence="0.850245666666667">
normal reversal
System 32.36 31.46
Rank 32.53 31.56
BR+System 32.37 31.44
BR+Rank 32.6 31.47
random 31.94
</table>
<tableCaption confidence="0.753959">
Table 4: Comparison between various orders of
hypotheses. ‘System’ means system-based or-
</tableCaption>
<bodyText confidence="0.864012416666667">
der; ‘Rank’ means N-best rank-based order; ‘BR’
means Bayes Risk order of systems. The numbers
are the BLEU scores on the test set.
point lower than the best order. Among the orders
with good performance, it is observed that N-best
rank order leads to about 0.2 to 0.3 BLEU point
improvement, and that the Bayes Risk order of
systems does not improve performance very much.
In sum, the performance of incremental alignment
is sensitive to the order of hypotheses, and the op-
timal order is defined in terms of the rank of each
hypothesis on some system’s n-best list.
</bodyText>
<sectionHeader confidence="0.999531" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999995333333333">
This paper investigates the application of the in-
cremental strategy to IHMM, one of the state-of-
the-art alignment methods for MT output com-
bination. Such a task is subject to the prob-
lem of how to define state transitions on a grad-
ually expanding CN. We proposed three differ-
ent solutions, which share the principle that tran-
sition over CN spans must be converted to the
transition over word sequences provided by the
component translations. While the consensus de-
coding approach does not improve performance
much, the two distortion models for incremental
IHMM (IncIHMM1 and IncIHMM2) give superb
performance in comparison with pair-wise TER,
pair-wise IHMM, and incremental TER. We also
showed that the order of hypotheses is important
as a deliberately bad order would reduce transla-
tion quality by one BLEU point.
</bodyText>
<sectionHeader confidence="0.999636" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997120142857143">
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore 2008. Indirect-HMM-
based Hypothesis Alignment for Combining Out-
puts from Machine Translation Systems. Proceed-
ings of EMNLP 2008.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer 2008. Machine Translation
</reference>
<page confidence="0.986193">
956
</page>
<reference confidence="0.998722780487804">
System Combination using ITG-based Alignments.
Proceedings of ACL 2008.
Evgeny Matusov, Nicola Ueffing and Hermann Ney.
2006. Computing Consensus Translation from Mul-
tiple Machine Translation Systems using Enhanced
Hypothesis Alignment. Proceedings of EACL.
Nishanth Ulhas Nair and T.V. Sreenivas. 2007. Joint
Decoding of Multiple Speech Patterns for Robust
Speech Recognition. Proceedings of ASRU.
Dennis Nilsson and Jacob Goldberger 2001. Sequen-
tially Finding the N-Best List in Hidden Markov
Models. Proceedings of IJCAI 2001.
NIST 2008. The NIST Open Machine
Translation Evaluation. www.nist.gov/
speech/tests/mt/2008/doc/
Franz J. Och and Hermann Ney 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics 29(1):pp 19-51
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. Proceedings of
ACL 2002
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz 2007. Improved Word-level System Com-
bination for Machine Translation. Proceedings of
ACL 2007.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz 2008. Incremental Hypoth-
esis Alignment for Building Confusion Networks
with Application to Machine Translation System
Combination. Proceedings of the 3rd ACL Work-
shop on SMT.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland 2007. Con-
sensus Network Decoding for Statistical Machine
Translation System Combination. Proceedings of
ICASSP vol. 4.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla and John Makhoul 2006. A Study of
Translation Edit Rate with Targeted Human Anno-
tation. Proceedings of AMTA 2006
</reference>
<page confidence="0.99757">
957
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.622471">
<title confidence="0.998906">Incremental HMM Alignment for MT System Combination</title>
<author confidence="0.997729">Chi-Ho Li</author>
<affiliation confidence="0.999957">Microsoft Research Asia</affiliation>
<address confidence="0.999175">49 Zhichun Road, Beijing, China</address>
<email confidence="0.999855">chl@microsoft.com</email>
<author confidence="0.907993">Yupeng Liu</author>
<affiliation confidence="0.999987">Harbin Institute of Technology</affiliation>
<address confidence="0.999433">92 Xidazhi Street, Harbin, China</address>
<email confidence="0.988851">ypliu@mtlab.hit.edu.cn</email>
<abstract confidence="0.987900222222222">Inspired by the incremental TER alignment, we re-designed the Indirect HMM (IHMM) alignment, which is one of the best hypothesis alignment methods for conventional MT system combination, in an incremental manner. One crucial problem of incremental alignment is to align a hypothesis to a confusion network (CN). Our incremental IHMM alignment is implemented in three different ways: 1) treat CN spans as HMM states and define state as distortion over covered grams between two spans; 2) treat CN spans as HMM states and define state transition as distortion over words in component translations in the CN; and 3) use a consensus decoding algorithm over one hypothesis and multiple IHMMs, each of which corresponds to a component translation in the CN. All these three approaches of incremental alignment based on IHMM are shown to be superior to both incremental TER alignment and conventional IHMM alignment in the setting of the Chinese-to-English track of the 2008 NIST Open MT evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-HMMbased Hypothesis Alignment for Combining Outputs from Machine Translation Systems.</title>
<date>2008</date>
<booktitle>Proceedings of EMNLP</booktitle>
<contexts>
<context position="1953" citStr="He et al. (2008)" startWordPosition="304" endWordPosition="307">Rosti et al. (2007)) is a widely adopted approach for combining Machine Translation (MT) systems’ output. Word alignment between a backbone (or skeleton) translation and a hypothesis translation is a key problem in this approach. Translation Edit Rate (TER, Snover et al. (2006)) based alignment proposed in Sim Xiaodong He Microsoft Research One Microsoft Way, Redmond, USA xiaohe@microsoft.com Ning Xi Nanjing University 8 Hankou Road, Nanjing, China xin@nlp.nju.edu.cn et al. (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al. (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported. With an alignment method, each hypothesis is aligned against the backbone and all the alignments are then used to build a confusion network (CN) for generating a better translation. However, as pointed out by Rosti et al. (2008), such a pair-wise alignment strategy will produce a low-quality CN if there are errors in the alignment of any of the hypotheses, no matter how good the alignments of other hypotheses are. For example, suppose we have the backbone “he buys a computer” and two hyp</context>
<context position="8024" citStr="He et al. (2008)" startWordPosition="1387" endWordPosition="1390"> the k-th row and the i-th column. W(k) is the weight for E(k), and Wi(k) = W (k) is the weight for Ei(k). pi(k) is the normalized weight for the cell Ei(k), such that pi(k) = Wi(k)Note that E(k) contains Ei Wi (k) the same bag-of-words as the k-th original translation, but may have different word order. Note also that E(k) represents a word sequence with inserted empty symbols; the sequence with all inserted symbols removed is known as the compact form of E(k). 3 The Basic IncIHMM Model A naive application of the incremental strategy to IHMM is to treat a span in the CN as an HMM state. Like He et al. (2008), the conditional probability of the hypothesis given the backbone CN can be decomposed into similarity model and distortion model in accordance with equation 1 [p(aj|aj−1,I)p(e0j|eaj)1 (1) The similarity between a hypothesis word e0j and a span Ei is simply a weighted sum of the similarities between e0j and each word contained in Ei as equation 2: p(e0 j|Ei) = � pi(k) - p(e0j|Ei(k)) (2) Ei(k)2Ei The similarity between two words is estimated in exactly the same way as in conventional IHMM alignment. As to the distortion model, the incremental IHMM model also groups distortion parameters into a</context>
<context position="19387" citStr="He et al. (2008)" startWordPosition="3453" endWordPosition="3456">o emission probability as K P&apos;(ej|Eat) = Y P(ej|Eat(k))W (k) k=1 and a pseudo transition probability as K P&apos;(j|i) = Y P(�S(j, k)|�S(i, k), L(k))W(k) k=1 Note that P&apos;(ej|Eat) and P&apos;(j|i) are not true probabilities and do not have the sum-to-one property. 5 Alignment Normalization After alignment, the backbone CN and the hypothesis can be combined to form an even larger CN. The same principles and heuristics for the construction of CN in conventional system combination approaches can be applied. Our incremental alignment approaches adopt the same heuristics for alignment normalization stated in He et al. (2008). There is one exception, though. All 1- N mappings are not converted to N − 1 2-1 mappings since this conversion leads to N − 1 insertion in the CN and therefore extending the network to an unreasonable length. The Viterbi alignment is abandoned if it contains an 1-N mapping. The best alignment which contains no 1-N mapping is searched in the N-Best alignments in a way inspired by Nilsson and Goldberger (2001). For example, if both hypothesis words e&apos;1 and e&apos;2 are aligned to the same backbone span E1, then all alignments aj=11,21 = i (where i =� 1) will be examined. The alignment leading to t</context>
<context position="22627" citStr="He et al. (2008)" startWordPosition="4024" endWordPosition="4027">..1:N ... 5:1 5:2 ... 5:N Bayes Risk + Rank-based 4:1 ...1:1 ... 5:1 4:2 ...1:2 ... 5:2 ... 4:N ...1:N ... 5:N Table 1: The list of order of hypothesis and examples. Note that ‘m:n’ refers to the n-th translation from the m-th system. Evaluation (NIST (2008)). In the following sections, the incremental IHMM approaches using distortion model 1 and 2 are named as IncIHMM1 and IncIHMM2 respectively, and the consensus decoding of multiple IHMMs as CD-IHMM. The baselines include the TER-based method in Rosti et al. (2007), the incremental TER method in Rosti et al. (2008), and the IHMM approach in He et al. (2008). The development (dev) set comprises the newswire and newsgroup sections of MT06, whereas the test set is the entire MT08. The 10- best translations for every source sentence in the dev and test sets are collected from eight MT systems. Case-insensitive BLEU-4, presented in percentage, is used as evaluation metric. The various parameters in the IHMM model are set as the optimal values found in He et al. (2008). The lexical translation probabilities used in the semantic similarity model are estimated from a small portion (FBIS + GALE) of the constrained track training data, using standard HMM </context>
<context position="25022" citStr="He et al. (2008)" startWordPosition="4423" endWordPosition="4426">ainst Baselines Table 2 lists the BLEU scores achieved by the three baseline combination methods and IncIHMM2. The comparison between pairwise and incremental TER methods justifies the superiority of the incremental strategy. However, the benefit of incremental TER over pair-wise TER is smaller than that mentioned in Rosti et al. (2008), which may be because of the difference between test sets and other experimental conditions. The comparison between the two pair-wise alignment methods shows that IHMM gives a 0.7 BLEU point gain over TER, which is a bit smaller than the difference reported in He et al. (2008). The possible causes of such discrepancy include the different dev set and the smaller training set for estimating semantic similarity parameters. Despite that, the pair-wise IHMM method is still a strong baseline. Table 2 also shows the performance of IncIHMM2, our best incremental IHMM approach. It is almost one BLEU point higher than the pair-wise IHMM baseline and much higher than the two TER baselines. 7.2 Comparison among the Incremental IHMM Models Table 3 lists the BLEU scores achieved by the three incremental IHMM approaches. The two distortion models for IncIHMM approach lead to alm</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore 2008. Indirect-HMMbased Hypothesis Alignment for Combining Outputs from Machine Translation Systems. Proceedings of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damianos Karakos</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
<author>Markus Dreyer</author>
</authors>
<title>Machine Translation System Combination using ITG-based Alignments.</title>
<date>2008</date>
<booktitle>Proceedings of ACL</booktitle>
<contexts>
<context position="2005" citStr="Karakos et al. (2008)" startWordPosition="312" endWordPosition="315">ch for combining Machine Translation (MT) systems’ output. Word alignment between a backbone (or skeleton) translation and a hypothesis translation is a key problem in this approach. Translation Edit Rate (TER, Snover et al. (2006)) based alignment proposed in Sim Xiaodong He Microsoft Research One Microsoft Way, Redmond, USA xiaohe@microsoft.com Ning Xi Nanjing University 8 Hankou Road, Nanjing, China xin@nlp.nju.edu.cn et al. (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al. (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported. With an alignment method, each hypothesis is aligned against the backbone and all the alignments are then used to build a confusion network (CN) for generating a better translation. However, as pointed out by Rosti et al. (2008), such a pair-wise alignment strategy will produce a low-quality CN if there are errors in the alignment of any of the hypotheses, no matter how good the alignments of other hypotheses are. For example, suppose we have the backbone “he buys a computer” and two hypotheses “he bought a laptop computer” and “he buys a</context>
</contexts>
<marker>Karakos, Eisner, Khudanpur, Dreyer, 2008</marker>
<rawString>Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, and Markus Dreyer 2008. Machine Translation System Combination using ITG-based Alignments. Proceedings of ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing Consensus Translation from Multiple Machine Translation Systems using Enhanced Hypothesis Alignment.</title>
<date>2006</date>
<booktitle>Proceedings of EACL.</booktitle>
<contexts>
<context position="1332" citStr="Matusov et al. (2006)" startWordPosition="206" endWordPosition="209">d ngrams between two spans; 2) treat CN spans as HMM states and define state transition as distortion over words in component translations in the CN; and 3) use a consensus decoding algorithm over one hypothesis and multiple IHMMs, each of which corresponds to a component translation in the CN. All these three approaches of incremental alignment based on IHMM are shown to be superior to both incremental TER alignment and conventional IHMM alignment in the setting of the Chinese-to-English track of the 2008 NIST Open MT evaluation. 1 Introduction Word-level combination using confusion network (Matusov et al. (2006) and Rosti et al. (2007)) is a widely adopted approach for combining Machine Translation (MT) systems’ output. Word alignment between a backbone (or skeleton) translation and a hypothesis translation is a key problem in this approach. Translation Edit Rate (TER, Snover et al. (2006)) based alignment proposed in Sim Xiaodong He Microsoft Research One Microsoft Way, Redmond, USA xiaohe@microsoft.com Ning Xi Nanjing University 8 Hankou Road, Nanjing, China xin@nlp.nju.edu.cn et al. (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (I</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing and Hermann Ney. 2006. Computing Consensus Translation from Multiple Machine Translation Systems using Enhanced Hypothesis Alignment. Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nishanth Ulhas Nair</author>
<author>T V Sreenivas</author>
</authors>
<title>Joint Decoding of Multiple Speech Patterns for Robust Speech Recognition.</title>
<date>2007</date>
<booktitle>Proceedings of ASRU.</booktitle>
<contexts>
<context position="16815" citStr="Nair and Sreenivas (2007)" startWordPosition="2962" endWordPosition="2965">coding over Multiple IHMMs The previous section describes an incremental IHMM model in which the state space is based on the CN taken as a whole. An alternative approach is to conceive the rows (component translations) in the CN as individuals, and transforms the alignment of a hypothesis against an entire network to that against the individual translations. Each individual translation constitutes an IHMM and the optimal alignment is obtained from consensus decoding over these multiple IHMMs. Alignment over multiple sequential patterns has been investigated in different contexts. For example, Nair and Sreenivas (2007) proposed multipattern dynamic time warping (MPDTW) to align multiple speech utterances to each other. However, these methods usually assume that the alignment is monotonic. In this section, a consensus decoding algorithm that searches for the optimal (non-monotonic) alignment between a hypothesis and a set of translations in a CN (which are already aligned to each other) is developed as follows. A prerequisite of the algorithm is a function for converting a span index to the corresponding HMM state index of a component translation. The two functions δ and δ2 s defined in section 3.2 are used </context>
</contexts>
<marker>Nair, Sreenivas, 2007</marker>
<rawString>Nishanth Ulhas Nair and T.V. Sreenivas. 2007. Joint Decoding of Multiple Speech Patterns for Robust Speech Recognition. Proceedings of ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Nilsson</author>
<author>Jacob Goldberger</author>
</authors>
<title>Sequentially Finding the N-Best List in Hidden Markov Models.</title>
<date>2001</date>
<booktitle>Proceedings of IJCAI</booktitle>
<contexts>
<context position="19801" citStr="Nilsson and Goldberger (2001)" startWordPosition="3531" endWordPosition="3534">uristics for the construction of CN in conventional system combination approaches can be applied. Our incremental alignment approaches adopt the same heuristics for alignment normalization stated in He et al. (2008). There is one exception, though. All 1- N mappings are not converted to N − 1 2-1 mappings since this conversion leads to N − 1 insertion in the CN and therefore extending the network to an unreasonable length. The Viterbi alignment is abandoned if it contains an 1-N mapping. The best alignment which contains no 1-N mapping is searched in the N-Best alignments in a way inspired by Nilsson and Goldberger (2001). For example, if both hypothesis words e&apos;1 and e&apos;2 are aligned to the same backbone span E1, then all alignments aj=11,21 = i (where i =� 1) will be examined. The alignment leading to the least reduction of Viterbi probability when replacing the alignment aj=11,21 = 1 will be selected. 6 Order of Hypotheses The default order of hypotheses in Rosti et al. (2008) is to rank the hypotheses in descending of their TER scores against the backbone. This paper attempts several other orders. The first one is system-based order, i.e. assume an arbitrary order of the MT systems and feeds all the transla</context>
</contexts>
<marker>Nilsson, Goldberger, 2001</marker>
<rawString>Dennis Nilsson and Jacob Goldberger 2001. Sequentially Finding the N-Best List in Hidden Markov Models. Proceedings of IJCAI 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The NIST Open Machine Translation Evaluation.</title>
<date>2008</date>
<note>www.nist.gov/ speech/tests/mt/2008/doc/</note>
<contexts>
<context position="22269" citStr="NIST (2008)" startWordPosition="3966" endWordPosition="3967">valuation The proposed approaches of incremental IHMM are evaluated with respect to the constrained Chinese-to-English track of 2008 NIST Open MT W (k)[ X [ j [ 954 Order Example System-based 1:1 ...1:N 2:1 ... 2:N ... M:1 ... M:N N-best Rank-based 1:1 2:1 ... M:1 ...1:2 2:2 ... M:2 ...1:N ... M:N Bayes Risk + System-based 4:1 4:2 ... 4:N ...1:1 1:2 ...1:N ... 5:1 5:2 ... 5:N Bayes Risk + Rank-based 4:1 ...1:1 ... 5:1 4:2 ...1:2 ... 5:2 ... 4:N ...1:N ... 5:N Table 1: The list of order of hypothesis and examples. Note that ‘m:n’ refers to the n-th translation from the m-th system. Evaluation (NIST (2008)). In the following sections, the incremental IHMM approaches using distortion model 1 and 2 are named as IncIHMM1 and IncIHMM2 respectively, and the consensus decoding of multiple IHMMs as CD-IHMM. The baselines include the TER-based method in Rosti et al. (2007), the incremental TER method in Rosti et al. (2008), and the IHMM approach in He et al. (2008). The development (dev) set comprises the newswire and newsgroup sections of MT06, whereas the test set is the entire MT08. The 10- best translations for every source sentence in the dev and test sets are collected from eight MT systems. Case</context>
</contexts>
<marker>NIST, 2008</marker>
<rawString>NIST 2008. The NIST Open Machine Translation Evaluation. www.nist.gov/ speech/tests/mt/2008/doc/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="23262" citStr="Och and Ney (2003)" startWordPosition="4130" endWordPosition="4133"> (dev) set comprises the newswire and newsgroup sections of MT06, whereas the test set is the entire MT08. The 10- best translations for every source sentence in the dev and test sets are collected from eight MT systems. Case-insensitive BLEU-4, presented in percentage, is used as evaluation metric. The various parameters in the IHMM model are set as the optimal values found in He et al. (2008). The lexical translation probabilities used in the semantic similarity model are estimated from a small portion (FBIS + GALE) of the constrained track training data, using standard HMM alignment model (Och and Ney (2003)). The backbone of CN is selected by MBR. The loss function used for TER-based approaches is TER and that for IHMM-based approaches is BLEU. As to the incremental systems, the default order of hypotheses is the ascending order of TER score against the backbone, which is the order proposed in Rosti et al. (2008). The default order of hypotheses for our three incremental IHMM approaches is N-best rank order with Bayes Risk system order, which is empirically found to be giving the highest BLEU score. Once the CN is built, the final system combination output can be obtained by decoding it with a s</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics 29(1):pp 19-51</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
</authors>
<title>Salim Roukos, Todd Ward and WeiJing Zhu</title>
<date>2002</date>
<booktitle>Proceedings of ACL</booktitle>
<marker>Papineni, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward and WeiJing Zhu 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. Proceedings of ACL 2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved Word-level System Combination for Machine Translation.</title>
<date>2007</date>
<booktitle>Proceedings of ACL</booktitle>
<contexts>
<context position="1356" citStr="Rosti et al. (2007)" startWordPosition="211" endWordPosition="214">; 2) treat CN spans as HMM states and define state transition as distortion over words in component translations in the CN; and 3) use a consensus decoding algorithm over one hypothesis and multiple IHMMs, each of which corresponds to a component translation in the CN. All these three approaches of incremental alignment based on IHMM are shown to be superior to both incremental TER alignment and conventional IHMM alignment in the setting of the Chinese-to-English track of the 2008 NIST Open MT evaluation. 1 Introduction Word-level combination using confusion network (Matusov et al. (2006) and Rosti et al. (2007)) is a widely adopted approach for combining Machine Translation (MT) systems’ output. Word alignment between a backbone (or skeleton) translation and a hypothesis translation is a key problem in this approach. Translation Edit Rate (TER, Snover et al. (2006)) based alignment proposed in Sim Xiaodong He Microsoft Research One Microsoft Way, Redmond, USA xiaohe@microsoft.com Ning Xi Nanjing University 8 Hankou Road, Nanjing, China xin@nlp.nju.edu.cn et al. (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al. (2008)) a</context>
<context position="5816" citStr="Rosti et al. (2007)" startWordPosition="961" endWordPosition="964">nd the order of hypotheses being added into a CN are discussed in sections 5 and 6 respectively. Experiment results and analysis are presented in section 7. 2 Preliminaries: Notation on Confusion Network Before the elaboration of the models, let us first clarify the notation on CN. A CN is usually described as a finite state graph with many spans. Each span corresponds to a word position and contains several arcs, each of which represents an alternative word (could be the empty symbol , E) at that position. Each arc is also associated with M weights in an M-way system combination task. Follow Rosti et al. (2007), the i-th weight of an arc is Er 11 , where r is the rank of the hypothesis in the i-th system that votes for the word represented by the arc. This conception of CN is called the conventional or compact form of CN. The networks in Figures 1b and 2b are examples. On the other hand, as a CN is an integration of the skeleton and all hypotheses, it can be conceived as a list of the component translations. For example, the CN in Figure 2b can be converted to the form in Figure 3. In such an expanded or tabular form, each row represents a component translation. Each column, which is equivalent to a</context>
<context position="22533" citStr="Rosti et al. (2007)" startWordPosition="4006" endWordPosition="4009"> ... M:1 ...1:2 2:2 ... M:2 ...1:N ... M:N Bayes Risk + System-based 4:1 4:2 ... 4:N ...1:1 1:2 ...1:N ... 5:1 5:2 ... 5:N Bayes Risk + Rank-based 4:1 ...1:1 ... 5:1 4:2 ...1:2 ... 5:2 ... 4:N ...1:N ... 5:N Table 1: The list of order of hypothesis and examples. Note that ‘m:n’ refers to the n-th translation from the m-th system. Evaluation (NIST (2008)). In the following sections, the incremental IHMM approaches using distortion model 1 and 2 are named as IncIHMM1 and IncIHMM2 respectively, and the consensus decoding of multiple IHMMs as CD-IHMM. The baselines include the TER-based method in Rosti et al. (2007), the incremental TER method in Rosti et al. (2008), and the IHMM approach in He et al. (2008). The development (dev) set comprises the newswire and newsgroup sections of MT06, whereas the test set is the entire MT08. The 10- best translations for every source sentence in the dev and test sets are collected from eight MT systems. Case-insensitive BLEU-4, presented in percentage, is used as evaluation metric. The various parameters in the IHMM model are set as the optimal values found in He et al. (2008). The lexical translation probabilities used in the semantic similarity model are estimated </context>
<context position="24169" citStr="Rosti et al. (2007)" startWordPosition="4289" endWordPosition="4292">d in Rosti et al. (2008). The default order of hypotheses for our three incremental IHMM approaches is N-best rank order with Bayes Risk system order, which is empirically found to be giving the highest BLEU score. Once the CN is built, the final system combination output can be obtained by decoding it with a set of features and decoding parameters. The features we used include word confidences, language model score, word penalty and empty word penalty. The decoding parameters are trained by maximum BLEU training on the dev set. The training and decoding processes are the same as described by Rosti et al. (2007). Method dev test best single system 32.60 27.75 pair-wise TER 37.90 30.96 incremental TER 38.10 31.23 pair-wise IHMM 38.52 31.65 incremental IHMM 39.22 32.63 Table 2: Comparison between IncIHMM2 and the three baselines 7.1 Comparison against Baselines Table 2 lists the BLEU scores achieved by the three baseline combination methods and IncIHMM2. The comparison between pairwise and incremental TER methods justifies the superiority of the incremental strategy. However, the benefit of incremental TER over pair-wise TER is smaller than that mentioned in Rosti et al. (2008), which may be because of</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard Schwartz 2007. Improved Word-level System Combination for Machine Translation. Proceedings of ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Incremental Hypothesis Alignment for Building Confusion Networks with Application to Machine Translation System Combination.</title>
<date>2008</date>
<booktitle>Proceedings of the 3rd ACL Workshop on SMT.</booktitle>
<contexts>
<context position="2289" citStr="Rosti et al. (2008)" startWordPosition="358" endWordPosition="361"> Research One Microsoft Way, Redmond, USA xiaohe@microsoft.com Ning Xi Nanjing University 8 Hankou Road, Nanjing, China xin@nlp.nju.edu.cn et al. (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al. (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported. With an alignment method, each hypothesis is aligned against the backbone and all the alignments are then used to build a confusion network (CN) for generating a better translation. However, as pointed out by Rosti et al. (2008), such a pair-wise alignment strategy will produce a low-quality CN if there are errors in the alignment of any of the hypotheses, no matter how good the alignments of other hypotheses are. For example, suppose we have the backbone “he buys a computer” and two hypotheses “he bought a laptop computer” and “he buys a laptop”. It will be natural for most alignment methods to produce the alignments in Figure 1a. The alignment of hypothesis 2 against the backbone cannot be considered an error if we consider only these two translations; nevertheless, when added with the alignment of another hypothes</context>
<context position="4142" citStr="Rosti et al. (2008)" startWordPosition="680" endWordPosition="683">complete sentence “he bought a”, which is nevertheless unlikely to be selected as it leads to low language model score. 949 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 949–957, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP Figure 1: An example bad confusion network due to pair-wise alignment strategy Figure 2: An example good confusion network due to incremental alignment strategy correct result depends on the order of hypotheses. If hypothesis 2 is aligned before hypothesis 1, the final CN will not be good. Therefore, the observation in Rosti et al. (2008) that different order of hypotheses does not affect translation quality is counter-intuitive. This paper attempts to answer two questions: 1) as incremental TER alignment gives better performance than pair-wise TER alignment, would the incremental strategy still be better than the pairwise strategy if the TER method is replaced by another alignment method? 2) how does translation quality vary for different orders of hypotheses being incrementally added into a CN? For question 1, we will focus on the IHMM alignment method and propose three different ways of implementing incremental IHMM alignme</context>
<context position="20165" citStr="Rosti et al. (2008)" startWordPosition="3596" endWordPosition="3599">ore extending the network to an unreasonable length. The Viterbi alignment is abandoned if it contains an 1-N mapping. The best alignment which contains no 1-N mapping is searched in the N-Best alignments in a way inspired by Nilsson and Goldberger (2001). For example, if both hypothesis words e&apos;1 and e&apos;2 are aligned to the same backbone span E1, then all alignments aj=11,21 = i (where i =� 1) will be examined. The alignment leading to the least reduction of Viterbi probability when replacing the alignment aj=11,21 = 1 will be selected. 6 Order of Hypotheses The default order of hypotheses in Rosti et al. (2008) is to rank the hypotheses in descending of their TER scores against the backbone. This paper attempts several other orders. The first one is system-based order, i.e. assume an arbitrary order of the MT systems and feeds all the translations (in their original order) from a system before the translations from the next system. The rationale behind the system-based order is that the translations from the same system are much more similar to each other than to the translations from other systems, and it might be better to build CN by incorporating similar translations first. The second one is N-b</context>
<context position="22584" citStr="Rosti et al. (2008)" startWordPosition="4015" endWordPosition="4018">sk + System-based 4:1 4:2 ... 4:N ...1:1 1:2 ...1:N ... 5:1 5:2 ... 5:N Bayes Risk + Rank-based 4:1 ...1:1 ... 5:1 4:2 ...1:2 ... 5:2 ... 4:N ...1:N ... 5:N Table 1: The list of order of hypothesis and examples. Note that ‘m:n’ refers to the n-th translation from the m-th system. Evaluation (NIST (2008)). In the following sections, the incremental IHMM approaches using distortion model 1 and 2 are named as IncIHMM1 and IncIHMM2 respectively, and the consensus decoding of multiple IHMMs as CD-IHMM. The baselines include the TER-based method in Rosti et al. (2007), the incremental TER method in Rosti et al. (2008), and the IHMM approach in He et al. (2008). The development (dev) set comprises the newswire and newsgroup sections of MT06, whereas the test set is the entire MT08. The 10- best translations for every source sentence in the dev and test sets are collected from eight MT systems. Case-insensitive BLEU-4, presented in percentage, is used as evaluation metric. The various parameters in the IHMM model are set as the optimal values found in He et al. (2008). The lexical translation probabilities used in the semantic similarity model are estimated from a small portion (FBIS + GALE) of the constrain</context>
<context position="24744" citStr="Rosti et al. (2008)" startWordPosition="4376" endWordPosition="4379">the same as described by Rosti et al. (2007). Method dev test best single system 32.60 27.75 pair-wise TER 37.90 30.96 incremental TER 38.10 31.23 pair-wise IHMM 38.52 31.65 incremental IHMM 39.22 32.63 Table 2: Comparison between IncIHMM2 and the three baselines 7.1 Comparison against Baselines Table 2 lists the BLEU scores achieved by the three baseline combination methods and IncIHMM2. The comparison between pairwise and incremental TER methods justifies the superiority of the incremental strategy. However, the benefit of incremental TER over pair-wise TER is smaller than that mentioned in Rosti et al. (2008), which may be because of the difference between test sets and other experimental conditions. The comparison between the two pair-wise alignment methods shows that IHMM gives a 0.7 BLEU point gain over TER, which is a bit smaller than the difference reported in He et al. (2008). The possible causes of such discrepancy include the different dev set and the smaller training set for estimating semantic similarity parameters. Despite that, the pair-wise IHMM method is still a strong baseline. Table 2 also shows the performance of IncIHMM2, our best incremental IHMM approach. It is almost one BLEU </context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2008</marker>
<rawString>Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz 2008. Incremental Hypothesis Alignment for Building Confusion Networks with Application to Machine Translation System Combination. Proceedings of the 3rd ACL Workshop on SMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khe Chai Sim</author>
<author>William J Byrne</author>
<author>Mark J F Gales</author>
<author>Hichem Sahbi</author>
<author>Phil C Woodland</author>
</authors>
<title>Consensus Network Decoding for Statistical Machine Translation System Combination.</title>
<date>2007</date>
<journal>Proceedings of ICASSP</journal>
<volume>4</volume>
<marker>Sim, Byrne, Gales, Sahbi, Woodland, 2007</marker>
<rawString>Khe Chai Sim, William J. Byrne, Mark J.F. Gales, Hichem Sahbi, and Phil C. Woodland 2007. Consensus Network Decoding for Statistical Machine Translation System Combination. Proceedings of ICASSP vol. 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Rich Schwartz</author>
</authors>
<title>Linnea Micciulla and John Makhoul</title>
<date>2006</date>
<booktitle>Proceedings of AMTA</booktitle>
<contexts>
<context position="1615" citStr="Snover et al. (2006)" startWordPosition="251" endWordPosition="254"> in the CN. All these three approaches of incremental alignment based on IHMM are shown to be superior to both incremental TER alignment and conventional IHMM alignment in the setting of the Chinese-to-English track of the 2008 NIST Open MT evaluation. 1 Introduction Word-level combination using confusion network (Matusov et al. (2006) and Rosti et al. (2007)) is a widely adopted approach for combining Machine Translation (MT) systems’ output. Word alignment between a backbone (or skeleton) translation and a hypothesis translation is a key problem in this approach. Translation Edit Rate (TER, Snover et al. (2006)) based alignment proposed in Sim Xiaodong He Microsoft Research One Microsoft Way, Redmond, USA xiaohe@microsoft.com Ning Xi Nanjing University 8 Hankou Road, Nanjing, China xin@nlp.nju.edu.cn et al. (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al. (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported. With an alignment method, each hypothesis is aligned against the backbone and all the alignments are then used to build a confusion network (CN) for gener</context>
</contexts>
<marker>Snover, Dorr, Schwartz, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla and John Makhoul 2006. A Study of Translation Edit Rate with Targeted Human Annotation. Proceedings of AMTA 2006</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>