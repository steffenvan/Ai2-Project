<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000096">
<title confidence="0.998025">
Coupling Semi-Supervised Learning of Categories and Relations
</title>
<author confidence="0.999581">
Andrew Carlson1, Justin Betteridge1, Estevam R. Hruschka Jr.1,2 and Tom M. Mitchell1
</author>
<affiliation confidence="0.9872385">
1School of Computer Science
Carnegie Mellon University
</affiliation>
<address confidence="0.711162">
Pittsburgh, PA 15213
</address>
<email confidence="0.995668">
{acarlson,jbetter,tom.mitchell}@cs.cmu.edu
</email>
<affiliation confidence="0.973744">
2Federal University of Sao Carlos
</affiliation>
<address confidence="0.946434">
Sao Carlos, SP - Brazil
</address>
<email confidence="0.998925">
estevam@dc.ufscar.br
</email>
<sectionHeader confidence="0.998598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99969235">
We consider semi-supervised learning of
information extraction methods, especially
for extracting instances of noun categories
(e.g., ‘athlete,’ ‘team’) and relations (e.g.,
‘playsForTeam(athlete,team)’). Semi-
supervised approaches using a small number
of labeled examples together with many un-
labeled examples are often unreliable as they
frequently produce an internally consistent,
but nevertheless incorrect set of extractions.
We propose that this problem can be over-
come by simultaneously learning classifiers
for many different categories and relations
in the presence of an ontology defining
constraints that couple the training of these
classifiers. Experimental results show that
simultaneously learning a coupled collection
of classifiers for 30 categories and relations
results in much more accurate extractions
than training classifiers individually.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99990675">
A great wealth of knowledge is expressed on the web
in natural language. Translating this into a struc-
tured knowledge base containing facts about enti-
ties (e.g., ‘Disney’) and relations between those en-
tities (e.g. CompanyIndustry(‘Disney’, ‘entertain-
ment’)) would be of great use to many applications.
Although fully supervised methods for learning to
extract such facts from text work well, the cost
of collecting many labeled examples of each type
of knowledge to be extracted is impractical. Re-
searchers have also explored semi-supervised learn-
ing methods that rely primarily on unlabeled data,
</bodyText>
<page confidence="0.913032">
1
</page>
<figureCaption confidence="0.943019">
Figure 1: We show that significant improvements in ac-
</figureCaption>
<bodyText confidence="0.977045217391305">
curacy result from coupling the training of information
extractors for many inter-related categories and relations
(B), compared with the simpler but much more difficult
task of learning a single information extractor (A).
but these approaches tend to suffer from the fact that
they face an under-constrained learning task, result-
ing in extractions that are often inaccurate.
We present an approach to semi-supervised learn-
ing that yields more accurate results by coupling the
training of many information extractors. The intu-
ition behind our approach (summarized in Figure 1)
is that semi-supervised training of a single type of
extractor such as ‘coach’ is much more difficult than
simultaneously training many extractors that cover
a variety of inter-related entity and relation types.
In particular, prior knowledge about the relation-
ships between these different entities and relations
(e.g., that ‘coach(x)’ implies ‘person(x)’ and ‘not
sport(x)’) allows unlabeled data to become a much
more useful constraint during training.
Although previous work has coupled the learning
of multiple categories, or used static category rec-
ognizers to check arguments for learned relation ex-
</bodyText>
<note confidence="0.9882565">
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 1–9,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999834">
tractors, our work is the first we know of to couple
the simultaneous semi-supervised training of multi-
ple categories and relations. Our experiments show
that this coupling results in more accurate extrac-
tions. Based on our results reported here, we hy-
pothesize that significant accuracy improvements in
information extraction will be possible by coupling
the training of hundreds or thousands of extractors.
</bodyText>
<sectionHeader confidence="0.980158" genericHeader="introduction">
2 Problem Statement
</sectionHeader>
<bodyText confidence="0.99998124137931">
It will be helpful to first explain our use of common
terms. An ontology is a collection of unary and bi-
nary predicates, also called categories and relations,
respectively.1 An instance of a category, or a cate-
gory instance, is a noun phrase; an instance of a rela-
tion, or a relation instance, is a pair of noun phrases.
Instances can be positive or negative with respect
to a specific predicate, meaning that the predicate
holds or does not hold for that particular instance.
A promoted instance is an instance which our algo-
rithm believes to be a positive instance of some pred-
icate. Also associated with both categories and rela-
tions are patterns: strings of tokens with placehold-
ers (e.g., ‘game against arg1’ and ‘arg1 , head coach
of arg2’). A promoted pattern is a pattern believed
to be a high-probability indicator for some predicate.
The challenge addressed by this work is to learn
extractors to automatically populate the categories
and relations of a specified ontology with high-
confidence instances, starting from a few seed pos-
itive instances and patterns for each predicate and
a large corpus of sentences annotated with part-of-
speech (POS) tags. We focus on extracting facts that
are stated multiple times in the corpus, which we
can assess probabilistically using corpus statistics.
We do not resolve strings to real-world entities— the
problems of synonym resolution and disambiguation
of strings that can refer to multiple entities are left
for future work.
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.9847326">
Work on multitask learning has demonstrated that
supervised learning of multiple “related” functions
together can yield higher accuracy than learning the
functions separately (Thrun, 1996; Caruana, 1997).
Semi-supervised multitask learning has been shown
</bodyText>
<footnote confidence="0.780877">
1We do not consider predicates of higher arity in this work.
</footnote>
<bodyText confidence="0.999679833333333">
to increase accuracy when tasks are related, allow-
ing one to use a prior that encourages similar pa-
rameters (Liu et al., 2008). Our work also involves
semi-supervised training of multiple coupled func-
tions, but differs in that we assume explicit prior
knowledge of the precise way in which our multi-
ple functions are related (e.g., that the values of the
functions applied to the same input are mutually ex-
clusive, or that one implies the other).
In this paper, we focus on a ‘bootstrapping’
method for semi-supervised learning. Bootstrap-
ping approaches start with a small number of la-
beled ‘seed’ examples, use those seed examples to
train an initial model, then use this model to la-
bel some of the unlabeled data. The model is
then retrained, using the original seed examples plus
the self-labeled examples. This process iterates,
gradually expanding the amount of labeled data.
Such approaches have shown promise in applica-
tions such as web page classification (Blum and
Mitchell, 1998), named entity classification (Collins
and Singer, 1999), parsing (McClosky et al., 2006),
and machine translation (Ueffing, 2006).
Bootstrapping approaches to information extrac-
tion can yield impressive results with little initial
human effort (Brin, 1998; Agichtein and Gravano,
2000; Ravichandran and Hovy, 2002; Pasca et al.,
2006). However, after many iterations, they usu-
ally suffer from semantic drift, where errors in label-
ing accumulate and the learned concept ‘drifts’ from
what was intended (Curran et al., 2007). Coupling
the learning of predicates by using positive exam-
ples of one predicate as negative examples for oth-
ers has been shown to help limit this drift (Riloff and
Jones, 1999; Yangarber, 2003). Additionally, ensur-
ing that relation arguments are of certain, expected
types can help mitigate the promotion of incorrect
instances (Pas¸ca et al., 2006; Rosenfeld and Feld-
man, 2007). Our work builds on these ideas to cou-
ple the simultaneous bootstrapped training of multi-
ple categories and multiple relations.
Our approach to information extraction is based
on using high precision contextual patterns (e.g., ‘is
mayor of arg1’ suggests that arg1 is a city). An early
pattern-based approach to information extraction ac-
quired ‘is a’ relations from text using generic con-
textual patterns (Hearst, 1992). This approach was
later scaled up to the web by Etzioni et al. (2005).
</bodyText>
<page confidence="0.988283">
2
</page>
<bodyText confidence="0.9999865">
Other research explores the task of ‘open informa-
tion extraction’, where the predicates to be learned
are not specified in advance (Shinyama and Sekine,
2006; Banko et al., 2007), but emerge instead from
analysis of the data. In contrast, our approach re-
lies strongly on knowledge in the ontology about the
predicates to be learned, and relationships among
them, in order to achieve high accuracy.
Chang et al. (2007) present a framework for
learning that optimizes the data likelihood plus
constraint-based penalty terms than capture prior
knowledge, and demonstrate it with semi-supervised
learning of segmentation models. Constraints that
capture domain knowledge guide bootstrap learn-
ing of a structured model by penalizing or disallow-
ing violations of those constraints. While similar in
spirit, our work differs in that we consider learning
many models, rather than one structured model, and
that we are consider a much larger scale application
in a different domain.
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="method">
4 Approach
</sectionHeader>
<subsectionHeader confidence="0.999998">
4.1 Coupling of Predicates
</subsectionHeader>
<bodyText confidence="0.999791">
As mentioned above, our approach hinges on the no-
tion of coupling the learning of multiple functions
in order to constrain the semi-supervised learning
problem we face. Our system learns four different
types of functions. For each category c:
</bodyText>
<listItem confidence="0.99955">
1. fc,inst : NP(C) → [0, 1]
2. fc,patt : PattC(C) → [0, 1]
and for each relation r:
1. fr,inst : NP(C) × NP(C) → [0, 1]
2. fr,patt : PattR(C) → [0, 1]
</listItem>
<bodyText confidence="0.996408125">
where C is the input corpus, NP(C) is the set of
valid noun phrases in C, PattC(C) is the set of valid
category patterns in C, and PattR(C) is the set of
valid relation patterns in C. “Valid” noun phrases,
category patterns, and relation patterns are defined
in Section 4.2.2.
The learning of these functions is coupled in two
ways:
</bodyText>
<listItem confidence="0.998258666666667">
1. Sharing among same-arity predicates according
to logical relations
2. Relation argument type-checking
</listItem>
<bodyText confidence="0.999106">
These methods of coupling are made possible by
prior knowledge in the input ontology, beyond the
lists of categories and relations mentioned above.
We provide general descriptions of these methods
of coupling in the next sections, while the details are
given in section 4.2.
</bodyText>
<subsectionHeader confidence="0.798062">
4.1.1 Sharing among same-arity predicates
</subsectionHeader>
<bodyText confidence="0.999380875">
Each predicate P in the ontology has a list of other
same-arity predicates with which P is mutually
exclusive, where mutuallyExclusive(P, P&apos;) ≡
(P(arg1) ⇒ ¬P&apos;(arg1)) ∧ (P&apos;(arg1) ⇒
¬P(arg1)), and similarly for relations. These mu-
tually exclusive relationships are used to carry out
the following simple but crucial coupling: if predi-
cate A is mutually exclusive with predicate B, A’s
positive instances and patterns become negative in-
stances and negative patterns for B. For example,
if ‘city’, having an instance ‘Boston’ and a pattern
‘mayor of arg1’, is mutually exclusive with ‘scien-
tist’, then ‘Boston’ and ‘mayor of arg1’ will become
a negative instance and a negative pattern respec-
tively for ‘scientist.’ Such negative instances and
patterns provide negative evidence to constrain the
bootstrapping process and forestall divergence.
Some categories are declared to be a subset of
one of the other categories being populated, where
subset(P, P&apos;) ≡ P(arg1) ⇒ P&apos;(arg1), (e.g., ‘ath-
lete’ is a subset of ‘person’). This prior knowledge
is used to share instances and patterns of the subcat-
egory (e.g., ‘athlete’) as positive instances and pat-
terns for the super-category (e.g., ‘person’).
</bodyText>
<subsubsectionHeader confidence="0.890971">
4.1.2 Relation argument type-checking
</subsubsectionHeader>
<bodyText confidence="0.999992090909091">
The last type of prior knowledge we use to couple
the learning of functions is type checking informa-
tion which couples the learning of relations with cat-
egories. For example, the arguments of the ‘ceoOf’
relation are declared to be of the categories ‘person’
and ‘company’. Our approach does not promote a
pair of noun phrases as an instance of a relation un-
less the two noun phrases are classified as belonging
to the correct argument types. Additionally, when a
relation instance is promoted, the arguments become
promoted instances of their respective categories.
</bodyText>
<subsectionHeader confidence="0.996081">
4.2 Algorithm Description
</subsectionHeader>
<bodyText confidence="0.99974925">
In this section, we describe our algorithm, CBL
(Coupled Bootstrap Learner), in detail.
The inputs to CBL are a large corpus of POS-
tagged sentences and an initial ontology with pre-
</bodyText>
<page confidence="0.993947">
3
</page>
<figureCaption confidence="0.425801">
Algorithm 1: CBL Algorithm
</figureCaption>
<bodyText confidence="0.79072">
Input: An ontology O, and text corpus C
Output: Trusted instances/patterns for each
predicate
SHARE initial instances/patterns among
predicates;
for i = 1, 2, ... , oc do
foreach predicate p E O do
</bodyText>
<equation confidence="0.447079333333333">
EXTRACT candidate instances/patterns;
FILTER candidates;
TRAIN instance/pattern classifiers;
ASSESS candidates using classifiers;
PROMOTE highest-confidence candidates;
end
</equation>
<bodyText confidence="0.985928115384615">
SHARE promoted items among predicates;
end
defined categories, relations, mutually exclusive re-
lationships between same-arity predicates, subset re-
lationships between some categories, seed instances
for all predicates, and seed patterns for the cate-
gories. Categories in the input ontology also have
a flag indicating whether instances must be proper
nouns, common nouns, or whether they can be ei-
ther (e.g., instances of ‘city’ are proper nouns).
Algorithm 1 gives a summary of the CBL algo-
rithm. First, seed instances and patterns are shared
among predicates using the available mutual exclu-
sion, subset, and type-checking relations. Then,
for an indefinite number of iterations, CBL expands
the sets of promoted instances and patterns for each
predicate, as detailed below.
CBL was designed to allow learning many pred-
icates simultaneously from a large sample of text
from the web. In each iteration of the algorithm, the
information needed from the text corpus is gathered
in two passes through the corpus using the MapRe-
duce framework (Dean and Ghemawat, 2008). This
allows us to complete an iteration of the system in
1 hour using a corpus containing millions of web
pages (see Section 5.3 for details on the corpus).
</bodyText>
<subsectionHeader confidence="0.820066">
4.2.1 Sharing
</subsectionHeader>
<bodyText confidence="0.9780336">
At the start of execution, seed instances and pat-
terns are shared among predicates according to the
mutual exclusion, subset, and type-checking con-
straints. Newly promoted instances and patterns are
shared at the end of each iteration.
</bodyText>
<subsubsectionHeader confidence="0.725877">
4.2.2 Candidate Extraction
</subsubsectionHeader>
<bodyText confidence="0.995125818181818">
CBL finds new candidate instances by using
newly promoted patterns to extract the noun phrases
that co-occur with those patterns in the text corpus.
To keep the size of this set manageable, CBL lim-
its the number of new candidate instances for each
predicate to 1000 by selecting the ones that occur
with the most newly promoted patterns. An analo-
gous procedure is used to extract candidate patterns.
Candidate extraction is performed for all predicates
in a single pass through the corpus using the MapRe-
duce framework.
The candidate extraction procedure has defini-
tions for valid instances and patterns that limit ex-
traction to instances that look like noun phrases and
patterns that are likely to be informative. Here we
provide brief descriptions of those definitions.
Category Instances In the placeholder of a cate-
gory pattern, CBL looks for a noun phrase. It uses
part-of-speech tags to segment noun phrases, ignor-
ing determiners. Proper nouns containing prepo-
sitions are segmented using a reimplementation of
the Lex algorithm (Downey et al., 2007). Cate-
gory instances are only extracted if they obey the
proper/common noun specification of the category.
Category Patterns If a promoted category in-
stance is found in a sentence, CBL extracts the pre-
ceding words as a candidate pattern if they are verbs
followed by a sequence of adjectives, prepositions,
or determiners (e.g., ‘being acquired by arg1’) or
nouns and adjectives followed by a sequence of ad-
jectives, prepositions, or determiners (e.g., ‘former
CEO of arg1’).
CBL extracts the words following the instance as
a candidate pattern if they are verbs followed option-
ally by a noun phrase (e.g., ‘arg1 broke the home run
record’), or verbs followed by a preposition (e.g.,
‘arg1 said that’).
Relation Instances If a promoted relation pattern
(e.g., ‘arg1 is mayor of arg2’) is found, a candi-
date relation instance is extracted if both placehold-
ers are valid noun phrases, and if they obey the
proper/common specifications for their categories.
Relation Patterns If both arguments from a pro-
moted relation instance are found in a sentence then
</bodyText>
<page confidence="0.988065">
4
</page>
<bodyText confidence="0.99374075">
the intervening sequence of words is extracted as a
candidate relation pattern if it contains no more than
5 tokens, has a content word, has an uncapitalized
word, and has at least one non-noun.
</bodyText>
<subsubsectionHeader confidence="0.596658">
4.2.3 Candidate Filtering
</subsubsectionHeader>
<bodyText confidence="0.998005352941176">
Candidate instances and patterns are filtered to
maintain high precision, and to avoid extremely spe-
cific patterns. An instance is only considered for as-
sessment if it co-occurs with at least two promoted
patterns in the text corpus, and if its co-occurrence
count with all promoted patterns is at least three
times greater than its co-occurrence count with neg-
ative patterns. Candidate patterns are filtered in the
same manner using instances.
All co-occurrence counts needed by the filtering
step are obtained with an additional pass through
the corpus using MapReduce. This implementa-
tion is much more efficient than one that relies on
web search queries. CBL typically requires co-
occurrence counts of at least 10,000 instances with
any of at least 10,000 patterns, which would require
100 million hit count queries.
</bodyText>
<subsubsectionHeader confidence="0.672933">
4.2.4 Candidate Assessment
</subsubsectionHeader>
<bodyText confidence="0.999948636363636">
Next, for each predicate CBL trains a discretized
Naive Bayes classifier to classify the candidate in-
stances. Its features include pointwise mutual infor-
mation (PMI) scores (Turney, 2001) of the candidate
instance with each of the positive and negative pat-
terns associated with the class. The current sets of
promoted and negative instances are used as training
examples for the classifier. Attributes are discretized
based on information gain (Fayyad and Irani, 1993).
Patterns are assessed using an estimate of the pre-
cision of each pattern p:
</bodyText>
<equation confidence="0.9987535">
Precision(p)
Ei∈I count (i, p)
=
count(p)
</equation>
<bodyText confidence="0.999968076923077">
where Z is the set of promoted instances for the
predicate currently being considered, count(i, p) is
the co-occurrence count of instance i with pattern p,
and count(p) is the hit count of the pattern p. This
is a pessimistic estimate because it assumes that the
rest of the occurrences of pattern p are not with pos-
itive examples of the predicate. We also penalize
extremely rare patterns by thresholding the denomi-
nator using the 25th percentile candidate pattern hit
count (McDowell and Cafarella, 2006).
All of the co-occurrence counts needed for the as-
sessment step are collected in the same MapReduce
pass as those required for filtering candidates.
</bodyText>
<subsubsectionHeader confidence="0.771623">
4.2.5 Candidate Promotion
</subsubsectionHeader>
<bodyText confidence="0.999862">
CBL then ranks the candidates according to their
assessment scores and promotes at most 100 in-
stances and 5 patterns for each predicate.
</bodyText>
<sectionHeader confidence="0.99777" genericHeader="method">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.9999976">
We designed our experimental evaluation to try to
answer the following questions: Can CBL iterate
many times and still achieve high precision? How
helpful are the types of coupling that we employ?
Can we extend existing semantic resources?
</bodyText>
<subsectionHeader confidence="0.998634">
5.1 Configurations of the Algorithm
</subsectionHeader>
<bodyText confidence="0.999815">
We ran our algorithm in three configurations:
</bodyText>
<listItem confidence="0.6930834">
• Full: The algorithm as described in Section 4.2.
• No Sharing Among Same-Arity Predicates (NS):
This configuration couples predicates only us-
ing type-checking constraints. It uses the full
algorithm, except that predicates of the same ar-
ity do not share promoted instances and patterns
with each other. Seed instances and patterns are
shared, though, so each predicate has a small,
fixed pool of negative evidence.
• No Category/Relation coupling (NCR): This
</listItem>
<bodyText confidence="0.8897406">
configuration couples predicates using mutual
exclusion and subset constraints, but not type-
checking. It uses the full algorithm, except
that relation instance arguments are not fil-
tered or assessed using their specified categories,
and arguments of promoted relations are not
shared as promoted instances of categories. The
only type-checking information used is the com-
mon/proper noun specifications of arguments for
filtering out implausible instances.
</bodyText>
<subsectionHeader confidence="0.997941">
5.2 Initial ontology
</subsectionHeader>
<bodyText confidence="0.999954375">
Our ontology contained categories and relations re-
lated to two domains: companies and sports. Ex-
tra categories were added to provide negative evi-
dence to the domain-related categories: ‘hobby’ for
‘economic sector’; ‘actor,’ ‘politician,’ and ‘scien-
tist’ for ‘athlete’ and ‘coach’; and ‘board game’ for
‘sport’. Table 1 lists each predicate in the leftmost
column. Categories were started with 10–20 seed
</bodyText>
<page confidence="0.988132">
5
</page>
<table confidence="0.9998718">
Predicate 5 iterations NCR 10 iterations NCR 15 iterations NCR
Full NS Full NS Full NS
Actor 93 100 100 93 97 100 100 97 100
Athlete 100 100 100 100 93 100 100 73 100
Board Game 93 76 93 89 27 93 89 30 93
City 100 100 100 100 97 100 100 100 100
Coach 100 63 73 97 53 43 97 47 47
Company 100 100 100 97 90 97 100 90 100
Country 60 40 60 30 43 27 40 23 40
Economic Sector 77 63 73 57 67 67 50 63 40
Hobby 67 63 67 40 40 57 20 23 30
Person 97 97 90 97 93 97 93 97 93
Politician 93 93 97 73 53 90 90 53 87
Product 97 87 90 90 87 100 97 90 77
Product Type 93 93 90 70 73 97 77 80 67
Scientist 100 90 97 97 63 97 93 60 100
Sport 100 90 100 93 67 83 97 27 90
Sports Team 100 97 100 97 70 100 90 50 100
Category Average 92 84 89 82 70 84 83 63 79
Acquired(Company, Company) 77 77 80 67 80 47 70 63 47
CeoOf(Person, Company) 97 87 100 90 87 97 90 80 83
CoachesTeam(Coach, Sports Team) 100 100 100 100 100 97 100 100 90
CompetesIn(Company, Econ. Sector) 97 97 80 100 93 67 97 63 60
CompetesWith(Company, Company) 93 80 60 77 70 37 70 60 43
HasOfficesIn(Company, City) 97 93 40 93 90 27 93 57 30
HasOperationsIn(Company, Country) 100 95 50 100 97 40 90 83 13
HeadquarteredIn(Company, City) 77 90 20 70 77 27 70 60 7
LocatedIn(City, Country) 90 67 57 63 50 43 73 50 30
PlaysFor(Athlete, Sports Team) 100 100 0 100 97 7 100 43 0
PlaysSport(Athlete, Sport) 100 100 27 93 80 10 100 40 30
TeamPlaysSport(Sports Team, Sport) 100 100 77 100 97 80 93 83 67
Produces(Company, Product) 91 83 90 83 93 67 93 80 57
HasType(Product, Product Type) 73 63 17 33 67 33 40 57 27
Relation Average 92 88 57 84 84 48 84 66 42
All 92 86 74 83 76 68 84 64 62
</table>
<tableCaption confidence="0.99452125">
Table 1: Precision (%) for each predicate. Results are presented after 5, 10, and 15 iterations, for the Full, No Sharing
(NS), and No Category/Relation Coupling (NCR) configurations of CBL . Note that we expect Full and NCR to
perform similarly for categories, but for Full to outperform NCR on relations and for Full to outperform NS on both
categories and relations.
</tableCaption>
<page confidence="0.999141">
6
</page>
<bodyText confidence="0.999623545454546">
instances and 5 seed patterns. The seed instances
were specified by a human, and the seed patterns
were derived from the generic patterns of Hearst
for each predicate (Hearst, 1992). Relations were
started with similar numbers of seed instances, and
no seed patterns (it is less obvious how to gener-
ate good seed patterns from relation names). Most
predicates were declared as mutually exclusive with
most others, except for special cases (e.g., ‘hobby’
and ‘sport’; ‘university’ and ‘sports team’; and ‘has
offices in’ and ‘headquartered in’).
</bodyText>
<subsectionHeader confidence="0.997876">
5.3 Corpus
</subsectionHeader>
<bodyText confidence="0.999974166666667">
Our text corpus was from a 200-million page web
crawl. We parsed the HTML, filtered out non-
English pages using a stop word ratio threshold, then
filtered out web spam and adult content using a ‘bad
word’ list. The pages were then segmented into sen-
tences, tokenized, and tagged with parts-of-speech
using the OpenNLP package. Finally, we filtered
the sentences to eliminate those that were likely to
be noisy and not useful for learning (e.g., sentences
without a verb, without any lowercase words, with
too many words that were all capital letters). This
yielded a corpus of roughly 514-million sentences.
</bodyText>
<subsectionHeader confidence="0.976236">
5.4 Experimental Procedure
</subsectionHeader>
<bodyText confidence="0.999998857142857">
We ran each configuration for 15 iterations. To eval-
uate the precision of promoted instances, we sam-
pled 30 instances from the promoted set for each
predicate in each configuration after 5, 10, and 15 it-
erations, pooled together the samples for each pred-
icate, and then judged their correctness. The judge
did not know which run an instance was sampled
from. We estimated the precision of the promoted
instances from each run after 5, 10, and 15 itera-
tions as the number of correct promoted instances
divided by the number sampled. While samples of
30 instances do not produce tight confidence inter-
vals around individual estimates, they are sufficient
for testing for the effects in which we are interested.
</bodyText>
<sectionHeader confidence="0.558244" genericHeader="evaluation">
5.5 Results
</sectionHeader>
<bodyText confidence="0.99997134">
Table 1 shows the precision of each of the three al-
gorithm configurations for each category and rela-
tion after 5, 10, and 15 iterations. As is apparent
in this table, fully coupled training (Full) outper-
forms training when coupling is removed between
categories and relations (NCR), and also when cou-
pling is removed among predicates of the same ar-
ity (NS). The net effect is substantial, as is appar-
ent from the bottom row of Table 1, which shows
that the precision of Full outperforms NS by 6% and
NCR by 18% after the first 5 iterations, and by an
even larger 20% and 22% after 15 iterations. This
increasing gap in precision as iterations increase re-
flects the ability of coupled learning to constrain the
system to reduce the otherwise common drift asso-
ciated with self-trained classifiers.
Using Student’s paired t-test, we found that for
categories, the difference in performance between
Full and NS is statistically significant after 5, 10,
and 15 iterations (p-value &lt; 0.05).2 No significant
difference was found between Full and NCR for cat-
egories, but this is not a surprise, because NCR still
uses mutually exclusive and subset constraints. The
same test finds that the differences between Full and
NS are significant for relations after 15 iterations,
and the differences between Full and NCR are sig-
nificant after 5, 10, and 15 iterations for relations.
The worst-performing categories after 15 itera-
tions of Full are ‘country,’ ‘economic sector,’ and
‘hobby.’ The Full configuration of CBL promoted
1637 instances for ‘country,’ far more than the num-
ber of correct answers. Many of these are general
geographic regions like ‘Bayfield Peninsula’ and
‘Baltic Republics.’ In the ‘hobby’ case, promoting
patterns like ‘the types of arg1’ led to the category
drifting into a general list of plural common nouns.
‘Economic sector’ drifted into academic fields like
‘Behavioral Science’ and ‘Political Sciences.’ We
expect that the learning of these categories would
be significantly better if there were even more cat-
egories being learned to provide additional negative
evidence during the filtering and assessment steps of
the algorithm.
At this stage of development, obtaining high re-
call is not a priority because our intent is to create
a continuously running and continuously improving
system; it is our hope that high recall will come with
time. However, to very roughly convey the com-
pleteness of the current results we show in Table 2
the average number of instances promoted for cate-
</bodyText>
<footnote confidence="0.967282">
2Our selection of the paired t-test was motivated by the work
of Smucker et al. (2007), but the Wilcoxon signed rank test
gives the same results.
</footnote>
<page confidence="0.99888">
7
</page>
<table confidence="0.9992348">
Configuration Categories Relations
Instances Prec. Instances Prec.
Full 970 83 191 84
NS 1337 63 307 66
NCR 916 79 458 42
</table>
<tableCaption confidence="0.971655666666667">
Table 2: Average numbers of promoted category and re-
lation instances and estimates of their precision for each
configuration of CBL after 15 iterations.
</tableCaption>
<figureCaption confidence="0.854032">
Figure 2: Extracted facts for two companies discovered
</figureCaption>
<bodyText confidence="0.8870848">
by CBL Full. These two companies were extracted by
the learned ‘company’ extractor, and the relations shown
were extracted by learned relation extractors.
gories and relations for each of the three configura-
tions of CBL after 15 iterations. For categories, not
sharing examples results in fewer negative examples
during the filtering and assessment steps. This yields
more promoted instances on average. For relations,
not using type checking yields higher relative recall,
but at a much lower level of precision.
Figure 2 gives one view of the type of information
extracted by the collection of learned category and
relation classifiers. Note the initial seed examples
provided to CBL did not include information about
either company or any of these relation instances.3
</bodyText>
<subsectionHeader confidence="0.99183">
5.6 Comparison to an Existing Database
</subsectionHeader>
<bodyText confidence="0.998146076923077">
To estimate the capacity of our algorithm to con-
tribute additional facts to publicly available seman-
tic resources, we compared the complete lists of in-
stances promoted during the Full 15 iteration run
for certain categories to corresponding lists in the
Freebase database (Metaweb Technologies, 2009).
Excluding the categories that did not have a di-
rectly corresponding Freebase list, we computed for
each category: Precision x |CBLInstances |−
|Matches|, where Precision is the estimated pre-
cision from our random sample of 30 instances,
|CBLInstances |is the total number of instances
promoted for that category, and |Matches |is the
</bodyText>
<footnote confidence="0.97766">
3See http://rtw.ml.cmu.edu/sslnlp09 for re-
sults from a full run of the system.
</footnote>
<table confidence="0.9996815">
Category Est. CBL Freebase Est. New
Prec. Instances Matches Instances
Actor 100 522 465 57
Athlete 100 117 54 63
Board Game 89 18 6 10
City 100 1799 1665 134
Company 100 1937 995 942
Econ. Sector 50 1541 137 634
Politician 90 962 74 792
Product 97 1259 0 1221
Sports Team 90 414 139 234
Sport 97 613 134 461
</table>
<tableCaption confidence="0.997675">
Table 3: Estimated numbers of “new instances” (correct
instances promoted by CBL in the Full 15 iteration run
which do not have a match in Freebase) and the values
used in calculating them.
</tableCaption>
<bodyText confidence="0.999870625">
number of promoted instances that had an exact
match in Freebase. While exact matches may under-
estimate the number of matches, it should be noted
that rather than make definitive claims, our intent
here is simply to give rough estimates, which are
shown in Table 3. These approximate numbers in-
dicate a potential to use CBL to extend existing se-
mantic resources like Freebase.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999962">
We have presented a method of coupling the semi-
supervised learning of categories and relations and
demonstrated empirically that the coupling forestalls
the problem of semantic drift associated with boot-
strap learning methods. We suspect that learning
additional predicates simultaneously will yield even
more accurate learning. An approximate compari-
son with an existing repository of semantic knowl-
edge, Freebase, suggests that our methods can con-
tribute new facts to existing resources.
</bodyText>
<sectionHeader confidence="0.999333" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998222142857143">
This work is supported in part by DARPA, Google,
a Yahoo! Fellowship to Andrew Carlson, and the
Brazilian research agency CNPq. We also gratefully
acknowledge Jamie Callan for making available his
collection of web pages, Yahoo! for use of their M45
computing cluster, and the anonymous reviewers for
their comments.
</bodyText>
<page confidence="0.996987">
8
</page>
<sectionHeader confidence="0.998322" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999806692307692">
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In JCDL.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In IJCAI.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In WebDB Workshop at
6th International Conference on Extending Database
Technology.
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28:41–75.
Ming-Wei Chang, Lev-Arie Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In ACL.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In EMNLP.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In PACLING.
Jeffrey Dean and Sanjay Ghemawat. 2008. Mapreduce:
simplified data processing on large clusters. Commun.
ACM, 51(1):107–113.
Doug Downey, Matthew Broadhead, and Oren Etzioni.
2007. Locating complex named entities in web text.
In IJCAI.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell., 165(1):91–134.
Usama M. Fayyad and Keki B. Irani. 1993. Multi-
interval discretization of continuous-valued attributes
for classification learning. In UAI.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING.
Qiuhua Liu, Xuejun Liao, and Lawrence Carin. 2008.
Semi-supervised multitask learning. In NIPS.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In NAACL.
Luke K. McDowell and Michael Cafarella. 2006.
Ontology-driven information extraction with on-
tosyphon. In ISWC.
Metaweb Technologies. 2009. Freebase data dumps.
http://download.freebase.com/datadumps/.
Marius Pas¸ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Names and similarities on
the web: fact extraction in the fast lane. In ACL.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Organizing and search-
ing the world wide web of facts - step one: The one-
million fact extraction challenge. In AAAI.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In ACL.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction by multi-level bootstrap-
ping. In AAAI.
Benjamin Rosenfeld and Ronen Feldman. 2007. Us-
ing corpus statistics on entities to improve semi-
supervised relation extraction from the web. In ACL.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In HLT-NAACL.
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests for
information retrieval evaluation. In CIKM.
Sebastian Thrun. 1996. Is learning the n-th thing any
easier than learning the first? In NIPS.
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In EMCL.
Nicola Ueffing. 2006. Self-training for machine trans-
lation. In NIPS workshop on Machine Learning for
Multilingual Information Access.
Roman Yangarber. 2003. Counter-training in discovery
of semantic patterns. In ACL.
</reference>
<page confidence="0.997113">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.413822">
<title confidence="0.9998">Coupling Semi-Supervised Learning of Categories and Relations</title>
<author confidence="0.98855">Justin Estevam R Hruschka</author>
<author confidence="0.98855">M</author>
<affiliation confidence="0.9971775">of Computer Carnegie Mellon</affiliation>
<address confidence="0.554804">Pittsburgh, PA</address>
<affiliation confidence="0.997618">University of Sao</affiliation>
<address confidence="0.777211">Sao Carlos, SP -</address>
<email confidence="0.987812">estevam@dc.ufscar.br</email>
<abstract confidence="0.9992605">We consider semi-supervised learning of information extraction methods, especially for extracting instances of noun categories (e.g., ‘athlete,’ ‘team’) and relations (e.g., supervised approaches using a small number of labeled examples together with many unlabeled examples are often unreliable as they frequently produce an internally consistent, but nevertheless incorrect set of extractions. We propose that this problem can be overcome by simultaneously learning classifiers for many different categories and relations in the presence of an ontology defining constraints that couple the training of these classifiers. Experimental results show that simultaneously learning a coupled collection of classifiers for 30 categories and relations results in much more accurate extractions than training classifiers individually.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In JCDL.</booktitle>
<contexts>
<context position="6770" citStr="Agichtein and Gravano, 2000" startWordPosition="1015" endWordPosition="1018">tial model, then use this model to label some of the unlabeled data. The model is then retrained, using the original seed examples plus the self-labeled examples. This process iterates, gradually expanding the amount of labeled data. Such approaches have shown promise in applications such as web page classification (Blum and Mitchell, 1998), named entity classification (Collins and Singer, 1999), parsing (McClosky et al., 2006), and machine translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and Jones, 1999; Yangarber, 2003). Additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (Pas¸ca et al., 2006; Rosenfeld </context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In JCDL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web. In IJCAI.</title>
<date>2007</date>
<contexts>
<context position="8061" citStr="Banko et al., 2007" startWordPosition="1223" endWordPosition="1226">neous bootstrapped training of multiple categories and multiple relations. Our approach to information extraction is based on using high precision contextual patterns (e.g., ‘is mayor of arg1’ suggests that arg1 is a city). An early pattern-based approach to information extraction acquired ‘is a’ relations from text using generic contextual patterns (Hearst, 1992). This approach was later scaled up to the web by Etzioni et al. (2005). 2 Other research explores the task of ‘open information extraction’, where the predicates to be learned are not specified in advance (Shinyama and Sekine, 2006; Banko et al., 2007), but emerge instead from analysis of the data. In contrast, our approach relies strongly on knowledge in the ontology about the predicates to be learned, and relationships among them, in order to achieve high accuracy. Chang et al. (2007) present a framework for learning that optimizes the data likelihood plus constraint-based penalty terms than capture prior knowledge, and demonstrate it with semi-supervised learning of segmentation models. Constraints that capture domain knowledge guide bootstrap learning of a structured model by penalizing or disallowing violations of those constraints. Wh</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT.</booktitle>
<contexts>
<context position="6485" citStr="Blum and Mitchell, 1998" startWordPosition="977" endWordPosition="980">applied to the same input are mutually exclusive, or that one implies the other). In this paper, we focus on a ‘bootstrapping’ method for semi-supervised learning. Bootstrapping approaches start with a small number of labeled ‘seed’ examples, use those seed examples to train an initial model, then use this model to label some of the unlabeled data. The model is then retrained, using the original seed examples plus the self-labeled examples. This process iterates, gradually expanding the amount of labeled data. Such approaches have shown promise in applications such as web page classification (Blum and Mitchell, 1998), named entity classification (Collins and Singer, 1999), parsing (McClosky et al., 2006), and machine translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predi</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
</authors>
<title>Extracting patterns and relations from the world wide web.</title>
<date>1998</date>
<booktitle>In WebDB Workshop at 6th International Conference on Extending Database Technology.</booktitle>
<contexts>
<context position="6741" citStr="Brin, 1998" startWordPosition="1013" endWordPosition="1014">train an initial model, then use this model to label some of the unlabeled data. The model is then retrained, using the original seed examples plus the self-labeled examples. This process iterates, gradually expanding the amount of labeled data. Such approaches have shown promise in applications such as web page classification (Blum and Mitchell, 1998), named entity classification (Collins and Singer, 1999), parsing (McClosky et al., 2006), and machine translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and Jones, 1999; Yangarber, 2003). Additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (Pa</context>
</contexts>
<marker>Brin, 1998</marker>
<rawString>Sergey Brin. 1998. Extracting patterns and relations from the world wide web. In WebDB Workshop at 6th International Conference on Extending Database Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich Caruana</author>
</authors>
<date>1997</date>
<booktitle>Multitask learning. Machine Learning,</booktitle>
<pages>28--41</pages>
<contexts>
<context position="5383" citStr="Caruana, 1997" startWordPosition="798" endWordPosition="799">edicate and a large corpus of sentences annotated with part-ofspeech (POS) tags. We focus on extracting facts that are stated multiple times in the corpus, which we can assess probabilistically using corpus statistics. We do not resolve strings to real-world entities— the problems of synonym resolution and disambiguation of strings that can refer to multiple entities are left for future work. 3 Related Work Work on multitask learning has demonstrated that supervised learning of multiple “related” functions together can yield higher accuracy than learning the functions separately (Thrun, 1996; Caruana, 1997). Semi-supervised multitask learning has been shown 1We do not consider predicates of higher arity in this work. to increase accuracy when tasks are related, allowing one to use a prior that encourages similar parameters (Liu et al., 2008). Our work also involves semi-supervised training of multiple coupled functions, but differs in that we assume explicit prior knowledge of the precise way in which our multiple functions are related (e.g., that the values of the functions applied to the same input are mutually exclusive, or that one implies the other). In this paper, we focus on a ‘bootstrapp</context>
</contexts>
<marker>Caruana, 1997</marker>
<rawString>Rich Caruana. 1997. Multitask learning. Machine Learning, 28:41–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Lev-Arie Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Guiding semi-supervision with constraintdriven learning.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8300" citStr="Chang et al. (2007)" startWordPosition="1263" endWordPosition="1266">n-based approach to information extraction acquired ‘is a’ relations from text using generic contextual patterns (Hearst, 1992). This approach was later scaled up to the web by Etzioni et al. (2005). 2 Other research explores the task of ‘open information extraction’, where the predicates to be learned are not specified in advance (Shinyama and Sekine, 2006; Banko et al., 2007), but emerge instead from analysis of the data. In contrast, our approach relies strongly on knowledge in the ontology about the predicates to be learned, and relationships among them, in order to achieve high accuracy. Chang et al. (2007) present a framework for learning that optimizes the data likelihood plus constraint-based penalty terms than capture prior knowledge, and demonstrate it with semi-supervised learning of segmentation models. Constraints that capture domain knowledge guide bootstrap learning of a structured model by penalizing or disallowing violations of those constraints. While similar in spirit, our work differs in that we consider learning many models, rather than one structured model, and that we are consider a much larger scale application in a different domain. 4 Approach 4.1 Coupling of Predicates As me</context>
</contexts>
<marker>Chang, Ratinov, Roth, 2007</marker>
<rawString>Ming-Wei Chang, Lev-Arie Ratinov, and Dan Roth. 2007. Guiding semi-supervision with constraintdriven learning. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6541" citStr="Collins and Singer, 1999" startWordPosition="984" endWordPosition="987">at one implies the other). In this paper, we focus on a ‘bootstrapping’ method for semi-supervised learning. Bootstrapping approaches start with a small number of labeled ‘seed’ examples, use those seed examples to train an initial model, then use this model to label some of the unlabeled data. The model is then retrained, using the original seed examples plus the self-labeled examples. This process iterates, gradually expanding the amount of labeled data. Such approaches have shown promise in applications such as web page classification (Blum and Mitchell, 1998), named entity classification (Collins and Singer, 1999), parsing (McClosky et al., 2006), and machine translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to h</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Tara Murphy</author>
<author>Bernhard Scholz</author>
</authors>
<title>Minimising semantic drift with mutual exclusion bootstrapping.</title>
<date>2007</date>
<booktitle>In PACLING.</booktitle>
<contexts>
<context position="7008" citStr="Curran et al., 2007" startWordPosition="1053" endWordPosition="1056">ches have shown promise in applications such as web page classification (Blum and Mitchell, 1998), named entity classification (Collins and Singer, 1999), parsing (McClosky et al., 2006), and machine translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and Jones, 1999; Yangarber, 2003). Additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (Pas¸ca et al., 2006; Rosenfeld and Feldman, 2007). Our work builds on these ideas to couple the simultaneous bootstrapped training of multiple categories and multiple relations. Our approach to information extraction is based on using high precision contextual patterns</context>
</contexts>
<marker>Curran, Murphy, Scholz, 2007</marker>
<rawString>James R. Curran, Tara Murphy, and Bernhard Scholz. 2007. Minimising semantic drift with mutual exclusion bootstrapping. In PACLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>Mapreduce: simplified data processing on large clusters.</title>
<date>2008</date>
<journal>Commun. ACM,</journal>
<volume>51</volume>
<issue>1</issue>
<contexts>
<context position="13511" citStr="Dean and Ghemawat, 2008" startWordPosition="2086" endWordPosition="2089">ouns). Algorithm 1 gives a summary of the CBL algorithm. First, seed instances and patterns are shared among predicates using the available mutual exclusion, subset, and type-checking relations. Then, for an indefinite number of iterations, CBL expands the sets of promoted instances and patterns for each predicate, as detailed below. CBL was designed to allow learning many predicates simultaneously from a large sample of text from the web. In each iteration of the algorithm, the information needed from the text corpus is gathered in two passes through the corpus using the MapReduce framework (Dean and Ghemawat, 2008). This allows us to complete an iteration of the system in 1 hour using a corpus containing millions of web pages (see Section 5.3 for details on the corpus). 4.2.1 Sharing At the start of execution, seed instances and patterns are shared among predicates according to the mutual exclusion, subset, and type-checking constraints. Newly promoted instances and patterns are shared at the end of each iteration. 4.2.2 Candidate Extraction CBL finds new candidate instances by using newly promoted patterns to extract the noun phrases that co-occur with those patterns in the text corpus. To keep the siz</context>
</contexts>
<marker>Dean, Ghemawat, 2008</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2008. Mapreduce: simplified data processing on large clusters. Commun. ACM, 51(1):107–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Locating complex named entities in web text.</title>
<date>2007</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="15002" citStr="Downey et al., 2007" startWordPosition="2328" endWordPosition="2331">or all predicates in a single pass through the corpus using the MapReduce framework. The candidate extraction procedure has definitions for valid instances and patterns that limit extraction to instances that look like noun phrases and patterns that are likely to be informative. Here we provide brief descriptions of those definitions. Category Instances In the placeholder of a category pattern, CBL looks for a noun phrase. It uses part-of-speech tags to segment noun phrases, ignoring determiners. Proper nouns containing prepositions are segmented using a reimplementation of the Lex algorithm (Downey et al., 2007). Category instances are only extracted if they obey the proper/common noun specification of the category. Category Patterns If a promoted category instance is found in a sentence, CBL extracts the preceding words as a candidate pattern if they are verbs followed by a sequence of adjectives, prepositions, or determiners (e.g., ‘being acquired by arg1’) or nouns and adjectives followed by a sequence of adjectives, prepositions, or determiners (e.g., ‘former CEO of arg1’). CBL extracts the words following the instance as a candidate pattern if they are verbs followed optionally by a noun phrase </context>
</contexts>
<marker>Downey, Broadhead, Etzioni, 2007</marker>
<rawString>Doug Downey, Matthew Broadhead, and Oren Etzioni. 2007. Locating complex named entities in web text. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artif. Intell.,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="7879" citStr="Etzioni et al. (2005)" startWordPosition="1193" endWordPosition="1196"> certain, expected types can help mitigate the promotion of incorrect instances (Pas¸ca et al., 2006; Rosenfeld and Feldman, 2007). Our work builds on these ideas to couple the simultaneous bootstrapped training of multiple categories and multiple relations. Our approach to information extraction is based on using high precision contextual patterns (e.g., ‘is mayor of arg1’ suggests that arg1 is a city). An early pattern-based approach to information extraction acquired ‘is a’ relations from text using generic contextual patterns (Hearst, 1992). This approach was later scaled up to the web by Etzioni et al. (2005). 2 Other research explores the task of ‘open information extraction’, where the predicates to be learned are not specified in advance (Shinyama and Sekine, 2006; Banko et al., 2007), but emerge instead from analysis of the data. In contrast, our approach relies strongly on knowledge in the ontology about the predicates to be learned, and relationships among them, in order to achieve high accuracy. Chang et al. (2007) present a framework for learning that optimizes the data likelihood plus constraint-based penalty terms than capture prior knowledge, and demonstrate it with semi-supervised lear</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artif. Intell., 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Usama M Fayyad</author>
<author>Keki B Irani</author>
</authors>
<title>Multiinterval discretization of continuous-valued attributes for classification learning.</title>
<date>1993</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="17594" citStr="Fayyad and Irani, 1993" startWordPosition="2744" endWordPosition="2747">ce counts of at least 10,000 instances with any of at least 10,000 patterns, which would require 100 million hit count queries. 4.2.4 Candidate Assessment Next, for each predicate CBL trains a discretized Naive Bayes classifier to classify the candidate instances. Its features include pointwise mutual information (PMI) scores (Turney, 2001) of the candidate instance with each of the positive and negative patterns associated with the class. The current sets of promoted and negative instances are used as training examples for the classifier. Attributes are discretized based on information gain (Fayyad and Irani, 1993). Patterns are assessed using an estimate of the precision of each pattern p: Precision(p) Ei∈I count (i, p) = count(p) where Z is the set of promoted instances for the predicate currently being considered, count(i, p) is the co-occurrence count of instance i with pattern p, and count(p) is the hit count of the pattern p. This is a pessimistic estimate because it assumes that the rest of the occurrences of pattern p are not with positive examples of the predicate. We also penalize extremely rare patterns by thresholding the denominator using the 25th percentile candidate pattern hit count (McD</context>
</contexts>
<marker>Fayyad, Irani, 1993</marker>
<rawString>Usama M. Fayyad and Keki B. Irani. 1993. Multiinterval discretization of continuous-valued attributes for classification learning. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="7808" citStr="Hearst, 1992" startWordPosition="1181" endWordPosition="1182">r, 2003). Additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (Pas¸ca et al., 2006; Rosenfeld and Feldman, 2007). Our work builds on these ideas to couple the simultaneous bootstrapped training of multiple categories and multiple relations. Our approach to information extraction is based on using high precision contextual patterns (e.g., ‘is mayor of arg1’ suggests that arg1 is a city). An early pattern-based approach to information extraction acquired ‘is a’ relations from text using generic contextual patterns (Hearst, 1992). This approach was later scaled up to the web by Etzioni et al. (2005). 2 Other research explores the task of ‘open information extraction’, where the predicates to be learned are not specified in advance (Shinyama and Sekine, 2006; Banko et al., 2007), but emerge instead from analysis of the data. In contrast, our approach relies strongly on knowledge in the ontology about the predicates to be learned, and relationships among them, in order to achieve high accuracy. Chang et al. (2007) present a framework for learning that optimizes the data likelihood plus constraint-based penalty terms tha</context>
<context position="22404" citStr="Hearst, 1992" startWordPosition="3620" endWordPosition="3621">e 92 88 57 84 84 48 84 66 42 All 92 86 74 83 76 68 84 64 62 Table 1: Precision (%) for each predicate. Results are presented after 5, 10, and 15 iterations, for the Full, No Sharing (NS), and No Category/Relation Coupling (NCR) configurations of CBL . Note that we expect Full and NCR to perform similarly for categories, but for Full to outperform NCR on relations and for Full to outperform NS on both categories and relations. 6 instances and 5 seed patterns. The seed instances were specified by a human, and the seed patterns were derived from the generic patterns of Hearst for each predicate (Hearst, 1992). Relations were started with similar numbers of seed instances, and no seed patterns (it is less obvious how to generate good seed patterns from relation names). Most predicates were declared as mutually exclusive with most others, except for special cases (e.g., ‘hobby’ and ‘sport’; ‘university’ and ‘sports team’; and ‘has offices in’ and ‘headquartered in’). 5.3 Corpus Our text corpus was from a 200-million page web crawl. We parsed the HTML, filtered out nonEnglish pages using a stop word ratio threshold, then filtered out web spam and adult content using a ‘bad word’ list. The pages were </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiuhua Liu</author>
<author>Xuejun Liao</author>
<author>Lawrence Carin</author>
</authors>
<title>Semi-supervised multitask learning.</title>
<date>2008</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="5622" citStr="Liu et al., 2008" startWordPosition="836" endWordPosition="839"> strings to real-world entities— the problems of synonym resolution and disambiguation of strings that can refer to multiple entities are left for future work. 3 Related Work Work on multitask learning has demonstrated that supervised learning of multiple “related” functions together can yield higher accuracy than learning the functions separately (Thrun, 1996; Caruana, 1997). Semi-supervised multitask learning has been shown 1We do not consider predicates of higher arity in this work. to increase accuracy when tasks are related, allowing one to use a prior that encourages similar parameters (Liu et al., 2008). Our work also involves semi-supervised training of multiple coupled functions, but differs in that we assume explicit prior knowledge of the precise way in which our multiple functions are related (e.g., that the values of the functions applied to the same input are mutually exclusive, or that one implies the other). In this paper, we focus on a ‘bootstrapping’ method for semi-supervised learning. Bootstrapping approaches start with a small number of labeled ‘seed’ examples, use those seed examples to train an initial model, then use this model to label some of the unlabeled data. The model </context>
</contexts>
<marker>Liu, Liao, Carin, 2008</marker>
<rawString>Qiuhua Liu, Xuejun Liao, and Lawrence Carin. 2008. Semi-supervised multitask learning. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="6574" citStr="McClosky et al., 2006" startWordPosition="989" endWordPosition="992">aper, we focus on a ‘bootstrapping’ method for semi-supervised learning. Bootstrapping approaches start with a small number of labeled ‘seed’ examples, use those seed examples to train an initial model, then use this model to label some of the unlabeled data. The model is then retrained, using the original seed examples plus the self-labeled examples. This process iterates, gradually expanding the amount of labeled data. Such approaches have shown promise in applications such as web page classification (Blum and Mitchell, 1998), named entity classification (Collins and Singer, 1999), parsing (McClosky et al., 2006), and machine translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and </context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke K McDowell</author>
<author>Michael Cafarella</author>
</authors>
<title>Ontology-driven information extraction with ontosyphon.</title>
<date>2006</date>
<booktitle>In ISWC.</booktitle>
<contexts>
<context position="18220" citStr="McDowell and Cafarella, 2006" startWordPosition="2850" endWordPosition="2853">93). Patterns are assessed using an estimate of the precision of each pattern p: Precision(p) Ei∈I count (i, p) = count(p) where Z is the set of promoted instances for the predicate currently being considered, count(i, p) is the co-occurrence count of instance i with pattern p, and count(p) is the hit count of the pattern p. This is a pessimistic estimate because it assumes that the rest of the occurrences of pattern p are not with positive examples of the predicate. We also penalize extremely rare patterns by thresholding the denominator using the 25th percentile candidate pattern hit count (McDowell and Cafarella, 2006). All of the co-occurrence counts needed for the assessment step are collected in the same MapReduce pass as those required for filtering candidates. 4.2.5 Candidate Promotion CBL then ranks the candidates according to their assessment scores and promotes at most 100 instances and 5 patterns for each predicate. 5 Experimental Evaluation We designed our experimental evaluation to try to answer the following questions: Can CBL iterate many times and still achieve high precision? How helpful are the types of coupling that we employ? Can we extend existing semantic resources? 5.1 Configurations of</context>
</contexts>
<marker>McDowell, Cafarella, 2006</marker>
<rawString>Luke K. McDowell and Michael Cafarella. 2006. Ontology-driven information extraction with ontosyphon. In ISWC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Metaweb Technologies</author>
</authors>
<date>2009</date>
<note>Freebase data dumps. http://download.freebase.com/datadumps/.</note>
<contexts>
<context position="28224" citStr="Technologies, 2009" startWordPosition="4567" endWordPosition="4568">r level of precision. Figure 2 gives one view of the type of information extracted by the collection of learned category and relation classifiers. Note the initial seed examples provided to CBL did not include information about either company or any of these relation instances.3 5.6 Comparison to an Existing Database To estimate the capacity of our algorithm to contribute additional facts to publicly available semantic resources, we compared the complete lists of instances promoted during the Full 15 iteration run for certain categories to corresponding lists in the Freebase database (Metaweb Technologies, 2009). Excluding the categories that did not have a directly corresponding Freebase list, we computed for each category: Precision x |CBLInstances |− |Matches|, where Precision is the estimated precision from our random sample of 30 instances, |CBLInstances |is the total number of instances promoted for that category, and |Matches |is the 3See http://rtw.ml.cmu.edu/sslnlp09 for results from a full run of the system. Category Est. CBL Freebase Est. New Prec. Instances Matches Instances Actor 100 522 465 57 Athlete 100 117 54 63 Board Game 89 18 6 10 City 100 1799 1665 134 Company 100 1937 995 942 Ec</context>
</contexts>
<marker>Technologies, 2009</marker>
<rawString>Metaweb Technologies. 2009. Freebase data dumps. http://download.freebase.com/datadumps/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
<author>Dekang Lin</author>
<author>Jeffrey Bigham</author>
<author>Andrei Lifchits</author>
<author>Alpa Jain</author>
</authors>
<title>Names and similarities on the web: fact extraction in the fast lane.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<marker>Pas¸ca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>Marius Pas¸ca, Dekang Lin, Jeffrey Bigham, Andrei Lifchits, and Alpa Jain. 2006. Names and similarities on the web: fact extraction in the fast lane. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
<author>Dekang Lin</author>
<author>Jeffrey Bigham</author>
<author>Andrei Lifchits</author>
<author>Alpa Jain</author>
</authors>
<title>Organizing and searching the world wide web of facts - step one: The onemillion fact extraction challenge.</title>
<date>2006</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="6820" citStr="Pasca et al., 2006" startWordPosition="1023" endWordPosition="1026">eled data. The model is then retrained, using the original seed examples plus the self-labeled examples. This process iterates, gradually expanding the amount of labeled data. Such approaches have shown promise in applications such as web page classification (Blum and Mitchell, 1998), named entity classification (Collins and Singer, 1999), parsing (McClosky et al., 2006), and machine translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and Jones, 1999; Yangarber, 2003). Additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (Pas¸ca et al., 2006; Rosenfeld and Feldman, 2007). Our work builds on these ideas</context>
</contexts>
<marker>Pasca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lifchits, and Alpa Jain. 2006. Organizing and searching the world wide web of facts - step one: The onemillion fact extraction challenge. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6799" citStr="Ravichandran and Hovy, 2002" startWordPosition="1019" endWordPosition="1022">el to label some of the unlabeled data. The model is then retrained, using the original seed examples plus the self-labeled examples. This process iterates, gradually expanding the amount of labeled data. Such approaches have shown promise in applications such as web page classification (Blum and Mitchell, 1998), named entity classification (Collins and Singer, 1999), parsing (McClosky et al., 2006), and machine translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and Jones, 1999; Yangarber, 2003). Additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (Pas¸ca et al., 2006; Rosenfeld and Feldman, 2007). Our work </context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="7185" citStr="Riloff and Jones, 1999" startWordPosition="1084" endWordPosition="1087"> al., 2006), and machine translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and Jones, 1999; Yangarber, 2003). Additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (Pas¸ca et al., 2006; Rosenfeld and Feldman, 2007). Our work builds on these ideas to couple the simultaneous bootstrapped training of multiple categories and multiple relations. Our approach to information extraction is based on using high precision contextual patterns (e.g., ‘is mayor of arg1’ suggests that arg1 is a city). An early pattern-based approach to information extraction acquired ‘is a’ relations from text using generic contextual </context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Rosenfeld</author>
<author>Ronen Feldman</author>
</authors>
<title>Using corpus statistics on entities to improve semisupervised relation extraction from the web. In ACL.</title>
<date>2007</date>
<contexts>
<context position="7388" citStr="Rosenfeld and Feldman, 2007" startWordPosition="1113" endWordPosition="1117">vano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and Jones, 1999; Yangarber, 2003). Additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (Pas¸ca et al., 2006; Rosenfeld and Feldman, 2007). Our work builds on these ideas to couple the simultaneous bootstrapped training of multiple categories and multiple relations. Our approach to information extraction is based on using high precision contextual patterns (e.g., ‘is mayor of arg1’ suggests that arg1 is a city). An early pattern-based approach to information extraction acquired ‘is a’ relations from text using generic contextual patterns (Hearst, 1992). This approach was later scaled up to the web by Etzioni et al. (2005). 2 Other research explores the task of ‘open information extraction’, where the predicates to be learned are</context>
</contexts>
<marker>Rosenfeld, Feldman, 2007</marker>
<rawString>Benjamin Rosenfeld and Ronen Feldman. 2007. Using corpus statistics on entities to improve semisupervised relation extraction from the web. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive information extraction using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="8040" citStr="Shinyama and Sekine, 2006" startWordPosition="1219" endWordPosition="1222">ideas to couple the simultaneous bootstrapped training of multiple categories and multiple relations. Our approach to information extraction is based on using high precision contextual patterns (e.g., ‘is mayor of arg1’ suggests that arg1 is a city). An early pattern-based approach to information extraction acquired ‘is a’ relations from text using generic contextual patterns (Hearst, 1992). This approach was later scaled up to the web by Etzioni et al. (2005). 2 Other research explores the task of ‘open information extraction’, where the predicates to be learned are not specified in advance (Shinyama and Sekine, 2006; Banko et al., 2007), but emerge instead from analysis of the data. In contrast, our approach relies strongly on knowledge in the ontology about the predicates to be learned, and relationships among them, in order to achieve high accuracy. Chang et al. (2007) present a framework for learning that optimizes the data likelihood plus constraint-based penalty terms than capture prior knowledge, and demonstrate it with semi-supervised learning of segmentation models. Constraints that capture domain knowledge guide bootstrap learning of a structured model by penalizing or disallowing violations of </context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation discovery. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark D Smucker</author>
<author>James Allan</author>
<author>Ben Carterette</author>
</authors>
<title>A comparison of statistical significance tests for information retrieval evaluation.</title>
<date>2007</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="26721" citStr="Smucker et al. (2007)" startWordPosition="4328" endWordPosition="4331">es would be significantly better if there were even more categories being learned to provide additional negative evidence during the filtering and assessment steps of the algorithm. At this stage of development, obtaining high recall is not a priority because our intent is to create a continuously running and continuously improving system; it is our hope that high recall will come with time. However, to very roughly convey the completeness of the current results we show in Table 2 the average number of instances promoted for cate2Our selection of the paired t-test was motivated by the work of Smucker et al. (2007), but the Wilcoxon signed rank test gives the same results. 7 Configuration Categories Relations Instances Prec. Instances Prec. Full 970 83 191 84 NS 1337 63 307 66 NCR 916 79 458 42 Table 2: Average numbers of promoted category and relation instances and estimates of their precision for each configuration of CBL after 15 iterations. Figure 2: Extracted facts for two companies discovered by CBL Full. These two companies were extracted by the learned ‘company’ extractor, and the relations shown were extracted by learned relation extractors. gories and relations for each of the three configurat</context>
</contexts>
<marker>Smucker, Allan, Carterette, 2007</marker>
<rawString>Mark D. Smucker, James Allan, and Ben Carterette. 2007. A comparison of statistical significance tests for information retrieval evaluation. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Thrun</author>
</authors>
<title>Is learning the n-th thing any easier than learning the first?</title>
<date>1996</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="5367" citStr="Thrun, 1996" startWordPosition="796" endWordPosition="797">s for each predicate and a large corpus of sentences annotated with part-ofspeech (POS) tags. We focus on extracting facts that are stated multiple times in the corpus, which we can assess probabilistically using corpus statistics. We do not resolve strings to real-world entities— the problems of synonym resolution and disambiguation of strings that can refer to multiple entities are left for future work. 3 Related Work Work on multitask learning has demonstrated that supervised learning of multiple “related” functions together can yield higher accuracy than learning the functions separately (Thrun, 1996; Caruana, 1997). Semi-supervised multitask learning has been shown 1We do not consider predicates of higher arity in this work. to increase accuracy when tasks are related, allowing one to use a prior that encourages similar parameters (Liu et al., 2008). Our work also involves semi-supervised training of multiple coupled functions, but differs in that we assume explicit prior knowledge of the precise way in which our multiple functions are related (e.g., that the values of the functions applied to the same input are mutually exclusive, or that one implies the other). In this paper, we focus </context>
</contexts>
<marker>Thrun, 1996</marker>
<rawString>Sebastian Thrun. 1996. Is learning the n-th thing any easier than learning the first? In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the web for synonyms: Pmi-ir versus lsa on toefl.</title>
<date>2001</date>
<booktitle>In EMCL.</booktitle>
<contexts>
<context position="17313" citStr="Turney, 2001" startWordPosition="2702" endWordPosition="2703">manner using instances. All co-occurrence counts needed by the filtering step are obtained with an additional pass through the corpus using MapReduce. This implementation is much more efficient than one that relies on web search queries. CBL typically requires cooccurrence counts of at least 10,000 instances with any of at least 10,000 patterns, which would require 100 million hit count queries. 4.2.4 Candidate Assessment Next, for each predicate CBL trains a discretized Naive Bayes classifier to classify the candidate instances. Its features include pointwise mutual information (PMI) scores (Turney, 2001) of the candidate instance with each of the positive and negative patterns associated with the class. The current sets of promoted and negative instances are used as training examples for the classifier. Attributes are discretized based on information gain (Fayyad and Irani, 1993). Patterns are assessed using an estimate of the precision of each pattern p: Precision(p) Ei∈I count (i, p) = count(p) where Z is the set of promoted instances for the predicate currently being considered, count(i, p) is the co-occurrence count of instance i with pattern p, and count(p) is the hit count of the patter</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the web for synonyms: Pmi-ir versus lsa on toefl. In EMCL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
</authors>
<title>Self-training for machine translation.</title>
<date>2006</date>
<booktitle>In NIPS workshop on Machine Learning for Multilingual Information Access.</booktitle>
<contexts>
<context position="6615" citStr="Ueffing, 2006" startWordPosition="996" endWordPosition="997">mi-supervised learning. Bootstrapping approaches start with a small number of labeled ‘seed’ examples, use those seed examples to train an initial model, then use this model to label some of the unlabeled data. The model is then retrained, using the original seed examples plus the self-labeled examples. This process iterates, gradually expanding the amount of labeled data. Such approaches have shown promise in applications such as web page classification (Blum and Mitchell, 1998), named entity classification (Collins and Singer, 1999), parsing (McClosky et al., 2006), and machine translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and Jones, 1999; Yangarber, 2003). Additional</context>
</contexts>
<marker>Ueffing, 2006</marker>
<rawString>Nicola Ueffing. 2006. Self-training for machine translation. In NIPS workshop on Machine Learning for Multilingual Information Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
</authors>
<title>Counter-training in discovery of semantic patterns.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7203" citStr="Yangarber, 2003" startWordPosition="1088" endWordPosition="1089"> translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and Jones, 1999; Yangarber, 2003). Additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (Pas¸ca et al., 2006; Rosenfeld and Feldman, 2007). Our work builds on these ideas to couple the simultaneous bootstrapped training of multiple categories and multiple relations. Our approach to information extraction is based on using high precision contextual patterns (e.g., ‘is mayor of arg1’ suggests that arg1 is a city). An early pattern-based approach to information extraction acquired ‘is a’ relations from text using generic contextual patterns (Hearst, </context>
</contexts>
<marker>Yangarber, 2003</marker>
<rawString>Roman Yangarber. 2003. Counter-training in discovery of semantic patterns. In ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>