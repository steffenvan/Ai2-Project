<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.572092">
An Analysis of Frequency- and Memory-Based Processing Costs
</title>
<author confidence="0.740842">
Marten van Schijndel William Schuler
</author>
<affiliation confidence="0.805007">
The Ohio State University The Ohio State University
</affiliation>
<email confidence="0.994586">
vanschm@ling.osu.edu schuler@ling.osu.edu
</email>
<sectionHeader confidence="0.997332" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995633125">
The frequency of words and syntactic con-
structions has been observed to have a sub-
stantial effect on language processing. This
begs the question of what causes certain con-
structions to be more or less frequent. A the-
ory of grounding (Phillips, 2010) would sug-
gest that cognitive limitations might cause lan-
guages to develop frequent constructions in
such a way as to avoid processing costs. This
paper studies how current theories of working
memory fit into theories of language process-
ing and what influence memory limitations
may have over reading times. Measures of
such limitations are evaluated on eye-tracking
data and the results are compared with predic-
tions made by different theories of processing.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953764705882">
Frequency effects in language have been isolated
and observed in many studies (Trueswell, 1996;
Jurafsky, 1996; Hale, 2001; Demberg and Keller,
2008). These effects are important because they il-
luminate the ontogeny of language (how individual
speakers have acquired language), but they do not
answer questions about the phylogeny of language
(how the language came to its current form).
Phillips (2010) has hypothesized that grammar
rule probabilities may be grounded in memory lim-
itations. Increased delays in processing center-
embedded sentences as the number of embeddings
increases, for example, are often explained in terms
of a complexity cost associated with maintaining in-
complete dependencies in working memory (Gib-
son, 2000; Lewis and Vasishth, 2005). Other stud-
ies have shown a link between processing delays
</bodyText>
<page confidence="0.983299">
95
</page>
<bodyText confidence="0.999876185185185">
and the low frequency of center-embedded construc-
tions like object relatives (Hale, 2001), but they
have not explored the source of this low frequency.
A grounding hypothesis would claim that the low
probability of generating such a structure may arise
from an associated memory load. In this account,
while these complexity costs may involve language-
specific concepts such as referent or argument link-
ing, the underlying explanation would be one of
memory limitations (Gibson, 2000) or neural acti-
vation (Lewis and Vasishth, 2005).
This paper seeks to explore the different predic-
tions made by these theories on a broad-coverage
corpus of eye-tracking data (Kennedy et al., 2003).
In addition, the current experiment seeks to isolate
memory effects from frequency effects in the same
task. The results show that memory load measures
are a significant factor even when frequency mea-
sures are residualized out.
The remainder of this paper is organized as fol-
lows: Sections 2 and 3 describe several frequency
and memory measures. Section 4 describes a proba-
bilistic hierarchic sequence model that allows all of
these measures to be directly computed. Section 5
describes how these measures were used to predict
reading time durations on the Dundee eye-tracking
corpus. Sections 6 and 7 present results and discuss.
</bodyText>
<sectionHeader confidence="0.997201" genericHeader="method">
2 Frequency Measures
</sectionHeader>
<subsectionHeader confidence="0.821351">
2.1 Surprisal
</subsectionHeader>
<bodyText confidence="0.9998784">
One of the strongest predictors of processing com-
plexity is surprisal (Hale, 2001). It has been shown
in numerous studies to have a strong correlation
with reading time durations in eye-tracking and self-
paced reading studies when calculated with a variety
</bodyText>
<note confidence="0.451468">
Proceedings of NAACL-HLT 2013, pages 95–105,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.995046">
of models (Levy, 2008; Roark et al., 2009; Wu et al.,
2010).
Surprisal predicts the integration difficulty that a
word xt at time step t presents given the preceding
context and is calculated as follows:
</bodyText>
<equation confidence="0.9846054">
E�
s∈S(x1...xt) P(s)
E
s∈S(x1...xt−1) P(s)
(1)
</equation>
<bodyText confidence="0.999530428571428">
where 5(x1 ... xt) is the set of syntactic trees whose
leaves have x1 ... xt as a prefix.1
In essence, surprisal measures how unexpected
constructions are in a given context. What it does
not provide is an explanation for why certain con-
structions would be less common and thus more sur-
prising.
</bodyText>
<subsectionHeader confidence="0.971118">
2.2 Entropy Reduction
</subsectionHeader>
<bodyText confidence="0.996890333333333">
Processing difficulty can also be measured in terms
of entropy (Shannon, 1948). A larger entropy over a
random variable corresponds to greater uncertainty
over the observed value it will take. The entropy of
a syntactic derivation over the sequence x1 ... xt is
calculated as:2
</bodyText>
<equation confidence="0.987844">
J: H(x1...t) = −P(s) · lo92 P(s) (2)
s∈S(x1...xt)
</equation>
<bodyText confidence="0.923825333333333">
Reduction in entropy has been found to predict
processing complexity (Hale, 2003; Hale, 2006;
Roark et al., 2009; Wu et al., 2010; Hale, 2011):
</bodyText>
<equation confidence="0.998614">
AH(x1...t) = max(0, H(x1...t−1)−H(x1...t)) (3)
</equation>
<bodyText confidence="0.999939">
This measures the change in uncertainty about the
discourse as each new word is processed.
</bodyText>
<sectionHeader confidence="0.997074" genericHeader="method">
3 Memory Measures
</sectionHeader>
<subsectionHeader confidence="0.993252">
3.1 Dependency Locality
</subsectionHeader>
<bodyText confidence="0.9994585">
In Dependency Locality Theory (DLT) (Gibson,
2000), complexity arises from intervening referents
introduced between a predicate and its argument.
Under the original formulation of DLT, there is a
</bodyText>
<footnote confidence="0.970372333333334">
1The parser in this study uses a beam. However, given high
parser accuracy, Roark (2001) showed that calculating com-
plexity metrics over a beam should obtain similar results to the
full complexity calculation.
2The incremental formulation used here was first proposed
in Wu et al. (2010).
</footnote>
<bodyText confidence="0.999928894736842">
storage cost for each new referent introduced and an
integration cost for each referent intervening in a de-
pendency projection. This is a simplification made
for ease of computation, and subsequent work has
found DLT to be more accurate cross-linguistically
if the intervening elements are structurally defined
rather than defined in terms of referents (Kwon et
al., 2010). That is, simply having a particular ref-
erent intervene in a dependency projection may not
have as great an effect on processing complexity as
the syntactic construction the referent appears in.
Therefore, this work reinterprets the costs of depen-
dency locality to be related to the events of begin-
ning a center embedding (storage) and completing
a center embedding (integration). Note that anti-
locality effects (where longer dependencies are eas-
ier to process) have also been observed in some lan-
guages, and DLT is unable to account for these phe-
nomena (Vasishth and Lewis, 2006).
</bodyText>
<subsectionHeader confidence="0.999455">
3.2 ACT-R
</subsectionHeader>
<bodyText confidence="0.999906592592593">
Processing complexity has also been attributed to
confusability (Lewis and Vasishth, 2005) as defined
in domain-general cognitive models like ACT-R
(Anderson et al., 2004).
ACT-R is based on theories of neural activation.
Each new word is encoded and stored in working
memory until it is retrieved at a later point for mod-
ification before being re-encoded into the parse. A
newly observed sign (word) associatively activates
any appropriate arguments from working memory,
so multiple similarly appropriate arguments would
slow processing as the parser must choose between
the highly activated hypotheses. Any intervening
signs (words or phrases) that modify a previously
encoded sign re-activate it and raise its resting acti-
vation potential. This can ease later retrieval of that
sign in what is termed an anti-locality effect, con-
tra predictions of DLT. In this way, returning out of
an embedded clause can actually speed processing
by having primed the retrieved sign before it was
needed. ACT-R attributes locality phenomena to fre-
quency effects (e.g. unusual constructions) overrid-
ing such priming and to activation decay if embed-
ded signs do not prime the target sign through mod-
ification (as in parentheticals). Finally, ACT-R pre-
dicts something like DLT’s storage cost due to the
need to differentiate each newly encoded sign from
</bodyText>
<equation confidence="0.994291">
surprisal(xt) = −lo92
</equation>
<page confidence="0.998006">
96
</page>
<figureCaption confidence="0.9988885">
Figure 1: Two disjoint connected components of a phrase
structure tree for the sentence The studio bought the pub-
lisher’s rights, shown immediately prior to the word pub-
lisher.
</figureCaption>
<bodyText confidence="0.9771085">
those previously encoded (similarity-based encod-
ing interference) (Lewis et al., 2006).
</bodyText>
<subsectionHeader confidence="0.999237">
3.3 Hierarchic Sequential Prediction
</subsectionHeader>
<bodyText confidence="0.99994813559322">
Current models of working memory in structured
tasks are defined in terms of hierarchies of sequen-
tial processes, in which superordinate sequences can
be interrupted by subordinate sequences and resume
when the subordinate sequences have concluded
(Botvinick, 2007). These models rely on temporal
cueing as well as content-based cueing to explain
how an interrupted sequence may be recalled for
continuation.
Temporal cueing is based on a context of temporal
features for the current state (Howard and Kahana,
2002). The temporal context in which the subor-
dinate sequence concludes must be similar enough
to the temporal context in which it was initiated to
recall where in the superordinate sequence the sub-
ordinate sequence occurred. For example, the act
of making breakfast may be interrupted by a phone
call. Once the call is complete, the temporal context
is sufficiently similar to when the call began that one
is able to continue preparing breakfast. The associ-
ation between the current temporal context and the
temporal context prior to the interruption is strong
enough to cue the next action.
Temporal cueing is complemented by sequential
(content-based) cueing (Botvinick, 2007) in which
the content of an individual element is associated
with, and thus cues, the following element. For ex-
ample, recalling the 20th note of a song is difficult,
but when playing the song, each note cues the fol-
lowing note, leading one to play the 20th note with-
out difficulty.
Hierarchic sequential prediction may be directly
applicable to processing syntactic center embed-
dings (van Schijndel et al., in press). An ongoing
parse may be viewed graph-theoretically as one or
more connected components of incomplete phrase
structure trees (see Figure 1). Beginning a new sub-
ordinate sequence (a center embedding) introduces
a new connected component, disjoint from that of
the superordinate sequence. As the subordinate se-
quence proceeds, the new component gains asso-
ciated discourse referents, each sequentially cued
from the last, until finally it merges with the super-
ordinate connected component at the end of the em-
bedded clause, forming a single connected compo-
nent representing the parse up to that point. Since
it is not connected to the subordinate connected
component prior to merging, the superordinate con-
nected component must be recalled through tempo-
ral cueing.
McElree (2001; 2006) has found that retrieval
of any non-focused (or in this case, unconnected)
element from memory leads to slower processing.
Therefore, integrating two disjoint connected com-
ponents should be expected to incur a processing
cost due to the need to recall the current state of the
superordinate sequence to continue the parse. Such
a cost would corroborate a DLT-like theory where
integration slows processing.
</bodyText>
<subsectionHeader confidence="0.985602">
3.4 Dynamic Recruitment of Additional
Processing Resources
</subsectionHeader>
<bodyText confidence="0.999965866666667">
Language processing is typically centered in the left
hemisphere of the brain (for right-handed individ-
uals). Just and Varma (2007) provide fMRI re-
sults suggesting readers dynamically recruit addi-
tional processing resources such as the right-side ho-
mologues of the language processing areas of the
brain when processing center-embedded construc-
tions. Once an embedded construction terminates,
the reader may still have temporary access to these
extra processing resources, which may briefly speed
processing.
This hypothesis would, therefore, predict an en-
coding cost when a center embedding is initiated.
The resulting inhibition would trigger recruitment of
additional processing resources, which would then
</bodyText>
<figure confidence="0.997971736842105">
N
D
the
S/NP
}
NP
N
studio
VP
NP
NP
�
–
NP/N
D
the
V
bought
S
</figure>
<page confidence="0.995013">
97
</page>
<bodyText confidence="0.999983625">
allow the rest of the embedded structure to be pro-
cessed at the usual speed. Upon completing an em-
bedding, the difficulty arising from memory retrieval
(McElree, 2001) would be ameliorated by these ex-
tra processing resources, and the reduced process-
ing complexity arising from reduced memory load
would yield a temporary facilitation in processing.
No longer requiring the additional resources to cope
with the increased embedding, the processor would
release them, returning the processor to its usual
speed. Unlike anti-locality, where processing is
facilitated in longer passages due to accumulating
probabilistic evidence, a model of dynamic recruit-
ment of additional processing resources would pre-
dict universal facilitation after a center embedding
of any length, modulo frequency effects.
</bodyText>
<subsectionHeader confidence="0.976966">
3.5 Embedding Difference
</subsectionHeader>
<bodyText confidence="0.999973">
Wu et al. (2010) propose an explicit measure of
the difficulty associated with processing center-
embedded constructions, which is similar to the pre-
dictions of dynamic recruitment and is defined in
terms of changes in memory load. They calcu-
late a probabilistically-weighted average embedding
depth as follows:
</bodyText>
<equation confidence="0.9878435">
J: µemb(x1 ... xt) = d(s) · P(s) (4)
sES(x1...xt)
</equation>
<bodyText confidence="0.9992735">
where d(s) returns the embedding depth of the
derivation s at xt in a variant of a left-corner pars-
ing process.3 Embedding difference may then be de-
rived as:
</bodyText>
<equation confidence="0.996578">
EmbDiff (x1 ... xt) =µemb(x1 ... xt)− (5)
µemb(x1 . . . xt−1)
</equation>
<bodyText confidence="0.986597571428571">
This is hypothesized to correlate positively with
processing load: increasing the embedding depth in-
creases processing load and decreasing it reduces
processing load. Note that embedding difference
makes the opposite prediction from DLT in that in-
tegrating an embedded clause is predicted to speed
processing. In fact, the predictions of embedding
3As pointed out by Wu et al. (2010), in practice this can be
computed over a beam of potential parses in which case it must
be normalized by the total probability of the beam.
difference are such that it may be viewed as an im-
plementation of the predictions of a hierarchic se-
quential processing model with dynamic recruitment
of additional resources.
</bodyText>
<sectionHeader confidence="0.99152" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.899121666666667">
This paper uses a hierarchic sequence model imple-
mentation of a left-corner parser variant (van Schijn-
del et al., in press), which represents connected com-
ponents of phrase structure trees in hierarchies of
hidden random variables. This requires, at each time
step t:
</bodyText>
<listItem confidence="0.9193538">
• a hierarchically-organized set of N connected
component states qt, each consisting of an ac-
tive sign of category aqt , and an awaited sign
of category bqt , separated by a slash ‘/’; and
• an observed word xt.
</listItem>
<bodyText confidence="0.9997965">
Each connected component state in this model then
represents a contiguous portion of a phrase structure
tree (see Figure 1 on preceding page).
The operations of this parser can be defined as a
deductive system (Shieber et al., 1995) with an input
sequence consisting of a top-level connected com-
ponent state T/T, corresponding to an existing dis-
course context, followed by a sequence of observed
words x1, x2, ...4 If an observation xt can attach as
the awaited sign of the most recent (most subordi-
nate) connected component a/b, it is hypothesized
to do so, turning this incomplete sign into a com-
plete sign a (F–, below); or if the observation can
serve as a lower descendant of this awaited sign, it
is hypothesized to form the first complete sign a′ in
a newly initiated connected component (F+):
</bodyText>
<equation confidence="0.664343">
b → xt (F–)
</equation>
<bodyText confidence="0.991577">
Then, if either of these complete signs (a or a′
above, matched to a″below) can attach as an initial
</bodyText>
<subsectionHeader confidence="0.191042">
4A deductive system consists of inferences or productions
</subsectionHeader>
<bodyText confidence="0.556386333333333">
P
of the form: QR, meaning premise P entails conclusion Q ac-
cording to rule R.
</bodyText>
<figure confidence="0.911368">
a/b xt
a
a/b a′ b a ... ; a xt (F+)
a/b xt →+
</figure>
<page confidence="0.99697">
98
</page>
<table confidence="0.995624894736842">
⊤/⊤, NP/N L– studio
⊤/⊤ the F+
⊤/⊤, D
⊤/⊤, S/VP L– bought
⊤/⊤, NP F–
⊤/⊤, S/NP L+ the
F+
⊤/⊤, S/VP, V
⊤/⊤, S L+ F–
⊤/⊤
⊤/⊤, S/NP, NP/N L– publisher
F+
⊤/⊤, S/NP, D
⊤/⊤, S/NP, D/G L– ’s
F–
⊤/⊤, S/NP, NP
F–
⊤/⊤, S/NP, D
⊤/⊤, S/N L+ rights
</table>
<figureCaption confidence="0.5285891">
Figure 2: Example parse (in the form of a deductive proof) of the sentence The studio bought the publisher’s rights,
using F+, F–, L+, and L– productions. Each pair of deductions combines a context of one or more connected compo-
nent states with a sign (word) observed in that context. By applying the F and L rules to the observed sign and context,
the parser is able to generate a consequent context. Initially, the context corresponds to a connected pre-sentential
dialogue state ⊤/⊤. When the is observed, the parser applies F+ to begin a new connected component state D. By
applying L–, the parser determines that this new connected component is unfinished and generates an appropriate
incomplete connected component state NP/N, encoding the superordinate state ⊤/⊤ for later retrieval. Further on, the
parser observes ’s and uses F– to avoid generating a new connected component, which completes the sign D. The
parser follows this up with L+ to recall the superordinate connected component state S/NP and integrate it into the
most deeply embedded connected component, which results in a less deeply embedded structure.
</figureCaption>
<bodyText confidence="0.999961375">
child of the awaited sign of the immediately superor-
dinate connected component state a/b, it is hypoth-
esized to do so and terminate the subordinate con-
nected component state, with xt as the last observa-
tion of the terminated connected component (L+); or
if the observation can serve as a lower descendant of
this awaited sign, it is hypothesized to remain dis-
joint and form its own connected component (L–):
</bodyText>
<equation confidence="0.672348333333333">
a/b a′′ a/b′′ b → a′′ b′′ (L+)
a/b a
a/b a′/b′′ b→ a′ ...; a′ → a′′ b′′ (L–)
</equation>
<bodyText confidence="0.909900064516129">
These operations can be made probabilistic. The
probability a of a transition at time step t is defined
in terms of (i) a probability 0 of initiating a new con-
nected component state with xt as its first observa-
tion, multiplied by (ii) the probability A of terminat-
ing a connected component state with xt as its last
observation, multiplied by (iii) the probabilities a
and Q of generating categories for active and awaited
signs aqt and bqt in the resulting most subordinate
connected component state qt . This kind of model
can be defined directly on PCFG probabilities and
trained to produce state-of-the-art accuracy by using
the latent variable annotation of Petrov et al. (2006)
(van Schijndel et al., in press).5
An example parse is shown in Figure 2. Since
two binary structural decisions (F and L) must be
made in order to generate each word, there are four
possible structures that may be generated (see Ta-
ble 1). The F+L– transition initiates a new level
of embedding at word xt and so requires the super-
ordinate state to be encoded for later retrieval (e.g.
on observing the in Figure 2). The F–L+ transi-
tion completes the deepest level of embedding and
therefore requires the recall of the current superor-
dinate connected component state with which the
5The model has been shown to achieve an F-score of 87.8,
within .2 points of the Petrov and Klein (2007) parser, which
obtains an F-score of 88.0 on the same task. Because the se-
quence model is defined over binary-branching phrase structure,
both parsers were evaluated on binary-branching phrase struc-
ture trees to provide a fair comparison.
</bodyText>
<page confidence="0.980591">
99
</page>
<table confidence="0.99971825">
F–L– Cue Active Sign
F+L– Initiate/Encode
F–L+ Terminate/Integrate
F+L+ Cue Awaited Sign
</table>
<tableCaption confidence="0.817668666666667">
Table 1: The hierarchical structure decisions and the op-
erations they represent. F+L– initiates a new connected
component, F–L+ integrates two disjoint connected com-
</tableCaption>
<bodyText confidence="0.97988047368421">
ponents into a single connected component, and F–L–
and F+L+ sequentially cue, respectively, a new active
sign (along with an associated awaited sign) and a new
awaited sign from the most recent connected component.
subordinate connected component state will be in-
tegrated. For example, in Figure 2, upon observ-
ing ’s, the parser must use temporal cueing to re-
call that it is in the middle of processing an NP (to
complete an S), which sequentially cues a prediction
of N. F–L– transitions complete the awaited sign of
the most subordinate state and so sequentially cue
a following connected component state at the same
tier of the hierarchy. For example, in Figure 2, after
observing studio, the parser uses the completed NP
to sequentially cue the prediction that it has finished
the left child of an S. F+L+ transitions locally ex-
pand the awaited sign of the most subordinate state
and so should also not require any recall or encod-
ing. For example, in Figure 2, observing bought
while awaiting a VP sequentially cues a prediction
of NP.
F+L–, then, loosely corresponds to a storage ac-
tion under DLT as more hierarchic levels must now
be maintained at each future step of the parse. As
stated before, it differs from DLT in that it is sensi-
tive to the depth of embedding rather than a partic-
ular subset of syntactic categories. Wu et al. (2010)
found that increasing the embedding depth led to
longer reading times in a self-paced reading experi-
ment. In ACT-R terms, F+L– corresponds to an en-
coding action, potentially causing processing diffi-
culty resulting from the similarity of the current sign
to previously encoded signs.
F–L+, by contrast, is similar to DLT’s integra-
tion action since a subordinate connected compo-
nent is integrated into the rest of the parse structure.
This represents a temporal cueing event in which
the awaited category of the superordinate connected
</bodyText>
<table confidence="0.998872166666667">
Theory F+L– F–L+
DLT positive positive
ACT-R positive positive
Hier. Sequential Prediction positive positive
Dynamic Recruitment positive negative
Embedding Difference negative
</table>
<tableCaption confidence="0.9949">
Table 2: Each theory’s prediction of the direction of
</tableCaption>
<bodyText confidence="0.966776">
the correlation between each hierachical structure predic-
tor and reading times. Hierarchic sequential prediction
is agnostic about the processing speed of F+L– opera-
tions, and none of the theories make any predictions as to
the sign associated with the within-embedding measures
F–L– and F+L+.
component is recalled. In contrast to DLT, embed-
ding difference and dynamic recruitment would pre-
dict a shorter reading time in the F–L+ case be-
cause of the reduction in memory load. In an ACT-R
framework, reading time durations can increase at
the retrieval site because the retrieval causes compe-
tition among similarly encoded signs in the context
set. While it is possible for reading times to decrease
when completing a center embedding in ACT-R (Va-
sishth and Lewis, 2006), this would be expressed
as a frequency effect due to certain argument types
commonly foreshadowing their predicates (Jaeger et
al., 2008). Since frequency effects are factored sep-
arately from memory effects in this study, ACT-R
would predict longer residual (memory-based) read-
ing times when completing an embedding.
Predicted correlations to reading times for the F
and L transitions are summarized in Table 2.
</bodyText>
<sectionHeader confidence="0.964208" genericHeader="method">
5 Eye-tracking
</sectionHeader>
<bodyText confidence="0.999903769230769">
Eye-tracking and reading time data are often used to
test complexity measures (Gibson, 2000; Demberg
and Keller, 2008; Roark et al., 2009) under the as-
sumption that readers slow down when reading more
complex passages. Readers saccade over portions of
text and regress back to preceding text in complex
patterns, but studies have correlated certain mea-
sures with certain processing constraints (see Clifton
et al. 2007 for a review). For example, the initial
length of time fixated on a single word is correlated
with word identification time; whereas regression
durations after a word is fixated (but prior to a fix-
ation in a new region) are hypothesized to correlate
</bodyText>
<page confidence="0.983129">
100
</page>
<bodyText confidence="0.999838466666667">
with integration difficulty.
Since this work focuses on incremental process-
ing, all processing that occurs up to a given point in
the sentence is of interest. Therefore, in this study,
predictions will be compared to go-past durations.
Go-past durations are calculated by summing all fix-
ations in a region of text, including regressions, un-
til a new region is fixated, which accounts for addi-
tional processing that may take place after initial lex-
ical access, but before the next region is processed.
For example, if one region ends at word 5 in a sen-
tence, and the next fixation lands on word 8, then the
go-past region consists of words 6-8 and the go-past
duration sums all fixations until a fixation occurs af-
ter word 8.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.99909424137931">
The measures presented in this paper were evaluated
on the Dundee eye-tracking corpus (Kennedy et al.,
2003). The corpus consists of 2388 sentences of nat-
urally occurring news text written in standard British
English. The corpus also includes eye-tracking data
from 10 native English speakers, which provides
a test corpus of 260,124 subject-duration pairs of
reading time data. Of this, any fixated words ap-
pearing fewer than 5 times in the training data were
considered unknown and were filtered out to obtain
accurate predictions. Fixations on the first or last
words of a line were also filtered out to avoid any
‘wrap-up’ effects resulting from preparing to sac-
cade to the beginning of the next line or resulting
from orienting to a new line. Additionally, following
Demberg and Keller (2008), any fixations that skip
more than 4 words were attributed to track loss by
the eyetracker or lack of attention of the reader and
so were excluded from the analysis. This left the fi-
nal evaluation corpus with 151,331 subject-duration
pairs.
The evaluation consisted of fitting a linear mixed-
effects model (Baayen et al., 2008) to reading time
durations using the lmer function of the lme4 R
package (Bates et al., 2011; R Development Core
Team, 2010). This allowed by-subject and by-item
variation to be included in the initial regression as
random intercepts in addition to several baseline pre-
dictors.6 Before fitting, the durations extracted from
</bodyText>
<footnote confidence="0.712426">
6Each fixed effect was centered to reduce collinearity.
</footnote>
<bodyText confidence="0.999983975609756">
the corpus were log-transformed, producing more
normally distributed data to obey the assumptions of
linear mixed effects models.7
Included among the fixed effects were the posi-
tion in the sentence that initiated the go-past region
(SENTPOS) and the number of characters in the ini-
tiating word (NRCHAR). The difficulty of integrat-
ing a word may be seen in whether the immediately
following word was fixated (NEXTISFIX), and sim-
ilarly if the immediately previous word was fixated
(PREVISFIX) the current word probably need not be
fixated for as long. Finally, unigram (LOGPROB)
and bigram probabilities are included. The bigram
probabilities are those of the current word given the
previous word (LOGFWPROB) and the current word
given the following word (LOGBWPROB). Fossum
and Levy (2012) showed that for n-gram probabili-
ties to be effective predictors on the Dundee corpus,
they must be calculated from a wide variety of texts,
so following them, this study used the Brown corpus
(Francis and Kucera, 1979), the WSJ Sections 02-21
(Marcus et al., 1993), the written text portion of the
British National Corpus (BNC Consortium, 2007),
and the Dundee corpus (Kennedy et al., 2003). This
amounted to an n-gram training corpus of roughly
87 million words. These statistics were smoothed
using the SRILM (Stolcke, 2002) implementation of
modified Kneser-Ney smoothing (Chen and Good-
man, 1998). Finally, total surprisal (SURP) was in-
cluded to account for frequency effects in the base-
line.
The preceding measures are commonly used in
baseline models to fit reading time data (Demberg
and Keller, 2008; Frank and Bod, 2011; Fossum and
Levy, 2012) and were calculated from the final word
of each go-past region. The following measures
create a more sophisticated baseline by accumulat-
ing over the entire go-past region to capture what
must be integrated into the discourse to continue the
parse. One factor (CWDELTA) simply counts the
number of words in each go-past region. Cumula-
</bodyText>
<footnote confidence="0.653780857142857">
7In particular, these models assume the noise in the data is
normally distributed. Initial exploratory trials showed that the
residuals of fitting any sensible baseline also become more nor-
mally distributed if the response variable is log-transformed. Fi-
nally, the directions of the effects remain the same whether or
not the reading times are log-transformed, though significance
cannot be ascertained without the transform.
</footnote>
<page confidence="0.997941">
101
</page>
<bodyText confidence="0.999860833333333">
tive total surprisal (CUMUSURP) and cumulative en-
tropy reduction (ENTRED) give the surprisal (Hale,
2001) and entropy reduction (Hale, 2003) summed
over the go-past region. To avoid convergence is-
sues, each of the cumulative measures is residual-
ized from the next simpler model in the following
order: CWDELTA from the standard baseline, CU-
MUSURP from the baseline with CWDELTA, and EN-
TRED from the baseline with all other effects.
Residualization was accomplished by using the
simpler mixed-effects model to fit the measure of in-
terest. The residuals from that model fit were then
used in place of the factor of interest. All joint inter-
actions were included in the baseline model as well.
Finally, to account for spillover effects (Just et al.,
1982) where processing from a previous region con-
tributes to the following duration, the above baseline
predictors from the previous go-past region were in-
cluded as factors for the current region.
Having SURP as a predictor with CUMUSURP may
seem redundant, but initial analyses showed SURP
was a significant predictor over CUMUSURP when
CWDELTA was a separate factor in the baseline (cur-
rent: p = 2.2 · 10−16 spillover: p = 2 · 10−15)
and vice versa (current: p = 2.2 · 10−16 spillover:
p = 6 · 10−5). One reason for this could be that
go-past durations conflate complexity experienced
when initially fixating on a region with the difficulty
experienced during regressions. By including both
versions of surprisal, the model is able to account
for frequency effects occurring in both conditions.
This study is only interested in how well the pro-
posed memory-based measures fit the data over the
baseline, so to avoid fitting to the test data or weak-
ening the baseline by overfitting to training data, the
full baseline was used in the final evaluation.
Each measure proposed in this paper was summed
over go-past regions to make it cumulative and
was residualized from all non-spillover factors be-
fore being included on top of the full baseline as a
main effect. Likewise, the spillover version of each
proposed measure was residualized from the other
spillover factors before being included as a main ef-
fect. Only a single proposed measure (or its spillover
corrollary) was included in each model. The results
shown in Table 3 reflect the probability of the full
model fit being obtained by the model lacking each
factor of interest. This was found via posterior sam-
</bodyText>
<table confidence="0.9993778">
Factor Operation t-score p-value
F–L– Cue Active 0.60 0.55
F+L– Initiate 7.10 2.22·10−14
F–L+ Integrate -5.44 5.23·10−8
F+L+ Cue Awaited -1.55 0.12
</table>
<tableCaption confidence="0.93371025">
Table 3: Significance of each of the structure generation
outcomes at predicting log-transformed durations when
added to the baseline as a main effect after being residu-
alized from it. The sign of the t-score indicates the direc-
tion of the correlation between the residualized factor and
go-past durations. Note that these factors are all based
on the current go-past region; the spillover corollaries of
these were not significant predictors of reading times.
</tableCaption>
<bodyText confidence="0.999591875">
pling of each factor using the Markov chain Monte
Carlo implementation of the languageR R package
(Baayen, 2008).
The results indicate that the F+L– and F–L+ mea-
sures were both significant predictors of duration as
expected. Further, F–L– and F+L+, which both sim-
ply reflect sequential cueing, were not significant
predictors of go-past duration, also as expected.
</bodyText>
<sectionHeader confidence="0.984909" genericHeader="discussions">
7 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999851782608696">
The fact that F+L– was strongly predictive over the
baseline is encouraging as it suggests that memory
limitations could provide at least a partial explana-
tion of why certain constructions are less frequent in
corpora and thus yield a high surprisal. Moreover,
it indicates that the model corroborates the shared
prediction of most of the memory-based models that
initiating a new connected component slows pro-
cessing.
The fact that F–L+ is predictive but has a neg-
ative coefficient could be evidence of anti-locality,
or it could be an indication of some sort of pro-
cessing momentum due to dynamic recruitment of
additional processing resources (Just and Varma,
2007). Since anti-locality is an expectation-based
frequency effect, and since this study controlled for
frequency effects with n-grams, surprisal, and en-
tropy reduction, an anti-locality explanation would
rely on either (i) more precise variants of the met-
rics used in this study or (ii) other frequency metrics
altogether. Future work could investigate the possi-
bility of anti-locality by looking at the distance be-
tween an encoding operation and its corresponding
</bodyText>
<page confidence="0.99754">
102
</page>
<bodyText confidence="0.999982070422536">
integration action to see if the integration facilita-
tion observed in this study is driven by longer em-
beddings or if there is simply a general facilitation
effect when completing embeddings.
The finding of a negative integration cost was pre-
viously observed by Wu et al. (2010) as well as
Demberg and Keller (2008), although Demberg and
Keller calculated it using the original referent-based
definitions of Gibson (1998; 2000) and varied which
parts of speech counted for calculating integration
cost. Ultimately, Demberg and Keller (2008) con-
cluded that the negative coefficient was evidence
that integration cost was not a good broad-coverage
predictor of reading times; however, this study has
replicated the effect and showed it to be a very strong
predictor of reading times, albeit one that is corre-
lated with facilitation rather than inhibition.
It is interesting that many studies have found
negative integration cost using naturalistic stimuli
while others have consistently found positive inte-
gration cost when using constructed stimuli with
multiple center embeddings presented without con-
text (Gibson, 2000; Chen et al., 2005; Kwon et al.,
2010). It may be the case that any dynamic re-
cruitment is overwhelmed by the memory demands
of multiply center-embedded stimuli. Alternatively,
it may be that the difficulty of processing multiply
center-embedded sentences containing ambiguities
produces anxiety in subjects, which slows process-
ing at implicit prosodic boundaries (Fodor, 2002;
Mitchell et al., 2008). In any case, the source of this
discrepancy presents an attractive target for future
research.
In general, sequential prediction does not seem
to present people with any special ease or difficulty
as evidenced by the lack of significance of F–L–
and F+L+ predictions when frequency effects are
factored out. This supports a theory of sequential,
content-based cueing (Botvinick, 2007) that predicts
that certain states would directly cue other states and
thus avoid recall difficulty. An example of this may
be seen in the case of a transitive verb triggering
the prediction of a direct object. This kind of cue-
ing would show up as a frequency effect predicted
by surprisal rather than as a memory-based cost,
due to frequent occurrences becoming ingrained as
a learned skill. Future work could use these sequen-
tial cueing operations to investigate further claims
of the dynamic recruitment hypothesis. One of the
implications of the hypothesis is that recruitment of
resources alleviates the initial encoding cost, which
allows the parser to continue on as before the em-
bedding. DLT, on the other hand, predicts that there
is a storage cost for maintaining unresolved depen-
dencies during a parse (Gibson, 2000). By weight-
ing each of the sequential cueing operations with the
embedding depth at which it occurs, an experiment
may be able to test these two predictions.
This study has shown that measures based on
working memory operations have strong predictivity
over other previously proposed measures including
those associated with frequency effects. This sug-
gests that memory limitations may provide a partial
explanation of what gives rise to frequency effects.
Lastly, this paper provides evidence that there is a
robust facilitation effect in English that arises from
completing center embeddings.
The hierarchic sequence model, all evaluation
scripts, and regression results for all baseline pre-
dictors used in this paper are freely available at
http://sourceforge.net/projects/modelblocks/.
</bodyText>
<sectionHeader confidence="0.996923" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999295">
Thanks to Peter Culicover, Micha Elsner, and three
anonymous reviewers for helpful suggestions. This
work was funded by an OSU Department of Lin-
guistics Targeted Investment for Excellence (TIE)
grant for collaborative interdisciplinary projects
conducted during the academic year 2012-13.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983130714285714">
John R. Anderson, Dan Bothell, Michael D. Byrne,
S. Douglass, Christian Lebiere, and Y. Qin. 2004. An
integrated theory of the mind. Psychological Review,
111(4):1036–1060.
R. Harald Baayen, D. J. Davidson, and Douglas M. Bates.
2008. Mixed-effects modeling with crossed random
effects for subjects and items. Journal of Memory and
Language, 59:390–412.
R. Harald Baayen. 2008. Analyzing Linguistic Data:
A Practical Introduction to Statistics using R. Cam-
bridge University Press, New York, NY.
Douglas Bates, Martin Maechler, and Ben Bolker, 2011.
lme4: Linear mixed-effects models using S4 classes.
BNC Consortium. 2007. The british national corpus.
</reference>
<page confidence="0.995592">
103
</page>
<reference confidence="0.999531981132075">
Matthew Botvinick. 2007. Multilevel structure in behav-
ior and in the brain: a computational model of Fuster’s
hierarchy. Philosophical Transactions of the Royal So-
ciety, Series B: Biological Sciences, 362:1615–1626.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University.
Evan Chen, Edward Gibson, and Florian Wolf. 2005.
Online syntactic storage costs in sentence comprehen-
sion. Journal of Memory and Language, 52(1):144–
169.
Charles Clifton, Adrian Staub, and Keith Rayner. 2007.
Eye movements in reading words and sentences. In
Eye movements: A window on mind and brain, pages
341–372. Elsevier.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193–210.
Janet Fodor. 2002. Prosodic disambiguation in silent
reading. In M. Hirotani, editor, In Proceedings of
NELS 32.
Victoria Fossum and Roger Levy. 2012. Sequential
vs. hierarchical syntactic models of human incremen-
tal sentence processing. In Proceedings of CMCL-
NAACL 2012. Association for Computational Linguis-
tics.
W. Nelson Francis and Henry Kucera. 1979. The brown
corpus: A standard corpus of present-day edited amer-
ican english.
Stefan Frank and Rens Bod. 2011. Insensitivity of
the human sentence-processing system to hierarchical
structure. Psychological Science.
Edward Gibson. 1998. Linguistic complexity: Locality
of syntactic dependencies. Cognition, 68(1):1–76.
Edward Gibson. 2000. The dependency locality theory:
A distance-based theory of linguistic complexity. In
Image, language, brain: Papers from the first mind ar-
ticulation project symposium, pages 95–126.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the second
meeting of the North American chapter of the Associ-
ation for Computational Linguistics, pages 159–166,
Pittsburgh, PA.
John Hale. 2003. Grammar, Uncertainty and Sen-
tence Processing. Ph.D. thesis, Cognitive Science,
The Johns Hopkins University.
John Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30(4):609–642.
John Hale. 2011. What a rational parser would do. Cog-
nitive Science, 35(3):399–443.
Marc W. Howard and Michael J. Kahana. 2002. A dis-
tributed representation of temporal context. Journal of
Mathematical Psychology, 45:269–299.
F. T. Jaeger, E. Fedorenko, P. Hofmeister, and E. Gib-
son. 2008. Expectation-based syntactic processing:
Antilocality outside of head-final languages. In The
21st CUNY Sentence Processing Conference.
Daniel Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science: A Multidisciplinary Journal, 20(2):137–194.
Marcel Adam Just and Sashank Varma. 2007. The or-
ganization of thinking: What functional brain imaging
reveals about the neuroarchitecture of complex cogni-
tion. Cognitive, Affective, &amp; Behavioral Neuroscience,
7:153–191.
Marcel Adam Just, Patricia A. Carpenter, and Jacque-
line D. Woolley. 1982. Paradigms and processes in
reading comprehension. Journal of Experimental Psy-
chology: General, 111:228–238.
Alan Kennedy, James Pynte, and Robin Hill. 2003. The
Dundee corpus. In Proceedings of the 12th European
conference on eye movement.
Nayoung Kwon, Yoonhyoung Lee, Peter C. Gordon,
Robert Kluender, and Maria Polinsky. 2010. Cog-
nitive and linguistic factors affecting subject/object
asymmetry: An eye-tracking study of pre-nominal rel-
ative clauses in korean. Language, 86(3):561.
Roger Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106(3):1126–1177.
Richard L. Lewis and Shravan Vasishth. 2005.
An activation-based model of sentence processing
as skilled memory retrieval. Cognitive Science,
29(3):375–419.
Richard L. Lewis, Shravan Vasishth, and Jane A. Van
Dyke. 2006. Computational principles of working
memory in sentence comprehension. Trends in Cog-
nitive Science, 10(10):447–454.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313–330.
Brian McElree. 2001. Working memory and focal atten-
tion. Journal of Experimental Psychology, Learning
Memory and Cognition, 27(3):817–835.
Brian McElree. 2006. Accessing recent events. The Psy-
chology of Learning and Motivation, 46:155–200.
D. Mitchell, X. Shen, M. Green, and T. Hodgson. 2008.
Accounting for regressive eye-movements in models
of sentence processing: A reappraisal of the selective
reanalysis hypothesis. Journal of Memory and Lan-
guage, 59:266–293.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
NAACL HLT 2007, pages 404–411, Rochester, New
York, April. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.983644">
104
</page>
<reference confidence="0.99912270212766">
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics (COLING/ACL’06).
Colin Phillips. 2010. Some arguments and non-
arguments for reductionist accounts of syntactic phe-
nomena. Language and Cognitive Processes, 28:156–
187.
R Development Core Team, 2010. R: A Language and
Environment for Statistical Computing. R Foundation
for Statistical Computing, Vienna, Austria. ISBN 3-
900051-07-0.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and syn-
tactic expectation-based measures for psycholinguistic
modeling via incremental top-down parsing. Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Langauge Processing, pages 324–333.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
Claude Shannon. 1948. A mathematical theory of com-
munication. Bell System Technical Journal, 27:379–
423, 623–656.
Stuart M. Shieber, Yves Schabes, and Fernando C.N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. Journal of Logic Programming, 24:3–
36.
Andreas Stolcke. 2002. Srilm – an extensible language
modeling toolkit. In Seventh International Conference
on Spoken Language Processing.
John Trueswell. 1996. The role of lexical frequency
in syntactic ambiguity resolution. Journal of Memory
and Language, 35:566–585.
Marten van Schijndel, Andy Exley, and William Schuler.
in press. A model of language processing as hierarchic
sequential prediction. Topics in Cognitive Science.
Shravan Vasishth and Richard L. Lewis. 2006.
Argument-head distance and processing complexity:
Explaining both locality and antilocality effects. Lan-
guage, 82(4):767–794.
Stephen Wu, Asaf Bachrach, Carlos Cardenas, and
William Schuler. 2010. Complexity metrics in an in-
cremental right-corner parser. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL’10), pages 1189–1198.
</reference>
<page confidence="0.998971">
105
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.981022">
<title confidence="0.999864">An Analysis of Frequencyand Memory-Based Processing Costs</title>
<author confidence="0.99998">Marten van_Schijndel William Schuler</author>
<affiliation confidence="0.99978">The Ohio State University The Ohio State University</affiliation>
<email confidence="0.992805">vanschm@ling.osu.eduschuler@ling.osu.edu</email>
<abstract confidence="0.999315588235294">The frequency of words and syntactic constructions has been observed to have a substantial effect on language processing. This begs the question of what causes certain constructions to be more or less frequent. A theory of grounding (Phillips, 2010) would suggest that cognitive limitations might cause languages to develop frequent constructions in such a way as to avoid processing costs. This paper studies how current theories of working memory fit into theories of language processing and what influence memory limitations may have over reading times. Measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John R Anderson</author>
<author>Dan Bothell</author>
<author>Michael D Byrne</author>
<author>S Douglass</author>
<author>Christian Lebiere</author>
<author>Y Qin</author>
</authors>
<title>An integrated theory of the mind.</title>
<date>2004</date>
<journal>Psychological Review,</journal>
<volume>111</volume>
<issue>4</issue>
<contexts>
<context position="6320" citStr="Anderson et al., 2004" startWordPosition="987" endWordPosition="990">s the syntactic construction the referent appears in. Therefore, this work reinterprets the costs of dependency locality to be related to the events of beginning a center embedding (storage) and completing a center embedding (integration). Note that antilocality effects (where longer dependencies are easier to process) have also been observed in some languages, and DLT is unable to account for these phenomena (Vasishth and Lewis, 2006). 3.2 ACT-R Processing complexity has also been attributed to confusability (Lewis and Vasishth, 2005) as defined in domain-general cognitive models like ACT-R (Anderson et al., 2004). ACT-R is based on theories of neural activation. Each new word is encoded and stored in working memory until it is retrieved at a later point for modification before being re-encoded into the parse. A newly observed sign (word) associatively activates any appropriate arguments from working memory, so multiple similarly appropriate arguments would slow processing as the parser must choose between the highly activated hypotheses. Any intervening signs (words or phrases) that modify a previously encoded sign re-activate it and raise its resting activation potential. This can ease later retrieva</context>
</contexts>
<marker>Anderson, Bothell, Byrne, Douglass, Lebiere, Qin, 2004</marker>
<rawString>John R. Anderson, Dan Bothell, Michael D. Byrne, S. Douglass, Christian Lebiere, and Y. Qin. 2004. An integrated theory of the mind. Psychological Review, 111(4):1036–1060.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Harald Baayen</author>
<author>D J Davidson</author>
<author>Douglas M Bates</author>
</authors>
<title>Mixed-effects modeling with crossed random effects for subjects and items.</title>
<date>2008</date>
<journal>Journal of Memory and Language,</journal>
<pages>59--390</pages>
<contexts>
<context position="24694" citStr="Baayen et al., 2008" startWordPosition="4007" endWordPosition="4010"> obtain accurate predictions. Fixations on the first or last words of a line were also filtered out to avoid any ‘wrap-up’ effects resulting from preparing to saccade to the beginning of the next line or resulting from orienting to a new line. Additionally, following Demberg and Keller (2008), any fixations that skip more than 4 words were attributed to track loss by the eyetracker or lack of attention of the reader and so were excluded from the analysis. This left the final evaluation corpus with 151,331 subject-duration pairs. The evaluation consisted of fitting a linear mixedeffects model (Baayen et al., 2008) to reading time durations using the lmer function of the lme4 R package (Bates et al., 2011; R Development Core Team, 2010). This allowed by-subject and by-item variation to be included in the initial regression as random intercepts in addition to several baseline predictors.6 Before fitting, the durations extracted from 6Each fixed effect was centered to reduce collinearity. the corpus were log-transformed, producing more normally distributed data to obey the assumptions of linear mixed effects models.7 Included among the fixed effects were the position in the sentence that initiated the go-</context>
</contexts>
<marker>Baayen, Davidson, Bates, 2008</marker>
<rawString>R. Harald Baayen, D. J. Davidson, and Douglas M. Bates. 2008. Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59:390–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Harald Baayen</author>
</authors>
<title>Analyzing Linguistic Data: A Practical Introduction to Statistics using R.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="30614" citStr="Baayen, 2008" startWordPosition="4962" endWordPosition="4963">4 5.23·10−8 F+L+ Cue Awaited -1.55 0.12 Table 3: Significance of each of the structure generation outcomes at predicting log-transformed durations when added to the baseline as a main effect after being residualized from it. The sign of the t-score indicates the direction of the correlation between the residualized factor and go-past durations. Note that these factors are all based on the current go-past region; the spillover corollaries of these were not significant predictors of reading times. pling of each factor using the Markov chain Monte Carlo implementation of the languageR R package (Baayen, 2008). The results indicate that the F+L– and F–L+ measures were both significant predictors of duration as expected. Further, F–L– and F+L+, which both simply reflect sequential cueing, were not significant predictors of go-past duration, also as expected. 7 Discussion and Conclusion The fact that F+L– was strongly predictive over the baseline is encouraging as it suggests that memory limitations could provide at least a partial explanation of why certain constructions are less frequent in corpora and thus yield a high surprisal. Moreover, it indicates that the model corroborates the shared predic</context>
</contexts>
<marker>Baayen, 2008</marker>
<rawString>R. Harald Baayen. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics using R. Cambridge University Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Bates</author>
<author>Martin Maechler</author>
<author>Ben Bolker</author>
</authors>
<title>lme4: Linear mixed-effects models using S4 classes. BNC Consortium.</title>
<date>2011</date>
<contexts>
<context position="24786" citStr="Bates et al., 2011" startWordPosition="4024" endWordPosition="4027">d out to avoid any ‘wrap-up’ effects resulting from preparing to saccade to the beginning of the next line or resulting from orienting to a new line. Additionally, following Demberg and Keller (2008), any fixations that skip more than 4 words were attributed to track loss by the eyetracker or lack of attention of the reader and so were excluded from the analysis. This left the final evaluation corpus with 151,331 subject-duration pairs. The evaluation consisted of fitting a linear mixedeffects model (Baayen et al., 2008) to reading time durations using the lmer function of the lme4 R package (Bates et al., 2011; R Development Core Team, 2010). This allowed by-subject and by-item variation to be included in the initial regression as random intercepts in addition to several baseline predictors.6 Before fitting, the durations extracted from 6Each fixed effect was centered to reduce collinearity. the corpus were log-transformed, producing more normally distributed data to obey the assumptions of linear mixed effects models.7 Included among the fixed effects were the position in the sentence that initiated the go-past region (SENTPOS) and the number of characters in the initiating word (NRCHAR). The diff</context>
</contexts>
<marker>Bates, Maechler, Bolker, 2011</marker>
<rawString>Douglas Bates, Martin Maechler, and Ben Bolker, 2011. lme4: Linear mixed-effects models using S4 classes. BNC Consortium. 2007. The british national corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Botvinick</author>
</authors>
<title>Multilevel structure in behavior and in the brain: a computational model of Fuster’s hierarchy.</title>
<date>2007</date>
<journal>Philosophical Transactions of the Royal Society, Series B: Biological Sciences,</journal>
<pages>362--1615</pages>
<contexts>
<context position="8080" citStr="Botvinick, 2007" startWordPosition="1260" endWordPosition="1261">newly encoded sign from surprisal(xt) = −lo92 96 Figure 1: Two disjoint connected components of a phrase structure tree for the sentence The studio bought the publisher’s rights, shown immediately prior to the word publisher. those previously encoded (similarity-based encoding interference) (Lewis et al., 2006). 3.3 Hierarchic Sequential Prediction Current models of working memory in structured tasks are defined in terms of hierarchies of sequential processes, in which superordinate sequences can be interrupted by subordinate sequences and resume when the subordinate sequences have concluded (Botvinick, 2007). These models rely on temporal cueing as well as content-based cueing to explain how an interrupted sequence may be recalled for continuation. Temporal cueing is based on a context of temporal features for the current state (Howard and Kahana, 2002). The temporal context in which the subordinate sequence concludes must be similar enough to the temporal context in which it was initiated to recall where in the superordinate sequence the subordinate sequence occurred. For example, the act of making breakfast may be interrupted by a phone call. Once the call is complete, the temporal context is s</context>
<context position="33935" citStr="Botvinick, 2007" startWordPosition="5474" endWordPosition="5475">ively, it may be that the difficulty of processing multiply center-embedded sentences containing ambiguities produces anxiety in subjects, which slows processing at implicit prosodic boundaries (Fodor, 2002; Mitchell et al., 2008). In any case, the source of this discrepancy presents an attractive target for future research. In general, sequential prediction does not seem to present people with any special ease or difficulty as evidenced by the lack of significance of F–L– and F+L+ predictions when frequency effects are factored out. This supports a theory of sequential, content-based cueing (Botvinick, 2007) that predicts that certain states would directly cue other states and thus avoid recall difficulty. An example of this may be seen in the case of a transitive verb triggering the prediction of a direct object. This kind of cueing would show up as a frequency effect predicted by surprisal rather than as a memory-based cost, due to frequent occurrences becoming ingrained as a learned skill. Future work could use these sequential cueing operations to investigate further claims of the dynamic recruitment hypothesis. One of the implications of the hypothesis is that recruitment of resources allevi</context>
</contexts>
<marker>Botvinick, 2007</marker>
<rawString>Matthew Botvinick. 2007. Multilevel structure in behavior and in the brain: a computational model of Fuster’s hierarchy. Philosophical Transactions of the Royal Society, Series B: Biological Sciences, 362:1615–1626.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="26462" citStr="Chen and Goodman, 1998" startWordPosition="4284" endWordPosition="4288"> Fossum and Levy (2012) showed that for n-gram probabilities to be effective predictors on the Dundee corpus, they must be calculated from a wide variety of texts, so following them, this study used the Brown corpus (Francis and Kucera, 1979), the WSJ Sections 02-21 (Marcus et al., 1993), the written text portion of the British National Corpus (BNC Consortium, 2007), and the Dundee corpus (Kennedy et al., 2003). This amounted to an n-gram training corpus of roughly 87 million words. These statistics were smoothed using the SRILM (Stolcke, 2002) implementation of modified Kneser-Ney smoothing (Chen and Goodman, 1998). Finally, total surprisal (SURP) was included to account for frequency effects in the baseline. The preceding measures are commonly used in baseline models to fit reading time data (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012) and were calculated from the final word of each go-past region. The following measures create a more sophisticated baseline by accumulating over the entire go-past region to capture what must be integrated into the discourse to continue the parse. One factor (CWDELTA) simply counts the number of words in each go-past region. Cumula7In particular</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Chen</author>
<author>Edward Gibson</author>
<author>Florian Wolf</author>
</authors>
<title>Online syntactic storage costs in sentence comprehension.</title>
<date>2005</date>
<journal>Journal of Memory and Language,</journal>
<volume>52</volume>
<issue>1</issue>
<pages>169</pages>
<contexts>
<context position="33167" citStr="Chen et al., 2005" startWordPosition="5357" endWordPosition="5360">g and Keller (2008) concluded that the negative coefficient was evidence that integration cost was not a good broad-coverage predictor of reading times; however, this study has replicated the effect and showed it to be a very strong predictor of reading times, albeit one that is correlated with facilitation rather than inhibition. It is interesting that many studies have found negative integration cost using naturalistic stimuli while others have consistently found positive integration cost when using constructed stimuli with multiple center embeddings presented without context (Gibson, 2000; Chen et al., 2005; Kwon et al., 2010). It may be the case that any dynamic recruitment is overwhelmed by the memory demands of multiply center-embedded stimuli. Alternatively, it may be that the difficulty of processing multiply center-embedded sentences containing ambiguities produces anxiety in subjects, which slows processing at implicit prosodic boundaries (Fodor, 2002; Mitchell et al., 2008). In any case, the source of this discrepancy presents an attractive target for future research. In general, sequential prediction does not seem to present people with any special ease or difficulty as evidenced by the</context>
</contexts>
<marker>Chen, Gibson, Wolf, 2005</marker>
<rawString>Evan Chen, Edward Gibson, and Florian Wolf. 2005. Online syntactic storage costs in sentence comprehension. Journal of Memory and Language, 52(1):144– 169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Clifton</author>
<author>Adrian Staub</author>
<author>Keith Rayner</author>
</authors>
<title>Eye movements in reading words and sentences.</title>
<date>2007</date>
<booktitle>In Eye movements: A window on mind and brain,</booktitle>
<pages>341--372</pages>
<publisher>Elsevier.</publisher>
<contexts>
<context position="22577" citStr="Clifton et al. 2007" startWordPosition="3650" endWordPosition="3653">R would predict longer residual (memory-based) reading times when completing an embedding. Predicted correlations to reading times for the F and L transitions are summarized in Table 2. 5 Eye-tracking Eye-tracking and reading time data are often used to test complexity measures (Gibson, 2000; Demberg and Keller, 2008; Roark et al., 2009) under the assumption that readers slow down when reading more complex passages. Readers saccade over portions of text and regress back to preceding text in complex patterns, but studies have correlated certain measures with certain processing constraints (see Clifton et al. 2007 for a review). For example, the initial length of time fixated on a single word is correlated with word identification time; whereas regression durations after a word is fixated (but prior to a fixation in a new region) are hypothesized to correlate 100 with integration difficulty. Since this work focuses on incremental processing, all processing that occurs up to a given point in the sentence is of interest. Therefore, in this study, predictions will be compared to go-past durations. Go-past durations are calculated by summing all fixations in a region of text, including regressions, until a</context>
</contexts>
<marker>Clifton, Staub, Rayner, 2007</marker>
<rawString>Charles Clifton, Adrian Staub, and Keith Rayner. 2007. Eye movements in reading words and sentences. In Eye movements: A window on mind and brain, pages 341–372. Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
<author>Frank Keller</author>
</authors>
<title>Data from eyetracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>109</volume>
<issue>2</issue>
<contexts>
<context position="1073" citStr="Demberg and Keller, 2008" startWordPosition="159" endWordPosition="162">) would suggest that cognitive limitations might cause languages to develop frequent constructions in such a way as to avoid processing costs. This paper studies how current theories of working memory fit into theories of language processing and what influence memory limitations may have over reading times. Measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing. 1 Introduction Frequency effects in language have been isolated and observed in many studies (Trueswell, 1996; Jurafsky, 1996; Hale, 2001; Demberg and Keller, 2008). These effects are important because they illuminate the ontogeny of language (how individual speakers have acquired language), but they do not answer questions about the phylogeny of language (how the language came to its current form). Phillips (2010) has hypothesized that grammar rule probabilities may be grounded in memory limitations. Increased delays in processing centerembedded sentences as the number of embeddings increases, for example, are often explained in terms of a complexity cost associated with maintaining incomplete dependencies in working memory (Gibson, 2000; Lewis and Vasi</context>
<context position="22276" citStr="Demberg and Keller, 2008" startWordPosition="3602" endWordPosition="3605">ecrease when completing a center embedding in ACT-R (Vasishth and Lewis, 2006), this would be expressed as a frequency effect due to certain argument types commonly foreshadowing their predicates (Jaeger et al., 2008). Since frequency effects are factored separately from memory effects in this study, ACT-R would predict longer residual (memory-based) reading times when completing an embedding. Predicted correlations to reading times for the F and L transitions are summarized in Table 2. 5 Eye-tracking Eye-tracking and reading time data are often used to test complexity measures (Gibson, 2000; Demberg and Keller, 2008; Roark et al., 2009) under the assumption that readers slow down when reading more complex passages. Readers saccade over portions of text and regress back to preceding text in complex patterns, but studies have correlated certain measures with certain processing constraints (see Clifton et al. 2007 for a review). For example, the initial length of time fixated on a single word is correlated with word identification time; whereas regression durations after a word is fixated (but prior to a fixation in a new region) are hypothesized to correlate 100 with integration difficulty. Since this work</context>
<context position="24367" citStr="Demberg and Keller (2008)" startWordPosition="3952" endWordPosition="3955">ews text written in standard British English. The corpus also includes eye-tracking data from 10 native English speakers, which provides a test corpus of 260,124 subject-duration pairs of reading time data. Of this, any fixated words appearing fewer than 5 times in the training data were considered unknown and were filtered out to obtain accurate predictions. Fixations on the first or last words of a line were also filtered out to avoid any ‘wrap-up’ effects resulting from preparing to saccade to the beginning of the next line or resulting from orienting to a new line. Additionally, following Demberg and Keller (2008), any fixations that skip more than 4 words were attributed to track loss by the eyetracker or lack of attention of the reader and so were excluded from the analysis. This left the final evaluation corpus with 151,331 subject-duration pairs. The evaluation consisted of fitting a linear mixedeffects model (Baayen et al., 2008) to reading time durations using the lmer function of the lme4 R package (Bates et al., 2011; R Development Core Team, 2010). This allowed by-subject and by-item variation to be included in the initial regression as random intercepts in addition to several baseline predict</context>
<context position="26669" citStr="Demberg and Keller, 2008" startWordPosition="4319" endWordPosition="4322">wn corpus (Francis and Kucera, 1979), the WSJ Sections 02-21 (Marcus et al., 1993), the written text portion of the British National Corpus (BNC Consortium, 2007), and the Dundee corpus (Kennedy et al., 2003). This amounted to an n-gram training corpus of roughly 87 million words. These statistics were smoothed using the SRILM (Stolcke, 2002) implementation of modified Kneser-Ney smoothing (Chen and Goodman, 1998). Finally, total surprisal (SURP) was included to account for frequency effects in the baseline. The preceding measures are commonly used in baseline models to fit reading time data (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012) and were calculated from the final word of each go-past region. The following measures create a more sophisticated baseline by accumulating over the entire go-past region to capture what must be integrated into the discourse to continue the parse. One factor (CWDELTA) simply counts the number of words in each go-past region. Cumula7In particular, these models assume the noise in the data is normally distributed. Initial exploratory trials showed that the residuals of fitting any sensible baseline also become more normally distributed if the respons</context>
<context position="32344" citStr="Demberg and Keller (2008)" startWordPosition="5235" endWordPosition="5238">eduction, an anti-locality explanation would rely on either (i) more precise variants of the metrics used in this study or (ii) other frequency metrics altogether. Future work could investigate the possibility of anti-locality by looking at the distance between an encoding operation and its corresponding 102 integration action to see if the integration facilitation observed in this study is driven by longer embeddings or if there is simply a general facilitation effect when completing embeddings. The finding of a negative integration cost was previously observed by Wu et al. (2010) as well as Demberg and Keller (2008), although Demberg and Keller calculated it using the original referent-based definitions of Gibson (1998; 2000) and varied which parts of speech counted for calculating integration cost. Ultimately, Demberg and Keller (2008) concluded that the negative coefficient was evidence that integration cost was not a good broad-coverage predictor of reading times; however, this study has replicated the effect and showed it to be a very strong predictor of reading times, albeit one that is correlated with facilitation rather than inhibition. It is interesting that many studies have found negative integ</context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>Vera Demberg and Frank Keller. 2008. Data from eyetracking corpora as evidence for theories of syntactic processing complexity. Cognition, 109(2):193–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Fodor</author>
</authors>
<title>Prosodic disambiguation in silent reading.</title>
<date>2002</date>
<booktitle>In Proceedings of NELS 32.</booktitle>
<editor>In M. Hirotani, editor,</editor>
<contexts>
<context position="33525" citStr="Fodor, 2002" startWordPosition="5411" endWordPosition="5412">dies have found negative integration cost using naturalistic stimuli while others have consistently found positive integration cost when using constructed stimuli with multiple center embeddings presented without context (Gibson, 2000; Chen et al., 2005; Kwon et al., 2010). It may be the case that any dynamic recruitment is overwhelmed by the memory demands of multiply center-embedded stimuli. Alternatively, it may be that the difficulty of processing multiply center-embedded sentences containing ambiguities produces anxiety in subjects, which slows processing at implicit prosodic boundaries (Fodor, 2002; Mitchell et al., 2008). In any case, the source of this discrepancy presents an attractive target for future research. In general, sequential prediction does not seem to present people with any special ease or difficulty as evidenced by the lack of significance of F–L– and F+L+ predictions when frequency effects are factored out. This supports a theory of sequential, content-based cueing (Botvinick, 2007) that predicts that certain states would directly cue other states and thus avoid recall difficulty. An example of this may be seen in the case of a transitive verb triggering the prediction</context>
</contexts>
<marker>Fodor, 2002</marker>
<rawString>Janet Fodor. 2002. Prosodic disambiguation in silent reading. In M. Hirotani, editor, In Proceedings of NELS 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Roger Levy</author>
</authors>
<title>Sequential vs. hierarchical syntactic models of human incremental sentence processing.</title>
<date>2012</date>
<booktitle>In Proceedings of CMCLNAACL 2012. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="25862" citStr="Fossum and Levy (2012)" startWordPosition="4189" endWordPosition="4192">re the position in the sentence that initiated the go-past region (SENTPOS) and the number of characters in the initiating word (NRCHAR). The difficulty of integrating a word may be seen in whether the immediately following word was fixated (NEXTISFIX), and similarly if the immediately previous word was fixated (PREVISFIX) the current word probably need not be fixated for as long. Finally, unigram (LOGPROB) and bigram probabilities are included. The bigram probabilities are those of the current word given the previous word (LOGFWPROB) and the current word given the following word (LOGBWPROB). Fossum and Levy (2012) showed that for n-gram probabilities to be effective predictors on the Dundee corpus, they must be calculated from a wide variety of texts, so following them, this study used the Brown corpus (Francis and Kucera, 1979), the WSJ Sections 02-21 (Marcus et al., 1993), the written text portion of the British National Corpus (BNC Consortium, 2007), and the Dundee corpus (Kennedy et al., 2003). This amounted to an n-gram training corpus of roughly 87 million words. These statistics were smoothed using the SRILM (Stolcke, 2002) implementation of modified Kneser-Ney smoothing (Chen and Goodman, 1998)</context>
</contexts>
<marker>Fossum, Levy, 2012</marker>
<rawString>Victoria Fossum and Roger Levy. 2012. Sequential vs. hierarchical syntactic models of human incremental sentence processing. In Proceedings of CMCLNAACL 2012. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Nelson Francis</author>
<author>Henry Kucera</author>
</authors>
<title>The brown corpus: A standard corpus of present-day edited american english.</title>
<date>1979</date>
<contexts>
<context position="26081" citStr="Francis and Kucera, 1979" startWordPosition="4226" endWordPosition="4229">llowing word was fixated (NEXTISFIX), and similarly if the immediately previous word was fixated (PREVISFIX) the current word probably need not be fixated for as long. Finally, unigram (LOGPROB) and bigram probabilities are included. The bigram probabilities are those of the current word given the previous word (LOGFWPROB) and the current word given the following word (LOGBWPROB). Fossum and Levy (2012) showed that for n-gram probabilities to be effective predictors on the Dundee corpus, they must be calculated from a wide variety of texts, so following them, this study used the Brown corpus (Francis and Kucera, 1979), the WSJ Sections 02-21 (Marcus et al., 1993), the written text portion of the British National Corpus (BNC Consortium, 2007), and the Dundee corpus (Kennedy et al., 2003). This amounted to an n-gram training corpus of roughly 87 million words. These statistics were smoothed using the SRILM (Stolcke, 2002) implementation of modified Kneser-Ney smoothing (Chen and Goodman, 1998). Finally, total surprisal (SURP) was included to account for frequency effects in the baseline. The preceding measures are commonly used in baseline models to fit reading time data (Demberg and Keller, 2008; Frank and </context>
</contexts>
<marker>Francis, Kucera, 1979</marker>
<rawString>W. Nelson Francis and Henry Kucera. 1979. The brown corpus: A standard corpus of present-day edited american english.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Frank</author>
<author>Rens Bod</author>
</authors>
<title>Insensitivity of the human sentence-processing system to hierarchical structure.</title>
<date>2011</date>
<publisher>Psychological Science.</publisher>
<contexts>
<context position="26690" citStr="Frank and Bod, 2011" startWordPosition="4323" endWordPosition="4326">era, 1979), the WSJ Sections 02-21 (Marcus et al., 1993), the written text portion of the British National Corpus (BNC Consortium, 2007), and the Dundee corpus (Kennedy et al., 2003). This amounted to an n-gram training corpus of roughly 87 million words. These statistics were smoothed using the SRILM (Stolcke, 2002) implementation of modified Kneser-Ney smoothing (Chen and Goodman, 1998). Finally, total surprisal (SURP) was included to account for frequency effects in the baseline. The preceding measures are commonly used in baseline models to fit reading time data (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012) and were calculated from the final word of each go-past region. The following measures create a more sophisticated baseline by accumulating over the entire go-past region to capture what must be integrated into the discourse to continue the parse. One factor (CWDELTA) simply counts the number of words in each go-past region. Cumula7In particular, these models assume the noise in the data is normally distributed. Initial exploratory trials showed that the residuals of fitting any sensible baseline also become more normally distributed if the response variable is log-tra</context>
</contexts>
<marker>Frank, Bod, 2011</marker>
<rawString>Stefan Frank and Rens Bod. 2011. Insensitivity of the human sentence-processing system to hierarchical structure. Psychological Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>Linguistic complexity: Locality of syntactic dependencies.</title>
<date>1998</date>
<journal>Cognition,</journal>
<volume>68</volume>
<issue>1</issue>
<contexts>
<context position="32449" citStr="Gibson (1998" startWordPosition="5251" endWordPosition="5252">y or (ii) other frequency metrics altogether. Future work could investigate the possibility of anti-locality by looking at the distance between an encoding operation and its corresponding 102 integration action to see if the integration facilitation observed in this study is driven by longer embeddings or if there is simply a general facilitation effect when completing embeddings. The finding of a negative integration cost was previously observed by Wu et al. (2010) as well as Demberg and Keller (2008), although Demberg and Keller calculated it using the original referent-based definitions of Gibson (1998; 2000) and varied which parts of speech counted for calculating integration cost. Ultimately, Demberg and Keller (2008) concluded that the negative coefficient was evidence that integration cost was not a good broad-coverage predictor of reading times; however, this study has replicated the effect and showed it to be a very strong predictor of reading times, albeit one that is correlated with facilitation rather than inhibition. It is interesting that many studies have found negative integration cost using naturalistic stimuli while others have consistently found positive integration cost whe</context>
</contexts>
<marker>Gibson, 1998</marker>
<rawString>Edward Gibson. 1998. Linguistic complexity: Locality of syntactic dependencies. Cognition, 68(1):1–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>The dependency locality theory: A distance-based theory of linguistic complexity. In Image, language, brain: Papers from the first mind articulation project symposium,</title>
<date>2000</date>
<pages>95--126</pages>
<contexts>
<context position="1657" citStr="Gibson, 2000" startWordPosition="248" endWordPosition="250">1; Demberg and Keller, 2008). These effects are important because they illuminate the ontogeny of language (how individual speakers have acquired language), but they do not answer questions about the phylogeny of language (how the language came to its current form). Phillips (2010) has hypothesized that grammar rule probabilities may be grounded in memory limitations. Increased delays in processing centerembedded sentences as the number of embeddings increases, for example, are often explained in terms of a complexity cost associated with maintaining incomplete dependencies in working memory (Gibson, 2000; Lewis and Vasishth, 2005). Other studies have shown a link between processing delays 95 and the low frequency of center-embedded constructions like object relatives (Hale, 2001), but they have not explored the source of this low frequency. A grounding hypothesis would claim that the low probability of generating such a structure may arise from an associated memory load. In this account, while these complexity costs may involve languagespecific concepts such as referent or argument linking, the underlying explanation would be one of memory limitations (Gibson, 2000) or neural activation (Lewi</context>
<context position="4749" citStr="Gibson, 2000" startWordPosition="743" endWordPosition="744">entropy over a random variable corresponds to greater uncertainty over the observed value it will take. The entropy of a syntactic derivation over the sequence x1 ... xt is calculated as:2 J: H(x1...t) = −P(s) · lo92 P(s) (2) s∈S(x1...xt) Reduction in entropy has been found to predict processing complexity (Hale, 2003; Hale, 2006; Roark et al., 2009; Wu et al., 2010; Hale, 2011): AH(x1...t) = max(0, H(x1...t−1)−H(x1...t)) (3) This measures the change in uncertainty about the discourse as each new word is processed. 3 Memory Measures 3.1 Dependency Locality In Dependency Locality Theory (DLT) (Gibson, 2000), complexity arises from intervening referents introduced between a predicate and its argument. Under the original formulation of DLT, there is a 1The parser in this study uses a beam. However, given high parser accuracy, Roark (2001) showed that calculating complexity metrics over a beam should obtain similar results to the full complexity calculation. 2The incremental formulation used here was first proposed in Wu et al. (2010). storage cost for each new referent introduced and an integration cost for each referent intervening in a dependency projection. This is a simplification made for eas</context>
<context position="22250" citStr="Gibson, 2000" startWordPosition="3600" endWordPosition="3601">ing times to decrease when completing a center embedding in ACT-R (Vasishth and Lewis, 2006), this would be expressed as a frequency effect due to certain argument types commonly foreshadowing their predicates (Jaeger et al., 2008). Since frequency effects are factored separately from memory effects in this study, ACT-R would predict longer residual (memory-based) reading times when completing an embedding. Predicted correlations to reading times for the F and L transitions are summarized in Table 2. 5 Eye-tracking Eye-tracking and reading time data are often used to test complexity measures (Gibson, 2000; Demberg and Keller, 2008; Roark et al., 2009) under the assumption that readers slow down when reading more complex passages. Readers saccade over portions of text and regress back to preceding text in complex patterns, but studies have correlated certain measures with certain processing constraints (see Clifton et al. 2007 for a review). For example, the initial length of time fixated on a single word is correlated with word identification time; whereas regression durations after a word is fixated (but prior to a fixation in a new region) are hypothesized to correlate 100 with integration d</context>
<context position="33148" citStr="Gibson, 2000" startWordPosition="5355" endWordPosition="5356">mately, Demberg and Keller (2008) concluded that the negative coefficient was evidence that integration cost was not a good broad-coverage predictor of reading times; however, this study has replicated the effect and showed it to be a very strong predictor of reading times, albeit one that is correlated with facilitation rather than inhibition. It is interesting that many studies have found negative integration cost using naturalistic stimuli while others have consistently found positive integration cost when using constructed stimuli with multiple center embeddings presented without context (Gibson, 2000; Chen et al., 2005; Kwon et al., 2010). It may be the case that any dynamic recruitment is overwhelmed by the memory demands of multiply center-embedded stimuli. Alternatively, it may be that the difficulty of processing multiply center-embedded sentences containing ambiguities produces anxiety in subjects, which slows processing at implicit prosodic boundaries (Fodor, 2002; Mitchell et al., 2008). In any case, the source of this discrepancy presents an attractive target for future research. In general, sequential prediction does not seem to present people with any special ease or difficulty </context>
<context position="34762" citStr="Gibson, 2000" startWordPosition="5610" endWordPosition="5611">s kind of cueing would show up as a frequency effect predicted by surprisal rather than as a memory-based cost, due to frequent occurrences becoming ingrained as a learned skill. Future work could use these sequential cueing operations to investigate further claims of the dynamic recruitment hypothesis. One of the implications of the hypothesis is that recruitment of resources alleviates the initial encoding cost, which allows the parser to continue on as before the embedding. DLT, on the other hand, predicts that there is a storage cost for maintaining unresolved dependencies during a parse (Gibson, 2000). By weighting each of the sequential cueing operations with the embedding depth at which it occurs, an experiment may be able to test these two predictions. This study has shown that measures based on working memory operations have strong predictivity over other previously proposed measures including those associated with frequency effects. This suggests that memory limitations may provide a partial explanation of what gives rise to frequency effects. Lastly, this paper provides evidence that there is a robust facilitation effect in English that arises from completing center embeddings. The h</context>
</contexts>
<marker>Gibson, 2000</marker>
<rawString>Edward Gibson. 2000. The dependency locality theory: A distance-based theory of linguistic complexity. In Image, language, brain: Papers from the first mind articulation project symposium, pages 95–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>A probabilistic earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the second meeting of the North American chapter of the Association for Computational Linguistics,</booktitle>
<pages>159--166</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1046" citStr="Hale, 2001" startWordPosition="157" endWordPosition="158">illips, 2010) would suggest that cognitive limitations might cause languages to develop frequent constructions in such a way as to avoid processing costs. This paper studies how current theories of working memory fit into theories of language processing and what influence memory limitations may have over reading times. Measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing. 1 Introduction Frequency effects in language have been isolated and observed in many studies (Trueswell, 1996; Jurafsky, 1996; Hale, 2001; Demberg and Keller, 2008). These effects are important because they illuminate the ontogeny of language (how individual speakers have acquired language), but they do not answer questions about the phylogeny of language (how the language came to its current form). Phillips (2010) has hypothesized that grammar rule probabilities may be grounded in memory limitations. Increased delays in processing centerembedded sentences as the number of embeddings increases, for example, are often explained in terms of a complexity cost associated with maintaining incomplete dependencies in working memory (G</context>
<context position="3177" citStr="Hale, 2001" startWordPosition="490" endWordPosition="491"> load measures are a significant factor even when frequency measures are residualized out. The remainder of this paper is organized as follows: Sections 2 and 3 describe several frequency and memory measures. Section 4 describes a probabilistic hierarchic sequence model that allows all of these measures to be directly computed. Section 5 describes how these measures were used to predict reading time durations on the Dundee eye-tracking corpus. Sections 6 and 7 present results and discuss. 2 Frequency Measures 2.1 Surprisal One of the strongest predictors of processing complexity is surprisal (Hale, 2001). It has been shown in numerous studies to have a strong correlation with reading time durations in eye-tracking and selfpaced reading studies when calculated with a variety Proceedings of NAACL-HLT 2013, pages 95–105, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics of models (Levy, 2008; Roark et al., 2009; Wu et al., 2010). Surprisal predicts the integration difficulty that a word xt at time step t presents given the preceding context and is calculated as follows: E� s∈S(x1...xt) P(s) E s∈S(x1...xt−1) P(s) (1) where 5(x1 ... xt) is the set of syntactic tree</context>
<context position="27583" citStr="Hale, 2001" startWordPosition="4462" endWordPosition="4463">r (CWDELTA) simply counts the number of words in each go-past region. Cumula7In particular, these models assume the noise in the data is normally distributed. Initial exploratory trials showed that the residuals of fitting any sensible baseline also become more normally distributed if the response variable is log-transformed. Finally, the directions of the effects remain the same whether or not the reading times are log-transformed, though significance cannot be ascertained without the transform. 101 tive total surprisal (CUMUSURP) and cumulative entropy reduction (ENTRED) give the surprisal (Hale, 2001) and entropy reduction (Hale, 2003) summed over the go-past region. To avoid convergence issues, each of the cumulative measures is residualized from the next simpler model in the following order: CWDELTA from the standard baseline, CUMUSURP from the baseline with CWDELTA, and ENTRED from the baseline with all other effects. Residualization was accomplished by using the simpler mixed-effects model to fit the measure of interest. The residuals from that model fit were then used in place of the factor of interest. All joint interactions were included in the baseline model as well. Finally, to ac</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>John Hale. 2001. A probabilistic earley parser as a psycholinguistic model. In Proceedings of the second meeting of the North American chapter of the Association for Computational Linguistics, pages 159–166, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>Grammar, Uncertainty and Sentence Processing.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Cognitive Science, The Johns Hopkins University.</institution>
<contexts>
<context position="4455" citStr="Hale, 2003" startWordPosition="698" endWordPosition="699">asures how unexpected constructions are in a given context. What it does not provide is an explanation for why certain constructions would be less common and thus more surprising. 2.2 Entropy Reduction Processing difficulty can also be measured in terms of entropy (Shannon, 1948). A larger entropy over a random variable corresponds to greater uncertainty over the observed value it will take. The entropy of a syntactic derivation over the sequence x1 ... xt is calculated as:2 J: H(x1...t) = −P(s) · lo92 P(s) (2) s∈S(x1...xt) Reduction in entropy has been found to predict processing complexity (Hale, 2003; Hale, 2006; Roark et al., 2009; Wu et al., 2010; Hale, 2011): AH(x1...t) = max(0, H(x1...t−1)−H(x1...t)) (3) This measures the change in uncertainty about the discourse as each new word is processed. 3 Memory Measures 3.1 Dependency Locality In Dependency Locality Theory (DLT) (Gibson, 2000), complexity arises from intervening referents introduced between a predicate and its argument. Under the original formulation of DLT, there is a 1The parser in this study uses a beam. However, given high parser accuracy, Roark (2001) showed that calculating complexity metrics over a beam should obtain si</context>
<context position="27618" citStr="Hale, 2003" startWordPosition="4467" endWordPosition="4468">r of words in each go-past region. Cumula7In particular, these models assume the noise in the data is normally distributed. Initial exploratory trials showed that the residuals of fitting any sensible baseline also become more normally distributed if the response variable is log-transformed. Finally, the directions of the effects remain the same whether or not the reading times are log-transformed, though significance cannot be ascertained without the transform. 101 tive total surprisal (CUMUSURP) and cumulative entropy reduction (ENTRED) give the surprisal (Hale, 2001) and entropy reduction (Hale, 2003) summed over the go-past region. To avoid convergence issues, each of the cumulative measures is residualized from the next simpler model in the following order: CWDELTA from the standard baseline, CUMUSURP from the baseline with CWDELTA, and ENTRED from the baseline with all other effects. Residualization was accomplished by using the simpler mixed-effects model to fit the measure of interest. The residuals from that model fit were then used in place of the factor of interest. All joint interactions were included in the baseline model as well. Finally, to account for spillover effects (Just e</context>
</contexts>
<marker>Hale, 2003</marker>
<rawString>John Hale. 2003. Grammar, Uncertainty and Sentence Processing. Ph.D. thesis, Cognitive Science, The Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>Uncertainty about the rest of the sentence.</title>
<date>2006</date>
<journal>Cognitive Science,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="4467" citStr="Hale, 2006" startWordPosition="700" endWordPosition="701">nexpected constructions are in a given context. What it does not provide is an explanation for why certain constructions would be less common and thus more surprising. 2.2 Entropy Reduction Processing difficulty can also be measured in terms of entropy (Shannon, 1948). A larger entropy over a random variable corresponds to greater uncertainty over the observed value it will take. The entropy of a syntactic derivation over the sequence x1 ... xt is calculated as:2 J: H(x1...t) = −P(s) · lo92 P(s) (2) s∈S(x1...xt) Reduction in entropy has been found to predict processing complexity (Hale, 2003; Hale, 2006; Roark et al., 2009; Wu et al., 2010; Hale, 2011): AH(x1...t) = max(0, H(x1...t−1)−H(x1...t)) (3) This measures the change in uncertainty about the discourse as each new word is processed. 3 Memory Measures 3.1 Dependency Locality In Dependency Locality Theory (DLT) (Gibson, 2000), complexity arises from intervening referents introduced between a predicate and its argument. Under the original formulation of DLT, there is a 1The parser in this study uses a beam. However, given high parser accuracy, Roark (2001) showed that calculating complexity metrics over a beam should obtain similar result</context>
</contexts>
<marker>Hale, 2006</marker>
<rawString>John Hale. 2006. Uncertainty about the rest of the sentence. Cognitive Science, 30(4):609–642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>What a rational parser would do.</title>
<date>2011</date>
<journal>Cognitive Science,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="4517" citStr="Hale, 2011" startWordPosition="710" endWordPosition="711">hat it does not provide is an explanation for why certain constructions would be less common and thus more surprising. 2.2 Entropy Reduction Processing difficulty can also be measured in terms of entropy (Shannon, 1948). A larger entropy over a random variable corresponds to greater uncertainty over the observed value it will take. The entropy of a syntactic derivation over the sequence x1 ... xt is calculated as:2 J: H(x1...t) = −P(s) · lo92 P(s) (2) s∈S(x1...xt) Reduction in entropy has been found to predict processing complexity (Hale, 2003; Hale, 2006; Roark et al., 2009; Wu et al., 2010; Hale, 2011): AH(x1...t) = max(0, H(x1...t−1)−H(x1...t)) (3) This measures the change in uncertainty about the discourse as each new word is processed. 3 Memory Measures 3.1 Dependency Locality In Dependency Locality Theory (DLT) (Gibson, 2000), complexity arises from intervening referents introduced between a predicate and its argument. Under the original formulation of DLT, there is a 1The parser in this study uses a beam. However, given high parser accuracy, Roark (2001) showed that calculating complexity metrics over a beam should obtain similar results to the full complexity calculation. 2The increme</context>
</contexts>
<marker>Hale, 2011</marker>
<rawString>John Hale. 2011. What a rational parser would do. Cognitive Science, 35(3):399–443.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc W Howard</author>
<author>Michael J Kahana</author>
</authors>
<title>A distributed representation of temporal context.</title>
<date>2002</date>
<journal>Journal of Mathematical Psychology,</journal>
<pages>45--269</pages>
<contexts>
<context position="8330" citStr="Howard and Kahana, 2002" startWordPosition="1298" endWordPosition="1301"> encoded (similarity-based encoding interference) (Lewis et al., 2006). 3.3 Hierarchic Sequential Prediction Current models of working memory in structured tasks are defined in terms of hierarchies of sequential processes, in which superordinate sequences can be interrupted by subordinate sequences and resume when the subordinate sequences have concluded (Botvinick, 2007). These models rely on temporal cueing as well as content-based cueing to explain how an interrupted sequence may be recalled for continuation. Temporal cueing is based on a context of temporal features for the current state (Howard and Kahana, 2002). The temporal context in which the subordinate sequence concludes must be similar enough to the temporal context in which it was initiated to recall where in the superordinate sequence the subordinate sequence occurred. For example, the act of making breakfast may be interrupted by a phone call. Once the call is complete, the temporal context is sufficiently similar to when the call began that one is able to continue preparing breakfast. The association between the current temporal context and the temporal context prior to the interruption is strong enough to cue the next action. Temporal cue</context>
</contexts>
<marker>Howard, Kahana, 2002</marker>
<rawString>Marc W. Howard and Michael J. Kahana. 2002. A distributed representation of temporal context. Journal of Mathematical Psychology, 45:269–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F T Jaeger</author>
<author>E Fedorenko</author>
<author>P Hofmeister</author>
<author>E Gibson</author>
</authors>
<title>Expectation-based syntactic processing: Antilocality outside of head-final languages.</title>
<date>2008</date>
<booktitle>In The 21st CUNY Sentence Processing Conference.</booktitle>
<contexts>
<context position="21869" citStr="Jaeger et al., 2008" startWordPosition="3540" endWordPosition="3543">+L+. component is recalled. In contrast to DLT, embedding difference and dynamic recruitment would predict a shorter reading time in the F–L+ case because of the reduction in memory load. In an ACT-R framework, reading time durations can increase at the retrieval site because the retrieval causes competition among similarly encoded signs in the context set. While it is possible for reading times to decrease when completing a center embedding in ACT-R (Vasishth and Lewis, 2006), this would be expressed as a frequency effect due to certain argument types commonly foreshadowing their predicates (Jaeger et al., 2008). Since frequency effects are factored separately from memory effects in this study, ACT-R would predict longer residual (memory-based) reading times when completing an embedding. Predicted correlations to reading times for the F and L transitions are summarized in Table 2. 5 Eye-tracking Eye-tracking and reading time data are often used to test complexity measures (Gibson, 2000; Demberg and Keller, 2008; Roark et al., 2009) under the assumption that readers slow down when reading more complex passages. Readers saccade over portions of text and regress back to preceding text in complex pattern</context>
</contexts>
<marker>Jaeger, Fedorenko, Hofmeister, Gibson, 2008</marker>
<rawString>F. T. Jaeger, E. Fedorenko, P. Hofmeister, and E. Gibson. 2008. Expectation-based syntactic processing: Antilocality outside of head-final languages. In The 21st CUNY Sentence Processing Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
</authors>
<title>A probabilistic model of lexical and syntactic access and disambiguation.</title>
<date>1996</date>
<journal>Cognitive Science: A Multidisciplinary Journal,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1034" citStr="Jurafsky, 1996" startWordPosition="155" endWordPosition="156">of grounding (Phillips, 2010) would suggest that cognitive limitations might cause languages to develop frequent constructions in such a way as to avoid processing costs. This paper studies how current theories of working memory fit into theories of language processing and what influence memory limitations may have over reading times. Measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing. 1 Introduction Frequency effects in language have been isolated and observed in many studies (Trueswell, 1996; Jurafsky, 1996; Hale, 2001; Demberg and Keller, 2008). These effects are important because they illuminate the ontogeny of language (how individual speakers have acquired language), but they do not answer questions about the phylogeny of language (how the language came to its current form). Phillips (2010) has hypothesized that grammar rule probabilities may be grounded in memory limitations. Increased delays in processing centerembedded sentences as the number of embeddings increases, for example, are often explained in terms of a complexity cost associated with maintaining incomplete dependencies in worki</context>
</contexts>
<marker>Jurafsky, 1996</marker>
<rawString>Daniel Jurafsky. 1996. A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science: A Multidisciplinary Journal, 20(2):137–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcel Adam Just</author>
<author>Sashank Varma</author>
</authors>
<title>The organization of thinking: What functional brain imaging reveals about the neuroarchitecture of complex cognition.</title>
<date>2007</date>
<journal>Cognitive, Affective, &amp; Behavioral Neuroscience,</journal>
<pages>7--153</pages>
<contexts>
<context position="10801" citStr="Just and Varma (2007)" startWordPosition="1686" endWordPosition="1689">g. McElree (2001; 2006) has found that retrieval of any non-focused (or in this case, unconnected) element from memory leads to slower processing. Therefore, integrating two disjoint connected components should be expected to incur a processing cost due to the need to recall the current state of the superordinate sequence to continue the parse. Such a cost would corroborate a DLT-like theory where integration slows processing. 3.4 Dynamic Recruitment of Additional Processing Resources Language processing is typically centered in the left hemisphere of the brain (for right-handed individuals). Just and Varma (2007) provide fMRI results suggesting readers dynamically recruit additional processing resources such as the right-side homologues of the language processing areas of the brain when processing center-embedded constructions. Once an embedded construction terminates, the reader may still have temporary access to these extra processing resources, which may briefly speed processing. This hypothesis would, therefore, predict an encoding cost when a center embedding is initiated. The resulting inhibition would trigger recruitment of additional processing resources, which would then N D the S/NP } NP N s</context>
<context position="31563" citStr="Just and Varma, 2007" startWordPosition="5111" endWordPosition="5114"> the baseline is encouraging as it suggests that memory limitations could provide at least a partial explanation of why certain constructions are less frequent in corpora and thus yield a high surprisal. Moreover, it indicates that the model corroborates the shared prediction of most of the memory-based models that initiating a new connected component slows processing. The fact that F–L+ is predictive but has a negative coefficient could be evidence of anti-locality, or it could be an indication of some sort of processing momentum due to dynamic recruitment of additional processing resources (Just and Varma, 2007). Since anti-locality is an expectation-based frequency effect, and since this study controlled for frequency effects with n-grams, surprisal, and entropy reduction, an anti-locality explanation would rely on either (i) more precise variants of the metrics used in this study or (ii) other frequency metrics altogether. Future work could investigate the possibility of anti-locality by looking at the distance between an encoding operation and its corresponding 102 integration action to see if the integration facilitation observed in this study is driven by longer embeddings or if there is simply </context>
</contexts>
<marker>Just, Varma, 2007</marker>
<rawString>Marcel Adam Just and Sashank Varma. 2007. The organization of thinking: What functional brain imaging reveals about the neuroarchitecture of complex cognition. Cognitive, Affective, &amp; Behavioral Neuroscience, 7:153–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcel Adam Just</author>
<author>Patricia A Carpenter</author>
<author>Jacqueline D Woolley</author>
</authors>
<title>Paradigms and processes in reading comprehension.</title>
<date>1982</date>
<journal>Journal of Experimental Psychology: General,</journal>
<pages>111--228</pages>
<contexts>
<context position="28230" citStr="Just et al., 1982" startWordPosition="4568" endWordPosition="4571"> 2003) summed over the go-past region. To avoid convergence issues, each of the cumulative measures is residualized from the next simpler model in the following order: CWDELTA from the standard baseline, CUMUSURP from the baseline with CWDELTA, and ENTRED from the baseline with all other effects. Residualization was accomplished by using the simpler mixed-effects model to fit the measure of interest. The residuals from that model fit were then used in place of the factor of interest. All joint interactions were included in the baseline model as well. Finally, to account for spillover effects (Just et al., 1982) where processing from a previous region contributes to the following duration, the above baseline predictors from the previous go-past region were included as factors for the current region. Having SURP as a predictor with CUMUSURP may seem redundant, but initial analyses showed SURP was a significant predictor over CUMUSURP when CWDELTA was a separate factor in the baseline (current: p = 2.2 · 10−16 spillover: p = 2 · 10−15) and vice versa (current: p = 2.2 · 10−16 spillover: p = 6 · 10−5). One reason for this could be that go-past durations conflate complexity experienced when initially fix</context>
</contexts>
<marker>Just, Carpenter, Woolley, 1982</marker>
<rawString>Marcel Adam Just, Patricia A. Carpenter, and Jacqueline D. Woolley. 1982. Paradigms and processes in reading comprehension. Journal of Experimental Psychology: General, 111:228–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Kennedy</author>
<author>James Pynte</author>
<author>Robin Hill</author>
</authors>
<title>The Dundee corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th European conference on eye movement.</booktitle>
<contexts>
<context position="2427" citStr="Kennedy et al., 2003" startWordPosition="369" endWordPosition="372"> object relatives (Hale, 2001), but they have not explored the source of this low frequency. A grounding hypothesis would claim that the low probability of generating such a structure may arise from an associated memory load. In this account, while these complexity costs may involve languagespecific concepts such as referent or argument linking, the underlying explanation would be one of memory limitations (Gibson, 2000) or neural activation (Lewis and Vasishth, 2005). This paper seeks to explore the different predictions made by these theories on a broad-coverage corpus of eye-tracking data (Kennedy et al., 2003). In addition, the current experiment seeks to isolate memory effects from frequency effects in the same task. The results show that memory load measures are a significant factor even when frequency measures are residualized out. The remainder of this paper is organized as follows: Sections 2 and 3 describe several frequency and memory measures. Section 4 describes a probabilistic hierarchic sequence model that allows all of these measures to be directly computed. Section 5 describes how these measures were used to predict reading time durations on the Dundee eye-tracking corpus. Sections 6 an</context>
<context position="23678" citStr="Kennedy et al., 2003" startWordPosition="3839" endWordPosition="3842">durations. Go-past durations are calculated by summing all fixations in a region of text, including regressions, until a new region is fixated, which accounts for additional processing that may take place after initial lexical access, but before the next region is processed. For example, if one region ends at word 5 in a sentence, and the next fixation lands on word 8, then the go-past region consists of words 6-8 and the go-past duration sums all fixations until a fixation occurs after word 8. 6 Evaluation The measures presented in this paper were evaluated on the Dundee eye-tracking corpus (Kennedy et al., 2003). The corpus consists of 2388 sentences of naturally occurring news text written in standard British English. The corpus also includes eye-tracking data from 10 native English speakers, which provides a test corpus of 260,124 subject-duration pairs of reading time data. Of this, any fixated words appearing fewer than 5 times in the training data were considered unknown and were filtered out to obtain accurate predictions. Fixations on the first or last words of a line were also filtered out to avoid any ‘wrap-up’ effects resulting from preparing to saccade to the beginning of the next line or </context>
<context position="26253" citStr="Kennedy et al., 2003" startWordPosition="4254" endWordPosition="4257"> unigram (LOGPROB) and bigram probabilities are included. The bigram probabilities are those of the current word given the previous word (LOGFWPROB) and the current word given the following word (LOGBWPROB). Fossum and Levy (2012) showed that for n-gram probabilities to be effective predictors on the Dundee corpus, they must be calculated from a wide variety of texts, so following them, this study used the Brown corpus (Francis and Kucera, 1979), the WSJ Sections 02-21 (Marcus et al., 1993), the written text portion of the British National Corpus (BNC Consortium, 2007), and the Dundee corpus (Kennedy et al., 2003). This amounted to an n-gram training corpus of roughly 87 million words. These statistics were smoothed using the SRILM (Stolcke, 2002) implementation of modified Kneser-Ney smoothing (Chen and Goodman, 1998). Finally, total surprisal (SURP) was included to account for frequency effects in the baseline. The preceding measures are commonly used in baseline models to fit reading time data (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012) and were calculated from the final word of each go-past region. The following measures create a more sophisticated baseline by accumulatin</context>
</contexts>
<marker>Kennedy, Pynte, Hill, 2003</marker>
<rawString>Alan Kennedy, James Pynte, and Robin Hill. 2003. The Dundee corpus. In Proceedings of the 12th European conference on eye movement.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nayoung Kwon</author>
<author>Yoonhyoung Lee</author>
<author>Peter C Gordon</author>
<author>Robert Kluender</author>
<author>Maria Polinsky</author>
</authors>
<title>Cognitive and linguistic factors affecting subject/object asymmetry: An eye-tracking study of pre-nominal relative clauses in korean.</title>
<date>2010</date>
<journal>Language,</journal>
<volume>86</volume>
<issue>3</issue>
<contexts>
<context position="5556" citStr="Kwon et al., 2010" startWordPosition="866" endWordPosition="869">ver, given high parser accuracy, Roark (2001) showed that calculating complexity metrics over a beam should obtain similar results to the full complexity calculation. 2The incremental formulation used here was first proposed in Wu et al. (2010). storage cost for each new referent introduced and an integration cost for each referent intervening in a dependency projection. This is a simplification made for ease of computation, and subsequent work has found DLT to be more accurate cross-linguistically if the intervening elements are structurally defined rather than defined in terms of referents (Kwon et al., 2010). That is, simply having a particular referent intervene in a dependency projection may not have as great an effect on processing complexity as the syntactic construction the referent appears in. Therefore, this work reinterprets the costs of dependency locality to be related to the events of beginning a center embedding (storage) and completing a center embedding (integration). Note that antilocality effects (where longer dependencies are easier to process) have also been observed in some languages, and DLT is unable to account for these phenomena (Vasishth and Lewis, 2006). 3.2 ACT-R Process</context>
<context position="33187" citStr="Kwon et al., 2010" startWordPosition="5361" endWordPosition="5364"> concluded that the negative coefficient was evidence that integration cost was not a good broad-coverage predictor of reading times; however, this study has replicated the effect and showed it to be a very strong predictor of reading times, albeit one that is correlated with facilitation rather than inhibition. It is interesting that many studies have found negative integration cost using naturalistic stimuli while others have consistently found positive integration cost when using constructed stimuli with multiple center embeddings presented without context (Gibson, 2000; Chen et al., 2005; Kwon et al., 2010). It may be the case that any dynamic recruitment is overwhelmed by the memory demands of multiply center-embedded stimuli. Alternatively, it may be that the difficulty of processing multiply center-embedded sentences containing ambiguities produces anxiety in subjects, which slows processing at implicit prosodic boundaries (Fodor, 2002; Mitchell et al., 2008). In any case, the source of this discrepancy presents an attractive target for future research. In general, sequential prediction does not seem to present people with any special ease or difficulty as evidenced by the lack of significanc</context>
</contexts>
<marker>Kwon, Lee, Gordon, Kluender, Polinsky, 2010</marker>
<rawString>Nayoung Kwon, Yoonhyoung Lee, Peter C. Gordon, Robert Kluender, and Maria Polinsky. 2010. Cognitive and linguistic factors affecting subject/object asymmetry: An eye-tracking study of pre-nominal relative clauses in korean. Language, 86(3):561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>106</volume>
<issue>3</issue>
<contexts>
<context position="3500" citStr="Levy, 2008" startWordPosition="538" endWordPosition="539">uted. Section 5 describes how these measures were used to predict reading time durations on the Dundee eye-tracking corpus. Sections 6 and 7 present results and discuss. 2 Frequency Measures 2.1 Surprisal One of the strongest predictors of processing complexity is surprisal (Hale, 2001). It has been shown in numerous studies to have a strong correlation with reading time durations in eye-tracking and selfpaced reading studies when calculated with a variety Proceedings of NAACL-HLT 2013, pages 95–105, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics of models (Levy, 2008; Roark et al., 2009; Wu et al., 2010). Surprisal predicts the integration difficulty that a word xt at time step t presents given the preceding context and is calculated as follows: E� s∈S(x1...xt) P(s) E s∈S(x1...xt−1) P(s) (1) where 5(x1 ... xt) is the set of syntactic trees whose leaves have x1 ... xt as a prefix.1 In essence, surprisal measures how unexpected constructions are in a given context. What it does not provide is an explanation for why certain constructions would be less common and thus more surprising. 2.2 Entropy Reduction Processing difficulty can also be measured in terms o</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Roger Levy. 2008. Expectation-based syntactic comprehension. Cognition, 106(3):1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard L Lewis</author>
<author>Shravan Vasishth</author>
</authors>
<title>An activation-based model of sentence processing as skilled memory retrieval.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="1684" citStr="Lewis and Vasishth, 2005" startWordPosition="251" endWordPosition="254"> Keller, 2008). These effects are important because they illuminate the ontogeny of language (how individual speakers have acquired language), but they do not answer questions about the phylogeny of language (how the language came to its current form). Phillips (2010) has hypothesized that grammar rule probabilities may be grounded in memory limitations. Increased delays in processing centerembedded sentences as the number of embeddings increases, for example, are often explained in terms of a complexity cost associated with maintaining incomplete dependencies in working memory (Gibson, 2000; Lewis and Vasishth, 2005). Other studies have shown a link between processing delays 95 and the low frequency of center-embedded constructions like object relatives (Hale, 2001), but they have not explored the source of this low frequency. A grounding hypothesis would claim that the low probability of generating such a structure may arise from an associated memory load. In this account, while these complexity costs may involve languagespecific concepts such as referent or argument linking, the underlying explanation would be one of memory limitations (Gibson, 2000) or neural activation (Lewis and Vasishth, 2005). This</context>
<context position="6239" citStr="Lewis and Vasishth, 2005" startWordPosition="975" endWordPosition="978">n a dependency projection may not have as great an effect on processing complexity as the syntactic construction the referent appears in. Therefore, this work reinterprets the costs of dependency locality to be related to the events of beginning a center embedding (storage) and completing a center embedding (integration). Note that antilocality effects (where longer dependencies are easier to process) have also been observed in some languages, and DLT is unable to account for these phenomena (Vasishth and Lewis, 2006). 3.2 ACT-R Processing complexity has also been attributed to confusability (Lewis and Vasishth, 2005) as defined in domain-general cognitive models like ACT-R (Anderson et al., 2004). ACT-R is based on theories of neural activation. Each new word is encoded and stored in working memory until it is retrieved at a later point for modification before being re-encoded into the parse. A newly observed sign (word) associatively activates any appropriate arguments from working memory, so multiple similarly appropriate arguments would slow processing as the parser must choose between the highly activated hypotheses. Any intervening signs (words or phrases) that modify a previously encoded sign re-act</context>
</contexts>
<marker>Lewis, Vasishth, 2005</marker>
<rawString>Richard L. Lewis and Shravan Vasishth. 2005. An activation-based model of sentence processing as skilled memory retrieval. Cognitive Science, 29(3):375–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard L Lewis</author>
<author>Shravan Vasishth</author>
<author>Jane A Van Dyke</author>
</authors>
<title>Computational principles of working memory in sentence comprehension.</title>
<date>2006</date>
<journal>Trends in Cognitive Science,</journal>
<volume>10</volume>
<issue>10</issue>
<marker>Lewis, Vasishth, Van Dyke, 2006</marker>
<rawString>Richard L. Lewis, Shravan Vasishth, and Jane A. Van Dyke. 2006. Computational principles of working memory in sentence comprehension. Trends in Cognitive Science, 10(10):447–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="26127" citStr="Marcus et al., 1993" startWordPosition="4234" endWordPosition="4237"> if the immediately previous word was fixated (PREVISFIX) the current word probably need not be fixated for as long. Finally, unigram (LOGPROB) and bigram probabilities are included. The bigram probabilities are those of the current word given the previous word (LOGFWPROB) and the current word given the following word (LOGBWPROB). Fossum and Levy (2012) showed that for n-gram probabilities to be effective predictors on the Dundee corpus, they must be calculated from a wide variety of texts, so following them, this study used the Brown corpus (Francis and Kucera, 1979), the WSJ Sections 02-21 (Marcus et al., 1993), the written text portion of the British National Corpus (BNC Consortium, 2007), and the Dundee corpus (Kennedy et al., 2003). This amounted to an n-gram training corpus of roughly 87 million words. These statistics were smoothed using the SRILM (Stolcke, 2002) implementation of modified Kneser-Ney smoothing (Chen and Goodman, 1998). Finally, total surprisal (SURP) was included to account for frequency effects in the baseline. The preceding measures are commonly used in baseline models to fit reading time data (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012) and were cal</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian McElree</author>
</authors>
<title>Working memory and focal attention.</title>
<date>2001</date>
<journal>Journal of Experimental Psychology, Learning Memory and Cognition,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="10196" citStr="McElree (2001" startWordPosition="1597" endWordPosition="1598">dinate sequence (a center embedding) introduces a new connected component, disjoint from that of the superordinate sequence. As the subordinate sequence proceeds, the new component gains associated discourse referents, each sequentially cued from the last, until finally it merges with the superordinate connected component at the end of the embedded clause, forming a single connected component representing the parse up to that point. Since it is not connected to the subordinate connected component prior to merging, the superordinate connected component must be recalled through temporal cueing. McElree (2001; 2006) has found that retrieval of any non-focused (or in this case, unconnected) element from memory leads to slower processing. Therefore, integrating two disjoint connected components should be expected to incur a processing cost due to the need to recall the current state of the superordinate sequence to continue the parse. Such a cost would corroborate a DLT-like theory where integration slows processing. 3.4 Dynamic Recruitment of Additional Processing Resources Language processing is typically centered in the left hemisphere of the brain (for right-handed individuals). Just and Varma (</context>
<context position="11612" citStr="McElree, 2001" startWordPosition="1817" endWordPosition="1818">mbedded constructions. Once an embedded construction terminates, the reader may still have temporary access to these extra processing resources, which may briefly speed processing. This hypothesis would, therefore, predict an encoding cost when a center embedding is initiated. The resulting inhibition would trigger recruitment of additional processing resources, which would then N D the S/NP } NP N studio VP NP NP � – NP/N D the V bought S 97 allow the rest of the embedded structure to be processed at the usual speed. Upon completing an embedding, the difficulty arising from memory retrieval (McElree, 2001) would be ameliorated by these extra processing resources, and the reduced processing complexity arising from reduced memory load would yield a temporary facilitation in processing. No longer requiring the additional resources to cope with the increased embedding, the processor would release them, returning the processor to its usual speed. Unlike anti-locality, where processing is facilitated in longer passages due to accumulating probabilistic evidence, a model of dynamic recruitment of additional processing resources would predict universal facilitation after a center embedding of any lengt</context>
</contexts>
<marker>McElree, 2001</marker>
<rawString>Brian McElree. 2001. Working memory and focal attention. Journal of Experimental Psychology, Learning Memory and Cognition, 27(3):817–835.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian McElree</author>
</authors>
<title>Accessing recent events.</title>
<date>2006</date>
<booktitle>The Psychology of Learning and Motivation,</booktitle>
<pages>46--155</pages>
<marker>McElree, 2006</marker>
<rawString>Brian McElree. 2006. Accessing recent events. The Psychology of Learning and Motivation, 46:155–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mitchell</author>
<author>X Shen</author>
<author>M Green</author>
<author>T Hodgson</author>
</authors>
<title>Accounting for regressive eye-movements in models of sentence processing: A reappraisal of the selective reanalysis hypothesis.</title>
<date>2008</date>
<journal>Journal of Memory and Language,</journal>
<pages>59--266</pages>
<contexts>
<context position="33549" citStr="Mitchell et al., 2008" startWordPosition="5413" endWordPosition="5416">nd negative integration cost using naturalistic stimuli while others have consistently found positive integration cost when using constructed stimuli with multiple center embeddings presented without context (Gibson, 2000; Chen et al., 2005; Kwon et al., 2010). It may be the case that any dynamic recruitment is overwhelmed by the memory demands of multiply center-embedded stimuli. Alternatively, it may be that the difficulty of processing multiply center-embedded sentences containing ambiguities produces anxiety in subjects, which slows processing at implicit prosodic boundaries (Fodor, 2002; Mitchell et al., 2008). In any case, the source of this discrepancy presents an attractive target for future research. In general, sequential prediction does not seem to present people with any special ease or difficulty as evidenced by the lack of significance of F–L– and F+L+ predictions when frequency effects are factored out. This supports a theory of sequential, content-based cueing (Botvinick, 2007) that predicts that certain states would directly cue other states and thus avoid recall difficulty. An example of this may be seen in the case of a transitive verb triggering the prediction of a direct object. Thi</context>
</contexts>
<marker>Mitchell, Shen, Green, Hodgson, 2008</marker>
<rawString>D. Mitchell, X. Shen, M. Green, and T. Hodgson. 2008. Accounting for regressive eye-movements in models of sentence processing: A reappraisal of the selective reanalysis hypothesis. Journal of Memory and Language, 59:266–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="18351" citStr="Petrov and Klein (2007)" startWordPosition="2972" endWordPosition="2975">n Figure 2. Since two binary structural decisions (F and L) must be made in order to generate each word, there are four possible structures that may be generated (see Table 1). The F+L– transition initiates a new level of embedding at word xt and so requires the superordinate state to be encoded for later retrieval (e.g. on observing the in Figure 2). The F–L+ transition completes the deepest level of embedding and therefore requires the recall of the current superordinate connected component state with which the 5The model has been shown to achieve an F-score of 87.8, within .2 points of the Petrov and Klein (2007) parser, which obtains an F-score of 88.0 on the same task. Because the sequence model is defined over binary-branching phrase structure, both parsers were evaluated on binary-branching phrase structure trees to provide a fair comparison. 99 F–L– Cue Active Sign F+L– Initiate/Encode F–L+ Terminate/Integrate F+L+ Cue Awaited Sign Table 1: The hierarchical structure decisions and the operations they represent. F+L– initiates a new connected component, F–L+ integrates two disjoint connected components into a single connected component, and F–L– and F+L+ sequentially cue, respectively, a new activ</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of NAACL HLT 2007, pages 404–411, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL’06).</booktitle>
<contexts>
<context position="17665" citStr="Petrov et al. (2006)" startWordPosition="2850" endWordPosition="2853">f a transition at time step t is defined in terms of (i) a probability 0 of initiating a new connected component state with xt as its first observation, multiplied by (ii) the probability A of terminating a connected component state with xt as its last observation, multiplied by (iii) the probabilities a and Q of generating categories for active and awaited signs aqt and bqt in the resulting most subordinate connected component state qt . This kind of model can be defined directly on PCFG probabilities and trained to produce state-of-the-art accuracy by using the latent variable annotation of Petrov et al. (2006) (van Schijndel et al., in press).5 An example parse is shown in Figure 2. Since two binary structural decisions (F and L) must be made in order to generate each word, there are four possible structures that may be generated (see Table 1). The F+L– transition initiates a new level of embedding at word xt and so requires the superordinate state to be encoded for later retrieval (e.g. on observing the in Figure 2). The F–L+ transition completes the deepest level of embedding and therefore requires the recall of the current superordinate connected component state with which the 5The model has bee</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Phillips</author>
</authors>
<title>Some arguments and nonarguments for reductionist accounts of syntactic phenomena. Language and Cognitive Processes,</title>
<date>2010</date>
<pages>28--156</pages>
<contexts>
<context position="1327" citStr="Phillips (2010)" startWordPosition="200" endWordPosition="201">ory limitations may have over reading times. Measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing. 1 Introduction Frequency effects in language have been isolated and observed in many studies (Trueswell, 1996; Jurafsky, 1996; Hale, 2001; Demberg and Keller, 2008). These effects are important because they illuminate the ontogeny of language (how individual speakers have acquired language), but they do not answer questions about the phylogeny of language (how the language came to its current form). Phillips (2010) has hypothesized that grammar rule probabilities may be grounded in memory limitations. Increased delays in processing centerembedded sentences as the number of embeddings increases, for example, are often explained in terms of a complexity cost associated with maintaining incomplete dependencies in working memory (Gibson, 2000; Lewis and Vasishth, 2005). Other studies have shown a link between processing delays 95 and the low frequency of center-embedded constructions like object relatives (Hale, 2001), but they have not explored the source of this low frequency. A grounding hypothesis would</context>
</contexts>
<marker>Phillips, 2010</marker>
<rawString>Colin Phillips. 2010. Some arguments and nonarguments for reductionist accounts of syntactic phenomena. Language and Cognitive Processes, 28:156– 187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Development Core Team</author>
</authors>
<title>R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing,</title>
<date>2010</date>
<journal>ISBN</journal>
<pages>3--900051</pages>
<location>Vienna,</location>
<contexts>
<context position="24818" citStr="Team, 2010" startWordPosition="4031" endWordPosition="4032">sulting from preparing to saccade to the beginning of the next line or resulting from orienting to a new line. Additionally, following Demberg and Keller (2008), any fixations that skip more than 4 words were attributed to track loss by the eyetracker or lack of attention of the reader and so were excluded from the analysis. This left the final evaluation corpus with 151,331 subject-duration pairs. The evaluation consisted of fitting a linear mixedeffects model (Baayen et al., 2008) to reading time durations using the lmer function of the lme4 R package (Bates et al., 2011; R Development Core Team, 2010). This allowed by-subject and by-item variation to be included in the initial regression as random intercepts in addition to several baseline predictors.6 Before fitting, the durations extracted from 6Each fixed effect was centered to reduce collinearity. the corpus were log-transformed, producing more normally distributed data to obey the assumptions of linear mixed effects models.7 Included among the fixed effects were the position in the sentence that initiated the go-past region (SENTPOS) and the number of characters in the initiating word (NRCHAR). The difficulty of integrating a word may</context>
</contexts>
<marker>Team, 2010</marker>
<rawString>R Development Core Team, 2010. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Asaf Bachrach</author>
<author>Carlos Cardenas</author>
<author>Christophe Pallier</author>
</authors>
<title>Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing.</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Conference on Empirical Methods in Natural Langauge Processing,</booktitle>
<pages>324--333</pages>
<contexts>
<context position="3520" citStr="Roark et al., 2009" startWordPosition="540" endWordPosition="543">n 5 describes how these measures were used to predict reading time durations on the Dundee eye-tracking corpus. Sections 6 and 7 present results and discuss. 2 Frequency Measures 2.1 Surprisal One of the strongest predictors of processing complexity is surprisal (Hale, 2001). It has been shown in numerous studies to have a strong correlation with reading time durations in eye-tracking and selfpaced reading studies when calculated with a variety Proceedings of NAACL-HLT 2013, pages 95–105, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics of models (Levy, 2008; Roark et al., 2009; Wu et al., 2010). Surprisal predicts the integration difficulty that a word xt at time step t presents given the preceding context and is calculated as follows: E� s∈S(x1...xt) P(s) E s∈S(x1...xt−1) P(s) (1) where 5(x1 ... xt) is the set of syntactic trees whose leaves have x1 ... xt as a prefix.1 In essence, surprisal measures how unexpected constructions are in a given context. What it does not provide is an explanation for why certain constructions would be less common and thus more surprising. 2.2 Entropy Reduction Processing difficulty can also be measured in terms of entropy (Shannon, </context>
<context position="22297" citStr="Roark et al., 2009" startWordPosition="3606" endWordPosition="3609">center embedding in ACT-R (Vasishth and Lewis, 2006), this would be expressed as a frequency effect due to certain argument types commonly foreshadowing their predicates (Jaeger et al., 2008). Since frequency effects are factored separately from memory effects in this study, ACT-R would predict longer residual (memory-based) reading times when completing an embedding. Predicted correlations to reading times for the F and L transitions are summarized in Table 2. 5 Eye-tracking Eye-tracking and reading time data are often used to test complexity measures (Gibson, 2000; Demberg and Keller, 2008; Roark et al., 2009) under the assumption that readers slow down when reading more complex passages. Readers saccade over portions of text and regress back to preceding text in complex patterns, but studies have correlated certain measures with certain processing constraints (see Clifton et al. 2007 for a review). For example, the initial length of time fixated on a single word is correlated with word identification time; whereas regression durations after a word is fixated (but prior to a fixation in a new region) are hypothesized to correlate 100 with integration difficulty. Since this work focuses on increment</context>
</contexts>
<marker>Roark, Bachrach, Cardenas, Pallier, 2009</marker>
<rawString>Brian Roark, Asaf Bachrach, Carlos Cardenas, and Christophe Pallier. 2009. Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing. Proceedings of the 2009 Conference on Empirical Methods in Natural Langauge Processing, pages 324–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="4983" citStr="Roark (2001)" startWordPosition="779" endWordPosition="780">xt) Reduction in entropy has been found to predict processing complexity (Hale, 2003; Hale, 2006; Roark et al., 2009; Wu et al., 2010; Hale, 2011): AH(x1...t) = max(0, H(x1...t−1)−H(x1...t)) (3) This measures the change in uncertainty about the discourse as each new word is processed. 3 Memory Measures 3.1 Dependency Locality In Dependency Locality Theory (DLT) (Gibson, 2000), complexity arises from intervening referents introduced between a predicate and its argument. Under the original formulation of DLT, there is a 1The parser in this study uses a beam. However, given high parser accuracy, Roark (2001) showed that calculating complexity metrics over a beam should obtain similar results to the full complexity calculation. 2The incremental formulation used here was first proposed in Wu et al. (2010). storage cost for each new referent introduced and an integration cost for each referent intervening in a dependency projection. This is a simplification made for ease of computation, and subsequent work has found DLT to be more accurate cross-linguistically if the intervening elements are structurally defined rather than defined in terms of referents (Kwon et al., 2010). That is, simply having a </context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<volume>27</volume>
<pages>423--623</pages>
<contexts>
<context position="4125" citStr="Shannon, 1948" startWordPosition="644" endWordPosition="645">al., 2009; Wu et al., 2010). Surprisal predicts the integration difficulty that a word xt at time step t presents given the preceding context and is calculated as follows: E� s∈S(x1...xt) P(s) E s∈S(x1...xt−1) P(s) (1) where 5(x1 ... xt) is the set of syntactic trees whose leaves have x1 ... xt as a prefix.1 In essence, surprisal measures how unexpected constructions are in a given context. What it does not provide is an explanation for why certain constructions would be less common and thus more surprising. 2.2 Entropy Reduction Processing difficulty can also be measured in terms of entropy (Shannon, 1948). A larger entropy over a random variable corresponds to greater uncertainty over the observed value it will take. The entropy of a syntactic derivation over the sequence x1 ... xt is calculated as:2 J: H(x1...t) = −P(s) · lo92 P(s) (2) s∈S(x1...xt) Reduction in entropy has been found to predict processing complexity (Hale, 2003; Hale, 2006; Roark et al., 2009; Wu et al., 2010; Hale, 2011): AH(x1...t) = max(0, H(x1...t−1)−H(x1...t)) (3) This measures the change in uncertainty about the discourse as each new word is processed. 3 Memory Measures 3.1 Dependency Locality In Dependency Locality The</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Claude Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal, 27:379– 423, 623–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<volume>24</volume>
<pages>36</pages>
<contexts>
<context position="14266" citStr="Shieber et al., 1995" startWordPosition="2244" endWordPosition="2247">(van Schijndel et al., in press), which represents connected components of phrase structure trees in hierarchies of hidden random variables. This requires, at each time step t: • a hierarchically-organized set of N connected component states qt, each consisting of an active sign of category aqt , and an awaited sign of category bqt , separated by a slash ‘/’; and • an observed word xt. Each connected component state in this model then represents a contiguous portion of a phrase structure tree (see Figure 1 on preceding page). The operations of this parser can be defined as a deductive system (Shieber et al., 1995) with an input sequence consisting of a top-level connected component state T/T, corresponding to an existing discourse context, followed by a sequence of observed words x1, x2, ...4 If an observation xt can attach as the awaited sign of the most recent (most subordinate) connected component a/b, it is hypothesized to do so, turning this incomplete sign into a complete sign a (F–, below); or if the observation can serve as a lower descendant of this awaited sign, it is hypothesized to form the first complete sign a′ in a newly initiated connected component (F+): b → xt (F–) Then, if either of </context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart M. Shieber, Yves Schabes, and Fernando C.N. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24:3– 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Seventh International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="26389" citStr="Stolcke, 2002" startWordPosition="4277" endWordPosition="4278">PROB) and the current word given the following word (LOGBWPROB). Fossum and Levy (2012) showed that for n-gram probabilities to be effective predictors on the Dundee corpus, they must be calculated from a wide variety of texts, so following them, this study used the Brown corpus (Francis and Kucera, 1979), the WSJ Sections 02-21 (Marcus et al., 1993), the written text portion of the British National Corpus (BNC Consortium, 2007), and the Dundee corpus (Kennedy et al., 2003). This amounted to an n-gram training corpus of roughly 87 million words. These statistics were smoothed using the SRILM (Stolcke, 2002) implementation of modified Kneser-Ney smoothing (Chen and Goodman, 1998). Finally, total surprisal (SURP) was included to account for frequency effects in the baseline. The preceding measures are commonly used in baseline models to fit reading time data (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012) and were calculated from the final word of each go-past region. The following measures create a more sophisticated baseline by accumulating over the entire go-past region to capture what must be integrated into the discourse to continue the parse. One factor (CWDELTA) simpl</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm – an extensible language modeling toolkit. In Seventh International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Trueswell</author>
</authors>
<title>The role of lexical frequency in syntactic ambiguity resolution.</title>
<date>1996</date>
<journal>Journal of Memory and Language,</journal>
<pages>35--566</pages>
<contexts>
<context position="1018" citStr="Trueswell, 1996" startWordPosition="153" endWordPosition="154">equent. A theory of grounding (Phillips, 2010) would suggest that cognitive limitations might cause languages to develop frequent constructions in such a way as to avoid processing costs. This paper studies how current theories of working memory fit into theories of language processing and what influence memory limitations may have over reading times. Measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing. 1 Introduction Frequency effects in language have been isolated and observed in many studies (Trueswell, 1996; Jurafsky, 1996; Hale, 2001; Demberg and Keller, 2008). These effects are important because they illuminate the ontogeny of language (how individual speakers have acquired language), but they do not answer questions about the phylogeny of language (how the language came to its current form). Phillips (2010) has hypothesized that grammar rule probabilities may be grounded in memory limitations. Increased delays in processing centerembedded sentences as the number of embeddings increases, for example, are often explained in terms of a complexity cost associated with maintaining incomplete depen</context>
</contexts>
<marker>Trueswell, 1996</marker>
<rawString>John Trueswell. 1996. The role of lexical frequency in syntactic ambiguity resolution. Journal of Memory and Language, 35:566–585.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marten van Schijndel</author>
<author>Andy Exley</author>
<author>William Schuler</author>
</authors>
<title>in press. A model of language processing as hierarchic sequential prediction. Topics in Cognitive Science.</title>
<marker>van Schijndel, Exley, Schuler, </marker>
<rawString>Marten van Schijndel, Andy Exley, and William Schuler. in press. A model of language processing as hierarchic sequential prediction. Topics in Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shravan Vasishth</author>
<author>Richard L Lewis</author>
</authors>
<title>Argument-head distance and processing complexity: Explaining both locality and antilocality effects.</title>
<date>2006</date>
<journal>Language,</journal>
<volume>82</volume>
<issue>4</issue>
<contexts>
<context position="6137" citStr="Vasishth and Lewis, 2006" startWordPosition="961" endWordPosition="964">ed in terms of referents (Kwon et al., 2010). That is, simply having a particular referent intervene in a dependency projection may not have as great an effect on processing complexity as the syntactic construction the referent appears in. Therefore, this work reinterprets the costs of dependency locality to be related to the events of beginning a center embedding (storage) and completing a center embedding (integration). Note that antilocality effects (where longer dependencies are easier to process) have also been observed in some languages, and DLT is unable to account for these phenomena (Vasishth and Lewis, 2006). 3.2 ACT-R Processing complexity has also been attributed to confusability (Lewis and Vasishth, 2005) as defined in domain-general cognitive models like ACT-R (Anderson et al., 2004). ACT-R is based on theories of neural activation. Each new word is encoded and stored in working memory until it is retrieved at a later point for modification before being re-encoded into the parse. A newly observed sign (word) associatively activates any appropriate arguments from working memory, so multiple similarly appropriate arguments would slow processing as the parser must choose between the highly activ</context>
<context position="21730" citStr="Vasishth and Lewis, 2006" startWordPosition="3518" endWordPosition="3522"> speed of F+L– operations, and none of the theories make any predictions as to the sign associated with the within-embedding measures F–L– and F+L+. component is recalled. In contrast to DLT, embedding difference and dynamic recruitment would predict a shorter reading time in the F–L+ case because of the reduction in memory load. In an ACT-R framework, reading time durations can increase at the retrieval site because the retrieval causes competition among similarly encoded signs in the context set. While it is possible for reading times to decrease when completing a center embedding in ACT-R (Vasishth and Lewis, 2006), this would be expressed as a frequency effect due to certain argument types commonly foreshadowing their predicates (Jaeger et al., 2008). Since frequency effects are factored separately from memory effects in this study, ACT-R would predict longer residual (memory-based) reading times when completing an embedding. Predicted correlations to reading times for the F and L transitions are summarized in Table 2. 5 Eye-tracking Eye-tracking and reading time data are often used to test complexity measures (Gibson, 2000; Demberg and Keller, 2008; Roark et al., 2009) under the assumption that reader</context>
</contexts>
<marker>Vasishth, Lewis, 2006</marker>
<rawString>Shravan Vasishth and Richard L. Lewis. 2006. Argument-head distance and processing complexity: Explaining both locality and antilocality effects. Language, 82(4):767–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wu</author>
<author>Asaf Bachrach</author>
<author>Carlos Cardenas</author>
<author>William Schuler</author>
</authors>
<title>Complexity metrics in an incremental right-corner parser.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL’10),</booktitle>
<pages>1189--1198</pages>
<contexts>
<context position="3538" citStr="Wu et al., 2010" startWordPosition="544" endWordPosition="547">ese measures were used to predict reading time durations on the Dundee eye-tracking corpus. Sections 6 and 7 present results and discuss. 2 Frequency Measures 2.1 Surprisal One of the strongest predictors of processing complexity is surprisal (Hale, 2001). It has been shown in numerous studies to have a strong correlation with reading time durations in eye-tracking and selfpaced reading studies when calculated with a variety Proceedings of NAACL-HLT 2013, pages 95–105, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics of models (Levy, 2008; Roark et al., 2009; Wu et al., 2010). Surprisal predicts the integration difficulty that a word xt at time step t presents given the preceding context and is calculated as follows: E� s∈S(x1...xt) P(s) E s∈S(x1...xt−1) P(s) (1) where 5(x1 ... xt) is the set of syntactic trees whose leaves have x1 ... xt as a prefix.1 In essence, surprisal measures how unexpected constructions are in a given context. What it does not provide is an explanation for why certain constructions would be less common and thus more surprising. 2.2 Entropy Reduction Processing difficulty can also be measured in terms of entropy (Shannon, 1948). A larger en</context>
<context position="5182" citStr="Wu et al. (2010)" startWordPosition="808" endWordPosition="811">This measures the change in uncertainty about the discourse as each new word is processed. 3 Memory Measures 3.1 Dependency Locality In Dependency Locality Theory (DLT) (Gibson, 2000), complexity arises from intervening referents introduced between a predicate and its argument. Under the original formulation of DLT, there is a 1The parser in this study uses a beam. However, given high parser accuracy, Roark (2001) showed that calculating complexity metrics over a beam should obtain similar results to the full complexity calculation. 2The incremental formulation used here was first proposed in Wu et al. (2010). storage cost for each new referent introduced and an integration cost for each referent intervening in a dependency projection. This is a simplification made for ease of computation, and subsequent work has found DLT to be more accurate cross-linguistically if the intervening elements are structurally defined rather than defined in terms of referents (Kwon et al., 2010). That is, simply having a particular referent intervene in a dependency projection may not have as great an effect on processing complexity as the syntactic construction the referent appears in. Therefore, this work reinterpr</context>
<context position="12282" citStr="Wu et al. (2010)" startWordPosition="1912" endWordPosition="1915">ces, and the reduced processing complexity arising from reduced memory load would yield a temporary facilitation in processing. No longer requiring the additional resources to cope with the increased embedding, the processor would release them, returning the processor to its usual speed. Unlike anti-locality, where processing is facilitated in longer passages due to accumulating probabilistic evidence, a model of dynamic recruitment of additional processing resources would predict universal facilitation after a center embedding of any length, modulo frequency effects. 3.5 Embedding Difference Wu et al. (2010) propose an explicit measure of the difficulty associated with processing centerembedded constructions, which is similar to the predictions of dynamic recruitment and is defined in terms of changes in memory load. They calculate a probabilistically-weighted average embedding depth as follows: J: µemb(x1 ... xt) = d(s) · P(s) (4) sES(x1...xt) where d(s) returns the embedding depth of the derivation s at xt in a variant of a left-corner parsing process.3 Embedding difference may then be derived as: EmbDiff (x1 ... xt) =µemb(x1 ... xt)− (5) µemb(x1 . . . xt−1) This is hypothesized to correlate po</context>
<context position="20194" citStr="Wu et al. (2010)" startWordPosition="3281" endWordPosition="3284">quentially cue the prediction that it has finished the left child of an S. F+L+ transitions locally expand the awaited sign of the most subordinate state and so should also not require any recall or encoding. For example, in Figure 2, observing bought while awaiting a VP sequentially cues a prediction of NP. F+L–, then, loosely corresponds to a storage action under DLT as more hierarchic levels must now be maintained at each future step of the parse. As stated before, it differs from DLT in that it is sensitive to the depth of embedding rather than a particular subset of syntactic categories. Wu et al. (2010) found that increasing the embedding depth led to longer reading times in a self-paced reading experiment. In ACT-R terms, F+L– corresponds to an encoding action, potentially causing processing difficulty resulting from the similarity of the current sign to previously encoded signs. F–L+, by contrast, is similar to DLT’s integration action since a subordinate connected component is integrated into the rest of the parse structure. This represents a temporal cueing event in which the awaited category of the superordinate connected Theory F+L– F–L+ DLT positive positive ACT-R positive positive Hi</context>
<context position="32307" citStr="Wu et al. (2010)" startWordPosition="5228" endWordPosition="5231">ms, surprisal, and entropy reduction, an anti-locality explanation would rely on either (i) more precise variants of the metrics used in this study or (ii) other frequency metrics altogether. Future work could investigate the possibility of anti-locality by looking at the distance between an encoding operation and its corresponding 102 integration action to see if the integration facilitation observed in this study is driven by longer embeddings or if there is simply a general facilitation effect when completing embeddings. The finding of a negative integration cost was previously observed by Wu et al. (2010) as well as Demberg and Keller (2008), although Demberg and Keller calculated it using the original referent-based definitions of Gibson (1998; 2000) and varied which parts of speech counted for calculating integration cost. Ultimately, Demberg and Keller (2008) concluded that the negative coefficient was evidence that integration cost was not a good broad-coverage predictor of reading times; however, this study has replicated the effect and showed it to be a very strong predictor of reading times, albeit one that is correlated with facilitation rather than inhibition. It is interesting that m</context>
</contexts>
<marker>Wu, Bachrach, Cardenas, Schuler, 2010</marker>
<rawString>Stephen Wu, Asaf Bachrach, Carlos Cardenas, and William Schuler. 2010. Complexity metrics in an incremental right-corner parser. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL’10), pages 1189–1198.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>