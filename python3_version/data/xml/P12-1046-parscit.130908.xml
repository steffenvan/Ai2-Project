<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005037">
<title confidence="0.998768">
Bayesian Symbol-Refined Tree Substitution Grammars
for Syntactic Parsing
</title>
<author confidence="0.911974">
Hiroyuki Shindo† Yusuke Miyao‡ Akinori Fujino† Masaaki Nagata†
</author>
<affiliation confidence="0.698732">
†NTT Communication Science Laboratories, NTT Corporation
</affiliation>
<address confidence="0.8973">
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
</address>
<email confidence="0.994983">
{shindo.hiroyuki,fujino.akinori,nagata.masaaki}@lab.ntt.co.jp
</email>
<affiliation confidence="0.898419">
‡National Institute of Informatics
</affiliation>
<address confidence="0.988583">
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan
</address>
<email confidence="0.999519">
yusuke@nii.ac.jp
</email>
<sectionHeader confidence="0.993909" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99959119047619">
We propose Symbol-Refined Tree Substitu-
tion Grammars (SR-TSGs) for syntactic pars-
ing. An SR-TSG is an extension of the con-
ventional TSG model where each nonterminal
symbol can be refined (subcategorized) to fit
the training data. We aim to provide a unified
model where TSG rules and symbol refine-
ment are learned from training data in a fully
automatic and consistent fashion. We present
a novel probabilistic SR-TSG model based
on the hierarchical Pitman-Yor Process to en-
code backoff smoothing from a fine-grained
SR-TSG to simpler CFG rules, and develop
an efficient training method based on Markov
Chain Monte Carlo (MCMC) sampling. Our
SR-TSG parser achieves an F1 score of 92.4%
in the Wall Street Journal (WSJ) English Penn
Treebank parsing task, which is a 7.7 point im-
provement over a conventional Bayesian TSG
parser, and better than state-of-the-art discrim-
inative reranking parsers.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901306122449">
Syntactic parsing has played a central role in natural
language processing. The resulting syntactic analy-
sis can be used for various applications such as ma-
chine translation (Galley et al., 2004; DeNeefe and
Knight, 2009), sentence compression (Cohn and La-
pata, 2009; Yamangil and Shieber, 2010), and ques-
tion answering (Wang et al., 2007). Probabilistic
context-free grammar (PCFG) underlies many sta-
tistical parsers, however, it is well known that the
PCFG rules extracted from treebank data via maxi-
mum likelihood estimation do not perform well due
to unrealistic context freedom assumptions (Klein
and Manning, 2003).
In recent years, there has been an increasing inter-
est in tree substitution grammar (TSG) as an alter-
native to CFG for modeling syntax trees (Post and
Gildea, 2009; Tenenbaum et al., 2009; Cohn et al.,
2010). TSG is a natural extension of CFG in which
nonterminal symbols can be rewritten (substituted)
with arbitrarily large tree fragments. These tree frag-
ments have great advantages over tiny CFG rules
since they can capture non-local contexts explic-
itly such as predicate-argument structures, idioms
and grammatical agreements (Cohn et al., 2010).
Previous work on TSG parsing (Cohn et al., 2010;
Post and Gildea, 2009; Bansal and Klein, 2010) has
consistently shown that a probabilistic TSG (PTSG)
parser is significantly more accurate than a PCFG
parser, but is still inferior to state-of-the-art parsers
(e.g., the Berkeley parser (Petrov et al., 2006) and
the Charniak parser (Charniak and Johnson, 2005)).
One major drawback of TSG is that the context free-
dom assumptions still remain at substitution sites,
that is, TSG tree fragments are generated that are
conditionally independent of all others given root
nonterminal symbols. Furthermore, when a sentence
is unparsable with large tree fragments, the PTSG
parser usually uses naive CFG rules derived from
its backoff model, which diminishes the benefits ob-
tained from large tree fragments.
On the other hand, current state-of-the-art parsers
use symbol refinement techniques (Johnson, 1998;
Collins, 2003; Matsuzaki et al., 2005). Symbol
refinement is a successful approach for weaken-
ing context freedom assumptions by dividing coarse
treebank symbols (e.g. NP and VP) into sub-
categories, rather than extracting large tree frag-
ments. As shown in several studies on TSG pars-
ing (Zuidema, 2007; Bansal and Klein, 2010), large
</bodyText>
<page confidence="0.974214">
440
</page>
<note confidence="0.985732">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99986576">
tree fragments and symbol refinement work comple-
mentarily for syntactic parsing. For example, Bansal
and Klein (2010) have reported that deterministic
symbol refinement with heuristics helps improve the
accuracy of a TSG parser.
In this paper, we propose Symbol-Refined Tree
Substitution Grammars (SR-TSGs) for syntactic
parsing. SR-TSG is an extension of the conventional
TSG model where each nonterminal symbol can be
refined (subcategorized) to fit the training data. Our
work differs from previous studies in that we focus
on a unified model where TSG rules and symbol re-
finement are learned from training data in a fully au-
tomatic and consistent fashion. We also propose a
novel probabilistic SR-TSG model with the hierar-
chical Pitman-Yor Process (Pitman and Yor, 1997),
namely a sort of nonparametric Bayesian model, to
encode backoff smoothing from a fine-grained SR-
TSG to simpler CFG rules, and develop an efficient
training method based on blocked MCMC sampling.
Our SR-TSG parser achieves an F1 score of
92.4% in the WSJ English Penn Treebank pars-
ing task, which is a 7.7 point improvement over a
conventional Bayesian TSG parser, and superior to
state-of-the-art discriminative reranking parsers.
</bodyText>
<sectionHeader confidence="0.802458" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.999944117647059">
Our SR-TSG work is built upon recent work on
Bayesian TSG induction from parse trees (Post and
Gildea, 2009; Cohn et al., 2010). We firstly review
the Bayesian TSG model used in that work, and then
present related work on TSGs and symbol refine-
ment.
A TSG consists of a 4-tuple, G = (T, N, S, R),
where T is a set of terminal symbols, N is a set of
nonterminal symbols, S E N is the distinguished
start nonterminal symbol and R is a set of produc-
tions (a.k.a. rules). The productions take the form
of elementary trees i.e., tree fragments of height
&gt; 1. The root and internal nodes of the elemen-
tary trees are labeled with nonterminal symbols, and
leaf nodes are labeled with either terminal or nonter-
minal symbols. Nonterminal leaves are referred to
as frontier nonterminals, and form the substitution
sites to be combined with other elementary trees.
A derivation is a process of forming a parse tree.
It starts with a root symbol and rewrites (substi-
tutes) nonterminal symbols with elementary trees
until there are no remaining frontier nonterminals.
Figure 1a shows an example parse tree and Figure
1b shows its example TSG derivation. Since differ-
ent derivations may produce the same parse tree, re-
cent work on TSG induction (Post and Gildea, 2009;
Cohn et al., 2010) employs a probabilistic model of
a TSG and predicts derivations from observed parse
trees in an unsupervised way.
A Probabilistic Tree Substitution Grammar
(PTSG) assigns a probability to each rule in the
grammar. The probability of a derivation is defined
as the product of the probabilities of its component
elementary trees as follows.
</bodyText>
<equation confidence="0.9764725">
p (e) = 11 p (e |x),
x-4eEe
</equation>
<bodyText confidence="0.997201555555556">
where e = (e1, e2, ...) is a sequence of elemen-
tary trees used for the derivation, x = root (e) is the
root symbol of e, and p (e |x) is the probability of
generating e given its root symbol x. As in a PCFG,
e is generated conditionally independent of all oth-
ers given x.
The posterior distribution over elementary trees
given a parse tree t can be computed by using the
Bayes’ rule:
</bodyText>
<equation confidence="0.888002">
p(e|t) °c p(t |e) p(e).
</equation>
<bodyText confidence="0.999930421052632">
where p (t |e) is either equal to 1 (when t and e
are consistent) or 0 (otherwise). Therefore, the task
of TSG induction from parse trees turns out to con-
sist of modeling the prior distribution p (e). Recent
work on TSG induction defines p (e) as a nonpara-
metric Bayesian model such as the Dirichlet Pro-
cess (Ferguson, 1973) or the Pitman-Yor Process to
encourage sparse and compact grammars.
Several studies have combined TSG induction and
symbol refinement. An adaptor grammar (Johnson
et al., 2007a) is a sort of nonparametric Bayesian
TSG model with symbol refinement, and is thus
closely related to our SR-TSG model. However,
an adaptor grammar differs from ours in that all its
rules are complete: all leaf nodes must be termi-
nal symbols, while our model permits nonterminal
symbols as leaf nodes. Furthermore, adaptor gram-
mars have largely been applied to the task of unsu-
pervised structural induction from raw texts such as
</bodyText>
<page confidence="0.997446">
441
</page>
<figure confidence="0.997796">
(a) (b) (c)
</figure>
<figureCaption confidence="0.982782">
Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of
(a). The refinement annotation is hyphenated with a nonterminal symbol.
</figureCaption>
<bodyText confidence="0.9839676">
morphology analysis, word segmentation (Johnson
and Goldwater, 2009), and dependency grammar in-
duction (Cohen et al., 2010), rather than constituent
syntax parsing.
An all-fragments grammar (Bansal and Klein,
2010) is another variant of TSG that aims to uti-
lize all possible subtrees as rules. It maps a TSG
to an implicit representation to make the grammar
tractable and practical for large-scale parsing. The
manual symbol refinement described in (Klein and
Manning, 2003) was applied to an all-fragments
grammar and this improved accuracy in the English
WSJ parsing task. As mentioned in the introduc-
tion, our model focuses on the automatic learning of
a TSG and symbol refinement without heuristics.
</bodyText>
<sectionHeader confidence="0.95339" genericHeader="method">
3 Symbol-Refined Tree Substitution
Grammars
</sectionHeader>
<bodyText confidence="0.999971142857143">
In this section, we propose Symbol-Refined Tree
Substitution Grammars (SR-TSGs) for syntactic
parsing. Our SR-TSG model is an extension of
the conventional TSG model where every symbol of
the elementary trees can be refined to fit the train-
ing data. Figure 1c shows an example of SR-TSG
derivation. As with previous work on TSG induc-
tion, our task is the induction of SR-TSG deriva-
tions from a corpus of parse trees in an unsupervised
fashion. That is, we wish to infer the symbol sub-
categories of every node and substitution site (i.e.,
nodes where substitution occurs) from parse trees.
Extracted rules and their probabilities can be used to
parse new raw sentences.
</bodyText>
<subsectionHeader confidence="0.975075">
3.1 Probabilistic Model
</subsectionHeader>
<bodyText confidence="0.999998285714286">
We define a probabilistic model of an SR-TSG based
on the Pitman-Yor Process (PYP) (Pitman and Yor,
1997), namely a sort of nonparametric Bayesian
model. The PYP produces power-law distributions,
which have been shown to be well-suited for such
uses as language modeling (Teh, 2006b), and TSG
induction (Cohn et al., 2010). One major issue as
regards modeling an SR-TSG is that the space of the
grammar rules will be very sparse since SR-TSG al-
lows for arbitrarily large tree fragments and also an
arbitrarily large set of symbol subcategories. To ad-
dress the sparseness problem, we employ a hierar-
chical PYP to encode a backoff scheme from the SR-
TSG rules to simpler CFG rules, inspired by recent
work on dependency parsing (Blunsom and Cohn,
2010).
Our model consists of a three-level hierarchy. Ta-
ble 1 shows an example of the SR-TSG rule and its
backoff tree fragments as an illustration of this three-
level hierarchy. The topmost level of our model is a
distribution over the SR-TSG rules as follows.
</bodyText>
<equation confidence="0.9264215">
e |xk — Gxk
Gxk — PYP (dxk, exk, Psr-tsg (· |xk )) ,
</equation>
<bodyText confidence="0.999583666666667">
where xk is a refined root symbol of an elemen-
tary tree e, while x is a raw nonterminal symbol
in the corpus and k = 0,1, ... is an index of the
symbol subcategory. Suppose x is NP and its sym-
bol subcategory is 0, then xk is NPO. The PYP has
three parameters: (dxk, 0xk, Psr-tsg). Psr-tsg (· |xk )
</bodyText>
<page confidence="0.995272">
442
</page>
<table confidence="0.452657">
SR-TSG SR-CFG RU-CFG
</table>
<tableCaption confidence="0.998278">
Table 1: Example three-level backoff.
</tableCaption>
<bodyText confidence="0.99965875">
is a base distribution over infinite space of symbol-
refined elementary trees rooted with xk, which pro-
vides the backoff probability of e. The remaining
parameters dxk and θxk control the strength of the
base distribution.
The backoff probability Psr-tsg (e |xk ) is given by
the product of symbol-refined CFG (SR-CFG) rules
that e contains as follows.
</bodyText>
<equation confidence="0.9826448">
11 P sr-tsg (e |xk ) = 11 scf x (1 — sc;)
fEF(e) iEI(e)
x H (cfg-rules (e |xk ))
α |xk — Hxk
Hxk — PYP (dx, θx, Psr-cfg (·|xk )) ,
</equation>
<bodyText confidence="0.98754675">
where F (e) is a set of frontier nonterminal nodes
and I (e) is a set of internal nodes in e. cf and ci
are nonterminal symbols of nodes f and i, respec-
tively. s, is the probability of stopping the expan-
sion of a node labeled with c. SR-CFG rules are
CFG rules where every symbol is refined, as shown
in Table 1. The function cfg-rules (e |xk ) returns
the SR-CFG rules that e contains, which take the
form of xk —* α. Each SR-CFG rule α rooted
with xk is drawn from the backoff distribution Hxk,
and Hxk is produced by the PYP with parameters:
(dx, θx, Psr-cfg). This distribution over the SR-CFG
rules forms the second level hierarchy of our model.
The backoff probability of the SR-CFG rule,
Psr-cfg (α |xk ),
(RU-CFG) rule as follows,
</bodyText>
<equation confidence="0.996816333333333">
Psr-cfg (α |xk ) = I (root-unrefine (α |xk ))
α |x — Ix
Ix — PYP (dx, θ&apos;, Pru-cfg (· |x)) ,
</equation>
<bodyText confidence="0.999972090909091">
where the function root-unrefine (α |xk ) returns
the RU-CFG rule of α, which takes the form of x —*
α. The RU-CFG rule is a CFG rule where the root
symbol is unrefined and all leaf nonterminal sym-
bols are refined, as shown in Table 1. Each RU-CFG
rule α rooted with x is drawn from the backoff distri-
bution Ix, and Ix is produced by a PYP. This distri-
bution over the RU-CFG rules forms the third level
hierarchy of our model. Finally, we set the back-
off probability of the RU-CFG rule, Pru-cfg (α |x),
so that it is uniform as follows.
</bodyText>
<equation confidence="0.504435">
|x —* ·|.
</equation>
<bodyText confidence="0.9999662">
where |x —* · |is the number of RU-CFG rules
rooted with x. Overall, our hierarchical model en-
codes backoff smoothing consistently from the SR-
TSG rules to the SR-CFG rules, and from the SR-
CFG rules to the RU-CFG rules. As shown in (Blun-
som and Cohn, 2010; Cohen et al., 2010), the pars-
ing accuracy of the TSG model is strongly affected
by its backoff model. The effects of our hierarchical
backoff model on parsing performance are evaluated
in Section 5.
</bodyText>
<sectionHeader confidence="0.999848" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.979041272727273">
We use Markov Chain Monte Carlo (MCMC) sam-
pling to infer the SR-TSG derivations from parse
trees. MCMC sampling is a widely used approach
for obtaining random samples from a probability
distribution. In our case, we wish to obtain deriva-
tion samples of an SR-TSG from the posterior dis-
tribution, p (e |t, d, 0, s).
The inference of the SR-TSG derivations corre-
sponds to inferring two kinds of latent variables:
latent symbol subcategories and latent substitution
is given by the root-unrefined CFG
</bodyText>
<equation confidence="0.980886">
Pru-cfg (α |x) = 1
</equation>
<page confidence="0.990253">
443
</page>
<bodyText confidence="0.999989727272727">
sites. We first infer latent symbol subcategories for
every symbol in the parse trees, and then infer latent
substitution sites stepwise. During the inference of
symbol subcategories, every internal node is fixed as
a substitution site. After that, we unfix that assump-
tion and infer latent substitution sites given symbol-
refined parse trees. This stepwise learning is simple
and efficient in practice, but we believe that the joint
learning of both latent variables is possible, and we
will deal with this in future work. Here we describe
each inference algorithm in detail.
</bodyText>
<subsectionHeader confidence="0.989442">
4.1 Inference of Symbol Subcategories
</subsectionHeader>
<bodyText confidence="0.999972693877551">
For the inference of latent symbol subcategories, we
adopt split and merge training (Petrov et al., 2006)
as follows. In each split-merge step, each symbol
is split into at most two subcategories. For exam-
ple, every NP symbol in the training data is split into
either NP0 or NP1 to maximize the posterior prob-
ability. After convergence, we measure the loss of
each split symbol in terms of the likelihood incurred
when removing it, then the smallest 50% of the
newly split symbols as regards that loss are merged
to avoid overfitting. The split-merge algorithm ter-
minates when the total number of steps reaches the
user-specified value.
In each splitting step, we use two types of blocked
MCMC algorithm: the sentence-level blocked
Metroporil-Hastings (MH) sampler and the tree-
level blocked Gibbs sampler, while (Petrov et al.,
2006) use a different MLE-based model and the EM
algorithm. Our sampler iterates sentence-level sam-
pling and tree-level sampling alternately.
The sentence-level MH sampler is a recently pro-
posed algorithm for grammar induction (Johnson et
al., 2007b; Cohn et al., 2010). In this work, we apply
it to the training of symbol splitting. The MH sam-
pler consists of the following three steps: for each
sentence, 1) calculate the inside probability (Lari
and Young, 1991) in a bottom-up manner, 2) sample
a derivation tree in a top-down manner, and 3) ac-
cept or reject the derivation sample by using the MH
test. See (Cohn et al., 2010) for details. This sampler
simultaneously updates blocks of latent variables as-
sociated with a sentence, thus it can find MAP solu-
tions efficiently.
The tree-level blocked Gibbs sampler focuses on
the type of SR-TSG rules and simultaneously up-
dates all root and child nodes that are annotated
with the same SR-TSG rule. For example, the
sampler collects all nodes that are annotated with
S0 —* NP1VP2, then updates those nodes to an-
other subcategory such as S0 —* NP2VP0 according
to the posterior distribution. This sampler is simi-
lar to table label resampling (Johnson and Goldwa-
ter, 2009), but differs in that our sampler can update
multiple table labels simultaneously when multiple
tables are labeled with the same elementary tree.
The tree-level sampler also simultaneously updates
blocks of latent variables associated with the type of
SR-TSG rules, thus it can find MAP solutions effi-
ciently.
</bodyText>
<subsectionHeader confidence="0.983551">
4.2 Inference of Substitution Sites
</subsectionHeader>
<bodyText confidence="0.999965421052632">
After the inference of symbol subcategories, we
use Gibbs sampling to infer the substitution sites of
parse trees as described in (Cohn and Lapata, 2009;
Post and Gildea, 2009). We assign a binary variable
to each internal node in the training data, which in-
dicates whether that node is a substitution site or not.
For each iteration, the Gibbs sampler works by sam-
pling the value of each binary variable in random
order. See (Cohn et al., 2010) for details.
During the inference, our sampler ignores
the symbol subcategories of internal nodes of
elementary trees since they do not affect the
derivation of the SR-TSG. For example, the
elementary trees “(S0 (NP0 NNP0) VP0)” and
“(S0 (NP1 NNP0) VP0)” are regarded as being the
same when we calculate the generation probabilities
according to our model. This heuristics is help-
ful for finding large tree fragments and learning
compact grammars.
</bodyText>
<subsectionHeader confidence="0.996246">
4.3 Hyperparameter Estimation
</subsectionHeader>
<bodyText confidence="0.999798166666667">
We treat hyperparameters {d, 01 as random vari-
ables and update their values for every MCMC it-
eration. We place a prior on the hyperparameters as
follows: d — Beta (1, 1), 0 — Gamma (1, 1). The
values of d and 0 are optimized with the auxiliary
variable technique (Teh, 2006a).
</bodyText>
<page confidence="0.999213">
444
</page>
<sectionHeader confidence="0.998001" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.990012">
5.1 Settings
</subsectionHeader>
<subsubsectionHeader confidence="0.943104">
5.1.1 Data Preparation
</subsubsectionHeader>
<bodyText confidence="0.999958529411765">
We ran experiments on the Wall Street Journal
(WSJ) portion of the English Penn Treebank data
set (Marcus et al., 1993), using a standard data
split (sections 2–21 for training, 22 for development
and 23 for testing). We also used section 2 as a
small training set for evaluating the performance of
our model under low-resource conditions. Hence-
forth, we distinguish the small training set (section
2) from the full training set (sections 2-21). The tree-
bank data is right-binarized (Matsuzaki et al., 2005)
to construct grammars with only unary and binary
productions. We replace lexical words with count
&lt; 5 in the training data with one of 50 unknown
words using lexical features, following (Petrov et al.,
2006). We also split off all the function tags and
eliminated empty nodes from the data set, follow-
ing (Johnson, 1998).
</bodyText>
<subsubsectionHeader confidence="0.718468">
5.1.2 Training and Parsing
</subsubsectionHeader>
<bodyText confidence="0.99997025">
For the inference of symbol subcategories, we
trained our model with the MCMC sampler by us-
ing 6 split-merge steps for the full training set and 3
split-merge steps for the small training set. There-
fore, each symbol can be subdivided into a maxi-
mum of 26 = 64 and 23 = 8 subcategories, respec-
tively. In each split-merge step, we initialized the
sampler by randomly splitting every symbol in two
subcategories and ran the MCMC sampler for 1000
iterations. After that, to infer the substitution sites,
we initialized the model with the final sample from
a run on the small training set, and used the Gibbs
sampler for 2000 iterations. We estimated the opti-
mal values of the stopping probabilities s by using
the development set.
We obtained the parsing results with the MAX-
RULE-PRODUCT algorithm (Petrov et al., 2006) by
using the SR-TSG rules extracted from our model.
We evaluated the accuracy of our parser by brack-
eting F1 score of predicted parse trees. We used
EVALB1 to compute the F1 score. In all our exper-
iments, we conducted ten independent runs to train
our model, and selected the one that performed best
on the development set in terms of parsing accuracy.
</bodyText>
<footnote confidence="0.980267">
1http://nlp.cs.nyu.edu/evalb/
</footnote>
<table confidence="0.998112666666667">
Model F1 (small) F1 (full)
CFG 61.9 63.6
*TSG 77.1 85.0
SR-TSG (Psr-tsg) 73.0 86.4
SR-TSG (Psr-tsg, Psr-cfg) 79.4 89.7
SR-TSG (Psr-tsg, Psr-cfg, Pru-cfg) 81.7 91.1
</table>
<tableCaption confidence="0.792529666666667">
Table 2: Comparison of parsing accuracy with the
small and full training sets. *Our reimplementation
of (Cohn et al., 2010).
</tableCaption>
<figureCaption confidence="0.893105">
Figure 2: Histogram of SR-TSG and TSG rule sizes
</figureCaption>
<bodyText confidence="0.938351333333333">
on the small training set. The size is defined as the
number of CFG rules that the elementary tree con-
tains.
</bodyText>
<subsectionHeader confidence="0.867681">
5.2 Results and Discussion
5.2.1 Comparison of SR-TSG with TSG
</subsectionHeader>
<bodyText confidence="0.9998653125">
We compared the SR-TSG model with the CFG
and TSG models as regards parsing accuracy. We
also tested our model with three backoff hierarchy
settings to evaluate the effects of backoff smoothing
on parsing accuracy. Table 2 shows the F1 scores
of the CFG, TSG and SR-TSG parsers for small and
full training sets. In Table 2, SR-TSG (Psr-tsg) de-
notes that we used only the topmost level of the hi-
erarchy. Similary, SR-TSG (Psr-tsg, Psr-cfg) denotes
that we used only the Psr-tsg and Psr-cfg backoff mod-
els.
Our best model, SR-TSG (Psr-tsg, Psr-cfg, Pru-cfg),
outperformed both the CFG and TSG models on
both the small and large training sets. This result
suggests that the conventional TSG model trained
from the vanilla treebank is insufficient to resolve
</bodyText>
<page confidence="0.997256">
445
</page>
<table confidence="0.999780470588235">
Model F1 (&lt; 40) F1 (all)
TSG (no symbol refinement)
Post and Gildea (2009) 82.6 -
Cohn et al. (2010) 85.4 84.7
TSG with Symbol Refinement
Zuidema (2007) - *83.8
Bansal et al. (2010) 88.7 88.1
SR-TSG (single) 91.6 91.1
SR-TSG (multiple) 92.9 92.4
CFG with Symbol Refinement
Collins (1999) 88.6 88.2
Petrov and Klein (2007) 90.6 90.1
Petrov (2010) - 91.8
Discriminative
Carreras et al. (2008) - 91.1
Charniak and Johnson (2005) 92.0 91.4
Huang (2008) 92.3 91.7
</table>
<tableCaption confidence="0.8705325">
Table 3: Our parsing performance for the testing set compared with those of other parsers. *Results for the
development set (&lt; 100).
</tableCaption>
<bodyText confidence="0.999842294117647">
structural ambiguities caused by coarse symbol an-
notations in a training corpus. As we expected, sym-
bol refinement can be helpful with the TSG model
for further fitting the training set and improving the
parsing accuracy.
The performance of the SR-TSG parser was
strongly affected by its backoff models. For exam-
ple, the simplest model, Psr-tsg, performed poorly
compared with our best model. This result suggests
that the SR-TSG rules extracted from the training
set are very sparse and cannot cover the space of
unknown syntax patterns in the testing set. There-
fore, sophisticated backoff modeling is essential for
the SR-TSG parser. Our hierarchical PYP model-
ing technique is a successful way to achieve back-
off smoothing from sparse SR-TSG rules to simpler
CFG rules, and offers the advantage of automatically
estimating the optimal backoff probabilities from the
training set.
We compared the rule sizes and frequencies of
SR-TSG with those of TSG. The rule sizes of SR-
TSG and TSG are defined as the number of CFG
rules that the elementary tree contains. Figure 2
shows a histogram of the SR-TSG and TSG rule
sizes (by unrefined token) on the small training set.
For example, SR-TSG rules: S1 —* NP0VP1 and
S0 —* NP1VP2 were considered to be the same to-
ken. In Figure 2, we can see that there are almost
the same number of SR-TSG rules and TSG rules
with size = 1. However, there are more SR-TSG
rules than TSG rules with size &gt; 2. This shows
that an SR-TSG can use various large tree fragments
depending on the context, which is specified by the
symbol subcategories.
</bodyText>
<subsectionHeader confidence="0.987164">
5.2.2 Comparison of SR-TSG with Other
Models
</subsectionHeader>
<bodyText confidence="0.9999388">
We compared the accuracy of the SR-TSG parser
with that of conventional high-performance parsers.
Table 3 shows the F1 scores of an SR-TSG and con-
ventional parsers with the full training set. In Ta-
ble 3, SR-TSG (single) is a standard SR-TSG parser,
</bodyText>
<page confidence="0.99815">
446
</page>
<bodyText confidence="0.998077861111111">
and SR-TSG (multiple) is a combination of sixteen
independently trained SR-TSG models, following
the work of (Petrov, 2010).
Our SR-TSG (single) parser achieved an F1 score
of 91.1%, which is a 6.4 point improvement over
the conventional Bayesian TSG parser reported by
(Cohn et al., 2010). Our model can be viewed as
an extension of Cohn’s work by the incorporation
of symbol refinement. Therefore, this result con-
firms that a TSG and symbol refinement work com-
plementarily in improving parsing accuracy. Com-
pared with a symbol-refined CFG model such as the
Berkeley parser (Petrov et al., 2006), the SR-TSG
model can use large tree fragments, which strength-
ens the probability of frequent syntax patterns in
the training set. Indeed, the few very large rules of
our model memorized full parse trees of sentences,
which were repeated in the training set.
The SR-TSG (single) is a pure generative model
of syntax trees but it achieved results comparable to
those of discriminative parsers. It should be noted
that discriminative reranking parsers such as (Char-
niak and Johnson, 2005) and (Huang, 2008) are con-
structed on a generative parser. The reranking parser
takes the k-best lists of candidate trees or a packed
forest produced by a baseline parser (usually a gen-
erative model), and then reranks the candidates us-
ing arbitrary features. Hence, we can expect that
combining our SR-TSG model with a discriminative
reranking parser would provide better performance
than SR-TSG alone.
Recently, (Petrov, 2010) has reported that com-
bining multiple grammars trained independently
gives significantly improved performance over a sin-
gle grammar alone. We applied his method (referred
to as a TREE-LEVEL inference) to the SR-TSG
model as follows. We first trained sixteen SR-TSG
models independently and produced a 100-best list
of the derivations for each model. Then, we erased
the subcategory information of parse trees and se-
lected the best tree that achieved the highest likeli-
hood under the product of sixteen models. The com-
bination model, SR-TSG (multiple), achieved an F1
score of 92.4%, which is a state-of-the-art result for
the WSJ parsing task. Compared with discriminative
reranking parsers, combining multiple grammars by
using the product model provides the advantage that
it does not require any additional training. Several
studies (Fossum and Knight, 2009; Zhang et al.,
2009) have proposed different approaches that in-
volve combining k-best lists of candidate trees. We
will deal with those methods in future work.
Let us note the relation between SR-CFG, TSG
and SR-TSG. TSG is weakly equivalent to CFG and
generates the same set of strings. For example, the
TSG rule “S —* (NP NNP) VP” with probability p
can be converted to the equivalent CFG rules as fol-
lows: “S NPNNP VP ” with probability p and
“NPNNP NNP” with probability 1. From this
viewpoint, TSG utilizes surrounding symbols (NNP
of NPNNP in the above example) as latent variables
with which to capture context information. The
search space of learning a TSG given a parse tree
is O (2n) where n is the number of internal nodes
of the parse tree. On the other hand, an SR-CFG
utilizes an arbitrary index such as 0,1, ... as latent
variables and the search space is larger than that of a
TSG when the symbol refinement model allows for
more than two subcategories for each symbol. Our
experimental results comfirm that jointly modeling
both latent variables using our SR-TSG assists accu-
rate parsing.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999971642857143">
We have presented an SR-TSG, which is an exten-
sion of the conventional TSG model where each
symbol of tree fragments can be automatically sub-
categorized to address the problem of the condi-
tional independence assumptions of a TSG. We pro-
posed a novel backoff modeling of an SR-TSG
based on the hierarchical Pitman-Yor Process and
sentence-level and tree-level blocked MCMC sam-
pling for training our model. Our best model sig-
nificantly outperformed the conventional TSG and
achieved state-of-the-art result in a WSJ parsing
task. Future work will involve examining the SR-
TSG model for different languages and for unsuper-
vised grammar induction.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997002">
We would like to thank Liang Huang for helpful
comments and the three anonymous reviewers for
thoughtful suggestions. We would also like to thank
Slav Petrov and Hui Zhang for answering our ques-
tions about their parsers.
</bodyText>
<page confidence="0.998061">
447
</page>
<sectionHeader confidence="0.990073" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999895198019802">
Mohit Bansal and Dan Klein. 2010. Simple, Accurate
Parsing with an All-Fragments Grammar. In In Proc.
ofACL, pages 1098–1107.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. In Proc. of EMNLP, pages 1204–1213.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proc. ofACL, 1:173–180.
Shay B Cohen, David M Blei, and Noah A Smith. 2010.
Variational Inference for Adaptor Grammars. In In
Proc. of HLT-NAACL, pages 564–572.
Trevor Cohn and Mirella Lapata. 2009. Sentence Com-
pression as Tree Transduction. Journal of Artificial
Intelligence Research, 34:637–674.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing Tree-Substitution Grammars. Journal
of Machine Learning Research, 11:3053–3096.
Michael Collins. 2003. Head-Driven Statistical Mod-
els for Natural Language Parsing. Computational Lin-
guistics, 29:589–637.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
Tree Adjoining Machine Translation. In Proc. of
EMNLP, page 727.
Thomas S Ferguson. 1973. A Bayesian Analysis of
Some Nonparametric Problems. Annals of Statistics,
1:209–230.
Victoria Fossum and Kevin Knight. 2009. Combining
Constituent Parsers. In Proc. of HLT-NAACL, pages
253–256.
Michel Galley, Mark Hopkins, Kevin Knight, Daniel
Marcu, Los Angeles, and Marina Del Rey. 2004.
What’s in a Translation Rule? Information Sciences,
pages 273–280.
Liang Huang. 2008. Forest Reranking : Discriminative
Parsing with Non-Local Features. In Proc. of ACL,
19104:0.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In In Proc. of HLT-NAACL, pages 317–325.
Mark Johnson, Thomas L Griffiths, and Sharon Gold-
water. 2007a. Adaptor Grammars : A Frame-
work for Specifying Compositional Nonparametric
Bayesian Models. Advances in Neural Information
Processing Systems 19, 19:641–648.
Mark Johnson, Thomas L Griffiths, and Sharon Goldwa-
ter. 2007b. Bayesian Inference for PCFGs via Markov
chain Monte Carlo. In In Proc. of HLT-NAACL, pages
139–146.
Mark Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24:613–
632.
Dan Klein and Christopher D Manning. 2003. Accurate
Unlexicalized Parsing. In Proc. ofACL, 1:423–430.
K Lari and S J Young. 1991. Applications of Stochas-
tic Context-Free Grammars Using the Inside–Outside
Algorithm. Computer Speech and Language, 5:237–
257.
Mitchell P Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313–330.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. ofACL, pages 75–82.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of ACL, pages
433–440.
Slav Petrov. 2010. Products of Random Latent Variable
Grammars. In Proc. of HLT-NAACL, pages 19–27.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25:855–900.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. In In Proc. of ACL-
IJCNLP, pages 45–48.
Yee Whye Teh. 2006a. A Bayesian Interpretation of
Interpolated Kneser-Ney. NUS School of Computing
Technical Report TRA2/06.
YW Teh. 2006b. A Hierarchical Bayesian Language
Model based on Pitman-Yor Processes. In Proc. of
ACL, 44:985–992.
J Tenenbaum, TJ O’Donnell, and ND Goodman. 2009.
Fragment Grammars: Exploring Computation and
Reuse in Language. MIT Computer Science and Arti-
ficial Intelligence Laboratory Technical Report Series.
Mengqiu Wang, Noah A Smith, and Teruko Mitamura.
2007. What is the Jeopardy Model ? A Quasi-
Synchronous Grammar for QA. In Proc. of EMNLP-
CoNLL, pages 22–32.
Elif Yamangil and Stuart M Shieber. 2010. Bayesian
Synchronous Tree-Substitution Grammar Induction
and Its Application to Sentence Compression. In In
Proc. ofACL, pages 937–947.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou Li.
2009. K-Best Combination of Syntactic Parsers. In
Proc. of EMNLP, pages 1552–1560.
Willem Zuidema. 2007. Parsimonious Data-Oriented
Parsing. In Proc. of EMNLP-CoNLL, pages 551–560.
</reference>
<page confidence="0.997721">
448
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.321984">
<title confidence="0.814034333333333">Bayesian Symbol-Refined Tree Substitution for Syntactic Parsing Communication Science Laboratories, NTT</title>
<address confidence="0.884736">2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan</address>
<email confidence="0.964393">shindo.hiroyuki@lab.ntt.co.jp</email>
<email confidence="0.964393">fujino.akinori@lab.ntt.co.jp</email>
<email confidence="0.964393">nagata.masaaki@lab.ntt.co.jp</email>
<affiliation confidence="0.997175">Institute of Informatics</affiliation>
<address confidence="0.993679">2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo,</address>
<email confidence="0.995827">yusuke@nii.ac.jp</email>
<abstract confidence="0.998463954545455">We propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. An SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data. We aim to provide a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion. We present a novel probabilistic SR-TSG model based on the hierarchical Pitman-Yor Process to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on Markov Chain Monte Carlo (MCMC) sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discriminative reranking parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Simple, Accurate Parsing with an All-Fragments Grammar. In</title>
<date>2010</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>1098--1107</pages>
<contexts>
<context position="2592" citStr="Bansal and Klein, 2010" startWordPosition="381" endWordPosition="384">as been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser (Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CF</context>
<context position="4047" citStr="Bansal and Klein (2010)" startWordPosition="599" endWordPosition="602">et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for syntactic parsing. For example, Bansal and Klein (2010) have reported that deterministic symbol refinement with heuristics helps improve the accuracy of a TSG parser. In this paper, we propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data. Our work differs from previous studies in that we focus on a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion. We also propose a novel probabilistic SR-TSG model with t</context>
<context position="8552" citStr="Bansal and Klein, 2010" startWordPosition="1360" endWordPosition="1363">nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as 441 (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with a nonterminal symbol. morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing. An all-fragments grammar (Bansal and Klein, 2010) is another variant of TSG that aims to utilize all possible subtrees as rules. It maps a TSG to an implicit representation to make the grammar tractable and practical for large-scale parsing. The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. As mentioned in the introduction, our model focuses on the automatic learning of a TSG and symbol refinement without heuristics. 3 Symbol-Refined Tree Substitution Grammars In this section, we propose Symbol-Refined Tree Substitution Gramm</context>
</contexts>
<marker>Bansal, Klein, 2010</marker>
<rawString>Mohit Bansal and Dan Klein. 2010. Simple, Accurate Parsing with an All-Fragments Grammar. In In Proc. ofACL, pages 1098–1107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1204--1213</pages>
<contexts>
<context position="10528" citStr="Blunsom and Cohn, 2010" startWordPosition="1687" endWordPosition="1690">nparametric Bayesian model. The PYP produces power-law distributions, which have been shown to be well-suited for such uses as language modeling (Teh, 2006b), and TSG induction (Cohn et al., 2010). One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG allows for arbitrarily large tree fragments and also an arbitrarily large set of symbol subcategories. To address the sparseness problem, we employ a hierarchical PYP to encode a backoff scheme from the SRTSG rules to simpler CFG rules, inspired by recent work on dependency parsing (Blunsom and Cohn, 2010). Our model consists of a three-level hierarchy. Table 1 shows an example of the SR-TSG rule and its backoff tree fragments as an illustration of this threelevel hierarchy. The topmost level of our model is a distribution over the SR-TSG rules as follows. e |xk — Gxk Gxk — PYP (dxk, exk, Psr-tsg (· |xk )) , where xk is a refined root symbol of an elementary tree e, while x is a raw nonterminal symbol in the corpus and k = 0,1, ... is an index of the symbol subcategory. Suppose x is NP and its symbol subcategory is 0, then xk is NPO. The PYP has three parameters: (dxk, 0xk, Psr-tsg). Psr-tsg (·</context>
<context position="13313" citStr="Blunsom and Cohn, 2010" startWordPosition="2228" endWordPosition="2232">af nonterminal symbols are refined, as shown in Table 1. Each RU-CFG rule α rooted with x is drawn from the backoff distribution Ix, and Ix is produced by a PYP. This distribution over the RU-CFG rules forms the third level hierarchy of our model. Finally, we set the backoff probability of the RU-CFG rule, Pru-cfg (α |x), so that it is uniform as follows. |x —* ·|. where |x —* · |is the number of RU-CFG rules rooted with x. Overall, our hierarchical model encodes backoff smoothing consistently from the SRTSG rules to the SR-CFG rules, and from the SRCFG rules to the RU-CFG rules. As shown in (Blunsom and Cohn, 2010; Cohen et al., 2010), the parsing accuracy of the TSG model is strongly affected by its backoff model. The effects of our hierarchical backoff model on parsing performance are evaluated in Section 5. 4 Inference We use Markov Chain Monte Carlo (MCMC) sampling to infer the SR-TSG derivations from parse trees. MCMC sampling is a widely used approach for obtaining random samples from a probability distribution. In our case, we wish to obtain derivation samples of an SR-TSG from the posterior distribution, p (e |t, d, 0, s). The inference of the SR-TSG derivations corresponds to inferring two kin</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2010. Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing. In Proc. of EMNLP, pages 1204–1213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-Fine n-Best Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>1--173</pages>
<contexts>
<context position="2857" citStr="Charniak and Johnson, 2005" startWordPosition="421" endWordPosition="424">ten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser (Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement </context>
<context position="22060" citStr="Charniak and Johnson (2005)" startWordPosition="3691" endWordPosition="3694">outperformed both the CFG and TSG models on both the small and large training sets. This result suggests that the conventional TSG model trained from the vanilla treebank is insufficient to resolve 445 Model F1 (&lt; 40) F1 (all) TSG (no symbol refinement) Post and Gildea (2009) 82.6 - Cohn et al. (2010) 85.4 84.7 TSG with Symbol Refinement Zuidema (2007) - *83.8 Bansal et al. (2010) 88.7 88.1 SR-TSG (single) 91.6 91.1 SR-TSG (multiple) 92.9 92.4 CFG with Symbol Refinement Collins (1999) 88.6 88.2 Petrov and Klein (2007) 90.6 90.1 Petrov (2010) - 91.8 Discriminative Carreras et al. (2008) - 91.1 Charniak and Johnson (2005) 92.0 91.4 Huang (2008) 92.3 91.7 Table 3: Our parsing performance for the testing set compared with those of other parsers. *Results for the development set (&lt; 100). structural ambiguities caused by coarse symbol annotations in a training corpus. As we expected, symbol refinement can be helpful with the TSG model for further fitting the training set and improving the parsing accuracy. The performance of the SR-TSG parser was strongly affected by its backoff models. For example, the simplest model, Psr-tsg, performed poorly compared with our best model. This result suggests that the SR-TSG rul</context>
<context position="25182" citStr="Charniak and Johnson, 2005" startWordPosition="4216" endWordPosition="4220">rk complementarily in improving parsing accuracy. Compared with a symbol-refined CFG model such as the Berkeley parser (Petrov et al., 2006), the SR-TSG model can use large tree fragments, which strengthens the probability of frequent syntax patterns in the training set. Indeed, the few very large rules of our model memorized full parse trees of sentences, which were repeated in the training set. The SR-TSG (single) is a pure generative model of syntax trees but it achieved results comparable to those of discriminative parsers. It should be noted that discriminative reranking parsers such as (Charniak and Johnson, 2005) and (Huang, 2008) are constructed on a generative parser. The reranking parser takes the k-best lists of candidate trees or a packed forest produced by a baseline parser (usually a generative model), and then reranks the candidates using arbitrary features. Hence, we can expect that combining our SR-TSG model with a discriminative reranking parser would provide better performance than SR-TSG alone. Recently, (Petrov, 2010) has reported that combining multiple grammars trained independently gives significantly improved performance over a single grammar alone. We applied his method (referred to</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-Fine n-Best Parsing and MaxEnt Discriminative Reranking. In Proc. ofACL, 1:173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>David M Blei</author>
<author>Noah A Smith</author>
</authors>
<title>Variational Inference for Adaptor Grammars. In</title>
<date>2010</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>564--572</pages>
<contexts>
<context position="8461" citStr="Cohen et al., 2010" startWordPosition="1348" endWordPosition="1351">ver, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as 441 (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with a nonterminal symbol. morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing. An all-fragments grammar (Bansal and Klein, 2010) is another variant of TSG that aims to utilize all possible subtrees as rules. It maps a TSG to an implicit representation to make the grammar tractable and practical for large-scale parsing. The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. As mentioned in the introduction, our model focuses on the automatic learning of a TSG and symbol refinement without heuristics. 3 Symbol-Refined Tr</context>
<context position="13334" citStr="Cohen et al., 2010" startWordPosition="2233" endWordPosition="2236">re refined, as shown in Table 1. Each RU-CFG rule α rooted with x is drawn from the backoff distribution Ix, and Ix is produced by a PYP. This distribution over the RU-CFG rules forms the third level hierarchy of our model. Finally, we set the backoff probability of the RU-CFG rule, Pru-cfg (α |x), so that it is uniform as follows. |x —* ·|. where |x —* · |is the number of RU-CFG rules rooted with x. Overall, our hierarchical model encodes backoff smoothing consistently from the SRTSG rules to the SR-CFG rules, and from the SRCFG rules to the RU-CFG rules. As shown in (Blunsom and Cohn, 2010; Cohen et al., 2010), the parsing accuracy of the TSG model is strongly affected by its backoff model. The effects of our hierarchical backoff model on parsing performance are evaluated in Section 5. 4 Inference We use Markov Chain Monte Carlo (MCMC) sampling to infer the SR-TSG derivations from parse trees. MCMC sampling is a widely used approach for obtaining random samples from a probability distribution. In our case, we wish to obtain derivation samples of an SR-TSG from the posterior distribution, p (e |t, d, 0, s). The inference of the SR-TSG derivations corresponds to inferring two kinds of latent variable</context>
</contexts>
<marker>Cohen, Blei, Smith, 2010</marker>
<rawString>Shay B Cohen, David M Blei, and Noah A Smith. 2010. Variational Inference for Adaptor Grammars. In In Proc. of HLT-NAACL, pages 564–572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence Compression as Tree Transduction.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>34--637</pages>
<contexts>
<context position="1589" citStr="Cohn and Lapata, 2009" startWordPosition="222" endWordPosition="226">p an efficient training method based on Markov Chain Monte Carlo (MCMC) sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discriminative reranking parsers. 1 Introduction Syntactic parsing has played a central role in natural language processing. The resulting syntactic analysis can be used for various applications such as machine translation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and question answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003). In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG i</context>
<context position="17208" citStr="Cohn and Lapata, 2009" startWordPosition="2869" endWordPosition="2872">P0 according to the posterior distribution. This sampler is similar to table label resampling (Johnson and Goldwater, 2009), but differs in that our sampler can update multiple table labels simultaneously when multiple tables are labeled with the same elementary tree. The tree-level sampler also simultaneously updates blocks of latent variables associated with the type of SR-TSG rules, thus it can find MAP solutions efficiently. 4.2 Inference of Substitution Sites After the inference of symbol subcategories, we use Gibbs sampling to infer the substitution sites of parse trees as described in (Cohn and Lapata, 2009; Post and Gildea, 2009). We assign a binary variable to each internal node in the training data, which indicates whether that node is a substitution site or not. For each iteration, the Gibbs sampler works by sampling the value of each binary variable in random order. See (Cohn et al., 2010) for details. During the inference, our sampler ignores the symbol subcategories of internal nodes of elementary trees since they do not affect the derivation of the SR-TSG. For example, the elementary trees “(S0 (NP0 NNP0) VP0)” and “(S0 (NP1 NNP0) VP0)” are regarded as being the same when we calculate th</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2009. Sentence Compression as Tree Transduction. Journal of Artificial Intelligence Research, 34:637–674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
<author>Sharon Goldwater</author>
</authors>
<title>Inducing Tree-Substitution Grammars.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--3053</pages>
<contexts>
<context position="2152" citStr="Cohn et al., 2010" startWordPosition="313" endWordPosition="316">ght, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and question answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003). In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art pars</context>
<context position="5295" citStr="Cohn et al., 2010" startWordPosition="798" endWordPosition="801">cess (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model, to encode backoff smoothing from a fine-grained SRTSG to simpler CFG rules, and develop an efficient training method based on blocked MCMC sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the WSJ English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and superior to state-of-the-art discriminative reranking parsers. 2 Background and Related Work Our SR-TSG work is built upon recent work on Bayesian TSG induction from parse trees (Post and Gildea, 2009; Cohn et al., 2010). We firstly review the Bayesian TSG model used in that work, and then present related work on TSGs and symbol refinement. A TSG consists of a 4-tuple, G = (T, N, S, R), where T is a set of terminal symbols, N is a set of nonterminal symbols, S E N is the distinguished start nonterminal symbol and R is a set of productions (a.k.a. rules). The productions take the form of elementary trees i.e., tree fragments of height &gt; 1. The root and internal nodes of the elementary trees are labeled with nonterminal symbols, and leaf nodes are labeled with either terminal or nonterminal symbols. Nonterminal</context>
<context position="10101" citStr="Cohn et al., 2010" startWordPosition="1612" endWordPosition="1615">pus of parse trees in an unsupervised fashion. That is, we wish to infer the symbol subcategories of every node and substitution site (i.e., nodes where substitution occurs) from parse trees. Extracted rules and their probabilities can be used to parse new raw sentences. 3.1 Probabilistic Model We define a probabilistic model of an SR-TSG based on the Pitman-Yor Process (PYP) (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model. The PYP produces power-law distributions, which have been shown to be well-suited for such uses as language modeling (Teh, 2006b), and TSG induction (Cohn et al., 2010). One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG allows for arbitrarily large tree fragments and also an arbitrarily large set of symbol subcategories. To address the sparseness problem, we employ a hierarchical PYP to encode a backoff scheme from the SRTSG rules to simpler CFG rules, inspired by recent work on dependency parsing (Blunsom and Cohn, 2010). Our model consists of a three-level hierarchy. Table 1 shows an example of the SR-TSG rule and its backoff tree fragments as an illustration of this threelevel hierarchy. </context>
<context position="15758" citStr="Cohn et al., 2010" startWordPosition="2628" endWordPosition="2631">s as regards that loss are merged to avoid overfitting. The split-merge algorithm terminates when the total number of steps reaches the user-specified value. In each splitting step, we use two types of blocked MCMC algorithm: the sentence-level blocked Metroporil-Hastings (MH) sampler and the treelevel blocked Gibbs sampler, while (Petrov et al., 2006) use a different MLE-based model and the EM algorithm. Our sampler iterates sentence-level sampling and tree-level sampling alternately. The sentence-level MH sampler is a recently proposed algorithm for grammar induction (Johnson et al., 2007b; Cohn et al., 2010). In this work, we apply it to the training of symbol splitting. The MH sampler consists of the following three steps: for each sentence, 1) calculate the inside probability (Lari and Young, 1991) in a bottom-up manner, 2) sample a derivation tree in a top-down manner, and 3) accept or reject the derivation sample by using the MH test. See (Cohn et al., 2010) for details. This sampler simultaneously updates blocks of latent variables associated with a sentence, thus it can find MAP solutions efficiently. The tree-level blocked Gibbs sampler focuses on the type of SR-TSG rules and simultaneousl</context>
<context position="17501" citStr="Cohn et al., 2010" startWordPosition="2922" endWordPosition="2925"> simultaneously updates blocks of latent variables associated with the type of SR-TSG rules, thus it can find MAP solutions efficiently. 4.2 Inference of Substitution Sites After the inference of symbol subcategories, we use Gibbs sampling to infer the substitution sites of parse trees as described in (Cohn and Lapata, 2009; Post and Gildea, 2009). We assign a binary variable to each internal node in the training data, which indicates whether that node is a substitution site or not. For each iteration, the Gibbs sampler works by sampling the value of each binary variable in random order. See (Cohn et al., 2010) for details. During the inference, our sampler ignores the symbol subcategories of internal nodes of elementary trees since they do not affect the derivation of the SR-TSG. For example, the elementary trees “(S0 (NP0 NNP0) VP0)” and “(S0 (NP1 NNP0) VP0)” are regarded as being the same when we calculate the generation probabilities according to our model. This heuristics is helpful for finding large tree fragments and learning compact grammars. 4.3 Hyperparameter Estimation We treat hyperparameters {d, 01 as random variables and update their values for every MCMC iteration. We place a prior on</context>
<context position="20653" citStr="Cohn et al., 2010" startWordPosition="3450" endWordPosition="3453">We evaluated the accuracy of our parser by bracketing F1 score of predicted parse trees. We used EVALB1 to compute the F1 score. In all our experiments, we conducted ten independent runs to train our model, and selected the one that performed best on the development set in terms of parsing accuracy. 1http://nlp.cs.nyu.edu/evalb/ Model F1 (small) F1 (full) CFG 61.9 63.6 *TSG 77.1 85.0 SR-TSG (Psr-tsg) 73.0 86.4 SR-TSG (Psr-tsg, Psr-cfg) 79.4 89.7 SR-TSG (Psr-tsg, Psr-cfg, Pru-cfg) 81.7 91.1 Table 2: Comparison of parsing accuracy with the small and full training sets. *Our reimplementation of (Cohn et al., 2010). Figure 2: Histogram of SR-TSG and TSG rule sizes on the small training set. The size is defined as the number of CFG rules that the elementary tree contains. 5.2 Results and Discussion 5.2.1 Comparison of SR-TSG with TSG We compared the SR-TSG model with the CFG and TSG models as regards parsing accuracy. We also tested our model with three backoff hierarchy settings to evaluate the effects of backoff smoothing on parsing accuracy. Table 2 shows the F1 scores of the CFG, TSG and SR-TSG parsers for small and full training sets. In Table 2, SR-TSG (Psr-tsg) denotes that we used only the topmos</context>
<context position="24388" citStr="Cohn et al., 2010" startWordPosition="4087" endWordPosition="4090">d by the symbol subcategories. 5.2.2 Comparison of SR-TSG with Other Models We compared the accuracy of the SR-TSG parser with that of conventional high-performance parsers. Table 3 shows the F1 scores of an SR-TSG and conventional parsers with the full training set. In Table 3, SR-TSG (single) is a standard SR-TSG parser, 446 and SR-TSG (multiple) is a combination of sixteen independently trained SR-TSG models, following the work of (Petrov, 2010). Our SR-TSG (single) parser achieved an F1 score of 91.1%, which is a 6.4 point improvement over the conventional Bayesian TSG parser reported by (Cohn et al., 2010). Our model can be viewed as an extension of Cohn’s work by the incorporation of symbol refinement. Therefore, this result confirms that a TSG and symbol refinement work complementarily in improving parsing accuracy. Compared with a symbol-refined CFG model such as the Berkeley parser (Petrov et al., 2006), the SR-TSG model can use large tree fragments, which strengthens the probability of frequent syntax patterns in the training set. Indeed, the few very large rules of our model memorized full parse trees of sentences, which were repeated in the training set. The SR-TSG (single) is a pure gen</context>
</contexts>
<marker>Cohn, Blunsom, Goldwater, 2010</marker>
<rawString>Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing Tree-Substitution Grammars. Journal of Machine Learning Research, 11:3053–3096.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing. Computational Linguistics,</title>
<date>2003</date>
<pages>29--589</pages>
<contexts>
<context position="3412" citStr="Collins, 2003" startWordPosition="506" endWordPosition="507">2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for syntactic parsing. F</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-Driven Statistical Models for Natural Language Parsing. Computational Linguistics, 29:589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous Tree Adjoining Machine Translation.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>727</pages>
<contexts>
<context position="1544" citStr="DeNeefe and Knight, 2009" startWordPosition="216" endWordPosition="219">e-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on Markov Chain Monte Carlo (MCMC) sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discriminative reranking parsers. 1 Introduction Syntactic parsing has played a central role in natural language processing. The resulting syntactic analysis can be used for various applications such as machine translation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and question answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003). In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al</context>
</contexts>
<marker>DeNeefe, Knight, 2009</marker>
<rawString>Steve DeNeefe and Kevin Knight. 2009. Synchronous Tree Adjoining Machine Translation. In Proc. of EMNLP, page 727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Ferguson</author>
</authors>
<title>A Bayesian Analysis of Some Nonparametric Problems. Annals of Statistics,</title>
<date>1973</date>
<pages>1--209</pages>
<contexts>
<context position="7540" citStr="Ferguson, 1973" startWordPosition="1204" endWordPosition="1205"> e, and p (e |x) is the probability of generating e given its root symbol x. As in a PCFG, e is generated conditionally independent of all others given x. The posterior distribution over elementary trees given a parse tree t can be computed by using the Bayes’ rule: p(e|t) °c p(t |e) p(e). where p (t |e) is either equal to 1 (when t and e are consistent) or 0 (otherwise). Therefore, the task of TSG induction from parse trees turns out to consist of modeling the prior distribution p (e). Recent work on TSG induction defines p (e) as a nonparametric Bayesian model such as the Dirichlet Process (Ferguson, 1973) or the Pitman-Yor Process to encourage sparse and compact grammars. Several studies have combined TSG induction and symbol refinement. An adaptor grammar (Johnson et al., 2007a) is a sort of nonparametric Bayesian TSG model with symbol refinement, and is thus closely related to our SR-TSG model. However, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts </context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>Thomas S Ferguson. 1973. A Bayesian Analysis of Some Nonparametric Problems. Annals of Statistics, 1:209–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
</authors>
<title>Combining Constituent Parsers.</title>
<date>2009</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>253--256</pages>
<contexts>
<context position="26469" citStr="Fossum and Knight, 2009" startWordPosition="4416" endWordPosition="4419">irst trained sixteen SR-TSG models independently and produced a 100-best list of the derivations for each model. Then, we erased the subcategory information of parse trees and selected the best tree that achieved the highest likelihood under the product of sixteen models. The combination model, SR-TSG (multiple), achieved an F1 score of 92.4%, which is a state-of-the-art result for the WSJ parsing task. Compared with discriminative reranking parsers, combining multiple grammars by using the product model provides the advantage that it does not require any additional training. Several studies (Fossum and Knight, 2009; Zhang et al., 2009) have proposed different approaches that involve combining k-best lists of candidate trees. We will deal with those methods in future work. Let us note the relation between SR-CFG, TSG and SR-TSG. TSG is weakly equivalent to CFG and generates the same set of strings. For example, the TSG rule “S —* (NP NNP) VP” with probability p can be converted to the equivalent CFG rules as follows: “S NPNNP VP ” with probability p and “NPNNP NNP” with probability 1. From this viewpoint, TSG utilizes surrounding symbols (NNP of NPNNP in the above example) as latent variables with which </context>
</contexts>
<marker>Fossum, Knight, 2009</marker>
<rawString>Victoria Fossum and Kevin Knight. 2009. Combining Constituent Parsers. In Proc. of HLT-NAACL, pages 253–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Los Angeles</author>
<author>Marina Del Rey</author>
</authors>
<title>What’s in a Translation Rule? Information Sciences,</title>
<date>2004</date>
<pages>273--280</pages>
<contexts>
<context position="1517" citStr="Galley et al., 2004" startWordPosition="212" endWordPosition="215"> smoothing from a fine-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on Markov Chain Monte Carlo (MCMC) sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discriminative reranking parsers. 1 Introduction Syntactic parsing has played a central role in natural language processing. The resulting syntactic analysis can be used for various applications such as machine translation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and question answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003). In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenba</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, Angeles, Rey, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, Daniel Marcu, Los Angeles, and Marina Del Rey. 2004. What’s in a Translation Rule? Information Sciences, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest Reranking : Discriminative Parsing with Non-Local Features.</title>
<date>2008</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>19104--0</pages>
<contexts>
<context position="22083" citStr="Huang (2008)" startWordPosition="3697" endWordPosition="3698">ls on both the small and large training sets. This result suggests that the conventional TSG model trained from the vanilla treebank is insufficient to resolve 445 Model F1 (&lt; 40) F1 (all) TSG (no symbol refinement) Post and Gildea (2009) 82.6 - Cohn et al. (2010) 85.4 84.7 TSG with Symbol Refinement Zuidema (2007) - *83.8 Bansal et al. (2010) 88.7 88.1 SR-TSG (single) 91.6 91.1 SR-TSG (multiple) 92.9 92.4 CFG with Symbol Refinement Collins (1999) 88.6 88.2 Petrov and Klein (2007) 90.6 90.1 Petrov (2010) - 91.8 Discriminative Carreras et al. (2008) - 91.1 Charniak and Johnson (2005) 92.0 91.4 Huang (2008) 92.3 91.7 Table 3: Our parsing performance for the testing set compared with those of other parsers. *Results for the development set (&lt; 100). structural ambiguities caused by coarse symbol annotations in a training corpus. As we expected, symbol refinement can be helpful with the TSG model for further fitting the training set and improving the parsing accuracy. The performance of the SR-TSG parser was strongly affected by its backoff models. For example, the simplest model, Psr-tsg, performed poorly compared with our best model. This result suggests that the SR-TSG rules extracted from the t</context>
<context position="25200" citStr="Huang, 2008" startWordPosition="4222" endWordPosition="4223">arsing accuracy. Compared with a symbol-refined CFG model such as the Berkeley parser (Petrov et al., 2006), the SR-TSG model can use large tree fragments, which strengthens the probability of frequent syntax patterns in the training set. Indeed, the few very large rules of our model memorized full parse trees of sentences, which were repeated in the training set. The SR-TSG (single) is a pure generative model of syntax trees but it achieved results comparable to those of discriminative parsers. It should be noted that discriminative reranking parsers such as (Charniak and Johnson, 2005) and (Huang, 2008) are constructed on a generative parser. The reranking parser takes the k-best lists of candidate trees or a packed forest produced by a baseline parser (usually a generative model), and then reranks the candidates using arbitrary features. Hence, we can expect that combining our SR-TSG model with a discriminative reranking parser would provide better performance than SR-TSG alone. Recently, (Petrov, 2010) has reported that combining multiple grammars trained independently gives significantly improved performance over a single grammar alone. We applied his method (referred to as a TREE-LEVEL i</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest Reranking : Discriminative Parsing with Non-Local Features. In Proc. of ACL, 19104:0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In</title>
<date>2009</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>317--325</pages>
<contexts>
<context position="8406" citStr="Johnson and Goldwater, 2009" startWordPosition="1339" endWordPosition="1342">efinement, and is thus closely related to our SR-TSG model. However, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as 441 (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with a nonterminal symbol. morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing. An all-fragments grammar (Bansal and Klein, 2010) is another variant of TSG that aims to utilize all possible subtrees as rules. It maps a TSG to an implicit representation to make the grammar tractable and practical for large-scale parsing. The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. As mentioned in the introduction, our model focuses on the automatic learning of a TSG and sy</context>
<context position="16710" citStr="Johnson and Goldwater, 2009" startWordPosition="2791" endWordPosition="2795"> the MH test. See (Cohn et al., 2010) for details. This sampler simultaneously updates blocks of latent variables associated with a sentence, thus it can find MAP solutions efficiently. The tree-level blocked Gibbs sampler focuses on the type of SR-TSG rules and simultaneously updates all root and child nodes that are annotated with the same SR-TSG rule. For example, the sampler collects all nodes that are annotated with S0 —* NP1VP2, then updates those nodes to another subcategory such as S0 —* NP2VP0 according to the posterior distribution. This sampler is similar to table label resampling (Johnson and Goldwater, 2009), but differs in that our sampler can update multiple table labels simultaneously when multiple tables are labeled with the same elementary tree. The tree-level sampler also simultaneously updates blocks of latent variables associated with the type of SR-TSG rules, thus it can find MAP solutions efficiently. 4.2 Inference of Substitution Sites After the inference of symbol subcategories, we use Gibbs sampling to infer the substitution sites of parse trees as described in (Cohn and Lapata, 2009; Post and Gildea, 2009). We assign a binary variable to each internal node in the training data, whic</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In In Proc. of HLT-NAACL, pages 317–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor Grammars : A Framework for Specifying Compositional Nonparametric Bayesian Models.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>19--641</pages>
<contexts>
<context position="7716" citStr="Johnson et al., 2007" startWordPosition="1228" endWordPosition="1231">stribution over elementary trees given a parse tree t can be computed by using the Bayes’ rule: p(e|t) °c p(t |e) p(e). where p (t |e) is either equal to 1 (when t and e are consistent) or 0 (otherwise). Therefore, the task of TSG induction from parse trees turns out to consist of modeling the prior distribution p (e). Recent work on TSG induction defines p (e) as a nonparametric Bayesian model such as the Dirichlet Process (Ferguson, 1973) or the Pitman-Yor Process to encourage sparse and compact grammars. Several studies have combined TSG induction and symbol refinement. An adaptor grammar (Johnson et al., 2007a) is a sort of nonparametric Bayesian TSG model with symbol refinement, and is thus closely related to our SR-TSG model. However, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as 441 (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with </context>
<context position="15737" citStr="Johnson et al., 2007" startWordPosition="2624" endWordPosition="2627"> the newly split symbols as regards that loss are merged to avoid overfitting. The split-merge algorithm terminates when the total number of steps reaches the user-specified value. In each splitting step, we use two types of blocked MCMC algorithm: the sentence-level blocked Metroporil-Hastings (MH) sampler and the treelevel blocked Gibbs sampler, while (Petrov et al., 2006) use a different MLE-based model and the EM algorithm. Our sampler iterates sentence-level sampling and tree-level sampling alternately. The sentence-level MH sampler is a recently proposed algorithm for grammar induction (Johnson et al., 2007b; Cohn et al., 2010). In this work, we apply it to the training of symbol splitting. The MH sampler consists of the following three steps: for each sentence, 1) calculate the inside probability (Lari and Young, 1991) in a bottom-up manner, 2) sample a derivation tree in a top-down manner, and 3) accept or reject the derivation sample by using the MH test. See (Cohn et al., 2010) for details. This sampler simultaneously updates blocks of latent variables associated with a sentence, thus it can find MAP solutions efficiently. The tree-level blocked Gibbs sampler focuses on the type of SR-TSG ru</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007a. Adaptor Grammars : A Framework for Specifying Compositional Nonparametric Bayesian Models. Advances in Neural Information Processing Systems 19, 19:641–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian Inference for PCFGs via Markov chain Monte Carlo. In</title>
<date>2007</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>139--146</pages>
<contexts>
<context position="7716" citStr="Johnson et al., 2007" startWordPosition="1228" endWordPosition="1231">stribution over elementary trees given a parse tree t can be computed by using the Bayes’ rule: p(e|t) °c p(t |e) p(e). where p (t |e) is either equal to 1 (when t and e are consistent) or 0 (otherwise). Therefore, the task of TSG induction from parse trees turns out to consist of modeling the prior distribution p (e). Recent work on TSG induction defines p (e) as a nonparametric Bayesian model such as the Dirichlet Process (Ferguson, 1973) or the Pitman-Yor Process to encourage sparse and compact grammars. Several studies have combined TSG induction and symbol refinement. An adaptor grammar (Johnson et al., 2007a) is a sort of nonparametric Bayesian TSG model with symbol refinement, and is thus closely related to our SR-TSG model. However, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as 441 (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with </context>
<context position="15737" citStr="Johnson et al., 2007" startWordPosition="2624" endWordPosition="2627"> the newly split symbols as regards that loss are merged to avoid overfitting. The split-merge algorithm terminates when the total number of steps reaches the user-specified value. In each splitting step, we use two types of blocked MCMC algorithm: the sentence-level blocked Metroporil-Hastings (MH) sampler and the treelevel blocked Gibbs sampler, while (Petrov et al., 2006) use a different MLE-based model and the EM algorithm. Our sampler iterates sentence-level sampling and tree-level sampling alternately. The sentence-level MH sampler is a recently proposed algorithm for grammar induction (Johnson et al., 2007b; Cohn et al., 2010). In this work, we apply it to the training of symbol splitting. The MH sampler consists of the following three steps: for each sentence, 1) calculate the inside probability (Lari and Young, 1991) in a bottom-up manner, 2) sample a derivation tree in a top-down manner, and 3) accept or reject the derivation sample by using the MH test. See (Cohn et al., 2010) for details. This sampler simultaneously updates blocks of latent variables associated with a sentence, thus it can find MAP solutions efficiently. The tree-level blocked Gibbs sampler focuses on the type of SR-TSG ru</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007b. Bayesian Inference for PCFGs via Markov chain Monte Carlo. In In Proc. of HLT-NAACL, pages 139–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG Models of Linguistic Tree Representations. Computational Linguistics,</title>
<date>1998</date>
<pages>24--613</pages>
<contexts>
<context position="3397" citStr="Johnson, 1998" startWordPosition="504" endWordPosition="505">Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for synta</context>
<context position="19138" citStr="Johnson, 1998" startWordPosition="3197" endWordPosition="3198">g). We also used section 2 as a small training set for evaluating the performance of our model under low-resource conditions. Henceforth, we distinguish the small training set (section 2) from the full training set (sections 2-21). The treebank data is right-binarized (Matsuzaki et al., 2005) to construct grammars with only unary and binary productions. We replace lexical words with count &lt; 5 in the training data with one of 50 unknown words using lexical features, following (Petrov et al., 2006). We also split off all the function tags and eliminated empty nodes from the data set, following (Johnson, 1998). 5.1.2 Training and Parsing For the inference of symbol subcategories, we trained our model with the MCMC sampler by using 6 split-merge steps for the full training set and 3 split-merge steps for the small training set. Therefore, each symbol can be subdivided into a maximum of 26 = 64 and 23 = 8 subcategories, respectively. In each split-merge step, we initialized the sampler by randomly splitting every symbol in two subcategories and ran the MCMC sampler for 1000 iterations. After that, to infer the substitution sites, we initialized the model with the final sample from a run on the small </context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG Models of Linguistic Tree Representations. Computational Linguistics, 24:613– 632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>1--423</pages>
<contexts>
<context position="1943" citStr="Klein and Manning, 2003" startWordPosition="276" endWordPosition="279">ion Syntactic parsing has played a central role in natural language processing. The resulting syntactic analysis can be used for various applications such as machine translation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and question answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003). In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 201</context>
<context position="8812" citStr="Klein and Manning, 2003" startWordPosition="1403" endWordPosition="1406">rse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with a nonterminal symbol. morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing. An all-fragments grammar (Bansal and Klein, 2010) is another variant of TSG that aims to utilize all possible subtrees as rules. It maps a TSG to an implicit representation to make the grammar tractable and practical for large-scale parsing. The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. As mentioned in the introduction, our model focuses on the automatic learning of a TSG and symbol refinement without heuristics. 3 Symbol-Refined Tree Substitution Grammars In this section, we propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. Our SR-TSG model is an extension of the conventional TSG model where every symbol of the elementary trees can be refined to fit the training data. Figure 1c shows an example of SR-TSG derivation. As with previous work on TS</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate Unlexicalized Parsing. In Proc. ofACL, 1:423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>Applications of Stochastic Context-Free Grammars Using the Inside–Outside Algorithm. Computer Speech and Language,</title>
<date>1991</date>
<pages>5--237</pages>
<contexts>
<context position="15954" citStr="Lari and Young, 1991" startWordPosition="2662" endWordPosition="2665">types of blocked MCMC algorithm: the sentence-level blocked Metroporil-Hastings (MH) sampler and the treelevel blocked Gibbs sampler, while (Petrov et al., 2006) use a different MLE-based model and the EM algorithm. Our sampler iterates sentence-level sampling and tree-level sampling alternately. The sentence-level MH sampler is a recently proposed algorithm for grammar induction (Johnson et al., 2007b; Cohn et al., 2010). In this work, we apply it to the training of symbol splitting. The MH sampler consists of the following three steps: for each sentence, 1) calculate the inside probability (Lari and Young, 1991) in a bottom-up manner, 2) sample a derivation tree in a top-down manner, and 3) accept or reject the derivation sample by using the MH test. See (Cohn et al., 2010) for details. This sampler simultaneously updates blocks of latent variables associated with a sentence, thus it can find MAP solutions efficiently. The tree-level blocked Gibbs sampler focuses on the type of SR-TSG rules and simultaneously updates all root and child nodes that are annotated with the same SR-TSG rule. For example, the sampler collects all nodes that are annotated with S0 —* NP1VP2, then updates those nodes to anoth</context>
</contexts>
<marker>Lari, Young, 1991</marker>
<rawString>K Lari and S J Young. 1991. Applications of Stochastic Context-Free Grammars Using the Inside–Outside Algorithm. Computer Speech and Language, 5:237– 257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="18429" citStr="Marcus et al., 1993" startWordPosition="3077" endWordPosition="3080"> generation probabilities according to our model. This heuristics is helpful for finding large tree fragments and learning compact grammars. 4.3 Hyperparameter Estimation We treat hyperparameters {d, 01 as random variables and update their values for every MCMC iteration. We place a prior on the hyperparameters as follows: d — Beta (1, 1), 0 — Gamma (1, 1). The values of d and 0 are optimized with the auxiliary variable technique (Teh, 2006a). 444 5 Experiment 5.1 Settings 5.1.1 Data Preparation We ran experiments on the Wall Street Journal (WSJ) portion of the English Penn Treebank data set (Marcus et al., 1993), using a standard data split (sections 2–21 for training, 22 for development and 23 for testing). We also used section 2 as a small training set for evaluating the performance of our model under low-resource conditions. Henceforth, we distinguish the small training set (section 2) from the full training set (sections 2-21). The treebank data is right-binarized (Matsuzaki et al., 2005) to construct grammars with only unary and binary productions. We replace lexical words with count &lt; 5 in the training data with one of 50 unknown words using lexical features, following (Petrov et al., 2006). We</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>75--82</pages>
<contexts>
<context position="3437" citStr="Matsuzaki et al., 2005" startWordPosition="508" endWordPosition="511">harniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for syntactic parsing. For example, Bansal and Kl</context>
<context position="18817" citStr="Matsuzaki et al., 2005" startWordPosition="3140" endWordPosition="3143"> optimized with the auxiliary variable technique (Teh, 2006a). 444 5 Experiment 5.1 Settings 5.1.1 Data Preparation We ran experiments on the Wall Street Journal (WSJ) portion of the English Penn Treebank data set (Marcus et al., 1993), using a standard data split (sections 2–21 for training, 22 for development and 23 for testing). We also used section 2 as a small training set for evaluating the performance of our model under low-resource conditions. Henceforth, we distinguish the small training set (section 2) from the full training set (sections 2-21). The treebank data is right-binarized (Matsuzaki et al., 2005) to construct grammars with only unary and binary productions. We replace lexical words with count &lt; 5 in the training data with one of 50 unknown words using lexical features, following (Petrov et al., 2006). We also split off all the function tags and eliminated empty nodes from the data set, following (Johnson, 1998). 5.1.2 Training and Parsing For the inference of symbol subcategories, we trained our model with the MCMC sampler by using 6 split-merge steps for the full training set and 3 split-merge steps for the small training set. Therefore, each symbol can be subdivided into a maximum o</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proc. ofACL, pages 75–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>433--440</pages>
<contexts>
<context position="2804" citStr="Petrov et al., 2006" startWordPosition="413" endWordPosition="416">CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser (Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Colli</context>
<context position="14766" citStr="Petrov et al., 2006" startWordPosition="2468" endWordPosition="2471">er latent substitution sites stepwise. During the inference of symbol subcategories, every internal node is fixed as a substitution site. After that, we unfix that assumption and infer latent substitution sites given symbolrefined parse trees. This stepwise learning is simple and efficient in practice, but we believe that the joint learning of both latent variables is possible, and we will deal with this in future work. Here we describe each inference algorithm in detail. 4.1 Inference of Symbol Subcategories For the inference of latent symbol subcategories, we adopt split and merge training (Petrov et al., 2006) as follows. In each split-merge step, each symbol is split into at most two subcategories. For example, every NP symbol in the training data is split into either NP0 or NP1 to maximize the posterior probability. After convergence, we measure the loss of each split symbol in terms of the likelihood incurred when removing it, then the smallest 50% of the newly split symbols as regards that loss are merged to avoid overfitting. The split-merge algorithm terminates when the total number of steps reaches the user-specified value. In each splitting step, we use two types of blocked MCMC algorithm: </context>
<context position="19025" citStr="Petrov et al., 2006" startWordPosition="3175" endWordPosition="3178">et (Marcus et al., 1993), using a standard data split (sections 2–21 for training, 22 for development and 23 for testing). We also used section 2 as a small training set for evaluating the performance of our model under low-resource conditions. Henceforth, we distinguish the small training set (section 2) from the full training set (sections 2-21). The treebank data is right-binarized (Matsuzaki et al., 2005) to construct grammars with only unary and binary productions. We replace lexical words with count &lt; 5 in the training data with one of 50 unknown words using lexical features, following (Petrov et al., 2006). We also split off all the function tags and eliminated empty nodes from the data set, following (Johnson, 1998). 5.1.2 Training and Parsing For the inference of symbol subcategories, we trained our model with the MCMC sampler by using 6 split-merge steps for the full training set and 3 split-merge steps for the small training set. Therefore, each symbol can be subdivided into a maximum of 26 = 64 and 23 = 8 subcategories, respectively. In each split-merge step, we initialized the sampler by randomly splitting every symbol in two subcategories and ran the MCMC sampler for 1000 iterations. Aft</context>
<context position="24695" citStr="Petrov et al., 2006" startWordPosition="4138" endWordPosition="4141">dard SR-TSG parser, 446 and SR-TSG (multiple) is a combination of sixteen independently trained SR-TSG models, following the work of (Petrov, 2010). Our SR-TSG (single) parser achieved an F1 score of 91.1%, which is a 6.4 point improvement over the conventional Bayesian TSG parser reported by (Cohn et al., 2010). Our model can be viewed as an extension of Cohn’s work by the incorporation of symbol refinement. Therefore, this result confirms that a TSG and symbol refinement work complementarily in improving parsing accuracy. Compared with a symbol-refined CFG model such as the Berkeley parser (Petrov et al., 2006), the SR-TSG model can use large tree fragments, which strengthens the probability of frequent syntax patterns in the training set. Indeed, the few very large rules of our model memorized full parse trees of sentences, which were repeated in the training set. The SR-TSG (single) is a pure generative model of syntax trees but it achieved results comparable to those of discriminative parsers. It should be noted that discriminative reranking parsers such as (Charniak and Johnson, 2005) and (Huang, 2008) are constructed on a generative parser. The reranking parser takes the k-best lists of candida</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proc. of ACL, pages 433–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<title>Products of Random Latent Variable Grammars.</title>
<date>2010</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>pages</pages>
<contexts>
<context position="21980" citStr="Petrov (2010)" startWordPosition="3680" endWordPosition="3681">ckoff models. Our best model, SR-TSG (Psr-tsg, Psr-cfg, Pru-cfg), outperformed both the CFG and TSG models on both the small and large training sets. This result suggests that the conventional TSG model trained from the vanilla treebank is insufficient to resolve 445 Model F1 (&lt; 40) F1 (all) TSG (no symbol refinement) Post and Gildea (2009) 82.6 - Cohn et al. (2010) 85.4 84.7 TSG with Symbol Refinement Zuidema (2007) - *83.8 Bansal et al. (2010) 88.7 88.1 SR-TSG (single) 91.6 91.1 SR-TSG (multiple) 92.9 92.4 CFG with Symbol Refinement Collins (1999) 88.6 88.2 Petrov and Klein (2007) 90.6 90.1 Petrov (2010) - 91.8 Discriminative Carreras et al. (2008) - 91.1 Charniak and Johnson (2005) 92.0 91.4 Huang (2008) 92.3 91.7 Table 3: Our parsing performance for the testing set compared with those of other parsers. *Results for the development set (&lt; 100). structural ambiguities caused by coarse symbol annotations in a training corpus. As we expected, symbol refinement can be helpful with the TSG model for further fitting the training set and improving the parsing accuracy. The performance of the SR-TSG parser was strongly affected by its backoff models. For example, the simplest model, Psr-tsg, perform</context>
<context position="24222" citStr="Petrov, 2010" startWordPosition="4061" endWordPosition="4062">re are more SR-TSG rules than TSG rules with size &gt; 2. This shows that an SR-TSG can use various large tree fragments depending on the context, which is specified by the symbol subcategories. 5.2.2 Comparison of SR-TSG with Other Models We compared the accuracy of the SR-TSG parser with that of conventional high-performance parsers. Table 3 shows the F1 scores of an SR-TSG and conventional parsers with the full training set. In Table 3, SR-TSG (single) is a standard SR-TSG parser, 446 and SR-TSG (multiple) is a combination of sixteen independently trained SR-TSG models, following the work of (Petrov, 2010). Our SR-TSG (single) parser achieved an F1 score of 91.1%, which is a 6.4 point improvement over the conventional Bayesian TSG parser reported by (Cohn et al., 2010). Our model can be viewed as an extension of Cohn’s work by the incorporation of symbol refinement. Therefore, this result confirms that a TSG and symbol refinement work complementarily in improving parsing accuracy. Compared with a symbol-refined CFG model such as the Berkeley parser (Petrov et al., 2006), the SR-TSG model can use large tree fragments, which strengthens the probability of frequent syntax patterns in the training </context>
<context position="25609" citStr="Petrov, 2010" startWordPosition="4286" endWordPosition="4287">model of syntax trees but it achieved results comparable to those of discriminative parsers. It should be noted that discriminative reranking parsers such as (Charniak and Johnson, 2005) and (Huang, 2008) are constructed on a generative parser. The reranking parser takes the k-best lists of candidate trees or a packed forest produced by a baseline parser (usually a generative model), and then reranks the candidates using arbitrary features. Hence, we can expect that combining our SR-TSG model with a discriminative reranking parser would provide better performance than SR-TSG alone. Recently, (Petrov, 2010) has reported that combining multiple grammars trained independently gives significantly improved performance over a single grammar alone. We applied his method (referred to as a TREE-LEVEL inference) to the SR-TSG model as follows. We first trained sixteen SR-TSG models independently and produced a 100-best list of the derivations for each model. Then, we erased the subcategory information of parse trees and selected the best tree that achieved the highest likelihood under the product of sixteen models. The combination model, SR-TSG (multiple), achieved an F1 score of 92.4%, which is a state-</context>
</contexts>
<marker>Petrov, 2010</marker>
<rawString>Slav Petrov. 2010. Products of Random Latent Variable Grammars. In Proc. of HLT-NAACL, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
<author>Marc Yor</author>
</authors>
<title>The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. The Annals of Probability,</title>
<date>1997</date>
<pages>25--855</pages>
<contexts>
<context position="4704" citStr="Pitman and Yor, 1997" startWordPosition="702" endWordPosition="705">mbol refinement with heuristics helps improve the accuracy of a TSG parser. In this paper, we propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data. Our work differs from previous studies in that we focus on a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion. We also propose a novel probabilistic SR-TSG model with the hierarchical Pitman-Yor Process (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model, to encode backoff smoothing from a fine-grained SRTSG to simpler CFG rules, and develop an efficient training method based on blocked MCMC sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the WSJ English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and superior to state-of-the-art discriminative reranking parsers. 2 Background and Related Work Our SR-TSG work is built upon recent work on Bayesian TSG induction from parse trees (Post and Gildea, 2009; Cohn et al., 2010). We firs</context>
<context position="9884" citStr="Pitman and Yor, 1997" startWordPosition="1578" endWordPosition="1581"> symbol of the elementary trees can be refined to fit the training data. Figure 1c shows an example of SR-TSG derivation. As with previous work on TSG induction, our task is the induction of SR-TSG derivations from a corpus of parse trees in an unsupervised fashion. That is, we wish to infer the symbol subcategories of every node and substitution site (i.e., nodes where substitution occurs) from parse trees. Extracted rules and their probabilities can be used to parse new raw sentences. 3.1 Probabilistic Model We define a probabilistic model of an SR-TSG based on the Pitman-Yor Process (PYP) (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model. The PYP produces power-law distributions, which have been shown to be well-suited for such uses as language modeling (Teh, 2006b), and TSG induction (Cohn et al., 2010). One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG allows for arbitrarily large tree fragments and also an arbitrarily large set of symbol subcategories. To address the sparseness problem, we employ a hierarchical PYP to encode a backoff scheme from the SRTSG rules to simpler CFG rules, inspired by recent work on</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>Jim Pitman and Marc Yor. 1997. The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. The Annals of Probability, 25:855–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian Learning of a Tree Substitution Grammar. In</title>
<date>2009</date>
<booktitle>In Proc. of ACLIJCNLP,</booktitle>
<pages>45--48</pages>
<contexts>
<context position="2108" citStr="Post and Gildea, 2009" startWordPosition="305" endWordPosition="308">anslation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and question answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003). In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, bu</context>
<context position="5275" citStr="Post and Gildea, 2009" startWordPosition="794" endWordPosition="797">archical Pitman-Yor Process (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model, to encode backoff smoothing from a fine-grained SRTSG to simpler CFG rules, and develop an efficient training method based on blocked MCMC sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the WSJ English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and superior to state-of-the-art discriminative reranking parsers. 2 Background and Related Work Our SR-TSG work is built upon recent work on Bayesian TSG induction from parse trees (Post and Gildea, 2009; Cohn et al., 2010). We firstly review the Bayesian TSG model used in that work, and then present related work on TSGs and symbol refinement. A TSG consists of a 4-tuple, G = (T, N, S, R), where T is a set of terminal symbols, N is a set of nonterminal symbols, S E N is the distinguished start nonterminal symbol and R is a set of productions (a.k.a. rules). The productions take the form of elementary trees i.e., tree fragments of height &gt; 1. The root and internal nodes of the elementary trees are labeled with nonterminal symbols, and leaf nodes are labeled with either terminal or nonterminal </context>
<context position="17232" citStr="Post and Gildea, 2009" startWordPosition="2873" endWordPosition="2876">terior distribution. This sampler is similar to table label resampling (Johnson and Goldwater, 2009), but differs in that our sampler can update multiple table labels simultaneously when multiple tables are labeled with the same elementary tree. The tree-level sampler also simultaneously updates blocks of latent variables associated with the type of SR-TSG rules, thus it can find MAP solutions efficiently. 4.2 Inference of Substitution Sites After the inference of symbol subcategories, we use Gibbs sampling to infer the substitution sites of parse trees as described in (Cohn and Lapata, 2009; Post and Gildea, 2009). We assign a binary variable to each internal node in the training data, which indicates whether that node is a substitution site or not. For each iteration, the Gibbs sampler works by sampling the value of each binary variable in random order. See (Cohn et al., 2010) for details. During the inference, our sampler ignores the symbol subcategories of internal nodes of elementary trees since they do not affect the derivation of the SR-TSG. For example, the elementary trees “(S0 (NP0 NNP0) VP0)” and “(S0 (NP1 NNP0) VP0)” are regarded as being the same when we calculate the generation probabiliti</context>
<context position="21709" citStr="Post and Gildea (2009)" startWordPosition="3632" endWordPosition="3635">cy. Table 2 shows the F1 scores of the CFG, TSG and SR-TSG parsers for small and full training sets. In Table 2, SR-TSG (Psr-tsg) denotes that we used only the topmost level of the hierarchy. Similary, SR-TSG (Psr-tsg, Psr-cfg) denotes that we used only the Psr-tsg and Psr-cfg backoff models. Our best model, SR-TSG (Psr-tsg, Psr-cfg, Pru-cfg), outperformed both the CFG and TSG models on both the small and large training sets. This result suggests that the conventional TSG model trained from the vanilla treebank is insufficient to resolve 445 Model F1 (&lt; 40) F1 (all) TSG (no symbol refinement) Post and Gildea (2009) 82.6 - Cohn et al. (2010) 85.4 84.7 TSG with Symbol Refinement Zuidema (2007) - *83.8 Bansal et al. (2010) 88.7 88.1 SR-TSG (single) 91.6 91.1 SR-TSG (multiple) 92.9 92.4 CFG with Symbol Refinement Collins (1999) 88.6 88.2 Petrov and Klein (2007) 90.6 90.1 Petrov (2010) - 91.8 Discriminative Carreras et al. (2008) - 91.1 Charniak and Johnson (2005) 92.0 91.4 Huang (2008) 92.3 91.7 Table 3: Our parsing performance for the testing set compared with those of other parsers. *Results for the development set (&lt; 100). structural ambiguities caused by coarse symbol annotations in a training corpus. A</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Bayesian Learning of a Tree Substitution Grammar. In In Proc. of ACLIJCNLP, pages 45–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A Bayesian Interpretation of Interpolated Kneser-Ney.</title>
<date>2006</date>
<journal>NUS School of Computing</journal>
<tech>Technical Report TRA2/06.</tech>
<contexts>
<context position="10060" citStr="Teh, 2006" startWordPosition="1607" endWordPosition="1608">of SR-TSG derivations from a corpus of parse trees in an unsupervised fashion. That is, we wish to infer the symbol subcategories of every node and substitution site (i.e., nodes where substitution occurs) from parse trees. Extracted rules and their probabilities can be used to parse new raw sentences. 3.1 Probabilistic Model We define a probabilistic model of an SR-TSG based on the Pitman-Yor Process (PYP) (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model. The PYP produces power-law distributions, which have been shown to be well-suited for such uses as language modeling (Teh, 2006b), and TSG induction (Cohn et al., 2010). One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG allows for arbitrarily large tree fragments and also an arbitrarily large set of symbol subcategories. To address the sparseness problem, we employ a hierarchical PYP to encode a backoff scheme from the SRTSG rules to simpler CFG rules, inspired by recent work on dependency parsing (Blunsom and Cohn, 2010). Our model consists of a three-level hierarchy. Table 1 shows an example of the SR-TSG rule and its backoff tree fragments as an il</context>
<context position="18253" citStr="Teh, 2006" startWordPosition="3050" endWordPosition="3051"> the derivation of the SR-TSG. For example, the elementary trees “(S0 (NP0 NNP0) VP0)” and “(S0 (NP1 NNP0) VP0)” are regarded as being the same when we calculate the generation probabilities according to our model. This heuristics is helpful for finding large tree fragments and learning compact grammars. 4.3 Hyperparameter Estimation We treat hyperparameters {d, 01 as random variables and update their values for every MCMC iteration. We place a prior on the hyperparameters as follows: d — Beta (1, 1), 0 — Gamma (1, 1). The values of d and 0 are optimized with the auxiliary variable technique (Teh, 2006a). 444 5 Experiment 5.1 Settings 5.1.1 Data Preparation We ran experiments on the Wall Street Journal (WSJ) portion of the English Penn Treebank data set (Marcus et al., 1993), using a standard data split (sections 2–21 for training, 22 for development and 23 for testing). We also used section 2 as a small training set for evaluating the performance of our model under low-resource conditions. Henceforth, we distinguish the small training set (section 2) from the full training set (sections 2-21). The treebank data is right-binarized (Matsuzaki et al., 2005) to construct grammars with only una</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006a. A Bayesian Interpretation of Interpolated Kneser-Ney. NUS School of Computing Technical Report TRA2/06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>YW Teh</author>
</authors>
<title>A Hierarchical Bayesian Language Model based on Pitman-Yor Processes.</title>
<date>2006</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>44--985</pages>
<contexts>
<context position="10060" citStr="Teh, 2006" startWordPosition="1607" endWordPosition="1608">of SR-TSG derivations from a corpus of parse trees in an unsupervised fashion. That is, we wish to infer the symbol subcategories of every node and substitution site (i.e., nodes where substitution occurs) from parse trees. Extracted rules and their probabilities can be used to parse new raw sentences. 3.1 Probabilistic Model We define a probabilistic model of an SR-TSG based on the Pitman-Yor Process (PYP) (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model. The PYP produces power-law distributions, which have been shown to be well-suited for such uses as language modeling (Teh, 2006b), and TSG induction (Cohn et al., 2010). One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG allows for arbitrarily large tree fragments and also an arbitrarily large set of symbol subcategories. To address the sparseness problem, we employ a hierarchical PYP to encode a backoff scheme from the SRTSG rules to simpler CFG rules, inspired by recent work on dependency parsing (Blunsom and Cohn, 2010). Our model consists of a three-level hierarchy. Table 1 shows an example of the SR-TSG rule and its backoff tree fragments as an il</context>
<context position="18253" citStr="Teh, 2006" startWordPosition="3050" endWordPosition="3051"> the derivation of the SR-TSG. For example, the elementary trees “(S0 (NP0 NNP0) VP0)” and “(S0 (NP1 NNP0) VP0)” are regarded as being the same when we calculate the generation probabilities according to our model. This heuristics is helpful for finding large tree fragments and learning compact grammars. 4.3 Hyperparameter Estimation We treat hyperparameters {d, 01 as random variables and update their values for every MCMC iteration. We place a prior on the hyperparameters as follows: d — Beta (1, 1), 0 — Gamma (1, 1). The values of d and 0 are optimized with the auxiliary variable technique (Teh, 2006a). 444 5 Experiment 5.1 Settings 5.1.1 Data Preparation We ran experiments on the Wall Street Journal (WSJ) portion of the English Penn Treebank data set (Marcus et al., 1993), using a standard data split (sections 2–21 for training, 22 for development and 23 for testing). We also used section 2 as a small training set for evaluating the performance of our model under low-resource conditions. Henceforth, we distinguish the small training set (section 2) from the full training set (sections 2-21). The treebank data is right-binarized (Matsuzaki et al., 2005) to construct grammars with only una</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>YW Teh. 2006b. A Hierarchical Bayesian Language Model based on Pitman-Yor Processes. In Proc. of ACL, 44:985–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tenenbaum</author>
<author>TJ O’Donnell</author>
<author>ND Goodman</author>
</authors>
<title>Fragment Grammars: Exploring Computation and Reuse</title>
<date>2009</date>
<booktitle>in Language. MIT Computer Science and Artificial Intelligence Laboratory</booktitle>
<tech>Technical Report Series.</tech>
<marker>Tenenbaum, O’Donnell, Goodman, 2009</marker>
<rawString>J Tenenbaum, TJ O’Donnell, and ND Goodman. 2009. Fragment Grammars: Exploring Computation and Reuse in Language. MIT Computer Science and Artificial Intelligence Laboratory Technical Report Series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Noah A Smith</author>
<author>Teruko Mitamura</author>
</authors>
<title>What is the Jeopardy Model ? A QuasiSynchronous Grammar for QA.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLPCoNLL,</booktitle>
<pages>22--32</pages>
<contexts>
<context position="1662" citStr="Wang et al., 2007" startWordPosition="235" endWordPosition="238">ling. Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discriminative reranking parsers. 1 Introduction Syntactic parsing has played a central role in natural language processing. The resulting syntactic analysis can be used for various applications such as machine translation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and question answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003). In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrari</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>Mengqiu Wang, Noah A Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model ? A QuasiSynchronous Grammar for QA. In Proc. of EMNLPCoNLL, pages 22–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elif Yamangil</author>
<author>Stuart M Shieber</author>
</authors>
<title>Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression. In</title>
<date>2010</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>937--947</pages>
<contexts>
<context position="1618" citStr="Yamangil and Shieber, 2010" startWordPosition="227" endWordPosition="230"> method based on Markov Chain Monte Carlo (MCMC) sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discriminative reranking parsers. 1 Introduction Syntactic parsing has played a central role in natural language processing. The resulting syntactic analysis can be used for various applications such as machine translation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and question answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003). In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols c</context>
</contexts>
<marker>Yamangil, Shieber, 2010</marker>
<rawString>Elif Yamangil and Stuart M Shieber. 2010. Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression. In In Proc. ofACL, pages 937–947.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
<author>Haizhou Li</author>
</authors>
<title>K-Best Combination of Syntactic Parsers.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1552--1560</pages>
<contexts>
<context position="26490" citStr="Zhang et al., 2009" startWordPosition="4420" endWordPosition="4423">SG models independently and produced a 100-best list of the derivations for each model. Then, we erased the subcategory information of parse trees and selected the best tree that achieved the highest likelihood under the product of sixteen models. The combination model, SR-TSG (multiple), achieved an F1 score of 92.4%, which is a state-of-the-art result for the WSJ parsing task. Compared with discriminative reranking parsers, combining multiple grammars by using the product model provides the advantage that it does not require any additional training. Several studies (Fossum and Knight, 2009; Zhang et al., 2009) have proposed different approaches that involve combining k-best lists of candidate trees. We will deal with those methods in future work. Let us note the relation between SR-CFG, TSG and SR-TSG. TSG is weakly equivalent to CFG and generates the same set of strings. For example, the TSG rule “S —* (NP NNP) VP” with probability p can be converted to the equivalent CFG rules as follows: “S NPNNP VP ” with probability p and “NPNNP NNP” with probability 1. From this viewpoint, TSG utilizes surrounding symbols (NNP of NPNNP in the above example) as latent variables with which to capture context in</context>
</contexts>
<marker>Zhang, Zhang, Tan, Li, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou Li. 2009. K-Best Combination of Syntactic Parsers. In Proc. of EMNLP, pages 1552–1560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem Zuidema</author>
</authors>
<title>Parsimonious Data-Oriented Parsing.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>551--560</pages>
<contexts>
<context position="3699" citStr="Zuidema, 2007" startWordPosition="552" endWordPosition="553">urthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for syntactic parsing. For example, Bansal and Klein (2010) have reported that deterministic symbol refinement with heuristics helps improve the accuracy of a TSG parser. In this paper, we propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. SR-TSG is an extension of the conventio</context>
<context position="21787" citStr="Zuidema (2007)" startWordPosition="3648" endWordPosition="3649">training sets. In Table 2, SR-TSG (Psr-tsg) denotes that we used only the topmost level of the hierarchy. Similary, SR-TSG (Psr-tsg, Psr-cfg) denotes that we used only the Psr-tsg and Psr-cfg backoff models. Our best model, SR-TSG (Psr-tsg, Psr-cfg, Pru-cfg), outperformed both the CFG and TSG models on both the small and large training sets. This result suggests that the conventional TSG model trained from the vanilla treebank is insufficient to resolve 445 Model F1 (&lt; 40) F1 (all) TSG (no symbol refinement) Post and Gildea (2009) 82.6 - Cohn et al. (2010) 85.4 84.7 TSG with Symbol Refinement Zuidema (2007) - *83.8 Bansal et al. (2010) 88.7 88.1 SR-TSG (single) 91.6 91.1 SR-TSG (multiple) 92.9 92.4 CFG with Symbol Refinement Collins (1999) 88.6 88.2 Petrov and Klein (2007) 90.6 90.1 Petrov (2010) - 91.8 Discriminative Carreras et al. (2008) - 91.1 Charniak and Johnson (2005) 92.0 91.4 Huang (2008) 92.3 91.7 Table 3: Our parsing performance for the testing set compared with those of other parsers. *Results for the development set (&lt; 100). structural ambiguities caused by coarse symbol annotations in a training corpus. As we expected, symbol refinement can be helpful with the TSG model for further</context>
</contexts>
<marker>Zuidema, 2007</marker>
<rawString>Willem Zuidema. 2007. Parsimonious Data-Oriented Parsing. In Proc. of EMNLP-CoNLL, pages 551–560.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>