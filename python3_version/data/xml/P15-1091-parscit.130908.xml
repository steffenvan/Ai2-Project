<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997612">
A Unified Kernel Approach for Learning Typed Sentence Rewritings
</title>
<author confidence="0.971996">
Martin Gleize Brigitte Grau
</author>
<affiliation confidence="0.852485">
LIMSI-CNRS, Orsay, France LIMSI-CNRS, Orsay, France
Universit´e Paris-Sud, Orsay, France ENSIIE, Evry, France
</affiliation>
<email confidence="0.990932">
gleize@limsi.fr bg@limsi.fr
</email>
<sectionHeader confidence="0.99368" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961631578947">
Many high level natural language process-
ing problems can be framed as determin-
ing if two given sentences are a rewrit-
ing of each other. In this paper, we pro-
pose a class of kernel functions, referred
to as type-enriched string rewriting ker-
nels, which, used in kernel-based machine
learning algorithms, allow to learn sen-
tence rewritings. Unlike previous work,
this method can be fed external lexical se-
mantic relations to capture a wider class
of rewriting rules. It also does not assume
preliminary syntactic parsing but is still
able to provide a unified framework to cap-
ture syntactic structure and alignments be-
tween the two sentences. We experiment
on three different natural sentence rewrit-
ing tasks and obtain state-of-the-art results
for all of them.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999803685185185">
Detecting implications of sense between state-
ments stands as one of the most sought-after goals
in computational linguistics. Several high level
tasks look for either one-way rewriting between
single sentences, like recognizing textual entail-
ment (RTE) (Dagan et al., 2006), or two-way
rewritings like paraphrase identification (Dolan et
al., 2004) and semantic textual similarity (Agirre
et al., 2012). In a similar fashion, selecting sen-
tences containing the answer to a question can be
seen as finding the best rewritings of the ques-
tion among answer candidates. These problems
are naturally framed as classification tasks, and
as such most current solutions make use of super-
vised machine learning. They have to tackle sev-
eral challenges: picking an adequate language rep-
resentation, aligning semantically equivalent el-
ements and extracting relevant features to learn
the final decision. Bag-of-words and by extension
bag-of-ngrams are traditionally the most direct ap-
proach and features rely mostly on lexical match-
ing (Wan et al., 2006; Lintean and Rus, 2011;
Jimenez et al., 2013). Moreover, a good solving
method has to account for typically scarce labeled
training data, by enriching its model with lexical
semantic resources like WordNet (Miller, 1995)
to bridge gaps between surface forms (Mihalcea
et al., 2006; Islam and Inkpen, 2009; Yih et al.,
2013). Models based on syntactic trees remain the
typical choice to account for the structure of the
sentences (Heilman and Smith, 2010; Wang and
Manning, 2010; Socher et al., 2011; Calvo et al.,
2014). Usually the best systems manage to com-
bine effectively different methods, like Madnani et
al.’s meta-classifier with machine translation met-
rics (Madnani et al., 2012).
A few methods (Zanzotto et al., 2007; Zanzotto
et al., 2010; Bu et al., 2012) use kernel func-
tions to learn what makes two sentence pairs sim-
ilar. Building on this work, we present a type-
enriched string rewriting kernel giving the oppor-
tunity to specify in a fine-grained way how words
match each other. Unlike previous work, rewrit-
ing rules learned using our framework account for
syntactic structure, term alignments and lexico-
semantic typed variations in a unified approach.
We detail how to efficiently compute our kernel
and lastly experiment on three different high-level
NLP tasks, demonstrating the vast applicability of
our method. Our system based on type-enriched
string rewriting kernels obtains state-of-the-art re-
sults on paraphrase identification and answer sen-
tence selection and outperforms comparable meth-
ods on RTE.
</bodyText>
<sectionHeader confidence="0.9945825" genericHeader="method">
2 Type-Enriched String Rewriting
Kernel
</sectionHeader>
<bodyText confidence="0.989891">
Kernel functions measure the similarity between
two elements. Used in machine learning methods
</bodyText>
<page confidence="0.975388">
939
</page>
<note confidence="0.978163333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 939–949,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.997276333333333">
like SVM, they allow complex decision functions
to be learned in classification tasks (Vapnik, 2000).
The goal of a well-designed kernel function is to
have a high value when computed on two instances
of same label, and a low value for two instances of
different label.
</bodyText>
<subsectionHeader confidence="0.99846">
2.1 String rewriting kernel
</subsectionHeader>
<bodyText confidence="0.997880818181818">
String rewriting kernels (Bu et al., 2012) count
the number of common rewritings between two
pairs of sentences seen as sequences of words.
The rewriting rule (A) in Figure 1 can be viewed
as a kind of phrasal paraphrase with linked vari-
ables (Madnani and Dorr, 2010). Rule (A) rewrites
(B)’s first sentence into its second but it does not
however rewrite the sentences in (C), which is
what we try to fix in this paper.
Following the terminology of string kernels, we
use the term string and character instead of sen-
tence and word. We denote (s, t) E (Σ∗ x Σ∗) an
instance of string rewriting, with a source string
s and a target string t, both finite sequences of
elements in Σ the finite set of characters. Sup-
pose that we are given training data of such in-
stances labeled in {+1, −1}, for paraphrase/non-
paraphrase or entailment/non-entailment in appli-
cations. We can use a kernel method to train on
this data and learn to automatically classify unla-
beled instances. A kernel on string rewriting in-
stances is a map:
</bodyText>
<equation confidence="0.961728">
K : (Σ∗ x Σ∗) x (Σ∗ x Σ∗) —* R
such that for all (s1, t1), (s2, t2) E Σ∗ x Σ∗,
K((s1, t1), (s2, t2)) = (Φ(s1, t1), Φ(s2, t2)) (1)
</equation>
<bodyText confidence="0.999978545454545">
where Φ maps each instance into a high dimen-
sion feature space. Kernels allow us to avoid the
potentially expensive explicit representation of Φ
through the inner product space they define. The
purpose of the string rewriting kernels is to mea-
sure the similarity between two pairs of strings in
term of the number of rewriting rules of a set R
that they share. Φ is thus naturally defined by
Φ(s, t) = (φr(s, t))r∈R with φr(s, t) = n the
number of contiguous substring pairs of (s, t) that
rewriting rule r matches.
</bodyText>
<subsectionHeader confidence="0.999278">
2.2 Typed rewriting rules
</subsectionHeader>
<bodyText confidence="0.95712825">
Let the wildcard domain D C Σ∗ be the set of
strings which can be replaced by wildcards. We
now present the formal framework of the type-
enriched string rewriting kernels.
Let Γp be the set of pattern types and Γv the set of
variable types.
To a type -yp E Γp, we associate the typing relation
γp
</bodyText>
<equation confidence="0.91890375">
� C Σ x Σ.
To a type -yv E Γv,we associate the typing relation
❀ C D x D.
γv
</equation>
<bodyText confidence="0.998428333333333">
Together with the typing relations, we call the as-
sociation of Γp and Γv the typing scheme of the
kernel. Let Σp be defined as
</bodyText>
<equation confidence="0.9736195">
�Σp = {[a|b]  |]a, b E Σ, a γ� b} (2)
γ∈r
</equation>
<bodyText confidence="0.9990654">
We finally define typed rewriting rules. A typed
rewriting rule is a triple r = (0s, 0t, T), where
0s, 0t E (Σp U {*})∗ denote source and target
string typed patterns and T C ind∗(0s)xind∗(0t)
denotes the alignments between the wildcards in
the two string patterns. Here ind∗(0) denotes the
set of indices of wildcards in 0.
We say that a rewriting rule (0s, 0t, T) matches a
pair of strings (s, t) if and only if the following
conditions are true:
</bodyText>
<listItem confidence="0.892722818181818">
• string patterns 0s, resp. 0t, can be turned into
s, resp. t, by:
– substituting each element [a|b] of Σp in
the string pattern with an a or b (E Σ)
– substituting each wildcard in the string
pattern with an element of the wildcard
domain D
• V(i, j) E T, s, resp. t, substitutes the wild-
cards at index i, resp. j, by s∗ E D, resp. t∗,
such that there exists a variable type -y E Γv
γ
</listItem>
<equation confidence="0.580745">
with s∗ ❀ t∗.
</equation>
<bodyText confidence="0.9999492">
A type-enriched string rewriting kernel (TESRK)
is simply a string rewriting kernel as defined in
Equation 1 but with R a set of typed rewriting
rules. This class of kernels depends on wildcard
domain D and the typed rewriting rules R which
can be tuned to allow for more flexibility in the
matching of pairs of characters in a rewriting rule.
Within this framework, the k-gram bijective string
rewriting kernel (kb-SRK) is defined by the wild-
card domain D = Σ and the ruleset
</bodyText>
<equation confidence="0.862868">
R = {(0s, 0t, T)  |0s, 0t E (ΣpU{*})k, T bijective}
id
</equation>
<construct confidence="0.5670845">
under Γp = Γv = {id} with a � b, resp. a ❀id b,
if and only if a = b.
</construct>
<page confidence="0.987569">
940
</page>
<figure confidence="0.998053714285714">
Mary was shouting.
I caught him snoring.
He was sleeping.
I heard Mary shouting.
heard
was
(A) (B) (C)
</figure>
<figureCaption confidence="0.999959">
Figure 1: Rewriting rule (A) matches pair of strings (B) but does not match (C).
</figureCaption>
<bodyText confidence="0.994482642857143">
We now present an example of how kb-SRK is
applied to real pairs of sentences, what its limita-
tions are and how we can deal with them by re-
working its typing scheme. Let us consider again
Figure 1, (A) is a rewriting rule with βs = (heard,
*, *), βt = (*, was, *), τ = {(2,1); (3,3)}. Each
string pattern has the same length, and pairs of
wildcards in the two patterns are aligned bijec-
tively. This is a valid rule for kb-SRK. It matches
the pair of strings (B): each aligned pair of wild-
cards is substituted in source and target sentences
by the same word and string patterns of (A) can in-
deed be turned into pairs of substrings of the sen-
tences. However, it cannot match the pair of sen-
</bodyText>
<equation confidence="0.605484">
tences (C) in the original kb-SRK. We change Fp
hypernym
to {hypernym, id} where a R b if and only
</equation>
<bodyText confidence="0.994167875">
if a and b have a common hypernym in WordNet.
And we change F, to F, = {same pronoun, en-
tailment, id} where a same pronoun ❀b if and only if
a and b are a pronoun of the same person and same
number, and a entailment ❀b if and only if verb a has
a relation of entailment with b in WordNet.
By redefining the typing scheme, rule (A) can now
match (C).
</bodyText>
<sectionHeader confidence="0.970593" genericHeader="method">
3 Computing TESRK
</sectionHeader>
<subsectionHeader confidence="0.988658">
3.1 Formulation
</subsectionHeader>
<bodyText confidence="0.975887466666667">
The k-gram bijective string rewriting kernel can be
computed efficiently (Bu et al., 2012). We show
that we can compute its type-enriched equivalent
at the price of a seemingly insurmountable loosen-
ing of theoretical complexity boundaries. Experi-
ments however show that its computing time is of
the same order as the original kernel.
A type-enriched kb-SRK is parameterized by k the
length of k-grams, and its typing scheme the sets
Fp and F, and their associated relations. The an-
notations of Fp and F, to Kk and ¯Kk will be omit-
ted for clarity and because they typically will not
change while we test different values for k.
We rewrite the inner product in Equation 1 to bet-
ter fit the k-gram framework:
</bodyText>
<equation confidence="0.9980028">
Kk((s1, t1), (s2, t2))
Kk((αs1, αt1), (αs2, αt2))
αs2 Ek-grams(s2)
αt2 Ek-grams(t2)
(3)
</equation>
<bodyText confidence="0.998855">
where Kk is the number of different rewriting
rules which match two pairs of k-grams (the same
rule cannot trigger twice in k-gram substrings):
</bodyText>
<equation confidence="0.998023333333333">
¯Kk((αs1, αt1), (αs2, αt2))
�= 1r(αs1, αt1)1r(αs2, αt2) (4)
r∈R
</equation>
<bodyText confidence="0.991699285714286">
with Ir the indicator function of rule r: 1 if r
matches the pair of k-grams, 0 otherwise.
Computing Kk as defined in Equation 3 is obvi-
ously intractable. There is O((n − k + 1)4) terms
in the sum, where n is the length of the longest
string, and each term involves enumerating every
rewriting rule in R.
</bodyText>
<subsectionHeader confidence="0.999785">
3.2 Computing ¯Kk in type-enriched kb-SRK
</subsectionHeader>
<bodyText confidence="0.999587470588235">
Enumerating all rewriting rules in Equation 4 is
itself intractable: there are more than JΣJ2k rules
without wildcards, where JΣJ is conceivably the
size of a typical lexicon. In fact, we just have
to constructively generate the rules which substi-
tute their string patterns correctly to simultane-
ously produce both pairs of k-grams (αs1, αt1) and
(αs2, αt2).
Let the operator ® be such that α1 ® α2 =
((α1[1], α2[1]), ..., (α1[k], α2[k])). This operation
is generally known as zipping in functional pro-
gramming. We use the function CountPerfect-
Matchings computed by Algorithm 1 to recur-
sively count the number of rewriting rules match-
ing both (αs1, αt1) and (αs2, αt2). The workings
of the algorithm will make clearer why we can
compute ¯Kk with the following formula:
</bodyText>
<equation confidence="0.995929666666667">
¯Kk((αs1, αt1), (αs2, αt2))
= CountPerfectMatchings(αs1 ® αs2, αt1 ® αt2)
(5)
�=
αs1 Ek-grams(s1)
αt1 Ek-grams(t1)
</equation>
<page confidence="0.945202">
941
</page>
<bodyText confidence="0.999869470588235">
Algorithm 1 takes as input remaining character
pairs in αs1 ® αs2 and αt1 ® αt2, and outputs the
number of ways they can substitute aligned wild-
cards in a matching rule.
First (lines 2 and 3) we have the base case where
both remaining sets are empty. There is exactly 1
way the empty set’s wildcards can be aligned with
each other: nothing is aligned. In lines 4 to 9, there
is no source pairs anymore, so the algorithm con-
tinues to deplete target pairs as long as they have
a common pattern type, i.e. as long as they do
not have to substitute a wildcard. If a candidate
wildcard is found, as the opposing set is empty,
we cannot align it and we return 0. In the general
case (lines 11 to 19), consider the first character
pair (a1, a2) in the reminder of αs1 ® αs2 in line
12. What follows in the computation depends on
its types. Every character pair in αt1 ® αt2 that
can be paired through variable types with (a1, a2)
(lines 15 to 19) is a new potential wildcard align-
ment, so we try all the possible alignment and re-
cursively continue the computation after removing
both aligned pairs. And if (a1, a2) does not need to
substitute a wildcard because it has common pat-
tern types (lines 13 and 14), we can choose to not
create any wildcard pairing with it and ignore it in
the recursive call.
This algorithm enumerates all configurations such
that each character pair has a common pattern type
or is matched 1-for-1 with a character pair with
common variable types, which is exactly the defi-
nition of a rewriting rule in TESRK.
This problem is actually equivalent to count-
ing the perfect matchings of the bipartite graph
of potential wildcards. It has been shown in-
tractable (Valiant, 1979) and Algorithm 1 is a
naive recursive algorithm to solve it. In our im-
plementation we represent the graph with its bi-
adjacency matrix, and if our typing relations are
independent of k, the function has a O(k) time
complexity without including its recursive calls.
The number of recursive calls can be greater than
k!2 which is the number of perfect matchings in a
complete bipartite graph of 2k vertices. In our ex-
periments on linguistic data however, we observed
a linear number of recursive calls for low values
of k, and up to a quadratic number for k &gt; 10
–which is way past the point where the kernel be-
comes ineffective.
As an example, Figure 2 shows the zipped k-
grams for source and target as a bipartite graph
</bodyText>
<listItem confidence="0.925279666666667">
Algorithm 1: Counting perfect matchings
1 CountPerfectMatchings(remS, remT)
Data: remS: remaining char. pairs in source
remT: remaining char. pairs in target
graph: αs1 ® αs2 and αt1 ® αt2 as a bipartite
graph, not added in the arguments to avoid
cluttering the recursive calls
ruleSet: Fp and Fv
Result: Number of rewriting rules matching
</listItem>
<equation confidence="0.957227222222222">
(αs1, αt1) and (αs2, αt2)
2 if remS == 0 and remT == 0 then
3 return 1;
4 else if remS == 0 then
(b1, b2) = remT.first();
γ
if 1&apos;y E Fp  |b1 i b2 then
return CountPerfectMatchings(0,
remT - {(b1, b2)});
</equation>
<figureCaption confidence="0.977936">
Figure 2: Bipartite graph of character pairs, with
edges between potential wildcards
</figureCaption>
<bodyText confidence="0.999698">
with 2k vertices and potential wildcard edges. As-
suming that vertices (a, a) and (b, b&apos;) have com-
mon pattern types, they can be ignored as in lines
7 and 14. (c1, c2) to (f1, f2) however must substi-
tute wildcards in a matching rewriting rule. If we
align (c1, c2) with (e1, e2) in line 16, the recur-
sive call will return 0 because the other two pairs
cannot be aligned. A valid rule is generated if c’s
are paired with f’s and d’s with e’s. This kind of
choices is the main source of computational cost.
</bodyText>
<equation confidence="0.979200714285714">
);
10 else
result = 0;
(a1, a2) = remS.first();
γ
if 1&apos;y E Fp  |a1 i a2 then
res += CountPerfectMatchings(remS -
{(a1, a2)}, remT);
for (b1, b2) E remT
γ γ
 |1&apos;y E Fv  |a1❀ b1 and a2 ❀ b2 do
res += CountPerfectMatchings(
remS - {(a1, a2)},
remT - {(b1, b2)}
</equation>
<page confidence="0.649267">
11
</page>
<figure confidence="0.933641041666667">
12
13
14
15
16
17
18
19
20
5
6
7
8
9
else
return 0;
942
A, we have 1AUB(x) = max(1A(x), 1B(x)) and
1AnB(x) = min(1A(x), 1B(x)).
This problem did not arise in the original kb-SRK
because of the transitivity of its only type (iden-
tity). In type-enriched kb-SRK, wildcard pairing
is less constrained.
Algorithm 2: Computing a set including all
</figure>
<subsectionHeader confidence="0.955205">
3.3 Computing Kk
</subsectionHeader>
<bodyText confidence="0.99324">
Even with an efficient method for computing ¯Kk,
implementing Kk directly by applying Equation 3
remains impractical. The main idea is to effi-
ciently compute a reasonably sized set C of el-
ements ((αs1, αt1), (αs2, αt2)) which has the es-
sential property of including all elements such that
¯Kk((αs1, αt1), (αs2, αt2)) =� 0.
By definition of C, we can compute efficiently
</bodyText>
<equation confidence="0.998477">
Kk((s1,t1),(s2,t2))
�= ¯Kk((αs1,αt1),(αs2, αt2)) (6)
((α31,α32),(αt1,αt2))EC
</equation>
<bodyText confidence="0.994010428571429">
There are a number of ways to do it, with a
trade-off between computation time and num-
ber of elements in the reduced domain C.
The main idea of our own algorithm is that
¯Kk((αs1, αt1), (αs2, αt2)) = 0 if the character
pairs (a1, a2) E αs1 ® αs2 with no common pat-
tern type are not all matched with pairs (b1, b2) E
</bodyText>
<equation confidence="0.9363975">
γ γ
αt1 ®αt2 such that a1 ❀ b1 and a2 ❀ b2 for some
</equation>
<bodyText confidence="0.99919436">
γ E Fv. This is conversely true for character pairs
in αt1 ® αt2 with no common pattern type. More
simply, character pairs with no common pattern
type are mismatched and have to substitute a wild-
card in a rewriting rule matching both (αs1, αt1)
and (αs2, αt2). But introducing a wildcard on one
side of the rule means that there is a matching
wildcard on the other side, so we can eliminate
k-gram quadruples that do not fill this wildcard
inclusion. This filtering can be done efficiently
and yields a manageable number of quadruples on
which to compute ¯Kk.
Algorithm 2 computes a set C to be used in
Equation 6 for computing the final value of kernel
Kk. In our experiments, it efficiently produces a
reasonable number of inputs. All maps in the algo-
rithm are maps to multisets, and multisets are used
extensively throughout. Multisets are an extension
of sets where elements can appear multiple times,
the number of times being called the multiplicity.
Typically implemented as hash tables from set
elements to integers, they allow for constant-time
retrieval of the number of a given element. Union
(U) and intersection (n) have special definitions
on multisets. If 1A(x) is the multiplicity of x in
</bodyText>
<table confidence="0.7578416">
elements on which Kk =� 0
Data: s1, t1, s2, t2 strings, and k an integer
Result: Set C which include all inputs such
that ¯Kk =� 0
1 Initialize maps eis,t and maps eit,s, for
</table>
<equation confidence="0.919350576923077">
i E 11, 21;
2 for i E 11, 21 do
for aEsi,bEti  |a❀ γ b,γ EFvdo
eis,t[a] += (b, γ); eit,s[b] += (a, γ);
5 ws,t, aPt =
OneWayInclusion(s1, s2, t1, t2, e1s,t, e2s,t);
6 wt,s, aPs =
OneWayInclusion(t1, t2, s1, s2, e1t,s, e2t,s);
7 Initialize multiset res;
8 for (αs1, αs2) E aPs do
for (αt1, αt2) E aPt do
res += ((αs1, αs2), (αt1, αt2));
11 res = res Uws,t U wt,s.map(swap);
12 return res;
13
14 OneWayInclusion(s1, s2, t1, t2, e1, e2)
Initialize map d multisets resWildcards,
resAllPatterns;
15 for (αs1, αs2) E kgrams(s1) x kgrams(s2) do
for (b1, b2)  |1γ E Fv, (a1, a2) E
αs1 ® αs2, (bi,γ) E ei[ai] bi E 11, 21 do
d[(b1, b2)] += (αs1, αs2);
18 for (αt1, αt2) E kgrams(t1) x kgrams(t2) do
γ
for (b1, b2) E αt1 ® αt2  |b1 =� b2bγ E Fp
do
</equation>
<bodyText confidence="0.628827333333333">
if compatWkgrms not initialized then
Initialize multiset compatWkgrms
= d[(b1, b2)];
</bodyText>
<equation confidence="0.982711">
compatWkgrms = compatWkgrms
n d[(b1, b2)];
</equation>
<bodyText confidence="0.899012375">
if compatWkgrms not initialized then
resAllPatterns += (αt1, αt2);
for (αs1, αs2) E compatWkgrms do
resWildcards+=((αs1, αs2), (αt1, αt2));
27 return (resWildcards, resAllPatterns);
Let us now comment on how the algorithm un-
folds. In lines 1 to 4, we index characters in source
strings by characters in target strings which have
</bodyText>
<figure confidence="0.967801">
3
4
9
10
16
17
19
20
21
22
23
24
25
26
</figure>
<page confidence="0.996512">
943
</page>
<bodyText confidence="0.999966888888889">
common variable types, and vice versa. It allows
in lines 15 to 19 to quickly map a character pair to
the set of opposing k-gram pairs with a matching
–in the sense of variable types– character pair, i.e.
potential aligned wildcards. In lines 20 to 28 we
keep only the k-gram quadruples whose wildcard
candidates (character pairs with no common pat-
tern) from one side all find matches on the other
side. We do not check for the other inclusion,
hence the name of the function OneWayInclusion.
At line 26, we did not find any character pair with
no common pattern, so we save the k-gram pair as
”all-pattern”. All-pattern k-grams will be paired
in lines 8 to 10 in the result. Finally, in line 11,
we add the union of one-way compatible k-gram
quadruples; calling swap on all the pairs of one
set is necessary to consistently have sources on the
left side and targets on the right side in the result.
</bodyText>
<sectionHeader confidence="0.999688" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.902869">
4.1 Systems
</subsectionHeader>
<bodyText confidence="0.999983863636364">
We experimented on three tasks: paraphrase iden-
tification, recognizing textual entailment and an-
swer sentence selection. The setup we used for all
experiments was the same save for the few param-
eters we explored such as: k, and typing scheme.
We implemented 2 kernels, kb-SRK, henceforth
simply denoted SRK, and the type-enriched kb-
SRK, denoted TESRK. All sentences were tok-
enized and POS-tagged using OpenNLP (Mor-
ton et al., 2005). Then they were stemmed us-
ing the Porter stemmer (Porter, 2001) in the case
of SRK. Various other pre-processing steps were
applied in the case of TESRK: they are consid-
ered as types in the model and are detailed in Ta-
ble 1. We used LIBSVM (Chang and Lin, 2011)
to train a binary SVM classifier on the training
data with our two kernels. The default SVM al-
gorithm in LIBSVM uses a parameter C, roughly
akin to a regularization parameter. We 10-fold
cross-validated this parameter on the training data,
optimizing with a grid search for f-score, or MRR
for question-answering. All kernels were normal-
</bodyText>
<equation confidence="0.425976">
ized using ˜K(x, y) = K(x,y) We de-
√ K(x,x) √K(y,y)
</equation>
<bodyText confidence="0.999788722222222">
note by ”+” a sum of kernels, with normalizations
applied both before and after summing. Follow-
ing Bu et al. (Bu et al., 2012) experimental setup,
we introduced an auxiliary vector kernel denoted
PR of features named unigram precision and re-
call, defined in (Wan et al., 2006). In our experi-
ments a linear kernel seemed to yield the best re-
sults. Our Scala implementation of kb-SRKs has
an average throughput of about 1500 original kb-
SRK computations per second, versus 500 type-
enriched kb-SRK computations per second on a 8-
core machine. It typically takes a few hours on
a 32-core machine to train, cross-validate and test
on a full dataset.
Finally, Table 1 presents an overview of our types
with how they are defined and implemented. Ev-
ery type can be used both as a pattern type or
as a variable type, but the two roles are differ-
ent. Pattern types are useful to unify different sur-
face forms of rewriting rules that are semantically
equivalent, i.e. having semantically similar pat-
terns. Variable types are useful for when the se-
mantic relation between 2 entities across the same
rewriting is more important than the entities them-
selves. That is why some types in Table 1 are in-
herently more fitted to be used for one role rather
than the other. For example, it is unlikely that
replacing a word in a pattern of a rewriting rule
by one of its holonyms will yield a semantically
similar rewriting rule, so holonym would not be a
good pattern type for most applications. On the
contrary, it can be very useful in a rewriting rule
to type a wildcard link with the relation holonym,
as this provides constrained semantic roles to the
linked wildcards in the rule, thus holonym would
be a good variable type.
</bodyText>
<subsectionHeader confidence="0.988379">
4.2 Paraphrase identification
</subsectionHeader>
<bodyText confidence="0.9994563">
Paraphrase identification asks whether two sen-
tences have the same meaning. The dataset we
used to evaluate our systems is the MSR Para-
phrase Corpus (Dolan and Brockett, 2005), con-
taining 4,076 training pairs of sentences and 1,725
testing pairs. For example, the sentences ”An in-
jured woman co-worker also was hospitalized and
was listed in good condition.” and ”A woman was
listed in good condition at Memorial’s HealthPark
campus, he said.” are paraphrases in this corpus.
On the other hand, ”’There are a number of lo-
cations in our community, which are essentially
vulnerable,’ Mr Ruddock said.” and ”’There are
a range of risks which are being seriously exam-
ined by competent authorities,’ Mr Ruddock said.”
are not paraphrases.
We report in Table 2 our best results, the sys-
tem TESRK + PR, defined by the sum of PR and
typed-enriched kb-SRKs with k from 1 to 4, with
types Γp = Γv = {stem, synonym}. We observe
</bodyText>
<page confidence="0.977266">
944
</page>
<bodyText confidence="0.9951626">
Type Typing relation on words (a, b) Tool/resources
id words have same surface form and tag OpenNLP tagger
idMinusTag words have same surface form OpenNLP tokenizer
lemma words have same lemma WordNetStemmer
stem words have same stem Porter stemmer
synonym, antonym words are [type] WordNet
hypernym, hyponym b is a [type] of a WordNet
entailment, holonym a and b are both tagged with the same Named Entity BBN Identifinder
ne
lvhsn words are at edit distance of 1 Levenshtein distance
</bodyText>
<tableCaption confidence="0.999037">
Table 1: Types
</tableCaption>
<table confidence="0.9278561">
Paraphrase system Accuracy F-score
All paraphrase 66.5 79.9
Wan et al. (2006) 75.6 83.0
Bu et al. (2012) 76.3 N/A
Socher et al. (2011) 76.8 83.6
Madnani et al. (2012) 77.4 84.1
PR 73.5 82.1
SRK + PR 76.2 83.6
TESRK 76.6 83.7
TESRK + PR 77.2 84.0
</table>
<figure confidence="0.974515875">
logk(#recursive calls)
2.2
1.8
1.6
1.4
1.2
2
1
</figure>
<tableCaption confidence="0.98756">
Table 2: Evaluation results on MSR Paraphrase
</tableCaption>
<bodyText confidence="0.99983356">
that our results are state-of-the-art and in particu-
lar, they improve on the orignal kb-SRK by a good
margin. We tried other combinations of types but
it did not yield good results, this is probably due to
the nature of the MSR corpus, which did not con-
tain much more advanced variations from Word-
Net. The only statistically significant improve-
ment we obtained was between TESRK + PR and
our PR baseline (p &lt; 0.05). The performances
obtained by all the cited systems and ours are not
significantly different in any statistical sense. We
made a special effort to try to reproduce as best as
we could the original kb-SRK performances (Bu et
al., 2012), although our implementation and theirs
should theoretically be equivalent.
Figure 3 plots the average number of recursive
calls to CountPerfectMatchings (algorithm 1) dur-
ing a kernel computation, as a function of k. Com-
posing with logk, we can observe whether the em-
piric number of recursive calls is closer to O(k) or
O(k2). We conclude that this element of complex-
ity is linear for low values of k, but tends to ex-
plode past k = 7. Thankfully, counting common
rewriting rules on pairs of 7-to-10-grams rarely
yields non-zero results, so in practice using high
</bodyText>
<figure confidence="0.749519">
0 2 4 6 8 10
k
</figure>
<figureCaption confidence="0.9958475">
Figure 3: Evolution of the number of recursive
calls to CountPerfectMatchings with k
</figureCaption>
<figure confidence="0.583171">
2 4 6 8 10
k
</figure>
<figureCaption confidence="0.999814">
Figure 4: Evolution of the size of C with k
</figureCaption>
<bodyText confidence="0.8578458">
values of k is not interesting.
Figure 4 plots the average size of set C computed
by algorithm 2, as a function of k (divided by
the sum of lengths of the 4 sentences involved in
the kernel computation). We can observe that this
</bodyText>
<figure confidence="0.98351075">
ICI
Σsentence lengths
0.5
2.5
1.5
0
2
1
</figure>
<page confidence="0.9898">
945
</page>
<table confidence="0.980102636363636">
RTE system Accuracy
All entailments 51.2
Heilman and Smith (2010) 62.8
Bu et al. (2012) 65.1
Zanzotto et al. (2007) 65.8
Hickl et al. (2006) 80.0
PR 61.8
TESRK (All) 62.1
SRK + PR 63.8
TESRK (Syn) + PR 64.1
TESRK (All) + PR 66.1
</table>
<tableCaption confidence="0.999778">
Table 3: Evaluation results on RTE-3
</tableCaption>
<bodyText confidence="0.996933333333333">
quantity is small, except for a peak at low values of
k, which is not an issue because the computation
of ¯Kk is very fast for those values of k.
</bodyText>
<subsectionHeader confidence="0.99975">
4.3 Recognizing textual entailment
</subsectionHeader>
<bodyText confidence="0.999840264150944">
Recognizing Textual Entailment asks whether the
meaning of a sentence hypothesis can be inferred
by reading a sentence text. The dataset we used
to evaluate our systems is RTE-3. Following sim-
ilar work (Heilman and Smith, 2010; Bu et al.,
2012), we took as training data (text, hypothe-
sis) pairs from RTE-1 and RTE-2’s whole datasets
and from RTE-3’s training data, which amounts to
3,767 sentence pairs. We tested on RTE-3 test-
ing data containing 800 sentence pairs. For ex-
ample, a valid textual entailment in this dataset is
the pair of sentences ”In a move widely viewed
as surprising, the Bank of England raised UK in-
terest rates from 5% to 5.25%, the highest in five
years.” and ”UK interest rates went up from 5% to
5.25%.”: the first entails the second. On the other
hand, the pair ”Former French president General
Charles de Gaulle died in November. More than
6,000 people attended a requiem mass for him at
Notre Dame cathedral in Paris.” and ”Charles de
Gaulle died in 1970.” does not constitute a textual
entailment.
We report in Table 3 our best results, the sys-
tem TESRK (All) + PR, defined by the sum of
PR, 1b-SRK and typed-enriched kb-SRKs with k
from 2 to 4, with types Γp = {stem, synonym}
and Γv = {stem, synonym, hypernym, hyponym,
entailment, holonym}. Our results are to be com-
pared with systems using techniques and resources
of similar nature, but as reference the top perfor-
mance at RTE-3 is still reported. This time we did
not manage to fully reproduce Bu et al. 2012’s
performance, but we observe that type-enriched
kb-SRK greatly improves upon our original imple-
mentation of kb-SRK and outperforms their sys-
tem anyway. Combining TESRK and the PR base-
line yields significantly better results than either
one alone (p &lt; 0.05), and performs significantly
better than the system of (Heilman and Smith,
2010), the only one which was evaluated on the
same three tasks as us (p &lt; 0.10). We tried
with less types in our system TESRK (Syn) + PR
by removing all WordNet types but synonyms but
got lower performance. This seems to indicate
that rich types indeed help capturing more com-
plex sentence rewritings. Note that we needed for
k = 1 to replace the type-enriched kb-SRK by the
original kernel in the sum, otherwise the perfor-
mance dropped significantly. Our conclusion is
that including richer types is only beneficial if they
are captured within a context of a couple of words
and that including all those variations on unigrams
only add noise.
</bodyText>
<subsectionHeader confidence="0.99969">
4.4 Answer sentence selection
</subsectionHeader>
<bodyText confidence="0.988716888888889">
Answer sentence selection is the problem of se-
lecting among single candidate sentences the ones
containing the correct answer to an open-domain
factoid question. The dataset we used to evalu-
ate our system on this task was created by (Wang
et al., 2007) based on the QA track of past Text
REtrieval Conferences (TREC-QA)1. The train-
ing set contains 4718 question/answer pairs, for
94 questions, originating from TREC 8 to 12.
The testing set contains 1517 pairs for 89 ques-
tions. As an example, a correct answer to the
question ”What do practitioners of Wicca wor-
ship?” is ”An estimated 50,000 Americans prac-
tice Wicca, a form ofpolytheistic nature worship.”
On the other hand, the answer candidate ”When
people think of Wicca, they think of either Sa-
tanism or silly mumbo jumbo.” is incorrect. Sen-
tences with more than 40 words and questions with
only positive or only negative answers were fil-
tered out (Yao et al., 2013). The average frac-
tion of correct answers per question is 7.4% for
training and 18.7% for testing. Performances are
evaluated as for a re-ranking problem, in term of
Mean Average Precision (MAP) and Mean Re-
ciprocal Rank (MRR). We report our results in
Table 4. We evaluated several combinations of
features. IDF word-count (IDF) is a baseline of
</bodyText>
<footnote confidence="0.9988765">
1Available at http://nlp.stanford.edu/
mengqiu/data/qg-emnlp07-data.tgz
</footnote>
<page confidence="0.989242">
946
</page>
<table confidence="0.999618642857143">
System MAP MRR
Random baseline 0.397 0.493
Wang et al. (2007) 0.603 0.685
Heilman and Smith (2010) 0.609 0.692
Wang and Manning (2010) 0.595 0.695
Yao et al. (2013) 0.631 0.748
Yih et al. (2013) LCLR 0.709 0.770
IDF word-count (IDF) 0.596 0.650
SRK 0.609 0.669
SRK + IDF 0.620 0.677
TESRK (WN) 0.642 0.725
TESRK (WN+NE) 0.656 0.744
TESRK (WN) + IDF 0.678 0.759
TESRK (WN+NE) + IDF 0.672 0.768
</table>
<tableCaption confidence="0.999858">
Table 4: Evaluation results on QA
</tableCaption>
<bodyText confidence="0.999922521739131">
IDF-weighted common word counting, integrated
in a linear kernel. Then we implemented SRK
and TESRK (with k from 1 to 5) with two typing
schemes: WN stands for Fp _ {stem, synonym}
and Fv _ {stem, synonym, hypernym, hyponym,
entailment, holonym}, and WN+NE adds type ne
to both sets of types. We finally summed our ker-
nels with the IDF baseline kernel. We observe that
types which make use of WordNet variations seem
to increase the most our performance. Our as-
sumption was that named entities would be useful
for question answering and that we could learn as-
sociations between question type and answer type
through variations: NE does seem to help a little
when combined with WN alone, but is less use-
ful once TESRK is combined with our baseline of
IDF-weighted common words. Overall, typing ca-
pabilities allow TESRK to obtain way better per-
formances than SRK in both MAP and MRR, and
our best system combining all our features is com-
parable to state-of-the-art systems in MRR, and
significantly outperforms SRK + IDF, the system
without types (p &lt; 0.05).
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999981289473684">
Lodhi et al. (Lodhi et al., 2002) were among the
first in NLP to use kernels: they apply string ker-
nels which count common subsequences to text
classification. Sentence pair classification how-
ever require the capture of 2 types of links: the
link between sentences within a pair, and the link
between pairs. Zanzotto et al. (Zanzotto et al.,
2007) used a kernel method on syntactic tree pairs.
They expanded on graph kernels in (Zanzotto et
al., 2010). Their method first aligns tree nodes
of a pair of sentences to form a single tree with
placeholders. They then use tree kernel (Mos-
chitti, 2006) to compute the number of common
subtrees of those trees. Bu et al. (Bu et al.,
2012) introduced a string rewriting kernel which
can capture at once lexical equivalents and com-
mon syntactic dependencies on pair of sentences.
All these kernel methods require an exact match
or assume prior partial matches between words,
thus limiting the kind of learned rewriting rules.
Our contribution addresses this issue with a type-
enriched string rewriting kernel which can account
for lexico-semantic variations of words. Limita-
tions of our rewriting rules include the impossibil-
ity to skip a pattern word and to replace wildcards
by multiple words.
Some recent contributions (Chang et al., 2010;
Wang and Manning, 2010) also provide a uniform
way to learn both intermediary representations and
a decision function using potentially rich feature
sets. They use heuristics in the joint learning pro-
cess to reduce the computational cost, while our
kernel approach with a simple sequential repre-
sentation of sentences has the benefit of efficiently
computing an exact number of common rewriting
rules between rewriting pairs. This in turn allows
to precisely fine-tune the shape of desired rewrit-
ing rules through the design of the typing scheme.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999874333333334">
We developed a unified kernel-based framework
for solving sentence rewriting tasks. Types al-
low for an increased flexibility in counting com-
mon rewriting rules, and can also add a semantic
layer to the rewritings. We show that we can effi-
ciently compute a kernel which takes types into ac-
count, called type-enriched k-gram bijective string
rewriting kernel. A SVM classifier with this kernel
yields state-of-the-art results in paraphrase identi-
fication and answer sentence selection and outper-
forms comparable systems in recognizing textual
entailment.
</bodyText>
<sectionHeader confidence="0.998465" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9558505">
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics-Volume 1: Proceedings of the
main conference and the shared task, and Volume
</reference>
<page confidence="0.991072">
947
</page>
<reference confidence="0.999675536363636">
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation, pages 385–393. Associa-
tion for Computational Linguistics.
Fan Bu, Hang Li, and Xiaoyan Zhu. 2012. String
re-writing kernel. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 449–458.
Association for Computational Linguistics.
Hiram Calvo, Andrea Segura-Olivares, and Alejandro
Garc´ıa. 2014. Dependency vs. constituent based
syntactic n-grams in text similarity measures for
paraphrase recognition. Computaci´on y Sistemas,
18(3):517–554.
Chih-Chung Chang and Chih-Jen Lin. 2011. Lib-
svm: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(TIST), 2(3):27.
Ming-Wei Chang, Dan Goldwasser, Dan Roth, and
Vivek Srikumar. 2010. Discriminative learning over
constrained latent representations. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 429–437. As-
sociation for Computational Linguistics.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Machine learning challenges. evalu-
ating predictive uncertainty, visual object classifica-
tion, and recognising tectual entailment, pages 177–
190. Springer.
William B Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proc. of IWP.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the 20th international conference
on Computational Linguistics, page 350. Associa-
tion for Computational Linguistics.
Michael Heilman and Noah A Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 1011–1019.
Association for Computational Linguistics.
Aminul Islam and Diana Inkpen. 2009. Semantic sim-
ilarity of short texts. Recent Advances in Natural
Language Processing V, 309:227–236.
Sergio Jimenez, Claudia Becerra, Alexander Gelbukh,
Av Juan Dios B´atiz, and Av Mendiz´abal. 2013.
Softcardinality: hierarchical text overlap for student
response analysis. In Proceedings of the 2nd joint
conference on lexical and computational semantics,
volume 2, pages 280–284.
Mihai C Lintean and Vasile Rus. 2011. Dissimilar-
ity kernels for paraphrase identification. In FLAIRS
Conference.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. The Journal of
Machine Learning Research, 2:419–444.
Nitin Madnani and Bonnie J Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341–387.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 182–190. Asso-
ciation for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, vol-
ume 6, pages 775–780.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Thomas Morton, Joern Kottmann, Jason Baldridge, and
Gann Bierner. 2005. Opennlp: A java-based nlp
toolkit. http://opennlp.sourceforge.net.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318–329.
Springer.
Martin F Porter. 2001. Snowball: A language for stem-
ming algorithms.
Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural In-
formation Processing Systems, pages 801–809.
Leslie G Valiant. 1979. The complexity of enumer-
ation and reliability problems. SIAM Journal on
Computing, 8(3):410–421.
Vladimir Vapnik. 2000. The nature of statistical learn-
ing theory. Springer Science &amp; Business Media.
Stephen Wan, Mark Dras, Robert Dale, and C´ecile
Paris. 2006. Using dependency-based features
to take the para-farce out of paraphrase. In Pro-
ceedings of the Australasian Language Technology
Workshop, volume 2006.
Mengqiu Wang and Christopher D Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 1164–
1172. Association for Computational Linguistics.
</reference>
<page confidence="0.979396">
948
</page>
<reference confidence="0.999135666666667">
Mengqiu Wang, Noah A Smith, and Teruko Mita-
mura. 2007. What is the jeopardy model? a quasi-
synchronous grammar for qa. In EMNLP-CoNLL,
volume 7, pages 22–32.
Xuchen Yao, Benjamin Van Durme, Chris Callison-
Burch, and Peter Clark. 2013. Answer extraction
as sequence tagging with tree edit distance. In HLT-
NAACL, pages 858–867. Citeseer.
Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and
Andrzej Pastusiak. 2013. Question answering using
enhanced lexical semantic models. In Proceedings
of the 26rd International Conference on Compu-
tational Linguistics. Association for Computational
Linguistics.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2007. Shallow semantics in
fast textual entailment rule learners. In Proceed-
ings of the ACL-PASCAL workshop on textual en-
tailment and paraphrasing, pages 72–77. Associa-
tion for Computational Linguistics.
Fabio Massimo Zanzotto, Lorenzo DellArciprete, and
Alessandro Moschitti. 2010. Efficient graph kernels
for textual entailment recognition. Fundamenta In-
formaticae.
</reference>
<page confidence="0.999003">
949
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.502588">
<title confidence="0.999065">A Unified Kernel Approach for Learning Typed Sentence Rewritings</title>
<author confidence="0.999282">Martin Gleize Brigitte Grau</author>
<affiliation confidence="0.972848">LIMSI-CNRS, Orsay, France LIMSI-CNRS, Orsay, France</affiliation>
<address confidence="0.548257">Universit´e Paris-Sud, Orsay, France ENSIIE, Evry, France</address>
<email confidence="0.969187">gleize@limsi.frbg@limsi.fr</email>
<abstract confidence="0.9980442">Many high level natural language processing problems can be framed as determining if two given sentences are a rewriting of each other. In this paper, we propose a class of kernel functions, referred to as type-enriched string rewriting kernels, which, used in kernel-based machine learning algorithms, allow to learn sentence rewritings. Unlike previous work, this method can be fed external lexical semantic relations to capture a wider class of rewriting rules. It also does not assume preliminary syntactic parsing but is still able to provide a unified framework to capture syntactic structure and alignments between the two sentences. We experiment on three different natural sentence rewriting tasks and obtain state-of-the-art results for all of them.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>385--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1417" citStr="Agirre et al., 2012" startWordPosition="210" endWordPosition="213"> framework to capture syntactic structure and alignments between the two sentences. We experiment on three different natural sentence rewriting tasks and obtain state-of-the-art results for all of them. 1 Introduction Detecting implications of sense between statements stands as one of the most sought-after goals in computational linguistics. Several high level tasks look for either one-way rewriting between single sentences, like recognizing textual entailment (RTE) (Dagan et al., 2006), or two-way rewritings like paraphrase identification (Dolan et al., 2004) and semantic textual similarity (Agirre et al., 2012). In a similar fashion, selecting sentences containing the answer to a question can be seen as finding the best rewritings of the question among answer candidates. These problems are naturally framed as classification tasks, and as such most current solutions make use of supervised machine learning. They have to tackle several challenges: picking an adequate language representation, aligning semantically equivalent elements and extracting relevant features to learn the final decision. Bag-of-words and by extension bag-of-ngrams are traditionally the most direct approach and features rely mostl</context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 385–393. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Bu</author>
<author>Hang Li</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>String re-writing kernel.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>449--458</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2824" citStr="Bu et al., 2012" startWordPosition="435" endWordPosition="438">its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenriched string rewriting kernel giving the opportunity to specify in a fine-grained way how words match each other. Unlike previous work, rewriting rules learned using our framework account for syntactic structure, term alignments and lexicosemantic typed variations in a unified approach. We detail how to efficiently compute our kernel and lastly experiment on three different high-level NLP tasks, demonstrating the vast applicability of our method. Our system based on type-enriched st</context>
<context position="4322" citStr="Bu et al., 2012" startWordPosition="661" endWordPosition="664">ng methods 939 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 939–949, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics like SVM, they allow complex decision functions to be learned in classification tasks (Vapnik, 2000). The goal of a well-designed kernel function is to have a high value when computed on two instances of same label, and a low value for two instances of different label. 2.1 String rewriting kernel String rewriting kernels (Bu et al., 2012) count the number of common rewritings between two pairs of sentences seen as sequences of words. The rewriting rule (A) in Figure 1 can be viewed as a kind of phrasal paraphrase with linked variables (Madnani and Dorr, 2010). Rule (A) rewrites (B)’s first sentence into its second but it does not however rewrite the sentences in (C), which is what we try to fix in this paper. Following the terminology of string kernels, we use the term string and character instead of sentence and word. We denote (s, t) E (Σ∗ x Σ∗) an instance of string rewriting, with a source string s and a target string t, b</context>
<context position="9411" citStr="Bu et al., 2012" startWordPosition="1653" endWordPosition="1656">s. However, it cannot match the pair of sentences (C) in the original kb-SRK. We change Fp hypernym to {hypernym, id} where a R b if and only if a and b have a common hypernym in WordNet. And we change F, to F, = {same pronoun, entailment, id} where a same pronoun ❀b if and only if a and b are a pronoun of the same person and same number, and a entailment ❀b if and only if verb a has a relation of entailment with b in WordNet. By redefining the typing scheme, rule (A) can now match (C). 3 Computing TESRK 3.1 Formulation The k-gram bijective string rewriting kernel can be computed efficiently (Bu et al., 2012). We show that we can compute its type-enriched equivalent at the price of a seemingly insurmountable loosening of theoretical complexity boundaries. Experiments however show that its computing time is of the same order as the original kernel. A type-enriched kb-SRK is parameterized by k the length of k-grams, and its typing scheme the sets Fp and F, and their associated relations. The annotations of Fp and F, to Kk and ¯Kk will be omitted for clarity and because they typically will not change while we test different values for k. We rewrite the inner product in Equation 1 to better fit the k-</context>
<context position="21278" citStr="Bu et al., 2012" startWordPosition="3795" endWordPosition="3798">re considered as types in the model and are detailed in Table 1. We used LIBSVM (Chang and Lin, 2011) to train a binary SVM classifier on the training data with our two kernels. The default SVM algorithm in LIBSVM uses a parameter C, roughly akin to a regularization parameter. We 10-fold cross-validated this parameter on the training data, optimizing with a grid search for f-score, or MRR for question-answering. All kernels were normalized using ˜K(x, y) = K(x,y) We de√ K(x,x) √K(y,y) note by ”+” a sum of kernels, with normalizations applied both before and after summing. Following Bu et al. (Bu et al., 2012) experimental setup, we introduced an auxiliary vector kernel denoted PR of features named unigram precision and recall, defined in (Wan et al., 2006). In our experiments a linear kernel seemed to yield the best results. Our Scala implementation of kb-SRKs has an average throughput of about 1500 original kbSRK computations per second, versus 500 typeenriched kb-SRK computations per second on a 8- core machine. It typically takes a few hours on a 32-core machine to train, cross-validate and test on a full dataset. Finally, Table 1 presents an overview of our types with how they are defined and </context>
<context position="24419" citStr="Bu et al. (2012)" startWordPosition="4339" endWordPosition="4342">bserve 944 Type Typing relation on words (a, b) Tool/resources id words have same surface form and tag OpenNLP tagger idMinusTag words have same surface form OpenNLP tokenizer lemma words have same lemma WordNetStemmer stem words have same stem Porter stemmer synonym, antonym words are [type] WordNet hypernym, hyponym b is a [type] of a WordNet entailment, holonym a and b are both tagged with the same Named Entity BBN Identifinder ne lvhsn words are at edit distance of 1 Levenshtein distance Table 1: Types Paraphrase system Accuracy F-score All paraphrase 66.5 79.9 Wan et al. (2006) 75.6 83.0 Bu et al. (2012) 76.3 N/A Socher et al. (2011) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 PR 73.5 82.1 SRK + PR 76.2 83.6 TESRK 76.6 83.7 TESRK + PR 77.2 84.0 logk(#recursive calls) 2.2 1.8 1.6 1.4 1.2 2 1 Table 2: Evaluation results on MSR Paraphrase that our results are state-of-the-art and in particular, they improve on the orignal kb-SRK by a good margin. We tried other combinations of types but it did not yield good results, this is probably due to the nature of the MSR corpus, which did not contain much more advanced variations from WordNet. The only statistically significant improvement we obtained was </context>
<context position="26384" citStr="Bu et al. (2012)" startWordPosition="4700" endWordPosition="4703">ewriting rules on pairs of 7-to-10-grams rarely yields non-zero results, so in practice using high 0 2 4 6 8 10 k Figure 3: Evolution of the number of recursive calls to CountPerfectMatchings with k 2 4 6 8 10 k Figure 4: Evolution of the size of C with k values of k is not interesting. Figure 4 plots the average size of set C computed by algorithm 2, as a function of k (divided by the sum of lengths of the 4 sentences involved in the kernel computation). We can observe that this ICI Σsentence lengths 0.5 2.5 1.5 0 2 1 945 RTE system Accuracy All entailments 51.2 Heilman and Smith (2010) 62.8 Bu et al. (2012) 65.1 Zanzotto et al. (2007) 65.8 Hickl et al. (2006) 80.0 PR 61.8 TESRK (All) 62.1 SRK + PR 63.8 TESRK (Syn) + PR 64.1 TESRK (All) + PR 66.1 Table 3: Evaluation results on RTE-3 quantity is small, except for a peak at low values of k, which is not an issue because the computation of ¯Kk is very fast for those values of k. 4.3 Recognizing textual entailment Recognizing Textual Entailment asks whether the meaning of a sentence hypothesis can be inferred by reading a sentence text. The dataset we used to evaluate our systems is RTE-3. Following similar work (Heilman and Smith, 2010; Bu et al., 2</context>
<context position="28238" citStr="Bu et al. 2012" startWordPosition="5035" endWordPosition="5038"> attended a requiem mass for him at Notre Dame cathedral in Paris.” and ”Charles de Gaulle died in 1970.” does not constitute a textual entailment. We report in Table 3 our best results, the system TESRK (All) + PR, defined by the sum of PR, 1b-SRK and typed-enriched kb-SRKs with k from 2 to 4, with types Γp = {stem, synonym} and Γv = {stem, synonym, hypernym, hyponym, entailment, holonym}. Our results are to be compared with systems using techniques and resources of similar nature, but as reference the top performance at RTE-3 is still reported. This time we did not manage to fully reproduce Bu et al. 2012’s performance, but we observe that type-enriched kb-SRK greatly improves upon our original implementation of kb-SRK and outperforms their system anyway. Combining TESRK and the PR baseline yields significantly better results than either one alone (p &lt; 0.05), and performs significantly better than the system of (Heilman and Smith, 2010), the only one which was evaluated on the same three tasks as us (p &lt; 0.10). We tried with less types in our system TESRK (Syn) + PR by removing all WordNet types but synonyms but got lower performance. This seems to indicate that rich types indeed help capturin</context>
<context position="32771" citStr="Bu et al., 2012" startWordPosition="5809" endWordPosition="5812">se kernels: they apply string kernels which count common subsequences to text classification. Sentence pair classification however require the capture of 2 types of links: the link between sentences within a pair, and the link between pairs. Zanzotto et al. (Zanzotto et al., 2007) used a kernel method on syntactic tree pairs. They expanded on graph kernels in (Zanzotto et al., 2010). Their method first aligns tree nodes of a pair of sentences to form a single tree with placeholders. They then use tree kernel (Moschitti, 2006) to compute the number of common subtrees of those trees. Bu et al. (Bu et al., 2012) introduced a string rewriting kernel which can capture at once lexical equivalents and common syntactic dependencies on pair of sentences. All these kernel methods require an exact match or assume prior partial matches between words, thus limiting the kind of learned rewriting rules. Our contribution addresses this issue with a typeenriched string rewriting kernel which can account for lexico-semantic variations of words. Limitations of our rewriting rules include the impossibility to skip a pattern word and to replace wildcards by multiple words. Some recent contributions (Chang et al., 2010</context>
</contexts>
<marker>Bu, Li, Zhu, 2012</marker>
<rawString>Fan Bu, Hang Li, and Xiaoyan Zhu. 2012. String re-writing kernel. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 449–458. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiram Calvo</author>
<author>Andrea Segura-Olivares</author>
<author>Alejandro Garc´ıa</author>
</authors>
<title>Dependency vs. constituent based syntactic n-grams in text similarity measures for paraphrase recognition.</title>
<date>2014</date>
<journal>Computaci´on y Sistemas,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>Calvo, Segura-Olivares, Garc´ıa, 2014</marker>
<rawString>Hiram Calvo, Andrea Segura-Olivares, and Alejandro Garc´ıa. 2014. Dependency vs. constituent based syntactic n-grams in text similarity measures for paraphrase recognition. Computaci´on y Sistemas, 18(3):517–554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Libsvm: a library for support vector machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>2--3</pages>
<contexts>
<context position="20763" citStr="Chang and Lin, 2011" startWordPosition="3705" endWordPosition="3708">xtual entailment and answer sentence selection. The setup we used for all experiments was the same save for the few parameters we explored such as: k, and typing scheme. We implemented 2 kernels, kb-SRK, henceforth simply denoted SRK, and the type-enriched kbSRK, denoted TESRK. All sentences were tokenized and POS-tagged using OpenNLP (Morton et al., 2005). Then they were stemmed using the Porter stemmer (Porter, 2001) in the case of SRK. Various other pre-processing steps were applied in the case of TESRK: they are considered as types in the model and are detailed in Table 1. We used LIBSVM (Chang and Lin, 2011) to train a binary SVM classifier on the training data with our two kernels. The default SVM algorithm in LIBSVM uses a parameter C, roughly akin to a regularization parameter. We 10-fold cross-validated this parameter on the training data, optimizing with a grid search for f-score, or MRR for question-answering. All kernels were normalized using ˜K(x, y) = K(x,y) We de√ K(x,x) √K(y,y) note by ”+” a sum of kernels, with normalizations applied both before and after summing. Following Bu et al. (Bu et al., 2012) experimental setup, we introduced an auxiliary vector kernel denoted PR of features </context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Dan Goldwasser</author>
<author>Dan Roth</author>
<author>Vivek Srikumar</author>
</authors>
<title>Discriminative learning over constrained latent representations.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>429--437</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33371" citStr="Chang et al., 2010" startWordPosition="5902" endWordPosition="5905"> (Bu et al., 2012) introduced a string rewriting kernel which can capture at once lexical equivalents and common syntactic dependencies on pair of sentences. All these kernel methods require an exact match or assume prior partial matches between words, thus limiting the kind of learned rewriting rules. Our contribution addresses this issue with a typeenriched string rewriting kernel which can account for lexico-semantic variations of words. Limitations of our rewriting rules include the impossibility to skip a pattern word and to replace wildcards by multiple words. Some recent contributions (Chang et al., 2010; Wang and Manning, 2010) also provide a uniform way to learn both intermediary representations and a decision function using potentially rich feature sets. They use heuristics in the joint learning process to reduce the computational cost, while our kernel approach with a simple sequential representation of sentences has the benefit of efficiently computing an exact number of common rewriting rules between rewriting pairs. This in turn allows to precisely fine-tune the shape of desired rewriting rules through the design of the typing scheme. 6 Conclusion We developed a unified kernel-based fr</context>
</contexts>
<marker>Chang, Goldwasser, Roth, Srikumar, 2010</marker>
<rawString>Ming-Wei Chang, Dan Goldwasser, Dan Roth, and Vivek Srikumar. 2010. Discriminative learning over constrained latent representations. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 429–437. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment,</title>
<date>2006</date>
<pages>177--190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1288" citStr="Dagan et al., 2006" startWordPosition="192" endWordPosition="195">e a wider class of rewriting rules. It also does not assume preliminary syntactic parsing but is still able to provide a unified framework to capture syntactic structure and alignments between the two sentences. We experiment on three different natural sentence rewriting tasks and obtain state-of-the-art results for all of them. 1 Introduction Detecting implications of sense between statements stands as one of the most sought-after goals in computational linguistics. Several high level tasks look for either one-way rewriting between single sentences, like recognizing textual entailment (RTE) (Dagan et al., 2006), or two-way rewritings like paraphrase identification (Dolan et al., 2004) and semantic textual similarity (Agirre et al., 2012). In a similar fashion, selecting sentences containing the answer to a question can be seen as finding the best rewritings of the question among answer candidates. These problems are naturally framed as classification tasks, and as such most current solutions make use of supervised machine learning. They have to tackle several challenges: picking an adequate language representation, aligning semantically equivalent elements and extracting relevant features to learn t</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pages 177– 190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>In Proc. of IWP.</booktitle>
<contexts>
<context position="23068" citStr="Dolan and Brockett, 2005" startWordPosition="4107" endWordPosition="4110">cing a word in a pattern of a rewriting rule by one of its holonyms will yield a semantically similar rewriting rule, so holonym would not be a good pattern type for most applications. On the contrary, it can be very useful in a rewriting rule to type a wildcard link with the relation holonym, as this provides constrained semantic roles to the linked wildcards in the rule, thus holonym would be a good variable type. 4.2 Paraphrase identification Paraphrase identification asks whether two sentences have the same meaning. The dataset we used to evaluate our systems is the MSR Paraphrase Corpus (Dolan and Brockett, 2005), containing 4,076 training pairs of sentences and 1,725 testing pairs. For example, the sentences ”An injured woman co-worker also was hospitalized and was listed in good condition.” and ”A woman was listed in good condition at Memorial’s HealthPark campus, he said.” are paraphrases in this corpus. On the other hand, ”’There are a number of locations in our community, which are essentially vulnerable,’ Mr Ruddock said.” and ”’There are a range of risks which are being seriously examined by competent authorities,’ Mr Ruddock said.” are not paraphrases. We report in Table 2 our best results, th</context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proc. of IWP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>350</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1363" citStr="Dolan et al., 2004" startWordPosition="202" endWordPosition="205">tactic parsing but is still able to provide a unified framework to capture syntactic structure and alignments between the two sentences. We experiment on three different natural sentence rewriting tasks and obtain state-of-the-art results for all of them. 1 Introduction Detecting implications of sense between statements stands as one of the most sought-after goals in computational linguistics. Several high level tasks look for either one-way rewriting between single sentences, like recognizing textual entailment (RTE) (Dagan et al., 2006), or two-way rewritings like paraphrase identification (Dolan et al., 2004) and semantic textual similarity (Agirre et al., 2012). In a similar fashion, selecting sentences containing the answer to a question can be seen as finding the best rewritings of the question among answer candidates. These problems are naturally framed as classification tasks, and as such most current solutions make use of supervised machine learning. They have to tackle several challenges: picking an adequate language representation, aligning semantically equivalent elements and extracting relevant features to learn the final decision. Bag-of-words and by extension bag-of-ngrams are traditio</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th international conference on Computational Linguistics, page 350. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>1011--1019</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2510" citStr="Heilman and Smith, 2010" startWordPosition="383" endWordPosition="386">the final decision. Bag-of-words and by extension bag-of-ngrams are traditionally the most direct approach and features rely mostly on lexical matching (Wan et al., 2006; Lintean and Rus, 2011; Jimenez et al., 2013). Moreover, a good solving method has to account for typically scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenriched string rewriting kernel giving the opportunity to specify in a fine-grained way how words match each other. Unlike previous work, rewriting rules learned using our fra</context>
<context position="26362" citStr="Heilman and Smith (2010)" startWordPosition="4695" endWordPosition="4698"> Thankfully, counting common rewriting rules on pairs of 7-to-10-grams rarely yields non-zero results, so in practice using high 0 2 4 6 8 10 k Figure 3: Evolution of the number of recursive calls to CountPerfectMatchings with k 2 4 6 8 10 k Figure 4: Evolution of the size of C with k values of k is not interesting. Figure 4 plots the average size of set C computed by algorithm 2, as a function of k (divided by the sum of lengths of the 4 sentences involved in the kernel computation). We can observe that this ICI Σsentence lengths 0.5 2.5 1.5 0 2 1 945 RTE system Accuracy All entailments 51.2 Heilman and Smith (2010) 62.8 Bu et al. (2012) 65.1 Zanzotto et al. (2007) 65.8 Hickl et al. (2006) 80.0 PR 61.8 TESRK (All) 62.1 SRK + PR 63.8 TESRK (Syn) + PR 64.1 TESRK (All) + PR 66.1 Table 3: Evaluation results on RTE-3 quantity is small, except for a peak at low values of k, which is not an issue because the computation of ¯Kk is very fast for those values of k. 4.3 Recognizing textual entailment Recognizing Textual Entailment asks whether the meaning of a sentence hypothesis can be inferred by reading a sentence text. The dataset we used to evaluate our systems is RTE-3. Following similar work (Heilman and Smi</context>
<context position="28576" citStr="Heilman and Smith, 2010" startWordPosition="5087" endWordPosition="5090">and Γv = {stem, synonym, hypernym, hyponym, entailment, holonym}. Our results are to be compared with systems using techniques and resources of similar nature, but as reference the top performance at RTE-3 is still reported. This time we did not manage to fully reproduce Bu et al. 2012’s performance, but we observe that type-enriched kb-SRK greatly improves upon our original implementation of kb-SRK and outperforms their system anyway. Combining TESRK and the PR baseline yields significantly better results than either one alone (p &lt; 0.05), and performs significantly better than the system of (Heilman and Smith, 2010), the only one which was evaluated on the same three tasks as us (p &lt; 0.10). We tried with less types in our system TESRK (Syn) + PR by removing all WordNet types but synonyms but got lower performance. This seems to indicate that rich types indeed help capturing more complex sentence rewritings. Note that we needed for k = 1 to replace the type-enriched kb-SRK by the original kernel in the sum, otherwise the performance dropped significantly. Our conclusion is that including richer types is only beneficial if they are captured within a context of a couple of words and that including all those</context>
<context position="30688" citStr="Heilman and Smith (2010)" startWordPosition="5442" endWordPosition="5445"> 40 words and questions with only positive or only negative answers were filtered out (Yao et al., 2013). The average fraction of correct answers per question is 7.4% for training and 18.7% for testing. Performances are evaluated as for a re-ranking problem, in term of Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). We report our results in Table 4. We evaluated several combinations of features. IDF word-count (IDF) is a baseline of 1Available at http://nlp.stanford.edu/ mengqiu/data/qg-emnlp07-data.tgz 946 System MAP MRR Random baseline 0.397 0.493 Wang et al. (2007) 0.603 0.685 Heilman and Smith (2010) 0.609 0.692 Wang and Manning (2010) 0.595 0.695 Yao et al. (2013) 0.631 0.748 Yih et al. (2013) LCLR 0.709 0.770 IDF word-count (IDF) 0.596 0.650 SRK 0.609 0.669 SRK + IDF 0.620 0.677 TESRK (WN) 0.642 0.725 TESRK (WN+NE) 0.656 0.744 TESRK (WN) + IDF 0.678 0.759 TESRK (WN+NE) + IDF 0.672 0.768 Table 4: Evaluation results on QA IDF-weighted common word counting, integrated in a linear kernel. Then we implemented SRK and TESRK (with k from 1 to 5) with two typing schemes: WN stands for Fp _ {stem, synonym} and Fv _ {stem, synonym, hypernym, hyponym, entailment, holonym}, and WN+NE adds type ne t</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 1011–1019. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic similarity of short texts.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing V,</booktitle>
<pages>309--227</pages>
<contexts>
<context position="2361" citStr="Islam and Inkpen, 2009" startWordPosition="358" endWordPosition="361">everal challenges: picking an adequate language representation, aligning semantically equivalent elements and extracting relevant features to learn the final decision. Bag-of-words and by extension bag-of-ngrams are traditionally the most direct approach and features rely mostly on lexical matching (Wan et al., 2006; Lintean and Rus, 2011; Jimenez et al., 2013). Moreover, a good solving method has to account for typically scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenriched string rewriting k</context>
</contexts>
<marker>Islam, Inkpen, 2009</marker>
<rawString>Aminul Islam and Diana Inkpen. 2009. Semantic similarity of short texts. Recent Advances in Natural Language Processing V, 309:227–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
<author>Av Juan Dios B´atiz</author>
<author>Av Mendiz´abal</author>
</authors>
<title>Softcardinality: hierarchical text overlap for student response analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2nd joint conference on lexical and computational semantics,</booktitle>
<volume>2</volume>
<pages>280--284</pages>
<marker>Jimenez, Becerra, Gelbukh, B´atiz, Mendiz´abal, 2013</marker>
<rawString>Sergio Jimenez, Claudia Becerra, Alexander Gelbukh, Av Juan Dios B´atiz, and Av Mendiz´abal. 2013. Softcardinality: hierarchical text overlap for student response analysis. In Proceedings of the 2nd joint conference on lexical and computational semantics, volume 2, pages 280–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai C Lintean</author>
<author>Vasile Rus</author>
</authors>
<title>Dissimilarity kernels for paraphrase identification.</title>
<date>2011</date>
<booktitle>In FLAIRS Conference.</booktitle>
<contexts>
<context position="2079" citStr="Lintean and Rus, 2011" startWordPosition="314" endWordPosition="317">es containing the answer to a question can be seen as finding the best rewritings of the question among answer candidates. These problems are naturally framed as classification tasks, and as such most current solutions make use of supervised machine learning. They have to tackle several challenges: picking an adequate language representation, aligning semantically equivalent elements and extracting relevant features to learn the final decision. Bag-of-words and by extension bag-of-ngrams are traditionally the most direct approach and features rely mostly on lexical matching (Wan et al., 2006; Lintean and Rus, 2011; Jimenez et al., 2013). Moreover, a good solving method has to account for typically scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-</context>
</contexts>
<marker>Lintean, Rus, 2011</marker>
<rawString>Mihai C Lintean and Vasile Rus. 2011. Dissimilarity kernels for paraphrase identification. In FLAIRS Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="32122" citStr="Lodhi et al., 2002" startWordPosition="5695" endWordPosition="5698">ntities would be useful for question answering and that we could learn associations between question type and answer type through variations: NE does seem to help a little when combined with WN alone, but is less useful once TESRK is combined with our baseline of IDF-weighted common words. Overall, typing capabilities allow TESRK to obtain way better performances than SRK in both MAP and MRR, and our best system combining all our features is comparable to state-of-the-art systems in MRR, and significantly outperforms SRK + IDF, the system without types (p &lt; 0.05). 5 Related work Lodhi et al. (Lodhi et al., 2002) were among the first in NLP to use kernels: they apply string kernels which count common subsequences to text classification. Sentence pair classification however require the capture of 2 types of links: the link between sentences within a pair, and the link between pairs. Zanzotto et al. (Zanzotto et al., 2007) used a kernel method on syntactic tree pairs. They expanded on graph kernels in (Zanzotto et al., 2010). Their method first aligns tree nodes of a pair of sentences to form a single tree with placeholders. They then use tree kernel (Moschitti, 2006) to compute the number of common sub</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. The Journal of Machine Learning Research, 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of data-driven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="4547" citStr="Madnani and Dorr, 2010" startWordPosition="701" endWordPosition="704">1, 2015. c�2015 Association for Computational Linguistics like SVM, they allow complex decision functions to be learned in classification tasks (Vapnik, 2000). The goal of a well-designed kernel function is to have a high value when computed on two instances of same label, and a low value for two instances of different label. 2.1 String rewriting kernel String rewriting kernels (Bu et al., 2012) count the number of common rewritings between two pairs of sentences seen as sequences of words. The rewriting rule (A) in Figure 1 can be viewed as a kind of phrasal paraphrase with linked variables (Madnani and Dorr, 2010). Rule (A) rewrites (B)’s first sentence into its second but it does not however rewrite the sentences in (C), which is what we try to fix in this paper. Following the terminology of string kernels, we use the term string and character instead of sentence and word. We denote (s, t) E (Σ∗ x Σ∗) an instance of string rewriting, with a source string s and a target string t, both finite sequences of elements in Σ the finite set of characters. Suppose that we are given training data of such instances labeled in {+1, −1}, for paraphrase/nonparaphrase or entailment/non-entailment in applications. We </context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie J Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of data-driven methods. Computational Linguistics, 36(3):341–387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>182--190</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2745" citStr="Madnani et al., 2012" startWordPosition="420" endWordPosition="423">ving method has to account for typically scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenriched string rewriting kernel giving the opportunity to specify in a fine-grained way how words match each other. Unlike previous work, rewriting rules learned using our framework account for syntactic structure, term alignments and lexicosemantic typed variations in a unified approach. We detail how to efficiently compute our kernel and lastly experiment on three different high-level NLP tasks, demonstra</context>
<context position="24481" citStr="Madnani et al. (2012)" startWordPosition="4351" endWordPosition="4354">urces id words have same surface form and tag OpenNLP tagger idMinusTag words have same surface form OpenNLP tokenizer lemma words have same lemma WordNetStemmer stem words have same stem Porter stemmer synonym, antonym words are [type] WordNet hypernym, hyponym b is a [type] of a WordNet entailment, holonym a and b are both tagged with the same Named Entity BBN Identifinder ne lvhsn words are at edit distance of 1 Levenshtein distance Table 1: Types Paraphrase system Accuracy F-score All paraphrase 66.5 79.9 Wan et al. (2006) 75.6 83.0 Bu et al. (2012) 76.3 N/A Socher et al. (2011) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 PR 73.5 82.1 SRK + PR 76.2 83.6 TESRK 76.6 83.7 TESRK + PR 77.2 84.0 logk(#recursive calls) 2.2 1.8 1.6 1.4 1.2 2 1 Table 2: Evaluation results on MSR Paraphrase that our results are state-of-the-art and in particular, they improve on the orignal kb-SRK by a good margin. We tried other combinations of types but it did not yield good results, this is probably due to the nature of the MSR corpus, which did not contain much more advanced variations from WordNet. The only statistically significant improvement we obtained was between TESRK + PR and our PR baseline (p &lt; 0.05). The perform</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182–190. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In AAAI,</booktitle>
<volume>6</volume>
<pages>775--780</pages>
<contexts>
<context position="2337" citStr="Mihalcea et al., 2006" startWordPosition="354" endWordPosition="357">. They have to tackle several challenges: picking an adequate language representation, aligning semantically equivalent elements and extracting relevant features to learn the final decision. Bag-of-words and by extension bag-of-ngrams are traditionally the most direct approach and features rely mostly on lexical matching (Wan et al., 2006; Lintean and Rus, 2011; Jimenez et al., 2013). Moreover, a good solving method has to account for typically scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenr</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In AAAI, volume 6, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="2277" citStr="Miller, 1995" startWordPosition="346" endWordPosition="347">nt solutions make use of supervised machine learning. They have to tackle several challenges: picking an adequate language representation, aligning semantically equivalent elements and extracting relevant features to learn the final decision. Bag-of-words and by extension bag-of-ngrams are traditionally the most direct approach and features rely mostly on lexical matching (Wan et al., 2006; Lintean and Rus, 2011; Jimenez et al., 2013). Moreover, a good solving method has to account for typically scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentenc</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Morton</author>
<author>Joern Kottmann</author>
<author>Jason Baldridge</author>
<author>Gann Bierner</author>
</authors>
<date>2005</date>
<note>Opennlp: A java-based nlp toolkit. http://opennlp.sourceforge.net.</note>
<contexts>
<context position="20501" citStr="Morton et al., 2005" startWordPosition="3654" endWordPosition="3658">-gram quadruples; calling swap on all the pairs of one set is necessary to consistently have sources on the left side and targets on the right side in the result. 4 Experiments 4.1 Systems We experimented on three tasks: paraphrase identification, recognizing textual entailment and answer sentence selection. The setup we used for all experiments was the same save for the few parameters we explored such as: k, and typing scheme. We implemented 2 kernels, kb-SRK, henceforth simply denoted SRK, and the type-enriched kbSRK, denoted TESRK. All sentences were tokenized and POS-tagged using OpenNLP (Morton et al., 2005). Then they were stemmed using the Porter stemmer (Porter, 2001) in the case of SRK. Various other pre-processing steps were applied in the case of TESRK: they are considered as types in the model and are detailed in Table 1. We used LIBSVM (Chang and Lin, 2011) to train a binary SVM classifier on the training data with our two kernels. The default SVM algorithm in LIBSVM uses a parameter C, roughly akin to a regularization parameter. We 10-fold cross-validated this parameter on the training data, optimizing with a grid search for f-score, or MRR for question-answering. All kernels were normal</context>
</contexts>
<marker>Morton, Kottmann, Baldridge, Bierner, 2005</marker>
<rawString>Thomas Morton, Joern Kottmann, Jason Baldridge, and Gann Bierner. 2005. Opennlp: A java-based nlp toolkit. http://opennlp.sourceforge.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Machine Learning: ECML</booktitle>
<pages>318--329</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="32686" citStr="Moschitti, 2006" startWordPosition="5793" endWordPosition="5795">5). 5 Related work Lodhi et al. (Lodhi et al., 2002) were among the first in NLP to use kernels: they apply string kernels which count common subsequences to text classification. Sentence pair classification however require the capture of 2 types of links: the link between sentences within a pair, and the link between pairs. Zanzotto et al. (Zanzotto et al., 2007) used a kernel method on syntactic tree pairs. They expanded on graph kernels in (Zanzotto et al., 2010). Their method first aligns tree nodes of a pair of sentences to form a single tree with placeholders. They then use tree kernel (Moschitti, 2006) to compute the number of common subtrees of those trees. Bu et al. (Bu et al., 2012) introduced a string rewriting kernel which can capture at once lexical equivalents and common syntactic dependencies on pair of sentences. All these kernel methods require an exact match or assume prior partial matches between words, thus limiting the kind of learned rewriting rules. Our contribution addresses this issue with a typeenriched string rewriting kernel which can account for lexico-semantic variations of words. Limitations of our rewriting rules include the impossibility to skip a pattern word and </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Machine Learning: ECML 2006, pages 318–329. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>Snowball: A language for stemming algorithms.</title>
<date>2001</date>
<contexts>
<context position="20565" citStr="Porter, 2001" startWordPosition="3668" endWordPosition="3669"> to consistently have sources on the left side and targets on the right side in the result. 4 Experiments 4.1 Systems We experimented on three tasks: paraphrase identification, recognizing textual entailment and answer sentence selection. The setup we used for all experiments was the same save for the few parameters we explored such as: k, and typing scheme. We implemented 2 kernels, kb-SRK, henceforth simply denoted SRK, and the type-enriched kbSRK, denoted TESRK. All sentences were tokenized and POS-tagged using OpenNLP (Morton et al., 2005). Then they were stemmed using the Porter stemmer (Porter, 2001) in the case of SRK. Various other pre-processing steps were applied in the case of TESRK: they are considered as types in the model and are detailed in Table 1. We used LIBSVM (Chang and Lin, 2011) to train a binary SVM classifier on the training data with our two kernels. The default SVM algorithm in LIBSVM uses a parameter C, roughly akin to a regularization parameter. We 10-fold cross-validated this parameter on the training data, optimizing with a grid search for f-score, or MRR for question-answering. All kernels were normalized using ˜K(x, y) = K(x,y) We de√ K(x,x) √K(y,y) note by ”+” a</context>
</contexts>
<marker>Porter, 2001</marker>
<rawString>Martin F Porter. 2001. Snowball: A language for stemming algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennin</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>801--809</pages>
<contexts>
<context position="2555" citStr="Socher et al., 2011" startWordPosition="391" endWordPosition="394"> bag-of-ngrams are traditionally the most direct approach and features rely mostly on lexical matching (Wan et al., 2006; Lintean and Rus, 2011; Jimenez et al., 2013). Moreover, a good solving method has to account for typically scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenriched string rewriting kernel giving the opportunity to specify in a fine-grained way how words match each other. Unlike previous work, rewriting rules learned using our framework account for syntactic structure, term </context>
<context position="24449" citStr="Socher et al. (2011)" startWordPosition="4345" endWordPosition="4348">ation on words (a, b) Tool/resources id words have same surface form and tag OpenNLP tagger idMinusTag words have same surface form OpenNLP tokenizer lemma words have same lemma WordNetStemmer stem words have same stem Porter stemmer synonym, antonym words are [type] WordNet hypernym, hyponym b is a [type] of a WordNet entailment, holonym a and b are both tagged with the same Named Entity BBN Identifinder ne lvhsn words are at edit distance of 1 Levenshtein distance Table 1: Types Paraphrase system Accuracy F-score All paraphrase 66.5 79.9 Wan et al. (2006) 75.6 83.0 Bu et al. (2012) 76.3 N/A Socher et al. (2011) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 PR 73.5 82.1 SRK + PR 76.2 83.6 TESRK 76.6 83.7 TESRK + PR 77.2 84.0 logk(#recursive calls) 2.2 1.8 1.6 1.4 1.2 2 1 Table 2: Evaluation results on MSR Paraphrase that our results are state-of-the-art and in particular, they improve on the orignal kb-SRK by a good margin. We tried other combinations of types but it did not yield good results, this is probably due to the nature of the MSR corpus, which did not contain much more advanced variations from WordNet. The only statistically significant improvement we obtained was between TESRK + PR and our PR </context>
</contexts>
<marker>Socher, Huang, Pennin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Y Ng. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems, pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leslie G Valiant</author>
</authors>
<title>The complexity of enumeration and reliability problems.</title>
<date>1979</date>
<journal>SIAM Journal on Computing,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="13251" citStr="Valiant, 1979" startWordPosition="2335" endWordPosition="2336">igned pairs. And if (a1, a2) does not need to substitute a wildcard because it has common pattern types (lines 13 and 14), we can choose to not create any wildcard pairing with it and ignore it in the recursive call. This algorithm enumerates all configurations such that each character pair has a common pattern type or is matched 1-for-1 with a character pair with common variable types, which is exactly the definition of a rewriting rule in TESRK. This problem is actually equivalent to counting the perfect matchings of the bipartite graph of potential wildcards. It has been shown intractable (Valiant, 1979) and Algorithm 1 is a naive recursive algorithm to solve it. In our implementation we represent the graph with its biadjacency matrix, and if our typing relations are independent of k, the function has a O(k) time complexity without including its recursive calls. The number of recursive calls can be greater than k!2 which is the number of perfect matchings in a complete bipartite graph of 2k vertices. In our experiments on linguistic data however, we observed a linear number of recursive calls for low values of k, and up to a quadratic number for k &gt; 10 –which is way past the point where the k</context>
</contexts>
<marker>Valiant, 1979</marker>
<rawString>Leslie G Valiant. 1979. The complexity of enumeration and reliability problems. SIAM Journal on Computing, 8(3):410–421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>2000</date>
<publisher>Springer Science &amp; Business Media.</publisher>
<contexts>
<context position="4082" citStr="Vapnik, 2000" startWordPosition="621" endWordPosition="622">rt results on paraphrase identification and answer sentence selection and outperforms comparable methods on RTE. 2 Type-Enriched String Rewriting Kernel Kernel functions measure the similarity between two elements. Used in machine learning methods 939 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 939–949, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics like SVM, they allow complex decision functions to be learned in classification tasks (Vapnik, 2000). The goal of a well-designed kernel function is to have a high value when computed on two instances of same label, and a low value for two instances of different label. 2.1 String rewriting kernel String rewriting kernels (Bu et al., 2012) count the number of common rewritings between two pairs of sentences seen as sequences of words. The rewriting rule (A) in Figure 1 can be viewed as a kind of phrasal paraphrase with linked variables (Madnani and Dorr, 2010). Rule (A) rewrites (B)’s first sentence into its second but it does not however rewrite the sentences in (C), which is what we try to </context>
</contexts>
<marker>Vapnik, 2000</marker>
<rawString>Vladimir Vapnik. 2000. The nature of statistical learning theory. Springer Science &amp; Business Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>C´ecile Paris</author>
</authors>
<title>Using dependency-based features to take the para-farce out of paraphrase.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop,</booktitle>
<volume>volume</volume>
<contexts>
<context position="2056" citStr="Wan et al., 2006" startWordPosition="310" endWordPosition="313"> selecting sentences containing the answer to a question can be seen as finding the best rewritings of the question among answer candidates. These problems are naturally framed as classification tasks, and as such most current solutions make use of supervised machine learning. They have to tackle several challenges: picking an adequate language representation, aligning semantically equivalent elements and extracting relevant features to learn the final decision. Bag-of-words and by extension bag-of-ngrams are traditionally the most direct approach and features rely mostly on lexical matching (Wan et al., 2006; Lintean and Rus, 2011; Jimenez et al., 2013). Moreover, a good solving method has to account for typically scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like</context>
<context position="21428" citStr="Wan et al., 2006" startWordPosition="3819" endWordPosition="3822">g data with our two kernels. The default SVM algorithm in LIBSVM uses a parameter C, roughly akin to a regularization parameter. We 10-fold cross-validated this parameter on the training data, optimizing with a grid search for f-score, or MRR for question-answering. All kernels were normalized using ˜K(x, y) = K(x,y) We de√ K(x,x) √K(y,y) note by ”+” a sum of kernels, with normalizations applied both before and after summing. Following Bu et al. (Bu et al., 2012) experimental setup, we introduced an auxiliary vector kernel denoted PR of features named unigram precision and recall, defined in (Wan et al., 2006). In our experiments a linear kernel seemed to yield the best results. Our Scala implementation of kb-SRKs has an average throughput of about 1500 original kbSRK computations per second, versus 500 typeenriched kb-SRK computations per second on a 8- core machine. It typically takes a few hours on a 32-core machine to train, cross-validate and test on a full dataset. Finally, Table 1 presents an overview of our types with how they are defined and implemented. Every type can be used both as a pattern type or as a variable type, but the two roles are different. Pattern types are useful to unify d</context>
<context position="24392" citStr="Wan et al. (2006)" startWordPosition="4333" endWordPosition="4336">= Γv = {stem, synonym}. We observe 944 Type Typing relation on words (a, b) Tool/resources id words have same surface form and tag OpenNLP tagger idMinusTag words have same surface form OpenNLP tokenizer lemma words have same lemma WordNetStemmer stem words have same stem Porter stemmer synonym, antonym words are [type] WordNet hypernym, hyponym b is a [type] of a WordNet entailment, holonym a and b are both tagged with the same Named Entity BBN Identifinder ne lvhsn words are at edit distance of 1 Levenshtein distance Table 1: Types Paraphrase system Accuracy F-score All paraphrase 66.5 79.9 Wan et al. (2006) 75.6 83.0 Bu et al. (2012) 76.3 N/A Socher et al. (2011) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 PR 73.5 82.1 SRK + PR 76.2 83.6 TESRK 76.6 83.7 TESRK + PR 77.2 84.0 logk(#recursive calls) 2.2 1.8 1.6 1.4 1.2 2 1 Table 2: Evaluation results on MSR Paraphrase that our results are state-of-the-art and in particular, they improve on the orignal kb-SRK by a good margin. We tried other combinations of types but it did not yield good results, this is probably due to the nature of the MSR corpus, which did not contain much more advanced variations from WordNet. The only statistically significant i</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris. 2006. Using dependency-based features to take the para-farce out of paraphrase. In Proceedings of the Australasian Language Technology Workshop, volume 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Probabilistic tree-edit models with structured latent variables for textual entailment and question answering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1164--1172</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="2534" citStr="Wang and Manning, 2010" startWordPosition="387" endWordPosition="390">f-words and by extension bag-of-ngrams are traditionally the most direct approach and features rely mostly on lexical matching (Wan et al., 2006; Lintean and Rus, 2011; Jimenez et al., 2013). Moreover, a good solving method has to account for typically scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenriched string rewriting kernel giving the opportunity to specify in a fine-grained way how words match each other. Unlike previous work, rewriting rules learned using our framework account for synta</context>
<context position="30724" citStr="Wang and Manning (2010)" startWordPosition="5448" endWordPosition="5451">itive or only negative answers were filtered out (Yao et al., 2013). The average fraction of correct answers per question is 7.4% for training and 18.7% for testing. Performances are evaluated as for a re-ranking problem, in term of Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). We report our results in Table 4. We evaluated several combinations of features. IDF word-count (IDF) is a baseline of 1Available at http://nlp.stanford.edu/ mengqiu/data/qg-emnlp07-data.tgz 946 System MAP MRR Random baseline 0.397 0.493 Wang et al. (2007) 0.603 0.685 Heilman and Smith (2010) 0.609 0.692 Wang and Manning (2010) 0.595 0.695 Yao et al. (2013) 0.631 0.748 Yih et al. (2013) LCLR 0.709 0.770 IDF word-count (IDF) 0.596 0.650 SRK 0.609 0.669 SRK + IDF 0.620 0.677 TESRK (WN) 0.642 0.725 TESRK (WN+NE) 0.656 0.744 TESRK (WN) + IDF 0.678 0.759 TESRK (WN+NE) + IDF 0.672 0.768 Table 4: Evaluation results on QA IDF-weighted common word counting, integrated in a linear kernel. Then we implemented SRK and TESRK (with k from 1 to 5) with two typing schemes: WN stands for Fp _ {stem, synonym} and Fv _ {stem, synonym, hypernym, hyponym, entailment, holonym}, and WN+NE adds type ne to both sets of types. We finally sum</context>
<context position="33396" citStr="Wang and Manning, 2010" startWordPosition="5906" endWordPosition="5909">ntroduced a string rewriting kernel which can capture at once lexical equivalents and common syntactic dependencies on pair of sentences. All these kernel methods require an exact match or assume prior partial matches between words, thus limiting the kind of learned rewriting rules. Our contribution addresses this issue with a typeenriched string rewriting kernel which can account for lexico-semantic variations of words. Limitations of our rewriting rules include the impossibility to skip a pattern word and to replace wildcards by multiple words. Some recent contributions (Chang et al., 2010; Wang and Manning, 2010) also provide a uniform way to learn both intermediary representations and a decision function using potentially rich feature sets. They use heuristics in the joint learning process to reduce the computational cost, while our kernel approach with a simple sequential representation of sentences has the benefit of efficiently computing an exact number of common rewriting rules between rewriting pairs. This in turn allows to precisely fine-tune the shape of desired rewriting rules through the design of the typing scheme. 6 Conclusion We developed a unified kernel-based framework for solving sente</context>
</contexts>
<marker>Wang, Manning, 2010</marker>
<rawString>Mengqiu Wang and Christopher D Manning. 2010. Probabilistic tree-edit models with structured latent variables for textual entailment and question answering. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1164– 1172. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Noah A Smith</author>
<author>Teruko Mitamura</author>
</authors>
<title>What is the jeopardy model? a quasisynchronous grammar for qa.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<volume>7</volume>
<pages>22--32</pages>
<contexts>
<context position="29498" citStr="Wang et al., 2007" startWordPosition="5247" endWordPosition="5250">hat we needed for k = 1 to replace the type-enriched kb-SRK by the original kernel in the sum, otherwise the performance dropped significantly. Our conclusion is that including richer types is only beneficial if they are captured within a context of a couple of words and that including all those variations on unigrams only add noise. 4.4 Answer sentence selection Answer sentence selection is the problem of selecting among single candidate sentences the ones containing the correct answer to an open-domain factoid question. The dataset we used to evaluate our system on this task was created by (Wang et al., 2007) based on the QA track of past Text REtrieval Conferences (TREC-QA)1. The training set contains 4718 question/answer pairs, for 94 questions, originating from TREC 8 to 12. The testing set contains 1517 pairs for 89 questions. As an example, a correct answer to the question ”What do practitioners of Wicca worship?” is ”An estimated 50,000 Americans practice Wicca, a form ofpolytheistic nature worship.” On the other hand, the answer candidate ”When people think of Wicca, they think of either Satanism or silly mumbo jumbo.” is incorrect. Sentences with more than 40 words and questions with only </context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>Mengqiu Wang, Noah A Smith, and Teruko Mitamura. 2007. What is the jeopardy model? a quasisynchronous grammar for qa. In EMNLP-CoNLL, volume 7, pages 22–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
<author>Chris CallisonBurch</author>
<author>Peter Clark</author>
</authors>
<title>Answer extraction as sequence tagging with tree edit distance.</title>
<date>2013</date>
<booktitle>In HLTNAACL,</booktitle>
<pages>858--867</pages>
<publisher>Citeseer.</publisher>
<marker>Yao, Van Durme, CallisonBurch, Clark, 2013</marker>
<rawString>Xuchen Yao, Benjamin Van Durme, Chris CallisonBurch, and Peter Clark. 2013. Answer extraction as sequence tagging with tree edit distance. In HLTNAACL, pages 858–867. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Ming-Wei Chang</author>
<author>Christopher Meek</author>
<author>Andrzej Pastusiak</author>
</authors>
<title>Question answering using enhanced lexical semantic models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 26rd International Conference on Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2380" citStr="Yih et al., 2013" startWordPosition="362" endWordPosition="365">ng an adequate language representation, aligning semantically equivalent elements and extracting relevant features to learn the final decision. Bag-of-words and by extension bag-of-ngrams are traditionally the most direct approach and features rely mostly on lexical matching (Wan et al., 2006; Lintean and Rus, 2011; Jimenez et al., 2013). Moreover, a good solving method has to account for typically scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenriched string rewriting kernel giving the op</context>
<context position="30784" citStr="Yih et al. (2013)" startWordPosition="5460" endWordPosition="5463">). The average fraction of correct answers per question is 7.4% for training and 18.7% for testing. Performances are evaluated as for a re-ranking problem, in term of Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). We report our results in Table 4. We evaluated several combinations of features. IDF word-count (IDF) is a baseline of 1Available at http://nlp.stanford.edu/ mengqiu/data/qg-emnlp07-data.tgz 946 System MAP MRR Random baseline 0.397 0.493 Wang et al. (2007) 0.603 0.685 Heilman and Smith (2010) 0.609 0.692 Wang and Manning (2010) 0.595 0.695 Yao et al. (2013) 0.631 0.748 Yih et al. (2013) LCLR 0.709 0.770 IDF word-count (IDF) 0.596 0.650 SRK 0.609 0.669 SRK + IDF 0.620 0.677 TESRK (WN) 0.642 0.725 TESRK (WN+NE) 0.656 0.744 TESRK (WN) + IDF 0.678 0.759 TESRK (WN+NE) + IDF 0.672 0.768 Table 4: Evaluation results on QA IDF-weighted common word counting, integrated in a linear kernel. Then we implemented SRK and TESRK (with k from 1 to 5) with two typing schemes: WN stands for Fp _ {stem, synonym} and Fv _ {stem, synonym, hypernym, hyponym, entailment, holonym}, and WN+NE adds type ne to both sets of types. We finally summed our kernels with the IDF baseline kernel. We observe tha</context>
</contexts>
<marker>Yih, Chang, Meek, Pastusiak, 2013</marker>
<rawString>Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. Question answering using enhanced lexical semantic models. In Proceedings of the 26rd International Conference on Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Shallow semantics in fast textual entailment rule learners.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing,</booktitle>
<pages>72--77</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2783" citStr="Zanzotto et al., 2007" startWordPosition="427" endWordPosition="430">ly scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenriched string rewriting kernel giving the opportunity to specify in a fine-grained way how words match each other. Unlike previous work, rewriting rules learned using our framework account for syntactic structure, term alignments and lexicosemantic typed variations in a unified approach. We detail how to efficiently compute our kernel and lastly experiment on three different high-level NLP tasks, demonstrating the vast applicability of our met</context>
<context position="26412" citStr="Zanzotto et al. (2007)" startWordPosition="4705" endWordPosition="4708">s of 7-to-10-grams rarely yields non-zero results, so in practice using high 0 2 4 6 8 10 k Figure 3: Evolution of the number of recursive calls to CountPerfectMatchings with k 2 4 6 8 10 k Figure 4: Evolution of the size of C with k values of k is not interesting. Figure 4 plots the average size of set C computed by algorithm 2, as a function of k (divided by the sum of lengths of the 4 sentences involved in the kernel computation). We can observe that this ICI Σsentence lengths 0.5 2.5 1.5 0 2 1 945 RTE system Accuracy All entailments 51.2 Heilman and Smith (2010) 62.8 Bu et al. (2012) 65.1 Zanzotto et al. (2007) 65.8 Hickl et al. (2006) 80.0 PR 61.8 TESRK (All) 62.1 SRK + PR 63.8 TESRK (Syn) + PR 64.1 TESRK (All) + PR 66.1 Table 3: Evaluation results on RTE-3 quantity is small, except for a peak at low values of k, which is not an issue because the computation of ¯Kk is very fast for those values of k. 4.3 Recognizing textual entailment Recognizing Textual Entailment asks whether the meaning of a sentence hypothesis can be inferred by reading a sentence text. The dataset we used to evaluate our systems is RTE-3. Following similar work (Heilman and Smith, 2010; Bu et al., 2012), we took as training da</context>
<context position="32436" citStr="Zanzotto et al., 2007" startWordPosition="5748" endWordPosition="5751">bilities allow TESRK to obtain way better performances than SRK in both MAP and MRR, and our best system combining all our features is comparable to state-of-the-art systems in MRR, and significantly outperforms SRK + IDF, the system without types (p &lt; 0.05). 5 Related work Lodhi et al. (Lodhi et al., 2002) were among the first in NLP to use kernels: they apply string kernels which count common subsequences to text classification. Sentence pair classification however require the capture of 2 types of links: the link between sentences within a pair, and the link between pairs. Zanzotto et al. (Zanzotto et al., 2007) used a kernel method on syntactic tree pairs. They expanded on graph kernels in (Zanzotto et al., 2010). Their method first aligns tree nodes of a pair of sentences to form a single tree with placeholders. They then use tree kernel (Moschitti, 2006) to compute the number of common subtrees of those trees. Bu et al. (Bu et al., 2012) introduced a string rewriting kernel which can capture at once lexical equivalents and common syntactic dependencies on pair of sentences. All these kernel methods require an exact match or assume prior partial matches between words, thus limiting the kind of lear</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2007</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessandro Moschitti. 2007. Shallow semantics in fast textual entailment rule learners. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 72–77. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Lorenzo DellArciprete</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient graph kernels for textual entailment recognition. Fundamenta Informaticae.</title>
<date>2010</date>
<contexts>
<context position="2806" citStr="Zanzotto et al., 2010" startWordPosition="431" endWordPosition="434">ing data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenriched string rewriting kernel giving the opportunity to specify in a fine-grained way how words match each other. Unlike previous work, rewriting rules learned using our framework account for syntactic structure, term alignments and lexicosemantic typed variations in a unified approach. We detail how to efficiently compute our kernel and lastly experiment on three different high-level NLP tasks, demonstrating the vast applicability of our method. Our system based o</context>
<context position="32540" citStr="Zanzotto et al., 2010" startWordPosition="5766" endWordPosition="5769"> combining all our features is comparable to state-of-the-art systems in MRR, and significantly outperforms SRK + IDF, the system without types (p &lt; 0.05). 5 Related work Lodhi et al. (Lodhi et al., 2002) were among the first in NLP to use kernels: they apply string kernels which count common subsequences to text classification. Sentence pair classification however require the capture of 2 types of links: the link between sentences within a pair, and the link between pairs. Zanzotto et al. (Zanzotto et al., 2007) used a kernel method on syntactic tree pairs. They expanded on graph kernels in (Zanzotto et al., 2010). Their method first aligns tree nodes of a pair of sentences to form a single tree with placeholders. They then use tree kernel (Moschitti, 2006) to compute the number of common subtrees of those trees. Bu et al. (Bu et al., 2012) introduced a string rewriting kernel which can capture at once lexical equivalents and common syntactic dependencies on pair of sentences. All these kernel methods require an exact match or assume prior partial matches between words, thus limiting the kind of learned rewriting rules. Our contribution addresses this issue with a typeenriched string rewriting kernel w</context>
</contexts>
<marker>Zanzotto, DellArciprete, Moschitti, 2010</marker>
<rawString>Fabio Massimo Zanzotto, Lorenzo DellArciprete, and Alessandro Moschitti. 2010. Efficient graph kernels for textual entailment recognition. Fundamenta Informaticae.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>