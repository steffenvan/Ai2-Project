<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988572">
Deterministic Part-of-Speech Tagging
with Finite-State Transducers
</title>
<author confidence="0.992033">
Emmanuel Roche* Yves Schabes*
</author>
<sectionHeader confidence="0.471508" genericHeader="abstract">
MERL MERL
</sectionHeader>
<bodyText confidence="0.998807333333333">
Stochastic approaches to natural language processing have often been preferred to rule-based
approaches because of their robustness and their automatic training capabilities. This was the
case for part-of-speech tagging until Brill showed how state-of-the-art part-of-speech tagging can
be achieved with a rule-based tagger by inferring rules from a training corpus. However, current
implementations of the rule-based tagger run more slowly than previous approaches. In this
paper, we present a finite-state tagger, inspired by the rule-based tagger, that operates in optimal
time in the sense that the time to assign tags to a sentence corresponds to the time required to
follow a single path in a deterministic finite-state machine. This result is achieved by encoding
the application of the rules found in the tagger as a nondeterministic finite-state transducer and
then turning it into a deterministic transducer. The resulting deterministic transducer yields a
part-of-speech tagger whose speed is dominated by the access time of mass storage devices. We
then generalize the techniques to the class of transformation-based systems.
</bodyText>
<sectionHeader confidence="0.990203" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999903904761905">
Finite-state devices have important applications to many areas of computer science, in-
cluding pattern matching, databases, and compiler technology. Although their linguis-
tic adequacy to natural language processing has been questioned in the past (Chomsky,
1964), there has recently been a dramatic renewal of interest in the application of finite-
state devices to several aspects of natural language processing. This renewal of interest
is due to the speed and compactness of finite-state representations. This efficiency is ex-
plained by two properties: finite-state devices can be made deterministic, and they can
be turned into a minimal form. Such representations have been successfully applied to
different aspects of natural language processing, such as morphological analysis and
generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche
1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay
1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state
machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993;
Silberztein 1993), none of these approaches has the same flexibility as stochastic tech-
niques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec
1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to
now the knowledge found in finite-state taggers has been handcrafted and was not
automatically acquired.
Recently, Brill (1992) described a rule-based tagger that performs as well as taggers
based upon probabilistic models and overcomes the limitations common in rule-based
approaches to language processing: it is robust and the rules are automatically ac-
</bodyText>
<footnote confidence="0.6170755">
* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139. E-mail:
roche/schabes@merl.com.
</footnote>
<note confidence="0.875693">
Â© 1995 Association for Computational Linguistics
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.999514810810811">
quired. In addition, the tagger requires drastically less space than stochastic taggers.
However, current implementations of Brill&apos;s tagger are considerably slower than the
ones based on probabilistic models since it may require RKn elementary steps to tag
an input of n words with R rules requiring at most K tokens of context.
Although the speed of current part-of-speech taggers is acceptable for interac-
tive systems where a sentence at a time is being processed, it is not adequate for
applications where large bodies of text need to be tagged, such as in information re-
trieval, indexing applications, and grammar-checking systems. Furthermore, the space
required for part-of-speech taggers is also an issue in commercial personal computer
applications such as grammar-checking systems. In addition, part-of-speech taggers
are often being coupled with a syntactic analysis module. Usually these two modules
are written in different frameworks, making it very difficult to integrate interactions
between the two modules.
In this paper, we design a tagger that requires n steps to tag a sentence of length
n, independently of the number of rules and the length of the context they require.
The tagger is represented by a finite-state transducer, a framework that can also be
the basis for syntactic analysis. This finite-state tagger will also be found useful when
combined with other language components, since it can be naturally extended by
composing it with finite-state transducers that could encode other aspects of natural
language syntax.
Relying on algorithms and formal characterizations described in later sections, we
explain how each rule in Brill&apos;s tagger can be viewed as a nondeterministic finite-state
transducer. We also show how the application of all rules in Brill&apos;s tagger is achieved
by composing each of these nondeterministic transducers and why nondeterminism
arises in this transducer. We then prove the correctness of the general algorithm for
determinizing (whenever possible) finite-state transducers, and we successfully apply
this algorithm to the previously obtained nondeterministic transducer. The resulting
deterministic transducer yields a part-of-speech tagger that operates in optimal time
in the sense that the time to assign tags to a sentence corresponds to the time required
to follow a single path in this deterministic finite-state machine. We also show how
the lexicon used by the tagger can be optimally encoded using a finite-state machine.
The techniques used for the construction of the finite-state tagger are then for-
malized and mathematically proven correct. We introduce a proof of soundness and
completeness with a worst-case complexity analysis for the algorithm for determiniz-
ing finite-state transducers.
We conclude by proving that the method can be applied to the class of transformation-
based error-driven systems.
</bodyText>
<sectionHeader confidence="0.836171" genericHeader="introduction">
2. Overview of Brill&apos;s Tagger
</sectionHeader>
<bodyText confidence="0.999333125">
Brill&apos;s tagger is comprised of three parts, each of which is inferred from a training cor-
pus: a lexical tagger, an unknown word tagger, and a contextual tagger. For purposes
of exposition, we will postpone the discussion of the unknown word tagger and focus
mainly on the contextual rule tagger, which is the core of the tagger.
The lexical tagger initially tags each word with its most likely tag, estimated by
examining a large tagged corpus, without regard to context. For example, assuming
that vbn is the most likely tag for the word &amp;quot;killed&amp;quot; and vbd for &amp;quot;shot,&amp;quot; the lexical
tagger might assign the following part-of-speech tags:&apos;
</bodyText>
<footnote confidence="0.578681">
1 The notation for part-of-speech tags is adapted from the one used in the Brown Corpus (Francis and
</footnote>
<page confidence="0.992251">
228
</page>
<note confidence="0.639562">
Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging
</note>
<figure confidence="0.9605095">
1. vbn vbd PREVTAG np
2. vbd vbn NEXTTAG by
</figure>
<figureCaption confidence="0.839117">
Figure 1
Sample rules.
</figureCaption>
<listItem confidence="0.978868333333333">
(1) Chapman/np killed/vbn John/np Lennon/np
(2) John/np Lennon/np was/bedz shot/vbd by/by Chapman/np
(3) He/pps witnessed/vbd Lennon/np killed/vbn by/by Chapman/np
</listItem>
<bodyText confidence="0.999372818181818">
Since the lexical tagger does not use any contextual information, many words can
be tagged incorrectly. For example, in (1), the word &amp;quot;killed&amp;quot; is erroneously tagged as
a verb in past participle form, and in (2), &amp;quot;shot&amp;quot; is incorrectly tagged as a verb in past
tense.
Given the initial tagging obtained by the lexical tagger, the contextual tagger ap-
plies a sequence of rules in order and attempts to remedy the errors made by the initial
tagging. For example, the rules in Figure 1 might be found in a contextual tagger.
The first rule says to change tag vbn to vbd if the previous tag is np. The second
rule says to change vbd to tag vbn if the next tag is by. Once the first rule is applied,
the tag for &amp;quot;killed&amp;quot; in (1) and (3) is changed from vbn to vbd and the following tagged
sentences are obtained:
</bodyText>
<listItem confidence="0.998245">
(4) Chapman/np killed/vbd John/np Lennon/np
(5) john/np Lennon/np was/bedz shot/vbd by/by Chapman/np
(6) He/pps witnessed/vbd Lennon/np killed/vbd by/by Chapman/np
</listItem>
<bodyText confidence="0.639708666666667">
And once the second rule is applied, the tag for &amp;quot;shot&amp;quot; in (5) is changed from vbd
to vbn, resulting in (8), and the tag for &amp;quot;killed&amp;quot; in (6) is changed back from vbd to vbn,
resulting in (9):
</bodyText>
<listItem confidence="0.998034333333333">
(7) Chapman/np killed/vbd john/np Lennon/np
(8) John/np Lennon/np was/bedz shot/vbn by/by Chapman/np
(9) He/pps witnessed/vbd Lennon/np killed/vbn by/by Chapman/np
</listItem>
<bodyText confidence="0.999926272727273">
It is relevant to our following discussion to note that the application of the NEXT-
TAG rule must look ahead one token in the sentence before it can be applied, and that
the application of two rules may perform a series of operations resulting in no net
change. As we will see in the next section, these two aspects are the source of local
nondeterminism in Brill&apos;s tagger.
The sequence of contextual rules is automatically inferred from a training corpus.
A list of tagging errors (with their counts) is compiled by comparing the output of
the lexical tagger to the correct part-of-speech assignment. Then, for each error, it is
determined which instantiation of a set of rule templates results in the greatest error
reduction. Then the set of new errors caused by applying the rule is computed and
the process is repeated until the error reduction drops below a given threshold.
</bodyText>
<footnote confidence="0.424115">
KuCera 1982): pps stands for singular nominative pronoun in third person, vbd for verb in past tense, np
for proper noun, vbn for verb in past participle form, by for the word &amp;quot;by,&amp;quot; at for determiner, nn for
singular noun, and bedz for the word &amp;quot;was.&amp;quot;
</footnote>
<page confidence="0.996652">
229
</page>
<figure confidence="0.91834952173913">
Computational Linguistics
Volume 21, Number 2
A B PREVTAG C
A B PREV10R2OR3TAG C
A B PREV10R2TAG C
A B NEXT1OR2TAG C
A B NEXTTAG C
A B SURROUNDTAG C D
A B NEXTBIGRAM C D
A B PREVBIGRAM C D
change A to B if previous tag is C
change A to B if previous one or two or three tag is C
change A to B if previous one or two tag is C
change A to B if next one or two tag is C
change A to B if next tag is C
change A to B if surrounding tags are C and D
change A to B if next bigram tag is C D
change A to B if previous bigram tag is C D
Figure 2
Contextual rule templates.
KWIC
CIA CICIA C IC IA
(1) (2) (3)
</figure>
<figureCaption confidence="0.64526">
Figure 3
</figureCaption>
<subsectionHeader confidence="0.949564">
Partial matches of A B PREVBIGRAM C C on the input CDCC A.
</subsectionHeader>
<bodyText confidence="0.999808714285714">
Using the set of contextual rule templates shown in Figure 2, after training on
the Brown Corpus, 280 contextual rules are obtained. The resulting rule-based tagger
performs as well as state-of-the-art taggers based upon probabilistic models. It also
overcomes the limitations common in rule-based approaches to language processing:
it is robust, and the rules are automatically acquired. In addition, the tagger requires
drastically less space than stochastic taggers. However, as we will see in the next
section, Brill&apos;s tagger is inherently slow.
</bodyText>
<listItem confidence="0.600292">
3. Complexity of Brill&apos;s Tagger
</listItem>
<bodyText confidence="0.998788842105263">
Once the lexical assignment is performed, in Brill&apos;s algorithm, each contextual rule
acquired during the training phase is applied to each sentence to be tagged. For each
individual rule, the algorithm scans the input from left to right while attempting to
match the rule.
This simple algorithm is computationally inefficient for two reasons. The first rea-
son for inefficiency is the fact that an individual rule is compared at each token of the
input, regardless of the fact that some of the current tokens may have been previously
examined when matching the same rule at a previous position. The algorithm treats
each rule as a template of tags and slides it along the input, one word at a time.
Consider, for example, the rule A B PREVBIGRAM C C that changes tag A to tag B if
the previous two tags are C.
When applied to the input CDCCA, the pattern CCA is compared three times to
the input, as shown in Figure 3. At each step no record of previous partial matches
or mismatches is remembered. In this example, C is compared with the second input
token D during the first and second steps, and therefore, the second step could have
been skipped by remembering the comparisons from the first step. This method is
similar to a naive pattern-matching algorithm.
The second reason for inefficiency is the potential interaction between rules. For
example, when the rules in Figure 1 are applied to sentence (3), the first rule results
</bodyText>
<sectionHeader confidence="0.369413" genericHeader="method">
CIA A
</sectionHeader>
<page confidence="0.973731">
230
</page>
<note confidence="0.711772">
Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging
</note>
<bodyText confidence="0.9895584">
in a change (6) that is undone by the second rule as shown in (9). The algorithm may
therefore perform unnecessary computation.
In summary, Brill&apos;s algorithm for implementing the contextual tagger may require
RKn elementary steps to tag an input of n words with R contextual rules requiring at
most K tokens of context.
</bodyText>
<subsectionHeader confidence="0.345659">
4. Construction of the Finite-State Tagger
</subsectionHeader>
<bodyText confidence="0.998218692307692">
We show how the function represented by each contextual rule can be represented
as a nondeterministic finite-state transducer and how the sequential application of
each contextual rule also corresponds to a nondeterministic finite-state transducer
being the result of the composition of each individual transducer. We will then turn
the nondeterministic transducer into a deterministic transducer. The resulting part-
of-speech tagger operates in linear time independent of the number of rules and the
length of the context. The new tagger operates in optimal time in the sense that the
time to assign tags to a sentence corresponds to the time required to follow a single
path in the resulting deterministic finite-state machine.
Our work relies on two central notions: the notion of a finite-state transducer and
the notion of a subsequential transducer. Informally speaking, a finite-state transducer
is a finite-state automaton whose transitions are labeled by pairs of symbols. The first
symbol is the input and the second is the output. Applying a finite-state transducer to
an input consists of following a path according to the input symbols while storing the
output symbols, the result being the sequence of output symbols stored. Section 8.1
formally defines the notion of transducer.
Finite-state transducers can be composed, intersected, merged with the union op-
eration and sometimes determinized. Basically, one can manipulate finite-state trans-
ducers as easily as finite-state automata. However, whereas every finite-state automa-
ton is equivalent to some deterministic finite-state automaton, there are finite-state
transducers that are not equivalent to any deterministic finite-state transducer. Trans-
ductions that can be computed by some deterministic finite-state transducer are called
subsequential functions. We will see that the final step of the compilation of our tag-
ger consists of transforming a finite-state transducer into an equivalent subsequential
transducer.
We will use the following notation when pictorially describing a finite-state trans-
ducer: final states are depicted with two concentric circles; c represents the empty
string; on a transition from state i to state j, a/b indicates a transition on input symbol
a and output symbol(s) b;2 a question mark (?) on an input transition (for example
labeled ?/b) originating at state i stands for any input symbol that does not appear as
input symbol on any other outgoing arc from i. In this document, each depicted finite-
state transducer will be assumed to have a single initial state, namely the leftmost
state (usually labeled 0).
We are now ready to construct the tagger. Given a set of rules, the tagger is
constructed in four steps.
The first step consists of turning each contextual rule found in Brill&apos;s tagger into a
finite-state transducer. Following the example discussed in Section 2, the functionality
of the rule vbn vbd PREVTAG np is represented by the transducer shown on the left of
Figure 4.
</bodyText>
<footnote confidence="0.69388">
2 When multiple output symbols are emitted, a comma symbolizes the concatenation of the output
symbols.
</footnote>
<page confidence="0.993056">
231
</page>
<figure confidence="0.995894">
Computational Linguistics Volume 21, Number 2
np/np vbn/vbd
</figure>
<figureCaption confidence="0.932219666666667">
Figure 4
Left: Transducer T1 representing the contextual rule vbn vbd PREVTAG np. Right: Local
extension LocExt(T1) of T1.
</figureCaption>
<figure confidence="0.997676166666667">
?/? by/by
vbd/vbd
vbd/vbn
b /by
vbd/vbd
0 12246..yb 02
</figure>
<figureCaption confidence="0.93101">
Figure 5
</figureCaption>
<bodyText confidence="0.97909025">
Left: Transducer T2 representing vbd vbn NEXTTAG by. Right: Local extension LocExt(T2) of T2
Each contextual rule is defined locally; that is, the transformation it describes must
be applied at each position of the input sequence. For instance, the rule
A B PREV10R2TAG C,
which changes A into B if the previous tag or the one before is C, must be applied
twice on CA A (resulting in the output C B B). As we have seen in the previous section,
this method is not efficient.
The second step consists of turning the transducers produced by the preceding step
into transducers that operate globally on the input in one pass. This transformation
is performed for each transducer associated with each rule. Given a function fi that
transforms, say, a into b (i.e. fi (a) =- b), we want to extend it to a function f2 such
that f2 (w) = w&apos; where w&apos; is the word built from the word w where each occurrence
of a has been replaced by b. We say that 12 is the local extension&apos; of Ii, and we write
f2 LocExt(fi). Section 8.2 formally defines this notion and gives an algorithm for
computing the local extension.
Referring to the example of Section 2, the local extension of the transducer for the
rule vbn vbd PREVTAG np is shown to the right of Figure 4. Similarly, the transducer for
the contextual rule vbd vbn NEXTTAG by and its local extension are shown in Figure 5.
The transducers obtained in the previous step still need to be applied one after
the other.
</bodyText>
<footnote confidence="0.566981">
3 This notion was introduced by Roche (1993).
</footnote>
<page confidence="0.956876">
232
</page>
<figure confidence="0.7489856">
Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging
vbd/vbn
Figure 6
Composition T3 = LocExt(Ti) o LocExt(T2).
a:a
</figure>
<figureCaption confidence="0.991303">
Figure 7
</figureCaption>
<subsectionHeader confidence="0.511308">
Example of a transducer not equivalent to any subsequential transducer.
</subsectionHeader>
<bodyText confidence="0.99999096">
The third step combines all transducers into one single transducer. This corre-
sponds to the formal operation of composition defined on transducers. The formaliza-
tion of this notion and an algorithm for computing the composed transducer are well
known and are described originally by Elgot and Mezei (1965).
Returning to our running example of Section 2, the transducer obtained by com-
posing the local extension of T2 (right in Figure 5) with the local extension of T1 (right
in Figure 4) is shown in Figure 6.
The fourth and final step consists of transforming the finite-state transducer ob-
tained in the previous step into an equivalent subsequential (deterministic) transducer.
The transducer obtained in the previous step may contain some nondeterminism. The
fourth step tries to turn it into a deterministic machine. This determinization is not al-
ways possible for any given finite-state transducer. For example, the transducer shown
in Figure 7 is not equivalent to any subsequential transducer. Intuitively speaking, this
transducer has to look ahead an unbounded distance in order to correctly generate
the output. This intuition will be formalized in Section 9.2.
However, as proven in Section 10, the rules inferred in Brill&apos;s tagger can always
be turned into a deterministic machine. Section 9.1 describes an algorithm for deter-
minizing finite-state transducers. This algorithm will not terminate when applied to
transducers representing nonsubsequential functions.
In our running example, the transducer in Figure 6 has some nondeterministic
paths. For example, from state 0 on input symbol vbd, two possible emissions are
possible: vbn (from 0 to 2) and vbd (from 0 to 3). This nondeterminism is due to the
rule vbd vbn NEXTTAG by, since this rule has to read the second symbol before it can
know which symbol must be emitted. The deterministic version of the transducer T3 is
shown in Figure 8. Whenever nondeterminism arises in T3, the deterministic machine
</bodyText>
<page confidence="0.991192">
233
</page>
<figure confidence="0.988461">
Computational Linguistics Volume 21, Number 2
?/vbd,?
</figure>
<figureCaption confidence="0.989624">
Figure 8
</figureCaption>
<bodyText confidence="0.986531538461539">
Subsequential form for T3.
emits the empty symbol e, and postpones the emission of the output symbol. For
example, from the start state 0, the empty string is emitted on input vbd, while the
current state is set to 2. If the following word is by, the two token string vbn by is
emitted (from 2 to 0), otherwise vbd is emitted (depending on the input from 2 to 2 or
from 2 to 0).
Using an appropriate implementation for finite-state transducers (see Section 11),
the resulting part-of-speech tagger operates in linear time, independently of the num-
ber of rules and the length of the context. The new tagger therefore operates in optimal
time.
We have shown how the contextual rules can be implemented very efficiently. We
now turn our attention to lexical assignment, the step that precedes the application of
the contextual transducer. This step can also be made very efficient.
</bodyText>
<subsectionHeader confidence="0.511677">
5. Lexical Tagger
</subsectionHeader>
<bodyText confidence="0.999839823529412">
The first step of the tagging process consists of looking up each word in a dictionary.
Since the dictionary is the largest part of the tagger in terms of space, a compact rep-
resentation is crucial. Moreover, the lookup process has to be very fast too otherwise
the improvement in speed of the contextual manipulations would be of little practical
interest.
To achieve high speed for this procedure, the dictionary is represented by a deter-
ministic finite-state automaton with both fast access and small storage space. Suppose
one wants to encode the sample dictionary of Figure 9. The algorithm, as described by
Revuz (1991), consists of first building a tree whose branches are labeled by letters and
whose leaves are labeled by a list of tags (such as nn vb) , and then minimizing it into
a directed acyclic graph (DAG). The result of applying this procedure to the sample
dictionary of Figure 9 is the DAG of Figure 10. When a dictionary is represented as
a DAG, looking up a word in it consists simply of following one path in the DAG.
The complexity of the lookup procedure depends only on the length of the word; in
particular, it is independent of the size of the dictionary.
The lexicon used in our system encodes 54,000 words. The corresponding DAG
takes 360Kb of space and provides an access time of 12,000 words per second.&apos;
</bodyText>
<page confidence="0.70482">
4 The size of the dictionary in plain text (ASCII form) is 742KB.
234
</page>
<figure confidence="0.959132333333333">
Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging
ads nns
bag nn vb
bagged vbn vbd
bayed vbn vbd
bids nns
</figure>
<figureCaption confidence="0.922214">
Figure 9
Sample dictionary.
Figure 10
DAG representation of the dictionary of Figure 9.
</figureCaption>
<sectionHeader confidence="0.931263" genericHeader="method">
6. Tagging Unknown Words
</sectionHeader>
<bodyText confidence="0.999957777777778">
The rule-based system described by Brill (1992) contains a module that operates after
all known wordsâthat is, words listed in the dictionaryâhave been tagged with their
most frequent tag, and before contextual rules are applied. This module guesses a
tag for a word according to its suffix (e.g. a word with an &amp;quot;ing&amp;quot; suffix is likely to be
a verb), its prefix (e.g. a word starting with an uppercase character is likely to be a
proper noun), and other relevant properties.
This module basically follows the same techniques as the ones used to implement
the lexicon. Because of the similarity of the methods used, we do not provide further
details about this module.
</bodyText>
<sectionHeader confidence="0.696597" genericHeader="method">
7. Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.995173315789474">
The tagger we constructed has an accuracy identical&apos; to Brill&apos;s tagger and comparable
to statistical-based methods. However, it runs at a much higher speed. The tagger
runs nearly ten times faster than the fastest of the other systems. Moreover, the finite-
state tagger inherits from the rule-based system its compactness compared with a
stochastic tagger. In fact, whereas stochastic taggers have to store word-tag, bigram,
and trigram probabilities, the rule-based tagger and therefore the finite-state one only
have to encode a small number of rules (between 200 and 300).
We empirically compared our tagger with Eric Brill&apos;s implementation of his tagger,
and with our implementation of a trigram tagger adapted from the work of Church
(1988) that we previously implemented for another purpose. We ran the three programs
on large files and piped their output into a file. In the times reported, we included
the time spent reading the input and writing the output. Figure 11 summarizes the
results. All taggers were trained on a portion of the Brown corpus. The experiments
were run on an HP720 with 32MB of memory. In order to conduct a fair comparison,
the dictionary lookup part of the stochastic tagger has also been implemented using
the techniques described in Section 5. All three taggers have approximately the same
5 Our current implementation is functionally equivalent to the tagger as described by Brill (1992).
However, the tagger could be extended to include recent improvements described in more recent
papers (Brill 1994).
</bodyText>
<figure confidence="0.859059">
â¢ 0 â¢ C) (nns)
nn,vb)
e d
(vbd,vbn)
235
Computational Linguistics Volume 21, Number 2
Stochastic Tagger Rule-Based Tagger Finite-State Tagger
Speed 1,200 w/s 500 w/s 10,800 w/s
Space 2,158KB 379KB 815KB
Figure 11
Overall performance comparison.
dictionary lookup unknown words contextual
Speed 12,800 w/s 16,600 w/s 125,100 w/s
Percent of the time 85% 6.5% 8.5%
</figure>
<figureCaption confidence="0.963527">
Figure 12
</figureCaption>
<bodyText confidence="0.960640391304348">
Speeds of the different parts of the program.
precision (95% of the tags are correct).6 By design, the finite-state tagger produces
the same output as the rule-based tagger. The rule-based taggerâand the finite-state
taggerâdo not always produce the exact same tagging as the stochastic tagger (they do
not make the same errors); however, no significant difference in performance between
the systems was detected.&apos;
Independently, Cutting et al. (1992) quote a performance of 800 words per second
for their part-of-speech tagger based on hidden Markov models.
The space required by the finite-state tagger (815KB) is distributed as follows:
363KB for the dictionary, 440KB for the subsequential transducer and 12KB for the
module for unknown words.
The speeds of the different parts of our system are shown in Figure 12.8
Our system reaches a performance level in speed for which other, very low-level
factors (such as storage access) may dominate the computation. At such speeds, the
time spent reading the input file, breaking the file into sentences, breaking the sen-
tences into words, and writing the result into a file is no longer negligible.
8. Finite-State Transducers
The methods used in the construction of the finite-state tagger described in the previ-
ous sections were described informally. In the following section, the notion of finite-
state transducer and the notion of local extension are defined. We also provide an
algorithm for computing the local extension of a finite-state transducer. Issues related
to the determinization of finite-state transducers are discussed in the section following
this one.
</bodyText>
<subsectionHeader confidence="0.99892">
8.1 Definition of Finite-State Transducers
</subsectionHeader>
<bodyText confidence="0.994868666666667">
A finite-state transducer T is a five-tuple (E, Q, i,F,E) where: E is a finite alphabet; Q is
a finite set of states or vertices; i E Q is the initial state; F C Q is the set of final states;
ECQx (E U {â¬}) x E* x Q is the set of edges or transitions.
</bodyText>
<footnote confidence="0.891907714285714">
6 For evaluation purposes, we randomly selected 90% of the Brown corpus for training purposes and
10% for testing.
7 An extended discussion of the precision of the rule-based tagger can be found in Brill (1992).
8 In Figure 12, the dictionary lookup includes reading the file, splitting it into sentences, looking up each
word in the dictionary, and writing the final result to a file. The dictionary lookup and the tagging of
unknown words take roughly the same amount of time, but since the second procedure only applies
on unknown words (around 10% in our experiments), the percentage of time it takes is much smaller.
</footnote>
<page confidence="0.994603">
236
</page>
<note confidence="0.794873">
Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging
</note>
<figureCaption confidence="0.947468">
Figure 13
</figureCaption>
<bodyText confidence="0.839122">
T4: Example of a finite-state transducer.
For instance, Figure 13 is the graphical representation of the transducer:
</bodyText>
<equation confidence="0.51825">
T4 = ({a, b, c, h, e}, {0, 1, 2,3}, 0, {3}, { (0, a, b,1), (0, a, c,2), (1, h, h, 3), (2, e, ei3)1)â¢
</equation>
<bodyText confidence="0.942402333333333">
A finite-state transducer T also defines a function on words in the following way:
the extended set of edges E, the transitive closure of E, is defined by the following
recursive relation:
</bodyText>
<listItem confidence="0.996946">
â¢ if e E E then e E
â¢ if (q, a, b, q&apos;), (qc a&apos; , q&amp;quot;) E t then (q, aa&apos; bb&apos; q&amp;quot;) E E.
</listItem>
<bodyText confidence="0.977744892857143">
Then the function f from E* to E* defined by f (w) = w&apos; iff q E F such that (i, w, w&apos; , q) E
E is the function defined by T. One says that T represents f and writes f = 11.
The functions on words that are represented by finite-state transducers are called
rational functions. If, for some input w, more than one output is allowed (e.g. f (w) =
{w1, w2, .}) then f is called a rational transduction.
In the example of Figure 13, 1E11 is defined by ILII(ah) -= bh and I T41(ae) = ce.
Given a finite-state transducer T = (E, Q, F, E), the following additional notions
are useful: its state transition function d that maps Q x (E u {â¬}) into 2Q defined by
d(q, a) = {q&apos; E Q ]w&apos; E E* and (q, a, w&apos; , q&apos;) E E}; and its emission function 5 that maps
Q x (E U {c}) x Q into 2E* defined by (q , a, q&apos;) = {w&apos; E E I (q, a, w,&apos; q&apos;) E E}.
A finite-state transducer could be seen as a finite-state automaton, where each
transition label is a pair. In this respect, T4 would be deterministic; however, since
transducers are generally used to compute a function, a more relevant definition
of determinism consists of saying that both the transition function d and the emis-
sion function (5 lead to sets containing at most one element, that is, Id(q, a)1 &lt; 1 and
18 (q , a, (3&apos;)I &lt; 1 (and that these sets are empty for a = e). With this notion, if a finite-state
transducer is deterministic, one can apply the function to a given word by determin-
istically following a single path in the transducer. Deterministic transducers are called
subsequential transducers (Schfitzenberger 1977).9 Given a deterministic transducer, we
can define the partial functions q 0a = q&apos; iff d(q, a) -= {q&apos;} and q * a = w&apos; iff E Q such
that q 0 a = q&apos; and 6 (q , a, q&apos;) = {w&apos;} . This leads to the definition of subsequential trans-
ducers: a subsequential transducer T&apos; is a seven-tuple (E, Q, i, F, 0, *, p) where: E, Q, i, F
are defined as above; 0 is the deterministic state transition function that maps Q x E
on Q, one writes q Oa = q&apos;; * is the deterministic emission function that maps Q x E on
E*, one writes q * a = w&apos;; and the final emission function p maps F on E*, one writes
p(q) = w.
For instance, T4 is not deterministic because d(0, a) =- {1,2}, but it is equivalent
to Ty represented Figure 14 in the sense that they represent the same function, i.e.
</bodyText>
<listItem confidence="0.538672">
9 A sequential transducer is a deterministic transducer for which all states are final. Sequential transducers
are also called generalized sequential machines (Eilenberg 1974).
</listItem>
<page confidence="0.948766">
237
</page>
<figure confidence="0.939518090909091">
Computational Linguistics Volume 21, Number 2
tilbh
Figure 14
Subsequential transducer T5.
Figure 15
T6: a finite-state transducer to be extended.
a a b cab
a a b c a b
b c b c
a a b c a b
dca
</figure>
<figureCaption confidence="0.940387">
Figure 16
</figureCaption>
<bodyText confidence="0.667361">
Top: Input. Middle: First factorization. Bottom: Second factorization.
</bodyText>
<equation confidence="0.937283">
1T41 = IT5 I. T5 is defined by T5 = (fa, b, c, h, el, {0, 1, 2}, 0, {2}, 0, *, p) where 0 0 a = 1,
0 * a = 6, 1 h = 2, 1 * h = bh, 1 e = 2, 1 * e = ce, and p(2) = â¬.
</equation>
<subsectionHeader confidence="0.998838">
8.2 Local Extension
</subsectionHeader>
<bodyText confidence="0.865397473684211">
In this section, we will see how a function that needs to be applied at all input positions
can be transformed into a global function that needs to be applied once on the input.
For instance, consider T6 of Figure 15. It represents the function f6 = IT61 such that
f6 (ab) -= bc and f6 (bca) = dca. We want to build the function that, given a word w, each
time w contains ab (i.e. ab is a factor of the word) (resp. bca), this factor is transformed
into its image bc (resp. dca). Suppose, for instance, that the input word is w = aabcab, as
shown in Figure 16, and that the factors that are in dom(f6)1Â° can be found according
to two different factorizations: i.e. w1 = a â¢ w2 â¢ C &apos; w211, where w2 = ab, and wi =
aa â¢ w3 â¢ b, where w3 = bca. The local extension of f6 will be the transduction that takes
each possible factorization and transforms each factor according to f6, i.e. f6 (w2)=
bc and f6(w3) = dca, and leaves the other parts unchanged; here this leads to two
outputs: abccbc according to the first factorization, and aadcab according to the second
factorization.
The notion of local extension is formalized through the following definition.
Definition
If f is a rational transduction from E* to E*, the local extension F = LocExt(f) is
the rational transduction from E* on E* defined in the following way: if u
a1b1a2b2 â¢ â¢ â¢ anbnan+i e E* then v = a1b&apos;1a2b; â¢ â¢ â¢ anbn&apos;an+1 E F(u) if a, E E* â (E* â¢
dom(f) â¢ E*), b, E dom(f) and b; E f (b, ).
</bodyText>
<page confidence="0.737786">
10 dom(f) denotes the domain off, that is, the set of words that have at least one output through f.
11 If w1, zU2 E E*, w2 denotes the concatenation of w1 and w2. It may also be written w1w2.
238
</page>
<figure confidence="0.982460575757576">
Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging
LocalExtension(r = (E, Q&apos;, , F&apos;, T = (E, Q, i, F, E))
1 C&apos; [0] = ({i} , identity); q = 0;i&apos; 0;F&apos; 0;E&apos; 0; = 0;Q&apos; = 0; C&apos; [1] -= (0, transduction); n = 2;
2 do{
3 (S, type) = C&apos;[q]; = Q&apos; U {q};
4 if (type == identity)
5 = F&apos; u {q}; E&apos; = U {(q, ?, ?, 0};
6 for each w E (Eu {c}) s.t. 3x E S. d(x, w) 0 and Vy E S, d(y, w) n F = 0
7 if al- E [0,n â 1] such that C&apos;[r1 == U d(x, w), identity)
xES
8 e r;
9 else
10 C&apos; [e = n + +] =- ({i} U d(x, w), identity);
xES
11 E&apos; = E&apos; u {(q,w,w,e)};
12 for each (i, w, w&apos;, x) E E
13 if 3r E [0,n â 1] such that C&apos;[r] ==- ({x}, transduction)
14 e =-- r;
15 else
16 C&apos; [e = n + +] = ({x} , transduction);
17 E&apos; = E&apos; U {(q, w, wi, e)};
18 for each WE (Eu {e}) s.t. a ES d(x, w) n F 0 0 then E&apos; = E&apos; U {(q,w,w,1)};
19 else if (type == transduction)
20 if axi E Q s.t. S == {xi}
21 if (xi E F) then E&apos; = E&apos; U {(q , E, E, 0)};
22 for each (xi, w, w&apos;, y) E E
23 if 3r E [0,n â 1] such that C&apos;[r] == ({y}, transduction)
24 e = r;
25 else
26 C[e = n + +] = ({y}, transduction);
27 E&apos; = E&apos; U { (q, w, wc e)};
28 q++;
29 1while(q &lt;n);
</figure>
<figureCaption confidence="0.8354035">
Figure 17
Local extension algorithm.
</figureCaption>
<bodyText confidence="0.998658153846154">
Intuitively, if F = LocExt(f) and w E E*, each factor of w in dom(f) is transformed
into its image by f and the remaining part of w is left unchanged. If f is represented
by a finite-state transducer T and LocExt(f) is represented by a finite-state transducer
T&apos;, one writes T&apos; = LocExt(T).
It could also be seen that if -y7- is the identity function on E* â (E* â¢ dom(T) â¢ E*),
then LocExt(T) =- (T -y7-)*.12 Figure 17 gives an algorithm that computes the local
extension directly.
The idea is that an input word is processed nondeterministically from left to right.
Suppose, for instance, that we have the initial transducer T7 of Figure 18 and that we
want to build its local extension, T8 of Figure 19.
When the input is read, if a current input letter cannot be transformed at the
initial state of T7 (the letter c for instance), it is left unchanged: this is expressed by
the looping transition on the initial state 0 of T8 labeled ?/?.&amp;quot; On the other hand,
</bodyText>
<footnote confidence="0.945757">
12 In this last formula, the concatenation â¢ stands for the concatenation of the graphs of each function;
that is, for the concatenation of the transducers viewed as automata whose labels are of the form a/b.
13 As explained before, an input transition labeled by the symbol ? stands for all transitions labeled with
a letter that doesn&apos;t appear as input on any outgoing arc from this state. A transition labeled ?/? stands
</footnote>
<page confidence="0.975575">
239
</page>
<figure confidence="0.92504725">
Computational Linguistics Volume 21, Number 2
Figure 18
Sample transducer T7.
els
</figure>
<figureCaption confidence="0.962266">
Figure 19
</figureCaption>
<bodyText confidence="0.978958260869565">
Local extension T8 of T7: T8 = LOCEXt(T7).
if the input symbol, say a, can be processed at the initial state of T7, one doesn&apos;t
know yet whether a will be the beginning of a word that can be transformed (e.g. ab)
or whether it will be followed by a sequence that makes it impossible to apply the
transformation (e.g. ac). Hence one has to entertain two possibilities, namely (1) we
are processing the input according to T7 and the transitions should be a/b; or (2) we
are within the identity and the transition should be a/a. This leads to two kind of
states: the transduction states (marked transduction in the algorithm) and the identity
states (marked identity in the algorithm). It can be seen in Figure 19 that this leads
to a transducer that has a copy of the initial transducer and an additional part that
processes the identity while making sure it could not have been transformed. In other
words, the algorithm consists of building a copy of the original transducer and at the
same time the identity function that operates on E* â E* â¢ dom(T) â¢ E*.
Let us now see how the algorithm of Figure 17 applies step by step to the trans-
ducer T7 of Figure 18, producing the transducer T8 of Figure 19.
In Figure 17, C&apos; [0] = ({i}, identity) of line 1 states that state 0 of the transducer to
be built is of type identity and refers to the initial state i = 0 of T7. q represents the
current state and n the current number of states. In the loop do{. â¢} while (q &lt; n), one
builds the transitions of each state one after the other: if the transition points to a state
not already built, a new state is added, thus incrementing n. The program stops when
all states have been inspected and when no additional state is created. The number of
iterations is bounded by 211 TII*2, where TI = IQ&apos; is the number of states of the original
transducer.&amp;quot; Line 3 says that the current state within the loop is q and that this state
</bodyText>
<footnote confidence="0.4958405">
for all the diagonal pairs a la s.t. a is not an input symbol on any outgoing arc from this state.
14 In fact, Q&apos; C 2Q x ftransduction,identityl. Thus, q &lt; 221Q1.
</footnote>
<page confidence="0.994217">
240
</page>
<note confidence="0.89066">
Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging
</note>
<figureCaption confidence="0.934332">
Figure 20
Local extension T9 of T6: T9 = LOCEXt(T6)-
</figureCaption>
<bodyText confidence="0.999895153846154">
refers to the set of states S and is marked by the type type. In our example, at the
first occurrence of this line, S is instantiated to {0} and type = identity. Line 5 adds
the current identity state to the set of final states and a transition to the initial state
for all letters that do not appear on any outgoing arc from this state. Lines 6-11 build
the transitions from and to the identity states, keeping track of where this leads in the
original transducer. For instance, a is a label that verifies the conditions of line 6. Thus
a transition a/a is to be added to the identity state 2, which refers to 1 (because of the
transition a/b of T7) and to i 0 (because it is possible to start the transduction T7
from any identity state). Line 7 checks that this state doesn&apos;t already exist and adds it
if necessary. e -= n++ means that the arrival state for this transition, i.e. d(q, w), will be
the last added state and that the number of states being built has to be incremented.
Line 11 actually builds the transition between 0 and e = 2 labeled a/a. Lines 12-17
describe the fact that it is possible to start a transduction from any identity state. Here
a transition is added to a new state, i.e. a/b to 3. The next state to be considered is 2
and it is built like state 0, except that the symbol b should block the current output. In
fact, state 1 means that we already read a with a as output; thus, if one reads b, ab is
at the current point, and since ab should be transformed into bc, the current identity
transformation (that is a â&gt; a) should be blocked: this is expressed by the transition b/b
that leads to state 1 (this state is a &amp;quot;trash&amp;quot; state; that is, it has no outgoing transition
and it is not final).
The following state is 3, which is marked as being of type transduction, which
means that lines 19-27 should be applied. This consists simply of copying the transi-
tions of the original transducer. If the original state was final, as for 4 -= ({2}, transduction),
an â¬/e transition to the initial state is added (to get the behavior of T+).
The transducer T9 = LOCEXt(T6) of Figure 20 gives a more complete (and slightly
more complex) example of this algorithm.
</bodyText>
<page confidence="0.995166">
241
</page>
<note confidence="0.68527">
Computational Linguistics Volume 21, Number 2
</note>
<sectionHeader confidence="0.887403" genericHeader="method">
9. Determinization
</sectionHeader>
<bodyText confidence="0.999797666666667">
The basic idea behind the determinization algorithm comes from Mehryar Mohri.&apos;
In this section, after giving a formalization of the algorithm, we introduce a proof of
soundness and completeness, and we study its worst-case complexity.
</bodyText>
<subsectionHeader confidence="0.991035">
9.1 Determinization Algorithm
</subsectionHeader>
<bodyText confidence="0.99976">
In the following, for w1, w2 E E*, w1 A w2 denotes the longest common prefix of wi
and w2.
The finite-state transducers we use in our system have the property that they can be
made deterministic; that is, there exists a subsequential transducer that represents the
same function.&apos; If T = (E, Q, i,F,E) is such a finite-state transducer, the subsequential
</bodyText>
<listItem confidence="0.738415">
transducer T&apos; = (E, Q&apos;, 0, *, p) defined as follows will be later proved equivalent
to T:
â¢ Q&apos; C 212)&lt;E* . In fact, the determinization of the transducer is related to
</listItem>
<bodyText confidence="0.640052571428572">
the determinization of FSAs in the sense that it also involves a power set
construction. The difference is that one has to keep track of the set of
states of the original transducer one might be in and also of the words
whose emission have been postponed. For instance, a state
{(qi, w1), (q2, w2)} means that this state corresponds to a path that leads
to qi and q2 in the original transducer and that the emission of zai (resp.
w2) was delayed for qi (resp. q2).
</bodyText>
<listItem confidence="0.999696">
â¢ = f(i, E) }. There is no postponed emission at the initial state.
â¢ the emission function is defined by:
</listItem>
<equation confidence="0.97526">
S * a = A A u â¢ 6(q&apos;a&apos;q/)
0,0Es q&apos;Ed(q,a)
</equation>
<bodyText confidence="0.998245">
This means that, for a given symbol, the set of possible emissions is
obtained by concatenating the postponed emissions with the emission at
the current state. Since one wants the transition to be deterministic, the
actual emission is the longest common prefix of this set.
</bodyText>
<listItem confidence="0.984332">
â¢ the state transition function is defined by:
</listItem>
<equation confidence="0.873126">
S a = U U {(qi, (S * arl â¢ u â¢ S(q,a, q1))}
(q,u)ES q&apos;Ed(q,a)
Given u,v E E*, u v denotes the concatenation of u and v and
u-1 â¢ v = w, if w is such that u w = v, u-1 = 0 if no such w exists.
</equation>
<listItem confidence="0.997601333333333">
â¢ F&apos; = IS E Q&apos; I 3(q,u) Sand q E F1
â¢ if S E F&apos;, p(S) = u s.t. q c F, (q, u) ES. We will see in the proof of
correctness that p is properly defined.
</listItem>
<footnote confidence="0.490839">
15 Mohri (1994b) also gives a formalization of the algorithm.
16 As opposed to automata, a large class of finite-state transducers do not have any deterministic
representation; they cannot be determinized.
</footnote>
<page confidence="0.991838">
242
</page>
<note confidence="0.853171">
Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging
</note>
<bodyText confidence="0.99911725">
The determinization algorithm of Figure 21 computes the above subsequential
transducer.
Let us now apply the determinization algorithm of Figure 21 on the finite-state
transducer T4 of Figure 13 and show how it builds the subsequential transducer Tio
of Figure 22. Line 1 of the algorithm builds the first state and instantiates it with the
pair { (0, E) }. q and n respectively denote the current state and the number of states
having been built so far. At line 5, one takes all the possible input symbols w; here
only a is possible. w&apos; of line 6 is the output symbol,
</bodyText>
<equation confidence="0.99474475">
= â¢ ( A 8(0,a,q&apos;)),
thus w&apos; = 6(0,a, 1) A 5(0,a,2) = b Ac = c. Line 8 is then computed as follows:
= U U {,77&apos; , E-1 5(0,a,71)},
qE{O}F1&apos;Efl,21
</equation>
<bodyText confidence="0.9762636">
thus S&apos; = 1(1, 8(0,a,1))}U{ (2, S(0,a, 2)} = {(1, b), (2, c)}. Since nor verifies the condition
on line 9, a new state e is created to which the transition labeled alw=alc points and
n is incremented. On line 15, the program goes to the construction of the transitions
of state 1. On line 5, d and e are then two possible symbols. The first symbol, h, at line
6, is such that w&apos; is
</bodyText>
<equation confidence="0.8655">
W&apos; = A b â¢ 6(1, h,71-&apos;)) = bh.
7&apos;Ed(1,h)={2}
</equation>
<bodyText confidence="0.927139714285714">
Henceforth, the computation of line 8 leads to
s&apos;= U U {(4&apos; , (bh)-1 â¢ b â¢ h)} = {(2, Of.
State 2 labeled {(2, e)} is thus added, and a transition labeled hlbh that points to state
2 is also added. The transition for the input symbol e is computed the same way.
The subsequential transducer generated by this algorithm could in turn be min-
imized by an-algorithm described in Mohri (1994a). However, in our case, the trans-
ducer is nearly minimal.
</bodyText>
<subsectionHeader confidence="0.999911">
9.2 Proof of Correctness
</subsectionHeader>
<bodyText confidence="0.999280846153846">
Although it is decidable whether a function is subsequential or not (Choffrut 1977),
the determinization algorithm described in the previous section does not terminate
when run on a nonsubsequential function.
Two issues are addressed in this section. First, the proof of soundness: the fact that
if the algorithm terminates, then the output transducer is deterministic and represents
the same function. Second, the proof of completeness: the algorithm terminates in the
case of subsequential functions.
Soundness and completeness are a consequence of the main proposition, which
states that if a transducer T represents a subsequential function f, then the algorithm
DeterminizeTransducer described in the previous section applied on T computes a sub-
sequential transducer representing the same function.
In order to simplify the proofs, we will only consider transducers that do not have
E input transitions, that is ECQxEx E* x Q, and also without loss of generality,
</bodyText>
<page confidence="0.995622">
243
</page>
<figure confidence="0.744514545454546">
Computational Linguistics Volume 21, Number 2
DeterminizeTransducer(T&apos; = (E, Q&apos;, ,F&apos; , 0, *, p),T = (E, Q, i,F , E))
1 = 0;q = 0;n 1; C [0] = {(0 , 6)}; F&apos; = 0; = 0;
2 do{
3 S = C[q]; Q&apos; = U {q};
4 if ](4, u) E s s.t. E F then F&apos; =- F&apos; u {q} and p(q) = u;
5 foreach w such that ](q, E S and d(ij, w) 0 {
6 A A u â¢ 6(q,
(q,u)ES q&apos;Ed(i7,w)
7 q * w = w&apos; ;
8 = U U {w,w1-1 â¢ u â¢ 6(4,w4))};
(,,,Es F7&apos; Ed(i,w)
9 if 3r E [0,n â 1] such that Cqr] == S&apos;
10 e = r;
11 else
12 C&apos; [e = n + +] = S&apos; ;
13 q0w=-e;
14
15 q + +;
16 }while(q &lt; n);
Figure 21
Determinization algorithm.
</figure>
<figureCaption confidence="0.909148">
Figure 22
</figureCaption>
<bodyText confidence="0.951494125">
Subsequential transducer T10 such that ITto = T4
transducers that are reduced and that are deterministic in the sense of finite-state
automata.&apos;
In order to prove this proposition, we need to establish some preliminary notations
and lemmas.
First we extend the definition of the transition function d, the emission function Ã¶,
the deterministic transition function 0, and the deterministic emission function * on
words in the classical way. We then have the following properties:
</bodyText>
<equation confidence="0.9694466">
d(q, ab) -= U cl(qCb)
q&apos;Ed(q,a)
(q , ab, qz) = (5 ((lb, a, q&apos;) â¢
fq&apos;Ed(qi,a)1q2Ed(0)}
q ab = (q 0 a) b
</equation>
<page confidence="0.836922">
17 A transducer defines an automaton whose labels are the pairs &amp;quot;input/output&amp;quot;; this automaton is
</page>
<note confidence="0.722862">
assumed to be deterministic.
</note>
<page confidence="0.994899">
244
</page>
<note confidence="0.913331">
Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging
</note>
<equation confidence="0.560443">
q * ab = (q * a) â¢ (q a) * b
</equation>
<bodyText confidence="0.910294625">
For the following, it is useful to note that if 171 is a function, then 6&apos; is a function
too.
The following lemma states an invariant that holds for each state S built within
the algorithm. The lemma will later be used for the proof of soundness.
Lemma 1
Let I = C&apos;[0] be the initial state. At each iteration of the &amp;quot;do&amp;quot; loop in Determinize-
Transducer, , for each S = C&apos; [q] and for each w E E* such that I w S, the following
holds:
</bodyText>
<equation confidence="0.9826926">
(i) /* w = A S(i,w, q)
qEd(i,w)
(ii) S = I w = {(q , u) I q e d(i, w) and u = (1* w) â¢ (i, w, q)}
Proof
(i) and (ii) are obviously true for S = I (since d(i, c) = i and 8(i, â¬, i) = e), and we
</equation>
<bodyText confidence="0.988073666666667">
will show that given some w E E* if it is true for S w, then it is also true for
Si =S0a=Iowa for all a E E.
Assuming that (i) and (ii) hold for S and w, then for each a E E:
</bodyText>
<equation confidence="0.993033">
A (i, w, q) â¢ 5(q, a, q&apos;) = (1* w) â¢ . A ((1* w)-1 â¢ 6 (i, w, q)) â¢ 6 (q , a, q&apos;)
qEd(i,w),q&apos;Ed(q,a) qEd(i,w),q&apos;Ed(q,a)
=_ (1* â¢ A u â¢ (q, a, q&apos;)
(q,u)ES=10w,q&apos; Ed (q,a)
* W) â¢ (S * a)
I*wâ¢(I0w)*a
= I * wa
</equation>
<bodyText confidence="0.940760666666667">
This proves (i).
We now turn to (ii). Assuming that (i) and (ii) hold for S and w, then for each
a E E, let Si -=- S 0 a; the algorithm (line 8) is such that
</bodyText>
<equation confidence="0.932885">
Si = {(qc u&apos;) I 3 (q , u) E S, q&apos; d(q, a) and u&apos; = (S * a)-1 â¢ u â¢ (5(q , a, q&apos;)}
Let
S2 -= {(qc u&apos;) q&apos; E d(i,wa) and u&apos; = (I * wa).&apos; â¢ b(i,wa, q&apos;)}
</equation>
<bodyText confidence="0.743308">
We show that Si c S2. Let (qcu&apos;) E Si, then ](q,u) c S s.t. q&apos; E d(q, a) and
</bodyText>
<equation confidence="0.9858175">
u&apos; = (S * a)l â¢ u â¢ (q , a, q&apos;). Since u = (I * w)l â¢ w, q), then u&apos; = (S * a)l â¢ (I * w)-1 â¢
w, q) â¢ 8(q, a, q&apos;); that is, u&apos; (I * wa)l â¢ 6(i, wa, q&apos;). Thus (qc u&apos;) E S2. Hence Si C 52.
</equation>
<bodyText confidence="0.990378">
We now show that S2 C Si. Let (q&apos; , u&apos;) E 52, and let q E d(i,w) be s.t. q&apos; E d(q, a)
and u = (I * wri (i, q) then (q , u) E S and since u&apos; = (I * wa) â¢ 6 (i, wa, q&apos;) =
</bodyText>
<equation confidence="0.939473">
(S *a) u â¢ 6 (q , a, q&apos; ), (qc u&apos;) E
</equation>
<bodyText confidence="0.989039">
This concludes the proof of (ii).
</bodyText>
<page confidence="0.995545">
245
</page>
<note confidence="0.749667">
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.9964415">
The following lemma states a common property of the state S, which will be used
in the complexity analysis of the algorithm.
</bodyText>
<equation confidence="0.53840275">
Lemma 2
Each S -= C&apos;[q] built within the &amp;quot;do&amp;quot; loop is s.t. Vq E Q, there is at most one pair
(q, w) E S with q as first element.
Proof
</equation>
<bodyText confidence="0.831588666666667">
Suppose (q,wi) e Sand (q,w2) E 5, and let w be s.t. I w = S. Then w1 = (I * w)-1 â¢
6(i, w, q) and w2 = (I * w)-1 â¢ S(i,w,q). Thus w1 = w2. 0
The following lemma will also be used for soundness. It states that the final state
emission function is indeed a function.
Lemma 3
For each S built in the algorithm, if (q,u),(q&apos;,u&apos;) E 5, then q, q&apos; E F u = u&apos;
</bodyText>
<subsectionHeader confidence="0.373449">
Proof
</subsectionHeader>
<bodyText confidence="0.955440923076923">
Let S be one state set built in line 8 of the algorithm. Suppose (q,u),(q&apos;,u&apos;) E S and q,
q&apos; E F. According to (ii) of lemma 1, u = (I*w)-1 â¢ 6(i, w, q) and u&apos; = (I *w)-1 6(i, w, q&apos;).
Since I TI is a function and (5(i,w,q),(5(i,w,q/)} E I TI (w) then S(i,w,q) = 6(i,w,q/),
therefore u = u&apos;.
The following lemma will be used for completeness.
Lemma 4
Given a transducer T representing a subsequential function, there exists a bound M
s.t. for each S built at line 8, for each (q, u) E S, lul &lt; M.
We rely on the following theorem proven by Choffrut (1978):
Theorem 1
A function f on E* is subsequential iff it has bounded variations and for any rational
language L C E*, f&apos; (L)is also rational.
with the following two definitions:
</bodyText>
<subsectionHeader confidence="0.518244">
Definition
</subsectionHeader>
<bodyText confidence="0.971613">
The left distance between two strings u and v is II u, vii = u I + Iv&apos; - 2Iu A vi.
</bodyText>
<subsectionHeader confidence="0.527576">
Definition
</subsectionHeader>
<bodyText confidence="0.852246333333333">
A function f on E* has bounded variations iff for all k &gt; 0, there exists K &gt; 0 s.t.
u,v E dom(f), k = Ilf(u),f(v)ii &lt;K.
Proof of Lemma 4
II
Let f = I Ti. For each q E Q, let c(q) be a string w s.t. d(q,w) n F 0 and s.t. lw I is
minimal among such strings. Note that Ic(q)i &lt; ii Ti where TII is the number of states
</bodyText>
<page confidence="0.995469">
246
</page>
<note confidence="0.883897">
Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging
</note>
<bodyText confidence="0.681362333333333">
in T. For each q E Q let s(q) e Q be a state s.t. s(q) E d(q,c(q)) nF. Let us further define
max
qEQ
</bodyText>
<equation confidence="0.917578">
M2 max jc(q)1
qEQ
</equation>
<bodyText confidence="0.7301425">
Since f is subsequential, it is of bounded variations, therefore there exists K s.t. if
I lu, vii&lt; 2M2 then lif(u), f (v) &lt;K. Let M = K + 2M1.
Let S be a state set built at line 8, let w be s.t. /Ow = S and A = /*w. Let (qi, u) E S.
Let (q2, v) E S be s.t. u A v =- E. Such a pair always exists, since if not
</bodyText>
<equation confidence="0.919886">
I A u&apos;i &gt; 0
(q&apos;,u&apos;)ES
thus IA â¢ A u&apos;l = I A A â¢ u&apos;l &gt; 1A1
(qcu&apos;)ES (q&apos;,u9ES
Thus, because of (ii) in Lemma 1,
I A S(i,w, 171)1 &gt; * wl
q&apos;Ed(i,w)
</equation>
<bodyText confidence="0.981284454545455">
which contradicts (i) in Lemma 1.
Let co = (q c(qi), s(qi)) and co&apos; = 5(q2, c(q2), s(q2)).
Moreover, for any a,b,c,d E Ilab,cd11 + 1bl + &apos;di. In fact, Ilab,cdil =
labi + icd1-21abAcd1 -= lal+Ici +1/71+1d1 -21ab Acd1 = Ila,c11+21aAcl+lbl+Idl -21abAcdi
but labAcdl &lt; laAcl+Ibl+Idl and since Ilab,cdll = ha, cll-2(labAcdlâIaAcl--IbiâidDâIblâIdi
one has iia,clI ilab,cd11 + Ibi +
Therefore, in particular, AvIl 11 Auco, Avw&apos; II + Iwi + 1, thus lul 5- (w â¢
c(qi)), f (w â¢ c(q2))11 + 2/1/11. But Ilw â¢ c(qi),w â¢ c(q2)1I :5 Ic(91)1 + Ic(q2)1 2A42, thus Ilf(w â¢
c(q1)), f (w c(q2))II K and therefore ui K + 2A/11 = M. 0
The time is now ripe for the main proposition, which proves soundness and com-
pleteness.
</bodyText>
<subsectionHeader confidence="0.970286">
Proposition
</subsectionHeader>
<bodyText confidence="0.999839">
If a transducer T represents a subsequential function f, then the algorithm Determinize-
Transducer described in the previous section applied on T computes a subsequential
transducer T representing the same function.
</bodyText>
<subsectionHeader confidence="0.950602">
Proof
</subsectionHeader>
<bodyText confidence="0.977679142857143">
Lemma 4 shows that the algorithm always terminates if 171 is subsequential.
Let us show that dom(17-1) C dOM(iTI). Let w E E* s.t. w is not in dom(ITI), then
d(i, w) n F = O. Thus, according to (ii) of Lemma 1, for all (q, u) e I w, q is not in F,
thus I w is not terminal and therefore w is not in dom(r).
Conversely, let w E dOM(ITI). There exists a qf E F s.t. ITI(w) = S(i,w,qf) and s.t.
qf E d(i,w). Therefore 171(w) = (I * w) â¢ ((I * w)-1 w, qf)) and according to (ii) of
Lemma 1 (qf, (I * w)-1 â¢ 6(i,w,q1)) c I 0 w and since qf e F, Lemma 3 shows that
</bodyText>
<equation confidence="0.480651">
p(I 0w) =- (I * w)-1 â¢ 6(i, w, qf), thus ITI(w) = (I * w) â¢ p(I w) --= 17-1(w). 0
</equation>
<page confidence="0.992961">
247
</page>
<note confidence="0.744915">
Computational Linguistics Volume 21, Number 2
</note>
<subsectionHeader confidence="0.986512">
9.3 Worst-Case Complexity
</subsectionHeader>
<bodyText confidence="0.999874428571429">
In this section we give a worst-case upper bound of the size of the subsequential
transducer in terms of the size of the input transducer.
Let L = 1w E E* s.t. 1w1 &lt; Ml, where M is the bound defined in the proof
of Lemma 4. Since, according to Lemma 2, for each state set Q&apos;, for each q E Q, Q&apos;
contains at most one pair (q, w), the maximal number N of states built in the algorithm
is smaller than the sum of the number of functions from states to strings in L for each
state set, that is
</bodyText>
<equation confidence="0.925172">
N &lt; E
(2&apos;E2(2
</equation>
<bodyText confidence="0.918680545454546">
we thus have N &lt;21Q1 x 1410 = 210 x 21Q1xlog2 ILI
and therefore N &lt; 21(210+10g11,1)
Moreover,
ILI = 1 + 1E1 + â¢ â¢ â¢ +1E1A4 _ if 1E1 &gt; 1 = â 1
and IL 1 M + 1 if 1E1 -= 1. In this last formula, M = K + 2Mi, as described in Lemma 4.
Note that if P = MAX,EEIS (q , a, q&apos;)I is the maximal length of the simple transitions
emissions, MI &lt;IQI x P, thus M &lt; K + 2 x QI x P.
Therefore, if 1E1 &gt; 1, the number of states N is bounded:
N &lt; 21(21x (1-00g 1E 1(K+2x 1(21 xr+i
1E1-1 )
and if 1E1 = 1, N &lt; 2IQIx (1+log (KÂ±2x IQ&apos; xP+1)).
</bodyText>
<subsectionHeader confidence="0.598551">
10. Subsequentiality of Transformation-Based Systems
</subsectionHeader>
<bodyText confidence="0.99864">
The proof of correctness of the determinization algorithm and the fact that the algo-
rithm terminates on the transducer encoding Brill&apos;s tagger show that the final function
is subsequential and equivalent to Brill&apos;s original tagger.
In this section, we prove in general that any transformation-based system, such as
those used by Brill, is a subsequential function. In other words, any transformation-
based system can be turned into a deterministic finite-state transducer.
We define transformation-based systems as follows.
</bodyText>
<sectionHeader confidence="0.695066" genericHeader="method">
Definition
</sectionHeader>
<bodyText confidence="0.999919181818182">
A transformation-based system is a finite sequence (fi, fn) of subsequential func-
tions whose domains are bounded.
Applying a transformation-based system consists of applying each function f, one
after the other. Applying one function consists of looking for the first position in
the input at which the function can be triggered. When the function is triggered,
the longest possible string starting at that position is transformed according to this
function. After the string is transformed, the process is iterated starting at the end of
the previously transformed string. Then, the next function is applied. The program
ends when all functions have been applied.
It is not true that, in general, the local extension of a subsequential function is
subsequential.&apos; For instance, consider the function fa of Figure 23.
</bodyText>
<page confidence="0.9304235">
18 However, the local extensions of the functions we had to compute were subsequential.
248
</page>
<note confidence="0.7585065">
Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging
a:b a:b a:b
</note>
<figureCaption confidence="0.9438925">
Figure 23
Function fa.
</figureCaption>
<bodyText confidence="0.9994493">
The local extension of the function f, is not a function. In fact, consider the input
string daaaad; it can be decomposed either into d â¢ aaa ad or into da â¢ aaa d. The first
decomposition leads to the output dbbbad, and the second one to the output dabbbd.
The intended use of the rules in the tagger defined by Brill is to apply each
function from left to right. In addition, if several decompositions are possible, the one
that occurs first is the one chosen. In our previous example, it means that only the
output dbbbad is generated.
This notion is now defined precisely.
Let a be the rational function defined by a(a) = a for a E E, a(I) = a(I) = c on the
additional symbols T and &apos;1&apos;, with a such that a(u â¢ v) = a(u) â¢ a(v).
</bodyText>
<subsectionHeader confidence="0.40042">
Definition
</subsectionHeader>
<bodyText confidence="0.5423985">
Let Y c E+ and X = E*-E*â¢Yâ¢E*, a Y-decomposition of x is a stringy E Xâ¢([â¢Yâ¢Iâ¢X)*
s.t. a(y) = x
For instance, if Y = dom(fa) = {aaa}, the set of Y-decompositions of x = daaad is
{d[aaa]ad, da[aaa]d}
</bodyText>
<subsectionHeader confidence="0.665039">
Definition
</subsectionHeader>
<bodyText confidence="0.967223714285714">
Let &lt; be a total order on E and let E = E U {[, 1} be the alphabet E with the two
additional symbols T and &apos;1&apos;. Let extend the order &gt; to E by Va E E, T&lt; a and
a &lt; &lt; defines a lexicographic order on r* that we also denote &lt;. Let Y c E+
and x E E*, the minimal Y-decomposition of x is the Y-decomposition which is
minimal in (V, &lt;).
For instance, the minimal dom(fa)-decomposition of daaaad is d[aaa]ad. In fact,
d[aacdad &lt; da[aaa]cl.
</bodyText>
<subsectionHeader confidence="0.880552">
Proposition
</subsectionHeader>
<bodyText confidence="0.983812">
Given Y c E+ finite, the function mdy that to each x E E* associates its minimal
Y-decomposition, is subsequential and total.
</bodyText>
<subsectionHeader confidence="0.890246">
Proof
</subsectionHeader>
<bodyText confidence="0.9597964">
Let dec be defined by dec(w) = u â¢ [ â¢ v .1 dec((uv)-1 w), where u, v E E* are s.t. v E Y,
]v&apos; E E* with w = uvvi and lu I is minimal among such strings and dec(w) = w if no
such u, v exists. The function mdy is total because the function dec always returns an
output that is a &apos;Y-decomposition of w.
We shall now prove that the function is rational and then that it has bounded
variations; this will prove according to Theorem 1 that the function is subsequential.
In the following X = E* - E* â¢ Yâ¢E*. The transduction Ty that generates the set of
Y-decompositions is defined by
Ty -= Idx â¢ (E/[ â¢ Idy c/1 â¢ Idx)*
where Idx (resp. Idy) stands for the identity function on X (resp. Y). Furthermore,
</bodyText>
<page confidence="0.992115">
249
</page>
<figure confidence="0.680802">
Computational Linguistics Volume 21, Number 2
</figure>
<figureCaption confidence="0.7466026">
Figure 24
Transduction TE,âº.
the transduction TE,&gt; that to each string w E E* associates the set of strings strictly
greater than w, that is T&gt; (w) -= {w&apos; E E*1w &lt; wq, is defined by the transducer of
Figure 24, in which A = {(x,x)lx E E}, B = {(x,y) E E2IX &lt;y}, C = D = {c} x
</figureCaption>
<bodyText confidence="0.991001727272727">
and E = E x {c}.19
Therefore, the right-minimal Y-decomposition function mdy is defined by mdy =
Ty â (Tb,âº o Ty), which proves that mdy is rational.
Let k &gt; 0. Let K =6x k+6x M, where M = max,Ey Ixl. Let u, v E E* be s.t.
iiu,vii &lt;k. Let us consider two cases: (i) lu A 7.)I &lt;M and (ii) lu A vi &gt; M.
(i): IU A vi M, thus 1u1,1v1 &lt; lu A VI +1114/VII M k. Moreover, for each w E E*,
for each Y-decomposition w&apos; of w, Iw&apos;I &lt; 3 x iwi. In fact, Y doesn&apos;t contain c, thus the
number of [ (resp. 1) in w&apos; is smaller than Iwl. Therefore, Imdy(u)I, Imdy(v)I &lt; 3 x (M+k)
thus iimdy(u),mdy(v)11 &lt;K.
(ii): uAv = Aâ¢co with &apos;col = M. Let u, v be s.t. u Awp, and v = Awv. Let A&apos;,
co&apos;, it&apos;, A&amp;quot;, co&amp;quot; and v&amp;quot; be s.t. mdy(u) = A&apos;w&apos; mdy(v) = A&amp;quot;w&amp;quot;v&amp;quot;, a(A&apos;) = a(A&amp;quot;) -= A,
a(co&apos;) = a(w&amp;quot;) = w, a(p,&apos;) = p, and a(v&amp;quot;) = v. Suppose that A&apos; 0 A&amp;quot;, for instance
A&apos; &lt; A&amp;quot;. Let i be the first indice s.t. (A&apos;), &lt; (A&amp;quot;)1.2Â° We have two possible situations:
(ii.1) (A&apos;), = [ and A&amp;quot; E E or (A&amp;quot;), = 1. In that case, since the length of the elements in
Y is smaller than M = lwl ,one has A&apos;w&apos; = [A21A3 with lAi I = 1, A2 E Y and A3 E E*.
We also have A&amp;quot;w&amp;quot; = A1A;A&apos;3 with a(Al2) = a(A2) and the first letter of A/2 is different
from 1. Let A4 be a Y-decomposition of a(Av&amp;quot;), then Ai [A21A4 is a Y-decomposition of
v strictly smaller than A1 A&apos;2A&apos;3v&amp;quot; = mdy (v), which contradicts the minimality of mdy (v).
The second situation is (ii.2): (A&apos;), E E and (A&amp;quot;)/ = 1, then we have A&apos;w&apos; = A1[A2A3]A4
s.t. lA1[A21 i and A&amp;quot;w&amp;quot; = A1[A2]A&apos;3A&apos;4 s.t. a(A13) = a(A3) and a(A,) = a(A4). Let A5 be
a Y-decomposition of Au&amp;quot;, then A1[A2A3]A5 is a Y-decomposition of v strictly smaller
than A&amp;quot;w&amp;quot; v&amp;quot; , which leads to the same contradiction. Therefore, A&apos; = A&amp;quot; and since
</bodyText>
<equation confidence="0.622336">
11L/1-01&amp;quot;1 3 x (Ip1+10) = 3 x Ilu,v11 &lt; 3 xk, Ilnidy(u),m4(v)H lwii+ice&amp;quot;1+10+Iv&amp;quot;I 5_
2 x M + 3 xk&lt; K. This proves that mdy has bounded variations and therefore that it
is subsequential. 0
</equation>
<bodyText confidence="0.99885">
We can now define precisely what is the effect of a function when one applies it
from left to right, as was done in the original tagger.
</bodyText>
<footnote confidence="0.943938333333333">
19 This construction is similar to the transduction built within the proof of Eilenberg&apos;s cross section
theorem (Eilenberg 1974).
20 (w), refers to the 1th letter in w.
</footnote>
<page confidence="0.98861">
250
</page>
<note confidence="0.892028">
Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging
</note>
<bodyText confidence="0.9706255">
Definition
If f is a rational function with bounded domain, Y = dom(f) C E+ , the right-minimal
local extension of f, denoted RmLocExt(f), is the composition of a right-minimal
Y-decomposition mdy with IdE.. â¢ ([/e .1. Ve â¢ Id*).
RmLocExt being the composition of two subsequential functions, it is itself subse-
quential; this proves the following final proposition, which states that given a rule-
based system similar to Brill&apos;s system, one can build a subsequential transducer that
represents it:
</bodyText>
<subsectionHeader confidence="0.385524">
Proposition
</subsectionHeader>
<bodyText confidence="0.9769716">
If VI, ... ,f0 is a sequence of subsequential functions with bounded domains and such
that f,(â¬) = 0, then RmLocExt(R) o â¢ â¢ â¢ o RmLocExt(fâ) is subsequential.
We have proven in this section that our techniques apply to the class of transformation-
based systems. We now turn our attention to the implementation of finite-state trans-
ducers.
</bodyText>
<subsectionHeader confidence="0.500054">
11. Implementation of Finite-State Transducers
</subsectionHeader>
<bodyText confidence="0.999989769230769">
Once the final finite-state transducer is computed, applying it to an input is straight-
forward: it consists of following the unique sequence of transitions whose left labels
correspond to the input. However, in order to have a complexity fully independent of
the size of the grammar and in particular independent of the number of transitions
at each state, one should carefully choose an appropriate representation for the trans-
ducer. In our implementation, transitions can be accessed randomly. The transducer
is first represented by a two-dimensional table whose rows are indexed by states and
whose columns are indexed by the alphabet of all possible input letters. The content
of the table at line q and at column a is the word w such that the transition from q
with the input label a outputs w. Since only a few transitions are allowed from many
states, this table is very sparse and can be compressed. This compression is achieved
while maintaining random access using a procedure for sparse data tables following
the method given by Tarjan and Yao (1979).
</bodyText>
<sectionHeader confidence="0.91391" genericHeader="conclusions">
12. Conclusion
</sectionHeader>
<bodyText confidence="0.9999842">
The techniques described in this paper are more general than the problem of part-of-
speech tagging and are applicable to the class of problems dealing with local transfor-
mation rules.
We showed that any transformation-based program can be transformed into a
deterministic finite-state transducer. This yields to optimal time implementations of
transformation based programs.
As a case study, we applied these techniques to the problem of part-of-speech
tagging and presented a finite-state tagger that requires n steps to tag a sentence of
length n, independently of the number of rules and the length of the context they
require. We achieved this result by representing the rules acquired for Brill&apos;s tagger
as nondeterministic finite-state transducers. We composed each of these nondetermin-
istic transducers and turned the resulting transducer into a deterministic transducer.
The resulting deterministic transducer yields a part-of-speech tagger that operates in
optimal time in the sense that the time to assign tags to a sentence corresponds to
the time required to follow a single path in this deterministic finite-state machine. The
</bodyText>
<page confidence="0.977334">
251
</page>
<note confidence="0.495183">
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.999401571428572">
tagger outperforms in speed both Brill&apos;s tagger and stochastic taggers. Moreover, the
finite-state tagger inherits from the rule-based system its compactness compared with
stochastic taggers. We also proved the correctness and the generality of the methods.
We believe that this finite-state tagger will also be found useful when combined
with other language components, since it can be naturally extended by composing
it with finite-state transducers that could encode other aspects of natural language
syntax.
</bodyText>
<sectionHeader confidence="0.990299" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999271444444445">
We thank Eric Brill for providing us with
the code of his tagger and for many useful
discussions. We also thank Aravind K.
Joshi, Mark Liberman, and Mehryar Mohri
for valuable discussions. We thank the
anonymous reviewers for many helpful
comments that led to improvements in both
the content and the presentation of this
paper.
</bodyText>
<sectionHeader confidence="0.99669" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871674157304">
Brill, Eric (1992). &amp;quot;A simple rule-based part
of speech tagger.&amp;quot; In Proceedings, Third
Conference on Applied Natural Language
Processing. Trento, Italy, 152-155.
Brill, Eric (1994). &amp;quot;A report of recent
progress in transformation error-driven
learning.&amp;quot; In Proceedings, Tenth National
Conference on Artificial Intelligence
(AAAI-94). Seattle, Washington, 722-727.
Choffrut, Christian (1977). &amp;quot;Line
caracterisation des fonctions sequentielles
et des fonctions sous-sequentielles en tant
que relations rationnelles.&amp;quot; Theoretical
Computer Science, 5, 325-338.
Choffrut, Christian (1978). Contribution a
l&apos;etude de quelques families remarquables de
fonctions rationnelles. Doctoral dissertation,
Universite Paris VII (These d&apos;Etat).
Chomsky, Noam (1964). Syntactic Structures.
Mouton.
Church, Kenneth Ward (1988). &amp;quot;A stochastic
parts program and noun phrase parser for
unrestricted text.&amp;quot; In Proceedings, Second
Conference on Applied Natural Language
Processing. Austin, Texas, 136-143.
Clemenceau, David (1993). Structuration du
lexique et reconnaissance de mots derives.
Doctoral dissertation, Universite Paris 7.
Cutting, Doug; Kupiec, Julian; Pederson,
Jan; and Sibun, Penelope (1992). &amp;quot;A
practical part-of-speech tagger.&amp;quot; In
Proceedings, Third Conference on Applied
Natural Language Processing. Trento, Italy,
133-140.
DeRose, S. J. (1988). &amp;quot;Grammatical category
disambiguation by statistical
optimization.&amp;quot; Computational Linguistics,
14, 31-39.
Eilenberg, Samuel (1974). Automata,
Languages, and Machines. Academic Press.
Elgot, C. C., and Mezei, J. E. (1965). &amp;quot;On
relations defined by generalized finite
automata.&amp;quot; IBM Journal of Research and
Development, 9, 47-65.
Francis, W. Nelson, and KiiCera, Henry
(1982). Frequency Analysis of English Usage.
Houghton Mifflin.
Kaplan, Ronald M., and Kay, Martin (1994).
&amp;quot;Regular models of phonological rule
systems.&amp;quot; Computational Linguistics, 20(3),
331-378.
Karttunen, Lauri; Kaplan, Ronald M.; and
Zaenen, Annie (1992). &amp;quot;Two-level
morphology with composition.&amp;quot; In
Proceedings, 14th International Conference on
Computational Linguistics (COLING-92).
Nantes, France, 141-148.
Kupiec, J. M. (1992). &amp;quot;Robust part-of-speech
tagging using a hidden Markov model.&amp;quot;
Computer Speech and Language, 6, 225-242.
Laporte, Eric (1993). &amp;quot;Phonetique et
transducteurs.&amp;quot; Technical report,
Universite Paris 7, June.
Merialdo, Bernard (1990). &amp;quot;Tagging text
with a probabilistic model.&amp;quot; Technical
Report RC 15972, IBM Research Division.
Mohri, Mehryar (1994a). &amp;quot;Minimisation of
sequential transducers.&amp;quot; In Proceedings,
Fifth Annual Symposium on Combinatorial
Pattern Matching. Lecture Notes in
Computer Science, Springer-Verlag.
Mohri, Mehryar (1994b). &amp;quot;On some
applications of finite-state automata
theory to natural language processing.&amp;quot;
Technical report, Institut Gaspard Monge.
Pereira, Fernando C. N.; Riley, Michael; and
Sproat, Richard W. (1994). &amp;quot;Weighted
rational transductions and their
application to human language
processing.&amp;quot; In Human Language Technology
Workshop. 262-267. Morgan Kaufmann.
Revuz, Dominique (1991). Dictionnaires et
lexiques, methodes et algorithmes. Doctoral
dissertation, Universite Paris 7.
Roche, Emmanuel (1993). Analyse syntaxique
transformationelle du francais par
transducteurs et lexique-grammaire. Doctoral
dissertation, Universite Paris 7.
Schfitzenberger, Marcel Paul (1977). &amp;quot;Sur
</reference>
<page confidence="0.968438">
252
</page>
<note confidence="0.754871">
Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging
</note>
<reference confidence="0.9913091">
une variante des fonctions sequentielles.&amp;quot;
Theoretical Computer Science, 4,47-57.
Silberztein, Max (1993). Dictionnaires
Elect roniques et Analyse Lexicale du
FrancaisâLe Systeme INTEX. Masson.
Tapanainen, Pasi, and Voutilainen, Atro
(1993). &amp;quot;Ambiguity resolution in a
reductionistic parser.&amp;quot; In Proceedings, Sixth
Conference of the European Chapter of the
ACL. Utrecht, Netherlands, 394-403.
Tarjan, Robert Endre, and Chi-Chih Yao,
Andrew (1979). &amp;quot;Storing a sparse table.&amp;quot;
Communications of the ACM, 22(11),
606-611.
Weischedel, Ralph; Meteer, Marie; Schwartz,
Richard; Ramshaw, Lance; and Palmucci,
Jeff (1993). &amp;quot;Coping with ambiguity and
unknown words through probabilistic
models.&amp;quot; Computational Linguistics, 19(2),
359-382.
</reference>
<page confidence="0.998942">
253
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.539263">
<title confidence="0.9995105">Deterministic Part-of-Speech Tagging with Finite-State Transducers</title>
<author confidence="0.998917">Emmanuel Roche Yves Schabes</author>
<affiliation confidence="0.550681">MERL MERL</affiliation>
<abstract confidence="0.997530333333333">Stochastic approaches to natural language processing have often been preferred to rule-based approaches because of their robustness and their automatic training capabilities. This was the case for part-of-speech tagging until Brill showed how state-of-the-art part-of-speech tagging can be achieved with a rule-based tagger by inferring rules from a training corpus. However, current implementations of the rule-based tagger run more slowly than previous approaches. In this paper, we present a finite-state tagger, inspired by the rule-based tagger, that operates in optimal time in the sense that the time to assign tags to a sentence corresponds to the time required to follow a single path in a deterministic finite-state machine. This result is achieved by encoding the application of the rules found in the tagger as a nondeterministic finite-state transducer and then turning it into a deterministic transducer. The resulting deterministic transducer yields a part-of-speech tagger whose speed is dominated by the access time of mass storage devices. We then generalize the techniques to the class of transformation-based systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, Third Conference on Applied Natural Language Processing.</booktitle>
<pages>152--155</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="2762" citStr="Brill (1992)" startWordPosition="392" endWordPosition="393">utilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139. E-mail: roche/schabes@merl.com. Â© 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 quired. In addition, the tagger requires drastically less space than stochastic taggers. However, current implementations of Brill&apos;s tagger are considerably slower </context>
<context position="22422" citStr="Brill (1992)" startWordPosition="3643" endWordPosition="3644">depends only on the length of the word; in particular, it is independent of the size of the dictionary. The lexicon used in our system encodes 54,000 words. The corresponding DAG takes 360Kb of space and provides an access time of 12,000 words per second.&apos; 4 The size of the dictionary in plain text (ASCII form) is 742KB. 234 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging ads nns bag nn vb bagged vbn vbd bayed vbn vbd bids nns Figure 9 Sample dictionary. Figure 10 DAG representation of the dictionary of Figure 9. 6. Tagging Unknown Words The rule-based system described by Brill (1992) contains a module that operates after all known wordsâthat is, words listed in the dictionaryâhave been tagged with their most frequent tag, and before contextual rules are applied. This module guesses a tag for a word according to its suffix (e.g. a word with an &amp;quot;ing&amp;quot; suffix is likely to be a verb), its prefix (e.g. a word starting with an uppercase character is likely to be a proper noun), and other relevant properties. This module basically follows the same techniques as the ones used to implement the lexicon. Because of the similarity of the methods used, we do not provide further details</context>
<context position="24489" citStr="Brill (1992)" startWordPosition="3981" endWordPosition="3982">ee programs on large files and piped their output into a file. In the times reported, we included the time spent reading the input and writing the output. Figure 11 summarizes the results. All taggers were trained on a portion of the Brown corpus. The experiments were run on an HP720 with 32MB of memory. In order to conduct a fair comparison, the dictionary lookup part of the stochastic tagger has also been implemented using the techniques described in Section 5. All three taggers have approximately the same 5 Our current implementation is functionally equivalent to the tagger as described by Brill (1992). However, the tagger could be extended to include recent improvements described in more recent papers (Brill 1994). â¢ 0 â¢ C) (nns) nn,vb) e d (vbd,vbn) 235 Computational Linguistics Volume 21, Number 2 Stochastic Tagger Rule-Based Tagger Finite-State Tagger Speed 1,200 w/s 500 w/s 10,800 w/s Space 2,158KB 379KB 815KB Figure 11 Overall performance comparison. dictionary lookup unknown words contextual Speed 12,800 w/s 16,600 w/s 125,100 w/s Percent of the time 85% 6.5% 8.5% Figure 12 Speeds of the different parts of the program. precision (95% of the tags are correct).6 By design, the finite-s</context>
<context position="27103" citStr="Brill (1992)" startWordPosition="4406" endWordPosition="4407">elated to the determinization of finite-state transducers are discussed in the section following this one. 8.1 Definition of Finite-State Transducers A finite-state transducer T is a five-tuple (E, Q, i,F,E) where: E is a finite alphabet; Q is a finite set of states or vertices; i E Q is the initial state; F C Q is the set of final states; ECQx (E U {â¬}) x E* x Q is the set of edges or transitions. 6 For evaluation purposes, we randomly selected 90% of the Brown corpus for training purposes and 10% for testing. 7 An extended discussion of the precision of the rule-based tagger can be found in Brill (1992). 8 In Figure 12, the dictionary lookup includes reading the file, splitting it into sentences, looking up each word in the dictionary, and writing the final result to a file. The dictionary lookup and the tagging of unknown words take roughly the same amount of time, but since the second procedure only applies on unknown words (around 10% in our experiments), the percentage of time it takes is much smaller. 236 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging Figure 13 T4: Example of a finite-state transducer. For instance, Figure 13 is the graphical representation of the </context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Brill, Eric (1992). &amp;quot;A simple rule-based part of speech tagger.&amp;quot; In Proceedings, Third Conference on Applied Natural Language Processing. Trento, Italy, 152-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A report of recent progress in transformation error-driven learning.&amp;quot;</title>
<date>1994</date>
<booktitle>In Proceedings, Tenth National Conference on Artificial Intelligence (AAAI-94).</booktitle>
<pages>722--727</pages>
<location>Seattle, Washington,</location>
<contexts>
<context position="24604" citStr="Brill 1994" startWordPosition="3998" endWordPosition="3999">ding the input and writing the output. Figure 11 summarizes the results. All taggers were trained on a portion of the Brown corpus. The experiments were run on an HP720 with 32MB of memory. In order to conduct a fair comparison, the dictionary lookup part of the stochastic tagger has also been implemented using the techniques described in Section 5. All three taggers have approximately the same 5 Our current implementation is functionally equivalent to the tagger as described by Brill (1992). However, the tagger could be extended to include recent improvements described in more recent papers (Brill 1994). â¢ 0 â¢ C) (nns) nn,vb) e d (vbd,vbn) 235 Computational Linguistics Volume 21, Number 2 Stochastic Tagger Rule-Based Tagger Finite-State Tagger Speed 1,200 w/s 500 w/s 10,800 w/s Space 2,158KB 379KB 815KB Figure 11 Overall performance comparison. dictionary lookup unknown words contextual Speed 12,800 w/s 16,600 w/s 125,100 w/s Percent of the time 85% 6.5% 8.5% Figure 12 Speeds of the different parts of the program. precision (95% of the tags are correct).6 By design, the finite-state tagger produces the same output as the rule-based tagger. The rule-based taggerâand the finite-state taggerâdo</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>Brill, Eric (1994). &amp;quot;A report of recent progress in transformation error-driven learning.&amp;quot; In Proceedings, Tenth National Conference on Artificial Intelligence (AAAI-94). Seattle, Washington, 722-727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Choffrut</author>
</authors>
<title>Line caracterisation des fonctions sequentielles et des fonctions sous-sequentielles en tant que relations rationnelles.&amp;quot;</title>
<date>1977</date>
<journal>Theoretical Computer Science,</journal>
<volume>5</volume>
<pages>325--338</pages>
<contexts>
<context position="43857" citStr="Choffrut 1977" startWordPosition="7670" endWordPosition="7671">uch that w&apos; is W&apos; = A b â¢ 6(1, h,71-&apos;)) = bh. 7&apos;Ed(1,h)={2} Henceforth, the computation of line 8 leads to s&apos;= U U {(4&apos; , (bh)-1 â¢ b â¢ h)} = {(2, Of. State 2 labeled {(2, e)} is thus added, and a transition labeled hlbh that points to state 2 is also added. The transition for the input symbol e is computed the same way. The subsequential transducer generated by this algorithm could in turn be minimized by an-algorithm described in Mohri (1994a). However, in our case, the transducer is nearly minimal. 9.2 Proof of Correctness Although it is decidable whether a function is subsequential or not (Choffrut 1977), the determinization algorithm described in the previous section does not terminate when run on a nonsubsequential function. Two issues are addressed in this section. First, the proof of soundness: the fact that if the algorithm terminates, then the output transducer is deterministic and represents the same function. Second, the proof of completeness: the algorithm terminates in the case of subsequential functions. Soundness and completeness are a consequence of the main proposition, which states that if a transducer T represents a subsequential function f, then the algorithm DeterminizeTrans</context>
</contexts>
<marker>Choffrut, 1977</marker>
<rawString>Choffrut, Christian (1977). &amp;quot;Line caracterisation des fonctions sequentielles et des fonctions sous-sequentielles en tant que relations rationnelles.&amp;quot; Theoretical Computer Science, 5, 325-338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Choffrut</author>
</authors>
<title>Contribution a l&apos;etude de quelques families remarquables de fonctions rationnelles. Doctoral dissertation, Universite Paris VII (These d&apos;Etat).</title>
<date>1978</date>
<contexts>
<context position="49176" citStr="Choffrut (1978)" startWordPosition="8818" endWordPosition="8819">algorithm, if (q,u),(q&apos;,u&apos;) E 5, then q, q&apos; E F u = u&apos; Proof Let S be one state set built in line 8 of the algorithm. Suppose (q,u),(q&apos;,u&apos;) E S and q, q&apos; E F. According to (ii) of lemma 1, u = (I*w)-1 â¢ 6(i, w, q) and u&apos; = (I *w)-1 6(i, w, q&apos;). Since I TI is a function and (5(i,w,q),(5(i,w,q/)} E I TI (w) then S(i,w,q) = 6(i,w,q/), therefore u = u&apos;. The following lemma will be used for completeness. Lemma 4 Given a transducer T representing a subsequential function, there exists a bound M s.t. for each S built at line 8, for each (q, u) E S, lul &lt; M. We rely on the following theorem proven by Choffrut (1978): Theorem 1 A function f on E* is subsequential iff it has bounded variations and for any rational language L C E*, f&apos; (L)is also rational. with the following two definitions: Definition The left distance between two strings u and v is II u, vii = u I + Iv&apos; - 2Iu A vi. Definition A function f on E* has bounded variations iff for all k &gt; 0, there exists K &gt; 0 s.t. u,v E dom(f), k = Ilf(u),f(v)ii &lt;K. Proof of Lemma 4 II Let f = I Ti. For each q E Q, let c(q) be a string w s.t. d(q,w) n F 0 and s.t. lw I is minimal among such strings. Note that Ic(q)i &lt; ii Ti where TII is the number of states 246</context>
</contexts>
<marker>Choffrut, 1978</marker>
<rawString>Choffrut, Christian (1978). Contribution a l&apos;etude de quelques families remarquables de fonctions rationnelles. Doctoral dissertation, Universite Paris VII (These d&apos;Etat).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Syntactic Structures.</title>
<date>1964</date>
<publisher>Mouton.</publisher>
<contexts>
<context position="1522" citStr="Chomsky, 1964" startWordPosition="215" endWordPosition="216">ound in the tagger as a nondeterministic finite-state transducer and then turning it into a deterministic transducer. The resulting deterministic transducer yields a part-of-speech tagger whose speed is dominated by the access time of mass storage devices. We then generalize the techniques to the class of transformation-based systems. 1. Introduction Finite-state devices have important applications to many areas of computer science, including pattern matching, databases, and compiler technology. Although their linguistic adequacy to natural language processing has been questioned in the past (Chomsky, 1964), there has recently been a dramatic renewal of interest in the application of finitestate devices to several aspects of natural language processing. This renewal of interest is due to the speed and compactness of finite-state representations. This efficiency is explained by two properties: finite-state devices can be made deterministic, and they can be turned into a minimal form. Such representations have been successfully applied to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (R</context>
</contexts>
<marker>Chomsky, 1964</marker>
<rawString>Chomsky, Noam (1964). Syntactic Structures. Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, Second Conference on Applied Natural Language Processing.</booktitle>
<pages>136--143</pages>
<location>Austin, Texas,</location>
<contexts>
<context position="2540" citStr="Church 1988" startWordPosition="358" endWordPosition="359">ve been successfully applied to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139. E-mail: roche/schabes@merl.com. Â© 1995 Association for Computational </context>
<context position="23810" citStr="Church (1988)" startWordPosition="3868" endWordPosition="3869">s at a much higher speed. The tagger runs nearly ten times faster than the fastest of the other systems. Moreover, the finitestate tagger inherits from the rule-based system its compactness compared with a stochastic tagger. In fact, whereas stochastic taggers have to store word-tag, bigram, and trigram probabilities, the rule-based tagger and therefore the finite-state one only have to encode a small number of rules (between 200 and 300). We empirically compared our tagger with Eric Brill&apos;s implementation of his tagger, and with our implementation of a trigram tagger adapted from the work of Church (1988) that we previously implemented for another purpose. We ran the three programs on large files and piped their output into a file. In the times reported, we included the time spent reading the input and writing the output. Figure 11 summarizes the results. All taggers were trained on a portion of the Brown corpus. The experiments were run on an HP720 with 32MB of memory. In order to conduct a fair comparison, the dictionary lookup part of the stochastic tagger has also been implemented using the techniques described in Section 5. All three taggers have approximately the same 5 Our current imple</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, Kenneth Ward (1988). &amp;quot;A stochastic parts program and noun phrase parser for unrestricted text.&amp;quot; In Proceedings, Second Conference on Applied Natural Language Processing. Austin, Texas, 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Clemenceau</author>
</authors>
<title>Structuration du lexique et reconnaissance de mots derives. Doctoral dissertation, Universite Paris 7.</title>
<date>1993</date>
<contexts>
<context position="2110" citStr="Clemenceau 1993" startWordPosition="301" endWordPosition="302"> in the past (Chomsky, 1964), there has recently been a dramatic renewal of interest in the application of finitestate devices to several aspects of natural language processing. This renewal of interest is due to the speed and compactness of finite-state representations. This efficiency is explained by two properties: finite-state devices can be made deterministic, and they can be turned into a minimal form. Such representations have been successfully applied to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and wa</context>
</contexts>
<marker>Clemenceau, 1993</marker>
<rawString>Clemenceau, David (1993). Structuration du lexique et reconnaissance de mots derives. Doctoral dissertation, Universite Paris 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Cutting</author>
<author>Julian Kupiec</author>
<author>Jan Pederson</author>
<author>Penelope Sibun</author>
</authors>
<title>A practical part-of-speech tagger.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, Third Conference on Applied Natural Language Processing.</booktitle>
<pages>133--140</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="2574" citStr="Cutting et al. 1992" startWordPosition="362" endWordPosition="365">ed to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139. E-mail: roche/schabes@merl.com. Â© 1995 Association for Computational Linguistics Computational Linguist</context>
<context position="25429" citStr="Cutting et al. (1992)" startWordPosition="4123" endWordPosition="4126"> Figure 11 Overall performance comparison. dictionary lookup unknown words contextual Speed 12,800 w/s 16,600 w/s 125,100 w/s Percent of the time 85% 6.5% 8.5% Figure 12 Speeds of the different parts of the program. precision (95% of the tags are correct).6 By design, the finite-state tagger produces the same output as the rule-based tagger. The rule-based taggerâand the finite-state taggerâdo not always produce the exact same tagging as the stochastic tagger (they do not make the same errors); however, no significant difference in performance between the systems was detected.&apos; Independently, Cutting et al. (1992) quote a performance of 800 words per second for their part-of-speech tagger based on hidden Markov models. The space required by the finite-state tagger (815KB) is distributed as follows: 363KB for the dictionary, 440KB for the subsequential transducer and 12KB for the module for unknown words. The speeds of the different parts of our system are shown in Figure 12.8 Our system reaches a performance level in speed for which other, very low-level factors (such as storage access) may dominate the computation. At such speeds, the time spent reading the input file, breaking the file into sentences</context>
</contexts>
<marker>Cutting, Kupiec, Pederson, Sibun, 1992</marker>
<rawString>Cutting, Doug; Kupiec, Julian; Pederson, Jan; and Sibun, Penelope (1992). &amp;quot;A practical part-of-speech tagger.&amp;quot; In Proceedings, Third Conference on Applied Natural Language Processing. Trento, Italy, 133-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J DeRose</author>
</authors>
<title>Grammatical category disambiguation by statistical optimization.&amp;quot;</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>14</volume>
<pages>31--39</pages>
<contexts>
<context position="2602" citStr="DeRose 1988" startWordPosition="368" endWordPosition="369">anguage processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139. E-mail: roche/schabes@merl.com. Â© 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 quir</context>
</contexts>
<marker>DeRose, 1988</marker>
<rawString>DeRose, S. J. (1988). &amp;quot;Grammatical category disambiguation by statistical optimization.&amp;quot; Computational Linguistics, 14, 31-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Eilenberg</author>
</authors>
<title>Automata, Languages, and Machines.</title>
<date>1974</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="30590" citStr="Eilenberg 1974" startWordPosition="5080" endWordPosition="5081">re defined as above; 0 is the deterministic state transition function that maps Q x E on Q, one writes q Oa = q&apos;; * is the deterministic emission function that maps Q x E on E*, one writes q * a = w&apos;; and the final emission function p maps F on E*, one writes p(q) = w. For instance, T4 is not deterministic because d(0, a) =- {1,2}, but it is equivalent to Ty represented Figure 14 in the sense that they represent the same function, i.e. 9 A sequential transducer is a deterministic transducer for which all states are final. Sequential transducers are also called generalized sequential machines (Eilenberg 1974). 237 Computational Linguistics Volume 21, Number 2 tilbh Figure 14 Subsequential transducer T5. Figure 15 T6: a finite-state transducer to be extended. a a b cab a a b c a b b c b c a a b c a b dca Figure 16 Top: Input. Middle: First factorization. Bottom: Second factorization. 1T41 = IT5 I. T5 is defined by T5 = (fa, b, c, h, el, {0, 1, 2}, 0, {2}, 0, *, p) where 0 0 a = 1, 0 * a = 6, 1 h = 2, 1 * h = bh, 1 e = 2, 1 * e = ce, and p(2) = â¬. 8.2 Local Extension In this section, we will see how a function that needs to be applied at all input positions can be transformed into a global function </context>
<context position="59414" citStr="Eilenberg 1974" startWordPosition="10849" endWordPosition="10850">composition of Au&amp;quot;, then A1[A2A3]A5 is a Y-decomposition of v strictly smaller than A&amp;quot;w&amp;quot; v&amp;quot; , which leads to the same contradiction. Therefore, A&apos; = A&amp;quot; and since 11L/1-01&amp;quot;1 3 x (Ip1+10) = 3 x Ilu,v11 &lt; 3 xk, Ilnidy(u),m4(v)H lwii+ice&amp;quot;1+10+Iv&amp;quot;I 5_ 2 x M + 3 xk&lt; K. This proves that mdy has bounded variations and therefore that it is subsequential. 0 We can now define precisely what is the effect of a function when one applies it from left to right, as was done in the original tagger. 19 This construction is similar to the transduction built within the proof of Eilenberg&apos;s cross section theorem (Eilenberg 1974). 20 (w), refers to the 1th letter in w. 250 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging Definition If f is a rational function with bounded domain, Y = dom(f) C E+ , the right-minimal local extension of f, denoted RmLocExt(f), is the composition of a right-minimal Y-decomposition mdy with IdE.. â¢ ([/e .1. Ve â¢ Id*). RmLocExt being the composition of two subsequential functions, it is itself subsequential; this proves the following final proposition, which states that given a rulebased system similar to Brill&apos;s system, one can build a subsequential transducer that repr</context>
</contexts>
<marker>Eilenberg, 1974</marker>
<rawString>Eilenberg, Samuel (1974). Automata, Languages, and Machines. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C Elgot</author>
<author>J E Mezei</author>
</authors>
<title>On relations defined by generalized finite automata.&amp;quot;</title>
<date>1965</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>9</volume>
<pages>47--65</pages>
<contexts>
<context position="18099" citStr="Elgot and Mezei (1965)" startWordPosition="2917" endWordPosition="2920">us step still need to be applied one after the other. 3 This notion was introduced by Roche (1993). 232 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging vbd/vbn Figure 6 Composition T3 = LocExt(Ti) o LocExt(T2). a:a Figure 7 Example of a transducer not equivalent to any subsequential transducer. The third step combines all transducers into one single transducer. This corresponds to the formal operation of composition defined on transducers. The formalization of this notion and an algorithm for computing the composed transducer are well known and are described originally by Elgot and Mezei (1965). Returning to our running example of Section 2, the transducer obtained by composing the local extension of T2 (right in Figure 5) with the local extension of T1 (right in Figure 4) is shown in Figure 6. The fourth and final step consists of transforming the finite-state transducer obtained in the previous step into an equivalent subsequential (deterministic) transducer. The transducer obtained in the previous step may contain some nondeterminism. The fourth step tries to turn it into a deterministic machine. This determinization is not always possible for any given finite-state transducer. F</context>
</contexts>
<marker>Elgot, Mezei, 1965</marker>
<rawString>Elgot, C. C., and Mezei, J. E. (1965). &amp;quot;On relations defined by generalized finite automata.&amp;quot; IBM Journal of Research and Development, 9, 47-65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Nelson Francis</author>
<author>Henry KiiCera</author>
</authors>
<title>Frequency Analysis of English Usage.</title>
<date>1982</date>
<publisher>Houghton Mifflin.</publisher>
<marker>Francis, KiiCera, 1982</marker>
<rawString>Francis, W. Nelson, and KiiCera, Henry (1982). Frequency Analysis of English Usage. Houghton Mifflin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Martin Kay</author>
</authors>
<title>Regular models of phonological rule systems.&amp;quot;</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<pages>331--378</pages>
<contexts>
<context position="2212" citStr="Kaplan and Kay 1994" startWordPosition="313" endWordPosition="316">ation of finitestate devices to several aspects of natural language processing. This renewal of interest is due to the speed and compactness of finite-state representations. This efficiency is explained by two properties: finite-state devices can be made deterministic, and they can be turned into a minimal form. Such representations have been successfully applied to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as we</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Kaplan, Ronald M., and Kay, Martin (1994). &amp;quot;Regular models of phonological rule systems.&amp;quot; Computational Linguistics, 20(3), 331-378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Ronald M Kaplan</author>
<author>Annie Zaenen</author>
</authors>
<title>Two-level morphology with composition.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, 14th International Conference on Computational Linguistics (COLING-92).</booktitle>
<pages>141--148</pages>
<location>Nantes, France,</location>
<marker>Karttunen, Kaplan, Zaenen, 1992</marker>
<rawString>Karttunen, Lauri; Kaplan, Ronald M.; and Zaenen, Annie (1992). &amp;quot;Two-level morphology with composition.&amp;quot; In Proceedings, 14th International Conference on Computational Linguistics (COLING-92). Nantes, France, 141-148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model.&amp;quot;</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<volume>6</volume>
<pages>225--242</pages>
<contexts>
<context position="2553" citStr="Kupiec 1992" startWordPosition="360" endWordPosition="361">ssfully applied to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139. E-mail: roche/schabes@merl.com. Â© 1995 Association for Computational Linguistics C</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>Kupiec, J. M. (1992). &amp;quot;Robust part-of-speech tagging using a hidden Markov model.&amp;quot; Computer Speech and Language, 6, 225-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Laporte</author>
</authors>
<title>Phonetique et transducteurs.&amp;quot;</title>
<date>1993</date>
<tech>Technical report,</tech>
<institution>Universite Paris</institution>
<contexts>
<context position="2190" citStr="Laporte 1993" startWordPosition="311" endWordPosition="312"> in the application of finitestate devices to several aspects of natural language processing. This renewal of interest is due to the speed and compactness of finite-state representations. This efficiency is explained by two properties: finite-state devices can be made deterministic, and they can be turned into a minimal form. Such representations have been successfully applied to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagg</context>
</contexts>
<marker>Laporte, 1993</marker>
<rawString>Laporte, Eric (1993). &amp;quot;Phonetique et transducteurs.&amp;quot; Technical report, Universite Paris 7, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging text with a probabilistic model.&amp;quot;</title>
<date>1990</date>
<tech>Technical Report RC 15972,</tech>
<institution>IBM Research Division.</institution>
<contexts>
<context position="2589" citStr="Merialdo 1990" startWordPosition="366" endWordPosition="367">ts of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139. E-mail: roche/schabes@merl.com. Â© 1995 Association for Computational Linguistics Computational Linguistics Volume 21, </context>
</contexts>
<marker>Merialdo, 1990</marker>
<rawString>Merialdo, Bernard (1990). &amp;quot;Tagging text with a probabilistic model.&amp;quot; Technical Report RC 15972, IBM Research Division.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Minimisation of sequential transducers.&amp;quot;</title>
<date>1994</date>
<booktitle>In Proceedings, Fifth Annual Symposium on Combinatorial Pattern Matching. Lecture Notes in Computer Science,</booktitle>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="41897" citStr="Mohri (1994" startWordPosition="7313" endWordPosition="7314">s is obtained by concatenating the postponed emissions with the emission at the current state. Since one wants the transition to be deterministic, the actual emission is the longest common prefix of this set. â¢ the state transition function is defined by: S a = U U {(qi, (S * arl â¢ u â¢ S(q,a, q1))} (q,u)ES q&apos;Ed(q,a) Given u,v E E*, u v denotes the concatenation of u and v and u-1 â¢ v = w, if w is such that u w = v, u-1 = 0 if no such w exists. â¢ F&apos; = IS E Q&apos; I 3(q,u) Sand q E F1 â¢ if S E F&apos;, p(S) = u s.t. q c F, (q, u) ES. We will see in the proof of correctness that p is properly defined. 15 Mohri (1994b) also gives a formalization of the algorithm. 16 As opposed to automata, a large class of finite-state transducers do not have any deterministic representation; they cannot be determinized. 242 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging The determinization algorithm of Figure 21 computes the above subsequential transducer. Let us now apply the determinization algorithm of Figure 21 on the finite-state transducer T4 of Figure 13 and show how it builds the subsequential transducer Tio of Figure 22. Line 1 of the algorithm builds the first state and instantiates it wit</context>
<context position="43689" citStr="Mohri (1994" startWordPosition="7643" endWordPosition="7644">n line 15, the program goes to the construction of the transitions of state 1. On line 5, d and e are then two possible symbols. The first symbol, h, at line 6, is such that w&apos; is W&apos; = A b â¢ 6(1, h,71-&apos;)) = bh. 7&apos;Ed(1,h)={2} Henceforth, the computation of line 8 leads to s&apos;= U U {(4&apos; , (bh)-1 â¢ b â¢ h)} = {(2, Of. State 2 labeled {(2, e)} is thus added, and a transition labeled hlbh that points to state 2 is also added. The transition for the input symbol e is computed the same way. The subsequential transducer generated by this algorithm could in turn be minimized by an-algorithm described in Mohri (1994a). However, in our case, the transducer is nearly minimal. 9.2 Proof of Correctness Although it is decidable whether a function is subsequential or not (Choffrut 1977), the determinization algorithm described in the previous section does not terminate when run on a nonsubsequential function. Two issues are addressed in this section. First, the proof of soundness: the fact that if the algorithm terminates, then the output transducer is deterministic and represents the same function. Second, the proof of completeness: the algorithm terminates in the case of subsequential functions. Soundness an</context>
</contexts>
<marker>Mohri, 1994</marker>
<rawString>Mohri, Mehryar (1994a). &amp;quot;Minimisation of sequential transducers.&amp;quot; In Proceedings, Fifth Annual Symposium on Combinatorial Pattern Matching. Lecture Notes in Computer Science, Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>On some applications of finite-state automata theory to natural language processing.&amp;quot;</title>
<date>1994</date>
<tech>Technical report,</tech>
<institution>Institut Gaspard Monge.</institution>
<contexts>
<context position="41897" citStr="Mohri (1994" startWordPosition="7313" endWordPosition="7314">s is obtained by concatenating the postponed emissions with the emission at the current state. Since one wants the transition to be deterministic, the actual emission is the longest common prefix of this set. â¢ the state transition function is defined by: S a = U U {(qi, (S * arl â¢ u â¢ S(q,a, q1))} (q,u)ES q&apos;Ed(q,a) Given u,v E E*, u v denotes the concatenation of u and v and u-1 â¢ v = w, if w is such that u w = v, u-1 = 0 if no such w exists. â¢ F&apos; = IS E Q&apos; I 3(q,u) Sand q E F1 â¢ if S E F&apos;, p(S) = u s.t. q c F, (q, u) ES. We will see in the proof of correctness that p is properly defined. 15 Mohri (1994b) also gives a formalization of the algorithm. 16 As opposed to automata, a large class of finite-state transducers do not have any deterministic representation; they cannot be determinized. 242 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging The determinization algorithm of Figure 21 computes the above subsequential transducer. Let us now apply the determinization algorithm of Figure 21 on the finite-state transducer T4 of Figure 13 and show how it builds the subsequential transducer Tio of Figure 22. Line 1 of the algorithm builds the first state and instantiates it wit</context>
<context position="43689" citStr="Mohri (1994" startWordPosition="7643" endWordPosition="7644">n line 15, the program goes to the construction of the transitions of state 1. On line 5, d and e are then two possible symbols. The first symbol, h, at line 6, is such that w&apos; is W&apos; = A b â¢ 6(1, h,71-&apos;)) = bh. 7&apos;Ed(1,h)={2} Henceforth, the computation of line 8 leads to s&apos;= U U {(4&apos; , (bh)-1 â¢ b â¢ h)} = {(2, Of. State 2 labeled {(2, e)} is thus added, and a transition labeled hlbh that points to state 2 is also added. The transition for the input symbol e is computed the same way. The subsequential transducer generated by this algorithm could in turn be minimized by an-algorithm described in Mohri (1994a). However, in our case, the transducer is nearly minimal. 9.2 Proof of Correctness Although it is decidable whether a function is subsequential or not (Choffrut 1977), the determinization algorithm described in the previous section does not terminate when run on a nonsubsequential function. Two issues are addressed in this section. First, the proof of soundness: the fact that if the algorithm terminates, then the output transducer is deterministic and represents the same function. Second, the proof of completeness: the algorithm terminates in the case of subsequential functions. Soundness an</context>
</contexts>
<marker>Mohri, 1994</marker>
<rawString>Mohri, Mehryar (1994b). &amp;quot;On some applications of finite-state automata theory to natural language processing.&amp;quot; Technical report, Institut Gaspard Monge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
<author>Richard W Sproat</author>
</authors>
<title>Weighted rational transductions and their application to human language processing.&amp;quot;</title>
<date>1994</date>
<booktitle>In Human Language Technology Workshop.</booktitle>
<pages>262--267</pages>
<publisher>Morgan Kaufmann.</publisher>
<marker>Pereira, Riley, Sproat, 1994</marker>
<rawString>Pereira, Fernando C. N.; Riley, Michael; and Sproat, Richard W. (1994). &amp;quot;Weighted rational transductions and their application to human language processing.&amp;quot; In Human Language Technology Workshop. 262-267. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominique Revuz</author>
</authors>
<title>Dictionnaires et lexiques, methodes et algorithmes. Doctoral dissertation, Universite Paris 7.</title>
<date>1991</date>
<contexts>
<context position="21355" citStr="Revuz (1991)" startWordPosition="3453" endWordPosition="3454">p of the tagging process consists of looking up each word in a dictionary. Since the dictionary is the largest part of the tagger in terms of space, a compact representation is crucial. Moreover, the lookup process has to be very fast too otherwise the improvement in speed of the contextual manipulations would be of little practical interest. To achieve high speed for this procedure, the dictionary is represented by a deterministic finite-state automaton with both fast access and small storage space. Suppose one wants to encode the sample dictionary of Figure 9. The algorithm, as described by Revuz (1991), consists of first building a tree whose branches are labeled by letters and whose leaves are labeled by a list of tags (such as nn vb) , and then minimizing it into a directed acyclic graph (DAG). The result of applying this procedure to the sample dictionary of Figure 9 is the DAG of Figure 10. When a dictionary is represented as a DAG, looking up a word in it consists simply of following one path in the DAG. The complexity of the lookup procedure depends only on the length of the word; in particular, it is independent of the size of the dictionary. The lexicon used in our system encodes 54</context>
</contexts>
<marker>Revuz, 1991</marker>
<rawString>Revuz, Dominique (1991). Dictionnaires et lexiques, methodes et algorithmes. Doctoral dissertation, Universite Paris 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Roche</author>
</authors>
<title>Analyse syntaxique transformationelle du francais par transducteurs et lexique-grammaire. Doctoral dissertation, Universite Paris 7.</title>
<date>1993</date>
<contexts>
<context position="2131" citStr="Roche 1993" startWordPosition="304" endWordPosition="305">), there has recently been a dramatic renewal of interest in the application of finitestate devices to several aspects of natural language processing. This renewal of interest is due to the speed and compactness of finite-state representations. This efficiency is explained by two properties: finite-state devices can be made deterministic, and they can be turned into a minimal form. Such representations have been successfully applied to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically a</context>
<context position="17575" citStr="Roche (1993)" startWordPosition="2840" endWordPosition="2841">ch occurrence of a has been replaced by b. We say that 12 is the local extension&apos; of Ii, and we write f2 LocExt(fi). Section 8.2 formally defines this notion and gives an algorithm for computing the local extension. Referring to the example of Section 2, the local extension of the transducer for the rule vbn vbd PREVTAG np is shown to the right of Figure 4. Similarly, the transducer for the contextual rule vbd vbn NEXTTAG by and its local extension are shown in Figure 5. The transducers obtained in the previous step still need to be applied one after the other. 3 This notion was introduced by Roche (1993). 232 Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging vbd/vbn Figure 6 Composition T3 = LocExt(Ti) o LocExt(T2). a:a Figure 7 Example of a transducer not equivalent to any subsequential transducer. The third step combines all transducers into one single transducer. This corresponds to the formal operation of composition defined on transducers. The formalization of this notion and an algorithm for computing the composed transducer are well known and are described originally by Elgot and Mezei (1965). Returning to our running example of Section 2, the transducer obtained by </context>
</contexts>
<marker>Roche, 1993</marker>
<rawString>Roche, Emmanuel (1993). Analyse syntaxique transformationelle du francais par transducteurs et lexique-grammaire. Doctoral dissertation, Universite Paris 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcel Paul Schfitzenberger</author>
</authors>
<title>Sur une variante des fonctions sequentielles.&amp;quot;</title>
<date>1977</date>
<journal>Theoretical Computer Science,</journal>
<pages>4--47</pages>
<contexts>
<context position="29654" citStr="Schfitzenberger 1977" startWordPosition="4895" endWordPosition="4896">pect, T4 would be deterministic; however, since transducers are generally used to compute a function, a more relevant definition of determinism consists of saying that both the transition function d and the emission function (5 lead to sets containing at most one element, that is, Id(q, a)1 &lt; 1 and 18 (q , a, (3&apos;)I &lt; 1 (and that these sets are empty for a = e). With this notion, if a finite-state transducer is deterministic, one can apply the function to a given word by deterministically following a single path in the transducer. Deterministic transducers are called subsequential transducers (Schfitzenberger 1977).9 Given a deterministic transducer, we can define the partial functions q 0a = q&apos; iff d(q, a) -= {q&apos;} and q * a = w&apos; iff E Q such that q 0 a = q&apos; and 6 (q , a, q&apos;) = {w&apos;} . This leads to the definition of subsequential transducers: a subsequential transducer T&apos; is a seven-tuple (E, Q, i, F, 0, *, p) where: E, Q, i, F are defined as above; 0 is the deterministic state transition function that maps Q x E on Q, one writes q Oa = q&apos;; * is the deterministic emission function that maps Q x E on E*, one writes q * a = w&apos;; and the final emission function p maps F on E*, one writes p(q) = w. For insta</context>
</contexts>
<marker>Schfitzenberger, 1977</marker>
<rawString>Schfitzenberger, Marcel Paul (1977). &amp;quot;Sur une variante des fonctions sequentielles.&amp;quot; Theoretical Computer Science, 4,47-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Silberztein</author>
</authors>
<title>Dictionnaires Elect roniques et Analyse Lexicale du FrancaisâLe Systeme INTEX.</title>
<date>1993</date>
<publisher>Masson.</publisher>
<contexts>
<context position="2395" citStr="Silberztein 1993" startWordPosition="338" endWordPosition="339">ncy is explained by two properties: finite-state devices can be made deterministic, and they can be turned into a minimal form. Such representations have been successfully applied to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac* Mi</context>
</contexts>
<marker>Silberztein, 1993</marker>
<rawString>Silberztein, Max (1993). Dictionnaires Elect roniques et Analyse Lexicale du FrancaisâLe Systeme INTEX. Masson.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Atro Voutilainen</author>
</authors>
<title>Ambiguity resolution in a reductionistic parser.&amp;quot;</title>
<date>1993</date>
<booktitle>In Proceedings, Sixth Conference of the European Chapter of the ACL.</booktitle>
<pages>394--403</pages>
<location>Utrecht, Netherlands,</location>
<contexts>
<context position="2165" citStr="Tapanainen and Voutilainen 1993" startWordPosition="306" endWordPosition="309"> recently been a dramatic renewal of interest in the application of finitestate devices to several aspects of natural language processing. This renewal of interest is due to the speed and compactness of finite-state representations. This efficiency is explained by two properties: finite-state devices can be made deterministic, and they can be turned into a minimal form. Such representations have been successfully applied to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) de</context>
</contexts>
<marker>Tapanainen, Voutilainen, 1993</marker>
<rawString>Tapanainen, Pasi, and Voutilainen, Atro (1993). &amp;quot;Ambiguity resolution in a reductionistic parser.&amp;quot; In Proceedings, Sixth Conference of the European Chapter of the ACL. Utrecht, Netherlands, 394-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Endre Tarjan</author>
<author>Chi-Chih Yao</author>
</authors>
<title>Storing a sparse table.&amp;quot;</title>
<date>1979</date>
<journal>Communications of the ACM,</journal>
<volume>22</volume>
<issue>11</issue>
<pages>606--611</pages>
<location>Andrew</location>
<contexts>
<context position="61484" citStr="Tarjan and Yao (1979)" startWordPosition="11186" endWordPosition="11189">r implementation, transitions can be accessed randomly. The transducer is first represented by a two-dimensional table whose rows are indexed by states and whose columns are indexed by the alphabet of all possible input letters. The content of the table at line q and at column a is the word w such that the transition from q with the input label a outputs w. Since only a few transitions are allowed from many states, this table is very sparse and can be compressed. This compression is achieved while maintaining random access using a procedure for sparse data tables following the method given by Tarjan and Yao (1979). 12. Conclusion The techniques described in this paper are more general than the problem of part-ofspeech tagging and are applicable to the class of problems dealing with local transformation rules. We showed that any transformation-based program can be transformed into a deterministic finite-state transducer. This yields to optimal time implementations of transformation based programs. As a case study, we applied these techniques to the problem of part-of-speech tagging and presented a finite-state tagger that requires n steps to tag a sentence of length n, independently of the number of rul</context>
</contexts>
<marker>Tarjan, Yao, 1979</marker>
<rawString>Tarjan, Robert Endre, and Chi-Chih Yao, Andrew (1979). &amp;quot;Storing a sparse table.&amp;quot; Communications of the ACM, 22(11), 606-611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Marie Meteer</author>
<author>Richard Schwartz</author>
<author>Lance Ramshaw</author>
<author>Jeff Palmucci</author>
</authors>
<title>Coping with ambiguity and unknown words through probabilistic models.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>359--382</pages>
<contexts>
<context position="2627" citStr="Weischedel et al. 1993" startWordPosition="370" endWordPosition="373">ssing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139. E-mail: roche/schabes@merl.com. Â© 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 quired. In addition, the tagg</context>
</contexts>
<marker>Weischedel, Meteer, Schwartz, Ramshaw, Palmucci, 1993</marker>
<rawString>Weischedel, Ralph; Meteer, Marie; Schwartz, Richard; Ramshaw, Lance; and Palmucci, Jeff (1993). &amp;quot;Coping with ambiguity and unknown words through probabilistic models.&amp;quot; Computational Linguistics, 19(2), 359-382.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>