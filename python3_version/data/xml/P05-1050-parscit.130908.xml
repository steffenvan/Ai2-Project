<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001550">
<note confidence="0.714491">
Domain Kernels for Word Sense Disambiguation
Alfio Gliozzo and Claudio Giuliano and Carlo Strapparava
ITC-irst, Istituto per la Ricerca Scientifica e Tecnologica
I-38050, Trento, ITALY
</note>
<email confidence="0.993308">
{gliozzo,giuliano,strappa}@itc.it
</email>
<sectionHeader confidence="0.996727" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995040368421053">
In this paper we present a supervised
Word Sense Disambiguation methodol-
ogy, that exploits kernel methods to model
sense distinctions. In particular a combi-
nation of kernel functions is adopted to
estimate independently both syntagmatic
and domain similarity. We defined a ker-
nel function, namely the Domain Kernel,
that allowed us to plug “external knowl-
edge” into the supervised learning pro-
cess. External knowledge is acquired from
unlabeled data in a totally unsupervised
way, and it is represented by means of Do-
main Models. We evaluated our method-
ology on several lexical sample tasks in
different languages, outperforming sig-
nificantly the state-of-the-art for each of
them, while reducing the amount of la-
beled training data required for learning.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935866666667">
The main limitation of many supervised approaches
for Natural Language Processing (NLP) is the lack
of available annotated training data. This problem is
known as the Knowledge Acquisition Bottleneck.
To reach high accuracy, state-of-the-art systems
for Word Sense Disambiguation (WSD) are de-
signed according to a supervised learning frame-
work, in which the disambiguation of each word
in the lexicon is performed by constructing a dif-
ferent classifier. A large set of sense tagged exam-
ples is then required to train each classifier. This
methodology is called word expert approach (Small,
1980; Yarowsky and Florian, 2002). However this
is clearly unfeasible for all-words WSD tasks, in
which all the words of an open text should be dis-
ambiguated.
On the other hand, the word expert approach
works very well for lexical sample WSD tasks (i.e.
tasks in which it is required to disambiguate only
those words for which enough training data is pro-
vided). As the original rationale of the lexical sam-
ple tasks was to define a clear experimental settings
to enhance the comprehension of WSD, they should
be considered as preceding exercises to all-words
tasks. However this is not the actual case. Algo-
rithms designed for lexical sample WSD are often
based on pure supervision and hence “data hungry”.
We think that lexical sample WSD should regain
its original explorative role and possibly use a min-
imal amount of training data, exploiting instead ex-
ternal knowledge acquired in an unsupervised way
to reach the actual state-of-the-art performance.
By the way, minimal supervision is the basis
of state-of-the-art systems for all-words tasks (e.g.
(Mihalcea and Faruque, 2004; Decadt et al., 2004)),
that are trained on small sense tagged corpora (e.g.
SemCor), in which few examples for a subset of the
ambiguous words in the lexicon can be found. Thus
improving the performance of WSD systems with
few learning examples is a fundamental step towards
the direction of designing a WSD system that works
well on real texts.
In addition, it is a common opinion that the per-
formance of state-of-the-art WSD systems is not sat-
isfactory from an applicative point of view yet.
</bodyText>
<page confidence="0.988038">
403
</page>
<note confidence="0.9917685">
Proceedings of the 43rd Annual Meeting of the ACL, pages 403–410,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.9666115">
To achieve these goals we identified two promis-
ing research directions:
</bodyText>
<listItem confidence="0.988225833333333">
1. Modeling independently domain and syntag-
matic aspects of sense distinction, to improve
the feature representation of sense tagged ex-
amples (Gliozzo et al., 2004).
2. Leveraging external knowledge acquired from
unlabeled corpora.
</listItem>
<bodyText confidence="0.999983569230769">
The first direction is motivated by the linguistic
assumption that syntagmatic and domain (associa-
tive) relations are both crucial to represent sense
distictions, while they are basically originated by
very different phenomena. Syntagmatic relations
hold among words that are typically located close
to each other in the same sentence in a given tempo-
ral order, while domain relations hold among words
that are typically used in the same semantic domain
(i.e. in texts having similar topics (Gliozzo et al.,
2004)). Their different nature suggests to adopt dif-
ferent learning strategies to detect them.
Regarding the second direction, external knowl-
edge would be required to help WSD algorithms to
better generalize over the data available for train-
ing. On the other hand, most of the state-of-the-art
supervised approaches to WSD are still completely
based on “internal” information only (i.e. the only
information available to the training algorithm is the
set of manually annotated examples). For exam-
ple, in the Senseval-3 evaluation exercise (Mihal-
cea and Edmonds, 2004) many lexical sample tasks
were provided, beyond the usual labeled training
data, with a large set of unlabeled data. However,
at our knowledge, none of the participants exploited
this unlabeled material. Exploring this direction is
the main focus of this paper. In particular we ac-
quire a Domain Model (DM) for the lexicon (i.e.
a lexical resource representing domain associations
among terms), and we exploit this information in-
side our supervised WSD algorithm. DMs can be
automatically induced from unlabeled corpora, al-
lowing the portability of the methodology among
languages.
We identified kernel methods as a viable frame-
work in which to implement the assumptions above
(Strapparava et al., 2004).
Exploiting the properties of kernels, we have de-
fined independently a set of domain and syntagmatic
kernels and we combined them in order to define a
complete kernel for WSD. The domain kernels esti-
mate the (domain) similarity (Magnini et al., 2002)
among contexts, while the syntagmatic kernels eval-
uate the similarity among collocations.
We will demonstrate that using DMs induced
from unlabeled corpora is a feasible strategy to in-
crease the generalization capability of the WSD al-
gorithm. Our system far outperforms the state-of-
the-art systems in all the tasks in which it has been
tested. Moreover, a comparative analysis of the
learning curves shows that the use of DMs allows
us to remarkably reduce the amount of sense-tagged
examples, opening new scenarios to develop sys-
tems for all-words tasks with minimal supervision.
The paper is structured as follows. Section 2 in-
troduces the notion of Domain Model. In particular
an automatic acquisition technique based on Latent
Semantic Analysis (LSA) is described. In Section 3
we present a WSD system based on a combination
of kernels. In particular we define a Domain Ker-
nel (see Section 3.1) and a Syntagmatic Kernel (see
Section 3.2), to model separately syntagmatic and
domain aspects. In Section 4 our WSD system is
evaluated in the Senseval-3 English, Italian, Spanish
and Catalan lexical sample tasks.
</bodyText>
<sectionHeader confidence="0.991611" genericHeader="method">
2 Domain Models
</sectionHeader>
<bodyText confidence="0.998866588235294">
The simplest methodology to estimate the similar-
ity among the topics of two texts is to represent
them by means of vectors in the Vector Space Model
(VSM), and to exploit the cosine similarity. More
formally, let C = {t1, t2, ... , tn} be a corpus, let
V = {w1, w2, ... , wk} be its vocabulary, let T be
the k x n term-by-document matrix representing C,
such that tij is the frequency of word wz into the text
tj. The VSM is a k-dimensional space Rk, in which
the text tj E C is represented by means of the vec-
tor tj such that the ith component of t`j is tij. The
similarity among two texts in the VSM is estimated
by computing the cosine among them.
However this approach does not deal well with
lexical variability and ambiguity. For example the
two sentences “he is affected by AIDS” and “HIV is
a virus” do not have any words in common. In the
</bodyText>
<page confidence="0.99845">
404
</page>
<bodyText confidence="0.999712235294118">
VSM their similarity is zero because they have or-
thogonal vectors, even if the concepts they express
are very closely related. On the other hand, the sim-
ilarity between the two sentences “the laptop has
been infected by a virus” and “HIV is a virus” would
turn out very high, due to the ambiguity of the word
virus.
To overcome this problem we introduce the notion
of Domain Model (DM), and we show how to use it
in order to define a domain VSM in which texts and
terms are represented in a uniform way.
A DM is composed by soft clusters of terms. Each
cluster represents a semantic domain, i.e. a set of
terms that often co-occur in texts having similar top-
ics. A DM is represented by a kxk&apos; rectangular ma-
trix D, containing the degree of association among
terms and domains, as illustrated in Table 1.
</bodyText>
<table confidence="0.904177">
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
</table>
<tableCaption confidence="0.999396">
Table 1: Example of Domain Matrix
</tableCaption>
<bodyText confidence="0.999669823529412">
DMs can be used to describe lexical ambiguity
and variability. Lexical ambiguity is represented
by associating one term to more than one domain,
while variability is represented by associating dif-
ferent terms to the same domain. For example the
term virus is associated to both the domain COM-
PUTER SCIENCE and the domain MEDICINE (ambi-
guity) while the domain MEDICINE is associated to
both the terms AIDS and HIV (variability).
More formally, let D = {D1, D2,..., Dk0} be a
set of domains, such that k&apos; « k. A DM is fully
defined by a k x k&apos; domain matrix D representing in
each cell di,z the domain relevance of term wi with
respect to the domain Dz. The domain matrix D is
used to define a function D : ][Rk —* ][Sk0, that maps
the vectors t� expressed into the classical VSM, into
the vectors �tj in the domain VSM. D is defined by1
</bodyText>
<equation confidence="0.910276">
D(t�) = t�(IIDFD) = ��&apos; (1)
j
</equation>
<tableCaption confidence="0.750809">
1In (Wong et al., 1985) the formula 1 is used to define a
Generalized Vector Space Model, of which the Domain VSM is
a particular instance.
</tableCaption>
<bodyText confidence="0.980843">
where IIDF is a k x k diagonal matrix such that
</bodyText>
<equation confidence="0.9436555">
iIDF
i,i = IDF(wi), t� is represented as a row vector,
</equation>
<bodyText confidence="0.987962842105263">
and IDF(wi) is the Inverse Document Frequency of
wi.
Vectors in the domain VSM are called Domain
Vectors (DVs). DVs for texts are estimated by ex-
ploiting the formula 1, while the DV �w&apos;i, correspond-
ing to the word wi E V is the ith row of the domain
matrix D. To be a valid domain matrix such vectors
should be normalized (i,e. (�w&apos;i, �w&apos;i) = 1).
In the Domain VSM the similarity among DVs is
estimated by taking into account second order rela-
tions among terms. For example the similarity of the
two sentences “He is affected by AIDS” and “HIV
is a virus” is very high, because the terms AIDS,
HIV and virus are highly associated to the domain
MEDICINE.
A DM can be estimated from hand made lexical
resources such as WORDNET DOMAINS (Magnini
and Cavagli`a, 2000), or by performing a term clus-
tering process on a large corpus. We think that the
second methodology is more attractive, because it
allows us to automatically acquire DMs for different
languages.
In this work we propose the use of Latent Seman-
tic Analysis (LSA) to induce DMs from corpora.
LSA is an unsupervised technique for estimating the
similarity among texts and terms in a corpus. LSA
is performed by means of a Singular Value Decom-
position (SVD) of the term-by-document matrix T
describing the corpus. The SVD algorithm can be
exploited to acquire a domain matrix D from a large
corpus C in a totally unsupervised way. SVD de-
composes the term-by-document matrix T into three
matrixes T — VEk0UT where Ek0 is the diagonal
k x k matrix containing the highest k&apos; « k eigen-
values of T, and all the remaining elements set to
0. The parameter k&apos; is the dimensionality of the Do-
main VSM and can be fixed in advance2. Under this
setting we define the domain matrix DLSA as
</bodyText>
<equation confidence="0.9008905">
V
DLSA = INV Ek0 (2)
</equation>
<bodyText confidence="0.9868915">
where IN is a diagonal matrix such that iNi,i =
�w&apos;i is the ith row of the matrix V✓Ek0.3
</bodyText>
<footnote confidence="0.998491333333333">
2It is not clear how to choose the right dimensionality. In
our experiments we used 50 dimensions.
3When DLSA is substituted in Equation 1 the Domain VSM
</footnote>
<equation confidence="0.84944">
1
� �),
( ~w0 �, ~w0
</equation>
<page confidence="0.996961">
405
</page>
<sectionHeader confidence="0.999215" genericHeader="method">
3 Kernel Methods for WSD
</sectionHeader>
<bodyText confidence="0.999969307692308">
In the introduction we discussed two promising di-
rections for improving the performance of a super-
vised disambiguation system. In this section we
show how these requirements can be efficiently im-
plemented in a natural and elegant way by using ker-
nel methods.
The basic idea behind kernel methods is to embed
the data into a suitable feature space F via a map-
ping function 0 : X — F, and then use a linear al-
gorithm for discovering nonlinear patterns. Instead
of using the explicit mapping 0, we can use a kernel
function K : X x X —* R, that corresponds to the
inner product in a feature space which is, in general,
different from the input space.
Kernel methods allow us to build a modular sys-
tem, as the kernel function acts as an interface be-
tween the data and the learning algorithm. Thus
the kernel function becomes the only domain spe-
cific module of the system, while the learning algo-
rithm is a general purpose component. Potentially
any kernel function can work with any kernel-based
algorithm. In our system we use Support Vector Ma-
chines (Cristianini and Shawe-Taylor, 2000).
Exploiting the properties of the kernel func-
tions, it is possible to define the kernel combination
schema as
</bodyText>
<equation confidence="0.944982">
Kl(xi, xj)
3
1/Kl(xj, xj)Kl(xi, xi)
</equation>
<bodyText confidence="0.99462475">
Our WSD system is then defined as combination
of n basic kernels. Each kernel adds some addi-
tional dimensions to the feature space. In particular,
we have defined two families of kernels: Domain
and Syntagmatic kernels. The former is composed
by both the Domain Kernel (KD) and the Bag-of-
Words kernel (KBoW), that captures domain aspects
(see Section 3.1). The latter captures the syntag-
matic aspects of sense distinction and it is composed
by two kernels: the collocation kernel (KColl) and
is equivalent to a Latent Semantic Space (Deerwester et al.,
1990). The only difference in our formulation is that the vectors
representing the terms in the Domain VSM are normalized by
the matrix IN, and then rescaled, according to their IDF value,
by matrix IIDF. Note the analogy with the tf idf term weighting
schema (Salton and McGill, 1983), widely adopted in Informa-
tion Retrieval.
the Part of Speech kernel (KPoS) (see Section 3.2).
The WSD kernels (K�WSD and KWSD) are then de-
fined by combining them (see Section 3.3).
</bodyText>
<subsectionHeader confidence="0.997984">
3.1 Domain Kernels
</subsectionHeader>
<bodyText confidence="0.996843722222222">
In (Magnini et al., 2002), it has been claimed that
knowing the domain of the text in which the word
is located is a crucial information for WSD. For
example the (domain) polysemy among the COM-
PUTER SCIENCE and the MEDICINE senses of the
word virus can be solved by simply considering
the domain of the context in which it is located.
This assumption can be modeled by defining a
kernel that estimates the domain similarity among
the contexts of the words to be disambiguated,
namely the Domain Kernel. The Domain Kernel es-
timates the similarity among the topics (domains) of
two texts, so to capture domain aspects of sense dis-
tinction. It is a variation of the Latent Semantic Ker-
nel (Shawe-Taylor and Cristianini, 2004), in which a
DM (see Section 2) is exploited to define an explicit
mapping D : Rk —* Rk&apos; from the classical VSM into
the Domain VSM. The Domain Kernel is defined by
</bodyText>
<equation confidence="0.997799666666667">
(D(ti), D(tj))
KD(ti, tj) = (4)
1/(D(ti), D(tj)) (D (ti), D(tj))
</equation>
<bodyText confidence="0.99954025">
where D is the Domain Mapping defined in equa-
tion 1. Thus the Domain Kernel requires a Domain
Matrix D. For our experiments we acquire the ma-
trix DLSA, described in equation 2, from a generic
collection of unlabeled documents, as explained in
Section 2.
A more traditional approach to detect topic (do-
main) similarity is to extract Bag-of-Words (BoW)
features from a large window of text around the
word to be disambiguated. The BoW kernel, de-
noted by KBoW, is a particular case of the Domain
Kernel, in which D = I, and I is the identity ma-
trix. The BoW kernel does not require a DM, then it
can be applied to the “strictly” supervised settings,
in which an external knowledge source is not pro-
vided.
</bodyText>
<subsectionHeader confidence="0.999503">
3.2 Syntagmatic kernels
</subsectionHeader>
<bodyText confidence="0.999622333333333">
Kernel functions are not restricted to operate on vec-
torial objects x� E Rk. In principle kernels can be
defined for any kind of object representation, as for
</bodyText>
<equation confidence="0.968070333333333">
n
KC(xi, xj) =
l=1
</equation>
<page confidence="0.987579">
406
</page>
<bodyText confidence="0.999686444444444">
example sequences and trees. As stated in Section 1,
syntagmatic relations hold among words collocated
in a particular temporal order, thus they can be mod-
eled by analyzing sequences of words.
We identified the string kernel (or word se-
quence kernel) (Shawe-Taylor and Cristianini, 2004)
as a valid instrument to model our assumptions.
The string kernel counts how many times a (non-
contiguous) subsequence of symbols u of length
n occurs in the input string s, and penalizes non-
contiguous occurrences according to the number of
gaps they contain (gap-weighted subsequence ker-
nel).
Formally, let V be the vocabulary, the feature
space associated with the gap-weighted subsequence
kernel of length n is indexed by a set I of subse-
quences over V of length n. The (explicit) mapping
function is defined by
</bodyText>
<equation confidence="0.9937025">
φnu(s) = � λl(i), u E V n (5)
isu=s(i)
</equation>
<bodyText confidence="0.999730333333333">
where u = s(i) is a subsequence of s in the posi-
tions given by the tuple i, l(i) is the length spanned
by u, and λ E]0,1] is the decay factor used to penal-
ize non-contiguous subsequences.
The associate gap-weighted subsequence kernel is
defined by
</bodyText>
<equation confidence="0.9719545">
kn(s%, s 7) = (φn(s,), φn(s,)) = E φn(s,)φn(s,) (6)
uEV9d
</equation>
<bodyText confidence="0.998149793103448">
We modified the generic definition of the string
kernel in order to make it able to recognize collo-
cations in a local window of the word to be disam-
biguated. In particular we defined two Syntagmatic
kernels: the n-gram Collocation Kernel and the n-
gram PoS Kernel. The n-gram Collocation ker-
nel KnColl is defined as a gap-weighted subsequence
kernel applied to sequences of lemmata around the
word l0 to be disambiguated (i.e. l−3, l−2, l−1, l0,
l+1, l+2, l+3). This formulation allows us to esti-
mate the number of common (sparse) subsequences
of lemmata (i.e. collocations) between two exam-
ples, in order to capture syntagmatic similarity. In
analogy we defined the PoS kernel KnPoS, by setting
s to the sequence of PoSs p−3, p−2, p−1, p0, p+1,
p+2, p+3, where p0 is the PoS of the word to be dis-
ambiguated.
The definition of the gap-weighted subsequence
kernel, provided by equation 6, depends on the pa-
rameter n, that represents the length of the sub-
sequences analyzed when estimating the similarity
among sequences. For example, K2Coll allows us to
represent the bigrams around the word to be disam-
biguated in a more flexible way (i.e. bigrams can be
sparse). In WSD, typical features are bigrams and
trigrams of lemmata and PoSs around the word to
be disambiguated, then we defined the Collocation
Kernel and the PoS Kernel respectively by equations
7 and 84.
</bodyText>
<equation confidence="0.99755675">
KColl(si,sj) = � p KlColl(si, sj) (7)
l=1
KPoS(si, sj) = � p KlPoS(si, sj) (8)
l=1
</equation>
<subsectionHeader confidence="0.984023">
3.3 WSD kernels
</subsectionHeader>
<bodyText confidence="0.999862583333333">
In order to show the impact of using Domain Models
in the supervised learning process, we defined two
WSD kernels, by applying the kernel combination
schema described by equation 3. Thus the following
WSD kernels are fully specified by the list of the
kernels that compose them.
Kwsd composed by KColl, KPoS and KBoW
KK,sd composed by KColl, KPoS, KBoW and KD
The only difference between the two systems is
that K�wsd uses Domain Kernel KD. K�wsd exploits
external knowledge, in contrast to Kwsd, whose only
available information is the labeled training data.
</bodyText>
<sectionHeader confidence="0.977302" genericHeader="evaluation">
4 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.997004">
In this section we present the performance of our
kernel-based algorithms for WSD. The objectives of
these experiments are:
</bodyText>
<listItem confidence="0.9951448">
• to study the combination of different kernels,
• to understand the benefits of plugging external
information using domain models,
• to verify the portability of our methodology
among different languages.
</listItem>
<footnote confidence="0.837732">
4The parameters p and λ are optimized by cross-validation.
The best results are obtained setting p = 2, λ = 0.5 for KCou
and λ __+ 0 for KPoS.
</footnote>
<page confidence="0.993106">
407
</page>
<subsectionHeader confidence="0.992022">
4.1 WSD tasks
</subsectionHeader>
<bodyText confidence="0.999595333333333">
We conducted the experiments on four lexical sam-
ple tasks (English, Catalan, Italian and Spanish)
of the Senseval-3 competition (Mihalcea and Ed-
monds, 2004). Table 2 describes the tasks by re-
porting the number of words to be disambiguated,
the mean polysemy, and the dimension of training,
test and unlabeled corpora. Note that the organiz-
ers of the English task did not provide any unlabeled
material. So for English we used a domain model
built from a portion of BNC corpus, while for Span-
ish, Italian and Catalan we acquired DMs from the
unlabeled corpora made available by the organizers.
</bodyText>
<table confidence="0.9988018">
#w pol # train # test # unlab
Catalan 27 3.11 4469 2253 23935
English 57 6.47 7860 3944 -
Italian 45 6.30 5145 2439 74788
Spanish 46 3.30 8430 4195 61252
</table>
<tableCaption confidence="0.995194">
Table 2: Dataset descriptions
</tableCaption>
<subsectionHeader confidence="0.995971">
4.2 Kernel Combination
</subsectionHeader>
<bodyText confidence="0.999854375">
In this section we present an experiment to em-
pirically study the kernel combination. The basic
kernels (i.e. KBoW, KD, KColl and KPoS) have
been compared to the combined ones (i.e. Kwsd and
K�wsd) on the English lexical sample task.
The results are reported in Table 3. The results
show that combining kernels significantly improves
the performance of the system.
</bodyText>
<table confidence="0.842653">
KD KBoW KPo3 KCod! Kwsd Kwsd
F1 65.5 63.7 62.9 66.7 69.7 73.3
</table>
<tableCaption confidence="0.915410666666667">
Table 3: The performance (F1) of each basic ker-
nel and their combination for English lexical sample
task.
</tableCaption>
<subsectionHeader confidence="0.999302">
4.3 Portability and Performance
</subsectionHeader>
<bodyText confidence="0.999426916666667">
We evaluated the performance of Kwsd and Kwsd on
the lexical sample tasks described above. The results
are showed in Table 4 and indicate that using DMs
allowed Kwsd to significantly outperform Kwsd.
In addition, K�wsd turns out the best systems for
all the tested Senseval-3 tasks.
Finally, the performance of K�wsd are higher than
the human agreement for the English and Spanish
taskss.
Note that, in order to guarantee an uniform appli-
cation to any language, we do not use any syntactic
information provided by a parser.
</bodyText>
<subsectionHeader confidence="0.999869">
4.4 Learning Curves
</subsectionHeader>
<bodyText confidence="0.999049090909091">
The Figures 1, 2, 3 and 4 show the learning curves
evaluated on K�wsd and Kwsd for all the lexical sam-
ple tasks.
The learning curves indicate that Kwsd is far su-
perior to Kwsd for all the tasks, even with few ex-
amples. The result is extremely promising, for it
demonstrates that DMs allow to drastically reduce
the amount of sense tagged data required for learn-
ing. It is worth noting, as reported in Table 5, that
K�wsd achieves the same performance of Kwsd using
about half of the training data.
</bodyText>
<table confidence="0.9799182">
% of training
English 54
Catalan 46
Italian 51
Spanish 50
</table>
<tableCaption confidence="0.874322666666667">
Table 5: Percentage of sense tagged examples re-
quired by K�wsd to achieve the same performance of
Kwsd with full training.
</tableCaption>
<sectionHeader confidence="0.990528" genericHeader="conclusions">
5 Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.989687571428571">
In this paper we presented a supervised algorithm
for WSD, based on a combination of kernel func-
tions. In particular we modeled domain and syn-
tagmatic aspects of sense distinctions by defining
respectively domain and syntagmatic kernels. The
Domain kernel exploits Domain Models, acquired
from “external” untagged corpora, to estimate the
similarity among the contexts of the words to be dis-
ambiguated. The syntagmatic kernels evaluate the
similarity between collocations.
We evaluated our algorithm on several Senseval-
3 lexical sample tasks (i.e. English, Spanish, Ital-
ian and Catalan) significantly improving the state-ot-
the-art for all of them. In addition, the performance
</bodyText>
<footnote confidence="0.9968805">
5It is not clear if the inter-annotator-agreement can be con-
siderated the upper bound for a WSD system.
</footnote>
<page confidence="0.987332">
408
</page>
<table confidence="0.9998316">
MF Agreement BEST Kwsd K�wsd DM+
English 55.2 67.3 72.9 69.7 73.3 3.6
Catalan 66.3 93.1 85.2 85.2 89.0 3.8
Italian 18.0 89.0 53.1 53.1 61.3 8.2
Spanish 67.7 85.3 84.2 84.2 88.2 4.0
</table>
<tableCaption confidence="0.9731235">
Table 4: Comparative evaluation on the lexical sample tasks. Columns report: the Most Frequent baseline,
the inter annotator agreement, the F1 of the best system at Senseval-3, the F1 of Kwsd, the F1 of Kwsd,
</tableCaption>
<figure confidence="0.948013">
DM+ (the improvement due to DM, i.e. Kwsd − Kwsd).
0.75
0.7
0.65
F1
0.6
0.55
0.5
0 0.2 0.4 0.6 0.8 1
Percentage of training set
</figure>
<figureCaption confidence="0.8870625">
Figure 1: Learning curves for English lexical sample
task.
</figureCaption>
<figure confidence="0.98566225">
K&apos;wsd
K wsd
0 0.2 0.4 0.6 0.8 1
Percentage of training set
</figure>
<figureCaption confidence="0.891715">
Figure 2: Learning curves for Catalan lexical sample
task.
</figureCaption>
<figure confidence="0.997302">
0 0.2 0.4 0.6 0.8 1
Percentage of training set
</figure>
<figureCaption confidence="0.936155">
Figure 3: Learning curves for Italian lexical sample
task.
</figureCaption>
<figure confidence="0.994275">
0 0.2 0.4 0.6 0.8 1
Percentage of training set
</figure>
<figureCaption confidence="0.9940225">
Figure 4: Learning curves for Spanish lexical sam-
ple task.
</figureCaption>
<figure confidence="0.999002413793103">
K&apos;wsd
K wsd
K&apos;wsd
K wsd
K&apos;wsd
K wsd
F1 0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.9
0.85
0.8
F1
0.75
0.7
0.65
F1 0.9
0.85
0.8
0.75
0.7
0.65
0.6
</figure>
<bodyText confidence="0.995356625">
of our system outperforms the inter annotator agree-
ment in both English and Spanish, achieving the up-
per bound performance.
We demonstrated that using external knowledge
inside a supervised framework is a viable method-
ology to reduce the amount of training data required
for learning. In our approach the external knowledge
is represented by means of Domain Models automat-
</bodyText>
<page confidence="0.99694">
409
</page>
<bodyText confidence="0.999975789473684">
ically acquired from corpora in a totally unsuper-
vised way. Experimental results show that the use
of Domain Models allows us to reduce the amount
of training data, opening an interesting research di-
rection for all those NLP tasks for which the Knowl-
edge Acquisition Bottleneck is a crucial problem. In
particular we plan to apply the same methodology to
Text Categorization, by exploiting the Domain Ker-
nel to estimate the similarity among texts. In this im-
plementation, our WSD system does not exploit syn-
tactic information produced by a parser. For the fu-
ture we plan to integrate such information by adding
a tree kernel (i.e. a kernel function that evaluates the
similarity among parse trees) to the kernel combi-
nation schema presented in this paper. Last but not
least, we are going to apply our approach to develop
supervised systems for all-words tasks, where the
quantity of data available to train each word expert
classifier is very low.
</bodyText>
<sectionHeader confidence="0.998724" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.729548">
Alfio Gliozzo and Carlo Strapparava were partially
supported by the EU project Meaning (IST-2001-
34460). Claudio Giuliano was supported by the EU
project Dot.Kom (IST-2001-34038). We would like
to thank Oier Lopez de Lacalle for useful comments.
</bodyText>
<sectionHeader confidence="0.976094" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999655829787234">
N. Cristianini and J. Shawe-Taylor. 2000. An introduc-
tion to Support Vector Machines. Cambridge Univer-
sity Press.
B. Decadt, V. Hoste, W. Daelemens, and A. van den
Bosh. 2004. Gambl, genetic algorithm optimiza-
tion of memory-based wsd. In Proc. of Senseval-3,
Barcelona, July.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18(3):275–299.
B. Magnini and G. Cavagli`a. 2000. Integrating subject
field codes into WordNet. In Proceedings of LREC-
2000, pages 1413–1418, Athens, Greece, June.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359–373.
R. Mihalcea and P. Edmonds, editors. 2004. Proceedings
of SENSEVAL-3, Barcelona, Spain, July.
R. Mihalcea and E. Faruque. 2004. Senselearner: Min-
imally supervised WSD for all words in open text. In
Proceedings of SENSEVAL-3, Barcelona, Spain, July.
G. Salton and M.H. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
S. Small. 1980. Word Expert Parsing: A Theory of Dis-
tributed Word-based Natural Language Understand-
ing. Ph.D. Thesis, Department of Computer Science,
University of Maryland.
C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pat-
tern abstraction and term similarity for word sense
disambiguation: Irst at senseval-3. In Proc. of
SENSEVAL-3 Third International Workshop on Eval-
uation of Systems for the Semantic Analysis of Text,
pages 229–234, Barcelona, Spain, July.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Gen-
eralized vector space model in information retrieval.
In Proceedings of the 81h ACM SIGIR Conference.
D. Yarowsky and R. Florian. 2002. Evaluating sense dis-
ambiguation across diverse parameter space. Natural
Language Engineering, 8(4):293–310.
</reference>
<page confidence="0.99702">
410
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930383">
<title confidence="0.999903">Domain Kernels for Word Sense Disambiguation</title>
<author confidence="0.999267">Alfio Gliozzo</author>
<author confidence="0.999267">Claudio Giuliano</author>
<author confidence="0.999267">Carlo Strapparava</author>
<affiliation confidence="0.974809">ITC-irst, Istituto per la Ricerca Scientifica e Tecnologica</affiliation>
<address confidence="0.997457">I-38050, Trento, ITALY</address>
<abstract confidence="0.99782895">In this paper we present a supervised Word Sense Disambiguation methodology, that exploits kernel methods to model sense distinctions. In particular a combination of kernel functions is adopted to independently both We defined a kernel function, namely the Domain Kernel, that allowed us to plug “external knowledge” into the supervised learning process. External knowledge is acquired from unlabeled data in a totally unsupervised way, and it is represented by means of Domain Models. We evaluated our methodology on several lexical sample tasks in different languages, outperforming significantly the state-of-the-art for each of them, while reducing the amount of labeled training data required for learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
</authors>
<title>An introduction to Support Vector Machines.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="12774" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="2182" endWordPosition="2185">. Instead of using the explicit mapping 0, we can use a kernel function K : X x X —* R, that corresponds to the inner product in a feature space which is, in general, different from the input space. Kernel methods allow us to build a modular system, as the kernel function acts as an interface between the data and the learning algorithm. Thus the kernel function becomes the only domain specific module of the system, while the learning algorithm is a general purpose component. Potentially any kernel function can work with any kernel-based algorithm. In our system we use Support Vector Machines (Cristianini and Shawe-Taylor, 2000). Exploiting the properties of the kernel functions, it is possible to define the kernel combination schema as Kl(xi, xj) 3 1/Kl(xj, xj)Kl(xi, xi) Our WSD system is then defined as combination of n basic kernels. Each kernel adds some additional dimensions to the feature space. In particular, we have defined two families of kernels: Domain and Syntagmatic kernels. The former is composed by both the Domain Kernel (KD) and the Bag-ofWords kernel (KBoW), that captures domain aspects (see Section 3.1). The latter captures the syntagmatic aspects of sense distinction and it is composed by two kerne</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>N. Cristianini and J. Shawe-Taylor. 2000. An introduction to Support Vector Machines. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Decadt</author>
<author>V Hoste</author>
<author>W Daelemens</author>
<author>A van den Bosh</author>
</authors>
<title>Gambl, genetic algorithm optimization of memory-based wsd.</title>
<date>2004</date>
<booktitle>In Proc. of Senseval-3,</booktitle>
<location>Barcelona,</location>
<marker>Decadt, Hoste, Daelemens, van den Bosh, 2004</marker>
<rawString>B. Decadt, V. Hoste, W. Daelemens, and A. van den Bosh. 2004. Gambl, genetic algorithm optimization of memory-based wsd. In Proc. of Senseval-3, Barcelona, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S Dumais</author>
<author>G Furnas</author>
<author>T Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science.</journal>
<contexts>
<context position="13479" citStr="Deerwester et al., 1990" startWordPosition="2298" endWordPosition="2301"> kernel combination schema as Kl(xi, xj) 3 1/Kl(xj, xj)Kl(xi, xi) Our WSD system is then defined as combination of n basic kernels. Each kernel adds some additional dimensions to the feature space. In particular, we have defined two families of kernels: Domain and Syntagmatic kernels. The former is composed by both the Domain Kernel (KD) and the Bag-ofWords kernel (KBoW), that captures domain aspects (see Section 3.1). The latter captures the syntagmatic aspects of sense distinction and it is composed by two kernels: the collocation kernel (KColl) and is equivalent to a Latent Semantic Space (Deerwester et al., 1990). The only difference in our formulation is that the vectors representing the terms in the Domain VSM are normalized by the matrix IN, and then rescaled, according to their IDF value, by matrix IIDF. Note the analogy with the tf idf term weighting schema (Salton and McGill, 1983), widely adopted in Information Retrieval. the Part of Speech kernel (KPoS) (see Section 3.2). The WSD kernels (K�WSD and KWSD) are then defined by combining them (see Section 3.3). 3.1 Domain Kernels In (Magnini et al., 2002), it has been claimed that knowing the domain of the text in which the word is located is a cr</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
<author>C Strapparava</author>
<author>I Dagan</author>
</authors>
<title>Unsupervised and supervised exploitation of semantic domains in lexical disambiguation.</title>
<date>2004</date>
<journal>Computer Speech and Language,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="3544" citStr="Gliozzo et al., 2004" startWordPosition="554" endWordPosition="557">fundamental step towards the direction of designing a WSD system that works well on real texts. In addition, it is a common opinion that the performance of state-of-the-art WSD systems is not satisfactory from an applicative point of view yet. 403 Proceedings of the 43rd Annual Meeting of the ACL, pages 403–410, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics To achieve these goals we identified two promising research directions: 1. Modeling independently domain and syntagmatic aspects of sense distinction, to improve the feature representation of sense tagged examples (Gliozzo et al., 2004). 2. Leveraging external knowledge acquired from unlabeled corpora. The first direction is motivated by the linguistic assumption that syntagmatic and domain (associative) relations are both crucial to represent sense distictions, while they are basically originated by very different phenomena. Syntagmatic relations hold among words that are typically located close to each other in the same sentence in a given temporal order, while domain relations hold among words that are typically used in the same semantic domain (i.e. in texts having similar topics (Gliozzo et al., 2004)). Their different </context>
</contexts>
<marker>Gliozzo, Strapparava, Dagan, 2004</marker>
<rawString>A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsupervised and supervised exploitation of semantic domains in lexical disambiguation. Computer Speech and Language, 18(3):275–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>G Cavagli`a</author>
</authors>
<title>Integrating subject field codes into WordNet.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC2000,</booktitle>
<pages>1413--1418</pages>
<location>Athens, Greece,</location>
<marker>Magnini, Cavagli`a, 2000</marker>
<rawString>B. Magnini and G. Cavagli`a. 2000. Integrating subject field codes into WordNet. In Proceedings of LREC2000, pages 1413–1418, Athens, Greece, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>C Strapparava</author>
<author>G Pezzulo</author>
<author>A Gliozzo</author>
</authors>
<title>The role of domain information in word sense disambiguation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="5642" citStr="Magnini et al., 2002" startWordPosition="880" endWordPosition="883">xical resource representing domain associations among terms), and we exploit this information inside our supervised WSD algorithm. DMs can be automatically induced from unlabeled corpora, allowing the portability of the methodology among languages. We identified kernel methods as a viable framework in which to implement the assumptions above (Strapparava et al., 2004). Exploiting the properties of kernels, we have defined independently a set of domain and syntagmatic kernels and we combined them in order to define a complete kernel for WSD. The domain kernels estimate the (domain) similarity (Magnini et al., 2002) among contexts, while the syntagmatic kernels evaluate the similarity among collocations. We will demonstrate that using DMs induced from unlabeled corpora is a feasible strategy to increase the generalization capability of the WSD algorithm. Our system far outperforms the state-ofthe-art systems in all the tasks in which it has been tested. Moreover, a comparative analysis of the learning curves shows that the use of DMs allows us to remarkably reduce the amount of sense-tagged examples, opening new scenarios to develop systems for all-words tasks with minimal supervision. The paper is struc</context>
<context position="13985" citStr="Magnini et al., 2002" startWordPosition="2385" endWordPosition="2388"> two kernels: the collocation kernel (KColl) and is equivalent to a Latent Semantic Space (Deerwester et al., 1990). The only difference in our formulation is that the vectors representing the terms in the Domain VSM are normalized by the matrix IN, and then rescaled, according to their IDF value, by matrix IIDF. Note the analogy with the tf idf term weighting schema (Salton and McGill, 1983), widely adopted in Information Retrieval. the Part of Speech kernel (KPoS) (see Section 3.2). The WSD kernels (K�WSD and KWSD) are then defined by combining them (see Section 3.3). 3.1 Domain Kernels In (Magnini et al., 2002), it has been claimed that knowing the domain of the text in which the word is located is a crucial information for WSD. For example the (domain) polysemy among the COMPUTER SCIENCE and the MEDICINE senses of the word virus can be solved by simply considering the domain of the context in which it is located. This assumption can be modeled by defining a kernel that estimates the domain similarity among the contexts of the words to be disambiguated, namely the Domain Kernel. The Domain Kernel estimates the similarity among the topics (domains) of two texts, so to capture domain aspects of sense </context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2002</marker>
<rawString>B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo. 2002. The role of domain information in word sense disambiguation. Natural Language Engineering, 8(4):359–373.</rawString>
</citation>
<citation valid="true">
<date>2004</date>
<booktitle>Proceedings of SENSEVAL-3,</booktitle>
<editor>R. Mihalcea and P. Edmonds, editors.</editor>
<location>Barcelona, Spain,</location>
<marker>2004</marker>
<rawString>R. Mihalcea and P. Edmonds, editors. 2004. Proceedings of SENSEVAL-3, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>E Faruque</author>
</authors>
<title>Senselearner: Minimally supervised WSD for all words in open text.</title>
<date>2004</date>
<booktitle>In Proceedings of SENSEVAL-3,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2671" citStr="Mihalcea and Faruque, 2004" startWordPosition="411" endWordPosition="414">ngs to enhance the comprehension of WSD, they should be considered as preceding exercises to all-words tasks. However this is not the actual case. Algorithms designed for lexical sample WSD are often based on pure supervision and hence “data hungry”. We think that lexical sample WSD should regain its original explorative role and possibly use a minimal amount of training data, exploiting instead external knowledge acquired in an unsupervised way to reach the actual state-of-the-art performance. By the way, minimal supervision is the basis of state-of-the-art systems for all-words tasks (e.g. (Mihalcea and Faruque, 2004; Decadt et al., 2004)), that are trained on small sense tagged corpora (e.g. SemCor), in which few examples for a subset of the ambiguous words in the lexicon can be found. Thus improving the performance of WSD systems with few learning examples is a fundamental step towards the direction of designing a WSD system that works well on real texts. In addition, it is a common opinion that the performance of state-of-the-art WSD systems is not satisfactory from an applicative point of view yet. 403 Proceedings of the 43rd Annual Meeting of the ACL, pages 403–410, Ann Arbor, June 2005. c�2005 Assoc</context>
</contexts>
<marker>Mihalcea, Faruque, 2004</marker>
<rawString>R. Mihalcea and E. Faruque. 2004. Senselearner: Minimally supervised WSD for all words in open text. In Proceedings of SENSEVAL-3, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M H McGill</author>
</authors>
<title>Introduction to modern information retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="13759" citStr="Salton and McGill, 1983" startWordPosition="2346" endWordPosition="2349">nels. The former is composed by both the Domain Kernel (KD) and the Bag-ofWords kernel (KBoW), that captures domain aspects (see Section 3.1). The latter captures the syntagmatic aspects of sense distinction and it is composed by two kernels: the collocation kernel (KColl) and is equivalent to a Latent Semantic Space (Deerwester et al., 1990). The only difference in our formulation is that the vectors representing the terms in the Domain VSM are normalized by the matrix IN, and then rescaled, according to their IDF value, by matrix IIDF. Note the analogy with the tf idf term weighting schema (Salton and McGill, 1983), widely adopted in Information Retrieval. the Part of Speech kernel (KPoS) (see Section 3.2). The WSD kernels (K�WSD and KWSD) are then defined by combining them (see Section 3.3). 3.1 Domain Kernels In (Magnini et al., 2002), it has been claimed that knowing the domain of the text in which the word is located is a crucial information for WSD. For example the (domain) polysemy among the COMPUTER SCIENCE and the MEDICINE senses of the word virus can be solved by simply considering the domain of the context in which it is located. This assumption can be modeled by defining a kernel that estimat</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M.H. McGill. 1983. Introduction to modern information retrieval. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="14682" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="2506" endWordPosition="2509">hich the word is located is a crucial information for WSD. For example the (domain) polysemy among the COMPUTER SCIENCE and the MEDICINE senses of the word virus can be solved by simply considering the domain of the context in which it is located. This assumption can be modeled by defining a kernel that estimates the domain similarity among the contexts of the words to be disambiguated, namely the Domain Kernel. The Domain Kernel estimates the similarity among the topics (domains) of two texts, so to capture domain aspects of sense distinction. It is a variation of the Latent Semantic Kernel (Shawe-Taylor and Cristianini, 2004), in which a DM (see Section 2) is exploited to define an explicit mapping D : Rk —* Rk&apos; from the classical VSM into the Domain VSM. The Domain Kernel is defined by (D(ti), D(tj)) KD(ti, tj) = (4) 1/(D(ti), D(tj)) (D (ti), D(tj)) where D is the Domain Mapping defined in equation 1. Thus the Domain Kernel requires a Domain Matrix D. For our experiments we acquire the matrix DLSA, described in equation 2, from a generic collection of unlabeled documents, as explained in Section 2. A more traditional approach to detect topic (domain) similarity is to extract Bag-of-Words (BoW) features from a lar</context>
<context position="16107" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="2760" endWordPosition="2763">l does not require a DM, then it can be applied to the “strictly” supervised settings, in which an external knowledge source is not provided. 3.2 Syntagmatic kernels Kernel functions are not restricted to operate on vectorial objects x� E Rk. In principle kernels can be defined for any kind of object representation, as for n KC(xi, xj) = l=1 406 example sequences and trees. As stated in Section 1, syntagmatic relations hold among words collocated in a particular temporal order, thus they can be modeled by analyzing sequences of words. We identified the string kernel (or word sequence kernel) (Shawe-Taylor and Cristianini, 2004) as a valid instrument to model our assumptions. The string kernel counts how many times a (noncontiguous) subsequence of symbols u of length n occurs in the input string s, and penalizes noncontiguous occurrences according to the number of gaps they contain (gap-weighted subsequence kernel). Formally, let V be the vocabulary, the feature space associated with the gap-weighted subsequence kernel of length n is indexed by a set I of subsequences over V of length n. The (explicit) mapping function is defined by φnu(s) = � λl(i), u E V n (5) isu=s(i) where u = s(i) is a subsequence of s in the po</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Small</author>
</authors>
<title>Word Expert Parsing: A Theory of Distributed Word-based Natural Language Understanding.</title>
<date>1980</date>
<tech>Ph.D. Thesis,</tech>
<institution>Department of Computer Science, University of Maryland.</institution>
<contexts>
<context position="1593" citStr="Small, 1980" startWordPosition="237" endWordPosition="238">. 1 Introduction The main limitation of many supervised approaches for Natural Language Processing (NLP) is the lack of available annotated training data. This problem is known as the Knowledge Acquisition Bottleneck. To reach high accuracy, state-of-the-art systems for Word Sense Disambiguation (WSD) are designed according to a supervised learning framework, in which the disambiguation of each word in the lexicon is performed by constructing a different classifier. A large set of sense tagged examples is then required to train each classifier. This methodology is called word expert approach (Small, 1980; Yarowsky and Florian, 2002). However this is clearly unfeasible for all-words WSD tasks, in which all the words of an open text should be disambiguated. On the other hand, the word expert approach works very well for lexical sample WSD tasks (i.e. tasks in which it is required to disambiguate only those words for which enough training data is provided). As the original rationale of the lexical sample tasks was to define a clear experimental settings to enhance the comprehension of WSD, they should be considered as preceding exercises to all-words tasks. However this is not the actual case. A</context>
</contexts>
<marker>Small, 1980</marker>
<rawString>S. Small. 1980. Word Expert Parsing: A Theory of Distributed Word-based Natural Language Understanding. Ph.D. Thesis, Department of Computer Science, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>A Gliozzo</author>
<author>C Giuliano</author>
</authors>
<title>Pattern abstraction and term similarity for word sense disambiguation: Irst at senseval-3.</title>
<date>2004</date>
<booktitle>In Proc. of SENSEVAL-3 Third International Workshop on Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>229--234</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="5391" citStr="Strapparava et al., 2004" startWordPosition="838" endWordPosition="841">a, with a large set of unlabeled data. However, at our knowledge, none of the participants exploited this unlabeled material. Exploring this direction is the main focus of this paper. In particular we acquire a Domain Model (DM) for the lexicon (i.e. a lexical resource representing domain associations among terms), and we exploit this information inside our supervised WSD algorithm. DMs can be automatically induced from unlabeled corpora, allowing the portability of the methodology among languages. We identified kernel methods as a viable framework in which to implement the assumptions above (Strapparava et al., 2004). Exploiting the properties of kernels, we have defined independently a set of domain and syntagmatic kernels and we combined them in order to define a complete kernel for WSD. The domain kernels estimate the (domain) similarity (Magnini et al., 2002) among contexts, while the syntagmatic kernels evaluate the similarity among collocations. We will demonstrate that using DMs induced from unlabeled corpora is a feasible strategy to increase the generalization capability of the WSD algorithm. Our system far outperforms the state-ofthe-art systems in all the tasks in which it has been tested. More</context>
</contexts>
<marker>Strapparava, Gliozzo, Giuliano, 2004</marker>
<rawString>C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pattern abstraction and term similarity for word sense disambiguation: Irst at senseval-3. In Proc. of SENSEVAL-3 Third International Workshop on Evaluation of Systems for the Semantic Analysis of Text, pages 229–234, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K M Wong</author>
<author>W Ziarko</author>
<author>P C N Wong</author>
</authors>
<title>Generalized vector space model in information retrieval.</title>
<date>1985</date>
<booktitle>In Proceedings of the 81h ACM SIGIR Conference.</booktitle>
<contexts>
<context position="9420" citStr="Wong et al., 1985" startWordPosition="1566" endWordPosition="1569">o both the domain COMPUTER SCIENCE and the domain MEDICINE (ambiguity) while the domain MEDICINE is associated to both the terms AIDS and HIV (variability). More formally, let D = {D1, D2,..., Dk0} be a set of domains, such that k&apos; « k. A DM is fully defined by a k x k&apos; domain matrix D representing in each cell di,z the domain relevance of term wi with respect to the domain Dz. The domain matrix D is used to define a function D : ][Rk —* ][Sk0, that maps the vectors t� expressed into the classical VSM, into the vectors �tj in the domain VSM. D is defined by1 D(t�) = t�(IIDFD) = ��&apos; (1) j 1In (Wong et al., 1985) the formula 1 is used to define a Generalized Vector Space Model, of which the Domain VSM is a particular instance. where IIDF is a k x k diagonal matrix such that iIDF i,i = IDF(wi), t� is represented as a row vector, and IDF(wi) is the Inverse Document Frequency of wi. Vectors in the domain VSM are called Domain Vectors (DVs). DVs for texts are estimated by exploiting the formula 1, while the DV �w&apos;i, corresponding to the word wi E V is the ith row of the domain matrix D. To be a valid domain matrix such vectors should be normalized (i,e. (�w&apos;i, �w&apos;i) = 1). In the Domain VSM the similarity </context>
</contexts>
<marker>Wong, Ziarko, Wong, 1985</marker>
<rawString>S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Generalized vector space model in information retrieval. In Proceedings of the 81h ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>R Florian</author>
</authors>
<title>Evaluating sense disambiguation across diverse parameter space.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="1622" citStr="Yarowsky and Florian, 2002" startWordPosition="239" endWordPosition="242">ion The main limitation of many supervised approaches for Natural Language Processing (NLP) is the lack of available annotated training data. This problem is known as the Knowledge Acquisition Bottleneck. To reach high accuracy, state-of-the-art systems for Word Sense Disambiguation (WSD) are designed according to a supervised learning framework, in which the disambiguation of each word in the lexicon is performed by constructing a different classifier. A large set of sense tagged examples is then required to train each classifier. This methodology is called word expert approach (Small, 1980; Yarowsky and Florian, 2002). However this is clearly unfeasible for all-words WSD tasks, in which all the words of an open text should be disambiguated. On the other hand, the word expert approach works very well for lexical sample WSD tasks (i.e. tasks in which it is required to disambiguate only those words for which enough training data is provided). As the original rationale of the lexical sample tasks was to define a clear experimental settings to enhance the comprehension of WSD, they should be considered as preceding exercises to all-words tasks. However this is not the actual case. Algorithms designed for lexica</context>
</contexts>
<marker>Yarowsky, Florian, 2002</marker>
<rawString>D. Yarowsky and R. Florian. 2002. Evaluating sense disambiguation across diverse parameter space. Natural Language Engineering, 8(4):293–310.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>