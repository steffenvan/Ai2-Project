<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.048881">
<title confidence="0.967975">
The role of positive feedback in Intelligent Tutoring Systems
</title>
<author confidence="0.998562">
Davide Fossati
</author>
<affiliation confidence="0.999128">
Department of Computer Science
University of Illinois at Chicago
</affiliation>
<address confidence="0.735435">
Chicago, IL, USA
</address>
<email confidence="0.998897">
dfossa1@uic.edu
</email>
<sectionHeader confidence="0.995637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999791083333333">
The focus of this study is positive feedback in
one-on-one tutoring, its computational model-
ing, and its application to the design of more
effective Intelligent Tutoring Systems. A data
collection of tutoring sessions in the domain
of basic Computer Science data structures has
been carried out. A methodology based on
multiple regression is proposed, and some pre-
liminary results are presented. A prototype In-
telligent Tutoring System on linked lists has
been developed and deployed in a college-
level Computer Science class.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999918528301887">
One-on-one tutoring has been shown to be a very
effective form of instruction (Bloom, 1984). The
research community is working on discovering the
characteristics of tutoring. One of the goals is to un-
derstand the strategies tutors use, in order to design
effective learning environments and tools to support
learning. Among the tools, particular attention is
given to Intelligent Tutoring Systems (ITSs), which
are sophisticated software systems that can provide
personalized instruction to students, in some respect
similar to one-on-one tutoring (Beck et al., 1996).
Many of these systems have been shown to be very
effective (Evens and Michael, 2006; Van Lehn et al.,
2005; Di Eugenio et al., 2005; Mitrovi´c et al., 2004;
Person et al., 2001). In many experiments, ITSs in-
duced learning gains higher than those measured in
a classroom environment, but lower than those ob-
tained with one-on-one interactions with human tu-
tors. The belief of the research community is that
knowing more about human tutoring would help im-
prove the design of ITSs. In particular, the effective
use of natural language might be a key element. In
most of the studies mentioned above, systems with
more sophisticated language interfaces performed
better than other experimental conditions.
An important form of student-tutor interaction is
feedback. Negative feedback can be provided by the
tutor in response to students’ mistakes. An effective
use of negative feedback can help the student cor-
rect a mistake and prevent him/her from repeating
the same or a similar mistake again, effectively pro-
viding a learning opportunity to the student. Posi-
tive feedback is usually provided in response to some
correct input from the student. Positive feedback can
help students reinforce the correct knowledge they
already have, or successfully integrate new knowl-
edge, if the correct input provided by the student was
originated by a random or tentative step.
The goal of this study is to assess the relevance of
positive feedback in tutoring, and build a computa-
tional model of positive feedback that can be imple-
mented in ITSs. Even though some form of positive
feedback is present in many successful ITSs, the pre-
dominant type of feedback generated by those sys-
tems is negative feedback, as those systems are de-
signed to react to students mistakes. To date, there
is no systematic study of the role of positive feed-
back in ITSs in the literature. However, there is
an increasing amount of evidence that suggests that
positive feedback may be very important in enhanc-
ing students’ learning. In a detailed study in a con-
trolled environment and domain, the letter pattern
extrapolation task, Corrigan-Halpern (2006) found
</bodyText>
<page confidence="0.998321">
31
</page>
<bodyText confidence="0.985177447368421">
Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 31–36,
Columbus, June 2008. c�2008 Association for Computational Linguistics
that subjects given positive feedback performed bet-
ter in an assessment task than subjects receiving neg-
ative feedback. In another study on the same do-
main, Lu (2007) found that the ratio of the positive
over negative messages in her corpus of expert tu-
toring dialogues is about 4 to 1, and the ratio is even
higher in the messages presented by her successful
ITS modeled after an expert tutor, being about 10
to 1. In the dataset subject of this study, which is
on a completely different domain —Computer Sci-
ence data structures— such a high ratio of positive
over negative feedback messages still holds, in the
order of about 8 to 1. In a recent study, Barrow et al.
(2008) showed that a version of their SQL-Tutor en-
riched with positive feedback generation helped stu-
dents learn faster than another version of the same
system delivering negative feedback only.
What might be the educational value of positive
feedback in ITSs? First of all, positive feedback
may be an effective motivational technique (Lepper
et al., 1997). Positive feedback can also have cog-
nitive value. In a problem solving setting, the stu-
dent can make a tentative (maybe random) step to-
wards the correct solution. At this point, positive
feedback from the tutor may be important in help-
ing the student consolidate this step and learn from
it. Some researchers outlined the importance of self-
explanation in learning (Chi, 1996; Renkl, 2002).
Positive feedback has the potential to improve self-
explanation, in terms of quantity and effectiveness.
Another issue is how students perceive and accept
feedback (Weaver, 2006), and, in the case of auto-
mated tutoring systems, whether students read feed-
back messages at all (Heift, 2001). Positive feed-
back might also make students more willing to ac-
cept help and advice from the tutor.
</bodyText>
<sectionHeader confidence="0.816941" genericHeader="method">
2 A study of human tutoring
</sectionHeader>
<bodyText confidence="0.999931111111111">
The domain of this study is Computer Science data
structures, specifically linked lists, stacks, and bi-
nary search trees. A corpus of 54 one-on-one tutor-
ing sessions has been collected. Each individual stu-
dent participated in only one tutoring session, with
a tutor randomly assigned from a pool of two tutors.
One of the tutors is an experienced Computer Sci-
ence professor, with more than 30 years of teaching
experience. The other tutor is a senior undergrad-
</bodyText>
<table confidence="0.999583571428572">
Topic Tutor Avg Stdev t df P
List Novice .09 .22 -2.00 23 .057
Expert .18 .26 -3.85 29 &lt; .01
Both .14 .25 -4.24 53 &lt; .01
None .01 .15 -0.56 52 ns
iList .09 .17 -3.04 32 &lt; .01
Stack Novice .35 .25 -6.90 23 &lt; .01
Expert .27 .22 -6.15 23 &lt; .01
Both .31 .24 -9.20 47 &lt; .01
No .05 .17 -2.15 52 &lt; .05
Tree Novice .33 .26 -6.13 23 &lt; .01
Expert .29 .23 -6.84 29 &lt; .01
Both .30 .24 -9.23 53 &lt; .01
No .04 .16 -1.78 52 ns
</table>
<tableCaption confidence="0.99996">
Table 1: Learning gains and t-test statistics
</tableCaption>
<bodyText confidence="0.99927646875">
uate student in Computer Science, with only one
semester of previous tutoring experience. The tutor-
ing sessions have been videotaped and transcribed.
Student took a pre-test right before the tutoring ses-
sion, and a post-test immediately after. An addi-
tional group of 53 students (control group) took the
pre and post tests, but they did not participate in a tu-
toring session, and attended a lecture about a totally
unrelated topic instead.
Paired samples t-tests revealed that post-test
scores are significantly higher than pre-test scores
in the two tutored conditions for all the topics, ex-
cept for linked lists with the less experienced tu-
tor, where the difference is only marginally signifi-
cant. If the two tutored groups are aggregated, there
is significant difference for all the topics. Students
in the control group did not show significant learn-
ing for linked lists and binary search trees, and only
marginally significant learning for stacks. Means,
standard deviations, and t-test statistic values are re-
ported in Table 1.
There is no significant difference between the two
tutored conditions in terms of learning gain, ex-
pressed as the difference between post-score and
pre-score. This is revealed by ANOVA between
the two groups of students in the tutored condition.
For lists, F(1, 53) = 1.82, P = ns. For stacks,
F(1, 47) = 1.35, P = ns. For trees, F(1, 53) =
0.32, P = ns.
The learning gain of students that received tutor-
ing is significantly higher than the learning gain of
the students in the control group, for all the topics.
</bodyText>
<page confidence="0.996049">
32
</page>
<bodyText confidence="0.999155666666667">
This is showed by ANOVA between the group of
tutored students (with both tutors) and the control
group. For lists, F(1,106) = 11.0, P &lt; 0.01. For
stacks, F(1,100) = 41.4, P &lt; 0.01. For trees,
F(1,106) = 43.9, P &lt; 0.01. Means and standard
deviations are reported in Table 1.
</bodyText>
<sectionHeader confidence="0.979437" genericHeader="method">
3 Regression-based analysis
</sectionHeader>
<bodyText confidence="0.98713634920635">
The distribution of scores across sessions shows a lot
of variability (Table 1). In all the conditions, there
are sessions with very high learning gains, and ses-
sions with very low ones. This observation and the
previous results suggest a new direction for subse-
quent analysis: instead of looking at the character-
istics of a particular tutor, it is better to look at the
features that discriminate the most successful ses-
sions from the least successful ones. As advocated
in (Ohlsson et al., 2007), a sensible way to do that
is to adopt an approach based on multiple regression
of learning outcomes per tutoring session onto the
frequencies of the different features. The following
analysis has been done adopting a hierarchical, lin-
ear regression model.
Prior knowledge First of all, we want to factor out
the effect of prior knowledge, measured by the pre-
test score. A linear regression model reveals strong
effect of pre-test scores on learning gain (Table 2).
However, the R2 values show that there is a lot of
variance left to be explained, especially for lists and
stacks, although not so much for trees. Notice that
the Q weights are negative. That means students
with higher pre-test scores learn less then students
with lower pre-test scores. A possible explanation
is that students with more previous knowledge have
less learning opportunity than students with less pre-
vious knowledge.
Time on task Another variable that is recognized
as important by the educational research commu-
nity is time on task, and we can approximate it with
the length of the tutoring session. In the hierarchi-
cal regression model, session length follows pre-test
score. Surprisingly, session length has a significant
effect only on linked lists (Table 2).
Student activity Another hypothesis is that the
degree of student activity, in the sense of the amount
of student’s participation in the discussion, might
relate to learning (Lepper et al., 1997; Chi et al.,
2001). To test this hypothesis, the following defi-
nition of student activity has been adopted:
# of turns − # of short turns
session length
Turns are the sequences of uninterrupted speech of
the student. Short turns are the student turns shorter
than three words. The regression analysis revealed
no significant effect of this measure of students’ ac-
tivity on learning gain.
Feedback The dataset has been manually anno-
tated for episodes where positive or negative feed-
back is delivered. All the protocols have been
annotated by one coder, and some of them have
been double-coded by a second one (intercoder
agreement: kappa = 0.67). Examples of feedback
episodes are reported in Figure 1.
The number of positive feedback episodes and the
number of negative feedback episodes have been in-
troduced in the regression model (Table 2). The
model showed a significant effect of feedback for
linked lists and stacks, but no significant effect on
trees. Interestingly, the effect of positive feedback is
positive, but the effect of negative feedback is nega-
tive, as can be seen by the sign of the Q value.
</bodyText>
<sectionHeader confidence="0.975488" genericHeader="method">
4 A tutoring system for linked lists
</sectionHeader>
<bodyText confidence="0.975193894736842">
A new ITS in the domain of linked lists, iList, is
being developed (Figure 2).
The iList system is based on the constraint-based
design paradigm. Originally developed from a cog-
nitive theory of how people might learn from per-
formance errors (Ohlsson, 1996), constraint-based
modeling has grown into a methodology used to
build full-fledged ITSs, and an alternative to the
model tracing approach adopted by many ITSs. In a
constraint-based system, domain knowledge is mod-
eled with a set of constraints, logic units composed
of a relevance condition and a satisfaction condi-
tion. A constraint is irrelevant when the relevance
condition is not satisfied; it is satisfied when both
relevance and satisfaction conditions are satisfied; it
is violated when the relevance condition is satisfied
but the satisfaction condition is not. In the context
of tutoring, constraints are matched against student
student activity =
</bodyText>
<page confidence="0.842573">
33
</page>
<figure confidence="0.8066515">
T: do you see a problem?
T: I have found the node a@l, see here I found the node b@l, and
then I put g@l in after it.
Begin + T: here I have found the node a@l and now the link I have to
change is +...
S: ++ you have to link e@l &lt;over xxx.&gt; [&gt;]
End+ T: [&lt;] &lt;yeah&gt; I have to go back to this one.
S: *mmhm
T: so I *uh once I’m here, this key is here, I can’t go backwards.
Begin- S: &lt;so you&gt; [&gt;] &lt;you won’t get the same&gt; [//] would you get the
same point out of writing t@l close to c@l at the top?
T: oh, t@l equals c@l.
T: no because you would have a type mismatch.
End- T: t@l &lt;is a pointer&gt; [//] is an address, and this is contents.
</figure>
<figureCaption confidence="0.989973">
Figure 1: Positive and negative feedback (T = tutor, S = student)
</figureCaption>
<table confidence="0.999872103448276">
Topic Model Predictor 0 R2 P
List 1 Pre-test -.45 .18 &lt; .05
2 Pre-test -.40 .28 &lt; .05
Session length .35 &lt; .05
3 Pre-test -.35 .46 &lt; .05
Session length .33 .36 .05
+ feedback -.53 .05
-feedback &lt; .05
Stack 1 Pre-test -.53 .26 &lt; .01
2 Pre-test -.52 24 .ns
Session length .05 &lt; .01
3 Pre-test -.58 .33 &lt; .01
Session length .01 ns
+ feedback .61 &lt; .05
-feedback -.55 &lt; .05
Tree 1 Pre-test -.79 .61 &lt; .01
2 Pre-test -.78 .60 &lt; .01
Session length .03 ns
3 Pre-test -.77 .06 &lt; .01
Session length .04 .59 ns
+ feedback -.12 ns
-feedback ns
All 1 Pre-test -.52 .26 &lt; .01
2 Pre-test -.54 .29 &lt; .01
Session length .20 &lt; .05
3 Pre-test -.57 .30 &lt; .01
Session length .16 .32 .06
+ feedback -.23 &lt; .05
-feedback .05
</table>
<tableCaption confidence="0.979691">
Table 2: Linear regression
</tableCaption>
<figureCaption confidence="0.996389">
Figure 2: The iList system
</figureCaption>
<bodyText confidence="0.999747055555556">
solutions. Satisfied constraints correspond to knowl-
edge that students have acquired, whereas violated
constraints correspond to gaps or incorrect knowl-
edge. An important feature is that there is no need
for an explicit model of students’ mistakes, as op-
posed to buggy rules in model tracing. The possible
errors are implicitly specified as the possible ways
in which constraints can be violated.
The architecture of iList includes a problem
model, a constraint evaluator, a feedback manager,
and a graphical user interface. Student model and
pedagogical module, important components of a
complete ITS (Beck et al., 1996), have not been
implemented yet, and will be included in a future
version. Currently, the system provides only simple
negative feedback in response to students’ mistakes,
as customary in constraint-based ITSs.
A first version of the system has been deployed
</bodyText>
<page confidence="0.997631">
34
</page>
<bodyText confidence="0.999963216216216">
into a Computer Science class of a partner institu-
tion. 33 students took a pre-test before using the
system, and a post-test immediately afterwards. The
students also filled in a questionnaire about their
subjective impressions on the system. The interac-
tion of the students with the system was logged.
T-test on test scores revealed that students did
learn during the interaction with iList (Table 1). The
learning gain is somewhere in between the one ob-
served in the control condition and the one of the
tutored condition. ANOVA revealed no significant
difference between the control group and the iList
group, nor between the iList group and the tutored
group, whereas the difference between control and
tutored groups is significant.
A preliminary analysis of the questionnaires re-
vealed that students felt that iList helped them learn
linked lists to a moderate degree (on a 1 to 5 scale:
avg = 2.88, stdev = 1.18), but working with iList
was interesting to them (avg = 4.0, stdev = 1.27).
Students found the feedback provided by the sys-
tem somewhat repetitive (avg = 3.88, stdev = 1.18),
which is not surprising given the simple template-
based generation mechanism. Also, the feedback
was considered not very useful (avg = 2.31, 1.23),
but at least not too misleading (avg = 2.22, stdev
= 1.21). Interestingly, students declared that they
read the feedback provided by the system (avg =
4.25, stdev = 1.05), but the logs of the system re-
veal just the opposite. In fact, on average, students
read feedback messages for 3.56 seconds (stdev =
2.66 seconds), resulting in a reading speed of 532
words/minute (stdev = 224 words/minute). Accord-
ing to Carver’s taxonomy (Carver, 1990), such speed
indicates a quick skimming of the text, whereas
reading for learning typically has a lower speed, in
the order of 200 words/minute.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="method">
5 Future work
</sectionHeader>
<bodyText confidence="0.999984678571429">
The main goal of this research is to build a compu-
tational model of positive feedback that can be used
in ITSs. The study of empirical data and the sys-
tem design and development will proceed in paral-
lel, helping and informing each other as new results
are obtained.
The conditions and the modalities of positive
feedback delivery by tutors will be investigated from
the human tutoring dataset. To do so, more coding
categories will be defined, and the data will be anno-
tated with these categories. The results of the statis-
tical analysis over the first few coding categories will
be used to guide the definition of more categories,
that will be in turn used to annotate the data, and
so on. An example of potential coding category is
whether the student’s action that triggered the feed-
back was prompted by the tutor or volunteered by
the student. Another example is whether the feed-
back’s content was a repetition of what the student
just said or included additional explanation.
The first experiment with iList provided a com-
prehensive log of the students’ interaction with the
system. Additional analysis of this data will be im-
portant, especially because the nature of the interac-
tion of a student with a computer system differs from
the interaction with a human tutor. When working
with a computer system, most of the interaction hap-
pens through a graphical interface, instead of natu-
ral language dialogue. Also, the interaction with a
computer system is mostly student-driven, whereas
our human protocols show a clear predominance of
the tutor in the conversation. In the CS protocols,
on average, 94% of the words belong to the tutor,
and most of the tutors’ discourse is some form of di-
rect instruction. On the other hand, the interaction
with the system will mostly consist of actions that
students make to solve the problems that they will
be asked to solve, with few interventions from the
system. An interesting analysis that could be done
on the logs is the discovery of sequential patterns us-
ing data mining algorithms, such as MS-GSP (Liu,
2006). Such patterns could then be regressed against
learning outcomes, in order to assess their correla-
tion with learning.
After the relevant features are discovered, a com-
putational model of positive feedback will be built
and integrated into iList. The model will en-
code knowledge extracted with machine learning ap-
proaches, and such knowledge will inform a dis-
course planner, responsible of organizing and gen-
erating appropriate positive feedback. The choiche
of the specific machine learning and discourse plan-
ning methods will require extensive empirical inves-
tigation. Specifically, among the different machine
learning methods, some are able to provide some
sort of human-readable symbolic model, which can
</bodyText>
<page confidence="0.997455">
35
</page>
<bodyText confidence="0.999940692307692">
be inspected to gain some insights on how the model
works. Decision trees and association rules belong
to this category. Other methods provide a less read-
able, black-box type of models, but they may be very
useful and effective as well. Examples of such meth-
ods include Neural Networks and Markov Models.
The ultimate goal of this research is to get both an ef-
fective model and to gain insights on tutoring. Thus,
both classes of machine learning methods will be
tried, with the goal of finding a balance between
model effectiveness and model readability.
Finally, the system with enhanced feedback capa-
bilities will be deployed and evaluated.
</bodyText>
<sectionHeader confidence="0.998562" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99387025">
This work is supported by award N00014-07-1-0040
from the Office of Naval Research, and additionally
by awards ALT-0536968 and IIS-0133123 from the
National Science Foundation.
</bodyText>
<sectionHeader confidence="0.998041" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999774759493671">
Devon Barrow, Antonija Mitrovi´c, Stellan Ohlsson, and
Michael Grimley. 2008. Assessing the impact of pos-
itive feedback in constraint-based tutors. In ITS 2008,
The 9th International Conference on Intelligent Tutor-
ing Systems, Montreal, Canada.
Joseph Beck, Mia Stern, and Erik Haugsjaa. 1996.
Applications of AI in education. ACM crossroads.
http://www.acm.org/crossroads/xrds3-1/aied.html.
B. S. Bloom. 1984. The 2 sigma problem: The search for
methods of group instruction as effective as one-to-one
tutoring. Educational Researcher, 13:4–16.
Ronald P. Carver. 1990. Reading Rate: A Review of
Research and Theory. Academic Press, San Diego,
CA.
Michelene T.H. Chi, Stephanie A. Siler, Heisawn Jeong,
Takashi Yamauchi, and Robert G. Hausmann. 2001.
Learning from human tutoring. Cognitive Science,
25:471–533.
Michelene T.H. Chi. 1996. Constructing self-
explanations and scaffolded explanations in tutoring.
Applied Cognitive Psychology, 10:33–49.
Andrew Corrigan-Halpern. 2006. Feedback in Complex
Learning: Considering the Relationship Between Util-
ity and Processing Demands. Ph.D. thesis, University
of Illinois at Chicago.
Barbara Di Eugenio, Davide Fossati, Dan Yu, Susan
Haller, and Michael Glass. 2005. Aggregation im-
proves learning: Experiments in natural language gen-
eration for intelligent tutoring systems. In ACL05,
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics, Ann Arbor, MI.
Martha Evens and Joel Michael. 2006. One-on-one
Tutoring by Humans and Machines. Mahwah, NJ:
Lawrence Erlbaum Associates.
Trude Heift. 2001. Error-specific and individualized
feedback in a web-based language tutoring system: Do
they read it? ReCALL Journal, 13(2):129–142.
M. R. Lepper, M. Drake, and T. M. O’Donnell-Johnson.
1997. Scaffolding techniques of expert human tutors.
In K. Hogan and M. Pressley, editors, Scaffolding stu-
dent learning: Instructional approaches and issues,
pages 108–144. Brookline Books, New York.
Bing Liu. 2006. Web Data Mining. Springer, Berlin.
Xin Lu. 2007. Expert Tutoring and Natural Language
Feedback in Intelligent Tutoring Systems. Ph.D. thesis,
University of Illinois at Chicago.
Antonija Mitrovi´c, Pramuditha Suraweera, Brent Mar-
tin, and A. Weerasinghe. 2004. DB-suite: Ex-
periences with three intelligent, web-based database
tutors. Journal of Interactive Learning Research,
15(4):409–432.
Stellan Ohlsson, Barbara Di Eugenio, Bettina Chow, Da-
vide Fossati, Xin Lu, and Trina C. Kershaw. 2007.
Beyond the code-and-count analysis of tutoring dia-
logues. In AIED07, 13th International Conference on
Artificial Intelligence in Education.
Stellan Ohlsson. 1996. Learning from performance er-
rors. Psychological Review, 103:241–262.
N. K. Person, A. C. Graesser, L. Bautista, E. C. Mathews,
and the Tutoring Research Group. 2001. Evaluating
student learning gains in two versions of AutoTutor.
In J. D. Moore, C. L. Redfield, and W. L. Johnson,
editors, Artificial intelligence in education: AI-ED in
the wired and wireless future, pages 286–293. Amster-
dam: IOS Press.
Alexander Renkl. 2002. Learning from worked-out ex-
amples: Instructional explanations supplement self-
explanations. Learning and Instruction, 12:529–556.
Kurt Van Lehn, Collin Lynch, Kay Schulze, Joel A.
Shapiro, Robert H. Shelby, Linwood Taylor, Don J.
Treacy, Anders Weinstein, and Mary C. Wintersgill.
2005. The Andes physics tutoring system: Five years
of evaluations. In G. I. McCalla and C. K. Looi, ed-
itors, Artificial Intelligence in Education Conference.
Amsterdam: IOS Press.
Melanie R. Weaver. 2006. Do students value feed-
back? Student perceptions of tutors’ written re-
sponses. Assessment and Evaluation in Higher Edu-
cation, 31(3):379–394.
</reference>
<page confidence="0.99893">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.871090">
<title confidence="0.999988">The role of positive feedback in Intelligent Tutoring Systems</title>
<author confidence="0.999975">Davide Fossati</author>
<affiliation confidence="0.999935">Department of Computer Science University of Illinois at Chicago</affiliation>
<address confidence="0.98413">Chicago, IL, USA</address>
<email confidence="0.998292">dfossa1@uic.edu</email>
<abstract confidence="0.991193615384615">The focus of this study is positive feedback in one-on-one tutoring, its computational modeling, and its application to the design of more effective Intelligent Tutoring Systems. A data collection of tutoring sessions in the domain of basic Computer Science data structures has been carried out. A methodology based on multiple regression is proposed, and some preliminary results are presented. A prototype Intelligent Tutoring System on linked lists has been developed and deployed in a collegelevel Computer Science class.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Devon Barrow</author>
<author>Antonija Mitrovi´c</author>
<author>Stellan Ohlsson</author>
<author>Michael Grimley</author>
</authors>
<title>Assessing the impact of positive feedback in constraint-based tutors.</title>
<date>2008</date>
<booktitle>In ITS 2008, The 9th International Conference on Intelligent Tutoring Systems,</booktitle>
<location>Montreal, Canada.</location>
<marker>Barrow, Mitrovi´c, Ohlsson, Grimley, 2008</marker>
<rawString>Devon Barrow, Antonija Mitrovi´c, Stellan Ohlsson, and Michael Grimley. 2008. Assessing the impact of positive feedback in constraint-based tutors. In ITS 2008, The 9th International Conference on Intelligent Tutoring Systems, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Beck</author>
<author>Mia Stern</author>
<author>Erik Haugsjaa</author>
</authors>
<date>1996</date>
<journal>Applications of AI in education. ACM</journal>
<volume>crossroads.</volume>
<pages>3--1</pages>
<contexts>
<context position="1292" citStr="Beck et al., 1996" startWordPosition="190" endWordPosition="193">ollegelevel Computer Science class. 1 Introduction One-on-one tutoring has been shown to be a very effective form of instruction (Bloom, 1984). The research community is working on discovering the characteristics of tutoring. One of the goals is to understand the strategies tutors use, in order to design effective learning environments and tools to support learning. Among the tools, particular attention is given to Intelligent Tutoring Systems (ITSs), which are sophisticated software systems that can provide personalized instruction to students, in some respect similar to one-on-one tutoring (Beck et al., 1996). Many of these systems have been shown to be very effective (Evens and Michael, 2006; Van Lehn et al., 2005; Di Eugenio et al., 2005; Mitrovi´c et al., 2004; Person et al., 2001). In many experiments, ITSs induced learning gains higher than those measured in a classroom environment, but lower than those obtained with one-on-one interactions with human tutors. The belief of the research community is that knowing more about human tutoring would help improve the design of ITSs. In particular, the effective use of natural language might be a key element. In most of the studies mentioned above, sy</context>
<context position="14264" citStr="Beck et al., 1996" startWordPosition="2430" endWordPosition="2433">utions. Satisfied constraints correspond to knowledge that students have acquired, whereas violated constraints correspond to gaps or incorrect knowledge. An important feature is that there is no need for an explicit model of students’ mistakes, as opposed to buggy rules in model tracing. The possible errors are implicitly specified as the possible ways in which constraints can be violated. The architecture of iList includes a problem model, a constraint evaluator, a feedback manager, and a graphical user interface. Student model and pedagogical module, important components of a complete ITS (Beck et al., 1996), have not been implemented yet, and will be included in a future version. Currently, the system provides only simple negative feedback in response to students’ mistakes, as customary in constraint-based ITSs. A first version of the system has been deployed 34 into a Computer Science class of a partner institution. 33 students took a pre-test before using the system, and a post-test immediately afterwards. The students also filled in a questionnaire about their subjective impressions on the system. The interaction of the students with the system was logged. T-test on test scores revealed that </context>
</contexts>
<marker>Beck, Stern, Haugsjaa, 1996</marker>
<rawString>Joseph Beck, Mia Stern, and Erik Haugsjaa. 1996. Applications of AI in education. ACM crossroads. http://www.acm.org/crossroads/xrds3-1/aied.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B S Bloom</author>
</authors>
<title>The 2 sigma problem: The search for methods of group instruction as effective as one-to-one tutoring. Educational Researcher,</title>
<date>1984</date>
<pages>13--4</pages>
<contexts>
<context position="816" citStr="Bloom, 1984" startWordPosition="122" endWordPosition="123">udy is positive feedback in one-on-one tutoring, its computational modeling, and its application to the design of more effective Intelligent Tutoring Systems. A data collection of tutoring sessions in the domain of basic Computer Science data structures has been carried out. A methodology based on multiple regression is proposed, and some preliminary results are presented. A prototype Intelligent Tutoring System on linked lists has been developed and deployed in a collegelevel Computer Science class. 1 Introduction One-on-one tutoring has been shown to be a very effective form of instruction (Bloom, 1984). The research community is working on discovering the characteristics of tutoring. One of the goals is to understand the strategies tutors use, in order to design effective learning environments and tools to support learning. Among the tools, particular attention is given to Intelligent Tutoring Systems (ITSs), which are sophisticated software systems that can provide personalized instruction to students, in some respect similar to one-on-one tutoring (Beck et al., 1996). Many of these systems have been shown to be very effective (Evens and Michael, 2006; Van Lehn et al., 2005; Di Eugenio et </context>
</contexts>
<marker>Bloom, 1984</marker>
<rawString>B. S. Bloom. 1984. The 2 sigma problem: The search for methods of group instruction as effective as one-to-one tutoring. Educational Researcher, 13:4–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald P Carver</author>
</authors>
<title>Reading Rate: A Review of Research and Theory.</title>
<date>1990</date>
<publisher>Academic Press,</publisher>
<location>San Diego, CA.</location>
<contexts>
<context position="16207" citStr="Carver, 1990" startWordPosition="2755" endWordPosition="2756">g = 3.88, stdev = 1.18), which is not surprising given the simple templatebased generation mechanism. Also, the feedback was considered not very useful (avg = 2.31, 1.23), but at least not too misleading (avg = 2.22, stdev = 1.21). Interestingly, students declared that they read the feedback provided by the system (avg = 4.25, stdev = 1.05), but the logs of the system reveal just the opposite. In fact, on average, students read feedback messages for 3.56 seconds (stdev = 2.66 seconds), resulting in a reading speed of 532 words/minute (stdev = 224 words/minute). According to Carver’s taxonomy (Carver, 1990), such speed indicates a quick skimming of the text, whereas reading for learning typically has a lower speed, in the order of 200 words/minute. 5 Future work The main goal of this research is to build a computational model of positive feedback that can be used in ITSs. The study of empirical data and the system design and development will proceed in parallel, helping and informing each other as new results are obtained. The conditions and the modalities of positive feedback delivery by tutors will be investigated from the human tutoring dataset. To do so, more coding categories will be define</context>
</contexts>
<marker>Carver, 1990</marker>
<rawString>Ronald P. Carver. 1990. Reading Rate: A Review of Research and Theory. Academic Press, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelene T H Chi</author>
<author>Stephanie A Siler</author>
<author>Heisawn Jeong</author>
<author>Takashi Yamauchi</author>
<author>Robert G Hausmann</author>
</authors>
<title>Learning from human tutoring.</title>
<date>2001</date>
<journal>Cognitive Science,</journal>
<pages>25--471</pages>
<contexts>
<context position="10138" citStr="Chi et al., 2001" startWordPosition="1689" endWordPosition="1692"> opportunity than students with less previous knowledge. Time on task Another variable that is recognized as important by the educational research community is time on task, and we can approximate it with the length of the tutoring session. In the hierarchical regression model, session length follows pre-test score. Surprisingly, session length has a significant effect only on linked lists (Table 2). Student activity Another hypothesis is that the degree of student activity, in the sense of the amount of student’s participation in the discussion, might relate to learning (Lepper et al., 1997; Chi et al., 2001). To test this hypothesis, the following definition of student activity has been adopted: # of turns − # of short turns session length Turns are the sequences of uninterrupted speech of the student. Short turns are the student turns shorter than three words. The regression analysis revealed no significant effect of this measure of students’ activity on learning gain. Feedback The dataset has been manually annotated for episodes where positive or negative feedback is delivered. All the protocols have been annotated by one coder, and some of them have been double-coded by a second one (intercode</context>
</contexts>
<marker>Chi, Siler, Jeong, Yamauchi, Hausmann, 2001</marker>
<rawString>Michelene T.H. Chi, Stephanie A. Siler, Heisawn Jeong, Takashi Yamauchi, and Robert G. Hausmann. 2001. Learning from human tutoring. Cognitive Science, 25:471–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelene T H Chi</author>
</authors>
<title>Constructing selfexplanations and scaffolded explanations in tutoring.</title>
<date>1996</date>
<journal>Applied Cognitive Psychology,</journal>
<pages>10--33</pages>
<contexts>
<context position="4982" citStr="Chi, 1996" startWordPosition="802" endWordPosition="803">n another version of the same system delivering negative feedback only. What might be the educational value of positive feedback in ITSs? First of all, positive feedback may be an effective motivational technique (Lepper et al., 1997). Positive feedback can also have cognitive value. In a problem solving setting, the student can make a tentative (maybe random) step towards the correct solution. At this point, positive feedback from the tutor may be important in helping the student consolidate this step and learn from it. Some researchers outlined the importance of selfexplanation in learning (Chi, 1996; Renkl, 2002). Positive feedback has the potential to improve selfexplanation, in terms of quantity and effectiveness. Another issue is how students perceive and accept feedback (Weaver, 2006), and, in the case of automated tutoring systems, whether students read feedback messages at all (Heift, 2001). Positive feedback might also make students more willing to accept help and advice from the tutor. 2 A study of human tutoring The domain of this study is Computer Science data structures, specifically linked lists, stacks, and binary search trees. A corpus of 54 one-on-one tutoring sessions has</context>
</contexts>
<marker>Chi, 1996</marker>
<rawString>Michelene T.H. Chi. 1996. Constructing selfexplanations and scaffolded explanations in tutoring. Applied Cognitive Psychology, 10:33–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Corrigan-Halpern</author>
</authors>
<title>Feedback in Complex Learning: Considering the Relationship Between Utility and Processing Demands.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Illinois at Chicago.</institution>
<contexts>
<context position="3411" citStr="Corrigan-Halpern (2006)" startWordPosition="540" endWordPosition="541">back that can be implemented in ITSs. Even though some form of positive feedback is present in many successful ITSs, the predominant type of feedback generated by those systems is negative feedback, as those systems are designed to react to students mistakes. To date, there is no systematic study of the role of positive feedback in ITSs in the literature. However, there is an increasing amount of evidence that suggests that positive feedback may be very important in enhancing students’ learning. In a detailed study in a controlled environment and domain, the letter pattern extrapolation task, Corrigan-Halpern (2006) found 31 Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 31–36, Columbus, June 2008. c�2008 Association for Computational Linguistics that subjects given positive feedback performed better in an assessment task than subjects receiving negative feedback. In another study on the same domain, Lu (2007) found that the ratio of the positive over negative messages in her corpus of expert tutoring dialogues is about 4 to 1, and the ratio is even higher in the messages presented by her successful ITS modeled after an expert tutor, being about 10 to 1. In the dataset</context>
</contexts>
<marker>Corrigan-Halpern, 2006</marker>
<rawString>Andrew Corrigan-Halpern. 2006. Feedback in Complex Learning: Considering the Relationship Between Utility and Processing Demands. Ph.D. thesis, University of Illinois at Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Davide Fossati</author>
<author>Dan Yu</author>
<author>Susan Haller</author>
<author>Michael Glass</author>
</authors>
<title>Aggregation improves learning: Experiments in natural language generation for intelligent tutoring systems.</title>
<date>2005</date>
<booktitle>In ACL05, Proceedings of the 42nd Meeting of the Association for Computational Linguistics,</booktitle>
<location>Ann Arbor, MI.</location>
<marker>Di Eugenio, Fossati, Yu, Haller, Glass, 2005</marker>
<rawString>Barbara Di Eugenio, Davide Fossati, Dan Yu, Susan Haller, and Michael Glass. 2005. Aggregation improves learning: Experiments in natural language generation for intelligent tutoring systems. In ACL05, Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Evens</author>
<author>Joel Michael</author>
</authors>
<title>One-on-one Tutoring by Humans and Machines. Mahwah, NJ: Lawrence Erlbaum Associates.</title>
<date>2006</date>
<contexts>
<context position="1377" citStr="Evens and Michael, 2006" startWordPosition="205" endWordPosition="208">shown to be a very effective form of instruction (Bloom, 1984). The research community is working on discovering the characteristics of tutoring. One of the goals is to understand the strategies tutors use, in order to design effective learning environments and tools to support learning. Among the tools, particular attention is given to Intelligent Tutoring Systems (ITSs), which are sophisticated software systems that can provide personalized instruction to students, in some respect similar to one-on-one tutoring (Beck et al., 1996). Many of these systems have been shown to be very effective (Evens and Michael, 2006; Van Lehn et al., 2005; Di Eugenio et al., 2005; Mitrovi´c et al., 2004; Person et al., 2001). In many experiments, ITSs induced learning gains higher than those measured in a classroom environment, but lower than those obtained with one-on-one interactions with human tutors. The belief of the research community is that knowing more about human tutoring would help improve the design of ITSs. In particular, the effective use of natural language might be a key element. In most of the studies mentioned above, systems with more sophisticated language interfaces performed better than other experim</context>
</contexts>
<marker>Evens, Michael, 2006</marker>
<rawString>Martha Evens and Joel Michael. 2006. One-on-one Tutoring by Humans and Machines. Mahwah, NJ: Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trude Heift</author>
</authors>
<title>Error-specific and individualized feedback in a web-based language tutoring system: Do they read it?</title>
<date>2001</date>
<journal>ReCALL Journal,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="5285" citStr="Heift, 2001" startWordPosition="849" endWordPosition="850">lving setting, the student can make a tentative (maybe random) step towards the correct solution. At this point, positive feedback from the tutor may be important in helping the student consolidate this step and learn from it. Some researchers outlined the importance of selfexplanation in learning (Chi, 1996; Renkl, 2002). Positive feedback has the potential to improve selfexplanation, in terms of quantity and effectiveness. Another issue is how students perceive and accept feedback (Weaver, 2006), and, in the case of automated tutoring systems, whether students read feedback messages at all (Heift, 2001). Positive feedback might also make students more willing to accept help and advice from the tutor. 2 A study of human tutoring The domain of this study is Computer Science data structures, specifically linked lists, stacks, and binary search trees. A corpus of 54 one-on-one tutoring sessions has been collected. Each individual student participated in only one tutoring session, with a tutor randomly assigned from a pool of two tutors. One of the tutors is an experienced Computer Science professor, with more than 30 years of teaching experience. The other tutor is a senior undergradTopic Tutor </context>
</contexts>
<marker>Heift, 2001</marker>
<rawString>Trude Heift. 2001. Error-specific and individualized feedback in a web-based language tutoring system: Do they read it? ReCALL Journal, 13(2):129–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Lepper</author>
<author>M Drake</author>
<author>T M O’Donnell-Johnson</author>
</authors>
<title>Scaffolding techniques of expert human tutors. In</title>
<date>1997</date>
<booktitle>Scaffolding student learning: Instructional approaches and issues,</booktitle>
<pages>108--144</pages>
<editor>K. Hogan and M. Pressley, editors,</editor>
<publisher>Brookline Books,</publisher>
<location>New York.</location>
<marker>Lepper, Drake, O’Donnell-Johnson, 1997</marker>
<rawString>M. R. Lepper, M. Drake, and T. M. O’Donnell-Johnson. 1997. Scaffolding techniques of expert human tutors. In K. Hogan and M. Pressley, editors, Scaffolding student learning: Instructional approaches and issues, pages 108–144. Brookline Books, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Web Data Mining.</title>
<date>2006</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="18426" citStr="Liu, 2006" startWordPosition="3134" endWordPosition="3135">system is mostly student-driven, whereas our human protocols show a clear predominance of the tutor in the conversation. In the CS protocols, on average, 94% of the words belong to the tutor, and most of the tutors’ discourse is some form of direct instruction. On the other hand, the interaction with the system will mostly consist of actions that students make to solve the problems that they will be asked to solve, with few interventions from the system. An interesting analysis that could be done on the logs is the discovery of sequential patterns using data mining algorithms, such as MS-GSP (Liu, 2006). Such patterns could then be regressed against learning outcomes, in order to assess their correlation with learning. After the relevant features are discovered, a computational model of positive feedback will be built and integrated into iList. The model will encode knowledge extracted with machine learning approaches, and such knowledge will inform a discourse planner, responsible of organizing and generating appropriate positive feedback. The choiche of the specific machine learning and discourse planning methods will require extensive empirical investigation. Specifically, among the diffe</context>
</contexts>
<marker>Liu, 2006</marker>
<rawString>Bing Liu. 2006. Web Data Mining. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Lu</author>
</authors>
<title>Expert Tutoring and Natural Language Feedback in Intelligent Tutoring Systems.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Illinois at Chicago.</institution>
<contexts>
<context position="3747" citStr="Lu (2007)" startWordPosition="590" endWordPosition="591">rature. However, there is an increasing amount of evidence that suggests that positive feedback may be very important in enhancing students’ learning. In a detailed study in a controlled environment and domain, the letter pattern extrapolation task, Corrigan-Halpern (2006) found 31 Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 31–36, Columbus, June 2008. c�2008 Association for Computational Linguistics that subjects given positive feedback performed better in an assessment task than subjects receiving negative feedback. In another study on the same domain, Lu (2007) found that the ratio of the positive over negative messages in her corpus of expert tutoring dialogues is about 4 to 1, and the ratio is even higher in the messages presented by her successful ITS modeled after an expert tutor, being about 10 to 1. In the dataset subject of this study, which is on a completely different domain —Computer Science data structures— such a high ratio of positive over negative feedback messages still holds, in the order of about 8 to 1. In a recent study, Barrow et al. (2008) showed that a version of their SQL-Tutor enriched with positive feedback generation helped</context>
</contexts>
<marker>Lu, 2007</marker>
<rawString>Xin Lu. 2007. Expert Tutoring and Natural Language Feedback in Intelligent Tutoring Systems. Ph.D. thesis, University of Illinois at Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonija Mitrovi´c</author>
<author>Pramuditha Suraweera</author>
<author>Brent Martin</author>
<author>A Weerasinghe</author>
</authors>
<title>DB-suite: Experiences with three intelligent, web-based database tutors.</title>
<date>2004</date>
<journal>Journal of Interactive Learning Research,</journal>
<volume>15</volume>
<issue>4</issue>
<marker>Mitrovi´c, Suraweera, Martin, Weerasinghe, 2004</marker>
<rawString>Antonija Mitrovi´c, Pramuditha Suraweera, Brent Martin, and A. Weerasinghe. 2004. DB-suite: Experiences with three intelligent, web-based database tutors. Journal of Interactive Learning Research, 15(4):409–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stellan Ohlsson</author>
<author>Barbara Di Eugenio</author>
<author>Bettina Chow</author>
<author>Davide Fossati</author>
<author>Xin Lu</author>
<author>Trina C Kershaw</author>
</authors>
<title>Beyond the code-and-count analysis of tutoring dialogues.</title>
<date>2007</date>
<booktitle>In AIED07, 13th International Conference on Artificial Intelligence in Education.</booktitle>
<marker>Ohlsson, Di Eugenio, Chow, Fossati, Lu, Kershaw, 2007</marker>
<rawString>Stellan Ohlsson, Barbara Di Eugenio, Bettina Chow, Davide Fossati, Xin Lu, and Trina C. Kershaw. 2007. Beyond the code-and-count analysis of tutoring dialogues. In AIED07, 13th International Conference on Artificial Intelligence in Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stellan Ohlsson</author>
</authors>
<title>Learning from performance errors.</title>
<date>1996</date>
<journal>Psychological Review,</journal>
<pages>103--241</pages>
<contexts>
<context position="11524" citStr="Ohlsson, 1996" startWordPosition="1924" endWordPosition="1925">ve been introduced in the regression model (Table 2). The model showed a significant effect of feedback for linked lists and stacks, but no significant effect on trees. Interestingly, the effect of positive feedback is positive, but the effect of negative feedback is negative, as can be seen by the sign of the Q value. 4 A tutoring system for linked lists A new ITS in the domain of linked lists, iList, is being developed (Figure 2). The iList system is based on the constraint-based design paradigm. Originally developed from a cognitive theory of how people might learn from performance errors (Ohlsson, 1996), constraint-based modeling has grown into a methodology used to build full-fledged ITSs, and an alternative to the model tracing approach adopted by many ITSs. In a constraint-based system, domain knowledge is modeled with a set of constraints, logic units composed of a relevance condition and a satisfaction condition. A constraint is irrelevant when the relevance condition is not satisfied; it is satisfied when both relevance and satisfaction conditions are satisfied; it is violated when the relevance condition is satisfied but the satisfaction condition is not. In the context of tutoring, c</context>
</contexts>
<marker>Ohlsson, 1996</marker>
<rawString>Stellan Ohlsson. 1996. Learning from performance errors. Psychological Review, 103:241–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N K Person</author>
<author>A C Graesser</author>
<author>L Bautista</author>
<author>E C Mathews</author>
</authors>
<title>and the Tutoring Research Group.</title>
<date>2001</date>
<booktitle>Artificial intelligence in education: AI-ED in the wired and wireless future,</booktitle>
<pages>286--293</pages>
<editor>J. D. Moore, C. L. Redfield, and W. L. Johnson, editors,</editor>
<publisher>IOS Press.</publisher>
<location>Amsterdam:</location>
<contexts>
<context position="1471" citStr="Person et al., 2001" startWordPosition="223" endWordPosition="226"> on discovering the characteristics of tutoring. One of the goals is to understand the strategies tutors use, in order to design effective learning environments and tools to support learning. Among the tools, particular attention is given to Intelligent Tutoring Systems (ITSs), which are sophisticated software systems that can provide personalized instruction to students, in some respect similar to one-on-one tutoring (Beck et al., 1996). Many of these systems have been shown to be very effective (Evens and Michael, 2006; Van Lehn et al., 2005; Di Eugenio et al., 2005; Mitrovi´c et al., 2004; Person et al., 2001). In many experiments, ITSs induced learning gains higher than those measured in a classroom environment, but lower than those obtained with one-on-one interactions with human tutors. The belief of the research community is that knowing more about human tutoring would help improve the design of ITSs. In particular, the effective use of natural language might be a key element. In most of the studies mentioned above, systems with more sophisticated language interfaces performed better than other experimental conditions. An important form of student-tutor interaction is feedback. Negative feedbac</context>
</contexts>
<marker>Person, Graesser, Bautista, Mathews, 2001</marker>
<rawString>N. K. Person, A. C. Graesser, L. Bautista, E. C. Mathews, and the Tutoring Research Group. 2001. Evaluating student learning gains in two versions of AutoTutor. In J. D. Moore, C. L. Redfield, and W. L. Johnson, editors, Artificial intelligence in education: AI-ED in the wired and wireless future, pages 286–293. Amsterdam: IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Renkl</author>
</authors>
<title>Learning from worked-out examples: Instructional explanations supplement selfexplanations. Learning and Instruction,</title>
<date>2002</date>
<pages>12--529</pages>
<contexts>
<context position="4996" citStr="Renkl, 2002" startWordPosition="804" endWordPosition="805">ersion of the same system delivering negative feedback only. What might be the educational value of positive feedback in ITSs? First of all, positive feedback may be an effective motivational technique (Lepper et al., 1997). Positive feedback can also have cognitive value. In a problem solving setting, the student can make a tentative (maybe random) step towards the correct solution. At this point, positive feedback from the tutor may be important in helping the student consolidate this step and learn from it. Some researchers outlined the importance of selfexplanation in learning (Chi, 1996; Renkl, 2002). Positive feedback has the potential to improve selfexplanation, in terms of quantity and effectiveness. Another issue is how students perceive and accept feedback (Weaver, 2006), and, in the case of automated tutoring systems, whether students read feedback messages at all (Heift, 2001). Positive feedback might also make students more willing to accept help and advice from the tutor. 2 A study of human tutoring The domain of this study is Computer Science data structures, specifically linked lists, stacks, and binary search trees. A corpus of 54 one-on-one tutoring sessions has been collecte</context>
</contexts>
<marker>Renkl, 2002</marker>
<rawString>Alexander Renkl. 2002. Learning from worked-out examples: Instructional explanations supplement selfexplanations. Learning and Instruction, 12:529–556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Van Lehn</author>
<author>Collin Lynch</author>
<author>Kay Schulze</author>
<author>Joel A Shapiro</author>
<author>Robert H Shelby</author>
</authors>
<title>The Andes physics tutoring system: Five years of evaluations.</title>
<date>2005</date>
<booktitle>Artificial Intelligence in Education Conference.</booktitle>
<editor>Taylor, Don J. Treacy, Anders Weinstein, and Mary C. Wintersgill.</editor>
<publisher>IOS Press.</publisher>
<location>Linwood</location>
<marker>Van Lehn, Lynch, Schulze, Shapiro, Shelby, 2005</marker>
<rawString>Kurt Van Lehn, Collin Lynch, Kay Schulze, Joel A. Shapiro, Robert H. Shelby, Linwood Taylor, Don J. Treacy, Anders Weinstein, and Mary C. Wintersgill. 2005. The Andes physics tutoring system: Five years of evaluations. In G. I. McCalla and C. K. Looi, editors, Artificial Intelligence in Education Conference. Amsterdam: IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie R Weaver</author>
</authors>
<title>Do students value feedback? Student perceptions of tutors’ written responses. Assessment and Evaluation</title>
<date>2006</date>
<booktitle>in Higher Education,</booktitle>
<pages>31--3</pages>
<contexts>
<context position="5175" citStr="Weaver, 2006" startWordPosition="830" endWordPosition="831"> motivational technique (Lepper et al., 1997). Positive feedback can also have cognitive value. In a problem solving setting, the student can make a tentative (maybe random) step towards the correct solution. At this point, positive feedback from the tutor may be important in helping the student consolidate this step and learn from it. Some researchers outlined the importance of selfexplanation in learning (Chi, 1996; Renkl, 2002). Positive feedback has the potential to improve selfexplanation, in terms of quantity and effectiveness. Another issue is how students perceive and accept feedback (Weaver, 2006), and, in the case of automated tutoring systems, whether students read feedback messages at all (Heift, 2001). Positive feedback might also make students more willing to accept help and advice from the tutor. 2 A study of human tutoring The domain of this study is Computer Science data structures, specifically linked lists, stacks, and binary search trees. A corpus of 54 one-on-one tutoring sessions has been collected. Each individual student participated in only one tutoring session, with a tutor randomly assigned from a pool of two tutors. One of the tutors is an experienced Computer Scienc</context>
</contexts>
<marker>Weaver, 2006</marker>
<rawString>Melanie R. Weaver. 2006. Do students value feedback? Student perceptions of tutors’ written responses. Assessment and Evaluation in Higher Education, 31(3):379–394.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>