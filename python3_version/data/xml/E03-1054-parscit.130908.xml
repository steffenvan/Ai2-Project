<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.985906">
Information Structure in Topological Dependency Grammar
</title>
<author confidence="0.955714">
Geert-Jan Kruijff
</author>
<affiliation confidence="0.825929">
Computational Linguistics
Saarland University
Saarbrticken, Germany
</affiliation>
<email confidence="0.97715">
gj@coli.uni-sb.de
</email>
<note confidence="0.456423666666667">
Denys Duchier
INRIA Lorraine
Nancy, France
</note>
<email confidence="0.9092">
denys.duchier@loria.fr
</email>
<sectionHeader confidence="0.993156" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997342214285714">
Topological Dependency Grammar
(TDG) is a lexicalized dependency
grammar formalism, able to model lan-
guages with a relatively free word order.
In such languages, word order variation
often has an important function: the
realization of information structure.
The paper discusses how to integrate
information structure into TDG, and
presents a constraint-based approach
to modelling information structure and
the various means to realize it, focusing
on (possibly simultaneous use of) word
order and tune.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999801339285714">
In this paper, we present an extension to Topolog-
ical Dependency Grammar (Duchier and Debus-
mann, 2001) enabling us to analyse e.g. word or-
der variation and tune as means to indicate what
is the topic and what is the focus of an expression
— i.e. its information structure (cf. §2, §4). Us-
ing a constraint-based approach, we can analyse
the surface form of an expression in terms of the
information structure that it realizes.
The information structure of an expression is a
core part of its meaning: it indicates how the ex-
pression relates to the discourse context. Informa-
tion structure thus constitutes a crucial factor in
determining an expression&apos;s contextual appropri-
ateness or interpretability. Particularly in applica-
tions that involve human-computer interaction, in-
formation structure has thus been found to have a
great impact on the understandability of computer-
generated language, e.g. question/answering dia-
logues (Prevost and Steedman, 1994; Hoffman,
1995; Kruijff-Korbayova et al., 2003) or genera-
tion (Kruijff-Korbayova et al., 2002).
In this paper we concentrate on information
structure and the syntax/semantics-interface: We
want to be able to reconstruct an expression&apos;s in-
formation structure at the level of meaning, given
the expression&apos;s surface form.
To realize information structure a language may
employ a variety of means, not only word order or
tune but also morphology or marked syntactic con-
structions. Collectively we call these means struc-
tural indications of informativity, after (Vallduvf
and Engdahl, 1996; Kruijff, 2001).
As §2 illustrates, languages are not restricted to
using just a single means. Within a single expres-
sion several types of indications can normally be
used simultaneously. The indications may con-
strain the expression&apos;s well-formedness, and it is
through their interaction that the indications help
realize information structure.
It is precisely this interaction that presents
a problem for existing accounts of information
structure and its realization. Although accounts
normally acknowledge that there are various types
of structural indications, most of them focus solely
on modelling the use of a single type of struc-
tural indication. For example, (Steedman, 2000)
focuses on tune, (Hoffman, 1995) or (Hajieova et
al., 1995) focus on word order.
Such focus would be unproblematic if it were
clear how these accounts could be extended to
cover multiple, interacting types of structural in-
dications. However, even for (Steedman, 2000;
Hoffman, 1995), which are the formally most de-
tailed, this is by no means obvious. CCG&apos;s un-
derlying principles (notably, the Principle of Ad-
</bodyText>
<page confidence="0.998053">
219
</page>
<bodyText confidence="0.99994108">
jacency) forces Hoffman to introduce separate
derivations for establishing an expression&apos;s syn-
tactic structure (incl. word order) and its informa-
tion structure. This detaches information structure
from word order as an indication of the former, a
problem that arguably gets aggravated if one were
to try to incorporate Steedman&apos;s model of tune.
The contribution we make here is the presenta-
tion of a framework that (i) can describe the use of
any number of structural indications in realizing
information structure in a perspicuous way, and
that (ii) is amenable to a formalization in the style
of TDG to extend the latter&apos;s efficient constraint-
based parser. A proviso: Given the limited space,
we do not deal with contrast in this paper.
Overview: §2 presents data motivating our point
that languages can use several types of structural
indications of informativity simultaneously, and
the effect this may have on grammaticality. §3
introduces the necessary basic concepts of TDG.
In §4 we discuss how to extend TDG to deal with
information structure: We outline the underlying
linguistic model, and specify the formal details of
the extension. The resulting model we then apply
to the data of §2. We close with conclusions.
</bodyText>
<sectionHeader confidence="0.950107" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.998272956521739">
When a speaker wants to communicate some
meaning to a hearer, she does that against a back-
ground of discourse referents that have already
been activated in the context, and which are (pre-
sumably) shared between speaker and hearer. The
meaning a speaker communicates relates to these
already established referents, and presents more
(&amp;quot;new&amp;quot;) information about these referents. The
former part of the meaning we call the topic,
the latter the focus. An expression&apos;s information
structure is the division of its meaning into a topic
and a focus (Sgall et al., 1986; Vallduvi, 1990).
Languages may realize information structure in
various ways. For example, in a language with a
relatively free word order, variations in lineariza-
tion are prototypically used to indicate different
information structure (Sgall et al., 1986; Hoffman,
1995; Kruijff, 2001). This explains why different
variations, though equally grammatical, are usu-
ally not equally interchangeable in a given context.
To illustrate the idea of context-dependence,
consider the Czech example in (1) and its gram-
matical variations in (2).1
</bodyText>
<equation confidence="0.362441333333333">
(1) [Snedl]F [Honza]F [koblihu]F.
eat-PAST John donut
&amp;quot;John ate a donut.&amp;quot;
</equation>
<listItem confidence="0.7214885">
(2) a. [Honza] T snail [koblihu] F
b. [Koblihu] T snedl [Honza] F.
c. [Honza koblihu] T [snedl] F
(1) illustrates an &amp;quot;all-focus&amp;quot; sentence — the en-
</listItem>
<bodyText confidence="0.998646642857143">
tire meaning is new. The examples in (2) pre-
suppose different items to be present (&amp;quot;salient&amp;quot;)
in the already established dialogue. For example,
if the speaker utters (2b) in a context where there
is no donut, the hearer would most likely reply
with &amp;quot;What donut?!&amp;quot;, whereas (2a) assumes that
&amp;quot;Honza&amp;quot; is a person the hearer can identify.
Not every language has a relatively free word
order, though. English has a fixed word order
where it concerns complements, and therefore
usually resorts to using tune to realize information
structure. The examples in (3) illustrate several
possible information structures, given the place-
ment of the pitch accent.2
</bodyText>
<listItem confidence="0.927637666666667">
(3) a. [John] F gave [Mary] F [&amp;quot;Moby DICK&amp;quot;] F.
b. [JOHN]F [gaVe] T [Mani] T [&amp;quot;Moby Dickl T.
C. [John] [gaV e]T [MARY] p [&amp;quot;Moby Diekl T •
</listItem>
<bodyText confidence="0.999110272727273">
Particularly in languages that have a degree of
word order freedom inbetween English and Slavic
languages like Czech, we can find examples of a
strong interaction between word order and tune.
For example, consider the Dutch examples in (4)
and (5). (4) illustrates the all-focus case. (5a—c)
show well-formed variations interpretable on dif-
ferent contexts. (5d) however, is ill-formed. By
placing &amp;quot;Moby Dick&amp;quot; sentence initial and putting
a non-contrastive stress on it, it gets interpreted as
the subject of the (active) verb &amp;quot;lezen&amp;quot;.
</bodyText>
<figure confidence="0.924439818181818">
(4) Jan las &amp;quot;Moby DICK&amp;quot;
John read-PAST &amp;quot;Moby Dick&amp;quot;
John read &amp;quot;Moby DICK&amp;quot;
(5) a. (Who read &amp;quot;Moby Dick&amp;quot;?)
[JAN] F [las] [&amp;quot;Moby Dickl T •
&amp;quot;JOHN read &amp;quot;Moby Dick&amp;quot;.&amp;quot;
b. (Who read &amp;quot;Moby Dick&amp;quot;?)
[&amp;quot;Moby Dick&amp;quot;&apos; T [las] T [JAN] F.
&amp;quot;JOHN read &amp;quot;Moby Dick&amp;quot;.&amp;quot;
&apos;Subscript T indicates that the item belongs to the topic,
F that it belongs to the focus.
</figure>
<footnote confidence="0.908465">
2SMALL CAPS indicate pitch accent.
</footnote>
<page confidence="0.985204">
220
</page>
<reference confidence="0.759676666666667">
c. (What did John read?)
[Jan] T [las]T [&amp;quot;MoBv DICK1F.
&amp;quot;John read &amp;quot;Mos Y DICK&amp;quot;.&amp;quot;
d. (What did John read?)
[&amp;quot;MOB DIcK1F [las] T [Jan]T.
*&amp;quot;MOBY DICK&amp;quot; read John.
</reference>
<bodyText confidence="0.9997658">
We would like to argue that similar interactions
between word order and tune can also be observed
in English. English has more freedom in ordering
adjuncts, as (7) illustrates (Sgall et al., 1986). (6)
presents the all-focus case.
</bodyText>
<listItem confidence="0.921972666666667">
(6) John flew from London to Paris on Tuesday.
(7) a. [On Tuesday] T, [John] F [flew]F [from
London] F [TO PARIS]F•
b. [On Tuesday] 2,, [John] T [flew] 2, [to Paris]T[FROM LONDON]F.
c. [From London] T, [John] T [flew] T [to Paris] T
[ON TUESDAY]F.
</listItem>
<bodyText confidence="0.999897952380952">
The boundaries between topic and focus in (7)
arise from non-canonical ordering of adjuncts, and
the tendency of SVO languages like English to
place focus items towards the end of the sen-
tence. For example, in (7b) the to-PP and from-
PP are inverted — the from-PP is part of the focus,
whereas the non-canonical ordering of the to-PP
and the from-PP makes us place the topic/focus-
boundary between these two PPs. The same idea
applies to (7a): Only the on-PP is ordered non-
canonically with respect to the rest of the comple-
ments and adjuncts, hence we put the topic/focus-
boundary between the on-PP and the subject. We
elaborate this in §4.
English is relatively free in placing pitch accent
— given a canonical order, (3). When varying the
word order as in (7), we find that the interaction
between word order and tune leads to strong pref-
erences in interpretation.3 The examples in (8) il-
lustrate this effect. We interpret elements from the
question as topical (in the answer).
</bodyText>
<listItem confidence="0.590178714285714">
(8) On Tuesday, what flight did John take?
a. [On Tuesday] T, [John] T [flew] T [from
London]F [T(:) PARNF.
b. ?#[On Tuesday] [John] T [flew] T [to Paris]F
[FROM LONDON]F.
c. #[On Tuesday] T, [John] T [flew] T [TO PARIS]F
[from London] T.
</listItem>
<bodyText confidence="0.99893615">
The example in (8c) leads to a dispreferred (#)
interpretation: In English, constituents coming af-
ter the pitch accent (here, TO PARIS) are inter-
preted by default as given (resulting in rfrom
London&amp;quot;] ). Though the word order is well-
formed, as is the placement of the pitch accent on
the to-PP, the resulting surface form is not appro-
priate in the given context. (8b) is #&apos;d because its
non-canonical ordering of the PPs would suggest
a topic/focus-boundary between the to-PP and the
FROM-PP, suggesting the to-PP to be given.4
To recapitulate, variation in the placement of
(non-contrastive) pitch accent or in word order
helps indicate the boundary between topic and fo-
cus. Furthermore, when tune and word order are
both used to realize information structure, they
constrain one another. In §4 we present a for-
malization in TDG that captures these phenomena.
Before that, we use the next section to present the
necessary basics of TDG.
</bodyText>
<sectionHeader confidence="0.983182" genericHeader="method">
3 Topological Dependency Grammar
</sectionHeader>
<bodyText confidence="0.99073335">
Duchier and Debusmann (2001) introduced TDG,
a lexicalized formalism for dependency grammar,
to tackle linearization phenomena in freer word-
order languages. These are explained as emerg-
ing from the interaction of a non-ordered tree of
syntactic dependencies, where edges are labeled
by grammatical functions, with an ordered and a
projective tree of topological dependencies, where
edges are labeled by topological fields. Both trees
are simultaneously constrained by a lexical assign-
ment that e.g. restricts the licensed edges. Further-
more TDG stipulates that they must be related by
an emancipation mechanism whereby a word is al-
lowed to climb up and land in the topological do-
main of a syntactic ancestor.
For example, the German sentence
(9) Maria tiben-edet ihn em n Buch zu lesen
Mary convinces him a book to read
receives the following analysis, where (10) is the
syntax tree and (11) the topological tree:
</bodyText>
<footnote confidence="0.916031">
3We see these preferences as a weaker version of the effect 4Native speakers prefer (8a) over (8b), yet do not rule out
such interaction has on well-formedness observed for Dutch. (8b) as strongly as (8c); hence the ? with (8b).
</footnote>
<page confidence="0.996636">
221
</page>
<bodyText confidence="0.997030666666667">
model formalizes, after which we present the for-
malization itself in §4.2. We apply the model to
various examples from §2 in §4.3.
</bodyText>
<equation confidence="0.947683">
Vin f
(10) 0,0\
Maria iiberredet ihn em n nth zu lesen
D--&apos; v12
n (I ii V
Maria Uberredet ilin em n Buch zu lesen
</equation>
<bodyText confidence="0.997958">
Notice that, while &amp;quot;Buch&amp;quot; is the syntactic object of
&amp;quot;lesen&amp;quot; it lands in the Mittelfeld (mf) of the main
verb &amp;quot;iiberredet&amp;quot;.
On-going work on the development of a syn-
tax/semantics interface for TDG extends the same
methodology to the recovery of deep semantic de-
pendencies. An additional structure is introduced:
the semantic argument structure. This is a directed
acyclic graph with edges labeled by semantic rela-
tions. For sentence (9) above, the corresponding
argument structure is given in (12):
</bodyText>
<equation confidence="0.9223945">
Er
(12)
0-‘)
Maria iiberredet ihn em n Buch zu lesen
</equation>
<bodyText confidence="0.999869833333333">
Notice that &amp;quot;ihn&amp;quot; is now both the patient of
&amp;quot;iiberredet&amp;quot; and the actor of &amp;quot;lesen&amp;quot;. Again, TDG
postulates an emancipation mechanism relating
the argument structure to the syntax tree, that e.g.
allows a (subject) semantic dependent to climb up
and be realized as a raised syntactic argument of a
dominating control or raising verb.
In the present paper, we take advantage of
this extension to TDG, and avail ourselves of
the argument structure. For more details on
how TDG can model word order, we refer to
Duchier and Debusmann (2001).
</bodyText>
<sectionHeader confidence="0.996272" genericHeader="method">
4 Modelling information structure
realization
</sectionHeader>
<bodyText confidence="0.99833225">
The goal of the current section is to present a
TDG-based model of how word order and intona-
tion may together help realize information struc-
ture. In §4.1 we present the linguistic theory our
</bodyText>
<subsectionHeader confidence="0.981506">
4.1 Linguistic background
</subsectionHeader>
<bodyText confidence="0.999788690476191">
Some theories define topic and focus as atomic
terms, often corresponding to a concrete division
of an expression&apos;s surface form, e.g. (Vallduvi,
1990). Here, we take a more recursive perspec-
tive, like (Sgall et al., 1986; Hajieova et al., 1998;
Steedman, 2000): topic and focus are established
(recursively) on the basis of the informativity of
individual (discourse) referents that make up an
expression&apos;s meaning. If the speaker presents a
referent as activated in the preceding context (as-
sociation/direct introduction), then we call that
referent contextually bound (CB). If a referent has
not been activated yet, we call it contextually non-
bound (NB).
Decoupling the definition of topic and focus
from surface realization and defining them recur-
sively enables us to deal in a perspicuous way with
discontinuous topics/foci and embedding.
There are numerous sources providing indica-
tions of whether a referent is CB or NB: con-
textual activation, lexical semantics, variations in
word order, tune, morphology, etc. The challenge
is to meaningfully combine them. In this paper, we
consider a simple approach based on the classical
4-valued Boolean lattice: T is the top of the lattice
and indicates the absence of information, CB and
NB are the two boolean options, and I represents
a contradiction. Such an approach is well-suited
for integration into TDG&apos;S constraint-based frame-
work. Below we describe several principles that
derive indications of CB/NB-ness in the form of
values in the Boolean lattice. Their conclusions
are then combined by Fl (lattice meet) for contri-
bution to the expression&apos;s information structure.
Contextual activation. If a discourse referent is
activated in the preceding context either through
association or direct introduction, then it is as-
signed CB, else T.
Tune. Tune is another source of partial informa-
tion about CB/NB-ness. We assume that a pitch
accent indicates NB. Following (Steedman, 2000),
we assume that CB is assigned to the siblings (or
</bodyText>
<page confidence="0.987494">
222
</page>
<bodyText confidence="0.993140941176471">
dependents, if the verb has a pitch accent) right-
ward of the pitch accent. Otherwise, we assign T.
Lexical semantics. Lexical semantics may also
provide indications about CB/NB-ness. For ex-
ample, in the simplified setting of this paper, we
assume that the English indefinite article &amp;quot;a&amp;quot; pro-
totypically indicates NB, while the definite article
&amp;quot;the&amp;quot; indicates CB. In other cases, lexical seman-
tics simply assigns T.
Systemic ordering. Like Sgall et al. (1986), we
assume that there is a canonical ordering over de-
pendents such as ACTOR, PATIENT, LOCATION
etc, and that variation on this order indicates dif-
ferences in informativity (cf. the examples in (7)).
We call this order the systemic ordering (SO), and
allow each verb to have its own lexicalized SO.5
The SO for many English verbal dependents is:
</bodyText>
<listItem confidence="0.9379755">
(13) ACTOR &lt; ADDRESSEE &lt; PATIENT &lt;
FROMWHERE&lt; WHERETO &lt; TIME WHEN
</listItem>
<bodyText confidence="0.743187583333333">
SO relates to CB/NB-ness as follows. For SVO
and OV languages, we assume that the trailing se-
quence of verbal dependents that are realized in
canonical order at the clause level are assigned T,
while all preceding ones are considered to be CB.
Thus we are mostly interested in the rightmost vi-
olation of SO among the dependents of a given
verbal head. For example, given the SO of (13),
we can explain why &amp;quot;Tuesday&amp;quot; is CB in (14): Its
actual linearization is non-canonical wrt. the SO,
while all following dependents of the clause are
linearized in canonical order.
</bodyText>
<listItem confidence="0.750676">
(14) On Tuesdaycg, JohnT flew T from
LondonT TO PARIST.
</listItem>
<figureCaption confidence="0.7079286">
Projection. It is possible that no source of infor-
mation determines the NB/CB-ness of a particular
word. In this case, the principle of projection en-
ables us to extend an assignment starting from a
referent whose NB/CB-ness is known:
For SVO and OV languages, if a referent 6 is
NB, then referents left of 6 can also be consid-
ered NB (projection) if they are (incl. 6) ordered
canonically wrt. SO and are not already deter-
mined to be CB. CB-ness can project leftwards
5(Sgall et al., 1986) posit SO as a universal order, holding
equally across all verbs. However, that seems to contradict
the results in (Kurz et al., 2000).
over referents ordered either canonically or non-
canonically wrt. SO.
</figureCaption>
<bodyText confidence="0.933636">
For example, consider (15).
</bodyText>
<listItem confidence="0.957547">
(15) a. John gave Mary a book TODAYNB•
b. John gave Mary a bookNg TODAYNB•
c. John gave MaryNB a bookNg TODAYNB•
</listItem>
<bodyText confidence="0.954969538461539">
All dependents in (15) are ordered canonically
wrt. SO. Hence, when the pitch accent on &amp;quot;to-
day&amp;quot; specifies it as NB, we can project NB-ness
leftwards over all the preceding referents (result-
ing in an all-focus sentence). If we would have
&amp;quot;the book&amp;quot; instead, we could not project NB-ness.
Instead, we could project CB leftwards from &amp;quot;the
book&amp;quot;.
In the next sections we formalize and illustrate
the principles on examples involving indications
following from all of the factors mentioned above:
Word order, tune, lexical semantics, projection,
and contextual activation.
</bodyText>
<subsectionHeader confidence="0.972058">
4.2 Formalization in TDG
</subsectionHeader>
<bodyText confidence="0.998345">
In this section, we outline how the model theo-
retic approach of TDG (Duchier, 2001) can be ex-
tended in the same spirit with a formalization of
systemic order violations, thus setting the stage for
a contraint-based account of information structure.
We write E for the set of lexical entries, i.e.
the lexicon, and LTH for the set of semantic de-
pendency relations. Each lexical entry stipulates a
systemic ordering on LTH, which we model using
the function:
</bodyText>
<equation confidence="0.776887">
SO : GTH X LTH
</equation>
<bodyText confidence="0.999793">
Given a lexical assignment a : V —&gt; g of lexical
entries to the words V of a sentence, we overload
the function as follows to obtain the systemic order
lexically assigned to each word w E V:
</bodyText>
<equation confidence="0.936118">
so(w) = so(a(w))
</equation>
<bodyText confidence="0.9919745">
The semantic argument structure (V, Ern) is a
DAG with edges E10 c17&gt;&lt;V&gt;&lt;L. Each se-
mantic role 6 can also be interpreted as a function
from words to sets of words:
</bodyText>
<equation confidence="0.589301">
0(0 = {WI E V (W, W1,19) E End
</equation>
<page confidence="0.994915">
223
</page>
<bodyText confidence="0.998272142857143">
In this paper, we assume that each 0(w) contains
at most one element and that for any 0/ 0 02 E
LTH, 0/ (W) n 02(w) = 0, i.e. that the semantic
arguments of one head are all distinct.
Given so (w) we can define the systemic order
so:args(w) c VxV induced on w&apos;s actual se-
mantic dependents:
</bodyText>
<equation confidence="0.999794">
so:args(w) =
U{0/ (w) x 02(w) I (01,02) G SO(W)}
</equation>
<bodyText confidence="0.97655375">
The topological structure, which is part of a TDG
analysis, provides us with a total order on V.
We write LTH(w) = U{0(w) 0 E LTH} for the
set of w&apos;s semantic dependents and ILTH (w) for
the restriction of to LTH (10).
The set nso:args(w) of non-systematically or-
dered pairs of w&apos;s semantic dependents can be ob-
tained by the following set difference:
</bodyText>
<equation confidence="0.9792475">
,()
nso:args(w) = - ILTHw \ so:args(w)
</equation>
<bodyText confidence="0.998108571428571">
we wish to identify the set of all semantic depen-
dents of w that either violate systemic order or are
left of one that does. Given an ordering R, we
write dom(R) for its underlying domain, 71(R)
resp. 72(R) for its 1st resp. 2nd projections, and
eqleft(w)R for the set of elements left of or equal
to w in R:
</bodyText>
<equation confidence="0.99643975">
7ri (R) = {x (x, y) G
7r2(R) = {Y (X/ Y) E
dom(R) = (R) U (R)
eqleft(w)R = {w} u {w&apos; (w&apos;, w) c
</equation>
<bodyText confidence="0.999007">
Thus the set of dependents to be assigned CB ac-
cording to the systemic ordering principle is:
</bodyText>
<equation confidence="0.949327">
t111(w) n
ufeqleft(w&apos;) c 7r/ (nso:args(w))}
</equation>
<bodyText confidence="0.9996204">
Other principles, such as tune and projection, can
be similarly addressed: tune assigns CB to right
siblings of a pitch accent, while projection non-
deterministically extends an assignment leftward
within so-constrained limits
</bodyText>
<subsectionHeader confidence="0.999209">
4.3 Case studies
</subsectionHeader>
<bodyText confidence="0.998500555555556">
In this section we apply our formalization to var-
ious examples, both illustrating how the theory of
§4.1 works out and how it relates to other frame-
works.
We start with a few simple examples. Through-
out this section we present the inferences from
the principles in a tabular fashion, with the T/F
column showing the inferred CB/NB-ness of each
referent.
</bodyText>
<figure confidence="0.450551470588235">
(16) (What did you do?)
I gave Kathy a BOOK.
For (16) we have the following inferences.
Word Ctxt SO Tune Det Proj T/F
gave CB NB CB
NB
Kathy NB NB
book NB NB NB
(17) presents a variation on (16), with a topicalized
PATIENT. The inferences are given in the table.
(17) (What did you do with the book?)
The book, I gave to KATHY.
Now, consider again (8a,b), repeated as (18a,b).
(18) (On Tuesday, what flight did John take?)
a. On Tuesday, John flew from London to PARIS
b. # On Tuesday, John flew to PARIS from Lon-
don.
</figure>
<bodyText confidence="0.740287">
For (18a) we get the following inferences from
the different principles, and the context.
</bodyText>
<table confidence="0.996278">
Word Ctxt SO Tune Det Proj T/F
Tuesday CB CB T T T CB
John CB T T T T CB
flew CB T T T T CB
London T T T T NB NB
Paris T T NB T T NB
</table>
<bodyText confidence="0.989014666666667">
(18a) is similar to (16): The topicalization of
&amp;quot;on Tuesday&amp;quot; makes it CB, whereas the pitch ac-
cent on &amp;quot;Paris&amp;quot; indicates it is NB. In the end, pro-
jection makes &amp;quot;from London&amp;quot; CB.
For (18b) we get a different analysis, correctly
inferring it is dispreferred.
</bodyText>
<figure confidence="0.87434875">
CB CB NB CB NB CB
CB CB
NB
NB
book
gave
Kathy
Word Ctxt SO Tune Det Proj T/F
</figure>
<page confidence="0.903225">
224
</page>
<table confidence="0.998127">
Word Ctxt SO Tune Det Proj T/F
Tuesday CB CB T T T CB
John CB T T T T CB
flew CB T T T T CB
Paris T CB NB T T 1
London T T CB T T CB
</table>
<bodyText confidence="0.994906222222222">
Due to the pitch accent on &amp;quot;Paris&amp;quot;, we infer that
&amp;quot;Paris&amp;quot; is NB and that &amp;quot;from London&amp;quot; (as its right-
adjacent sister) is CB. However from SO we also
infer that &amp;quot;Paris&amp;quot; is CB, resulting in a conflict, pro-
viding one ground to rule out the example. An-
other ground would result from further discourse
interpretation: &amp;quot;London&amp;quot; cannot be interpreted as
CB, as it has not been activated in the context.
To illustrate embedded foci, consider (19).
</bodyText>
<figure confidence="0.3210797">
(19) (Which teacher did you give what book?)
I gave the book ON SYNTAX to the lecturer OF EN-
GLISH.
Word Ctxt SO Tune Det Proj T/F
I CB T T T CB CB
gave CB T T T CB CB
book CB T T CB T CB
syntax T T NB T T NB
teacher CB T T T T CB
English T T NB T T NB
</figure>
<bodyText confidence="0.999621">
The pitch accents on &amp;quot;syntax&amp;quot; and &amp;quot;English&amp;quot;
establish them as NB, though not determining
&amp;quot;teacher&amp;quot; as CB since &amp;quot;teacher&amp;quot; is not a sibling
of &amp;quot;syntax&amp;quot;. Using projection we can confirm &amp;quot;I&amp;quot;
and &amp;quot;gave&amp;quot; being CB, given that &amp;quot;the book&amp;quot; is CB
on account of the definite determiner.
Information packaging (Vallduvi, 1990) is un-
able to establish a topic and focus for (19), due to
the embedding coupled with discontinuity. Using
our recursive procedure, we have no such prob-
lems, arriving at a focus being constituted by &amp;quot;syn-
tax&amp;quot; and &amp;quot;English&amp;quot;.
Finally, we turn to the Dutch examples. We
only examine the variations in (5), repeated here as
(20); the all-focus case in (4) is trivial, projecting
NB leftwards from the sentence-final pitch accent.
</bodyText>
<reference confidence="0.985932625">
(20) a. (Who read &amp;quot;Moby Dick&amp;quot;?)
JANNB lasc8 &amp;quot;Moby Dick&amp;quot;ce•
b. (Who read &amp;quot;Moby Dick&amp;quot;?)
&amp;quot;Moby Dick&amp;quot;cB lasCB JANNB.
c. (What did John read?)
JanoB las cu &amp;quot;MonY DicK&amp;quot;NB•
d. (What did John read?)
&amp;quot;Mons., Dtcx&amp;quot;NB lascB Jance•
</reference>
<bodyText confidence="0.970785333333333">
The analysis of (20a) is as follows. Observe that
the dependents are ordered canonically, hence the
SO principle yields only T.
</bodyText>
<table confidence="0.89498775">
Word Ctxt SO Tune Det Proj T/F
Jan T T NB T T NB
las CB T T T CB
Moby Dick CB T T T CB
</table>
<bodyText confidence="0.9682315">
The analysis of (20b) differs from the one for
(20a) because of the order variation. The SO prin-
ciple now assigns CB to &amp;quot;Moby Dick&amp;quot;, while the
pitch accent on &amp;quot;Jan&amp;quot; again makes it NB.
</bodyText>
<table confidence="0.98672175">
Word Ctxt SO Tune Det Proj T/F
Moby Dick CB CB CB
las CB T CB
Jan T T NB NB
</table>
<bodyText confidence="0.9573196">
For the analysis of (20c) given below, observe
that in the given context it is the contextual activa-
tion of &amp;quot;Jan&amp;quot; and &amp;quot;las&amp;quot; that prevent the projection
principle to assign NB to the referents leftwards of
&amp;quot;Moby Dick&amp;quot;.
</bodyText>
<table confidence="0.8198915">
Word Ctxt SO Tune Det Proj T/F
Jan CB T T T CB
las CB T T T CB
Moby Dick T T NB T T NB
</table>
<bodyText confidence="0.9995655">
Finally, consider (20d). Our principles predict
that a referent with a pitch accent is NB, while a
referent violating SO is CB — both cannot be si-
multaneously the case. Thus, in general a depen-
dent that appears sentence-initial, and which re-
ceives pitch accent, must fill a semantic role that
is leftmost in its head&apos;s SO. In a declarative sen-
tence in active voice this typically is the ACTOR.
This is why (20d) is ruled out, as the analysis be-
low shows.
</bodyText>
<table confidence="0.89039325">
Word Ctxt SO Tune Det Proj T/F
Moby Dick T CB NB T T
las CB T T T T CB
Jan CB T T T T CB
</table>
<bodyText confidence="0.997954181818182">
Because (Hoffman, 1995) or (Haji6ova et al.,
1995) provide no account in which word order and
tune are integrated, it is difficult to see how they
would deal with the examples above. Using dif-
ferent lexical entries to deal with the word order
variations in (20), (Steedman, 2000) is in princi-
ple able to deal with these examples. However,
CCG lacks the mechanisms to extend the account
to the degree of word order freedom found e.g. in
German — whereas TDG is able to do so (Duchier
and Debusmann, 2001).
</bodyText>
<page confidence="0.993234">
225
</page>
<subsectionHeader confidence="0.961613">
4.4 Final remarks
</subsectionHeader>
<bodyText confidence="0.999992666666667">
The lattice-based model presented in this paper
is of course only an idealization. A more realis-
tic and robust model will need to appeal to pref-
erences. However, considerable mileage can be
derived from slightly more elaborate lattices that
capture essential aspects of preference models.
</bodyText>
<sectionHeader confidence="0.999121" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.957031763157895">
In this paper, we presented an extension to
Topological Dependency Grammar to address the
derivation of an expression&apos;s information struc-
ture. We indentified a number of principles which
on the basis of structural indications of informa-
tivity contribute to the determination of CB/N B-
ness. Our contribution is two-fold: first, our prin-
ciples derive evidence of CB/NB-ness in the 4-
valued Boolean lattice, thus supporting both un-
derspecification by lattice top T and easy combi-
nation by lattice meet n; second we have shown
that our formulation naturally fits in the concurrent
constraint approach of TDG. As a consequence,
we have access to practically efficient constraint-
based parsers, and we take advantage of the fact
that multiple sources of structural indications can
simultaneously influence the realization of infor-
mation structure. In this, we reach beyond existing
approaches such as (Steedman, 2000), (Hoffman,
1995), or (Hajieova et al., 1995).
The approach is conceptually related to (Krui-
jff, 2001), who presents a framework in which
different types of structural indications can inter-
act. However, Kruijff&apos;s framework does not come
with an efficient implementation, and is formally
more intricate than the constraint-based approach
we present here.
One topic for further research is how to derive
a logical representation from the analysis we now
obtain, similar to (Kruijff, 2001; Copestake et al.,
1999) or (Baldridge and Kruijff, 2002). Having a
logical representation would provide a convenient
bridge to discourse interpretation.
Acknowledgements: We would like to thank
Mark Steedman for comments. Geert-Jan Krui-
jff&apos;s work is supported by the DFG SFB 378
Resource-Sensitive Cognitive Processes, Project
NEGRA EM6.
</bodyText>
<sectionHeader confidence="0.860523" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998377094339623">
Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling CCG
and hybrid logic dependency semantics. In Proceedings
ACL&apos;02, pages 319-326, Philadelphia, Pennsylvania.
Ann Copestake, Dan Flickinger, and Ivan A. Sag. 1999.
Minimal recursion semantics, an introduction. Unpub-
lished Manuscript. CSL1/Stanford University.
Denys Duchier and Ralph Debusmann. 2001. Topological
dependency trees: A constraint-based account of linear
precedence. In Proceedings ACL&apos;01, Toulouse, France.
Denys Duchier. 2001. Lexicalized syntax and topology for
non-projective dependency grammar. In MOL 8 Proceed-
ings.
Eva Haji6ova, Barbara H. Partee, and Petr Sgall. 1998.
Topic-Focus Articulation, Tripartite Structures, and Se-
mantic Context. Kluwer Academic Publishers.
Eva Hajieovti, Hana Skoumalova, and Petr Sgall. 1995. An
automatic procedure for topic-focus identification. Com-
putational Linguistics, 21(1):81-94, March.
Beryl Hoffman. 1995. Integrating &amp;quot;free&amp;quot; word order syn-
tax and information structure. In Proceedings EACL&apos;95,
Dublin, March.
Ivana Kruijff-Korbayova, Geert-Jan M. Kruijff, and John
Bateman 2002. Generation of contextually appropriate
word order. In Kees van Deemter and Roger Kibble, edi-
tors, Information Sharing: Reference and Presupposition
in Language Generation and Interpretation, pages 193-
22!. CSLI Publications, Stanford CA.
Ivana Kruijff-Korbayova, Stina Ericsson, Kepa-Joseba
Rodriguez, and Elena Karagjsova. 2003. Producing con-
textually appropriate intonation in an information-states
based dialogue system. In Proceedings EACL&apos;03, Bu-
dapest, Hungary.
Geert-Jan M. Kruijff. 2001. A Categorial-Modal Logical Ar-
chitecture of Informativity: Dependency Grammar Logic
&amp; Information Structure. Ph.D. thesis, Charles University,
Prague, Czech Republic.
Daniela Kurz, Wojciech Skut, and Hans Uszkoreit. 2000.
German factors constraining word order variation. In
Thirteenth Annual Conference on Human Sentence Pro-
cessing CUNY 2000, La Jolla, California.
Scott Prevost and Mark Steedman. 1994. Specifying intona-
tion from context for speech synthesis. Speech Communi-
cation, 15(1-2):139-153.
Petr Sgall, Eva Hajioovii, and Jarmila Panevova. 1986. The
Meaning of the Sentence in Its Semantic and Pragmatic
Aspects. D. Reidel Publishing Company.
Mark Steedman. 2000. Information structure and the syntax-
phonology interface. Linguistic Inquiry, 31(4):649-689.
Enric Vallduvi and Elisabet Engdahl. 1996. The linguistic re-
alization of information packaging. Linguistics, 34:459-
5i9.
Enric Vallduvi. 1990. The Informational Component. Ph.D.
thesis, University of Pennsylvania, Philadelphia, PA.
</reference>
<page confidence="0.998888">
226
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.881856">
<title confidence="0.999773">Information Structure in Topological Dependency Grammar</title>
<author confidence="0.966968">Geert-Jan Kruijff</author>
<affiliation confidence="0.997307">Computational Linguistics Saarland University</affiliation>
<address confidence="0.955723">Saarbrticken, Germany</address>
<author confidence="0.99198">Denys Duchier</author>
<affiliation confidence="0.999247">INRIA Lorraine</affiliation>
<address confidence="0.981003">Nancy, France</address>
<email confidence="0.998506">denys.duchier@loria.fr</email>
<abstract confidence="0.998955333333333">Topological Dependency Grammar (TDG) is a lexicalized dependency grammar formalism, able to model languages with a relatively free word order. In such languages, word order variation often has an important function: the realization of information structure. The paper discusses how to integrate information structure into TDG, and presents a constraint-based approach to modelling information structure and the various means to realize it, focusing on (possibly simultaneous use of) word order and tune.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>c</author>
</authors>
<note>(What did John read?) [Jan] T [las]T [&amp;quot;MoBv DICK1F.</note>
<marker>c, </marker>
<rawString>c. (What did John read?) [Jan] T [las]T [&amp;quot;MoBv DICK1F.</rawString>
</citation>
<citation valid="false">
<authors>
<author>John read</author>
</authors>
<title>Mos Y DICK&amp;quot;.&amp;quot; d. (What did John read?) [&amp;quot;MOB DIcK1F [las] T [Jan]T. *&amp;quot;MOBY DICK&amp;quot; read John.</title>
<marker>read, </marker>
<rawString>&amp;quot;John read &amp;quot;Mos Y DICK&amp;quot;.&amp;quot; d. (What did John read?) [&amp;quot;MOB DIcK1F [las] T [Jan]T. *&amp;quot;MOBY DICK&amp;quot; read John.</rawString>
</citation>
<citation valid="false">
<authors>
<author>a</author>
</authors>
<title>(Who read &amp;quot;Moby Dick&amp;quot;?)</title>
<booktitle>JANNB lasc8 &amp;quot;Moby Dick&amp;quot;ce•</booktitle>
<marker>a, </marker>
<rawString>(20) a. (Who read &amp;quot;Moby Dick&amp;quot;?) JANNB lasc8 &amp;quot;Moby Dick&amp;quot;ce•</rawString>
</citation>
<citation valid="false">
<authors>
<author>b</author>
</authors>
<title>(Who read &amp;quot;Moby Dick&amp;quot;?) &amp;quot;Moby Dick&amp;quot;cB</title>
<tech>lasCB JANNB.</tech>
<marker>b, </marker>
<rawString>b. (Who read &amp;quot;Moby Dick&amp;quot;?) &amp;quot;Moby Dick&amp;quot;cB lasCB JANNB.</rawString>
</citation>
<citation valid="false">
<authors>
<author>c</author>
</authors>
<title>(What did John read?) JanoB las cu &amp;quot;MonY DicK&amp;quot;NB• d. (What did John read?) &amp;quot;Mons., Dtcx&amp;quot;NB</title>
<tech>lascB Jance•</tech>
<marker>c, </marker>
<rawString>c. (What did John read?) JanoB las cu &amp;quot;MonY DicK&amp;quot;NB• d. (What did John read?) &amp;quot;Mons., Dtcx&amp;quot;NB lascB Jance•</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Geert-Jan Kruijff</author>
</authors>
<title>Coupling CCG and hybrid logic dependency semantics.</title>
<date>2002</date>
<booktitle>In Proceedings ACL&apos;02,</booktitle>
<pages>319--326</pages>
<location>Philadelphia, Pennsylvania.</location>
<marker>Baldridge, Kruijff, 2002</marker>
<rawString>Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling CCG and hybrid logic dependency semantics. In Proceedings ACL&apos;02, pages 319-326, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Ivan A Sag</author>
</authors>
<title>Minimal recursion semantics, an introduction. Unpublished Manuscript.</title>
<date>1999</date>
<tech>CSL1/Stanford</tech>
<institution>University.</institution>
<marker>Copestake, Flickinger, Sag, 1999</marker>
<rawString>Ann Copestake, Dan Flickinger, and Ivan A. Sag. 1999. Minimal recursion semantics, an introduction. Unpublished Manuscript. CSL1/Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denys Duchier</author>
<author>Ralph Debusmann</author>
</authors>
<title>Topological dependency trees: A constraint-based account of linear precedence.</title>
<date>2001</date>
<booktitle>In Proceedings ACL&apos;01,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="856" citStr="Duchier and Debusmann, 2001" startWordPosition="110" endWordPosition="114">stract Topological Dependency Grammar (TDG) is a lexicalized dependency grammar formalism, able to model languages with a relatively free word order. In such languages, word order variation often has an important function: the realization of information structure. The paper discusses how to integrate information structure into TDG, and presents a constraint-based approach to modelling information structure and the various means to realize it, focusing on (possibly simultaneous use of) word order and tune. 1 Introduction In this paper, we present an extension to Topological Dependency Grammar (Duchier and Debusmann, 2001) enabling us to analyse e.g. word order variation and tune as means to indicate what is the topic and what is the focus of an expression — i.e. its information structure (cf. §2, §4). Using a constraint-based approach, we can analyse the surface form of an expression in terms of the information structure that it realizes. The information structure of an expression is a core part of its meaning: it indicates how the expression relates to the discourse context. Information structure thus constitutes a crucial factor in determining an expression&apos;s contextual appropriateness or interpretability. P</context>
</contexts>
<marker>Duchier, Debusmann, 2001</marker>
<rawString>Denys Duchier and Ralph Debusmann. 2001. Topological dependency trees: A constraint-based account of linear precedence. In Proceedings ACL&apos;01, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denys Duchier</author>
</authors>
<title>Lexicalized syntax and topology for non-projective dependency grammar.</title>
<date>2001</date>
<booktitle>In MOL 8 Proceedings.</booktitle>
<marker>Duchier, 2001</marker>
<rawString>Denys Duchier. 2001. Lexicalized syntax and topology for non-projective dependency grammar. In MOL 8 Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Haji6ova</author>
<author>Barbara H Partee</author>
<author>Petr Sgall</author>
</authors>
<title>Topic-Focus Articulation, Tripartite Structures, and Semantic Context.</title>
<date>1998</date>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Haji6ova, Partee, Sgall, 1998</marker>
<rawString>Eva Haji6ova, Barbara H. Partee, and Petr Sgall. 1998. Topic-Focus Articulation, Tripartite Structures, and Semantic Context. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hajieovti</author>
<author>Hana Skoumalova</author>
<author>Petr Sgall</author>
</authors>
<title>An automatic procedure for topic-focus identification.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--1</pages>
<marker>Hajieovti, Skoumalova, Sgall, 1995</marker>
<rawString>Eva Hajieovti, Hana Skoumalova, and Petr Sgall. 1995. An automatic procedure for topic-focus identification. Computational Linguistics, 21(1):81-94, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beryl Hoffman</author>
</authors>
<title>Integrating &amp;quot;free&amp;quot; word order syntax and information structure.</title>
<date>1995</date>
<booktitle>In Proceedings EACL&apos;95,</booktitle>
<location>Dublin,</location>
<contexts>
<context position="1722" citStr="Hoffman, 1995" startWordPosition="249" endWordPosition="250"> an expression in terms of the information structure that it realizes. The information structure of an expression is a core part of its meaning: it indicates how the expression relates to the discourse context. Information structure thus constitutes a crucial factor in determining an expression&apos;s contextual appropriateness or interpretability. Particularly in applications that involve human-computer interaction, information structure has thus been found to have a great impact on the understandability of computergenerated language, e.g. question/answering dialogues (Prevost and Steedman, 1994; Hoffman, 1995; Kruijff-Korbayova et al., 2003) or generation (Kruijff-Korbayova et al., 2002). In this paper we concentrate on information structure and the syntax/semantics-interface: We want to be able to reconstruct an expression&apos;s information structure at the level of meaning, given the expression&apos;s surface form. To realize information structure a language may employ a variety of means, not only word order or tune but also morphology or marked syntactic constructions. Collectively we call these means structural indications of informativity, after (Vallduvf and Engdahl, 1996; Kruijff, 2001). As §2 illus</context>
<context position="3011" citStr="Hoffman, 1995" startWordPosition="440" endWordPosition="441"> single expression several types of indications can normally be used simultaneously. The indications may constrain the expression&apos;s well-formedness, and it is through their interaction that the indications help realize information structure. It is precisely this interaction that presents a problem for existing accounts of information structure and its realization. Although accounts normally acknowledge that there are various types of structural indications, most of them focus solely on modelling the use of a single type of structural indication. For example, (Steedman, 2000) focuses on tune, (Hoffman, 1995) or (Hajieova et al., 1995) focus on word order. Such focus would be unproblematic if it were clear how these accounts could be extended to cover multiple, interacting types of structural indications. However, even for (Steedman, 2000; Hoffman, 1995), which are the formally most detailed, this is by no means obvious. CCG&apos;s underlying principles (notably, the Principle of Ad219 jacency) forces Hoffman to introduce separate derivations for establishing an expression&apos;s syntactic structure (incl. word order) and its information structure. This detaches information structure from word order as an i</context>
<context position="5462" citStr="Hoffman, 1995" startWordPosition="833" endWordPosition="834">and hearer. The meaning a speaker communicates relates to these already established referents, and presents more (&amp;quot;new&amp;quot;) information about these referents. The former part of the meaning we call the topic, the latter the focus. An expression&apos;s information structure is the division of its meaning into a topic and a focus (Sgall et al., 1986; Vallduvi, 1990). Languages may realize information structure in various ways. For example, in a language with a relatively free word order, variations in linearization are prototypically used to indicate different information structure (Sgall et al., 1986; Hoffman, 1995; Kruijff, 2001). This explains why different variations, though equally grammatical, are usually not equally interchangeable in a given context. To illustrate the idea of context-dependence, consider the Czech example in (1) and its grammatical variations in (2).1 (1) [Snedl]F [Honza]F [koblihu]F. eat-PAST John donut &amp;quot;John ate a donut.&amp;quot; (2) a. [Honza] T snail [koblihu] F b. [Koblihu] T snedl [Honza] F. c. [Honza koblihu] T [snedl] F (1) illustrates an &amp;quot;all-focus&amp;quot; sentence — the entire meaning is new. The examples in (2) presuppose different items to be present (&amp;quot;salient&amp;quot;) in the already estab</context>
</contexts>
<marker>Hoffman, 1995</marker>
<rawString>Beryl Hoffman. 1995. Integrating &amp;quot;free&amp;quot; word order syntax and information structure. In Proceedings EACL&apos;95, Dublin, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivana Kruijff-Korbayova</author>
<author>Geert-Jan M Kruijff</author>
<author>John Bateman</author>
</authors>
<title>Generation of contextually appropriate word order.</title>
<date>2002</date>
<booktitle>In Kees van Deemter and</booktitle>
<pages>193--22</pages>
<editor>Roger Kibble, editors,</editor>
<publisher>CSLI Publications,</publisher>
<location>Stanford CA.</location>
<contexts>
<context position="1802" citStr="Kruijff-Korbayova et al., 2002" startWordPosition="258" endWordPosition="261">ealizes. The information structure of an expression is a core part of its meaning: it indicates how the expression relates to the discourse context. Information structure thus constitutes a crucial factor in determining an expression&apos;s contextual appropriateness or interpretability. Particularly in applications that involve human-computer interaction, information structure has thus been found to have a great impact on the understandability of computergenerated language, e.g. question/answering dialogues (Prevost and Steedman, 1994; Hoffman, 1995; Kruijff-Korbayova et al., 2003) or generation (Kruijff-Korbayova et al., 2002). In this paper we concentrate on information structure and the syntax/semantics-interface: We want to be able to reconstruct an expression&apos;s information structure at the level of meaning, given the expression&apos;s surface form. To realize information structure a language may employ a variety of means, not only word order or tune but also morphology or marked syntactic constructions. Collectively we call these means structural indications of informativity, after (Vallduvf and Engdahl, 1996; Kruijff, 2001). As §2 illustrates, languages are not restricted to using just a single means. Within a sing</context>
</contexts>
<marker>Kruijff-Korbayova, Kruijff, Bateman, 2002</marker>
<rawString>Ivana Kruijff-Korbayova, Geert-Jan M. Kruijff, and John Bateman 2002. Generation of contextually appropriate word order. In Kees van Deemter and Roger Kibble, editors, Information Sharing: Reference and Presupposition in Language Generation and Interpretation, pages 193-22!. CSLI Publications, Stanford CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivana Kruijff-Korbayova</author>
<author>Stina Ericsson</author>
<author>Kepa-Joseba Rodriguez</author>
<author>Elena Karagjsova</author>
</authors>
<title>Producing contextually appropriate intonation in an information-states based dialogue system.</title>
<date>2003</date>
<booktitle>In Proceedings EACL&apos;03,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="1755" citStr="Kruijff-Korbayova et al., 2003" startWordPosition="251" endWordPosition="254">in terms of the information structure that it realizes. The information structure of an expression is a core part of its meaning: it indicates how the expression relates to the discourse context. Information structure thus constitutes a crucial factor in determining an expression&apos;s contextual appropriateness or interpretability. Particularly in applications that involve human-computer interaction, information structure has thus been found to have a great impact on the understandability of computergenerated language, e.g. question/answering dialogues (Prevost and Steedman, 1994; Hoffman, 1995; Kruijff-Korbayova et al., 2003) or generation (Kruijff-Korbayova et al., 2002). In this paper we concentrate on information structure and the syntax/semantics-interface: We want to be able to reconstruct an expression&apos;s information structure at the level of meaning, given the expression&apos;s surface form. To realize information structure a language may employ a variety of means, not only word order or tune but also morphology or marked syntactic constructions. Collectively we call these means structural indications of informativity, after (Vallduvf and Engdahl, 1996; Kruijff, 2001). As §2 illustrates, languages are not restric</context>
</contexts>
<marker>Kruijff-Korbayova, Ericsson, Rodriguez, Karagjsova, 2003</marker>
<rawString>Ivana Kruijff-Korbayova, Stina Ericsson, Kepa-Joseba Rodriguez, and Elena Karagjsova. 2003. Producing contextually appropriate intonation in an information-states based dialogue system. In Proceedings EACL&apos;03, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geert-Jan M Kruijff</author>
</authors>
<title>A Categorial-Modal Logical Architecture of Informativity:</title>
<date>2001</date>
<booktitle>Dependency Grammar Logic &amp; Information Structure. Ph.D. thesis,</booktitle>
<institution>Charles University,</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2309" citStr="Kruijff, 2001" startWordPosition="336" endWordPosition="337">eedman, 1994; Hoffman, 1995; Kruijff-Korbayova et al., 2003) or generation (Kruijff-Korbayova et al., 2002). In this paper we concentrate on information structure and the syntax/semantics-interface: We want to be able to reconstruct an expression&apos;s information structure at the level of meaning, given the expression&apos;s surface form. To realize information structure a language may employ a variety of means, not only word order or tune but also morphology or marked syntactic constructions. Collectively we call these means structural indications of informativity, after (Vallduvf and Engdahl, 1996; Kruijff, 2001). As §2 illustrates, languages are not restricted to using just a single means. Within a single expression several types of indications can normally be used simultaneously. The indications may constrain the expression&apos;s well-formedness, and it is through their interaction that the indications help realize information structure. It is precisely this interaction that presents a problem for existing accounts of information structure and its realization. Although accounts normally acknowledge that there are various types of structural indications, most of them focus solely on modelling the use of </context>
<context position="5478" citStr="Kruijff, 2001" startWordPosition="835" endWordPosition="836"> meaning a speaker communicates relates to these already established referents, and presents more (&amp;quot;new&amp;quot;) information about these referents. The former part of the meaning we call the topic, the latter the focus. An expression&apos;s information structure is the division of its meaning into a topic and a focus (Sgall et al., 1986; Vallduvi, 1990). Languages may realize information structure in various ways. For example, in a language with a relatively free word order, variations in linearization are prototypically used to indicate different information structure (Sgall et al., 1986; Hoffman, 1995; Kruijff, 2001). This explains why different variations, though equally grammatical, are usually not equally interchangeable in a given context. To illustrate the idea of context-dependence, consider the Czech example in (1) and its grammatical variations in (2).1 (1) [Snedl]F [Honza]F [koblihu]F. eat-PAST John donut &amp;quot;John ate a donut.&amp;quot; (2) a. [Honza] T snail [koblihu] F b. [Koblihu] T snedl [Honza] F. c. [Honza koblihu] T [snedl] F (1) illustrates an &amp;quot;all-focus&amp;quot; sentence — the entire meaning is new. The examples in (2) presuppose different items to be present (&amp;quot;salient&amp;quot;) in the already established dialogue.</context>
</contexts>
<marker>Kruijff, 2001</marker>
<rawString>Geert-Jan M. Kruijff. 2001. A Categorial-Modal Logical Architecture of Informativity: Dependency Grammar Logic &amp; Information Structure. Ph.D. thesis, Charles University, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniela Kurz</author>
<author>Wojciech Skut</author>
<author>Hans Uszkoreit</author>
</authors>
<title>German factors constraining word order variation.</title>
<date>2000</date>
<booktitle>In Thirteenth Annual Conference on Human Sentence Processing CUNY</booktitle>
<location>La Jolla, California.</location>
<marker>Kurz, Skut, Uszkoreit, 2000</marker>
<rawString>Daniela Kurz, Wojciech Skut, and Hans Uszkoreit. 2000. German factors constraining word order variation. In Thirteenth Annual Conference on Human Sentence Processing CUNY 2000, La Jolla, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Prevost</author>
<author>Mark Steedman</author>
</authors>
<title>Specifying intonation from context for speech synthesis.</title>
<date>1994</date>
<journal>Speech Communication,</journal>
<pages>15--1</pages>
<contexts>
<context position="1707" citStr="Prevost and Steedman, 1994" startWordPosition="245" endWordPosition="248"> analyse the surface form of an expression in terms of the information structure that it realizes. The information structure of an expression is a core part of its meaning: it indicates how the expression relates to the discourse context. Information structure thus constitutes a crucial factor in determining an expression&apos;s contextual appropriateness or interpretability. Particularly in applications that involve human-computer interaction, information structure has thus been found to have a great impact on the understandability of computergenerated language, e.g. question/answering dialogues (Prevost and Steedman, 1994; Hoffman, 1995; Kruijff-Korbayova et al., 2003) or generation (Kruijff-Korbayova et al., 2002). In this paper we concentrate on information structure and the syntax/semantics-interface: We want to be able to reconstruct an expression&apos;s information structure at the level of meaning, given the expression&apos;s surface form. To realize information structure a language may employ a variety of means, not only word order or tune but also morphology or marked syntactic constructions. Collectively we call these means structural indications of informativity, after (Vallduvf and Engdahl, 1996; Kruijff, 200</context>
</contexts>
<marker>Prevost, Steedman, 1994</marker>
<rawString>Scott Prevost and Mark Steedman. 1994. Specifying intonation from context for speech synthesis. Speech Communication, 15(1-2):139-153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajioovii</author>
<author>Jarmila Panevova</author>
</authors>
<date>1986</date>
<booktitle>The Meaning of the Sentence in Its Semantic and Pragmatic</booktitle>
<publisher>Reidel Publishing Company.</publisher>
<contexts>
<context position="5190" citStr="Sgall et al., 1986" startWordPosition="792" endWordPosition="795">o the data of §2. We close with conclusions. 2 Motivation When a speaker wants to communicate some meaning to a hearer, she does that against a background of discourse referents that have already been activated in the context, and which are (presumably) shared between speaker and hearer. The meaning a speaker communicates relates to these already established referents, and presents more (&amp;quot;new&amp;quot;) information about these referents. The former part of the meaning we call the topic, the latter the focus. An expression&apos;s information structure is the division of its meaning into a topic and a focus (Sgall et al., 1986; Vallduvi, 1990). Languages may realize information structure in various ways. For example, in a language with a relatively free word order, variations in linearization are prototypically used to indicate different information structure (Sgall et al., 1986; Hoffman, 1995; Kruijff, 2001). This explains why different variations, though equally grammatical, are usually not equally interchangeable in a given context. To illustrate the idea of context-dependence, consider the Czech example in (1) and its grammatical variations in (2).1 (1) [Snedl]F [Honza]F [koblihu]F. eat-PAST John donut &amp;quot;John at</context>
</contexts>
<marker>Sgall, Hajioovii, Panevova, 1986</marker>
<rawString>Petr Sgall, Eva Hajioovii, and Jarmila Panevova. 1986. The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. D. Reidel Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Information structure and the syntaxphonology interface. Linguistic Inquiry,</title>
<date>2000</date>
<pages>31--4</pages>
<contexts>
<context position="2978" citStr="Steedman, 2000" startWordPosition="435" endWordPosition="436">sing just a single means. Within a single expression several types of indications can normally be used simultaneously. The indications may constrain the expression&apos;s well-formedness, and it is through their interaction that the indications help realize information structure. It is precisely this interaction that presents a problem for existing accounts of information structure and its realization. Although accounts normally acknowledge that there are various types of structural indications, most of them focus solely on modelling the use of a single type of structural indication. For example, (Steedman, 2000) focuses on tune, (Hoffman, 1995) or (Hajieova et al., 1995) focus on word order. Such focus would be unproblematic if it were clear how these accounts could be extended to cover multiple, interacting types of structural indications. However, even for (Steedman, 2000; Hoffman, 1995), which are the formally most detailed, this is by no means obvious. CCG&apos;s underlying principles (notably, the Principle of Ad219 jacency) forces Hoffman to introduce separate derivations for establishing an expression&apos;s syntactic structure (incl. word order) and its information structure. This detaches information </context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. Information structure and the syntaxphonology interface. Linguistic Inquiry, 31(4):649-689.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enric Vallduvi</author>
<author>Elisabet Engdahl</author>
</authors>
<title>The linguistic realization of information packaging.</title>
<date>1996</date>
<journal>Linguistics,</journal>
<pages>34--459</pages>
<marker>Vallduvi, Engdahl, 1996</marker>
<rawString>Enric Vallduvi and Elisabet Engdahl. 1996. The linguistic realization of information packaging. Linguistics, 34:459-5i9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enric Vallduvi</author>
</authors>
<title>The Informational Component.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5207" citStr="Vallduvi, 1990" startWordPosition="796" endWordPosition="797"> close with conclusions. 2 Motivation When a speaker wants to communicate some meaning to a hearer, she does that against a background of discourse referents that have already been activated in the context, and which are (presumably) shared between speaker and hearer. The meaning a speaker communicates relates to these already established referents, and presents more (&amp;quot;new&amp;quot;) information about these referents. The former part of the meaning we call the topic, the latter the focus. An expression&apos;s information structure is the division of its meaning into a topic and a focus (Sgall et al., 1986; Vallduvi, 1990). Languages may realize information structure in various ways. For example, in a language with a relatively free word order, variations in linearization are prototypically used to indicate different information structure (Sgall et al., 1986; Hoffman, 1995; Kruijff, 2001). This explains why different variations, though equally grammatical, are usually not equally interchangeable in a given context. To illustrate the idea of context-dependence, consider the Czech example in (1) and its grammatical variations in (2).1 (1) [Snedl]F [Honza]F [koblihu]F. eat-PAST John donut &amp;quot;John ate a donut.&amp;quot; (2) a</context>
</contexts>
<marker>Vallduvi, 1990</marker>
<rawString>Enric Vallduvi. 1990. The Informational Component. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>