<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.97999">
Parsing and Subcategorization Data
</title>
<author confidence="0.997732">
Jianguo Li and Chris Brew
</author>
<affiliation confidence="0.9950255">
Department of Linguistics
The Ohio State University
</affiliation>
<address confidence="0.928521">
Columbus, OH, USA
</address>
<email confidence="0.999821">
{jianguo|cbrew}@ling.ohio-state.edu
</email>
<sectionHeader confidence="0.997403" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999903">
In this paper, we compare the per-
formance of a state-of-the-art statistical
parser (Bikel, 2004) in parsing written and
spoken language and in generating sub-
categorization cues from written and spo-
ken language. Although Bikel’s parser
achieves a higher accuracy for parsing
written language, it achieves a higher ac-
curacy when extracting subcategorization
cues from spoken language. Our exper-
iments also show that current technology
for extracting subcategorization frames
initially designed for written texts works
equally well for spoken language. Addi-
tionally, we explore the utility of punctu-
ation in helping parsing and extraction of
subcategorization cues. Our experiments
show that punctuation is of little help in
parsing spoken language and extracting
subcategorization cues from spoken lan-
guage. This indicates that there is no need
to add punctuation in transcribing spoken
corpora simply in order to help parsers.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999911477272727">
Robust statistical syntactic parsers, made possi-
ble by new statistical techniques (Collins, 1999;
Charniak, 2000; Bikel, 2004) and by the avail-
ability of large, hand-annotated training corpora
such as WSJ (Marcus et al., 1993) and Switch-
board (Godefrey et al., 1992), have had a major
impact on the field of natural language process-
ing. There are many ways to make use of parsers’
output. One particular form of data that can be ex-
tracted from parses is information about subcate-
gorization. Subcategorization data comes in two
forms: subcategorization frame (SCF) and sub-
categorization cue (SCC). SCFs differ from SCCs
in that SCFs contain only arguments while SCCs
contain both arguments and adjuncts. Both SCFs
and SCCs have been crucial to NLP tasks. For ex-
ample, SCFs have been used for verb disambigua-
tion and classification (Schulte im Walde, 2000;
Merlo and Stevenson, 2001; Lapata and Brew,
2004; Merlo et al., 2005) and SCCs for semantic
role labeling (Xue and Palmer, 2004; Punyakanok
et al., 2005).
Current technology for automatically acquiring
subcategorization data from corpora usually relies
on statistical parsers to generate SCCs. While
great efforts have been made in parsing written
texts and extracting subcategorization data from
written texts, spoken corpora have received little
attention. This is understandable given that spoken
language poses several challenges that are absent
in written texts, including disfluency, uncertainty
about utterance segmentation and lack of punctu-
ation. Roland and Jurafsky (1998) have suggested
that there are substantial subcategorization differ-
ences between written corpora and spoken cor-
pora. For example, while written corpora show a
much higher percentage of passive structures, spo-
ken corpora usually have a higher percentage of
zero-anaphora constructions. We believe that sub-
categorization data derived from spoken language,
if of acceptable quality, would be of more value to
NLP tasks involving a syntactic analysis of spoken
language. We do not show this here.
The goals of this study are as follows:
</bodyText>
<listItem confidence="0.998937">
1. Test the performance of Bikel’s parser in
parsing written and spoken language.
2. Compare the accuracy level of SCCs gen-
erated from parsed written and spoken lan-
</listItem>
<page confidence="0.974429">
515
</page>
<note confidence="0.723916">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 515–522,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.931975166666667">
guage. We hope that such a comparison will
shed some light on the feasibility of acquiring
subcategorization data from spoken language
using the current SCF acquisition technology
initially designed for written language.
label clause type desired SCCs
</bodyText>
<figure confidence="0.888424">
gerundive (NP)-GERUND
S small clause NP-NP, (NP)-ADJP
control (NP)-INF-to
control (NP)-INF-wh-to
SBAR with a complementizer (NP)-S-wh, (NP)-S-that
without a complementizer (NP)-S-that
</figure>
<listItem confidence="0.999571476190476">
3. Apply our SCF extraction system (Li and
Brew, 2005) to spoken and written lan-
guage separately and compare the accuracy
achieved for the acquired SCFs from spoken
and written language.
4. Explore the utility of punctuation1 in pars-
ing and extraction of SCCs. It is gen-
erally recognized that punctuation helps in
parsing written texts. For example, Roark
(2001) finds that removing punctuation from
both training and test data (WSJ) decreases
his parser’s accuracy from 86.4%/86.8%
(LR/LP) to 83.4%/84.1%. However, spo-
ken language does not come with punctua-
tion. Even when punctuation is added in the
process of transcription, its utility in help-
ing parsing is slight. Both Roark (2001)
and Engel et al. (2002) report that removing
punctuation from both training and test data
(Switchboard) results in only 1% decrease in
their parser’s accuracy.
</listItem>
<sectionHeader confidence="0.972257" genericHeader="method">
2 Experiment Design
</sectionHeader>
<bodyText confidence="0.989694">
Three models will be investigated for parsing and
extracting SCCs from the parser’s output:
</bodyText>
<listItem confidence="0.999489166666667">
1. punc: leaving punctuation in both training
and test data.
2. no-punc: removing punctuation from both
training and test data.
3. punc-no-punc: removing punctuation from
only the test data.
</listItem>
<bodyText confidence="0.9941605">
Following the convention in the parsing com-
munity, for written language, we selected sections
02-21 of WSJ as training data and section 23 as
test data (Collins, 1999). For spoken language, we
designated section 2 and 3 of Switchboard as train-
ing data and files of sw4004 to sw4135 of section 4
as test data (Roark, 2001). Since we are also inter-
ested in extracting SCCs from the parser’s output,
</bodyText>
<footnote confidence="0.7965765">
1We use punctuation to refer to sentence-internal punctu-
ation unless otherwise specified.
</footnote>
<tableCaption confidence="0.98846">
Table 1: SCCs for different clauses
</tableCaption>
<bodyText confidence="0.977128333333333">
we eliminated from the two test corpora all sen-
tences that do not contain verbs. Our experiments
proceed in the following three steps:
</bodyText>
<listItem confidence="0.999203">
1. Tag test data using the POS-tagger described
in Ratnaparkhi (1996).
2. Parse the POS-tagged data using Bikel’s
parser.
3. Extract SCCs from the parser’s output. The
</listItem>
<bodyText confidence="0.8411568">
extractor we built first locates each verb in the
parser’s output and then identifies the syntac-
tic categories of all its sisters and combines
them into an SCC. However, there are cases
where the extractor has more work to do.
</bodyText>
<listItem confidence="0.9842939">
• Finite and Infinite Clauses: In the Penn
Treebank, S and SBAR are used to label
different types of clauses, obscuring too
much detail about the internal structure
of each clause. Our extractor is designed
to identify the internal structure of dif-
ferent types of clause, as shown in Table
1.
• Passive Structures: As noted above,
Roland and Jurafsky (Roland and Juraf-
</listItem>
<bodyText confidence="0.606408">
sky, 1998) have noticed that written lan-
guage tends to have a much higher per-
centage of passive structures than spo-
ken language. Our extractor is also
designed to identify passive structures
from the parser’s output.
</bodyText>
<sectionHeader confidence="0.97065" genericHeader="method">
3 Experiment Results
</sectionHeader>
<subsectionHeader confidence="0.99964">
3.1 Parsing and SCCs
</subsectionHeader>
<bodyText confidence="0.998393833333333">
We used EVALB measures Labeled Recall (LR)
and Labeled Precision (LP) to compare the pars-
ing performance of different models. To compare
the accuracy of SCCs proposed from the parser’s
output, we calculated SCC Recall (SR) and SCC
Precision (SP). SR and SP are defined as follows:
</bodyText>
<equation confidence="0.571654">
SR =
</equation>
<bodyText confidence="0.644525">
number of correct cues from the parser’s output (1)
number of cues from treebank parse
</bodyText>
<page confidence="0.99597">
516
</page>
<table confidence="0.9760917">
WSJ
model LR/LP SR/SP
punc 87.92%/88.29% 76.93%/77.70%
no-punc 86.25%/86.91% 76.96%/76.47%
punc-no-punc 82.31%/83.70% 74.62%/74.88%
Switchboard
model LR/LP SR/SP
punc 83.14%/83.80% 79.04%/78.62%
no-punc 82.42%/83.74% 78.81%/78.37%
punc-no-punc 78.62%/80.68% 75.51%/75.02%
</table>
<tableCaption confidence="0.999594">
Table 2: Results of parsing and extraction of SCCs
</tableCaption>
<bodyText confidence="0.688558">
number of correct cues from the parser’s output
</bodyText>
<equation confidence="0.9056334">
SP = (2)
number of cues from the parser’s output
2 * SR * SP
SCC Balanced F-measure = (3)
SR + SP
</equation>
<bodyText confidence="0.999983333333333">
The results for parsing WSJ and Switchboard
and extracting SCCs are summarized in Table 2.
The LR/LP figures show the following trends:
</bodyText>
<listItem confidence="0.9456568">
1. Roark (2001) showed LR/LP of
86.4%/86.8% for punctuated written
language, 83.4%/84.1% for unpunctuated
written language. We achieve a higher
accuracy in both punctuated and unpunctu-
ated written language, and the decrease if
punctuation is removed is less
2. For spoken language, Roark (2001) showed
LR/LP of 85.2%/85.6% for punctuated spo-
ken language, 84.0%/84.6% for unpunctu-
ated spoken language. We achieve a lower
accuracy in both punctuated and unpunctu-
ated spoken language, and the decrease if
punctuation is removed is less. The trends in
(1) and (2) may be due to parser differences,
or to the removal of sentences lacking verbs.
3. Unsurprisingly, if the test data is unpunctu-
ated, but the models have been trained on
punctuated language, performance decreases
sharply.
</listItem>
<bodyText confidence="0.999641153846154">
In terms of the accuracy of extraction of SCCs,
the results follow a similar pattern. However, the
utility of punctuation turns out to be even smaller.
Removing punctuation from both the training and
test data results in a 0.8% drop in the accuracy of
SCC extraction for written language and a 0.3%
drop for spoken language.
Figure 1 exhibits the relation between the ac-
curacy of parsing and that of extracting SCCs.
If we consider WSJ and Switchboard individu-
ally, there seems to exist a positive correlation be-
tween the accuracy of parsing and that of extract-
ing SCCs. In other words, higher LR/LP indicates
</bodyText>
<figure confidence="0.805355">
punc no−punc punc−no−punc
Models
</figure>
<figureCaption confidence="0.9650995">
Figure 1: F-measure for parsing and extraction of
SCCs
</figureCaption>
<bodyText confidence="0.999931068965517">
higher SR/SP. However, Figure 1 also shows that
although the parser achieves a higher F-measure
value for paring WSJ, it achieves a higher F-
measure value for generating SCCs from Switch-
board.
The fact that the parser achieves a higher ac-
curacy of extracting SCCs from Switchboard than
WSJ merits further discussion. Intuitively, it
seems to be true that the shorter an SCC is, the
more likely that the parser is to get it right. This
intuition is confirmed by the data shown in Fig-
ure 2. Figure 2 plots the accuracy level of extract-
ing SCCs by SCC’s length. It is clear from Fig-
ure 2 that as SCCs get longer, the F-measure value
drops progressively for both WSJ and Switch-
board. Again, Roland and Jurafsky (1998) have
suggested that one major subcategorization differ-
ence between written and spoken corpora is that
spoken corpora have a much higher percentage of
the zero-anaphora construction. We then exam-
ined the distribution of SCCs of different length in
WSJ and Switchboard. Figure 3 shows that SCCs
of length 02 account for a much higher percentage
in Switchboard than WSJ, but it is always the other
way around for SCCs of non-zero length. This
observation led us to believe that the better per-
formance that Bikel’s parser achieves in extracting
SCCs from Switchboard may be attributed to the
following two factors:
</bodyText>
<listItem confidence="0.9904655">
1. Switchboard has a much higher percentage of
SCCs of length 0.
2. The parser is very accurate in extracting
shorter SCCs.
</listItem>
<footnote confidence="0.8400705">
2Verbs have a length-0 SCC if they are intransitive and
have no modifiers.
</footnote>
<figure confidence="0.977843833333333">
90
88
86
84
82
80
78
76
74
WSJ parsing
Switchboard parsing
WSJ SCC
Switchboard SCC
F−measure(%)
517
0 1 2 3 4
Length of SCC
Length of SCCs
</figure>
<figureCaption confidence="0.999977">
Figure 3: Distribution of SCCs by length
</figureCaption>
<subsectionHeader confidence="0.997421">
3.2 Extraction of Dependents
</subsectionHeader>
<bodyText confidence="0.999958176470588">
In order to estimate the effects of SCCs of length
0, we examined the parser’s performance in re-
trieving dependents of verbs. Every constituent
(whether an argument or adjunct) in an SCC gen-
erated by the parser is considered a dependent of
that verb. SCCs of length 0 will be discounted be-
cause verbs that do not take any arguments or ad-
juncts have no dependents3. In addition, this way
of evaluating the extraction of SCCs also matches
the practice in some NLP tasks such as semantic
role labeling (Xue and Palmer, 2004). For the task
of semantic role labeling, the total number of de-
pendents correctly retrieved from the parser’s out-
put affects the accuracy level of the task.
To do this, we calculated the number of depen-
dents shared by between each SCC proposed from
the parser’s output and its corresponding SCC pro-
</bodyText>
<footnote confidence="0.916764">
3We are aware that subjects are typically also consid-
ered dependents, but we did not include subjects in our
experiments
</footnote>
<equation confidence="0.931957">
shared-dependents[i.j] = MAX(
shared-dependents[i-1,j],
shared-dependents[i-1,j-1]+1 iftarget[i] = source[j],
shared-dependents[i-1,j-1] if target[i] != source[j],
shared-dependents[i,j-1])
</equation>
<tableCaption confidence="0.854911">
Table 3: The algorithm for computing shared de-
pendents
</tableCaption>
<table confidence="0.999971285714286">
INF #5 1 1 2 3
ADVP #4 1 1 2 2
PP-in #3 1 1 2 2
NP #2 1 1 1 1
NP #1 1 1 1 1
#0 #1 #2 #3 #4
NP S-that PP-in INF
</table>
<tableCaption confidence="0.9611985">
Table 4: An example of computing the number of
shared dependents
</tableCaption>
<bodyText confidence="0.949104586206897">
posed from Penn Treebank. We based our cal-
culation on a modified version of Minimum Edit
Distance Algorithm. Our algorithm works by cre-
ating a shared-dependents matrix with one col-
umn for each constituent in the target sequence
(SCCs proposed from Penn Treebank) and one
row for each constituent in the source sequence
(SCCs proposed from the parser’s output). Each
cell shared-dependent[i,j] contains the number of
constituents shared between the first i constituents
of the target sequence and the first j constituents of
the source sequence. Each cell can then be com-
puted as a simple function of the three possible
paths through the matrix that arrive there. The al-
gorithm is illustrated in Table 3.
Table 4 shows an example of how the algo-
rithm works with NP-S-that-PP-in-INF as the tar-
get sequence and NP-NP-PP-in-ADVP-INF as the
source sequence. The algorithm returns 3 as the
number of dependents shared by two SCCs.
We compared the performance of Bikel’s parser
in retrieving dependents from written and spo-
ken language over all three models using De-
pendency Recall (DR) and Dependency Precision
(DP). These metrics are defined as follows:
number of correct dependents from parser’s output
DR =
number of dependents from treebank parse
number of correct dependents from parser’s output
</bodyText>
<equation confidence="0.750896">
DP =
number of dependents from parser’s output
2 * DR * DP
Dependency F-measure = (6)
DR + DP
WSJ
Switchboard
</equation>
<figureCaption confidence="0.800461">
Figure 2: F-measure for SCCs of different length
</figureCaption>
<figure confidence="0.9885875">
WSJ
Switchboard
0 1 2 3 4
Percentage(%) 60
50
40
30
20
10
0
F−measure(%) 90
80
70
60
50
40
30
20
10
518
punc no−punc punc−no−punc
Models
</figure>
<figureCaption confidence="0.999941">
Figure 4: F-measure for extracting dependents
</figureCaption>
<bodyText confidence="0.999944583333333">
The results of Bikel’s parser in retrieving depen-
dents are summarized in Figure 4. Overall, the
parser achieves a better performance for WSJ over
all three models, just the opposite of what have
been observed for SCC extraction. Interestingly,
removing punctuation from both the training and
test data actually slightly improves the F-measure.
This holds true for both WSJ and Switchboard.
This Dependency F-measure differs in detail from
similar measures in Xue and Palmer (2004). For
present purposes all that matters is the relative
value for WSJ and Switchboard.
</bodyText>
<sectionHeader confidence="0.9642405" genericHeader="method">
4 Extraction of SCFs from Spoken
Language
</sectionHeader>
<bodyText confidence="0.999980611111111">
Our experiments indicate that the SCCs generated
by the parser from spoken language are as accurate
as those generated from written texts. Hence, we
would expect that the current technology for ex-
tracting SCFs, initially designed for written texts,
should work equally well for spoken language.
We previously built a system for automatically ex-
tracting SCFs from spoken BNC, and reported ac-
curacy comparable to previous systems that work
with only written texts (Li and Brew, 2005). How-
ever, Korhonen (2002) has shown that a direct
comparison of different systems is very difficult to
interpret because of the variations in the number
of targeted SCFs, test verbs, gold standards and in
the size of the test data. For this reason, we apply
our SCF acquisition system separately to a written
and spoken corpus of similar size from BNC and
compare the accuracy of acquired SCF sets.
</bodyText>
<subsectionHeader confidence="0.903257">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.952">
As noted above, previous studies on automatic ex-
traction of SCFs from corpora usually proceed in
two steps and we adopt this approach.
</bodyText>
<listItem confidence="0.99956375">
1. Hypothesis Generation: Identify all SCCs
from the corpus data.
2. Hypothesis Selection: Determine which SCC
is a valid SCF for a particular verb.
</listItem>
<subsectionHeader confidence="0.995522">
4.2 SCF Extraction System
</subsectionHeader>
<bodyText confidence="0.99963625">
We briefly outline our SCF extraction system
for automatically extracting SCFs from corpora,
which was based on the design proposed in
Briscoe and Carroll (1997).
</bodyText>
<listItem confidence="0.991358">
1. A Statistical Parser: Bikel’s parser is used
to parse input sentences.
2. An SCF Extractor: An extractor is use to
extract SCCs from the parser’s output.
3. An English Lemmatizer: MORPHA (Min-
nen et al., 2000) is used to lemmatize each
verb.
4. An SCF Evaluator: An evaluator is used
to filter out false SCCs based on their like-
lihood.
</listItem>
<bodyText confidence="0.998475466666667">
An SCC generated by the parser and extractor
may be a correct SCC, or it may contain an ad-
junct, or it may simply be wrong due to tagging or
parsing errors. We therefore need an SCF evalua-
tor capable of filtering out false cues. Our evalu-
ator has two parts: the Binomial Hypothesis Test
(Brent, 1993) and a back-off algorithm (Sarkar and
Zeman, 2000).
1. The Binomial Hypothesis Test (BHT): Let
p be the probability that an scfi occurs with
verbj that is not supposed to take scfi. If a
verb occurs n times and m of those times it
co-occurs with scfi, then the scfi cues are
false cues is estimated by the summation of
the binomial distribution for m &lt; k &lt; n:
</bodyText>
<equation confidence="0.968014">
k(1 − P)(n�k, (7)
</equation>
<bodyText confidence="0.9997802">
If the value of P(m+, n, p) is less than or
equal to a small threshold value, then the null
hypothesis that verbj does not take scfi is ex-
tremely unlikely to be true. Hence, scfi is
very likely to be a valid SCF for verbj. The
</bodyText>
<figure confidence="0.996792111111111">
WSJ
Switchboard
F−measure(%) 86
84
82
80
78
P(M+,n,P) = En n!
k=m k!(n − k)!P
</figure>
<page confidence="0.815455">
519
</page>
<bodyText confidence="0.700346857142857">
SCCs SCFs
NP-PP-before
NP-S-when NP
NP-PP-at-S-before
NP-PP-to-S-when
NP-PP-to-PP-at NP-PP-to
NP-PP-to-S-because-ADVP
</bodyText>
<tableCaption confidence="0.912749">
Table 5: SCCs and correct SCFs for introduce
</tableCaption>
<table confidence="0.429907333333333">
corpus
number of verb tokens
number of verb types
verb types seen more than 10 times
number of acquired SCFs
average number of SCFs per verb
</table>
<tableCaption confidence="0.993567">
Table 6: Training data for WC and SC
</tableCaption>
<bodyText confidence="0.989366095238095">
value of m and n can be directly computed
from the extractor’s output, but the value of
p is not easy to obtain. Following Manning
(1993), we empirically determined the value
of p. It was between 0.005 to 0.4 depend-
ing on the likelihood of an SCC being a valid
SCF.
2. Back-off Algorithm: Many SCCs generated
by the parser and extractor tend to contain
some adjuncts. However, for many SCCs,
one of its subsets is likely to be the correct
SCF. Table 5 shows some SCCs generated by
the extractor and the corresponding SCFs.
The Back-off Algorithm always starts with
the longest SCC for each verb. Assume that
this SCC fails the BHT. The evaluator then
eliminates the last constituent from the re-
jected cue, transfers its frequency to its suc-
cessor and submits the successor to the BHT
again. In this way, frequency can accumulate
and more valid frames survive the BHT.
</bodyText>
<subsectionHeader confidence="0.979463">
4.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.996880666666667">
We evaluated our SCF extraction system on writ-
ten and spoken BNC. We chose one million word
written corpus (WC) and a comparable spoken
corpus (SC) from BNC. Table 6 provides relevant
information on the two corpora. We only keep the
verbs that occur at least 10 times in our training
data.
To compare the performance of our system on
WC and SC, we calculated the type precision, type
</bodyText>
<table confidence="0.9958386">
gold standard COMLEX Manually Constructed
corpus WC SC WC SC
type precision 93.1% 92.9% 93.1% 92.9%
type recall 49.2% 47.7% 56.5% 57.6%
F-measure 64.4% 63.1% 70.3% 71.1%
</table>
<tableCaption confidence="0.998306">
Table 7: Type precision and recall and F-measure
</tableCaption>
<bodyText confidence="0.99971868">
recall and F-measure. Type precision is the per-
centage of SCF types that our system proposes
which are correct according some gold standard
and type recall is the percentage of correct SCF
types proposed by our system that are listed in the
gold standard. We used the 14 verbs 4 selected
by Briscoe and Carroll (1997) and evaluated our
results of these verbs against the SCF entries in
two gold standards: COMLEX (Grishman et al.,
1994) and a manually constructed SCF set from
the training data. It makes sense to use a manually
constructed SCF set while calculating type preci-
sion and recall because some of the SCFs in a syn-
tax dictionary such as COMLEX might not occur
in the training data at all. We constructed separate
SCF sets for the written and spoken BNC.
The results are summarized in Table 7. As
shown in Table 7, the accuracy achieved for WC
and SC are very comparable: Our system achieves
a slightly better result for WC when using COM-
LEX as the gold standard and for SC when using
manually constructed SCF set as gold standard,
suggesting that it is feasible to apply the current
technology for automatically extracting SCFs to
spoken language.
</bodyText>
<sectionHeader confidence="0.990254" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<subsectionHeader confidence="0.999659">
5.1 Use of Parser’s Output
</subsectionHeader>
<bodyText confidence="0.995304">
In this paper, we have shown that it is not nec-
essarily true that statistical parsers always per-
form worse when dealing with spoken language.
The conventional accuracy metrics for parsing
(LR/LP) should not be taken as the only metrics
in determining the feasibility of applying statisti-
cal parsers to spoken language. It is necessary to
consider what information we want to extract out
of parsers’ output and make use of.
1. Extraction of SCFs from Corpora: This task
takes SCCs generated by the parser and ex-
tractor as input. Our experiments show that
</bodyText>
<footnote confidence="0.95416225">
4The 14 verbs used in Briscoe and Carroll (1997) are ask,
begin, believe, cause, expect, find, give, help, like, move, pro-
duce, provide, seem and sway. We replaced sway with show
because sway occurs less than 10 times in our training data.
</footnote>
<table confidence="0.9631065">
WC SC
115,524 109,678
5,234 4,789
1,102 998
2,688 1,984
2.43 1.99
</table>
<page confidence="0.993092">
520
</page>
<bodyText confidence="0.998974705882353">
the SCCs generated for spoken language are
as accurate as those generated for written lan-
guage. We have also shown that it is feasible
to apply the current SCF extraction technol-
ogy to spoken language.
2. Semantic Role Labeling: This task usually
operates on parsers’ output and the number
of dependents of each verb that are correctly
retrieved by the parser clearly affects the ac-
curacy of the task. Our experiments show
that the parser achieves a much lower accu-
racy in retrieving dependents from the spoken
language than written language. This seems
to suggest that a lower accuracy is likely to
be achieved for a semantic role labeling task
performed on spoken language. We are not
aware that this has yet been tried.
</bodyText>
<subsectionHeader confidence="0.959056">
5.2 Punctuation and Speech Transcription
Practice
</subsectionHeader>
<bodyText confidence="0.99997485106383">
Both our experiments and Roark’s experiments
show that parsing accuracy measured by LR/LP
experiences a sharper decrease for WSJ than
Switchboard after we removed punctuation from
training and test data. In spoken language, com-
mas are largely used to delimit disfluency ele-
ments. As noted in Engel et al. (2002), statis-
tical parsers usually condition the probability of
a constituent on the types of its neighboring con-
stituents. The way that commas are used in speech
transcription seems to have the effect of increasing
the range of neighboring constituents, thus frag-
menting the data and making it less reliable. On
the other hand, in written texts, commas serve as
more reliable cues for parsers to identify phrasal
and clausal boundaries.
In addition, our experiment demonstrates that
punctuation does not help much with extraction of
SCCs from spoken language. Removing punctu-
ation from both the training and test data results
in rougly a 0.3% decrease in SR/SP. Furthermore,
removing punctuation from both training and test
data actually slightly improves the performance
of Bikel’s parser in retrieving dependents from
spoken language. All these results seem to sug-
gest that adding punctuation in speech transcrip-
tion is of little help to statistical parsers includ-
ing at least three state-of-the-art statistical parsers
(Collins, 1999; Charniak, 2000; Bikel, 2004). As a
result, there may be other good reasons why some-
one who wants to build a Switchboard-like corpus
should choose to provide punctuation, but there is
no need to do so simply in order to help parsers.
However, segmenting utterances into individual
units is necessary because statistical parsers re-
quire sentence boundaries to be clearly delimited.
Current statistical parsers are unable to handle an
input string consisting of two sentences. For ex-
ample, when presented with an input string as in
(1) and (2), if the two sentences are separated by a
period (1), Bikel’s parser wrongly treats the sec-
ond sentence as a sentential complement of the
main verb like in the first sentence. As a result, the
extractor generates an SCC NP-S for like, which is
incorrect. The parser returns the same parse after
we removed the period (2) and let the parser parse
it again.
</bodyText>
<listItem confidence="0.951039666666667">
(1) I like the long hair. It was back in high
school.
(2) I like the long hair It was back in high school.
</listItem>
<bodyText confidence="0.999839714285714">
Hence, while adding punctuation in transcribing
a Switchboard-like corpus is not of much help to
statistical parsers, segmenting utterances into in-
dividual units is crucial for statistical parsers. In
future work, we plan to develop a system capa-
ble of automatically segmenting speech utterances
into individual units.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99983475">
This study was supported by NSF grant 0347799.
Our thanks go to Eric Fosler-Lussier, Mike White
and three anonymous reviewers for their valuable
comments.
</bodyText>
<sectionHeader confidence="0.999479" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999887">
D. Bikel. 2004. Intricacies of Collin’s parsing models.
Computational Linguistics, 30(2):479–511.
M. Brent. 1993. From grammar to lexicon: Unsu-
pervised learning of lexical syntax. Computational
Linguistics, 19(3):243–262.
T. Briscoe and J. Carroll. 1997. Automatic extraction
of subcategorization from corpora. In Proceedings
of the 5th ACL Conference on Applied Natural Lan-
guage Processing, pages 356–363.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 2000 Conference of
the North American Chapter of the Association for
Computation Linguistics, pages 132–139.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University
of Pennsylvania.
</reference>
<page confidence="0.972807">
521
</page>
<reference confidence="0.999924661764706">
D. Engel, E. Charniak, and M. Johnson. 2002. Parsing
and disfluency placement. In Proceedings of 2002
Conference on Empirical Methods of Natural Lan-
guage Processing, pages 49–54.
J. Godefrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for
research and development. In Proceedings of
ICASSP-92, pages 517–520.
R. Grishman, C. Macleod, and A. Meryers. 1994.
Comlex syntax: Building a computational lexicon.
In Proceedings ofthe 1994 International Conference
of Computational Linguistics, pages 268–272.
A. Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, Cambridge University.
M. Lapata and C. Brew. 2004. Verb class disambigua-
tion using informative priors. Computational Lin-
guistics, 30(1):45–73.
J. Li and C. Brew. 2005. Automatic extraction of sub-
categorization frames from spoken corpora. In Pro-
ceedings of the Interdisciplinary Workshop on the
Identification and Representation of Verb Features
and Verb Classes, Saarbracken, Germany.
C. Manning. 1993. Automatic extraction of a large
subcategorization dictionary from corpora. In Pro-
ceedings of 31st Annual Meeting of the Association
for Computational Linguistics, pages 235–242.
M. Marcus, G. Kim, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics,
19(2):313–330.
P. Merlo and S. Stevenson. 2001. Automatic
verb classification based on statistical distribution
of argument structure. Computational Linguistics,
27(3):373–408.
P. Merlo, E. Joanis, and J. Henderson. 2005. Unsuper-
vised verb class disambiguation based on diathesis
alternations. manuscripts.
G. Minnen, J. Carroll, and D. Pearce. 2000. Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(3):207–223.
V. Punyakanok, D. Roth, and W. Yih. 2005. The neces-
sity of syntactic parsing for semantic role labeling.
In Proceedings of the 2nd Midwest Computational
Linguistics Colloquium, pages 15–22.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Con-
ference on Empirical Methods ofNatural Language
Processing, pages 133–142.
B. Roark. 2001. Robust Probabilistic Predictive
Processing: Motivation, Models, and Applications.
Ph.D. thesis, Brown University.
D. Roland and D. Jurafsky. 1998. How verb sub-
categorization frequency is affected by the corpus
choice. In Proceedings of the 17th International
Conference on Computational Linguistics, pages
1122–1128.
A. Sarkar and D. Zeman. 2000. Automatic extraction
of subcategorization frames for Czech. In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics, pages 691–697.
S. Schulte im Walde. 2000. Clustering verbs semanti-
cally according to alternation behavior. In Proceed-
ings of the 18th International Conference on Com-
putational Linguistics, pages 747–753.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 88–94.
</reference>
<page confidence="0.997414">
522
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987973">
<title confidence="0.999947">Parsing and Subcategorization Data</title>
<author confidence="0.999762">Jianguo Li</author>
<author confidence="0.999762">Chris Brew</author>
<affiliation confidence="0.9997915">Department of Linguistics The Ohio State University</affiliation>
<address confidence="0.999989">Columbus, OH, USA</address>
<abstract confidence="0.999509875">In this paper, we compare the performance of a state-of-the-art statistical parser (Bikel, 2004) in parsing written and spoken language and in generating subcategorization cues from written and spoken language. Although Bikel’s parser achieves a higher accuracy for parsing written language, it achieves a higher accuracy when extracting subcategorization cues from spoken language. Our experiments also show that current technology for extracting subcategorization frames initially designed for written texts works equally well for spoken language. Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues. Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language. This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bikel</author>
</authors>
<title>Intricacies of Collin’s parsing models.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="1243" citStr="Bikel, 2004" startWordPosition="176" endWordPosition="177">gorization frames initially designed for written texts works equally well for spoken language. Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues. Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language. This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers. 1 Introduction Robust statistical syntactic parsers, made possible by new statistical techniques (Collins, 1999; Charniak, 2000; Bikel, 2004) and by the availability of large, hand-annotated training corpora such as WSJ (Marcus et al., 1993) and Switchboard (Godefrey et al., 1992), have had a major impact on the field of natural language processing. There are many ways to make use of parsers’ output. One particular form of data that can be extracted from parses is information about subcategorization. Subcategorization data comes in two forms: subcategorization frame (SCF) and subcategorization cue (SCC). SCFs differ from SCCs in that SCFs contain only arguments while SCCs contain both arguments and adjuncts. Both SCFs and SCCs have</context>
<context position="23332" citStr="Bikel, 2004" startWordPosition="3857" endWordPosition="3858">experiment demonstrates that punctuation does not help much with extraction of SCCs from spoken language. Removing punctuation from both the training and test data results in rougly a 0.3% decrease in SR/SP. Furthermore, removing punctuation from both training and test data actually slightly improves the performance of Bikel’s parser in retrieving dependents from spoken language. All these results seem to suggest that adding punctuation in speech transcription is of little help to statistical parsers including at least three state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000; Bikel, 2004). As a result, there may be other good reasons why someone who wants to build a Switchboard-like corpus should choose to provide punctuation, but there is no need to do so simply in order to help parsers. However, segmenting utterances into individual units is necessary because statistical parsers require sentence boundaries to be clearly delimited. Current statistical parsers are unable to handle an input string consisting of two sentences. For example, when presented with an input string as in (1) and (2), if the two sentences are separated by a period (1), Bikel’s parser wrongly treats the </context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>D. Bikel. 2004. Intricacies of Collin’s parsing models. Computational Linguistics, 30(2):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>From grammar to lexicon: Unsupervised learning of lexical syntax.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="16579" citStr="Brent, 1993" startWordPosition="2699" endWordPosition="2700">ikel’s parser is used to parse input sentences. 2. An SCF Extractor: An extractor is use to extract SCCs from the parser’s output. 3. An English Lemmatizer: MORPHA (Minnen et al., 2000) is used to lemmatize each verb. 4. An SCF Evaluator: An evaluator is used to filter out false SCCs based on their likelihood. An SCC generated by the parser and extractor may be a correct SCC, or it may contain an adjunct, or it may simply be wrong due to tagging or parsing errors. We therefore need an SCF evaluator capable of filtering out false cues. Our evaluator has two parts: the Binomial Hypothesis Test (Brent, 1993) and a back-off algorithm (Sarkar and Zeman, 2000). 1. The Binomial Hypothesis Test (BHT): Let p be the probability that an scfi occurs with verbj that is not supposed to take scfi. If a verb occurs n times and m of those times it co-occurs with scfi, then the scfi cues are false cues is estimated by the summation of the binomial distribution for m &lt; k &lt; n: k(1 − P)(n�k, (7) If the value of P(m+, n, p) is less than or equal to a small threshold value, then the null hypothesis that verbj does not take scfi is extremely unlikely to be true. Hence, scfi is very likely to be a valid SCF for verbj.</context>
</contexts>
<marker>Brent, 1993</marker>
<rawString>M. Brent. 1993. From grammar to lexicon: Unsupervised learning of lexical syntax. Computational Linguistics, 19(3):243–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th ACL Conference on Applied Natural Language Processing,</booktitle>
<pages>356--363</pages>
<contexts>
<context position="15939" citStr="Briscoe and Carroll (1997)" startWordPosition="2579" endWordPosition="2582"> our SCF acquisition system separately to a written and spoken corpus of similar size from BNC and compare the accuracy of acquired SCF sets. 4.1 Overview As noted above, previous studies on automatic extraction of SCFs from corpora usually proceed in two steps and we adopt this approach. 1. Hypothesis Generation: Identify all SCCs from the corpus data. 2. Hypothesis Selection: Determine which SCC is a valid SCF for a particular verb. 4.2 SCF Extraction System We briefly outline our SCF extraction system for automatically extracting SCFs from corpora, which was based on the design proposed in Briscoe and Carroll (1997). 1. A Statistical Parser: Bikel’s parser is used to parse input sentences. 2. An SCF Extractor: An extractor is use to extract SCCs from the parser’s output. 3. An English Lemmatizer: MORPHA (Minnen et al., 2000) is used to lemmatize each verb. 4. An SCF Evaluator: An evaluator is used to filter out false SCCs based on their likelihood. An SCC generated by the parser and extractor may be a correct SCC, or it may contain an adjunct, or it may simply be wrong due to tagging or parsing errors. We therefore need an SCF evaluator capable of filtering out false cues. Our evaluator has two parts: th</context>
<context position="19422" citStr="Briscoe and Carroll (1997)" startWordPosition="3204" endWordPosition="3207">ompare the performance of our system on WC and SC, we calculated the type precision, type gold standard COMLEX Manually Constructed corpus WC SC WC SC type precision 93.1% 92.9% 93.1% 92.9% type recall 49.2% 47.7% 56.5% 57.6% F-measure 64.4% 63.1% 70.3% 71.1% Table 7: Type precision and recall and F-measure recall and F-measure. Type precision is the percentage of SCF types that our system proposes which are correct according some gold standard and type recall is the percentage of correct SCF types proposed by our system that are listed in the gold standard. We used the 14 verbs 4 selected by Briscoe and Carroll (1997) and evaluated our results of these verbs against the SCF entries in two gold standards: COMLEX (Grishman et al., 1994) and a manually constructed SCF set from the training data. It makes sense to use a manually constructed SCF set while calculating type precision and recall because some of the SCFs in a syntax dictionary such as COMLEX might not occur in the training data at all. We constructed separate SCF sets for the written and spoken BNC. The results are summarized in Table 7. As shown in Table 7, the accuracy achieved for WC and SC are very comparable: Our system achieves a slightly bet</context>
<context position="20924" citStr="Briscoe and Carroll (1997)" startWordPosition="3465" endWordPosition="3468"> Use of Parser’s Output In this paper, we have shown that it is not necessarily true that statistical parsers always perform worse when dealing with spoken language. The conventional accuracy metrics for parsing (LR/LP) should not be taken as the only metrics in determining the feasibility of applying statistical parsers to spoken language. It is necessary to consider what information we want to extract out of parsers’ output and make use of. 1. Extraction of SCFs from Corpora: This task takes SCCs generated by the parser and extractor as input. Our experiments show that 4The 14 verbs used in Briscoe and Carroll (1997) are ask, begin, believe, cause, expect, find, give, help, like, move, produce, provide, seem and sway. We replaced sway with show because sway occurs less than 10 times in our training data. WC SC 115,524 109,678 5,234 4,789 1,102 998 2,688 1,984 2.43 1.99 520 the SCCs generated for spoken language are as accurate as those generated for written language. We have also shown that it is feasible to apply the current SCF extraction technology to spoken language. 2. Semantic Role Labeling: This task usually operates on parsers’ output and the number of dependents of each verb that are correctly re</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>T. Briscoe and J. Carroll. 1997. Automatic extraction of subcategorization from corpora. In Proceedings of the 5th ACL Conference on Applied Natural Language Processing, pages 356–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Conference of the North American Chapter of the Association for Computation Linguistics,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1229" citStr="Charniak, 2000" startWordPosition="174" endWordPosition="175">tracting subcategorization frames initially designed for written texts works equally well for spoken language. Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues. Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language. This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers. 1 Introduction Robust statistical syntactic parsers, made possible by new statistical techniques (Collins, 1999; Charniak, 2000; Bikel, 2004) and by the availability of large, hand-annotated training corpora such as WSJ (Marcus et al., 1993) and Switchboard (Godefrey et al., 1992), have had a major impact on the field of natural language processing. There are many ways to make use of parsers’ output. One particular form of data that can be extracted from parses is information about subcategorization. Subcategorization data comes in two forms: subcategorization frame (SCF) and subcategorization cue (SCC). SCFs differ from SCCs in that SCFs contain only arguments while SCCs contain both arguments and adjuncts. Both SCFs</context>
<context position="23318" citStr="Charniak, 2000" startWordPosition="3855" endWordPosition="3856">n addition, our experiment demonstrates that punctuation does not help much with extraction of SCCs from spoken language. Removing punctuation from both the training and test data results in rougly a 0.3% decrease in SR/SP. Furthermore, removing punctuation from both training and test data actually slightly improves the performance of Bikel’s parser in retrieving dependents from spoken language. All these results seem to suggest that adding punctuation in speech transcription is of little help to statistical parsers including at least three state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000; Bikel, 2004). As a result, there may be other good reasons why someone who wants to build a Switchboard-like corpus should choose to provide punctuation, but there is no need to do so simply in order to help parsers. However, segmenting utterances into individual units is necessary because statistical parsers require sentence boundaries to be clearly delimited. Current statistical parsers are unable to handle an input string consisting of two sentences. For example, when presented with an input string as in (1) and (2), if the two sentences are separated by a period (1), Bikel’s parser wrong</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 2000 Conference of the North American Chapter of the Association for Computation Linguistics, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1213" citStr="Collins, 1999" startWordPosition="172" endWordPosition="173">chnology for extracting subcategorization frames initially designed for written texts works equally well for spoken language. Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues. Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language. This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers. 1 Introduction Robust statistical syntactic parsers, made possible by new statistical techniques (Collins, 1999; Charniak, 2000; Bikel, 2004) and by the availability of large, hand-annotated training corpora such as WSJ (Marcus et al., 1993) and Switchboard (Godefrey et al., 1992), have had a major impact on the field of natural language processing. There are many ways to make use of parsers’ output. One particular form of data that can be extracted from parses is information about subcategorization. Subcategorization data comes in two forms: subcategorization frame (SCF) and subcategorization cue (SCC). SCFs differ from SCCs in that SCFs contain only arguments while SCCs contain both arguments and adj</context>
<context position="5260" citStr="Collins, 1999" startWordPosition="794" endWordPosition="795"> (2002) report that removing punctuation from both training and test data (Switchboard) results in only 1% decrease in their parser’s accuracy. 2 Experiment Design Three models will be investigated for parsing and extracting SCCs from the parser’s output: 1. punc: leaving punctuation in both training and test data. 2. no-punc: removing punctuation from both training and test data. 3. punc-no-punc: removing punctuation from only the test data. Following the convention in the parsing community, for written language, we selected sections 02-21 of WSJ as training data and section 23 as test data (Collins, 1999). For spoken language, we designated section 2 and 3 of Switchboard as training data and files of sw4004 to sw4135 of section 4 as test data (Roark, 2001). Since we are also interested in extracting SCCs from the parser’s output, 1We use punctuation to refer to sentence-internal punctuation unless otherwise specified. Table 1: SCCs for different clauses we eliminated from the two test corpora all sentences that do not contain verbs. Our experiments proceed in the following three steps: 1. Tag test data using the POS-tagger described in Ratnaparkhi (1996). 2. Parse the POS-tagged data using Bik</context>
<context position="23302" citStr="Collins, 1999" startWordPosition="3853" endWordPosition="3854">l boundaries. In addition, our experiment demonstrates that punctuation does not help much with extraction of SCCs from spoken language. Removing punctuation from both the training and test data results in rougly a 0.3% decrease in SR/SP. Furthermore, removing punctuation from both training and test data actually slightly improves the performance of Bikel’s parser in retrieving dependents from spoken language. All these results seem to suggest that adding punctuation in speech transcription is of little help to statistical parsers including at least three state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000; Bikel, 2004). As a result, there may be other good reasons why someone who wants to build a Switchboard-like corpus should choose to provide punctuation, but there is no need to do so simply in order to help parsers. However, segmenting utterances into individual units is necessary because statistical parsers require sentence boundaries to be clearly delimited. Current statistical parsers are unable to handle an input string consisting of two sentences. For example, when presented with an input string as in (1) and (2), if the two sentences are separated by a period (1), Bike</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Engel</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Parsing and disfluency placement.</title>
<date>2002</date>
<booktitle>In Proceedings of 2002 Conference on Empirical Methods of Natural Language Processing,</booktitle>
<pages>49--54</pages>
<contexts>
<context position="4653" citStr="Engel et al. (2002)" startWordPosition="698" endWordPosition="701">tely and compare the accuracy achieved for the acquired SCFs from spoken and written language. 4. Explore the utility of punctuation1 in parsing and extraction of SCCs. It is generally recognized that punctuation helps in parsing written texts. For example, Roark (2001) finds that removing punctuation from both training and test data (WSJ) decreases his parser’s accuracy from 86.4%/86.8% (LR/LP) to 83.4%/84.1%. However, spoken language does not come with punctuation. Even when punctuation is added in the process of transcription, its utility in helping parsing is slight. Both Roark (2001) and Engel et al. (2002) report that removing punctuation from both training and test data (Switchboard) results in only 1% decrease in their parser’s accuracy. 2 Experiment Design Three models will be investigated for parsing and extracting SCCs from the parser’s output: 1. punc: leaving punctuation in both training and test data. 2. no-punc: removing punctuation from both training and test data. 3. punc-no-punc: removing punctuation from only the test data. Following the convention in the parsing community, for written language, we selected sections 02-21 of WSJ as training data and section 23 as test data (Collins</context>
<context position="22270" citStr="Engel et al. (2002)" startWordPosition="3691" endWordPosition="3694">racy in retrieving dependents from the spoken language than written language. This seems to suggest that a lower accuracy is likely to be achieved for a semantic role labeling task performed on spoken language. We are not aware that this has yet been tried. 5.2 Punctuation and Speech Transcription Practice Both our experiments and Roark’s experiments show that parsing accuracy measured by LR/LP experiences a sharper decrease for WSJ than Switchboard after we removed punctuation from training and test data. In spoken language, commas are largely used to delimit disfluency elements. As noted in Engel et al. (2002), statistical parsers usually condition the probability of a constituent on the types of its neighboring constituents. The way that commas are used in speech transcription seems to have the effect of increasing the range of neighboring constituents, thus fragmenting the data and making it less reliable. On the other hand, in written texts, commas serve as more reliable cues for parsers to identify phrasal and clausal boundaries. In addition, our experiment demonstrates that punctuation does not help much with extraction of SCCs from spoken language. Removing punctuation from both the training </context>
</contexts>
<marker>Engel, Charniak, Johnson, 2002</marker>
<rawString>D. Engel, E. Charniak, and M. Johnson. 2002. Parsing and disfluency placement. In Proceedings of 2002 Conference on Empirical Methods of Natural Language Processing, pages 49–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Godefrey</author>
<author>E Holliman</author>
<author>J McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proceedings of ICASSP-92,</booktitle>
<pages>517--520</pages>
<contexts>
<context position="1383" citStr="Godefrey et al., 1992" startWordPosition="198" endWordPosition="201">y of punctuation in helping parsing and extraction of subcategorization cues. Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language. This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers. 1 Introduction Robust statistical syntactic parsers, made possible by new statistical techniques (Collins, 1999; Charniak, 2000; Bikel, 2004) and by the availability of large, hand-annotated training corpora such as WSJ (Marcus et al., 1993) and Switchboard (Godefrey et al., 1992), have had a major impact on the field of natural language processing. There are many ways to make use of parsers’ output. One particular form of data that can be extracted from parses is information about subcategorization. Subcategorization data comes in two forms: subcategorization frame (SCF) and subcategorization cue (SCC). SCFs differ from SCCs in that SCFs contain only arguments while SCCs contain both arguments and adjuncts. Both SCFs and SCCs have been crucial to NLP tasks. For example, SCFs have been used for verb disambiguation and classification (Schulte im Walde, 2000; Merlo and S</context>
</contexts>
<marker>Godefrey, Holliman, McDaniel, 1992</marker>
<rawString>J. Godefrey, E. Holliman, and J. McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and development. In Proceedings of ICASSP-92, pages 517–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>C Macleod</author>
<author>A Meryers</author>
</authors>
<title>Comlex syntax: Building a computational lexicon.</title>
<date>1994</date>
<booktitle>In Proceedings ofthe 1994 International Conference of Computational Linguistics,</booktitle>
<pages>268--272</pages>
<contexts>
<context position="19541" citStr="Grishman et al., 1994" startWordPosition="3224" endWordPosition="3227">structed corpus WC SC WC SC type precision 93.1% 92.9% 93.1% 92.9% type recall 49.2% 47.7% 56.5% 57.6% F-measure 64.4% 63.1% 70.3% 71.1% Table 7: Type precision and recall and F-measure recall and F-measure. Type precision is the percentage of SCF types that our system proposes which are correct according some gold standard and type recall is the percentage of correct SCF types proposed by our system that are listed in the gold standard. We used the 14 verbs 4 selected by Briscoe and Carroll (1997) and evaluated our results of these verbs against the SCF entries in two gold standards: COMLEX (Grishman et al., 1994) and a manually constructed SCF set from the training data. It makes sense to use a manually constructed SCF set while calculating type precision and recall because some of the SCFs in a syntax dictionary such as COMLEX might not occur in the training data at all. We constructed separate SCF sets for the written and spoken BNC. The results are summarized in Table 7. As shown in Table 7, the accuracy achieved for WC and SC are very comparable: Our system achieves a slightly better result for WC when using COMLEX as the gold standard and for SC when using manually constructed SCF set as gold sta</context>
</contexts>
<marker>Grishman, Macleod, Meryers, 1994</marker>
<rawString>R. Grishman, C. Macleod, and A. Meryers. 1994. Comlex syntax: Building a computational lexicon. In Proceedings ofthe 1994 International Conference of Computational Linguistics, pages 268–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
</authors>
<title>Subcategorization Acquisition.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Cambridge University.</institution>
<contexts>
<context position="15081" citStr="Korhonen (2002)" startWordPosition="2438" endWordPosition="2439">s all that matters is the relative value for WSJ and Switchboard. 4 Extraction of SCFs from Spoken Language Our experiments indicate that the SCCs generated by the parser from spoken language are as accurate as those generated from written texts. Hence, we would expect that the current technology for extracting SCFs, initially designed for written texts, should work equally well for spoken language. We previously built a system for automatically extracting SCFs from spoken BNC, and reported accuracy comparable to previous systems that work with only written texts (Li and Brew, 2005). However, Korhonen (2002) has shown that a direct comparison of different systems is very difficult to interpret because of the variations in the number of targeted SCFs, test verbs, gold standards and in the size of the test data. For this reason, we apply our SCF acquisition system separately to a written and spoken corpus of similar size from BNC and compare the accuracy of acquired SCF sets. 4.1 Overview As noted above, previous studies on automatic extraction of SCFs from corpora usually proceed in two steps and we adopt this approach. 1. Hypothesis Generation: Identify all SCCs from the corpus data. 2. Hypothesi</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>A. Korhonen. 2002. Subcategorization Acquisition. Ph.D. thesis, Cambridge University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>C Brew</author>
</authors>
<title>Verb class disambiguation using informative priors.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="2020" citStr="Lapata and Brew, 2004" startWordPosition="304" endWordPosition="307">or impact on the field of natural language processing. There are many ways to make use of parsers’ output. One particular form of data that can be extracted from parses is information about subcategorization. Subcategorization data comes in two forms: subcategorization frame (SCF) and subcategorization cue (SCC). SCFs differ from SCCs in that SCFs contain only arguments while SCCs contain both arguments and adjuncts. Both SCFs and SCCs have been crucial to NLP tasks. For example, SCFs have been used for verb disambiguation and classification (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Lapata and Brew, 2004; Merlo et al., 2005) and SCCs for semantic role labeling (Xue and Palmer, 2004; Punyakanok et al., 2005). Current technology for automatically acquiring subcategorization data from corpora usually relies on statistical parsers to generate SCCs. While great efforts have been made in parsing written texts and extracting subcategorization data from written texts, spoken corpora have received little attention. This is understandable given that spoken language poses several challenges that are absent in written texts, including disfluency, uncertainty about utterance segmentation and lack of punct</context>
</contexts>
<marker>Lapata, Brew, 2004</marker>
<rawString>M. Lapata and C. Brew. 2004. Verb class disambiguation using informative priors. Computational Linguistics, 30(1):45–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Li</author>
<author>C Brew</author>
</authors>
<title>Automatic extraction of subcategorization frames from spoken corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the Interdisciplinary Workshop on the Identification and Representation of Verb Features and Verb Classes,</booktitle>
<location>Saarbracken, Germany.</location>
<contexts>
<context position="3996" citStr="Li and Brew, 2005" startWordPosition="592" endWordPosition="595">NG/ACL 2006 Main Conference Poster Sessions, pages 515–522, Sydney, July 2006. c�2006 Association for Computational Linguistics guage. We hope that such a comparison will shed some light on the feasibility of acquiring subcategorization data from spoken language using the current SCF acquisition technology initially designed for written language. label clause type desired SCCs gerundive (NP)-GERUND S small clause NP-NP, (NP)-ADJP control (NP)-INF-to control (NP)-INF-wh-to SBAR with a complementizer (NP)-S-wh, (NP)-S-that without a complementizer (NP)-S-that 3. Apply our SCF extraction system (Li and Brew, 2005) to spoken and written language separately and compare the accuracy achieved for the acquired SCFs from spoken and written language. 4. Explore the utility of punctuation1 in parsing and extraction of SCCs. It is generally recognized that punctuation helps in parsing written texts. For example, Roark (2001) finds that removing punctuation from both training and test data (WSJ) decreases his parser’s accuracy from 86.4%/86.8% (LR/LP) to 83.4%/84.1%. However, spoken language does not come with punctuation. Even when punctuation is added in the process of transcription, its utility in helping par</context>
<context position="15055" citStr="Li and Brew, 2005" startWordPosition="2432" endWordPosition="2435">r (2004). For present purposes all that matters is the relative value for WSJ and Switchboard. 4 Extraction of SCFs from Spoken Language Our experiments indicate that the SCCs generated by the parser from spoken language are as accurate as those generated from written texts. Hence, we would expect that the current technology for extracting SCFs, initially designed for written texts, should work equally well for spoken language. We previously built a system for automatically extracting SCFs from spoken BNC, and reported accuracy comparable to previous systems that work with only written texts (Li and Brew, 2005). However, Korhonen (2002) has shown that a direct comparison of different systems is very difficult to interpret because of the variations in the number of targeted SCFs, test verbs, gold standards and in the size of the test data. For this reason, we apply our SCF acquisition system separately to a written and spoken corpus of similar size from BNC and compare the accuracy of acquired SCF sets. 4.1 Overview As noted above, previous studies on automatic extraction of SCFs from corpora usually proceed in two steps and we adopt this approach. 1. Hypothesis Generation: Identify all SCCs from the</context>
</contexts>
<marker>Li, Brew, 2005</marker>
<rawString>J. Li and C. Brew. 2005. Automatic extraction of subcategorization frames from spoken corpora. In Proceedings of the Interdisciplinary Workshop on the Identification and Representation of Verb Features and Verb Classes, Saarbracken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
</authors>
<title>Automatic extraction of a large subcategorization dictionary from corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>235--242</pages>
<contexts>
<context position="17744" citStr="Manning (1993)" startWordPosition="2914" endWordPosition="2915"> scfi is very likely to be a valid SCF for verbj. The WSJ Switchboard F−measure(%) 86 84 82 80 78 P(M+,n,P) = En n! k=m k!(n − k)!P 519 SCCs SCFs NP-PP-before NP-S-when NP NP-PP-at-S-before NP-PP-to-S-when NP-PP-to-PP-at NP-PP-to NP-PP-to-S-because-ADVP Table 5: SCCs and correct SCFs for introduce corpus number of verb tokens number of verb types verb types seen more than 10 times number of acquired SCFs average number of SCFs per verb Table 6: Training data for WC and SC value of m and n can be directly computed from the extractor’s output, but the value of p is not easy to obtain. Following Manning (1993), we empirically determined the value of p. It was between 0.005 to 0.4 depending on the likelihood of an SCC being a valid SCF. 2. Back-off Algorithm: Many SCCs generated by the parser and extractor tend to contain some adjuncts. However, for many SCCs, one of its subsets is likely to be the correct SCF. Table 5 shows some SCCs generated by the extractor and the corresponding SCFs. The Back-off Algorithm always starts with the longest SCC for each verb. Assume that this SCC fails the BHT. The evaluator then eliminates the last constituent from the rejected cue, transfers its frequency to its </context>
</contexts>
<marker>Manning, 1993</marker>
<rawString>C. Manning. 1993. Automatic extraction of a large subcategorization dictionary from corpora. In Proceedings of 31st Annual Meeting of the Association for Computational Linguistics, pages 235–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>G Kim</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1343" citStr="Marcus et al., 1993" startWordPosition="191" endWordPosition="194">e. Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues. Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language. This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers. 1 Introduction Robust statistical syntactic parsers, made possible by new statistical techniques (Collins, 1999; Charniak, 2000; Bikel, 2004) and by the availability of large, hand-annotated training corpora such as WSJ (Marcus et al., 1993) and Switchboard (Godefrey et al., 1992), have had a major impact on the field of natural language processing. There are many ways to make use of parsers’ output. One particular form of data that can be extracted from parses is information about subcategorization. Subcategorization data comes in two forms: subcategorization frame (SCF) and subcategorization cue (SCC). SCFs differ from SCCs in that SCFs contain only arguments while SCCs contain both arguments and adjuncts. Both SCFs and SCCs have been crucial to NLP tasks. For example, SCFs have been used for verb disambiguation and classificat</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, G. Kim, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Merlo</author>
<author>S Stevenson</author>
</authors>
<title>Automatic verb classification based on statistical distribution of argument structure.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="1997" citStr="Merlo and Stevenson, 2001" startWordPosition="300" endWordPosition="303"> al., 1992), have had a major impact on the field of natural language processing. There are many ways to make use of parsers’ output. One particular form of data that can be extracted from parses is information about subcategorization. Subcategorization data comes in two forms: subcategorization frame (SCF) and subcategorization cue (SCC). SCFs differ from SCCs in that SCFs contain only arguments while SCCs contain both arguments and adjuncts. Both SCFs and SCCs have been crucial to NLP tasks. For example, SCFs have been used for verb disambiguation and classification (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Lapata and Brew, 2004; Merlo et al., 2005) and SCCs for semantic role labeling (Xue and Palmer, 2004; Punyakanok et al., 2005). Current technology for automatically acquiring subcategorization data from corpora usually relies on statistical parsers to generate SCCs. While great efforts have been made in parsing written texts and extracting subcategorization data from written texts, spoken corpora have received little attention. This is understandable given that spoken language poses several challenges that are absent in written texts, including disfluency, uncertainty about utterance segment</context>
</contexts>
<marker>Merlo, Stevenson, 2001</marker>
<rawString>P. Merlo and S. Stevenson. 2001. Automatic verb classification based on statistical distribution of argument structure. Computational Linguistics, 27(3):373–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Merlo</author>
<author>E Joanis</author>
<author>J Henderson</author>
</authors>
<title>Unsupervised verb class disambiguation based on diathesis alternations.</title>
<date>2005</date>
<note>manuscripts.</note>
<contexts>
<context position="2041" citStr="Merlo et al., 2005" startWordPosition="308" endWordPosition="311">of natural language processing. There are many ways to make use of parsers’ output. One particular form of data that can be extracted from parses is information about subcategorization. Subcategorization data comes in two forms: subcategorization frame (SCF) and subcategorization cue (SCC). SCFs differ from SCCs in that SCFs contain only arguments while SCCs contain both arguments and adjuncts. Both SCFs and SCCs have been crucial to NLP tasks. For example, SCFs have been used for verb disambiguation and classification (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Lapata and Brew, 2004; Merlo et al., 2005) and SCCs for semantic role labeling (Xue and Palmer, 2004; Punyakanok et al., 2005). Current technology for automatically acquiring subcategorization data from corpora usually relies on statistical parsers to generate SCCs. While great efforts have been made in parsing written texts and extracting subcategorization data from written texts, spoken corpora have received little attention. This is understandable given that spoken language poses several challenges that are absent in written texts, including disfluency, uncertainty about utterance segmentation and lack of punctuation. Roland and Ju</context>
</contexts>
<marker>Merlo, Joanis, Henderson, 2005</marker>
<rawString>P. Merlo, E. Joanis, and J. Henderson. 2005. Unsupervised verb class disambiguation based on diathesis alternations. manuscripts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>J Carroll</author>
<author>D Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="16152" citStr="Minnen et al., 2000" startWordPosition="2615" endWordPosition="2619"> from corpora usually proceed in two steps and we adopt this approach. 1. Hypothesis Generation: Identify all SCCs from the corpus data. 2. Hypothesis Selection: Determine which SCC is a valid SCF for a particular verb. 4.2 SCF Extraction System We briefly outline our SCF extraction system for automatically extracting SCFs from corpora, which was based on the design proposed in Briscoe and Carroll (1997). 1. A Statistical Parser: Bikel’s parser is used to parse input sentences. 2. An SCF Extractor: An extractor is use to extract SCCs from the parser’s output. 3. An English Lemmatizer: MORPHA (Minnen et al., 2000) is used to lemmatize each verb. 4. An SCF Evaluator: An evaluator is used to filter out false SCCs based on their likelihood. An SCC generated by the parser and extractor may be a correct SCC, or it may contain an adjunct, or it may simply be wrong due to tagging or parsing errors. We therefore need an SCF evaluator capable of filtering out false cues. Our evaluator has two parts: the Binomial Hypothesis Test (Brent, 1993) and a back-off algorithm (Sarkar and Zeman, 2000). 1. The Binomial Hypothesis Test (BHT): Let p be the probability that an scfi occurs with verbj that is not supposed to ta</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2000</marker>
<rawString>G. Minnen, J. Carroll, and D. Pearce. 2000. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The necessity of syntactic parsing for semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd Midwest Computational Linguistics Colloquium,</booktitle>
<pages>15--22</pages>
<contexts>
<context position="2125" citStr="Punyakanok et al., 2005" startWordPosition="322" endWordPosition="325">put. One particular form of data that can be extracted from parses is information about subcategorization. Subcategorization data comes in two forms: subcategorization frame (SCF) and subcategorization cue (SCC). SCFs differ from SCCs in that SCFs contain only arguments while SCCs contain both arguments and adjuncts. Both SCFs and SCCs have been crucial to NLP tasks. For example, SCFs have been used for verb disambiguation and classification (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Lapata and Brew, 2004; Merlo et al., 2005) and SCCs for semantic role labeling (Xue and Palmer, 2004; Punyakanok et al., 2005). Current technology for automatically acquiring subcategorization data from corpora usually relies on statistical parsers to generate SCCs. While great efforts have been made in parsing written texts and extracting subcategorization data from written texts, spoken corpora have received little attention. This is understandable given that spoken language poses several challenges that are absent in written texts, including disfluency, uncertainty about utterance segmentation and lack of punctuation. Roland and Jurafsky (1998) have suggested that there are substantial subcategorization difference</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2005</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2005. The necessity of syntactic parsing for semantic role labeling. In Proceedings of the 2nd Midwest Computational Linguistics Colloquium, pages 15–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods ofNatural Language Processing,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="5820" citStr="Ratnaparkhi (1996)" startWordPosition="888" endWordPosition="889"> training data and section 23 as test data (Collins, 1999). For spoken language, we designated section 2 and 3 of Switchboard as training data and files of sw4004 to sw4135 of section 4 as test data (Roark, 2001). Since we are also interested in extracting SCCs from the parser’s output, 1We use punctuation to refer to sentence-internal punctuation unless otherwise specified. Table 1: SCCs for different clauses we eliminated from the two test corpora all sentences that do not contain verbs. Our experiments proceed in the following three steps: 1. Tag test data using the POS-tagger described in Ratnaparkhi (1996). 2. Parse the POS-tagged data using Bikel’s parser. 3. Extract SCCs from the parser’s output. The extractor we built first locates each verb in the parser’s output and then identifies the syntactic categories of all its sisters and combines them into an SCC. However, there are cases where the extractor has more work to do. • Finite and Infinite Clauses: In the Penn Treebank, S and SBAR are used to label different types of clauses, obscuring too much detail about the internal structure of each clause. Our extractor is designed to identify the internal structure of different types of clause, as</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the Conference on Empirical Methods ofNatural Language Processing, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Robust Probabilistic Predictive Processing: Motivation, Models, and Applications.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="4304" citStr="Roark (2001)" startWordPosition="644" endWordPosition="645">lly designed for written language. label clause type desired SCCs gerundive (NP)-GERUND S small clause NP-NP, (NP)-ADJP control (NP)-INF-to control (NP)-INF-wh-to SBAR with a complementizer (NP)-S-wh, (NP)-S-that without a complementizer (NP)-S-that 3. Apply our SCF extraction system (Li and Brew, 2005) to spoken and written language separately and compare the accuracy achieved for the acquired SCFs from spoken and written language. 4. Explore the utility of punctuation1 in parsing and extraction of SCCs. It is generally recognized that punctuation helps in parsing written texts. For example, Roark (2001) finds that removing punctuation from both training and test data (WSJ) decreases his parser’s accuracy from 86.4%/86.8% (LR/LP) to 83.4%/84.1%. However, spoken language does not come with punctuation. Even when punctuation is added in the process of transcription, its utility in helping parsing is slight. Both Roark (2001) and Engel et al. (2002) report that removing punctuation from both training and test data (Switchboard) results in only 1% decrease in their parser’s accuracy. 2 Experiment Design Three models will be investigated for parsing and extracting SCCs from the parser’s output: 1.</context>
<context position="7770" citStr="Roark (2001)" startWordPosition="1206" endWordPosition="1207">l LR/LP SR/SP punc 87.92%/88.29% 76.93%/77.70% no-punc 86.25%/86.91% 76.96%/76.47% punc-no-punc 82.31%/83.70% 74.62%/74.88% Switchboard model LR/LP SR/SP punc 83.14%/83.80% 79.04%/78.62% no-punc 82.42%/83.74% 78.81%/78.37% punc-no-punc 78.62%/80.68% 75.51%/75.02% Table 2: Results of parsing and extraction of SCCs number of correct cues from the parser’s output SP = (2) number of cues from the parser’s output 2 * SR * SP SCC Balanced F-measure = (3) SR + SP The results for parsing WSJ and Switchboard and extracting SCCs are summarized in Table 2. The LR/LP figures show the following trends: 1. Roark (2001) showed LR/LP of 86.4%/86.8% for punctuated written language, 83.4%/84.1% for unpunctuated written language. We achieve a higher accuracy in both punctuated and unpunctuated written language, and the decrease if punctuation is removed is less 2. For spoken language, Roark (2001) showed LR/LP of 85.2%/85.6% for punctuated spoken language, 84.0%/84.6% for unpunctuated spoken language. We achieve a lower accuracy in both punctuated and unpunctuated spoken language, and the decrease if punctuation is removed is less. The trends in (1) and (2) may be due to parser differences, or to the removal of </context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>B. Roark. 2001. Robust Probabilistic Predictive Processing: Motivation, Models, and Applications. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roland</author>
<author>D Jurafsky</author>
</authors>
<title>How verb subcategorization frequency is affected by the corpus choice.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics,</booktitle>
<pages>1122--1128</pages>
<contexts>
<context position="2654" citStr="Roland and Jurafsky (1998)" startWordPosition="393" endWordPosition="396">et al., 2005) and SCCs for semantic role labeling (Xue and Palmer, 2004; Punyakanok et al., 2005). Current technology for automatically acquiring subcategorization data from corpora usually relies on statistical parsers to generate SCCs. While great efforts have been made in parsing written texts and extracting subcategorization data from written texts, spoken corpora have received little attention. This is understandable given that spoken language poses several challenges that are absent in written texts, including disfluency, uncertainty about utterance segmentation and lack of punctuation. Roland and Jurafsky (1998) have suggested that there are substantial subcategorization differences between written corpora and spoken corpora. For example, while written corpora show a much higher percentage of passive structures, spoken corpora usually have a higher percentage of zero-anaphora constructions. We believe that subcategorization data derived from spoken language, if of acceptable quality, would be of more value to NLP tasks involving a syntactic analysis of spoken language. We do not show this here. The goals of this study are as follows: 1. Test the performance of Bikel’s parser in parsing written and sp</context>
<context position="6524" citStr="Roland and Jurafsky, 1998" startWordPosition="1006" endWordPosition="1010">parser’s output. The extractor we built first locates each verb in the parser’s output and then identifies the syntactic categories of all its sisters and combines them into an SCC. However, there are cases where the extractor has more work to do. • Finite and Infinite Clauses: In the Penn Treebank, S and SBAR are used to label different types of clauses, obscuring too much detail about the internal structure of each clause. Our extractor is designed to identify the internal structure of different types of clause, as shown in Table 1. • Passive Structures: As noted above, Roland and Jurafsky (Roland and Jurafsky, 1998) have noticed that written language tends to have a much higher percentage of passive structures than spoken language. Our extractor is also designed to identify passive structures from the parser’s output. 3 Experiment Results 3.1 Parsing and SCCs We used EVALB measures Labeled Recall (LR) and Labeled Precision (LP) to compare the parsing performance of different models. To compare the accuracy of SCCs proposed from the parser’s output, we calculated SCC Recall (SR) and SCC Precision (SP). SR and SP are defined as follows: SR = number of correct cues from the parser’s output (1) number of cue</context>
<context position="9946" citStr="Roland and Jurafsky (1998)" startWordPosition="1569" endWordPosition="1572">F-measure value for paring WSJ, it achieves a higher Fmeasure value for generating SCCs from Switchboard. The fact that the parser achieves a higher accuracy of extracting SCCs from Switchboard than WSJ merits further discussion. Intuitively, it seems to be true that the shorter an SCC is, the more likely that the parser is to get it right. This intuition is confirmed by the data shown in Figure 2. Figure 2 plots the accuracy level of extracting SCCs by SCC’s length. It is clear from Figure 2 that as SCCs get longer, the F-measure value drops progressively for both WSJ and Switchboard. Again, Roland and Jurafsky (1998) have suggested that one major subcategorization difference between written and spoken corpora is that spoken corpora have a much higher percentage of the zero-anaphora construction. We then examined the distribution of SCCs of different length in WSJ and Switchboard. Figure 3 shows that SCCs of length 02 account for a much higher percentage in Switchboard than WSJ, but it is always the other way around for SCCs of non-zero length. This observation led us to believe that the better performance that Bikel’s parser achieves in extracting SCCs from Switchboard may be attributed to the following t</context>
</contexts>
<marker>Roland, Jurafsky, 1998</marker>
<rawString>D. Roland and D. Jurafsky. 1998. How verb subcategorization frequency is affected by the corpus choice. In Proceedings of the 17th International Conference on Computational Linguistics, pages 1122–1128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sarkar</author>
<author>D Zeman</author>
</authors>
<title>Automatic extraction of subcategorization frames for Czech.</title>
<date>2000</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>691--697</pages>
<contexts>
<context position="16629" citStr="Sarkar and Zeman, 2000" startWordPosition="2705" endWordPosition="2708">ntences. 2. An SCF Extractor: An extractor is use to extract SCCs from the parser’s output. 3. An English Lemmatizer: MORPHA (Minnen et al., 2000) is used to lemmatize each verb. 4. An SCF Evaluator: An evaluator is used to filter out false SCCs based on their likelihood. An SCC generated by the parser and extractor may be a correct SCC, or it may contain an adjunct, or it may simply be wrong due to tagging or parsing errors. We therefore need an SCF evaluator capable of filtering out false cues. Our evaluator has two parts: the Binomial Hypothesis Test (Brent, 1993) and a back-off algorithm (Sarkar and Zeman, 2000). 1. The Binomial Hypothesis Test (BHT): Let p be the probability that an scfi occurs with verbj that is not supposed to take scfi. If a verb occurs n times and m of those times it co-occurs with scfi, then the scfi cues are false cues is estimated by the summation of the binomial distribution for m &lt; k &lt; n: k(1 − P)(n�k, (7) If the value of P(m+, n, p) is less than or equal to a small threshold value, then the null hypothesis that verbj does not take scfi is extremely unlikely to be true. Hence, scfi is very likely to be a valid SCF for verbj. The WSJ Switchboard F−measure(%) 86 84 82 80 78 P</context>
</contexts>
<marker>Sarkar, Zeman, 2000</marker>
<rawString>A. Sarkar and D. Zeman. 2000. Automatic extraction of subcategorization frames for Czech. In Proceedings of the 19th International Conference on Computational Linguistics, pages 691–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schulte im Walde</author>
</authors>
<title>Clustering verbs semantically according to alternation behavior.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>747--753</pages>
<contexts>
<context position="1970" citStr="Walde, 2000" startWordPosition="298" endWordPosition="299"> (Godefrey et al., 1992), have had a major impact on the field of natural language processing. There are many ways to make use of parsers’ output. One particular form of data that can be extracted from parses is information about subcategorization. Subcategorization data comes in two forms: subcategorization frame (SCF) and subcategorization cue (SCC). SCFs differ from SCCs in that SCFs contain only arguments while SCCs contain both arguments and adjuncts. Both SCFs and SCCs have been crucial to NLP tasks. For example, SCFs have been used for verb disambiguation and classification (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Lapata and Brew, 2004; Merlo et al., 2005) and SCCs for semantic role labeling (Xue and Palmer, 2004; Punyakanok et al., 2005). Current technology for automatically acquiring subcategorization data from corpora usually relies on statistical parsers to generate SCCs. While great efforts have been made in parsing written texts and extracting subcategorization data from written texts, spoken corpora have received little attention. This is understandable given that spoken language poses several challenges that are absent in written texts, including disfluency, uncertai</context>
</contexts>
<marker>Walde, 2000</marker>
<rawString>S. Schulte im Walde. 2000. Clustering verbs semantically according to alternation behavior. In Proceedings of the 18th International Conference on Computational Linguistics, pages 747–753.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>M Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>88--94</pages>
<contexts>
<context position="2099" citStr="Xue and Palmer, 2004" startWordPosition="318" endWordPosition="321">ke use of parsers’ output. One particular form of data that can be extracted from parses is information about subcategorization. Subcategorization data comes in two forms: subcategorization frame (SCF) and subcategorization cue (SCC). SCFs differ from SCCs in that SCFs contain only arguments while SCCs contain both arguments and adjuncts. Both SCFs and SCCs have been crucial to NLP tasks. For example, SCFs have been used for verb disambiguation and classification (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Lapata and Brew, 2004; Merlo et al., 2005) and SCCs for semantic role labeling (Xue and Palmer, 2004; Punyakanok et al., 2005). Current technology for automatically acquiring subcategorization data from corpora usually relies on statistical parsers to generate SCCs. While great efforts have been made in parsing written texts and extracting subcategorization data from written texts, spoken corpora have received little attention. This is understandable given that spoken language poses several challenges that are absent in written texts, including disfluency, uncertainty about utterance segmentation and lack of punctuation. Roland and Jurafsky (1998) have suggested that there are substantial su</context>
<context position="11487" citStr="Xue and Palmer, 2004" startWordPosition="1838" endWordPosition="1841"> Length of SCC Length of SCCs Figure 3: Distribution of SCCs by length 3.2 Extraction of Dependents In order to estimate the effects of SCCs of length 0, we examined the parser’s performance in retrieving dependents of verbs. Every constituent (whether an argument or adjunct) in an SCC generated by the parser is considered a dependent of that verb. SCCs of length 0 will be discounted because verbs that do not take any arguments or adjuncts have no dependents3. In addition, this way of evaluating the extraction of SCCs also matches the practice in some NLP tasks such as semantic role labeling (Xue and Palmer, 2004). For the task of semantic role labeling, the total number of dependents correctly retrieved from the parser’s output affects the accuracy level of the task. To do this, we calculated the number of dependents shared by between each SCC proposed from the parser’s output and its corresponding SCC pro3We are aware that subjects are typically also considered dependents, but we did not include subjects in our experiments shared-dependents[i.j] = MAX( shared-dependents[i-1,j], shared-dependents[i-1,j-1]+1 iftarget[i] = source[j], shared-dependents[i-1,j-1] if target[i] != source[j], shared-dependent</context>
<context position="14445" citStr="Xue and Palmer (2004)" startWordPosition="2334" endWordPosition="2337"> 50 40 30 20 10 0 F−measure(%) 90 80 70 60 50 40 30 20 10 518 punc no−punc punc−no−punc Models Figure 4: F-measure for extracting dependents The results of Bikel’s parser in retrieving dependents are summarized in Figure 4. Overall, the parser achieves a better performance for WSJ over all three models, just the opposite of what have been observed for SCC extraction. Interestingly, removing punctuation from both the training and test data actually slightly improves the F-measure. This holds true for both WSJ and Switchboard. This Dependency F-measure differs in detail from similar measures in Xue and Palmer (2004). For present purposes all that matters is the relative value for WSJ and Switchboard. 4 Extraction of SCFs from Spoken Language Our experiments indicate that the SCCs generated by the parser from spoken language are as accurate as those generated from written texts. Hence, we would expect that the current technology for extracting SCFs, initially designed for written texts, should work equally well for spoken language. We previously built a system for automatically extracting SCFs from spoken BNC, and reported accuracy comparable to previous systems that work with only written texts (Li and B</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>N. Xue and M. Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of2004 Conference on Empirical Methods in Natural Language Processing, pages 88–94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>