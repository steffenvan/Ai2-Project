<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.969568">
Resolving Discourse-Deictic Pronouns: A Two-Stage Approach to Do It
</title>
<author confidence="0.99011">
Sujay Kumar Jauhar
</author>
<affiliation confidence="0.836863">
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.993009">
sjauhar@cs.cmu.edu
</email>
<author confidence="0.960406">
Edgar Gonzàlez
</author>
<affiliation confidence="0.840253">
Google Research
</affiliation>
<address confidence="0.86179">
Mountain View, CA 94043, USA
</address>
<email confidence="0.997795">
edgargip@google.com
</email>
<sectionHeader confidence="0.993873" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999898739130435">
Discourse deixis is a linguistic phenomenon in
which pronouns have verbal or clausal, rather
than nominal, antecedents. Studies have esti-
mated that between 5% and 10% of pronouns
in non-conversational data are discourse deic-
tic. However, current coreference resolution
systems ignore this phenomenon. This paper
presents an automatic system for the detec-
tion and resolution of discourse-deictic pro-
nouns. We introduce a two-step approach that
first recognizes instances of discourse-deictic
pronouns, and then resolves them to their ver-
bal antecedent. Both components rely on lin-
guistically motivated features. We evaluate
the components in isolation and in combina-
tion with two state-of-the-art coreference re-
solvers. Results show that our system out-
performs several baselines, including the only
comparable discourse deixis system, and leads
to small but statistically significant improve-
ments over the full coreference resolution sys-
tems. An error analysis lays bare the need for
a less strict evaluation of this task.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9988356">
Coreference resolution is a central problem in Nat-
ural Language Processing with a broad range of ap-
plications such as summarization (Steinberger et al.,
2007), textual entailment (Mirkin et al., 2010), in-
formation extraction (McCarthy and Lehnert, 1995),
and dialogue systems (Strube and Müller, 2003).
Traditionally, the resolution of noun phrases (NPs)
has been the focus of coreference research (Ng,
2010). However, NPs are not the only participants
in coreference, since verbal or clausal mentions can
</bodyText>
<note confidence="0.528510333333333">
Raul D. Guerra
University of Maryland
College Park, MD 20742, USA
</note>
<email confidence="0.906473">
rguerra@cs.umd.edu
</email>
<author confidence="0.427935">
Marta Recasens
</author>
<affiliation confidence="0.203217">
Google Research
</affiliation>
<address confidence="0.524056">
Mountain View, CA 94043, USA
</address>
<email confidence="0.971518">
recasens@google.com
</email>
<bodyText confidence="0.6809795">
also take part in coreference relations. For example,
consider:
</bodyText>
<listItem confidence="0.9967935">
(1) The United States says it may invite Israeli
and Palestinian negotiators to Washington.
(2) Without planning it in advance, they chose
to settle here.
</listItem>
<bodyText confidence="0.999798666666667">
In (1), the antecedent of the pronoun is an NP, while
in (2) the antecedent1 is a clause2 (Webber, 1988).
Current state-of-the-art coreference resolution sys-
tems (Lee et al., 2011; Fernandes et al., 2012; Dur-
rett and Klein, 2014; Björkelund and Kuhn, 2014)
focus on the former and ignore the latter cases.
Corpus studies across several languages (Eckert
and Strube, 2000; Botley, 2006; Recasens, 2008)
have estimated that between 5% and 10% of pro-
nouns in non-conversational data, and up to 20% in
conversational, have verbal antecedents. A corefer-
ence system that is able to handle discourse deixis
will thus be more accurate, and benefit downstream
applications.
In this paper we present an automatic system that
processes discourse-deictic pronouns. We resolve
the three pronouns it, this and that, which can appear
in linguistic contexts that reflect the phenomenon il-
lustrated in (2). Our system has a modular archi-
tecture consisting of two independent stages: clas-
sification and resolution. The first stage classifies a
pronoun as discourse deictic (or not), and the second
stage resolves discourse-deictic pronouns to verbal
antecedents. Both stages use linguistically moti-
</bodyText>
<footnote confidence="0.9990595">
1Since the pronoun in (2) is cataphoric, it has a postcedent
rather than an antecedent, but we use the two indistinctively.
2Following the OntoNotes convention, we represent clausal
antecedents by their verbal head.
</footnote>
<page confidence="0.940412">
299
</page>
<note confidence="0.9553915">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 299–308,
Denver, Colorado, June 4–5, 2015.
</note>
<bodyText confidence="0.995088666666667">
vated features.
We first evaluate our system by measuring the
performance of the detection and resolution com-
ponents in isolation. They outperform several base-
lines, including Müller’s (2007) approach, which is
the only other comparable discourse deixis system,
to the best of our knowledge. We also measure the
impact of our system on two state-of-the-art coref-
erence resolution systems (Durrett and Klein, 2014;
Björkelund and Kuhn, 2014). The results show the
benefits of stacking a discourse deixis engine on top
of NP coreference resolution.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.981739828571428">
Coreference resolution systems mostly focus on
NPs. Although some isolated efforts have been
made to study discourse-deictic pronouns, they con-
sist mostly of theoretical inquiries or corpus analy-
ses. A few practical implementations have been pro-
posed as well, but most rely on manual intervention
or only apply to restricted domains.
Webber (1988) presents a seminal account of
discourse-deictic pronouns. She catalogs how the
usage of certain pronouns varies based on discourse
context. She also provides an insight into the distin-
guishing characteristics of discourse deixis.
Several empirical studies have also been con-
ducted to evaluate the prevalence of discourse deixis
in corpora across languages. These have been ap-
plied to English for dialogues (Byron and Allen,
1998; Eckert and Strube, 2000) and news and liter-
ature (Botley, 2006), Danish and Italian (Navarretta
and Olsen, 2008; Poesio and Artstein, 2008; Caselli
and Prodanof, 2010), and Spanish (Recasens, 2008).
These studies find that discourse deixis occurs in dif-
ferent languages, although prevalence depends on
the domain in question. While discourse deixis can
account for up to 20% of pronouns in dialogue and
conversational text, a more general figure is between
5% to 10% for other genres.
In addition to a corpus analysis, Eckert and Strube
(2000) provide a schema for performing discourse
deixis resolution that they evaluate by measuring
inter-annotator agreement on five dialogues from the
Switchboard corpus. Byron (2002) presents an early
attempt at a practical system that handles discourse
deixis. However, it relies on sophisticated discourse
Algorithm 1
Discourse deixis resolution of pronoun p
</bodyText>
<equation confidence="0.988342818181818">
pc(p) ← Oc(p) &gt; Classify
if pc(p) &gt; thc then
for v ← Candidates(p) do
pr(v, p) = Or(v, p) &gt; Resolve
end for
vbest ← arg maxv pr(v, p)
if pr(vbest, p) &gt; thr then
return vbest
end if
end if
return ∅ &gt; No verbal antecedent
</equation>
<bodyText confidence="0.99879119047619">
and semantic features, thus only working with man-
ual intervention in a limited domain.
The first fully automatic system to handle
discourse-deictic pronouns was the one by Müller
(2007). In contrast to our two-stage approach, it
directly resolves pronouns to nominal or verbal an-
tecedents. The author targets coreference resolution
in dialogues, but includes several features that are
equally applicable to text data—thus making a com-
parison to our system viable.
Chen et al. (2011) present another unified ap-
proach to dealing with entity and event coreference.
Their system combines the predictions from seven
distinct mention-pair resolvers, each of which fo-
cuses on a specific pair of mention types (NP, pro-
noun, verb). In particular, their verb-pronoun re-
solver falls within the scope of discourse deixis.
Due to the tight coupling of multiple resolvers, a di-
rect comparison with systems focusing on discourse
deixis is hard. However, their features are among the
ones considered in this work.
</bodyText>
<sectionHeader confidence="0.972221" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.998204">
In this section we describe the architecture of our
two-stage system, and then detail the features used
in both stages.
</bodyText>
<subsectionHeader confidence="0.999513">
3.1 System Architecture
</subsectionHeader>
<bodyText confidence="0.9987892">
We propose a two-stage approach for discourse
deixis processing. Our system first classifies a po-
tential pronoun as discourse deictic (or not), and
then it optionally resolves discourse-deictic pro-
nouns with their antecedent.
</bodyText>
<page confidence="0.994361">
300
</page>
<table confidence="0.999877827586207">
Feature Description Cla. Res. Mül.
Pronoun word Word of p • -
Demonstrative p is this or that • - •
Token position Relative position of p in sentence •
Document position Relative position of sentence containing p -
Verb presence Sentences before p have verb •
Parent lemma Lemma of parent of p if verb •
Parent &amp; label Lemma of parent and dependency label of p • •
Tree depth Depth of p in parse tree -
Pronoun path Dependency label path of p to root • -
*Negated parent Parent of p is a negated verb -
*Parent transitivity Transitivity of parent verb of p •
*Clause-governing parent Probability of parent verb to govern a clause •
*Attribute lemma Lemma of attribute of p -
*Attribute POS POS of attribute of p -
Sentence distance Number of sentences between v and p • •
Token distance Log-distance between v and p in tokens • •
Verb distance Number of verbs between v and p -
Relative position v precedes p (anaphora/cataphora) •
Direct dominance v is the immediate parent of p •
Dominance v is an ancestor of p • •
Candidate path Dependency label path of v to root •
*Negated candidate v is negated •
*Candidate transitivity Transitivity of v • •
*Clause-governing candidate Probability of v to govern a clause -
*Right frontier v is in the right frontier of p • •
*I-incompatibility Attribute of p is a non-individual adjective • •
*Verb association strength NPMI between v and parent verb of p -
*Selectional preference Preference between v and parent verb of p -
</table>
<tableCaption confidence="0.992484333333333">
Table 1: Features used for pronoun p and candidate v in the classification (Cla.) and resolution (Res.) stages. Features
marked with • were selected, and those marked with - were discarded by feature selection. The last column (Mül.)
contains the features used by Müller (2007). Features marked with * are described in Section 3.2.
</tableCaption>
<bodyText confidence="0.999905363636364">
More specifically, and as described in Algo-
rithm 1, a classification model O, is applied to each
pronoun p to obtain its probability of being dis-
course deictic p,(p). If the probability is above a
threshold th,, the pronoun is considered for resolu-
tion. All verbs v in the current and n previous sen-
tences3 are considered as candidates. A resolution
model Or is applied to each candidate v to obtain its
probability of being the antecedent of p, pr(v, p); if
the candidate with the highest score vbeat is above a
threshold thr, then it is returned as the antecedent.
</bodyText>
<footnote confidence="0.315421">
3A window of 3 sentences is used in our experiments.
</footnote>
<bodyText confidence="0.999401">
Otherwise, the pronoun remains unlinked.
Both components are implemented as maximum
entropy classifiers. For simplicity, our approach is
independent from the NP–NP coreference resolution
component: competition between verbal and nomi-
nal antecedents is not considered.
</bodyText>
<subsectionHeader confidence="0.985967">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.9998762">
Table 1 gives an overview of the features that were
used by the classification and resolution models. We
consider all the features listed in the table, but some
of them (marked with -) are pruned by feature se-
lection (see Section 4.2). Real-valued features are
</bodyText>
<page confidence="0.994036">
301
</page>
<bodyText confidence="0.968313224489796">
quantized, and dependency label paths are consid-
ered up to length 2. Details for the more sophisti-
cated features (marked with * in the table) follow.
Negated parent/candidate We consider a verb to-
ken to be negated if it has a child connected with a
negation label.
Parent/candidate transitivity We consider a verb
token to be transitive if it has a child with a direct
object label.
Clause-governing parent/candidate This is the
probability of the parent/candidate to have a clausal
or verbal argument. Probabilities for every verbal
lemma are estimated from the Google News corpus.
We then use the logarithm of these probabilities as
the feature values.
Attribute lemma/POS If the pronoun is the sub-
ject of a copular verb, we consider the lemma and
POS of the attribute of this verb as features.
Right frontier Webber (1988) proposes the right
frontier condition to restrict the set of candidates
available as antecedents for discourse-deictic pro-
nouns. We define this condition in terms of what
Webber calls discourse units. These are minimal
discourse segments, and a sequence of several units
can also be nested and form a larger unit. She states
that only units on the right frontier (i.e., not followed
by another unit at the same nesting level) can be an-
tecedents for such pronouns.
(3) [President Obama arrived in San Francisco
on Sunday.] [ [When he held a press confer-
ence,] he reported [he would meet with busi-
ness leaders.] ] [He thought it went well.]
In (3), where discourse units are marked by square
brackets, the verbal heads of discourse segments that
are on the right frontier are underlined, while the
others are italicized to denote inaccessibility.
In our system, we approximate discourse units by
sentences and clauses. The candidate antecedents
are the respective verbal heads of these units. This
feature triggers if the antecedent candidate occurs
on the right frontier of the pronoun. Since we also
consider cataphoric relations, we reverse the rule to
check the left frontier for these cases.
I-incompatibility Eckert and Strube (2000) de-
fine an anaphor to be I-incompatible if it occurs in
a context in which it “cannot refer to an individ-
ual object.” Adjectives can be used as contextual
cues for I-incompatible anaphors in copular con-
structions (4).
</bodyText>
<listItem confidence="0.43755">
(4) It is true.
</listItem>
<bodyText confidence="0.999709285714286">
Similarly to Müller (2007), we define the I-
incompatibility score of an adjective as its condi-
tional probability of being the attribute of a non-
nominal subject given that it occurs in a copular con-
struction. This is estimated from the Google News
corpus as the number of occurrences of the adjective
in one of these patterns:
</bodyText>
<listItem confidence="0.98618975">
• clausal subject + BE + ADJ
(To read is healthy)
• IT + BE + ADJ + TO/THAT
(It is healthy to read)
• nominalized4 subject + BE + ADJ
(The construction was suspended)
• -ing subject + BE + ADJ
(Reading is healthy)
</listItem>
<bodyText confidence="0.9918356">
divided by its number of occurrences in the pattern
BE + ADJ. At classification time, if the pronoun is
in a copular construction with an adjective attribute,
the I-incompatibility score of the latter is used as
feature.
Verb association strength To capture the strength
of association between the candidate antecedent and
the parent verb of the pronoun, we use the normal-
ized pointwise mutual information of the two verbs
co-occurring within a window of 3 sentences, esti-
mated from counts in the Google News corpus.
Selectional preference We use selectional prefer-
ence, as defined by Resnik (1997), to capture the
degree to which the antecedent makes a reason-
able substitute of the pronoun in the context of its
parent verb. The selectional preference strength
of verb w is defined as SR(w) = KL(p(a|w) 11
p(a)), where KL denotes Kullback-Leibler di-
vergence and a are all possible arguments of w
in the Google News corpus. Larger values of
</bodyText>
<footnote confidence="0.9897065">
4Nominalizations were identified using NOMLEX
(Macleod et al., 1998).
</footnote>
<page confidence="0.980259">
302
</page>
<table confidence="0.997058">
Pronoun Total Discourse-Deictic
it 1310 75
that 400 120
this 365 57
Overall 2075 252
</table>
<tableCaption confidence="0.9889435">
Table 2: Distribution of discourse-deictic pronouns in the
test set of the CoNLL-2012 English corpus.
</tableCaption>
<bodyText confidence="0.990250166666667">
this quantity correspond to more selective predi-
cates. Then, the selectional preference strength of
a verb w for a particular argument a is defined as
AR(w, a) = p(a|w) · log (p(a|w)/p(a)) /SR(w).
To account for nominalizations, verbs and nouns are
stemmed following Porter (1980).
</bodyText>
<sectionHeader confidence="0.999423" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.997651">
In this section we describe the setup for evaluating
our system.
</bodyText>
<subsectionHeader confidence="0.940199">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999950047619047">
We perform all our experiments on the English sec-
tion of the CoNLL-2012 corpus (Pradhan et al.,
2012), which is based on OntoNotes (Pradhan et al.,
2007). It consists of 2384 documents (1.6M words)
from a variety of domains: news, broadcast conver-
sation, weblogs, etc. It is annotated with POS tags,
syntax trees, word sense annotation, coreference re-
lations, etc. The coreference layer includes verbal
mentions.
Given these annotations, we consider a pronoun
to be discourse deictic if the preceding mention in
its coreference cluster is verbal, or if it is the first
mention in the cluster and the next one is verbal.
The distribution of potentially discourse-deictic pro-
nouns (it, this and that) in the test set is summarized
in Table 2.
For all our experiments we train, tune and test ac-
cording to the CoNLL-2012 split of OntoNotes. The
gold analyses provided for the shared task are used
for training, and the system analyses for develop-
ment and testing.
</bodyText>
<subsectionHeader confidence="0.752228">
4.2 Experiments
</subsectionHeader>
<bodyText confidence="0.996646130434783">
We train the two components of our system sepa-
rately. For each of them, a maximum entropy model
is learned on the train partition. Feature selection
and threshold tuning are performed by hill climbing
on the development set. We use separate thresholds
for it, this, and that, since their distributions in the
corpus are quite different.
We perform two evaluations of our system: first
classification and resolution are evaluated in isola-
tion, and then both components are stacked on top
of an NP coreference engine.
For classification, we measure system perfor-
mance on standard precision (P), recall (R) and F1 of
correctly predicting whether a pronoun is discourse
deictic or not. For resolution, precision is computed
as the fraction of predicted antecedents that are cor-
rect, and recall as the fraction of gold antecedents
that are correctly predicted. To decouple the eval-
uation of both stages, we also include results with
oracle classifications as input to the resolution stage.
Finally, we use the output of our system to ex-
tend the predictions of two state-of-the-art NP coref-
erence systems:
</bodyText>
<listItem confidence="0.918851666666667">
• BERKELEY (Durrett and Klein, 2014), a joint
model for coreference resolution, named entity
recognition, and entity linking.
• HOTCOREF (Björkelund and Kuhn, 2014), a
latent-antecedent model which exploits non-
local features via beam search.
</listItem>
<bodyText confidence="0.999913181818182">
We only add our predictions for pronouns it, this,
that that are output as singletons by the NP corefer-
ence system.
We report the standard coreference measures on
the combined outputs using the updated CoNLL
scorer v7 (Pradhan et al., 2014). Here, the systems
are evaluated on all nominal, pronominal, and verbal
mentions. The metrics include precision, recall and
F1 for MUC, B3 and CEAFe, and the CoNLL met-
ric, which is the arithmetic mean of the first three F1
scores.
</bodyText>
<subsectionHeader confidence="0.998328">
4.3 Baselines
</subsectionHeader>
<bodyText confidence="0.977401">
We compare our classification component against
two baselines:
</bodyText>
<listItem confidence="0.956483">
• ALL, which blindly classifies all mentions as
discourse deictic.
• NAIVEc, which classifies all this and that men-
tions as discourse deictic, and all it mentions
as non-discourse-deictic. This is motivated by
</listItem>
<page confidence="0.997726">
303
</page>
<table confidence="0.8491274">
it that this Overall
P R F1 P R F1 P R F1 P R F1
ALL 5.7 100.0 10.8 30.0 100.0 46.2 15.6 100.0 27.0 12.1 100.0 21.7
NAIVE, 0.0 0.0 0.0 30.0 100.0 46.2 15.6 100.0 27.0 23.1 70.2 34.8
TWOSTAGE 33.3 4.0 7.1 33.6 77.5 46.9 57.1 21.1 30.8 35.2 42.9 38.6
</table>
<tableCaption confidence="0.995472">
Table 3: Classification evaluation (TWOSTAGE corresponds to our system).
</tableCaption>
<table confidence="0.900743">
it that this Overall
P R F1 P R F1 P R F1 P R F1
NAIVE, 30.7 30.7 30.7 47.5 47.5 47.5 33.3 33.3 33.3 39.3 39.3 39.3
MÜLLER, 30.7 30.7 30.7 47.8 45.0 46.4 43.9 43.9 43.9 41.6 40.5 41.0
TWOSTAGE 46.3 33.3 38.8 59.6 46.7 52.3 59.1 45.6 51.5 55.7 42.5 48.2
</table>
<tableCaption confidence="0.998461">
Table 4: Resolution evaluation with oracle classification (TWOSTAGE corresponds to our system).
</tableCaption>
<table confidence="0.876099">
it that this Overall
P R F1 P R F1 P R F1 P R F1
NAIVE, 0.0 0.0 0.0 15.3 34.2 21.1 20.0 7.0 10.4 15.3 17.9 16.5
MÜLLER, 0.0 0.0 0.0 16.7 36.7 22.9 20.0 7.0 10.4 16.5 19.0 17.7
TWOSTAGE 14.3 1.3 2.4 21.5 40.0 28.0 46.2 10.5 17.1 22.6 21.8 22.2
</table>
<tableCaption confidence="0.999846">
Table 5: Resolution evaluation with system classification (TWOSTAGE corresponds to our system).
</tableCaption>
<bodyText confidence="0.981594333333333">
the distribution of discourse deixis in the cor-
pus (see Table 2).
For resolution, we use the baselines:
</bodyText>
<listItem confidence="0.9943105">
• NAIVEr, which resolves a pronoun to the clos-
est verb in the previous sentence. This is moti-
vated by corpus analyses studying the position
of discourse-deictic pronouns relative to their
antecedents (Navarretta, 2011).
• MÜLLERr, which is an equivalent maximum
</listItem>
<bodyText confidence="0.839608833333333">
entropy model using the subset of our features
also considered by Müller (2007). See column
Mül. in Table 1.
Finally, when measuring the impact of our system
on top of an NP coreference resolution engine, we
consider the following baselines:
</bodyText>
<listItem confidence="0.96631975">
• NAIVE, which uses NAIVEc and NAIVEr.
• MÜLLER, which does not include a classifica-
tion stage, and uses MÜLLERr for resolution.
• ONESTAGE, which does not include a classifi-
cation stage, and uses our complete feature set
for resolution.5
• ORACLE, which outputs the gold annotations
for discourse-deictic relations.
</listItem>
<sectionHeader confidence="0.999809" genericHeader="conclusions">
5 Results
</sectionHeader>
<bodyText confidence="0.999982533333333">
The results for the classification stage are presented
in Table 3, broken down by pronoun type. ALL per-
forms the poorest overall, penalized by a precision
just above 12%. Since in the case of it only 5.7%
of the occurrences are discourse deictic, NAIVEc
gets better results overall by always classifying it
as non-deictic. Our TWOSTAGE system improves
over NAIVEc by an additional 4% F1. However, the
scores remain low—partly because of the difficulty
of the problem (especially the class imbalance), and
partly because despite using a rich set of features,
most of them focus on local context and ignore cues
at the discourse level. The classification of it is par-
ticularly difficult, reflecting the fact that the pronoun
has a wide variety of usages in English.
</bodyText>
<footnote confidence="0.789159666666667">
5Feature selection and threshold tuning were done sepa-
rately for this model. The exact subset of resolution features
that were chosen is omitted for brevity.
</footnote>
<page confidence="0.995034">
304
</page>
<bodyText confidence="0.9824098125">
The scores for resolution are shown in Tables 4 Error type %
and 5. The former uses oracle classification whereas
the latter uses the system output of our classifier.
With oracle classification, NAIVE, and MÜLLER,
perform very similar, except for the case of this. Our
TWOSTAGE resolver outperforms both of them for
all pronouns and metrics, except for the recall of
that. Overall, the difference in F1 is 9 points over
NAIVE, and 7 points over MÜLLER,. The evalua-
tion actually penalizes recall for our system, since
we do not take advantage of the fact that all con-
sidered pronouns are discourse deictic: we trust the
threshold and do not force the assignment of an an-
tecedent.
All the results are lower with system classifica-
tion. Given that our classifier performs the best for
that, the drop for this pronoun is not as high as for
the other two. Again, it stands out as the hardest
pronoun to resolve. Neither NAIVE, nor MÜLLER,
recover any correct antecedent for it. TWOSTAGE
obtains the highest scores across all pronouns and
metrics.
Finally, Table 6 contains the coreference mea-
sures for end-to-end evaluation on top of the
BERKELEY and HOTCOREF systems. The ORA-
CLE row shows an upper bound of 2% in CoNLL
score improvement. All three baselines—NAIVE,
MÜLLER and ONESTAGE—actually cause a de-
crease of up to 0.9% CoNLL.
Our system TWOSTAGE achieves a small fraction
of the headroom. The total number of discourse-
deictic entities that it predicts on the test set is 248,
of which 204 end up merged in the BERKELEY out-
put, and 210 in HOTCOREF. This allows it to ob-
tain the best B3, CEAFe and CoNLL values, de-
spite the fact that the low recall in the classification
of discourse-deictic it reduces our margin for recall
gains by one third. The drop in MUC highlights the
difficulty of keeping the precision level, but our sys-
tem is able to reach a better precision-recall balance
than the other compared approaches.
We assess the statistical significance of the
improvements of TWOSTAGE over BERKELEY
and HOTCOREF using paired bootstrap resam-
pling (Koehn, 2004) followed by two-tailed
Wilcoxon signed-rank tests. All the differences are
significant at the 1% level, except for the B3 F1 dif-
ferences.
</bodyText>
<table confidence="0.949626333333333">
305
System errors
Classification 22.9
Resolution 20.0
Preprocessing 5.7
Annotation errors
Missing 11.4
Multiple antecedents 20.0
System &amp; Annotation errors
Debatable 20.0
Overall 100.0
Table 7: Distribution of errors.
</table>
<sectionHeader confidence="0.66993" genericHeader="references">
6 Error Analysis
</sectionHeader>
<bodyText confidence="0.998544407407407">
In order to gain insight into the precision errors of
our system, we manually analyzed 50 of its de-
cisions on the CoNLL-2012 development set. Of
these, 30% were correct, matching the gold anno-
tation, as in (5).6
(5) Ah, we have established the year 2006 as
Discover Hong Kong Year. Why is that?
The distribution of errors for the remaining cases is
shown in Table 7. While half of the errors are due to
actual errors in the model learned by our system—
either in classification (6) or resolution (7)—or due
to a pre-processing error, another third of them are
not true errors but missing (8) or partial annota-
tions (9)–(10) in the gold standard corpus.
(6) If pictures are taken without permission, that
is to say, it will at all times be pursued by
legal action, a big hassle.
(7) Do we even know if these two medications
are going to be effective against a strain that
hasn’t even presented itself? Here’s the im-
portant thing about that.
(8) You will be taken to stand before governors
and kings. People will do this to you because
you follow me.
6The pronoun to be resolved is in boldface, the antecedent
annotated in the gold standard (if any) is in italics, and the an-
tecedent predicted by our system is underlined.
</bodyText>
<table confidence="0.999881785714286">
MUC B3 CEAF, CoNLL
P R F1 P R F1 P R F1 F1
Durrett and Klein (2014) 72.61 69.91 71.23 61.18 56.43 58.71 56.16 54.23 55.18 61.71
+ NAIVE 70.10 70.33 70.21 58.64 57.49 58.06 52.02 57.21 54.50 60.92
+ MÜLLER 71.57 70.18 70.86 60.15 57.02 58.54 54.55 55.86 55.20 61.53
BERKELEY + ONESTAGE 71.63 70.19 70.90 60.21 57.03 58.58 54.66 55.88 55.26 61.58
+ TWOSTAGE 71.87 70.19 71.02 60.50 57.02 58.71 55.14 55.77 55.45 61.73
+ ORACLE 73.09 71.64 72.36 61.95 58.77 60.32 58.05 58.51 58.28 63.65
Björkelund and Kuhn (2014) 74.30 67.46 70.72 62.71 54.96 58.58 59.40 52.27 55.61 61.64
+ NAIVE 71.38 67.92 69.61 59.72 56.09 57.85 54.14 55.45 54.79 60.75
+ MÜLLER 73.11 67.74 70.32 61.51 55.58 58.39 57.32 54.00 55.61 61.44
HOTCOREF + ONESTAGE 73.15 67.79 70.37 61.54 55.61 58.43 57.35 54.02 55.64 61.48
+ TWOSTAGE 73.49 67.77 70.51 61.94 55.58 58.59 58.14 53.93 55.96 61.69
+ ORACLE 74.79 69.20 71.88 63.59 57.33 60.30 61.33 56.87 59.02 63.73
</table>
<tableCaption confidence="0.990779">
Table 6: End-to-end coreference resolution evaluation (TWOSTAGE corresponds to our system). All differences be-
tween the baseline system and TWOSTAGE are significant at the 1% level except for the B3 F1 differences.
</tableCaption>
<bodyText confidence="0.960171981818182">
(9) At this point they’ve wittled it down to one
aircraft and a missing crew of four individ-
uals. So we’ve gone from several possible
aircraft to one aircraft and from several miss-
ing airmen to four. So how much easier will
that make it for you to unlock this case, do
you think?
(10) What do you mean by that? Either she
either passed out regurgitated. Something
had happened. And on top of that now
there’s a statement...
The examples (8)–(10) show the difficulty of an-
notating discourse deixis relations under guidelines
that require a unique verbal antecedent (Poesio and
Artstein, 2008; Recasens, 2008). In our analysis
we found several cases in which more than one an-
tecedent is acceptable. This is usually the case when
there is an elaboration (i.e., both the first clause and
the follow-up clause restating or elaborating on the
first one are acceptable antecedents, as in (9)) or
a sequence of related and overlapping events. As
pointed out by Poesio and Artstein (2008), “it is not
completely clear the extent to which humans agree
on the interpretation of such expressions,” and the
inconsistencies observed in the data are evidence of
this.
Another class of hard cases are the discourse-
deictic pronouns that are used for packaging a previ-
ous fragment or set of clauses (10). It is very hard to
pick an antecedent for them, even deciding whether
the antecedent is an NP or a clause (Francis, 1994).
Finally, in 20% of the cases the system and the
annotation are in disagreement, but both decisions
are debatable. In many of them, the system did not
make any prediction, but the one in the gold anno-
tation is incorrect. In (11), act is a more plausible
antecedent for that.
(11) “Why didn’t the Bank Board act sooner?”
he said. “That is what Common Cause
should ask be investigated.”
As a result, even though our system obviously makes
multiple mistakes in its decisions, we believe that the
evaluation overpenalizes its performance due to the
debatable and not always clear-cut annotations dis-
cussed above. Discourse deixis resolution is a hard
problem in itself (the chances of selecting a wrong
antecedent for a pronoun are many times greater
than picking the right one), and this difficulty is
accentuated by the problematic annotations in the
training and test data.
Given the difficulty of identifying a single an-
tecedent to discourse-deictic pronouns, as evidenced
by the low inter-annotator agreement on this task,
a more flexible evaluation measure for discourse
deixis systems is needed.
</bodyText>
<page confidence="0.989949">
306
</page>
<bodyText confidence="0.923818871794872">
7 Conclusion In Proceedings of the 2nd Colloquium on Discourse,
We have presented an automatic system for dis- Anaphora and Reference Resolution.
course deixis resolution. The system works in two Donna K Byron. 2002. Resolving pronominal reference
stages: first classifying pronouns as discourse deic- to abstract entities. In Proceedings of ACL, pages 80–
tic or not, and then assigning an antecedent. 87.
Empirical evaluations show that our system out- Tommaso Caselli and Irina Prodanof. 2010. Annotat-
performs naive baselines as well as the only exist- ing event anaphora: A case study. In Proceedings of
ing comparable system. Additionally, when stacked LREC, pages 723–728.
on top of two different state-of-the-art NP corefer- Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim Tan.
ence resolvers, our system yields improvements on 2011. A unified event coreference resolution by inte-
the B3, CEAF, and CoNLL measures. The results grating multiple resolvers. In Proceedings of IJCNLP,
are still far from the upper bound achievable by an pages 102–110.
oracle. However, our research highlights the incon- Greg Durrett and Dan Klein. 2014. A joint model for en-
sistencies in the annotation of discourse deixis in tity analysis: Coreference, typing, and linking. Trans-
existing resources, and thus the performance of our actions of the Association for Computational Linguis-
system is likely underestimated. tics, 2:477–490.
These inconsistencies call for future work to im- Miriam Eckert and Michael Strube. 2000. Dialogue acts,
prove existing annotated corpora so that similar sys- synchronizing units, and anaphora resolution. Journal
tems may be more fairly evaluated. Regarding our of Semantics, 17(1):51–89.
approach, a tighter integration between the NP and Eraldo Rezende Fernandes, Cícero Nogueira dos Santos,
discourse deixis components could help them make and Ruy Luiz Milidiú. 2012. Latent structure per-
more informed decisions. Other future research in- ceptron with feature induction for unrestricted coref-
cludes jointly learning the classification and reso- erence resolution. In Proceedings of CoNLL: Shared
lution stages of our system, and exploring semi- Task, pages 41–48.
supervised learning techniques to compensate for Gill Francis. 1994. Labelling discourse: An aspect of
the paucity of annotated data. Finally, we would like nominal-group lexical cohesion. In M. Coulthard, ed-
to transfer our system to other languages. itor, Advances in Written Text Analysis, pages 83–101.
Acknowledgments Routledge, London.
This work was done when the first two authors were Philipp Koehn. 2004. Statistical significance tests for
interns at Google. We would like to thank Greg machine translation evaluation. In Proceedings of
Durrett and Anders Björkelund for kindly sharing EMNLP, pages 388–395.
the outputs of their systems. Thanks also go to Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Rajhans Samdani and the anonymous reviewers for Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
their valuable comments and suggestions to improve Stanford’s multi-pass sieve coreference resolution sys-
the quality of the paper. tem at the CoNLL-2011 Shared Task. In Proceedings
References of CoNLL: Shared Task, pages 28–34.
Anders Björkelund and Jonas Kuhn. 2014. Learning Catherine Macleod, Ralph Grishman, Adam Meyers,
structured perceptrons for coreference resolution with Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
latent antecedents and non-local features. In Proceed- A lexicon of nominalizations. In Proceedings of EU-
</bodyText>
<reference confidence="0.959144086206897">
ings of ACL, pages 47–57. RALEX, pages 187–193.
Simon Philip Botley. 2006. Indirect anaphora: Testing Joseph F. McCarthy and Wendy G. Lehnert. 1995. Using
the limits of corpus-based linguistics. International decision trees for coreference resolution. In Proceed-
Journal of Corpus Linguistics, 11(1):73–112. ings of IJCAI, pages 1060–1065.
Donna K Byron and James F Allen. 1998. Resolv- Shachar Mirkin, Jonathan Berant, Ido Dagan, and Eyal
ing demonstrative anaphora in the TRAINS93 corpus. Shnarch. 2010. Recognising entailment within dis-
307 course. In Proceedings of COLING, pages 770–778.
Christoph Müller. 2007. Resolving it, this, and that
in unrestricted multi-party dialog. In Proceedings of
ACL, pages 816–823.
Costanza Navarretta and Sussi Olsen. 2008. Annotating
abstract pronominal anaphora in the DAD project. In
Proceedings of LREC, pages 2046–2052.
Costanza Navarretta. 2011. Antecedent and referent
types of abstract pronominal anaphora. In Proceed-
ings of the Workshop Beyond Semantics: Corpus-
based investigations of pragmatic and discourse phe-
nomena, pages 99–10.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of
ACL, pages 1396–1411.
Massimo Poesio and Ron Artstein. 2008. Anaphoric
annotation in the ARRAU corpus. In Proceedings of
LREC, pages 1170–1174.
Martin F Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events in
OntoNotes. In Proceedings of ICSC, pages 446–453.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of CoNLL:
Shared Task, pages 1–40.
Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring coreference partitions of predicted mentions:
A reference implementation. In Proceedings of ACL,
pages 30–35.
Marta Recasens. 2008. Discourse deixis and coref-
erence: Evidence from AnCora. In Proceedings of
the 2nd Workshop on Anaphora Resolution (WAR II),
pages 73–82.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How, pages 52–57.
Josef Steinberger, Massimo Poesio, Mijail A Kabadjov,
and Karel Ježek. 2007. Two uses of anaphora res-
olution in summarization. Information Processing &amp;
Management, 43(6):1663–1680.
Michael Strube and Christoph Müller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of ACL, pages 168–175.
Bonnie L Webber. 1988. Discourse deixis: Reference
to discourse segments. In Proceedings of ACL, pages
113–122.
</reference>
<page confidence="0.998572">
308
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.790437">
<title confidence="0.983435">Discourse-Deictic Pronouns: A Two-Stage Approach to Do</title>
<author confidence="0.999371">Sujay Kumar Jauhar</author>
<affiliation confidence="0.999993">Carnegie Mellon University</affiliation>
<address confidence="0.997778">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999747">sjauhar@cs.cmu.edu</email>
<author confidence="0.93186">Edgar</author>
<affiliation confidence="0.866956">Google</affiliation>
<address confidence="0.995591">Mountain View, CA 94043,</address>
<email confidence="0.999787">edgargip@google.com</email>
<abstract confidence="0.999022166666666">Discourse deixis is a linguistic phenomenon in which pronouns have verbal or clausal, rather than nominal, antecedents. Studies have estimated that between 5% and 10% of pronouns in non-conversational data are discourse deictic. However, current coreference resolution systems ignore this phenomenon. This paper presents an automatic system for the detection and resolution of discourse-deictic pronouns. We introduce a two-step approach that first recognizes instances of discourse-deictic pronouns, and then resolves them to their verbal antecedent. Both components rely on linguistically motivated features. We evaluate the components in isolation and in combination with two state-of-the-art coreference resolvers. Results show that our system outperforms several baselines, including the only comparable discourse deixis system, and leads to small but statistically significant improvements over the full coreference resolution systems. An error analysis lays bare the need for a less strict evaluation of this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>