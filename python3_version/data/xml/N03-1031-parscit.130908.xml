<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<note confidence="0.959768">
Proceedings of HLT-NAACL 2003
Main Papers , pp. 157-164
Edmonton, May-June 2003
</note>
<title confidence="0.997202">
Example Selection for Bootstrapping Statistical Parsers
</title>
<author confidence="0.9940555">
Mark Steedman*, Rebecca Hwa§, Stephen Clark*, Miles Osborne*, Anoop Sarkar¶
Julia Hockenmaier*, Paul Ruhlen† Steven Baker$, Jeremiah Crim†
</author>
<affiliation confidence="0.999966">
*School of Informatics, University of Edinburgh
</affiliation>
<email confidence="0.988896">
{steedman,stephenc,julia,osborne}@cogsci.ed.ac.uk
</email>
<affiliation confidence="0.973497">
§Institute for Advanced Computer Studies, University of Maryland
</affiliation>
<email confidence="0.658037">
hwa@umiacs.umd.edu
</email>
<affiliation confidence="0.8831442">
¶School of Computing Science, Simon Fraser University
anoop@cs.sfu.ca
†Center for Language and Speech Processing, Johns Hopkins University
jcrim@jhu.edu,ruhlen@cs.jhu.edu
$Department of Computer Science, Cornell University
</affiliation>
<email confidence="0.985443">
sdb22@cornell.edu
</email>
<sectionHeader confidence="0.997423" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999698125">
This paper investigates bootstrapping for statis-
tical parsers to reduce their reliance on manu-
ally annotated training data. We consider both
a mostly-unsupervised approach, co-training,
in which two parsers are iteratively re-trained
on each other’s output; and a semi-supervised
approach, corrected co-training, in which a
human corrects each parser’s output before
adding it to the training data. The selection of
labeled training examples is an integral part of
both frameworks. We propose several selection
methods based on the criteria of minimizing er-
rors in the data and maximizing training util-
ity. We show that incorporating the utility cri-
terion into the selection method results in better
parsers for both frameworks.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998528023255814">
Current state-of-the-art statistical parsers (Collins, 1999;
Charniak, 2000) are trained on large annotated corpora
such as the Penn Treebank (Marcus et al., 1993). How-
ever, the production of such corpora is expensive and
labor-intensive. Given this bottleneck, there is consider-
able interest in (partially) automating the annotation pro-
cess.
To overcome this bottleneck, two approaches from ma-
chine learning have been applied to training parsers. One
is sample selection (Thompson et al., 1999; Hwa, 2000;
Tang et al., 2002), a variant of active learning (Cohn et al.,
1994), which tries to identify a small set of unlabeled sen-
tences with high training utility for the human to label1.
Sentences with high training utility are those most likely
to improve the parser. The other approach, and the fo-
cus of this paper, is co-training (Sarkar, 2001), a mostly-
unsupervised algorithm that replaces the human by hav-
ing two (or more) parsers label training examples for each
other. The goal is for both parsers to improve by boot-
strapping off each other’s strengths. Because the parsers
may label examples incorrectly, only a subset of their out-
put, chosen by some selection mechanism, is used in or-
der to minimize errors. The choice of selection method
significantly affects the quality of the resulting parsers.
We investigate a novel approach of selecting training
examples for co-training parsers by incorporating the idea
of maximizing training utility from sample selection. The
selection mechanism is integral to both sample selection
and co-training; however, because co-training and sam-
ple selection have different goals, their selection methods
focus on different criteria: co-training typically favors se-
lecting accurately labeled examples, while sample selec-
tion typically favors selecting examples with high train-
ing utility, which often are not sentences that the parsers
already label accurately. In this work, we investigate se-
lection methods for co-training that explore the trade-off
between maximizing training utility and minimizing er-
rors.
Empirical studies were conducted to compare selection
methods under both co-training and a semi-supervised
framework called corrected co-training (Pierce and
Cardie, 2001), in which the selected examples are man-
ually checked and corrected before being added to the
</bodyText>
<footnote confidence="0.956642">
1In the context of training parsers, a labeled example is a
sentence with its parse tree. Throughout this paper, we use the
term “label” and “parse” interchangeably.
</footnote>
<bodyText confidence="0.999297777777778">
training data. For co-training, we show that the benefit of
selecting examples with high training utility can offset the
additional errors they contain. For corrected co-training,
we show that selecting examples with high training util-
ity reduces the number of sentences the human annotator
has to check. For both frameworks, we show that selec-
tion methods that maximize training utility find labeled
examples that result in better trained parsers than those
that only minimize error.
</bodyText>
<sectionHeader confidence="0.926861" genericHeader="introduction">
2 Co-training
</sectionHeader>
<bodyText confidence="0.999930317073171">
Blum and Mitchell (1998) introduced co-training to
bootstrap two classifiers with different views of the data.
The two classifiers are initially trained on a small amount
of annotated seed data; then they label unannotated data
for each other in an iterative training process. Blum and
Mitchell prove that, when the two views are conditionally
independent given the label, and each view is sufficient
for learning the task, co-training can boost an initial
weak learner using unlabeled data.
The theory underlying co-training has been extended
by Dasgupta et al. (2002) to prove that, by maximizing
their agreement over the unlabeled data, the two learn-
ers make few generalization errors (under the same in-
dependence assumption adopted by Blum and Mitchell).
Abney (2002) argues that this assumption is extremely
strong and typically violated in the data, and he proposes
a weaker independence assumption.
Goldman and Zhou (2000) show that, through care-
ful selection of newly labeled examples, co-training can
work even when the classifiers’ views do not satisfy
the independence assumption. In this paper we investi-
gate methods for selecting labeled examples produced by
two statistical parsers. We do not explicitly maximize
agreement (along the lines of Abney’s algorithm (2002))
because it is too computationally intensive for training
parsers.
The pseudocode for our co-training framework is given
in Figure 1. It consists of two different parsers and a cen-
tral control that interfaces between the two parsers and
the data. At each co-training iteration, a small set of sen-
tences is drawn from a large pool of unlabeled sentences
and stored in a cache. Both parsers then attempt to label
every sentence in the cache. Next, a subset of the newly
labeled sentences is selected to be added to the train-
ing data. The examples added to the training set of one
parser (referred to as the student) are only those produced
by the other parser (referred to as the teacher), although
the methods we use generalize to the case in which the
parsers share a single training set. During selection, one
parser first acts as the teacher and the other as the student,
and then the roles are reversed.
</bodyText>
<equation confidence="0.833880214285714">
A and B are two different parsers.
MiA and MiB are the models of A and B at step i.
U is a large pool of unlabeled sentences.
Ui is a small cache holding a subset of U at step i.
L is the manually labeled seed data.
LiA and LiB are the labeled training examples for A and B
at step i.
Initialize:
L0A ← L0B ← L.
M0A ← Train(A, L0A)
M0B ← Train(B, L0B)
Loop:
Ui ← Add unlabeled sentences from U.
MiA and MiB parse the sentences in Ui and
</equation>
<tableCaption confidence="0.8922668">
assign scores to them according to their scoring
functions fA and fB.
Select new parses {PA} and {PB} according to some
selection method S, which uses the scores
from fA and fB.
</tableCaption>
<figure confidence="0.9872414">
Li+1
A is LiA augmented with {PB}
Li+1
B is LiB augmented with {PA}
Mi+1
A ← Train(A, Li+1
A )
Mi+1
B ← Train(B, Li+1
B )
</figure>
<figureCaption confidence="0.999984">
Figure 1: The pseudo-code for the co-training algorithm
</figureCaption>
<sectionHeader confidence="0.827563" genericHeader="method">
3 Selecting Training Examples
</sectionHeader>
<bodyText confidence="0.999404">
In each iteration, selection is performed in two steps.
First, each parser uses some scoringfunction, f, to assess
the parses it generated for the sentences in the cache.2
Second, the central control uses some selection method,
S, to choose a subset of these labeled sentences (based on
the scores assigned by f) to add to the parsers’ training
data. The focus of this paper is on the selection phase, but
to more fully investigate the effect of different selection
methods we also consider two possible scoring functions.
</bodyText>
<subsectionHeader confidence="0.999825">
3.1 Scoring functions
</subsectionHeader>
<bodyText confidence="0.999969538461538">
The scoring function attempts to quantify the correctness
of the parses produced by each parser. An ideal scor-
ing function would give the true accuracy rates (e.g., F-
score, the combined labeled precision and recall rates).
In practice, accuracy is approximated by some notion
of confidence. For example, one easy-to-compute scor-
ing function measures the conditional probability of the
(most likely) parse. If a high probability is assigned, the
parser is said to be confident in the label it produced.
In our experimental studies, we considered the selec-
tion methods’ interaction with two scoring functions: an
oracle scoring function fF-score that returns the F-score
of the parse as measured against a gold standard, and a
</bodyText>
<footnote confidence="0.7252635">
2In our experiments, both parsers use the same scoring func-
tion.
</footnote>
<bodyText confidence="0.995526">
practical scoring function fprob that returns the condi-
tional probability of the parse.3
</bodyText>
<subsectionHeader confidence="0.994495">
3.2 Selection methods
</subsectionHeader>
<bodyText confidence="0.998285033333333">
Based on the scores assigned by the scoring function,
the selection method chooses a subset of the parser la-
beled sentences that best satisfy some selection criteria.
One such criterion is the accuracy of the labeled exam-
ples, which may be estimated by the teacher parser’s con-
fidence in its labels. However, the examples that the
teacher correctly labeled may not be those that the stu-
dent needs. We hypothesize that the training utility of
the examples for the student parser is another important
criterion.
Training utility measures the improvement a parser
would make if that sentence were correctly labeled and
added to the training set. Like accuracy, the utility of
an unlabeled sentence is difficult to quantify; therefore,
we approximate it with values that can be computed from
features of the sentence. For example, sentences contain-
ing many unknown words may have high training util-
ity; so might sentences that a parser has trouble parsing.
Under the co-training framework, we estimate the train-
ing utility of a sentence for the student by comparing the
score the student assigned to its parse (according to its
scoring function) against the score the teacher assigned
to its own parse.
To investigate how the selection criteria of utility and
accuracy affect the co-training process, we considered a
number of selection methods that satisfy the requirements
of accuracy and training utility to varying degrees. The
different selection methods are shown below. For each
method, a sentence (as labeled by the teacher parser) is
selected if:
</bodyText>
<listItem confidence="0.979513857142857">
• above-n (Sabove-n): the score of the teacher’s parse
(using its scoring function) ≥ n.
• difference (Sdiff-n): the score of the teacher’s parse
is greater than the score of the student’s parse by
some threshold n.
• intersection (Sint-n): the score of the teacher’s parse
is in the set of the teacher’s n percent highest-
</listItem>
<bodyText confidence="0.926613833333333">
scoring labeled sentences, and the score of the stu-
dent’s parse for the same sentence is in the set of
the student’s n percent lowest-scoring labeled sen-
tences.
Each selection method has a control parameter, n, that
determines the number of labeled sentences to add at each
co-training iteration. It also serves as an indirect control
3A nice property of using conditional probability,
Pr(parse|sentence), as the scoring function is that it
normalizes for sentence length.
of the number of errors added to the training set. For ex-
ample, the Sabove-n method would allow more sentences
to be selected if n was set to a low value (with respect to
the scoring function); however, this is likely to reduce the
accuracy rate of the training set.
The above-n method attempts to maximize the accu-
racy of the data (assuming that parses with higher scores
are more accurate). The difference method attempts to
maximize training utility: as long as the teacher’s label-
ing is more accurate than that of the student, it is cho-
sen, even if its absolute accuracy rate is low. The inter-
section method attempts to maximize both: the selected
sentences are accurately labeled by the teacher and incor-
rectly labeled by the student.
</bodyText>
<sectionHeader confidence="0.999716" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999442666666667">
Experiments were performed to compare the effect of
the selection methods on co-training and corrected co-
training. We consider a selection method, S1, superior
to another, S2, if, when a large unlabeled pool of sen-
tences has been exhausted, the examples selected by S1
(as labeled by the machine, and possibly corrected by the
human) improve the parser more than those selected by
S2. All experiments shared the same general setup, as
described below.
</bodyText>
<subsectionHeader confidence="0.985235">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.98178576">
For two parsers to co-train, they should generate com-
parable output but use independent statistical models.
In our experiments, we used a lexicalized context free
grammar parser developed by Collins (1999), and a lex-
icalized Tree Adjoining Grammar parser developed by
Sarkar (2002). Both parsers were initialized with some
seed data. Since the goal is to minimize human annotated
data, the size of the seed data should be small. In this pa-
per we used a seed set size of 1, 000 sentences, taken from
section 2 of the Wall Street Journal (WSJ) Penn Tree-
bank. The total pool of unlabeled sentences was the re-
mainder of sections 2-21 (stripped of their annotations),
consisting of about 38,000 sentences. The cache size is
set at 500 sentences. We have explored using different
settings for the seed set size (Steedman et al., 2003).
The parsers were evaluated on unseen test sentences
(section 23 of the WSJ corpus). Section 0 was used as
a development set for determining parameters. The eval-
uation metric is the Parseval F-score over labeled con-
stituents: F-score = 2 LR+L P P, where LP and LR
R+L
are labeled precision and recall rate, respectively. Both
parsers were evaluated, but for brevity, all results reported
here are for the Collins parser, which received higher Par-
seval scores.
</bodyText>
<figure confidence="0.999895612903226">
0 2000 4000 6000 8000 10000 12000
Number of Training Sentences
0 2000 4000 6000 8000 10000 12000
Number of Training Sentences
above-70%
diff-10%
int-60%
No selection(Human annotated)
above-90%
diff-10%
int-30%
No selection(Human annotated)
Parsing Accuracy on Test Data (Fscore) 84
83.5
83
82.5
82
81.5
81
80.5
80
Parsing Accuracy on Test Data (Fscore) 84
83.5
83
82.5
82
81.5
81
80.5
80
(a) (b)
</figure>
<figureCaption confidence="0.999115">
Figure 2: A comparison of selection methods using the oracle scoring function, fF-score, controlling for the label
</figureCaption>
<bodyText confidence="0.773746">
quality of the training data. (a) The average accuracy rates are about 85%. (b) The average accuracy rates (except for
those selected by Sdiff-10%) are about 95%.
</bodyText>
<subsectionHeader confidence="0.972043">
4.2 Experiment 1: Selection Methods and
Co-Training
</subsectionHeader>
<bodyText confidence="0.999630227272727">
We first examine the effect of the three selection meth-
ods on co-training without correction (i.e., the chosen
machine-labeled training examples may contain errors).
Because the selection decisions are based on the scores
that the parsers assign to their outputs, the reliability of
the scoring function has a significant impact on the per-
formance of the selection methods. We evaluate the ef-
fectiveness of the selection methods using two scoring
functions. In Section 4.2.1, each parser assesses its out-
put with an oracle scoring function that returns the Par-
seval F-score of the output (as compared to the human
annotated gold-standard). This is an idealized condition
that gives us direct control over the error rate of the la-
beled training data. By keeping the error rates constant,
our goal is to determine which selection method is more
successful in finding sentences with high training utility.
In Section 4.2.2 we replace the oracle scoring function
with fprob, which returns the conditional probability of
the best parse as the score. We compare how the selection
methods’ performances degrade under the realistic con-
dition of basing selection decisions on unreliable parser
output assessment scores.
</bodyText>
<subsectionHeader confidence="0.856272">
4.2.1 Using the oracle scoring function, fF-score
</subsectionHeader>
<bodyText confidence="0.999488444444444">
The goal of this experiment is to evaluate the selection
methods using a reliable scoring function. We therefore
use an oracle scoring function, fF-score, which guaran-
tees a perfect assessment of the parser’s output. This,
however, may be too powerful. In practice, we expect
even a reliable scoring function to sometimes assign high
scores to inaccurate parses. We account for this effect by
adjusting the selection method’s control parameter to af-
fect two factors: the accuracy rate of the newly labeled
training data, and the number of labeled sentences added
at each training iteration. A relaxed parameter setting
adds more parses to the training data, but also reduces
the accuracy of the training data.
Figure 2 compares the effect of the three selection
methods on co-training for the relaxed (left graph) and
the strict (right graph) parameter settings. Each curve in
the two graphs charts the improvement in the parser’s ac-
curacy in parsing the test sentences (y-axis) as it is trained
on more data chosen by its selection method (x-axis).
The curves have different endpoints because the selection
methods chose a different number of sentences from the
same 38K unlabeled pool. For reference, we also plotted
the improvement of a fully-supervised parser (i.e., trained
on human-annotated data, with no selection).
For the more relaxed setting, the parameters are chosen
so that the newly labeled training data have an average
accuracy rate of about 85%:
</bodyText>
<listItem confidence="0.958336454545454">
• Sabove-70% requires the labels to have an F-score ≥
70%. It adds about 330 labeled sentences (out of the
500 sentence cache) with an average accuracy rate
of 85% to the training data per iteration.
• Sdiff-10% requires the score difference between the
teacher’s labeling and the student’s labeling to be at
least 10%. It adds about 50 labeled sentences with
an average accuracy rate of 80%.
• Sint-60% requires the teacher’s parse to be in the
top 60% of its output and the student’s parse for the
same sentence to be in its bottom 60%. It adds about
</listItem>
<bodyText confidence="0.964212">
150 labeled sentences with an average accuracy rate
of 85%.
Although none rivals the parser trained on human an-
notated data, the selection method that improves the
parser the most is Sdiff-10%. One interpretation is that
the training utility of the examples chosen by Sdiff-10%
outweighs the cost of errors introduced into the training
data. Another interpretation is that the other two selection
methods let in too many sentences containing errors. In
the right graph, we compare the same Sdiff-10% with the
other two selection methods using stricter control, such
that the average accuracy rate for these methods is now
about 95%:
</bodyText>
<listItem confidence="0.981123857142857">
• Sabove-90% now requires the parses to be at least
90% correct. It adds about 150 labeled sentences
per iteration.
• Sint-30% now requires the teacher’s parse to be in
the top 30% of its output and the student’s parse for
the same sentence in its bottom 30%. It adds about
15 labeled sentences.
</listItem>
<bodyText confidence="0.999973">
The stricter control on Sabove-90% improved the
parser’s performance, but not enough to overtake
Sdiff-10% after all the sentences in the unlabeled pool
had been considered, even though the training data of
Sdiff-10% contained many more errors. Sint-30% has a
faster initial improvement4, closely tracking the progress
of the fully-supervised parser. However, the stringent re-
quirement exhausted the unlabeled data pool before train-
ing the parser to convergence. Sint-30% might continue
to help the parser to improve if it had access to more un-
labeled data, which is easier to acquire than annotated
data5.
Comparing the three selection methods under both
strict and relaxed control settings, the results suggest that
training utility is an important criterion in selecting train-
ing examples, even at the cost of reduced accuracy.
</bodyText>
<subsectionHeader confidence="0.866069">
4.2.2 Using the fprob scoring function
</subsectionHeader>
<bodyText confidence="0.9997914">
To determine the effect of unreliable scores on the se-
lection methods, we replace the oracle scoring function,
fF-score, with fprob, which approximates the accuracy
of a parse with its conditional probability. Although this
is a poor estimate of accuracy (especially when computed
from a partially trained parser), it is very easy to compute.
The unreliable scores also reduce the correlation between
the selection control parameters and the level of errors in
the training data. In this experiment, we set the parame-
ters for all three selection methods so that approximately
</bodyText>
<footnote confidence="0.97093025">
4A fast improvement rate is not a central concern here, but
it will be more relevant for corrected co-training.
5This oracle experiment is bounded by the size of the anno-
tated portion of the WSJ corpus.
</footnote>
<figure confidence="0.9711555">
1000 1500 2000 2500 3000 3500 4000 4500 5000
Number of Training Sentences
</figure>
<figureCaption confidence="0.9665315">
Figure 3: A comparison of selection methods using the
conditional probability scoring function, fprob.
</figureCaption>
<bodyText confidence="0.9934528">
30-50 sentences were added to the training data per iter-
ation. The average accuracy rate of the training data for
Sabove-70% was about 85%, and the rate for Sdiff-30%
and Sint-30% was about 75%.
As expected, the parser performances of all three selec-
tion methods using fprob (shown in Figure 3) are lower
than using fF-score (see Figure 2). However, Sdiff-30%
and Sint-30% helped the co-training parsers to improve
with a 5% error reduction (1% absolute difference) over
the parser trained only on the initial seed data. In con-
trast, despite an initial improvement, using Sabove-70%
did not help to improve the parser. In their experiments on
NP identifiers, Pierce and Cardie (2001) observed a sim-
ilar effect. They hypothesize that co-training does not
scale well for natural language learning tasks that require
a huge amount of training data because too many errors
are accrued over time. Our experimental results suggest
that the use of training utility in the selection process can
make co-training parsers more tolerant to these accumu-
lated errors.
</bodyText>
<subsectionHeader confidence="0.998807">
4.3 Experiment 2: Selection Methods and
Corrected Co-training
</subsectionHeader>
<bodyText confidence="0.999938428571429">
To address the problem of the training data accumulating
too many errors over time, Pierce and Cardie proposed
a semi-supervised variant of co-training called corrected
co-training, which allows a human annotator to review
and correct the output of the parsers before adding it to
the training data. The main selection criterion in their
co-training system is accuracy (approximated by confi-
dence). They argue that selecting examples with nearly
correct labels would require few manual interventions
from the annotator.
We hypothesize that it may be beneficial to consider
the training utility criterion in this framework as well.
We perform experiments to determine whether select-
ing fewer (and possibly less accurately labeled) exam-
</bodyText>
<figure confidence="0.992304219512195">
81.2
81
80.8
80.6
80.4
80.2
80
79.8
above-70%
diff-30%
int-30%
Parsing Accuracy on Test Data (Fscore)
Parsing Accuracy on Test Data (Fscore)
87
86
85
84
83
82
81
80
above-90%
diff-10%
int-30%
No selection
Parsing Accuracy on Test Data (Fscore)
87
86
85
84
83
82
81
80
above-90%
diff-10%
int-30%
No selection
2000 4000 6000 8000 10000 12000 0 5000 10000 15000 20000 25000 30000 35000 40000 45000
Number of Training Sentences Number of Constituents to Correct in the Training Data
(a) (b)
</figure>
<figureCaption confidence="0.9875815">
Figure 4: A comparison of selection methods for corrected co-training using fF-score (a) in terms of the number of
sentences added to the training data; (b) in terms of the number of manually corrected constituents.
</figureCaption>
<bodyText confidence="0.950566166666667">
ples with higher training utility would require less effort
from the annotator. In our experiments, we simulated
the interactive sample selection process by revealing the
gold standard. As before, we compare the three selection
methods using both fF-score and fprob as scoring func-
tions.6
</bodyText>
<subsectionHeader confidence="0.950133">
4.3.1 Using the oracle scoring function, fF-score
</subsectionHeader>
<bodyText confidence="0.982171303030303">
Figure 4 shows the effect of the three selection meth-
ods (using the strict parameter setting) on corrected co-
training. As a point of reference, we plot the improve-
ment rate for a fully supervised parser (same as the one
in Figure 2). In addition to charting the parser’s perfor-
mance in terms of the number of labeled training sen-
tences (left graph), we also chart the parser’s performance
in terms of the the number of constituents the machine
mislabeled (right graph). The pair of graphs indicates the
amount of human effort required: the left graph shows
the number of sentences the human has to check, and the
right graph shows the number of constituents the human
has to correct.
Comparing Sabove-90% and Sdiff-10%, we see that
Sdiff-10% trains a better parser than Sabove-90% when all
the unlabeled sentences have been considered. It also im-
proves the parser using a smaller set of training exam-
ples. Thus, for the same parsing performance, it requires
the human to check fewer sentences than Sabove-90% and
the reference case of no selection (Figure 4(a)). On the
other hand, because the labeled sentences selected by
Sdiff-10% contain more mistakes than those selected by
Sabove-90%, Sdiff-10%
requires slightly more corre
ous set of experi
ments, using the strict setting (i.e., Figure 2(b))
for fF-score.
than Sabove-90% for the same level of parsing perfor-
mance; though both require fewer corrections than the
reference case of no selection (Figure 4(b)). Because
the amount of effort spent by the annotator depends on
the number of sentences checked as well as the amount
of corrections made, whether
</bodyText>
<equation confidence="0.689008">
or Sabove-90% is
</equation>
<bodyText confidence="0.999487583333333">
more effort reducing may be a matter of the
preference.
The selection method that improves the parser at the
fastest rate is
For the same parser performance
level, it selects the fewest number of sentences fora hu-
man to check and requires the human to make the least
number of corrections. However, as we have seen in the
earlier experiment, very few sentences in the unlabeled
pool satisfy its stringent criteria, so it ran out of data be-
fore the parser was trained to convergence. At this point
we cannot determine whether Sint-30% mi
</bodyText>
<equation confidence="0.315604333333333">
Sdiff-10%
annotator’s
Sint-30%.
</equation>
<bodyText confidence="0.947287">
ght continue to
improve the parser if we used a larger set of unlabeled
data.
</bodyText>
<footnote confidence="0.679762">
6The selection control parameters are the same as the previ-
</footnote>
<subsectionHeader confidence="0.965131">
4.3.2 Using the fprob scoring function
</subsectionHeader>
<bodyText confidence="0.968235173913043">
We also consider the effect of unreliable scores in the
corrected co-training framework. A comparison between
the selection methods using fprob is reported in Figure
5. The left graph charts parser performance in terms of
the number of sentences the human must check; the right
charts parser performance in terms of the number of con-
stituents the human must correct. As expected, the unreli-
able scoring function degrades the effectiveness of the se-
lection methods; however, compared to its unsupervised
counterpart (Figure 3), the degradation is not as severe.
In fact,
and Sint-30% still require fewer train-
ing data than the reference parser. Moreover, consistent
with the other experi
Sdiff-30%
ments, the selection methods that at-
tempt to maximize training utility achieve better parsing
ctions
performance than Sabove-70%. Finally, in terms of reduc-
ing human effort, the three selection methods require the
human to correct comparable amount of parser errors for
the same level of parsing performance, but for Sdiff-30%
and Sint-30%, fewer sentences need to be checked.
</bodyText>
<sectionHeader confidence="0.640909" genericHeader="method">
4.3.3 Discussion
</sectionHeader>
<bodyText confidence="0.999900961538462">
Corrected co-training can be seen as a form of active
learning, whose goal is to identify the smallest set of un-
labeled data with high training utility for the human to
label. Active learning can be applied to a single learner
(Lewis and Catlett, 1994) and to multiple learners (Fre-
und et al., 1997; Engelson and Dagan, 1996; Ngai and
Yarowsky, 2000). In the context of parsing, all previ-
ous work (Thompson et al., 1999; Hwa, 2000; Tang et
al., 2002) has focussed on single learners. Corrected co-
training is the first application of active learning for mul-
tiple parsers. We are currently investigating comparisons
to the single learner approaches.
Our approach is similar to co-testing (Muslea et al.,
2002), an active learning technique that uses two classi-
fiers to find contentious examples (i.e., data for which the
classifiers’ labels disagree) for a human to label. There is
a subtle but significant difference, however, in that their
goal is to reduce the total number of labeled training ex-
amples whereas we also wish to reduce the number of
corrections made by the human. Therefore, our selection
methods must take into account the quality of the parse
produced by the teacher in addition to how different its
parse is from the one produced by the student. The inter-
section method precisely aims at selecting sentences that
satisfy both requirements. Exploring different selection
methods is part of our on-going research effort.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999988763157895">
We have considered three selection methods that have dif-
ferent priorities in balancing the two (often competing)
criteria of accuracy and training utility. We have em-
pirically compared their effect on co-training, in which
two parsers label data for each other, as well as corrected
co-training, in which a human corrects the parser labeled
data before adding it to the training set. Our results sug-
gest that training utility is an important selection criterion
to consider, even at the cost of potentially reducing the ac-
curacy of the training data. In our empirical studies, the
selection method that aims to maximize training utility,
Sdiff-n, consistently finds better examples than the one
that aims to maximize accuracy, Sabove-n. Our results
also suggest that the selection method that aims to maxi-
mize both accuracy and utility, Sint-n, shows promise in
improving co-training parsers and in reducing human ef-
fort for corrected co-training; however, a much larger un-
labeled data set is needed to verify the benefit of Sint-n.
The results of this study indicate the need for scor-
ing functions that are better estimates of the accuracy of
the parser’s output than conditional probabilities. Our
oracle experiments show that, by using effective selec-
tion methods, the co-training process can improve parser
peformance even when the newly labeled parses are
not completely accurate. This suggests that co-training
may still be beneficial when using a practical scoring
function that might only coarsely distinguish accurate
parses from inaccurate parses. Further avenues to ex-
plore include the development of selection methods to
efficiently approximate maximizing the objective func-
tion of parser agreement on unlabeled data, following the
work of Dasgupta et al. (2002) and Abney (2002). Also,
co-training might be made more effective if partial parses
were used as training data. Finally, we are conducting ex-
periments to compare corrected co-training with other ac-
tive learning methods. We hope these studies will reveal
ways to combine the strengths of co-training and active
learning to make better use of unlabeled data.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999115">
This work has been supported, in part, by NSF/DARPA
funded 2002 Human Language Engineering Workshop
at JHU, EPSRC grant GR/M96889, the Department of
Defense contract RD-02-5700, and ONR MURI Con-
tract FCPO.810548265. We would like to thank Chris
Callison-Burch, Michael Collins, John Henderson, Lil-
lian Lee, Andrew McCallum, and Fernando Pereira for
helpful discussions; to Ric Crabbe, Adam Lopez, the par-
ticipants of CS775 at Cornell University, and the review-
ers for their comments on this paper.
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998620944444444">
Steven Abney. 2002. Bootstrapping. In Proceedings of the
40th Annual Meeting of the Association for Computational
Linguistics, pages 360–367, Philadelphia, PA.
Avrim Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings of the
11th Annual Conference on Computational Learning Theory,
pages 92–100, Madison, WI.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Annual Meeting of the NAACL.
David Cohn, Les Atlas, and Richard Ladner. 1994. Improv-
ing generalization with active learning. Machine Learning,
15(2):201–221.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
Sanjoy Dasgupta, Michael Littman, and David McAllester.
2002. PAC generalization bounds for co-training. In T. G.
Dietterich, S. Becker, and Z. Ghahramani, editors, Advances
</reference>
<figure confidence="0.991923933333334">
Parsing Accuracy on Test Data (Fscore)
2000 4000 6000 8000 10000 12000
Number of Training Sentences
above-70%
diff-30%
int-30%
No selection
0 5000 10000 15000 20000 25000 30000 35000 40000 45000
Number of Constituents to Correct in the Training Data
87
86
85
84
83
82
81
80
above-70%
diff-30%
int-30%
No selection
Parsing Accuracy on Test Data (Fscore) 87
86
85
84
83
82
81
80
(a) (b)
</figure>
<figureCaption confidence="0.993362">
Figure 5: A comparison of selection methods for corrected co-training using fprob (a) in terms of the number of
sentences added to the training data; (b) in terms of the number of manually corrected constituents.
</figureCaption>
<reference confidence="0.998978732142857">
in Neural Information Processing Systems 14, Cambridge,
MA. MIT Press.
Sean P. Engelson and Ido Dagan. 1996. Minimizing manual
annotation cost in supervised training from copora. In Pro-
ceedings of the 34th Annual Meeting of the ACL, pages 319–
326.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali
Tishby. 1997. Selective sampling using the query by com-
mittee algorithm. Machine Learning, 28(2-3):133–168.
Sally Goldman and Yan Zhou. 2000. Enhancing supervised
learning with unlabeled data. In Proceedings of the 17th In-
ternational Conference on Machine Learning, Stanford, CA.
Rebecca Hwa. 2000. Sample selection for statistical grammar
induction. In Proceedings of the 2000 Joint SIGDAT Confer-
ence on EMNLP and VLC, pages 45–52, Hong Kong, China,
October.
David D. Lewis and Jason Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In Proceedings
of the Eleventh International Conference on Machine Learn-
ing, pages 148–156.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313–330.
Ion Muslea, Steve Minton, and Craig Knoblock. 2002. Selec-
tive sampling with redundant views. In Proceedings of the
Seventeenth National Conference on Artificial Intelligence,
pages 621–626.
Grace Ngai and David Yarowsky. 2000. Rule writing or an-
notation: Cost-efficient resource usage for base noun phrase
chunking. In Proceedings of the 38th Annual Meeting of the
ACL, pages 117–125, Hong Kong, China, October.
David Pierce and Claire Cardie. 2001. Limitations of co-
training for natural language learning from large datasets. In
Proceedings of the Empirical Methods in NLP Conference,
Pittsburgh, PA.
Anoop Sarkar. 2001. Applying co-training methods to statisti-
cal parsing. In Proceedings of the 2nd Annual Meeting of the
NAACL, pages 95–102, Pittsburgh, PA.
Anoop Sarkar. 2002. Statistical Parsing Algorithms for Lexi-
calized Tree Adjoining Grammars. Ph.D. thesis, University
of Pennsylvania.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark,
Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven
Baker, and Jeremiah Crim. 2003. Bootstrapping statistical
parsers from small datasets. In The Proceedings of the An-
nual Meeting of the European Chapter of the ACL. To ap-
pear.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Active
learning for statistical natural language parsing. In Proceed-
ings of the 40th Annual Meeting of the ACL, pages 120–127,
July.
Cynthia A. Thompson, Mary Elaine Califf, and Raymond J.
Mooney. 1999. Active learning for natural language pars-
ing and information extraction. In Proceedings of ICML-99,
pages 406–414, Bled, Slovenia.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.082118">
<note confidence="0.979482333333333">Proceedings of HLT-NAACL 2003 Main Papers , pp. 157-164 Edmonton, May-June 2003</note>
<title confidence="0.989031">Example Selection for Bootstrapping Statistical Parsers</title>
<author confidence="0.9708135">Rebecca Stephen Miles Anoop Paul Jeremiah</author>
<degree confidence="0.385595">of Informatics, University of for Advanced Computer Studies, University of</degree>
<email confidence="0.945922">hwa@umiacs.umd.edu</email>
<author confidence="0.765118">of Computing Science</author>
<author confidence="0.765118">Simon Fraser</author>
<email confidence="0.899821">anoop@cs.sfu.ca</email>
<author confidence="0.494872">for Language</author>
<author confidence="0.494872">Speech Processing</author>
<author confidence="0.494872">Johns Hopkins</author>
<email confidence="0.988675">jcrim@jhu.edu,ruhlen@cs.jhu.edu</email>
<affiliation confidence="0.754908">of Computer Science, Cornell</affiliation>
<email confidence="0.980437">sdb22@cornell.edu</email>
<abstract confidence="0.998460411764706">This paper investigates bootstrapping for statistical parsers to reduce their reliance on manually annotated training data. We consider both mostly-unsupervised approach, in which two parsers are iteratively re-trained on each other’s output; and a semi-supervised in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<date>2002</date>
<booktitle>Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>360--367</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5208" citStr="Abney (2002)" startWordPosition="767" endWordPosition="768">mall amount of annotated seed data; then they label unannotated data for each other in an iterative training process. Blum and Mitchell prove that, when the two views are conditionally independent given the label, and each view is sufficient for learning the task, co-training can boost an initial weak learner using unlabeled data. The theory underlying co-training has been extended by Dasgupta et al. (2002) to prove that, by maximizing their agreement over the unlabeled data, the two learners make few generalization errors (under the same independence assumption adopted by Blum and Mitchell). Abney (2002) argues that this assumption is extremely strong and typically violated in the data, and he proposes a weaker independence assumption. Goldman and Zhou (2000) show that, through careful selection of newly labeled examples, co-training can work even when the classifiers’ views do not satisfy the independence assumption. In this paper we investigate methods for selecting labeled examples produced by two statistical parsers. We do not explicitly maximize agreement (along the lines of Abney’s algorithm (2002)) because it is too computationally intensive for training parsers. The pseudocode for our</context>
<context position="30119" citStr="Abney (2002)" startWordPosition="4858" endWordPosition="4859">l probabilities. Our oracle experiments show that, by using effective selection methods, the co-training process can improve parser peformance even when the newly labeled parses are not completely accurate. This suggests that co-training may still be beneficial when using a practical scoring function that might only coarsely distinguish accurate parses from inaccurate parses. Further avenues to explore include the development of selection methods to efficiently approximate maximizing the objective function of parser agreement on unlabeled data, following the work of Dasgupta et al. (2002) and Abney (2002). Also, co-training might be made more effective if partial parses were used as training data. Finally, we are conducting experiments to compare corrected co-training with other active learning methods. We hope these studies will reveal ways to combine the strengths of co-training and active learning to make better use of unlabeled data. Acknowledgments This work has been supported, in part, by NSF/DARPA funded 2002 Human Language Engineering Workshop at JHU, EPSRC grant GR/M96889, the Department of Defense contract RD-02-5700, and ONR MURI Contract FCPO.810548265. We would like to thank Chris</context>
</contexts>
<marker>Abney, 2002</marker>
<rawString>Steven Abney. 2002. Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 360–367, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<location>Madison, WI.</location>
<contexts>
<context position="4461" citStr="Blum and Mitchell (1998)" startWordPosition="648" endWordPosition="651">nce with its parse tree. Throughout this paper, we use the term “label” and “parse” interchangeably. training data. For co-training, we show that the benefit of selecting examples with high training utility can offset the additional errors they contain. For corrected co-training, we show that selecting examples with high training utility reduces the number of sentences the human annotator has to check. For both frameworks, we show that selection methods that maximize training utility find labeled examples that result in better trained parsers than those that only minimize error. 2 Co-training Blum and Mitchell (1998) introduced co-training to bootstrap two classifiers with different views of the data. The two classifiers are initially trained on a small amount of annotated seed data; then they label unannotated data for each other in an iterative training process. Blum and Mitchell prove that, when the two views are conditionally independent given the label, and each view is sufficient for learning the task, co-training can boost an initial weak learner using unlabeled data. The theory underlying co-training has been extended by Dasgupta et al. (2002) to prove that, by maximizing their agreement over the </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 92–100, Madison, WI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Annual Meeting of the NAACL.</booktitle>
<contexts>
<context position="1527" citStr="Charniak, 2000" startWordPosition="196" endWordPosition="197">s are iteratively re-trained on each other’s output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st Annual Meeting of the NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Cohn</author>
<author>Les Atlas</author>
<author>Richard Ladner</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>1994</date>
<booktitle>Machine Learning,</booktitle>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="2026" citStr="Cohn et al., 1994" startWordPosition="274" endWordPosition="277">arsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1. Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a subset of their output, chosen by some selection mecha</context>
</contexts>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>David Cohn, Les Atlas, and Richard Ladner. 1994. Improving generalization with active learning. Machine Learning, 15(2):201–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1510" citStr="Collins, 1999" startWordPosition="194" endWordPosition="195">hich two parsers are iteratively re-trained on each other’s output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training util</context>
<context position="12676" citStr="Collins (1999)" startWordPosition="2024" endWordPosition="2025">methods on co-training and corrected cotraining. We consider a selection method, S1, superior to another, S2, if, when a large unlabeled pool of sentences has been exhausted, the examples selected by S1 (as labeled by the machine, and possibly corrected by the human) improve the parser more than those selected by S2. All experiments shared the same general setup, as described below. 4.1 Experimental Setup For two parsers to co-train, they should generate comparable output but use independent statistical models. In our experiments, we used a lexicalized context free grammar parser developed by Collins (1999), and a lexicalized Tree Adjoining Grammar parser developed by Sarkar (2002). Both parsers were initialized with some seed data. Since the goal is to minimize human annotated data, the size of the seed data should be small. In this paper we used a seed set size of 1, 000 sentences, taken from section 2 of the Wall Street Journal (WSJ) Penn Treebank. The total pool of unlabeled sentences was the remainder of sections 2-21 (stripped of their annotations), consisting of about 38,000 sentences. The cache size is set at 500 sentences. We have explored using different settings for the seed set size </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjoy Dasgupta</author>
<author>Michael Littman</author>
<author>David McAllester</author>
</authors>
<title>PAC generalization bounds for co-training. In</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems 14,</booktitle>
<editor>T. G. Dietterich, S. Becker, and Z. Ghahramani, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5006" citStr="Dasgupta et al. (2002)" startWordPosition="733" endWordPosition="736">s than those that only minimize error. 2 Co-training Blum and Mitchell (1998) introduced co-training to bootstrap two classifiers with different views of the data. The two classifiers are initially trained on a small amount of annotated seed data; then they label unannotated data for each other in an iterative training process. Blum and Mitchell prove that, when the two views are conditionally independent given the label, and each view is sufficient for learning the task, co-training can boost an initial weak learner using unlabeled data. The theory underlying co-training has been extended by Dasgupta et al. (2002) to prove that, by maximizing their agreement over the unlabeled data, the two learners make few generalization errors (under the same independence assumption adopted by Blum and Mitchell). Abney (2002) argues that this assumption is extremely strong and typically violated in the data, and he proposes a weaker independence assumption. Goldman and Zhou (2000) show that, through careful selection of newly labeled examples, co-training can work even when the classifiers’ views do not satisfy the independence assumption. In this paper we investigate methods for selecting labeled examples produced </context>
<context position="30102" citStr="Dasgupta et al. (2002)" startWordPosition="4853" endWordPosition="4856">er’s output than conditional probabilities. Our oracle experiments show that, by using effective selection methods, the co-training process can improve parser peformance even when the newly labeled parses are not completely accurate. This suggests that co-training may still be beneficial when using a practical scoring function that might only coarsely distinguish accurate parses from inaccurate parses. Further avenues to explore include the development of selection methods to efficiently approximate maximizing the objective function of parser agreement on unlabeled data, following the work of Dasgupta et al. (2002) and Abney (2002). Also, co-training might be made more effective if partial parses were used as training data. Finally, we are conducting experiments to compare corrected co-training with other active learning methods. We hope these studies will reveal ways to combine the strengths of co-training and active learning to make better use of unlabeled data. Acknowledgments This work has been supported, in part, by NSF/DARPA funded 2002 Human Language Engineering Workshop at JHU, EPSRC grant GR/M96889, the Department of Defense contract RD-02-5700, and ONR MURI Contract FCPO.810548265. We would li</context>
</contexts>
<marker>Dasgupta, Littman, McAllester, 2002</marker>
<rawString>Sanjoy Dasgupta, Michael Littman, and David McAllester. 2002. PAC generalization bounds for co-training. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean P Engelson</author>
<author>Ido Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from copora.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL,</booktitle>
<pages>319--326</pages>
<contexts>
<context position="27201" citStr="Engelson and Dagan, 1996" startWordPosition="4393" endWordPosition="4396">ctions performance than Sabove-70%. Finally, in terms of reducing human effort, the three selection methods require the human to correct comparable amount of parser errors for the same level of parsing performance, but for Sdiff-30% and Sint-30%, fewer sentences need to be checked. 4.3.3 Discussion Corrected co-training can be seen as a form of active learning, whose goal is to identify the smallest set of unlabeled data with high training utility for the human to label. Active learning can be applied to a single learner (Lewis and Catlett, 1994) and to multiple learners (Freund et al., 1997; Engelson and Dagan, 1996; Ngai and Yarowsky, 2000). In the context of parsing, all previous work (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002) has focussed on single learners. Corrected cotraining is the first application of active learning for multiple parsers. We are currently investigating comparisons to the single learner approaches. Our approach is similar to co-testing (Muslea et al., 2002), an active learning technique that uses two classifiers to find contentious examples (i.e., data for which the classifiers’ labels disagree) for a human to label. There is a subtle but significant difference, however</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>Sean P. Engelson and Ido Dagan. 1996. Minimizing manual annotation cost in supervised training from copora. In Proceedings of the 34th Annual Meeting of the ACL, pages 319– 326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>H Sebastian Seung</author>
<author>Eli Shamir</author>
<author>Naftali Tishby</author>
</authors>
<title>Selective sampling using the query by committee algorithm.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>28--2</pages>
<contexts>
<context position="27175" citStr="Freund et al., 1997" startWordPosition="4388" endWordPosition="4392">hieve better parsing ctions performance than Sabove-70%. Finally, in terms of reducing human effort, the three selection methods require the human to correct comparable amount of parser errors for the same level of parsing performance, but for Sdiff-30% and Sint-30%, fewer sentences need to be checked. 4.3.3 Discussion Corrected co-training can be seen as a form of active learning, whose goal is to identify the smallest set of unlabeled data with high training utility for the human to label. Active learning can be applied to a single learner (Lewis and Catlett, 1994) and to multiple learners (Freund et al., 1997; Engelson and Dagan, 1996; Ngai and Yarowsky, 2000). In the context of parsing, all previous work (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002) has focussed on single learners. Corrected cotraining is the first application of active learning for multiple parsers. We are currently investigating comparisons to the single learner approaches. Our approach is similar to co-testing (Muslea et al., 2002), an active learning technique that uses two classifiers to find contentious examples (i.e., data for which the classifiers’ labels disagree) for a human to label. There is a subtle but signi</context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1997</marker>
<rawString>Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali Tishby. 1997. Selective sampling using the query by committee algorithm. Machine Learning, 28(2-3):133–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sally Goldman</author>
<author>Yan Zhou</author>
</authors>
<title>Enhancing supervised learning with unlabeled data.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th International Conference on Machine Learning,</booktitle>
<location>Stanford, CA.</location>
<contexts>
<context position="5366" citStr="Goldman and Zhou (2000)" startWordPosition="789" endWordPosition="792"> when the two views are conditionally independent given the label, and each view is sufficient for learning the task, co-training can boost an initial weak learner using unlabeled data. The theory underlying co-training has been extended by Dasgupta et al. (2002) to prove that, by maximizing their agreement over the unlabeled data, the two learners make few generalization errors (under the same independence assumption adopted by Blum and Mitchell). Abney (2002) argues that this assumption is extremely strong and typically violated in the data, and he proposes a weaker independence assumption. Goldman and Zhou (2000) show that, through careful selection of newly labeled examples, co-training can work even when the classifiers’ views do not satisfy the independence assumption. In this paper we investigate methods for selecting labeled examples produced by two statistical parsers. We do not explicitly maximize agreement (along the lines of Abney’s algorithm (2002)) because it is too computationally intensive for training parsers. The pseudocode for our co-training framework is given in Figure 1. It consists of two different parsers and a central control that interfaces between the two parsers and the data. </context>
</contexts>
<marker>Goldman, Zhou, 2000</marker>
<rawString>Sally Goldman and Yan Zhou. 2000. Enhancing supervised learning with unlabeled data. In Proceedings of the 17th International Conference on Machine Learning, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample selection for statistical grammar induction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT Conference on EMNLP and VLC,</booktitle>
<pages>45--52</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="1956" citStr="Hwa, 2000" startWordPosition="263" endWordPosition="264">ility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1. Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples inco</context>
<context position="27307" citStr="Hwa, 2000" startWordPosition="4414" endWordPosition="4415">uman to correct comparable amount of parser errors for the same level of parsing performance, but for Sdiff-30% and Sint-30%, fewer sentences need to be checked. 4.3.3 Discussion Corrected co-training can be seen as a form of active learning, whose goal is to identify the smallest set of unlabeled data with high training utility for the human to label. Active learning can be applied to a single learner (Lewis and Catlett, 1994) and to multiple learners (Freund et al., 1997; Engelson and Dagan, 1996; Ngai and Yarowsky, 2000). In the context of parsing, all previous work (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002) has focussed on single learners. Corrected cotraining is the first application of active learning for multiple parsers. We are currently investigating comparisons to the single learner approaches. Our approach is similar to co-testing (Muslea et al., 2002), an active learning technique that uses two classifiers to find contentious examples (i.e., data for which the classifiers’ labels disagree) for a human to label. There is a subtle but significant difference, however, in that their goal is to reduce the total number of labeled training examples whereas we also wish to re</context>
</contexts>
<marker>Hwa, 2000</marker>
<rawString>Rebecca Hwa. 2000. Sample selection for statistical grammar induction. In Proceedings of the 2000 Joint SIGDAT Conference on EMNLP and VLC, pages 45–52, Hong Kong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Jason Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>In Proceedings of the Eleventh International Conference on Machine Learning,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="27129" citStr="Lewis and Catlett, 1994" startWordPosition="4380" endWordPosition="4383">ethods that attempt to maximize training utility achieve better parsing ctions performance than Sabove-70%. Finally, in terms of reducing human effort, the three selection methods require the human to correct comparable amount of parser errors for the same level of parsing performance, but for Sdiff-30% and Sint-30%, fewer sentences need to be checked. 4.3.3 Discussion Corrected co-training can be seen as a form of active learning, whose goal is to identify the smallest set of unlabeled data with high training utility for the human to label. Active learning can be applied to a single learner (Lewis and Catlett, 1994) and to multiple learners (Freund et al., 1997; Engelson and Dagan, 1996; Ngai and Yarowsky, 2000). In the context of parsing, all previous work (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002) has focussed on single learners. Corrected cotraining is the first application of active learning for multiple parsers. We are currently investigating comparisons to the single learner approaches. Our approach is similar to co-testing (Muslea et al., 2002), an active learning technique that uses two classifiers to find contentious examples (i.e., data for which the classifiers’ labels disagree) for</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>David D. Lewis and Jason Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Proceedings of the Eleventh International Conference on Machine Learning, pages 148–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1614" citStr="Marcus et al., 1993" startWordPosition="209" endWordPosition="212">h, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1. Sentences with high training utility are those most likely to improve the p</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ion Muslea</author>
<author>Steve Minton</author>
<author>Craig Knoblock</author>
</authors>
<title>Selective sampling with redundant views.</title>
<date>2002</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence,</booktitle>
<pages>621--626</pages>
<contexts>
<context position="27584" citStr="Muslea et al., 2002" startWordPosition="4455" endWordPosition="4458">fy the smallest set of unlabeled data with high training utility for the human to label. Active learning can be applied to a single learner (Lewis and Catlett, 1994) and to multiple learners (Freund et al., 1997; Engelson and Dagan, 1996; Ngai and Yarowsky, 2000). In the context of parsing, all previous work (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002) has focussed on single learners. Corrected cotraining is the first application of active learning for multiple parsers. We are currently investigating comparisons to the single learner approaches. Our approach is similar to co-testing (Muslea et al., 2002), an active learning technique that uses two classifiers to find contentious examples (i.e., data for which the classifiers’ labels disagree) for a human to label. There is a subtle but significant difference, however, in that their goal is to reduce the total number of labeled training examples whereas we also wish to reduce the number of corrections made by the human. Therefore, our selection methods must take into account the quality of the parse produced by the teacher in addition to how different its parse is from the one produced by the student. The intersection method precisely aims at </context>
</contexts>
<marker>Muslea, Minton, Knoblock, 2002</marker>
<rawString>Ion Muslea, Steve Minton, and Craig Knoblock. 2002. Selective sampling with redundant views. In Proceedings of the Seventeenth National Conference on Artificial Intelligence, pages 621–626.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>David Yarowsky</author>
</authors>
<title>Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the ACL,</booktitle>
<pages>117--125</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="27227" citStr="Ngai and Yarowsky, 2000" startWordPosition="4397" endWordPosition="4400">bove-70%. Finally, in terms of reducing human effort, the three selection methods require the human to correct comparable amount of parser errors for the same level of parsing performance, but for Sdiff-30% and Sint-30%, fewer sentences need to be checked. 4.3.3 Discussion Corrected co-training can be seen as a form of active learning, whose goal is to identify the smallest set of unlabeled data with high training utility for the human to label. Active learning can be applied to a single learner (Lewis and Catlett, 1994) and to multiple learners (Freund et al., 1997; Engelson and Dagan, 1996; Ngai and Yarowsky, 2000). In the context of parsing, all previous work (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002) has focussed on single learners. Corrected cotraining is the first application of active learning for multiple parsers. We are currently investigating comparisons to the single learner approaches. Our approach is similar to co-testing (Muslea et al., 2002), an active learning technique that uses two classifiers to find contentious examples (i.e., data for which the classifiers’ labels disagree) for a human to label. There is a subtle but significant difference, however, in that their goal is to</context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>Grace Ngai and David Yarowsky. 2000. Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking. In Proceedings of the 38th Annual Meeting of the ACL, pages 117–125, Hong Kong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pierce</author>
<author>Claire Cardie</author>
</authors>
<title>Limitations of cotraining for natural language learning from large datasets.</title>
<date>2001</date>
<booktitle>In Proceedings of the Empirical Methods in NLP Conference,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="3678" citStr="Pierce and Cardie, 2001" startWordPosition="524" endWordPosition="527"> have different goals, their selection methods focus on different criteria: co-training typically favors selecting accurately labeled examples, while sample selection typically favors selecting examples with high training utility, which often are not sentences that the parsers already label accurately. In this work, we investigate selection methods for co-training that explore the trade-off between maximizing training utility and minimizing errors. Empirical studies were conducted to compare selection methods under both co-training and a semi-supervised framework called corrected co-training (Pierce and Cardie, 2001), in which the selected examples are manually checked and corrected before being added to the 1In the context of training parsers, a labeled example is a sentence with its parse tree. Throughout this paper, we use the term “label” and “parse” interchangeably. training data. For co-training, we show that the benefit of selecting examples with high training utility can offset the additional errors they contain. For corrected co-training, we show that selecting examples with high training utility reduces the number of sentences the human annotator has to check. For both frameworks, we show that s</context>
<context position="21202" citStr="Pierce and Cardie (2001)" startWordPosition="3416" endWordPosition="3419">on. The average accuracy rate of the training data for Sabove-70% was about 85%, and the rate for Sdiff-30% and Sint-30% was about 75%. As expected, the parser performances of all three selection methods using fprob (shown in Figure 3) are lower than using fF-score (see Figure 2). However, Sdiff-30% and Sint-30% helped the co-training parsers to improve with a 5% error reduction (1% absolute difference) over the parser trained only on the initial seed data. In contrast, despite an initial improvement, using Sabove-70% did not help to improve the parser. In their experiments on NP identifiers, Pierce and Cardie (2001) observed a similar effect. They hypothesize that co-training does not scale well for natural language learning tasks that require a huge amount of training data because too many errors are accrued over time. Our experimental results suggest that the use of training utility in the selection process can make co-training parsers more tolerant to these accumulated errors. 4.3 Experiment 2: Selection Methods and Corrected Co-training To address the problem of the training data accumulating too many errors over time, Pierce and Cardie proposed a semi-supervised variant of co-training called correct</context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>David Pierce and Claire Cardie. 2001. Limitations of cotraining for natural language learning from large datasets. In Proceedings of the Empirical Methods in NLP Conference, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Annual Meeting of the NAACL,</booktitle>
<pages>95--102</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2299" citStr="Sarkar, 2001" startWordPosition="323" endWordPosition="324">ve. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1. Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a subset of their output, chosen by some selection mechanism, is used in order to minimize errors. The choice of selection method significantly affects the quality of the resulting parsers. We investigate a novel approach of selecting training examples for co-training parsers by incorporating the idea of maximizing training uti</context>
</contexts>
<marker>Sarkar, 2001</marker>
<rawString>Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings of the 2nd Annual Meeting of the NAACL, pages 95–102, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Statistical Parsing Algorithms for Lexicalized Tree Adjoining Grammars.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="12752" citStr="Sarkar (2002)" startWordPosition="2036" endWordPosition="2037">od, S1, superior to another, S2, if, when a large unlabeled pool of sentences has been exhausted, the examples selected by S1 (as labeled by the machine, and possibly corrected by the human) improve the parser more than those selected by S2. All experiments shared the same general setup, as described below. 4.1 Experimental Setup For two parsers to co-train, they should generate comparable output but use independent statistical models. In our experiments, we used a lexicalized context free grammar parser developed by Collins (1999), and a lexicalized Tree Adjoining Grammar parser developed by Sarkar (2002). Both parsers were initialized with some seed data. Since the goal is to minimize human annotated data, the size of the seed data should be small. In this paper we used a seed set size of 1, 000 sentences, taken from section 2 of the Wall Street Journal (WSJ) Penn Treebank. The total pool of unlabeled sentences was the remainder of sections 2-21 (stripped of their annotations), consisting of about 38,000 sentences. The cache size is set at 500 sentences. We have explored using different settings for the seed set size (Steedman et al., 2003). The parsers were evaluated on unseen test sentences</context>
</contexts>
<marker>Sarkar, 2002</marker>
<rawString>Anoop Sarkar. 2002. Statistical Parsing Algorithms for Lexicalized Tree Adjoining Grammars. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Miles Osborne</author>
<author>Anoop Sarkar</author>
<author>Stephen Clark</author>
<author>Rebecca Hwa</author>
<author>Julia Hockenmaier</author>
<author>Paul Ruhlen</author>
<author>Steven Baker</author>
<author>Jeremiah Crim</author>
</authors>
<title>Bootstrapping statistical parsers from small datasets.</title>
<date>2003</date>
<booktitle>In The Proceedings of the Annual Meeting of the European Chapter of the ACL.</booktitle>
<note>To appear.</note>
<contexts>
<context position="13299" citStr="Steedman et al., 2003" startWordPosition="2132" endWordPosition="2135"> and a lexicalized Tree Adjoining Grammar parser developed by Sarkar (2002). Both parsers were initialized with some seed data. Since the goal is to minimize human annotated data, the size of the seed data should be small. In this paper we used a seed set size of 1, 000 sentences, taken from section 2 of the Wall Street Journal (WSJ) Penn Treebank. The total pool of unlabeled sentences was the remainder of sections 2-21 (stripped of their annotations), consisting of about 38,000 sentences. The cache size is set at 500 sentences. We have explored using different settings for the seed set size (Steedman et al., 2003). The parsers were evaluated on unseen test sentences (section 23 of the WSJ corpus). Section 0 was used as a development set for determining parameters. The evaluation metric is the Parseval F-score over labeled constituents: F-score = 2 LR+L P P, where LP and LR R+L are labeled precision and recall rate, respectively. Both parsers were evaluated, but for brevity, all results reported here are for the Collins parser, which received higher Parseval scores. 0 2000 4000 6000 8000 10000 12000 Number of Training Sentences 0 2000 4000 6000 8000 10000 12000 Number of Training Sentences above-70% dif</context>
</contexts>
<marker>Steedman, Osborne, Sarkar, Clark, Hwa, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In The Proceedings of the Annual Meeting of the European Chapter of the ACL. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Tang</author>
<author>Xiaoqiang Luo</author>
<author>Salim Roukos</author>
</authors>
<title>Active learning for statistical natural language parsing.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="1976" citStr="Tang et al., 2002" startWordPosition="265" endWordPosition="268">rion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1. Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a subs</context>
<context position="27327" citStr="Tang et al., 2002" startWordPosition="4416" endWordPosition="4419">rect comparable amount of parser errors for the same level of parsing performance, but for Sdiff-30% and Sint-30%, fewer sentences need to be checked. 4.3.3 Discussion Corrected co-training can be seen as a form of active learning, whose goal is to identify the smallest set of unlabeled data with high training utility for the human to label. Active learning can be applied to a single learner (Lewis and Catlett, 1994) and to multiple learners (Freund et al., 1997; Engelson and Dagan, 1996; Ngai and Yarowsky, 2000). In the context of parsing, all previous work (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002) has focussed on single learners. Corrected cotraining is the first application of active learning for multiple parsers. We are currently investigating comparisons to the single learner approaches. Our approach is similar to co-testing (Muslea et al., 2002), an active learning technique that uses two classifiers to find contentious examples (i.e., data for which the classifiers’ labels disagree) for a human to label. There is a subtle but significant difference, however, in that their goal is to reduce the total number of labeled training examples whereas we also wish to reduce the number of c</context>
</contexts>
<marker>Tang, Luo, Roukos, 2002</marker>
<rawString>Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Active learning for statistical natural language parsing. In Proceedings of the 40th Annual Meeting of the ACL, pages 120–127, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Mary Elaine Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Active learning for natural language parsing and information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of ICML-99,</booktitle>
<pages>406--414</pages>
<location>Bled, Slovenia.</location>
<contexts>
<context position="1945" citStr="Thompson et al., 1999" startWordPosition="259" endWordPosition="262">at incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1. Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label ex</context>
<context position="27296" citStr="Thompson et al., 1999" startWordPosition="4410" endWordPosition="4413">n methods require the human to correct comparable amount of parser errors for the same level of parsing performance, but for Sdiff-30% and Sint-30%, fewer sentences need to be checked. 4.3.3 Discussion Corrected co-training can be seen as a form of active learning, whose goal is to identify the smallest set of unlabeled data with high training utility for the human to label. Active learning can be applied to a single learner (Lewis and Catlett, 1994) and to multiple learners (Freund et al., 1997; Engelson and Dagan, 1996; Ngai and Yarowsky, 2000). In the context of parsing, all previous work (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002) has focussed on single learners. Corrected cotraining is the first application of active learning for multiple parsers. We are currently investigating comparisons to the single learner approaches. Our approach is similar to co-testing (Muslea et al., 2002), an active learning technique that uses two classifiers to find contentious examples (i.e., data for which the classifiers’ labels disagree) for a human to label. There is a subtle but significant difference, however, in that their goal is to reduce the total number of labeled training examples whereas we also</context>
</contexts>
<marker>Thompson, Califf, Mooney, 1999</marker>
<rawString>Cynthia A. Thompson, Mary Elaine Califf, and Raymond J. Mooney. 1999. Active learning for natural language parsing and information extraction. In Proceedings of ICML-99, pages 406–414, Bled, Slovenia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>