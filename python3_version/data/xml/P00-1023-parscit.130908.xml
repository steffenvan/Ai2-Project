<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.255585">
Coreference for NLP Applications
</title>
<author confidence="0.546094">
Thomas S. Morton
</author>
<affiliation confidence="0.9841945">
Department of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.8948">
Philadelphia, PA 19104
</address>
<email confidence="0.999416">
tsmorton@cis.upenn.edu
</email>
<sectionHeader confidence="0.987727" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999806538461539">
This paper presents several tech-
niques for performing automatic
coreference annotation and perfor-
mance results for each of them. To
demonstrate that they can be ap-
plied to real-world data, we have
built a simple question-answering
system which uses the techniques.
A system using coreference is com-
pared to a baseline system with the
result that the addition of the coref-
erence annotation improves perfor-
mance.
</bodyText>
<sectionHeader confidence="0.9956" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999754571428571">
The fact that two pieces of text specify the
same thing in the world can be very help-
ful in a variety of natural language processing
tasks. While there is a vast body of literature
on anaphora resolution (Hobbs, 1976; Lap-
pin and Leass, 1994; Mitkov, 1997; Ge et al.,
1998), many of these techniques require hand-
crafted resources, which are difficult to con-
struct, or unrealistic sources of information as
input to their algorithms Here we present a
series of techniques for performing coreference
between noun phrases which require a limited
amount of phrase structure as input and no
domain-specific knowledge engineering.
</bodyText>
<sectionHeader confidence="0.969531" genericHeader="introduction">
2 Coreference
</sectionHeader>
<bodyText confidence="0.999793357142857">
Coreference annotation involves determining
whether or not two noun phrases are used to
refer to the same thing. While this is a single
task, different types of noun phrases behave
differently in terms of how they co-refer. This
leads us to use different approaches depend-
ing on the type of noun phrase under consid-
eration. Here we consider pronouns, proper
nouns, definite nouns, and appositives. For
each of these classes we determine what the
set of possible antecedents is, a set of factors
which influence whether the two nouns co-
refer with one another, and a decision process
for determining which nouns are coreferring.
</bodyText>
<subsectionHeader confidence="0.98298">
2.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999926555555556">
Before coreference on noun phrases can be
annotated, the noun phrases, themselves,
must first be determined. This processing is
common to most NLP applications and in-
cludes sentence detection, tokenization, part-
of-speech tagging and noun-phrase chunking.
The sentence detector we use is described
in Reynar and Ratnaparkhi (1997), the to-
kenizer in Reynar (1998), and the part-of-
speech tagger in Ratnaparkhi (1996). The
noun-phrase chunking is also done as a tag-
ging task in which tokens are tagged as the
start of a noun phrase, the continuation of a
noun phrase or other. It employs modeling
techniques similar to those used for the part-
of-speech tagger. These tools were all trained
automatically with data from the Penn Tree-
bank (Marcus et al., 1994).
</bodyText>
<subsectionHeader confidence="0.838531">
2.2 Pronoun Model
</subsectionHeader>
<bodyText confidence="0.996235368421052">
Pronoun resolution is the most well-studied of
the types of coreference discussed here. The
pronouns we wish to resolve are all forms of
the singular pronouns, he, she, and it. The
set of possible antecedents we have chosen is
all the basal noun phrases which occur before
the pronoun in the same sentence or in the
previous two sentences. For these pronouns,
and the data we used, a look-back of two sen-
tences makes the antecedent available 98.7%
of the time. A look-back of only one sen-
tence makes the antecedent available 96.9%
of the time, which concurs with the findings
in Hobbs (1976).
The factors we take into account when de-
termining what noun phrases a pronoun is
coreferent with are, in broad terms, locality,
gender, syntax, and accessibility or salience.
These factors however are not independent
and the specific features we use often model
more than one factor. Table 1 lists each of
the features used and which factors we were
attempting to model with them.
Features 1-2 indicate how close the an-
tecedent is to the pronoun. The first fea-
ture counts NPs from right to left and the
second counts them from left to right. We re-
fer to the latter as the Hobbs distance as it
approximates the naive syntax-based ranking
presented in Hobbs (1976). Feature 3 gives
the antecedent&apos;s distance in sentences. It is
paired with the pronoun, which allows the
model to learn that reflexives should be re-
solved in the same sentence. Feature 4 pro-
vides a crude measure of whether or not the
NP is a subject, as the first NP in a sentence
is often its subject. Feature 5 models the syn-
tactic context of the antecedent; it can indi-
cate that the antecedent is within a preposi-
tional phrase or modified by a prepositional
phrase or relative clause. Feature 6 gives a
notion of salience, as an entity which is men-
tioned repeatedly is likely to be pronominal-
ized. Features 7-10 help determine whether
the candidate antecedent is of the correct gen-
der. Thus, the model can learn that a pairing
of Mr. and male gender makes a good an-
tecedent for he, him, or his. The last two fea-
tures are only used when determining whether
or not the pronoun has a referent. These cases
include pleonastic it or any other time a pro-
noun does not refer to a noun phrase.
Using these features and a collection of an-
notated data we have trained a statistical
model to decide when a pronoun is corefer-
ring with another noun phrase or has no ref-
erent. Specifically, we employ the maximum
</bodyText>
<listItem confidence="0.962228333333333">
Features SCL A
1. The distance in NPs be- x
tween the pronoun and the
antecedent
2. Hobbs distance in NPs x x x x
between the pronoun and
the antecedent, and the
pronoun&apos;s gender
3. The distance in sen- x x x
</listItem>
<bodyText confidence="0.991573480519481">
tences between the pro-
noun and the antecedent
and the pronoun
4 The NP&apos;s position in the x x
sentence
5 The word and POS- x x
tag preceding and follow-
ing the antecedent
6 The number of times the x
antecedent has been men-
tioned
7 The head word of an- x
tecedent and the pro-
noun&apos;s gender
8 The head POS-tag of the x
antecedent and the pro-
noun&apos;s gender
9 The modifier words and x
POS-tags of antecedent
and the pronoun&apos;s gender
10 The modifier POS-tags x
of antecedent and the pro-
noun&apos;s gender
11 The word and POS- x
tag preceding and follow-
ing the pronoun
12 The pronoun x
Table 1: Features for a Maximum En-
tropy Pronoun Model and their Motiva-
tion. (S=Syntax, G=Gender, L=Locality,
A=Accessibility)
entropy framework, which allows us to use a
set of binary features, and provides a prob-
ability distribution over the set of possible
outcomes.&apos; The task of resolving pronouns
is not naturally a classification task, since the
set of possible antecedents differs depending
on context. Here we have the model make a
binary decision on pronoun/antecedent pairs,
and then select the pair with the highest prob-
ability. Included as a possible pairing is that
the pronoun is non-referential. For this pair-
ing only Features 11-12 are used. If this pair-
ing has the highest probability then the pro-
noun is left unresolved.
The decision to resolve a particular pro-
noun is not independent of other coreference
decisions. One of the ways these decisions in-
teract is in the computation of Feature 6, the
number of times an antecedent has been men-
tioned. The accuracy of this feature depends
on the accuracy of previous coreference de-
cisions including those which do not involve
pronouns. Another example of this depen-
dence involves the computation of gender for
an antecedent. When a person or company is
introduced, often their name is accompanied
by an honorific or corporate designator which
makes the gender of the entity clear. Later
references to this entity might only include a
single term, sometimes making determination
of gender impossible unless the results of pre-
vious coreference decisions are known. One
way to capture this phenomenon is by merg-
ing antecedents and referents when any coref-
erence relation is posited. When this happens
the features of each noun phrase are merged
in the following way: The head and modifier
words and tags are added to a set of such
words and tags for the entity. Thus features
7-10 generate contextual predicates, used by
the maximum entropy model, based on ev-
ery head and modifier word which has been
found to be coreferent with the antecedent in
question. In contrast, only a single distance
measure is kept, which is based on the refer-
ent with the lowest Hobbs distance. Feature
</bodyText>
<footnote confidence="0.874333666666667">
1Ratnaparkhi (1997) provides a good introduction
on how to use generalized iterative scaling, GIS, to
compute the parameters of these models.
</footnote>
<table confidence="0.996716333333333">
Model P R F
w/o entity merging 94.8 71.5 81.5
with entity merging 94.4 76.0 84.2
</table>
<tableCaption confidence="0.9726755">
Table 2: Precision, Recall, and F-Measure for
Pronoun Evaluation
</tableCaption>
<bodyText confidence="0.4314435">
6, the number of times this entity has been
mentioned is also incremented.
</bodyText>
<subsectionHeader confidence="0.998996">
2.3 Pronoun Evaluation
</subsectionHeader>
<bodyText confidence="0.999990761904762">
We evaluated our model on the data used in
Ge et al. (1998). There are 1307 pronouns in
this corpus which are forms of he, she, or
it. We trained our model on 90% of the data
and tested on the other 10%. A feature had
to occur at least 5 times in order to be in-
cluded in the model, and the model param-
eters were computed using 100 iterations of
GIS. The task here was to determine which
noun phrase the pronoun referred to or that
it was non-referential, given all the previous
coreference relationships. While this task is
not representative of how the model would
be used in practice, namely the other coref-
erence relations are not given and must be
computed, the task allows us to evaluate the
pronoun model in isolation. Average results
for ten-fold cross-validation are presented in
Table 2 for our model with and without the
entity merging described above. We find that
the entity merging improves performance.
</bodyText>
<subsectionHeader confidence="0.998016">
2.4 Proper Noun Rules
</subsectionHeader>
<bodyText confidence="0.995078333333333">
To apply the pronoun model in practice
we need to compute the other types of
coreference that contribute to the mention
count statistics and word or tag/gender pairs.
Coreference between proper nouns is very
common in newswire domains and accounts
for approximately one third of all coreference
relationships (Baldwin et al., 1995). Here we
are concerned only with the coreference rela-
tionships between two proper nouns and not
how proper nouns interact with other nouns
or pronouns. A noun is considered a proper
noun if its last word has the tag &amp;quot;NNP&amp;quot;
or &amp;quot;NNPS&amp;quot;. Proper nouns do not have the
same locality constraints that pronouns have.
</bodyText>
<table confidence="0.83392">
Model P R F
string matching 92.1 88.0 90.0
</table>
<tableCaption confidence="0.976963">
Table 3: Precision, Recall, and F-Measure for
Proper Noun Evaluation
</tableCaption>
<bodyText confidence="0.999882037037037">
When searching for a possible referent we may
have to consider all previous proper nouns as
candidates. Determining these types of rela-
tionships can be done quite accurately with
simple string-matching techniques. The ap-
proach we take is that a proper noun is coref-
erent with a previously occurring proper noun
if the subsequent proper noun is a substring
of the previous one. The later proper noun is
normalized by only considering tokens occur-
ring after and including the first token tagged
as a proper noun which does not match the
patterns &amp;quot;series of letters ending in a period&amp;quot;
or &amp;quot;capital letter followed by sequence of non-
vowels&amp;quot;. This has the effect of generally re-
moving non-proper noun modifiers and hon-
orifics. The same patterns are also applied to
the end of the proper noun to remove corpo-
rate designators. Proper nouns can also occur
as modifiers to common nouns such as in the
example, &amp;quot;A Hitachi spokesman&amp;quot;. These can
be treated in the same way as other proper
nouns. To do this we simply add any sequence
of words tagged with &amp;quot;NNP&amp;quot; or &amp;quot;NNPS&amp;quot; that
modify a head noun tagged with &amp;quot;NN&amp;quot; or
&amp;quot;NNS&amp;quot; to our list of proper nouns and pro-
ceed as we did before.
</bodyText>
<subsectionHeader confidence="0.851929">
2.5 Proper Noun Evaluation
</subsectionHeader>
<bodyText confidence="0.971326">
We performed an evaluation of the above
techniques using 80 hand-annotated Wall
Street Journal articles. The articles contained
726 coreferring proper nouns. Performance
for the above techniques is presented in Ta-
ble 3. This evaluation does not include mod-
ifiers that are proper nouns but we suspect
that such an evaluation would produce sim-
ilar results. Most precision errors involved
type mismatches between entities and could
likely be addressed with a named-entity de-
tector. Recall was primarily hurt by a lack of
treatment of acronyms.
2.6 Common Nouns: Rules and
Model
Coreference between common nouns and
other nouns is a difficult class in general,
but a subset of these cases can be identified
with reasonable precision. The cases we con-
sider are definite noun phrases and appos-
itives. Definite noun phrases indicate that
the referent should be known to the reader
and thus we are likely to find an antecedent
for this noun phrase. We consider a noun
phrase definite if it is modified by the deter-
miner the. For determining coreference be-
tween these noun phrases we employ the ap-
proach of Vieira and Poesio (1997). The first
noun phrase that occurs within the previ-
ous five sentences has a string-equivalent head
word, and no additional modifiers is consid-
ered coreferent with the definite noun phrase.
Another case of common noun coreference
is appositives. The coreference between The
asbestos fiber and crocidolite in the following
sentence is an example of an appositive:
The asbestos fiber, crocidolite, is un-
usually resilient once it enters the
lungs, with even brief exposures to
it causing symptoms that show up
decades later, researchers said.
We consider any two noun phrases separated
by a comma as candidates for being an appos-
itive and use the words the noun phrases con-
tain as well as their syntactic context to deter-
mine whether they are in fact appositives. To
weight these factors we again employ a max-
imum entropy model. The features used by
our model are described in Table 4.
</bodyText>
<subsectionHeader confidence="0.90312">
2.7 Common Noun Evaluation
</subsectionHeader>
<bodyText confidence="0.999881">
For the definite nouns we hand-tagged 80
Wall Street Journal articles containing 568
referring definite nouns. To train and test
our appositive model, we hand-tagged 1000
examples of noun phrases, which were sepa-
rated by a comma, as either appositives or
non-appositives. A feature had to occur at
least 5 times in order to be included in the
model, and the model parameters were com-
puted using 100 iterations of GIS. For testing,
</bodyText>
<table confidence="0.997301666666667">
type P R F
definite NPs 82.5 47.4 60.2
appositives 88.1 79.9 83.8
</table>
<tableCaption confidence="0.9671835">
Table 5: Precision, Recall, and F-Measure for
Common Noun Evaluation
</tableCaption>
<listItem confidence="0.813818666666667">
1. The token or tag preceding the left
noun phrase.
2. The token or tag following the right
noun phrase.
5. Either noun phrase contains the head
word wh.
6. Either noun phrase contains the head
tag th.
7. Either noun phrase contains the modi-
</listItem>
<bodyText confidence="0.917320206896552">
fier word Wm.
8. Either noun phrase contains the modi-
fier tag tm.
9. The left noun phrase contains the token
WI and the right noun phrase contains the
token W.
10. The left noun phrase contains the to-
ken WI and the right noun phrase contains
the tag 4.
11. The left noun phrase contains the tag
t1 and the right noun phrase contains the
token W.
12. The left noun phrase contains the tag
t1 and the right noun phrase contains the
tag 4.
Table 4: Features for a Maximum Entropy
Appositive Model
ten-fold cross-validation was used with 90% of
the data used for training and the other 10%
left for testing. The results for both types
of common nouns are given in Table 5. Re-
call for the definite nouns is quite low. At-
tention to definite nouns which do not share
the same head as their antecedent could im-
prove this. Specifically, modeling &amp;quot;the com-
pany&amp;quot;, which occurs frequently in these ar-
ticles, would help considerably. Appositives
suffer from the fact that only adjacent basal
noun phrases are considered.
</bodyText>
<subsectionHeader confidence="0.965879">
2.8 MUC Evaluation
</subsectionHeader>
<bodyText confidence="0.999761035714286">
The different types of coreference described
above interact with one another as each one
asserts coreference relationships. This occurs
because asserting a coreference relationship
implies that an entity has been mentioned
again. This subsequent mention also effects
how local the entity appears to all the coref-
erence models. Failing to assert a corefer-
ence relationship may cause an entity to be-
come inaccessible to a pronoun or common
noun which is coreferent with it. In order to
determine how these components would per-
form together, we integrated them, and eval-
uated this system using the MUC-6 Corefer-
ence Task (Def, 1995). The two data sets
each consist of 30 Wall Street Journal arti-
cles that have been hand-annotated for coref-
erence relationships. In these articles almost
all coreference between noun phrases has been
marked. Results for this task are presented in
Table 6. These results are based on the scor-
ing algorithm used for MUC-6 and described
in Vilain et al. (1995). Recall for these re-
sults is low because there are many types of
coreference relationships which we have not
attempted to annotate. Precision has re-
mained high, so we can be fairly confident in
the coreference relationships that have been
</bodyText>
<table confidence="0.997724333333333">
data P R F
dry-run 79.3 41.2 54.2
evaluation 79.6 44.5 57.1
</table>
<tableCaption confidence="0.956951">
Table 6: Precision, Recall, and F-Measure for
MUG Evaluation
</tableCaption>
<bodyText confidence="0.795275">
found.
</bodyText>
<subsectionHeader confidence="0.710296">
2.9 Related Work in Coreference
</subsectionHeader>
<bodyText confidence="0.999728361111111">
Our approach for pronoun resolution is sim-
ilar to that of Ge et al. (1998) in the fol-
lowing ways. Both approaches use statisti-
cal modeling to rank antecedents and then
select the top ranked one. The features
used by both these models include the Hobbs
distance and the mention count statistics.
They mainly differ in the following ways.
Our approach requires noun phrases for in-
put while Ge et al. (1998) uses full parse
trees. Ge et al. (1998) does not employ en-
tity merging which we find helps our model.
Most importantly, our model handles non-
referential pronouns while Ge et al. (1998)
excludes them from the data. These differ-
ences are primarily the result of our desire to
use this model in applications. For an ap-
plication, full parsing is computationally ex-
pensive and non-referential pronouns cannot
be excluded from the data. These differences
also lead to the exclusion of some helpful fea-
tures which are employed by Ge et al. (1998)
such as a more accurate computation of the
Hobbs distance, the probability that an an-
tecedent can occur in the same syntactic po-
sition as the pronoun, and the use of unsu-
pervised techniques for gender determination.
The result of these trade-offs is poorer per-
formance at pronoun resolution when com-
pared to Ge et al. (1998).2 The techniques
we used for proper noun coreference are simi-
lar to those used by Baldwin et al. (1995) and
others. For definite nouns we use the ap-
proach of Vieira and Poesio (1997). To our
knowledge, the approach used for determin-
ing appositives is novel.
</bodyText>
<footnote confidence="0.910151">
2Evaluating using only referential pronouns gives
us an accuracy of 79.1% compared to the 84.2% re-
ported in Ge et al. (1998).
</footnote>
<sectionHeader confidence="0.958893" genericHeader="method">
3 Applications
</sectionHeader>
<bodyText confidence="0.999942833333333">
There are numerous applications which use
coreference as a base annotation. These in-
clude work in summarization (Baldwin and
Morton, 1998; Azzam et al., 1999) and ques-
tion answering (Morton, 1999; Breck et al.,
1999).
</bodyText>
<subsectionHeader confidence="0.999914">
3.1 Question Answering
</subsectionHeader>
<bodyText confidence="0.999994304347826">
Here we present results for a question answer-
ing task. Specifically, a system is given a
query and then asked to find a 250-byte an-
swer string from a collection of documents.
Systems generate a ranked list of the top 5
answer strings. This task is the same as
the question answering task used in TREC-
8 and described in Voorhees and Tice (1999).
As a baseline, we have implemented the
system used by AT&amp;T as described in
Singal et al. (1999). A description of how
that system ranks sentences is given in Ta-
ble 7.
When we apply this approach to documents
with coreference relationships annotated we
consider any additional terms which are in
coreference chains with noun phrases in a
sentence to have occurred in that sentence
as well. The terms added via a coreference
chains are given 90% of the weight of regularly
occurring terms. When presenting the top 5
passages, referents are indicated in parenthe-
ses next to the referring terms.
</bodyText>
<subsectionHeader confidence="0.999945">
3.2 Question Answering Evaluation
</subsectionHeader>
<bodyText confidence="0.9999896">
We evaluated both approaches using the 200
questions used in TREC-8 and an evaluation
script designed for these questions and pro-
vided by NIST. The output is scored using
the mean reciprocal rank (MRR).
</bodyText>
<equation confidence="0.953489666666667">
n
E rank(q)
q=1
</equation>
<bodyText confidence="0.9874968">
Here n is the number of questions processed,
and rank(q) is the ranking of the first 250-
byte string which answers the question q. The
results for each approach are presented in Ta-
ble 8.
</bodyText>
<figure confidence="0.281252333333333">
1
Passage Ranking:
1. The top 50 documents for a question
</figure>
<bodyText confidence="0.954976625">
are retrieved using a straight vector match
(no query expansion) .3
2. Each section of these 50 documents is
broken into sentences and each sentence
is assigned a score based on the following
algorithm.
3. The query term weight of every ques-
tion word that appears in the sentence is
added to the sentence score, the passage
size is set to the sentence size (in bytes).
4. If a query word bigram appears in the
sentence, extra credit4 is assigned to the
sentence.
5. If an adjoining sentence contains a
question word not contained in this sen-
tence, and if by adding this adjoining
sentence to the passage, the passage size
doesn&apos;t exceed 500 bytes, half the query
term weight for this word is added to the
sentence score.
6. If the next adjoining sentence contains
a question word not covered yet, and if
by adding this adjoining sentence to the
passage, the passage size doesn&apos;t exceed
</bodyText>
<table confidence="0.847706">
500 bytes, a quarter of the query-term
weight for this word is added to the sen-
tence score.
Answer Selection:
1. A single top ranked sentence is selected
from each document. Ties are broken in
favor of longer sentences.
2. Near-duplicate passages are removed.
If a low-scoring passage has a cosine-
similarity of over 0.50 with a highly
ranked passage, the low-scoring passage is
removed.
3. The top five sentences from the re-
maining ones are printed in order of their
scores. If a sentence is under 250 bytes
the later bytes of the previous sentence
are included and then the earlier bytes of
the next sentence.
Model MRR
baseline 52.3%
coreference 53.8%
</table>
<tableCaption confidence="0.8593545">
Table 8: Mean Reciprocal Rank for TREC-8
Question Answering Task
</tableCaption>
<sectionHeader confidence="0.998646" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999774472222222">
The coreference annotation produces a small
increase in the performance of the question-
and-answer system. Since the baseline es-
timates coreference by including terms with
partial weights from the surrounding sen-
tences, the coreference system will only out-
perform the baseline when it includes query
terms that only occur many sentences away.
This scenario occurs infrequently, which is
why we only see a slight improvement in a
200 question evaluation. Coreference is essen-
tial for finding answers to some of the ques-
tions in this set. For the question, &amp;quot;When did
Beethoven die?&amp;quot; the system with coreference
was able to select the sentence:
Still, as news spread of his
(Beethoven&apos;s) final illness (he
would die of jaundice and ascites on
March 26, 1827), the Philharmonic
Society of London sent him a
get-well gift of 100 pounds.
having resolved the &amp;quot;his&amp;quot; and &amp;quot;he&amp;quot; to
Beethoven while the baseline opted only for
sentences which actually contain the term
&amp;quot;Beethoven&amp;quot;. The coreference annotation de-
scribed here can be used in other ways which
would likely be beneficial to an application.
Filling in referents in the extracted text can
make text much more coherent to the reader.
We see this has been done by the system in
the above example. For question-answering
this may provide an answer, and in summa-
rization this may allow the reader to deter-
mine what the document is about. Corefer-
ence between appositives can help determine
candidate answers for question-answering as
</bodyText>
<tableCaption confidence="0.629348">
Table 7: AT&amp;T Passage Ranking Algorithm
</tableCaption>
<footnote confidence="0.630765666666667">
3These documents were provided by AT&amp;T
O.25 x (lower of the two component query term
weights)
</footnote>
<bodyText confidence="0.999878">
they often provide a category for the noun
phrase they are coreferent with. We plan to
further explore these areas in the future.
</bodyText>
<sectionHeader confidence="0.997868" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999960571428571">
We have presented several techniques for
coreference resolution and evaluated each of
them. We have demonstrated that these tech-
niques can be integrated to provide automatic
coreference annotation on real-world data and
that that annotation can be used to improve
natural language processing applications.
</bodyText>
<sectionHeader confidence="0.999009" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999909113636363">
Saliha Azzam, Kevin Humphreys, and Robert
Gaizauskas. 1999. Using coreference chains
for text summarization. In Proceedings of the
Workshop on Coreference and Its Applications,
College Park, Maryland, June.
Breck Baldwin and Thomas Morton. 1998. Dy-
namic coreference-based summarization. In
Proceedings of the Third Conference on Empir-
ical Methods in Natural Language Processing,
Granada, Spain, June.
Breck Baldwin, Jeff Reynar, Mike Collins, Ja-
son Eisner, Adwait Ratnaparki, Joseph Rosen-
zweig, Anoop Sarkar, and Srinivas Bangalore.
1995. Description of the University of Pennsyl-
vania system used for MUC-6. In Proceedings
of the Sixth Message Understanding Conference
(M UC- 6), Columbia, Maryland.
E. Breck, J. Burger, L. Ferro, D. House, M. Light,
and I. Mani. 1999. A Sys called Qanda. In
Proceedings of the Eighth Text REtrieval Con-
ference (TREC-8), Gaithersburg, Maryland,
November. NIST.
Defense Advanced Research Projects Agency
(DARPA). 1995. Proceedings of the Sixth
Message Understanding Conference (MUC-6),
Columbia, Maryland, November.
Niyu Ge, John Hale, and Eugene Charniak. 1998.
A statistical approach to anaphora resolution.
In Proceedings of the Sixth Workshop on Very
Large Corpora, Montreal, Quebec, Canada, Au-
gust.
Jerry R. Hobbs. 1976. Pronoun resolution. Tech-
nical Report 76-1, City College, New York.
Shalom Lappin and Herbert J. Leass. 1994. An
algorithm for pronominal anaphora resolution.
Computational Linguistics, pages 535-561.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Lin-
guistics, 19(2):313-330.
Ruslan Mitkov. 1997. Factors in anaphora reso-
lution: They are not the only things that mat-
ter. A case study based on two different ap-
proaches. In Proceedings of the ACL &apos;97/EACL
&apos;97 Workshop on Operational Factors in Prac-
tical Robust Anaphora Resolution.
Thomas Morton. 1999. Using coreference in
question answering. In Proceedings of the
Eighth Text REtrieval Conference (TREC-8),
Gaithersburg, Maryland, November. NIST.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Part of Speech Tagger. In Proceeding of the
Conference on Empirical Methods in Natural
Language Processing, University of Pennsylva-
nia, May 17-18.
Adwait Ratnaparkhi. 1997. A simple introduc-
tion to maximum entropy models for natu-
ral language processing. Technical Report 97-
08, Institute for Research in Cognitive Science,
University of Pennsylvania.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997.
A maximum entropy approach to identifying
sentence boundaries. In Proceedings of the Fifth
Conference on Applied Natural Language Pro-
cessing, pages 16-19, Washington, D.C., April.
Jeff Reynar. 1998. Topic Segmentation: Algo-
rithms and Applications. Ph.D. thesis, Univer-
sity of Pennsylvania.
Amit Singal, Steve Abney, Michiel Bacchiani,
Michael Collins, Donald Hindle, and Fernando
Pereira. 1999. AT&amp;T at TREC-8. In Proceed-
ings of the Eighth Text REtrieval Conference
(TREC-8), Gaithersburg, Maryland, Novem-
ber. NIST.
Renata Vieira and Massimo Poesio. 1997. Pro-
cessing definite descriptions in corpora. In
Corpus-based and Computational Approaches
to Discourse Anaphora. UCL Press.
Marc Vilain, John Burger, John Aberdeen, Den-
nis Connolly, and Lynette Hirschman. 1995. A
model-theoretic coreference scoring scheme. In
Proceedings of the Sixth Message Understand-
ing Conference (MUC-6), Columbia, Maryland.
E. Voorhees and D. Tice. 1999. The TREC-8
question answering track evaluation. In Pro-
ceedings of the Eighth Text REtrieval Con-
ference (TREC-8), Gaithersburg, Maryland,
November. NIST.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945074">
<title confidence="0.999602">Coreference for NLP Applications</title>
<author confidence="0.999984">Thomas S Morton</author>
<affiliation confidence="0.999904">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.999381">Philadelphia, PA 19104</address>
<email confidence="0.999529">tsmorton@cis.upenn.edu</email>
<abstract confidence="0.995897428571428">This paper presents several techniques for performing automatic coreference annotation and performance results for each of them. To demonstrate that they can be applied to real-world data, we have built a simple question-answering system which uses the techniques. A system using coreference is compared to a baseline system with the result that the addition of the coreference annotation improves performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Saliha Azzam</author>
<author>Kevin Humphreys</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Using coreference chains for text summarization.</title>
<date>1999</date>
<booktitle>In Proceedings of the Workshop on Coreference and Its Applications,</booktitle>
<location>College Park, Maryland,</location>
<contexts>
<context position="18390" citStr="Azzam et al., 1999" startWordPosition="3158" endWordPosition="3161">mance at pronoun resolution when compared to Ge et al. (1998).2 The techniques we used for proper noun coreference are similar to those used by Baldwin et al. (1995) and others. For definite nouns we use the approach of Vieira and Poesio (1997). To our knowledge, the approach used for determining appositives is novel. 2Evaluating using only referential pronouns gives us an accuracy of 79.1% compared to the 84.2% reported in Ge et al. (1998). 3 Applications There are numerous applications which use coreference as a base annotation. These include work in summarization (Baldwin and Morton, 1998; Azzam et al., 1999) and question answering (Morton, 1999; Breck et al., 1999). 3.1 Question Answering Here we present results for a question answering task. Specifically, a system is given a query and then asked to find a 250-byte answer string from a collection of documents. Systems generate a ranked list of the top 5 answer strings. This task is the same as the question answering task used in TREC8 and described in Voorhees and Tice (1999). As a baseline, we have implemented the system used by AT&amp;T as described in Singal et al. (1999). A description of how that system ranks sentences is given in Table 7. When </context>
</contexts>
<marker>Azzam, Humphreys, Gaizauskas, 1999</marker>
<rawString>Saliha Azzam, Kevin Humphreys, and Robert Gaizauskas. 1999. Using coreference chains for text summarization. In Proceedings of the Workshop on Coreference and Its Applications, College Park, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Breck Baldwin</author>
<author>Thomas Morton</author>
</authors>
<title>Dynamic coreference-based summarization.</title>
<date>1998</date>
<booktitle>In Proceedings of the Third Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Granada, Spain,</location>
<contexts>
<context position="18369" citStr="Baldwin and Morton, 1998" startWordPosition="3154" endWordPosition="3157">rade-offs is poorer performance at pronoun resolution when compared to Ge et al. (1998).2 The techniques we used for proper noun coreference are similar to those used by Baldwin et al. (1995) and others. For definite nouns we use the approach of Vieira and Poesio (1997). To our knowledge, the approach used for determining appositives is novel. 2Evaluating using only referential pronouns gives us an accuracy of 79.1% compared to the 84.2% reported in Ge et al. (1998). 3 Applications There are numerous applications which use coreference as a base annotation. These include work in summarization (Baldwin and Morton, 1998; Azzam et al., 1999) and question answering (Morton, 1999; Breck et al., 1999). 3.1 Question Answering Here we present results for a question answering task. Specifically, a system is given a query and then asked to find a 250-byte answer string from a collection of documents. Systems generate a ranked list of the top 5 answer strings. This task is the same as the question answering task used in TREC8 and described in Voorhees and Tice (1999). As a baseline, we have implemented the system used by AT&amp;T as described in Singal et al. (1999). A description of how that system ranks sentences is gi</context>
</contexts>
<marker>Baldwin, Morton, 1998</marker>
<rawString>Breck Baldwin and Thomas Morton. 1998. Dynamic coreference-based summarization. In Proceedings of the Third Conference on Empirical Methods in Natural Language Processing, Granada, Spain, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Breck Baldwin</author>
<author>Jeff Reynar</author>
<author>Mike Collins</author>
<author>Jason Eisner</author>
<author>Adwait Ratnaparki</author>
<author>Joseph Rosenzweig</author>
<author>Anoop Sarkar</author>
<author>Srinivas Bangalore</author>
</authors>
<title>Description of the University of Pennsylvania system used for MUC-6.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (M UC- 6),</booktitle>
<location>Columbia, Maryland.</location>
<contexts>
<context position="9697" citStr="Baldwin et al., 1995" startWordPosition="1661" endWordPosition="1664">be computed, the task allows us to evaluate the pronoun model in isolation. Average results for ten-fold cross-validation are presented in Table 2 for our model with and without the entity merging described above. We find that the entity merging improves performance. 2.4 Proper Noun Rules To apply the pronoun model in practice we need to compute the other types of coreference that contribute to the mention count statistics and word or tag/gender pairs. Coreference between proper nouns is very common in newswire domains and accounts for approximately one third of all coreference relationships (Baldwin et al., 1995). Here we are concerned only with the coreference relationships between two proper nouns and not how proper nouns interact with other nouns or pronouns. A noun is considered a proper noun if its last word has the tag &amp;quot;NNP&amp;quot; or &amp;quot;NNPS&amp;quot;. Proper nouns do not have the same locality constraints that pronouns have. Model P R F string matching 92.1 88.0 90.0 Table 3: Precision, Recall, and F-Measure for Proper Noun Evaluation When searching for a possible referent we may have to consider all previous proper nouns as candidates. Determining these types of relationships can be done quite accurately with </context>
<context position="17936" citStr="Baldwin et al. (1995)" startWordPosition="3082" endWordPosition="3085">tationally expensive and non-referential pronouns cannot be excluded from the data. These differences also lead to the exclusion of some helpful features which are employed by Ge et al. (1998) such as a more accurate computation of the Hobbs distance, the probability that an antecedent can occur in the same syntactic position as the pronoun, and the use of unsupervised techniques for gender determination. The result of these trade-offs is poorer performance at pronoun resolution when compared to Ge et al. (1998).2 The techniques we used for proper noun coreference are similar to those used by Baldwin et al. (1995) and others. For definite nouns we use the approach of Vieira and Poesio (1997). To our knowledge, the approach used for determining appositives is novel. 2Evaluating using only referential pronouns gives us an accuracy of 79.1% compared to the 84.2% reported in Ge et al. (1998). 3 Applications There are numerous applications which use coreference as a base annotation. These include work in summarization (Baldwin and Morton, 1998; Azzam et al., 1999) and question answering (Morton, 1999; Breck et al., 1999). 3.1 Question Answering Here we present results for a question answering task. Specific</context>
</contexts>
<marker>Baldwin, Reynar, Collins, Eisner, Ratnaparki, Rosenzweig, Sarkar, Bangalore, 1995</marker>
<rawString>Breck Baldwin, Jeff Reynar, Mike Collins, Jason Eisner, Adwait Ratnaparki, Joseph Rosenzweig, Anoop Sarkar, and Srinivas Bangalore. 1995. Description of the University of Pennsylvania system used for MUC-6. In Proceedings of the Sixth Message Understanding Conference (M UC- 6), Columbia, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Breck</author>
<author>J Burger</author>
<author>L Ferro</author>
<author>D House</author>
<author>M Light</author>
<author>I Mani</author>
</authors>
<title>A Sys called Qanda.</title>
<date>1999</date>
<booktitle>In Proceedings of the Eighth Text REtrieval Conference (TREC-8),</booktitle>
<publisher>NIST.</publisher>
<location>Gaithersburg, Maryland,</location>
<contexts>
<context position="18448" citStr="Breck et al., 1999" startWordPosition="3168" endWordPosition="3171">98).2 The techniques we used for proper noun coreference are similar to those used by Baldwin et al. (1995) and others. For definite nouns we use the approach of Vieira and Poesio (1997). To our knowledge, the approach used for determining appositives is novel. 2Evaluating using only referential pronouns gives us an accuracy of 79.1% compared to the 84.2% reported in Ge et al. (1998). 3 Applications There are numerous applications which use coreference as a base annotation. These include work in summarization (Baldwin and Morton, 1998; Azzam et al., 1999) and question answering (Morton, 1999; Breck et al., 1999). 3.1 Question Answering Here we present results for a question answering task. Specifically, a system is given a query and then asked to find a 250-byte answer string from a collection of documents. Systems generate a ranked list of the top 5 answer strings. This task is the same as the question answering task used in TREC8 and described in Voorhees and Tice (1999). As a baseline, we have implemented the system used by AT&amp;T as described in Singal et al. (1999). A description of how that system ranks sentences is given in Table 7. When we apply this approach to documents with coreference relat</context>
</contexts>
<marker>Breck, Burger, Ferro, House, Light, Mani, 1999</marker>
<rawString>E. Breck, J. Burger, L. Ferro, D. House, M. Light, and I. Mani. 1999. A Sys called Qanda. In Proceedings of the Eighth Text REtrieval Conference (TREC-8), Gaithersburg, Maryland, November. NIST.</rawString>
</citation>
<citation valid="true">
<title>Defense Advanced Research Projects Agency (DARPA).</title>
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<location>Columbia, Maryland,</location>
<contexts>
<context position="16163" citStr="(1995)" startWordPosition="2777" endWordPosition="2777">e an entity to become inaccessible to a pronoun or common noun which is coreferent with it. In order to determine how these components would perform together, we integrated them, and evaluated this system using the MUC-6 Coreference Task (Def, 1995). The two data sets each consist of 30 Wall Street Journal articles that have been hand-annotated for coreference relationships. In these articles almost all coreference between noun phrases has been marked. Results for this task are presented in Table 6. These results are based on the scoring algorithm used for MUC-6 and described in Vilain et al. (1995). Recall for these results is low because there are many types of coreference relationships which we have not attempted to annotate. Precision has remained high, so we can be fairly confident in the coreference relationships that have been data P R F dry-run 79.3 41.2 54.2 evaluation 79.6 44.5 57.1 Table 6: Precision, Recall, and F-Measure for MUG Evaluation found. 2.9 Related Work in Coreference Our approach for pronoun resolution is similar to that of Ge et al. (1998) in the following ways. Both approaches use statistical modeling to rank antecedents and then select the top ranked one. The f</context>
<context position="17936" citStr="(1995)" startWordPosition="3085" endWordPosition="3085">nsive and non-referential pronouns cannot be excluded from the data. These differences also lead to the exclusion of some helpful features which are employed by Ge et al. (1998) such as a more accurate computation of the Hobbs distance, the probability that an antecedent can occur in the same syntactic position as the pronoun, and the use of unsupervised techniques for gender determination. The result of these trade-offs is poorer performance at pronoun resolution when compared to Ge et al. (1998).2 The techniques we used for proper noun coreference are similar to those used by Baldwin et al. (1995) and others. For definite nouns we use the approach of Vieira and Poesio (1997). To our knowledge, the approach used for determining appositives is novel. 2Evaluating using only referential pronouns gives us an accuracy of 79.1% compared to the 84.2% reported in Ge et al. (1998). 3 Applications There are numerous applications which use coreference as a base annotation. These include work in summarization (Baldwin and Morton, 1998; Azzam et al., 1999) and question answering (Morton, 1999; Breck et al., 1999). 3.1 Question Answering Here we present results for a question answering task. Specific</context>
</contexts>
<marker>1995</marker>
<rawString>Defense Advanced Research Projects Agency (DARPA). 1995. Proceedings of the Sixth Message Understanding Conference (MUC-6), Columbia, Maryland, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
<author>John Hale</author>
<author>Eugene Charniak</author>
</authors>
<title>A statistical approach to anaphora resolution.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="877" citStr="Ge et al., 1998" startWordPosition="136" endWordPosition="139">tion and performance results for each of them. To demonstrate that they can be applied to real-world data, we have built a simple question-answering system which uses the techniques. A system using coreference is compared to a baseline system with the result that the addition of the coreference annotation improves performance. 1 Introduction The fact that two pieces of text specify the same thing in the world can be very helpful in a variety of natural language processing tasks. While there is a vast body of literature on anaphora resolution (Hobbs, 1976; Lappin and Leass, 1994; Mitkov, 1997; Ge et al., 1998), many of these techniques require handcrafted resources, which are difficult to construct, or unrealistic sources of information as input to their algorithms Here we present a series of techniques for performing coreference between noun phrases which require a limited amount of phrase structure as input and no domain-specific knowledge engineering. 2 Coreference Coreference annotation involves determining whether or not two noun phrases are used to refer to the same thing. While this is a single task, different types of noun phrases behave differently in terms of how they co-refer. This leads</context>
<context position="8486" citStr="Ge et al. (1998)" startWordPosition="1453" endWordPosition="1456">be coreferent with the antecedent in question. In contrast, only a single distance measure is kept, which is based on the referent with the lowest Hobbs distance. Feature 1Ratnaparkhi (1997) provides a good introduction on how to use generalized iterative scaling, GIS, to compute the parameters of these models. Model P R F w/o entity merging 94.8 71.5 81.5 with entity merging 94.4 76.0 84.2 Table 2: Precision, Recall, and F-Measure for Pronoun Evaluation 6, the number of times this entity has been mentioned is also incremented. 2.3 Pronoun Evaluation We evaluated our model on the data used in Ge et al. (1998). There are 1307 pronouns in this corpus which are forms of he, she, or it. We trained our model on 90% of the data and tested on the other 10%. A feature had to occur at least 5 times in order to be included in the model, and the model parameters were computed using 100 iterations of GIS. The task here was to determine which noun phrase the pronoun referred to or that it was non-referential, given all the previous coreference relationships. While this task is not representative of how the model would be used in practice, namely the other coreference relations are not given and must be compute</context>
<context position="16637" citStr="Ge et al. (1998)" startWordPosition="2856" endWordPosition="2859"> for this task are presented in Table 6. These results are based on the scoring algorithm used for MUC-6 and described in Vilain et al. (1995). Recall for these results is low because there are many types of coreference relationships which we have not attempted to annotate. Precision has remained high, so we can be fairly confident in the coreference relationships that have been data P R F dry-run 79.3 41.2 54.2 evaluation 79.6 44.5 57.1 Table 6: Precision, Recall, and F-Measure for MUG Evaluation found. 2.9 Related Work in Coreference Our approach for pronoun resolution is similar to that of Ge et al. (1998) in the following ways. Both approaches use statistical modeling to rank antecedents and then select the top ranked one. The features used by both these models include the Hobbs distance and the mention count statistics. They mainly differ in the following ways. Our approach requires noun phrases for input while Ge et al. (1998) uses full parse trees. Ge et al. (1998) does not employ entity merging which we find helps our model. Most importantly, our model handles nonreferential pronouns while Ge et al. (1998) excludes them from the data. These differences are primarily the result of our desir</context>
<context position="18215" citStr="Ge et al. (1998)" startWordPosition="3131" endWordPosition="3134">nt can occur in the same syntactic position as the pronoun, and the use of unsupervised techniques for gender determination. The result of these trade-offs is poorer performance at pronoun resolution when compared to Ge et al. (1998).2 The techniques we used for proper noun coreference are similar to those used by Baldwin et al. (1995) and others. For definite nouns we use the approach of Vieira and Poesio (1997). To our knowledge, the approach used for determining appositives is novel. 2Evaluating using only referential pronouns gives us an accuracy of 79.1% compared to the 84.2% reported in Ge et al. (1998). 3 Applications There are numerous applications which use coreference as a base annotation. These include work in summarization (Baldwin and Morton, 1998; Azzam et al., 1999) and question answering (Morton, 1999; Breck et al., 1999). 3.1 Question Answering Here we present results for a question answering task. Specifically, a system is given a query and then asked to find a 250-byte answer string from a collection of documents. Systems generate a ranked list of the top 5 answer strings. This task is the same as the question answering task used in TREC8 and described in Voorhees and Tice (1999</context>
</contexts>
<marker>Ge, Hale, Charniak, 1998</marker>
<rawString>Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical approach to anaphora resolution. In Proceedings of the Sixth Workshop on Very Large Corpora, Montreal, Quebec, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Pronoun resolution.</title>
<date>1976</date>
<tech>Technical Report 76-1,</tech>
<location>City College, New York.</location>
<contexts>
<context position="821" citStr="Hobbs, 1976" startWordPosition="127" endWordPosition="128">hniques for performing automatic coreference annotation and performance results for each of them. To demonstrate that they can be applied to real-world data, we have built a simple question-answering system which uses the techniques. A system using coreference is compared to a baseline system with the result that the addition of the coreference annotation improves performance. 1 Introduction The fact that two pieces of text specify the same thing in the world can be very helpful in a variety of natural language processing tasks. While there is a vast body of literature on anaphora resolution (Hobbs, 1976; Lappin and Leass, 1994; Mitkov, 1997; Ge et al., 1998), many of these techniques require handcrafted resources, which are difficult to construct, or unrealistic sources of information as input to their algorithms Here we present a series of techniques for performing coreference between noun phrases which require a limited amount of phrase structure as input and no domain-specific knowledge engineering. 2 Coreference Coreference annotation involves determining whether or not two noun phrases are used to refer to the same thing. While this is a single task, different types of noun phrases beha</context>
<context position="3253" citStr="Hobbs (1976)" startWordPosition="528" endWordPosition="529">). 2.2 Pronoun Model Pronoun resolution is the most well-studied of the types of coreference discussed here. The pronouns we wish to resolve are all forms of the singular pronouns, he, she, and it. The set of possible antecedents we have chosen is all the basal noun phrases which occur before the pronoun in the same sentence or in the previous two sentences. For these pronouns, and the data we used, a look-back of two sentences makes the antecedent available 98.7% of the time. A look-back of only one sentence makes the antecedent available 96.9% of the time, which concurs with the findings in Hobbs (1976). The factors we take into account when determining what noun phrases a pronoun is coreferent with are, in broad terms, locality, gender, syntax, and accessibility or salience. These factors however are not independent and the specific features we use often model more than one factor. Table 1 lists each of the features used and which factors we were attempting to model with them. Features 1-2 indicate how close the antecedent is to the pronoun. The first feature counts NPs from right to left and the second counts them from left to right. We refer to the latter as the Hobbs distance as it appro</context>
</contexts>
<marker>Hobbs, 1976</marker>
<rawString>Jerry R. Hobbs. 1976. Pronoun resolution. Technical Report 76-1, City College, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
<author>Herbert J Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution. Computational Linguistics,</title>
<date>1994</date>
<pages>535--561</pages>
<contexts>
<context position="845" citStr="Lappin and Leass, 1994" startWordPosition="129" endWordPosition="133">erforming automatic coreference annotation and performance results for each of them. To demonstrate that they can be applied to real-world data, we have built a simple question-answering system which uses the techniques. A system using coreference is compared to a baseline system with the result that the addition of the coreference annotation improves performance. 1 Introduction The fact that two pieces of text specify the same thing in the world can be very helpful in a variety of natural language processing tasks. While there is a vast body of literature on anaphora resolution (Hobbs, 1976; Lappin and Leass, 1994; Mitkov, 1997; Ge et al., 1998), many of these techniques require handcrafted resources, which are difficult to construct, or unrealistic sources of information as input to their algorithms Here we present a series of techniques for performing coreference between noun phrases which require a limited amount of phrase structure as input and no domain-specific knowledge engineering. 2 Coreference Coreference annotation involves determining whether or not two noun phrases are used to refer to the same thing. While this is a single task, different types of noun phrases behave differently in terms </context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>Shalom Lappin and Herbert J. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, pages 535-561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="2642" citStr="Marcus et al., 1994" startWordPosition="419" endWordPosition="422">n to most NLP applications and includes sentence detection, tokenization, partof-speech tagging and noun-phrase chunking. The sentence detector we use is described in Reynar and Ratnaparkhi (1997), the tokenizer in Reynar (1998), and the part-ofspeech tagger in Ratnaparkhi (1996). The noun-phrase chunking is also done as a tagging task in which tokens are tagged as the start of a noun phrase, the continuation of a noun phrase or other. It employs modeling techniques similar to those used for the partof-speech tagger. These tools were all trained automatically with data from the Penn Treebank (Marcus et al., 1994). 2.2 Pronoun Model Pronoun resolution is the most well-studied of the types of coreference discussed here. The pronouns we wish to resolve are all forms of the singular pronouns, he, she, and it. The set of possible antecedents we have chosen is all the basal noun phrases which occur before the pronoun in the same sentence or in the previous two sentences. For these pronouns, and the data we used, a look-back of two sentences makes the antecedent available 98.7% of the time. A look-back of only one sentence makes the antecedent available 96.9% of the time, which concurs with the findings in H</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Factors in anaphora resolution: They are not the only things that matter. A case study based on two different approaches.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL &apos;97/EACL &apos;97 Workshop on Operational Factors in Practical Robust Anaphora Resolution.</booktitle>
<contexts>
<context position="859" citStr="Mitkov, 1997" startWordPosition="134" endWordPosition="135">ference annotation and performance results for each of them. To demonstrate that they can be applied to real-world data, we have built a simple question-answering system which uses the techniques. A system using coreference is compared to a baseline system with the result that the addition of the coreference annotation improves performance. 1 Introduction The fact that two pieces of text specify the same thing in the world can be very helpful in a variety of natural language processing tasks. While there is a vast body of literature on anaphora resolution (Hobbs, 1976; Lappin and Leass, 1994; Mitkov, 1997; Ge et al., 1998), many of these techniques require handcrafted resources, which are difficult to construct, or unrealistic sources of information as input to their algorithms Here we present a series of techniques for performing coreference between noun phrases which require a limited amount of phrase structure as input and no domain-specific knowledge engineering. 2 Coreference Coreference annotation involves determining whether or not two noun phrases are used to refer to the same thing. While this is a single task, different types of noun phrases behave differently in terms of how they co</context>
</contexts>
<marker>Mitkov, 1997</marker>
<rawString>Ruslan Mitkov. 1997. Factors in anaphora resolution: They are not the only things that matter. A case study based on two different approaches. In Proceedings of the ACL &apos;97/EACL &apos;97 Workshop on Operational Factors in Practical Robust Anaphora Resolution.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Morton</author>
</authors>
<title>Using coreference in question answering.</title>
<date>1999</date>
<booktitle>In Proceedings of the Eighth Text REtrieval Conference (TREC-8),</booktitle>
<publisher>NIST.</publisher>
<location>Gaithersburg, Maryland,</location>
<contexts>
<context position="18427" citStr="Morton, 1999" startWordPosition="3166" endWordPosition="3167"> Ge et al. (1998).2 The techniques we used for proper noun coreference are similar to those used by Baldwin et al. (1995) and others. For definite nouns we use the approach of Vieira and Poesio (1997). To our knowledge, the approach used for determining appositives is novel. 2Evaluating using only referential pronouns gives us an accuracy of 79.1% compared to the 84.2% reported in Ge et al. (1998). 3 Applications There are numerous applications which use coreference as a base annotation. These include work in summarization (Baldwin and Morton, 1998; Azzam et al., 1999) and question answering (Morton, 1999; Breck et al., 1999). 3.1 Question Answering Here we present results for a question answering task. Specifically, a system is given a query and then asked to find a 250-byte answer string from a collection of documents. Systems generate a ranked list of the top 5 answer strings. This task is the same as the question answering task used in TREC8 and described in Voorhees and Tice (1999). As a baseline, we have implemented the system used by AT&amp;T as described in Singal et al. (1999). A description of how that system ranks sentences is given in Table 7. When we apply this approach to documents w</context>
</contexts>
<marker>Morton, 1999</marker>
<rawString>Thomas Morton. 1999. Using coreference in question answering. In Proceedings of the Eighth Text REtrieval Conference (TREC-8), Gaithersburg, Maryland, November. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Part of Speech Tagger.</title>
<date>1996</date>
<booktitle>In Proceeding of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>17--18</pages>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="2302" citStr="Ratnaparkhi (1996)" startWordPosition="360" endWordPosition="361"> the set of possible antecedents is, a set of factors which influence whether the two nouns corefer with one another, and a decision process for determining which nouns are coreferring. 2.1 Pre-processing Before coreference on noun phrases can be annotated, the noun phrases, themselves, must first be determined. This processing is common to most NLP applications and includes sentence detection, tokenization, partof-speech tagging and noun-phrase chunking. The sentence detector we use is described in Reynar and Ratnaparkhi (1997), the tokenizer in Reynar (1998), and the part-ofspeech tagger in Ratnaparkhi (1996). The noun-phrase chunking is also done as a tagging task in which tokens are tagged as the start of a noun phrase, the continuation of a noun phrase or other. It employs modeling techniques similar to those used for the partof-speech tagger. These tools were all trained automatically with data from the Penn Treebank (Marcus et al., 1994). 2.2 Pronoun Model Pronoun resolution is the most well-studied of the types of coreference discussed here. The pronouns we wish to resolve are all forms of the singular pronouns, he, she, and it. The set of possible antecedents we have chosen is all the basal</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Part of Speech Tagger. In Proceeding of the Conference on Empirical Methods in Natural Language Processing, University of Pennsylvania, May 17-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A simple introduction to maximum entropy models for natural language processing.</title>
<date>1997</date>
<tech>Technical Report 97-08,</tech>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania.</institution>
<contexts>
<context position="2218" citStr="Ratnaparkhi (1997)" startWordPosition="346" endWordPosition="347"> nouns, definite nouns, and appositives. For each of these classes we determine what the set of possible antecedents is, a set of factors which influence whether the two nouns corefer with one another, and a decision process for determining which nouns are coreferring. 2.1 Pre-processing Before coreference on noun phrases can be annotated, the noun phrases, themselves, must first be determined. This processing is common to most NLP applications and includes sentence detection, tokenization, partof-speech tagging and noun-phrase chunking. The sentence detector we use is described in Reynar and Ratnaparkhi (1997), the tokenizer in Reynar (1998), and the part-ofspeech tagger in Ratnaparkhi (1996). The noun-phrase chunking is also done as a tagging task in which tokens are tagged as the start of a noun phrase, the continuation of a noun phrase or other. It employs modeling techniques similar to those used for the partof-speech tagger. These tools were all trained automatically with data from the Penn Treebank (Marcus et al., 1994). 2.2 Pronoun Model Pronoun resolution is the most well-studied of the types of coreference discussed here. The pronouns we wish to resolve are all forms of the singular pronou</context>
<context position="8060" citStr="Ratnaparkhi (1997)" startWordPosition="1382" endWordPosition="1383">re this phenomenon is by merging antecedents and referents when any coreference relation is posited. When this happens the features of each noun phrase are merged in the following way: The head and modifier words and tags are added to a set of such words and tags for the entity. Thus features 7-10 generate contextual predicates, used by the maximum entropy model, based on every head and modifier word which has been found to be coreferent with the antecedent in question. In contrast, only a single distance measure is kept, which is based on the referent with the lowest Hobbs distance. Feature 1Ratnaparkhi (1997) provides a good introduction on how to use generalized iterative scaling, GIS, to compute the parameters of these models. Model P R F w/o entity merging 94.8 71.5 81.5 with entity merging 94.4 76.0 84.2 Table 2: Precision, Recall, and F-Measure for Pronoun Evaluation 6, the number of times this entity has been mentioned is also incremented. 2.3 Pronoun Evaluation We evaluated our model on the data used in Ge et al. (1998). There are 1307 pronouns in this corpus which are forms of he, she, or it. We trained our model on 90% of the data and tested on the other 10%. A feature had to occur at lea</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A simple introduction to maximum entropy models for natural language processing. Technical Report 97-08, Institute for Research in Cognitive Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>16--19</pages>
<location>Washington, D.C.,</location>
<contexts>
<context position="2218" citStr="Reynar and Ratnaparkhi (1997)" startWordPosition="344" endWordPosition="347">uns, proper nouns, definite nouns, and appositives. For each of these classes we determine what the set of possible antecedents is, a set of factors which influence whether the two nouns corefer with one another, and a decision process for determining which nouns are coreferring. 2.1 Pre-processing Before coreference on noun phrases can be annotated, the noun phrases, themselves, must first be determined. This processing is common to most NLP applications and includes sentence detection, tokenization, partof-speech tagging and noun-phrase chunking. The sentence detector we use is described in Reynar and Ratnaparkhi (1997), the tokenizer in Reynar (1998), and the part-ofspeech tagger in Ratnaparkhi (1996). The noun-phrase chunking is also done as a tagging task in which tokens are tagged as the start of a noun phrase, the continuation of a noun phrase or other. It employs modeling techniques similar to those used for the partof-speech tagger. These tools were all trained automatically with data from the Penn Treebank (Marcus et al., 1994). 2.2 Pronoun Model Pronoun resolution is the most well-studied of the types of coreference discussed here. The pronouns we wish to resolve are all forms of the singular pronou</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 16-19, Washington, D.C., April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Reynar</author>
</authors>
<title>Topic Segmentation: Algorithms and Applications.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2250" citStr="Reynar (1998)" startWordPosition="352" endWordPosition="353">es. For each of these classes we determine what the set of possible antecedents is, a set of factors which influence whether the two nouns corefer with one another, and a decision process for determining which nouns are coreferring. 2.1 Pre-processing Before coreference on noun phrases can be annotated, the noun phrases, themselves, must first be determined. This processing is common to most NLP applications and includes sentence detection, tokenization, partof-speech tagging and noun-phrase chunking. The sentence detector we use is described in Reynar and Ratnaparkhi (1997), the tokenizer in Reynar (1998), and the part-ofspeech tagger in Ratnaparkhi (1996). The noun-phrase chunking is also done as a tagging task in which tokens are tagged as the start of a noun phrase, the continuation of a noun phrase or other. It employs modeling techniques similar to those used for the partof-speech tagger. These tools were all trained automatically with data from the Penn Treebank (Marcus et al., 1994). 2.2 Pronoun Model Pronoun resolution is the most well-studied of the types of coreference discussed here. The pronouns we wish to resolve are all forms of the singular pronouns, he, she, and it. The set of </context>
</contexts>
<marker>Reynar, 1998</marker>
<rawString>Jeff Reynar. 1998. Topic Segmentation: Algorithms and Applications. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Singal</author>
<author>Steve Abney</author>
<author>Michiel Bacchiani</author>
<author>Michael Collins</author>
<author>Donald Hindle</author>
<author>Fernando Pereira</author>
</authors>
<title>AT&amp;T at TREC-8.</title>
<date>1999</date>
<booktitle>In Proceedings of the Eighth Text REtrieval Conference (TREC-8),</booktitle>
<publisher>NIST.</publisher>
<location>Gaithersburg, Maryland,</location>
<contexts>
<context position="18913" citStr="Singal et al. (1999)" startWordPosition="3253" endWordPosition="3256">se annotation. These include work in summarization (Baldwin and Morton, 1998; Azzam et al., 1999) and question answering (Morton, 1999; Breck et al., 1999). 3.1 Question Answering Here we present results for a question answering task. Specifically, a system is given a query and then asked to find a 250-byte answer string from a collection of documents. Systems generate a ranked list of the top 5 answer strings. This task is the same as the question answering task used in TREC8 and described in Voorhees and Tice (1999). As a baseline, we have implemented the system used by AT&amp;T as described in Singal et al. (1999). A description of how that system ranks sentences is given in Table 7. When we apply this approach to documents with coreference relationships annotated we consider any additional terms which are in coreference chains with noun phrases in a sentence to have occurred in that sentence as well. The terms added via a coreference chains are given 90% of the weight of regularly occurring terms. When presenting the top 5 passages, referents are indicated in parentheses next to the referring terms. 3.2 Question Answering Evaluation We evaluated both approaches using the 200 questions used in TREC-8 a</context>
</contexts>
<marker>Singal, Abney, Bacchiani, Collins, Hindle, Pereira, 1999</marker>
<rawString>Amit Singal, Steve Abney, Michiel Bacchiani, Michael Collins, Donald Hindle, and Fernando Pereira. 1999. AT&amp;T at TREC-8. In Proceedings of the Eighth Text REtrieval Conference (TREC-8), Gaithersburg, Maryland, November. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renata Vieira</author>
<author>Massimo Poesio</author>
</authors>
<title>Processing definite descriptions in corpora.</title>
<date>1997</date>
<booktitle>In Corpus-based and Computational Approaches to Discourse Anaphora.</booktitle>
<publisher>UCL Press.</publisher>
<contexts>
<context position="12434" citStr="Vieira and Poesio (1997)" startWordPosition="2133" endWordPosition="2136">y hurt by a lack of treatment of acronyms. 2.6 Common Nouns: Rules and Model Coreference between common nouns and other nouns is a difficult class in general, but a subset of these cases can be identified with reasonable precision. The cases we consider are definite noun phrases and appositives. Definite noun phrases indicate that the referent should be known to the reader and thus we are likely to find an antecedent for this noun phrase. We consider a noun phrase definite if it is modified by the determiner the. For determining coreference between these noun phrases we employ the approach of Vieira and Poesio (1997). The first noun phrase that occurs within the previous five sentences has a string-equivalent head word, and no additional modifiers is considered coreferent with the definite noun phrase. Another case of common noun coreference is appositives. The coreference between The asbestos fiber and crocidolite in the following sentence is an example of an appositive: The asbestos fiber, crocidolite, is unusually resilient once it enters the lungs, with even brief exposures to it causing symptoms that show up decades later, researchers said. We consider any two noun phrases separated by a comma as can</context>
<context position="18015" citStr="Vieira and Poesio (1997)" startWordPosition="3097" endWordPosition="3100">e data. These differences also lead to the exclusion of some helpful features which are employed by Ge et al. (1998) such as a more accurate computation of the Hobbs distance, the probability that an antecedent can occur in the same syntactic position as the pronoun, and the use of unsupervised techniques for gender determination. The result of these trade-offs is poorer performance at pronoun resolution when compared to Ge et al. (1998).2 The techniques we used for proper noun coreference are similar to those used by Baldwin et al. (1995) and others. For definite nouns we use the approach of Vieira and Poesio (1997). To our knowledge, the approach used for determining appositives is novel. 2Evaluating using only referential pronouns gives us an accuracy of 79.1% compared to the 84.2% reported in Ge et al. (1998). 3 Applications There are numerous applications which use coreference as a base annotation. These include work in summarization (Baldwin and Morton, 1998; Azzam et al., 1999) and question answering (Morton, 1999; Breck et al., 1999). 3.1 Question Answering Here we present results for a question answering task. Specifically, a system is given a query and then asked to find a 250-byte answer string</context>
</contexts>
<marker>Vieira, Poesio, 1997</marker>
<rawString>Renata Vieira and Massimo Poesio. 1997. Processing definite descriptions in corpora. In Corpus-based and Computational Approaches to Discourse Anaphora. UCL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<location>Columbia, Maryland.</location>
<contexts>
<context position="16163" citStr="Vilain et al. (1995)" startWordPosition="2774" endWordPosition="2777">nship may cause an entity to become inaccessible to a pronoun or common noun which is coreferent with it. In order to determine how these components would perform together, we integrated them, and evaluated this system using the MUC-6 Coreference Task (Def, 1995). The two data sets each consist of 30 Wall Street Journal articles that have been hand-annotated for coreference relationships. In these articles almost all coreference between noun phrases has been marked. Results for this task are presented in Table 6. These results are based on the scoring algorithm used for MUC-6 and described in Vilain et al. (1995). Recall for these results is low because there are many types of coreference relationships which we have not attempted to annotate. Precision has remained high, so we can be fairly confident in the coreference relationships that have been data P R F dry-run 79.3 41.2 54.2 evaluation 79.6 44.5 57.1 Table 6: Precision, Recall, and F-Measure for MUG Evaluation found. 2.9 Related Work in Coreference Our approach for pronoun resolution is similar to that of Ge et al. (1998) in the following ways. Both approaches use statistical modeling to rank antecedents and then select the top ranked one. The f</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proceedings of the Sixth Message Understanding Conference (MUC-6), Columbia, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
<author>D Tice</author>
</authors>
<title>The TREC-8 question answering track evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of the Eighth Text REtrieval Conference (TREC-8),</booktitle>
<publisher>NIST.</publisher>
<location>Gaithersburg, Maryland,</location>
<contexts>
<context position="18816" citStr="Voorhees and Tice (1999)" startWordPosition="3235" endWordPosition="3238">ted in Ge et al. (1998). 3 Applications There are numerous applications which use coreference as a base annotation. These include work in summarization (Baldwin and Morton, 1998; Azzam et al., 1999) and question answering (Morton, 1999; Breck et al., 1999). 3.1 Question Answering Here we present results for a question answering task. Specifically, a system is given a query and then asked to find a 250-byte answer string from a collection of documents. Systems generate a ranked list of the top 5 answer strings. This task is the same as the question answering task used in TREC8 and described in Voorhees and Tice (1999). As a baseline, we have implemented the system used by AT&amp;T as described in Singal et al. (1999). A description of how that system ranks sentences is given in Table 7. When we apply this approach to documents with coreference relationships annotated we consider any additional terms which are in coreference chains with noun phrases in a sentence to have occurred in that sentence as well. The terms added via a coreference chains are given 90% of the weight of regularly occurring terms. When presenting the top 5 passages, referents are indicated in parentheses next to the referring terms. 3.2 Qu</context>
</contexts>
<marker>Voorhees, Tice, 1999</marker>
<rawString>E. Voorhees and D. Tice. 1999. The TREC-8 question answering track evaluation. In Proceedings of the Eighth Text REtrieval Conference (TREC-8), Gaithersburg, Maryland, November. NIST.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>