<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005384">
<title confidence="0.996596">
A Case for Shorter Queries, and Helping Users Create Them
</title>
<author confidence="0.995548">
Giridhar Kumaran and James Allan
</author>
<affiliation confidence="0.88259425">
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003, USA
</affiliation>
<email confidence="0.999576">
{giridhar,allan}@cs.umass.edu
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999950684210526">
Information retrieval systems are fre-
quently required to handle long queries.
Simply using all terms in the query or re-
lying on the underlying retrieval model
to appropriately weight terms often leads
to ineffective retrieval. We show that re-
writing the query to a version that com-
prises a small subset of appropriate terms
from the original query greatly improves
effectiveness. Targeting a demonstrated
potential improvement of almost 50% on
some difficult TREC queries and their as-
sociated collections, we develop a suite of
automatic techniques to re-write queries
and study their characteristics. We show
that the shortcomings of automatic meth-
ods can be ameliorated by some simple
user interaction, and report results that are
on average 25% better than the baseline.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935302325581">
Query expansion has long been a focus of infor-
mation retrieval research. Given an arbitrary short
query, the goal was to find and include additional
related and suitably-weighted terms to the original
query to produce a more effective version. In this pa-
per we focus on a complementary problem – query
re-writing. Given a long query we explore whether
there is utility in modifying it to a more concise ver-
sion such that the original information need is still
expressed.
The Y!Q beta1 search engine allows users to se-
lect large portions of text from documents and issue
them as queries. The search engine is designed to
encourage users to submit long queries such as this
example from the web site “I need to know the gas
mileage for my Audi A8 2004 model”. The moti-
vation for encouraging this type of querying is that
longer queries would provide more information in
the form of context (Kraft et al., 2006), and this ad-
ditional information could be leveraged to provide
a better search experience. However, handling such
long queries is a challenge. The use of all the terms
from the user’s input can rapidly narrow down the
set of matching documents, especially if a boolean
retrieval model is adopted. While one would ex-
pect the underlying retrieval model to appropriately
assign weights to different terms in the query and
return only relevant content, it is widely acknowl-
edged that models fail due to a variety of reasons
(Harman and Buckley, 2004), and are not suited to
tackle every possible query.
Recently, there has been great interest in personal-
ized search (Teevan et al., 2005), where the query is
modified based on a user’s profile. The profile usu-
ally consists of documents previously viewed, web
sites recently visited, e-mail correspondence and so
on. Common procedures for using this large amount
of information usually involve creating huge query
vectors with some sort of term-weighting mecha-
nism to favor different portions of the profile.
The queries used in the TREC ad-hoc tracks con-
sist of title, description and narrative sections, of
progressively increasing length. The title, of length
</bodyText>
<footnote confidence="0.984778">
1http://yq.search.yahoo.com/
</footnote>
<page confidence="0.924397">
220
</page>
<note confidence="0.8022575">
Proceedings of NAACL HLT 2007, pages 220–227,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999943620689655">
ranging from a single term to four terms is consid-
ered a concise query, while the description is consid-
ered a longer version of the title expressing the same
information need. Almost all research on the TREC
ad-hoc retrieval track reports results using only the
title portion as the query, and a combination of the
title and description as a separate query. Most re-
ported results show that the latter is more effective
than the former, though in the case of some hard col-
lections the opposite is true. However, as we shall
show later, there is tremendous scope for improve-
ment. Formulating a shorter query from the descrip-
tion can lead to significant improvements in perfor-
mance.
In the light of the above, we believe there is great
utility in creating query-rewriting mechanisms for
handling long queries. This paper is organized in
the following way. We start with some examples
and explore ways by which we can create concise
high-quality reformulations of long queries in Sec-
tion 2. We describe our baseline system in Section 3
and motivate our investigations with experiments in
Section 4. Since automatic methods have shortfalls,
we present a procedure in Section 5 to involve users
in selecting a good shorter query from a small selec-
tion of alternatives. We report and discuss the results
of this approach in Section 6. Related work is pre-
sented in Section 7. We wrap up with conclusions
and future directions in Section 8.
</bodyText>
<sectionHeader confidence="0.951967" genericHeader="introduction">
2 Selecting sub-queries
</sectionHeader>
<bodyText confidence="0.9768365625">
Consider the following query:
Define Argentine and British international rela-
tions.
When this query was issued to a search engine,
the average precision (AP, Section 3) of the results
was 0.424. When we selected subsets of terms (sub-
queries) from the query, and ran them as distinct
queries, the performance was as shown in Table 1. It
can be observed that there are seven different ways
of re-writing the original query to attain better per-
formance. The best query, also among the shortest,
did not have a natural-language flavor to it. It how-
ever had an effectiveness almost 50% more than the
original query. This immense potential for improve-
ment by query re-writing is the motivation for this
paper.
</bodyText>
<table confidence="0.997571176470588">
Query AP
.... ....
international relate 0.000
define international relate 0.000
.... ....
define argentina 0.123
international relate argentina 0.130
define relate argentina 0.141
relate argentina 0.173
define britain international relate argentina 0.424
define britain international argentina 0.469
britain international relate argentina 0.490
define britain relate argentina 0.494
britain international argentina 0.528
define britain argentina 0.546
britain relate argentina 0.563
britain argentina 0.626
</table>
<tableCaption confidence="0.998286">
Table 1: The results of using all possible subsets (ex-
</tableCaption>
<bodyText confidence="0.889901833333333">
cluding singletons) of the original query as queries.
The query terms were stemmed and stopped.
Analysis of the terms in the sub-queries and the
relationship of the sub-queries with the original
query revealed a few interesting insights that had po-
tential to be leveraged to aid sub-query selection.
</bodyText>
<listItem confidence="0.986553541666667">
1. Terms in the original query that a human would
consider vital in conveying the type of infor-
mation desired were missing from the best sub-
queries. For example, the best sub-query for
the example was britain argentina, omitting
any reference to international relations. This
also reveals a mismatch between the user’s
query and the way terms occurred in the corpus,
and suggests that an approximate query could
at times be a better starting point for search.
2. The sub-query would often contain only terms
that a human would consider vital to the query
while the original query would also (naturally)
contain them, albeit weighted lower with re-
spect to other terms. This is a common prob-
lem (Harman and Buckley, 2004), and the fo-
cus of efforts to isolate the key concept terms
in queries (Buckley et al., 2000; Allan et al.,
1996).
3. Good sub-queries were missing many of the
noise terms found in the original query. Ideally
the retrieval model would weight them lower,
but dropping them completely from the query
appeared to be more effective.
</listItem>
<page confidence="0.994704">
221
</page>
<bodyText confidence="0.992993666666667">
4. Sub-queries a human would consider as an in-
complete expression of information need some-
times performed better than the original query.
Our example illustrates this point.
Given the above empirical observations, we ex-
plored a variety of procedures to refine a long query
into a shorter one that retained the key terms. We ex-
pected the set of terms of a good sub-query to have
the following properties.
</bodyText>
<listItem confidence="0.900589555555555">
A. Minimal Cardinality: Any set that contains
more than the minimum number of terms to retrieve
relevant documents could suffer from concept drift.
B. Coherency: The terms that constitute the sub-
query should be coherent, i.e. they should buttress
each other in representing the information need. If
need be, terms that the user considered important but
led to retrieval of non-relevant documents should be
dropped.
</listItem>
<bodyText confidence="0.987093666666667">
Some of the sub-query selection methods we ex-
plored with these properties in mind are reported be-
low.
</bodyText>
<subsectionHeader confidence="0.974024">
2.1 Mutual Information
</subsectionHeader>
<bodyText confidence="0.99852075">
Let X and Y be two random variables, with joint
distribution P(x, y) and marginal distributions P(x)
and P(y) respectively. The mutual information is
then defined as:
</bodyText>
<equation confidence="0.998915">
P(x, y)log P(x, y)
p(x)p(y)
(1)
</equation>
<bodyText confidence="0.997089444444444">
Intuitively, mutual information measures the infor-
mation about X that is shared by Y . If X and Y are
independent, then X contains no information about
Y and vice versa and hence their mutual information
is zero. Mutual Information is attractive because it is
not only easy to compute, but also takes into consid-
eration corpus statistics and semantics. The mutual
information between two terms (Church and Hanks,
1989) can be calculated using Equation 2.
</bodyText>
<equation confidence="0.961505">
I(x,y) = log
</equation>
<bodyText confidence="0.945806333333333">
n(x, y) is the number of times terms x and y oc-
curred within a term window of 100 terms across the
corpus, while n(x) and n(y) are the frequencies of
x and y in the collection of size N terms.
To tackle the situation where we have an arbi-
trary number of variables (terms) we extend the two-
variable case to the multivariate case. The extension,
called multivariate mutual information (MVMI) can
be generalized from Equation 1 to:
</bodyText>
<equation confidence="0.99927475">
I(X1; X2; X3; ...; XN) =
N
(�1)i−1 E H(X) (3)
i=1 XC(X1,X2,X3,...,XN),|X|=k
</equation>
<bodyText confidence="0.9999929">
The calculation of multivariate information using
Equation 3 was very cumbersome, and we instead
worked with the approximation (Kern et al., 2003)
given below.
For the case involving multiple terms, we calcu-
lated MVMI as the sum of the pair-wise mutual in-
formation for all terms in the candidate sub-query.
This can be also viewed as the creation of a com-
pletely connected graph G = (V, E), where the ver-
tices V are the terms and the edges E are weighted
using the mutual information between the vertices
they connect.
To select a score representative of the quality of
a sub-query we considered several options includ-
ing the sum, average, median and minimum of the
edge weights. We performed experiments on a set
of candidate queries to determine how well each of
these measures tracked AP, and found that the aver-
age worked best. We refer to the sub-query selection
procedure using the average score as Average.
</bodyText>
<subsectionHeader confidence="0.999113">
2.2 Maximum Spanning Tree
</subsectionHeader>
<bodyText confidence="0.999978428571428">
It is well-known that an average is easily skewed
by outliers. In other words, the existence of one or
more terms that have low mutual information with
every other term could potentially distort results.
This problem could be further compounded by the
fact that mutual information measured using Equa-
tion 2 could have a negative value. We attempted
</bodyText>
<equation confidence="0.999004">
I(X; Y ) = E E
x y
(2)
n(x)
N
n(x,y)
N
n(y)
N
I(X1; X2; X3; ...; XN) = (4)
E I(Xi; Xj) (5)
i,j={1,2,3,...,N;i�=j}
</equation>
<page confidence="0.988946">
222
</page>
<bodyText confidence="0.9999439">
to tackle this problem by considering another mea-
sure that involved creating a maximum spanning tree
(MaxST) over the fully connected graph G, and us-
ing the weight of the identified tree as a measure rep-
resentative of the candidate query’s quality (Rijsber-
gen, 1979). We used Kruskal’s minimum spanning
tree (Cormen et al., 2001) algorithm after negating
the edge weights to obtain a MaxST. We refer to the
sub-query selection procedure using the weight of
the maximum spanning tree as MaxST.
</bodyText>
<subsectionHeader confidence="0.993304">
2.3 Named Entities
</subsectionHeader>
<bodyText confidence="0.999690846153846">
Named entities (names of persons, places, organiza-
tions, dates, etc.) are known to play an important
anchor role in many information retrieval applica-
tions. In our example from Section 2, sub-queries
without Britain or Argentina will not be effective
even though the mutual information score of the
other two terms international and relations might
indicate otherwise. We experimented with another
version of sub-query selection that considered only
sub-queries that retained at least one of the named
entities from the original query. We refer to the vari-
ants that retained named entities as NE Average and
NE MasT.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999609578947368">
We used version 2.3.2 of the Indri search engine, de-
veloped as part of the Lemur2 project. While the
inference network-based retrieval framework of In-
dri permits the use of structured queries, the use
of language modeling techniques provides better es-
timates of probabilities for query evaluation. The
pseudo-relevance feedback mechanism we used is
based on relevance models (Lavrenko and Croft,
2001).
To extract named entities from the queries, we
used BBN Identifinder (Bikel et al., 1999). The
named entities identified were of type Person, Lo-
cation, Organization, Date, and Time.
We used the TREC Robust 2004 and Robust 2005
(Voorhees, 2006) document collections for our ex-
periments. The 2004 Robust collection contains
around half a million documents from the Finan-
cial Times, the Federal Register, the LA Times, and
FBIS. The Robust 2005 collection is the one-million
</bodyText>
<footnote confidence="0.928604">
2http://www.lemurproject.org
</footnote>
<bodyText confidence="0.998433482758621">
document AQUAINT collection. All the documents
were from English newswire. We chose these col-
lections because they and their associated queries
are known to be hard, and hence present a chal-
lenging environment. We stemmed the collections
using the Krovetz stemmer provided as part of In-
dri, and used a manually-created stoplist of twenty
terms (a, an, and, are, at, as, be, for, in, is, it, of, on,
or, that, the, to, was, with and what). To determine
the best query selection procedure, we analyzed 163
queries from the Robust 2004 track, and used 30 and
50 queries from the 2004 and 2005 Robust tracks re-
spectively for evaluation and user studies.
For all systems, we report mean average preci-
sion (MAP) and geometric mean average precision
(GMAP). MAP is the most widely used measure in
Information Retrieval. While precision is the frac-
tion of the retrieved documents that are relevant, av-
erage precision (AP) is a single value obtained by
averaging the precision values at each new relevant
document observed. MAP is the arithmetic mean of
the APs of a set of queries. Similarly, GMAP is the
geometric mean of the APs of a set of queries. The
GMAP measure is more indicative of performance
across an entire set of queries. MAP can be skewed
by the presence of a few well-performing queries,
and hence is not as good a measure as GMAP from
the perspective of measure comprehensive perfor-
mance.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999698125">
We first ran two baseline experiments to record the
quality of the available long query and the shorter
version. As mentioned in Section 1, we used the
description and title sections of each TREC query
as surrogates for the long and short versions re-
spectively of a query. The results are presented in
the first two rows, Baseline and Pseudo-relevance
Feedback (PRF), of Table 2. Measured in terms of
MAP and GMAP (Section3), using just the title re-
sults in better performance than using the descrip-
tion. This clearly indicates the existence of terms in
the description that while elaborating an information
need hurt retrieval performance. The result of using
pseudo-relevance feedback (PRF) on both the title
and description show moderate gains - a known fact
about this particular collection and associated train-
</bodyText>
<page confidence="0.997823">
223
</page>
<table confidence="0.999477428571429">
MAP GMAP
Long Query Baseline 0.243 0.136
(Description) PRF 0.270 0.124
Short Query Baseline 0.249 0.154
(Title) PRF 0.269 0.148
Best sub-query Baseline 0.342 0.270
(Combination) PRF 0.343 0.241
</table>
<tableCaption confidence="0.99814">
Table 2: Results across 163 training queries on the
</tableCaption>
<bodyText confidence="0.966254787878788">
Robust 2004 collection. Using the best sub-query
results in almost 50% improvement over the baseline
ing queries.
To show the potential and utility of query re-
writing, we first present results that show the upper
bound on performance that can obtained by doing
so. We ran retrieval experiments with every combi-
nation of query terms. For a query of length n, there
are 2n combinations. We limited our experiments
to queries of length n &lt; 12. Selecting the perfor-
mance obtained by the best sub-query of each query
revealed an upper bound in performance almost 50%
better than the baseline (Table 2).
To evaluate the automatic sub-query selection
procedures developed in Section 2, we performed
retrieval experiments using the sub-queries selected
using them. The results, which are presented in Ta-
ble 3, show that the automatic sub-query selection
process was a failure. The results of automatic se-
lection were worse than even the baseline, and there
was no significant difference between using any of
the different sub-query selection procedures.
The failure of the automatic techniques could be
attributed to the fact that we were working with the
assumption that term co-occurrence could be used
to model a user’s information need. To see if there
was any general utility in using the procedures to
select sub-queries, we selected the best-performing
sub-query from the top 10 ranked by each selection
procedure (Table 4). While the effectiveness in each
case as measured by MAP is not close to the best
possible MAP, 0.342, they are all significantly better
than the baseline of 0.243.
</bodyText>
<sectionHeader confidence="0.951125" genericHeader="method">
5 Interacting with the user
</sectionHeader>
<bodyText confidence="0.9569695">
The final results we presented in the last section
hinted at a potential for user interaction. We envi-
</bodyText>
<table confidence="0.998449666666667">
MAP GMAP
Baseline 0.243 0.136
Average 0.172 0.025
MaxST 0.172 0.025
NE Average 0.170 0.023
NE MaxST 0.182 0.029
</table>
<tableCaption confidence="0.998058">
Table 3: Score of the highest rank sub-query by var-
ious measures.
</tableCaption>
<table confidence="0.999759333333333">
MAP GMAP
Baseline 0.243 0.136
AverageTop10 0.296 0.167
MaxSTTop10 0.293 0.150
NE AverageTop10 0.278 0.156
NE MaxSTTop10 0.286 0.159
</table>
<tableCaption confidence="0.919987">
Table 4: Score of the best sub-query in the top 10
ranked by various measures
</tableCaption>
<bodyText confidence="0.999910285714286">
sioned providing the user with a list of the top 10
sub-query candidates using a good ranking proce-
dure, and asking her to select the sub-query she felt
was most appropriate. This additional round of hu-
man intervention could potentially compensate for
the inability of the ranking measures to select the
best sub-query automatically.
</bodyText>
<subsectionHeader confidence="0.949486">
5.1 User interface design
</subsectionHeader>
<bodyText confidence="0.9999931875">
We displayed the description (the long query) and
narrative portion of each TREC query in the inter-
face. The narrative was provided to help the partic-
ipant understand what information the user who is-
sued the query was interested in. The title was kept
hidden to avoid influencing the participant’s choice
of the best sub-query. A list of candidate sub-queries
was displayed along with links that could be clicked
on to display a short section of text in a designated
area. The intention was to provide an example of
what would potentially be retrieved with a high rank
if the candidate sub-query were used. The partici-
pant used this information to make two decisions -
the perceived quality of each sub-query, and the best
sub-query from the list. A facility to indicate that
none of the candidates were good was also included.
</bodyText>
<page confidence="0.99548">
224
</page>
<table confidence="0.9996735">
Percentage of candidates
better than baseline
Average 28.5%
MaxST 35.5%
NE Average 31.1%
NE MaxST 36.6%
</table>
<tableCaption confidence="0.991796">
Table 5: Number of candidates from top 10 that ex-
ceeded the baseline
</tableCaption>
<subsectionHeader confidence="0.993481">
5.2 User interface content issues
</subsectionHeader>
<bodyText confidence="0.997068861111111">
The two key issues we faced while determining the
content of the user interface were:
A. Deciding which sub-query selection procedure
to use to get the top 10 candidate sub-queries: To
determine this in the absence of any significant dif-
ference in performance due to the top-ranked can-
didate selected by each procedure, we looked at the
number of candidates each procedure brought into
the top 10 that were better than the baseline query,
as measured by MAP. This was guided by the belief
that greater the number of better candidates in the
top 10, the higher the probability that the user would
select a better sub-query. Table 5 shows how each of
the selection procedures compared. The NE MaxST
ranking procedure had the most number of better
sub-queries in the top 10, and hence was chosen.
B. Displaying context: Simply displaying a list
of 10 candidates without any supportive information
would make the task of the user difficult. This was in
contrast to query expansion techniques (Anick and
Tipirneni, 1999) where displaying a list of terms suf-
ficed as the task of the user was to disambiguate
or expand a short query. An experiment was per-
formed in which a single user worked with a set of
30 queries from Robust 2004, and an accompanying
set of 10 candidate sub-queries each, twice - once
with passages providing context and one with snip-
pets providing context. The top-ranked passage was
generated by modifying the candidate query into
one that retrieved passages of fixed length instead
of documents. Snippets, like those seen along with
links to top-ranked documents in the results from
almost all popular search engines, were generated
after a document-level query was used to query the
collection. The order in which the two contexts were
presented to the user was randomized to prevent the
</bodyText>
<table confidence="0.930307666666667">
MAP GMAP
Snippet as Context 0.348 0.170
Passage as Context 0.296 0.151
</table>
<tableCaption confidence="0.915507">
Table 6: Results showing the MAP over 19 of 30
</tableCaption>
<bodyText confidence="0.986362916666667">
queries that the user provided selections for using
each context type.
user from assuming a quality order. We see that pre-
senting the snippet led to better MAP that presenting
the passage (Table 6). The reason for this could be
that the top-ranking passage we displayed was from
a document ranked lower by the document-focussed
version of the query. Since we finally measure MAP
only with respect to document ranking, and the snip-
pet was generated from the top-ranked document,
we hypothesize that this led to the snippet being a
better context to display.
</bodyText>
<sectionHeader confidence="0.989692" genericHeader="method">
6 User Evaluation
</sectionHeader>
<bodyText confidence="0.999506291666667">
We conducted an exploratory study with five par-
ticipants - four of them were graduate students in
computer science while the fifth had a background
in the social sciences and was reasonably proficient
in the use of computers and internet search engines.
The participants worked with 30 queries from Ro-
bust 2004, and 50 from Robust 20053. The baseline
values reported are automatic runs with the descrip-
tion as the query.
Table 7 shows that all five participants4 were
able to choose sub-queries that led to an improve-
ment in performance over the baseline (TREC title
query only). This improvement is not only on MAP
but also on GMAP, indicating that user interaction
helped improve a wide spectrum of queries. Most
notable were the improvements in P@5 and P@10.
This attested to the fact that the interaction tech-
nique we explored was precision-enhancing. An-
other interesting result, from # sub-queries selected
was that participants were able to decide in a large
number of cases that re-writing was either not useful
for a query, or that none of the options presented to
them were better. Showing context appears to have
helped.
</bodyText>
<footnote confidence="0.994177">
3Participant 4 looked that only 34 of the 50 queries presented
4The P value for testing statistical significance of MAP im-
provement for Participant 5 was 0.053 - the result very narrowly
missed being statistically significant.
</footnote>
<page confidence="0.989459">
225
</page>
<table confidence="0.999961823529412">
# Queries # sub-queries % sub-queries MAP GMAP P@5 p@10
selected better
Baseline 0.203 0.159 0. 476 0.507
1 50 26 80.7% With Interaction 0.249 0.199 0.615 0.580
Upper Bound 0.336 0.282 0.784 0.719
Baseline 0.224 0.156 0.484 0.526
2 50 19 78.9% With Interaction 0.277 0.209 0.652 0.621
Upper Bound 0.359 0.293 0.810 0.742
Baseline 0.217 0.126 0.452 0.432
3 80 53 73.5% With Interaction 0.276 0.166 0.573 0.501
Upper Bound 0.354 0.263 0.762 0.654
Baseline 0.192 0.142 0.462 0.525
4 50(34) 19 68.7% With Interaction 0.255 0.175 0.612 0.600
Upper Bound 0.344 0.310 0.862 0.800
Baseline 0.206 0.111 0.433 0.410
5 80 65 61.5% With Interaction 0.231 0.115 0.486 0.429
Upper Bound 0.341 0.245 0.738 0.640
</table>
<tableCaption confidence="0.976284">
Table 7: # Queries refers to the number of queries that were presented to the participant while # sub-queries
</tableCaption>
<bodyText confidence="0.956186">
selected refers to the number of queries for which the participant chose a sub-query. All scores including
upper bounds were calculated only considering the queries for which the participant selected a sub-query.
An entry in bold means that the improvement in MAP is statistically significant. Statistical significance was
measured using a paired t-test, with α set to 0.05.
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999782953488372">
Our interest in finding a concise sub-query that ef-
fectively captures the information need is reminis-
cent of previous work in (Buckley et al., 2000).
However, the focus was more on balancing the ef-
fect of query expansion techniques such that differ-
ent concepts in the query were equally benefited.
Mutual information has been used previously in
(Church and Hanks, 1989) to identify collocations of
terms for identifying semantic relationships in text.
Experiments were confined to bigrams. The use of
MaST over a graph of mutual information values
to incorporate the most significant dependencies be-
tween terms was first noted in (Rijsbergen, 1979).
Extensions can be found in a different field - image
processing (Kern et al., 2003) - where multivariate
mutual information is frequently used.
Work done by (White et al., 2005) provided a ba-
sis for our decision to show context for sub-query se-
lection. The useful result that top-ranked sentences
could be used to guide users towards relevant mate-
rial helped us design an user interface that the par-
ticipants found very convenient to use.
A related problem addressed by (Cronen-
Townsend et al., 2002) was determining query qual-
ity. This is known to be a very hard problem, and
various efforts (Carmel et al., 2006; Vinay et al.,
2006) have been made towards formalizing and un-
derstanding it.
Previous work (Shapiro and Taksa, 2003) in the
web environment attempted to convert a user’s natu-
ral language query into one suited for use with web
search engines. However, the focus was on merg-
ing the results from using different sub-queries, and
not selection of a single sub-query. Our approach
of re-writing queries could be compared to query re-
formulation, wherein a user follows up a query with
successive reformulations of the original. In the web
environment, studies have shown that most users
still enter only one or two queries, and conduct lim-
ited query reformulation (Spink et al., 2002). We hy-
pothesize that the techniques we have developed will
be well-suited for search engines like Ask Jeeves
where 50% of the queries are in question format
</bodyText>
<page confidence="0.994138">
226
</page>
<bodyText confidence="0.6528382">
(Spink and Ozmultu, 2002). More experimentation
in the Web domain is required to substantiate this.
David Carmel, Elad Yom-Tov, Adam Darlow, and Dan Pelleg.
2006. What makes a query difficult? In 29th ACMSIGIR
Proceedings, pages 390–397.
</bodyText>
<sectionHeader confidence="0.998041" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999994125">
Our results clearly show that shorter reformulations
of long queries can greatly impact performance. We
believe that our technique has great potential to be
used in an adaptive information retrieval environ-
ment, where the user starts off with a more general
information need and a looser notion of relevance.
The initial query can then be made longer to express
a most focused information need.
As part of future work, we plan to conduct a more
elaborate study with more interaction strategies in-
cluded. Better techniques to select effective sub-
queries are also in the pipeline. Since we used mu-
tual information as the basis for most of our sub-
query selection procedures, we could not consider
sub-queries that comprised of a single term. We plan
to address this issue too in future work.
</bodyText>
<sectionHeader confidence="0.998445" genericHeader="acknowledgments">
9 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999864222222222">
This work was supported in part by the Center
for Intelligent Information Retrieval and in part by
the Defense Advanced Research Projects Agency
(DARPA) under contract number HR0011-06-C-
0023. Any opinions, findings and conclusions or
recommendations expressed in this material are the
authors and do not necessarily reflect those of the
sponsor. We also thank the anonymous reviewers
for their valuable comments.
</bodyText>
<sectionHeader confidence="0.999479" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999804833333333">
James Allan, James P. Callan, W. Bruce Croft, Lisa Ballesteros,
John Broglio, Jinxi Xu, and Hongming Shu. 1996. Inquery
at TREC-5. In TREC.
Peter G. Anick and Suresh Tipirneni. 1999. The paraphrase
search assistant: terminological feedback for iterative infor-
mation seeking. In 22nd ACM SIGIR Proceedings, pages
153–159.
Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel.
1999. An algorithm that learns what’s in a name. Machine
Learning, 34(1-3):211–231.
Chris Buckley, Mandar Mitra, Janet Walz, and Claire Cardie.
2000. Using clustering and superconcepts within smart:
TREC 6. Information Processing and Management,
36(1):109–131.
Kenneth Ward Church and Patrick Hanks. 1989. Word associ-
ation norms, mutual information, and lexicography. In 27th
ACL Proceedings, pages 76–83.
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest,
and Clifford Stein. 2001. Introduction to Algorithms, Sec-
ond Edition. The MIT Electrical Engineering and Computer
Science Series. The MIT Press.
Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. 2002.
Predicting query performance. In 25th ACMSIGIR Proceed-
ings, pages 299–306.
Donna Harman and Chris Buckley. 2004. The NRRC reliable
information access (RIA) workshop. In 27th ACM SIGIR
Proceedings, pages 528–529.
Jeffrey P. Kern, Marios Pattichis, and Samuel D. Stearns. 2003.
Registration of image cubes using multivariate mutual infor-
mation. In Thirty-Seventh Asilomar Conference, volume 2,
pages 1645–1649.
Reiner Kraft, Chi Chao Chang, Farzin Maghoul, and Ravi Ku-
mar. 2006. Searching with context. In 15th International
CIKM Conference Proceedings, pages 477–486.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance based
language models. In 24th ACM SIGIR Conference Proceed-
ings, pages 120–127.
C. J. Van Rijsbergen. 1979. Information Retrieval.
Butterworth-Heinemann, Newton, MA, USA, 2 edition.
Jacob Shapiro and Isak Taksa. 2003. Constructing web search
queries from the user’s information need expressed in a nat-
ural language. In Proceedings of the 2003 ACM Symposium
on Applied Computing, pages 1157–1162.
Amanda Spink and H. Cenk Ozmultu. 2002. Characteristics of
question format web queries: An exploratory study. Infor-
mation Processing and Management, 38(4):453–471.
Amanda Spink, Bernard J. Jansen, Dietmar Wolfram, and Tefko
Saracevic. 2002. From e-sex to e-commerce: Web search
changes. Computer, 35(3):107–109.
Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005. Per-
sonalizing search via automated analysis of interests and ac-
tivities. In 28th ACM SIGIR Proceedings, pages 449–456.
Vishwa Vinay, Ingemar J. Cox, Natasa Milic-Frayling, and Ken
Wood. 2006. On ranking the effectiveness of searches. In
29th ACM SIGIR Proceedings, pages 398–404.
Ellen M. Voorhees. 2006. The TREC 2005 robust track. SIGIR
Forum, 40(1):41–48.
Ryen W. White, Joemon M. Jose, and Ian Ruthven. 2005. Us-
ing top-ranking sentences to facilitate effective information
access: Book reviews. JAIST, 56(10):1113–1125.
</reference>
<page confidence="0.997934">
227
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.874823">
<title confidence="0.998684">A Case for Shorter Queries, and Helping Users Create Them</title>
<author confidence="0.889538">Giridhar Kumaran</author>
<author confidence="0.889538">James</author>
<affiliation confidence="0.997689">Center for Intelligent Information Department of Computer University of Massachusetts</affiliation>
<address confidence="0.999835">Amherst, MA 01003,</address>
<abstract confidence="0.99923735">Information retrieval systems are frequently required to handle long queries. Simply using all terms in the query or relying on the underlying retrieval model to appropriately weight terms often leads to ineffective retrieval. We show that rewriting the query to a version that comprises a small subset of appropriate terms from the original query greatly improves effectiveness. Targeting a demonstrated potential improvement of almost 50% on queries and their associated collections, we develop a suite of automatic techniques to re-write queries and study their characteristics. We show that the shortcomings of automatic methcan be ameliorated by some user interaction, and report results that are on average 25% better than the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>James P Callan</author>
<author>W Bruce Croft</author>
</authors>
<title>Inquery at TREC-5. In TREC.</title>
<date>1996</date>
<location>Lisa Ballesteros, John Broglio, Jinxi</location>
<contexts>
<context position="7179" citStr="Allan et al., 1996" startWordPosition="1159" endWordPosition="1162">ntina, omitting any reference to international relations. This also reveals a mismatch between the user’s query and the way terms occurred in the corpus, and suggests that an approximate query could at times be a better starting point for search. 2. The sub-query would often contain only terms that a human would consider vital to the query while the original query would also (naturally) contain them, albeit weighted lower with respect to other terms. This is a common problem (Harman and Buckley, 2004), and the focus of efforts to isolate the key concept terms in queries (Buckley et al., 2000; Allan et al., 1996). 3. Good sub-queries were missing many of the noise terms found in the original query. Ideally the retrieval model would weight them lower, but dropping them completely from the query appeared to be more effective. 221 4. Sub-queries a human would consider as an incomplete expression of information need sometimes performed better than the original query. Our example illustrates this point. Given the above empirical observations, we explored a variety of procedures to refine a long query into a shorter one that retained the key terms. We expected the set of terms of a good sub-query to have th</context>
</contexts>
<marker>Allan, Callan, Croft, 1996</marker>
<rawString>James Allan, James P. Callan, W. Bruce Croft, Lisa Ballesteros, John Broglio, Jinxi Xu, and Hongming Shu. 1996. Inquery at TREC-5. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter G Anick</author>
<author>Suresh Tipirneni</author>
</authors>
<title>The paraphrase search assistant: terminological feedback for iterative information seeking.</title>
<date>1999</date>
<booktitle>In 22nd ACM SIGIR Proceedings,</booktitle>
<pages>153--159</pages>
<contexts>
<context position="19973" citStr="Anick and Tipirneni, 1999" startWordPosition="3296" endWordPosition="3299"> top 10 that were better than the baseline query, as measured by MAP. This was guided by the belief that greater the number of better candidates in the top 10, the higher the probability that the user would select a better sub-query. Table 5 shows how each of the selection procedures compared. The NE MaxST ranking procedure had the most number of better sub-queries in the top 10, and hence was chosen. B. Displaying context: Simply displaying a list of 10 candidates without any supportive information would make the task of the user difficult. This was in contrast to query expansion techniques (Anick and Tipirneni, 1999) where displaying a list of terms sufficed as the task of the user was to disambiguate or expand a short query. An experiment was performed in which a single user worked with a set of 30 queries from Robust 2004, and an accompanying set of 10 candidate sub-queries each, twice - once with passages providing context and one with snippets providing context. The top-ranked passage was generated by modifying the candidate query into one that retrieved passages of fixed length instead of documents. Snippets, like those seen along with links to top-ranked documents in the results from almost all popu</context>
</contexts>
<marker>Anick, Tipirneni, 1999</marker>
<rawString>Peter G. Anick and Suresh Tipirneni. 1999. The paraphrase search assistant: terminological feedback for iterative information seeking. In 22nd ACM SIGIR Proceedings, pages 153–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph M Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="12564" citStr="Bikel et al., 1999" startWordPosition="2064" endWordPosition="2067">from the original query. We refer to the variants that retained named entities as NE Average and NE MasT. 3 Experimental Setup We used version 2.3.2 of the Indri search engine, developed as part of the Lemur2 project. While the inference network-based retrieval framework of Indri permits the use of structured queries, the use of language modeling techniques provides better estimates of probabilities for query evaluation. The pseudo-relevance feedback mechanism we used is based on relevance models (Lavrenko and Croft, 2001). To extract named entities from the queries, we used BBN Identifinder (Bikel et al., 1999). The named entities identified were of type Person, Location, Organization, Date, and Time. We used the TREC Robust 2004 and Robust 2005 (Voorhees, 2006) document collections for our experiments. The 2004 Robust collection contains around half a million documents from the Financial Times, the Federal Register, the LA Times, and FBIS. The Robust 2005 collection is the one-million 2http://www.lemurproject.org document AQUAINT collection. All the documents were from English newswire. We chose these collections because they and their associated queries are known to be hard, and hence present a ch</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning, 34(1-3):211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Buckley</author>
<author>Mandar Mitra</author>
<author>Janet Walz</author>
<author>Claire Cardie</author>
</authors>
<title>Using clustering and superconcepts within smart:</title>
<date>2000</date>
<booktitle>TREC 6. Information Processing and Management,</booktitle>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="7158" citStr="Buckley et al., 2000" startWordPosition="1155" endWordPosition="1158">ample was britain argentina, omitting any reference to international relations. This also reveals a mismatch between the user’s query and the way terms occurred in the corpus, and suggests that an approximate query could at times be a better starting point for search. 2. The sub-query would often contain only terms that a human would consider vital to the query while the original query would also (naturally) contain them, albeit weighted lower with respect to other terms. This is a common problem (Harman and Buckley, 2004), and the focus of efforts to isolate the key concept terms in queries (Buckley et al., 2000; Allan et al., 1996). 3. Good sub-queries were missing many of the noise terms found in the original query. Ideally the retrieval model would weight them lower, but dropping them completely from the query appeared to be more effective. 221 4. Sub-queries a human would consider as an incomplete expression of information need sometimes performed better than the original query. Our example illustrates this point. Given the above empirical observations, we explored a variety of procedures to refine a long query into a shorter one that retained the key terms. We expected the set of terms of a good</context>
<context position="24164" citStr="Buckley et al., 2000" startWordPosition="3999" endWordPosition="4002">o the number of queries that were presented to the participant while # sub-queries selected refers to the number of queries for which the participant chose a sub-query. All scores including upper bounds were calculated only considering the queries for which the participant selected a sub-query. An entry in bold means that the improvement in MAP is statistically significant. Statistical significance was measured using a paired t-test, with α set to 0.05. 7 Related Work Our interest in finding a concise sub-query that effectively captures the information need is reminiscent of previous work in (Buckley et al., 2000). However, the focus was more on balancing the effect of query expansion techniques such that different concepts in the query were equally benefited. Mutual information has been used previously in (Church and Hanks, 1989) to identify collocations of terms for identifying semantic relationships in text. Experiments were confined to bigrams. The use of MaST over a graph of mutual information values to incorporate the most significant dependencies between terms was first noted in (Rijsbergen, 1979). Extensions can be found in a different field - image processing (Kern et al., 2003) - where multiv</context>
</contexts>
<marker>Buckley, Mitra, Walz, Cardie, 2000</marker>
<rawString>Chris Buckley, Mandar Mitra, Janet Walz, and Claire Cardie. 2000. Using clustering and superconcepts within smart: TREC 6. Information Processing and Management, 36(1):109–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1989</date>
<booktitle>In 27th ACL Proceedings,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="8960" citStr="Church and Hanks, 1989" startWordPosition="1453" endWordPosition="1456">ation Let X and Y be two random variables, with joint distribution P(x, y) and marginal distributions P(x) and P(y) respectively. The mutual information is then defined as: P(x, y)log P(x, y) p(x)p(y) (1) Intuitively, mutual information measures the information about X that is shared by Y . If X and Y are independent, then X contains no information about Y and vice versa and hence their mutual information is zero. Mutual Information is attractive because it is not only easy to compute, but also takes into consideration corpus statistics and semantics. The mutual information between two terms (Church and Hanks, 1989) can be calculated using Equation 2. I(x,y) = log n(x, y) is the number of times terms x and y occurred within a term window of 100 terms across the corpus, while n(x) and n(y) are the frequencies of x and y in the collection of size N terms. To tackle the situation where we have an arbitrary number of variables (terms) we extend the twovariable case to the multivariate case. The extension, called multivariate mutual information (MVMI) can be generalized from Equation 1 to: I(X1; X2; X3; ...; XN) = N (�1)i−1 E H(X) (3) i=1 XC(X1,X2,X3,...,XN),|X|=k The calculation of multivariate information u</context>
<context position="24385" citStr="Church and Hanks, 1989" startWordPosition="4035" endWordPosition="4038">only considering the queries for which the participant selected a sub-query. An entry in bold means that the improvement in MAP is statistically significant. Statistical significance was measured using a paired t-test, with α set to 0.05. 7 Related Work Our interest in finding a concise sub-query that effectively captures the information need is reminiscent of previous work in (Buckley et al., 2000). However, the focus was more on balancing the effect of query expansion techniques such that different concepts in the query were equally benefited. Mutual information has been used previously in (Church and Hanks, 1989) to identify collocations of terms for identifying semantic relationships in text. Experiments were confined to bigrams. The use of MaST over a graph of mutual information values to incorporate the most significant dependencies between terms was first noted in (Rijsbergen, 1979). Extensions can be found in a different field - image processing (Kern et al., 2003) - where multivariate mutual information is frequently used. Work done by (White et al., 2005) provided a basis for our decision to show context for sub-query selection. The useful result that top-ranked sentences could be used to guide</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1989. Word association norms, mutual information, and lexicography. In 27th ACL Proceedings, pages 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
<author>Clifford Stein</author>
</authors>
<title>Introduction to Algorithms, Second Edition. The MIT Electrical Engineering and Computer Science Series.</title>
<date>2001</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="11251" citStr="Cormen et al., 2001" startWordPosition="1857" endWordPosition="1860">otentially distort results. This problem could be further compounded by the fact that mutual information measured using Equation 2 could have a negative value. We attempted I(X; Y ) = E E x y (2) n(x) N n(x,y) N n(y) N I(X1; X2; X3; ...; XN) = (4) E I(Xi; Xj) (5) i,j={1,2,3,...,N;i�=j} 222 to tackle this problem by considering another measure that involved creating a maximum spanning tree (MaxST) over the fully connected graph G, and using the weight of the identified tree as a measure representative of the candidate query’s quality (Rijsbergen, 1979). We used Kruskal’s minimum spanning tree (Cormen et al., 2001) algorithm after negating the edge weights to obtain a MaxST. We refer to the sub-query selection procedure using the weight of the maximum spanning tree as MaxST. 2.3 Named Entities Named entities (names of persons, places, organizations, dates, etc.) are known to play an important anchor role in many information retrieval applications. In our example from Section 2, sub-queries without Britain or Argentina will not be effective even though the mutual information score of the other two terms international and relations might indicate otherwise. We experimented with another version of sub-quer</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, Stein, 2001</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2001. Introduction to Algorithms, Second Edition. The MIT Electrical Engineering and Computer Science Series. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Cronen-Townsend</author>
<author>Yun Zhou</author>
<author>W Bruce Croft</author>
</authors>
<title>Predicting query performance.</title>
<date>2002</date>
<booktitle>In 25th ACMSIGIR Proceedings,</booktitle>
<pages>299--306</pages>
<marker>Cronen-Townsend, Zhou, Croft, 2002</marker>
<rawString>Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. 2002. Predicting query performance. In 25th ACMSIGIR Proceedings, pages 299–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Harman</author>
<author>Chris Buckley</author>
</authors>
<title>The NRRC reliable information access (RIA) workshop.</title>
<date>2004</date>
<booktitle>In 27th ACM SIGIR Proceedings,</booktitle>
<pages>528--529</pages>
<contexts>
<context position="2509" citStr="Harman and Buckley, 2004" startWordPosition="405" endWordPosition="408">ies would provide more information in the form of context (Kraft et al., 2006), and this additional information could be leveraged to provide a better search experience. However, handling such long queries is a challenge. The use of all the terms from the user’s input can rapidly narrow down the set of matching documents, especially if a boolean retrieval model is adopted. While one would expect the underlying retrieval model to appropriately assign weights to different terms in the query and return only relevant content, it is widely acknowledged that models fail due to a variety of reasons (Harman and Buckley, 2004), and are not suited to tackle every possible query. Recently, there has been great interest in personalized search (Teevan et al., 2005), where the query is modified based on a user’s profile. The profile usually consists of documents previously viewed, web sites recently visited, e-mail correspondence and so on. Common procedures for using this large amount of information usually involve creating huge query vectors with some sort of term-weighting mechanism to favor different portions of the profile. The queries used in the TREC ad-hoc tracks consist of title, description and narrative secti</context>
<context position="7066" citStr="Harman and Buckley, 2004" startWordPosition="1137" endWordPosition="1140">rmation desired were missing from the best subqueries. For example, the best sub-query for the example was britain argentina, omitting any reference to international relations. This also reveals a mismatch between the user’s query and the way terms occurred in the corpus, and suggests that an approximate query could at times be a better starting point for search. 2. The sub-query would often contain only terms that a human would consider vital to the query while the original query would also (naturally) contain them, albeit weighted lower with respect to other terms. This is a common problem (Harman and Buckley, 2004), and the focus of efforts to isolate the key concept terms in queries (Buckley et al., 2000; Allan et al., 1996). 3. Good sub-queries were missing many of the noise terms found in the original query. Ideally the retrieval model would weight them lower, but dropping them completely from the query appeared to be more effective. 221 4. Sub-queries a human would consider as an incomplete expression of information need sometimes performed better than the original query. Our example illustrates this point. Given the above empirical observations, we explored a variety of procedures to refine a long </context>
</contexts>
<marker>Harman, Buckley, 2004</marker>
<rawString>Donna Harman and Chris Buckley. 2004. The NRRC reliable information access (RIA) workshop. In 27th ACM SIGIR Proceedings, pages 528–529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey P Kern</author>
<author>Marios Pattichis</author>
<author>Samuel D Stearns</author>
</authors>
<title>Registration of image cubes using multivariate mutual information.</title>
<date>2003</date>
<booktitle>In Thirty-Seventh Asilomar Conference,</booktitle>
<volume>2</volume>
<pages>1645--1649</pages>
<contexts>
<context position="9661" citStr="Kern et al., 2003" startWordPosition="1576" endWordPosition="1579">rms x and y occurred within a term window of 100 terms across the corpus, while n(x) and n(y) are the frequencies of x and y in the collection of size N terms. To tackle the situation where we have an arbitrary number of variables (terms) we extend the twovariable case to the multivariate case. The extension, called multivariate mutual information (MVMI) can be generalized from Equation 1 to: I(X1; X2; X3; ...; XN) = N (�1)i−1 E H(X) (3) i=1 XC(X1,X2,X3,...,XN),|X|=k The calculation of multivariate information using Equation 3 was very cumbersome, and we instead worked with the approximation (Kern et al., 2003) given below. For the case involving multiple terms, we calculated MVMI as the sum of the pair-wise mutual information for all terms in the candidate sub-query. This can be also viewed as the creation of a completely connected graph G = (V, E), where the vertices V are the terms and the edges E are weighted using the mutual information between the vertices they connect. To select a score representative of the quality of a sub-query we considered several options including the sum, average, median and minimum of the edge weights. We performed experiments on a set of candidate queries to determin</context>
<context position="24749" citStr="Kern et al., 2003" startWordPosition="4092" endWordPosition="4095">us work in (Buckley et al., 2000). However, the focus was more on balancing the effect of query expansion techniques such that different concepts in the query were equally benefited. Mutual information has been used previously in (Church and Hanks, 1989) to identify collocations of terms for identifying semantic relationships in text. Experiments were confined to bigrams. The use of MaST over a graph of mutual information values to incorporate the most significant dependencies between terms was first noted in (Rijsbergen, 1979). Extensions can be found in a different field - image processing (Kern et al., 2003) - where multivariate mutual information is frequently used. Work done by (White et al., 2005) provided a basis for our decision to show context for sub-query selection. The useful result that top-ranked sentences could be used to guide users towards relevant material helped us design an user interface that the participants found very convenient to use. A related problem addressed by (CronenTownsend et al., 2002) was determining query quality. This is known to be a very hard problem, and various efforts (Carmel et al., 2006; Vinay et al., 2006) have been made towards formalizing and understand</context>
</contexts>
<marker>Kern, Pattichis, Stearns, 2003</marker>
<rawString>Jeffrey P. Kern, Marios Pattichis, and Samuel D. Stearns. 2003. Registration of image cubes using multivariate mutual information. In Thirty-Seventh Asilomar Conference, volume 2, pages 1645–1649.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reiner Kraft</author>
<author>Chi Chao Chang</author>
<author>Farzin Maghoul</author>
<author>Ravi Kumar</author>
</authors>
<title>Searching with context.</title>
<date>2006</date>
<booktitle>In 15th International CIKM Conference Proceedings,</booktitle>
<pages>477--486</pages>
<contexts>
<context position="1962" citStr="Kraft et al., 2006" startWordPosition="314" endWordPosition="317">blem – query re-writing. Given a long query we explore whether there is utility in modifying it to a more concise version such that the original information need is still expressed. The Y!Q beta1 search engine allows users to select large portions of text from documents and issue them as queries. The search engine is designed to encourage users to submit long queries such as this example from the web site “I need to know the gas mileage for my Audi A8 2004 model”. The motivation for encouraging this type of querying is that longer queries would provide more information in the form of context (Kraft et al., 2006), and this additional information could be leveraged to provide a better search experience. However, handling such long queries is a challenge. The use of all the terms from the user’s input can rapidly narrow down the set of matching documents, especially if a boolean retrieval model is adopted. While one would expect the underlying retrieval model to appropriately assign weights to different terms in the query and return only relevant content, it is widely acknowledged that models fail due to a variety of reasons (Harman and Buckley, 2004), and are not suited to tackle every possible query. </context>
</contexts>
<marker>Kraft, Chang, Maghoul, Kumar, 2006</marker>
<rawString>Reiner Kraft, Chi Chao Chang, Farzin Maghoul, and Ravi Kumar. 2006. Searching with context. In 15th International CIKM Conference Proceedings, pages 477–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>W Bruce Croft</author>
</authors>
<title>Relevance based language models.</title>
<date>2001</date>
<booktitle>In 24th ACM SIGIR Conference Proceedings,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="12473" citStr="Lavrenko and Croft, 2001" startWordPosition="2049" endWordPosition="2052">uery selection that considered only sub-queries that retained at least one of the named entities from the original query. We refer to the variants that retained named entities as NE Average and NE MasT. 3 Experimental Setup We used version 2.3.2 of the Indri search engine, developed as part of the Lemur2 project. While the inference network-based retrieval framework of Indri permits the use of structured queries, the use of language modeling techniques provides better estimates of probabilities for query evaluation. The pseudo-relevance feedback mechanism we used is based on relevance models (Lavrenko and Croft, 2001). To extract named entities from the queries, we used BBN Identifinder (Bikel et al., 1999). The named entities identified were of type Person, Location, Organization, Date, and Time. We used the TREC Robust 2004 and Robust 2005 (Voorhees, 2006) document collections for our experiments. The 2004 Robust collection contains around half a million documents from the Financial Times, the Federal Register, the LA Times, and FBIS. The Robust 2005 collection is the one-million 2http://www.lemurproject.org document AQUAINT collection. All the documents were from English newswire. We chose these collect</context>
</contexts>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>Victor Lavrenko and W. Bruce Croft. 2001. Relevance based language models. In 24th ACM SIGIR Conference Proceedings, pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<volume>2</volume>
<pages>edition.</pages>
<location>Butterworth-Heinemann, Newton, MA, USA,</location>
<marker>Van Rijsbergen, 1979</marker>
<rawString>C. J. Van Rijsbergen. 1979. Information Retrieval. Butterworth-Heinemann, Newton, MA, USA, 2 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Shapiro</author>
<author>Isak Taksa</author>
</authors>
<title>Constructing web search queries from the user’s information need expressed in a natural language.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 ACM Symposium on Applied Computing,</booktitle>
<pages>1157--1162</pages>
<contexts>
<context position="25396" citStr="Shapiro and Taksa, 2003" startWordPosition="4202" endWordPosition="4205">utual information is frequently used. Work done by (White et al., 2005) provided a basis for our decision to show context for sub-query selection. The useful result that top-ranked sentences could be used to guide users towards relevant material helped us design an user interface that the participants found very convenient to use. A related problem addressed by (CronenTownsend et al., 2002) was determining query quality. This is known to be a very hard problem, and various efforts (Carmel et al., 2006; Vinay et al., 2006) have been made towards formalizing and understanding it. Previous work (Shapiro and Taksa, 2003) in the web environment attempted to convert a user’s natural language query into one suited for use with web search engines. However, the focus was on merging the results from using different sub-queries, and not selection of a single sub-query. Our approach of re-writing queries could be compared to query reformulation, wherein a user follows up a query with successive reformulations of the original. In the web environment, studies have shown that most users still enter only one or two queries, and conduct limited query reformulation (Spink et al., 2002). We hypothesize that the techniques w</context>
</contexts>
<marker>Shapiro, Taksa, 2003</marker>
<rawString>Jacob Shapiro and Isak Taksa. 2003. Constructing web search queries from the user’s information need expressed in a natural language. In Proceedings of the 2003 ACM Symposium on Applied Computing, pages 1157–1162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Spink</author>
<author>H Cenk Ozmultu</author>
</authors>
<title>Characteristics of question format web queries: An exploratory study.</title>
<date>2002</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>38</volume>
<issue>4</issue>
<contexts>
<context position="26145" citStr="Spink and Ozmultu, 2002" startWordPosition="4328" endWordPosition="4331"> However, the focus was on merging the results from using different sub-queries, and not selection of a single sub-query. Our approach of re-writing queries could be compared to query reformulation, wherein a user follows up a query with successive reformulations of the original. In the web environment, studies have shown that most users still enter only one or two queries, and conduct limited query reformulation (Spink et al., 2002). We hypothesize that the techniques we have developed will be well-suited for search engines like Ask Jeeves where 50% of the queries are in question format 226 (Spink and Ozmultu, 2002). More experimentation in the Web domain is required to substantiate this. David Carmel, Elad Yom-Tov, Adam Darlow, and Dan Pelleg. 2006. What makes a query difficult? In 29th ACMSIGIR Proceedings, pages 390–397. 8 Conclusions Our results clearly show that shorter reformulations of long queries can greatly impact performance. We believe that our technique has great potential to be used in an adaptive information retrieval environment, where the user starts off with a more general information need and a looser notion of relevance. The initial query can then be made longer to express a most focu</context>
</contexts>
<marker>Spink, Ozmultu, 2002</marker>
<rawString>Amanda Spink and H. Cenk Ozmultu. 2002. Characteristics of question format web queries: An exploratory study. Information Processing and Management, 38(4):453–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Spink</author>
<author>Bernard J Jansen</author>
<author>Dietmar Wolfram</author>
<author>Tefko Saracevic</author>
</authors>
<title>From e-sex to e-commerce: Web search changes.</title>
<date>2002</date>
<journal>Computer,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="25958" citStr="Spink et al., 2002" startWordPosition="4296" endWordPosition="4299">understanding it. Previous work (Shapiro and Taksa, 2003) in the web environment attempted to convert a user’s natural language query into one suited for use with web search engines. However, the focus was on merging the results from using different sub-queries, and not selection of a single sub-query. Our approach of re-writing queries could be compared to query reformulation, wherein a user follows up a query with successive reformulations of the original. In the web environment, studies have shown that most users still enter only one or two queries, and conduct limited query reformulation (Spink et al., 2002). We hypothesize that the techniques we have developed will be well-suited for search engines like Ask Jeeves where 50% of the queries are in question format 226 (Spink and Ozmultu, 2002). More experimentation in the Web domain is required to substantiate this. David Carmel, Elad Yom-Tov, Adam Darlow, and Dan Pelleg. 2006. What makes a query difficult? In 29th ACMSIGIR Proceedings, pages 390–397. 8 Conclusions Our results clearly show that shorter reformulations of long queries can greatly impact performance. We believe that our technique has great potential to be used in an adaptive informati</context>
</contexts>
<marker>Spink, Jansen, Wolfram, Saracevic, 2002</marker>
<rawString>Amanda Spink, Bernard J. Jansen, Dietmar Wolfram, and Tefko Saracevic. 2002. From e-sex to e-commerce: Web search changes. Computer, 35(3):107–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Teevan</author>
<author>Susan T Dumais</author>
<author>Eric Horvitz</author>
</authors>
<title>Personalizing search via automated analysis of interests and activities.</title>
<date>2005</date>
<booktitle>In 28th ACM SIGIR Proceedings,</booktitle>
<pages>449--456</pages>
<contexts>
<context position="2646" citStr="Teevan et al., 2005" startWordPosition="428" endWordPosition="431">a better search experience. However, handling such long queries is a challenge. The use of all the terms from the user’s input can rapidly narrow down the set of matching documents, especially if a boolean retrieval model is adopted. While one would expect the underlying retrieval model to appropriately assign weights to different terms in the query and return only relevant content, it is widely acknowledged that models fail due to a variety of reasons (Harman and Buckley, 2004), and are not suited to tackle every possible query. Recently, there has been great interest in personalized search (Teevan et al., 2005), where the query is modified based on a user’s profile. The profile usually consists of documents previously viewed, web sites recently visited, e-mail correspondence and so on. Common procedures for using this large amount of information usually involve creating huge query vectors with some sort of term-weighting mechanism to favor different portions of the profile. The queries used in the TREC ad-hoc tracks consist of title, description and narrative sections, of progressively increasing length. The title, of length 1http://yq.search.yahoo.com/ 220 Proceedings of NAACL HLT 2007, pages 220–2</context>
</contexts>
<marker>Teevan, Dumais, Horvitz, 2005</marker>
<rawString>Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005. Personalizing search via automated analysis of interests and activities. In 28th ACM SIGIR Proceedings, pages 449–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vishwa Vinay</author>
<author>Ingemar J Cox</author>
<author>Natasa Milic-Frayling</author>
<author>Ken Wood</author>
</authors>
<title>On ranking the effectiveness of searches.</title>
<date>2006</date>
<booktitle>In 29th ACM SIGIR Proceedings,</booktitle>
<pages>398--404</pages>
<contexts>
<context position="25299" citStr="Vinay et al., 2006" startWordPosition="4187" endWordPosition="4190"> be found in a different field - image processing (Kern et al., 2003) - where multivariate mutual information is frequently used. Work done by (White et al., 2005) provided a basis for our decision to show context for sub-query selection. The useful result that top-ranked sentences could be used to guide users towards relevant material helped us design an user interface that the participants found very convenient to use. A related problem addressed by (CronenTownsend et al., 2002) was determining query quality. This is known to be a very hard problem, and various efforts (Carmel et al., 2006; Vinay et al., 2006) have been made towards formalizing and understanding it. Previous work (Shapiro and Taksa, 2003) in the web environment attempted to convert a user’s natural language query into one suited for use with web search engines. However, the focus was on merging the results from using different sub-queries, and not selection of a single sub-query. Our approach of re-writing queries could be compared to query reformulation, wherein a user follows up a query with successive reformulations of the original. In the web environment, studies have shown that most users still enter only one or two queries, a</context>
</contexts>
<marker>Vinay, Cox, Milic-Frayling, Wood, 2006</marker>
<rawString>Vishwa Vinay, Ingemar J. Cox, Natasa Milic-Frayling, and Ken Wood. 2006. On ranking the effectiveness of searches. In 29th ACM SIGIR Proceedings, pages 398–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>robust track.</title>
<date>2006</date>
<journal>SIGIR Forum,</journal>
<booktitle>The TREC</booktitle>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="12718" citStr="Voorhees, 2006" startWordPosition="2091" endWordPosition="2092">dri search engine, developed as part of the Lemur2 project. While the inference network-based retrieval framework of Indri permits the use of structured queries, the use of language modeling techniques provides better estimates of probabilities for query evaluation. The pseudo-relevance feedback mechanism we used is based on relevance models (Lavrenko and Croft, 2001). To extract named entities from the queries, we used BBN Identifinder (Bikel et al., 1999). The named entities identified were of type Person, Location, Organization, Date, and Time. We used the TREC Robust 2004 and Robust 2005 (Voorhees, 2006) document collections for our experiments. The 2004 Robust collection contains around half a million documents from the Financial Times, the Federal Register, the LA Times, and FBIS. The Robust 2005 collection is the one-million 2http://www.lemurproject.org document AQUAINT collection. All the documents were from English newswire. We chose these collections because they and their associated queries are known to be hard, and hence present a challenging environment. We stemmed the collections using the Krovetz stemmer provided as part of Indri, and used a manually-created stoplist of twenty term</context>
</contexts>
<marker>Voorhees, 2006</marker>
<rawString>Ellen M. Voorhees. 2006. The TREC 2005 robust track. SIGIR Forum, 40(1):41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryen W White</author>
<author>Joemon M Jose</author>
<author>Ian Ruthven</author>
</authors>
<title>Using top-ranking sentences to facilitate effective information access: Book reviews.</title>
<date>2005</date>
<journal>JAIST,</journal>
<volume>56</volume>
<issue>10</issue>
<contexts>
<context position="24843" citStr="White et al., 2005" startWordPosition="4107" endWordPosition="4110">ry expansion techniques such that different concepts in the query were equally benefited. Mutual information has been used previously in (Church and Hanks, 1989) to identify collocations of terms for identifying semantic relationships in text. Experiments were confined to bigrams. The use of MaST over a graph of mutual information values to incorporate the most significant dependencies between terms was first noted in (Rijsbergen, 1979). Extensions can be found in a different field - image processing (Kern et al., 2003) - where multivariate mutual information is frequently used. Work done by (White et al., 2005) provided a basis for our decision to show context for sub-query selection. The useful result that top-ranked sentences could be used to guide users towards relevant material helped us design an user interface that the participants found very convenient to use. A related problem addressed by (CronenTownsend et al., 2002) was determining query quality. This is known to be a very hard problem, and various efforts (Carmel et al., 2006; Vinay et al., 2006) have been made towards formalizing and understanding it. Previous work (Shapiro and Taksa, 2003) in the web environment attempted to convert a </context>
</contexts>
<marker>White, Jose, Ruthven, 2005</marker>
<rawString>Ryen W. White, Joemon M. Jose, and Ian Ruthven. 2005. Using top-ranking sentences to facilitate effective information access: Book reviews. JAIST, 56(10):1113–1125.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>