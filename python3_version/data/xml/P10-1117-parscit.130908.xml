<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000036">
<title confidence="0.9947795">
PCFGs, Topic Models, Adaptor Grammars and Learning Topical
Collocations and the Structure of Proper Names
</title>
<author confidence="0.998447">
Mark Johnson
</author>
<affiliation confidence="0.994235">
Department of Computing
Macquarie University
</affiliation>
<email confidence="0.997399">
mjohnson@science.mq.edu.au
</email>
<sectionHeader confidence="0.994766" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999992">
This paper establishes a connection be-
tween two apparently very different kinds
of probabilistic models. Latent Dirich-
let Allocation (LDA) models are used
as “topic models” to produce a low-
dimensional representation of documents,
while Probabilistic Context-Free Gram-
mars (PCFGs) define distributions over
trees. The paper begins by showing that
LDA topic models can be viewed as a
special kind of PCFG, so Bayesian in-
ference for PCFGs can be used to infer
Topic Models as well. Adaptor Grammars
(AGs) are a hierarchical, non-parameteric
Bayesian extension of PCFGs. Exploit-
ing the close relationship between LDA
and PCFGs just described, we propose
two novel probabilistic models that com-
bine insights from LDA and AG models.
The first replaces the unigram component
of LDA topic models with multi-word se-
quences or collocations generated by an
AG. The second extension builds on the
first one to learn aspects of the internal
structure of proper names.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975326086957">
Over the last few years there has been consider-
able interest in Bayesian inference for complex hi-
erarchical models both in machine learning and in
computational linguistics. This paper establishes
a theoretical connection between two very differ-
ent kinds of probabilistic models: Probabilistic
Context-Free Grammars (PCFGs) and a class of
models known as Latent Dirichlet Allocation (Blei
et al., 2003; Griffiths and Steyvers, 2004) models
that have been used for a variety of tasks in ma-
chine learning. Specifically, we show that an LDA
model can be expressed as a certain kind of PCFG,
so Bayesian inference for PCFGs can be used to
learn LDA topic models as well. The importance
of this observation is primarily theoretical, as cur-
rent Bayesian inference algorithms for PCFGs are
less efficient than those for LDA inference. How-
ever, once this link is established it suggests a vari-
ety of extensions to the LDA topic models, two of
which we explore in this paper. The first involves
extending the LDA topic model so that it generates
collocations (sequences of words) rather than indi-
vidual words. The second applies this idea to the
problem of automatically learning internal struc-
ture of proper names (NPs), which is useful for
definite NP coreference models and other applica-
tions.
The rest of this paper is structured as follows.
The next section reviews Latent Dirichlet Alloca-
tion (LDA) topic models, and the following sec-
tion reviews Probabilistic Context-Free Grammars
(PCFGs). Section 4 shows how an LDA topic
model can be expressed as a PCFG, which pro-
vides the fundamental connection between LDA
and PCFGs that we exploit in the rest of the
paper, and shows how it can be used to define
a “sticky topic” version of LDA. The follow-
ing section reviews Adaptor Grammars (AGs), a
non-parametric extension of PCFGs introduced by
Johnson et al. (2007b). Section 6 exploits the con-
nection between LDA and PCFGs to propose an
AG-based topic model that extends LDA by defin-
ing distributions over collocations rather than indi-
vidual words, and section 7 applies this extension
to the problem of finding the structure of proper
names.
</bodyText>
<sectionHeader confidence="0.985032" genericHeader="introduction">
2 Latent Dirichlet Allocation Models
</sectionHeader>
<bodyText confidence="0.9980726">
Latent Dirichlet Allocation (LDA) was introduced
as an explicit probabilistic counterpart to La-
tent Semantic Indexing (LSI) (Blei et al., 2003).
Like LSI, LDA is intended to produce a low-
dimensional characterisation or summary of a doc-
</bodyText>
<page confidence="0.941988">
1148
</page>
<note confidence="0.9874655">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1148–1157,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.78758075">
Figure 1: A graphical model “plate” representa-
tion of an LDA topic model. Here ` is the number
of topics, m is the number of documents and n is
the number of words per document.
</figureCaption>
<bodyText confidence="0.998120548387097">
ument in a collection of documents for informa-
tion retrieval purposes. Both LSI and LDA do
this by mapping documents to points in a rela-
tively low-dimensional real-valued vector space;
distance in this space is intended to correspond to
document similarity.
An LDA model is an explicit generative proba-
bilistic model of a collection of documents. We
describe the “smoothed” LDA model here (see
page 1006 of Blei et al. (2003)) as it corresponds
precisely to the Bayesian PCFGs described in sec-
tion 4. It generates a collection of documents by
first generating multinomials φi over the vocab-
ulary V for each topic i ∈ 1, ... , `, where ` is
the number of topics and φi,w is the probability
of generating word w in topic i. Then it gen-
erates each document Dj, j = 1, ... , m in turn
by first generating a multinomial θj over topics,
where θj,i is the probability of topic i appearing
in document j. (θj serves as the low-dimensional
representation of document Dj). Finally it gener-
ates each of the n words of document Dj by first
selecting a topic z for the word according to θj,
and then drawing a word from φz. Dirichlet priors
with parameters β and α respectively are placed
on the φi and the θj in order to avoid the zeros
that can arise from maximum likelihood estima-
tion (i.e., sparse data problems).
The LDA generative model can be compactly
expressed as follows, where “∼” should be read
as “is distributed according to”.
</bodyText>
<equation confidence="0.99598675">
φi ∼ Dir(β) i = 1, . . . , `
θj ∼ Dir(α) j = 1, ... , m
zj,k ∼ θj j = 1,...,m;k = 1,...,n
wj,k ∼ φzj,k j = 1,...,m;k = 1,...,n
</equation>
<bodyText confidence="0.999920666666667">
In inference, the parameters α and β of the
Dirichlet priors are either fixed (i.e., chosen by
the model designer), or else themselves inferred,
e.g., by Bayesian inference. (The adaptor gram-
mar software we used in the experiments de-
scribed below automatically does this kind of
hyper-parameter inference).
The inference task is to find the topic probabil-
ity vector θj of each document Dj given the words
wj,k of the documents; in general this also requires
inferring the topic to word distributions φ and the
topic assigned to each word zj,k. Blei et al. (2003)
describe a Variational Bayes inference algorithm
for LDA models based on a mean-field approx-
imation, while Griffiths and Steyvers (2004) de-
scribe an Markov Chain Monte Carlo inference al-
gorithm based on Gibbs sampling; both are quite
effective in practice.
</bodyText>
<sectionHeader confidence="0.986804" genericHeader="method">
3 Probabilistic Context-Free Grammars
</sectionHeader>
<bodyText confidence="0.9872346">
Context-Free Grammars are a simple model of hi-
erarchical structure often used to describe natu-
ral language syntax. A Context-Free Grammar
(CFG) is a quadruple (N, W, R, S) where N and
W are disjoint finite sets of nonterminal and ter-
minal symbols respectively, R is a finite set of pro-
ductions or rules of the form A → β where A ∈ N
and β ∈ (N∪W)?, and S ∈ N is the start symbol.
In what follows, it will be useful to interpret a
CFG as generating sets of finite, labelled, ordered
trees TA for each X ∈ N ∪ W. Informally, TX
consists of all trees t rooted in X where for each
local tree (B, β) in t (i.e., where B is a parent’s
label and β is the sequence of labels of its imme-
diate children) there is a rule B → β ∈ R.
Formally, the sets TX are the smallest sets of
trees that satisfy the following equations.
If X ∈ W (i.e., if X is a terminal) then TX =
{X}, i.e., TX consists of a single tree, which in
turn only consists of a single node labelled X.
</bodyText>
<equation confidence="0.979462666666667">
If X ∈ N (i.e., if X is a nonterminal) then
�TX = TREEX(TB1, ... , TBn)
X→B1...BnERX
</equation>
<bodyText confidence="0.9983845">
where RA = {A → β : A → β ∈ R} for each
A ∈ N, and
</bodyText>
<equation confidence="0.99569125">
TREEX(TB1, ... , TBn)
� X :ti ∈ TBi,
= � �i = 1,...,n
t1 ... tom,
</equation>
<bodyText confidence="0.913032666666667">
That is, TREEX(TB1, ... , TBn) consists of the set
of trees with whose root node is labelled X and
whose ith child is a member of TBi.
</bodyText>
<figure confidence="0.662419714285714">
α
θ
Z
W n
m
φ `
β
</figure>
<page confidence="0.989323">
1149
</page>
<bodyText confidence="0.942588636363636">
The set of trees generated by the CFG is TS,
where 5 is the start symbol, and the set of strings
generated by the CFG is the set of yields (i.e., ter-
minal strings) of the trees in TS.
A Probabilistic Context-Free Grammar (PCFG)
is a pair consisting of a CFG and set of multino-
mial probability vectors 9X indexed by nontermi-
nals X E N, where 9X is a distribution over the
rules RX (i.e., the rules expanding X). Informally,
OX→,a is the probability of X expanding to Q using
the rule X —* Q E RX. More formally, a PCFG
</bodyText>
<equation confidence="0.8519765">
associates each X E N U W with a distribution
GX over the trees TX as follows.
If X E W (i.e., if X is a terminal) then GX
is the distribution that puts probability 1 on the
single-node tree labelled X.
If X E N (i.e., if X is a nonterminal) then:
EGX = OX→B1...BnTDX(GB1, ... , GBn) (1)
X→B1...Bn∈RX
where:
� n
TDA (G1, ... , Gn)�X� = H Gi(ti).
tl ... to i=1
</equation>
<bodyText confidence="0.986354407407407">
That is, TDA(G1, ... , Gn) is a distribution over
TA where each subtree ti is generated indepen-
dently from Gi. These equations have solutions
(i.e., the PCFG is said to be “consistent”) when
the rule probabilities 9A obey certain conditions;
see e.g., Wetherell (1980) for details.
The PCFG generates the distribution over trees
GS, where 5 is the start symbol. The distribu-
tion over the strings it generates is obtained by
marginalising over the trees.
In a Bayesian PCFG one puts Dirichlet priors
Dir(αX) on each of the multinomial rule proba-
bility vectors 9X for each nonterminal X E N.
This means that there is one Dirichlet parameter
αX→,a for each rule X —* Q E R in the CFG.
In the “unsupervised” inference problem for a
PCFG one is given a CFG, parameters αX for the
Dirichlet priors over the rule probabilities, and a
corpus of strings. The task is to infer the cor-
responding posterior distribution over rule prob-
abilities 9X. Recently Bayesian inference algo-
rithms for PCFGs have been described. Kurihara
and Sato (2006) describe a Variational Bayes algo-
rithm for inferring PCFGs using a mean-field ap-
proximation, while Johnson et al. (2007a) describe
a Markov Chain Monte Carlo algorithm based on
Gibbs sampling.
</bodyText>
<sectionHeader confidence="0.882233" genericHeader="method">
4 LDA topic models as PCFGs
</sectionHeader>
<bodyText confidence="0.999978">
This section explains how to construct a PCFG
that generates the same distribution over a collec-
tion of documents as an LDA model, and where
Bayesian inference for the PCFG’s rule proba-
bilities yields the corresponding distributions as
Bayesian inference of the corresponding LDA
models. (There are several different ways of en-
coding LDA models as PCFGs; the one presented
here is not the most succinct — it is possible to
collapse the Doc and Doc0 nonterminals — but it
has the advantage that the LDA distributions map
straight-forwardly onto PCFG nonterminals).
The terminals W of the CFG consist of the vo-
cabulary V of the LDA model plus a set of special
“document identifier” terminals “ j” for each doc-
ument j E 1, ... , m, where m is the number of
documents. In the PCFG encoding strings from
document j are prefixed with “j”; this indicates
to the grammar which document the string comes
from. The nonterminals consist of the start symbol
Sentence, Docj and Doc0j for each j E 1, ... , m,
and Topici for each i E 1, ... , E, where E is the
number of topics in the LDA model.
The rules of the CFG are all instances of the
following schemata:
</bodyText>
<equation confidence="0.9908818">
Sentence —* Doc0j j E 1, ... , m
Doc0j —* j j E 1, ... , m
Doc0j —* Doc0jDocj j E 1, ... , m
Docj —* Topici i E 1, ... , E; j E 1, ... , m
Topici —* w i E 1, ... , E; w E V
</equation>
<bodyText confidence="0.999919777777778">
Figure 2 depicts a tree generated by such a
CFG. The relationship between the LDA model
and the PCFG can be understood by studying the
trees generated by the CFG. In these trees the left-
branching spine of nodes labelled Doc0j propagate
the document identifier throughout the whole tree.
The nodes labelled Topici indicate the topics as-
signed to particular words, and the local trees ex-
panding Docj to Topici (one per word in the docu-
ment) indicate the distribution of topics in the doc-
ument.
The corresponding Bayesian PCFG associates
probabilities with each of the rules in the CFG.
The probabilities 9T�picz associated with the rules
expanding the Topici nonterminals indicate how
words are distributed across topics; the 9T�picz
probabilities correspond exactly to to the iii prob-
abilities in the LDA model. The probabilities
</bodyText>
<page confidence="0.933813">
1150
</page>
<figure confidence="0.955734">
Sentence
Doc3&apos;
</figure>
<figureCaption confidence="0.997748">
Figure 2: A tree generated by the CFG encoding
</figureCaption>
<bodyText confidence="0.997261075949367">
an LDA topic model. The prefix “ 3” indicates
that this string belongs to document 3. The tree
also indicates the assignment of words to topics.
ODocj associated with rules expanding Docj spec-
ify the distribution of topics in document j; they
correspond exactly to the probabilities Oj of the
LDA model. (The PCFG also specifies several
other distributions that are suppressed in the LDA
model. For example OSentence specifies the distri-
bution of documents in the corpus. However, it is
easy to see that these distributions do not influence
the topic distributions; indeed, the expansions of
the Sentence nonterminal are completely deter-
mined by the document distribution in the corpus,
and are not affected by OSentence).
A Bayesian PCFG places Dirichlet priors
Dir(αA) on the corresponding rule probabilities
OA for each A ∈ N. In the PCFG encoding an
LDA model, the αTopici parameters correspond
exactly to the ,C3 parameters of the LDA model, and
the αDocj parameters correspond to the α param-
eters of the LDA model.
As suggested above, each document Dj in the
LDA model is mapped to a string in the corpus
used to train the corresponding PCFG by prefix-
ing it with a document identifier “j”. Given this
training data, the posterior distribution over rule
probabilities θDocj, Topici is the same as the pos-
terior distribution over topics given documents θj,i
in the original LDA model.
As we will see below, this connection between
PCFGs and LDA topic models suggests a num-
ber of interesting variants of both PCFGs and
topic models. Note that we are not suggesting
that Bayesian inference for PCFGs is necessar-
ily a good way of estimating LDA topic models.
Current Bayesian PCFG inference algorithms re-
quire time proportional to the cube of the length of
the longest string in the training corpus, and since
these strings correspond to entire documents in our
embedding, blindly applying a Bayesian PCFG in-
ference algorithm is likely to be impractical.
A little reflection shows that the embedding still
holds if the strings in the PCFG corpus correspond
to sentences or even smaller units of the original
document collection, so a single document would
be mapped to multiple strings in the PCFG infer-
ence task. In this way the cubic time complex-
ity of PCFG inference can be mitigated. Also, the
trees generated by these CFGs have a very spe-
cialized left-branching structure, and it is straight-
forward to modify the general-purpose CFG infer-
ence procedures to avoid the cubic time complex-
ity for such grammars: thus it may be practical to
estimate topic models via grammatical inference.
However, we believe that the primary value of
the embedding of LDA topic models into Bayesian
PCFGs is theoretical: it suggests a number of
novel extensions of both topic models and gram-
mars that may be worth exploring. Our claim here
is not that these models are the best algorithms for
performing these tasks, but that the relationship
we described between LDA models and PCFGs
suggests a variety of interesting novel models.
We end this section with a simple example of
such a modification to LDA. Inspired by the stan-
dard embedding of HMMs into PCFGs, we pro-
pose a “sticky topic” variant of LDA in which ad-
jacent words are more likely to be assigned the
same topic. Such an LDA extension is easy to
describe as a PCFG (see Fox et al. (2008) for a
similar model presented as an extended HMM).
The nonterminals Sentence and Topici for i =
1, ... , ` have the same interpretation as before, but
we introduce new nonterminals Docj,i that indi-
cate we have just generated a nonterminal in doc-
ument j belonging to topic i. Given a collection of
m documents and ` topics, the rule schemata are
as follows:
</bodyText>
<equation confidence="0.880801142857143">
Sentence → Docj,i i ∈ 1, . . . , `;
j∈ 1,...,m
Docj,1 → j j ∈ 1, ... , m
Doc j Doc 7� Topici i i&apos; E 1 E
,z ,z i ,
j∈ 1,...,m
Topici → w i ∈ 1, ... , `; w ∈ V
</equation>
<bodyText confidence="0.686403">
A sample parse generated by a “sticky topic”
</bodyText>
<figure confidence="0.98862852631579">
_3
Topic4
shallow
Doc3&apos;
Doc3&apos;
Doc3&apos;
Doc3
Topic4
circuits
Doc3
Topic7
faster
Doc3
Topic4
compute
Doc3&apos;
Doc3
1151
Sentence
</figure>
<figureCaption confidence="0.8753505">
Figure 3: A tree generated by the “sticky topic”
CFG. Here a nonterminal Doc3, 7 indicates we
</figureCaption>
<bodyText confidence="0.9301939375">
have just generated a word in document 3 belong-
ing to topic 7.
CFG is shown in Figure 3. The probabilities of
the rules Docj,i —* Docj,i, Topici in this PCFG
encode the probability of shifting from topic i to
topic i&apos; (this PCFG can be viewed as generating
the string from right to left).
We can use non-uniform sparse Dirichlet pri-
ors on the probabilities of these rules to encour-
age “topic stickiness”. Specifically, by setting
the Dirichlet parameters for the “topic shift” rules
Docj,i, —* Docj,i Topici where i&apos; =� i much lower
than the parameters for the “topic preservation”
rules Docj,i —* Docj,i Topici, Bayesian inference
will be biased to find distributions in which adja-
cent words will tend to have the same topic.
</bodyText>
<sectionHeader confidence="0.991944" genericHeader="method">
5 Adaptor Grammars
</sectionHeader>
<bodyText confidence="0.999964857142858">
Non-parametric Bayesian inference, where the in-
ference task involves learning not just the values
of a finite vector of parameters but which parame-
ters are relevant, has been the focus of intense re-
search in machine learning recently. In the topic-
modelling community this has lead to work on
Dirichlet Processes and Chinese Restaurant Pro-
cesses, which can be used to estimate the number
of topics as well as their distribution across docu-
ments (Teh et al., 2006).
There are two obvious non-parametric exten-
sions to PCFGs. In the first we regard the set
of nonterminals N as potentially unbounded, and
try to learn the set of nonterminals required to de-
scribe the training corpus. This approach goes un-
der the name of the “infinite HMM” or “infinite
PCFG” (Beal et al., 2002; Liang et al., 2007; Liang
et al., 2009). Informally, we are given a set of “ba-
sic categories”, say NP, VP, etc., and a set of rules
that use these basic categories, say S —* NP VP.
The inference task is to learn a set of refined cate-
gories and rules (e.g., S7 —* NP2 VP5) as well as
their probabilities; this approach can therefore be
viewed as a Bayesian version of the “split-merge”
approach to grammar induction (Petrov and Klein,
2007).
In the second approach, which we adopt here,
we regard the set of rules R as potentially un-
bounded, and try to learn the rules required to
describe a training corpus as well as their prob-
abilities. Adaptor grammars are an example of
this approach (Johnson et al., 2007b), where en-
tire subtrees generated by a “base grammar” can
be viewed as distinct rules (in that we learn a sep-
arate probability for each subtree). The inference
task is non-parametric if there are an unbounded
number of such subtrees.
We review the adaptor grammar generative pro-
cess below; for an informal introduction see John-
son (2008) and for details of the adaptor grammar
inference procedure see Johnson and Goldwater
(2009).
An adaptor grammar (N, W, R, 5, 0, A, C) con-
sists of a PCFG (N, W, R, 5, 0) in which a sub-
set A C_ N of the nonterminals are adapted, and
where each adapted nonterminal X E A has an
associated adaptor CX. An adaptor CX for X is a
function that maps a distribution over trees TX to
a distribution over distributions over TX (we give
examples of adaptors below).
Just as for a PCFG, an adaptor grammar de-
fines distributions GX over trees TX for each X E
N U W. If X E W or X E� A then GX is defined
just as for a PCFG above, i.e., using (1). How-
ever, if X E A then GX is defined in terms of an
additional distribution HX as follows:
</bodyText>
<equation confidence="0.998877">
GX — CX(HX)
�HX = 0X→Y1...YmTDX(GY1, ... , GYm)
X→Y1...YmERX
</equation>
<bodyText confidence="0.9985624">
That is, the distribution GX associated with an
adapted nonterminal X E A is a sample from
adapting (i.e., applying CX to) its “ordinary”
PCFG distribution HX. In general adaptors are
chosen for the specific properties they have. For
example, with the adaptors used here GX typically
concentrates mass on a smaller subset of the trees
TX than HX does.
Just as with the PCFG, an adaptor grammar gen-
erates the distribution over trees GS, where 5 E N
</bodyText>
<figure confidence="0.997129">
Doc3,7
Doc3,1
_3
Doc3,4
Topic4
shallow
Topic7
faster
Doc3,4
Topic4
circuits
Topic4
compute
Doc3,4
</figure>
<page confidence="0.984675">
1152
</page>
<listItem confidence="0.693521888888889">
is the start symbol. However, while GS in a PCFG
is a fixed distribution (given the rule probabili-
ties 0), in an adaptor grammar the distribution GS
is itself a random variable (because each GX for
X E A is random), i.e., an adaptor grammar gen-
erates a distribution over distributions over trees
TS. However, the posterior joint distribution Pr(t)
of a sequence t = (t1, ... , tn) of trees in TS is
well-defined:
</listItem>
<equation confidence="0.999255">
�Pr(t) = GS(t1) ... GS(tn) dG
</equation>
<bodyText confidence="0.999755833333333">
where the integral is over all of the random distri-
butions GX, X E A. The adaptors we use in this
paper are Dirichlet Processes or two-parameter
Poisson-Dirichlet Processes, for which it is pos-
sible to compute this integral. One way to do this
uses the predictive distributions:
</bodyText>
<equation confidence="0.992811">
Pr(tn+1  |t, HX)
�a GX(t1) ... GX(tn+1)CX(GX  |HX) dGX
</equation>
<bodyText confidence="0.9879406875">
where t = (t1, ... , tn) and each tz E TX. The pre-
dictive distribution for the Dirichlet Process is the
(labeled) Chinese Restaurant Process (CRP), and
the predictive distribution for the two-parameter
Poisson-Dirichlet process is the (labeled) Pitman-
Yor Process (PYP).
In the context of adaptor grammars, the CRP is:
CRP(t  |t, αX, HX) a nt(t) + αXHX(t)
where nt(t) is the number of times t appears in t
and αX &gt; 0 is a user-settable “concentration pa-
rameter”. In order to generate the next tree tn+1
a CRP either reuses a tree t with probability pro-
portional to number of times t has been previously
generated, or else it “backs off” to the “base distri-
bution” HX and generates a fresh tree t with prob-
ability proportional to αXHX(t).
</bodyText>
<equation confidence="0.976659333333333">
The PYP is a generalization of the CRP:
PYP(t  |t, aX, bX, HX)
a max(0, nt(t) − mt aX) + (maX + bX)HX(t)
</equation>
<bodyText confidence="0.980971914285715">
Here aX E [0, 1] and bX &gt; 0 are user-settable
parameters, and mt is the number of times the PYP
has generated t in t from the base distribution HX,
and m = EtETx mt is the number of times any
tree has been generated from HX. (In the Chinese
Restaurant metaphor, mt is the number of tables
labeled with t, and m is the number of occupied
tables). If aX = 0 then the PYP is equivalent to
a CRP with αX = bX, while if aX = 1 then the
PYP generates samples from HX.
Informally, the CRP has a strong preference
to regenerate trees that have been generated fre-
quently before, leading to a “rich-get-richer” dy-
namics. The PYP can mitigate this somewhat by
reducing the effective count of previously gener-
ated trees and redistributing that probability mass
to new trees generated from HX. As Goldwa-
ter et al. (2006) explain, Bayesian inference for
HX given samples from GX is effectively per-
formed from types if aX = 0 and from tokens
if aX = 1, so varying aX smoothly interpolates
between type-based and token-based inference.
Adaptor grammars have previously been used
primarily to study grammatical inference in the
context of language acquisition. The word seg-
mentation task involves segmenting a corpus
of unsegmented phonemic utterance representa-
tions into words (Elman, 1990; Bernstein-Ratner,
1987). For example, the phoneme string corre-
sponding to “you want to see the book” (with its
correct segmentation indicated) is as follows:
yMuNwMa MnMtNtMuNsMiNDM6NbMUMk
We can represent any possible segmentation of any
possible sentence as a tree generated by the fol-
lowing unigram adaptor grammar.
</bodyText>
<table confidence="0.7503384">
Sentence —* Word
Sentence —* Word Sentence
Word —* Phonemes
Phonemes —* Phoneme
Phonemes —* Phoneme Phonemes
</table>
<bodyText confidence="0.978222357142857">
The trees generated by this adaptor grammar are
the same as the trees generated by the CFG rules.
For example, the following skeletal parse in which
all but the Word nonterminals are suppressed (the
others are deterministically inferrable) shows the
parse that corresponds to the correct segmentation
of the string above.
(Word y u) (Word w a n t) (Word t u)
(Word s i) (Word d 6) (Word b u k)
Because the Word nonterminal is adapted (indi-
cated here by underlining) the adaptor grammar
learns the probability of the entire Word subtrees
(e.g., the probability that b u k is a Word); see
Johnson (2008) for further details.
</bodyText>
<page confidence="0.989763">
1153
</page>
<sectionHeader confidence="0.958349" genericHeader="method">
6 Topic models with collocations
</sectionHeader>
<bodyText confidence="0.999533384615385">
Here we combine ideas from the unigram word
segmentation adaptor grammar above and the
PCFG encoding of LDA topic models to present
a novel topic model that learns topical colloca-
tions. (For a non-grammar-based approach to this
problem see Wang et al. (2007)). Specifically, we
take the PCFG encoding of the LDA topic model
described above, but modify it so that the Topici
nodes generate sequences of words rather than sin-
gle words. Then we adapt each of the Topici non-
terminals, which means that we learn the probabil-
ity of each of the sequences of words it can expand
to.
</bodyText>
<equation confidence="0.979138">
Sentence → Docj j ∈ 1,... , m
Docj → j j ∈ 1, ... , m
Docj → Docj Topici i ∈ 1, . . . , `�
j∈ 1,...,m
Topici → Words i ∈ 1, . . . , `
Words → Word
Words → Words Word
Word → w w ∈ V
</equation>
<bodyText confidence="0.999987047619048">
In order to demonstrate that this model
works, we implemented this using the publically-
available adaptor grammar inference software,1
and ran it on the NIPS corpus (composed of pub-
lished NIPS abstracts), which has previously been
used for studying collocation-based topic models
(Griffiths et al., 2007). Because there is no gen-
erally accepted evaluation for collocation-finding,
we merely present some of the sample analyses
found by our adaptor grammar. We ran our adap-
tor grammar with ` = 20 topics (i.e., 20 distinct
Topici nonterminals). Adaptor grammar inference
on this corpus is actually relatively efficient be-
cause the corpus provided by Griffiths et al. (2007)
is already segmented by punctuation, so the termi-
nal strings are generally rather short. Rather than
set the Dirichlet parameters by hand, we placed
vague priors on them and estimated them as de-
scribed in Johnson and Goldwater (2009).
The following are some examples of colloca-
tions found by our adaptor grammar:
</bodyText>
<footnote confidence="0.9509336">
Topic0 → cost function
Topic0 → fixed point
Topic0 → gradient descent
Topic0 → learning rates
1http://web.science.mq.edu.au/ ˜mjohnson/Software.htm
</footnote>
<bodyText confidence="0.8421751">
Topic, → associative memory
Topic, → hamming distance
Topic, → randomly chosen
Topic, → standard deviation
Topica → action potentials
Topica → membrane potential
Topica → primary visual cortex
Topica → visual system
Topic,0 → nervous system
Topic,0 → action potential
Topic,0 → ocular dominance
Topic,0 → visual field
The following are skeletal sample parses, where
we have elided all but the adapted nonterminals
(i.e., all we show are the Topic nonterminals, since
the other structure can be inferred deterministi-
cally). Note that because Griffiths et al. (2007)
segmented the NIPS abstracts at punctuation sym-
bols, the training corpus contains more than one
string from each abstract.
</bodyText>
<table confidence="0.7880485">
3 (Topic5 polynomial size)
(Topic,5 threshold circuits)
4 (Topic,, studied)
(Topic,y pattern recognition algorithms)
4 (Topic2 feedforward neural network)
(Topic, implementation)
5 (Topic,, single)
(Topic,0 ocular dominance stripe)
(Topic,2 low) (Topica ocularity)
(Topic,2 drift rate)
</table>
<sectionHeader confidence="0.905978" genericHeader="method">
7 Finding the structure of proper names
</sectionHeader>
<bodyText confidence="0.99971475">
Grammars offer structural and positional sensitiv-
ity that is not exploited in the basic LDA topic
models. Here we explore the potential for us-
ing Bayesian inference for learning linear order-
ing constraints that hold between elements within
proper names.
The Penn WSJ treebank is a widely used re-
source within computational linguistics (Marcus
et al., 1993), but one of its weaknesses is that
it does not indicate any structure internal to base
noun phrases (i.e., it presents “flat” analyses of the
pre-head NP elements). For many applications it
would be extremely useful to have a more elab-
orated analysis of this kind of NP structure. For
example, in an NP coreference application, if we
could determine that Bill and Hillary are both first
</bodyText>
<page confidence="0.989935">
1154
</page>
<bodyText confidence="0.999981518518518">
names then we could infer that Bill Clinton and
Hillary Clinton are likely to refer to distinct in-
dividuals. On the other hand, because Mr in Mr
Clinton is not a first name, it is possible that Mr
Clinton and Bill Clinton refer to the same individ-
ual (Elsner et al., 2009).
Here we present an adaptor grammar based on
the insights of the PCFG encoding of LDA topic
models that learns some of the structure of proper
names. The key idea is that elements in proper
names typically appear in a fixed order; we expect
honorifics to appear before first names, which ap-
pear before middle names, which in turn appear
before surnames, etc. Similarly, many company
names end in fixed phrases such as Inc. Here
we think of first names as a kind of topic, albeit
one with a restricted positional location. One of
the challenges is that some of these structural ele-
ments can be filled by multiword expressions; e.g.,
de Groot can be a surname. We deal with this by
permitting multi-word collocations to fill the cor-
responding positions, and use the adaptor gram-
mar machinery to learn these collocations.
Inspired by the grammar presented in Elsner
et al. (2009), our adaptor grammar is as follows,
where adapted nonterminals are indicated by un-
derlining as before.
</bodyText>
<equation confidence="0.9978868">
NP → (A0) (A1) ... (A6)
NP → (B0) (B1) ... (B6)
NP → Unordered+
A0 → Word+
. . .
A6 → Word+
B0 → Word+
. . .
B6 → Word+
Unordered → Word+
</equation>
<bodyText confidence="0.999989590909091">
In this grammar parentheses indicate optional-
ity, and the Kleene plus indicates iteration (these
were manually expanded into ordinary CFG rules
in our experiments). The grammar provides three
different expansions for proper names. The first
expansion says that a proper name can consist of
some subset of the six different collocation classes
A0 through A6 in that order, while the second ex-
pansion says that a proper name can consist of
some subset of the collocation classes B0 through
B6, again in that order. Finally, the third expan-
sion says that a proper name can consist of an ar-
bitrary sequence of “unordered” collocations (this
is intended as a “catch-all” expansion to provide
analyses for proper names that don’t fit either of
the first two expansions).
We extracted all of the proper names (i.e.,
phrases of category NNP and NNPS) in the Penn
WSJ treebank and used them as the training cor-
pora for the adaptor grammar just described. The
adaptor grammar inference procedure found skele-
tal sample parses such as the following:
</bodyText>
<equation confidence="0.980382583333333">
(A0 barrett) (A3 smith)
(A0 albert) (A2 j.) (A3 smith) (A4 jr.)
(A0 robert) (A2 b.) (A3 van dover)
(B0 aim) (B1 prime rate) (B2 plus) (BS
fund) (B6 inc.)
(B0 balfour) (B1 maclaine) (BS interna-
tional) (B6 ltd.)
(B0 american express) (B1 information
services) (B6 co)
(U abc) (U sports)
(U sports illustrated)
(U sports unlimited)
</equation>
<bodyText confidence="0.999911666666667">
While a full evaluation will have to await further
study, in general it seems to distinguish person
names from company names reasonably reliably,
and it seems to have discovered that person names
consist of a first name (A0), a middle name or ini-
tial (A2), a surname (A3) and an optional suffix
(A4). Similarly, it seems to have uncovered that
company names typically end in a phrase such as
inc, ltd or co.
</bodyText>
<sectionHeader confidence="0.997654" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999991">
This paper establishes a connection between two
very different kinds of probabilistic models; LDA
models of the kind used for topic modelling, and
PCFGs, which are a standard model of hierarchi-
cal structure in language. The embedding we pre-
sented shows how to express an LDA model as a
PCFG, and has the property that Bayesian infer-
ence of the parameters of that PCFG produces an
equivalent model to that produced by Bayesian in-
ference of the LDA model’s parameters.
The primary value of this embedding is theoret-
ical rather than practical; we are not advocating
the use of PCFG estimation procedures to infer
LDA models. Instead, we claim that the embed-
ding suggests novel extensions to both the LDA
topic models and PCFG-style grammars. We jus-
tified this claim by presenting several hybrid mod-
els that combine aspects of both topic models and
</bodyText>
<page confidence="0.966656">
1155
</page>
<bodyText confidence="0.999983384615384">
grammars. We don’t claim that these are neces-
sarily the best models for performing any particu-
lar tasks; rather, we present them as examples of
models inspired by a combination of PCFGs and
LDA topic models. We showed how the LDA
to PCFG embedding suggested a “sticky topic”
model extension to LDA. We then discussed adap-
tor grammars, and inspired by the LDA topic mod-
els, presented a novel topic model whose prim-
itive elements are multi-word collocations rather
than words. We concluded with an adaptor gram-
mar that learns aspects of the internal structure of
proper names.
</bodyText>
<sectionHeader confidence="0.998464" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999962">
This research was funded by US NSF awards
0544127 and 0631667, as well as by a start-up
award from Macquarie University. I’d like to
thank the organisers and audience at the Topic
Modeling workshop at NIPS 2009, my former col-
leagues at Brown University (especially Eugene
Charniak, Micha Elsner, Sharon Goldwater, Tom
Griffiths and Erik Sudderth), my new colleagues
at Macquarie University and the ACL reviewers
for their excellent suggestions and comments on
this work. Naturally all errors remain my own.
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999642518518519">
M.J. Beal, Z. Ghahramani, and C.E. Rasmussen. 2002.
The infinite Hidden Markov Model. In T. Dietterich,
S. Becker, and Z. Ghahramani, editors, Advances in
Neural Information Processing Systems, volume 14,
pages 577–584. The MIT Press.
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck,
editors, Children’s Language, volume 6. Erlbaum,
Hillsdale, NJ.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Jeffrey Elman. 1990. Finding structure in time. Cog-
nitive Science, 14:197–211.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsuper-
vised named-entity clustering. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
164–172, Boulder, Colorado, June. Association for
Computational Linguistics.
E. Fox, E. Sudderth, M. Jordan, and A. Willsky. 2008.
An HDP-HMM for systems with state persistence.
In Andrew McCallum and Sam Roweis, editors,
Proceedings of the 25th Annual International Con-
ference on Machine Learning (ICML 2008), pages
312–319. Omnipress.
Sharon Goldwater, Tom Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Sch¨olkopf, and J. Platt, editors, Advances in Neu-
ral Information Processing Systems 18, pages 459–
466, Cambridge, MA. MIT Press.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:52285235.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114(2):211244.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317–325,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007a. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 139–146, Rochester, New York, April.
Association for Computational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007b. Adaptor Grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B. Sch¨olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641–648. MIT Press, Cambridge,
MA.
Mark Johnson. 2008. Using adaptor grammars to iden-
tifying synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th An-
nual Meeting of the Association of Computational
Linguistics, Columbus, Ohio. Association for Com-
putational Linguistics.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
Bayesian grammar induction for natural language.
In 8th International Colloquium on Grammatical In-
ference.
Percy Liang, Slav Petrov, Michael Jordan, and Dan
Klein. 2007. The infinite PCFG using hierarchi-
cal Dirichlet processes. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 688–
697.
</reference>
<page confidence="0.866336">
1156
</page>
<reference confidence="0.999204807692307">
Percy Liang, Michael Jordan, and Dan Klein. 2009.
Probabilistic grammars and hierarchical Dirichlet
processes. In The Oxford Handbook of Applied
Bayesian Analysis. Oxford University Press.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404–411, Rochester, New York. Associ-
ation for Computational Linguistics.
Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hi-
erarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101:1566–1581.
Xuerui Wang, Andrew McCallum, and Xing Wei.
2007. Topical n-grams: Phrase and topic discovery,
with an application to information retrieval. In Pro-
ceedings of the 7th IEEE International Conference
on Data Mining (ICDM), pages 697–702.
C.S. Wetherell. 1980. Probabilistic languages: A re-
view and some open questions. Computing Surveys,
12:361–379.
</reference>
<page confidence="0.995166">
1157
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.977772">
<title confidence="0.9997205">PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</title>
<author confidence="0.999998">Mark Johnson</author>
<affiliation confidence="0.999905">Department of Computing Macquarie University</affiliation>
<email confidence="0.987629">mjohnson@science.mq.edu.au</email>
<abstract confidence="0.999589384615385">This paper establishes a connection between two apparently very different kinds of probabilistic models. Latent Dirichlet Allocation (LDA) models are used as “topic models” to produce a lowdimensional representation of documents, while Probabilistic Context-Free Grammars (PCFGs) define distributions over trees. The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. Adaptor Grammars (AGs) are a hierarchical, non-parameteric Bayesian extension of PCFGs. Exploiting the close relationship between LDA and PCFGs just described, we propose two novel probabilistic models that combine insights from LDA and AG models. The first replaces the unigram component of LDA topic models with multi-word sequences or collocations generated by an AG. The second extension builds on the first one to learn aspects of the internal structure of proper names.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M J Beal</author>
<author>Z Ghahramani</author>
<author>C E Rasmussen</author>
</authors>
<title>The infinite Hidden Markov Model. In</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<volume>14</volume>
<pages>577--584</pages>
<editor>T. Dietterich, S. Becker, and Z. Ghahramani, editors,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="17658" citStr="Beal et al., 2002" startWordPosition="3144" endWordPosition="3147">relevant, has been the focus of intense research in machine learning recently. In the topicmodelling community this has lead to work on Dirichlet Processes and Chinese Restaurant Processes, which can be used to estimate the number of topics as well as their distribution across documents (Teh et al., 2006). There are two obvious non-parametric extensions to PCFGs. In the first we regard the set of nonterminals N as potentially unbounded, and try to learn the set of nonterminals required to describe the training corpus. This approach goes under the name of the “infinite HMM” or “infinite PCFG” (Beal et al., 2002; Liang et al., 2007; Liang et al., 2009). Informally, we are given a set of “basic categories”, say NP, VP, etc., and a set of rules that use these basic categories, say S —* NP VP. The inference task is to learn a set of refined categories and rules (e.g., S7 —* NP2 VP5) as well as their probabilities; this approach can therefore be viewed as a Bayesian version of the “split-merge” approach to grammar induction (Petrov and Klein, 2007). In the second approach, which we adopt here, we regard the set of rules R as potentially unbounded, and try to learn the rules required to describe a trainin</context>
</contexts>
<marker>Beal, Ghahramani, Rasmussen, 2002</marker>
<rawString>M.J. Beal, Z. Ghahramani, and C.E. Rasmussen. 2002. The infinite Hidden Markov Model. In T. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems, volume 14, pages 577–584. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bernstein-Ratner</author>
</authors>
<title>The phonology of parentchild speech.</title>
<date>1987</date>
<booktitle>Children’s Language,</booktitle>
<volume>6</volume>
<editor>In K. Nelson and A. van Kleeck, editors,</editor>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="22958" citStr="Bernstein-Ratner, 1987" startWordPosition="4109" endWordPosition="4110">nt of previously generated trees and redistributing that probability mass to new trees generated from HX. As Goldwater et al. (2006) explain, Bayesian inference for HX given samples from GX is effectively performed from types if aX = 0 and from tokens if aX = 1, so varying aX smoothly interpolates between type-based and token-based inference. Adaptor grammars have previously been used primarily to study grammatical inference in the context of language acquisition. The word segmentation task involves segmenting a corpus of unsegmented phonemic utterance representations into words (Elman, 1990; Bernstein-Ratner, 1987). For example, the phoneme string corresponding to “you want to see the book” (with its correct segmentation indicated) is as follows: yMuNwMa MnMtNtMuNsMiNDM6NbMUMk We can represent any possible segmentation of any possible sentence as a tree generated by the following unigram adaptor grammar. Sentence —* Word Sentence —* Word Sentence Word —* Phonemes Phonemes —* Phoneme Phonemes —* Phoneme Phonemes The trees generated by this adaptor grammar are the same as the trees generated by the CFG rules. For example, the following skeletal parse in which all but the Word nonterminals are suppressed (</context>
</contexts>
<marker>Bernstein-Ratner, 1987</marker>
<rawString>N. Bernstein-Ratner. 1987. The phonology of parentchild speech. In K. Nelson and A. van Kleeck, editors, Children’s Language, volume 6. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1571" citStr="Blei et al., 2003" startWordPosition="234" endWordPosition="237">ram component of LDA topic models with multi-word sequences or collocations generated by an AG. The second extension builds on the first one to learn aspects of the internal structure of proper names. 1 Introduction Over the last few years there has been considerable interest in Bayesian inference for complex hierarchical models both in machine learning and in computational linguistics. This paper establishes a theoretical connection between two very different kinds of probabilistic models: Probabilistic Context-Free Grammars (PCFGs) and a class of models known as Latent Dirichlet Allocation (Blei et al., 2003; Griffiths and Steyvers, 2004) models that have been used for a variety of tasks in machine learning. Specifically, we show that an LDA model can be expressed as a certain kind of PCFG, so Bayesian inference for PCFGs can be used to learn LDA topic models as well. The importance of this observation is primarily theoretical, as current Bayesian inference algorithms for PCFGs are less efficient than those for LDA inference. However, once this link is established it suggests a variety of extensions to the LDA topic models, two of which we explore in this paper. The first involves extending the L</context>
<context position="3488" citStr="Blei et al., 2003" startWordPosition="554" endWordPosition="557"> a “sticky topic” version of LDA. The following section reviews Adaptor Grammars (AGs), a non-parametric extension of PCFGs introduced by Johnson et al. (2007b). Section 6 exploits the connection between LDA and PCFGs to propose an AG-based topic model that extends LDA by defining distributions over collocations rather than individual words, and section 7 applies this extension to the problem of finding the structure of proper names. 2 Latent Dirichlet Allocation Models Latent Dirichlet Allocation (LDA) was introduced as an explicit probabilistic counterpart to Latent Semantic Indexing (LSI) (Blei et al., 2003). Like LSI, LDA is intended to produce a lowdimensional characterisation or summary of a doc1148 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1148–1157, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Figure 1: A graphical model “plate” representation of an LDA topic model. Here ` is the number of topics, m is the number of documents and n is the number of words per document. ument in a collection of documents for information retrieval purposes. Both LSI and LDA do this by mapping documents to points in a rela</context>
<context position="6069" citStr="Blei et al. (2003)" startWordPosition="1018" endWordPosition="1021">,...,m;k = 1,...,n wj,k ∼ φzj,k j = 1,...,m;k = 1,...,n In inference, the parameters α and β of the Dirichlet priors are either fixed (i.e., chosen by the model designer), or else themselves inferred, e.g., by Bayesian inference. (The adaptor grammar software we used in the experiments described below automatically does this kind of hyper-parameter inference). The inference task is to find the topic probability vector θj of each document Dj given the words wj,k of the documents; in general this also requires inferring the topic to word distributions φ and the topic assigned to each word zj,k. Blei et al. (2003) describe a Variational Bayes inference algorithm for LDA models based on a mean-field approximation, while Griffiths and Steyvers (2004) describe an Markov Chain Monte Carlo inference algorithm based on Gibbs sampling; both are quite effective in practice. 3 Probabilistic Context-Free Grammars Context-Free Grammars are a simple model of hierarchical structure often used to describe natural language syntax. A Context-Free Grammar (CFG) is a quadruple (N, W, R, S) where N and W are disjoint finite sets of nonterminal and terminal symbols respectively, R is a finite set of productions or rules o</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive Science,</journal>
<pages>14--197</pages>
<contexts>
<context position="22933" citStr="Elman, 1990" startWordPosition="4107" endWordPosition="4108">effective count of previously generated trees and redistributing that probability mass to new trees generated from HX. As Goldwater et al. (2006) explain, Bayesian inference for HX given samples from GX is effectively performed from types if aX = 0 and from tokens if aX = 1, so varying aX smoothly interpolates between type-based and token-based inference. Adaptor grammars have previously been used primarily to study grammatical inference in the context of language acquisition. The word segmentation task involves segmenting a corpus of unsegmented phonemic utterance representations into words (Elman, 1990; Bernstein-Ratner, 1987). For example, the phoneme string corresponding to “you want to see the book” (with its correct segmentation indicated) is as follows: yMuNwMa MnMtNtMuNsMiNDM6NbMUMk We can represent any possible segmentation of any possible sentence as a tree generated by the following unigram adaptor grammar. Sentence —* Word Sentence —* Word Sentence Word —* Phonemes Phonemes —* Phoneme Phonemes —* Phoneme Phonemes The trees generated by this adaptor grammar are the same as the trees generated by the CFG rules. For example, the following skeletal parse in which all but the Word nont</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey Elman. 1990. Finding structure in time. Cognitive Science, 14:197–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Structured generative models for unsupervised named-entity clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>164--172</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="27948" citStr="Elsner et al., 2009" startWordPosition="4942" endWordPosition="4945">ot indicate any structure internal to base noun phrases (i.e., it presents “flat” analyses of the pre-head NP elements). For many applications it would be extremely useful to have a more elaborated analysis of this kind of NP structure. For example, in an NP coreference application, if we could determine that Bill and Hillary are both first 1154 names then we could infer that Bill Clinton and Hillary Clinton are likely to refer to distinct individuals. On the other hand, because Mr in Mr Clinton is not a first name, it is possible that Mr Clinton and Bill Clinton refer to the same individual (Elsner et al., 2009). Here we present an adaptor grammar based on the insights of the PCFG encoding of LDA topic models that learns some of the structure of proper names. The key idea is that elements in proper names typically appear in a fixed order; we expect honorifics to appear before first names, which appear before middle names, which in turn appear before surnames, etc. Similarly, many company names end in fixed phrases such as Inc. Here we think of first names as a kind of topic, albeit one with a restricted positional location. One of the challenges is that some of these structural elements can be filled</context>
</contexts>
<marker>Elsner, Charniak, Johnson, 2009</marker>
<rawString>Micha Elsner, Eugene Charniak, and Mark Johnson. 2009. Structured generative models for unsupervised named-entity clustering. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 164–172, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Fox</author>
<author>E Sudderth</author>
<author>M Jordan</author>
<author>A Willsky</author>
</authors>
<title>An HDP-HMM for systems with state persistence.</title>
<date>2008</date>
<contexts>
<context position="15363" citStr="Fox et al. (2008)" startWordPosition="2722" endWordPosition="2725"> a number of novel extensions of both topic models and grammars that may be worth exploring. Our claim here is not that these models are the best algorithms for performing these tasks, but that the relationship we described between LDA models and PCFGs suggests a variety of interesting novel models. We end this section with a simple example of such a modification to LDA. Inspired by the standard embedding of HMMs into PCFGs, we propose a “sticky topic” variant of LDA in which adjacent words are more likely to be assigned the same topic. Such an LDA extension is easy to describe as a PCFG (see Fox et al. (2008) for a similar model presented as an extended HMM). The nonterminals Sentence and Topici for i = 1, ... , ` have the same interpretation as before, but we introduce new nonterminals Docj,i that indicate we have just generated a nonterminal in document j belonging to topic i. Given a collection of m documents and ` topics, the rule schemata are as follows: Sentence → Docj,i i ∈ 1, . . . , `; j∈ 1,...,m Docj,1 → j j ∈ 1, ... , m Doc j Doc 7� Topici i i&apos; E 1 E ,z ,z i , j∈ 1,...,m Topici → w i ∈ 1, ... , `; w ∈ V A sample parse generated by a “sticky topic” _3 Topic4 shallow Doc3&apos; Doc3&apos; Doc3&apos; Doc</context>
</contexts>
<marker>Fox, Sudderth, Jordan, Willsky, 2008</marker>
<rawString>E. Fox, E. Sudderth, M. Jordan, and A. Willsky. 2008. An HDP-HMM for systems with state persistence.</rawString>
</citation>
<citation valid="false">
<booktitle>Proceedings of the 25th Annual International Conference on Machine Learning (ICML 2008),</booktitle>
<pages>312--319</pages>
<editor>In Andrew McCallum and Sam Roweis, editors,</editor>
<publisher>Omnipress.</publisher>
<marker></marker>
<rawString>In Andrew McCallum and Sam Roweis, editors, Proceedings of the 25th Annual International Conference on Machine Learning (ICML 2008), pages 312–319. Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 18,</booktitle>
<pages>459--466</pages>
<editor>In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="22467" citStr="Goldwater et al. (2006)" startWordPosition="4032" endWordPosition="4036">mes any tree has been generated from HX. (In the Chinese Restaurant metaphor, mt is the number of tables labeled with t, and m is the number of occupied tables). If aX = 0 then the PYP is equivalent to a CRP with αX = bX, while if aX = 1 then the PYP generates samples from HX. Informally, the CRP has a strong preference to regenerate trees that have been generated frequently before, leading to a “rich-get-richer” dynamics. The PYP can mitigate this somewhat by reducing the effective count of previously generated trees and redistributing that probability mass to new trees generated from HX. As Goldwater et al. (2006) explain, Bayesian inference for HX given samples from GX is effectively performed from types if aX = 0 and from tokens if aX = 1, so varying aX smoothly interpolates between type-based and token-based inference. Adaptor grammars have previously been used primarily to study grammatical inference in the context of language acquisition. The word segmentation task involves segmenting a corpus of unsegmented phonemic utterance representations into words (Elman, 1990; Bernstein-Ratner, 1987). For example, the phoneme string corresponding to “you want to see the book” (with its correct segmentation </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Tom Griffiths, and Mark Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 459– 466, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>101--52285235</pages>
<contexts>
<context position="1602" citStr="Griffiths and Steyvers, 2004" startWordPosition="238" endWordPosition="241">A topic models with multi-word sequences or collocations generated by an AG. The second extension builds on the first one to learn aspects of the internal structure of proper names. 1 Introduction Over the last few years there has been considerable interest in Bayesian inference for complex hierarchical models both in machine learning and in computational linguistics. This paper establishes a theoretical connection between two very different kinds of probabilistic models: Probabilistic Context-Free Grammars (PCFGs) and a class of models known as Latent Dirichlet Allocation (Blei et al., 2003; Griffiths and Steyvers, 2004) models that have been used for a variety of tasks in machine learning. Specifically, we show that an LDA model can be expressed as a certain kind of PCFG, so Bayesian inference for PCFGs can be used to learn LDA topic models as well. The importance of this observation is primarily theoretical, as current Bayesian inference algorithms for PCFGs are less efficient than those for LDA inference. However, once this link is established it suggests a variety of extensions to the LDA topic models, two of which we explore in this paper. The first involves extending the LDA topic model so that it gener</context>
<context position="6206" citStr="Griffiths and Steyvers (2004)" startWordPosition="1038" endWordPosition="1041">r fixed (i.e., chosen by the model designer), or else themselves inferred, e.g., by Bayesian inference. (The adaptor grammar software we used in the experiments described below automatically does this kind of hyper-parameter inference). The inference task is to find the topic probability vector θj of each document Dj given the words wj,k of the documents; in general this also requires inferring the topic to word distributions φ and the topic assigned to each word zj,k. Blei et al. (2003) describe a Variational Bayes inference algorithm for LDA models based on a mean-field approximation, while Griffiths and Steyvers (2004) describe an Markov Chain Monte Carlo inference algorithm based on Gibbs sampling; both are quite effective in practice. 3 Probabilistic Context-Free Grammars Context-Free Grammars are a simple model of hierarchical structure often used to describe natural language syntax. A Context-Free Grammar (CFG) is a quadruple (N, W, R, S) where N and W are disjoint finite sets of nonterminal and terminal symbols respectively, R is a finite set of productions or rules of the form A → β where A ∈ N and β ∈ (N∪W)?, and S ∈ N is the start symbol. In what follows, it will be useful to interpret a CFG as gene</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101:52285235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="25083" citStr="Griffiths et al., 2007" startWordPosition="4487" endWordPosition="4490">en we adapt each of the Topici nonterminals, which means that we learn the probability of each of the sequences of words it can expand to. Sentence → Docj j ∈ 1,... , m Docj → j j ∈ 1, ... , m Docj → Docj Topici i ∈ 1, . . . , `� j∈ 1,...,m Topici → Words i ∈ 1, . . . , ` Words → Word Words → Words Word Word → w w ∈ V In order to demonstrate that this model works, we implemented this using the publicallyavailable adaptor grammar inference software,1 and ran it on the NIPS corpus (composed of published NIPS abstracts), which has previously been used for studying collocation-based topic models (Griffiths et al., 2007). Because there is no generally accepted evaluation for collocation-finding, we merely present some of the sample analyses found by our adaptor grammar. We ran our adaptor grammar with ` = 20 topics (i.e., 20 distinct Topici nonterminals). Adaptor grammar inference on this corpus is actually relatively efficient because the corpus provided by Griffiths et al. (2007) is already segmented by punctuation, so the terminal strings are generally rather short. Rather than set the Dirichlet parameters by hand, we placed vague priors on them and estimated them as described in Johnson and Goldwater (200</context>
<context position="26477" citStr="Griffiths et al. (2007)" startWordPosition="4704" endWordPosition="4707">ates 1http://web.science.mq.edu.au/ ˜mjohnson/Software.htm Topic, → associative memory Topic, → hamming distance Topic, → randomly chosen Topic, → standard deviation Topica → action potentials Topica → membrane potential Topica → primary visual cortex Topica → visual system Topic,0 → nervous system Topic,0 → action potential Topic,0 → ocular dominance Topic,0 → visual field The following are skeletal sample parses, where we have elided all but the adapted nonterminals (i.e., all we show are the Topic nonterminals, since the other structure can be inferred deterministically). Note that because Griffiths et al. (2007) segmented the NIPS abstracts at punctuation symbols, the training corpus contains more than one string from each abstract. 3 (Topic5 polynomial size) (Topic,5 threshold circuits) 4 (Topic,, studied) (Topic,y pattern recognition algorithms) 4 (Topic2 feedforward neural network) (Topic, implementation) 5 (Topic,, single) (Topic,0 ocular dominance stripe) (Topic,2 low) (Topica ocularity) (Topic,2 drift rate) 7 Finding the structure of proper names Grammars offer structural and positional sensitivity that is not exploited in the basic LDA topic models. Here we explore the potential for using Baye</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114(2):211244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>317--325</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="18800" citStr="Johnson and Goldwater (2009)" startWordPosition="3346" endWordPosition="3349"> R as potentially unbounded, and try to learn the rules required to describe a training corpus as well as their probabilities. Adaptor grammars are an example of this approach (Johnson et al., 2007b), where entire subtrees generated by a “base grammar” can be viewed as distinct rules (in that we learn a separate probability for each subtree). The inference task is non-parametric if there are an unbounded number of such subtrees. We review the adaptor grammar generative process below; for an informal introduction see Johnson (2008) and for details of the adaptor grammar inference procedure see Johnson and Goldwater (2009). An adaptor grammar (N, W, R, 5, 0, A, C) consists of a PCFG (N, W, R, 5, 0) in which a subset A C_ N of the nonterminals are adapted, and where each adapted nonterminal X E A has an associated adaptor CX. An adaptor CX for X is a function that maps a distribution over trees TX to a distribution over distributions over TX (we give examples of adaptors below). Just as for a PCFG, an adaptor grammar defines distributions GX over trees TX for each X E N U W. If X E W or X E� A then GX is defined just as for a PCFG above, i.e., using (1). However, if X E A then GX is defined in terms of an additi</context>
<context position="25685" citStr="Johnson and Goldwater (2009)" startWordPosition="4585" endWordPosition="4588">s (Griffiths et al., 2007). Because there is no generally accepted evaluation for collocation-finding, we merely present some of the sample analyses found by our adaptor grammar. We ran our adaptor grammar with ` = 20 topics (i.e., 20 distinct Topici nonterminals). Adaptor grammar inference on this corpus is actually relatively efficient because the corpus provided by Griffiths et al. (2007) is already segmented by punctuation, so the terminal strings are generally rather short. Rather than set the Dirichlet parameters by hand, we placed vague priors on them and estimated them as described in Johnson and Goldwater (2009). The following are some examples of collocations found by our adaptor grammar: Topic0 → cost function Topic0 → fixed point Topic0 → gradient descent Topic0 → learning rates 1http://web.science.mq.edu.au/ ˜mjohnson/Software.htm Topic, → associative memory Topic, → hamming distance Topic, → randomly chosen Topic, → standard deviation Topica → action potentials Topica → membrane potential Topica → primary visual cortex Topica → visual system Topic,0 → nervous system Topic,0 → action potential Topic,0 → ocular dominance Topic,0 → visual field The following are skeletal sample parses, where we hav</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 317–325, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>139--146</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="3028" citStr="Johnson et al. (2007" startWordPosition="482" endWordPosition="485">ite NP coreference models and other applications. The rest of this paper is structured as follows. The next section reviews Latent Dirichlet Allocation (LDA) topic models, and the following section reviews Probabilistic Context-Free Grammars (PCFGs). Section 4 shows how an LDA topic model can be expressed as a PCFG, which provides the fundamental connection between LDA and PCFGs that we exploit in the rest of the paper, and shows how it can be used to define a “sticky topic” version of LDA. The following section reviews Adaptor Grammars (AGs), a non-parametric extension of PCFGs introduced by Johnson et al. (2007b). Section 6 exploits the connection between LDA and PCFGs to propose an AG-based topic model that extends LDA by defining distributions over collocations rather than individual words, and section 7 applies this extension to the problem of finding the structure of proper names. 2 Latent Dirichlet Allocation Models Latent Dirichlet Allocation (LDA) was introduced as an explicit probabilistic counterpart to Latent Semantic Indexing (LSI) (Blei et al., 2003). Like LSI, LDA is intended to produce a lowdimensional characterisation or summary of a doc1148 Proceedings of the 48th Annual Meeting of t</context>
<context position="9709" citStr="Johnson et al. (2007" startWordPosition="1725" endWordPosition="1728">ial rule probability vectors 9X for each nonterminal X E N. This means that there is one Dirichlet parameter αX→,a for each rule X —* Q E R in the CFG. In the “unsupervised” inference problem for a PCFG one is given a CFG, parameters αX for the Dirichlet priors over the rule probabilities, and a corpus of strings. The task is to infer the corresponding posterior distribution over rule probabilities 9X. Recently Bayesian inference algorithms for PCFGs have been described. Kurihara and Sato (2006) describe a Variational Bayes algorithm for inferring PCFGs using a mean-field approximation, while Johnson et al. (2007a) describe a Markov Chain Monte Carlo algorithm based on Gibbs sampling. 4 LDA topic models as PCFGs This section explains how to construct a PCFG that generates the same distribution over a collection of documents as an LDA model, and where Bayesian inference for the PCFG’s rule probabilities yields the corresponding distributions as Bayesian inference of the corresponding LDA models. (There are several different ways of encoding LDA models as PCFGs; the one presented here is not the most succinct — it is possible to collapse the Doc and Doc0 nonterminals — but it has the advantage that the </context>
<context position="18369" citStr="Johnson et al., 2007" startWordPosition="3275" endWordPosition="3278">ies”, say NP, VP, etc., and a set of rules that use these basic categories, say S —* NP VP. The inference task is to learn a set of refined categories and rules (e.g., S7 —* NP2 VP5) as well as their probabilities; this approach can therefore be viewed as a Bayesian version of the “split-merge” approach to grammar induction (Petrov and Klein, 2007). In the second approach, which we adopt here, we regard the set of rules R as potentially unbounded, and try to learn the rules required to describe a training corpus as well as their probabilities. Adaptor grammars are an example of this approach (Johnson et al., 2007b), where entire subtrees generated by a “base grammar” can be viewed as distinct rules (in that we learn a separate probability for each subtree). The inference task is non-parametric if there are an unbounded number of such subtrees. We review the adaptor grammar generative process below; for an informal introduction see Johnson (2008) and for details of the adaptor grammar inference procedure see Johnson and Goldwater (2009). An adaptor grammar (N, W, R, 5, 0, A, C) consists of a PCFG (N, W, R, 5, 0) in which a subset A C_ N of the nonterminals are adapted, and where each adapted nontermina</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007a. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139–146, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>641--648</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3028" citStr="Johnson et al. (2007" startWordPosition="482" endWordPosition="485">ite NP coreference models and other applications. The rest of this paper is structured as follows. The next section reviews Latent Dirichlet Allocation (LDA) topic models, and the following section reviews Probabilistic Context-Free Grammars (PCFGs). Section 4 shows how an LDA topic model can be expressed as a PCFG, which provides the fundamental connection between LDA and PCFGs that we exploit in the rest of the paper, and shows how it can be used to define a “sticky topic” version of LDA. The following section reviews Adaptor Grammars (AGs), a non-parametric extension of PCFGs introduced by Johnson et al. (2007b). Section 6 exploits the connection between LDA and PCFGs to propose an AG-based topic model that extends LDA by defining distributions over collocations rather than individual words, and section 7 applies this extension to the problem of finding the structure of proper names. 2 Latent Dirichlet Allocation Models Latent Dirichlet Allocation (LDA) was introduced as an explicit probabilistic counterpart to Latent Semantic Indexing (LSI) (Blei et al., 2003). Like LSI, LDA is intended to produce a lowdimensional characterisation or summary of a doc1148 Proceedings of the 48th Annual Meeting of t</context>
<context position="9709" citStr="Johnson et al. (2007" startWordPosition="1725" endWordPosition="1728">ial rule probability vectors 9X for each nonterminal X E N. This means that there is one Dirichlet parameter αX→,a for each rule X —* Q E R in the CFG. In the “unsupervised” inference problem for a PCFG one is given a CFG, parameters αX for the Dirichlet priors over the rule probabilities, and a corpus of strings. The task is to infer the corresponding posterior distribution over rule probabilities 9X. Recently Bayesian inference algorithms for PCFGs have been described. Kurihara and Sato (2006) describe a Variational Bayes algorithm for inferring PCFGs using a mean-field approximation, while Johnson et al. (2007a) describe a Markov Chain Monte Carlo algorithm based on Gibbs sampling. 4 LDA topic models as PCFGs This section explains how to construct a PCFG that generates the same distribution over a collection of documents as an LDA model, and where Bayesian inference for the PCFG’s rule probabilities yields the corresponding distributions as Bayesian inference of the corresponding LDA models. (There are several different ways of encoding LDA models as PCFGs; the one presented here is not the most succinct — it is possible to collapse the Doc and Doc0 nonterminals — but it has the advantage that the </context>
<context position="18369" citStr="Johnson et al., 2007" startWordPosition="3275" endWordPosition="3278">ies”, say NP, VP, etc., and a set of rules that use these basic categories, say S —* NP VP. The inference task is to learn a set of refined categories and rules (e.g., S7 —* NP2 VP5) as well as their probabilities; this approach can therefore be viewed as a Bayesian version of the “split-merge” approach to grammar induction (Petrov and Klein, 2007). In the second approach, which we adopt here, we regard the set of rules R as potentially unbounded, and try to learn the rules required to describe a training corpus as well as their probabilities. Adaptor grammars are an example of this approach (Johnson et al., 2007b), where entire subtrees generated by a “base grammar” can be viewed as distinct rules (in that we learn a separate probability for each subtree). The inference task is non-parametric if there are an unbounded number of such subtrees. We review the adaptor grammar generative process below; for an informal introduction see Johnson (2008) and for details of the adaptor grammar inference procedure see Johnson and Goldwater (2009). An adaptor grammar (N, W, R, 5, 0, A, C) consists of a PCFG (N, W, R, 5, 0) in which a subset A C_ N of the nonterminals are adapted, and where each adapted nontermina</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007b. Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 641–648. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Using adaptor grammars to identifying synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="18708" citStr="Johnson (2008)" startWordPosition="3333" endWordPosition="3335">2007). In the second approach, which we adopt here, we regard the set of rules R as potentially unbounded, and try to learn the rules required to describe a training corpus as well as their probabilities. Adaptor grammars are an example of this approach (Johnson et al., 2007b), where entire subtrees generated by a “base grammar” can be viewed as distinct rules (in that we learn a separate probability for each subtree). The inference task is non-parametric if there are an unbounded number of such subtrees. We review the adaptor grammar generative process below; for an informal introduction see Johnson (2008) and for details of the adaptor grammar inference procedure see Johnson and Goldwater (2009). An adaptor grammar (N, W, R, 5, 0, A, C) consists of a PCFG (N, W, R, 5, 0) in which a subset A C_ N of the nonterminals are adapted, and where each adapted nonterminal X E A has an associated adaptor CX. An adaptor CX for X is a function that maps a distribution over trees TX to a distribution over distributions over TX (we give examples of adaptors below). Just as for a PCFG, an adaptor grammar defines distributions GX over trees TX for each X E N U W. If X E W or X E� A then GX is defined just as f</context>
<context position="23964" citStr="Johnson (2008)" startWordPosition="4278" endWordPosition="4279">eme Phonemes The trees generated by this adaptor grammar are the same as the trees generated by the CFG rules. For example, the following skeletal parse in which all but the Word nonterminals are suppressed (the others are deterministically inferrable) shows the parse that corresponds to the correct segmentation of the string above. (Word y u) (Word w a n t) (Word t u) (Word s i) (Word d 6) (Word b u k) Because the Word nonterminal is adapted (indicated here by underlining) the adaptor grammar learns the probability of the entire Word subtrees (e.g., the probability that b u k is a Word); see Johnson (2008) for further details. 1153 6 Topic models with collocations Here we combine ideas from the unigram word segmentation adaptor grammar above and the PCFG encoding of LDA topic models to present a novel topic model that learns topical collocations. (For a non-grammar-based approach to this problem see Wang et al. (2007)). Specifically, we take the PCFG encoding of the LDA topic model described above, but modify it so that the Topici nodes generate sequences of words rather than single words. Then we adapt each of the Topici nonterminals, which means that we learn the probability of each of the se</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. 2008. Using adaptor grammars to identifying synergies in the unsupervised acquisition of linguistic structure. In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenichi Kurihara</author>
<author>Taisuke Sato</author>
</authors>
<title>Variational Bayesian grammar induction for natural language.</title>
<date>2006</date>
<booktitle>In 8th International Colloquium on Grammatical Inference.</booktitle>
<contexts>
<context position="9589" citStr="Kurihara and Sato (2006)" startWordPosition="1706" endWordPosition="1709">es is obtained by marginalising over the trees. In a Bayesian PCFG one puts Dirichlet priors Dir(αX) on each of the multinomial rule probability vectors 9X for each nonterminal X E N. This means that there is one Dirichlet parameter αX→,a for each rule X —* Q E R in the CFG. In the “unsupervised” inference problem for a PCFG one is given a CFG, parameters αX for the Dirichlet priors over the rule probabilities, and a corpus of strings. The task is to infer the corresponding posterior distribution over rule probabilities 9X. Recently Bayesian inference algorithms for PCFGs have been described. Kurihara and Sato (2006) describe a Variational Bayes algorithm for inferring PCFGs using a mean-field approximation, while Johnson et al. (2007a) describe a Markov Chain Monte Carlo algorithm based on Gibbs sampling. 4 LDA topic models as PCFGs This section explains how to construct a PCFG that generates the same distribution over a collection of documents as an LDA model, and where Bayesian inference for the PCFG’s rule probabilities yields the corresponding distributions as Bayesian inference of the corresponding LDA models. (There are several different ways of encoding LDA models as PCFGs; the one presented here </context>
</contexts>
<marker>Kurihara, Sato, 2006</marker>
<rawString>Kenichi Kurihara and Taisuke Sato. 2006. Variational Bayesian grammar induction for natural language. In 8th International Colloquium on Grammatical Inference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite PCFG using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>688--697</pages>
<contexts>
<context position="17678" citStr="Liang et al., 2007" startWordPosition="3148" endWordPosition="3151">the focus of intense research in machine learning recently. In the topicmodelling community this has lead to work on Dirichlet Processes and Chinese Restaurant Processes, which can be used to estimate the number of topics as well as their distribution across documents (Teh et al., 2006). There are two obvious non-parametric extensions to PCFGs. In the first we regard the set of nonterminals N as potentially unbounded, and try to learn the set of nonterminals required to describe the training corpus. This approach goes under the name of the “infinite HMM” or “infinite PCFG” (Beal et al., 2002; Liang et al., 2007; Liang et al., 2009). Informally, we are given a set of “basic categories”, say NP, VP, etc., and a set of rules that use these basic categories, say S —* NP VP. The inference task is to learn a set of refined categories and rules (e.g., S7 —* NP2 VP5) as well as their probabilities; this approach can therefore be viewed as a Bayesian version of the “split-merge” approach to grammar induction (Petrov and Klein, 2007). In the second approach, which we adopt here, we regard the set of rules R as potentially unbounded, and try to learn the rules required to describe a training corpus as well as </context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 688– 697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Probabilistic grammars and hierarchical Dirichlet processes.</title>
<date>2009</date>
<booktitle>In The Oxford Handbook of Applied Bayesian Analysis.</booktitle>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="17699" citStr="Liang et al., 2009" startWordPosition="3152" endWordPosition="3155"> research in machine learning recently. In the topicmodelling community this has lead to work on Dirichlet Processes and Chinese Restaurant Processes, which can be used to estimate the number of topics as well as their distribution across documents (Teh et al., 2006). There are two obvious non-parametric extensions to PCFGs. In the first we regard the set of nonterminals N as potentially unbounded, and try to learn the set of nonterminals required to describe the training corpus. This approach goes under the name of the “infinite HMM” or “infinite PCFG” (Beal et al., 2002; Liang et al., 2007; Liang et al., 2009). Informally, we are given a set of “basic categories”, say NP, VP, etc., and a set of rules that use these basic categories, say S —* NP VP. The inference task is to learn a set of refined categories and rules (e.g., S7 —* NP2 VP5) as well as their probabilities; this approach can therefore be viewed as a Bayesian version of the “split-merge” approach to grammar induction (Petrov and Klein, 2007). In the second approach, which we adopt here, we regard the set of rules R as potentially unbounded, and try to learn the rules required to describe a training corpus as well as their probabilities. </context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Percy Liang, Michael Jordan, and Dan Klein. 2009. Probabilistic grammars and hierarchical Dirichlet processes. In The Oxford Handbook of Applied Bayesian Analysis. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="27283" citStr="Marcus et al., 1993" startWordPosition="4822" endWordPosition="4825">ic,, studied) (Topic,y pattern recognition algorithms) 4 (Topic2 feedforward neural network) (Topic, implementation) 5 (Topic,, single) (Topic,0 ocular dominance stripe) (Topic,2 low) (Topica ocularity) (Topic,2 drift rate) 7 Finding the structure of proper names Grammars offer structural and positional sensitivity that is not exploited in the basic LDA topic models. Here we explore the potential for using Bayesian inference for learning linear ordering constraints that hold between elements within proper names. The Penn WSJ treebank is a widely used resource within computational linguistics (Marcus et al., 1993), but one of its weaknesses is that it does not indicate any structure internal to base noun phrases (i.e., it presents “flat” analyses of the pre-head NP elements). For many applications it would be extremely useful to have a more elaborated analysis of this kind of NP structure. For example, in an NP coreference application, if we could determine that Bill and Hillary are both first 1154 names then we could infer that Bill Clinton and Hillary Clinton are likely to refer to distinct individuals. On the other hand, because Mr in Mr Clinton is not a first name, it is possible that Mr Clinton an</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Michell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York.</location>
<contexts>
<context position="18099" citStr="Petrov and Klein, 2007" startWordPosition="3226" endWordPosition="3229">nded, and try to learn the set of nonterminals required to describe the training corpus. This approach goes under the name of the “infinite HMM” or “infinite PCFG” (Beal et al., 2002; Liang et al., 2007; Liang et al., 2009). Informally, we are given a set of “basic categories”, say NP, VP, etc., and a set of rules that use these basic categories, say S —* NP VP. The inference task is to learn a set of refined categories and rules (e.g., S7 —* NP2 VP5) as well as their probabilities; this approach can therefore be viewed as a Bayesian version of the “split-merge” approach to grammar induction (Petrov and Klein, 2007). In the second approach, which we adopt here, we regard the set of rules R as potentially unbounded, and try to learn the rules required to describe a training corpus as well as their probabilities. Adaptor grammars are an example of this approach (Johnson et al., 2007b), where entire subtrees generated by a “base grammar” can be viewed as distinct rules (in that we learn a separate probability for each subtree). The inference task is non-parametric if there are an unbounded number of such subtrees. We review the adaptor grammar generative process below; for an informal introduction see Johns</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M Jordan</author>
<author>M Beal</author>
<author>D Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>101--1566</pages>
<contexts>
<context position="17347" citStr="Teh et al., 2006" startWordPosition="3089" endWordPosition="3092">j,i —* Docj,i Topici, Bayesian inference will be biased to find distributions in which adjacent words will tend to have the same topic. 5 Adaptor Grammars Non-parametric Bayesian inference, where the inference task involves learning not just the values of a finite vector of parameters but which parameters are relevant, has been the focus of intense research in machine learning recently. In the topicmodelling community this has lead to work on Dirichlet Processes and Chinese Restaurant Processes, which can be used to estimate the number of topics as well as their distribution across documents (Teh et al., 2006). There are two obvious non-parametric extensions to PCFGs. In the first we regard the set of nonterminals N as potentially unbounded, and try to learn the set of nonterminals required to describe the training corpus. This approach goes under the name of the “infinite HMM” or “infinite PCFG” (Beal et al., 2002; Liang et al., 2007; Liang et al., 2009). Informally, we are given a set of “basic categories”, say NP, VP, etc., and a set of rules that use these basic categories, say S —* NP VP. The inference task is to learn a set of refined categories and rules (e.g., S7 —* NP2 VP5) as well as thei</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101:1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuerui Wang</author>
<author>Andrew McCallum</author>
<author>Xing Wei</author>
</authors>
<title>Topical n-grams: Phrase and topic discovery, with an application to information retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th IEEE International Conference on Data Mining (ICDM),</booktitle>
<pages>697--702</pages>
<contexts>
<context position="24282" citStr="Wang et al. (2007)" startWordPosition="4328" endWordPosition="4331">of the string above. (Word y u) (Word w a n t) (Word t u) (Word s i) (Word d 6) (Word b u k) Because the Word nonterminal is adapted (indicated here by underlining) the adaptor grammar learns the probability of the entire Word subtrees (e.g., the probability that b u k is a Word); see Johnson (2008) for further details. 1153 6 Topic models with collocations Here we combine ideas from the unigram word segmentation adaptor grammar above and the PCFG encoding of LDA topic models to present a novel topic model that learns topical collocations. (For a non-grammar-based approach to this problem see Wang et al. (2007)). Specifically, we take the PCFG encoding of the LDA topic model described above, but modify it so that the Topici nodes generate sequences of words rather than single words. Then we adapt each of the Topici nonterminals, which means that we learn the probability of each of the sequences of words it can expand to. Sentence → Docj j ∈ 1,... , m Docj → j j ∈ 1, ... , m Docj → Docj Topici i ∈ 1, . . . , `� j∈ 1,...,m Topici → Words i ∈ 1, . . . , ` Words → Word Words → Words Word Word → w w ∈ V In order to demonstrate that this model works, we implemented this using the publicallyavailable adapt</context>
</contexts>
<marker>Wang, McCallum, Wei, 2007</marker>
<rawString>Xuerui Wang, Andrew McCallum, and Xing Wei. 2007. Topical n-grams: Phrase and topic discovery, with an application to information retrieval. In Proceedings of the 7th IEEE International Conference on Data Mining (ICDM), pages 697–702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Wetherell</author>
</authors>
<title>Probabilistic languages: A review and some open questions. Computing Surveys,</title>
<date>1980</date>
<pages>12--361</pages>
<contexts>
<context position="8827" citStr="Wetherell (1980)" startWordPosition="1575" endWordPosition="1576">ach X E N U W with a distribution GX over the trees TX as follows. If X E W (i.e., if X is a terminal) then GX is the distribution that puts probability 1 on the single-node tree labelled X. If X E N (i.e., if X is a nonterminal) then: EGX = OX→B1...BnTDX(GB1, ... , GBn) (1) X→B1...Bn∈RX where: � n TDA (G1, ... , Gn)�X� = H Gi(ti). tl ... to i=1 That is, TDA(G1, ... , Gn) is a distribution over TA where each subtree ti is generated independently from Gi. These equations have solutions (i.e., the PCFG is said to be “consistent”) when the rule probabilities 9A obey certain conditions; see e.g., Wetherell (1980) for details. The PCFG generates the distribution over trees GS, where 5 is the start symbol. The distribution over the strings it generates is obtained by marginalising over the trees. In a Bayesian PCFG one puts Dirichlet priors Dir(αX) on each of the multinomial rule probability vectors 9X for each nonterminal X E N. This means that there is one Dirichlet parameter αX→,a for each rule X —* Q E R in the CFG. In the “unsupervised” inference problem for a PCFG one is given a CFG, parameters αX for the Dirichlet priors over the rule probabilities, and a corpus of strings. The task is to infer t</context>
</contexts>
<marker>Wetherell, 1980</marker>
<rawString>C.S. Wetherell. 1980. Probabilistic languages: A review and some open questions. Computing Surveys, 12:361–379.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>