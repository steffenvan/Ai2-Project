<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000094">
<title confidence="0.7596345">
FCICU: The Integration between Sense-Based Kernel and Surface-
Based Methods to Measure Semantic Textual Similarity
</title>
<author confidence="0.889729">
Basma Hassan
</author>
<affiliation confidence="0.8434685">
Computer Science
Department, Faculty of
Computers and Information
Fayoum University
</affiliation>
<address confidence="0.480177">
Fayoum, Egypt
</address>
<email confidence="0.948744">
bhassan@fayoum.edu
</email>
<note confidence="0.822682705882353">
.eg
Samir AbdelRahman
Computer Science
Department, Faculty of
Computers and Information
Cairo University
Giza, Egypt
s.abdelrahman@fci-
cu.edu.eg
Reem Bahgat
Computer Science
Department, Faculty of
Computers and Information
Cairo University
Giza, Egypt
r.bahgat@fci-
cu.edu.eg
</note>
<sectionHeader confidence="0.939204" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992014">
This paper describes FCICU team participa-
tion in SemEval 2015 for Semantic Textual
Similarity challenge. Our main contribution is
to propose a word-sense similarity method us-
ing BabelNet relationships. In the English
subtask challenge, we submitted three systems
(runs) to assess the proposed method. In
Run1, we used our proposed method coupled
with a string kernel mapping function to cal-
culate the textual similarity. In Run2, we used
the method with a tree kernel function. In
Run3, we averaged Run1 with a previously
proposed surface-based approach as a kind of
integration. The three runs are ranked 41st,
57th, and 20th of 73 systems, with mean corre-
lation 0.702, 0.597, and 0.759 respectively.
For the interpretable task, we submitted a
modified version of Run1 achieving mean F1
0.846, 0.461, 0.722, and 0.44 for alignment,
type, score, and score with type respectively.
</bodyText>
<sectionHeader confidence="0.992047" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999786024390244">
Semantic Textual Similarity (STS) is the task of
measuring the similarity between two text snippets
according to their meaning. Human has an intrinsic
ability to recognize the degree of similarity and
difference between texts. Simulating the process of
human judgment in computers is still an extremely
difficult task and has recently drawn much atten-
tion. STS is very important because a wide range
of NLP applications such as information retrieval,
question answering, machine translation, etc. rely
heavily on this task.
This paper describes our proposed STS systems
by which we participated in two subtasks of STS
task (Task2) at SemEval 2015, namely English
STS and Interpretable STS. The former calculates
a graded similarity score from 0 to 5 between two
sentences (with 5 being the most similar), while
the latter is a pilot subtask that requires aligning
chunks of two sentences, describing what kind of
relation exists between each pair of chunks, and a
score for the similarity between the pair of chunks
(Agirre et al., 2015).
Sense or meaning of natural language text can
be inferred from several linguistic concepts, in-
cluding lexical, syntactic, and semantic knowledge
of the language. Our approach employs those as-
pects to calculate the similarity between senses of
text constituents, phrases or words, relying mainly
on BabelNet senses. The similarity between two
text snippets is firstly calculated using kernel func-
tions, which map a text snippet to the feature space
based on a proposed word sense similarity method.
Besides, the sense-based similarity score obtained
is combined with a surface-based similarity score
to study the consolidation impact in the STS task.
The paper is organized as follows. Section 2
explains our proposed word sense similarity meth-
od. Section 3 describes the proposed systems. Sec-
tion 4 presents the experiments conducted and
analyzes the results achieved. Section 5 concludes
the paper and suggests some future directions.
</bodyText>
<page confidence="0.916102">
154
</page>
<note confidence="0.9138255">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 154–158,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.8736135" genericHeader="method">
2 The proposed Word-Sense Similarity
(WSS) Method
</sectionHeader>
<bodyText confidence="0.999823384615385">
Several semantic textual similarity (STS) methods
have been proposed in literature. Sense-based
methods are qualified when different words are
used to convey the same meaning in different texts
(Pilehvar et al., 2013). Surface-based methods,
mostly fail in identifying similarity between texts
with maximal semantic overlap but minimal lexical
overlap. We present a sense-based STS approach
that produces similarity score between texts by
means of a kernel function (Shawe-Taylor and
Cristianini, 2004). Then, we integrate the sense-
based approach with the surface-based soft cardi-
nality approach presented in (Jimenez et al., 2012)
to demonstrate that both sense-based and surface-
based similarity methods are complementary to
each other.
The design of our kernel function relies on the
hypothesis that the greater the similarity of word
senses between two texts, the higher their semantic
equivalence will be. Accordingly, our kernel maps
a text to feature space using a similarity measure
between word senses. We proposed a WSS meas-
ure that computes the similarity score between two
word senses (wsi, wsj) using the arithmetic mean of
two measures: Semantic Distance (simD) and Con-
textual Similarity (simC). That is:
</bodyText>
<equation confidence="0.722115">
simD (wsi , wsj)  simC (wsi , wsj
)
2
</equation>
<subsectionHeader confidence="0.9825">
2.1 Semantic Distance
</subsectionHeader>
<bodyText confidence="0.999944733333333">
This measure computes the similarity between
word senses based on the distance between them in
a multilingual semantic network, named BabelNet
(Navigli and Ponzetto, 2010). BabelNet1 is a rich
semantic knowledge resource that covers a wide
range of concepts and named entities connected
with large numbers of semantic relations. Concepts
and relations are gathered from WordNet (Miller,
1995); and Wikipedia2. The semantic knowledge is
encoded as a labeled directed graph, where vertices
are BabelNet senses (concepts), and edges connect
pairs of senses with a label indicating the type of
the semantic relation between them. Our semantic
distance measure is a function of two similarity
scores: simBn and simNBn.
</bodyText>
<equation confidence="0.544271">
1 http://babelnet.org/
2 http://en.wikipedia.org/
</equation>
<bodyText confidence="0.99990775">
The first score (simBn) is based on the distance
between two word-senses, wsi and wsj; where, the
shorter the distance between them, the more se-
mantically related they are. That is:
</bodyText>
<equation confidence="0.974013">
Sim,,(WSi,WSJ) = I— (2)
Maxlen
</equation>
<bodyText confidence="0.999536230769231">
where Maxlen3 is the maximum path length con-
necting two senses in BabelNet, and len(wsi,wsj) is
the length of the shortest path between two senses,
wsi and wsj, in BabelNet in both directions; i.e wsi
 wsj, and wsj  wsi. The shortest path is calculat-
ed using Dijkstra&apos;s algorithm.
The second score (simNBn) represents the degree
of similarity between the neighbors of wsi and the
neighbors of wsj, which influences the degree of
similarity between the two senses. Hence, simNBn is
calculated by taking the arithmetic mean of all
neighbor-pairs similarity. That is:
(3)
where NSi and NSj are the sets of the most semanti-
cally related senses directly connected to wsi and
wsj respectively in BabelNet; ni =  |NSi |, and nj = |
NSj |; and simWuP (wsk, wsl) is Wu and Palmer simi-
larity measure (Wu and Palmer, 1994).
The values of the two scores presented above
determine the way of calculating the semantic dis-
tance measure (simD) for word senses’ pair (wsi,
wsj). For zero similarity of both scores, simD is
simply equals to Wu and Palmer similarity meas-
ure; i.e. simD (wsi,wsj) = simWuP (wsi,wsj). Generally,
for non-zero similarity scores, simD is calculated
using the arithmetic mean of the two scores.
</bodyText>
<subsectionHeader confidence="0.998328">
2.2 Contextual Similarity
</subsectionHeader>
<bodyText confidence="0.9960826">
This measure calculates the similarity between the
word senses pair (wsi, wsj) based on the overlap
between their contexts derived from a corpus. The
overlap coefficient used is Jaccard Coefficient.
That is:
</bodyText>
<equation confidence="0.8073595">
simC (wsi, wsj
)
</equation>
<bodyText confidence="0.998679666666667">
where Ci is the set of: 1) all the word senses that
co-occur with wsi in the corpus, and 2) all senses
directly connected to wsi in BabelNet; Cj is similar.
</bodyText>
<figure confidence="0.806966666666667">
3 We tried different values in experiments and the best was 7.
WSS(wsi,wsj) 
(1)
Ci  Cj (4)
Ci  Cj
155
</figure>
<sectionHeader confidence="0.863931" genericHeader="method">
3 Systems Description
</sectionHeader>
<subsectionHeader confidence="0.997666">
3.1 Text Preprocessing
</subsectionHeader>
<bodyText confidence="0.999925384615384">
The given input sentences are first preprocessed to
map the raw natural language text into structured
or annotated representation. This process includes
different tasks: tokenization, lemmatization, Part-
of-Speech tagging, and word-sense tagging. All
tasks except word-sense tagging are carried out
using Stanford CoreNLP (Manning et al., 2014).
Sense tagging is the task of attaching a sense to a
word or a token. It is performed by selecting the
most commonly used BabelNet sense that matches
the part of speech (POS) of the word. Accordingly,
we restricted sense tagging to: nouns, verbs,
adjectives, and adverbs.
</bodyText>
<subsectionHeader confidence="0.995799">
3.2 English STS Subtask
</subsectionHeader>
<bodyText confidence="0.996088">
We submitted three systems in this subtask, named
Run1, Run2, and Run3.
</bodyText>
<subsectionHeader confidence="0.76548">
3.2.1 Sense-based String Kernel (Run1)
</subsectionHeader>
<bodyText confidence="0.999269">
Given two sentences, s1 and s2, the similarity score
between s1 and s2 resulted by this system is the
value of a designed string kernel function between
the two sentences. This kernel is defined by an
embedded mapping from the space of sentences
possibly to a vector space F, whose coordinates are
indexed by a set I of word senses contained in s1
and s2; i.e.  : s  (ws(s))wsI  F. Thus, given a
sentence s, it can be represented by a row vector
as:  (s) = (ws1(s), ws2(s) ... wsN(s)), in which
each entry records how similar a particular word
sense (wsI) is to the sentence s. The mapping is
given by:
</bodyText>
<equation confidence="0.974876">
�ws(s) = max { WSS(ws,ws) }, (5)
15i5n
</equation>
<bodyText confidence="0.999464833333333">
where WSS(ws, wsi) is our defined word sense sim-
ilarity method ( Eq. (1) ), and n is the number of
word senses contained in sentence s.
The string kernel between two sentences s1 and
s2 is calculated as (Shawe-Taylor and Cristianini,
2004):
</bodyText>
<equation confidence="0.997013333333333">
KS(s0s2) _ O(sO,O(s2) _ E Ows(s1)- Ows(s2) (6)
ws I
s
</equation>
<bodyText confidence="0.991947333333333">
The last step remaining is normalizing the ker-
nel (i.e. range = [0,1]) to avoid any biasness to sen-
tence length. The normalized string kernel
KNS(s1,s2) is calculated by (Shawe-Taylor and
Cristianini, 2004):
Hence, .
</bodyText>
<subsectionHeader confidence="0.814478">
3.2.2 Sense-based Tree Kernel (Run2)
</subsectionHeader>
<bodyText confidence="0.9957488">
This system applies tree kernel instead of string
kernel. Tree kernels generally map a tree to the
feature space of subtrees. There are various types
of tree kernel designed in literature, among them is
the all-subtree kernel presented in (Shawe-Taylor
and Cristianini, 2004). The all-subtree kernel is
defined by an embedded mapping from the space
of all finite syntactic trees to a vector space F,
whose coordinates are indexed by a subset T of
syntactic subtrees; i.e.  : t  (st(t))stT  F. The
mapping st(t) is a simple exact matching function
that returns 1 if st is a subtree in t, and returns 0
otherwise. We modified the mapping of all-subtree
kernel to capture the semantic similarity between
subtrees instead of the structural similarity. The
semantic similarity between subtrees is calculated
recursively bottom-up from leaves to the root, in
which the similarity between leaves is calculated
using our defined word sense similarity method.
From this point, the remaining steps are typical
to the string kernel steps followed in the first sys-
tem. Hence, given two sentences s1 and s2, their
similarity score is the normalized kernel value be-
tween their syntactic parse trees t1 and t2;
i.e. siMR-2 (sl I SO = KIT (t11 t2 ) .
</bodyText>
<subsectionHeader confidence="0.938338">
3.2.3 Sense-based with Surface-based (Run3)
</subsectionHeader>
<bodyText confidence="0.956494615384615">
This system provides the results of taking the
arithmetic mean of: 1) our sense-based string ker-
nel (Run1); and 2) the surface-based similarity
function proposed by Jimenez et al. (2012). The
approach presented in (Jimenez et al., 2012)
represents sentence words as sets of q-grams on
which the notion of Soft Cardinality is applied. In
this system, all the calculations in the approach are
used unchanged with the following parameters set-
up: p=2, bias=0, and =0.5. Accordingly, the
similarity function is the Dice overlap coefficient
on q-grams; i.e. simsc(A,B) = 2 A n B&apos;l(A&apos;+B&apos;).
Hence,
</bodyText>
<equation confidence="0.926709">
&amp;quot;&apos;R.3 V31�S 2)= (N S(s1,sOsimSC(s1,s2))
simR
2
</equation>
<page confidence="0.67266">
156
</page>
<subsectionHeader confidence="0.989337">
3.3 Interpretable STS Subtask
</subsectionHeader>
<bodyText confidence="0.99998975">
The interpretable STS is a pilot subtask, which
aims to determine the parts of sentences, chunks,
that are equivalent in meaning and the parts that
are not. This is twofold: (a) aligning corresponding
chunks, and (b) assigning a similarity score, and a
type to each alignment. Given two sentences split-
ted into gold standard chunks, our system carries
out the task requirements using our sense-based
string kernel by considering each chunk as a text
snippet. Firstly, the similarity between chunks of
all possible chunk-pairs is calculated, upon which
chunks are aligned. Where, chunk pairs with a high
similarity score are aligned first, followed by pairs
with lower similarity. Thereafter, for each align-
ment of chunks c1 and c2, the alignment type is
determined according to the following rules:
</bodyText>
<listItem confidence="0.981656222222222">
• If the similarity score between c1 and c2 is 5,
the type is EQUI.
• If all word senses of c1 matched the word
senses in c2, the type is SPEC2; similarly for
SPEC1.
• If both c1 and c2 contain a single word sense,
and are directly connected by an antonym re-
lation in BabelNet, then the type is OPPO.
• If the similarity score between c1 and c2 is in
</listItem>
<bodyText confidence="0.6994915">
range [3,5[, the type is SIM; while if it is in
range ]0,3[, the type is REL.
</bodyText>
<listItem confidence="0.65260125">
• If any chunk has no corresponding chunk in
the other sentence, then the type is either
NOALI or ALIC based on the alignment re-
striction in the subtask.
</listItem>
<sectionHeader confidence="0.967945" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.990824">
4.1 English STS
</subsectionHeader>
<bodyText confidence="0.999901071428572">
The main evaluation measure selected by the task
organizers was the mean Pearson correlation be-
tween the system scores and the gold standard
scores calculated on the test set (3000 sentence
pairs from five datasets). Table 1 presents the offi-
cial results of our submissions in this subtask on
SemEval-2015 test set. It also includes the results
of the Soft Cardinality STS approach (SC) on the
same test set for analysis. Our best system (Run3)
achieved 0.7595 and ranked the 20th out of 73 sys-
tems.
We conducted preliminary experiments on the
training dataset of SemEval-2015 for evaluating
our sense-based string and tree kernel similarity
methods, and the integration between each of them
with the SC approach. The results of those experi-
ments led to the final submission of the two ker-
nels separately (Run1 and Run2) and integrating
the string kernel method with SC (Run3). Table 2
focuses on the results obtained from our integrated
system (Run3) and SC approach in training, but
includes also the recent SC approach (SC-ML)
proposed in (Jimenez et al., 2014).
It is noteworthy from the tables that Run3 im-
proved the SC system results on both the training
and testing sets for all the different settings for al-
pha value in the SC approach. The possible reason
based on our observation on the training datasets is
that the two systems have opposite strength and
weakness points. Figure 1 depicts the similarity
scores resulted from Run1, Run3, and SC systems
along with the gold standard scores (GS) on some
sentence pairs from images dataset. It is shown
from the figure that Run1 outperforms SC for se-
mantically equivalent sentence pairs (i.e. scores &gt;
3.5), while SC outperforms Run1 for less-related
sentence pairs (i.e. score &lt; 2). Hence, their integra-
tion by taking their average (Run3) improves the
performance of their individual use and did not
reduce the SC results. Also, though this integration
is simple, it outperformed SC-ML that applies ma-
chine learning on some extracted text features.
</bodyText>
<figureCaption confidence="0.933446">
Figure 1. Sample Results of Run1, Run3, and SC on
‘images’ Dataset of SemEval Training data.
</figureCaption>
<subsectionHeader confidence="0.892868">
4.2 Interpretable STS
</subsectionHeader>
<bodyText confidence="0.998704625">
There were two datasets only in the test set, name-
ly images and headlines. The results in this subtask
are evaluated by four F1 measures for alignment,
score, alignment type, and both score with align-
ment. The results of our submitted run (average of
the two datasets) were 0.846, 0.461, 0.722, and
0.44 for F1-Ali, F1-type, F1-score, and F1-
score+type respectively.
</bodyText>
<table confidence="0.915989">
157
System answers-forums answers-students belief headlines images Mean Rank
Run1 0.6152 0.6686 0.6109 0.7418 0.7853 0.7022 41st/73
Run2 0.3659 0.6460 0.5896 0.6448 0.6194 0.5970 57th/73
Run3 0.7091 0.7096 0.7184 0.7922 0.8223 0.7595 20th/73
SC 0.7078 0.7020 0.7232 0.7966 0.8120 0.7565 -
</table>
<tableCaption confidence="0.997662">
Table 1. Our Results on SemEval-2015 Test Datasets.
</tableCaption>
<table confidence="0.999960333333333">
 System deft-forum deft-news headlines images OnWI tweet-news Mean
- Run1 0.4259 0.7271 0.6914 0.7576 0.7597 0.7227 0.6955
- SC-ML 0.4607 0.7216 0.7605 0.7782 0.8426 0.6583 0.7209
0.25 Run3 0.5092 0.7479 0.7383 0.7902 0.7857 0.7744 0.7387
SC 0.5047 0.7311 0.7362 0.7785 0.7727 0.7709 0.7307
0.5 Run3 0.4937 0.7531 0.7377 0.7887 0.7834 0.7723 0.7359
SC 0.4789 0.7407 0.7374 0.7763 0.7671 0.7641 0.7257
0.7 Run3 0.4816 0.7541 0.7356 0.7862 0.7806 0.7681 0.7322
SC 0.4558 0.7396 0.7321 0.7694 0.7586 0.7496 0.7158
</table>
<tableCaption confidence="0.999858">
Table 2. Results of Run3 vs. SC on SemEval-2014 Test Datasets (SemEval-2015 Training dataset).
</tableCaption>
<sectionHeader confidence="0.980522" genericHeader="conclusions">
5 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.99982225">
Our experiments proved that sense-based and sur-
face-based similarity methods are complementary
to each other in STS. We also realized that string
kernel is more beneficial than tree kernel. Our po-
tential future work includes: 1) enhancing our
sense-based kernel approach, and 2) further en-
hancement in the integration between SC and our
sense-based approach.
</bodyText>
<sectionHeader confidence="0.998467" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999920470588235">
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirrea, Weiwei
Guo, Iñigo Lopez-Gazpio, Montse Maritxalar, Rada
Mihalcea, German Rigau, Larraitz Uria, and Janyce
Wiebe. 2015. SemEval-2015 Task 2: Semantic
Textual Similarity, English, Spanish and Pilot on
Interpretability. In Proceedings of the 9th
International Workshop on Semantic Evaluation
(SemEval 2015), Denver, CO, USA.
Sergio Jimenez, Claudia Becerra, and Alexander
Gelbukh. 2012. Soft Cardinality: A Parameterized
Similarity Function for Text Comparison. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics (*SEM), pages 449–
453, Montreal, Canada.
Sergio Jimenez, George Dueñas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combining
Soft Cardinality Features for Semantic Textual
Similarity, Relatedness and Entailment. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014), pages 732–
742, Duplin, Ireland.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David
McClosky. 2014. The Stanford CoreNLP Natural
Language Processing Toolkit. In Proceedings of the
52nd Annual Meeting of the Association for
Computational Linguistics: System Demonstrations,
pages 55–60, Baltimore, Maryland.
George A. Miller. 1995. WordNet: A Lexical Database
for English. Communications of the ACM, 38(11):
39–41.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
BabelNet: Building a Very Large Multilingual
Semantic Network. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 216–225, Uppsala, Sweden.
Mohammad Taher Pilehvar, David Jurgens, and Roberto
Navigli. 2013. Align, Disambiguate and Walk: A
Unified Approach for Measuring. In Proceedings of
the 51st Annual Meeting of the Association for
Computational Linguistics (ACL 2013), pages 1341–
1351, Sofia, Bulgaria.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Zhibiao Wu and Martha Palmer. 1994. Verbs
Semantics and Lexical Selection. In Proceedings of
the 32nd annual meeting on Association for
Computational Linguistics (ACL&apos;94), pages 133–138,
Stroudsburg, PA, USA.
</reference>
<page confidence="0.906689">
158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001500">
<title confidence="0.9648425">The Integration between Sense-Based Kernel and Based Methods to Measure Semantic Textual Similarity</title>
<author confidence="0.559">Basma</author>
<affiliation confidence="0.25887425">Computer Department, Faculty Computers and Fayoum</affiliation>
<address confidence="0.511835">Fayoum, Egypt</address>
<email confidence="0.8365025">bhassan@fayoum.edu.eg</email>
<author confidence="0.389597">Samir</author>
<affiliation confidence="0.68158625">Computer Department, Faculty Computers and Cairo</affiliation>
<address confidence="0.979189">Giza, Egypt</address>
<email confidence="0.8699425">s.abdelrahman@fcicu.edu.eg</email>
<author confidence="0.386307">Reem</author>
<affiliation confidence="0.688041">Computer Department, Faculty Computers and Cairo</affiliation>
<address confidence="0.969125">Giza, Egypt</address>
<email confidence="0.982125">cu.edu.eg</email>
<abstract confidence="0.991782714285714">This paper describes FCICU team participation in SemEval 2015 for Semantic Textual Similarity challenge. Our main contribution is to propose a word-sense similarity method using BabelNet relationships. In the English subtask challenge, we submitted three systems (runs) to assess the proposed method. In Run1, we used our proposed method coupled with a string kernel mapping function to calculate the textual similarity. In Run2, we used the method with a tree kernel function. In Run3, we averaged Run1 with a previously proposed surface-based approach as a kind of The three runs are ranked and of 73 systems, with mean correlation 0.702, 0.597, and 0.759 respectively. For the interpretable task, we submitted a modified version of Run1 achieving mean F1 0.846, 0.461, 0.722, and 0.44 for alignment, type, score, and score with type respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
</authors>
<title>Aitor Gonzalez-Agirrea, Weiwei Guo, Iñigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe.</title>
<date>2015</date>
<booktitle>SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, CO, USA.</location>
<contexts>
<context position="2458" citStr="Agirre et al., 2015" startWordPosition="369" endWordPosition="372">ns such as information retrieval, question answering, machine translation, etc. rely heavily on this task. This paper describes our proposed STS systems by which we participated in two subtasks of STS task (Task2) at SemEval 2015, namely English STS and Interpretable STS. The former calculates a graded similarity score from 0 to 5 between two sentences (with 5 being the most similar), while the latter is a pilot subtask that requires aligning chunks of two sentences, describing what kind of relation exists between each pair of chunks, and a score for the similarity between the pair of chunks (Agirre et al., 2015). Sense or meaning of natural language text can be inferred from several linguistic concepts, including lexical, syntactic, and semantic knowledge of the language. Our approach employs those aspects to calculate the similarity between senses of text constituents, phrases or words, relying mainly on BabelNet senses. The similarity between two text snippets is firstly calculated using kernel functions, which map a text snippet to the feature space based on a proposed word sense similarity method. Besides, the sense-based similarity score obtained is combined with a surface-based similarity score</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, 2015</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirrea, Weiwei Guo, Iñigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Soft Cardinality: A Parameterized Similarity Function for Text Comparison.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>449--453</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="4264" citStr="Jimenez et al., 2012" startWordPosition="634" endWordPosition="637">mantic textual similarity (STS) methods have been proposed in literature. Sense-based methods are qualified when different words are used to convey the same meaning in different texts (Pilehvar et al., 2013). Surface-based methods, mostly fail in identifying similarity between texts with maximal semantic overlap but minimal lexical overlap. We present a sense-based STS approach that produces similarity score between texts by means of a kernel function (Shawe-Taylor and Cristianini, 2004). Then, we integrate the sensebased approach with the surface-based soft cardinality approach presented in (Jimenez et al., 2012) to demonstrate that both sense-based and surfacebased similarity methods are complementary to each other. The design of our kernel function relies on the hypothesis that the greater the similarity of word senses between two texts, the higher their semantic equivalence will be. Accordingly, our kernel maps a text to feature space using a similarity measure between word senses. We proposed a WSS measure that computes the similarity score between two word senses (wsi, wsj) using the arithmetic mean of two measures: Semantic Distance (simD) and Contextual Similarity (simC). That is: simD (wsi , w</context>
<context position="11076" citStr="Jimenez et al. (2012)" startWordPosition="1772" endWordPosition="1775">root, in which the similarity between leaves is calculated using our defined word sense similarity method. From this point, the remaining steps are typical to the string kernel steps followed in the first system. Hence, given two sentences s1 and s2, their similarity score is the normalized kernel value between their syntactic parse trees t1 and t2; i.e. siMR-2 (sl I SO = KIT (t11 t2 ) . 3.2.3 Sense-based with Surface-based (Run3) This system provides the results of taking the arithmetic mean of: 1) our sense-based string kernel (Run1); and 2) the surface-based similarity function proposed by Jimenez et al. (2012). The approach presented in (Jimenez et al., 2012) represents sentence words as sets of q-grams on which the notion of Soft Cardinality is applied. In this system, all the calculations in the approach are used unchanged with the following parameters setup: p=2, bias=0, and =0.5. Accordingly, the similarity function is the Dice overlap coefficient on q-grams; i.e. simsc(A,B) = 2 A n B&apos;l(A&apos;+B&apos;). Hence, &amp;quot;&apos;R.3 V31�S 2)= (N S(s1,sOsimSC(s1,s2)) simR 2 156 3.3 Interpretable STS Subtask The interpretable STS is a pilot subtask, which aims to determine the parts of sentences, chunks, that are equiv</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2012</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2012. Soft Cardinality: A Parameterized Similarity Function for Text Comparison. In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM), pages 449– 453, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>George Dueñas</author>
<author>Julia Baquero</author>
<author>Alexander Gelbukh</author>
</authors>
<title>UNAL-NLP: Combining Soft Cardinality Features for Semantic Textual Similarity, Relatedness and Entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>732--742</pages>
<location>Duplin, Ireland.</location>
<contexts>
<context position="14049" citStr="Jimenez et al., 2014" startWordPosition="2279" endWordPosition="2282">) achieved 0.7595 and ranked the 20th out of 73 systems. We conducted preliminary experiments on the training dataset of SemEval-2015 for evaluating our sense-based string and tree kernel similarity methods, and the integration between each of them with the SC approach. The results of those experiments led to the final submission of the two kernels separately (Run1 and Run2) and integrating the string kernel method with SC (Run3). Table 2 focuses on the results obtained from our integrated system (Run3) and SC approach in training, but includes also the recent SC approach (SC-ML) proposed in (Jimenez et al., 2014). It is noteworthy from the tables that Run3 improved the SC system results on both the training and testing sets for all the different settings for alpha value in the SC approach. The possible reason based on our observation on the training datasets is that the two systems have opposite strength and weakness points. Figure 1 depicts the similarity scores resulted from Run1, Run3, and SC systems along with the gold standard scores (GS) on some sentence pairs from images dataset. It is shown from the figure that Run1 outperforms SC for semantically equivalent sentence pairs (i.e. scores &gt; 3.5),</context>
</contexts>
<marker>Jimenez, Dueñas, Baquero, Gelbukh, 2014</marker>
<rawString>Sergio Jimenez, George Dueñas, Julia Baquero, and Alexander Gelbukh. 2014. UNAL-NLP: Combining Soft Cardinality Features for Semantic Textual Similarity, Relatedness and Entailment. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 732– 742, Duplin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP Natural Language Processing Toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="7996" citStr="Manning et al., 2014" startWordPosition="1242" endWordPosition="1245"> the word senses that co-occur with wsi in the corpus, and 2) all senses directly connected to wsi in BabelNet; Cj is similar. 3 We tried different values in experiments and the best was 7. WSS(wsi,wsj)  (1) Ci  Cj (4) Ci  Cj 155 3 Systems Description 3.1 Text Preprocessing The given input sentences are first preprocessed to map the raw natural language text into structured or annotated representation. This process includes different tasks: tokenization, lemmatization, Partof-Speech tagging, and word-sense tagging. All tasks except word-sense tagging are carried out using Stanford CoreNLP (Manning et al., 2014). Sense tagging is the task of attaching a sense to a word or a token. It is performed by selecting the most commonly used BabelNet sense that matches the part of speech (POS) of the word. Accordingly, we restricted sense tagging to: nouns, verbs, adjectives, and adverbs. 3.2 English STS Subtask We submitted three systems in this subtask, named Run1, Run2, and Run3. 3.2.1 Sense-based String Kernel (Run1) Given two sentences, s1 and s2, the similarity score between s1 and s2 resulted by this system is the value of a designed string kernel function between the two sentences. This kernel is defin</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<contexts>
<context position="5304" citStr="Miller, 1995" startWordPosition="801" endWordPosition="802">ilarity score between two word senses (wsi, wsj) using the arithmetic mean of two measures: Semantic Distance (simD) and Contextual Similarity (simC). That is: simD (wsi , wsj)  simC (wsi , wsj ) 2 2.1 Semantic Distance This measure computes the similarity between word senses based on the distance between them in a multilingual semantic network, named BabelNet (Navigli and Ponzetto, 2010). BabelNet1 is a rich semantic knowledge resource that covers a wide range of concepts and named entities connected with large numbers of semantic relations. Concepts and relations are gathered from WordNet (Miller, 1995); and Wikipedia2. The semantic knowledge is encoded as a labeled directed graph, where vertices are BabelNet senses (concepts), and edges connect pairs of senses with a label indicating the type of the semantic relation between them. Our semantic distance measure is a function of two similarity scores: simBn and simNBn. 1 http://babelnet.org/ 2 http://en.wikipedia.org/ The first score (simBn) is based on the distance between two word-senses, wsi and wsj; where, the shorter the distance between them, the more semantically related they are. That is: Sim,,(WSi,WSJ) = I— (2) Maxlen where Maxlen3 i</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A Lexical Database for English. Communications of the ACM, 38(11): 39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: Building a Very Large Multilingual Semantic Network.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>216--225</pages>
<location>Uppsala,</location>
<contexts>
<context position="5083" citStr="Navigli and Ponzetto, 2010" startWordPosition="766" endWordPosition="769">arity of word senses between two texts, the higher their semantic equivalence will be. Accordingly, our kernel maps a text to feature space using a similarity measure between word senses. We proposed a WSS measure that computes the similarity score between two word senses (wsi, wsj) using the arithmetic mean of two measures: Semantic Distance (simD) and Contextual Similarity (simC). That is: simD (wsi , wsj)  simC (wsi , wsj ) 2 2.1 Semantic Distance This measure computes the similarity between word senses based on the distance between them in a multilingual semantic network, named BabelNet (Navigli and Ponzetto, 2010). BabelNet1 is a rich semantic knowledge resource that covers a wide range of concepts and named entities connected with large numbers of semantic relations. Concepts and relations are gathered from WordNet (Miller, 1995); and Wikipedia2. The semantic knowledge is encoded as a labeled directed graph, where vertices are BabelNet senses (concepts), and edges connect pairs of senses with a label indicating the type of the semantic relation between them. Our semantic distance measure is a function of two similarity scores: simBn and simNBn. 1 http://babelnet.org/ 2 http://en.wikipedia.org/ The fir</context>
</contexts>
<marker>Navigli, Ponzetto, 2010</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2010. BabelNet: Building a Very Large Multilingual Semantic Network. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 216–225, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Taher Pilehvar</author>
<author>David Jurgens</author>
<author>Roberto Navigli</author>
</authors>
<title>Align, Disambiguate and Walk: A Unified Approach for Measuring.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>1341--1351</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="3850" citStr="Pilehvar et al., 2013" startWordPosition="575" endWordPosition="578">he proposed systems. Section 4 presents the experiments conducted and analyzes the results achieved. Section 5 concludes the paper and suggests some future directions. 154 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 154–158, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2 The proposed Word-Sense Similarity (WSS) Method Several semantic textual similarity (STS) methods have been proposed in literature. Sense-based methods are qualified when different words are used to convey the same meaning in different texts (Pilehvar et al., 2013). Surface-based methods, mostly fail in identifying similarity between texts with maximal semantic overlap but minimal lexical overlap. We present a sense-based STS approach that produces similarity score between texts by means of a kernel function (Shawe-Taylor and Cristianini, 2004). Then, we integrate the sensebased approach with the surface-based soft cardinality approach presented in (Jimenez et al., 2012) to demonstrate that both sense-based and surfacebased similarity methods are complementary to each other. The design of our kernel function relies on the hypothesis that the greater the</context>
</contexts>
<marker>Pilehvar, Jurgens, Navigli, 2013</marker>
<rawString>Mohammad Taher Pilehvar, David Jurgens, and Roberto Navigli. 2013. Align, Disambiguate and Walk: A Unified Approach for Measuring. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 1341– 1351, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4135" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="614" endWordPosition="617">Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2 The proposed Word-Sense Similarity (WSS) Method Several semantic textual similarity (STS) methods have been proposed in literature. Sense-based methods are qualified when different words are used to convey the same meaning in different texts (Pilehvar et al., 2013). Surface-based methods, mostly fail in identifying similarity between texts with maximal semantic overlap but minimal lexical overlap. We present a sense-based STS approach that produces similarity score between texts by means of a kernel function (Shawe-Taylor and Cristianini, 2004). Then, we integrate the sensebased approach with the surface-based soft cardinality approach presented in (Jimenez et al., 2012) to demonstrate that both sense-based and surfacebased similarity methods are complementary to each other. The design of our kernel function relies on the hypothesis that the greater the similarity of word senses between two texts, the higher their semantic equivalence will be. Accordingly, our kernel maps a text to feature space using a similarity measure between word senses. We proposed a WSS measure that computes the similarity score between two word senses (wsi, </context>
<context position="9295" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="1477" endWordPosition="1480">o a vector space F, whose coordinates are indexed by a set I of word senses contained in s1 and s2; i.e.  : s  (ws(s))wsI  F. Thus, given a sentence s, it can be represented by a row vector as:  (s) = (ws1(s), ws2(s) ... wsN(s)), in which each entry records how similar a particular word sense (wsI) is to the sentence s. The mapping is given by: �ws(s) = max { WSS(ws,ws) }, (5) 15i5n where WSS(ws, wsi) is our defined word sense similarity method ( Eq. (1) ), and n is the number of word senses contained in sentence s. The string kernel between two sentences s1 and s2 is calculated as (Shawe-Taylor and Cristianini, 2004): KS(s0s2) _ O(sO,O(s2) _ E Ows(s1)- Ows(s2) (6) ws I s The last step remaining is normalizing the kernel (i.e. range = [0,1]) to avoid any biasness to sentence length. The normalized string kernel KNS(s1,s2) is calculated by (Shawe-Taylor and Cristianini, 2004): Hence, . 3.2.2 Sense-based Tree Kernel (Run2) This system applies tree kernel instead of string kernel. Tree kernels generally map a tree to the feature space of subtrees. There are various types of tree kernel designed in literature, among them is the all-subtree kernel presented in (Shawe-Taylor and Cristianini, 2004). The all-subtr</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs Semantics and Lexical Selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics (ACL&apos;94),</booktitle>
<pages>133--138</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6701" citStr="Wu and Palmer, 1994" startWordPosition="1032" endWordPosition="1035"> i.e wsi  wsj, and wsj  wsi. The shortest path is calculated using Dijkstra&apos;s algorithm. The second score (simNBn) represents the degree of similarity between the neighbors of wsi and the neighbors of wsj, which influences the degree of similarity between the two senses. Hence, simNBn is calculated by taking the arithmetic mean of all neighbor-pairs similarity. That is: (3) where NSi and NSj are the sets of the most semantically related senses directly connected to wsi and wsj respectively in BabelNet; ni = |NSi |, and nj = | NSj |; and simWuP (wsk, wsl) is Wu and Palmer similarity measure (Wu and Palmer, 1994). The values of the two scores presented above determine the way of calculating the semantic distance measure (simD) for word senses’ pair (wsi, wsj). For zero similarity of both scores, simD is simply equals to Wu and Palmer similarity measure; i.e. simD (wsi,wsj) = simWuP (wsi,wsj). Generally, for non-zero similarity scores, simD is calculated using the arithmetic mean of the two scores. 2.2 Contextual Similarity This measure calculates the similarity between the word senses pair (wsi, wsj) based on the overlap between their contexts derived from a corpus. The overlap coefficient used is Jac</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs Semantics and Lexical Selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics (ACL&apos;94), pages 133–138, Stroudsburg, PA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>