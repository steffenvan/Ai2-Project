<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.991112">
Towards Topic Labeling with Phrase Entailment and Aggregation
</title>
<author confidence="0.996954">
Yashar Mehdad Giuseppe Carenini Raymond T. Ng Shafiq Joty
</author>
<affiliation confidence="0.999698">
Department of Computer Science, University of British Columbia
</affiliation>
<address confidence="0.962738">
Vancouver, BC, V6T 1Z4, Canada
</address>
<email confidence="0.987919">
{mehdad, carenini, rng, rjoty}@cs.ubc.ca
</email>
<sectionHeader confidence="0.995512" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999906538461538">
We propose a novel framework for topic la-
beling that assigns the most representative
phrases for a given set of sentences cover-
ing the same topic. We build an entailment
graph over phrases that are extracted from the
sentences, and use the entailment relations to
identify and select the most relevant phrases.
We then aggregate those selected phrases by
means of phrase generalization and merging.
We motivate our approach by applying over
conversational data, and show that our frame-
work improves performance significantly over
baseline algorithms.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997849529411765">
Given text segments about the same topic written in
different ways (i.e., language variability), topic la-
beling deals with the problem of automatically gen-
erating semantically meaningful labels for those text
segments. The potential of integrating topic label-
ing as a prerequisite for higher-level analysis has
been reported in several areas, such as summariza-
tion (Harabagiu and Lacatusu, 2010; Kleinbauer et
al., 2007; Dias et al., 2007), information extraction
(Allan, 2002) and conversation visualization (Liu
et al., 2012). Moreover, the huge amount of tex-
tual data generated everyday specifically in conver-
sations (e.g., emails and blogs) calls for automated
methods to analyze and re-organize them into mean-
ingful coherent clusters.
Table 1 shows an example of two human written
topic labels for a topic cluster collected from a blog1,
</bodyText>
<footnote confidence="0.995481">
1http://slashdot.org
</footnote>
<note confidence="0.7784235">
Text: a: Where do you think the term “Horse laugh” comes
from?
</note>
<tableCaption confidence="0.55713">
b: And that rats also giggled when tickled.
c: My hypothesis- if an animal can play, it can “laugh” or at
least it is familiar with the concept of “laughing”.
Many animals play. There are various sorts of humour though.
Some involve you laughing because your brain suddenly made
a lots of unexpected connections.
Possible extracted phrases: animals play, rats have, laugh,
Horse laugh, rats also giggle, rats
Human-authored topic labels: animals which laugh, animal
laughter
Table 1: Topic labeling example.
</tableCaption>
<bodyText confidence="0.99980380952381">
and possible phrases that can be extracted from the
topic cluster using different approaches. This ex-
ample demonstrates that although most approaches
(Mei et al., 2007; Lau et al., 2011; Branavan et al.,
2007) advocate extracting phrase-level topic labels
from the text segments, topically related text seg-
ments do not always contain one keyword or key
phrase that can capture the meaning of the topic. As
shown in this example, such labels do not exist in the
original text and cannot be extracted using the exist-
ing probabilistic models (e.g., (Mei et al., 2007)).
The same problem can be observed with many other
examples. This suggests the idea of aggregating and
generating topic labels, instead of simply extracting
them, as a challenging scenario for this field of re-
search.
Moreover, to generate a label for a topic we have
to be able to capture the overall meaning of a topic.
However, most current methods disregard semantic
relations, in favor of statistical models of word dis-
tributions and frequencies. This calls for the integra-
</bodyText>
<page confidence="0.981533">
179
</page>
<note confidence="0.4727425">
Proceedings of NAACL-HLT 2013, pages 179–189,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.962326333333333">
tion of semantic models for topic labeling.
Towards the solution of the mentioned problems,
in this paper we focus on two novel contributions:
</bodyText>
<listItem confidence="0.981901">
1. Phrase aggregation. We propose to generate
</listItem>
<bodyText confidence="0.782766461538462">
topic labels using the extracted information by pro-
ducing the most representative phrases for each text
segment. We perform this task in two steps. First,
we generalize some lexically diverse concepts in
the extracted phrases. Second, we aggregate and
generate new phrases that can semantically imply
more than one original extracted phrase. For ex-
ample, the phrase “rats also giggle” and “horse
laugh” should be merged into a new phrase “animals
laugh”. Although our method is still relying on ex-
tracting phrases, we move beyond current extractive
approaches, by generating new phrases through gen-
eralization and aggregation of the extracted ones.
</bodyText>
<listItem confidence="0.819285875">
2. Building a multidirectional entailment graph
over the extracted phrases to identify and select the
relevant information. We set such problem as an
application-oriented variant of the Textual Entail-
ment (TE) recognition task (Dagan and Glickman,
2004), to identify the information that are seman-
tically equivalent, novel, or more informative with
respect to the content of the others. In this way, we
</listItem>
<bodyText confidence="0.959518826086957">
prune the redundant and less informative text por-
tions (e.g., phrases), and produce semantically in-
formed phrases for the generation phase. In the case
of the example in Table 1, we eliminate phrases such
as “rats have”, “rats” and “laugh” while keeping
“animal play”, “Horse laugh” and “rats also gig-
gle”.
The experimental results over conversational data
sets show that, in all cases, our approach outper-
forms other models significantly. Although conver-
sational data are known to be challenging (Carenini
et al., 2011), we choose to test our method on con-
versations because this is a genre in which topic
modeling is critically needed, as conversations lack
the structure and organization of, for instance, edited
monologues. The results indicate that our frame-
work is sufficiently robust to deal with topic labeling
in less structured, informal genres (when compared
with edited monologues). As an additional result of
our experiments, we show that the identification and
selection phase using semantic relations (entailment
graph) is a necessary step to perform the final step
(i.e., the phrase aggregation).
</bodyText>
<sectionHeader confidence="0.916235" genericHeader="method">
2 Topic Labeling Framework
</sectionHeader>
<bodyText confidence="0.999714666666667">
Each topic cluster contains the sentences that can
semantically represent a topic. The task of cluster-
ing the sentences into a set of coherent topic clus-
ters is called topic segmentation (Joty et al., 2011),
which is out of the scope of this paper. Our goal is to
generate an understandable label (i.e., a sequence of
words) that could capture the semantic of the topic,
and distinguish a topic from other topics (based on
definition of a good topic label by (Mei et al., 2007)),
given a set of topic clusters. Among possible choices
of word sequences as topic labels, in order to balance
the granularity, we set phrases as valid topic labels.
</bodyText>
<table confidence="0.813965833333333">
✲ ✲
Extract all Entailment Generalize
❄ Phrase extraction Entailment Graph Phrase
❄ aggregation
❄
Filter/select Identify Merge
</table>
<figureCaption confidence="0.998189">
Figure 1: Topic 1labeling framework.
</figureCaption>
<bodyText confidence="0.999464">
As shown in Figure 1, our framework consists of
three main components that we describe in more de-
tails in the following sections.
</bodyText>
<subsectionHeader confidence="0.997416">
2.1 Phrase extraction
</subsectionHeader>
<bodyText confidence="0.999942761904762">
We tokenize and preprocess each cluster in the col-
lection of topic clusters with lemmas, stems, part-of-
speech tags, sense tags and chunks. We also extract
n-grams up to length 5 which do not start or end with
a stop word. In this phase, we do not include any
frequency count feature in our candidate extraction
pipeline. Once we have built the candidates pool,
the next step is to identify a subset containing the
most significant of those candidates. Since most top
systems in key phrase extraction use supervised ap-
proaches, we follow the same method (Kim et al.,
2010b; Medelyan et al., 2008; Frank et al., 1999).
Initially, we consider a set of features used in the
other systems to determine whether a phrase is likely
to be a key phrase. However, since our dataset is
conversational (more details in Section 3), and the
text segments are not long, we aim for a classifier
with high recall. Thus, we only use TFxIDF (Salton
and McGill, 1986), position of the first occurrence
(Frank et al., 1999) and phrase length as our fea-
tures. We merge the training and test data released
</bodyText>
<page confidence="0.994798">
180
</page>
<bodyText confidence="0.999965916666667">
for SemEval-2010 Task #5 (Kim et al., 2010b),
which consists of 244 scientific articles and 3705
key phrases, to train a Naive Bayes classifier in or-
der to learn a supervised model. We then apply our
model to extract the candidate phrases from the col-
lected candidates pool.
As a further step, to increase the coverage (re-
call) of our extracted phrases and to reduce the num-
ber of very short phrases (frequent keywords), we
choose the chunks containing any of the extracted
keywords. We add those chunks to our extracted
phrases and eliminate the associated keywords.
</bodyText>
<subsectionHeader confidence="0.999632">
2.2 Entailment graph
</subsectionHeader>
<bodyText confidence="0.999219366666666">
So far, we have extracted a pool of key phrases from
each topic cluster. Many such phrases include re-
dundant information which are semantically equiv-
alent but vary in lexical choices. By identifying the
semantic relations between the phrases we can dis-
cover the information in one phrase that is seman-
tically equivalent, novel, or more/less informative
with respect to the content of the other phrase.
We set this problem as a variant of the Textual
Entailment (TE) recognition task (Mehdad et al.,
2010b; Adler et al., 2012; Berant et al., 2011). We
build an entailment graph for each topic cluster,
where nodes are the extracted phrases and edges are
the entailment relations between nodes. Given two
phrases (ph1 and ph2), we aim at identifying and
handling the following cases:
i) ph1 and ph2 express the same meaning (bidirec-
tional entailment). In such cases one of the phrases
should be eliminated;
ii) ph1 is more informative than ph2 (unidirectional
entailment). In such cases, the entailing phrase
should replace or complement the entailed one;
iii) ph1 contains facts that are not present in ph2,
and vice-versa (the “unknown” cases in TE par-
lance). In such cases, both phrases should remain.
Figure 2 shows how entailment relations can help
in selecting the phrases by removing the redun-
dant and less informative ones. For example, the
phrase “animals laugh” entails “rats giggle”, “Horse
laugh” and “Mice chuckle”,2 but not “Animals play”.
</bodyText>
<footnote confidence="0.98484">
2Assuming that “animals laugh” is interpreted as “all ani-
mals laugh”.
</footnote>
<figureCaption confidence="0.989462">
Figure 2: Building an entailment graph over phrases. Ar-
rows and “x” represent the entailment direction and un-
known cases respectively.
</figureCaption>
<bodyText confidence="0.9993092">
So we can keep “animals laugh” and “Animals play”
and eliminate others. In this way, TE-based phrase
identification method can be designed to distinguish
meaning-preserving variations from true divergence,
regardless of lexical choices and structures.
Similar to previous approaches in TE (e.g., (Be-
rant et al., 2011; Mehdad et al., 2010b; Mehdad et
al., 2010a)), we use supervised method. To train and
build the entailment graph, we perform the follow-
ing three steps.
</bodyText>
<subsectionHeader confidence="0.922084">
2.2.1 Training set collection
</subsectionHeader>
<bodyText confidence="0.9999859375">
In the last few years, TE corpora have been cre-
ated and distributed in the framework of several
evaluation campaigns, including the Recognizing
Textual Entailment (RTE) Challenge3 and Cross-
lingual textual entailment for content synchroniza-
tion4 (Negri et al., 2012). However, such datasets
cannot directly support our application. Specifi-
cally, our entailment graph is built over the extracted
phrases (with max. length of 5 tokens per phrase),
while the RTE datasets are composed of longer sen-
tences and paragraphs (Bentivogli et al., 2009; Negri
et al., 2011).
In order to collect a dataset which is more similar
to the goal of our entailment framework, we decide
to select a subset of the sixth and seventh RTE chal-
lenge main task (i.e., RTE within a Corpus). Our
</bodyText>
<footnote confidence="0.999472">
3http://pascallin.ecs.soton.ac.uk/Challenges/RTE/
4http://www.cs.york.ac.uk/semeval-2013/task8/
</footnote>
<figure confidence="0.997051928571428">
animals
laugh
x
rats
giggle
Animals
play
x
laugh
Horse
laugh
rats
Mice
chuckle
</figure>
<page confidence="0.986299">
181
</page>
<bodyText confidence="0.99997">
dataset choice is based on the following reasons: i)
the length of sentence pairs in RTE6 and RTE7 is
shorter than the others, and ii) RTE6 and RTE7 main
task datasets are originally created for summariza-
tion purpose which is closer to our work. We sort
the RTE6 and RTE7 dataset pairs based on the sen-
tence length and choose the first 2000 samples with
a equal number of positive and negative examples.
The average length of words in our training data is
6.7 words. There are certainly some differences be-
tween our training set and our phrases. However, the
collected training samples was the closest available
dataset to our purpose.
</bodyText>
<subsectionHeader confidence="0.590313">
2.2.2 Feature representation and training
</subsectionHeader>
<bodyText confidence="0.999991381818182">
Working at the phrase level imposes another con-
straint. Phrases are short and in terms of syntactic
structure, they are not as rich as sentences. This lim-
its our features to the lexical level. Lexical mod-
els, on the other hand, are less computationally ex-
pensive and easier to implement and often deliver a
strong performance for RTE (Sammons et al., 2011).
Our entailment decision criterion is based on
similarity scores calculated with a phrase-to-phrase
matching process. Each example pair of phrases
(ph1 and ph2) is represented by a feature vector,
where each feature is a specific similarity score esti-
mating whether ph1 entails ph2.
We compute 18 similarity scores for each pair of
phrases. In order to adapt the similarity scores to the
entailment score, we normalize the similarity scores
by the length of ph2 (in terms of lexical items), when
checking the entailment direction from ph1 to ph2.
In this way, we can check the portion of informa-
tion/facts in ph2 which is covered by ph1.
The first 5 scores are computed based on the exact
lexical overlap between the phrases: word overlap,
edit distance, ngram-overlap, longest common sub-
sequence and Lesk (Lesk, 1986). The other scores
were computed using lexical resources: Word-
Net (Fellbaum, 1998), VerbOcean (Chklovski and
Pantel, 2004), paraphrases (Denkowski and Lavie,
2010) and phrase matching (Mehdad et al., 2011).
We used WordNet to compute the word similarity
as the least common subsumer between two words
considering the synonymy-antonymy, hypernymy-
hyponymy, and meronymy relations. Then, we cal-
culated the sentence similarity as the sum of the sim-
ilarity scores of the word pairs in Text and Hypothe-
sis, normalized by the number of words in Hypothe-
sis. We also use phrase matching features described
in (Mehdad et al., 2011) which consists of phrasal
matching at the level on ngrams (1 to 5 grams). The
rationale behind using different entailment features
is that combining various scores will yield a better
model (Berant et al., 2011).
To combine the entailment scores and optimize
their relative weights, we train a Support Vector Ma-
chine binary classifier, SVMlight (Joachims, 1999),
over an equal number of positive and negative exam-
ples. This results in an entailment model with 95%
accuracy over 2-fold and 5-fold cross-validation,
which further proves the effectiveness of our fea-
ture set for this lexical entailment model. The reason
that we gained a very high accuracy is because our
selected sentences are a subset of RTE6 and RTE7
with a shorter length (less number of words) which
makes the entailment recognition task much easier
than recognizing entailment between paragraphs or
complex long sentences.
</bodyText>
<subsubsectionHeader confidence="0.547777">
2.2.3 Graph edge labeling
</subsubsectionHeader>
<bodyText confidence="0.999984733333333">
We set the edge labeling problem as a two-way
classification task. Two-way classification casts
multidirectional entailment as a unidirectional prob-
lem, where each pair is analyzed checking for en-
tailment in both directions (Mehdad et al., 2012). In
this condition, each original test example is correctly
classified if both pairs originated from it are cor-
rectly judged (“YES-YES” for bidirectional,“YES-
NO” and “NO-YES” for unidirectional entailment
and “NO-NO” for unknown cases). Two-way clas-
sification represents an intuitive solution to capture
multidimensional entailment relations. Moreover,
since our training examples are labeled with binary
judgments, we are not able to train a three-way clas-
sifier.
</bodyText>
<subsectionHeader confidence="0.52904">
2.2.4 Identification and selection
</subsectionHeader>
<bodyText confidence="0.999987666666667">
Assigning all entailment relations between the ex-
tracted phrase pairs, we are aiming at identifying
relevant phrases and eliminating the redundant (in
terms of meaning) and less informative ones. In or-
der to perform this task we follow a set of rules based
on the graph edge labels. Note that since entailment
</bodyText>
<page confidence="0.992041">
182
</page>
<table confidence="0.990056416666667">
# Merging patterns
1 merge ( cw11(CP OS=[N|V |J]) ..w1n , cw21(CP OS=[N|V |J]) ..w2n ) = w11..w1n and w22..w2n
E.g. merge ( challenging situation , challenging problem ) = challenging situation and problem
2 merge ( w11..cw1n(CP OS=[N|V |J]) , w21..cw2n(CP OS=[N|V |J]) ) = w11..w1n_1 and w21..w2n
E.g. merge ( wet Mars , warm Mars ) = wet and warm Mars
3 merge ( w11..cw1n(CP OS=[N|V |J]) , cw21(CP OS=[N|V |J]) ..w2n ) = w11..w1n w22..w2n
E.g. merge ( interesting story , story continues ) = interesting story continues
4 merge ( cw11(CP OS=[N|V |J]) ..w1n , w21..cw2n(CP OS=[N|V |J]) ) = w21..w2n w12..w1n
E.g. merge( LHC shutting down , details about LHC ) = details about LHC shutting down
5 merge ( w11Cpos , cw12(CP OS=[N|V |J]) , w13Cpos , w21Cpos , cw22(CP OS=[N|V |J]) , w23Cpos ) = w11 and w21 w22 w23
E.g. and w13
merge ( technology grow fast , media grow exponentially) =technology and media grow exponentially and fast
</table>
<tableCaption confidence="0.998838">
Table 2: Phrase merging patterns.
</tableCaption>
<bodyText confidence="0.998647260869565">
is a transitive relation, our entailment graph is transi-
tive i.e., if entail(ph1,ph2) and entail(ph2,ph3) then
entail(ph1,ph3) (Berant et al., 2011).
Rule 1) If there is a chain of entailing nodes, we
keep the one which is in the root of the chain and
eliminate others (e.g. “animals laugh” in Figure 2);
Rule 2) Among the nodes that are connected
with bidirectional entailment (semantically equiva-
lent nodes) we keep only the one with more outgoing
bidirectional and unidirectional entailment relations,
respectively;
Rule 3) Among the nodes that are connected with
unknown entailment (novel information with respect
to others) we keep the ones with no incoming entail-
ment relation (e.g., “Animals play” in Figure 2).
Although deleting might be harsh, in our current
framework, we only rely on the performance of an
entailment model which gives us a yes/no entailment
decision. In future, we are planning to improve our
entailment graph by weighting the edges. In this
way, we can take advantage of the weights to make
a more conservative decision in pruning the entail-
ment chains.
</bodyText>
<subsectionHeader confidence="0.999178">
2.3 Phrase aggregation
</subsectionHeader>
<bodyText confidence="0.99999">
Once we have identified and selected the informa-
tive phrases, the generation of topic labels can be
done in two steps. First, we generalize the phrases
containing the concepts that are lexically connected.
Second, we merge the phrases with a set of hand
written linguistically motivated patterns.
</bodyText>
<subsectionHeader confidence="0.59494">
2.3.1 Phrase generalization
</subsectionHeader>
<bodyText confidence="0.999979666666667">
In this step, we generalize phrases that contain
concepts which are lexically connected. For this
purpose, we search in phrases for different words
with the same part-of-speech and sense tag. Then,
we find the link between those words in WordNet. If
they are connected and the shortest path connecting
them is less than 3 (estimated over the development
set), we replace both by their common parent in the
WordNet. In the case that they belong to the same
synset, we can replace one by another. Note that we
limit our search to nouns and verbs. For example,
“rat” and “horse” can be replaced by “animal”, or
“giggle” and “chuckle” can be replaced by “laugh”.
The motivation behind the generalization step is to
enrich the common terms between the phrases in fa-
vor of increasing the chance that they could merge
to a single phrase. This also helps to move beyond
the limitation of original lexical choices.
</bodyText>
<subsectionHeader confidence="0.625453">
2.3.2 Phrase merging
</subsectionHeader>
<bodyText confidence="0.999939117647059">
The goal is to merge the phrases that are con-
nected, and to generate a human readable phrase that
contains more information than a single extracted
phrase. Several approaches have been proposed to
aggregate and merge sentences in Natural Language
Generation (NLG) (e.g. (Barzilay and Lapata, 2006;
Cheng and Mellish, 2000)), however most of them
use syntactic structure of the sentences. To merge
phrases at the lexical level, we set few common lin-
guistically motivated aggregation patterns such as:
simple conjunction, and conjunction via shared par-
ticipants (Reiter and Dale, 2000).
Table 2 demonstrates the merging patterns, where
wig is the jth word (or segment) in phrase i, cw
is the common word (or segment) in both phrases
and CPOS is the common part-of-speech tag of
the corresponding word. To illustrate, pattern 1
</bodyText>
<page confidence="0.996352">
183
</page>
<bodyText confidence="0.999982217391305">
looks for the first segment of each phrase (wil).
If they are same (cwil) and share the same POS
tag (CFOS), then we aggregate the first phrase
(w1j..w1n) and the second phrase removing the first
element (w22..w2n) by using the connective “and”.
For instance, the aggregation of “animals laugh” and
“animals play” results in “animals laugh and play”.
The rest of the patterns follow the same logic and for
the sake of brevity we avoid illustrating each pattern.
These patterns are among the most common domain
and application independent methods by which two
phrases/sentences can be aggregated, as described in
the NLG literature (Reiter and Dale, 2000).
In our aggregation pipeline, we group the phrases
based on their lexical overlap (number of common
words). The merging process is conducted over each
group in descending order (larger number of words
in common), in order to increase the chance of merg-
ing rules application. Then, we perform the merg-
ing over the resulting generated phrases from each
group. If our phrases cannot be merged (i.e., do not
match merging patterns), we select them as labels
for the topic cluster.
</bodyText>
<sectionHeader confidence="0.99419" genericHeader="method">
3 Datasets and Evaluation Metrics
</sectionHeader>
<subsectionHeader confidence="0.968134">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999985614035088">
To verify the effectiveness of our approach, we ex-
periment with two different conversational datasets.
Our interest in dealing with conversational texts de-
rives from two reasons. First, the huge amount of
textual data generated everyday in these conversa-
tions validates the need of text analysis frameworks
to process such conversational texts effectively. Sec-
ond, conversational texts pose challenges to the tra-
ditional techniques, including redundancies, disflu-
encies, higher language variabilities and ill-formed
sentence structure (Liu et al., 2011).
Our conversational datasets are from two differ-
ent asynchronous media: email and blog. For email,
we use the dataset presented in (Joty et al., 2010),
where three individuals annotated the publicly avail-
able BC3 email corpus (Ulrich et al., 2008) with top-
ics. The corpus contains 40 email threads (or conver-
sations) at an average of 5 emails per thread. On av-
erage it has 26.3 sentences and 2.5 topics per thread.
A topic has an average length of 12.6 sentences. In
total, the three annotators found 269 topics in a cor-
pus of 1,024 sentences.
There are no publicly available blog corpora an-
notated with topics. For this study, we build our
own blog corpus containing 20 blog conversations of
various lengths from Slashdot, each annotated with
topics by three human annotators.5 The number of
comments per conversation varies from 30 to 101
with an average of 60.3 and the number of sentences
per conversation varies from 105 to 430 with an av-
erage of 220.6. The annotators first read a conversa-
tion and list the topics discussed in the conversation
by a short description (e.g., Game contents or size,
Bugs or faults) which provides a high-level overview
of the topic. Then, they assign the most appropriate
topic to each sentence in the conversation. The short
high-level descriptions of the topics serve as refer-
ence (or gold) topic labels in our experiments. The
target number of topics was not given in advance and
the annotators were instructed to find as many topics
as needed to convey the overall content structure of
the conversation. The annotators found 5 to 23 top-
ics per conversation with an average of 10.77. The
number of sentences per topic varies from 11.7 to
61.2 with an average of 27.16. In total, the three
annotators found 512 topics in our blog corpus con-
taining 4,411 sentences overall.
Note that our annotators performed topic segmen-
tation and labeling independently. In the email cor-
pus, the three annotators found 100, 77 and 92 top-
ics respectively (269 in total), and in the blog corpus,
they found 251, 119 and 192 topics respectively (562
in total). For the evaluation, there is a single gold
standard per topic written by each annotator. Table
1 shows a case in which two annotators selected the
same topical cluster and so we have two labels for
the same cluster.
</bodyText>
<subsectionHeader confidence="0.998704">
3.2 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.999909571428572">
Traditionally, key phrase extraction is evaluated us-
ing precision, recall and f-measure based on exact
matches on all the extracted key phrases with gold
standards for a given text. However, as claimed
by (Kim et al., 2010a), this approach is not flexible
enough as it ignores the near-misses. Moreover, in
the case of topic labeling, most of the human written
</bodyText>
<footnote confidence="0.9916725">
5The new blog corpus annotated with topics will be made
publicly available for research purposes.
</footnote>
<page confidence="0.998418">
184
</page>
<bodyText confidence="0.999720583333333">
topic labels cannot be found in the text. Recently,
(Kim et al., 2010a) evaluated the utility of differ-
ent n-gram-based metrics for key phrase extraction
and showed that the metric R-precision correlates
most with human judgments. R-precision normal-
izes the approximate matching score by the maxi-
mum number of words in the reference and candi-
date phrases. Since this penalize our aggregation
phase, where the phrases tend to be longer than orig-
inal extracted phrase, we decide to use R-f1 as our
evaluation metric which considers length of both ref-
erence and candidate phrases.
</bodyText>
<equation confidence="0.994939428571429">
overlap(candi, ref)
#words(candi)
overlap(candi, ref)
#words(ref)
2 * R−precision * R−recall
R−f1 =
(R−precision + R−recall)
</equation>
<bodyText confidence="0.9999903">
The metric described above only considers word
overlap and ignores other semantic relations (e.g.,
synonymy, hypernymy) between words. However,
annotators write labels of their own and may use
words that are not directly from the conversation but
are semantically related. Therefore, we propose to
also use another variant of R-f1 that incorporates se-
mantic relation between words. To calculate the Se-
mantic R-f1, we count the number of overlaps not
only when they have the same form, but also when
they are connected in WordNet with a synonymy,
hypernymy, hyponymy and entailment relation.
Its worth noting that the generalizations phase and
the evaluation method are completely independent.
In the generalization step, we try to generalize the
phrases which are automatically extracted from the
text segments. While, in the evaluation, we compare
the human written gold standards with the system
output. Therefore, using WordNet in the generaliza-
tion step does not bias the results in the evaluation.
</bodyText>
<sectionHeader confidence="0.998249" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.982582">
4.1 Experimental settings
</subsectionHeader>
<bodyText confidence="0.997964888888889">
We conduct our experiments over the blog and email
datasets described in Section 3.1, after eliminating
the development set from the test datasets. In our ex-
periments, the development set was used for the pat-
tern extraction and the shortest path threshold con-
necting the words in Wordnet in the generalization
phase. Our test dataset consists of 461 topics (i.e.,
clusters and their associated topic labels) from 20
blog conversations and 242 topics from 40 email
conversations.
For preprocessing our dataset we use OpenNLP6
for tokenization, part-of-speech tagging and chunck-
ing. For sense disambiguation, we use the extended
gloss overlap measure with the window size of 5,
developed by (Pedersen et al., 2005). We also apply
Snowball algorithm (Porter, 2001) for stemming.
We compare our approach with two strong base-
lines. The first baseline Freq-BL ranks the words
according to their frequencies and select the top 5
candidates applying Maximum Marginal Relevance
algorithm (Carbonell and Goldstein, 1998) using
the same pre- and post-processing as the work by
(Mihalcea and Tarau, 2004). The second baseline
Lead-BL, ranks the words based on their relevance
to the leading sentences.7 The ranking criteria is
log(tfw,Lt + 1) x log(tfw,t + 1), where tfw,Lt and
tfw,t are the number of times word w appears in a
set of leading sentences Lt and topic cluster t, re-
spectively (Allan, 2002). The log expressions, as the
ranking criterion, assign more weights to the words
in the topic segment, that also appear in the leading
sentences. This is because topics tend to be intro-
duced in the first few sentences of a topical cluster.
We also measure the performance of our framework
at each step in order to compare the effectiveness of
each phase independently or in combination.
</bodyText>
<subsectionHeader confidence="0.573752">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.9998706">
We evaluate the performance of different models us-
ing the metrics R-f1 and Semantic R-f1 (Sem-R-f1),
described in Section 3.2. Table 4 shows the results
in percentage for different models. The results show
that our framework outperforms the baselines signif-
</bodyText>
<footnote confidence="0.9922015">
6http://opennlp.sourceforge.net/
7The key intuitions for this baseline is the leading sentences
</footnote>
<bodyText confidence="0.951771">
of a topic cluster carry the most informative clues for the topic
labels. Based on our development set, when we consider the
first three sentences, the coverage of content words that appear
in human labeled topics are 39% and 49% for blog and email,
respectively.
</bodyText>
<equation confidence="0.99817825">
�k
1
R−precision = k
i=1
1
R−recall = k
k
i=1
</equation>
<page confidence="0.998374">
185
</page>
<note confidence="0.527273833333333">
Blog Email
Human-authored system generated Human-authored system generated
Shutting down the LHC story about the LHC shutting down (#3) How it affects coding it screws my coding
typical shutdown and upgrade times typical and scheduled shutdown (#2) Opinions and preferences of tools opinion about what tools
MARS was warm and wet 3B years ago Mars was warm and wet early history (#3) white on black for disabled users white text on black background (#3)
Moon Treaty and outer space treaty Moon and Outer Space Treaty (#2) Contact with Steven email to Steven Pemberton (#3)
</note>
<tableCaption confidence="0.806162">
Table 3: Successful examples of human-authored and system generated labels for blog and email datasets. The number
near some examples refers to the aggregation patterns in Table 2.
</tableCaption>
<table confidence="0.999952777777778">
Models R-f1 Sem-R-f1
blog email blog email
Lead-BL 13.5 14.0 34.5 30.1
Freq-BL 15.3 13.1 34.7 29.1
Extraction-BL 13.9 16.0 31.6 33.2
Entailment 12.2 15.6 30.8 33.3
Extraction+Aggregation 15.1 18.5 35.5 37.6
Extraction+Entailment+ 17.9 20.4 38.7 41.6
Aggregation
</table>
<tableCaption confidence="0.99743">
Table 4: Results for candidate topic labels on blog and
email corpora.
</tableCaption>
<bodyText confidence="0.997220814814815">
icantly8 in both datasets.
On the blog corpus, our key phrase extraction
method (Extraction-BL) fails to beat the other base-
lines (Lead-BL and Freq-BL) in majority of cases
(except R-f1 for Lead-BL). However, in the email
dataset, it improves the performance over both base-
lines in both evaluation metrics. This might be due
to the shorter topic clusters (in terms of number of
sentences) in email corpus which causes a smaller
number of phrases to be extracted.
We also observe the effectiveness of the aggre-
gation phase. In all cases, there is a significant
improvement (p &lt; 0.05) after applying the ag-
gregation phase over the extracted phrases (Extrac-
tion+Aggregation).
Note that there is no improvement over the ex-
traction phase after the entailment (Entailment row).
This is mainly due to the fact that the entailment
phase filters the equivalent phrases. This affects the
results negatively when such filtered phrases share
many common words with our human-authored
phrases. However, the results improve more sig-
nificantly (p &lt; 0.01) when the aggregation is con-
ducted after the entailment. This demonstrates that,
the combination of these two steps are beneficial for
topic labeling over conversational datasets.
In addition, the differences between the results us-
</bodyText>
<footnote confidence="0.913845">
8The statistical significance tests was calculated by approx-
imate randomization described in (Yeh, 2000).
</footnote>
<bodyText confidence="0.684050444444444">
ing R-f1 and Sem-R-f1 metrics suggests the need for
more flexible automatic evaluation methods for this
task. Moreover, although the same trend of improve-
ment is observed in blog and email corpora, the dif-
ferences between their performance suggest the in-
vestigation of specialized methods for various con-
versational modalities.
0 100 200 300 400 500 600
Blog
</bodyText>
<figureCaption confidence="0.9805875">
Figure 3: Sem-R-f1 results distribution after each phase
of our pipeline for blog corpus. The x-axis represents the
</figureCaption>
<figure confidence="0.521186">
1
</figure>
<figureCaption confidence="0.418797">
examples sorted based on their Sem-R-f1 score.
</figureCaption>
<bodyText confidence="0.999920083333333">
To further analyze the performance, in Figure 3,
we show the Sem-R-f1 results distribution for our
blog dataset.9 We can observe that the aggrega-
tion after the entailment phase (bold curve) clearly
increase the number of correct labels, while such
improvement can be only achieved when the en-
tailment relations is used to identify the relevant
phrases. This further highlights the need of seman-
tics in this task. Comparing both datasets, this ef-
fect is more dominant in blogs. We believe that this
is due to the length of topic clusters. Presumably,
building an entailment graph over a greater pool of
</bodyText>
<footnote confidence="0.968916">
9For brevity’s sake we do not show the email dataset graph.
</footnote>
<figure confidence="0.995004272727273">
Sem-R-f1
0.8
0.6
0.4
0.2
0
1
Ext
Ext+Ent
Ext+Agg
Ext+Ent+Agg
</figure>
<page confidence="0.996828">
186
</page>
<bodyText confidence="0.9996575">
tem in dealing with topic labeling, specially on con-
versational data.
original phrases is more effective to filter the redun-
dant information and identify the relevant phrases.
</bodyText>
<sectionHeader confidence="0.996477" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.946716740740741">
After analyzing the results and through manual veri-
fication of some cases, we observe that our approach
led to some interestingly successful examples. Table
3 shows few generated labels and the human written
topics for such cases.
In general, given that the results are expressed in
percentage, it appears that the performance is still
far from satisfactory level. This leaves an interesting
challenge for the research community to tackle.
However, this is not always due to the weakness
of our proposed model. We have identified three
different system independent sources of error:10
Type 1: Abstractive human-authored labels: the
nature of our method is based on extraction (with
the exception of our simple generalization phase)
and in many cases the human-written labels cannot
be extracted from the text and require more complex
generalizations. In fact, only 9.81% of the labels
in blog and 12.74% of the labels in email appear
verbatim in their respective conversations. For
example:
Human-authored label: meeting schedule and location
Generated phrases: meeting, Boston area, mid October
Type 2: Evaluation methods: in this work, we
proposed a semantic method to evaluate our system.
However, the current evaluation methods fail to
capture the meaning. For example:
</bodyText>
<table confidence="0.470687714285714">
Human-authored label: Food choices
Generated phrase: I would ask what people want to eat
Type 3: Subjective topic labels: often is not easy
for human to agree on one label for a topic cluster.11
For example:
Human-authored label 1: Member introduction
Human-authored label 2: Bio of Len
</table>
<tableCaption confidence="0.26251">
Generated phrases: own intro, Len Kasday, chair
</tableCaption>
<bodyText confidence="0.998666">
In light of this analysis, we conclude that a more
comprehensive evaluation method (e.g., human eval-
uation) could better reveal the potential of our sys-
</bodyText>
<footnote confidence="0.6439398">
10There are many examples of such cases, however for
brevity we just mention one example for each type.
11The mean R-precision agreements computed based on one-
to-one mappings of the topic clusters are 20.22 and 36.84 on
blog and email data sets, respectively.
</footnote>
<sectionHeader confidence="0.995796" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999990185185185">
In this paper, we study the problem of automatic
topic labeling, and propose a novel framework to la-
bel topic clusters with meaningful readable phrases.
Within such framework, this paper makes two main
contributions. First, in contrast with most current
methods based on fully extractive models, we pro-
pose to aggregate topic labels by means of gener-
alizing and merging techniques. Second, beyond
current approaches which disregard semantic infor-
mation, we integrate semantics by means of build-
ing textual entailment graphs over the topic clusters.
To achieve our objectives, we successfully applied
our framework over two challenging conversational
datasets. Coherent results on both datasets demon-
strate the potential of our approach in dealing with
topic labeling task.
Future work will address both the improvement of
our aggregation phase and ranking the output candi-
date phrases for each topic cluster. On one hand,
we plan to accommodate more sophisticated NLG
techniques for the aggregation and generation phase.
Incorporating a better source of prior knowledge in
the generalization phase (e.g., YAGO or DBpedia) is
also an interesting research direction towards a bet-
ter phrase aggregation step. On the other hand, we
plan to apply a ranking strategy to select the top can-
didate phrases generated by our framework.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999928857142857">
We would like to thank the anonymous reviewers
and Frank Tompa for their valuable comments and
suggestions to improve the paper, and the NSERC
Business Intelligence Network for financial support.
Yashar Mehdad also would like to acknowledge the
early discussions on the related topics with Matteo
Negri.
</bodyText>
<sectionHeader confidence="0.998161" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.949731">
Meni Adler, Jonathan Berant, and Ido Dagan. 2012.
Entailment-based text exploration with application to
the health-care domain. In Proceedings of the ACL
2012 System Demonstrations, ACL ’12, pages 79–84,
</reference>
<page confidence="0.991018">
187
</page>
<reference confidence="0.994315698113208">
Stroudsburg, PA, USA. Association for Computational
Linguistics.
James Allan. 2002. Topic detection and tracking: event-
based information organization.
Regina Barzilay and Mirella Lapata. 2006. Aggregation
via set partitioning for natural language generation. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, HLT-NAACL ’06, pages 359–366, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
pascal recognizing textual entailment challenge. In In
Proc Text Analysis Conference (TAC09.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
SRK Branavan, Pawan Deshpande, and Regina Barzilay.
2007. Generating a table-of-contents. In ACL, vol-
ume 45, page 544.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ’98, pages 335–336, New York, NY, USA.
ACM.
Giuseppe Carenini, Gabriel Murray, and Raymond Ng.
2011. Methods for mining and summarizing text con-
versations.
Hua Cheng and Chris Mellish. 2000. Capturing the in-
teraction between aggregation and text planning in two
generation systems. In In Proceedings of the 1st In-
ternational Natural Language Generation Conference,
186193, Mitzpe.
Timothy Chklovski and Patrick Pantel. 2004. Verbocean:
Mining the web for fine-grained semantic verb rela-
tions. In Dekang Lin and Dekai Wu, editors, Proceed-
ings of EMNLP 2004, pages 33–40, Barcelona, Spain,
July. Association for Computational Linguistics.
I. Dagan and O. Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: improved evaluation
support for five target languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ’10, pages 339–342,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ga¨el Dias, Elsa Alves, and Jos´e Gabriel Pereira Lopes.
2007. Topic segmentation algorithms for text summa-
rization and passage retrieval: an exhaustive evalua-
tion. In Proceedings of the 22nd national conference
on Artificial intelligence - Volume 2, AAAI’07, pages
1334–1339. AAAI Press.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceedings
of the Sixteenth International Joint Conference on Ar-
tificial Intelligence, IJCAI ’99, pages 668–673, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Sanda Harabagiu and Finley Lacatusu. 2010. Us-
ing topic themes for multi-document summarization.
ACM Trans. Inf. Syst., 28(3):13:1–13:47, July.
T. Joachims. 1999. Making large-scale svm learning
practical. LS8-Report 24, Universit¨at Dortmund, LS
VIII-Report.
Shafiq Joty, Giuseppe Carenini, Gabriel Murray, and
Raymond T. Ng. 2010. Exploiting conversation struc-
ture in unsupervised topic segmentation for emails.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’10, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Shafiq Joty, Gabriel Murray, and Raymond T. Ng. 2011.
Supervised topic segmentation of email conversations.
In In ICWSM11. AAAI.
Su Nam Kim, Timothy Baldwin, and Min-Yen Kan.
2010a. Evaluating n-gram based evaluation metrics
for automatic keyphrase extraction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, COLING ’10, pages 572–580, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-
thy Baldwin. 2010b. Semeval-2010 task 5: Automatic
keyphrase extraction from scientific articles. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, SemEval ’10, pages 21–26, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Thomas Kleinbauer, Stephanie Becker, and Tilman
Becker. 2007. Combining multiple information lay-
ers for the automatic generation of indicative meeting
abstracts. In Proceedings of the Eleventh European
Workshop on Natural Language Generation, ENLG
’07, pages 151–154, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jey Han Lau, Karl Grieser, David Newman, and Timothy
Baldwin. 2011. Automatic labelling of topic models.
In ACL, pages 1536–1545.
</reference>
<page confidence="0.982335">
188
</page>
<reference confidence="0.99954109375">
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems docu-
mentation, SIGDOC ’86, pages 24–26, New York, NY,
USA. ACM.
F. Liu, Y. Liu, C. Busso, S. Harabagiu, and V. Ng. 2011.
Identifying the Gist of Conversational Text: Automatic
Keyword Extraction and Summarization. Ph.D. thesis,
THE UNIVERSITY OF TEXAS AT DALLAS.
Shixia Liu, Michelle X. Zhou, Shimei Pan, Yangqiu
Song, Weihong Qian, Weijia Cai, and Xiaoxiao Lian.
2012. Tiara: Interactive, topic-based visual text sum-
marization and analysis. ACM Trans. Intell. Syst.
Technol., 3(2):25:1–25:28, February.
O. Medelyan, I.H. Witten, and D. Milne. 2008. Topic
indexing with wikipedia. In Proceedings of the AAAI
WikiAI workshop.
Y. Mehdad, A. Moschitti, and F.M. Zanzotto. 2010a.
Syntactic semantic structures for textual entailment
recognition. Association for Computational Linguis-
tics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010b. Towards cross-lingual textual entailment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ’10,
pages 321–324, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using bilingual parallel corpora for cross-
lingual textual entailment. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ’11, pages 1336–1345, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting semantic equivalence and informa-
tion disparity in cross-lingual documents. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers - Volume 2,
ACL ’12, pages 120–124, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic mod-
els. In Proceedings of the 13th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, KDD ’07, pages 490–499, New York, NY,
USA. ACM.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of EMNLP-04and
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, July.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: crowdsourcing the creation of cross-
lingual textual entailment corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ’11, pages 670–679,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: cross-lingual textual entailment
for content synchronization. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics - Volume 1: Proceedings of the main con-
ference and the shared task, and Volume 2: Proceed-
ings of the Sixth International Workshop on Seman-
tic Evaluation, SemEval ’12, pages 399–407, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
T. Pedersen, S. Banerjee, and S. Patwardhan. 2005.
Maximizing Semantic Relatedness to Perform Word
Sense Disambiguation. Research Report UMSI
2005/25, University of Minnesota Supercomputing In-
stitute, March.
Martin F. Porter. 2001. Snowball: A language for stem-
ming algorithms.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems.
Gerard Salton and Michael J. McGill. 1986. Introduction
to modern information retrieval.
Mark Sammons, V.G.Vinod Vydiswaran, and Dan Roth.
2011. Recognizing Textual Entailment. In Daniel M.
Bikel and Imed Zitouni, editors, Multilingual Natu-
ral Language Applications: From Theory to Practice.
Prentice Hall, Jun.
Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.
2008. A publicly available annotated corpus for su-
pervised email summarization.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics -
Volume 2, COLING ’00, pages 947–953, Stroudsburg,
PA, USA. Association for Computational Linguistics.
</reference>
<page confidence="0.998918">
189
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978987">
<title confidence="0.999939">Towards Topic Labeling with Phrase Entailment and Aggregation</title>
<author confidence="0.996416">Yashar Mehdad Giuseppe Carenini Raymond T Ng Shafiq Joty</author>
<affiliation confidence="0.999867">Department of Computer Science, University of British</affiliation>
<address confidence="0.999274">Vancouver, BC, V6T 1Z4, Canada</address>
<email confidence="0.992992">carenini,rng,</email>
<abstract confidence="0.999301785714286">We propose a novel framework for topic labeling that assigns the most representative phrases for a given set of sentences covering the same topic. We build an entailment graph over phrases that are extracted from the sentences, and use the entailment relations to identify and select the most relevant phrases. We then aggregate those selected phrases by means of phrase generalization and merging. We motivate our approach by applying over conversational data, and show that our framework improves performance significantly over baseline algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Meni Adler</author>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
</authors>
<title>Entailment-based text exploration with application to the health-care domain.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations, ACL ’12,</booktitle>
<pages>79--84</pages>
<contexts>
<context position="8980" citStr="Adler et al., 2012" startWordPosition="1445" endWordPosition="1448">e chunks to our extracted phrases and eliminate the associated keywords. 2.2 Entailment graph So far, we have extracted a pool of key phrases from each topic cluster. Many such phrases include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the phrases we can discover the information in one phrase that is semantically equivalent, novel, or more/less informative with respect to the content of the other phrase. We set this problem as a variant of the Textual Entailment (TE) recognition task (Mehdad et al., 2010b; Adler et al., 2012; Berant et al., 2011). We build an entailment graph for each topic cluster, where nodes are the extracted phrases and edges are the entailment relations between nodes. Given two phrases (ph1 and ph2), we aim at identifying and handling the following cases: i) ph1 and ph2 express the same meaning (bidirectional entailment). In such cases one of the phrases should be eliminated; ii) ph1 is more informative than ph2 (unidirectional entailment). In such cases, the entailing phrase should replace or complement the entailed one; iii) ph1 contains facts that are not present in ph2, and vice-versa (t</context>
</contexts>
<marker>Adler, Berant, Dagan, 2012</marker>
<rawString>Meni Adler, Jonathan Berant, and Ido Dagan. 2012. Entailment-based text exploration with application to the health-care domain. In Proceedings of the ACL 2012 System Demonstrations, ACL ’12, pages 79–84,</rawString>
</citation>
<citation valid="false">
<authors>
<author>PA Stroudsburg</author>
</authors>
<institution>USA. Association for Computational Linguistics.</institution>
<marker>Stroudsburg, </marker>
<rawString>Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allan</author>
</authors>
<title>Topic detection and tracking: eventbased information organization.</title>
<date>2002</date>
<contexts>
<context position="1307" citStr="Allan, 2002" startWordPosition="192" endWordPosition="193">ying over conversational data, and show that our framework improves performance significantly over baseline algorithms. 1 Introduction Given text segments about the same topic written in different ways (i.e., language variability), topic labeling deals with the problem of automatically generating semantically meaningful labels for those text segments. The potential of integrating topic labeling as a prerequisite for higher-level analysis has been reported in several areas, such as summarization (Harabagiu and Lacatusu, 2010; Kleinbauer et al., 2007; Dias et al., 2007), information extraction (Allan, 2002) and conversation visualization (Liu et al., 2012). Moreover, the huge amount of textual data generated everyday specifically in conversations (e.g., emails and blogs) calls for automated methods to analyze and re-organize them into meaningful coherent clusters. Table 1 shows an example of two human written topic labels for a topic cluster collected from a blog1, 1http://slashdot.org Text: a: Where do you think the term “Horse laugh” comes from? b: And that rats also giggled when tickled. c: My hypothesis- if an animal can play, it can “laugh” or at least it is familiar with the concept of “la</context>
<context position="27831" citStr="Allan, 2002" startWordPosition="4504" endWordPosition="4505">compare our approach with two strong baselines. The first baseline Freq-BL ranks the words according to their frequencies and select the top 5 candidates applying Maximum Marginal Relevance algorithm (Carbonell and Goldstein, 1998) using the same pre- and post-processing as the work by (Mihalcea and Tarau, 2004). The second baseline Lead-BL, ranks the words based on their relevance to the leading sentences.7 The ranking criteria is log(tfw,Lt + 1) x log(tfw,t + 1), where tfw,Lt and tfw,t are the number of times word w appears in a set of leading sentences Lt and topic cluster t, respectively (Allan, 2002). The log expressions, as the ranking criterion, assign more weights to the words in the topic segment, that also appear in the leading sentences. This is because topics tend to be introduced in the first few sentences of a topical cluster. We also measure the performance of our framework at each step in order to compare the effectiveness of each phase independently or in combination. 4.2 Results We evaluate the performance of different models using the metrics R-f1 and Semantic R-f1 (Sem-R-f1), described in Section 3.2. Table 4 shows the results in percentage for different models. The results</context>
</contexts>
<marker>Allan, 2002</marker>
<rawString>James Allan. 2002. Topic detection and tracking: eventbased information organization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Aggregation via set partitioning for natural language generation.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06,</booktitle>
<pages>359--366</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="19633" citStr="Barzilay and Lapata, 2006" startWordPosition="3169" endWordPosition="3172">imal”, or “giggle” and “chuckle” can be replaced by “laugh”. The motivation behind the generalization step is to enrich the common terms between the phrases in favor of increasing the chance that they could merge to a single phrase. This also helps to move beyond the limitation of original lexical choices. 2.3.2 Phrase merging The goal is to merge the phrases that are connected, and to generate a human readable phrase that contains more information than a single extracted phrase. Several approaches have been proposed to aggregate and merge sentences in Natural Language Generation (NLG) (e.g. (Barzilay and Lapata, 2006; Cheng and Mellish, 2000)), however most of them use syntactic structure of the sentences. To merge phrases at the lexical level, we set few common linguistically motivated aggregation patterns such as: simple conjunction, and conjunction via shared participants (Reiter and Dale, 2000). Table 2 demonstrates the merging patterns, where wig is the jth word (or segment) in phrase i, cw is the common word (or segment) in both phrases and CPOS is the common part-of-speech tag of the corresponding word. To illustrate, pattern 1 183 looks for the first segment of each phrase (wil). If they are same </context>
</contexts>
<marker>Barzilay, Lapata, 2006</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2006. Aggregation via set partitioning for natural language generation. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06, pages 359–366, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
</authors>
<title>The fifth pascal recognizing textual entailment challenge. In</title>
<date>2009</date>
<booktitle>In Proc Text Analysis Conference (TAC09.</booktitle>
<contexts>
<context position="11152" citStr="Bentivogli et al., 2009" startWordPosition="1787" endWordPosition="1790">ailment graph, we perform the following three steps. 2.2.1 Training set collection In the last few years, TE corpora have been created and distributed in the framework of several evaluation campaigns, including the Recognizing Textual Entailment (RTE) Challenge3 and Crosslingual textual entailment for content synchronization4 (Negri et al., 2012). However, such datasets cannot directly support our application. Specifically, our entailment graph is built over the extracted phrases (with max. length of 5 tokens per phrase), while the RTE datasets are composed of longer sentences and paragraphs (Bentivogli et al., 2009; Negri et al., 2011). In order to collect a dataset which is more similar to the goal of our entailment framework, we decide to select a subset of the sixth and seventh RTE challenge main task (i.e., RTE within a Corpus). Our 3http://pascallin.ecs.soton.ac.uk/Challenges/RTE/ 4http://www.cs.york.ac.uk/semeval-2013/task8/ animals laugh x rats giggle Animals play x laugh Horse laugh rats Mice chuckle 181 dataset choice is based on the following reasons: i) the length of sentence pairs in RTE6 and RTE7 is shorter than the others, and ii) RTE6 and RTE7 main task datasets are originally created for</context>
</contexts>
<marker>Bentivogli, Dagan, Dang, Giampiccolo, Magnini, 2009</marker>
<rawString>Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference (TAC09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Global learning of typed entailment rules.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Portland, OR.</location>
<contexts>
<context position="9002" citStr="Berant et al., 2011" startWordPosition="1449" endWordPosition="1452">acted phrases and eliminate the associated keywords. 2.2 Entailment graph So far, we have extracted a pool of key phrases from each topic cluster. Many such phrases include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the phrases we can discover the information in one phrase that is semantically equivalent, novel, or more/less informative with respect to the content of the other phrase. We set this problem as a variant of the Textual Entailment (TE) recognition task (Mehdad et al., 2010b; Adler et al., 2012; Berant et al., 2011). We build an entailment graph for each topic cluster, where nodes are the extracted phrases and edges are the entailment relations between nodes. Given two phrases (ph1 and ph2), we aim at identifying and handling the following cases: i) ph1 and ph2 express the same meaning (bidirectional entailment). In such cases one of the phrases should be eliminated; ii) ph1 is more informative than ph2 (unidirectional entailment). In such cases, the entailing phrase should replace or complement the entailed one; iii) ph1 contains facts that are not present in ph2, and vice-versa (the “unknown” cases in </context>
<context position="10429" citStr="Berant et al., 2011" startWordPosition="1673" endWordPosition="1677">“animals laugh” entails “rats giggle”, “Horse laugh” and “Mice chuckle”,2 but not “Animals play”. 2Assuming that “animals laugh” is interpreted as “all animals laugh”. Figure 2: Building an entailment graph over phrases. Arrows and “x” represent the entailment direction and unknown cases respectively. So we can keep “animals laugh” and “Animals play” and eliminate others. In this way, TE-based phrase identification method can be designed to distinguish meaning-preserving variations from true divergence, regardless of lexical choices and structures. Similar to previous approaches in TE (e.g., (Berant et al., 2011; Mehdad et al., 2010b; Mehdad et al., 2010a)), we use supervised method. To train and build the entailment graph, we perform the following three steps. 2.2.1 Training set collection In the last few years, TE corpora have been created and distributed in the framework of several evaluation campaigns, including the Recognizing Textual Entailment (RTE) Challenge3 and Crosslingual textual entailment for content synchronization4 (Negri et al., 2012). However, such datasets cannot directly support our application. Specifically, our entailment graph is built over the extracted phrases (with max. leng</context>
<context position="14244" citStr="Berant et al., 2011" startWordPosition="2292" endWordPosition="2295">d WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the similarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 grams). The rationale behind using different entailment features is that combining various scores will yield a better model (Berant et al., 2011). To combine the entailment scores and optimize their relative weights, we train a Support Vector Machine binary classifier, SVMlight (Joachims, 1999), over an equal number of positive and negative examples. This results in an entailment model with 95% accuracy over 2-fold and 5-fold cross-validation, which further proves the effectiveness of our feature set for this lexical entailment model. The reason that we gained a very high accuracy is because our selected sentences are a subset of RTE6 and RTE7 with a shorter length (less number of words) which makes the entailment recognition task much</context>
<context position="17127" citStr="Berant et al., 2011" startWordPosition="2755" endWordPosition="2758">nteresting story continues 4 merge ( cw11(CP OS=[N|V |J]) ..w1n , w21..cw2n(CP OS=[N|V |J]) ) = w21..w2n w12..w1n E.g. merge( LHC shutting down , details about LHC ) = details about LHC shutting down 5 merge ( w11Cpos , cw12(CP OS=[N|V |J]) , w13Cpos , w21Cpos , cw22(CP OS=[N|V |J]) , w23Cpos ) = w11 and w21 w22 w23 E.g. and w13 merge ( technology grow fast , media grow exponentially) =technology and media grow exponentially and fast Table 2: Phrase merging patterns. is a transitive relation, our entailment graph is transitive i.e., if entail(ph1,ph2) and entail(ph2,ph3) then entail(ph1,ph3) (Berant et al., 2011). Rule 1) If there is a chain of entailing nodes, we keep the one which is in the root of the chain and eliminate others (e.g. “animals laugh” in Figure 2); Rule 2) Among the nodes that are connected with bidirectional entailment (semantically equivalent nodes) we keep only the one with more outgoing bidirectional and unidirectional entailment relations, respectively; Rule 3) Among the nodes that are connected with unknown entailment (novel information with respect to others) we keep the ones with no incoming entailment relation (e.g., “Animals play” in Figure 2). Although deleting might be ha</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2011</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Global learning of typed entailment rules. In Proceedings of ACL, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SRK Branavan</author>
<author>Pawan Deshpande</author>
<author>Regina Barzilay</author>
</authors>
<title>Generating a table-of-contents.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<volume>45</volume>
<pages>544</pages>
<contexts>
<context position="2474" citStr="Branavan et al., 2007" startWordPosition="376" endWordPosition="379">ugh” or at least it is familiar with the concept of “laughing”. Many animals play. There are various sorts of humour though. Some involve you laughing because your brain suddenly made a lots of unexpected connections. Possible extracted phrases: animals play, rats have, laugh, Horse laugh, rats also giggle, rats Human-authored topic labels: animals which laugh, animal laughter Table 1: Topic labeling example. and possible phrases that can be extracted from the topic cluster using different approaches. This example demonstrates that although most approaches (Mei et al., 2007; Lau et al., 2011; Branavan et al., 2007) advocate extracting phrase-level topic labels from the text segments, topically related text segments do not always contain one keyword or key phrase that can capture the meaning of the topic. As shown in this example, such labels do not exist in the original text and cannot be extracted using the existing probabilistic models (e.g., (Mei et al., 2007)). The same problem can be observed with many other examples. This suggests the idea of aggregating and generating topic labels, instead of simply extracting them, as a challenging scenario for this field of research. Moreover, to generate a lab</context>
</contexts>
<marker>Branavan, Deshpande, Barzilay, 2007</marker>
<rawString>SRK Branavan, Pawan Deshpande, and Regina Barzilay. 2007. Generating a table-of-contents. In ACL, volume 45, page 544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of mmr, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’98,</booktitle>
<pages>335--336</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="27450" citStr="Carbonell and Goldstein, 1998" startWordPosition="4435" endWordPosition="4438">ir associated topic labels) from 20 blog conversations and 242 topics from 40 email conversations. For preprocessing our dataset we use OpenNLP6 for tokenization, part-of-speech tagging and chuncking. For sense disambiguation, we use the extended gloss overlap measure with the window size of 5, developed by (Pedersen et al., 2005). We also apply Snowball algorithm (Porter, 2001) for stemming. We compare our approach with two strong baselines. The first baseline Freq-BL ranks the words according to their frequencies and select the top 5 candidates applying Maximum Marginal Relevance algorithm (Carbonell and Goldstein, 1998) using the same pre- and post-processing as the work by (Mihalcea and Tarau, 2004). The second baseline Lead-BL, ranks the words based on their relevance to the leading sentences.7 The ranking criteria is log(tfw,Lt + 1) x log(tfw,t + 1), where tfw,Lt and tfw,t are the number of times word w appears in a set of leading sentences Lt and topic cluster t, respectively (Allan, 2002). The log expressions, as the ranking criterion, assign more weights to the words in the topic segment, that also appear in the leading sentences. This is because topics tend to be introduced in the first few sentences </context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’98, pages 335–336, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Gabriel Murray</author>
<author>Raymond Ng</author>
</authors>
<title>Methods for mining and summarizing text conversations.</title>
<date>2011</date>
<contexts>
<context position="5201" citStr="Carenini et al., 2011" startWordPosition="808" endWordPosition="811">lly equivalent, novel, or more informative with respect to the content of the others. In this way, we prune the redundant and less informative text portions (e.g., phrases), and produce semantically informed phrases for the generation phase. In the case of the example in Table 1, we eliminate phrases such as “rats have”, “rats” and “laugh” while keeping “animal play”, “Horse laugh” and “rats also giggle”. The experimental results over conversational data sets show that, in all cases, our approach outperforms other models significantly. Although conversational data are known to be challenging (Carenini et al., 2011), we choose to test our method on conversations because this is a genre in which topic modeling is critically needed, as conversations lack the structure and organization of, for instance, edited monologues. The results indicate that our framework is sufficiently robust to deal with topic labeling in less structured, informal genres (when compared with edited monologues). As an additional result of our experiments, we show that the identification and selection phase using semantic relations (entailment graph) is a necessary step to perform the final step (i.e., the phrase aggregation). 2 Topic</context>
</contexts>
<marker>Carenini, Murray, Ng, 2011</marker>
<rawString>Giuseppe Carenini, Gabriel Murray, and Raymond Ng. 2011. Methods for mining and summarizing text conversations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Cheng</author>
<author>Chris Mellish</author>
</authors>
<title>Capturing the interaction between aggregation and text planning in two generation systems.</title>
<date>2000</date>
<booktitle>In In Proceedings of the 1st International Natural Language Generation Conference,</booktitle>
<pages>186193</pages>
<contexts>
<context position="19659" citStr="Cheng and Mellish, 2000" startWordPosition="3173" endWordPosition="3176">ckle” can be replaced by “laugh”. The motivation behind the generalization step is to enrich the common terms between the phrases in favor of increasing the chance that they could merge to a single phrase. This also helps to move beyond the limitation of original lexical choices. 2.3.2 Phrase merging The goal is to merge the phrases that are connected, and to generate a human readable phrase that contains more information than a single extracted phrase. Several approaches have been proposed to aggregate and merge sentences in Natural Language Generation (NLG) (e.g. (Barzilay and Lapata, 2006; Cheng and Mellish, 2000)), however most of them use syntactic structure of the sentences. To merge phrases at the lexical level, we set few common linguistically motivated aggregation patterns such as: simple conjunction, and conjunction via shared participants (Reiter and Dale, 2000). Table 2 demonstrates the merging patterns, where wig is the jth word (or segment) in phrase i, cw is the common word (or segment) in both phrases and CPOS is the common part-of-speech tag of the corresponding word. To illustrate, pattern 1 183 looks for the first segment of each phrase (wil). If they are same (cwil) and share the same </context>
</contexts>
<marker>Cheng, Mellish, 2000</marker>
<rawString>Hua Cheng and Chris Mellish. 2000. Capturing the interaction between aggregation and text planning in two generation systems. In In Proceedings of the 1st International Natural Language Generation Conference, 186193, Mitzpe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>Verbocean: Mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>33--40</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="13533" citStr="Chklovski and Pantel, 2004" startWordPosition="2176" endWordPosition="2179">s for each pair of phrases. In order to adapt the similarity scores to the entailment score, we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2. In this way, we can check the portion of information/facts in ph2 which is covered by ph1. The first 5 scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the similarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 grams). The rationale behind using</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relations. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 33–40, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
</authors>
<title>Probabilistic textual entailment: Generic applied modeling of language variability.</title>
<date>2004</date>
<contexts>
<context position="4531" citStr="Dagan and Glickman, 2004" startWordPosition="700" endWordPosition="703"> can semantically imply more than one original extracted phrase. For example, the phrase “rats also giggle” and “horse laugh” should be merged into a new phrase “animals laugh”. Although our method is still relying on extracting phrases, we move beyond current extractive approaches, by generating new phrases through generalization and aggregation of the extracted ones. 2. Building a multidirectional entailment graph over the extracted phrases to identify and select the relevant information. We set such problem as an application-oriented variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004), to identify the information that are semantically equivalent, novel, or more informative with respect to the content of the others. In this way, we prune the redundant and less informative text portions (e.g., phrases), and produce semantically informed phrases for the generation phase. In the case of the example in Table 1, we eliminate phrases such as “rats have”, “rats” and “laugh” while keeping “animal play”, “Horse laugh” and “rats also giggle”. The experimental results over conversational data sets show that, in all cases, our approach outperforms other models significantly. Although c</context>
</contexts>
<marker>Dagan, Glickman, 2004</marker>
<rawString>I. Dagan and O. Glickman. 2004. Probabilistic textual entailment: Generic applied modeling of language variability.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor-next and the meteor paraphrase tables: improved evaluation support for five target languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10,</booktitle>
<pages>339--342</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13574" citStr="Denkowski and Lavie, 2010" startWordPosition="2181" endWordPosition="2184">apt the similarity scores to the entailment score, we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2. In this way, we can check the portion of information/facts in ph2 which is covered by ph1. The first 5 scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the similarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 grams). The rationale behind using different entailment features is that co</context>
</contexts>
<marker>Denkowski, Lavie, 2010</marker>
<rawString>Michael Denkowski and Alon Lavie. 2010. Meteor-next and the meteor paraphrase tables: improved evaluation support for five target languages. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10, pages 339–342, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elsa Alves Ga¨el Dias</author>
<author>Jos´e Gabriel Pereira Lopes</author>
</authors>
<title>Topic segmentation algorithms for text summarization and passage retrieval: an exhaustive evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd national conference on Artificial intelligence - Volume 2, AAAI’07,</booktitle>
<pages>1334--1339</pages>
<publisher>AAAI Press.</publisher>
<marker>Ga¨el Dias, Lopes, 2007</marker>
<rawString>Ga¨el Dias, Elsa Alves, and Jos´e Gabriel Pereira Lopes. 2007. Topic segmentation algorithms for text summarization and passage retrieval: an exhaustive evaluation. In Proceedings of the 22nd national conference on Artificial intelligence - Volume 2, AAAI’07, pages 1334–1339. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="13493" citStr="Fellbaum, 1998" startWordPosition="2173" endWordPosition="2174"> compute 18 similarity scores for each pair of phrases. In order to adapt the similarity scores to the entailment score, we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2. In this way, we can check the portion of information/facts in ph2 which is covered by ph1. The first 5 scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the similarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Gordon W Paynter</author>
<author>Ian H Witten</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, IJCAI ’99,</booktitle>
<pages>668--673</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="7396" citStr="Frank et al., 1999" startWordPosition="1173" endWordPosition="1176">ion We tokenize and preprocess each cluster in the collection of topic clusters with lemmas, stems, part-ofspeech tags, sense tags and chunks. We also extract n-grams up to length 5 which do not start or end with a stop word. In this phase, we do not include any frequency count feature in our candidate extraction pipeline. Once we have built the candidates pool, the next step is to identify a subset containing the most significant of those candidates. Since most top systems in key phrase extraction use supervised approaches, we follow the same method (Kim et al., 2010b; Medelyan et al., 2008; Frank et al., 1999). Initially, we consider a set of features used in the other systems to determine whether a phrase is likely to be a key phrase. However, since our dataset is conversational (more details in Section 3), and the text segments are not long, we aim for a classifier with high recall. Thus, we only use TFxIDF (Salton and McGill, 1986), position of the first occurrence (Frank et al., 1999) and phrase length as our features. We merge the training and test data released 180 for SemEval-2010 Task #5 (Kim et al., 2010b), which consists of 244 scientific articles and 3705 key phrases, to train a Naive Ba</context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl Gutwin, and Craig G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, IJCAI ’99, pages 668–673, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Finley Lacatusu</author>
</authors>
<title>Using topic themes for multi-document summarization.</title>
<date>2010</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1224" citStr="Harabagiu and Lacatusu, 2010" startWordPosition="178" endWordPosition="181">se selected phrases by means of phrase generalization and merging. We motivate our approach by applying over conversational data, and show that our framework improves performance significantly over baseline algorithms. 1 Introduction Given text segments about the same topic written in different ways (i.e., language variability), topic labeling deals with the problem of automatically generating semantically meaningful labels for those text segments. The potential of integrating topic labeling as a prerequisite for higher-level analysis has been reported in several areas, such as summarization (Harabagiu and Lacatusu, 2010; Kleinbauer et al., 2007; Dias et al., 2007), information extraction (Allan, 2002) and conversation visualization (Liu et al., 2012). Moreover, the huge amount of textual data generated everyday specifically in conversations (e.g., emails and blogs) calls for automated methods to analyze and re-organize them into meaningful coherent clusters. Table 1 shows an example of two human written topic labels for a topic cluster collected from a blog1, 1http://slashdot.org Text: a: Where do you think the term “Horse laugh” comes from? b: And that rats also giggled when tickled. c: My hypothesis- if an</context>
</contexts>
<marker>Harabagiu, Lacatusu, 2010</marker>
<rawString>Sanda Harabagiu and Finley Lacatusu. 2010. Using topic themes for multi-document summarization. ACM Trans. Inf. Syst., 28(3):13:1–13:47, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale svm learning practical. LS8-Report 24, Universit¨at Dortmund, LS VIII-Report.</title>
<date>1999</date>
<contexts>
<context position="14394" citStr="Joachims, 1999" startWordPosition="2316" endWordPosition="2317">relations. Then, we calculated the sentence similarity as the sum of the similarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 grams). The rationale behind using different entailment features is that combining various scores will yield a better model (Berant et al., 2011). To combine the entailment scores and optimize their relative weights, we train a Support Vector Machine binary classifier, SVMlight (Joachims, 1999), over an equal number of positive and negative examples. This results in an entailment model with 95% accuracy over 2-fold and 5-fold cross-validation, which further proves the effectiveness of our feature set for this lexical entailment model. The reason that we gained a very high accuracy is because our selected sentences are a subset of RTE6 and RTE7 with a shorter length (less number of words) which makes the entailment recognition task much easier than recognizing entailment between paragraphs or complex long sentences. 2.2.3 Graph edge labeling We set the edge labeling problem as a two-</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale svm learning practical. LS8-Report 24, Universit¨at Dortmund, LS VIII-Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Gabriel Murray</author>
<author>Raymond T Ng</author>
</authors>
<title>Exploiting conversation structure in unsupervised topic segmentation for emails.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="22049" citStr="Joty et al., 2010" startWordPosition="3554" endWordPosition="3557">tional datasets. Our interest in dealing with conversational texts derives from two reasons. First, the huge amount of textual data generated everyday in these conversations validates the need of text analysis frameworks to process such conversational texts effectively. Second, conversational texts pose challenges to the traditional techniques, including redundancies, disfluencies, higher language variabilities and ill-formed sentence structure (Liu et al., 2011). Our conversational datasets are from two different asynchronous media: email and blog. For email, we use the dataset presented in (Joty et al., 2010), where three individuals annotated the publicly available BC3 email corpus (Ulrich et al., 2008) with topics. The corpus contains 40 email threads (or conversations) at an average of 5 emails per thread. On average it has 26.3 sentences and 2.5 topics per thread. A topic has an average length of 12.6 sentences. In total, the three annotators found 269 topics in a corpus of 1,024 sentences. There are no publicly available blog corpora annotated with topics. For this study, we build our own blog corpus containing 20 blog conversations of various lengths from Slashdot, each annotated with topics</context>
</contexts>
<marker>Joty, Carenini, Murray, Ng, 2010</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, Gabriel Murray, and Raymond T. Ng. 2010. Exploiting conversation structure in unsupervised topic segmentation for emails. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Gabriel Murray</author>
<author>Raymond T Ng</author>
</authors>
<title>Supervised topic segmentation of email conversations.</title>
<date>2011</date>
<booktitle>In In ICWSM11.</booktitle>
<publisher>AAAI.</publisher>
<contexts>
<context position="6027" citStr="Joty et al., 2011" startWordPosition="938" endWordPosition="941">The results indicate that our framework is sufficiently robust to deal with topic labeling in less structured, informal genres (when compared with edited monologues). As an additional result of our experiments, we show that the identification and selection phase using semantic relations (entailment graph) is a necessary step to perform the final step (i.e., the phrase aggregation). 2 Topic Labeling Framework Each topic cluster contains the sentences that can semantically represent a topic. The task of clustering the sentences into a set of coherent topic clusters is called topic segmentation (Joty et al., 2011), which is out of the scope of this paper. Our goal is to generate an understandable label (i.e., a sequence of words) that could capture the semantic of the topic, and distinguish a topic from other topics (based on definition of a good topic label by (Mei et al., 2007)), given a set of topic clusters. Among possible choices of word sequences as topic labels, in order to balance the granularity, we set phrases as valid topic labels. ✲ ✲ Extract all Entailment Generalize ❄ Phrase extraction Entailment Graph Phrase ❄ aggregation ❄ Filter/select Identify Merge Figure 1: Topic 1labeling framework</context>
</contexts>
<marker>Joty, Murray, Ng, 2011</marker>
<rawString>Shafiq Joty, Gabriel Murray, and Raymond T. Ng. 2011. Supervised topic segmentation of email conversations. In In ICWSM11. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Timothy Baldwin</author>
<author>Min-Yen Kan</author>
</authors>
<title>Evaluating n-gram based evaluation metrics for automatic keyphrase extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>572--580</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7351" citStr="Kim et al., 2010" startWordPosition="1165" endWordPosition="1168">the following sections. 2.1 Phrase extraction We tokenize and preprocess each cluster in the collection of topic clusters with lemmas, stems, part-ofspeech tags, sense tags and chunks. We also extract n-grams up to length 5 which do not start or end with a stop word. In this phase, we do not include any frequency count feature in our candidate extraction pipeline. Once we have built the candidates pool, the next step is to identify a subset containing the most significant of those candidates. Since most top systems in key phrase extraction use supervised approaches, we follow the same method (Kim et al., 2010b; Medelyan et al., 2008; Frank et al., 1999). Initially, we consider a set of features used in the other systems to determine whether a phrase is likely to be a key phrase. However, since our dataset is conversational (more details in Section 3), and the text segments are not long, we aim for a classifier with high recall. Thus, we only use TFxIDF (Salton and McGill, 1986), position of the first occurrence (Frank et al., 1999) and phrase length as our features. We merge the training and test data released 180 for SemEval-2010 Task #5 (Kim et al., 2010b), which consists of 244 scientific artic</context>
<context position="24441" citStr="Kim et al., 2010" startWordPosition="3966" endWordPosition="3969">nnotators found 100, 77 and 92 topics respectively (269 in total), and in the blog corpus, they found 251, 119 and 192 topics respectively (562 in total). For the evaluation, there is a single gold standard per topic written by each annotator. Table 1 shows a case in which two annotators selected the same topical cluster and so we have two labels for the same cluster. 3.2 Evaluation metrics Traditionally, key phrase extraction is evaluated using precision, recall and f-measure based on exact matches on all the extracted key phrases with gold standards for a given text. However, as claimed by (Kim et al., 2010a), this approach is not flexible enough as it ignores the near-misses. Moreover, in the case of topic labeling, most of the human written 5The new blog corpus annotated with topics will be made publicly available for research purposes. 184 topic labels cannot be found in the text. Recently, (Kim et al., 2010a) evaluated the utility of different n-gram-based metrics for key phrase extraction and showed that the metric R-precision correlates most with human judgments. R-precision normalizes the approximate matching score by the maximum number of words in the reference and candidate phrases. Sin</context>
</contexts>
<marker>Kim, Baldwin, Kan, 2010</marker>
<rawString>Su Nam Kim, Timothy Baldwin, and Min-Yen Kan. 2010a. Evaluating n-gram based evaluation metrics for automatic keyphrase extraction. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 572–580, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Olena Medelyan</author>
<author>Min-Yen Kan</author>
<author>Timothy Baldwin</author>
</authors>
<title>Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10,</booktitle>
<pages>21--26</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7351" citStr="Kim et al., 2010" startWordPosition="1165" endWordPosition="1168">the following sections. 2.1 Phrase extraction We tokenize and preprocess each cluster in the collection of topic clusters with lemmas, stems, part-ofspeech tags, sense tags and chunks. We also extract n-grams up to length 5 which do not start or end with a stop word. In this phase, we do not include any frequency count feature in our candidate extraction pipeline. Once we have built the candidates pool, the next step is to identify a subset containing the most significant of those candidates. Since most top systems in key phrase extraction use supervised approaches, we follow the same method (Kim et al., 2010b; Medelyan et al., 2008; Frank et al., 1999). Initially, we consider a set of features used in the other systems to determine whether a phrase is likely to be a key phrase. However, since our dataset is conversational (more details in Section 3), and the text segments are not long, we aim for a classifier with high recall. Thus, we only use TFxIDF (Salton and McGill, 1986), position of the first occurrence (Frank et al., 1999) and phrase length as our features. We merge the training and test data released 180 for SemEval-2010 Task #5 (Kim et al., 2010b), which consists of 244 scientific artic</context>
<context position="24441" citStr="Kim et al., 2010" startWordPosition="3966" endWordPosition="3969">nnotators found 100, 77 and 92 topics respectively (269 in total), and in the blog corpus, they found 251, 119 and 192 topics respectively (562 in total). For the evaluation, there is a single gold standard per topic written by each annotator. Table 1 shows a case in which two annotators selected the same topical cluster and so we have two labels for the same cluster. 3.2 Evaluation metrics Traditionally, key phrase extraction is evaluated using precision, recall and f-measure based on exact matches on all the extracted key phrases with gold standards for a given text. However, as claimed by (Kim et al., 2010a), this approach is not flexible enough as it ignores the near-misses. Moreover, in the case of topic labeling, most of the human written 5The new blog corpus annotated with topics will be made publicly available for research purposes. 184 topic labels cannot be found in the text. Recently, (Kim et al., 2010a) evaluated the utility of different n-gram-based metrics for key phrase extraction and showed that the metric R-precision correlates most with human judgments. R-precision normalizes the approximate matching score by the maximum number of words in the reference and candidate phrases. Sin</context>
</contexts>
<marker>Kim, Medelyan, Kan, Baldwin, 2010</marker>
<rawString>Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timothy Baldwin. 2010b. Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10, pages 21–26, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Kleinbauer</author>
<author>Stephanie Becker</author>
<author>Tilman Becker</author>
</authors>
<title>Combining multiple information layers for the automatic generation of indicative meeting abstracts.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh European Workshop on Natural Language Generation, ENLG ’07,</booktitle>
<pages>151--154</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1249" citStr="Kleinbauer et al., 2007" startWordPosition="182" endWordPosition="185">f phrase generalization and merging. We motivate our approach by applying over conversational data, and show that our framework improves performance significantly over baseline algorithms. 1 Introduction Given text segments about the same topic written in different ways (i.e., language variability), topic labeling deals with the problem of automatically generating semantically meaningful labels for those text segments. The potential of integrating topic labeling as a prerequisite for higher-level analysis has been reported in several areas, such as summarization (Harabagiu and Lacatusu, 2010; Kleinbauer et al., 2007; Dias et al., 2007), information extraction (Allan, 2002) and conversation visualization (Liu et al., 2012). Moreover, the huge amount of textual data generated everyday specifically in conversations (e.g., emails and blogs) calls for automated methods to analyze and re-organize them into meaningful coherent clusters. Table 1 shows an example of two human written topic labels for a topic cluster collected from a blog1, 1http://slashdot.org Text: a: Where do you think the term “Horse laugh” comes from? b: And that rats also giggled when tickled. c: My hypothesis- if an animal can play, it can </context>
</contexts>
<marker>Kleinbauer, Becker, Becker, 2007</marker>
<rawString>Thomas Kleinbauer, Stephanie Becker, and Tilman Becker. 2007. Combining multiple information layers for the automatic generation of indicative meeting abstracts. In Proceedings of the Eleventh European Workshop on Natural Language Generation, ENLG ’07, pages 151–154, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>David Newman</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic labelling of topic models.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>1536--1545</pages>
<contexts>
<context position="2450" citStr="Lau et al., 2011" startWordPosition="372" endWordPosition="375">n play, it can “laugh” or at least it is familiar with the concept of “laughing”. Many animals play. There are various sorts of humour though. Some involve you laughing because your brain suddenly made a lots of unexpected connections. Possible extracted phrases: animals play, rats have, laugh, Horse laugh, rats also giggle, rats Human-authored topic labels: animals which laugh, animal laughter Table 1: Topic labeling example. and possible phrases that can be extracted from the topic cluster using different approaches. This example demonstrates that although most approaches (Mei et al., 2007; Lau et al., 2011; Branavan et al., 2007) advocate extracting phrase-level topic labels from the text segments, topically related text segments do not always contain one keyword or key phrase that can capture the meaning of the topic. As shown in this example, such labels do not exist in the original text and cannot be extracted using the existing probabilistic models (e.g., (Mei et al., 2007)). The same problem can be observed with many other examples. This suggests the idea of aggregating and generating topic labels, instead of simply extracting them, as a challenging scenario for this field of research. Mor</context>
</contexts>
<marker>Lau, Grieser, Newman, Baldwin, 2011</marker>
<rawString>Jey Han Lau, Karl Grieser, David Newman, and Timothy Baldwin. 2011. Automatic labelling of topic models. In ACL, pages 1536–1545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th annual international conference on Systems documentation, SIGDOC ’86,</booktitle>
<pages>24--26</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="13411" citStr="Lesk, 1986" startWordPosition="2161" endWordPosition="2162"> feature is a specific similarity score estimating whether ph1 entails ph2. We compute 18 similarity scores for each pair of phrases. In order to adapt the similarity scores to the entailment score, we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2. In this way, we can check the portion of information/facts in ph2 which is covered by ph1. The first 5 scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the similarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in </context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 5th annual international conference on Systems documentation, SIGDOC ’86, pages 24–26, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Liu</author>
<author>Y Liu</author>
<author>C Busso</author>
<author>S Harabagiu</author>
<author>V Ng</author>
</authors>
<title>Identifying the Gist of Conversational Text: Automatic Keyword Extraction and Summarization.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>THE UNIVERSITY OF TEXAS AT DALLAS.</institution>
<contexts>
<context position="21898" citStr="Liu et al., 2011" startWordPosition="3529" endWordPosition="3532">e topic cluster. 3 Datasets and Evaluation Metrics 3.1 Datasets To verify the effectiveness of our approach, we experiment with two different conversational datasets. Our interest in dealing with conversational texts derives from two reasons. First, the huge amount of textual data generated everyday in these conversations validates the need of text analysis frameworks to process such conversational texts effectively. Second, conversational texts pose challenges to the traditional techniques, including redundancies, disfluencies, higher language variabilities and ill-formed sentence structure (Liu et al., 2011). Our conversational datasets are from two different asynchronous media: email and blog. For email, we use the dataset presented in (Joty et al., 2010), where three individuals annotated the publicly available BC3 email corpus (Ulrich et al., 2008) with topics. The corpus contains 40 email threads (or conversations) at an average of 5 emails per thread. On average it has 26.3 sentences and 2.5 topics per thread. A topic has an average length of 12.6 sentences. In total, the three annotators found 269 topics in a corpus of 1,024 sentences. There are no publicly available blog corpora annotated </context>
</contexts>
<marker>Liu, Liu, Busso, Harabagiu, Ng, 2011</marker>
<rawString>F. Liu, Y. Liu, C. Busso, S. Harabagiu, and V. Ng. 2011. Identifying the Gist of Conversational Text: Automatic Keyword Extraction and Summarization. Ph.D. thesis, THE UNIVERSITY OF TEXAS AT DALLAS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shixia Liu</author>
<author>Michelle X Zhou</author>
<author>Shimei Pan</author>
<author>Yangqiu Song</author>
<author>Weihong Qian</author>
<author>Weijia Cai</author>
<author>Xiaoxiao Lian</author>
</authors>
<title>Tiara: Interactive, topic-based visual text summarization and analysis.</title>
<date>2012</date>
<journal>ACM Trans. Intell. Syst. Technol.,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="1357" citStr="Liu et al., 2012" startWordPosition="197" endWordPosition="200">our framework improves performance significantly over baseline algorithms. 1 Introduction Given text segments about the same topic written in different ways (i.e., language variability), topic labeling deals with the problem of automatically generating semantically meaningful labels for those text segments. The potential of integrating topic labeling as a prerequisite for higher-level analysis has been reported in several areas, such as summarization (Harabagiu and Lacatusu, 2010; Kleinbauer et al., 2007; Dias et al., 2007), information extraction (Allan, 2002) and conversation visualization (Liu et al., 2012). Moreover, the huge amount of textual data generated everyday specifically in conversations (e.g., emails and blogs) calls for automated methods to analyze and re-organize them into meaningful coherent clusters. Table 1 shows an example of two human written topic labels for a topic cluster collected from a blog1, 1http://slashdot.org Text: a: Where do you think the term “Horse laugh” comes from? b: And that rats also giggled when tickled. c: My hypothesis- if an animal can play, it can “laugh” or at least it is familiar with the concept of “laughing”. Many animals play. There are various sort</context>
</contexts>
<marker>Liu, Zhou, Pan, Song, Qian, Cai, Lian, 2012</marker>
<rawString>Shixia Liu, Michelle X. Zhou, Shimei Pan, Yangqiu Song, Weihong Qian, Weijia Cai, and Xiaoxiao Lian. 2012. Tiara: Interactive, topic-based visual text summarization and analysis. ACM Trans. Intell. Syst. Technol., 3(2):25:1–25:28, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Medelyan</author>
<author>I H Witten</author>
<author>D Milne</author>
</authors>
<title>Topic indexing with wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of the AAAI WikiAI workshop.</booktitle>
<contexts>
<context position="7375" citStr="Medelyan et al., 2008" startWordPosition="1169" endWordPosition="1172">ons. 2.1 Phrase extraction We tokenize and preprocess each cluster in the collection of topic clusters with lemmas, stems, part-ofspeech tags, sense tags and chunks. We also extract n-grams up to length 5 which do not start or end with a stop word. In this phase, we do not include any frequency count feature in our candidate extraction pipeline. Once we have built the candidates pool, the next step is to identify a subset containing the most significant of those candidates. Since most top systems in key phrase extraction use supervised approaches, we follow the same method (Kim et al., 2010b; Medelyan et al., 2008; Frank et al., 1999). Initially, we consider a set of features used in the other systems to determine whether a phrase is likely to be a key phrase. However, since our dataset is conversational (more details in Section 3), and the text segments are not long, we aim for a classifier with high recall. Thus, we only use TFxIDF (Salton and McGill, 1986), position of the first occurrence (Frank et al., 1999) and phrase length as our features. We merge the training and test data released 180 for SemEval-2010 Task #5 (Kim et al., 2010b), which consists of 244 scientific articles and 3705 key phrases</context>
</contexts>
<marker>Medelyan, Witten, Milne, 2008</marker>
<rawString>O. Medelyan, I.H. Witten, and D. Milne. 2008. Topic indexing with wikipedia. In Proceedings of the AAAI WikiAI workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Mehdad</author>
<author>A Moschitti</author>
<author>F M Zanzotto</author>
</authors>
<title>Syntactic semantic structures for textual entailment recognition. Association for Computational Linguistics.</title>
<date>2010</date>
<contexts>
<context position="8959" citStr="Mehdad et al., 2010" startWordPosition="1441" endWordPosition="1444"> keywords. We add those chunks to our extracted phrases and eliminate the associated keywords. 2.2 Entailment graph So far, we have extracted a pool of key phrases from each topic cluster. Many such phrases include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the phrases we can discover the information in one phrase that is semantically equivalent, novel, or more/less informative with respect to the content of the other phrase. We set this problem as a variant of the Textual Entailment (TE) recognition task (Mehdad et al., 2010b; Adler et al., 2012; Berant et al., 2011). We build an entailment graph for each topic cluster, where nodes are the extracted phrases and edges are the entailment relations between nodes. Given two phrases (ph1 and ph2), we aim at identifying and handling the following cases: i) ph1 and ph2 express the same meaning (bidirectional entailment). In such cases one of the phrases should be eliminated; ii) ph1 is more informative than ph2 (unidirectional entailment). In such cases, the entailing phrase should replace or complement the entailed one; iii) ph1 contains facts that are not present in p</context>
<context position="10450" citStr="Mehdad et al., 2010" startWordPosition="1678" endWordPosition="1681">ls “rats giggle”, “Horse laugh” and “Mice chuckle”,2 but not “Animals play”. 2Assuming that “animals laugh” is interpreted as “all animals laugh”. Figure 2: Building an entailment graph over phrases. Arrows and “x” represent the entailment direction and unknown cases respectively. So we can keep “animals laugh” and “Animals play” and eliminate others. In this way, TE-based phrase identification method can be designed to distinguish meaning-preserving variations from true divergence, regardless of lexical choices and structures. Similar to previous approaches in TE (e.g., (Berant et al., 2011; Mehdad et al., 2010b; Mehdad et al., 2010a)), we use supervised method. To train and build the entailment graph, we perform the following three steps. 2.2.1 Training set collection In the last few years, TE corpora have been created and distributed in the framework of several evaluation campaigns, including the Recognizing Textual Entailment (RTE) Challenge3 and Crosslingual textual entailment for content synchronization4 (Negri et al., 2012). However, such datasets cannot directly support our application. Specifically, our entailment graph is built over the extracted phrases (with max. length of 5 tokens per ph</context>
</contexts>
<marker>Mehdad, Moschitti, Zanzotto, 2010</marker>
<rawString>Y. Mehdad, A. Moschitti, and F.M. Zanzotto. 2010a. Syntactic semantic structures for textual entailment recognition. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Towards cross-lingual textual entailment.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>321--324</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8959" citStr="Mehdad et al., 2010" startWordPosition="1441" endWordPosition="1444"> keywords. We add those chunks to our extracted phrases and eliminate the associated keywords. 2.2 Entailment graph So far, we have extracted a pool of key phrases from each topic cluster. Many such phrases include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the phrases we can discover the information in one phrase that is semantically equivalent, novel, or more/less informative with respect to the content of the other phrase. We set this problem as a variant of the Textual Entailment (TE) recognition task (Mehdad et al., 2010b; Adler et al., 2012; Berant et al., 2011). We build an entailment graph for each topic cluster, where nodes are the extracted phrases and edges are the entailment relations between nodes. Given two phrases (ph1 and ph2), we aim at identifying and handling the following cases: i) ph1 and ph2 express the same meaning (bidirectional entailment). In such cases one of the phrases should be eliminated; ii) ph1 is more informative than ph2 (unidirectional entailment). In such cases, the entailing phrase should replace or complement the entailed one; iii) ph1 contains facts that are not present in p</context>
<context position="10450" citStr="Mehdad et al., 2010" startWordPosition="1678" endWordPosition="1681">ls “rats giggle”, “Horse laugh” and “Mice chuckle”,2 but not “Animals play”. 2Assuming that “animals laugh” is interpreted as “all animals laugh”. Figure 2: Building an entailment graph over phrases. Arrows and “x” represent the entailment direction and unknown cases respectively. So we can keep “animals laugh” and “Animals play” and eliminate others. In this way, TE-based phrase identification method can be designed to distinguish meaning-preserving variations from true divergence, regardless of lexical choices and structures. Similar to previous approaches in TE (e.g., (Berant et al., 2011; Mehdad et al., 2010b; Mehdad et al., 2010a)), we use supervised method. To train and build the entailment graph, we perform the following three steps. 2.2.1 Training set collection In the last few years, TE corpora have been created and distributed in the framework of several evaluation campaigns, including the Recognizing Textual Entailment (RTE) Challenge3 and Crosslingual textual entailment for content synchronization4 (Negri et al., 2012). However, such datasets cannot directly support our application. Specifically, our entailment graph is built over the extracted phrases (with max. length of 5 tokens per ph</context>
</contexts>
<marker>Mehdad, Negri, Federico, 2010</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2010b. Towards cross-lingual textual entailment. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 321–324, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Using bilingual parallel corpora for crosslingual textual entailment.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>1336--1345</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13616" citStr="Mehdad et al., 2011" startWordPosition="2188" endWordPosition="2191">e, we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2. In this way, we can check the portion of information/facts in ph2 which is covered by ph1. The first 5 scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the similarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 grams). The rationale behind using different entailment features is that combining various scores will yield a better</context>
</contexts>
<marker>Mehdad, Negri, Federico, 2011</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2011. Using bilingual parallel corpora for crosslingual textual entailment. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 1336–1345, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Detecting semantic equivalence and information disparity in cross-lingual documents.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12,</booktitle>
<pages>120--124</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="15197" citStr="Mehdad et al., 2012" startWordPosition="2440" endWordPosition="2443">tiveness of our feature set for this lexical entailment model. The reason that we gained a very high accuracy is because our selected sentences are a subset of RTE6 and RTE7 with a shorter length (less number of words) which makes the entailment recognition task much easier than recognizing entailment between paragraphs or complex long sentences. 2.2.3 Graph edge labeling We set the edge labeling problem as a two-way classification task. Two-way classification casts multidirectional entailment as a unidirectional problem, where each pair is analyzed checking for entailment in both directions (Mehdad et al., 2012). In this condition, each original test example is correctly classified if both pairs originated from it are correctly judged (“YES-YES” for bidirectional,“YESNO” and “NO-YES” for unidirectional entailment and “NO-NO” for unknown cases). Two-way classification represents an intuitive solution to capture multidimensional entailment relations. Moreover, since our training examples are labeled with binary judgments, we are not able to train a three-way classifier. 2.2.4 Identification and selection Assigning all entailment relations between the extracted phrase pairs, we are aiming at identifying</context>
</contexts>
<marker>Mehdad, Negri, Federico, 2012</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012. Detecting semantic equivalence and information disparity in cross-lingual documents. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12, pages 120–124, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xuehua Shen</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Automatic labeling of multinomial topic models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’07,</booktitle>
<pages>490--499</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2432" citStr="Mei et al., 2007" startWordPosition="368" endWordPosition="371">s- if an animal can play, it can “laugh” or at least it is familiar with the concept of “laughing”. Many animals play. There are various sorts of humour though. Some involve you laughing because your brain suddenly made a lots of unexpected connections. Possible extracted phrases: animals play, rats have, laugh, Horse laugh, rats also giggle, rats Human-authored topic labels: animals which laugh, animal laughter Table 1: Topic labeling example. and possible phrases that can be extracted from the topic cluster using different approaches. This example demonstrates that although most approaches (Mei et al., 2007; Lau et al., 2011; Branavan et al., 2007) advocate extracting phrase-level topic labels from the text segments, topically related text segments do not always contain one keyword or key phrase that can capture the meaning of the topic. As shown in this example, such labels do not exist in the original text and cannot be extracted using the existing probabilistic models (e.g., (Mei et al., 2007)). The same problem can be observed with many other examples. This suggests the idea of aggregating and generating topic labels, instead of simply extracting them, as a challenging scenario for this fiel</context>
<context position="6298" citStr="Mei et al., 2007" startWordPosition="988" endWordPosition="991">antic relations (entailment graph) is a necessary step to perform the final step (i.e., the phrase aggregation). 2 Topic Labeling Framework Each topic cluster contains the sentences that can semantically represent a topic. The task of clustering the sentences into a set of coherent topic clusters is called topic segmentation (Joty et al., 2011), which is out of the scope of this paper. Our goal is to generate an understandable label (i.e., a sequence of words) that could capture the semantic of the topic, and distinguish a topic from other topics (based on definition of a good topic label by (Mei et al., 2007)), given a set of topic clusters. Among possible choices of word sequences as topic labels, in order to balance the granularity, we set phrases as valid topic labels. ✲ ✲ Extract all Entailment Generalize ❄ Phrase extraction Entailment Graph Phrase ❄ aggregation ❄ Filter/select Identify Merge Figure 1: Topic 1labeling framework. As shown in Figure 1, our framework consists of three main components that we describe in more details in the following sections. 2.1 Phrase extraction We tokenize and preprocess each cluster in the collection of topic clusters with lemmas, stems, part-ofspeech tags, s</context>
</contexts>
<marker>Mei, Shen, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. 2007. Automatic labeling of multinomial topic models. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’07, pages 490–499, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>TextRank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP-04and the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="27532" citStr="Mihalcea and Tarau, 2004" startWordPosition="4449" endWordPosition="4452">versations. For preprocessing our dataset we use OpenNLP6 for tokenization, part-of-speech tagging and chuncking. For sense disambiguation, we use the extended gloss overlap measure with the window size of 5, developed by (Pedersen et al., 2005). We also apply Snowball algorithm (Porter, 2001) for stemming. We compare our approach with two strong baselines. The first baseline Freq-BL ranks the words according to their frequencies and select the top 5 candidates applying Maximum Marginal Relevance algorithm (Carbonell and Goldstein, 1998) using the same pre- and post-processing as the work by (Mihalcea and Tarau, 2004). The second baseline Lead-BL, ranks the words based on their relevance to the leading sentences.7 The ranking criteria is log(tfw,Lt + 1) x log(tfw,t + 1), where tfw,Lt and tfw,t are the number of times word w appears in a set of leading sentences Lt and topic cluster t, respectively (Allan, 2002). The log expressions, as the ranking criterion, assign more weights to the words in the topic segment, that also appear in the leading sentences. This is because topics tend to be introduced in the first few sentences of a topical cluster. We also measure the performance of our framework at each ste</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>R. Mihalcea and P. Tarau. 2004. TextRank: Bringing order into texts. In Proceedings of EMNLP-04and the 2004 Conference on Empirical Methods in Natural Language Processing, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Luisa Bentivogli</author>
<author>Yashar Mehdad</author>
<author>Danilo Giampiccolo</author>
<author>Alessandro Marchetti</author>
</authors>
<title>Divide and conquer: crowdsourcing the creation of crosslingual textual entailment corpora.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>670--679</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11173" citStr="Negri et al., 2011" startWordPosition="1791" endWordPosition="1794"> the following three steps. 2.2.1 Training set collection In the last few years, TE corpora have been created and distributed in the framework of several evaluation campaigns, including the Recognizing Textual Entailment (RTE) Challenge3 and Crosslingual textual entailment for content synchronization4 (Negri et al., 2012). However, such datasets cannot directly support our application. Specifically, our entailment graph is built over the extracted phrases (with max. length of 5 tokens per phrase), while the RTE datasets are composed of longer sentences and paragraphs (Bentivogli et al., 2009; Negri et al., 2011). In order to collect a dataset which is more similar to the goal of our entailment framework, we decide to select a subset of the sixth and seventh RTE challenge main task (i.e., RTE within a Corpus). Our 3http://pascallin.ecs.soton.ac.uk/Challenges/RTE/ 4http://www.cs.york.ac.uk/semeval-2013/task8/ animals laugh x rats giggle Animals play x laugh Horse laugh rats Mice chuckle 181 dataset choice is based on the following reasons: i) the length of sentence pairs in RTE6 and RTE7 is shorter than the others, and ii) RTE6 and RTE7 main task datasets are originally created for summarization purpos</context>
</contexts>
<marker>Negri, Bentivogli, Mehdad, Giampiccolo, Marchetti, 2011</marker>
<rawString>Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo Giampiccolo, and Alessandro Marchetti. 2011. Divide and conquer: crowdsourcing the creation of crosslingual textual entailment corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 670–679, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Matteo Negri</author>
<author>Alessandro Marchetti</author>
<author>Yashar Mehdad</author>
<author>Luisa Bentivogli</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>Semeval-2012 task 8: cross-lingual textual entailment for content synchronization.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12,</booktitle>
<pages>399--407</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10877" citStr="Negri et al., 2012" startWordPosition="1744" endWordPosition="1747">tinguish meaning-preserving variations from true divergence, regardless of lexical choices and structures. Similar to previous approaches in TE (e.g., (Berant et al., 2011; Mehdad et al., 2010b; Mehdad et al., 2010a)), we use supervised method. To train and build the entailment graph, we perform the following three steps. 2.2.1 Training set collection In the last few years, TE corpora have been created and distributed in the framework of several evaluation campaigns, including the Recognizing Textual Entailment (RTE) Challenge3 and Crosslingual textual entailment for content synchronization4 (Negri et al., 2012). However, such datasets cannot directly support our application. Specifically, our entailment graph is built over the extracted phrases (with max. length of 5 tokens per phrase), while the RTE datasets are composed of longer sentences and paragraphs (Bentivogli et al., 2009; Negri et al., 2011). In order to collect a dataset which is more similar to the goal of our entailment framework, we decide to select a subset of the sixth and seventh RTE challenge main task (i.e., RTE within a Corpus). Our 3http://pascallin.ecs.soton.ac.uk/Challenges/RTE/ 4http://www.cs.york.ac.uk/semeval-2013/task8/ an</context>
</contexts>
<marker>Negri, Marchetti, Mehdad, Bentivogli, Giampiccolo, 2012</marker>
<rawString>Matteo Negri, Alessandro Marchetti, Yashar Mehdad, Luisa Bentivogli, and Danilo Giampiccolo. 2012. Semeval-2012 task 8: cross-lingual textual entailment for content synchronization. In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12, pages 399–407, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Banerjee</author>
<author>S Patwardhan</author>
</authors>
<title>Maximizing Semantic Relatedness to Perform Word Sense Disambiguation.</title>
<date>2005</date>
<tech>Research Report UMSI 2005/25,</tech>
<institution>University of Minnesota Supercomputing Institute,</institution>
<contexts>
<context position="27152" citStr="Pedersen et al., 2005" startWordPosition="4391" endWordPosition="4394">r eliminating the development set from the test datasets. In our experiments, the development set was used for the pattern extraction and the shortest path threshold connecting the words in Wordnet in the generalization phase. Our test dataset consists of 461 topics (i.e., clusters and their associated topic labels) from 20 blog conversations and 242 topics from 40 email conversations. For preprocessing our dataset we use OpenNLP6 for tokenization, part-of-speech tagging and chuncking. For sense disambiguation, we use the extended gloss overlap measure with the window size of 5, developed by (Pedersen et al., 2005). We also apply Snowball algorithm (Porter, 2001) for stemming. We compare our approach with two strong baselines. The first baseline Freq-BL ranks the words according to their frequencies and select the top 5 candidates applying Maximum Marginal Relevance algorithm (Carbonell and Goldstein, 1998) using the same pre- and post-processing as the work by (Mihalcea and Tarau, 2004). The second baseline Lead-BL, ranks the words based on their relevance to the leading sentences.7 The ranking criteria is log(tfw,Lt + 1) x log(tfw,t + 1), where tfw,Lt and tfw,t are the number of times word w appears i</context>
</contexts>
<marker>Pedersen, Banerjee, Patwardhan, 2005</marker>
<rawString>T. Pedersen, S. Banerjee, and S. Patwardhan. 2005. Maximizing Semantic Relatedness to Perform Word Sense Disambiguation. Research Report UMSI 2005/25, University of Minnesota Supercomputing Institute, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>Snowball: A language for stemming algorithms.</title>
<date>2001</date>
<contexts>
<context position="27201" citStr="Porter, 2001" startWordPosition="4400" endWordPosition="4401">In our experiments, the development set was used for the pattern extraction and the shortest path threshold connecting the words in Wordnet in the generalization phase. Our test dataset consists of 461 topics (i.e., clusters and their associated topic labels) from 20 blog conversations and 242 topics from 40 email conversations. For preprocessing our dataset we use OpenNLP6 for tokenization, part-of-speech tagging and chuncking. For sense disambiguation, we use the extended gloss overlap measure with the window size of 5, developed by (Pedersen et al., 2005). We also apply Snowball algorithm (Porter, 2001) for stemming. We compare our approach with two strong baselines. The first baseline Freq-BL ranks the words according to their frequencies and select the top 5 candidates applying Maximum Marginal Relevance algorithm (Carbonell and Goldstein, 1998) using the same pre- and post-processing as the work by (Mihalcea and Tarau, 2004). The second baseline Lead-BL, ranks the words based on their relevance to the leading sentences.7 The ranking criteria is log(tfw,Lt + 1) x log(tfw,t + 1), where tfw,Lt and tfw,t are the number of times word w appears in a set of leading sentences Lt and topic cluster</context>
</contexts>
<marker>Porter, 2001</marker>
<rawString>Martin F. Porter. 2001. Snowball: A language for stemming algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building natural language generation systems.</title>
<date>2000</date>
<contexts>
<context position="19920" citStr="Reiter and Dale, 2000" startWordPosition="3213" endWordPosition="3216"> lexical choices. 2.3.2 Phrase merging The goal is to merge the phrases that are connected, and to generate a human readable phrase that contains more information than a single extracted phrase. Several approaches have been proposed to aggregate and merge sentences in Natural Language Generation (NLG) (e.g. (Barzilay and Lapata, 2006; Cheng and Mellish, 2000)), however most of them use syntactic structure of the sentences. To merge phrases at the lexical level, we set few common linguistically motivated aggregation patterns such as: simple conjunction, and conjunction via shared participants (Reiter and Dale, 2000). Table 2 demonstrates the merging patterns, where wig is the jth word (or segment) in phrase i, cw is the common word (or segment) in both phrases and CPOS is the common part-of-speech tag of the corresponding word. To illustrate, pattern 1 183 looks for the first segment of each phrase (wil). If they are same (cwil) and share the same POS tag (CFOS), then we aggregate the first phrase (w1j..w1n) and the second phrase removing the first element (w22..w2n) by using the connective “and”. For instance, the aggregation of “animals laugh” and “animals play” results in “animals laugh and play”. The</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building natural language generation systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to modern information retrieval.</title>
<date>1986</date>
<contexts>
<context position="7727" citStr="Salton and McGill, 1986" startWordPosition="1232" endWordPosition="1235">nce we have built the candidates pool, the next step is to identify a subset containing the most significant of those candidates. Since most top systems in key phrase extraction use supervised approaches, we follow the same method (Kim et al., 2010b; Medelyan et al., 2008; Frank et al., 1999). Initially, we consider a set of features used in the other systems to determine whether a phrase is likely to be a key phrase. However, since our dataset is conversational (more details in Section 3), and the text segments are not long, we aim for a classifier with high recall. Thus, we only use TFxIDF (Salton and McGill, 1986), position of the first occurrence (Frank et al., 1999) and phrase length as our features. We merge the training and test data released 180 for SemEval-2010 Task #5 (Kim et al., 2010b), which consists of 244 scientific articles and 3705 key phrases, to train a Naive Bayes classifier in order to learn a supervised model. We then apply our model to extract the candidate phrases from the collected candidates pool. As a further step, to increase the coverage (recall) of our extracted phrases and to reduce the number of very short phrases (frequent keywords), we choose the chunks containing any of </context>
</contexts>
<marker>Salton, McGill, 1986</marker>
<rawString>Gerard Salton and Michael J. McGill. 1986. Introduction to modern information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Sammons</author>
<author>V G Vinod Vydiswaran</author>
<author>Dan Roth</author>
</authors>
<title>Recognizing Textual Entailment.</title>
<date>2011</date>
<booktitle>Multilingual Natural Language Applications: From Theory to Practice.</booktitle>
<editor>In Daniel M. Bikel and Imed Zitouni, editors,</editor>
<publisher>Prentice Hall,</publisher>
<contexts>
<context position="12592" citStr="Sammons et al., 2011" startWordPosition="2025" endWordPosition="2028">ge length of words in our training data is 6.7 words. There are certainly some differences between our training set and our phrases. However, the collected training samples was the closest available dataset to our purpose. 2.2.2 Feature representation and training Working at the phrase level imposes another constraint. Phrases are short and in terms of syntactic structure, they are not as rich as sentences. This limits our features to the lexical level. Lexical models, on the other hand, are less computationally expensive and easier to implement and often deliver a strong performance for RTE (Sammons et al., 2011). Our entailment decision criterion is based on similarity scores calculated with a phrase-to-phrase matching process. Each example pair of phrases (ph1 and ph2) is represented by a feature vector, where each feature is a specific similarity score estimating whether ph1 entails ph2. We compute 18 similarity scores for each pair of phrases. In order to adapt the similarity scores to the entailment score, we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2. In this way, we can check the portion of information</context>
</contexts>
<marker>Sammons, Vydiswaran, Roth, 2011</marker>
<rawString>Mark Sammons, V.G.Vinod Vydiswaran, and Dan Roth. 2011. Recognizing Textual Entailment. In Daniel M. Bikel and Imed Zitouni, editors, Multilingual Natural Language Applications: From Theory to Practice. Prentice Hall, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Ulrich</author>
<author>Gabriel Murray</author>
<author>Giuseppe Carenini</author>
</authors>
<title>A publicly available annotated corpus for supervised email summarization.</title>
<date>2008</date>
<contexts>
<context position="22146" citStr="Ulrich et al., 2008" startWordPosition="3569" endWordPosition="3572">rst, the huge amount of textual data generated everyday in these conversations validates the need of text analysis frameworks to process such conversational texts effectively. Second, conversational texts pose challenges to the traditional techniques, including redundancies, disfluencies, higher language variabilities and ill-formed sentence structure (Liu et al., 2011). Our conversational datasets are from two different asynchronous media: email and blog. For email, we use the dataset presented in (Joty et al., 2010), where three individuals annotated the publicly available BC3 email corpus (Ulrich et al., 2008) with topics. The corpus contains 40 email threads (or conversations) at an average of 5 emails per thread. On average it has 26.3 sentences and 2.5 topics per thread. A topic has an average length of 12.6 sentences. In total, the three annotators found 269 topics in a corpus of 1,024 sentences. There are no publicly available blog corpora annotated with topics. For this study, we build our own blog corpus containing 20 blog conversations of various lengths from Slashdot, each annotated with topics by three human annotators.5 The number of comments per conversation varies from 30 to 101 with a</context>
</contexts>
<marker>Ulrich, Murray, Carenini, 2008</marker>
<rawString>Jan Ulrich, Gabriel Murray, and Giuseppe Carenini. 2008. A publicly available annotated corpus for supervised email summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics -Volume 2, COLING ’00,</booktitle>
<pages>947--953</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="31359" citStr="Yeh, 2000" startWordPosition="5069" endWordPosition="5070">t row). This is mainly due to the fact that the entailment phase filters the equivalent phrases. This affects the results negatively when such filtered phrases share many common words with our human-authored phrases. However, the results improve more significantly (p &lt; 0.01) when the aggregation is conducted after the entailment. This demonstrates that, the combination of these two steps are beneficial for topic labeling over conversational datasets. In addition, the differences between the results us8The statistical significance tests was calculated by approximate randomization described in (Yeh, 2000). ing R-f1 and Sem-R-f1 metrics suggests the need for more flexible automatic evaluation methods for this task. Moreover, although the same trend of improvement is observed in blog and email corpora, the differences between their performance suggest the investigation of specialized methods for various conversational modalities. 0 100 200 300 400 500 600 Blog Figure 3: Sem-R-f1 results distribution after each phase of our pipeline for blog corpus. The x-axis represents the 1 examples sorted based on their Sem-R-f1 score. To further analyze the performance, in Figure 3, we show the Sem-R-f1 resu</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th conference on Computational linguistics -Volume 2, COLING ’00, pages 947–953, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>