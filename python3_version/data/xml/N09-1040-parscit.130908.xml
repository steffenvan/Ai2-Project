<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000114">
<title confidence="0.987208">
Hierarchical Text Segmentation from Multi-Scale Lexical Cohesion
</title>
<author confidence="0.997453">
Jacob Eisenstein
</author>
<affiliation confidence="0.995874">
Beckman Institute for Advanced Science and Technology
University of Illinois
</affiliation>
<address confidence="0.684746">
Urbana, IL 61801
</address>
<email confidence="0.998145">
jacobe@illinois.edu
</email>
<sectionHeader confidence="0.994784" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999673882352941">
This paper presents a novel unsupervised
method for hierarchical topic segmentation.
Lexical cohesion – the workhorse of unsu-
pervised linear segmentation – is treated as
a multi-scale phenomenon, and formalized
in a Bayesian setting. Each word token is
modeled as a draw from a pyramid of la-
tent topic models, where the structure of the
pyramid is constrained to induce a hierarchi-
cal segmentation. Inference takes the form
of a coordinate-ascent algorithm, iterating be-
tween two steps: a novel dynamic program
for obtaining the globally-optimal hierarchi-
cal segmentation, and collapsed variational
Bayesian inference over the hidden variables.
The resulting system is fast and accurate, and
compares well against heuristic alternatives.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999816519230769">
Recovering structural organization from unformat-
ted texts or transcripts is a fundamental problem
in natural language processing, with applications to
classroom lectures, meeting transcripts, and chat-
room logs. In the unsupervised setting, a variety
of successful systems have leveraged lexical cohe-
sion (Halliday and Hasan, 1976) – the idea that
topically-coherent segments display consistent lex-
ical distributions (Hearst, 1994; Utiyama and Isa-
hara, 2001; Eisenstein and Barzilay, 2008). How-
ever, such systems almost invariably focus on linear
segmentation, while it is widely believed that dis-
course displays a hierarchical structure (Grosz and
Sidner, 1986). This paper introduces the concept of
multi-scale lexical cohesion, and leverages this idea
in a Bayesian generative model for hierarchical topic
segmentation.
The idea of multi-scale cohesion is illustrated
by the following two examples, drawn from the
Wikipedia entry for the city of Buenos Aires.
There are over 150 city bus lines called Colec-
tivos ... Colectivos in Buenos Aires do not
have a fixed timetable, but run from 4 to sev-
eral per hour, depending on the bus line and
time of the day.
The Buenos Aires metro has six lines, 74 sta-
tions, and 52.3 km of track. An expansion
program is underway to extend existing lines
into the outer neighborhoods. Track length is
expected to reach 89 km...
The two sections are both part of a high-level seg-
ment on transportation. Words in bold are charac-
teristic of the subsections (buses and trains, respec-
tively), and do not occur elsewhere in the transporta-
tion section; words in italics occur throughout the
high-level section, but not elsewhere in the article.
This paper shows how multi-scale cohesion can be
captured in a Bayesian generative model and ex-
ploited for unsupervised hierarchical topic segmen-
tation.
Latent topic models (Blei et al., 2003) provide a
powerful statistical apparatus with which to study
discourse structure. A consistent theme is the treat-
ment of individual words as draws from multinomial
language models indexed by a hidden “topic” asso-
ciated with the word. In latent Dirichlet allocation
(LDA) and related models, the hidden topic for each
word is unconstrained and unrelated to the hidden
topic of neighboring words (given the parameters).
In this paper, the latent topics are constrained to pro-
duce a hierarchical segmentation structure, as shown
in Figure 1.
</bodyText>
<page confidence="0.989298">
353
</page>
<note confidence="0.907124">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 353–361,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<figure confidence="0.954542333333333">
8
6 7
1 2 3 4 5
</figure>
<figureCaption confidence="0.9990785">
Figure 1: Each word wt is drawn from a mixture of the
language models located above t in the pyramid.
</figureCaption>
<bodyText confidence="0.999796615384616">
These structural requirements simplify inference,
allowing the language models to be analytically
marginalized. The remaining hidden variables are
the scale-level assignments for each word token.
Given marginal distributions over these variables, it
is possible to search the entire space of hierarchical
segmentations in polynomial time, using a novel dy-
namic program. Collapsed variational Bayesian in-
ference is then used to update the marginals. This
approach achieves high quality segmentation on
multiple levels of the topic hierarchy.
Source code is available at http://people.
csail.mit.edu/jacobe/naacl09.html.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999975888888889">
The use of lexical cohesion (Halliday and Hasan,
1976) in unsupervised topic segmentation dates back
to Hearst’s seminal TEXTTILING system (1994).
Lexical cohesion was placed in a probabilistic
(though not Bayesian) framework by Utiyama and
Isahara (2001). The application of Bayesian topic
models to text segmentation was investigated first
by Blei and Moreno (2001) and later by Purver et
al. (2006), using HMM-like graphical models for
linear segmentation. Eisenstein and Barzilay (2008)
extend this work by marginalizing the language
models using the Dirichlet compound multinomial
distribution; this permits efficient inference to be
performed directly in the space of segmentations.
All of these papers consider only linear topic seg-
mentation; we introduce multi-scale lexical cohe-
sion, which posits that the distribution of some
words changes slowly with high-level topics, while
others change rapidly with lower-level subtopics.
This gives a principled mechanism to model hier-
archical topic segmentation.
The literature on hierarchical topic segmentation
is relatively sparse. Hsueh et al. (2006) describe a
supervised approach that trains separate classifiers
for topic and sub-topic segmentation; more relevant
for the current work is the unsupervised method
of Yaari (1997). As in TEXTTILING, cohesion is
measured using cosine similarity, and agglomerative
clustering is used to induce a dendrogram over para-
graphs; the dendrogram is transformed into a hier-
archical segmentation using a heuristic algorithm.
Such heuristic approaches are typically brittle, as
they include a number of parameters that must be
hand-tuned. These problems can be avoided by
working in a Bayesian probabilistic framework.
We note two orthogonal but related approaches
to extracting nonlinear discourse structures from
text. Rhetorical structure theory posits a hierarchi-
cal structure of discourse relations between spans of
text (Mann and Thompson, 1988). This structure is
richer than hierarchical topic segmentation, and the
base level of analysis is typically more fine-grained
– at the level of individual clauses. Unsupervised
approaches based purely on cohesion are unlikely to
succeed at this level of granularity.
Elsner and Charniak (2008) propose the task of
conversation disentanglement from internet chat-
room logs. Unlike hierarchical topic segmentation,
conversational threads may be disjoint, with un-
related threads interposed between two utterances
from the same thread. Elsner and Charniak present a
supervised approach to this problem, but the devel-
opment of cohesion-based unsupervised methods is
an interesting possibility for future work.
</bodyText>
<sectionHeader confidence="0.992955" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.9997265">
Topic modeling is premised on a generative frame-
work in which each word wt is drawn from a multi-
nomial Byt, where yt is a hidden topic indexing the
language model that generates wt. From a modeling
standpoint, linear topic segmentation merely adds
the constraint that yt ∈ {yt−1, yt−1 + 1}. Segmen-
tations that draw boundaries so as to induce com-
pact, low-entropy language models will achieve a
</bodyText>
<equation confidence="0.91363">
W1 ... WT
</equation>
<page confidence="0.993918">
354
</page>
<bodyText confidence="0.999605714285714">
With these pieces in place, we can write the ob-
servation likelihood,
high likelihood. Thus topic models situate lexical
cohesion in a probabilistic setting.
For hierarchical segmentation, we take the hy-
pothesis that lexical cohesion is a multi-scale phe-
nomenon. This is represented with a pyramid of lan-
guage models, shown in Figure 1. Each word may be
drawn from any language model above it in the pyra-
mid. Thus, the high-level language models will be
required to explain words throughout large parts of
the document, while the low-level language models
will be required to explain only a local set of words.
A hidden variable zt indicates which level is respon-
sible for generating the word wt.
Ideally we would like to choose the segmentation
yˆ = argmaxyp(w|y)p(y). However, we must deal
with the hidden language models Θ and scale-level
assignments z. The language models can be inte-
grated out analytically (Section 3.1). Given marginal
likelihoods for the hidden variables z, the globally
optimal segmentation yˆ can be found using a dy-
namic program (Section 4.1). Given a segmentation,
we can estimate marginals for the hidden variables,
using collapsed variational inference (Section 4.2).
We iterate between these procedures in an EM-like
coordinate-ascent algorithm (Section 4.4) until con-
vergence.
</bodyText>
<equation confidence="0.99931875">
p(wt |θytzt) )
ri
{t:y(zt)
t =j}
</equation>
<bodyText confidence="0.9838315">
where we have merely rearranged the product to
group terms that are drawn from the same language
model. As the goal is to obtain the hierarchical seg-
mentation and not the language models, the search
space can be reduced by marginalizing Θ. The
derivation is facilitated by a notational convenience:
xj represents the lexical counts induced by the set
of words {wt :y(zt)
</bodyText>
<equation confidence="0.994419692307692">
t = j�.
p(w|y, z, α) = K
ri J dθjp(θj|α)p(xj|θj)
j
= K pdcm(xj; α)
ri
j
= K Γ(Wα) W Γ(xji + α)
ri Γ(�Wi xji + α) ri Γ(α) .
j i (1)
T
p(w|y, z, Θ) = ri
t
</equation>
<bodyText confidence="0.43306825">
= K
ri
j
p(wt|θj),
</bodyText>
<subsectionHeader confidence="0.998349">
3.1 Language models
</subsectionHeader>
<bodyText confidence="0.988385607142857">
We begin the formal presentation of the model with
some notation. Each word wt is modeled as a single
draw from a multinomial language model θj. The
language models in turn are drawn from symmetric
Dirichlet distributions with parameter α. The num-
ber of language models is written K; the number of
words is W; the length of the document is T; and
the depth of the hierarchy is L.
For hierarchical segmentation, the vector yt indi-
cates the segment index of t at each level of the topic
hierarchy; the specific level of the hierarchy respon-
sible for wt is given by the hidden variable zt. Thus,
yt is the index of the language model that gener-
(zt)
ates wt.
Here, pdcm indicates the Dirichlet compound
multinomial distribution (Madsen et al., 2005),
which is the closed form solution to the integral over
language models. Also known as the multivariate
Polya distribution, the probability density function
can be computed exactly as a ratio of gamma func-
tions. Here we use a symmetric Dirichlet prior α,
though asymmetric priors can easily be applied.
Thus far we have treated the hidden variables
z as observed. In fact we will compute approxi-
mate marginal probabilities Qzt(zt), written γte �
Qzt(zt = `). Writing (x)Qz for the expectation of x
under distribution Qz, we approximate,
</bodyText>
<equation confidence="0.996914285714286">
(pdcm(xj; α))Qz ^ pdcm((xj)Qz; α)
L
E δ(wt = i)δ(y(e)
t = j)γte,
{t:jEyt}
E
(xj(i))Qz =
</equation>
<page confidence="0.986475">
355
</page>
<bodyText confidence="0.998359375">
where xj(i) indicates the count for word type i gen-
erated from segment j. In the outer sum, we con-
sider all t for possibly drawn from segment j. The
inner sum goes over all levels of the pyramid. The
delta functions take the value one if the enclosed
Boolean expression is true and zero otherwise, so
we are adding the fractional counts γt` only when
wt = i and y(`) t= j.
</bodyText>
<subsectionHeader confidence="0.999675">
3.2 Prior on segmentations
</subsectionHeader>
<bodyText confidence="0.966554625">
Maximizing the joint probability p(w, y) =
p(w|y)p(y) leaves the term p(y) as a prior on seg-
mentations. This prior can be used to favor segmen-
tations with the desired granularity. Consider a prior
of the form p(y) = IIL`=1 p(y(`)|y(`−1)); for nota-
tional convenience, we introduce a base level such
thaty(0) t= t, where every word is a segmentation
point. At every level ` &gt; 0, the prior is a Markov
</bodyText>
<equation confidence="0.9304934">
process, p(y(`)|y(`−1)) = IITt p(y(`)
t |y(`)
t−1, y(`−1)).
The constrainty(`) tE {y(`)
t−1, y(`) t−1+ 1} ensures a
</equation>
<bodyText confidence="0.998115333333333">
linear segmentation at each level. To enforce hierar-
chical consistency, each y(`) tcan be a segmentation
point only if t is also a segmentation point at the
lower level ` − 1. Zero probability is assigned to
segmentations that violate these constraints.
To quantify the prior probability of legal segmen-
tations, assume a set of parameters d`, indicating
the expected segment duration at each level. If t
is a valid potential segmentation point at level `
</bodyText>
<equation confidence="0.983902285714286">
(i.e., y(`−1) = 1 + y(`−1)
t−1 ), then the prior probabil-
t
ity of a segment transition is r` = d`−1/d`, with
d0 = 1. If there are N segments in level ` and
M &gt; N segments in level ` − 1, then the prior
p(y(`)|y(`−1)) = rN` (1 − r`)M−N, as long as the
</equation>
<bodyText confidence="0.960773111111111">
hierarchical segmentation constraint is obeyed.
For the purposes of inference it will be prefer-
able to have a prior that decomposes over levels and
segments. In particular, we do not want to have to
commit to a particular segmentation at level ` be-
fore segmenting level ` + 1. The above prior can
be approximated by replacing M with its expecta-
tion (M)d,,−1 = T/d`−1. Then a single segment
ranging from wu to wv (inclusive) will contribute
</bodyText>
<equation confidence="0.9225205">
log r` + v−u
d,,−1 log(1 − r`) to the log of the prior.
</equation>
<sectionHeader confidence="0.998209" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.999819666666667">
This section describes the inference for the segmen-
tation y, the approximate marginals QZ, and the hy-
perparameter α.
</bodyText>
<subsectionHeader confidence="0.7786865">
4.1 Dynamic programming for hierarchical
segmentation
</subsectionHeader>
<bodyText confidence="0.999915684210526">
While the model structure is reminiscent of a facto-
rial hidden Markov model (HMM), there are impor-
tant differences that prevent the direct application of
HMM inference. Hidden Markov models assume
that the parameters of the observation likelihood dis-
tributions are available directly, while we marginal-
ize them out. This has the effect of introducing de-
pendencies throughout the state space: the segment
assignment for each yt contributes to lexical counts
which in turn affect the observation likelihoods for
many other t&apos;. However, due to the left-to-right na-
ture of segmentation, efficient inference of the opti-
mal hierarchical segmentation (given the marginals
QZ) is still possible.
Let B(`)[u, v] represent the log-likelihood of
grouping together all contiguous words wu ... wv−1
at level ` of the segmentation hierarchy. Using xt
to indicate a vector of zeros with one at the position
wt, we can express B more formally:
</bodyText>
<equation confidence="0.89234425">
B(`)[u, v] =log pdcm
v − u − 1
+ log r` + log(1 − r`).
d`−1
</equation>
<bodyText confidence="0.998914357142857">
The last two terms are from the prior p(y), as ex-
plained in Section 3.2. The value of B(`)[u, v] is
computed for all u, all v &gt; u, and all `.
Next, we compute the log-likelihood of the op-
timal segmentation, which we write as A(L)[0, T].
This matrix can be filled in recursively:
A(`)[u, v] = max B(`)[t, v] + A(`−1)[t, v] + A(`)[u, t].
u&lt;t&lt;v
The first term adds in the log probability of the
segment from t to v at level `. The second term re-
turns the best score for segmenting this same interval
at a more detailed level of segmentation. The third
term recursively segments the interval from u to t at
the same level `. The boundary case A(`)[u, u] = 0.
</bodyText>
<figure confidence="0.89674525">
�xtγt`
v
E
t=u
</figure>
<page confidence="0.974034">
356
</page>
<subsubsectionHeader confidence="0.750609">
4.1.1 Computational complexity
</subsubsectionHeader>
<bodyText confidence="0.999965875">
The sizes of A and B are each O(LT2). The ma-
trix A can be constructed by iterating through the
layers and then iterating: u from 1 to T; v from u+1
to T; and t from u to v + 1. Thus, the time cost for
filling A is O(LT3). For computing the observation
likelihoods in B, the time complexity is O(LT2W),
where W is the size of the vocabulary – by keeping
cumulative lexical counts, we can compute B[u, v]
without iterating from u to v.
Eisenstein and Barzilay (2008) describe a dy-
namic program for linear segmentation with a
space complexity of O(T) and time complexities of
O(T 2) to compute the A matrix and O(TW) to fill
the B matrix.1 Thus, moving to hierarchical seg-
mentation introduces a factor of TL to the complex-
ity of inference.
</bodyText>
<sectionHeader confidence="0.439558" genericHeader="method">
4.1.2 Discussion
</sectionHeader>
<bodyText confidence="0.978271777777778">
Intuitively, efficient inference is possible because
the location of each segment boundary affects the
likelihood of only the adjoining segments at the
same level of the hierarchy, and their “children” at
the lower levels of the hierarchy. Thus, the observa-
tion likelihood at each level decomposes across the
segments of the level. This is due to the left-to-right
nature of segmentation – in general it is not possible
to marginalize the language models and still perform
efficient inference in HMMs. The prior (Section 3.2)
was designed to decompose across segments – if, for
example, p(y) explicitly referenced the total number
of segments, inference would be more difficult.
A simpler inference procedure would be a greedy
approach that makes a fixed decision about the top-
level segmentation, and then applies recursion to
achieve segmentation at the lower levels. The greedy
approach will not be optimal if the best top-level
segmentation leads to unsatisfactory results at the
lower levels, or if the lower levels could help to
disambiguate high-level segmentation. In contrast,
the algorithm presented here maximizes the overall
score across all levels of the segmentation hierarchy.
1The use of dynamic programming for linear topic segmen-
tation goes back at least to (Heinonen, 1998); however, we are
aware of no prior work on dynamic programming for hierarchi-
cal segmentation.
</bodyText>
<subsectionHeader confidence="0.982519">
4.2 Scale-level marginals
</subsectionHeader>
<bodyText confidence="0.999825617647059">
The hidden variable zt represents the level of the
segmentation hierarchy from which the word wt is
drawn. Given language models O, each wt can
be thought of as a draw from a Bayesian mixture
model, with zt as the index of the component that
generates wt. However, as we are marginalizing
the language models, standard mixture model infer-
ence techniques do not apply. One possible solu-
tion would be to instantiate the maximum a posteri-
ori language models after segmenting, but we would
prefer not to have to commit to specific language
models. Collapsed Gibbs sampling (Griffiths and
Steyvers, 2004) is another possibility, but sampling-
based solutions may not be ideal from a performance
standpoint.
Recent papers by Teh et al. (2007) and Sung et
al. (2008) point to an appealing alternative: col-
lapsed variational inference (called latent-state vari-
ational Bayes by Sung et al.). Collapsed variational
inference integrates over the parameters (in this
case, the language models) and computes marginal
distributions for the latent variables, QZ. However,
due to the difficulty of computing the expectation
of the normalizing term, these marginal probabili-
ties are available only in approximation.
More formally, we wish to compute the approx-
imate distribution Qz(z) = HTt Qzt(zt), factoriz-
ing across all latent variables. As is typical in vari-
ational approaches, we fit this distribution by opti-
mizing a lower bound on the data marginal likeli-
hood p(w, z|y) – we condition on the segmentation
y because we are treating it as fixed in this part of
the inference. The lower bound can be optimized by
iteratively setting,
</bodyText>
<equation confidence="0.790495">
Qzt(zt) ∝ exp {hlog P(x, z|y)i∼Qzt 1,
</equation>
<bodyText confidence="0.999937333333333">
indicating the expectation under Qzt for all t&apos; =6 t.
Due to the couplings across z, it is not possible
to compute this expectation directly, so we use the
first-order approximation described in (Sung et al.,
2008). In this approximation, the value Qzt(zt = f)
– which we abbreviate as γtt – takes the form of
the likelihood of the observation wt, given a mod-
ified mixture model. The parameters of the mixture
model are based on the priors and the counts of w
</bodyText>
<page confidence="0.995815">
357
</page>
<bodyText confidence="0.920058">
and γ for all t&apos; =6 t:
</bodyText>
<equation confidence="0.989169857142857">
γt� a βt ˜x�t
� (wt)
W -, (2)
Ei -7-it (2)
˜x�t
� (i) =αt(i) + � γtgδ(wt, = i). (3)
t,7�t
</equation>
<bodyText confidence="0.999993333333333">
The first term in equation 2 is the set of compo-
nent weights βt, which are fixed at 1/L for all `. The
fraction represents the posterior estimate of the lan-
guage models: standard Dirichlet-multinomial con-
jugacy gives a sum of counts plus a Dirichlet prior
(equation 3). Thus, the form of the update is ex-
tremely similar to collapsed Gibbs sampling, except
that we maintain the full distribution over zt rather
than sampling a specific value. The derivation of this
update is beyond the scope of this paper, but is sim-
ilar to the mixture of Bernoullis model presented in
Section 5 of (Sung et al., 2008).
Iterative updates of this form are applied until the
change in the lower bound is less than 10−3. This
procedure appears at step 5a of algorithm 1.
</bodyText>
<subsectionHeader confidence="0.995539">
4.3 Hyperparameter estimation
</subsectionHeader>
<bodyText confidence="0.933265956521739">
The inference procedure defined here includes two
parameters: α, the symmetric Dirichlet prior on the
language models; and d, the expected segment du-
rations. The granularity of segmentation is consid-
ered to be a user-defined characteristic, so there is
no “right answer” for how to set this parameter. We
simply use the oracle segment durations, and pro-
vide the same oracle to the baseline methods where
possible. As discussed in Section 6, this parameter
had little effect on system performance.
The α parameter controls the expected sparsity of
the induced language models; it will be set automat-
ically. Given a segmentation y and hidden-variable
marginals γ, we can maximize p(α,w|y,γ) =
pdcm(w|y, γ, α)p(α) through gradient descent. The
Dirichlet compound multinomial has a tractable gra-
dient, which can be computed using scaled counts,
˜xj =�t:y(zt)
t =j γtjxt (Minka, 2003). The scaled
counts are taken for each segment j across the entire
segmentation hierarchy. The likelihood p(˜x|α) then
has the same form as equation 1, with the xji terms
replaced by ˜xji. The gradient of the log-likelihood
</bodyText>
<figure confidence="0.976207818181818">
Algorithm 1 Complete segmentation inference
1. Input text w; expected durations d.
2. γ INITIALIZE-GAMMA(w)
3. yˆ EQUAL-WIDTH-SEG(w, d)
4. α .1
5. Do
(a) γ ESTIMATE-GAMMA(ˆy, w,γ, α)
(b) α ESTIMATE-ALPHA(ˆy, w,γ)
(c) y SEGMENT(w, γ, α, d)
(d) If y = yˆ then return y
(e) Else yˆ y
</figure>
<bodyText confidence="0.283044">
is thus a sum across segments,
</bodyText>
<equation confidence="0.9942625">
K
d`/dα = W(Ψ(Wα) − Ψ(α))
j
W W
+ Ψ(˜xji + α) − Ψ(Wα + ˜xji).
i i
</equation>
<bodyText confidence="0.9998714">
Here, Ψ indicates the digamma function, which
is the derivative of the log gamma function. The
prior p(α) takes the form of a Gamma distribution
with parameters x(1,1), which has the effect of dis-
couraging large values of α. With these parame-
ters, the gradient of the Gamma distribution with re-
spect to α is negative one. To optimize α, we inter-
pose an epoch of L-BFGS (Liu and Nocedal, 1989)
optimization after maximizing γ (Step 5b of algo-
rithm 1).
</bodyText>
<subsectionHeader confidence="0.991561">
4.4 Combined inference procedure
</subsectionHeader>
<bodyText confidence="0.997156375">
The final inference procedure alternates between up-
dating the marginals γ, the Dirichlet prior α, and the
MAP segmentation ˆy. Since the procedure makes
hard decisions on α and the segmentations y, it
can be thought of as a form of Viterbi expectation-
maximization (EM). When a repeated segmentation
is encountered, the procedure terminates. Initializa-
tion involves constructing a segmentation yˆ in which
each level is segmented uniformly, based on the ex-
pected segment duration dt. The hidden variable
marginals γ are initialized randomly. While there
is no guarantee of finding the global maximum, lit-
tle sensitivity to the random initialization of γ was
observed in preliminary experiments.
The dynamic program described in this section
achieves polynomial time complexity, but O(LT3)
</bodyText>
<page confidence="0.996871">
358
</page>
<bodyText confidence="0.999954181818182">
can still be slow when T is the number of word to-
kens in a large document such as a textbook. For
this reason, we only permit segment boundaries to
be placed at gold-standard sentence boundaries. The
only change to the algorithm is that the tables A
and B need contain only cells for each sentence
rather than for each word token – hidden variable
marginals are still computed for each word token.
Implemented in Java, the algorithm runs in roughly
five minutes for a document with 1000 sentences on
a dual core 2.4 GHz machine.
</bodyText>
<sectionHeader confidence="0.991308" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999821967741935">
Corpora The dataset for evaluation is drawn from
a medical textbook (Walker et al., 1990).2 The text
contains 17083 sentences, segmented hierarchically
into twelve high-level parts, 150 chapters, and 520
sub-chapter sections. Evaluation is performed sep-
arately on each of the twelve parts, with the task of
correctly identifying the chapter and section bound-
aries. Eisenstein and Barzilay (2008) use the same
dataset to evaluate linear topic segmentation, though
they evaluated only at the level of sections, given
gold standard chapter boundaries.
Practical applications of topic segmentation typ-
ically relate to more informal documents such as
blogs or speech transcripts (Hsueh et al., 2006), as
formal texts such as books already contain segmen-
tation markings provided by the author. The premise
of this evaluation is that textbook corpora provide a
reasonable proxy for performance on less structured
data. However, further clarification of this point is
an important direction for future research.
Metrics All experiments are evaluated in terms
of the commonly-used Pk and WindowDiff met-
rics (Pevzner and Hearst, 2002). Both metrics pass a
window through the document, and assess whether
the sentences on the edges of the window are prop-
erly segmented with respect to each other. Win-
dowDiff is stricter in that it requires that the number
of intervening segments between the two sentences
be identical in the hypothesized and the reference
segmentations, while Pk only asks whether the two
sentences are in the same segment or not. This eval-
</bodyText>
<footnote confidence="0.948599">
2The full text of this book is available for free download at
http://onlinebooks.library.upenn.edu.
</footnote>
<bodyText confidence="0.999699723404256">
uation uses source code provided by Malioutov and
Barzilay (2006).
Experimental system The joint hierarchical
Bayesian model described in this paper is called
HIERBAYES. It performs a three-level hierarchical
segmentation, in which the lowest level is for sub-
chapter sections, the middle level is for chapters, and
the top level spans the entire part. This top-level has
the effect of limiting the influence of words that are
common throughout the document.
Baseline systems As noted in Section 2, there is
little related work on unsupervised hierarchical seg-
mentation. However, a straightforward baseline is
a greedy approach: first segment at the top level,
and then recursively feed each top-level segment to
the segmenter again. Any linear segmenter can be
plugged into this baseline as a “black box.”
To isolate the contribution of joint inference, the
greedy framework can be combined with a one-level
version of the Bayesian segmentation algorithm de-
scribed here. This is equivalent to BAYESSEG,
which achieved the best reported performance on the
linear segmentation of this same dataset (Eisenstein
and Barzilay, 2008). The hierarchical segmenter
built by placing BAYESSEG in a greedy algorithm
is called GREEDY-BAYES.
To identify the contribution of the Bayesian
segmentation framework, we can plug in alter-
native linear segmenters. Two frequently-cited
systems are LCSEG (Galley et al., 2003) and
TEXTSEG (Utiyama and Isahara, 2001). LC-
SEG optimizes a metric of lexical cohesion based
on lexical chains. TEXTSEG employs a probabilis-
tic segmentation objective that is similar to ours,
but uses maximum a posteriori estimates of the lan-
guage models, rather than marginalizing them out.
Other key differences are that they set α = 1, and
use a minimum description length criterion to deter-
mine segmentation granularity. Both of these base-
lines were run using their default parametrization.
Finally, as a minimal baseline, UNIFORM pro-
duces a hierarchical segmentation with the ground
truth number of segments per level and uniform du-
ration per segment at each level.
Preprocessing The Porter (1980) stemming algo-
rithm is applied to group equivalent lexical items. A
set of stop-words is also removed, using the same
</bodyText>
<page confidence="0.995136">
359
</page>
<table confidence="0.998600428571429">
chapter WD section WD average
# segs Pk # segs Pk Pk WD
HIERBAYES 5.0 .248 .255 8.5 .312 .351 .280 .303
GREEDY-BAYES 19.0 .260 .372 19.5 .275 .340 .268 .356
GREEDY-LCSEG 7.8 .256 .286 52.2 .351 .455 .304 .371
GREEDY-TEXTSEG 11.5 .251 .277 88.4 .473 .630 .362 .454
UNIFORM 12.5 .487 .491 43.3 .505 .551 .496 .521
</table>
<tableCaption confidence="0.985369666666667">
Table 1: Segmentation accuracy and granularity. Both the Pk and WindowDiff (WD) metrics are penalties, so lower
scores are better. The # segs columns indicate the average number of segments at each level; the gold standard
segmentation granularity is given in the UNIFORM row, which obtains this granularity by construction.
</tableCaption>
<bodyText confidence="0.85434">
list originally employed by several competitive sys-
tems (Utiyama and Isahara, 2001).
</bodyText>
<sectionHeader confidence="0.999958" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999760282051282">
Table 1 presents performance results for the joint
hierarchical segmenter and the three greedy base-
lines. As shown in the table, the hierarchical system
achieves the top overall performance on the harsher
WindowDiff metric. In general, the greedy seg-
menters each perform well at one of the two levels
and poorly at the other level. The joint hierarchical
inference of HIERBAYES enables it to achieve bal-
anced performance at the two levels.
The GREEDY-BAYES system achieves a slightly
better average Pk than HIERBAYES, but has a very
large gap between its Pk and WindowDiff scores.
The Pk metric requires only that the system cor-
rectly classify whether two points are in the same
or different segments, while the WindowDiff metric
insists that the exact number of interposing segments
be identified correctly. Thus, the generation of spu-
rious short segments may explain the gap between
the metrics.
LCSEG and TEXTSEG use heuristics to deter-
mine segmentation granularity; even though these
methods did not score well in terms of segmentation
accuracy, they were generally closer to the correct
granularity. In the Bayesian methods, granularity
is enforced by the Markov prior described in Sec-
tion 3.2. This prior was particularly ineffective for
GREEDY-BAYES, which gave nearly the same num-
ber of segments at both levels, despite the different
settings of the expected duration parameter d.
The Dirichlet prior α was selected automatically,
but informal experiments with manual settings sug-
gest that this parameter exerts a stronger influence
on segmentation granularity. Low settings reflect an
expectation of sparse lexical counts and thus encour-
age short segments, while high settings reflect an ex-
pectation of evenly-distributed counts and thus lead
to long segments. Further investigation is needed
on how best to control segmentation granularity in a
Bayesian setting.
</bodyText>
<sectionHeader confidence="0.999415" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.959814576923077">
While it is widely agreed that language often dis-
plays hierarchical topic structure (Grosz, 1977),
there have been relatively few attempts to extract
such structure automatically. This paper shows
that the lexical features that have been successfully
exploited in linear segmentation can also be used
to extract a hierarchical segmentation, due to the
phenomenon of multi-scale lexical cohesion. The
Bayesian methodology offers a principled proba-
bilistic formalization of multi-scale cohesion, yield-
ing an accurate and fast segmentation algorithm with
a minimal set of tunable parameters.
It is interesting to consider how multi-scale seg-
mentation might be extended to finer-grain seg-
ments, such as paragraphs. The lexical counts at the
paragraph level will be sparse, so lexical cohesion
alone is unlikely to be sufficient. Rather it may be
necessary to model discourse connectors and lexical
semantics explicitly. The development of more com-
prehensive Bayesian models for discourse structure
seems an exciting direction for future research.
Acknowledgments Thanks to Michel Galley, Igor
Malioutov, and Masao Utiyama for making their topic
segmentation systems publicly available, and to the
anonymous reviewers for useful feedback. This research
is supported by the Beckman Postdoctoral Fellowship.
</bodyText>
<page confidence="0.996439">
360
</page>
<sectionHeader confidence="0.99384" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999777905405405">
David M. Blei and Pedro J. Moreno. 2001. Topic seg-
mentation with an aspect hidden markov model. In
SIGIR, pages 343–348.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP.
Micha Elsner and Eugene Charniak. 2008. You Talk-
ing to Me? A Corpus and Algorithm for Conversation
Disentanglement. In Proceedings of ACL.
Michel Galley, Katheen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. pages 562–569.
T.L. Griffiths and M. Steyvers. 2004. Finding scientific
topics.
Barbara Grosz and Candace Sidner. 1986. Attention,
intentions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175–204.
Barbara Grosz. 1977. The representation and use of fo-
cus in dialogue understanding. Technical Report 151,
Artificial Intelligence Center, SRI International.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of ACL, pages 9–16.
Oskari Heinonen. 1998. Optimal Multi-Paragraph Text
Segmentation by Dynamic Programming. In Proceed-
ings of ACL, pages 1484–1486.
P.Y. Hsueh, J. Moore, and S. Renals. 2006. Automatic
segmentation of multiparty dialogue. In Proccedings
of EACL.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503–528.
R.E. Madsen, D. Kauchak, and C. Elkan. 2005. Model-
ing word burstiness using the Dirichlet distribution. In
Proceedings of ICML.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL, pages 25–32.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8:243–281.
Thomas P. Minka. 2003. Estimating a dirichlet distri-
bution. Technical report, Massachusetts Institute of
Technology.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19–36.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14:130–137.
M. Purver, T.L. Griffiths, K.P. K¨ording, and J.B. Tenen-
baum. 2006. Unsupervised topic modelling for multi-
party spoken discourse. In Proceedings of ACL, pages
17–24.
Jaemo Sung, Zoubin Ghahramani, and Sung-Yang Bang.
2008. Latent-space variational bayes. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
30(12):2236–2242, Dec.
Y.W. Teh, D. Newman, and M. Welling. 2007. A Col-
lapsed Variational Bayesian Inference Algorithm for
Latent Dirichlet Allocation. In NIPS, volume 19, page
1353.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of ACL, pages 491–498.
H. Kenneth Walker, W. Dallas Hall, and J. Willis Hurst,
editors. 1990. Clinical Methods: The History, Physi-
cal, and Laboratory Examinations. Butterworths.
Y. Yaari. 1997. Segmentation of Expository Texts by
Hierarchical Agglomerative Clustering. In Recent Ad-
vances in Natural Language Processing.
</reference>
<page confidence="0.998698">
361
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.827691">
<title confidence="0.999951">Hierarchical Text Segmentation from Multi-Scale Lexical Cohesion</title>
<author confidence="0.995041">Jacob</author>
<affiliation confidence="0.99967">Beckman Institute for Advanced Science and University of</affiliation>
<address confidence="0.851048">Urbana, IL</address>
<email confidence="0.999457">jacobe@illinois.edu</email>
<abstract confidence="0.998355944444444">This paper presents a novel unsupervised method for hierarchical topic segmentation. Lexical cohesion – the workhorse of unsupervised linear segmentation – is treated as a multi-scale phenomenon, and formalized in a Bayesian setting. Each word token is modeled as a draw from a pyramid of latent topic models, where the structure of the pyramid is constrained to induce a hierarchical segmentation. Inference takes the form of a coordinate-ascent algorithm, iterating between two steps: a novel dynamic program for obtaining the globally-optimal hierarchical segmentation, and collapsed variational Bayesian inference over the hidden variables. The resulting system is fast and accurate, and compares well against heuristic alternatives.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Pedro J Moreno</author>
</authors>
<title>Topic segmentation with an aspect hidden markov model.</title>
<date>2001</date>
<booktitle>In SIGIR,</booktitle>
<pages>343--348</pages>
<contexts>
<context position="4678" citStr="Blei and Moreno (2001)" startWordPosition="708" endWordPosition="711"> variational Bayesian inference is then used to update the marginals. This approach achieves high quality segmentation on multiple levels of the topic hierarchy. Source code is available at http://people. csail.mit.edu/jacobe/naacl09.html. 2 Related Work The use of lexical cohesion (Halliday and Hasan, 1976) in unsupervised topic segmentation dates back to Hearst’s seminal TEXTTILING system (1994). Lexical cohesion was placed in a probabilistic (though not Bayesian) framework by Utiyama and Isahara (2001). The application of Bayesian topic models to text segmentation was investigated first by Blei and Moreno (2001) and later by Purver et al. (2006), using HMM-like graphical models for linear segmentation. Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. All of these papers consider only linear topic segmentation; we introduce multi-scale lexical cohesion, which posits that the distribution of some words changes slowly with high-level topics, while others change rapidly with lower-level subtopics. This gives a principled mechan</context>
</contexts>
<marker>Blei, Moreno, 2001</marker>
<rawString>David M. Blei and Pedro J. Moreno. 2001. Topic segmentation with an aspect hidden markov model. In SIGIR, pages 343–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="2823" citStr="Blei et al., 2003" startWordPosition="428" endWordPosition="431">ogram is underway to extend existing lines into the outer neighborhoods. Track length is expected to reach 89 km... The two sections are both part of a high-level segment on transportation. Words in bold are characteristic of the subsections (buses and trains, respectively), and do not occur elsewhere in the transportation section; words in italics occur throughout the high-level section, but not elsewhere in the article. This paper shows how multi-scale cohesion can be captured in a Bayesian generative model and exploited for unsupervised hierarchical topic segmentation. Latent topic models (Blei et al., 2003) provide a powerful statistical apparatus with which to study discourse structure. A consistent theme is the treatment of individual words as draws from multinomial language models indexed by a hidden “topic” associated with the word. In latent Dirichlet allocation (LDA) and related models, the hidden topic for each word is unconstrained and unrelated to the hidden topic of neighboring words (given the parameters). In this paper, the latent topics are constrained to produce a hierarchical segmentation structure, as shown in Figure 1. 353 Human Language Technologies: The 2009 Annual Conference </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1446" citStr="Eisenstein and Barzilay, 2008" startWordPosition="200" endWordPosition="203">erence over the hidden variables. The resulting system is fast and accurate, and compares well against heuristic alternatives. 1 Introduction Recovering structural organization from unformatted texts or transcripts is a fundamental problem in natural language processing, with applications to classroom lectures, meeting transcripts, and chatroom logs. In the unsupervised setting, a variety of successful systems have leveraged lexical cohesion (Halliday and Hasan, 1976) – the idea that topically-coherent segments display consistent lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). However, such systems almost invariably focus on linear segmentation, while it is widely believed that discourse displays a hierarchical structure (Grosz and Sidner, 1986). This paper introduces the concept of multi-scale lexical cohesion, and leverages this idea in a Bayesian generative model for hierarchical topic segmentation. The idea of multi-scale cohesion is illustrated by the following two examples, drawn from the Wikipedia entry for the city of Buenos Aires. There are over 150 city bus lines called Colectivos ... Colectivos in Buenos Aires do not have a fixed timetable, but run from</context>
<context position="4801" citStr="Eisenstein and Barzilay (2008)" startWordPosition="726" endWordPosition="729">ion on multiple levels of the topic hierarchy. Source code is available at http://people. csail.mit.edu/jacobe/naacl09.html. 2 Related Work The use of lexical cohesion (Halliday and Hasan, 1976) in unsupervised topic segmentation dates back to Hearst’s seminal TEXTTILING system (1994). Lexical cohesion was placed in a probabilistic (though not Bayesian) framework by Utiyama and Isahara (2001). The application of Bayesian topic models to text segmentation was investigated first by Blei and Moreno (2001) and later by Purver et al. (2006), using HMM-like graphical models for linear segmentation. Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. All of these papers consider only linear topic segmentation; we introduce multi-scale lexical cohesion, which posits that the distribution of some words changes slowly with high-level topics, while others change rapidly with lower-level subtopics. This gives a principled mechanism to model hierarchical topic segmentation. The literature on hierarchical topic segmentation is relatively sparse. Hsueh</context>
<context position="15112" citStr="Eisenstein and Barzilay (2008)" startWordPosition="2492" endWordPosition="2495">The third term recursively segments the interval from u to t at the same level `. The boundary case A(`)[u, u] = 0. �xtγt` v E t=u 356 4.1.1 Computational complexity The sizes of A and B are each O(LT2). The matrix A can be constructed by iterating through the layers and then iterating: u from 1 to T; v from u+1 to T; and t from u to v + 1. Thus, the time cost for filling A is O(LT3). For computing the observation likelihoods in B, the time complexity is O(LT2W), where W is the size of the vocabulary – by keeping cumulative lexical counts, we can compute B[u, v] without iterating from u to v. Eisenstein and Barzilay (2008) describe a dynamic program for linear segmentation with a space complexity of O(T) and time complexities of O(T 2) to compute the A matrix and O(TW) to fill the B matrix.1 Thus, moving to hierarchical segmentation introduces a factor of TL to the complexity of inference. 4.1.2 Discussion Intuitively, efficient inference is possible because the location of each segment boundary affects the likelihood of only the adjoining segments at the same level of the hierarchy, and their “children” at the lower levels of the hierarchy. Thus, the observation likelihood at each level decomposes across the s</context>
<context position="23532" citStr="Eisenstein and Barzilay (2008)" startWordPosition="3905" endWordPosition="3908">ach word token – hidden variable marginals are still computed for each word token. Implemented in Java, the algorithm runs in roughly five minutes for a document with 1000 sentences on a dual core 2.4 GHz machine. 5 Experimental Setup Corpora The dataset for evaluation is drawn from a medical textbook (Walker et al., 1990).2 The text contains 17083 sentences, segmented hierarchically into twelve high-level parts, 150 chapters, and 520 sub-chapter sections. Evaluation is performed separately on each of the twelve parts, with the task of correctly identifying the chapter and section boundaries. Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries. Practical applications of topic segmentation typically relate to more informal documents such as blogs or speech transcripts (Hsueh et al., 2006), as formal texts such as books already contain segmentation markings provided by the author. The premise of this evaluation is that textbook corpora provide a reasonable proxy for performance on less structured data. However, further clarification of this point is an important direction for future r</context>
<context position="25911" citStr="Eisenstein and Barzilay, 2008" startWordPosition="4275" endWordPosition="4278"> there is little related work on unsupervised hierarchical segmentation. However, a straightforward baseline is a greedy approach: first segment at the top level, and then recursively feed each top-level segment to the segmenter again. Any linear segmenter can be plugged into this baseline as a “black box.” To isolate the contribution of joint inference, the greedy framework can be combined with a one-level version of the Bayesian segmentation algorithm described here. This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008). The hierarchical segmenter built by placing BAYESSEG in a greedy algorithm is called GREEDY-BAYES. To identify the contribution of the Bayesian segmentation framework, we can plug in alternative linear segmenters. Two frequently-cited systems are LCSEG (Galley et al., 2003) and TEXTSEG (Utiyama and Isahara, 2001). LCSEG optimizes a metric of lexical cohesion based on lexical chains. TEXTSEG employs a probabilistic segmentation objective that is similar to ours, but uses maximum a posteriori estimates of the language models, rather than marginalizing them out. Other key differences are that t</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6549" citStr="Elsner and Charniak (2008)" startWordPosition="979" endWordPosition="982">must be hand-tuned. These problems can be avoided by working in a Bayesian probabilistic framework. We note two orthogonal but related approaches to extracting nonlinear discourse structures from text. Rhetorical structure theory posits a hierarchical structure of discourse relations between spans of text (Mann and Thompson, 1988). This structure is richer than hierarchical topic segmentation, and the base level of analysis is typically more fine-grained – at the level of individual clauses. Unsupervised approaches based purely on cohesion are unlikely to succeed at this level of granularity. Elsner and Charniak (2008) propose the task of conversation disentanglement from internet chatroom logs. Unlike hierarchical topic segmentation, conversational threads may be disjoint, with unrelated threads interposed between two utterances from the same thread. Elsner and Charniak present a supervised approach to this problem, but the development of cohesion-based unsupervised methods is an interesting possibility for future work. 3 Model Topic modeling is premised on a generative framework in which each word wt is drawn from a multinomial Byt, where yt is a hidden topic indexing the language model that generates wt.</context>
</contexts>
<marker>Elsner, Charniak, 2008</marker>
<rawString>Micha Elsner and Eugene Charniak. 2008. You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Katheen McKeown</author>
</authors>
<title>Eric Fosler-Lussier, and Hongyan Jing.</title>
<date>2003</date>
<pages>562--569</pages>
<marker>Galley, McKeown, 2003</marker>
<rawString>Michel Galley, Katheen McKeown, Eric Fosler-Lussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. pages 562–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<contexts>
<context position="17413" citStr="Griffiths and Steyvers, 2004" startWordPosition="2864" endWordPosition="2867">on. 4.2 Scale-level marginals The hidden variable zt represents the level of the segmentation hierarchy from which the word wt is drawn. Given language models O, each wt can be thought of as a draw from a Bayesian mixture model, with zt as the index of the component that generates wt. However, as we are marginalizing the language models, standard mixture model inference techniques do not apply. One possible solution would be to instantiate the maximum a posteriori language models after segmenting, but we would prefer not to have to commit to specific language models. Collapsed Gibbs sampling (Griffiths and Steyvers, 2004) is another possibility, but samplingbased solutions may not be ideal from a performance standpoint. Recent papers by Teh et al. (2007) and Sung et al. (2008) point to an appealing alternative: collapsed variational inference (called latent-state variational Bayes by Sung et al.). Collapsed variational inference integrates over the parameters (in this case, the language models) and computes marginal distributions for the latent variables, QZ. However, due to the difficulty of computing the expectation of the normalizing term, these marginal probabilities are available only in approximation. Mo</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T.L. Griffiths and M. Steyvers. 2004. Finding scientific topics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Candace Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="1619" citStr="Grosz and Sidner, 1986" startWordPosition="226" endWordPosition="229">m unformatted texts or transcripts is a fundamental problem in natural language processing, with applications to classroom lectures, meeting transcripts, and chatroom logs. In the unsupervised setting, a variety of successful systems have leveraged lexical cohesion (Halliday and Hasan, 1976) – the idea that topically-coherent segments display consistent lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). However, such systems almost invariably focus on linear segmentation, while it is widely believed that discourse displays a hierarchical structure (Grosz and Sidner, 1986). This paper introduces the concept of multi-scale lexical cohesion, and leverages this idea in a Bayesian generative model for hierarchical topic segmentation. The idea of multi-scale cohesion is illustrated by the following two examples, drawn from the Wikipedia entry for the city of Buenos Aires. There are over 150 city bus lines called Colectivos ... Colectivos in Buenos Aires do not have a fixed timetable, but run from 4 to several per hour, depending on the bus line and time of the day. The Buenos Aires metro has six lines, 74 stations, and 52.3 km of track. An expansion program is under</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara Grosz and Candace Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
</authors>
<title>The representation and use of focus in dialogue understanding.</title>
<date>1977</date>
<tech>Technical Report 151,</tech>
<institution>Artificial Intelligence Center, SRI International.</institution>
<contexts>
<context position="29726" citStr="Grosz, 1977" startWordPosition="4882" endWordPosition="4883">tion parameter d. The Dirichlet prior α was selected automatically, but informal experiments with manual settings suggest that this parameter exerts a stronger influence on segmentation granularity. Low settings reflect an expectation of sparse lexical counts and thus encourage short segments, while high settings reflect an expectation of evenly-distributed counts and thus lead to long segments. Further investigation is needed on how best to control segmentation granularity in a Bayesian setting. 7 Discussion While it is widely agreed that language often displays hierarchical topic structure (Grosz, 1977), there have been relatively few attempts to extract such structure automatically. This paper shows that the lexical features that have been successfully exploited in linear segmentation can also be used to extract a hierarchical segmentation, due to the phenomenon of multi-scale lexical cohesion. The Bayesian methodology offers a principled probabilistic formalization of multi-scale cohesion, yielding an accurate and fast segmentation algorithm with a minimal set of tunable parameters. It is interesting to consider how multi-scale segmentation might be extended to finer-grain segments, such a</context>
</contexts>
<marker>Grosz, 1977</marker>
<rawString>Barbara Grosz. 1977. The representation and use of focus in dialogue understanding. Technical Report 151, Artificial Intelligence Center, SRI International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<note>Cohesion in English. Longman.</note>
<contexts>
<context position="1288" citStr="Halliday and Hasan, 1976" startWordPosition="178" endWordPosition="181">iterating between two steps: a novel dynamic program for obtaining the globally-optimal hierarchical segmentation, and collapsed variational Bayesian inference over the hidden variables. The resulting system is fast and accurate, and compares well against heuristic alternatives. 1 Introduction Recovering structural organization from unformatted texts or transcripts is a fundamental problem in natural language processing, with applications to classroom lectures, meeting transcripts, and chatroom logs. In the unsupervised setting, a variety of successful systems have leveraged lexical cohesion (Halliday and Hasan, 1976) – the idea that topically-coherent segments display consistent lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). However, such systems almost invariably focus on linear segmentation, while it is widely believed that discourse displays a hierarchical structure (Grosz and Sidner, 1986). This paper introduces the concept of multi-scale lexical cohesion, and leverages this idea in a Bayesian generative model for hierarchical topic segmentation. The idea of multi-scale cohesion is illustrated by the following two examples, drawn from the Wikipedia entr</context>
<context position="4365" citStr="Halliday and Hasan, 1976" startWordPosition="663" endWordPosition="666">language models to be analytically marginalized. The remaining hidden variables are the scale-level assignments for each word token. Given marginal distributions over these variables, it is possible to search the entire space of hierarchical segmentations in polynomial time, using a novel dynamic program. Collapsed variational Bayesian inference is then used to update the marginals. This approach achieves high quality segmentation on multiple levels of the topic hierarchy. Source code is available at http://people. csail.mit.edu/jacobe/naacl09.html. 2 Related Work The use of lexical cohesion (Halliday and Hasan, 1976) in unsupervised topic segmentation dates back to Hearst’s seminal TEXTTILING system (1994). Lexical cohesion was placed in a probabilistic (though not Bayesian) framework by Utiyama and Isahara (2001). The application of Bayesian topic models to text segmentation was investigated first by Blei and Moreno (2001) and later by Purver et al. (2006), using HMM-like graphical models for linear segmentation. Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed direc</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="1387" citStr="Hearst, 1994" startWordPosition="193" endWordPosition="194">n, and collapsed variational Bayesian inference over the hidden variables. The resulting system is fast and accurate, and compares well against heuristic alternatives. 1 Introduction Recovering structural organization from unformatted texts or transcripts is a fundamental problem in natural language processing, with applications to classroom lectures, meeting transcripts, and chatroom logs. In the unsupervised setting, a variety of successful systems have leveraged lexical cohesion (Halliday and Hasan, 1976) – the idea that topically-coherent segments display consistent lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). However, such systems almost invariably focus on linear segmentation, while it is widely believed that discourse displays a hierarchical structure (Grosz and Sidner, 1986). This paper introduces the concept of multi-scale lexical cohesion, and leverages this idea in a Bayesian generative model for hierarchical topic segmentation. The idea of multi-scale cohesion is illustrated by the following two examples, drawn from the Wikipedia entry for the city of Buenos Aires. There are over 150 city bus lines called Colectivos ... Colectivos </context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Marti A. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proceedings of ACL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oskari Heinonen</author>
</authors>
<title>Optimal Multi-Paragraph Text Segmentation by Dynamic Programming.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1484--1486</pages>
<contexts>
<context position="16693" citStr="Heinonen, 1998" startWordPosition="2747" endWordPosition="2748">t. A simpler inference procedure would be a greedy approach that makes a fixed decision about the toplevel segmentation, and then applies recursion to achieve segmentation at the lower levels. The greedy approach will not be optimal if the best top-level segmentation leads to unsatisfactory results at the lower levels, or if the lower levels could help to disambiguate high-level segmentation. In contrast, the algorithm presented here maximizes the overall score across all levels of the segmentation hierarchy. 1The use of dynamic programming for linear topic segmentation goes back at least to (Heinonen, 1998); however, we are aware of no prior work on dynamic programming for hierarchical segmentation. 4.2 Scale-level marginals The hidden variable zt represents the level of the segmentation hierarchy from which the word wt is drawn. Given language models O, each wt can be thought of as a draw from a Bayesian mixture model, with zt as the index of the component that generates wt. However, as we are marginalizing the language models, standard mixture model inference techniques do not apply. One possible solution would be to instantiate the maximum a posteriori language models after segmenting, but we</context>
</contexts>
<marker>Heinonen, 1998</marker>
<rawString>Oskari Heinonen. 1998. Optimal Multi-Paragraph Text Segmentation by Dynamic Programming. In Proceedings of ACL, pages 1484–1486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Y Hsueh</author>
<author>J Moore</author>
<author>S Renals</author>
</authors>
<title>Automatic segmentation of multiparty dialogue.</title>
<date>2006</date>
<booktitle>In Proccedings of EACL.</booktitle>
<contexts>
<context position="5415" citStr="Hsueh et al. (2006)" startWordPosition="813" endWordPosition="816">2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. All of these papers consider only linear topic segmentation; we introduce multi-scale lexical cohesion, which posits that the distribution of some words changes slowly with high-level topics, while others change rapidly with lower-level subtopics. This gives a principled mechanism to model hierarchical topic segmentation. The literature on hierarchical topic segmentation is relatively sparse. Hsueh et al. (2006) describe a supervised approach that trains separate classifiers for topic and sub-topic segmentation; more relevant for the current work is the unsupervised method of Yaari (1997). As in TEXTTILING, cohesion is measured using cosine similarity, and agglomerative clustering is used to induce a dendrogram over paragraphs; the dendrogram is transformed into a hierarchical segmentation using a heuristic algorithm. Such heuristic approaches are typically brittle, as they include a number of parameters that must be hand-tuned. These problems can be avoided by working in a Bayesian probabilistic fra</context>
<context position="23831" citStr="Hsueh et al., 2006" startWordPosition="3950" endWordPosition="3953"> et al., 1990).2 The text contains 17083 sentences, segmented hierarchically into twelve high-level parts, 150 chapters, and 520 sub-chapter sections. Evaluation is performed separately on each of the twelve parts, with the task of correctly identifying the chapter and section boundaries. Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries. Practical applications of topic segmentation typically relate to more informal documents such as blogs or speech transcripts (Hsueh et al., 2006), as formal texts such as books already contain segmentation markings provided by the author. The premise of this evaluation is that textbook corpora provide a reasonable proxy for performance on less structured data. However, further clarification of this point is an important direction for future research. Metrics All experiments are evaluated in terms of the commonly-used Pk and WindowDiff metrics (Pevzner and Hearst, 2002). Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other. WindowDi</context>
</contexts>
<marker>Hsueh, Moore, Renals, 2006</marker>
<rawString>P.Y. Hsueh, J. Moore, and S. Renals. 2006. Automatic segmentation of multiparty dialogue. In Proccedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>45--503</pages>
<contexts>
<context position="21703" citStr="Liu and Nocedal, 1989" startWordPosition="3611" endWordPosition="3614">o (a) γ ESTIMATE-GAMMA(ˆy, w,γ, α) (b) α ESTIMATE-ALPHA(ˆy, w,γ) (c) y SEGMENT(w, γ, α, d) (d) If y = yˆ then return y (e) Else yˆ y is thus a sum across segments, K d`/dα = W(Ψ(Wα) − Ψ(α)) j W W + Ψ(˜xji + α) − Ψ(Wα + ˜xji). i i Here, Ψ indicates the digamma function, which is the derivative of the log gamma function. The prior p(α) takes the form of a Gamma distribution with parameters x(1,1), which has the effect of discouraging large values of α. With these parameters, the gradient of the Gamma distribution with respect to α is negative one. To optimize α, we interpose an epoch of L-BFGS (Liu and Nocedal, 1989) optimization after maximizing γ (Step 5b of algorithm 1). 4.4 Combined inference procedure The final inference procedure alternates between updating the marginals γ, the Dirichlet prior α, and the MAP segmentation ˆy. Since the procedure makes hard decisions on α and the segmentations y, it can be thought of as a form of Viterbi expectationmaximization (EM). When a repeated segmentation is encountered, the procedure terminates. Initialization involves constructing a segmentation yˆ in which each level is segmented uniformly, based on the expected segment duration dt. The hidden variable margi</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45:503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Madsen</author>
<author>D Kauchak</author>
<author>C Elkan</author>
</authors>
<title>Modeling word burstiness using the Dirichlet distribution.</title>
<date>2005</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="10025" citStr="Madsen et al., 2005" startWordPosition="1573" endWordPosition="1576"> language model θj. The language models in turn are drawn from symmetric Dirichlet distributions with parameter α. The number of language models is written K; the number of words is W; the length of the document is T; and the depth of the hierarchy is L. For hierarchical segmentation, the vector yt indicates the segment index of t at each level of the topic hierarchy; the specific level of the hierarchy responsible for wt is given by the hidden variable zt. Thus, yt is the index of the language model that gener(zt) ates wt. Here, pdcm indicates the Dirichlet compound multinomial distribution (Madsen et al., 2005), which is the closed form solution to the integral over language models. Also known as the multivariate Polya distribution, the probability density function can be computed exactly as a ratio of gamma functions. Here we use a symmetric Dirichlet prior α, though asymmetric priors can easily be applied. Thus far we have treated the hidden variables z as observed. In fact we will compute approximate marginal probabilities Qzt(zt), written γte � Qzt(zt = `). Writing (x)Qz for the expectation of x under distribution Qz, we approximate, (pdcm(xj; α))Qz ^ pdcm((xj)Qz; α) L E δ(wt = i)δ(y(e) t = j)γt</context>
</contexts>
<marker>Madsen, Kauchak, Elkan, 2005</marker>
<rawString>R.E. Madsen, D. Kauchak, and C. Elkan. 2005. Modeling word burstiness using the Dirichlet distribution. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="24849" citStr="Malioutov and Barzilay (2006)" startWordPosition="4111" endWordPosition="4114">owDiff metrics (Pevzner and Hearst, 2002). Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other. WindowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while Pk only asks whether the two sentences are in the same segment or not. This eval2The full text of this book is available for free download at http://onlinebooks.library.upenn.edu. uation uses source code provided by Malioutov and Barzilay (2006). Experimental system The joint hierarchical Bayesian model described in this paper is called HIERBAYES. It performs a three-level hierarchical segmentation, in which the lowest level is for subchapter sections, the middle level is for chapters, and the top level spans the entire part. This top-level has the effect of limiting the influence of words that are common throughout the document. Baseline systems As noted in Section 2, there is little related work on unsupervised hierarchical segmentation. However, a straightforward baseline is a greedy approach: first segment at the top level, and t</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Igor Malioutov and Regina Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In Proceedings of ACL, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8:243–281.</tech>
<contexts>
<context position="6255" citStr="Mann and Thompson, 1988" startWordPosition="935" endWordPosition="938">ured using cosine similarity, and agglomerative clustering is used to induce a dendrogram over paragraphs; the dendrogram is transformed into a hierarchical segmentation using a heuristic algorithm. Such heuristic approaches are typically brittle, as they include a number of parameters that must be hand-tuned. These problems can be avoided by working in a Bayesian probabilistic framework. We note two orthogonal but related approaches to extracting nonlinear discourse structures from text. Rhetorical structure theory posits a hierarchical structure of discourse relations between spans of text (Mann and Thompson, 1988). This structure is richer than hierarchical topic segmentation, and the base level of analysis is typically more fine-grained – at the level of individual clauses. Unsupervised approaches based purely on cohesion are unlikely to succeed at this level of granularity. Elsner and Charniak (2008) propose the task of conversation disentanglement from internet chatroom logs. Unlike hierarchical topic segmentation, conversational threads may be disjoint, with unrelated threads interposed between two utterances from the same thread. Elsner and Charniak present a supervised approach to this problem, b</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8:243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas P Minka</author>
</authors>
<title>Estimating a dirichlet distribution.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="20709" citStr="Minka, 2003" startWordPosition="3429" endWordPosition="3430">er” for how to set this parameter. We simply use the oracle segment durations, and provide the same oracle to the baseline methods where possible. As discussed in Section 6, this parameter had little effect on system performance. The α parameter controls the expected sparsity of the induced language models; it will be set automatically. Given a segmentation y and hidden-variable marginals γ, we can maximize p(α,w|y,γ) = pdcm(w|y, γ, α)p(α) through gradient descent. The Dirichlet compound multinomial has a tractable gradient, which can be computed using scaled counts, ˜xj =�t:y(zt) t =j γtjxt (Minka, 2003). The scaled counts are taken for each segment j across the entire segmentation hierarchy. The likelihood p(˜x|α) then has the same form as equation 1, with the xji terms replaced by ˜xji. The gradient of the log-likelihood Algorithm 1 Complete segmentation inference 1. Input text w; expected durations d. 2. γ INITIALIZE-GAMMA(w) 3. yˆ EQUAL-WIDTH-SEG(w, d) 4. α .1 5. Do (a) γ ESTIMATE-GAMMA(ˆy, w,γ, α) (b) α ESTIMATE-ALPHA(ˆy, w,γ) (c) y SEGMENT(w, γ, α, d) (d) If y = yˆ then return y (e) Else yˆ y is thus a sum across segments, K d`/dα = W(Ψ(Wα) − Ψ(α)) j W W + Ψ(˜xji + α) − Ψ(Wα + ˜xji). i </context>
</contexts>
<marker>Minka, 2003</marker>
<rawString>Thomas P. Minka. 2003. Estimating a dirichlet distribution. Technical report, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti A Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="24261" citStr="Pevzner and Hearst, 2002" startWordPosition="4016" endWordPosition="4019">sections, given gold standard chapter boundaries. Practical applications of topic segmentation typically relate to more informal documents such as blogs or speech transcripts (Hsueh et al., 2006), as formal texts such as books already contain segmentation markings provided by the author. The premise of this evaluation is that textbook corpora provide a reasonable proxy for performance on less structured data. However, further clarification of this point is an important direction for future research. Metrics All experiments are evaluated in terms of the commonly-used Pk and WindowDiff metrics (Pevzner and Hearst, 2002). Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other. WindowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while Pk only asks whether the two sentences are in the same segment or not. This eval2The full text of this book is available for free download at http://onlinebooks.library.upenn.edu. uation uses source code provided by Malioutov and Barzilay (2006). Experiment</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Lev Pevzner and Marti A. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28(1):19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<tech>Program,</tech>
<pages>14--130</pages>
<contexts>
<context position="26889" citStr="Porter (1980)" startWordPosition="4429" endWordPosition="4430">sed on lexical chains. TEXTSEG employs a probabilistic segmentation objective that is similar to ours, but uses maximum a posteriori estimates of the language models, rather than marginalizing them out. Other key differences are that they set α = 1, and use a minimum description length criterion to determine segmentation granularity. Both of these baselines were run using their default parametrization. Finally, as a minimal baseline, UNIFORM produces a hierarchical segmentation with the ground truth number of segments per level and uniform duration per segment at each level. Preprocessing The Porter (1980) stemming algorithm is applied to group equivalent lexical items. A set of stop-words is also removed, using the same 359 chapter WD section WD average # segs Pk # segs Pk Pk WD HIERBAYES 5.0 .248 .255 8.5 .312 .351 .280 .303 GREEDY-BAYES 19.0 .260 .372 19.5 .275 .340 .268 .356 GREEDY-LCSEG 7.8 .256 .286 52.2 .351 .455 .304 .371 GREEDY-TEXTSEG 11.5 .251 .277 88.4 .473 .630 .362 .454 UNIFORM 12.5 .487 .491 43.3 .505 .551 .496 .521 Table 1: Segmentation accuracy and granularity. Both the Pk and WindowDiff (WD) metrics are penalties, so lower scores are better. The # segs columns indicate the ave</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. F. Porter. 1980. An algorithm for suffix stripping. Program, 14:130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Purver</author>
<author>T L Griffiths</author>
<author>K P K¨ording</author>
<author>J B Tenenbaum</author>
</authors>
<title>Unsupervised topic modelling for multiparty spoken discourse.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>17--24</pages>
<marker>Purver, Griffiths, K¨ording, Tenenbaum, 2006</marker>
<rawString>M. Purver, T.L. Griffiths, K.P. K¨ording, and J.B. Tenenbaum. 2006. Unsupervised topic modelling for multiparty spoken discourse. In Proceedings of ACL, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaemo Sung</author>
<author>Zoubin Ghahramani</author>
<author>Sung-Yang Bang</author>
</authors>
<title>Latent-space variational bayes.</title>
<date>2008</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>30</volume>
<issue>12</issue>
<contexts>
<context position="17571" citStr="Sung et al. (2008)" startWordPosition="2891" endWordPosition="2894"> can be thought of as a draw from a Bayesian mixture model, with zt as the index of the component that generates wt. However, as we are marginalizing the language models, standard mixture model inference techniques do not apply. One possible solution would be to instantiate the maximum a posteriori language models after segmenting, but we would prefer not to have to commit to specific language models. Collapsed Gibbs sampling (Griffiths and Steyvers, 2004) is another possibility, but samplingbased solutions may not be ideal from a performance standpoint. Recent papers by Teh et al. (2007) and Sung et al. (2008) point to an appealing alternative: collapsed variational inference (called latent-state variational Bayes by Sung et al.). Collapsed variational inference integrates over the parameters (in this case, the language models) and computes marginal distributions for the latent variables, QZ. However, due to the difficulty of computing the expectation of the normalizing term, these marginal probabilities are available only in approximation. More formally, we wish to compute the approximate distribution Qz(z) = HTt Qzt(zt), factorizing across all latent variables. As is typical in variational approa</context>
<context position="19650" citStr="Sung et al., 2008" startWordPosition="3257" endWordPosition="3260">= i). (3) t,7�t The first term in equation 2 is the set of component weights βt, which are fixed at 1/L for all `. The fraction represents the posterior estimate of the language models: standard Dirichlet-multinomial conjugacy gives a sum of counts plus a Dirichlet prior (equation 3). Thus, the form of the update is extremely similar to collapsed Gibbs sampling, except that we maintain the full distribution over zt rather than sampling a specific value. The derivation of this update is beyond the scope of this paper, but is similar to the mixture of Bernoullis model presented in Section 5 of (Sung et al., 2008). Iterative updates of this form are applied until the change in the lower bound is less than 10−3. This procedure appears at step 5a of algorithm 1. 4.3 Hyperparameter estimation The inference procedure defined here includes two parameters: α, the symmetric Dirichlet prior on the language models; and d, the expected segment durations. The granularity of segmentation is considered to be a user-defined characteristic, so there is no “right answer” for how to set this parameter. We simply use the oracle segment durations, and provide the same oracle to the baseline methods where possible. As dis</context>
</contexts>
<marker>Sung, Ghahramani, Bang, 2008</marker>
<rawString>Jaemo Sung, Zoubin Ghahramani, and Sung-Yang Bang. 2008. Latent-space variational bayes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(12):2236–2242, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>D Newman</author>
<author>M Welling</author>
</authors>
<title>A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation.</title>
<date>2007</date>
<booktitle>In NIPS,</booktitle>
<volume>19</volume>
<pages>1353</pages>
<contexts>
<context position="17548" citStr="Teh et al. (2007)" startWordPosition="2886" endWordPosition="2889">uage models O, each wt can be thought of as a draw from a Bayesian mixture model, with zt as the index of the component that generates wt. However, as we are marginalizing the language models, standard mixture model inference techniques do not apply. One possible solution would be to instantiate the maximum a posteriori language models after segmenting, but we would prefer not to have to commit to specific language models. Collapsed Gibbs sampling (Griffiths and Steyvers, 2004) is another possibility, but samplingbased solutions may not be ideal from a performance standpoint. Recent papers by Teh et al. (2007) and Sung et al. (2008) point to an appealing alternative: collapsed variational inference (called latent-state variational Bayes by Sung et al.). Collapsed variational inference integrates over the parameters (in this case, the language models) and computes marginal distributions for the latent variables, QZ. However, due to the difficulty of computing the expectation of the normalizing term, these marginal probabilities are available only in approximation. More formally, we wish to compute the approximate distribution Qz(z) = HTt Qzt(zt), factorizing across all latent variables. As is typica</context>
</contexts>
<marker>Teh, Newman, Welling, 2007</marker>
<rawString>Y.W. Teh, D. Newman, and M. Welling. 2007. A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation. In NIPS, volume 19, page 1353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A statistical model for domain-independent text segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>491--498</pages>
<contexts>
<context position="1414" citStr="Utiyama and Isahara, 2001" startWordPosition="195" endWordPosition="199">ed variational Bayesian inference over the hidden variables. The resulting system is fast and accurate, and compares well against heuristic alternatives. 1 Introduction Recovering structural organization from unformatted texts or transcripts is a fundamental problem in natural language processing, with applications to classroom lectures, meeting transcripts, and chatroom logs. In the unsupervised setting, a variety of successful systems have leveraged lexical cohesion (Halliday and Hasan, 1976) – the idea that topically-coherent segments display consistent lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). However, such systems almost invariably focus on linear segmentation, while it is widely believed that discourse displays a hierarchical structure (Grosz and Sidner, 1986). This paper introduces the concept of multi-scale lexical cohesion, and leverages this idea in a Bayesian generative model for hierarchical topic segmentation. The idea of multi-scale cohesion is illustrated by the following two examples, drawn from the Wikipedia entry for the city of Buenos Aires. There are over 150 city bus lines called Colectivos ... Colectivos in Buenos Aires do not have</context>
<context position="4566" citStr="Utiyama and Isahara (2001)" startWordPosition="691" endWordPosition="694">o search the entire space of hierarchical segmentations in polynomial time, using a novel dynamic program. Collapsed variational Bayesian inference is then used to update the marginals. This approach achieves high quality segmentation on multiple levels of the topic hierarchy. Source code is available at http://people. csail.mit.edu/jacobe/naacl09.html. 2 Related Work The use of lexical cohesion (Halliday and Hasan, 1976) in unsupervised topic segmentation dates back to Hearst’s seminal TEXTTILING system (1994). Lexical cohesion was placed in a probabilistic (though not Bayesian) framework by Utiyama and Isahara (2001). The application of Bayesian topic models to text segmentation was investigated first by Blei and Moreno (2001) and later by Purver et al. (2006), using HMM-like graphical models for linear segmentation. Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. All of these papers consider only linear topic segmentation; we introduce multi-scale lexical cohesion, which posits that the distribution of some words changes slowl</context>
<context position="26227" citStr="Utiyama and Isahara, 2001" startWordPosition="4321" endWordPosition="4324">the contribution of joint inference, the greedy framework can be combined with a one-level version of the Bayesian segmentation algorithm described here. This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008). The hierarchical segmenter built by placing BAYESSEG in a greedy algorithm is called GREEDY-BAYES. To identify the contribution of the Bayesian segmentation framework, we can plug in alternative linear segmenters. Two frequently-cited systems are LCSEG (Galley et al., 2003) and TEXTSEG (Utiyama and Isahara, 2001). LCSEG optimizes a metric of lexical cohesion based on lexical chains. TEXTSEG employs a probabilistic segmentation objective that is similar to ours, but uses maximum a posteriori estimates of the language models, rather than marginalizing them out. Other key differences are that they set α = 1, and use a minimum description length criterion to determine segmentation granularity. Both of these baselines were run using their default parametrization. Finally, as a minimal baseline, UNIFORM produces a hierarchical segmentation with the ground truth number of segments per level and uniform durat</context>
<context position="27731" citStr="Utiyama and Isahara, 2001" startWordPosition="4568" endWordPosition="4571">.280 .303 GREEDY-BAYES 19.0 .260 .372 19.5 .275 .340 .268 .356 GREEDY-LCSEG 7.8 .256 .286 52.2 .351 .455 .304 .371 GREEDY-TEXTSEG 11.5 .251 .277 88.4 .473 .630 .362 .454 UNIFORM 12.5 .487 .491 43.3 .505 .551 .496 .521 Table 1: Segmentation accuracy and granularity. Both the Pk and WindowDiff (WD) metrics are penalties, so lower scores are better. The # segs columns indicate the average number of segments at each level; the gold standard segmentation granularity is given in the UNIFORM row, which obtains this granularity by construction. list originally employed by several competitive systems (Utiyama and Isahara, 2001). 6 Results Table 1 presents performance results for the joint hierarchical segmenter and the three greedy baselines. As shown in the table, the hierarchical system achieves the top overall performance on the harsher WindowDiff metric. In general, the greedy segmenters each perform well at one of the two levels and poorly at the other level. The joint hierarchical inference of HIERBAYES enables it to achieve balanced performance at the two levels. The GREEDY-BAYES system achieves a slightly better average Pk than HIERBAYES, but has a very large gap between its Pk and WindowDiff scores. The Pk </context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2001. A statistical model for domain-independent text segmentation. In Proceedings of ACL, pages 491–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kenneth Walker</author>
<author>W Dallas Hall</author>
</authors>
<date>1990</date>
<booktitle>Clinical Methods: The History, Physical, and Laboratory Examinations.</booktitle>
<editor>and J. Willis Hurst, editors.</editor>
<publisher>Butterworths.</publisher>
<marker>Walker, Hall, 1990</marker>
<rawString>H. Kenneth Walker, W. Dallas Hall, and J. Willis Hurst, editors. 1990. Clinical Methods: The History, Physical, and Laboratory Examinations. Butterworths.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yaari</author>
</authors>
<title>Segmentation of Expository Texts by Hierarchical Agglomerative Clustering.</title>
<date>1997</date>
<booktitle>In Recent Advances in Natural Language Processing.</booktitle>
<contexts>
<context position="5595" citStr="Yaari (1997)" startWordPosition="841" endWordPosition="842">f segmentations. All of these papers consider only linear topic segmentation; we introduce multi-scale lexical cohesion, which posits that the distribution of some words changes slowly with high-level topics, while others change rapidly with lower-level subtopics. This gives a principled mechanism to model hierarchical topic segmentation. The literature on hierarchical topic segmentation is relatively sparse. Hsueh et al. (2006) describe a supervised approach that trains separate classifiers for topic and sub-topic segmentation; more relevant for the current work is the unsupervised method of Yaari (1997). As in TEXTTILING, cohesion is measured using cosine similarity, and agglomerative clustering is used to induce a dendrogram over paragraphs; the dendrogram is transformed into a hierarchical segmentation using a heuristic algorithm. Such heuristic approaches are typically brittle, as they include a number of parameters that must be hand-tuned. These problems can be avoided by working in a Bayesian probabilistic framework. We note two orthogonal but related approaches to extracting nonlinear discourse structures from text. Rhetorical structure theory posits a hierarchical structure of discour</context>
</contexts>
<marker>Yaari, 1997</marker>
<rawString>Y. Yaari. 1997. Segmentation of Expository Texts by Hierarchical Agglomerative Clustering. In Recent Advances in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>