<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9226445">
Survey Article
Unsupervised Learning of Morphology
</title>
<author confidence="0.980638">
Harald Hammarström*
</author>
<affiliation confidence="0.888462">
Radboud Universiteit and Max Planck
Institute for Evolutionary Anthropology
</affiliation>
<author confidence="0.988337">
Lars Borin**
</author>
<affiliation confidence="0.996013">
University of Gothenburg
</affiliation>
<note confidence="0.414753">
This article surveys work on Unsupervised Learning of Morphology. We define Unsupervised
Learning of Morphology as the problem of inducing a description (of some kind, even if only
morpheme segmentation) of how orthographic words are built up given only raw text data of
a language. We briefly go through the history and motivation of this problem. Next, over 200
items of work are listed with a brief characterization, and the most important ideas in the field
are critically discussed. We summarize the achievements so far and give pointers for future
developments.
</note>
<sectionHeader confidence="0.995855" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99944025">
Morphology is understood here in its usual sense in linguistics, namely, as referring to
(the linguistic study and description of) the internal structure of words. More specifi-
cally, we understand morphology following Haspelmath (2002, page 2) as “the study
of systematic covariation in the form and meaning of words.” For our purposes, we
assume that we have a way of identifying the text words of a language, ignoring the
fact that the term word has eluded exhaustive cross-linguistic definition. Similarly, we
assume a number of commonly made distinctions in linguistic morphology, whose basic
import is indisputable, but where there is an ongoing discussion on exactly where to
draw the boundaries with respect to particular phenomena in individual languages.
Generally, a distinction is made between inflectional morphology and word forma-
tion. Inflectional morphology deals with the various realizations of the “same” lexical
word, depending on the particular syntactic context in which the word appears. Typical
examples of inflection are verbs agreeing with one or more of their arguments in the
clause, or nouns inflected in particular case forms in order to show their syntactic rela-
tion to other words in the phrase or clause, for example, showing which verb argument
they express. Word formation deals with the creation of new lexical words from existing
</bodyText>
<affiliation confidence="0.6882458">
* Centre for Language Studies, Radboud Universiteit, Postbus 9103, 6500 HD Nijmegen, The Netherlands/
Department of Linguistics, Max Planck Institute for Evolutionary Anthropology, Deutscher Platz 6,
04103 Leipzig, Germany. E-mail: h.hammarstrom@let.ru.nl.
** Språkbanken, Department of Swedish Language, University of Gothenburg, Box 200, SE-405 30
Göteborg, Sweden. E-mail: lars.borin@svenska.gu.se.
</affiliation>
<note confidence="0.814046">
Submission received: 2 March 2010; revised submission received: 19 August 2010; accepted for publication:
4 October 2010.
© 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 2
</note>
<bodyText confidence="0.988965425">
ones, for example, agent nouns from verbs. If the same kinds of mechanisms are used as
in inflectional morphology (i.e., the resulting word is derived out of only one existing
word), linguists talk about derivational morphology. If two or more existing lexical
words are combined in order to make up a new word, the terms compounding or
incorporation are used, depending on the categories of the words involved.
There is a fairly wide array of formal means available cross-linguistically for ex-
pressing inflectional and derivational categories in languages. Most commonly, how-
ever, some form of affixation is involved—that is, some phonological material is added
to the end of the word (suffixation), to the beginning of the word (prefixation), or
(much more rarely) inside the stem of the word (infixation). Suffixes and prefixes (but
rarely infixes) can form long chains, where the different positions, or “slots,” express
different kinds of inflectional or derivational categories. If a language has suffixing
and/or prefixing—sometimes called concatenative morphology—it obviously follows
that text words in that language can be segmented into a sequence of morphological
elements: a stem and a number of suffixes after the stem and/or prefixes before the
stem.1
Morphology is one of the oldest linguistic subdisciplines, and this brief presenta-
tion by necessity omits many intricacies and greatly simplifies a vast scholarship. (For
standard, in-depth, introductions to this fascinating field, see, e.g., Nida [1949], Jensen
[1990], Spencer and Zwicky [1998], or Haspelmath [2002].)
In language technology applications, a morphological component forms a bridge
between texts and structured information about the vocabulary of a language. Some
kind of morphological analysis and/or generation thus forms a basic component in
many natural language processing applications. Many languages have quite complex
morphological systems, with the number of potential inflected forms of a single lex-
ical word running into the thousands, requiring a substantial amount of work if the
linguistic knowledge of the morphological component is to be defined manually. For
this reason, researchers often turn to machine learning approaches. This survey article
is concerned with unsupervised approaches to morphology learning.
For the purposes of the present survey, we use the following definition of Un-
supervised Learning of Morphology (ULM).
Input: Raw (unannotated, non-selective2) natural language text data
Output: A description of the morphological structure (there are various levels to be
distinguished; see subsequent discussion) of the language of the input text
With: As little supervision (parameters, thresholds, human intervention, model
selection during development, etc.) as possible
Some approaches have explicit or implicit biases towards certain kinds of lan-
guages; they are nevertheless considered to be ULM for this survey. Morphology may
be narrowly taken as to include only derivational and inflectional affixation, where the
number of affixes a root may take is finite3 and the order of the affixes may not be
</bodyText>
<footnote confidence="0.967164857142857">
1 The picture is less simple in reality, because affixation is often accompanied by so-called
morphophonological changes—changes in the shape of the stem or affix involved, or both—which often
have the effect of blurring the boundaries between the elements.
2 With the term non-selective we intend to exclude text data that requires manual selection (e.g., curated
singular–plural pairs).
3 The number of inflectional affixes is finite by definition. The derivational affixes—especially in heavily
agglutinating languages—may be recursive, but are in practice finite.
</footnote>
<page confidence="0.991374">
310
</page>
<note confidence="0.907575">
Hammarström and Borin Unsupervised Learning of Morphology
</note>
<bodyText confidence="0.917548125">
permuted.4 This survey also subsumes attempts that take a broader view including
clitics5 and compounding (and there seems to be no reason in principle to exclude
incorporation and lexical affixes: see Mithun [1999], pages 37–67, for some examples).
Many, but not all, approaches focus on concatenative morphology/compounding only.
All works considered in this survey are designed to function on orthographic
words, that is, raw text data in an orthography that provides a ready-made segmen-
tation of text into words. Crucially, this excludes the rather large body of work that
only targets word segmentation, that is, segmenting a sentence or a full utterance into
words (cf. Goldsmith [2010] who also overviews word segmentation). However, works
that explicitly aim to treat both word segmentation and morpheme segmentation in
one algorithm are included. Hence, subsequent uses of the term segmentation in the
present survey are to be understood as morpheme segmentation rather than word
segmentation. We prefer the term segmentation to analysis because, in general in ULM,
the algorithm does not attempt to label the segments.
There have been other approaches to machine learning of morphology than pure
ULM as defined here, the most popular ones being:
</bodyText>
<listItem confidence="0.876815777777778">
• approaches that require selective input, such as “singular–plural pairs,”
or “all members of a paradigm” (Garvin 1967; Klein and Dennison 1976;
Golding and Thompson 1985; Wothke 1985; McClelland and Rumelhart
1986; Brasington, Jones, and Biggs 1988; Tufis 1989; Zhang and Kim 1990;
Borin 1991; Theron and Cloete 1997; Oflazer, McShane, and Nirenburg
2001, for example)
• approaches where some (small) amount of annotated data, some
(small) amount of existing rule sets, or resources such as a machine-
readable dictionary or a parallel corpus, are mandatory (Yarowsky and
</listItem>
<note confidence="0.871967666666667">
Wicentowski 2000; Yarowsky, Ngai, and Wicentowski 2001; Cucerzan and
Yarowsky 2002; Neuvel and Fulop 2002; Johnson and Martin 2003; Rogati,
McCarley, and Yang 2003, for example)
</note>
<bodyText confidence="0.994378526315789">
Such approaches are excluded from the present survey, unless the required data (e.g.,
paradigm members) are extracted from raw text in an unsupervised manner as well.
We also exclude the special case of the second approach where morphology learning
means not “learning the morphological system of a language,” but rather “learning the
inflectional classes of out-of-vocabulary words,” namely, approaches where an existing
morphological analysis component is used as the basis for guessing in which existing
paradigm an unknown text word should belong (e.g., Antworth 1990; Mikheev 1997;
Bharati et al. 2001; Forsberg, Hammarström, and Ranta 2006; Lindén 2008; Lindén 2009).
One of the matters that varies the most between different authors is the desired
outcome. It is useful to set up the implicational hierarchy shown in Table 1 (which
4 There are, however, rare cases of languages which allow the permutation of specific pairs of prefixes,
such as Kagulu (Petzell 2007), Yimas and Karawari (Foley 1991, pages 31–32) as well as Chintang,
Bantawa, and possibly other Kiranti languages where prefix ordering in general is very free (Rai 1984;
Bickel et al. 2007).
5 Clitics are affix-like elements that attach to words in particular syntactic positions, rather than to words
of particular categories as proper affixes do. The English genitive -’s is sometimes classified as a clitic,
because you can say things like The girl I met yesterday’s purse (the -’s attaches to the end of the noun
phrase, regardless of the part of speech of the last word, an adverb in this case). This could not happen
with an inflectional suffix like the plural -s: *The girl I met yesterdays cannot mean The girls I met yesterday.
</bodyText>
<page confidence="0.996198">
311
</page>
<note confidence="0.759454">
Computational Linguistics Volume 37, Number 2
</note>
<tableCaption confidence="0.972449">
Table 1
</tableCaption>
<bodyText confidence="0.359846">
Levels of power of morphological analysis.
</bodyText>
<subsectionHeader confidence="0.352904">
Form Meaning
</subsectionHeader>
<bodyText confidence="0.3781766">
Affix list A list of the affixes Given two words, decide if
11 they are affixations of the
Same-stem decision Given two words, decide if same lexeme
they are affixations of the
same stem
</bodyText>
<figure confidence="0.852844153846154">
11
Segmentation Given a word, segment it
into stem and affix(es)
Morphological analysis A functional labeling for the
affixes in the segmentation
11
Inflection tables A list of the affixation
possibilities for all stems
Paradigm list A list of the paradigms for
all stem types, complete
with functional labels for
paradigm slots
11
</figure>
<bodyText confidence="0.8986575">
Lexicon+Paradigm A list of the paradigms and a list of all stems with information
of which paradigm each stem belongs to
</bodyText>
<page confidence="0.575867">
11
</page>
<bodyText confidence="0.99862476">
Justification A linguistically and methodologically informed motivation
for the morphological description of a language
need of course not correspond to steps taken in an actual algorithm). The division
is implicational in the sense that if one can do the morphological analysis of a lower
level in the table, one can also easily produce the analysis of any of the levels above it.
Reflecting a fundamental assumption underlying most ULM work, form and meaning
(semantics) are kept separate in the table (see Section 2). For example, if one can perform
segmentation into stem and affixes, one can decide if two words are of the same stem
(if meaning is disregarded) or the same lexeme (if meaning is taken into account). The
converse need not hold; it is perfectly possible to answer the question of whether two
words are of the same stem with high accuracy, without having to commit to what the
actual stem should be.
Many recent articles fail to deal properly with previous and related work, some
reinvent heuristics that have been proposed earlier, and there is little modularization
taking place. Previous surveys and overviews for a general audience are Borin (1991),
Batchelder (1997, pages 66–68), Powers (1998), Clark (2001, pages 80–82), Xanthos
(2007, pages 95–107), Goldsmith (2001), Daelemans (2004, page 1898), Roark and Sproat
(2007, pages 116–136), Hammarström (2007b, pages 10–15), Chan (2008, pages 48–60),
Hammarström (2009b, pages 14–21), Borin (2009), Goldsmith (2010), and, to a more
limited extent, the related-work sections of individual research papers. Kurimo, Creutz,
and Turunen (2007), Kurimo, Creutz, and Varjokallio (2008a, 2008b), Kurimo and
Turunen (2008), Kurimo and Varjokallio (2008), and McNamee (2008) are overviews
of systems in the MorphoChallenge of the respective year. However, we will try to
be more comprehensive than previous surveys and discuss the ideas in the field
critically.
</bodyText>
<page confidence="0.989295">
312
</page>
<note confidence="0.85531">
Hammarström and Borin Unsupervised Learning of Morphology
</note>
<bodyText confidence="0.9477395">
We will not attempt a comparison in terms of accuracy figures as this is wholly
impossible, not only because of the great variation in goals but also because most
descriptions do not specify their algorithm(s) in enough detail. Fortunately, this aspect is
better handled in controlled competitions, such as the Unsupervised Morpheme Analysis—
MorphoChallenge6 which offers tasks of segmentation of Finnish, English, German,
Arabic, and Turkish.
</bodyText>
<sectionHeader confidence="0.899953" genericHeader="categories and subject descriptors">
2. History and Motivation of ULM
</sectionHeader>
<bodyText confidence="0.996962540540541">
Usually and justifiedly, the work of Harris (1955, 1967) is given as the starting point
of ULM. From another perspective, however, the same work by Harris can be said to
equally represent the culmination of an endeavor in the linguistic school of thought
known as American structuralism, to formalize the process of linguistic description
into so-called linguistic discovery procedures.
The variety of American structuralism which concerned itself most with the formal-
ization of linguistic discovery procedures is often connected with the name of Leonard
Bloomfield, and its core tenet may be succinctly summed up in Bloomfield’s oft-quoted
dictum: “The only useful generalizations about language are inductive generalizations”
(Bloomfield 1933, page 20). The so-called “extremist Post-Bloomfieldians” took this
program a step further: “From Bloomfield’s justified insistence on formal, rather than
semantic, features as the starting-point for linguistic analysis, this group (especially
Harris) set up as a theoretical aim the description of linguistic structure exclusively in
terms of distribution” (Hall 1987, page 156).
The earliest reason for interest in ULM was thus—at least in part—methodological
and arguably even ideological, but not (unlike at least some of the later ULM work)
motivated by, for example, a desire to simulate language acquisition in humans.
More or less simultaneously with but independently of Harris, the Russian linguist
Andreev launched a program much like that of Harris.7 Andreev’s work is much less
known than that of Harris’s, and for this reason we will describe it in some detail here.
In a series of publications (Andreev 1959, 1963, 1965b, 1967), he develops an “algorithm
for statistical-combinatory modeling of languages.” This is part of a research program
which, just like that of Harris, aims at eliminating semantics and considerations of
meaning completely from the process of “discovery” of language structure.
Thus, Andreev claims to be able to go from unsegmented transcribed speech all the
way up to syntax, using basically one and the same approach grounded in text (corpus)
statistics. Given our focus on ULM, here we will be concerned only with his approach
as applied to morphological segmentation.
Andreev’s approach is much more explicitly based in text statistics—and to some
extent in language typology—than Harris’s work. The algorithm for morphological
segmentation is described in some detail in the works of Andreev and his colleagues.
It relies on statistics of letter frequencies in a text corpus, and of average word length
in characters and average sentence length in words. From these statistics he calculates a
number of heuristic thresholds which are used to iteratively grow affix candidates from
characters at given positions in text words, and paradigm candidates from the resulting
segmentations. Instead of looking at successor/predecessor counts or transition proba-
bilities, Andreev looks at character positions in relation to word edges, from the first and
</bodyText>
<footnote confidence="0.455944">
6 Web site http://www.cis.hut.fi/morphochallenge2009/ accessed 10 September 2009.
7 To our knowledge, Andreev never refers to Harris’s work.
</footnote>
<page confidence="0.996858">
313
</page>
<note confidence="0.79863">
Computational Linguistics Volume 37, Number 2
</note>
<bodyText confidence="0.99967038">
the last character inwards no further than the average word length. At each position, the
amount of overrepresentation is calculated for each character found in this position in
some word. The overrepresentation (“correlative function” in Andreev’s terminology)
is defined as the relative frequency of the character in this word position divided by
its relative frequency in the corpus. The character–position combinations are used in
order of decreasing overrepresentation in an iterative see-saw procedure, where affix
and stem candidates are collected in alternating iterations of the algorithm. Andreev’s
approach reflects the same intuition as that of Harris; we would expect word-edge
sequences of highly overrepresented characters to be flanked by marked differences
in predecessor or successor counts calculated according to Harris’s method.
A concrete example of how Andreev’s method works (with the finer details omit-
ted) is the following, originally presented by Andreeva (1963), but the presentation here
is partly based on that in Andreev (1967).
In a 900,000-word corpus of electronics texts in Russian, the most overrepresented
letter was &lt;j&gt; (Russian &lt;˘i&gt;) in the last position of the word, where it is eight times as
frequent as in the corpus as a whole (Andreeva 1963, page 49). For the words ending in
&lt;j&gt;, its most overrepresented predecessor was &lt;o&gt;, and using some thresholds derived
from corpus statistics, the first affix candidate found was -oj (Russian -&lt;o˘i&gt;).Removing
this ending from all words in which it appears and matching the remainders of the
words (i.e., putative stems) against the other words of the corpus, yields a set of words
from which additional suffix candidates emerge (including the null suffix). This set of
words is then iteratively reduced, using the admissible suffix candidates (those below
a certain length exceeding a heuristic threshold of overrepresentation) in each step, as
long as at least two stem candidates remain. In other words: There must be at least two
stems in the corpus appearing with all the suffix candidates. In the Russian experiment
reported by Andreeva (1963), a complete adjective paradigm was induced, with 12
different suffixes. The initial suffix candidate, -oj, has a high functional load and conse-
quently a high text frequency: It is the most ambiguous of the Russian adjective suffixes,
appearing in four different slots in the adjective paradigm, and is also homonymous
with a noun suffix.
In Andreev (1965b) the method is tested extensively on Russian, which is the subject
of several papers in the volume, and a number of other languages: Albanian (Peršikov
1965), Armenian (Melkumjan 1965), Bulgarian (Fedulova 1965), Czech (Ožigova
1965), English (Malahovskij 1965), Estonian (Hol’m 1965), French (Kordi 1965), German
(Fitialova 1965), Hausa (Fihman 1965a), Hungarian (Andreev 1965a), Latvian (Jakubajtis
1965), Serbo-Croatian (Panina 1965), Swahili (Fihman 1965b), Ukrainian (Eliseeva 1965),
and Vietnamese (Jakuševa 1965). As an aside, we may note that only after the turn of
the millennium are we again seeing this variety of languages in ULM work. Most of
these studies are small-scale proof-of-concept experiments on corpora of varying sizes
(from a few thousand words in many of the studies up to close to one million words for
Russian). The outcomes are more often than not quite small “paradigmoid fragments,”
that is, incomplete and not always in correspondence with traditional segmentations.
It is noteworthy, however, that the method could not produce a single instance of
morphological segmentation for Vietnamese (Jakuševa 1965, page 228), which is as it
should be, because Vietnamese is often held forth as a language without morphology.
The papers describing these experiments are short, and it is not always clear exactly
what has been done. In fact, computers are not mentioned at all in most of the papers;
on the contrary, it is quite clear that at least some of the experiments have been carried
out manually. In principle, because Andreev and the other authors in Andreev (1965b)
describe the procedure in great detail, it should be possible to replicate some of the
</bodyText>
<page confidence="0.99522">
314
</page>
<note confidence="0.894035">
Hammarström and Borin Unsupervised Learning of Morphology
</note>
<bodyText confidence="0.98305976">
findings (cf. Altmann and Lehfeldt 1980, pages 195–198). To our knowledge, there
has been one attempt to do this, by Cromm (1997), who reimplemented the method
and tested it on the German Bible, experimenting with various parameter settings
and also making some changes to the method itself. He notes that several parameters
that Andreev provides mostly without motivation or comment in fact can be changed
in a more accepting direction, leading to much increased recall without much loss
in precision. Unfortunately, however, in his short paper, Cromm does not provide
enough information about the algorithm or the changes that he made to it, so that the
Russian original is still the only publicly available source for the details of Andreev’s
approach.
A very different, more practically oriented, motivation for ULM came in the 1980s,
beginning with the supervised morphology learning ideas by Wothke (1985, 1986) and
Klenk (1985a, 1985b) which later led to partly unsupervised methods (see the following).
Because full natural language lexica, at the time, were too big to fit in working memory,
these authors were looking for a way to analyze or stem running words in a “nicht-
lexikalisches” manner, that is, without the storage and use of a large lexicon. This
motivation is now obsolete.
The interest in purebred ULM was fairly low until about 1990, however, with only
a few works appearing between the mid 1960s and 1990. Especially in the 1980s, the
focus in computational morphology was on the development of finite-state approaches
with hand-written rules, but in the course of the following decade, interest in ULM
rose greatly, in the wake of a general increased attention during the 1990s to statistical
and information-theoretically informed approaches in natural language processing.
In speech processing, the problem of word segmentation is ever-present, and as the
computational tools for taking on this problem became increasingly sophisticated and
increasingly available not least as the result of a general development of computing
hardware and software, researchers in linguistics and computational linguistics started
taking a fresh look at the problems of word segmentation and ULM.
The work of Goldsmith (2000, 2001, 2006) represents a kind of focal point here. He
pulls together a number of strands from earlier work, sets them against a theoretical
background informed both by information theory (MDL) and linguistics, and uses
them specifically to address the problem of ULM—in particular, unsupervised learning
of inflectional morphology—and not, for instance, that of word segmentation or of
stemming for information retrieval, and so forth.
Further, there has been the idea that ULM could contribute to various open ques-
tions in the field of first-language acquisition (see, e.g., Brent, Murthy, and Lundberg
1995; Batchelder 1997; Brent 1999; Clark 2001; Goldwater 2007). However, the connec-
tion is still rather vague and even if ULM has matured, it is not clear what implications,
if any, this has for child language acquisition. Children have access to semantics and
pragmatics, not just text strings, and it would be very surprising if such cues were not
used at all in first language acquisition. Further, if some ULM technique was shown to
be successful on some reasonably sized corpora, it does not automatically follow that
children can (and do, if they can) use the same technique. Most current ULM techniques
crucially involve long series of number crunching that seem implausible for the child-
learning setting.
After the turn of the century, ULM has become something of a growth industry
in language technology. There are several reasons for this. One obvious reason is a
generally increased interest in machine learning, both theoretically (as a research area
interesting in itself and as a possible tool for modeling human language acquisition
and language learning) and for pragmatic reasons, as a way to reduce the manual work
</bodyText>
<page confidence="0.996933">
315
</page>
<note confidence="0.818209">
Computational Linguistics Volume 37, Number 2
</note>
<bodyText confidence="0.992231093023256">
involved in the construction of the lexical and grammatical knowledge bases needed for
the realization of sophisticated language technology applications.8
Another reason has to do with the acceptance of the world as multilingual and the
understanding that language communities are very unequally endowed with language
technology resources. There are on the order of 7,000 languages spoken in the world
today (Lewis 2009). Their size in number of first-language speakers is very unevenly
distributed. The top 30 languages in the world account for more than 60% of its popu-
lation. At the other end of the scale, we find that most languages are spoken by quite
small communities:
There are close to 7,000 languages in the world, and half of them have fewer than 7,000
speakers each, less than a village. What is more, 80% of the world’s languages have
fewer than 100,000 speakers, the size of a small town. (Ostler 2008, page 2)
On the whole, small language communities will tend to have correspondingly small
financial and other resources that could be spent on the development of language
technology, but the cost of, for example, constructing a lexicon or a parser for a lan-
guage is more or less constant, and not proportional to the number of speakers of the
language.
At the same time, it has been observed over and over again that the use or non-use
of a language in a particular situation—where the language could in principle be used,
but where there is a choice available between two or more languages—is intimately
connected with the attitudes towards the language among the participants. This is
perhaps the most reliable determiner of language use, and not factors such as effort,
lack of vocabulary, and so on, which in many cases seem to be post hoc rationalizations
motivating a choice made on attitudinal grounds. Another way of expressing this is that
languages are more or less prestigious in the eyes of their speakers, and that linguistic
inferiority complexes seem to be common in the world.
However, rather than taking status as an inherent and immutable characteristic of a
language, we should see it for what it is, namely, a perceived characteristic, something
that lies in the eye of the beholder. As such, it can be influenced by human action.
Important for our purposes here is that it has been suggested that making available
modern information and communication technologies for a language, including the
creation of linguistic resources and language technology for it, may serve to raise its
status (see, e.g., the papers in Saxena and Borin 2006).
This, then, is another reason for pursuing ULM: to be able to provide language
technology to language communities lacking the requisite resources. However, ULM,
at least as understood for the purposes of this survey, requires a written language,
which would still exclude a substantial majority of the world’s languages (Borin 2009).
Note that the remainder—languages with a tradition of writing—are not on the whole
small language communities; in the first instance, we are talking about the few hundred
most spoken languages in the world, for example, the 313 languages with at least one
million native speakers (accounting for about 80% of the world’s population) surveyed
by the Linguistic Data Consortium some years back in their Low-density language survey
(Strassel, Maxwell, and Cieri 2003; Borin 2009).
</bodyText>
<footnote confidence="0.9953635">
8 Another pragmatic, less savory reason is a general downplaying of linguistic knowledge in the language
technology research community (Reiter 2007).
</footnote>
<page confidence="0.997973">
316
</page>
<bodyText confidence="0.975565833333334">
Hammarström and Borin Unsupervised Learning of Morphology
The hope is often expressed in the literature that ULM and other unsupervised
methods could be employed in order to rapidly and cheaply (in terms of human effort)
bootstrap basic language technology resources for new languages.
It should be noted that, even for larger languages, because of the human effort
needed to build computational morphological resources, many such implementations
are not released to the public domain. Also, open domain texts will always contain a
fair share of (inflected) previously unknown words that are not in the lexicon. There
has to be strategy for such out-of-dictionary words—a ULM-solving algorithm is one
possibility. The ULM problem as specified, therefore, still has a role to play for larger
languages.
Finally, and closely related to the preceding reason, ULM and other kinds of ma-
chine learning of linguistic information are increasingly seen as providing potential
tools in language documentation.9
It has been realized for some time that languages are disappearing at a rapid rate
in the modern world (Krauss 1992, 2007). Many linguists see this loss of linguistic
diversity as a disaster in the cultural and intellectual sphere on a par with the loss of
the world’s biodiversity in the ecological sphere, only on a grander scale; languages
are going extinct more rapidly than species. Enter language documentation (Gippert,
Himmelmann, and Mosel 2006), which is construed as going well beyond traditional
descriptive linguistic fieldwork, aspiring as it does to capture all aspects—linguistic,
cultural, and social—of a language community’s day-to-day life, in video and audio
recordings of a wide range of sociocultural activities, in still images, and in representa-
tive artifacts. Basic linguistic descriptions of lexicon and grammar made on the basis of
transcribed recordings still form an important component of language documentation,
however, and with the realization that languages are disappearing at a far faster rate
than linguists can document them, it is natural to look for ways of making this process
less labor-intensive.10
In summary, we have seen the following motivations for ULM (in chronological
order):
</bodyText>
<listItem confidence="0.9999602">
• Linguistic theory
• Elimination of the lexicon
• Child language acquisition
• Morphological engine bootstrapping
• Language description and documentation bootstrapping
</listItem>
<bodyText confidence="0.9034162">
As noted, the motivation of eliminating the lexicon is now obsolete, whereas the others
are active to various degrees. By far the most popular motivation has been, and still is,
that of inducing a morphological analyzer/segmentation from raw text data (with little
human intervention) in a well-described language. However, as we have argued herein,
the timing is right for the momentum to carry over also to under-described languages.
</bodyText>
<footnote confidence="0.990083666666667">
9 To our knowledge, Eguchi (1987, page 168) is the first author to suggest ULM as one of several
computational aids to the language documentation fieldworker.
10 For example, in the instructions for the recent large-scale language documentation effort BOLD:PNG—
Basic Oral Language Documentation: Papua New Guinea—we read: “Try not to spend more than an hour
transcribing a minute of text.” and “As before, try not to spend more than an hour translating a minute
of text.” www.boldpng.info/bold/stage3.
</footnote>
<page confidence="0.986546">
317
</page>
<note confidence="0.372245">
Computational Linguistics Volume 37, Number 2
</note>
<sectionHeader confidence="0.571787" genericHeader="general terms">
3. Trends and Techniques in ULM
</sectionHeader>
<subsectionHeader confidence="0.987004">
3.1 Roadmap and Synopsis of Earlier Studies
</subsectionHeader>
<bodyText confidence="0.974288125">
A chronological listing of earlier work (with very short characterizations) is given in
Table 2. Several papers are co-indexed if they represent essentially the same line of work
by essentially the same author(s).
Given the number of algorithms proposed, it is impossible to go through the tech-
niques and ideas individually. However, we will attempt to cover the main trends and
look at some key questions in more detail.
The problem has been approached in four fundamentally different ways, which we
may summarize in the following way.
</bodyText>
<listItem confidence="0.999291742857143">
(a) Border and Frequency: In this family of methods, if a substring occurs
with a variety of substrings immediately adjacent to it, this is interpreted
as evidence for a segmentation border. In addition, frequent or somehow
overrepresented substrings are given a direct interpretation as candidates
for segmentation. A typical implementation is to subject the data to a
compression formula of some kind, where frequent long substrings with
clear borders offer the optimal compression gain. The outcome of such a
compression scheme gives the segmentation. In addition, for those
approaches which also target paradigms, stem–suffix co-occurrence
statistics are gathered given the segmentation produced, rather than all
possible segmentations.
(b) Group and Abstract: In this family of methods, morphologically related
words are first grouped (clustered into sets, paired, shortlisted, etc.)
according to some metric, which is typically string edit distance, but may
include semantic features (Schone 2001), distributional similarity (Freitag
2005), or frequency signatures (Wicentowski 2002). The next step is to
abstract some morphological pattern that recurs among the groups. Such
emergent patterns provide enough clues for segmentation and can
sometimes be formulated as rules or morphological paradigms.
(c) Features and Classes: In this family of methods, a word is seen as made
up of a set of features—n-grams in Mayfield and McNamee (2003) and
McNamee and Mayfield (2007), and initial/terminal/mid-substring in
De Pauw and Wagacha (2007). Features which occur on many words have
little selective power across the words, whereas features which occur
seldom pinpoint a specific word or stem. To formalize this intuition,
Mayfield and McNamee and McNamee and Mayfield use TF-IDF, and
De Pauw and Wagacha use entropy. Classifying an unseen word reduces
to using its features to select which word(s) it may be morphologically
related to. This decides whether the unseen word is a morphological
variant of some other word, and allows extracting the “variation” by
which they are related, such as an affix.
(d) Phonological Categories and Separation: In this family of methods, the
phonemes (approximated by graphemes) are first classed into categories,
foremostly, vowel versus consonant. Thereafter, each word is separated
into its vowel skeleton and its consonant skeleton, after which various
</listItem>
<page confidence="0.998167">
318
</page>
<note confidence="0.754567">
Hammarström and Borin Unsupervised Learning of Morphology
</note>
<tableCaption confidence="0.996898">
Table 2
</tableCaption>
<table confidence="0.98725">
Very brief roadmap of earlier studies.
Model Superv. Experimentation Learns what?
Harris 1955, 1968, 1970 C T English Segmentation
Andreev 1965a, Andreev 1967, C T Vietnamese to Segmentation
Chapter 2, Peršikov 1965; Hungarian (I)
Melkumjan 1965; Fedulova 1965;
Ožigova 1965; Malahovskij 1965;
Hol’m 1965; Kordi 1965; Fitialova
1965; Fihman 1965a; Andreev
1965a; Jakubajtis 1965; Panina
1965; Fihman 1965b; Eliseeva
1965; Jakuševa 1965
Gammon 1969 C T English Segmentation
Lehmann 1973, pages 71–93 C T French (I) Segmentation
de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms
1978
Faulk and Gustavson 1990 C T English (I) Segmentation
Hafer and Weiss 1974 C T English (IR) Segmentation
Klenk and Langer 1989 C T+SP German Segmentation
Langer 1991 C T+SP German Segmentation
Redlich 1993 C T English (I) Segmentation
Klenk 1992, 1991 C T+SP Spanish Segmentation
Flenner 1992, 1994, 1995 C T+SP Spanish Segmentation
Janßen 1992 C T+SP French Segmentation
Juola, Hall, and Boggs 1994 C T English Segmentation
Brent 1993, 1999; Brent, Murthy, C T English/Child- Segmentation
and Lundberg 1995; Snover 2002; English/Polish/
Snover, Jarosz, and Brent 2002; French
Snover and Brent 2001, 2003
Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation
1996
Yvon 1996 C T French (I) Segmentation
Kazakov 1997; Kazakov and C T French/English Segmentation
Manandhar 1998, 2001
Jacquemin 1997 C T English Segmentation
Cromm 1997 C T German Segmentation
Gaussier 1999 C T French/English (I) Lexicon+ Paradigms
Déjean 1998a, 1998b C T Turkish/English/ Affix Lists
Korean/French/
Swahili/
Vietnamese (I)
Medina Urrea 2000, 2003, 2006b C T Spanish Affix List
Schone and Jurafsky 2000, 2001a; C T English Segmentation
Schone 2001
Goldsmith 2000, 2001, 2006; Belkin C T English (I) Lexicon+ Paradigms
and Goldsmith 2002; Goldsmith,
Higgins, and Soglasnova 2001;
Hu et al. 2005b; Xanthos, Hu,
and Goldsmith 2006
Baroni 2000, 2003 C T Child-English/ Affix List
English
Cho and Han 2002 C T Korean Segmentation
Sharma, Kalita, and Das 2002, 2003; C T Assamese Lexicon+ Paradigms
Sharma and Das 2002
Baroni, Matiasek, and Trost 2002 C/NC T English/German (I) Related word pairs
Bati 2002 C/NC T Amharic Lexicon+ Paradigms
Creutz 2003, 2006; Creutz and C T Finnish/Turkish/ Segmentation
Lagus 2002, 2004, 2005a, 2005b, English
2005c, 2007; Creutz, Lagus, and
Virpioja 2005; Hirsimäki et al.
2003; Creutz et al. 2005
</table>
<page confidence="0.777301">
319
</page>
<table confidence="0.551716">
Computational Linguistics Volume 37, Number 2
</table>
<tableCaption confidence="0.938507">
Table 2
</tableCaption>
<table confidence="0.957111433333334">
(continued)
Model Superv. Experimentation Learns what?
Kontorovich, Don, and Singer 2003 C T English Segmentation
Medina Urrea and Díaz 2003; C T Chuj/Ralámuri/Czech Affix List
Medina-Urrea 2006a, 2008
Mayfield and McNamee 2003; - -8 West European Same-stem
McNamee and Mayfield 2007 languages (IR)
Zweigenbaum, Hadouche, and C T Medical French Segmentation
Grabar 2003; Hadouche 2002
Pirrelli et al. 2004; Pirrelli and C T Italian/English/Arabic Unclear
Herreros 2007; Calderone 2008
Johnson and Martin 2003b C T Inuktitut Unclear
Katrenko 2004 C T Ukrainian Lexicon+ Paradigms
´Cavar et al. 2004a, 2004b; ´Cavar, C T Child-English Unclear
Rodrigues, and Schrementi 2006;
´Cavar et al. 2006
Rodrigues and ´Cavar 2005, 2007 NC T Arabic Segmentation
Monson 2004, 2009; Monson et al. C T English/Spanish/ Segmentation
2004, 2007a, 2007b, 2008, 2008a, Mapudungun (I)
2008b
Yarowsky and Wicentowski 2000; C/NC T 30-ish mostly European Segmentation +
Wicentowski 2002, 2004 type languages Rewrite Rules
Gelbukh, Alexandrov, and Han 2004 C - English Segmentation
Argamon et al. 2004 C T English Segmentation
Goldsmith et al. 2005; Hu et al. 2005a C/NC T Unclear Unclear
Bacchin, Ferro, and Melucci 2005, C T Italian/English Segmentation
2002a, 2002b; Nunzio et al. 2004
Oliver 2004, Chapter 4–5 C T Catalan Paradigms
Bordag 2005a, 2005b, 2007, 2008 C T English/German Segmentation
Hammarström 2005, 2006a, 2006b, C - Maori to Warlpiri Same-stem
2007b, 2009a, 2009b
Bernhard 2005a, 2005b, 2006, 2007, C T Finnish/Turkish/English Segmentation+
2008 Related sets
of words
Keshava and Pitler 2005 C T Finnish/Turkish/English Segmentation
Johnsen 2005 C T Finnish/Turkish/English Segmentation
Atwell and Roberts 2005 C T Finnish/Turkish/English Segmentation
Dang and Choudri 2005 C T Finnish/Turkish/English Segmentation
ur Rehman and Hussain 2005 C T Finnish/Turkish/English Segmentation
Jordan, Healy, and Keselj 2005, 2006 C T Finnish/Turkish/English Segmentation
Goldwater, Griffiths, and Johnson C T English/Child-English Segmentation
2005; Goldwater 2007;
Naradowsky and Goldwater 2009
Freitag 2005 C T English Segmentation
Golcher 2006 C - English/German Lexicon+ Paradigms
Arabsorkhi and Shamsfard 2006 C T Persian Segmentation
Chan 2006, Chan 2008, C T English Paradigms
pages 101–139
Demberg 2007 C/NC T English/German/ Segmentation
Finnish/Turkish
Dasgupta and Ng 2006, 2007a; C T Bengali Segmentation
2007b; Dasgupta 2007
De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation
Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis
Xanthos 2007 NC T Arabic Lexicon+ Paradigms
Majumder et al. 2007; C T French/Bengali/French/ Analysis
Majumder, Mitra, and Pal Bulgar-
2008 ian/Hungarian
Zeman 2008, 2009 C - Czech/English/German/ Segmentation+
Finnish Paradigms
</table>
<page confidence="0.903537">
Kohonen, Virpioja, and Klami C T Finnish/Turkish/English Segmentation
2008
320
</page>
<tableCaption confidence="0.770012">
Hammarström and Borin Unsupervised Learning of Morphology
Table 2
</tableCaption>
<note confidence="0.642082833333333">
(continued)
Model Superv. Experimentation Learns what?
Goodman 2008 C T Finnish/Turkish/English Segmentation
Golénia 2008 C T Turkish/Russian Segmentation
Pandey and Siddiqui 2008 C T Hindi Segmentation+
Paradigms
Johnson 2008 C T Sesotho Segmentation
Snyder and Barzilay 2008 C/NC T Hebrew/Arabic/Aramaic/ Segmentation
English
Spiegler et al. 2008 C T Zulu Segmentation
Moon, Erk, and Baldridge 2009 C T English/Uspanteko Segmentation
Poon, Cherry, and Toutanova C T Arabic/Hebrew Segmentation
</note>
<equation confidence="0.894026">
2009
Abbreviations: C = Concatenative; I = Impressionistic evaluation; IR = Evaluation only in terms of Information Retrieval
Performance; NC = Non-concatenative; RR = Hand-written rewrite rules; SP = Some manually curated segmentation points;
T = Thresholds and Parameters to be set by a human.
</equation>
<bodyText confidence="0.989503857142857">
frequency techniques reminiscent of those of the (a) approaches can
be applied. This strategy is targeted towards the special kind of
non-concatenative morphology called intercalated morphology11 with
the observation that, empirically, in those (relatively few) languages
which have intercalated morphology, it does seem to depend on
vowel/consonant considerations. In Xanthos (2007), the phonological
categories are inferred in an unsupervised manner (cf. Goldsmith and
Xanthos 2009) whereas in Bati (2002) and Rodrigues and ´Cavar (2005,
2007) they are seen as given by the writing system.
The first two, (a) and (b), enjoy a fair amount of popularity in the reviewed collection of
work, though (a) is much more common and was the only kind used up to about 1997.
The last two, (c) and (d), have been utilized only by the sets of authors cited therein.
Let us now look at some salient questions in more detail. The following notation
will be used in formal statements:
</bodyText>
<listItem confidence="0.994946846153846">
• w, s, b, x, y, ... E E*: lowercase-letter variables range over strings of some
alphabet E and are variously called words, segments, strings, and so forth.
• W, S, ... C_ E*: capital-letter variables range over sets of
words/strings/segments.
•
C,...: capital-letter caligraphic variables range over multisets of
words/strings/segments.
•  |· |: is overloaded to denote both the length of a string and the cardinality
of a set.
• w[i]: denotes the character at position i in the string w. For example, if
w = hello then w[1] = h.
• w[i : j]: denotes the segment from position i to j (inclusive) of the string w.
For example, if w = hello then w[1 : |w|] = hello.
</listItem>
<page confidence="0.761694">
11 Also known as templatic morphology or root-and-pattern morphology.
321
</page>
<figure confidence="0.295737">
Computational Linguistics Volume 37, Number 2
</figure>
<listItem confidence="0.997411">
• WC is used to denote the set of words in a corpus C.
• fpW(x) = |{z|xz E W}|: the (prefix) frequency of x, that is, the number of
words in W with initial segment x.
• fsW(x) = |{z|zx E W}|: the (suffix) frequency of x, that is, the number of
words in W with final segment x.
</listItem>
<bodyText confidence="0.407661">
Subscript letters are dropped when understood from the context.
</bodyText>
<subsectionHeader confidence="0.998067">
3.2 Border and Frequency Methods
</subsectionHeader>
<subsubsectionHeader confidence="0.656195">
3.2.1 Letter Successor Varieties. Most (if not all) authors trace the inspiration for their
</subsubsectionHeader>
<bodyText confidence="0.935958375">
border heuristics back to Harris (1955). In fact, Harris defines a family of heuristics,
all based on letter successor/predecessor varieties. They were originally presented as
applying to utterances made up of phoneme sequences (Harris 1955), but they apply
just the same to words, namely, grapheme sequences (Harris 1970). The basic counting
strategy, labelled letter successor varieties (LSV) by Hafer and Weiss (1974), is as follows.
Given a set of words W, the letter successor variety of a string x of length i is defined as
the number of distinct letters that occupy the i + 1st position in words that begin with x
in W:
</bodyText>
<equation confidence="0.996732">
LSV(x) = |{z[|x |+ 1]|z = xy E W}|
</equation>
<bodyText confidence="0.99685275">
Table 3 shows an example of a letter successor count on a tiny contrived wordlist.
We may define the letter predecessor variety (LPV) analogously. For a given suffix
x, the LPV(x) is the number of distinct letters that occupy the position immediately
preceding x in the words of W that end in x. LSV/LPV counts for an example word are
shown in Table 4.
It should be noted that Harris (1955, page 192, footnote 4) explicitly targets the
variety in letter successors types (i.e., is only interested in which letters ever occur in the
successor position, as opposed to being interested in their frequencies). For example, if
there are two different letters occurring in successor position, one occurring a thousand
times and the other once, Harris’s letter successor variety is still two—the same as if
the two letters occurred once each. Subsequent authors have suggested that the full
frequency distribution of the token letter successors carries a better signal of morpheme
boundary. After all, if there is a significant token frequency skewing, this suggests
that we are in the middle of coherent morpheme. Moreover, mere type counts may
be influenced by phonotactic constraints (consonant after vowel, etc.), which come out
less significant in token frequency counts (Goldsmith 2006, page 6). Already the earliest
</bodyText>
<tableCaption confidence="0.707139">
Table 3
</tableCaption>
<bodyText confidence="0.743898">
Example of LSV-counts for some example prefixes (bottom) based on a small example word
list (top).
</bodyText>
<equation confidence="0.895882">
W = {abide, able, abode, and, art, at, bat}
x a ab abe ...
{z|z = xy E W} {abide, able, abode, and, art, at} {abide, able, abode} ∅ ...
LSV(x) 4 (b,n,r,t) 3 (i,l,o) 0 ...
</equation>
<page confidence="0.997179">
322
</page>
<note confidence="0.7499">
Hammarström and Borin Unsupervised Learning of Morphology
</note>
<tableCaption confidence="0.984595">
Table 4
</tableCaption>
<bodyText confidence="0.945871538461539">
LSV counts for d-, di-, dis-, ... , disturbance- and LPV counts for -e, -ce, -nce, ... , -disturbance. All
figures are computed on the Brown Corpus of English (Francis and Kucera 1964), using the
27 letter alphabet [a − z] plus the apostrophe. There are |W |= 42,353 word types in lowercase.
LSV 13 20 21 6 1 1 3 1 1 1 1
d i s t u r b a n c e
LPV 0 1 1 1 1 1 1 19 6 12 25
follow-ups to Harris (Gammon 1969; Hafer and Weiss 1974; Juola, Hall, and Boggs 1994)
experiment with replacing the raw LSV/LPV counts with the entropy of the character
token distribution. The character token distribution after a given segment can be seen as
a probability distribution whose events are the characters of the alphabet. The entropy
of this probability distribution then measures how unpredictable the next character is
after a given segment. In general, for a discrete random variable X with possible values
x1, ... , xn, the expression for entropy takes the following form:
</bodyText>
<equation confidence="0.996648">
n
H(X) = − p(xi) log2 p(xi)
i=1
Thus, with alphabet E, the letter successor entropy (LSE) for a prefix x is defined as
� fp(xc)fp(xc)
LSE(x) = − fp(x) log2 fp(x)
c∈Σ
</equation>
<bodyText confidence="0.999799611111111">
At least two authors (Golcher 2006; Hammarström 2009b) have questioned entropy as
the appropriate measure for highlighting a morpheme boundary. Entropy measures
how skewed the distribution is as a whole, that is, how deviant the most deviant
member is, in addition to the second member, the third, and so on. If there is no mor-
pheme boundary, the morpheme continues with (at least) one character. So one deviant,
highly predictable, character is necessary and sufficient to signal a non-break, and it is
arguably irrelevant if there are second- and third-place, and so forth, highly predictable
characters that also signal the absence of a morpheme boundary. For example, the
character token distribution before -ng is shown in Table 5. Obviously, the fact that of
the 3,352 occurrences of -ng, 3,258 of them are preceded by -i-, says that the absence of
a morpheme boundary is highly likely. Now, does it matter that also another 35 are -o-
versus only 4 for -e-? Entropy would also take into account the skewedness of -o- versus
-e-, whereas for Hammarström (2009b) and Golcher (2006) only the skewedness of the
most skewed character (i.e., the character that potentially constitutes the morpheme
continuation) is interesting, in this example -i-. Therefore, these approaches only use the
maximally skewed character to predict the presence/absence of a morpheme boundary.
The letter successor max-drop (LSM) for a prefix x is defined as the fraction not occupied
by its maximally skewed one-character continuation:
</bodyText>
<equation confidence="0.974353">
LSM(x) = 1 − maxc∈Σ f x) fp(xc)
</equation>
<page confidence="0.997779">
323
</page>
<table confidence="0.436878">
Computational Linguistics Volume 37, Number 2
</table>
<tableCaption confidence="0.625539333333333">
Table 5
The character token distributon for the character immediately preceding -ng, computed on the
Brown Corpus of English (Francis and Kucera 1964).
-ng 3,352
-n- 1 -l- l -h- l -e- 4 -u- 26 -a- 26 -o- 35 -i- 3,258
Table 6
</tableCaption>
<bodyText confidence="0.8863515">
Normalized LPV/LPE/LPM-scores for -e, -ce, -nce, ..., -disturbance. All figures are computed on
the Brown Corpus of English (Francis and Kucera 1964), using the 27-letter alphabet [a − z] plus
the apostrophe. There are |W |= 42,353 word types in lowercase.
d i s t u r b a n c e
LPV 0.03 0.03 0.03 0.03 0.03 0.03 0.70 0.22 0.44 0.92
LPE 0.0 0.0 0.0 0.0 0.0 0.0 0.74 0.28 0.38 0.81
LPM 0.0 0.0 0.0 0.0 0.0 0.0 0.83 0.53 0.37 0.85
Which one of LSV/LSE/LSM is the “correct” one? The answer, of course, depends
on one’s theory of affixation, for which the field has no single answer (see Section 3.6,
subsequently).
Empirically, however, the three measures are highly correlated. To compare the
three, we normalize them to their maxima in order to get a “border” score &lt; 1. The
maximum achievable LSV is the alphabet size, so the normalized LSV(x) = L |Σ|(x)
The maximum achievable LSE is a uniform distribution across the alphabet, so the
</bodyText>
<equation confidence="0.993235">
LSE(x)
normalized LSE(x) = −|Σ|·( 1
|Σ |log2 1
</equation>
<bodyText confidence="0.998725416666667">
|Σ |). The maximum achievable LSM is a uniform
distribution across the alphabet, so the normalized LSM(x) = i−111 The predecessor
analogues LPV, LPE, LPM are obvious. Table 6 shows an example word and its
normalized predecessor scores of the three kinds.
As in the example, the three different measures have nearly the same story to tell
in general, at least for English. For the three measures, Table 7 shows the Pearson
product-moment correlation coefficient between the LPH/LPE/LPM-values of all ter-
minal segments, as well as the Pearson product-moment correlation coefficient between
the LPH/LPE/LPM-ranks of all terminal segments. Most usages in the literature of the
letter successor counts have been relative to other counts on the same language. In such
cases, the rank correlations show that all three measures can be expected to have near
identical effects.
</bodyText>
<tableCaption confidence="0.876233">
Table 7
</tableCaption>
<bodyText confidence="0.890671428571429">
The Pearson product-moment correlation coefficient between LPH/LPE/LPM-values (r) and
the Pearson product-moment correlation coefficient between LPH/LPE/LPM-ranks (r-rank).
All values are computed on the Brown Corpus of English (Francis and Kucera 1964), using the
27-letter alphabet [a − z] plus the apostrophe. There are |W |= 42,353 word types in lowercase.
LPH&amp;LPE LPE&amp;LPM LPM&amp;LPH
r 0.872 0.957 0.729
r-rank 0.999 0.998 0.996
</bodyText>
<page confidence="0.782281">
324
</page>
<bodyText confidence="0.797317333333333">
Hammarström and Borin Unsupervised Learning of Morphology
A number of concrete ways to use LSV/LPVs for segmentation are suggested by
Harris (1955) and Hafer and Weiss (1974); for instance:
</bodyText>
<listItem confidence="0.9985221">
(a) Cutoff: By far the easiest way to segment a test word is first to pick some
cutoff threshold k and then break the word wherever its successor (or
predecessor or both) variety reaches or exceeds k.
(b) Peak and plateau: In the peak and plateau strategy, a cut in a word w is
made after a prefix x if and only if LSV(w[1 : |x |− 1]) &lt; LSV(x) &gt;
LSV(w[1 : |x |+ 1]); that is, if the successor count for x forms a local “peak”
or sits on a “plateau” of the LSV-sequence along the word.
(c) Complete word: A break is made after a word prefix (or before a word
suffix) if that prefix (or suffix) is found to be a complete word in the corpus
word list W.
</listItem>
<bodyText confidence="0.999741071428571">
These and similar strategies have been discussed and evaluated in various settings in
the literature, and it is unlikely that any strategy based on LSV/LSE/LSM-counts alone
will produce high-precision results. The example in Table 6 showing morpheme border
heuristics on a specific word illustrates the matter at heart. Any intuitively plausible
theory of affixation should allow abundant combination of morphemes without respect
to their phonological form, which predicts that high LSV/LSE/LSM values should
emerge at morpheme boundaries. However, there appears to be no reason why the
converse should hold—high LSV/LSE/LSM values could emerge in other places of the
word as well. Indeed, any frequent character at the end or beginning of a word may
also be expected to show high LSV/LSE/LSM around it, such as the -e at the end of
disturbance which has higher values than, for example, -ance. Therefore, simply inferring
that high LSV/LSE/LSM values indicate a morpheme border is not a sound principle in
general.
A different (but less successful, even when supervised) way to use character se-
quence counts is that associated with Ursula Klenk and various colleagues (Klenk and
Langer 1989; Klenk 1991, 1992; Langer 1991; Flenner 1992, 1994, 1995; Janßen 1992). For
each character bigram c1c2, they record, with some supervision in the form of manual
curation, at what percentage there is a morpheme boundary before |c1c2, between c1|c2,
after c1c2|, or none. A new word can then be segmented by sliding a bigram window
and taking the split which satisfies the corresponding bigrams the best. For example,
given a word singing, if the window happens to be positioned at -gi- in the middle,
the bigram splits ng|, g|i, and |in are relevant to deciding whether sing|ing is a good
segmentation. Exactly how to do the split by sliding the window and combining such
bigram split statistics is subject to a fair amount of discussion. It became apparent,
however, that the appropriateness of a bigram split is dependent on, for example, the
position in a word—-ed is likely at the end of a word, but hardly in any other position—
and exception lists and cover-up rules had to be introduced, before the approach was
abandoned altogether.
</bodyText>
<subsubsectionHeader confidence="0.780208">
3.2.2 Frequency Heuristics. For reasons just explained, most (if not all) recent authors
</subsubsectionHeader>
<bodyText confidence="0.9937552">
in the border-and-frequency tradition have incorporated another measure, comple-
mentary to a morpheme border heuristic. This measure is nearly always directly or
indirectly related to frequency, that is, frequent segments of some kind are singled out.
Frequency has been used in many different ways. The simplest way is to look at the
raw frequency of segments of any length, but, inevitably, this will sweep in any short
</bodyText>
<page confidence="0.986591">
325
</page>
<note confidence="0.279318">
Computational Linguistics Volume 37, Number 2
</note>
<bodyText confidence="0.985416">
segment. Indeed, better candidates for morphemic segmentation are segments which
are somehow overrepresented, that is, more frequent than random. There are various
ways to define this property as well, including the following.
Overrepresentation as more-frequent-than-its-length: For a segment x of |x |charac-
ters, it is overrepresented to the degree that it is more common than expected
from a segment of its length. This applies to a segment in any position.
</bodyText>
<equation confidence="0.8770085">
f(x)
|Σ||x|
</equation>
<bodyText confidence="0.991993">
Overrepresentation as more-frequent-than-its-parts: For a segment x = c1c2 ... cn of n
characters, it is overrepresented to the degree that it is more common than ex-
pected from a co-occurrence of its parts. This applies to a segment in any position.
</bodyText>
<equation confidence="0.9989145">
f(c1c2 ... cn)
f(c1)f(c2) ... (cn)
</equation>
<bodyText confidence="0.999913153846154">
Overrepresentation as more-frequent-as-suffix: For a segment x, it is overrepresented
to the degree that its probability as a suffix is higher than in any other (non-
final) position. This applies to a segment in terminal position (but with obvious
analogues for other positions).
With such measures, many authors have singled out affixes above a certain over-
representation-value threshold or overrepresentation-rank threshold.
Threshold values are unsatisfactory because typically there is no theory in which
to interpret them. Although they may be set ad hoc with some success, such settings
do not automatically generalize. Such considerations have led many authors to devise
compression-inspired models for exploiting skewed frequencies. In particular, several
different sets of authors have invoked Minimum Description Length (MDL) as the
motivation for a given formula to compress input data into a morphologically analyzed
representation.12
The MDL principle is a general-purpose method of statistical inference. It views the
learning/inference process as data compression: For a given set of hypotheses H and
data set D, we should try to find the hypothesis in H that compresses D most (Grünwald
2007, pages 3–40). Concretely, such a calculation can take the the following form. If L(H)
is the length, in bits, of the description of the hypothesis; and L(D|H) is the length, in
bits, of the description of the data when encoded with the help of the hypothesis, then
MDL aims to minimize L(H) + L(D|H).
In principle, all of the works that have invoked MDL in their ULM method act as
follows. A particular way Q of describing morphological regularities is conceived that
has two components which we may call patterns P and data D. A coding scheme is
devised to describe any P and to describe any collection of actual words with some
specific P and D. A greedy search is done for a local minimum of the sum L(P) + L(D|P)
to describe the set of words W (in some approaches) or the bag of word tokens C (in
</bodyText>
<page confidence="0.74947">
12 To our knowledge, Brent (1993) is the first author to do so for morphological segmentation.
326
</page>
<bodyText confidence="0.981660541666667">
Hammarström and Borin Unsupervised Learning of Morphology
other approaches) of the input text data.13 To take one concrete example, Goldsmith’s
(2006) particular way Q of describing morphological regularities is to allow for a list of
stems, a list of affixes, a list of signatures (structures indicating which stems may appear
with which affixes, i.e., a list of pointers to stems, and a list of pointers to suffixes). The
search is then among different lists of stems, affixes, and signatures to see which is the
shortest to account for the words of the corpus. Further details of such coding schemes
need not concern us here, but for a range of options see, for example, Goldsmith (2001,
2006), Xanthos, Hu, and Goldsmith (2006), Creutz and Lagus (2007), Argamon et al.
(2004), Arabsorkhi and Shamsfard (2006), ´Cavar et al. (2004b), Baroni (2003), or Brent,
Murthy, and Lundberg (1995).
It should be noted that the label MDL, in at least the terminology of Grünwald
(2007, pages 37–38), is infelicitous for such cases where the P, D-search is not among
different description languages, but among varations within a fixed language Q. For
example, in the stem-affixes-signatures way of description (a specific Q), the search
does not include other (possibly more parsimonious?) ways of description that do not
use stems, affixes, or signatures at all. For the MDL-label to apply with its full philo-
sophical underpinnings, the scope must include any possible compression algorithm,
namely, any Turing machine. In this respect it is important to note that, compared to the
schemes devised so far, Lempel-Ziv compression, another description language, should
yield a superior compression (as, in fact, conceded by Baroni 2000, pages 146–147).
MDL-inspired optimization schemes have achieved very competitive results in practice,
however, and must be considered the leading paradigm to exploit skewed frequencies
for morphological analysis.
</bodyText>
<subsubsectionHeader confidence="0.988594">
3.2.3 Paradigm Induction. The next step after segmentation is to induce systematic alter-
</subsubsectionHeader>
<bodyText confidence="0.999208833333333">
nation patterns, or (inflectional) paradigms,14 and this is usually done as an extension
of a border-and-frequency approach. For purposes of ULM, a paradigm is typically
defined as a maximally large set of affixes whose members systematically occur on an
open class of stems. For a number of reasons, finding paradigms is a major challenge.
The number of theoretically possible paradigms is exponential in the number of affixes
(as paradigms are sets of affixes). Paradigms do not need to be disjoint; in real languages
they are typically not. Rather, words in the same part of speech tend to share affixes
across paradigms (Carstairs 1983). In addition, without any language-specific knowl-
edge, basically the only evidence at hand is co-occurrence of stems and affixes (i.e.,
when a word occurs in the corpus it evidences the co-occurrence of a [hypothetical]
stem and suffix making up that word). Paradigm induction would be an easy problem
if all affixes that could legally appear on a word did appear on each such word in a raw
text corpus. This is, as is well known, far from the case. A typical corpus distribution
is that a few lexemes appear very frequently but by far most lexemes appear once or
only a few times (Baayen 2001). What this means for morphology is that most lexemes
will appear with only one or a minority of their possible affixes, even in languages with
relatively little morphology. Of course, there is also the risk that some rare affix, for
example, the Classical Greek alternative medial 3p. pl. aorist imperative ending -σAary
</bodyText>
<footnote confidence="0.5152356">
13 As most approaches define their task as capturing the set of legal morphological forms, their goal should
be to compress W, but see Goldwater (2007, pages 53–59) for arguments for compressing C.
14 Note also that paradigm information can be fed back into the segmentation process in that some affixes
which do poorly according to some paradigm-related measure (e.g., affixes that do not take part in many
paradigms) can be weeded out.
</footnote>
<page confidence="0.981495">
327
</page>
<note confidence="0.279786">
Computational Linguistics Volume 37, Number 2
</note>
<bodyText confidence="0.997549266666667">
(Blomqvist and Jastrup 1998), may not appear at all even in a very large Classical Greek
corpus.
More formally, consider a morphological paradigm (set of suffixes) P that is a true
paradigm according to linguistic analysis. If k lexemes that are inflected according to P
occur in a corpus, each of the k lexemes will occur in 1 ≤ i ≤ |P |forms. The number of
forms i that a lexeme occurs in is likely not to be normally distributed. Most lexemes
will occur in only one form, and only very few, if any, lexemes will occur in all |P|
forms. It appears that for most languages and most paradigms, the number of lexemes
that occur in i forms tends to decrease logarithmically in i (Chan 2008, pages 75–84). As
an example, consider the three most common paradigms in Swedish and the frequency
of forms in Table 8.
Works which have attempted nevertheless to tackle the matter of paradigms, at
least for languages with one-slot morphology, include Zeman 2008, 2009, Hammarström
(2009b), and Monson (2009). They explicitly or implicitly make use of the following two
heuristics to narrow down the search space:
</bodyText>
<listItem confidence="0.7654315">
• Languages tend to have a small number of paradigms (where “small”
means fewer than 100 paradigms with at least 100 member stems each).
• Languages tend to have only small paradigms (where “small” means
fewer than 50), that is, the number of affixes in each paradigm is small.
Agglutinative languages, which have several layers of affixes, can be said
to obey this generalization in the sense that each layer has few members,
whereas conversely, the full paradigm achieves considerable size
combinatorially.
</listItem>
<bodyText confidence="0.9866515">
Although we know of no empirical evulation of them, in the impression of the present
authors, the two heuristics appear to be cross-linguistically valid.
Chan (2006) is an exceptionally clean study of inducing paradigms, assuming that
the segmentation is already given. The problem then takes the form of a matrix with
</bodyText>
<tableCaption confidence="0.657247">
Table 8
</tableCaption>
<bodyText confidence="0.999685588235294">
The three most common paradigms in Swedish according to the SALDO lexicon and
morphological resources (Borin, Forsberg, and Lönngren 2008), as computed on the SUC 1.0
corpus (Ejerhed and Källgren 1997) of 55,000 word types.
Adjective 1st decl Noun 3rd decl Verb 1st conj
(e.g., gul ‘yellow’) (e.g., tid ‘time’) (e.g., lag- ‘fix’)
-a 2022 -” 1619 -a 1001
-” 1821 -en 1141 -ade 948
-t 1572 -er 1072 -ar 883
-e 221 -erna 583 -at 579
-are 208 -s 310 -as 482
-s 114 -ens 259 -ande 423
-aste 90 -ernas 136 -ad 387
-ast 46 -ers 40 -ades 273
-as 39 -ats 207
-es 13 -andes 5
-ts 4 -ads 3
-ares 1
</bodyText>
<page confidence="0.73059">
328
</page>
<bodyText confidence="0.951352">
Hammarström and Borin Unsupervised Learning of Morphology
stems on one axis and suffixes on the other axis. Chan then makes use of known
techniques from linear algebra, in particular Latent Dirichlet Allocation, to break the full
matrix into smaller dense submatrices, which, when multiplied together, resemble the
full matrix. There is only one humanly tuned threshold, namely, when to stop breaking
into smaller parts.
</bodyText>
<subsectionHeader confidence="0.9997">
3.3 Group and Abstract
</subsectionHeader>
<bodyText confidence="0.999901594594595">
In contrast to the methods that use a heuristic for finding morpheme boundaries, the
grouping methods are much less sensitive to continuous segments. String edit distance
is the most straightforward metric for which to find pairs or sets of morphologically
related words (see, e.g., Gaussier 1999; Yarowsky and Wicentowski 2000; Schone and
Jurafsky 2001a; Baroni, Matiasek, and Trost 2002; Hu et al. 2005a; Bernhard 2006,
pages 101–117; Bernhard 2007; Majumder et al. 2007; Majumder, Mitra, and Pal 2008).
In addition, as unsupervised methods for semantic clustering (e.g., Latent Semantic
Analysis) and distributional clustering became more mature, these could be included as
well (Schone and Jurafsky 2000, 2001a; Schone 2001; Baroni, Matiasek, and Trost 2002;
Freitag 2005). More remarkable, however, is that Yarowsky and Wicentowski (2000)
and Wicentowski (2002, 2004) have shown that frequency signatures can also be used
to (heuristically) find morphologically related words. The example they use is sang
versus sing, whose relative frequency distribution in a corpus is 1,427/1,204 (or 1.19/1),
whereas singed15 versus sing is 9/1,204 (Yarowsky and Wicentowski 2000, pages 209–
210). This way, sing can be heuristically said to be parallel to sang rather than singed,
and indeed the distribution for singed versus singe (its true relative) is 9/2, that is, much
closer to 1.
Suppose now that groups of morphologically related words are somehow heuris-
tically extracted. For example, one group might be {play, player, played, playing} and
another might be {bark, barks, barked, barking}. The next step would be to find what is
common among several groups, not just one. Abstracting morphological alternations
given a family of groups is a thorny issue. For instance, Baroni, Matiasek, and Trost
(2002) leave the matter largely in the exploration phase. Wicentowski (2004) presents a
finished theory based on constraining the abstraction to find patterns in terms of prefix,
suffix, and stem alternations.
The outstanding question for the group-and-abstract approaches, related not only to
grouping but also to abstracting, is how to find one and the same morphological process
(umlauting, adding a suffix, etc.) that operates over a maximal number of groups. The
search space is huge, considering not only the group space but also the large number of
potential morphological processes itself.
The group-and-abstract approaches are also characterized by the ubiquitous use of
ad hoc thresholds. However, there are clear advantages in that they are in principle
capable of handling non-concatenative morphology and in that issues of semantics (of
stems) are addressed from the beginning.
The work by de Kock and Bossaert (1969, 1974, 1978), Yvon (1996), Medina Urrea
(2003) and partly Moon, Erk, and Baldridge (2009) can favorably be seen as a mid-way
between the border-and-frequency and group-and-abstract approaches as they rely on
</bodyText>
<page confidence="0.865473">
15 That is, the past tense of the verb singe.
329
</page>
<table confidence="0.423588">
Computational Linguistics Volume 37, Number 2
</table>
<tableCaption confidence="0.971467">
Table 9
</tableCaption>
<bodyText confidence="0.95901875">
Example feature values for the words ngithii(I went) and t˜ugithii(we went) adapted from
De Pauw and Wagacha (2007, page 1518). B=-features describe a subset at the start of the word
form, E=-features indicate patterns at the end of the word, and I=-features describe patterns
inside the word form.
</bodyText>
<figure confidence="0.724803571428571">
class features
ng˜ıthi˜ı B=n B=ng B=ng˜ı B=ng˜ıt B=ng˜ıth B=ng˜ıthi I=g I=g˜ı I=g˜ıt I=g˜ıth I=g˜ıthi
E=g˜ıthi˜ı I=˜ı I=˜ıt I=˜ıth I=˜ıthi E=˜ıthi˜ı I=t I=th I=thi E=thi˜ı I=h I=hi E=hi˜ı
I=i E=i˜ı
t˜ug˜ıthi˜ı B=t B=tu˜ B=t˜ug B=t˜ug˜ı B=t˜ug˜ıt B=t˜ug˜ıth B=t˜ug˜ıthi I=˜u I=˜ug I=˜ug˜ı I=˜ug˜ıt
I=˜ug˜ıth I=˜ug˜ıthi E=˜ug˜ıthi˜ı I=g I=g˜ı I=g˜ıt I=g˜ıth I=g˜ıthi E=g˜ıthi˜ı I=˜ı I=˜ıt
I=˜ıth I=˜ıthi E=˜ıthi˜ı I=t I=th I=thi E=thi˜ı I=h I=hi E=hi˜ı I=i E=i˜ı
</figure>
<bodyText confidence="0.8966455">
sets of four members with a particular affixation arrangement (“squares”),16 whose
existence is governed much by the frequency of the affixes in question.
</bodyText>
<subsectionHeader confidence="0.986133">
3.4 Features and Classes
</subsectionHeader>
<bodyText confidence="0.997255482758621">
The features-and-classes methods share with the group-and-abstract methods the virtue
of not being tied to segmental morpheme choices. As mentioned earlier, in this family of
methods a word is seen as made up of a set of features which have no internal order—
n-grams in Mayfield and McNamee (2003) and McNamee and Mayfield (2007), and
beginning/terminal/internal segments in De Pauw and Wagacha (2007).
For example, Table 9 shows two words and their features in G˜ik˜uy˜u, a tonal Bantu
language of Kenya. As designed by De Pauw and Wagacha (2007), initial (B=), middle
(I=), or final (E=) segments of a given word constitute its features. A majority of features
enumerated this way will not be morphologically relevant, whereas a minority is. For
example, in this case, I=h is just an arbitrary character without morpheme status,
whereas I=ngithi happens to be equal to a stem. The idea is that arbitrary features such
as I=h will be too common in the training data to provide a useful constraint, whereas
a more specialized feature like I=ngithi might indeed trigger useful morphological
generalization properties.
The input word list W thus transforms into a training set of word–feature pairs,
which can be fed into a standard maximum entropy classifier. The next step is, for each
word, to ask the classifier for the k closest classes, namely, words (which will include the
word itself and k − 1 others with significant feature overlap). Clearly, such clusters may
capture relations that string-edit-distance clustering does not. De Pauw and Wagacha
16 Based on the famous Greenberg square, which is a concrete means of illustrating the minimal
requirement for postulating a paradigm: We need a minimum of two attested stems and two attested
suffixes (or in the general case arbitrary morphological processes), where both stems must occur with
both suffixes:
stemA+sx1 stemB+sx1
stemA+sx2 stemB+sx2
As it is used in linguistics, both the stems and the suffixes in the square must represent attested
form–meaning combinations. This requirement is normally given up in the work reviewed here, where
only the form of postulated stems and affixes is available, but not the meaning. The Greenberg square
goes back to the age-old linguistic notion of proportional analogy (Anttila 1977).
</bodyText>
<page confidence="0.914286">
330
</page>
<bodyText confidence="0.940034833333333">
Hammarström and Borin Unsupervised Learning of Morphology
(2007, pages 1517–1518) further suggest how specific morphological information, such
as prefixes, tonal changes, etc., may be abstracted from such clusters.
Clearly, feature-based methods provide an interesting new avenue for non-
segmental and long-distance phenomena, but are so far largely unexplored and not free
from thresholds and parameters.
</bodyText>
<subsectionHeader confidence="0.998322">
3.5 Phonological Categories and Separation
</subsectionHeader>
<bodyText confidence="0.99989484">
These approaches specifically target the special kind of non-concatenative morphology
called intercalated morphology (or templatic morphology or root-and-pattern mor-
phology) famous mainly from Semitic languages, such as Arabic. They start out by
assuming that graphemes can be subdivided into those that take part in the root, and
those that take part in the pattern. For the languages so far targeted, Arabic (Rodrigues
and ´Cavar 2005, 2007; Xanthos 2007) and Amharic (Bati 2002), this is largely true, or
a transcription is used where it is largely true. Rodrigues and ´Cavar (2005, 2007) and
Bati (2002) hard-code the transition from the graphemic representation of a word to
its (potential) root and pattern parts. This can be said to constitute a strong language
specific bias, tantamount to supervision. Xanthos (2007), on the other hand, starts out
only by assuming that there exists a distinction between root and pattern graphemes
and subsequently learns which graphemes are which. See Goldsmith and Xanthos
(2009) for an excellent survey on how to do this (something which falls under learn-
ing phonological categories rather than morphology learning). Basically, it is possible
only because there are systematic combination constraints between different phonemes
(approximated by graphemes); for example, vowels and consonants alternate in a very
non-random manner.
Once each word is divided into its potential root and pattern, the morphology
learning problem is similar to morphology learning given roots and suffixes, that is,
the typical model for learning concatenative morphology, where the task is to weed out
noise, to decide where patterns (“suffixes”) start and end, which patterns are spurious,
and so on. All these authors who have addressed intercalated morphology use a variant
of MDL (see the border-and-frequency techniques in Section 3.2). The accuracy of ULM
on languages with intercalated morphology appears to be similar to the accuracy on
other languages (cf. Section 4.3).
</bodyText>
<subsectionHeader confidence="0.999756">
3.6 General Strengths and Weaknesses
</subsectionHeader>
<bodyText confidence="0.996644923076923">
A perhaps worrying tendency is that, despite extensive cross-citation, there is little
transfer between different groups of authors and there is a fair amount of duplication
of work. The lack of a broadly accepted theoretical understanding is possibly related
to this fact. Few approaches have an abstract model of how words are formed, and
thus cannot explain why (or why not) the heuristics employed fail, what kind of errors
are to be expected, and how the heuristics can be improved. Nevertheless, a model for
the simplest kind of concatenative morphology is emerging, namely, that two sets of
random strings, B and S, combine in some way to form a set of words W. For Gelbukh,
Alexandrov, and Han (2004), the segmentation task is to find minimal size |X |+ |Y |such
that W C {xy|x E X, y E Y}. For example, if W = {ad, ae, bd, be, cd, ce}, then the minimal
size |X |+ |Y |= 5 with X = {a, b, c} and Y = {d, e}. For Bacchin, Ferro, and Melucci
(2005) as well as in the word-segmentation version of Deligne (1996), the segmentation
task is to find a configuration of splits si for each wi = xiyi E W such that each xi and
</bodyText>
<page confidence="0.982116">
331
</page>
<note confidence="0.255486">
Computational Linguistics Volume 37, Number 2
</note>
<bodyText confidence="0.7217825">
yi occur in as many splits as possible. More precisely, the product, over all words,
of the number of splits for the parts x and y should be maximized. Formally, let xiyi
be the parts of wi induced by splits si and let p(x) = |{i|x = xi} |= |{wi|xyi = wi} |be
the number of words in which x equals the first part of the split and similarly let
p&apos;(y) = |{i|y = yi} |= |{wi|xiy = wi} |be the number of words in which y equals the last
part of the split. Then the task is to find splits that maximize the following expression:
</bodyText>
<equation confidence="0.7585785">
11 arg max p(xi) · p,(yi)
[s1,...,s|W|] wi∈W
</equation>
<bodyText confidence="0.9916183">
For example, if W = {ad, ae, bd, be, cd, ce, ggg}, then the configuration of splits a|d, a|e, b|d,
b|e, c|d, c|e, g|gg yields the product (2 · 3)6 · (1 · 1).
Brent (1999) devises a precise, but more elaborate, way of constructing W from B
and S, but at the cost of a large search space, and whose global maximum is hard to
characterize intuitively. The same holds for the extension by Snover (2002). Kontorovich,
Don, and Singer (2003), Snyder and Barzilay (2008), Goldwater (2007), Johnson (2008),
and Poon, Cherry, and Toutanova (2009) should also be noted for containing generative
models.
Most approaches, of any of the kinds (a)–(d) described in Section 3.1, explicitly or
implicitly target languages which have (close to) one-slot morphology, that is, a word
(or stem) typically takes not more than one prefix and not more than one suffix. Many
(indeed most; Dryer 2005) languages deviate more or less from this model. At first,
it may seem that multi-slot morphology can be handled by the same algorithms as
one-slot morphology, by iterating the process used for one-slot morphology. A decade
of ULM has shown that the matter is not so simple, because heuristics for one slot
languages do not necessarily generalize to the outermost slot of a multi-slot language.
The (c) and (d) approaches do not combine easily with the others but it is con-
ceivable that the (a) and (b) type of approaches may be mutually enhancing. Results
from the (a) methods may serve to cut down the search space for the (b) methods,
and the (b) methods may provide a way to circumvent thresholds for the (a) methods.
There is also the possibility of serial combination where, for example, the (a) methods
target concatenative morphology and the (b)—or (c)—methods attempt the remaining
cases. Presumably because most methods so far do not produce a clean, well-defined
result, various forms of hybridization of techniques by different authors have yet to be
systematically explored.
Lastly, there are scattered attempts to address morphophonological changes in a
principled way, though so far these have been developed in close connection with a
particular segmentation method and target language (Schone 2001; Schone and Jurafsky
2001a; Wicentowski 2002, 2004; Tepper 2007; Kohonen, Virpioja, and Klami 2008; Tepper
and Xia 2008).
</bodyText>
<sectionHeader confidence="0.999288" genericHeader="keywords">
4. Discussion
</sectionHeader>
<subsectionHeader confidence="0.98866">
4.1 Language Dependence of ULM
</subsectionHeader>
<bodyText confidence="0.9998806">
As we mentioned in Section 3.6, most approaches have an explicit bias towards certain
kinds of morphological systems, those for which we introduced the label “one-slot mor-
phology.” This is of course not a problem, if the purpose is to bootstrap a morphology
for some languages which happen to belong to the right type. If the purpose is to say
something about human language acquisition or language learning, or if the aim is to
</bodyText>
<page confidence="0.973723">
332
</page>
<note confidence="0.593053">
Hammarström and Borin Unsupervised Learning of Morphology
</note>
<bodyText confidence="0.997405210526316">
devise a method that should work with any language, such a bias naturally becomes
problematic.
The two human learning analogues which have most frequently been proposed
in the literature on ULM and other machine learning of morphology are those of
language acquisition and of linguistic analysis (e.g., as carried out as part of linguistic
fieldwork). Depending on which of the two we choose, the kinds of biases that we may
or may not allow become different. Language acquisition in humans is oral (or sign, but
for practical reasons, we are leaving sign languages completely out of the discussion
here), so expecting written input with word delimiters would then be an inadmissible
bias. ULM as delimited in Section 1 is definitely closer to linguistic analysis than to
language acquisition.
It may be instructive at this point to see what kinds of knowledge are supposed to
be required in order to carry out the discovery procedures mentioned in Section 2:
An analyst approaches a language which either he already knows in some practical
way or with which he sets about to familiarize himself—preferably in a language
learning situation. The analyst’s background is the sum total of his practical knowledge
of other languages, his previous analytical experience, and what he has learned
from the linguistic research of other people. With this knowledge of the language
to be analyzed and with this background knowledge, he makes certain guesses
about the grammatical structure of the language. He then submits these guesses to a
series of systematic checks in which he confirms, disproves, or modifies his original
guesses—and makes a few better guesses en route. This systematic evaluation is
based on a theory of the structure of language, and the theory itself (while containing
elements of creative thinking) is based on empirical study. (Longacre 1964, page 12)
Mutatis mutandis, the procedure described in this quote, contains most elements of ULM
and related methods proposed in the literature. Note that the quote just given stresses
the importance of the knowledge that the linguist brings to the analysis and which in-
forms the whole analytical process. This suggests that there may be a level of general
knowledge about language (in general or a useful subset of languages), or about lin-
guistic analysis, or both, which would be useful to ULM in general, something like the
“knowledge” that white space is a word delimiter in written text, but on a higher level.
One component of a research program on ULM would then be to formulate this kind
of general knowledge in a way which makes sense given that the object of study is lan-
guage, to test it, and to share it with the community of linguistic scholars. A concrete
illustration could be the way that the old notion of proportional analogy (Anttila 1977)
is refined and formalized in various ways and used to test segmentation hypotheses
in works on ULM from the earliest times onwards (e.g., the “squares” mentioned in
Section 3.3).
</bodyText>
<subsectionHeader confidence="0.987865">
4.2 ULM and Semantics
</subsectionHeader>
<bodyText confidence="0.923987142857143">
As traditionally conceived, an inflectional paradigm links a set of word forms to
structural descriptions expressed in terms of a stem carrying a lexical meaning and
some formal expression of one or more morphosyntactic categories (or grammatical
meanings) taken from a closed, small set of such categories. This bears emphasizing,
because ULM work generally has been concerned only with the formal expression side
of morphology; that is, instead of the traditional
table-s ‘table N PL’
</bodyText>
<page confidence="0.977845">
333
</page>
<figure confidence="0.840677">
Computational Linguistics Volume 37, Number 2
table-s ‘table V 3SG’
it will give us simply
table-s
table-s
</figure>
<bodyText confidence="0.926757444444445">
although it may tell us that the stem table appears in two paradigms.
As far as we know, there have been no attempts to induce functional labels using
ULM, although it is conceivable that the same kind of techniques used, for example,
in order to cluster words semantically (e.g., Latent Semantic Indexing/Analysis or
Random Indexing), could be used also to classify the resulting morphs from a ULM
segmentation (cf. Schone and Jurafsky [2001b] for a study of inducing part-of-speech
class labels in a setting similar to that of ULM). The labelling problem can easily be
considered independent of ULM by using a hand-segmented (or segmented by a hand-
built morphological parser) input corpus.
</bodyText>
<subsectionHeader confidence="0.991109">
4.3 Is ULM of Any Use?
</subsectionHeader>
<bodyText confidence="0.99967776">
As we said in Section 2, there is an explicit expectation frequently encountered in the
more recent literature that ULM and other unsupervised methods could be employed
in order to rapidly and cheaply (in terms of human effort) bootstrap basic language
technology resources for new languages. However, looking at the literature, it seems
that—at least in the area of inflectional morphology—the only approaches that have
so far produced substantial results are the old-fashioned, hand-coded grammar-based
ones, such as the work described by Trosterud (2004), where finite-state morphological
processors and constraint grammar-based disambiguation components are developed
for a number of related languages. The fact that the languages are related is of great
help when dealing with successive languages after the first one. The morphological
component for the first language, North Sámi, required approximately 2.5 person-years
of highly qualified linguistic expert work to reach the prototype stage, whereas the
analogous module for the closely related Lule Sámi was completed in an additional
six months (Trosterud 2006).17 This and other work in the same vein reported in the
literature (e.g., by Artola-Zubillaga 2004 and Maxwell and David 2008) is characterized
by deep and long-lasting involvement by linguistic expertise and further often by the
creative use of digitized versions of conventional printed linguistic resources, especially
dictionaries. The following observation is perhaps trivial, but bears stressing, because
it is in fact often not heeded in practice: For this kind of approach to work, it is
necessary that tools for providing systems with linguistic knowledge use a conceptual
apparatus and notation familiar to the linguists who are supposed to be working with
them. Relevant to our purposes here, the same holds for any attempt to kickstart the
development of a morphological analyzer by using ULM: If the expectation is that
the output of ULM should be manually “post-edited,” this output must of course be
intelligible to the linguist doing the post-editing.
</bodyText>
<footnote confidence="0.513396">
17 As pointed out by one anonymous reviewer, this suggests that with the right organization of information
flow among machine-learning components, ULM, too, could benefit from working with several closely
related languages simultaneously.
</footnote>
<page confidence="0.988469">
334
</page>
<note confidence="0.592959">
Hammarström and Borin Unsupervised Learning of Morphology
</note>
<bodyText confidence="0.99958296">
Most ULM approaches reported in the literature are small proof-of-concept experi-
ments, which generally founder on the lack of evaluation data. The MorphoChallenge18
series does provide adequate gold-standard evaluation data for Finnish, English, Ger-
man, Arabic, and Turkish as well as task-based Information Retrieval (IR) evaluation
data for English, German, and Finnish. It can be seen that ULM systems are mature
enough to enhance IR, but so far, ULM systems are not close to full accuracy on the
gold standard and outside commentators have generally been unimpressed with these
results (e.g., Mahlow and Piotrowski 2009, page vi). However, many (most?) of the
strong-looking systems reported in the literature have not, for one reason or another,
taken part in the MorphoChallenge. Taking MorphoChallenge results and proof-of-
concept reports together, it seems that high accuracy by ULM systems is presently only
achievable if the language has small amounts of one-slot concatenative morphology,
whereas for morphologically more complex languages, parameter tuning and/or lower
accuracy is to be expected.
We are not yet in a position to assess whether there are other tasks than IR which, in
general, benefit significantly from (noisy) ULM, such as Speech Recognition (Hirsimäki
et al. 2003, 2005, 2006; Kurimo et al. 2006) or Machine Translation (Sereewattana 2003;
Virpioja et al. 2007; Bojar, Straˇnák, and Zeman 2008; Kirik and Fishel 2008; De Gispert
et al. 2009; Fishel and Kirik 2010) because almost only the Morfessor system has been
tested, and results are, if positive, not completely unambiguous. One usage of noisy
ULM, at least, is for smoothing language identification models (Hammarström 2007a;
Ceylan and Kim 2009).
Further, ULM approaches are data-hungry, which precludes their use with many
low-density languages. There is much ongoing work addressing these issues, however,
so we can probably expect some progress in this area (Bird 2009).
</bodyText>
<subsectionHeader confidence="0.992395">
4.4 Future Directions
</subsectionHeader>
<bodyText confidence="0.999093684210526">
In practice, the near future should define a high-accuracy threshold-minimal system for
one-slot morphology languages, using refinements of ideas already extant.
A major challenge, and the reason for duplication of work in the past, is to find a
theory that explains why (or why not) a given algorithm works. Further study of theo-
retical properties of (stochastic) combining of string sets/bags are likely to hold the key
to the culmination of the border-and-frequency methods—not further experimentation
with ad hoc heuristics. The recent increased interest in Bayesian generative models in
general in NLP may possibly serve as a catalyst.
In the group-and-abstract paradigm, working with feature sets of a word, as in
De Pauw and Wagacha (2007), is an ingenious generalization that holds numerous
advantages over string edit distances. Feature set comparisons are naturally defined
over arbitrary collections, whereas string edit distances work on pairs of strings. Many
morphologically related words differ in several characters and are therefore not particu-
larly close in edit distance. Features instead of edit distances provide a neat framework,
based on global properties of the feature distribution, of capturing the fact that some
character mismatches do not really matter, whereas some character matches (although
not necessarily long) are very significant.
For paradigm induction, it is clear that the ULM field has not made use of the large
literature on clustering in other fields. Chan (2006) is a step in this direction, but further
</bodyText>
<footnote confidence="0.510754">
18 Web site www.cis.hut.fi/morphochallenge2009/ accessed 10 September 2009.
</footnote>
<page confidence="0.974986">
335
</page>
<note confidence="0.47418">
Computational Linguistics Volume 37, Number 2
</note>
<bodyText confidence="0.998798625">
steps are lacking; in particular, spectral clustering (of some kind) has not been explored
for paradigm induction in ULM. Also here, given the typical skewed stem distributions
and skewed suffix distributions (exemplified in Section 3.2), some theoretical work is
needed to determine its implications for clustering.
Finally, we see ample opportunity for empirical investigations into lesser-known
languages for which data has become available only recently (Abney and Bird 2010).
This would clarify the potential of ULM usefulness for underdescribed and under-
resourced languages.
</bodyText>
<sectionHeader confidence="0.995522" genericHeader="introduction">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.9999733">
After more than half a century of research, the field of ULM has made good progress (as
have many other areas of computational linguistics), but there is still a long way to go
before it will become practically useful or even theoretically interesting to linguists. In
the terms of Table 1 in Section 1, the state of the art of ULM is somewhere in the region of
“Segmentation” and “Inflection tables,” if we are talking about linguistic form, but there
has been next to no progress at all when it comes to linguistic meaning (e.g., functional
labeling of affixes).
In the early days of ULM, the expectation was that it should constitute—when even-
tually achieved sometime in the future—a formalized version of a linguistic discovery
procedure, that is, a knowledge-heavy enterprise. Instead, recent successes in the area
have been largely contingent upon the rapid development in computational linguistics
of statistical and information-theoretic knowledge-light (but robust) methodologies.
We believe (like Wintner 2009 for computational linguistics in general), however,
that if ULM is to become a serious alternative to—or, equally likely, a natural component
of—manually built computational morphology systems for a wide and diverse range of
languages, and especially if we are to make headway in the area of semantics, we need
to see more interaction between the present approaches to ULM with the computational
techniques and mathematical modeling tools they can bring to bear on the problem on
the one hand, and typologically informed linguistic research on morphology founded
on a vast store of knowledge and methodology refined over two millennia on the other.
</bodyText>
<sectionHeader confidence="0.995736" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999901">
The authors wish to thank three anonymous
referees for helpful comments and
suggestions.
</bodyText>
<sectionHeader confidence="0.941187" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.870764333333333">
Abney, Steven and Steven Bird. 2010. The
human language project: Building a
universal corpus of the world’s languages.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 88–97, Uppsala.
Altmann, Gabriel and Werner Lehfeldt. 1980.
Einführung in die quantitative Phonologie,
volume 7 of Quantitative Linguistics.
Bochum: Brockmeyer.
Andreev, Nikolaj Dmitrieviˇc. 1959.
Modelirovanije jazyka na base ego
statistiˇceskoj i teoretiko-množestvennoj
struktury. In Tezisy sovešˇcanija po
matematiˇceskoj lingvistike, 14–21 Aprelja 1959
goda. Ministerstvo vysšego obrazovanija
SSSR, Leningrad, pages 15–22.
Andreev, Nikolaj Dmitrieviˇc. 1963.
Algoritmy statistiko-kombinatornogo
modelirovanija morfologii, sintaksisa,
slovoobrazovanija i semantiki. In
</bodyText>
<construct confidence="0.790775181818182">
Materialy po matematiˇceskoj lingvistike i
mašinomu perevodu: Sbornik II. Izdatel’stvo
Leningradskogo universiteta, Leningrad,
pages 3–44.
Andreev, Nikolaj Dmitrieviˇc. 1965a. Opyt
statistiko-kombinatornogo vydelenija
pervogo morfologiˇceskogo tipa v
vengerskom jazyke. In Nikolaj Dmitrieviˇc
Andreev, editor, Statistiko-kombinatornoe
modelirovanie jazykov. Nauka, Leningrad,
pages 205–211.
</construct>
<footnote confidence="0.607085">
Andreev, Nikolaj Dmitrieviˇc, editor. 1965b.
Statistiko-kombinatornoe modelirovanie
jazykov. Nauka, Leningrad.
</footnote>
<page confidence="0.985212">
336
</page>
<table confidence="0.879640838709677">
Hammarström and Borin Unsupervised Learning of Morphology
Andreev, Nikolaj Dmitrieviˇc. 1967.
Statistiko-kombinatornye metody v
teoretiˇceskom i prikladnom jazykovedenii.
Nauka, Leningrad.
Andreeva, L. D. 1963. Statistiko-
kombinatornoe vydelenie paradigmy
pervogo mofologiˇceskogo tipa v russkom
jazyke. In Materialy po matematiˇceskoj
lingvistike i mašinnomu pervodu: Sbornik II.
Izdatel’stvo Leningradskogo universiteta,
Leningrad, pages 45–60.
Anttila, Raimo. 1977. Analogy. Mouton,
The Hague.
Antworth, Evan L. 1990. PC-KIMMO: A
two-level processor for morphological analysis.
Occasional Publications in Academic
Computing 16. Summer Institute of
Linguistics, Dallas.
Arabsorkhi, Mohsen and Mehrnoush
Shamsfard. 2006. Unsupervised discovery
of Persian morphemes. In Proceedings of the
11th Conference of the European Chapter of the
Association for Computational Linguistics,
EACL 2006, pages 175–178, Trento.
Argamon, Shlomo, Navot Akiva, Amihood
Amir, and Oren Kapah. 2004. Efficient
unsupervised recursive word
segmentation using minimum description
length. In Proceedings of COLING 2004,
pages 1058–1064, Geneva.
</table>
<bodyText confidence="0.934718212121212">
Artola-Zubillaga, Xabier. 2004. Laying lexical
foundations for NLP: The case of Basque
at the ixa research group. In SALTMIL
Workshop at LREC 2004: First Steps in
Language Documentation for Minority
Languages, pages 9–18, Lisbon.
Atwell, Eric and Andrew Roberts. 2005.
Combinatory hybrid elementary analysis
of text. In Proceedings of MorphoChallenge
2005, pages 37–41, Helsinki University of
Technology, Helsinki.
Baayen, Harald R. 2001. Word Frequency
Distributions, volume 18 of Text, Speech,
and Language Technology. Kluwer,
Dordrecht.
Bacchin, Michela, Nicola Ferro, and Massimo
Melucci. 2002a. The effectiveness of a
graph-based algorithm for stemming.
In ICADL ’02: Proceedings of the 5th
International Conference on Asian Digital
Libraries, volume 2555 of Lecture Notes
in Computer Science, pages 117–128,
Singapore.
Bacchin, Michela, Nicola Ferro, and Massimo
Melucci. 2002b. University of Padua at
CLEF 2002: Experiments to evaluate a
statistical stemming algorithm. In Working
Notes for CLEF 2002: Cross-Language Evaluation
Forum Workshop, pages 161–168, Rome.
Bacchin, Michela, Nicola Ferro, and Massimo
Melucci. 2005. A probabilistic model for
stemmer generation. Information Processing
and Management, 41(1):121–137.
</bodyText>
<listItem confidence="0.929501333333333">
Baroni, Marco. 2000. Distributional Cues in
Morpheme Discovery: A Computational Model
and Empirical Evidence. Ph.D. thesis,
University of California, Los Angeles.
Baroni, Marco. 2003. Distribution-driven
morpheme discovery: A computational/
</listItem>
<bodyText confidence="0.938531510204082">
experimental study. Yearbook of Morphology,
213–248.
Baroni, Marco, Johannes Matiasek, and
Harald Trost. 2002. Unsupervised
discovery of morphologically related
words based on orthographic and
semantic similarity. In Proceedings of the
Workshop on Morphological and Phonological
Learning of ACL/SIGPHON-2002,
pages 48–57, Philadelphia.
Batchelder, E. O. 1997. Computational Evidence
for the Use of Frequency Information in
Discovery of the Infant’s First Lexicon. Ph.D.
thesis, City University of New York.
Bati, Tesfaye Bayu. 2002. Automatic
morphological analyser: An experiment
using unsupervised and autosegmental
approach. Master’s thesis, Addis Ababa
University, Ethiopia.
Belkin, Mikhail and John Goldsmith.
2002. Using eigenvectors of the bigram
graph to infer morpheme identity. In
Morphological and Phonological Learning:
Proceedings of the 6th Workshop of the ACL
Special Interest Group in Computational
Phonology (SIGPHON), pages 41–47,
Philadelphia, PA.
Bernhard, Delphine. 2005a. Segmentation
morphologique à partir de corpus. Actes
de TALN &amp; RÉCITAL 2005, volume 1,
pages 555–564, Dourdan.
Bernhard, Delphine. 2005b. Unsupervised
morphological segmentation based on
segment predictability and word segments
alignment. In Mikko Kurimo, Mathias
Creutz, and Krista Lagus, editors,
Unsupervised segmentation of words into
morphemes – Challenge 2005, pages 18–22,
Helsinki University of Technology,
Helsinki.
Bernhard, Delphine. 2006. Apprentissage
de connaissances morphologiques pour
l’acquisition automatique de ressources
lexicales. Ph.D. thesis, Université Joseph
Fourier – Grenoble I.
Bernhard, Delphine. 2007. Apprentissage
non supervisé de familles morphologiques
par classification ascendante hiérarchique.
In Actes de la 14e conférence sur le Traitement
</bodyText>
<page confidence="0.981209">
337
</page>
<figure confidence="0.849478">
Computational Linguistics Volume 37, Number 2
Automatique des Langues Naturelles, TALN
2007, volume 1, pages 367–376, Toulouse.
</figure>
<figureCaption confidence="0.972782">
Bernhard, Delphine. 2008. Simple morpheme
labelling in unsupervised morpheme
analysis. In Carol Peters, Valentin Jijkoun,
Thomas Mandl, Henning Müller, Douglas
W. Oard, Anselmo Peñas, Vivien Petras,
and Diana Santos, editors, Advances in
Multilingual and Multimodal Information
Retrieval, 8th Workshop of the Cross-
Language Evaluation Forum, CLEF 2007,
Budapest, Hungary, September 19–21,
2007, Revised Selected Papers,
pages 873–880. Springer-Verlag, Berlin.
</figureCaption>
<bodyText confidence="0.894687346153846">
Bharati, Akshar, S. M. Bendre Rajeev Sangal,
Pavan Kumar, and Aishwarya. 2001.
Unsupervised improvement of
morphological analyzer for inflectionally
rich languages. In Proceedings of the Sixth
Natural Language Processing Pacific Rim
Symposium (NLPRS-2001), pages 685–692,
Tokyo.
Bickel, Balthasar, Goma Banjade, Martin
Gaenszle, Elena Lieven, Netra Paudyal,
Ichchha Rai, Manoj Rai, Novel Kishor Rai,
and Sabine Stoll3. 2007. Free prefix ordering
in Chintang. Language, 83(1):43–73.
Bird, Steven. 2009. Last words: Natural
language processing and linguistic
fieldwork. Computational Linguistics,
35(3):469–474.
Blomqvist, Jerker and Poul Ole Jastrup. 1998.
Grekisk grammatik: Graesk grammatik,
2 edition. Akademisk Forlag, København.
Bloomfield, Leonard. 1933. Language. Henry
Holt &amp; Co, New York.
Bojar, Ondˇrej, Pavel Straˇnák, and Daniel
Zeman. 2008. English–Hindi translation in
21 days. In Proceedings of the ICON-2008
NLP Tools Contest, pages 4–7, Pune.
Bordag, S. 2005a. Unsupervised
knowledge-free morpheme boundary
detection. Paper presented at the
Proceedings of Recent Advances in Natural
Language Processing 2005 (RANLP ’05),
Borovets.
Bordag, Stefan. 2005b. Two-step approach to
unsupervised morpheme segmentation. In
Mikko Kurimo, Mathias Creutz, and Krista
Lagus, editors, Unsupervised segmentation
of words into morphemes – Challenge 2005,
pages 23–27, Helsinki University of
Technology, Helsinki.
Bordag, Stefan. 2007. Elements of
Knowledge-Free and Unsupervised Lexical
Acquisition. Ph.D. thesis, University of
Leipzig, Leipzig.
Bordag, Stefan. 2008. Unsupervised and
knowledge-free morpheme segmentation
and analysis. In Carol Peters, Valentin
Jijkoun, Thomas Mandl, Henning Müller,
Douglas W. Oard, Anselmo Peñas, Vivien
Petras, and Diana Santos, editors, Advances
in Multilingual and Multimodal Information
Retrieval, 8th Workshop of the Cross-
Language Evaluation Forum, CLEF 2007,
Budapest, Hungary, September 19–21,
2007, Revised Selected Papers,
pages 881–891. Springer-Verlag, Berlin.
Borin, Lars. 1991. The Automatic Induction of
Morphological Regularities. Ph.D. thesis,
Uppsala University.
Borin, Lars. 2009. One in the bush:
Low-density language technology.
Research Reports from the Department of
Swedish, No. GU-ISS-09-1. University of
Gothenburg.
Borin, Lars, Markus Forsberg, and Lennart
Lönngren. 2008. The hunting of the
BLARK - SALDO, a freely available lexical
database for Swedish language technology.
In Joakim Nivre, Mats Dahllöf, and Beáta
Megyesi, editors, Resourceful language
technology: Festschrift in honor of Anna
Sågvall Hein, volume 7 of Studia Linguistica
Upsaliensia. Acta Universitatis Upsaliensis,
Uppsala, pages 21–32.
Brasington, Ron, Steve Jones, and Colin
Biggs. 1988. The automatic induction of
morphological rules. Literary and Linguistic
Computing, 3(2):71–78.
Brent, Michael. 1993. Minimal generative
explanations: A middle ground between
neurons and triggers. In Proceedings of
the Fifteenth Annual Conference of the
Cognitive Science Society, pages 28–36,
Boulder, CO.
Brent, Michael R. 1999. An efficient,
probabilistically sound algorithm for
segmentation and word discovery.
Machine Learning, 34:71–105.
Brent, Michael R., S. Murthy, and
A. Lundberg. 1995. Discovering
morphemic suffixes: A case study in
minimum description length induction.
In Fifth International Workshop on Artificial
Intelligence and Statistics, pages 482–490.
Fort Lauderdale, FL.
Calderone, Basilio. 2008. Unsupervised
Learning of Linguistic Structures. Ph.D.
thesis, Scuola Normale Superiore, Pisa.
Carstairs, Andrew. 1983. Paradigm economy.
Journal of Linguistics, 19:115–125.
´Cavar, Damir, Joshua Herring, Toshikazu
Ikuta, Paul Rodrigues, and Giancarlo
Schrementi. 2004a. On induction of
morphology grammars and its role in
bootstrapping. In Proceedings of Formal
</bodyText>
<page confidence="0.993855">
338
</page>
<note confidence="0.642654">
Hammarström and Borin Unsupervised Learning of Morphology
</note>
<bodyText confidence="0.6471735">
Grammar 2004, pages 47–62, ESSLLI,
Nancy, France.
</bodyText>
<figureCaption confidence="0.851776">
´Cavar, Damir, Joshua Herring, Toshikazu
Ikuta, Paul Rodrigues, and Giancarlo
Schrementi. 2004b. On statistical
parameter setting. In Proceedings of the
First Workshop on Psycho-Computational
Models of Human Language Acquisition,
pages 9–16, Geneva.
´Cavar, Damir, Jushua Herring, Toshikazu
Ikuta, Paul Rodrigues, and Giancarlo
Schrementi. 2006. On unsupervised
grammar induction from untagged
corpora. In P. Kaszubski, editor, PSiCL:
</figureCaption>
<subsubsectionHeader confidence="0.506773">
Poznan’ Studies in Contemporary
</subsubsectionHeader>
<bodyText confidence="0.847948300970874">
Linguistics, volume 41. Poznan’,
Poland: Adam Mickiewicz University,
pages 57–71.
´Cavar, Damir, Paul Rodrigues, and
Giancarlo Schrementi. 2006. Unsupervised
morphology induction for part-of-speech
tagging. In Aviad Eilam, Tatjana Scheffler,
and Joshua Tauberer, editors, Proceedings of
the 29th Annual Penn Linguistics Colloquium,
volume 12(1) of U. Penn Working Papers in
Linguistics. University of Pennsylvania
Press, Philadelphia, pages 29–41.
Ceylan, Hakan and Yookyung Kim. 2009.
Language identification of search engine
queries. In ACL-IJCNLP ’09: Proceedings
of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language
Processing of the AFNLP: Volume 2,
pages 1066–1074, Morristown, NJ.
Chan, Erwin. 2006. Learning probabilistic
paradigms for morphology in a latent
class model. In Proceedings of the Eighth
Meeting of the ACL Special Interest Group on
Computational Phonology and Morphology at
HLT-NAACL 2006, pages 69–78, New York
City, NY.
Chan, Erwin. 2008. Structures and
Distributions in Morphology Learning. Ph.D.
thesis, University of Pennsylvania,
Philadelphia, PA.
Cho, Sehyeong and Seung-Soo Han. 2002.
Automatic stemming for indexing of an
agglutinative language. In T. Yakhno,
editor, Advances in Information Systems,
volume 2457 of Lecture Notes in Computer
Science. Springer-Verlag, Berlin,
pages 154–165.
Clark, Alexander. 2001. Unsupervised
Language Acquisition. Ph.D. thesis,
University of Sussex.
Creutz, Mathias. 2003. Unsupervised
segmentation of words using prior
distributions of morph length and
frequency. In Proceedings of the ACL
2003, pages 280–287, Sapporo.
Creutz, Mathias. 2006. Induction of the
Morphology of Natural Language:
Unsupervised Morpheme Segmentation
with Application to Automatic Speech
Recognition. Ph.D. thesis, Helsinki
University of Technology, Espoo,
Finland.
Creutz, Mathias and Krista Lagus. 2002.
Unsupervised discovery of morphemes.
In Proceedings of the 6th Workshop of the
ACL Special Interest Group in Computational
Phonology (SIGPHON), pages 21–30,
Philadelphia, PA.
Creutz, Mathias and Krista Lagus. 2004.
Induction of a simple morphology
for highly-inflecting languages. In
Proceedings of the 7th Meeting of the ACL
Special Interest Group in Computational
Phonology (SIGPHON), pages 43–51,
Barcelona.
Creutz, Mathias and Krista Lagus. 2005a.
Inducing the morphological lexicon of a
natural language from unannotated text.
In Proceedings of the International and
Interdisciplinary Conference on Adaptive
Knowledge Representation and Reasoning
(AKRR ’05), pages 106–113, Espoo.
Creutz, Mathias and Krista Lagus. 2005b.
Morfessor in the Morpho Challenge. In
Mikko Kurimo, Mathias Creutz, and Krista
Lagus, editors, Unsupervised segmentation of
words into morphemes – Challenge 2005,
pages 12–17, Helsinki University of
Technology, Helsinki.
Creutz, Mathias and Krista Lagus. 2005c.
Unsupervised morpheme segmentation
and morphology induction from text
corpora using morfessor 1.0. Technical
report A81, Publications in Computer and
Information Science, Helsinki University
of Technology.
Creutz, Mathias and Krista Lagus. 2007.
Unsupervised models for morpheme
segmentation and morphology learning.
ACM Transactions on Speech and Language
Processing, 4(1–3):1–33.
Creutz, Mathias, Krista Lagus, Krister
Lindén, and Sami Virpioja. 2005.
Morfessor and hutmegs: Unsupervised
morpheme segmentation for
highly-inflecting and compounding
languages. In Proceedings of the Second
Baltic Conference on Human Language
Technologies, pages 107–112, Tallinn.
Creutz, Mathias, Krista Lagus, and Sami
Virpioja. 2005. Unsupervised morphology
induction using morfessor. In Finite State
</bodyText>
<page confidence="0.99474">
339
</page>
<figure confidence="0.5680075">
Computational Linguistics Volume 37, Number 2
Methods in Natural Language Processing: 5th
International Workshop, FSMNLP 2005,
pages 300–301, Helsinki.
</figure>
<figureCaption confidence="0.839574828571429">
Cromm, Oliver. 1997. Affixerkennung in
deutschen wortformen: Ein nicht-
lexikalisches segmentierungsverfahren
nach N. D. Andreev. LDV-Forum,
14(2):4–13.
Cucerzan, Silviu and David Yarowsky.
2002. Bootstrapping a multilingual
part-of-speech tagger in one person-day.
In Proceedings of CoNLL-2002, pages 1–7,
Taipei.
Daelemans, Walter. 2004. Computational
linguistics. In Geert Booij, Christian
Lehmann, Joachim Mugdan, and Stavros
Skopetas, editors, Morphologie/Morphology:
Ein internationales Handbuch zur Flexion
und Wortbildung [An International
Handbook on Inflection and Word-Formation],
volume 17.2 of Handbücher zur Sprach- und
Kommunikationswissenschaft. Mouton de
Gruyter, Berlin, pages 1893–1900.
Dang, Minh Thang and Saad Choudri. 2005.
Simple unsupervised morphology analysis
algorithm (SUMAA). In Mikko Kurimo,
Mathias Creutz, and Krista Lagus, editors,
Proceedings of MorphoChallenge 2005,
pages 47–51, Helsinki University of
Technology, Helsinki.
Dasgupta, Sajib. 2007. Toward language-
independent morphological segmentation
and part-of-speech induction. Master’s
thesis, The University of Texas at Dallas.
Dasgupta, Sajib and Vincent Ng. 2006.
Unsupervised morphological parsing of
bengali. Language Resources and Evaluation,
3–4:311–330.
</figureCaption>
<bodyText confidence="0.78876448">
Dasgupta, Sajib and Vincent Ng. 2007a.
High-performance, language-independent
morphological segmentation. In Human
Language Technologies 2007: The Conference
of the North American Chapter of the
Association for Computational Linguistic,
pages 155–163, Rochester, NY,
Association for Computational
Linguistics.
Dasgupta, Sajib and Vincent Ng. 2007b.
Unsupervised word segmentation
for Bangla. In Proceedings of the 5th
International Conference on Natural
Language Processing (ICON 2007),
pages 15–24, Hyderabad.
De Gispert, Adrià, Sami Virpioja, Mikko
Kurimo, and William Byrne. 2009.
Minimum bayes risk combination of
translation hypotheses from alternative
morphological decompositions. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 73–76,
Boulder, CO.
</bodyText>
<figureCaption confidence="0.554760516129032">
de Kock, Josse and Walter Bossaert. 1969.
Towards an automatic morphological
segmentation. In International Conference
on Computational Linguistics, COLING,
pages 10–11, Sånga-Säby.
de Kock, Josse and Walter Bossaert. 1974.
Introducción a la lingüística automática en las
lenguas Románicas, volume 202 of Biblioteca
románica hispánica 2: Estudios y ensayos.
Gredos, Madrid.
de Kock, Josse and Walter Bossaert. 1978. The
Morpheme: An Experiment in Quantitative
and Computational Linguistics. Van Gorcum,
Amsterdam.
De Pauw, Guy and Peter W. Wagacha. 2007.
Bootstrapping morphological analysis of
G˜ık˜uy˜u using maximum entropy learning.
In Proceedings of the 8th Annual Conference
of the International Speech Communication
Association (INTERSPEECH 2007),
pages 1517–1520, Antwerp.
Déjean, Hervé. 1998a. Concepts et algorithmes
pour la découverte des structures formelles des
langues. Ph.D. thesis, Université de Caen
Basse Normandie.
Déjean, Hervé. 1998b. Morphemes as a
necessary concept for structures discovery
from untagged corpora. In NeMLaP3/
CoNLL98 Workshop on Paradigms and
Grounding in Language Learning,
pages 295–298, Philadephia, PA.
</figureCaption>
<bodyText confidence="0.94927675">
Deligne, Sabine. 1996. Modèles de séquences de
longueurs variables: application au traitement
du langage écrit et de la parole. Ph.D. thesis,
École Nationale Supérieure des
Télécommunications, Paris.
Deligne, Sabine and Frédéric Bimbot.1997.
Inference of variable-length linguistic
and acoustic units by multigrams. Speech
Communication, 23(3):223–241.
Demberg, Vera. 2007. A language-
independent unsupervised model for
morphological segmentation. In
Proceedings of the 45th Annual Meeting
of the Association of Computational
Linguistics, pages 920–927, Prague.
Dryer, Matthew S. 2005. Prefixing versus
suffixing in inflectional morphology.
In Bernard Comrie, Matthew S. Dryer,
David Gil, and Martin Haspelmath,
editors, World Atlas of Language Structures.
Oxford University Press, Oxford,
pages 110–113.
Eguchi, Paul K. 1987. Fieldworker and
computer: An end user’s view of computer
</bodyText>
<page confidence="0.980181">
340
</page>
<note confidence="0.574528">
Hammarström and Borin Unsupervised Learning of Morphology
</note>
<bodyText confidence="0.912201923076923">
ethnology. Senri Ethnological Studies,
20:165–174.
Ejerhed, Eva and Gunnel Källgren. 1997.
Stockholm Umeå Corpus version 1.0,
SUC 1.0. Technical report, Department
of Linguistics, Umeå University.
Eliseeva, K. A. 1965. Statistiko-
kombinatornoe modelirovanie
pervogo tipa v ukrainskoj morfologii.
In Nikolaj Dmitrieviˇc Andreev, editor,
Statistiko-kombinatornoe modelirovanie
jazykov. Nauka, Leningrad, pages 85–88.
Faulk, R. D. and F. Goertzel Gustavson. 1990.
Segmenting discrete data representing
continuous speech input. IBM Systems
Journal, 29(2):287–296.
Fedulova, N. I. 1965. Vydelenie pervogo
morfologiˇceskogo tipa v bolgarskom
jazyke. In Nikolaj Dmitrieviˇc Andreev,
editor, Statistiko-kombinatornoe
modelirovanie jazykov. Nauka,
Leningrad, pages 110–115.
Fihman, B. S. 1965a. Vydelenie pervogo
morfologiˇceskogo tipa v jazyke hausa po
algoritmu statistiko-kombinatornogo
modelirovanija. In Nikolaj Dmitrieviˇc
Andreev, editor, Statistiko-kombinatornoe
modelirovanie jazykov. Nauka, Leningrad,
pages 189–195.
Fihman, B. S. 1965b. Vydelenie pervogo
morfologiˇceskogo tipa v jazyke suahili po
algoritmu statistiko-kombinatornogo
modelirovanija. In Nikolaj Dmitrieviˇc
Andreev, editor, Statistiko-kombinatornoe
modelirovanie jazykov. Nauka, Leningrad,
pages 196–204.
Fishel, Mark and Harri Kirik. 2010.
Linguistically motivated unsupervised
segmentation for machine translation. In
Proceedings of the Seventh International
Language Resources and Evaluation
(LREC’10), pages 1741–1745, Valletta.
Fitialova, I. B. 1965. Statistiko-kombinatornoe
vydelenie pervogo morfologiˇceskogo tipa
v nemeckom jazyke. In Nikolaj Dmitrieviˇc
Andreev, editor, Statistiko-kombinatornoe
modelirovanie jazykov. Nauka, Leningrad,
pages 158–171.
Flenner, Gudrun. 1992. Ein quantitatives
Morphsegmentierungsverfahren für
spanische Wortformen. Ph.D. thesis,
Georg-August-Universität Göttingen.
Flenner, Gudrun. 1994. Ein quantitatives
Morphsegmentierungssystem für
spanische Wortformen. In Ursula Klenk,
editor, Computatio Linguae II. Aufsätze
zur algorithmischen und quantitativen
Analyse der Sprache, volume 83
of Zeitschrift für Dialektologie und
Linguistik. Beihefte. Franz Steiner,
Stuttgart, pages 31–62.
Flenner, Gudrun. 1995. Quantitative
Morphsegmentierung im Spanischen
auf phonologischer Basis. Sprache
und Datenverarbeitung, 19(2):63–78.
</bodyText>
<reference confidence="0.910711105263158">
Foley, William. 1991. The Yimas Language of
New Guinea. Stanford University Press,
Stanford, CA.
Forsberg, Markus, Harald Hammarström,
and Aarne Ranta. 2006. Lexicon extraction
from raw text data. In Proceedings of the
5th International Conference, FinTAL,
pages 488–499, Turku.
Francis, Nelson W. and Henry Kucera.1964.
Brown corpus. Department of Linguistics,
Brown University, Providence, RI.
Freitag, Dayne. 2005. Morphology induction
from term clusters. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL-2005),
pages 128–135, Ann Arbor, MI.
Gammon, Edward. 1969. Quantitative
approximations to the word. In
International Conference on Computational
</reference>
<bodyText confidence="0.853975375">
Linguistics, COLING, pages 1–28,
Sånga-Säby.
Garvin, Paul L. 1967. The automation of
discovery procedure in linguistics.
Language, 43(1):172–178.
Gaussier, Éric. 1999. Unsupervised learning
of derivational morphology from
inflectional lexicons. In Proceedings of the
</bodyText>
<reference confidence="0.954401615384616">
Workshop on Unsupervised Learning in
Natural Language Processing at the 37th
Annual Meeting of the Association for
Computational Linguistics (ACL-1999),
pages 24–30, Philadephia, PA.
Gelbukh, Alexander F., Mikhail Alexandrov,
and Sang-Yong Han. 2004. Detecting
inflection patterns in natural language
by minimization of morphological model.
In Alberto Sanfeliu, José Francisco
Martínez Trinidad, and Jesfzs Ariel
Carrasco-Ochoa, editors, Proceedings of
Progress in Pattern Recognition, Image
Analysis and Applications, 9th Iberoamerican
Congress on Pattern Recognition, CIARP ’04,
volume 3287 of Lecture Notes in Computer
Science. Springer-Verlag, Berlin,
pages 432–438.
Gippert, Jost, Nikolaus P. Himmelmann,
and Ulrike Mosel, editors. 2006. Essentials
of Language Documentation. Mouton de
Gruyter, Berlin.
Golcher, Felix. 2006. Statistical text
segmentation with partial structure
analysis. In Proceedings of KONVENS
2006, pages 44–51, Konstanz.
</reference>
<page confidence="0.995997">
341
</page>
<note confidence="0.6251">
Computational Linguistics Volume 37, Number 2
</note>
<reference confidence="0.976713273809524">
Golding, Andrew and Henry S. Thompson.
1985. A morphology component for
language programs. Linguistics,
23:263–284.
Goldsmith, John. 2000. Linguistica: An
automatic morphological analyzer. In
Proceedings from the Main Session of the
Chicago Linguistic Society’s 36th Meeting,
pages 125–139, Chicago, IL.
Goldsmith, John. 2001. Unsupervised
learning of the morphology of natural
language. Computational Linguistics,
27(2):153–198.
Goldsmith, John, Derrick Higgins, and
Svetlana Soglasnova. 2001. Automatic
language-specific stemming in
information retrieval. In Carol Peters,
editor, Cross-Language Information Retrieval
and Evaluation: Proceedings of the CLEF
2000 Workshop, Lecture Notes in
Computer Science. Springer-Verlag,
Berlin, pages 273–283.
Goldsmith, John, Yu Hu, Irina Matveeva,
and Colin Sprague. 2005. A heuristic
for morpheme discovery based on
string edit distance. Technical
Report of Computer Science
Department, University of Chicago, IL.
TR-2005-4.
Goldsmith, John and Aris Xanthos. 2009.
Learning phonological categories.
Language, 85(1):4–38.
Goldsmith, John A. 2006. An algorithm for
the unsupervised learning of morphology.
Natural Language Engineering,
12(4):353–371.
Goldsmith, John A. 2010. Segmentation and
morphology. In Alexander Clark, Chris
Fox, and Shalom Lappin, editors, Handbook
of Computational Linguistics and Natural
Language Processing, Blackwell Handbooks
in Linguistics. Wiley-Blackwell, Oxford,
pages 364–393.
Goldwater, Sharon. 2007. Nonparametric
Bayesian Models of Lexical Acquisition. Ph.D.
thesis, Brown University.
Goldwater, Sharon, Tom Griffiths, and
Mark Johnson. 2005. Interpolating between
types and tokens by estimating power-law
generators. In Advances in Neural
Information Processing Systems 18 [Neural
Information Processing Systems, NIPS 2005],
pages 459–466, Vancouver.
Golénia, Bruno. 2008. Learning rules in
morphology of complex synthetic
languages. Master’s thesis, Université
de Paris V.
Goodman, Sarah A. 2008. Morphological
induction through linguistic productivity.
In Working Notes for the CLEF 2008
Workshop, Aarhus.
Grünwald, Peter D. 2007. The Minimum
Description Length Principle: Adaptive
Computation and Machine Learning.
MIT Press, Cambridge, MA.
Hadouche, Fadila. 2002. Détection de
relations morphologiques en corpus basée
sur les cooccurrences. Master’s thesis,
DESS, Centre de Recherche en Ingénierie
Multilingue, CRIM, France.
Hafer, Margaret A. and Stephen F. Weiss.
1974. Word segmentation by letter
successor varieties. Information Storage
and Retrieval, 10:371–385.
Hall, Jr., Robert A. 1987. Bloomfield and
semantics. In Robert A. Hall, Jr., editor,
Leonard Bloomfield: Essays on his Life and
work. John Benjamins, Amsterdam,
pages 155–160.
Hammarström, Harald. 2005. A new
algorithm for unsupervised induction
of concatenative morphology. In Finite
State Methods in Natural Language
Processing: 5th International Workshop,
</reference>
<bodyText confidence="0.948513352941176">
FSMNLP 2005, pages 288–289, Helsinki.
Hammarström, Harald. 2006a. A naive
theory of morphology and an algorithm
for extraction. In SIGPHON 2006: Eighth
Meeting of the Proceedings of the ACL Special
Interest Group on Computational Phonology,
pages 79–88, New York, NY.
Hammarström, Harald. 2006b. Poor man’s
stemming: Unsupervised recognition of
same-stem words. In Information Retrieval
Technology: Proceedings of the Third Asia
Information Retrieval Symposium, AIRS
2006, pages 323–337, Singapore.
Hammarström, Harald. 2007a. A fine-
grained model for language identification.
In Proceedings of iNEWS-07 Workshop at
SIGIR 2007, pages 14–20, Amsterdam.
Hammarström, Harald. 2007b. Unsupervised
learning of morphology: Survey, model,
algorithm and experiments. Thesis for
the Degree of Licentiate of Engineering,
Department of Computer Science and
Engineering, Chalmers University.
Hammarström, Harald. 2009a. Poor man’s
word-segmentation: Unsupervised
morphological analysis for Indonesian.
In Proceedings of the Third International
Workshop on Malay and Indonesian Language
Engineering (MALINDO), Singapore.
Hammarström, Harald. 2009b. Unsupervised
Learning of Morphology and the Languages
of the World. Ph.D. thesis, Chalmers
University of Technology and University
of Gothenburg.
</bodyText>
<page confidence="0.986568">
342
</page>
<note confidence="0.678924">
Hammarström and Borin Unsupervised Learning of Morphology
</note>
<bodyText confidence="0.854840472727273">
Harris, Zellig. 1967. Morpheme boundaries
within words: Report on a computer test.
In Transformations and Discourse Analysis
Papers 73. Department of Linguistics,
University of Pennsylvania, Philadelphia.
Reprinted in Harris 1970.
Harris, Zellig S. 1955. From phoneme to
morpheme. Language, 31(2):190–222.
Harris, Zellig S. 1968. Recurrent dependence
process: Morphemes by phoneme
neighbors. In Mathematical Structures of
Language, volume 21 of Interscience tracts in
pure and applied mathematics. Interscience,
New York, pages 24–28.
Harris, Zellig S. 1970. Morpheme boundaries
within words: Report on a computer test.
In Zellig S. Harris, editor, Papers in
Structural and Transformational Linguistics,
volume 1 of Formal Linguistics Series.
D. Reidel, Dordrecht, pages 68–77.
Haspelmath, Martin. 2002. Understanding
morphology. Arnold, London.
Hirsimäki, Teemu, Mathias Creutz, Vesa
Siivola, and Mikko Kurimo. 2003.
Unlimited vocabulary speech recognition
based on morphs discovered in an
unsupervised manner. In Proceedings of
Eurospeech 2003, Geneva, pages 2293–2996.
Geneva.
Hirsimäki, Teemu, Mathias Creutz,
Vesa Siivola, and Mikko Kurimo.
2005. Morphologically motivated
language models in speech recognition.
In Proceedings of the International
and Interdisciplinary Conference on
Adaptive Knowledge Representation and
Reasoning (AKRR ’05), pages 121–126,
Espoo.
Hirsimäki, Teemu, Mathias Creutz, Vesa
Siivola, Mikko Kurimo, Sami Virpioja,
and Janne Pylkkönen. 2006. Unlimited
vocabulary speech recognition with
morph language models applied to
Finnish. Computer Speech and Language,
20(4):515–541.
Hol’m, H. A. 1965. Vydelenie pervogo
morfologiˇceskogo tipa v ˙estonskom jazyke
na osnove statistiko-kombinatornogo
modelirovanija. In Nikolaj Dmitrieviˇc
Andreev, editor, Statistiko-kombinatornoe
modelirovanie jazykov. Nauka, Leningrad,
pages 212–224.
Hu, Yu, Irina Matveeva, John Goldsmith,
and Colin Sprague. 2005a. Refining the
SED heuristic for morpheme discovery:
Another look at Swahili. In Proceedings
of the Workshop on Psychocomputational
Models of Human Language Acquisition,
pages 28–35, Ann Arbor, MI.
Hu, Yu, Irina Matveeva, John Goldsmith, and
Colin Sprague. 2005b. Using morphology
and syntax together in unsupervised
learning. In Proceedings of the Workshop
on Psychocomputational Models of Human
Language Acquisition, pages 20–27,
Ann Arbor, MI.
Jacquemin, Christian. 1997. Guessing
morphology from terms and corpora.
In Proceedings, 20th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval
(SIGIR ’97), pages 155–165,
Philadelphia, PA.
Jakubajtis, T. A. 1965. Statistiko-
kombinatornoe vydelenie pervogo
morfologiˇceskogo tipa v latyšskom
jazyke. In Nikolaj Dmitrieviˇc Andreev,
editor, Statistiko-kombinatornoe
modelirovanie jazykov. Nauka, Leningrad,
pages 116–122.
Jakuševa, D. A. 1965.Opyt primenenija
algoritma statistiko-kombinatornogo
modelirovanija k v’etnamskomu jazyku.
In Nikolaj Dmitrieviˇc Andreev, editor,
Statistiko-kombinatornoe modelirovanie
jazykov. Nauka, Leningrad, pages 225–228.
Janßen, Axel. 1992. Segmentierung
französischer Wortformen ohne Lexikon.
In Ursula Klenk, editor, Computatio
Linguae: Aufsätze zur algorithmischen
und quantitativen Analyse der Sprache,
volume 73 of Zeitschrift für Dialektologie
und Linguistik: Beihefte. Franz Steiner,
Stuttgart, pages 74–95.
Jensen, John T. 1990. Morphology. Word
Structure in Generative Grammar. John
Benjamins, Amsterdam.
Johnsen, Lars G. 2005. Morphological
learning as principled argument. In Mikko
Kurimo, Mathias Creutz, and Krista Lagus,
editors, Proceedings of MorphoChallenge
2005, pages 33–36, Helsinki University of
Technology, Helsinki.
Johnson, Howard and Joel Martin. 2003.
Unsupervised learning of morphology for
English and Inuktitut. In HLT-NAACL
2003, Human Language Technology
Conference of the North American Chapter
of the Association for Computational
Linguistics, pages 43–45, Edmonton.
</bodyText>
<reference confidence="0.97901625">
Johnson, Mark. 2008. Unsupervised word
segmentation for Sesotho using adaptor
grammars. In Proceedings of the Tenth
Meeting of ACL Special Interest Group on
Computational Morphology and Phonology,
pages 20–27, Columbus, OH.
Jordan, Chris, John Healy, and Vlado Keselj.
2005. Swordfish: Using n-grams in an
</reference>
<page confidence="0.998351">
343
</page>
<note confidence="0.4762608">
Computational Linguistics Volume 37, Number 2
unsupervised approach to morphological
analysis. In Mikko Kurimo, Mathias
Creutz, and Krista Lagus, editors,
Proceedings of MorphoChallenge 2005,
</note>
<reference confidence="0.829160863636364">
pages 42–46, Helsinki University of
Technology, Helsinki.
Jordan, Chris, John Healy, and Vlado Keselj.
2006. Swordfish: An unsupervised ngram
based approach to morphological analysis.
In SIGIR ’06: Proceedings of the 29th Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 657–658, New York, NY.
Juola, Patrick, Chris Hall, and Adam Boggs.
1994. Corpus-based morphological
segmentation by entropy changes. In
Third Conference on the Cognitive Science
of Natural Language Processing, Dublin.
Katrenko, Sophia. 2004. Towards
unsupervised learning of morphology
applied to Ukrainian. Student Session:
16th European Summer School in Logic,
Language and Information, pages 138–148,
Nancy.
Kazakov, Dimitar. 1997. Unsupervised
learning of naïve morphology with genetic
algorithms. In ECML’97 – Workshop Notes
on Empirical Learning of Natural Language
Tasks, pages 105–112, Prague.
Kazakov, Dimitar and Suresh Manandhar.
1998. A hybrid approach to word
segmentation. In Proceedings of the 8th
International Workshop on Inductive Logic
Programming (ILP-98), pages 125–134,
Madison, WI.
Kazakov, Dimitar and Suresh Manandhar.
2001. Unsupervised learning of word
segmentation rules with genetic
algorithms and inductive logic
programming. Machine Learning,
43:121–162.
Keshava, Samarth and Emily Pitler. 2005. A
simpler, intuitive approach to morpheme
induction. In Mikko Kurimo, Mathias
Creutz, and Krista Lagus, editors,
Proceedings of MorphoChallenge 2005,
pages 28–32, Helsinki University of
Technology, Helsinki.
</reference>
<bodyText confidence="0.872432">
Kirik, Harri and Mark Fishel. 2008.
Modelling linguistic phenomena with
unsupervised morphology for improving
statistical machine translation. Paper
presented at the Workshop on
Unsupervised Methods in NLP, held
in conjunction with SLTC’08, Royal
Institute of Technology, Stockholm,
Sweden.
</bodyText>
<reference confidence="0.994062655737705">
Klein, Sheldon and Terry A. Dennison.
1976. An interactive program for
learning the morphology of natural
languages. In Proceedings of the 3rd
International Meeting on Computational
Linguistics, pages 343–353, Debrecen.
Klenk, Ursula. 1985a. Ein nicht-lexikalisches
Verfahren zur Erkennung spanischer
Wortstämme. In Ursula Klenk, editor,
Strukturen und Verfahren in der maschinellen
Sprachverarbeitung. AQ-Verlag, Dudweiler,
pages 47–65.
Klenk, Ursula. 1985b. Recognition of
Spanish inflectional endings based on the
distribution of characters. In Computers
in Literary and Linguistic Computing:
Proceedings of the Eleventh International
Conference [L’ordinateur et les recherches
littéraires et linguistiques: actes de la XIe
Conférence internationale], pages 246–253,
Louvain.
Klenk, Ursula. 1991. Verfahren der
Segmentierung von Wörtern in Morphe:
Mit einer Untersuchung zum Spanischen.
In Jürgen Rolshoven und Dieter Seelbach,
editor, Romanistische Computerlinguistik:
Theorien und Implementationen, volume 266
of Linguistische Arbeiten. Niemeyer,
Tübingen, pages 197–206.
Klenk, Ursula. 1992. Verfahren
morphologischer Segmentierung und die
Wortstruktur des Spanischen. In Ursula
Klenk, editor, Computatio Linguae: Aufsätze
zur algorithmischen und quantitativen
Analyse der Sprache, volume 73 of
Zeitschrift für Dialektologie und Linguistik:
Beihefte. Franz Steiner, Stuttgart,
pages 110–124.
Klenk, Ursula and Hagen Langer. 1989.
Morphological segmentation without a
lexicon. Literary and Linguistic Computing,
4(4):247–253.
Kohonen, Oskar, Sami Virpioja, and
Mikaela Klami. 2008. Allomorfessor:
Towards unsupervised morpheme
analysis. In Carol Peters, Thomas
Deselaers, Nicola Ferro, Julio Gonzalo,
Gareth J. F. Jones, Mikko Kurimo,
Thomas Mandl, Anselmo Peñas, and
Vivien Petras, editors, Evaluating
Systems for Multilingual and Multimodal
Information Access, 9th Workshop of
the Cross-Language Evaluation Forum,
CLEF 2008, Aarthus, Denmark,
September 17–19, 2008, Revised Selected
Papers, pages 975–982, Springer-Verlag,
Berlin.
Kontorovich, L., D. Don, and Y. Singer.
2003. A Markov model for the acquisition
of morphological structure. Technical
report CMU-CS-03-147, School of
</reference>
<page confidence="0.997787">
344
</page>
<note confidence="0.887547">
Hammarström and Borin Unsupervised Learning of Morphology
</note>
<reference confidence="0.987863533898305">
Computer Science, Carnegie Mellon
University, June.
Kordi, E. E. 1965. Ishodnye dannye
dlja statistiko-kombinatornogo
modelirovanija morfologii sovremennogo
francuzckogo jazyka i vydelenie
pervogo morfologiˇceskogo tipa. In
Nikolaj Dmitrieviˇc Andreev, editor,
Statistiko-kombinatornoe modelirovanie
jazykov. Nauka, Leningrad, pages 172–180.
Krauss, Michael. 1992. The world’s
languages in crisis. Language, 68(1):4–10.
Krauss, Michael E. 2007. Mass language
extinction and documentation: The race
against time. In O. Miyaoka, O. Sakiyama,
and M. Krauss, editors, Vanishing
Languages of the Pacific Rim. Oxford
University Press, Oxford, pages 3–24.
Kurimo, Mikko, Mathias Creutz, and
Ville Turunen. 2007. Overview of
Morpho Challenge in CLEF 2007. In
Working Notes for the CLEF 2007 Workshop,
Budapest.
Kurimo, Mikko, Mathias Creutz, and
Matti Varjokallio. 2008a. Morpho challenge
evaluation by information retrieval
experiments. In Carol Peters, Valentin
Jijkoun, Thomas Mandl, Henning Müller,
Douglas W. Oard, and Anselmo Peñas,
editors, Advances in Multilingual and
Multimodal Information Retrieval, 8th
Workshop of the Cross-Language
Evaluation Forum, CLEF 2007, Budapest,
Hungary, September 19–21, 2007, Revised
Selected Papers, pages 991–998.
Springer-Verlag, Berlin.
Kurimo, Mikko, Mathias Creutz, and Matti
Varjokallio. 2008b. Morpho challenge
evaluation using a linguistic gold
standard. In Carol Peters, Valentin Jijkoun,
Thomas Mandl, Henning Müller, Douglas
W. Oard, and Anselmo Peñas, editors,
Advances in Multilingual and Multimodal
Information Retrieval, 8th Workshop of
the Cross-Language Evaluation Forum,
CLEF 2007, Budapest, Hungary,
September 19–21, 2007, Revised Selected
Papers, pages 864–872. Springer-Verlag,
Berlin.
Kurimo, Mikko, Antti Puurula, Ebru Arisoy,
Vesi Siivola, Teemu Hirsimäki, Janne
Pylkkönen, Tanel Alumäe, and Murat
Saraçlar. 2006. Unlimited vocabulary
speech recognition for agglutinative
languages. In Proceedings of the Main
Conference on Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 487–494, New York.
Kurimo, Mikko and Ville Turunen. 2008.
Unsupervised morpheme analysis
evaluation by IR experiments—Morpho
Challenge 2008. In Working Notes for the
CLEF 2008 Workshop, Aarhus.
Kurimo, Mikko and Matti Varjokallio.
2008. Unsupervised morpheme analysis
evaluation by a comparison to a linguistic
gold standard—Morpho Challenge 2008.
In Working Notes for the CLEF 2008
Workshop, Aarhus.
Langer, Hagen. 1991. Ein automatisches
Morphsegmentierungsverfahren für
deutsche Wortformen. Ph.D. thesis,
Georg-August-Universität zu Göttingen.
Lehmann, Hubert. 1973. Linguistische
Modellbildung und Methodologie. Max
Niemeyer Verlag, Tübingen.
Lewis, M. Paul, editor. 2009. Ethnologue:
Languages of the World. Available at
www.ethnologue.com/.
Lindén, Krister. 2008. A probabilistic model
for guessing base forms of new words by
analogy. In Proceedings of CICLing-2008: 9th
International Conference on Intelligent Text
Processing and Computational Linguistics,
pages 106–116, Springer, Berlin.
Lindén, Krister. 2009. Entry generation
by analogy—encoding new words for
morphological lexicons. Northern
European Journal of Language Technology,
1(1):1–25.
Longacre, Robert E. 1964. Grammar Discovery
Procedures. Mouton, The Hague.
Mahlow, Cerstin and Michael Piotrowski.
2009. Preface. In State of the Art in
Computational Morphology: Proceedings
of the Workshop on Systems and Frameworks
for Computational Morphology, SFCM 2009,
pages v–viii, Zurich.
Majumder, Prasenjit, Mandar Mitra, and
Dipasree Pal. 2008. Bulgarian, Hungarian
and Czech stemming using YASS. In
Advances in Multilingual and Multimodal
Information Retrieval: 8th Workshop of
the Cross-Language Evaluation Forum,
CLEF 2007, pages 49–56, Budapest.
Majumder, Prasenjit, Mandar Mitra,
Swapan K. Parui, Gobinda Kole, Pabitra
Mitra, and Kalyankumar Datta. 2007.
YASS: Yet another suffix stripper.
ACM Transactions on Information
Systems, 25(4):18:1–20.
Malahovskij, L. V. 1965. Naˇcal’nyj ˙etap
statistiko-kombinatornogo modelirovanija
morfologii anglijskogo jazyka. In
Nikolaj Dmitrieviˇc Andreev, editor,
Statistiko-kombinatornoe modelirovanie
jazykov. Nauka, Leningrad, pages 137–149.
</reference>
<page confidence="0.995203">
345
</page>
<note confidence="0.560497">
Computational Linguistics Volume 37, Number 2
</note>
<reference confidence="0.973457347457627">
Maxwell, Michael and Anne David. 2008.
Joint grammar development by linguists
and computer scientists. In Proceedings of
the IJCNLP-08 Workshop on NLP for Less
Privileged Languages, pages 27–34,
Hyderabad.
Mayfield, James and Paul McNamee. 2003.
Single n-gram stemming. In SIGIR ’03:
Proceedings of the 26th Annual International
ACM SIGIR Conference on Research and
Development in Informaion Retrieval,
pages 415–416, New York, NY.
McClelland, James L. and David E.
Rumelhart. 1986. On learning the past
tenses of English verbs. In Parallel
Distributed Processing: Explorations in the
Microstructure of Cognition. Volume 2:
Psychological and Biological Models. MIT
Press, Cambridge, MA, pages 216–271.
McNamee, Paul. 2008. Retrieval experiments
at Morpho Challenge 2008. In Working
Notes for the CLEF 2008 Workshop, Aarhus.
McNamee, Paul and James Mayfield. 2007.
N-gram morphemes for retrieval. In
Working Notes for the CLEF 2007 Workshop,
Budapest.
Medina Urrea, Alfonso. 2000. Automatic
discovery of affixes by means of a corpus:
A catalog of Spanish affixes. Journal of
Quantitative Linguistics, 7(2):97–114.
Medina Urrea, Alfonso. 2003. Investigación
cuantitativa de afijos y clíticos del español de
México: Glutinometría en el Corpus del
Español Mexicano Contemporáneo.
Ph.D. thesis, El Colegio de México,
México, D.F.
Medina-Urrea, Alfonso. 2006a. Affix
discovery by means of corpora:
Experiments for Spanish, Czech, Ralámuli
and Chuj. In Alexander Mehler and
Reinhard Köhler, editors, Aspects of
Automatic Text Analysis, volume 209 of
Studies in Fuzziness and Soft Computing.
Springer, Berlin, pages 277–299.
Medina Urrea, Alfonso. 2006b. Towards the
automatic lemmatization of 16th century
Mexican Spanish: A stemming scheme for
the CHEM. In Computational Linguistics and
Intelligent Text Processing, 7th International
Conference, CICLing 2006, pages 101–104,
Mexico City.
Medina-Urrea, Alfonso. 2008. Affix
discovery based on entropy and economy
measurements. In Nicholas Gaylord,
Alexis Palmer, and Elias Ponvert, editors,
Computational Linguistics for Less-Studied
Languages, volume X of Texas Linguistics
Society. CSLI Publications, Stanford, CA,
pages 99–112.
Medina Urrea, Alfonso and E. C. Buenrostro
Díaz. 2003. Características cuantitativas
de la flexión verbal del Chuj. Estudios de
Lingüística Aplicada, 38:15–31.
Melkumjan, M. R. 1965. Ishodnye dannye
i statistiko-kombinatornoe vydelenie
paradigmy pervogo morfologiˇceskogo
tipa v armjanskom jazyke. In
Nikolaj Dmitrieviˇc Andreev, editor,
Statistiko-kombinatornoe modelirovanie
jazykov. Nauka, Leningrad, pages 123–136.
Mikheev, Andrei. 1997. Automatic rule
induction for unknown-word guessing.
Computational Linguistics, 23(3):405–423.
Mithun, Marianne. 1999. The Languages
of Native North America. Cambridge
Language Surveys. Cambridge
University Press, Cambridge.
Monson, Christian. 2004. A framework
for unsupervised natural language
morphology induction. In ACL 2004:
Student Research Workshop, pages 67–72,
Barcelona.
Monson, Christian. 2009. ParaMor: From
Paradigm Structure to Natural Language
Morphology Induction. Ph.D. thesis,
Carnegie Mellon University.
Monson, Christian, Jaime Carbonell, Alon
Lavie, and Lori Levin. 2007a. ParaMor:
Finding paradigms across morphology. In
Carol Peters, Valentin Jijkoun, Thomas
Mandl, Henning Müller, Douglas W. Oard,
and Anselmo Peñas, editors, Advances in
Multilingual and Multimodal Information
Retrieval, 8th Workshop of the Cross-
Language Evaluation Forum, CLEF 2007,
Budapest, Hungary, September 19–21,
2007, Revised Selected Papers,
pages 892–899. Springer-Verlag, Berlin.
Monson, Christian, Jaime Carbonell, Alon
Lavie, and Lori Levin. 2007b. ParaMor:
Minimally supervised induction of
paradigm structure and morphological
analysis. In Proceedings of Ninth Meeting
of the ACL Special Interest Group in
Computational Morphology and Phonology,
pages 117–125, Prague.
Monson, Christian, Jaime Carbonell, Alon
Lavie, and Lori Levin. 2008. ParaMor
and Morpho Challenge 2008. Using
unsupervised paradigm acquisition
for prefixes. In Carol Peters, Thomas
Deselaers, Nicola Ferro, Julio Gonzalo,
Gareth J. F. Jones, Mikko Kurimo,
Thomas Mandl, Anselmo Peñas, and
Vivien Petras, editors, Evaluating
Systems for Multilingual and Multimodal
Information Access, 9th Workshop of the
Cross-Language Evaluation Forum,
</reference>
<page confidence="0.997911">
346
</page>
<note confidence="0.903963">
Hammarström and Borin Unsupervised Learning of Morphology
</note>
<reference confidence="0.998726279661017">
CLEF 2008, Aarthus, Denmark,
September 17–19, 2008, Revised Selected
Papers, pages 967–974. Springer-Verlag,
Berlin.
Monson, Christian, Alon Lavie, Jaime
Carbonell, and Lori Levin. 2004.
Unsupervised induction of natural
language morphology inflection classes.
In SIGPHON 2004: Proceedings of the
Seventh Meeting of the ACL Special Interest
Group in Computational Phonology,
pages 52–61, Barcelona.
Monson, Christian, Alon Lavie, Jaime
Carbonell, and Lori Levin. 2008a.
Evaluating an agglutinative segmentation
model for ParaMor. In Proceedings of the
Tenth Meeting of ACL Special Interest Group
on Computational Morphology and Phonology,
pages 49–58, Columbus, OH.
Monson, Christian, Ariadna Font Llitjós,
Vamshi Ambati, Lori Levin, Alon Lavie,
Alison Alvarez, Roberto Aranovich,
Jaime Carbonell, Robert Frederking,
Erik Peterson, and Katharina Probst.
2008b. Linguistic structure and bilingual
informants help induce machine
translation of lesser-resourced languages.
In Proceedings of the Sixth International
Language Resources and Evaluation
(LREC’08), pages 2854–2859, Marrakech.
Moon, Taesun, Katrin Erk, and Jason
Baldridge. 2009. Unsupervised
morphological segmentation and
clustering with document boundaries.
In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 668–677, Singapore.
Naradowsky, Jason and Sharon Goldwater.
2009. Improving morphology induction
by learning spelling rules. In UCAI 2009,
Proceedings of the 21st, International
Joint Conference on Artificial Intelligence,
Pasadena, California, USA, July 11–17,
2009, pages 1531–1537.
Neuvel, Sylvain and Sean A. Fulop. 2002.
Unsupervised learning of morphology
without morphemes. In Proceedings of the
ACL-02 Workshop on Morphological and
Phonological Learning, Philadelphia,
pages 31–40.
Nida, Eugene A. 1949. Morphology. The
Descriptive Analysis of Words, 2nd edition.
The University of Michigan Press, Ann
Arbor, MI.
Nunzio, G. M. Di, N. Ferro, M. Melucci,
and N. Orio. 2004. Experiments to evaluate
probabilistic models for automatic stemmer
generation and query word translation. In
Proceedings of the Cross-Language Evaluation
Forum (CLEF): Methodology and Metrics
(CLEF 2003), pages 220–235.
Oflazer, Kemal, Marjorie McShane, and
Sergei Nirenburg. 2001. Bootstrapping
morphological analyzers by combining
human elicitation and machine learning.
Computational Linguistics, 27(1):59–85.
Oliver, A. 2004. Adquisició d’informació
lèxica i morfosintàctica a partir de corpus
sense anotar: aplicació al rus i al croat. Ph.D.
thesis, Universitat de Barcelona.
Ostler, Nicholas. 2008. Is it globalization that
endangers languages? In UNESCO/UNU
Conference: Globalization and Languages:
Building our Rich Heritage, pages 206–211,
Paris.
Ožigova, G. I. 1965. Statistiko-kombinatornoe
modelirovanie paradigmy pervogo
morfologiˇceskogo tipa v ˇcešskom jazyke.
In Nikolaj Dmitrieviˇc Andreev, editor,
Statistiko-kombinatornoe modelirovanie
jazykov. Nauka, Leningrad, pages 89–103.
Pandey, Amaresh Kumar and Tanveer J.
Siddiqui. 2008. An unsupervised Hindi
stemmer with heuristic improvements. In
AND ’08: Proceedings of the Second Workshop
on Analytics for Noisy Unstructured Text
Data, pages 99–105, New York, NY.
Panina, N. A. 1965.Opyt statistiko-
kombinatornogo vydelenija paradigmy
pervogo morfologiˇceskogo tipa
v fserbohorvatskom jazyke. In
Nikolaj Dmitrieviˇc Andreev, editor,
Statistiko-kombinatornoe modelirovanie
jazykov. Nauka, Leningrad, pages 104–109.
Peršikov, V. F. 1965. Iz opyta statistiko-
kombinatornogo modelirovanija albanskoj
morfologii. In Nikolaj Dmitrieviˇc Andreev,
editor, Statistiko-kombinatornoe modelirovanie
jazykov. Nauka, Leningrad, pages 181–188.
Petzell, Malin. 2007. A lingustic description
of Kagulu. Ph.D. thesis, Göteborgs
Universitet.
Pirrelli, Vito, Basilio Calderone, Ivan
Herreros, and Michele Virgilio. 2004.
Non-locality all the way through:
Emergent global constraints in the
Italian morphological lexicon. In
SIGPHON 2004: Proceedings of the Seventh
Meeting of the ACL Special Interest Group in
Computational Phonology, pages 52–61,
Barcelona.
Pirrelli, Vito and Ivan Herreros. 2007.
Learning morphology by itself. In
Proceedings of the Fifth Mediterranean
Morphology Meeting (MMM5),
pages 269–290, Fréjus.
Poon, Hoifung, Colin Cherry, and Kristina
Toutanova. 2009. Unsupervised
</reference>
<page confidence="0.95831">
347
</page>
<reference confidence="0.994858697478992">
Computational Linguistics Volume 37, Number 2
morphological segmentation with
log-linear models. In Proceedings of
NAACL ’09: The 2009 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 209–217, Morristown, NJ.
Powers, David M. W. 1998. Reconciliation of
unsupervised clustering, segmentation
and cohesion. In NeMLaP3/CoNLL ’98
Workshop on Paradigms and Grounding in
Language Learning, Sydney, pages 307–310.
Rai, Novel Kishore. 1984. A Descriptive Study
of Bantawa. Ph.D. thesis, Poona University.
Redlich, A. Norman. 1993. Redundancy
reduction as a strategy for unsupervised
learning. Neural Computation, 5(2):289–304.
Reiter, Ehud. 2007. Last words: The shrinking
horizons of computational linguistics.
Computational Linguistics, 33(2):283–287.
Roark, B. and Richard W. Sproat. 2007.
Machine learning of morphology. In
Computational Approaches to Morphology and
Syntax, volume 4 of Oxford Surveys in
Syntax and Morphology. Oxford University
Press, Oxford, pages 116–136.
Rodrigues, Paul and Damir ´Cavar. 2005.
Learning Arabic morphology using
information theory. In The Panels 2005:
Proceedings from the Annual Meeting of the
Chicago Linguistic Society, volume 41–2,
pages 49–58, Chicago, IL.
Rodrigues, Paul and Damir ´Cavar. 2007.
Learning Arabic morphology using
statistical constraint-satisfaction models.
In Elabbas Benmamoun, editor,
Perspectives on Arabic Linguistics: Papers
from the Annual Symposium on Arabic
Linguistics, pages 63–75, Urbana, IL.
Rogati, Monica, Scott McCarley, and Yiming
Yang. 2003. Unsupervised learning of
Arabic stemming using a parallel corpus.
In Proceedings of the 41st Annual Meeting of
the ACL, pages 391–398, Sapporo.
Saxena, Anju and Lars Borin, editors. 2006.
Lesser-Known Languages of South Asia:
Status and Policies, Case Studies and
Applications of Information Technology.
Mouton de Gruyter, Berlin.
Schone, Patrick. 2001. Toward Knowledge-Free
Induction of Machine-Readable Dictionaries.
Ph.D. thesis, University of Colorado.
Schone, Patrick and Daniel Jurafsky. 2000.
Knowledge-free induction of inflectional
morphologies using latent semantic
analysis. In Conference on Natural
Language Learning 2000 (CoNLL-2000),
pages 67–72, Lisbon.
Schone, Patrick and Daniel Jurafsky. 2001a.
Knowledge-free induction of inflectional
morphologies. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics, pages 183–191,
Pittsburgh, PA.
Schone, Patrick and Daniel Jurafsky. 2001b.
Language-independent induction of part
of speech class labels using only language
universals. In &amp;quot;Machine Learning: Beyond
Supervision,&amp;quot; Workshop at IJCAI-2001,
pages 53–60, Seattle, WA.
Sereewattana, Siriwan. 2003. Unsupervised
segmentation for statistical machine
translation. Master’s thesis, University
of Edinburgh.
Sharma, Utpal and Rajib Das. 2002.
Classification of words based on affix
evidence. In International Conference on
Natural Language Processing, ICON-2002,
pages 31–39, Mumbai.
Sharma, Utpal, Jugal Kalita, and Rajib Das.
2002. Unsupervised learning of
morphology for building lexicon for a
highly inflectional language. In Proceedings
of the 6th Workshop of the ACL Special
Interest Group in Computational Phonology
(SIGPHON), pages 1–10, Philadelphia.
Sharma, Utpal, Jugal Kalita, and Rajib Das.
2003. Root word stemming by multiple
evidence from corpus. In Proceedings
of the 6th International Conference on
Computational Intelligence and Natural
Computation (CINC), pages 1593–1596,
Cary, NC.
Snover, Matthew G. 2002. An unsupervised
knowledge free algorithm for the
learning of morphology in natural
languages. Master’s thesis, Department
of Computer Science, Washington
University.
Snover, Matthew G. and Michael R. Brent.
2001. A Bayesian model for morpheme
and paradigm identification. In Proceedings
of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL-2001),
pages 482–490.
Snover, Matthew G. and Michael R. Brent.
2003. A probabilistic model for learning
concatenative morphology. In S. Becker,
S. Thrun, and K. Obermayer, editors,
Advances in Neural Information Processing
Systems 15. MIT Press, Cambridge, MA,
pages 1513–1520.
Snover, Matthew G., Gaja E. Jarosz, and
Michael R. Brent. 2002. Unsupervised
learning of morphology using a novel
directed search algorithm: Taking the first
step. In Proceedings of the ACL-02 Workshop
on Morphological and Phonological Learning,
pages 11–20, Philadelphia.
</reference>
<page confidence="0.98589">
348
</page>
<note confidence="0.687673">
Hammarström and Borin Unsupervised Learning of Morphology
</note>
<reference confidence="0.996981220338983">
Snyder, Benjamin and Regina Barzilay.
2008. Unsupervised multilingual learning
for morphological segmentation.
In Proceedings of ACL-08: HLT,
pages 737–745, Columbus, OH.
Spencer, Andrew and Arnold M. Zwicky,
editors. 1998. The Handbook of Morphology.
Blackwell, Oxford.
Spiegler, Sebastian, Bruno Golénia, Ksenia
Shalonova, Peter Flach, and Roger Tucker.
2008. Learning the morphology of Zulu
with different degrees of supervision. In
Spoken Language Technology Workshop,
2008 (SLT 2008), pages 9–12, Goa.
Strassel, Stephanie, Mike Maxwell, and
Christopher Cieri. 2003. Linguistic
resource creation for research and
technology development: A recent
experiment. ACM Transactions on Asian
Language Processing, 2(2):101–117.
Tepper, Michael. 2007. Knowledge-lite
induction of underlying morphology: A
hybrid approach to learning morphemes
using context-sensitive rewrite rules.
Master’s thesis, University of Washington.
Tepper, Michael and Fei Xia. 2008. A hybrid
approach to the induction of underlying
morphology. In Proceedings of the Third
International Joint Conference on Natural
Language Processing (IJCNLP 2008),
pages 17–24, Hyderabad.
Theron, Pieter and Ian Cloete. 1997.
Automatic acquisition of two-level
morphological rules. In Fifth Conference
on Applied Natural Language Processing,
pages 103–110, Washington, DC.
Trosterud, Trond. 2004. Porting
morphological analysis and
disambiguation to new languages.
In SALTMIL Workshop at LREC 2004:
First Steps in Language Documentation
for Minority Languages, pages 90–92,
Lisbon.
Trosterud, Trond. 2006. Grammatically
based language technology for minority
languages. In Anju Saxena and Lars
Borin, editors, Lesser-Known Languages
of South Asia: Status and Policies, Case
Studies and Applications of Information
Technology. Mouton de Gruyter, Berlin,
pages 293–315.
Tufis, Dan. 1989. It would be much easier if
WENT were GOED. In Proceedings of the
Fourth Conference of the European Chapter of
the ACL, pages 145–152, Manchester.
ur Rehman, Khalid and Iftikhar Hussain.
2005. Unsupervised morphemes
segmentation. In Mikko Kurimo, Mathias
Creutz, and Krista Lagus, editors,
Proceedings of MorphoChallenge 2005,
pages 52–56, Helsinki University of
Technology, Helsinki.
Virpioja, Sami, Jaakko J. Väyrynen, Mathias
Creutz, and Markus Sadeniemi. 2007.
Morphology-aware statistical machine
translation based on morphs induced in
an unsupervised manner. In Proceedings
of Machine Translation Summit XI,
pages 391–498, Copenhagen.
Wicentowski, Richard. 2002. Modeling
and Learning Multilingual Inflectional
Morphology in a Minimally Supervised
Framework. Ph.D. thesis, Johns Hopkins
University, Baltimore, MD.
Wicentowski, Richard. 2004. Multilingual
noise-robust supervised morphological
analysis using the wordframe model.
In Proceedings of the ACL Special Interest
Group on Computational Phonology
(SIGPHON), pages 70–77, Barcelona.
Wintner, Shuly. 2009. Last words: What
science underlies natural language
engineering? Computational Linguistics,
35(4):641–644.
Wothke, Klaus. 1986. Machine learning of
morphological rules by generalization
and analogy. In Proceedings of the 11th
Conference on Computational Linguistics,
pages 289–293, Morristown, NJ.
Wothke, Klaus Christian. 1985. Maschinelle
Erlernung und Simulation morphologischer
Ableitungsregeln. Ph.D. thesis, Rheinische
Friedrich-Wilhelms-Universität zu Bonn.
Xanthos, Aris. 2007. Apprentissage
automatique de la morphologie: Le cas des
structures racine-schème. Ph.D. thesis,
Université de Lausanne.
Xanthos, Aris, Yu Hu, and John Goldsmith.
2006. Exploring variant definitions of
pointer length in MDL. In Proceedings of the
Eighth Meeting of the ACL Special Interest
Group on Computational Phonology and
Morphology at HLT-NAACL 2006,
pages 32–40.
Yarowsky, David, Grace Ngai, and Richard
Wicentowski. 2001. Inducing multilingual
text analysis tools via robust projection
across aligned corpora. In Proceedings of the
First International Conference on Human
Language Technology Research, Stroudsburg,
PA, pages 1–8.
Yarowsky, David and Richard Wicentowski.
2000. Minimally supervised morphological
analysis by multimodal alignment. In
Proceedings of the 38th Annual Meeting
of the Association for Computational
Linguistics (ACL-2000), pages 207–216,
Hong Kong.
</reference>
<page confidence="0.977432">
349
</page>
<reference confidence="0.982429513513514">
Computational Linguistics Volume 37, Number 2
Yvon, François. 1996. Prononcer par analogie:
motivation, formalisation et evaluation. Ph.D.
thesis, École Nationale Supérieure des
Télécommunications, Paris.
Zeman, Daniel. 2008. Unsupervised
acquiring of morphological paradigms
from tokenized text. In Advances in
Multilingual and Multimodal Information
Retrieval: 8th Workshop of the Cross-
Language Evaluation Forum, CLEF 2007,
pages 892–899, Budapest.
Zeman, Daniel. 2009. Using unsupervised
paradigm acquisition for prefixes. In Carol
Peters, Thomas Deselaers, Nicola Ferro,
Julio Gonzalo, Gareth J. F. Jones, Mikko
Kurimo, Thomas Mandl, Anselmo Peñas,
and Vivien Petras, editors, Evaluating
Systems for Multilingual and Multimodal
Information Access, 9th Workshop of the
Cross-Language Evaluation Forum,
CLEF 2008, Aarthus, Denmark,
September 17–19, 2008, Revised Selected
Papers, pages 983–990. Springer-Verlag,
Berlin.
Zhang, Byoung-Tak and Yung-Taek Kim.
1990. Morphological analysis and
synthesis by automated discovery and
acquisition of linguistic rules. In Papers
Presented to the 13th International Conference
on Computational Linguistics (COLING
1990), volume 2, pages 431–436, Helsinki.
Zweigenbaum, P., F. Hadouche, and
N. Grabar. 2003. Apprentissage de
relations morphologiques en corpus.
In Actes de TALN 2003, pages 285–294,
Batz-sur-mer.
</reference>
<page confidence="0.99763">
350
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.710174">
<title confidence="0.9802535">Survey Article Unsupervised Learning of Morphology</title>
<author confidence="0.995445">Radboud Universiteit</author>
<author confidence="0.995445">Max Planck</author>
<affiliation confidence="0.9608575">Institute for Evolutionary Anthropology University of Gothenburg</affiliation>
<abstract confidence="0.963111">This article surveys work on Unsupervised Learning of Morphology. We define Unsupervised Learning of Morphology as the problem of inducing a description (of some kind, even if only morpheme segmentation) of how orthographic words are built up given only raw text data of a language. We briefly go through the history and motivation of this problem. Next, over 200 items of work are listed with a brief characterization, and the most important ideas in the field are critically discussed. We summarize the achievements so far and give pointers for future developments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>William Foley</author>
</authors>
<title>The Yimas Language of New Guinea.</title>
<date>1991</date>
<publisher>Stanford University Press,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="9517" citStr="Foley 1991" startWordPosition="1422" endWordPosition="1423">proaches where an existing morphological analysis component is used as the basis for guessing in which existing paradigm an unknown text word should belong (e.g., Antworth 1990; Mikheev 1997; Bharati et al. 2001; Forsberg, Hammarström, and Ranta 2006; Lindén 2008; Lindén 2009). One of the matters that varies the most between different authors is the desired outcome. It is useful to set up the implicational hierarchy shown in Table 1 (which 4 There are, however, rare cases of languages which allow the permutation of specific pairs of prefixes, such as Kagulu (Petzell 2007), Yimas and Karawari (Foley 1991, pages 31–32) as well as Chintang, Bantawa, and possibly other Kiranti languages where prefix ordering in general is very free (Rai 1984; Bickel et al. 2007). 5 Clitics are affix-like elements that attach to words in particular syntactic positions, rather than to words of particular categories as proper affixes do. The English genitive -’s is sometimes classified as a clitic, because you can say things like The girl I met yesterday’s purse (the -’s attaches to the end of the noun phrase, regardless of the part of speech of the last word, an adverb in this case). This could not happen with an </context>
</contexts>
<marker>Foley, 1991</marker>
<rawString>Foley, William. 1991. The Yimas Language of New Guinea. Stanford University Press, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Forsberg</author>
<author>Harald Hammarström</author>
<author>Aarne Ranta</author>
</authors>
<title>Lexicon extraction from raw text data.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference, FinTAL,</booktitle>
<pages>488--499</pages>
<location>Turku.</location>
<marker>Forsberg, Hammarström, Ranta, 2006</marker>
<rawString>Forsberg, Markus, Harald Hammarström, and Aarne Ranta. 2006. Lexicon extraction from raw text data. In Proceedings of the 5th International Conference, FinTAL, pages 488–499, Turku.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nelson W Francis</author>
<author>Henry Kucera 1964</author>
</authors>
<title>Brown corpus.</title>
<institution>Department of Linguistics, Brown University,</institution>
<location>Providence, RI.</location>
<marker>Francis, 1964, </marker>
<rawString>Francis, Nelson W. and Henry Kucera.1964. Brown corpus. Department of Linguistics, Brown University, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Morphology induction from term clusters.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005),</booktitle>
<pages>128--135</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="33523" citStr="Freitag 2005" startWordPosition="5162" endWordPosition="5163">gs with clear borders offer the optimal compression gain. The outcome of such a compression scheme gives the segmentation. In addition, for those approaches which also target paradigms, stem–suffix co-occurrence statistics are gathered given the segmentation produced, rather than all possible segmentations. (b) Group and Abstract: In this family of methods, morphologically related words are first grouped (clustered into sets, paired, shortlisted, etc.) according to some metric, which is typically string edit distance, but may include semantic features (Schone 2001), distributional similarity (Freitag 2005), or frequency signatures (Wicentowski 2002). The next step is to abstract some morphological pattern that recurs among the groups. Such emergent patterns provide enough clues for segmentation and can sometimes be formulated as rules or morphological paradigms. (c) Features and Classes: In this family of methods, a word is seen as made up of a set of features—n-grams in Mayfield and McNamee (2003) and McNamee and Mayfield (2007), and initial/terminal/mid-substring in De Pauw and Wagacha (2007). Features which occur on many words have little selective power across the words, whereas features wh</context>
<context position="39521" citStr="Freitag 2005" startWordPosition="6064" endWordPosition="6065"> 2006, 2007, C T Finnish/Turkish/English Segmentation+ 2008 Related sets of words Keshava and Pitler 2005 C T Finnish/Turkish/English Segmentation Johnsen 2005 C T Finnish/Turkish/English Segmentation Atwell and Roberts 2005 C T Finnish/Turkish/English Segmentation Dang and Choudri 2005 C T Finnish/Turkish/English Segmentation ur Rehman and Hussain 2005 C T Finnish/Turkish/English Segmentation Jordan, Healy, and Keselj 2005, 2006 C T Finnish/Turkish/English Segmentation Goldwater, Griffiths, and Johnson C T English/Child-English Segmentation 2005; Goldwater 2007; Naradowsky and Goldwater 2009 Freitag 2005 C T English Segmentation Golcher 2006 C - English/German Lexicon+ Paradigms Arabsorkhi and Shamsfard 2006 C T Persian Segmentation Chan 2006, Chan 2008, C T English Paradigms pages 101–139 Demberg 2007 C/NC T English/German/ Segmentation Finnish/Turkish Dasgupta and Ng 2006, 2007a; C T Bengali Segmentation 2007b; Dasgupta 2007 De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al. 2007; C T French/Bengali/French/ Analysis Majumder, Mitra, and Pal Bulgar2008 ian/Hungaria</context>
<context position="65243" citStr="Freitag 2005" startWordPosition="10318" endWordPosition="10319">istance is the most straightforward metric for which to find pairs or sets of morphologically related words (see, e.g., Gaussier 1999; Yarowsky and Wicentowski 2000; Schone and Jurafsky 2001a; Baroni, Matiasek, and Trost 2002; Hu et al. 2005a; Bernhard 2006, pages 101–117; Bernhard 2007; Majumder et al. 2007; Majumder, Mitra, and Pal 2008). In addition, as unsupervised methods for semantic clustering (e.g., Latent Semantic Analysis) and distributional clustering became more mature, these could be included as well (Schone and Jurafsky 2000, 2001a; Schone 2001; Baroni, Matiasek, and Trost 2002; Freitag 2005). More remarkable, however, is that Yarowsky and Wicentowski (2000) and Wicentowski (2002, 2004) have shown that frequency signatures can also be used to (heuristically) find morphologically related words. The example they use is sang versus sing, whose relative frequency distribution in a corpus is 1,427/1,204 (or 1.19/1), whereas singed15 versus sing is 9/1,204 (Yarowsky and Wicentowski 2000, pages 209– 210). This way, sing can be heuristically said to be parallel to sang rather than singed, and indeed the distribution for singed versus singe (its true relative) is 9/2, that is, much closer </context>
</contexts>
<marker>Freitag, 2005</marker>
<rawString>Freitag, Dayne. 2005. Morphology induction from term clusters. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages 128–135, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gammon</author>
</authors>
<title>Quantitative approximations to the word.</title>
<date>1969</date>
<booktitle>In International Conference on Computational Workshop on Unsupervised Learning in Natural Language Processing at the 37th Annual Meeting of the Association for Computational Linguistics (ACL-1999),</booktitle>
<pages>24--30</pages>
<location>Philadephia, PA.</location>
<contexts>
<context position="35391" citStr="Gammon 1969" startWordPosition="5443" endWordPosition="5444">ach word is separated into its vowel skeleton and its consonant skeleton, after which various 318 Hammarström and Borin Unsupervised Learning of Morphology Table 2 Very brief roadmap of earlier studies. Model Superv. Experimentation Learns what? Harris 1955, 1968, 1970 C T English Segmentation Andreev 1965a, Andreev 1967, C T Vietnamese to Segmentation Chapter 2, Peršikov 1965; Hungarian (I) Melkumjan 1965; Fedulova 1965; Ožigova 1965; Malahovskij 1965; Hol’m 1965; Kordi 1965; Fitialova 1965; Fihman 1965a; Andreev 1965a; Jakubajtis 1965; Panina 1965; Fihman 1965b; Eliseeva 1965; Jakuševa 1965 Gammon 1969 C T English Segmentation Lehmann 1973, pages 71–93 C T French (I) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1992, 1994, 1995 C T+SP Spanish Segmentation Janßen 1992 C T+SP French Segmentation Juola, Hall, and Boggs 1994 C T English Segmentation Brent 1993, 1999; B</context>
<context position="46085" citStr="Gammon 1969" startWordPosition="7158" endWordPosition="7159">at, bat} x a ab abe ... {z|z = xy E W} {abide, able, abode, and, art, at} {abide, able, abode} ∅ ... LSV(x) 4 (b,n,r,t) 3 (i,l,o) 0 ... 322 Hammarström and Borin Unsupervised Learning of Morphology Table 4 LSV counts for d-, di-, dis-, ... , disturbance- and LPV counts for -e, -ce, -nce, ... , -disturbance. All figures are computed on the Brown Corpus of English (Francis and Kucera 1964), using the 27 letter alphabet [a − z] plus the apostrophe. There are |W |= 42,353 word types in lowercase. LSV 13 20 21 6 1 1 3 1 1 1 1 d i s t u r b a n c e LPV 0 1 1 1 1 1 1 19 6 12 25 follow-ups to Harris (Gammon 1969; Hafer and Weiss 1974; Juola, Hall, and Boggs 1994) experiment with replacing the raw LSV/LPV counts with the entropy of the character token distribution. The character token distribution after a given segment can be seen as a probability distribution whose events are the characters of the alphabet. The entropy of this probability distribution then measures how unpredictable the next character is after a given segment. In general, for a discrete random variable X with possible values x1, ... , xn, the expression for entropy takes the following form: n H(X) = − p(xi) log2 p(xi) i=1 Thus, with </context>
</contexts>
<marker>Gammon, 1969</marker>
<rawString>Gammon, Edward. 1969. Quantitative approximations to the word. In International Conference on Computational Workshop on Unsupervised Learning in Natural Language Processing at the 37th Annual Meeting of the Association for Computational Linguistics (ACL-1999), pages 24–30, Philadephia, PA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alexander F Gelbukh</author>
<author>Mikhail Alexandrov</author>
<author>Sang-Yong Han</author>
</authors>
<title>Detecting inflection patterns in natural language by minimization of morphological model.</title>
<date>2004</date>
<booktitle>In Alberto Sanfeliu, José Francisco Martínez Trinidad, and Jesfzs Ariel Carrasco-Ochoa, editors, Proceedings of Progress in Pattern Recognition, Image Analysis and Applications, 9th Iberoamerican Congress on Pattern Recognition, CIARP ’04,</booktitle>
<volume>3287</volume>
<pages>432--438</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin,</location>
<marker>Gelbukh, Alexandrov, Han, 2004</marker>
<rawString>Gelbukh, Alexander F., Mikhail Alexandrov, and Sang-Yong Han. 2004. Detecting inflection patterns in natural language by minimization of morphological model. In Alberto Sanfeliu, José Francisco Martínez Trinidad, and Jesfzs Ariel Carrasco-Ochoa, editors, Proceedings of Progress in Pattern Recognition, Image Analysis and Applications, 9th Iberoamerican Congress on Pattern Recognition, CIARP ’04, volume 3287 of Lecture Notes in Computer Science. Springer-Verlag, Berlin, pages 432–438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jost Gippert</author>
<author>Nikolaus P Himmelmann</author>
</authors>
<date>2006</date>
<booktitle>Essentials of Language Documentation. Mouton de Gruyter,</booktitle>
<editor>and Ulrike Mosel, editors.</editor>
<location>Berlin.</location>
<marker>Gippert, Himmelmann, 2006</marker>
<rawString>Gippert, Jost, Nikolaus P. Himmelmann, and Ulrike Mosel, editors. 2006. Essentials of Language Documentation. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Golcher</author>
</authors>
<title>Statistical text segmentation with partial structure analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of KONVENS 2006,</booktitle>
<pages>44--51</pages>
<location>Konstanz.</location>
<contexts>
<context position="39559" citStr="Golcher 2006" startWordPosition="6070" endWordPosition="6071">sh Segmentation+ 2008 Related sets of words Keshava and Pitler 2005 C T Finnish/Turkish/English Segmentation Johnsen 2005 C T Finnish/Turkish/English Segmentation Atwell and Roberts 2005 C T Finnish/Turkish/English Segmentation Dang and Choudri 2005 C T Finnish/Turkish/English Segmentation ur Rehman and Hussain 2005 C T Finnish/Turkish/English Segmentation Jordan, Healy, and Keselj 2005, 2006 C T Finnish/Turkish/English Segmentation Goldwater, Griffiths, and Johnson C T English/Child-English Segmentation 2005; Goldwater 2007; Naradowsky and Goldwater 2009 Freitag 2005 C T English Segmentation Golcher 2006 C - English/German Lexicon+ Paradigms Arabsorkhi and Shamsfard 2006 C T Persian Segmentation Chan 2006, Chan 2008, C T English Paradigms pages 101–139 Demberg 2007 C/NC T English/German/ Segmentation Finnish/Turkish Dasgupta and Ng 2006, 2007a; C T Bengali Segmentation 2007b; Dasgupta 2007 De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al. 2007; C T French/Bengali/French/ Analysis Majumder, Mitra, and Pal Bulgar2008 ian/Hungarian Zeman 2008, 2009 C - Czech/English/G</context>
<context position="46842" citStr="Golcher 2006" startWordPosition="7285" endWordPosition="7286">tribution. The character token distribution after a given segment can be seen as a probability distribution whose events are the characters of the alphabet. The entropy of this probability distribution then measures how unpredictable the next character is after a given segment. In general, for a discrete random variable X with possible values x1, ... , xn, the expression for entropy takes the following form: n H(X) = − p(xi) log2 p(xi) i=1 Thus, with alphabet E, the letter successor entropy (LSE) for a prefix x is defined as � fp(xc)fp(xc) LSE(x) = − fp(x) log2 fp(x) c∈Σ At least two authors (Golcher 2006; Hammarström 2009b) have questioned entropy as the appropriate measure for highlighting a morpheme boundary. Entropy measures how skewed the distribution is as a whole, that is, how deviant the most deviant member is, in addition to the second member, the third, and so on. If there is no morpheme boundary, the morpheme continues with (at least) one character. So one deviant, highly predictable, character is necessary and sufficient to signal a non-break, and it is arguably irrelevant if there are second- and third-place, and so forth, highly predictable characters that also signal the absence</context>
</contexts>
<marker>Golcher, 2006</marker>
<rawString>Golcher, Felix. 2006. Statistical text segmentation with partial structure analysis. In Proceedings of KONVENS 2006, pages 44–51, Konstanz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Golding</author>
<author>Henry S Thompson</author>
</authors>
<title>A morphology component for language programs.</title>
<date>1985</date>
<journal>Linguistics,</journal>
<pages>23--263</pages>
<contexts>
<context position="7933" citStr="Golding and Thompson 1985" startWordPosition="1176" endWordPosition="1179"> morpheme segmentation in one algorithm are included. Hence, subsequent uses of the term segmentation in the present survey are to be understood as morpheme segmentation rather than word segmentation. We prefer the term segmentation to analysis because, in general in ULM, the algorithm does not attempt to label the segments. There have been other approaches to machine learning of morphology than pure ULM as defined here, the most popular ones being: • approaches that require selective input, such as “singular–plural pairs,” or “all members of a paradigm” (Garvin 1967; Klein and Dennison 1976; Golding and Thompson 1985; Wothke 1985; McClelland and Rumelhart 1986; Brasington, Jones, and Biggs 1988; Tufis 1989; Zhang and Kim 1990; Borin 1991; Theron and Cloete 1997; Oflazer, McShane, and Nirenburg 2001, for example) • approaches where some (small) amount of annotated data, some (small) amount of existing rule sets, or resources such as a machinereadable dictionary or a parallel corpus, are mandatory (Yarowsky and Wicentowski 2000; Yarowsky, Ngai, and Wicentowski 2001; Cucerzan and Yarowsky 2002; Neuvel and Fulop 2002; Johnson and Martin 2003; Rogati, McCarley, and Yang 2003, for example) Such approaches are e</context>
</contexts>
<marker>Golding, Thompson, 1985</marker>
<rawString>Golding, Andrew and Henry S. Thompson. 1985. A morphology component for language programs. Linguistics, 23:263–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Linguistica: An automatic morphological analyzer.</title>
<date>2000</date>
<booktitle>In Proceedings from the Main Session of the Chicago Linguistic Society’s 36th Meeting,</booktitle>
<pages>125--139</pages>
<location>Chicago, IL.</location>
<contexts>
<context position="23145" citStr="Goldsmith (2000" startWordPosition="3540" endWordPosition="3541"> in ULM rose greatly, in the wake of a general increased attention during the 1990s to statistical and information-theoretically informed approaches in natural language processing. In speech processing, the problem of word segmentation is ever-present, and as the computational tools for taking on this problem became increasingly sophisticated and increasingly available not least as the result of a general development of computing hardware and software, researchers in linguistics and computational linguistics started taking a fresh look at the problems of word segmentation and ULM. The work of Goldsmith (2000, 2001, 2006) represents a kind of focal point here. He pulls together a number of strands from earlier work, sets them against a theoretical background informed both by information theory (MDL) and linguistics, and uses them specifically to address the problem of ULM—in particular, unsupervised learning of inflectional morphology—and not, for instance, that of word segmentation or of stemming for information retrieval, and so forth. Further, there has been the idea that ULM could contribute to various open questions in the field of first-language acquisition (see, e.g., Brent, Murthy, and Lun</context>
<context position="36704" citStr="Goldsmith 2000" startWordPosition="5647" endWordPosition="5648">r, Jarosz, and Brent 2002; French Snover and Brent 2001, 2003 Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation 1996 Yvon 1996 C T French (I) Segmentation Kazakov 1997; Kazakov and C T French/English Segmentation Manandhar 1998, 2001 Jacquemin 1997 C T English Segmentation Cromm 1997 C T German Segmentation Gaussier 1999 C T French/English (I) Lexicon+ Paradigms Déjean 1998a, 1998b C T Turkish/English/ Affix Lists Korean/French/ Swahili/ Vietnamese (I) Medina Urrea 2000, 2003, 2006b C T Spanish Affix List Schone and Jurafsky 2000, 2001a; C T English Segmentation Schone 2001 Goldsmith 2000, 2001, 2006; Belkin C T English (I) Lexicon+ Paradigms and Goldsmith 2002; Goldsmith, Higgins, and Soglasnova 2001; Hu et al. 2005b; Xanthos, Hu, and Goldsmith 2006 Baroni 2000, 2003 C T Child-English/ Affix List English Cho and Han 2002 C T Korean Segmentation Sharma, Kalita, and Das 2002, 2003; C T Assamese Lexicon+ Paradigms Sharma and Das 2002 Baroni, Matiasek, and Trost 2002 C/NC T English/German (I) Related word pairs Bati 2002 C/NC T Amharic Lexicon+ Paradigms Creutz 2003, 2006; Creutz and C T Finnish/Turkish/ Segmentation Lagus 2002, 2004, 2005a, 2005b, English 2005c, 2007; Creutz, La</context>
</contexts>
<marker>Goldsmith, 2000</marker>
<rawString>Goldsmith, John. 2000. Linguistica: An automatic morphological analyzer. In Proceedings from the Main Session of the Chicago Linguistic Society’s 36th Meeting, pages 125–139, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="12273" citStr="Goldsmith (2001)" startWordPosition="1878" endWordPosition="1879"> lexeme (if meaning is taken into account). The converse need not hold; it is perfectly possible to answer the question of whether two words are of the same stem with high accuracy, without having to commit to what the actual stem should be. Many recent articles fail to deal properly with previous and related work, some reinvent heuristics that have been proposed earlier, and there is little modularization taking place. Previous surveys and overviews for a general audience are Borin (1991), Batchelder (1997, pages 66–68), Powers (1998), Clark (2001, pages 80–82), Xanthos (2007, pages 95–107), Goldsmith (2001), Daelemans (2004, page 1898), Roark and Sproat (2007, pages 116–136), Hammarström (2007b, pages 10–15), Chan (2008, pages 48–60), Hammarström (2009b, pages 14–21), Borin (2009), Goldsmith (2010), and, to a more limited extent, the related-work sections of individual research papers. Kurimo, Creutz, and Turunen (2007), Kurimo, Creutz, and Varjokallio (2008a, 2008b), Kurimo and Turunen (2008), Kurimo and Varjokallio (2008), and McNamee (2008) are overviews of systems in the MorphoChallenge of the respective year. However, we will try to be more comprehensive than previous surveys and discuss th</context>
<context position="58110" citStr="Goldsmith (2001" startWordPosition="9151" endWordPosition="9152">) of the input text data.13 To take one concrete example, Goldsmith’s (2006) particular way Q of describing morphological regularities is to allow for a list of stems, a list of affixes, a list of signatures (structures indicating which stems may appear with which affixes, i.e., a list of pointers to stems, and a list of pointers to suffixes). The search is then among different lists of stems, affixes, and signatures to see which is the shortest to account for the words of the corpus. Further details of such coding schemes need not concern us here, but for a range of options see, for example, Goldsmith (2001, 2006), Xanthos, Hu, and Goldsmith (2006), Creutz and Lagus (2007), Argamon et al. (2004), Arabsorkhi and Shamsfard (2006), ´Cavar et al. (2004b), Baroni (2003), or Brent, Murthy, and Lundberg (1995). It should be noted that the label MDL, in at least the terminology of Grünwald (2007, pages 37–38), is infelicitous for such cases where the P, D-search is not among different description languages, but among varations within a fixed language Q. For example, in the stem-affixes-signatures way of description (a specific Q), the search does not include other (possibly more parsimonious?) ways of d</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>Goldsmith, John. 2001. Unsupervised learning of the morphology of natural language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
<author>Derrick Higgins</author>
<author>Svetlana Soglasnova</author>
</authors>
<title>Automatic language-specific stemming in information retrieval.</title>
<date>2001</date>
<booktitle>Cross-Language Information Retrieval and Evaluation: Proceedings of the CLEF 2000 Workshop, Lecture Notes in Computer Science.</booktitle>
<pages>273--283</pages>
<editor>In Carol Peters, editor,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin,</location>
<marker>Goldsmith, Higgins, Soglasnova, 2001</marker>
<rawString>Goldsmith, John, Derrick Higgins, and Svetlana Soglasnova. 2001. Automatic language-specific stemming in information retrieval. In Carol Peters, editor, Cross-Language Information Retrieval and Evaluation: Proceedings of the CLEF 2000 Workshop, Lecture Notes in Computer Science. Springer-Verlag, Berlin, pages 273–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
<author>Yu Hu</author>
<author>Irina Matveeva</author>
<author>Colin Sprague</author>
</authors>
<title>A heuristic for morpheme discovery based on string edit distance.</title>
<date>2005</date>
<tech>Technical Report of</tech>
<pages>2005--4</pages>
<institution>Computer Science Department, University of Chicago,</institution>
<contexts>
<context position="38551" citStr="Goldsmith et al. 2005" startWordPosition="5929" endWordPosition="5932">n 2003b C T Inuktitut Unclear Katrenko 2004 C T Ukrainian Lexicon+ Paradigms ´Cavar et al. 2004a, 2004b; ´Cavar, C T Child-English Unclear Rodrigues, and Schrementi 2006; ´Cavar et al. 2006 Rodrigues and ´Cavar 2005, 2007 NC T Arabic Segmentation Monson 2004, 2009; Monson et al. C T English/Spanish/ Segmentation 2004, 2007a, 2007b, 2008, 2008a, Mapudungun (I) 2008b Yarowsky and Wicentowski 2000; C/NC T 30-ish mostly European Segmentation + Wicentowski 2002, 2004 type languages Rewrite Rules Gelbukh, Alexandrov, and Han 2004 C - English Segmentation Argamon et al. 2004 C T English Segmentation Goldsmith et al. 2005; Hu et al. 2005a C/NC T Unclear Unclear Bacchin, Ferro, and Melucci 2005, C T Italian/English Segmentation 2002a, 2002b; Nunzio et al. 2004 Oliver 2004, Chapter 4–5 C T Catalan Paradigms Bordag 2005a, 2005b, 2007, 2008 C T English/German Segmentation Hammarström 2005, 2006a, 2006b, C - Maori to Warlpiri Same-stem 2007b, 2009a, 2009b Bernhard 2005a, 2005b, 2006, 2007, C T Finnish/Turkish/English Segmentation+ 2008 Related sets of words Keshava and Pitler 2005 C T Finnish/Turkish/English Segmentation Johnsen 2005 C T Finnish/Turkish/English Segmentation Atwell and Roberts 2005 C T Finnish/Turki</context>
</contexts>
<marker>Goldsmith, Hu, Matveeva, Sprague, 2005</marker>
<rawString>Goldsmith, John, Yu Hu, Irina Matveeva, and Colin Sprague. 2005. A heuristic for morpheme discovery based on string edit distance. Technical Report of Computer Science Department, University of Chicago, IL. TR-2005-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
<author>Aris Xanthos</author>
</authors>
<title>Learning phonological categories.</title>
<date>2009</date>
<journal>Language,</journal>
<volume>85</volume>
<issue>1</issue>
<contexts>
<context position="41618" citStr="Goldsmith and Xanthos 2009" startWordPosition="6356" endWordPosition="6359">tenative; RR = Hand-written rewrite rules; SP = Some manually curated segmentation points; T = Thresholds and Parameters to be set by a human. frequency techniques reminiscent of those of the (a) approaches can be applied. This strategy is targeted towards the special kind of non-concatenative morphology called intercalated morphology11 with the observation that, empirically, in those (relatively few) languages which have intercalated morphology, it does seem to depend on vowel/consonant considerations. In Xanthos (2007), the phonological categories are inferred in an unsupervised manner (cf. Goldsmith and Xanthos 2009) whereas in Bati (2002) and Rodrigues and ´Cavar (2005, 2007) they are seen as given by the writing system. The first two, (a) and (b), enjoy a fair amount of popularity in the reviewed collection of work, though (a) is much more common and was the only kind used up to about 1997. The last two, (c) and (d), have been utilized only by the sets of authors cited therein. Let us now look at some salient questions in more detail. The following notation will be used in formal statements: • w, s, b, x, y, ... E E*: lowercase-letter variables range over strings of some alphabet E and are variously cal</context>
<context position="72213" citStr="Goldsmith and Xanthos (2009)" startWordPosition="11390" endWordPosition="11393">geted, Arabic (Rodrigues and ´Cavar 2005, 2007; Xanthos 2007) and Amharic (Bati 2002), this is largely true, or a transcription is used where it is largely true. Rodrigues and ´Cavar (2005, 2007) and Bati (2002) hard-code the transition from the graphemic representation of a word to its (potential) root and pattern parts. This can be said to constitute a strong language specific bias, tantamount to supervision. Xanthos (2007), on the other hand, starts out only by assuming that there exists a distinction between root and pattern graphemes and subsequently learns which graphemes are which. See Goldsmith and Xanthos (2009) for an excellent survey on how to do this (something which falls under learning phonological categories rather than morphology learning). Basically, it is possible only because there are systematic combination constraints between different phonemes (approximated by graphemes); for example, vowels and consonants alternate in a very non-random manner. Once each word is divided into its potential root and pattern, the morphology learning problem is similar to morphology learning given roots and suffixes, that is, the typical model for learning concatenative morphology, where the task is to weed </context>
</contexts>
<marker>Goldsmith, Xanthos, 2009</marker>
<rawString>Goldsmith, John and Aris Xanthos. 2009. Learning phonological categories. Language, 85(1):4–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Goldsmith</author>
</authors>
<title>An algorithm for the unsupervised learning of morphology.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="36869" citStr="Goldsmith 2006" startWordPosition="5673" endWordPosition="5674">entation Kazakov 1997; Kazakov and C T French/English Segmentation Manandhar 1998, 2001 Jacquemin 1997 C T English Segmentation Cromm 1997 C T German Segmentation Gaussier 1999 C T French/English (I) Lexicon+ Paradigms Déjean 1998a, 1998b C T Turkish/English/ Affix Lists Korean/French/ Swahili/ Vietnamese (I) Medina Urrea 2000, 2003, 2006b C T Spanish Affix List Schone and Jurafsky 2000, 2001a; C T English Segmentation Schone 2001 Goldsmith 2000, 2001, 2006; Belkin C T English (I) Lexicon+ Paradigms and Goldsmith 2002; Goldsmith, Higgins, and Soglasnova 2001; Hu et al. 2005b; Xanthos, Hu, and Goldsmith 2006 Baroni 2000, 2003 C T Child-English/ Affix List English Cho and Han 2002 C T Korean Segmentation Sharma, Kalita, and Das 2002, 2003; C T Assamese Lexicon+ Paradigms Sharma and Das 2002 Baroni, Matiasek, and Trost 2002 C/NC T English/German (I) Related word pairs Bati 2002 C/NC T Amharic Lexicon+ Paradigms Creutz 2003, 2006; Creutz and C T Finnish/Turkish/ Segmentation Lagus 2002, 2004, 2005a, 2005b, English 2005c, 2007; Creutz, Lagus, and Virpioja 2005; Hirsimäki et al. 2003; Creutz et al. 2005 319 Computational Linguistics Volume 37, Number 2 Table 2 (continued) Model Superv. Experimentation</context>
<context position="45300" citStr="Goldsmith 2006" startWordPosition="6995" endWordPosition="6996">cessor position, one occurring a thousand times and the other once, Harris’s letter successor variety is still two—the same as if the two letters occurred once each. Subsequent authors have suggested that the full frequency distribution of the token letter successors carries a better signal of morpheme boundary. After all, if there is a significant token frequency skewing, this suggests that we are in the middle of coherent morpheme. Moreover, mere type counts may be influenced by phonotactic constraints (consonant after vowel, etc.), which come out less significant in token frequency counts (Goldsmith 2006, page 6). Already the earliest Table 3 Example of LSV-counts for some example prefixes (bottom) based on a small example word list (top). W = {abide, able, abode, and, art, at, bat} x a ab abe ... {z|z = xy E W} {abide, able, abode, and, art, at} {abide, able, abode} ∅ ... LSV(x) 4 (b,n,r,t) 3 (i,l,o) 0 ... 322 Hammarström and Borin Unsupervised Learning of Morphology Table 4 LSV counts for d-, di-, dis-, ... , disturbance- and LPV counts for -e, -ce, -nce, ... , -disturbance. All figures are computed on the Brown Corpus of English (Francis and Kucera 1964), using the 27 letter alphabet [a − </context>
<context position="58152" citStr="Goldsmith (2006)" startWordPosition="9157" endWordPosition="9158">oncrete example, Goldsmith’s (2006) particular way Q of describing morphological regularities is to allow for a list of stems, a list of affixes, a list of signatures (structures indicating which stems may appear with which affixes, i.e., a list of pointers to stems, and a list of pointers to suffixes). The search is then among different lists of stems, affixes, and signatures to see which is the shortest to account for the words of the corpus. Further details of such coding schemes need not concern us here, but for a range of options see, for example, Goldsmith (2001, 2006), Xanthos, Hu, and Goldsmith (2006), Creutz and Lagus (2007), Argamon et al. (2004), Arabsorkhi and Shamsfard (2006), ´Cavar et al. (2004b), Baroni (2003), or Brent, Murthy, and Lundberg (1995). It should be noted that the label MDL, in at least the terminology of Grünwald (2007, pages 37–38), is infelicitous for such cases where the P, D-search is not among different description languages, but among varations within a fixed language Q. For example, in the stem-affixes-signatures way of description (a specific Q), the search does not include other (possibly more parsimonious?) ways of description that do not use stems, affixes,</context>
</contexts>
<marker>Goldsmith, 2006</marker>
<rawString>Goldsmith, John A. 2006. An algorithm for the unsupervised learning of morphology. Natural Language Engineering, 12(4):353–371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Goldsmith</author>
</authors>
<title>Segmentation and morphology.</title>
<date>2010</date>
<booktitle>Handbook of Computational Linguistics and Natural Language Processing, Blackwell Handbooks in Linguistics. Wiley-Blackwell,</booktitle>
<pages>364--393</pages>
<editor>In Alexander Clark, Chris Fox, and Shalom Lappin, editors,</editor>
<location>Oxford,</location>
<contexts>
<context position="12468" citStr="Goldsmith (2010)" startWordPosition="1904" endWordPosition="1905">g to commit to what the actual stem should be. Many recent articles fail to deal properly with previous and related work, some reinvent heuristics that have been proposed earlier, and there is little modularization taking place. Previous surveys and overviews for a general audience are Borin (1991), Batchelder (1997, pages 66–68), Powers (1998), Clark (2001, pages 80–82), Xanthos (2007, pages 95–107), Goldsmith (2001), Daelemans (2004, page 1898), Roark and Sproat (2007, pages 116–136), Hammarström (2007b, pages 10–15), Chan (2008, pages 48–60), Hammarström (2009b, pages 14–21), Borin (2009), Goldsmith (2010), and, to a more limited extent, the related-work sections of individual research papers. Kurimo, Creutz, and Turunen (2007), Kurimo, Creutz, and Varjokallio (2008a, 2008b), Kurimo and Turunen (2008), Kurimo and Varjokallio (2008), and McNamee (2008) are overviews of systems in the MorphoChallenge of the respective year. However, we will try to be more comprehensive than previous surveys and discuss the ideas in the field critically. 312 Hammarström and Borin Unsupervised Learning of Morphology We will not attempt a comparison in terms of accuracy figures as this is wholly impossible, not only</context>
</contexts>
<marker>Goldsmith, 2010</marker>
<rawString>Goldsmith, John A. 2010. Segmentation and morphology. In Alexander Clark, Chris Fox, and Shalom Lappin, editors, Handbook of Computational Linguistics and Natural Language Processing, Blackwell Handbooks in Linguistics. Wiley-Blackwell, Oxford, pages 364–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
</authors>
<title>Nonparametric Bayesian Models of Lexical Acquisition.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="23813" citStr="Goldwater 2007" startWordPosition="3641" endWordPosition="3642">He pulls together a number of strands from earlier work, sets them against a theoretical background informed both by information theory (MDL) and linguistics, and uses them specifically to address the problem of ULM—in particular, unsupervised learning of inflectional morphology—and not, for instance, that of word segmentation or of stemming for information retrieval, and so forth. Further, there has been the idea that ULM could contribute to various open questions in the field of first-language acquisition (see, e.g., Brent, Murthy, and Lundberg 1995; Batchelder 1997; Brent 1999; Clark 2001; Goldwater 2007). However, the connection is still rather vague and even if ULM has matured, it is not clear what implications, if any, this has for child language acquisition. Children have access to semantics and pragmatics, not just text strings, and it would be very surprising if such cues were not used at all in first language acquisition. Further, if some ULM technique was shown to be successful on some reasonably sized corpora, it does not automatically follow that children can (and do, if they can) use the same technique. Most current ULM techniques crucially involve long series of number crunching th</context>
<context position="39477" citStr="Goldwater 2007" startWordPosition="6058" endWordPosition="6059">tem 2007b, 2009a, 2009b Bernhard 2005a, 2005b, 2006, 2007, C T Finnish/Turkish/English Segmentation+ 2008 Related sets of words Keshava and Pitler 2005 C T Finnish/Turkish/English Segmentation Johnsen 2005 C T Finnish/Turkish/English Segmentation Atwell and Roberts 2005 C T Finnish/Turkish/English Segmentation Dang and Choudri 2005 C T Finnish/Turkish/English Segmentation ur Rehman and Hussain 2005 C T Finnish/Turkish/English Segmentation Jordan, Healy, and Keselj 2005, 2006 C T Finnish/Turkish/English Segmentation Goldwater, Griffiths, and Johnson C T English/Child-English Segmentation 2005; Goldwater 2007; Naradowsky and Goldwater 2009 Freitag 2005 C T English Segmentation Golcher 2006 C - English/German Lexicon+ Paradigms Arabsorkhi and Shamsfard 2006 C T Persian Segmentation Chan 2006, Chan 2008, C T English Paradigms pages 101–139 Demberg 2007 C/NC T English/German/ Segmentation Finnish/Turkish Dasgupta and Ng 2006, 2007a; C T Bengali Segmentation 2007b; Dasgupta 2007 De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al. 2007; C T French/Bengali/French/ Analysis Maju</context>
<context position="61155" citStr="Goldwater (2007" startWordPosition="9637" endWordPosition="9638"> typical corpus distribution is that a few lexemes appear very frequently but by far most lexemes appear once or only a few times (Baayen 2001). What this means for morphology is that most lexemes will appear with only one or a minority of their possible affixes, even in languages with relatively little morphology. Of course, there is also the risk that some rare affix, for example, the Classical Greek alternative medial 3p. pl. aorist imperative ending -σAary 13 As most approaches define their task as capturing the set of legal morphological forms, their goal should be to compress W, but see Goldwater (2007, pages 53–59) for arguments for compressing C. 14 Note also that paradigm information can be fed back into the segmentation process in that some affixes which do poorly according to some paradigm-related measure (e.g., affixes that do not take part in many paradigms) can be weeded out. 327 Computational Linguistics Volume 37, Number 2 (Blomqvist and Jastrup 1998), may not appear at all even in a very large Classical Greek corpus. More formally, consider a morphological paradigm (set of suffixes) P that is a true paradigm according to linguistic analysis. If k lexemes that are inflected accord</context>
<context position="75457" citStr="Goldwater (2007)" startWordPosition="11963" endWordPosition="11964">art of the split. Then the task is to find splits that maximize the following expression: 11 arg max p(xi) · p,(yi) [s1,...,s|W|] wi∈W For example, if W = {ad, ae, bd, be, cd, ce, ggg}, then the configuration of splits a|d, a|e, b|d, b|e, c|d, c|e, g|gg yields the product (2 · 3)6 · (1 · 1). Brent (1999) devises a precise, but more elaborate, way of constructing W from B and S, but at the cost of a large search space, and whose global maximum is hard to characterize intuitively. The same holds for the extension by Snover (2002). Kontorovich, Don, and Singer (2003), Snyder and Barzilay (2008), Goldwater (2007), Johnson (2008), and Poon, Cherry, and Toutanova (2009) should also be noted for containing generative models. Most approaches, of any of the kinds (a)–(d) described in Section 3.1, explicitly or implicitly target languages which have (close to) one-slot morphology, that is, a word (or stem) typically takes not more than one prefix and not more than one suffix. Many (indeed most; Dryer 2005) languages deviate more or less from this model. At first, it may seem that multi-slot morphology can be handled by the same algorithms as one-slot morphology, by iterating the process used for one-slot mo</context>
</contexts>
<marker>Goldwater, 2007</marker>
<rawString>Goldwater, Sharon. 2007. Nonparametric Bayesian Models of Lexical Acquisition. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS</booktitle>
<pages>459--466</pages>
<location>Vancouver.</location>
<marker>Goldwater, Griffiths, Johnson, 2005</marker>
<rawString>Goldwater, Sharon, Tom Griffiths, and Mark Johnson. 2005. Interpolating between types and tokens by estimating power-law generators. In Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005], pages 459–466, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Golénia</author>
</authors>
<title>Learning rules in morphology of complex synthetic languages. Master’s thesis, Université de Paris V.</title>
<date>2008</date>
<contexts>
<context position="40464" citStr="Golénia 2008" startWordPosition="6195" endWordPosition="6196">e Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al. 2007; C T French/Bengali/French/ Analysis Majumder, Mitra, and Pal Bulgar2008 ian/Hungarian Zeman 2008, 2009 C - Czech/English/German/ Segmentation+ Finnish Paradigms Kohonen, Virpioja, and Klami C T Finnish/Turkish/English Segmentation 2008 320 Hammarström and Borin Unsupervised Learning of Morphology Table 2 (continued) Model Superv. Experimentation Learns what? Goodman 2008 C T Finnish/Turkish/English Segmentation Golénia 2008 C T Turkish/Russian Segmentation Pandey and Siddiqui 2008 C T Hindi Segmentation+ Paradigms Johnson 2008 C T Sesotho Segmentation Snyder and Barzilay 2008 C/NC T Hebrew/Arabic/Aramaic/ Segmentation English Spiegler et al. 2008 C T Zulu Segmentation Moon, Erk, and Baldridge 2009 C T English/Uspanteko Segmentation Poon, Cherry, and Toutanova C T Arabic/Hebrew Segmentation 2009 Abbreviations: C = Concatenative; I = Impressionistic evaluation; IR = Evaluation only in terms of Information Retrieval Performance; NC = Non-concatenative; RR = Hand-written rewrite rules; SP = Some manually curated seg</context>
</contexts>
<marker>Golénia, 2008</marker>
<rawString>Golénia, Bruno. 2008. Learning rules in morphology of complex synthetic languages. Master’s thesis, Université de Paris V.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah A Goodman</author>
</authors>
<title>Morphological induction through linguistic productivity.</title>
<date>2008</date>
<contexts>
<context position="40410" citStr="Goodman 2008" startWordPosition="6189" endWordPosition="6190">2007a; C T Bengali Segmentation 2007b; Dasgupta 2007 De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al. 2007; C T French/Bengali/French/ Analysis Majumder, Mitra, and Pal Bulgar2008 ian/Hungarian Zeman 2008, 2009 C - Czech/English/German/ Segmentation+ Finnish Paradigms Kohonen, Virpioja, and Klami C T Finnish/Turkish/English Segmentation 2008 320 Hammarström and Borin Unsupervised Learning of Morphology Table 2 (continued) Model Superv. Experimentation Learns what? Goodman 2008 C T Finnish/Turkish/English Segmentation Golénia 2008 C T Turkish/Russian Segmentation Pandey and Siddiqui 2008 C T Hindi Segmentation+ Paradigms Johnson 2008 C T Sesotho Segmentation Snyder and Barzilay 2008 C/NC T Hebrew/Arabic/Aramaic/ Segmentation English Spiegler et al. 2008 C T Zulu Segmentation Moon, Erk, and Baldridge 2009 C T English/Uspanteko Segmentation Poon, Cherry, and Toutanova C T Arabic/Hebrew Segmentation 2009 Abbreviations: C = Concatenative; I = Impressionistic evaluation; IR = Evaluation only in terms of Information Retrieval Performance; NC = Non-concatenative; RR = Hand</context>
</contexts>
<marker>Goodman, 2008</marker>
<rawString>Goodman, Sarah A. 2008. Morphological induction through linguistic productivity.</rawString>
</citation>
<citation valid="true">
<date>2008</date>
<booktitle>In Working Notes for the CLEF</booktitle>
<location>Workshop, Aarhus.</location>
<contexts>
<context position="12667" citStr="(2008)" startWordPosition="1933" endWordPosition="1933">ation taking place. Previous surveys and overviews for a general audience are Borin (1991), Batchelder (1997, pages 66–68), Powers (1998), Clark (2001, pages 80–82), Xanthos (2007, pages 95–107), Goldsmith (2001), Daelemans (2004, page 1898), Roark and Sproat (2007, pages 116–136), Hammarström (2007b, pages 10–15), Chan (2008, pages 48–60), Hammarström (2009b, pages 14–21), Borin (2009), Goldsmith (2010), and, to a more limited extent, the related-work sections of individual research papers. Kurimo, Creutz, and Turunen (2007), Kurimo, Creutz, and Varjokallio (2008a, 2008b), Kurimo and Turunen (2008), Kurimo and Varjokallio (2008), and McNamee (2008) are overviews of systems in the MorphoChallenge of the respective year. However, we will try to be more comprehensive than previous surveys and discuss the ideas in the field critically. 312 Hammarström and Borin Unsupervised Learning of Morphology We will not attempt a comparison in terms of accuracy figures as this is wholly impossible, not only because of the great variation in goals but also because most descriptions do not specify their algorithm(s) in enough detail. Fortunately, this aspect is better handled in controlled competitions, </context>
<context position="75439" citStr="(2008)" startWordPosition="11962" endWordPosition="11962">e last part of the split. Then the task is to find splits that maximize the following expression: 11 arg max p(xi) · p,(yi) [s1,...,s|W|] wi∈W For example, if W = {ad, ae, bd, be, cd, ce, ggg}, then the configuration of splits a|d, a|e, b|d, b|e, c|d, c|e, g|gg yields the product (2 · 3)6 · (1 · 1). Brent (1999) devises a precise, but more elaborate, way of constructing W from B and S, but at the cost of a large search space, and whose global maximum is hard to characterize intuitively. The same holds for the extension by Snover (2002). Kontorovich, Don, and Singer (2003), Snyder and Barzilay (2008), Goldwater (2007), Johnson (2008), and Poon, Cherry, and Toutanova (2009) should also be noted for containing generative models. Most approaches, of any of the kinds (a)–(d) described in Section 3.1, explicitly or implicitly target languages which have (close to) one-slot morphology, that is, a word (or stem) typically takes not more than one prefix and not more than one suffix. Many (indeed most; Dryer 2005) languages deviate more or less from this model. At first, it may seem that multi-slot morphology can be handled by the same algorithms as one-slot morphology, by iterating the process us</context>
</contexts>
<marker>2008</marker>
<rawString>In Working Notes for the CLEF 2008 Workshop, Aarhus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Grünwald</author>
</authors>
<title>The Minimum Description Length Principle: Adaptive Computation and Machine Learning.</title>
<date>2007</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="56520" citStr="Grünwald 2007" startWordPosition="8869" endWordPosition="8870">o not automatically generalize. Such considerations have led many authors to devise compression-inspired models for exploiting skewed frequencies. In particular, several different sets of authors have invoked Minimum Description Length (MDL) as the motivation for a given formula to compress input data into a morphologically analyzed representation.12 The MDL principle is a general-purpose method of statistical inference. It views the learning/inference process as data compression: For a given set of hypotheses H and data set D, we should try to find the hypothesis in H that compresses D most (Grünwald 2007, pages 3–40). Concretely, such a calculation can take the the following form. If L(H) is the length, in bits, of the description of the hypothesis; and L(D|H) is the length, in bits, of the description of the data when encoded with the help of the hypothesis, then MDL aims to minimize L(H) + L(D|H). In principle, all of the works that have invoked MDL in their ULM method act as follows. A particular way Q of describing morphological regularities is conceived that has two components which we may call patterns P and data D. A coding scheme is devised to describe any P and to describe any collec</context>
<context position="58396" citStr="Grünwald (2007" startWordPosition="9197" endWordPosition="9198">of pointers to stems, and a list of pointers to suffixes). The search is then among different lists of stems, affixes, and signatures to see which is the shortest to account for the words of the corpus. Further details of such coding schemes need not concern us here, but for a range of options see, for example, Goldsmith (2001, 2006), Xanthos, Hu, and Goldsmith (2006), Creutz and Lagus (2007), Argamon et al. (2004), Arabsorkhi and Shamsfard (2006), ´Cavar et al. (2004b), Baroni (2003), or Brent, Murthy, and Lundberg (1995). It should be noted that the label MDL, in at least the terminology of Grünwald (2007, pages 37–38), is infelicitous for such cases where the P, D-search is not among different description languages, but among varations within a fixed language Q. For example, in the stem-affixes-signatures way of description (a specific Q), the search does not include other (possibly more parsimonious?) ways of description that do not use stems, affixes, or signatures at all. For the MDL-label to apply with its full philosophical underpinnings, the scope must include any possible compression algorithm, namely, any Turing machine. In this respect it is important to note that, compared to the sc</context>
</contexts>
<marker>Grünwald, 2007</marker>
<rawString>Grünwald, Peter D. 2007. The Minimum Description Length Principle: Adaptive Computation and Machine Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fadila Hadouche</author>
</authors>
<title>Détection de relations morphologiques en corpus basée sur les cooccurrences.</title>
<date>2002</date>
<booktitle>Master’s thesis, DESS, Centre de Recherche en Ingénierie</booktitle>
<location>Multilingue, CRIM,</location>
<contexts>
<context position="37812" citStr="Hadouche 2002" startWordPosition="5818" endWordPosition="5819">utz and C T Finnish/Turkish/ Segmentation Lagus 2002, 2004, 2005a, 2005b, English 2005c, 2007; Creutz, Lagus, and Virpioja 2005; Hirsimäki et al. 2003; Creutz et al. 2005 319 Computational Linguistics Volume 37, Number 2 Table 2 (continued) Model Superv. Experimentation Learns what? Kontorovich, Don, and Singer 2003 C T English Segmentation Medina Urrea and Díaz 2003; C T Chuj/Ralámuri/Czech Affix List Medina-Urrea 2006a, 2008 Mayfield and McNamee 2003; - -8 West European Same-stem McNamee and Mayfield 2007 languages (IR) Zweigenbaum, Hadouche, and C T Medical French Segmentation Grabar 2003; Hadouche 2002 Pirrelli et al. 2004; Pirrelli and C T Italian/English/Arabic Unclear Herreros 2007; Calderone 2008 Johnson and Martin 2003b C T Inuktitut Unclear Katrenko 2004 C T Ukrainian Lexicon+ Paradigms ´Cavar et al. 2004a, 2004b; ´Cavar, C T Child-English Unclear Rodrigues, and Schrementi 2006; ´Cavar et al. 2006 Rodrigues and ´Cavar 2005, 2007 NC T Arabic Segmentation Monson 2004, 2009; Monson et al. C T English/Spanish/ Segmentation 2004, 2007a, 2007b, 2008, 2008a, Mapudungun (I) 2008b Yarowsky and Wicentowski 2000; C/NC T 30-ish mostly European Segmentation + Wicentowski 2002, 2004 type languages </context>
</contexts>
<marker>Hadouche, 2002</marker>
<rawString>Hadouche, Fadila. 2002. Détection de relations morphologiques en corpus basée sur les cooccurrences. Master’s thesis, DESS, Centre de Recherche en Ingénierie Multilingue, CRIM, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret A Hafer</author>
<author>Stephen F Weiss</author>
</authors>
<title>Word segmentation by letter successor varieties. Information Storage and Retrieval,</title>
<date>1974</date>
<pages>10--371</pages>
<contexts>
<context position="35621" citStr="Hafer and Weiss 1974" startWordPosition="5479" endWordPosition="5482">rimentation Learns what? Harris 1955, 1968, 1970 C T English Segmentation Andreev 1965a, Andreev 1967, C T Vietnamese to Segmentation Chapter 2, Peršikov 1965; Hungarian (I) Melkumjan 1965; Fedulova 1965; Ožigova 1965; Malahovskij 1965; Hol’m 1965; Kordi 1965; Fitialova 1965; Fihman 1965a; Andreev 1965a; Jakubajtis 1965; Panina 1965; Fihman 1965b; Eliseeva 1965; Jakuševa 1965 Gammon 1969 C T English Segmentation Lehmann 1973, pages 71–93 C T French (I) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1992, 1994, 1995 C T+SP Spanish Segmentation Janßen 1992 C T+SP French Segmentation Juola, Hall, and Boggs 1994 C T English Segmentation Brent 1993, 1999; Brent, Murthy, C T English/Child- Segmentation and Lundberg 1995; Snover 2002; English/Polish/ Snover, Jarosz, and Brent 2002; French Snover and Brent 2001, 2003 Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation </context>
<context position="43774" citStr="Hafer and Weiss (1974)" startWordPosition="6733" endWordPosition="6736">l segment x. Subscript letters are dropped when understood from the context. 3.2 Border and Frequency Methods 3.2.1 Letter Successor Varieties. Most (if not all) authors trace the inspiration for their border heuristics back to Harris (1955). In fact, Harris defines a family of heuristics, all based on letter successor/predecessor varieties. They were originally presented as applying to utterances made up of phoneme sequences (Harris 1955), but they apply just the same to words, namely, grapheme sequences (Harris 1970). The basic counting strategy, labelled letter successor varieties (LSV) by Hafer and Weiss (1974), is as follows. Given a set of words W, the letter successor variety of a string x of length i is defined as the number of distinct letters that occupy the i + 1st position in words that begin with x in W: LSV(x) = |{z[|x |+ 1]|z = xy E W}| Table 3 shows an example of a letter successor count on a tiny contrived wordlist. We may define the letter predecessor variety (LPV) analogously. For a given suffix x, the LPV(x) is the number of distinct letters that occupy the position immediately preceding x in the words of W that end in x. LSV/LPV counts for an example word are shown in Table 4. It sh</context>
<context position="46107" citStr="Hafer and Weiss 1974" startWordPosition="7160" endWordPosition="7163">ab abe ... {z|z = xy E W} {abide, able, abode, and, art, at} {abide, able, abode} ∅ ... LSV(x) 4 (b,n,r,t) 3 (i,l,o) 0 ... 322 Hammarström and Borin Unsupervised Learning of Morphology Table 4 LSV counts for d-, di-, dis-, ... , disturbance- and LPV counts for -e, -ce, -nce, ... , -disturbance. All figures are computed on the Brown Corpus of English (Francis and Kucera 1964), using the 27 letter alphabet [a − z] plus the apostrophe. There are |W |= 42,353 word types in lowercase. LSV 13 20 21 6 1 1 3 1 1 1 1 d i s t u r b a n c e LPV 0 1 1 1 1 1 1 19 6 12 25 follow-ups to Harris (Gammon 1969; Hafer and Weiss 1974; Juola, Hall, and Boggs 1994) experiment with replacing the raw LSV/LPV counts with the entropy of the character token distribution. The character token distribution after a given segment can be seen as a probability distribution whose events are the characters of the alphabet. The entropy of this probability distribution then measures how unpredictable the next character is after a given segment. In general, for a discrete random variable X with possible values x1, ... , xn, the expression for entropy takes the following form: n H(X) = − p(xi) log2 p(xi) i=1 Thus, with alphabet E, the letter</context>
<context position="51097" citStr="Hafer and Weiss (1974)" startWordPosition="7984" endWordPosition="7987">ects. Table 7 The Pearson product-moment correlation coefficient between LPH/LPE/LPM-values (r) and the Pearson product-moment correlation coefficient between LPH/LPE/LPM-ranks (r-rank). All values are computed on the Brown Corpus of English (Francis and Kucera 1964), using the 27-letter alphabet [a − z] plus the apostrophe. There are |W |= 42,353 word types in lowercase. LPH&amp;LPE LPE&amp;LPM LPM&amp;LPH r 0.872 0.957 0.729 r-rank 0.999 0.998 0.996 324 Hammarström and Borin Unsupervised Learning of Morphology A number of concrete ways to use LSV/LPVs for segmentation are suggested by Harris (1955) and Hafer and Weiss (1974); for instance: (a) Cutoff: By far the easiest way to segment a test word is first to pick some cutoff threshold k and then break the word wherever its successor (or predecessor or both) variety reaches or exceeds k. (b) Peak and plateau: In the peak and plateau strategy, a cut in a word w is made after a prefix x if and only if LSV(w[1 : |x |− 1]) &lt; LSV(x) &gt; LSV(w[1 : |x |+ 1]); that is, if the successor count for x forms a local “peak” or sits on a “plateau” of the LSV-sequence along the word. (c) Complete word: A break is made after a word prefix (or before a word suffix) if that prefix (or</context>
</contexts>
<marker>Hafer, Weiss, 1974</marker>
<rawString>Hafer, Margaret A. and Stephen F. Weiss. 1974. Word segmentation by letter successor varieties. Information Storage and Retrieval, 10:371–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Hall</author>
</authors>
<title>Bloomfield and semantics.</title>
<date>1987</date>
<booktitle>Bloomfield: Essays on his Life and work. John Benjamins,</booktitle>
<pages>155--160</pages>
<editor>In Robert A. Hall, Jr., editor, Leonard</editor>
<location>Amsterdam,</location>
<contexts>
<context position="14558" citStr="Hall 1987" startWordPosition="2208" endWordPosition="2209">cedures is often connected with the name of Leonard Bloomfield, and its core tenet may be succinctly summed up in Bloomfield’s oft-quoted dictum: “The only useful generalizations about language are inductive generalizations” (Bloomfield 1933, page 20). The so-called “extremist Post-Bloomfieldians” took this program a step further: “From Bloomfield’s justified insistence on formal, rather than semantic, features as the starting-point for linguistic analysis, this group (especially Harris) set up as a theoretical aim the description of linguistic structure exclusively in terms of distribution” (Hall 1987, page 156). The earliest reason for interest in ULM was thus—at least in part—methodological and arguably even ideological, but not (unlike at least some of the later ULM work) motivated by, for example, a desire to simulate language acquisition in humans. More or less simultaneously with but independently of Harris, the Russian linguist Andreev launched a program much like that of Harris.7 Andreev’s work is much less known than that of Harris’s, and for this reason we will describe it in some detail here. In a series of publications (Andreev 1959, 1963, 1965b, 1967), he develops an “algorith</context>
</contexts>
<marker>Hall, 1987</marker>
<rawString>Hall, Jr., Robert A. 1987. Bloomfield and semantics. In Robert A. Hall, Jr., editor, Leonard Bloomfield: Essays on his Life and work. John Benjamins, Amsterdam, pages 155–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Hammarström</author>
</authors>
<title>A new algorithm for unsupervised induction of concatenative morphology.</title>
<date>2005</date>
<booktitle>In Finite State Methods in Natural Language Processing: 5th International Workshop,</booktitle>
<contexts>
<context position="38819" citStr="Hammarström 2005" startWordPosition="5973" endWordPosition="5974">et al. C T English/Spanish/ Segmentation 2004, 2007a, 2007b, 2008, 2008a, Mapudungun (I) 2008b Yarowsky and Wicentowski 2000; C/NC T 30-ish mostly European Segmentation + Wicentowski 2002, 2004 type languages Rewrite Rules Gelbukh, Alexandrov, and Han 2004 C - English Segmentation Argamon et al. 2004 C T English Segmentation Goldsmith et al. 2005; Hu et al. 2005a C/NC T Unclear Unclear Bacchin, Ferro, and Melucci 2005, C T Italian/English Segmentation 2002a, 2002b; Nunzio et al. 2004 Oliver 2004, Chapter 4–5 C T Catalan Paradigms Bordag 2005a, 2005b, 2007, 2008 C T English/German Segmentation Hammarström 2005, 2006a, 2006b, C - Maori to Warlpiri Same-stem 2007b, 2009a, 2009b Bernhard 2005a, 2005b, 2006, 2007, C T Finnish/Turkish/English Segmentation+ 2008 Related sets of words Keshava and Pitler 2005 C T Finnish/Turkish/English Segmentation Johnsen 2005 C T Finnish/Turkish/English Segmentation Atwell and Roberts 2005 C T Finnish/Turkish/English Segmentation Dang and Choudri 2005 C T Finnish/Turkish/English Segmentation ur Rehman and Hussain 2005 C T Finnish/Turkish/English Segmentation Jordan, Healy, and Keselj 2005, 2006 C T Finnish/Turkish/English Segmentation Goldwater, Griffiths, and Johnson C</context>
</contexts>
<marker>Hammarström, 2005</marker>
<rawString>Hammarström, Harald. 2005. A new algorithm for unsupervised induction of concatenative morphology. In Finite State Methods in Natural Language Processing: 5th International Workshop,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Unsupervised word segmentation for Sesotho using adaptor grammars.</title>
<date>2008</date>
<booktitle>In Proceedings of the Tenth Meeting of ACL Special Interest Group on Computational Morphology and Phonology,</booktitle>
<pages>20--27</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="40569" citStr="Johnson 2008" startWordPosition="6210" endWordPosition="6211">kish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al. 2007; C T French/Bengali/French/ Analysis Majumder, Mitra, and Pal Bulgar2008 ian/Hungarian Zeman 2008, 2009 C - Czech/English/German/ Segmentation+ Finnish Paradigms Kohonen, Virpioja, and Klami C T Finnish/Turkish/English Segmentation 2008 320 Hammarström and Borin Unsupervised Learning of Morphology Table 2 (continued) Model Superv. Experimentation Learns what? Goodman 2008 C T Finnish/Turkish/English Segmentation Golénia 2008 C T Turkish/Russian Segmentation Pandey and Siddiqui 2008 C T Hindi Segmentation+ Paradigms Johnson 2008 C T Sesotho Segmentation Snyder and Barzilay 2008 C/NC T Hebrew/Arabic/Aramaic/ Segmentation English Spiegler et al. 2008 C T Zulu Segmentation Moon, Erk, and Baldridge 2009 C T English/Uspanteko Segmentation Poon, Cherry, and Toutanova C T Arabic/Hebrew Segmentation 2009 Abbreviations: C = Concatenative; I = Impressionistic evaluation; IR = Evaluation only in terms of Information Retrieval Performance; NC = Non-concatenative; RR = Hand-written rewrite rules; SP = Some manually curated segmentation points; T = Thresholds and Parameters to be set by a human. frequency techniques reminiscent of</context>
<context position="75473" citStr="Johnson (2008)" startWordPosition="11965" endWordPosition="11966">Then the task is to find splits that maximize the following expression: 11 arg max p(xi) · p,(yi) [s1,...,s|W|] wi∈W For example, if W = {ad, ae, bd, be, cd, ce, ggg}, then the configuration of splits a|d, a|e, b|d, b|e, c|d, c|e, g|gg yields the product (2 · 3)6 · (1 · 1). Brent (1999) devises a precise, but more elaborate, way of constructing W from B and S, but at the cost of a large search space, and whose global maximum is hard to characterize intuitively. The same holds for the extension by Snover (2002). Kontorovich, Don, and Singer (2003), Snyder and Barzilay (2008), Goldwater (2007), Johnson (2008), and Poon, Cherry, and Toutanova (2009) should also be noted for containing generative models. Most approaches, of any of the kinds (a)–(d) described in Section 3.1, explicitly or implicitly target languages which have (close to) one-slot morphology, that is, a word (or stem) typically takes not more than one prefix and not more than one suffix. Many (indeed most; Dryer 2005) languages deviate more or less from this model. At first, it may seem that multi-slot morphology can be handled by the same algorithms as one-slot morphology, by iterating the process used for one-slot morphology. A deca</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Johnson, Mark. 2008. Unsupervised word segmentation for Sesotho using adaptor grammars. In Proceedings of the Tenth Meeting of ACL Special Interest Group on Computational Morphology and Phonology, pages 20–27, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Jordan</author>
<author>John Healy</author>
<author>Vlado Keselj</author>
</authors>
<title>Swordfish: Using n-grams in an</title>
<date>2005</date>
<pages>42--46</pages>
<institution>Helsinki University of Technology,</institution>
<location>Helsinki.</location>
<marker>Jordan, Healy, Keselj, 2005</marker>
<rawString>Jordan, Chris, John Healy, and Vlado Keselj. 2005. Swordfish: Using n-grams in an pages 42–46, Helsinki University of Technology, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Jordan</author>
<author>John Healy</author>
<author>Vlado Keselj</author>
</authors>
<title>Swordfish: An unsupervised ngram based approach to morphological analysis.</title>
<date>2006</date>
<booktitle>In SIGIR ’06: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>657--658</pages>
<location>New York, NY.</location>
<marker>Jordan, Healy, Keselj, 2006</marker>
<rawString>Jordan, Chris, John Healy, and Vlado Keselj. 2006. Swordfish: An unsupervised ngram based approach to morphological analysis. In SIGIR ’06: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 657–658, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Juola</author>
<author>Chris Hall</author>
<author>Adam Boggs</author>
</authors>
<title>Corpus-based morphological segmentation by entropy changes.</title>
<date>1994</date>
<booktitle>In Third Conference on the Cognitive Science of Natural Language Processing,</booktitle>
<location>Dublin.</location>
<marker>Juola, Hall, Boggs, 1994</marker>
<rawString>Juola, Patrick, Chris Hall, and Adam Boggs. 1994. Corpus-based morphological segmentation by entropy changes. In Third Conference on the Cognitive Science of Natural Language Processing, Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sophia Katrenko</author>
</authors>
<title>Towards unsupervised learning of morphology applied to Ukrainian. Student Session:</title>
<date>2004</date>
<booktitle>16th European Summer School in Logic, Language and Information,</booktitle>
<pages>138--148</pages>
<location>Nancy.</location>
<contexts>
<context position="37973" citStr="Katrenko 2004" startWordPosition="5842" endWordPosition="5843"> al. 2005 319 Computational Linguistics Volume 37, Number 2 Table 2 (continued) Model Superv. Experimentation Learns what? Kontorovich, Don, and Singer 2003 C T English Segmentation Medina Urrea and Díaz 2003; C T Chuj/Ralámuri/Czech Affix List Medina-Urrea 2006a, 2008 Mayfield and McNamee 2003; - -8 West European Same-stem McNamee and Mayfield 2007 languages (IR) Zweigenbaum, Hadouche, and C T Medical French Segmentation Grabar 2003; Hadouche 2002 Pirrelli et al. 2004; Pirrelli and C T Italian/English/Arabic Unclear Herreros 2007; Calderone 2008 Johnson and Martin 2003b C T Inuktitut Unclear Katrenko 2004 C T Ukrainian Lexicon+ Paradigms ´Cavar et al. 2004a, 2004b; ´Cavar, C T Child-English Unclear Rodrigues, and Schrementi 2006; ´Cavar et al. 2006 Rodrigues and ´Cavar 2005, 2007 NC T Arabic Segmentation Monson 2004, 2009; Monson et al. C T English/Spanish/ Segmentation 2004, 2007a, 2007b, 2008, 2008a, Mapudungun (I) 2008b Yarowsky and Wicentowski 2000; C/NC T 30-ish mostly European Segmentation + Wicentowski 2002, 2004 type languages Rewrite Rules Gelbukh, Alexandrov, and Han 2004 C - English Segmentation Argamon et al. 2004 C T English Segmentation Goldsmith et al. 2005; Hu et al. 2005a C/NC</context>
</contexts>
<marker>Katrenko, 2004</marker>
<rawString>Katrenko, Sophia. 2004. Towards unsupervised learning of morphology applied to Ukrainian. Student Session: 16th European Summer School in Logic, Language and Information, pages 138–148, Nancy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitar Kazakov</author>
</authors>
<title>Unsupervised learning of naïve morphology with genetic algorithms.</title>
<date>1997</date>
<booktitle>In ECML’97 – Workshop Notes on Empirical Learning of Natural Language Tasks,</booktitle>
<pages>105--112</pages>
<location>Prague.</location>
<contexts>
<context position="36276" citStr="Kazakov 1997" startWordPosition="5583" endWordPosition="5584">nger 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1992, 1994, 1995 C T+SP Spanish Segmentation Janßen 1992 C T+SP French Segmentation Juola, Hall, and Boggs 1994 C T English Segmentation Brent 1993, 1999; Brent, Murthy, C T English/Child- Segmentation and Lundberg 1995; Snover 2002; English/Polish/ Snover, Jarosz, and Brent 2002; French Snover and Brent 2001, 2003 Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation 1996 Yvon 1996 C T French (I) Segmentation Kazakov 1997; Kazakov and C T French/English Segmentation Manandhar 1998, 2001 Jacquemin 1997 C T English Segmentation Cromm 1997 C T German Segmentation Gaussier 1999 C T French/English (I) Lexicon+ Paradigms Déjean 1998a, 1998b C T Turkish/English/ Affix Lists Korean/French/ Swahili/ Vietnamese (I) Medina Urrea 2000, 2003, 2006b C T Spanish Affix List Schone and Jurafsky 2000, 2001a; C T English Segmentation Schone 2001 Goldsmith 2000, 2001, 2006; Belkin C T English (I) Lexicon+ Paradigms and Goldsmith 2002; Goldsmith, Higgins, and Soglasnova 2001; Hu et al. 2005b; Xanthos, Hu, and Goldsmith 2006 Baroni</context>
</contexts>
<marker>Kazakov, 1997</marker>
<rawString>Kazakov, Dimitar. 1997. Unsupervised learning of naïve morphology with genetic algorithms. In ECML’97 – Workshop Notes on Empirical Learning of Natural Language Tasks, pages 105–112, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitar Kazakov</author>
<author>Suresh Manandhar</author>
</authors>
<title>A hybrid approach to word segmentation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 8th International Workshop on Inductive Logic Programming (ILP-98),</booktitle>
<pages>125--134</pages>
<location>Madison, WI.</location>
<marker>Kazakov, Manandhar, 1998</marker>
<rawString>Kazakov, Dimitar and Suresh Manandhar. 1998. A hybrid approach to word segmentation. In Proceedings of the 8th International Workshop on Inductive Logic Programming (ILP-98), pages 125–134, Madison, WI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitar Kazakov</author>
<author>Suresh Manandhar</author>
</authors>
<title>Unsupervised learning of word segmentation rules with genetic algorithms and inductive logic programming.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>43--121</pages>
<marker>Kazakov, Manandhar, 2001</marker>
<rawString>Kazakov, Dimitar and Suresh Manandhar. 2001. Unsupervised learning of word segmentation rules with genetic algorithms and inductive logic programming. Machine Learning, 43:121–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samarth Keshava</author>
<author>Emily Pitler</author>
</authors>
<title>A simpler, intuitive approach to morpheme induction.</title>
<date>2005</date>
<booktitle>Proceedings of MorphoChallenge 2005,</booktitle>
<pages>28--32</pages>
<editor>In Mikko Kurimo, Mathias Creutz, and Krista Lagus, editors,</editor>
<institution>Helsinki University of Technology,</institution>
<location>Helsinki.</location>
<contexts>
<context position="39014" citStr="Keshava and Pitler 2005" startWordPosition="6000" endWordPosition="6003"> 2004 type languages Rewrite Rules Gelbukh, Alexandrov, and Han 2004 C - English Segmentation Argamon et al. 2004 C T English Segmentation Goldsmith et al. 2005; Hu et al. 2005a C/NC T Unclear Unclear Bacchin, Ferro, and Melucci 2005, C T Italian/English Segmentation 2002a, 2002b; Nunzio et al. 2004 Oliver 2004, Chapter 4–5 C T Catalan Paradigms Bordag 2005a, 2005b, 2007, 2008 C T English/German Segmentation Hammarström 2005, 2006a, 2006b, C - Maori to Warlpiri Same-stem 2007b, 2009a, 2009b Bernhard 2005a, 2005b, 2006, 2007, C T Finnish/Turkish/English Segmentation+ 2008 Related sets of words Keshava and Pitler 2005 C T Finnish/Turkish/English Segmentation Johnsen 2005 C T Finnish/Turkish/English Segmentation Atwell and Roberts 2005 C T Finnish/Turkish/English Segmentation Dang and Choudri 2005 C T Finnish/Turkish/English Segmentation ur Rehman and Hussain 2005 C T Finnish/Turkish/English Segmentation Jordan, Healy, and Keselj 2005, 2006 C T Finnish/Turkish/English Segmentation Goldwater, Griffiths, and Johnson C T English/Child-English Segmentation 2005; Goldwater 2007; Naradowsky and Goldwater 2009 Freitag 2005 C T English Segmentation Golcher 2006 C - English/German Lexicon+ Paradigms Arabsorkhi and S</context>
</contexts>
<marker>Keshava, Pitler, 2005</marker>
<rawString>Keshava, Samarth and Emily Pitler. 2005. A simpler, intuitive approach to morpheme induction. In Mikko Kurimo, Mathias Creutz, and Krista Lagus, editors, Proceedings of MorphoChallenge 2005, pages 28–32, Helsinki University of Technology, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheldon Klein</author>
<author>Terry A Dennison</author>
</authors>
<title>An interactive program for learning the morphology of natural languages.</title>
<date>1976</date>
<booktitle>In Proceedings of the 3rd International Meeting on Computational Linguistics,</booktitle>
<pages>343--353</pages>
<location>Debrecen.</location>
<contexts>
<context position="7906" citStr="Klein and Dennison 1976" startWordPosition="1172" endWordPosition="1175">oth word segmentation and morpheme segmentation in one algorithm are included. Hence, subsequent uses of the term segmentation in the present survey are to be understood as morpheme segmentation rather than word segmentation. We prefer the term segmentation to analysis because, in general in ULM, the algorithm does not attempt to label the segments. There have been other approaches to machine learning of morphology than pure ULM as defined here, the most popular ones being: • approaches that require selective input, such as “singular–plural pairs,” or “all members of a paradigm” (Garvin 1967; Klein and Dennison 1976; Golding and Thompson 1985; Wothke 1985; McClelland and Rumelhart 1986; Brasington, Jones, and Biggs 1988; Tufis 1989; Zhang and Kim 1990; Borin 1991; Theron and Cloete 1997; Oflazer, McShane, and Nirenburg 2001, for example) • approaches where some (small) amount of annotated data, some (small) amount of existing rule sets, or resources such as a machinereadable dictionary or a parallel corpus, are mandatory (Yarowsky and Wicentowski 2000; Yarowsky, Ngai, and Wicentowski 2001; Cucerzan and Yarowsky 2002; Neuvel and Fulop 2002; Johnson and Martin 2003; Rogati, McCarley, and Yang 2003, for exa</context>
</contexts>
<marker>Klein, Dennison, 1976</marker>
<rawString>Klein, Sheldon and Terry A. Dennison. 1976. An interactive program for learning the morphology of natural languages. In Proceedings of the 3rd International Meeting on Computational Linguistics, pages 343–353, Debrecen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ursula Klenk</author>
</authors>
<title>Ein nicht-lexikalisches Verfahren zur Erkennung spanischer Wortstämme.</title>
<date>1985</date>
<booktitle>Strukturen und Verfahren in der maschinellen Sprachverarbeitung. AQ-Verlag, Dudweiler,</booktitle>
<pages>47--65</pages>
<editor>In Ursula Klenk, editor,</editor>
<contexts>
<context position="21844" citStr="Klenk (1985" startWordPosition="3338" endWordPosition="3339">rs that Andreev provides mostly without motivation or comment in fact can be changed in a more accepting direction, leading to much increased recall without much loss in precision. Unfortunately, however, in his short paper, Cromm does not provide enough information about the algorithm or the changes that he made to it, so that the Russian original is still the only publicly available source for the details of Andreev’s approach. A very different, more practically oriented, motivation for ULM came in the 1980s, beginning with the supervised morphology learning ideas by Wothke (1985, 1986) and Klenk (1985a, 1985b) which later led to partly unsupervised methods (see the following). Because full natural language lexica, at the time, were too big to fit in working memory, these authors were looking for a way to analyze or stem running words in a “nichtlexikalisches” manner, that is, without the storage and use of a large lexicon. This motivation is now obsolete. The interest in purebred ULM was fairly low until about 1990, however, with only a few works appearing between the mid 1960s and 1990. Especially in the 1980s, the focus in computational morphology was on the development of finite-state a</context>
</contexts>
<marker>Klenk, 1985</marker>
<rawString>Klenk, Ursula. 1985a. Ein nicht-lexikalisches Verfahren zur Erkennung spanischer Wortstämme. In Ursula Klenk, editor, Strukturen und Verfahren in der maschinellen Sprachverarbeitung. AQ-Verlag, Dudweiler, pages 47–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ursula Klenk</author>
</authors>
<title>Recognition of Spanish inflectional endings based on the distribution of characters.</title>
<date>1985</date>
<booktitle>In Computers in Literary and Linguistic Computing: Proceedings of the Eleventh International Conference [L’ordinateur et les recherches littéraires et linguistiques: actes de la XIe Conférence internationale],</booktitle>
<pages>246--253</pages>
<location>Louvain.</location>
<contexts>
<context position="21844" citStr="Klenk (1985" startWordPosition="3338" endWordPosition="3339">rs that Andreev provides mostly without motivation or comment in fact can be changed in a more accepting direction, leading to much increased recall without much loss in precision. Unfortunately, however, in his short paper, Cromm does not provide enough information about the algorithm or the changes that he made to it, so that the Russian original is still the only publicly available source for the details of Andreev’s approach. A very different, more practically oriented, motivation for ULM came in the 1980s, beginning with the supervised morphology learning ideas by Wothke (1985, 1986) and Klenk (1985a, 1985b) which later led to partly unsupervised methods (see the following). Because full natural language lexica, at the time, were too big to fit in working memory, these authors were looking for a way to analyze or stem running words in a “nichtlexikalisches” manner, that is, without the storage and use of a large lexicon. This motivation is now obsolete. The interest in purebred ULM was fairly low until about 1990, however, with only a few works appearing between the mid 1960s and 1990. Especially in the 1980s, the focus in computational morphology was on the development of finite-state a</context>
</contexts>
<marker>Klenk, 1985</marker>
<rawString>Klenk, Ursula. 1985b. Recognition of Spanish inflectional endings based on the distribution of characters. In Computers in Literary and Linguistic Computing: Proceedings of the Eleventh International Conference [L’ordinateur et les recherches littéraires et linguistiques: actes de la XIe Conférence internationale], pages 246–253, Louvain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ursula Klenk</author>
</authors>
<title>Verfahren der Segmentierung von Wörtern in Morphe: Mit einer Untersuchung zum Spanischen.</title>
<date>1991</date>
<booktitle>In Jürgen Rolshoven und Dieter Seelbach, editor, Romanistische Computerlinguistik: Theorien und Implementationen, volume 266 of Linguistische Arbeiten.</booktitle>
<pages>197--206</pages>
<location>Niemeyer, Tübingen,</location>
<contexts>
<context position="52967" citStr="Klenk 1991" startWordPosition="8309" endWordPosition="8310">se should hold—high LSV/LSE/LSM values could emerge in other places of the word as well. Indeed, any frequent character at the end or beginning of a word may also be expected to show high LSV/LSE/LSM around it, such as the -e at the end of disturbance which has higher values than, for example, -ance. Therefore, simply inferring that high LSV/LSE/LSM values indicate a morpheme border is not a sound principle in general. A different (but less successful, even when supervised) way to use character sequence counts is that associated with Ursula Klenk and various colleagues (Klenk and Langer 1989; Klenk 1991, 1992; Langer 1991; Flenner 1992, 1994, 1995; Janßen 1992). For each character bigram c1c2, they record, with some supervision in the form of manual curation, at what percentage there is a morpheme boundary before |c1c2, between c1|c2, after c1c2|, or none. A new word can then be segmented by sliding a bigram window and taking the split which satisfies the corresponding bigrams the best. For example, given a word singing, if the window happens to be positioned at -gi- in the middle, the bigram splits ng|, g|i, and |in are relevant to deciding whether sing|ing is a good segmentation. Exactly h</context>
</contexts>
<marker>Klenk, 1991</marker>
<rawString>Klenk, Ursula. 1991. Verfahren der Segmentierung von Wörtern in Morphe: Mit einer Untersuchung zum Spanischen. In Jürgen Rolshoven und Dieter Seelbach, editor, Romanistische Computerlinguistik: Theorien und Implementationen, volume 266 of Linguistische Arbeiten. Niemeyer, Tübingen, pages 197–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ursula Klenk</author>
</authors>
<title>Verfahren morphologischer Segmentierung und die Wortstruktur des Spanischen.</title>
<date>1992</date>
<booktitle>Computatio Linguae: Aufsätze zur algorithmischen und quantitativen Analyse der Sprache,</booktitle>
<volume>73</volume>
<pages>110--124</pages>
<editor>In Ursula Klenk, editor,</editor>
<location>Stuttgart,</location>
<contexts>
<context position="35792" citStr="Klenk 1992" startWordPosition="5509" endWordPosition="5510">an 1965; Fedulova 1965; Ožigova 1965; Malahovskij 1965; Hol’m 1965; Kordi 1965; Fitialova 1965; Fihman 1965a; Andreev 1965a; Jakubajtis 1965; Panina 1965; Fihman 1965b; Eliseeva 1965; Jakuševa 1965 Gammon 1969 C T English Segmentation Lehmann 1973, pages 71–93 C T French (I) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1992, 1994, 1995 C T+SP Spanish Segmentation Janßen 1992 C T+SP French Segmentation Juola, Hall, and Boggs 1994 C T English Segmentation Brent 1993, 1999; Brent, Murthy, C T English/Child- Segmentation and Lundberg 1995; Snover 2002; English/Polish/ Snover, Jarosz, and Brent 2002; French Snover and Brent 2001, 2003 Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation 1996 Yvon 1996 C T French (I) Segmentation Kazakov 1997; Kazakov and C T French/English Segmentation Manandhar 1998, 2001 Jacquemin 1997 C T English Segmentation Cromm 199</context>
</contexts>
<marker>Klenk, 1992</marker>
<rawString>Klenk, Ursula. 1992. Verfahren morphologischer Segmentierung und die Wortstruktur des Spanischen. In Ursula Klenk, editor, Computatio Linguae: Aufsätze zur algorithmischen und quantitativen Analyse der Sprache, volume 73 of Zeitschrift für Dialektologie und Linguistik: Beihefte. Franz Steiner, Stuttgart, pages 110–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ursula Klenk</author>
<author>Hagen Langer</author>
</authors>
<title>Morphological segmentation without a lexicon.</title>
<date>1989</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>4--4</pages>
<contexts>
<context position="35673" citStr="Klenk and Langer 1989" startWordPosition="5488" endWordPosition="5491">T English Segmentation Andreev 1965a, Andreev 1967, C T Vietnamese to Segmentation Chapter 2, Peršikov 1965; Hungarian (I) Melkumjan 1965; Fedulova 1965; Ožigova 1965; Malahovskij 1965; Hol’m 1965; Kordi 1965; Fitialova 1965; Fihman 1965a; Andreev 1965a; Jakubajtis 1965; Panina 1965; Fihman 1965b; Eliseeva 1965; Jakuševa 1965 Gammon 1969 C T English Segmentation Lehmann 1973, pages 71–93 C T French (I) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1992, 1994, 1995 C T+SP Spanish Segmentation Janßen 1992 C T+SP French Segmentation Juola, Hall, and Boggs 1994 C T English Segmentation Brent 1993, 1999; Brent, Murthy, C T English/Child- Segmentation and Lundberg 1995; Snover 2002; English/Polish/ Snover, Jarosz, and Brent 2002; French Snover and Brent 2001, 2003 Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation 1996 Yvon 1996 C T French (I) Segmentation Kazakov 1</context>
<context position="52955" citStr="Klenk and Langer 1989" startWordPosition="8305" endWordPosition="8308">o reason why the converse should hold—high LSV/LSE/LSM values could emerge in other places of the word as well. Indeed, any frequent character at the end or beginning of a word may also be expected to show high LSV/LSE/LSM around it, such as the -e at the end of disturbance which has higher values than, for example, -ance. Therefore, simply inferring that high LSV/LSE/LSM values indicate a morpheme border is not a sound principle in general. A different (but less successful, even when supervised) way to use character sequence counts is that associated with Ursula Klenk and various colleagues (Klenk and Langer 1989; Klenk 1991, 1992; Langer 1991; Flenner 1992, 1994, 1995; Janßen 1992). For each character bigram c1c2, they record, with some supervision in the form of manual curation, at what percentage there is a morpheme boundary before |c1c2, between c1|c2, after c1c2|, or none. A new word can then be segmented by sliding a bigram window and taking the split which satisfies the corresponding bigrams the best. For example, given a word singing, if the window happens to be positioned at -gi- in the middle, the bigram splits ng|, g|i, and |in are relevant to deciding whether sing|ing is a good segmentatio</context>
</contexts>
<marker>Klenk, Langer, 1989</marker>
<rawString>Klenk, Ursula and Hagen Langer. 1989. Morphological segmentation without a lexicon. Literary and Linguistic Computing, 4(4):247–253.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Oskar Kohonen</author>
<author>Sami Virpioja</author>
<author>Mikaela Klami</author>
</authors>
<title>Allomorfessor: Towards unsupervised morpheme analysis.</title>
<date>2008</date>
<booktitle>Evaluating Systems for Multilingual and Multimodal Information Access, 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008,</booktitle>
<pages>975--982</pages>
<editor>In Carol Peters, Thomas Deselaers, Nicola Ferro, Julio Gonzalo, Gareth J. F. Jones, Mikko Kurimo, Thomas Mandl, Anselmo Peñas, and Vivien Petras, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Aarthus, Denmark,</location>
<marker>Kohonen, Virpioja, Klami, 2008</marker>
<rawString>Kohonen, Oskar, Sami Virpioja, and Mikaela Klami. 2008. Allomorfessor: Towards unsupervised morpheme analysis. In Carol Peters, Thomas Deselaers, Nicola Ferro, Julio Gonzalo, Gareth J. F. Jones, Mikko Kurimo, Thomas Mandl, Anselmo Peñas, and Vivien Petras, editors, Evaluating Systems for Multilingual and Multimodal Information Access, 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008, Aarthus, Denmark, September 17–19, 2008, Revised Selected Papers, pages 975–982, Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kontorovich</author>
<author>D Don</author>
<author>Y Singer</author>
</authors>
<title>A Markov model for the acquisition of morphological structure.</title>
<date>2003</date>
<tech>Technical report CMU-CS-03-147,</tech>
<institution>School of Computer Science, Carnegie Mellon University,</institution>
<marker>Kontorovich, Don, Singer, 2003</marker>
<rawString>Kontorovich, L., D. Don, and Y. Singer. 2003. A Markov model for the acquisition of morphological structure. Technical report CMU-CS-03-147, School of Computer Science, Carnegie Mellon University, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E E Kordi</author>
</authors>
<title>Ishodnye dannye dlja statistiko-kombinatornogo modelirovanija morfologii sovremennogo francuzckogo jazyka i vydelenie pervogo morfologiˇceskogo tipa.</title>
<date>1965</date>
<booktitle>In Nikolaj Dmitrieviˇc Andreev, editor, Statistiko-kombinatornoe modelirovanie jazykov. Nauka,</booktitle>
<pages>172--180</pages>
<location>Leningrad,</location>
<contexts>
<context position="19469" citStr="Kordi 1965" startWordPosition="2963" endWordPosition="2964">2 different suffixes. The initial suffix candidate, -oj, has a high functional load and consequently a high text frequency: It is the most ambiguous of the Russian adjective suffixes, appearing in four different slots in the adjective paradigm, and is also homonymous with a noun suffix. In Andreev (1965b) the method is tested extensively on Russian, which is the subject of several papers in the volume, and a number of other languages: Albanian (Peršikov 1965), Armenian (Melkumjan 1965), Bulgarian (Fedulova 1965), Czech (Ožigova 1965), English (Malahovskij 1965), Estonian (Hol’m 1965), French (Kordi 1965), German (Fitialova 1965), Hausa (Fihman 1965a), Hungarian (Andreev 1965a), Latvian (Jakubajtis 1965), Serbo-Croatian (Panina 1965), Swahili (Fihman 1965b), Ukrainian (Eliseeva 1965), and Vietnamese (Jakuševa 1965). As an aside, we may note that only after the turn of the millennium are we again seeing this variety of languages in ULM work. Most of these studies are small-scale proof-of-concept experiments on corpora of varying sizes (from a few thousand words in many of the studies up to close to one million words for Russian). The outcomes are more often than not quite small “paradigmoid fra</context>
<context position="35260" citStr="Kordi 1965" startWordPosition="5425" endWordPosition="5426">ods, the phonemes (approximated by graphemes) are first classed into categories, foremostly, vowel versus consonant. Thereafter, each word is separated into its vowel skeleton and its consonant skeleton, after which various 318 Hammarström and Borin Unsupervised Learning of Morphology Table 2 Very brief roadmap of earlier studies. Model Superv. Experimentation Learns what? Harris 1955, 1968, 1970 C T English Segmentation Andreev 1965a, Andreev 1967, C T Vietnamese to Segmentation Chapter 2, Peršikov 1965; Hungarian (I) Melkumjan 1965; Fedulova 1965; Ožigova 1965; Malahovskij 1965; Hol’m 1965; Kordi 1965; Fitialova 1965; Fihman 1965a; Andreev 1965a; Jakubajtis 1965; Panina 1965; Fihman 1965b; Eliseeva 1965; Jakuševa 1965 Gammon 1969 C T English Segmentation Lehmann 1973, pages 71–93 C T French (I) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1992, 1994, 1995 C T+SP S</context>
</contexts>
<marker>Kordi, 1965</marker>
<rawString>Kordi, E. E. 1965. Ishodnye dannye dlja statistiko-kombinatornogo modelirovanija morfologii sovremennogo francuzckogo jazyka i vydelenie pervogo morfologiˇceskogo tipa. In Nikolaj Dmitrieviˇc Andreev, editor, Statistiko-kombinatornoe modelirovanie jazykov. Nauka, Leningrad, pages 172–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Krauss</author>
</authors>
<title>The world’s languages in crisis.</title>
<date>1992</date>
<journal>Language,</journal>
<volume>68</volume>
<issue>1</issue>
<contexts>
<context position="29593" citStr="Krauss 1992" startWordPosition="4571" endWordPosition="4572">exts will always contain a fair share of (inflected) previously unknown words that are not in the lexicon. There has to be strategy for such out-of-dictionary words—a ULM-solving algorithm is one possibility. The ULM problem as specified, therefore, still has a role to play for larger languages. Finally, and closely related to the preceding reason, ULM and other kinds of machine learning of linguistic information are increasingly seen as providing potential tools in language documentation.9 It has been realized for some time that languages are disappearing at a rapid rate in the modern world (Krauss 1992, 2007). Many linguists see this loss of linguistic diversity as a disaster in the cultural and intellectual sphere on a par with the loss of the world’s biodiversity in the ecological sphere, only on a grander scale; languages are going extinct more rapidly than species. Enter language documentation (Gippert, Himmelmann, and Mosel 2006), which is construed as going well beyond traditional descriptive linguistic fieldwork, aspiring as it does to capture all aspects—linguistic, cultural, and social—of a language community’s day-to-day life, in video and audio recordings of a wide range of socio</context>
</contexts>
<marker>Krauss, 1992</marker>
<rawString>Krauss, Michael. 1992. The world’s languages in crisis. Language, 68(1):4–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Krauss</author>
</authors>
<title>Mass language extinction and documentation: The race against time.</title>
<date>2007</date>
<booktitle>Vanishing Languages of the Pacific Rim.</booktitle>
<pages>3--24</pages>
<editor>In O. Miyaoka, O. Sakiyama, and M. Krauss, editors,</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford,</location>
<marker>Krauss, 2007</marker>
<rawString>Krauss, Michael E. 2007. Mass language extinction and documentation: The race against time. In O. Miyaoka, O. Sakiyama, and M. Krauss, editors, Vanishing Languages of the Pacific Rim. Oxford University Press, Oxford, pages 3–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Mathias Creutz</author>
<author>Ville Turunen</author>
</authors>
<title>Overview of Morpho Challenge in CLEF</title>
<date>2007</date>
<booktitle>In Working Notes for the CLEF 2007 Workshop,</booktitle>
<location>Budapest.</location>
<marker>Kurimo, Creutz, Turunen, 2007</marker>
<rawString>Kurimo, Mikko, Mathias Creutz, and Ville Turunen. 2007. Overview of Morpho Challenge in CLEF 2007. In Working Notes for the CLEF 2007 Workshop, Budapest.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mikko Kurimo</author>
<author>Mathias Creutz</author>
<author>Matti Varjokallio</author>
</authors>
<title>Morpho challenge evaluation by information retrieval experiments.</title>
<date>2008</date>
<booktitle>Advances in Multilingual and Multimodal Information Retrieval, 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007,</booktitle>
<pages>991--998</pages>
<editor>In Carol Peters, Valentin Jijkoun, Thomas Mandl, Henning Müller, Douglas W. Oard, and Anselmo Peñas, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Budapest, Hungary,</location>
<marker>Kurimo, Creutz, Varjokallio, 2008</marker>
<rawString>Kurimo, Mikko, Mathias Creutz, and Matti Varjokallio. 2008a. Morpho challenge evaluation by information retrieval experiments. In Carol Peters, Valentin Jijkoun, Thomas Mandl, Henning Müller, Douglas W. Oard, and Anselmo Peñas, editors, Advances in Multilingual and Multimodal Information Retrieval, 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007, Budapest, Hungary, September 19–21, 2007, Revised Selected Papers, pages 991–998. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mikko Kurimo</author>
<author>Mathias Creutz</author>
<author>Matti Varjokallio</author>
</authors>
<title>Morpho challenge evaluation using a linguistic gold standard.</title>
<date>2008</date>
<booktitle>Advances in Multilingual and Multimodal Information Retrieval, 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007,</booktitle>
<pages>864--872</pages>
<editor>In Carol Peters, Valentin Jijkoun, Thomas Mandl, Henning Müller, Douglas W. Oard, and Anselmo Peñas, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Budapest, Hungary,</location>
<marker>Kurimo, Creutz, Varjokallio, 2008</marker>
<rawString>Kurimo, Mikko, Mathias Creutz, and Matti Varjokallio. 2008b. Morpho challenge evaluation using a linguistic gold standard. In Carol Peters, Valentin Jijkoun, Thomas Mandl, Henning Müller, Douglas W. Oard, and Anselmo Peñas, editors, Advances in Multilingual and Multimodal Information Retrieval, 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007, Budapest, Hungary, September 19–21, 2007, Revised Selected Papers, pages 864–872. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
</authors>
<title>Antti Puurula, Ebru Arisoy, Vesi Siivola, Teemu Hirsimäki, Janne Pylkkönen, Tanel Alumäe, and Murat Saraçlar.</title>
<date>2006</date>
<booktitle>In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>487--494</pages>
<location>New York.</location>
<marker>Kurimo, 2006</marker>
<rawString>Kurimo, Mikko, Antti Puurula, Ebru Arisoy, Vesi Siivola, Teemu Hirsimäki, Janne Pylkkönen, Tanel Alumäe, and Murat Saraçlar. 2006. Unlimited vocabulary speech recognition for agglutinative languages. In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 487–494, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Ville Turunen</author>
</authors>
<title>Unsupervised morpheme analysis evaluation by IR experiments—Morpho Challenge</title>
<date>2008</date>
<booktitle>In Working Notes for the CLEF</booktitle>
<location>Workshop, Aarhus.</location>
<contexts>
<context position="12667" citStr="Kurimo and Turunen (2008)" startWordPosition="1930" endWordPosition="1933">is little modularization taking place. Previous surveys and overviews for a general audience are Borin (1991), Batchelder (1997, pages 66–68), Powers (1998), Clark (2001, pages 80–82), Xanthos (2007, pages 95–107), Goldsmith (2001), Daelemans (2004, page 1898), Roark and Sproat (2007, pages 116–136), Hammarström (2007b, pages 10–15), Chan (2008, pages 48–60), Hammarström (2009b, pages 14–21), Borin (2009), Goldsmith (2010), and, to a more limited extent, the related-work sections of individual research papers. Kurimo, Creutz, and Turunen (2007), Kurimo, Creutz, and Varjokallio (2008a, 2008b), Kurimo and Turunen (2008), Kurimo and Varjokallio (2008), and McNamee (2008) are overviews of systems in the MorphoChallenge of the respective year. However, we will try to be more comprehensive than previous surveys and discuss the ideas in the field critically. 312 Hammarström and Borin Unsupervised Learning of Morphology We will not attempt a comparison in terms of accuracy figures as this is wholly impossible, not only because of the great variation in goals but also because most descriptions do not specify their algorithm(s) in enough detail. Fortunately, this aspect is better handled in controlled competitions, </context>
</contexts>
<marker>Kurimo, Turunen, 2008</marker>
<rawString>Kurimo, Mikko and Ville Turunen. 2008. Unsupervised morpheme analysis evaluation by IR experiments—Morpho Challenge 2008. In Working Notes for the CLEF 2008 Workshop, Aarhus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Matti Varjokallio</author>
</authors>
<title>Unsupervised morpheme analysis evaluation by a comparison to a linguistic gold standard—Morpho Challenge</title>
<date>2008</date>
<booktitle>In Working Notes for the CLEF</booktitle>
<location>Workshop, Aarhus.</location>
<contexts>
<context position="12698" citStr="Kurimo and Varjokallio (2008)" startWordPosition="1934" endWordPosition="1937">king place. Previous surveys and overviews for a general audience are Borin (1991), Batchelder (1997, pages 66–68), Powers (1998), Clark (2001, pages 80–82), Xanthos (2007, pages 95–107), Goldsmith (2001), Daelemans (2004, page 1898), Roark and Sproat (2007, pages 116–136), Hammarström (2007b, pages 10–15), Chan (2008, pages 48–60), Hammarström (2009b, pages 14–21), Borin (2009), Goldsmith (2010), and, to a more limited extent, the related-work sections of individual research papers. Kurimo, Creutz, and Turunen (2007), Kurimo, Creutz, and Varjokallio (2008a, 2008b), Kurimo and Turunen (2008), Kurimo and Varjokallio (2008), and McNamee (2008) are overviews of systems in the MorphoChallenge of the respective year. However, we will try to be more comprehensive than previous surveys and discuss the ideas in the field critically. 312 Hammarström and Borin Unsupervised Learning of Morphology We will not attempt a comparison in terms of accuracy figures as this is wholly impossible, not only because of the great variation in goals but also because most descriptions do not specify their algorithm(s) in enough detail. Fortunately, this aspect is better handled in controlled competitions, such as the Unsupervised Morphe</context>
</contexts>
<marker>Kurimo, Varjokallio, 2008</marker>
<rawString>Kurimo, Mikko and Matti Varjokallio. 2008. Unsupervised morpheme analysis evaluation by a comparison to a linguistic gold standard—Morpho Challenge 2008. In Working Notes for the CLEF 2008 Workshop, Aarhus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen Langer</author>
</authors>
<title>Ein automatisches Morphsegmentierungsverfahren für deutsche Wortformen.</title>
<date>1991</date>
<tech>Ph.D. thesis,</tech>
<institution>Georg-August-Universität zu Göttingen.</institution>
<contexts>
<context position="35712" citStr="Langer 1991" startWordPosition="5496" endWordPosition="5497">7, C T Vietnamese to Segmentation Chapter 2, Peršikov 1965; Hungarian (I) Melkumjan 1965; Fedulova 1965; Ožigova 1965; Malahovskij 1965; Hol’m 1965; Kordi 1965; Fitialova 1965; Fihman 1965a; Andreev 1965a; Jakubajtis 1965; Panina 1965; Fihman 1965b; Eliseeva 1965; Jakuševa 1965 Gammon 1969 C T English Segmentation Lehmann 1973, pages 71–93 C T French (I) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1992, 1994, 1995 C T+SP Spanish Segmentation Janßen 1992 C T+SP French Segmentation Juola, Hall, and Boggs 1994 C T English Segmentation Brent 1993, 1999; Brent, Murthy, C T English/Child- Segmentation and Lundberg 1995; Snover 2002; English/Polish/ Snover, Jarosz, and Brent 2002; French Snover and Brent 2001, 2003 Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation 1996 Yvon 1996 C T French (I) Segmentation Kazakov 1997; Kazakov and C T French/English Seg</context>
<context position="52986" citStr="Langer 1991" startWordPosition="8312" endWordPosition="8313">h LSV/LSE/LSM values could emerge in other places of the word as well. Indeed, any frequent character at the end or beginning of a word may also be expected to show high LSV/LSE/LSM around it, such as the -e at the end of disturbance which has higher values than, for example, -ance. Therefore, simply inferring that high LSV/LSE/LSM values indicate a morpheme border is not a sound principle in general. A different (but less successful, even when supervised) way to use character sequence counts is that associated with Ursula Klenk and various colleagues (Klenk and Langer 1989; Klenk 1991, 1992; Langer 1991; Flenner 1992, 1994, 1995; Janßen 1992). For each character bigram c1c2, they record, with some supervision in the form of manual curation, at what percentage there is a morpheme boundary before |c1c2, between c1|c2, after c1c2|, or none. A new word can then be segmented by sliding a bigram window and taking the split which satisfies the corresponding bigrams the best. For example, given a word singing, if the window happens to be positioned at -gi- in the middle, the bigram splits ng|, g|i, and |in are relevant to deciding whether sing|ing is a good segmentation. Exactly how to do the split </context>
</contexts>
<marker>Langer, 1991</marker>
<rawString>Langer, Hagen. 1991. Ein automatisches Morphsegmentierungsverfahren für deutsche Wortformen. Ph.D. thesis, Georg-August-Universität zu Göttingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hubert Lehmann</author>
</authors>
<title>Linguistische Modellbildung und Methodologie.</title>
<date>1973</date>
<publisher>Max Niemeyer Verlag,</publisher>
<location>Tübingen.</location>
<contexts>
<context position="35429" citStr="Lehmann 1973" startWordPosition="5449" endWordPosition="5450">skeleton and its consonant skeleton, after which various 318 Hammarström and Borin Unsupervised Learning of Morphology Table 2 Very brief roadmap of earlier studies. Model Superv. Experimentation Learns what? Harris 1955, 1968, 1970 C T English Segmentation Andreev 1965a, Andreev 1967, C T Vietnamese to Segmentation Chapter 2, Peršikov 1965; Hungarian (I) Melkumjan 1965; Fedulova 1965; Ožigova 1965; Malahovskij 1965; Hol’m 1965; Kordi 1965; Fitialova 1965; Fihman 1965a; Andreev 1965a; Jakubajtis 1965; Panina 1965; Fihman 1965b; Eliseeva 1965; Jakuševa 1965 Gammon 1969 C T English Segmentation Lehmann 1973, pages 71–93 C T French (I) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1992, 1994, 1995 C T+SP Spanish Segmentation Janßen 1992 C T+SP French Segmentation Juola, Hall, and Boggs 1994 C T English Segmentation Brent 1993, 1999; Brent, Murthy, C T English/Child- Segme</context>
</contexts>
<marker>Lehmann, 1973</marker>
<rawString>Lehmann, Hubert. 1973. Linguistische Modellbildung und Methodologie. Max Niemeyer Verlag, Tübingen.</rawString>
</citation>
<citation valid="true">
<date>2009</date>
<booktitle>Ethnologue: Languages of the World. Available at www.ethnologue.com/.</booktitle>
<editor>Lewis, M. Paul, editor.</editor>
<contexts>
<context position="12450" citStr="(2009)" startWordPosition="1903" endWordPosition="1903">ut having to commit to what the actual stem should be. Many recent articles fail to deal properly with previous and related work, some reinvent heuristics that have been proposed earlier, and there is little modularization taking place. Previous surveys and overviews for a general audience are Borin (1991), Batchelder (1997, pages 66–68), Powers (1998), Clark (2001, pages 80–82), Xanthos (2007, pages 95–107), Goldsmith (2001), Daelemans (2004, page 1898), Roark and Sproat (2007, pages 116–136), Hammarström (2007b, pages 10–15), Chan (2008, pages 48–60), Hammarström (2009b, pages 14–21), Borin (2009), Goldsmith (2010), and, to a more limited extent, the related-work sections of individual research papers. Kurimo, Creutz, and Turunen (2007), Kurimo, Creutz, and Varjokallio (2008a, 2008b), Kurimo and Turunen (2008), Kurimo and Varjokallio (2008), and McNamee (2008) are overviews of systems in the MorphoChallenge of the respective year. However, we will try to be more comprehensive than previous surveys and discuss the ideas in the field critically. 312 Hammarström and Borin Unsupervised Learning of Morphology We will not attempt a comparison in terms of accuracy figures as this is wholly im</context>
<context position="62490" citStr="(2009)" startWordPosition="9867" endWordPosition="9867">is likely not to be normally distributed. Most lexemes will occur in only one form, and only very few, if any, lexemes will occur in all |P| forms. It appears that for most languages and most paradigms, the number of lexemes that occur in i forms tends to decrease logarithmically in i (Chan 2008, pages 75–84). As an example, consider the three most common paradigms in Swedish and the frequency of forms in Table 8. Works which have attempted nevertheless to tackle the matter of paradigms, at least for languages with one-slot morphology, include Zeman 2008, 2009, Hammarström (2009b), and Monson (2009). They explicitly or implicitly make use of the following two heuristics to narrow down the search space: • Languages tend to have a small number of paradigms (where “small” means fewer than 100 paradigms with at least 100 member stems each). • Languages tend to have only small paradigms (where “small” means fewer than 50), that is, the number of affixes in each paradigm is small. Agglutinative languages, which have several layers of affixes, can be said to obey this generalization in the sense that each layer has few members, whereas conversely, the full paradigm achieves considerable size co</context>
<context position="67286" citStr="(2009)" startWordPosition="10634" endWordPosition="10634">c.) that operates over a maximal number of groups. The search space is huge, considering not only the group space but also the large number of potential morphological processes itself. The group-and-abstract approaches are also characterized by the ubiquitous use of ad hoc thresholds. However, there are clear advantages in that they are in principle capable of handling non-concatenative morphology and in that issues of semantics (of stems) are addressed from the beginning. The work by de Kock and Bossaert (1969, 1974, 1978), Yvon (1996), Medina Urrea (2003) and partly Moon, Erk, and Baldridge (2009) can favorably be seen as a mid-way between the border-and-frequency and group-and-abstract approaches as they rely on 15 That is, the past tense of the verb singe. 329 Computational Linguistics Volume 37, Number 2 Table 9 Example feature values for the words ngithii(I went) and t˜ugithii(we went) adapted from De Pauw and Wagacha (2007, page 1518). B=-features describe a subset at the start of the word form, E=-features indicate patterns at the end of the word, and I=-features describe patterns inside the word form. class features ng˜ıthi˜ı B=n B=ng B=ng˜ı B=ng˜ıt B=ng˜ıth B=ng˜ıthi I=g I=g˜ı </context>
<context position="72213" citStr="(2009)" startWordPosition="11393" endWordPosition="11393">es and ´Cavar 2005, 2007; Xanthos 2007) and Amharic (Bati 2002), this is largely true, or a transcription is used where it is largely true. Rodrigues and ´Cavar (2005, 2007) and Bati (2002) hard-code the transition from the graphemic representation of a word to its (potential) root and pattern parts. This can be said to constitute a strong language specific bias, tantamount to supervision. Xanthos (2007), on the other hand, starts out only by assuming that there exists a distinction between root and pattern graphemes and subsequently learns which graphemes are which. See Goldsmith and Xanthos (2009) for an excellent survey on how to do this (something which falls under learning phonological categories rather than morphology learning). Basically, it is possible only because there are systematic combination constraints between different phonemes (approximated by graphemes); for example, vowels and consonants alternate in a very non-random manner. Once each word is divided into its potential root and pattern, the morphology learning problem is similar to morphology learning given roots and suffixes, that is, the typical model for learning concatenative morphology, where the task is to weed </context>
<context position="75513" citStr="(2009)" startWordPosition="11972" endWordPosition="11972">e following expression: 11 arg max p(xi) · p,(yi) [s1,...,s|W|] wi∈W For example, if W = {ad, ae, bd, be, cd, ce, ggg}, then the configuration of splits a|d, a|e, b|d, b|e, c|d, c|e, g|gg yields the product (2 · 3)6 · (1 · 1). Brent (1999) devises a precise, but more elaborate, way of constructing W from B and S, but at the cost of a large search space, and whose global maximum is hard to characterize intuitively. The same holds for the extension by Snover (2002). Kontorovich, Don, and Singer (2003), Snyder and Barzilay (2008), Goldwater (2007), Johnson (2008), and Poon, Cherry, and Toutanova (2009) should also be noted for containing generative models. Most approaches, of any of the kinds (a)–(d) described in Section 3.1, explicitly or implicitly target languages which have (close to) one-slot morphology, that is, a word (or stem) typically takes not more than one prefix and not more than one suffix. Many (indeed most; Dryer 2005) languages deviate more or less from this model. At first, it may seem that multi-slot morphology can be handled by the same algorithms as one-slot morphology, by iterating the process used for one-slot morphology. A decade of ULM has shown that the matter is n</context>
</contexts>
<marker>2009</marker>
<rawString>Lewis, M. Paul, editor. 2009. Ethnologue: Languages of the World. Available at www.ethnologue.com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krister Lindén</author>
</authors>
<title>A probabilistic model for guessing base forms of new words by analogy.</title>
<date>2008</date>
<booktitle>In Proceedings of CICLing-2008: 9th International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>106--116</pages>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="9170" citStr="Lindén 2008" startWordPosition="1364" endWordPosition="1365">urvey, unless the required data (e.g., paradigm members) are extracted from raw text in an unsupervised manner as well. We also exclude the special case of the second approach where morphology learning means not “learning the morphological system of a language,” but rather “learning the inflectional classes of out-of-vocabulary words,” namely, approaches where an existing morphological analysis component is used as the basis for guessing in which existing paradigm an unknown text word should belong (e.g., Antworth 1990; Mikheev 1997; Bharati et al. 2001; Forsberg, Hammarström, and Ranta 2006; Lindén 2008; Lindén 2009). One of the matters that varies the most between different authors is the desired outcome. It is useful to set up the implicational hierarchy shown in Table 1 (which 4 There are, however, rare cases of languages which allow the permutation of specific pairs of prefixes, such as Kagulu (Petzell 2007), Yimas and Karawari (Foley 1991, pages 31–32) as well as Chintang, Bantawa, and possibly other Kiranti languages where prefix ordering in general is very free (Rai 1984; Bickel et al. 2007). 5 Clitics are affix-like elements that attach to words in particular syntactic positions, rat</context>
</contexts>
<marker>Lindén, 2008</marker>
<rawString>Lindén, Krister. 2008. A probabilistic model for guessing base forms of new words by analogy. In Proceedings of CICLing-2008: 9th International Conference on Intelligent Text Processing and Computational Linguistics, pages 106–116, Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krister Lindén</author>
</authors>
<title>Entry generation by analogy—encoding new words for morphological lexicons.</title>
<date>2009</date>
<journal>Northern European Journal of Language Technology,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="9184" citStr="Lindén 2009" startWordPosition="1366" endWordPosition="1367"> the required data (e.g., paradigm members) are extracted from raw text in an unsupervised manner as well. We also exclude the special case of the second approach where morphology learning means not “learning the morphological system of a language,” but rather “learning the inflectional classes of out-of-vocabulary words,” namely, approaches where an existing morphological analysis component is used as the basis for guessing in which existing paradigm an unknown text word should belong (e.g., Antworth 1990; Mikheev 1997; Bharati et al. 2001; Forsberg, Hammarström, and Ranta 2006; Lindén 2008; Lindén 2009). One of the matters that varies the most between different authors is the desired outcome. It is useful to set up the implicational hierarchy shown in Table 1 (which 4 There are, however, rare cases of languages which allow the permutation of specific pairs of prefixes, such as Kagulu (Petzell 2007), Yimas and Karawari (Foley 1991, pages 31–32) as well as Chintang, Bantawa, and possibly other Kiranti languages where prefix ordering in general is very free (Rai 1984; Bickel et al. 2007). 5 Clitics are affix-like elements that attach to words in particular syntactic positions, rather than to wo</context>
</contexts>
<marker>Lindén, 2009</marker>
<rawString>Lindén, Krister. 2009. Entry generation by analogy—encoding new words for morphological lexicons. Northern European Journal of Language Technology, 1(1):1–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Longacre</author>
</authors>
<title>Grammar Discovery Procedures.</title>
<date>1964</date>
<publisher>Mouton, The Hague.</publisher>
<contexts>
<context position="79684" citStr="Longacre 1964" startWordPosition="12644" endWordPosition="12645">lytical experience, and what he has learned from the linguistic research of other people. With this knowledge of the language to be analyzed and with this background knowledge, he makes certain guesses about the grammatical structure of the language. He then submits these guesses to a series of systematic checks in which he confirms, disproves, or modifies his original guesses—and makes a few better guesses en route. This systematic evaluation is based on a theory of the structure of language, and the theory itself (while containing elements of creative thinking) is based on empirical study. (Longacre 1964, page 12) Mutatis mutandis, the procedure described in this quote, contains most elements of ULM and related methods proposed in the literature. Note that the quote just given stresses the importance of the knowledge that the linguist brings to the analysis and which informs the whole analytical process. This suggests that there may be a level of general knowledge about language (in general or a useful subset of languages), or about linguistic analysis, or both, which would be useful to ULM in general, something like the “knowledge” that white space is a word delimiter in written text, but on</context>
</contexts>
<marker>Longacre, 1964</marker>
<rawString>Longacre, Robert E. 1964. Grammar Discovery Procedures. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cerstin Mahlow</author>
<author>Michael Piotrowski</author>
</authors>
<date>2009</date>
<booktitle>Preface. In State of the Art in Computational Morphology: Proceedings of the Workshop on Systems and Frameworks for Computational Morphology, SFCM</booktitle>
<pages>pages v–viii,</pages>
<location>Zurich.</location>
<contexts>
<context position="85173" citStr="Mahlow and Piotrowski 2009" startWordPosition="13504" endWordPosition="13507">gy Most ULM approaches reported in the literature are small proof-of-concept experiments, which generally founder on the lack of evaluation data. The MorphoChallenge18 series does provide adequate gold-standard evaluation data for Finnish, English, German, Arabic, and Turkish as well as task-based Information Retrieval (IR) evaluation data for English, German, and Finnish. It can be seen that ULM systems are mature enough to enhance IR, but so far, ULM systems are not close to full accuracy on the gold standard and outside commentators have generally been unimpressed with these results (e.g., Mahlow and Piotrowski 2009, page vi). However, many (most?) of the strong-looking systems reported in the literature have not, for one reason or another, taken part in the MorphoChallenge. Taking MorphoChallenge results and proof-ofconcept reports together, it seems that high accuracy by ULM systems is presently only achievable if the language has small amounts of one-slot concatenative morphology, whereas for morphologically more complex languages, parameter tuning and/or lower accuracy is to be expected. We are not yet in a position to assess whether there are other tasks than IR which, in general, benefit significan</context>
</contexts>
<marker>Mahlow, Piotrowski, 2009</marker>
<rawString>Mahlow, Cerstin and Michael Piotrowski. 2009. Preface. In State of the Art in Computational Morphology: Proceedings of the Workshop on Systems and Frameworks for Computational Morphology, SFCM 2009, pages v–viii, Zurich.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prasenjit Majumder</author>
<author>Mandar Mitra</author>
<author>Dipasree Pal</author>
</authors>
<title>Bulgarian, Hungarian and Czech stemming using YASS.</title>
<date>2008</date>
<booktitle>In Advances in Multilingual and Multimodal Information Retrieval: 8th Workshop of the Cross-Language Evaluation Forum, CLEF</booktitle>
<pages>49--56</pages>
<location>Budapest.</location>
<marker>Majumder, Mitra, Pal, 2008</marker>
<rawString>Majumder, Prasenjit, Mandar Mitra, and Dipasree Pal. 2008. Bulgarian, Hungarian and Czech stemming using YASS. In Advances in Multilingual and Multimodal Information Retrieval: 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007, pages 49–56, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prasenjit Majumder</author>
<author>Mandar Mitra</author>
<author>Swapan K Parui</author>
<author>Gobinda Kole</author>
<author>Pabitra Mitra</author>
<author>Kalyankumar Datta</author>
</authors>
<title>YASS: Yet another suffix stripper.</title>
<date>2007</date>
<contexts>
<context position="40035" citStr="Majumder et al. 2007" startWordPosition="6140" endWordPosition="6143">on C T English/Child-English Segmentation 2005; Goldwater 2007; Naradowsky and Goldwater 2009 Freitag 2005 C T English Segmentation Golcher 2006 C - English/German Lexicon+ Paradigms Arabsorkhi and Shamsfard 2006 C T Persian Segmentation Chan 2006, Chan 2008, C T English Paradigms pages 101–139 Demberg 2007 C/NC T English/German/ Segmentation Finnish/Turkish Dasgupta and Ng 2006, 2007a; C T Bengali Segmentation 2007b; Dasgupta 2007 De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al. 2007; C T French/Bengali/French/ Analysis Majumder, Mitra, and Pal Bulgar2008 ian/Hungarian Zeman 2008, 2009 C - Czech/English/German/ Segmentation+ Finnish Paradigms Kohonen, Virpioja, and Klami C T Finnish/Turkish/English Segmentation 2008 320 Hammarström and Borin Unsupervised Learning of Morphology Table 2 (continued) Model Superv. Experimentation Learns what? Goodman 2008 C T Finnish/Turkish/English Segmentation Golénia 2008 C T Turkish/Russian Segmentation Pandey and Siddiqui 2008 C T Hindi Segmentation+ Paradigms Johnson 2008 C T Sesotho Segmentation Snyder and Barzilay 2008 C/NC T Hebrew/A</context>
<context position="64939" citStr="Majumder et al. 2007" startWordPosition="10273" endWordPosition="10276">ther, resemble the full matrix. There is only one humanly tuned threshold, namely, when to stop breaking into smaller parts. 3.3 Group and Abstract In contrast to the methods that use a heuristic for finding morpheme boundaries, the grouping methods are much less sensitive to continuous segments. String edit distance is the most straightforward metric for which to find pairs or sets of morphologically related words (see, e.g., Gaussier 1999; Yarowsky and Wicentowski 2000; Schone and Jurafsky 2001a; Baroni, Matiasek, and Trost 2002; Hu et al. 2005a; Bernhard 2006, pages 101–117; Bernhard 2007; Majumder et al. 2007; Majumder, Mitra, and Pal 2008). In addition, as unsupervised methods for semantic clustering (e.g., Latent Semantic Analysis) and distributional clustering became more mature, these could be included as well (Schone and Jurafsky 2000, 2001a; Schone 2001; Baroni, Matiasek, and Trost 2002; Freitag 2005). More remarkable, however, is that Yarowsky and Wicentowski (2000) and Wicentowski (2002, 2004) have shown that frequency signatures can also be used to (heuristically) find morphologically related words. The example they use is sang versus sing, whose relative frequency distribution in a corpu</context>
</contexts>
<marker>Majumder, Mitra, Parui, Kole, Mitra, Datta, 2007</marker>
<rawString>Majumder, Prasenjit, Mandar Mitra, Swapan K. Parui, Gobinda Kole, Pabitra Mitra, and Kalyankumar Datta. 2007. YASS: Yet another suffix stripper.</rawString>
</citation>
<citation valid="false">
<journal>ACM Transactions on Information Systems,</journal>
<volume>25</volume>
<issue>4</issue>
<marker></marker>
<rawString>ACM Transactions on Information Systems, 25(4):18:1–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L V Malahovskij</author>
</authors>
<title>Naˇcal’nyj ˙etap statistiko-kombinatornogo modelirovanija morfologii anglijskogo jazyka.</title>
<date>1965</date>
<booktitle>In Nikolaj Dmitrieviˇc Andreev, editor, Statistiko-kombinatornoe modelirovanie jazykov. Nauka,</booktitle>
<pages>137--149</pages>
<location>Leningrad,</location>
<contexts>
<context position="19425" citStr="Malahovskij 1965" startWordPosition="2957" endWordPosition="2958"> a complete adjective paradigm was induced, with 12 different suffixes. The initial suffix candidate, -oj, has a high functional load and consequently a high text frequency: It is the most ambiguous of the Russian adjective suffixes, appearing in four different slots in the adjective paradigm, and is also homonymous with a noun suffix. In Andreev (1965b) the method is tested extensively on Russian, which is the subject of several papers in the volume, and a number of other languages: Albanian (Peršikov 1965), Armenian (Melkumjan 1965), Bulgarian (Fedulova 1965), Czech (Ožigova 1965), English (Malahovskij 1965), Estonian (Hol’m 1965), French (Kordi 1965), German (Fitialova 1965), Hausa (Fihman 1965a), Hungarian (Andreev 1965a), Latvian (Jakubajtis 1965), Serbo-Croatian (Panina 1965), Swahili (Fihman 1965b), Ukrainian (Eliseeva 1965), and Vietnamese (Jakuševa 1965). As an aside, we may note that only after the turn of the millennium are we again seeing this variety of languages in ULM work. Most of these studies are small-scale proof-of-concept experiments on corpora of varying sizes (from a few thousand words in many of the studies up to close to one million words for Russian). The outcomes are more</context>
<context position="35236" citStr="Malahovskij 1965" startWordPosition="5421" endWordPosition="5422">ration: In this family of methods, the phonemes (approximated by graphemes) are first classed into categories, foremostly, vowel versus consonant. Thereafter, each word is separated into its vowel skeleton and its consonant skeleton, after which various 318 Hammarström and Borin Unsupervised Learning of Morphology Table 2 Very brief roadmap of earlier studies. Model Superv. Experimentation Learns what? Harris 1955, 1968, 1970 C T English Segmentation Andreev 1965a, Andreev 1967, C T Vietnamese to Segmentation Chapter 2, Peršikov 1965; Hungarian (I) Melkumjan 1965; Fedulova 1965; Ožigova 1965; Malahovskij 1965; Hol’m 1965; Kordi 1965; Fitialova 1965; Fihman 1965a; Andreev 1965a; Jakubajtis 1965; Panina 1965; Fihman 1965b; Eliseeva 1965; Jakuševa 1965 Gammon 1969 C T English Segmentation Lehmann 1973, pages 71–93 C T French (I) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1</context>
</contexts>
<marker>Malahovskij, 1965</marker>
<rawString>Malahovskij, L. V. 1965. Naˇcal’nyj ˙etap statistiko-kombinatornogo modelirovanija morfologii anglijskogo jazyka. In Nikolaj Dmitrieviˇc Andreev, editor, Statistiko-kombinatornoe modelirovanie jazykov. Nauka, Leningrad, pages 137–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Maxwell</author>
<author>Anne David</author>
</authors>
<title>Joint grammar development by linguists and computer scientists.</title>
<date>2008</date>
<booktitle>In Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages,</booktitle>
<pages>27--34</pages>
<location>Hyderabad.</location>
<contexts>
<context position="83407" citStr="Maxwell and David 2008" startWordPosition="13238" endWordPosition="13241">based disambiguation components are developed for a number of related languages. The fact that the languages are related is of great help when dealing with successive languages after the first one. The morphological component for the first language, North Sámi, required approximately 2.5 person-years of highly qualified linguistic expert work to reach the prototype stage, whereas the analogous module for the closely related Lule Sámi was completed in an additional six months (Trosterud 2006).17 This and other work in the same vein reported in the literature (e.g., by Artola-Zubillaga 2004 and Maxwell and David 2008) is characterized by deep and long-lasting involvement by linguistic expertise and further often by the creative use of digitized versions of conventional printed linguistic resources, especially dictionaries. The following observation is perhaps trivial, but bears stressing, because it is in fact often not heeded in practice: For this kind of approach to work, it is necessary that tools for providing systems with linguistic knowledge use a conceptual apparatus and notation familiar to the linguists who are supposed to be working with them. Relevant to our purposes here, the same holds for any</context>
</contexts>
<marker>Maxwell, David, 2008</marker>
<rawString>Maxwell, Michael and Anne David. 2008. Joint grammar development by linguists and computer scientists. In Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 27–34, Hyderabad.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Mayfield</author>
<author>Paul McNamee</author>
</authors>
<title>Single n-gram stemming.</title>
<date>2003</date>
<booktitle>In SIGIR ’03: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval,</booktitle>
<pages>415--416</pages>
<location>New York, NY.</location>
<contexts>
<context position="33923" citStr="Mayfield and McNamee (2003)" startWordPosition="5223" endWordPosition="5226">rds are first grouped (clustered into sets, paired, shortlisted, etc.) according to some metric, which is typically string edit distance, but may include semantic features (Schone 2001), distributional similarity (Freitag 2005), or frequency signatures (Wicentowski 2002). The next step is to abstract some morphological pattern that recurs among the groups. Such emergent patterns provide enough clues for segmentation and can sometimes be formulated as rules or morphological paradigms. (c) Features and Classes: In this family of methods, a word is seen as made up of a set of features—n-grams in Mayfield and McNamee (2003) and McNamee and Mayfield (2007), and initial/terminal/mid-substring in De Pauw and Wagacha (2007). Features which occur on many words have little selective power across the words, whereas features which occur seldom pinpoint a specific word or stem. To formalize this intuition, Mayfield and McNamee and McNamee and Mayfield use TF-IDF, and De Pauw and Wagacha use entropy. Classifying an unseen word reduces to using its features to select which word(s) it may be morphologically related to. This decides whether the unseen word is a morphological variant of some other word, and allows extracting </context>
<context position="37655" citStr="Mayfield and McNamee 2003" startWordPosition="5793" endWordPosition="5796">igms Sharma and Das 2002 Baroni, Matiasek, and Trost 2002 C/NC T English/German (I) Related word pairs Bati 2002 C/NC T Amharic Lexicon+ Paradigms Creutz 2003, 2006; Creutz and C T Finnish/Turkish/ Segmentation Lagus 2002, 2004, 2005a, 2005b, English 2005c, 2007; Creutz, Lagus, and Virpioja 2005; Hirsimäki et al. 2003; Creutz et al. 2005 319 Computational Linguistics Volume 37, Number 2 Table 2 (continued) Model Superv. Experimentation Learns what? Kontorovich, Don, and Singer 2003 C T English Segmentation Medina Urrea and Díaz 2003; C T Chuj/Ralámuri/Czech Affix List Medina-Urrea 2006a, 2008 Mayfield and McNamee 2003; - -8 West European Same-stem McNamee and Mayfield 2007 languages (IR) Zweigenbaum, Hadouche, and C T Medical French Segmentation Grabar 2003; Hadouche 2002 Pirrelli et al. 2004; Pirrelli and C T Italian/English/Arabic Unclear Herreros 2007; Calderone 2008 Johnson and Martin 2003b C T Inuktitut Unclear Katrenko 2004 C T Ukrainian Lexicon+ Paradigms ´Cavar et al. 2004a, 2004b; ´Cavar, C T Child-English Unclear Rodrigues, and Schrementi 2006; ´Cavar et al. 2006 Rodrigues and ´Cavar 2005, 2007 NC T Arabic Segmentation Monson 2004, 2009; Monson et al. C T English/Spanish/ Segmentation 2004, 2007a</context>
<context position="68747" citStr="Mayfield and McNamee (2003)" startWordPosition="10855" endWordPosition="10858">˜ıthi E=˜ug˜ıthi˜ı I=g I=g˜ı I=g˜ıt I=g˜ıth I=g˜ıthi E=g˜ıthi˜ı I=˜ı I=˜ıt I=˜ıth I=˜ıthi E=˜ıthi˜ı I=t I=th I=thi E=thi˜ı I=h I=hi E=hi˜ı I=i E=i˜ı sets of four members with a particular affixation arrangement (“squares”),16 whose existence is governed much by the frequency of the affixes in question. 3.4 Features and Classes The features-and-classes methods share with the group-and-abstract methods the virtue of not being tied to segmental morpheme choices. As mentioned earlier, in this family of methods a word is seen as made up of a set of features which have no internal order— n-grams in Mayfield and McNamee (2003) and McNamee and Mayfield (2007), and beginning/terminal/internal segments in De Pauw and Wagacha (2007). For example, Table 9 shows two words and their features in G˜ik˜uy˜u, a tonal Bantu language of Kenya. As designed by De Pauw and Wagacha (2007), initial (B=), middle (I=), or final (E=) segments of a given word constitute its features. A majority of features enumerated this way will not be morphologically relevant, whereas a minority is. For example, in this case, I=h is just an arbitrary character without morpheme status, whereas I=ngithi happens to be equal to a stem. The idea is that a</context>
</contexts>
<marker>Mayfield, McNamee, 2003</marker>
<rawString>Mayfield, James and Paul McNamee. 2003. Single n-gram stemming. In SIGIR ’03: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, pages 415–416, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James L McClelland</author>
<author>David E Rumelhart</author>
</authors>
<title>On learning the past tenses of English verbs.</title>
<date>1986</date>
<booktitle>In Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 2: Psychological and Biological Models.</booktitle>
<pages>216--271</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="7977" citStr="McClelland and Rumelhart 1986" startWordPosition="1182" endWordPosition="1185">are included. Hence, subsequent uses of the term segmentation in the present survey are to be understood as morpheme segmentation rather than word segmentation. We prefer the term segmentation to analysis because, in general in ULM, the algorithm does not attempt to label the segments. There have been other approaches to machine learning of morphology than pure ULM as defined here, the most popular ones being: • approaches that require selective input, such as “singular–plural pairs,” or “all members of a paradigm” (Garvin 1967; Klein and Dennison 1976; Golding and Thompson 1985; Wothke 1985; McClelland and Rumelhart 1986; Brasington, Jones, and Biggs 1988; Tufis 1989; Zhang and Kim 1990; Borin 1991; Theron and Cloete 1997; Oflazer, McShane, and Nirenburg 2001, for example) • approaches where some (small) amount of annotated data, some (small) amount of existing rule sets, or resources such as a machinereadable dictionary or a parallel corpus, are mandatory (Yarowsky and Wicentowski 2000; Yarowsky, Ngai, and Wicentowski 2001; Cucerzan and Yarowsky 2002; Neuvel and Fulop 2002; Johnson and Martin 2003; Rogati, McCarley, and Yang 2003, for example) Such approaches are excluded from the present survey, unless the </context>
</contexts>
<marker>McClelland, Rumelhart, 1986</marker>
<rawString>McClelland, James L. and David E. Rumelhart. 1986. On learning the past tenses of English verbs. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 2: Psychological and Biological Models. MIT Press, Cambridge, MA, pages 216–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
</authors>
<title>Retrieval experiments at Morpho Challenge</title>
<date>2008</date>
<booktitle>In Working Notes for the CLEF</booktitle>
<location>Workshop, Aarhus.</location>
<contexts>
<context position="12718" citStr="McNamee (2008)" startWordPosition="1939" endWordPosition="1940">erviews for a general audience are Borin (1991), Batchelder (1997, pages 66–68), Powers (1998), Clark (2001, pages 80–82), Xanthos (2007, pages 95–107), Goldsmith (2001), Daelemans (2004, page 1898), Roark and Sproat (2007, pages 116–136), Hammarström (2007b, pages 10–15), Chan (2008, pages 48–60), Hammarström (2009b, pages 14–21), Borin (2009), Goldsmith (2010), and, to a more limited extent, the related-work sections of individual research papers. Kurimo, Creutz, and Turunen (2007), Kurimo, Creutz, and Varjokallio (2008a, 2008b), Kurimo and Turunen (2008), Kurimo and Varjokallio (2008), and McNamee (2008) are overviews of systems in the MorphoChallenge of the respective year. However, we will try to be more comprehensive than previous surveys and discuss the ideas in the field critically. 312 Hammarström and Borin Unsupervised Learning of Morphology We will not attempt a comparison in terms of accuracy figures as this is wholly impossible, not only because of the great variation in goals but also because most descriptions do not specify their algorithm(s) in enough detail. Fortunately, this aspect is better handled in controlled competitions, such as the Unsupervised Morpheme Analysis— MorphoC</context>
</contexts>
<marker>McNamee, 2008</marker>
<rawString>McNamee, Paul. 2008. Retrieval experiments at Morpho Challenge 2008. In Working Notes for the CLEF 2008 Workshop, Aarhus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>James Mayfield</author>
</authors>
<title>N-gram morphemes for retrieval.</title>
<date>2007</date>
<booktitle>In Working Notes for the CLEF</booktitle>
<location>Budapest.</location>
<contexts>
<context position="33955" citStr="McNamee and Mayfield (2007)" startWordPosition="5228" endWordPosition="5231"> into sets, paired, shortlisted, etc.) according to some metric, which is typically string edit distance, but may include semantic features (Schone 2001), distributional similarity (Freitag 2005), or frequency signatures (Wicentowski 2002). The next step is to abstract some morphological pattern that recurs among the groups. Such emergent patterns provide enough clues for segmentation and can sometimes be formulated as rules or morphological paradigms. (c) Features and Classes: In this family of methods, a word is seen as made up of a set of features—n-grams in Mayfield and McNamee (2003) and McNamee and Mayfield (2007), and initial/terminal/mid-substring in De Pauw and Wagacha (2007). Features which occur on many words have little selective power across the words, whereas features which occur seldom pinpoint a specific word or stem. To formalize this intuition, Mayfield and McNamee and McNamee and Mayfield use TF-IDF, and De Pauw and Wagacha use entropy. Classifying an unseen word reduces to using its features to select which word(s) it may be morphologically related to. This decides whether the unseen word is a morphological variant of some other word, and allows extracting the “variation” by which they ar</context>
<context position="37711" citStr="McNamee and Mayfield 2007" startWordPosition="5802" endWordPosition="5805">2 C/NC T English/German (I) Related word pairs Bati 2002 C/NC T Amharic Lexicon+ Paradigms Creutz 2003, 2006; Creutz and C T Finnish/Turkish/ Segmentation Lagus 2002, 2004, 2005a, 2005b, English 2005c, 2007; Creutz, Lagus, and Virpioja 2005; Hirsimäki et al. 2003; Creutz et al. 2005 319 Computational Linguistics Volume 37, Number 2 Table 2 (continued) Model Superv. Experimentation Learns what? Kontorovich, Don, and Singer 2003 C T English Segmentation Medina Urrea and Díaz 2003; C T Chuj/Ralámuri/Czech Affix List Medina-Urrea 2006a, 2008 Mayfield and McNamee 2003; - -8 West European Same-stem McNamee and Mayfield 2007 languages (IR) Zweigenbaum, Hadouche, and C T Medical French Segmentation Grabar 2003; Hadouche 2002 Pirrelli et al. 2004; Pirrelli and C T Italian/English/Arabic Unclear Herreros 2007; Calderone 2008 Johnson and Martin 2003b C T Inuktitut Unclear Katrenko 2004 C T Ukrainian Lexicon+ Paradigms ´Cavar et al. 2004a, 2004b; ´Cavar, C T Child-English Unclear Rodrigues, and Schrementi 2006; ´Cavar et al. 2006 Rodrigues and ´Cavar 2005, 2007 NC T Arabic Segmentation Monson 2004, 2009; Monson et al. C T English/Spanish/ Segmentation 2004, 2007a, 2007b, 2008, 2008a, Mapudungun (I) 2008b Yarowsky and </context>
<context position="68779" citStr="McNamee and Mayfield (2007)" startWordPosition="10860" endWordPosition="10863">˜ıt I=g˜ıth I=g˜ıthi E=g˜ıthi˜ı I=˜ı I=˜ıt I=˜ıth I=˜ıthi E=˜ıthi˜ı I=t I=th I=thi E=thi˜ı I=h I=hi E=hi˜ı I=i E=i˜ı sets of four members with a particular affixation arrangement (“squares”),16 whose existence is governed much by the frequency of the affixes in question. 3.4 Features and Classes The features-and-classes methods share with the group-and-abstract methods the virtue of not being tied to segmental morpheme choices. As mentioned earlier, in this family of methods a word is seen as made up of a set of features which have no internal order— n-grams in Mayfield and McNamee (2003) and McNamee and Mayfield (2007), and beginning/terminal/internal segments in De Pauw and Wagacha (2007). For example, Table 9 shows two words and their features in G˜ik˜uy˜u, a tonal Bantu language of Kenya. As designed by De Pauw and Wagacha (2007), initial (B=), middle (I=), or final (E=) segments of a given word constitute its features. A majority of features enumerated this way will not be morphologically relevant, whereas a minority is. For example, in this case, I=h is just an arbitrary character without morpheme status, whereas I=ngithi happens to be equal to a stem. The idea is that arbitrary features such as I=h wi</context>
</contexts>
<marker>McNamee, Mayfield, 2007</marker>
<rawString>McNamee, Paul and James Mayfield. 2007. N-gram morphemes for retrieval. In Working Notes for the CLEF 2007 Workshop, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Medina Urrea</author>
<author>Alfonso</author>
</authors>
<title>Automatic discovery of affixes by means of a corpus: A catalog of Spanish affixes.</title>
<date>2000</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>7</volume>
<issue>2</issue>
<marker>Urrea, Alfonso, 2000</marker>
<rawString>Medina Urrea, Alfonso. 2000. Automatic discovery of affixes by means of a corpus: A catalog of Spanish affixes. Journal of Quantitative Linguistics, 7(2):97–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Medina Urrea</author>
<author>Alfonso</author>
</authors>
<title>Investigación cuantitativa de afijos y clíticos del español de México: Glutinometría en el</title>
<date>2003</date>
<booktitle>Corpus del Español Mexicano Contemporáneo. Ph.D. thesis, El Colegio de</booktitle>
<location>México, México, D.F.</location>
<marker>Urrea, Alfonso, 2003</marker>
<rawString>Medina Urrea, Alfonso. 2003. Investigación cuantitativa de afijos y clíticos del español de México: Glutinometría en el Corpus del Español Mexicano Contemporáneo. Ph.D. thesis, El Colegio de México, México, D.F.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfonso Medina-Urrea</author>
</authors>
<title>Affix discovery by means of corpora: Experiments for Spanish, Czech, Ralámuli and Chuj.</title>
<date>2006</date>
<booktitle>Aspects of Automatic Text Analysis, volume 209 of Studies in Fuzziness and Soft Computing.</booktitle>
<pages>277--299</pages>
<editor>In Alexander Mehler and Reinhard Köhler, editors,</editor>
<publisher>Springer,</publisher>
<location>Berlin,</location>
<contexts>
<context position="37622" citStr="Medina-Urrea 2006" startWordPosition="5790" endWordPosition="5791">T Assamese Lexicon+ Paradigms Sharma and Das 2002 Baroni, Matiasek, and Trost 2002 C/NC T English/German (I) Related word pairs Bati 2002 C/NC T Amharic Lexicon+ Paradigms Creutz 2003, 2006; Creutz and C T Finnish/Turkish/ Segmentation Lagus 2002, 2004, 2005a, 2005b, English 2005c, 2007; Creutz, Lagus, and Virpioja 2005; Hirsimäki et al. 2003; Creutz et al. 2005 319 Computational Linguistics Volume 37, Number 2 Table 2 (continued) Model Superv. Experimentation Learns what? Kontorovich, Don, and Singer 2003 C T English Segmentation Medina Urrea and Díaz 2003; C T Chuj/Ralámuri/Czech Affix List Medina-Urrea 2006a, 2008 Mayfield and McNamee 2003; - -8 West European Same-stem McNamee and Mayfield 2007 languages (IR) Zweigenbaum, Hadouche, and C T Medical French Segmentation Grabar 2003; Hadouche 2002 Pirrelli et al. 2004; Pirrelli and C T Italian/English/Arabic Unclear Herreros 2007; Calderone 2008 Johnson and Martin 2003b C T Inuktitut Unclear Katrenko 2004 C T Ukrainian Lexicon+ Paradigms ´Cavar et al. 2004a, 2004b; ´Cavar, C T Child-English Unclear Rodrigues, and Schrementi 2006; ´Cavar et al. 2006 Rodrigues and ´Cavar 2005, 2007 NC T Arabic Segmentation Monson 2004, 2009; Monson et al. C T English/</context>
</contexts>
<marker>Medina-Urrea, 2006</marker>
<rawString>Medina-Urrea, Alfonso. 2006a. Affix discovery by means of corpora: Experiments for Spanish, Czech, Ralámuli and Chuj. In Alexander Mehler and Reinhard Köhler, editors, Aspects of Automatic Text Analysis, volume 209 of Studies in Fuzziness and Soft Computing. Springer, Berlin, pages 277–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Medina Urrea</author>
<author>Alfonso</author>
</authors>
<title>Towards the automatic lemmatization of 16th century Mexican Spanish: A stemming scheme for the CHEM.</title>
<date>2006</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing, 7th International Conference, CICLing</booktitle>
<pages>101--104</pages>
<location>Mexico City.</location>
<marker>Urrea, Alfonso, 2006</marker>
<rawString>Medina Urrea, Alfonso. 2006b. Towards the automatic lemmatization of 16th century Mexican Spanish: A stemming scheme for the CHEM. In Computational Linguistics and Intelligent Text Processing, 7th International Conference, CICLing 2006, pages 101–104, Mexico City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfonso Medina-Urrea</author>
</authors>
<title>Affix discovery based on entropy and economy measurements.</title>
<date>2008</date>
<booktitle>Computational Linguistics for Less-Studied Languages, volume X of Texas Linguistics Society. CSLI Publications,</booktitle>
<pages>99--112</pages>
<editor>In Nicholas Gaylord, Alexis Palmer, and Elias Ponvert, editors,</editor>
<location>Stanford, CA,</location>
<marker>Medina-Urrea, 2008</marker>
<rawString>Medina-Urrea, Alfonso. 2008. Affix discovery based on entropy and economy measurements. In Nicholas Gaylord, Alexis Palmer, and Elias Ponvert, editors, Computational Linguistics for Less-Studied Languages, volume X of Texas Linguistics Society. CSLI Publications, Stanford, CA, pages 99–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Medina Urrea</author>
<author>Alfonso</author>
<author>E C Buenrostro Díaz</author>
</authors>
<title>Características cuantitativas de la flexión verbal del Chuj. Estudios de Lingüística Aplicada,</title>
<date>2003</date>
<pages>38--15</pages>
<marker>Urrea, Alfonso, Díaz, 2003</marker>
<rawString>Medina Urrea, Alfonso and E. C. Buenrostro Díaz. 2003. Características cuantitativas de la flexión verbal del Chuj. Estudios de Lingüística Aplicada, 38:15–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Melkumjan</author>
</authors>
<title>Ishodnye dannye i statistiko-kombinatornoe vydelenie paradigmy pervogo morfologiˇceskogo tipa v armjanskom jazyke.</title>
<date>1965</date>
<booktitle>In Nikolaj Dmitrieviˇc Andreev, editor, Statistiko-kombinatornoe modelirovanie jazykov. Nauka,</booktitle>
<pages>123--136</pages>
<location>Leningrad,</location>
<contexts>
<context position="19348" citStr="Melkumjan 1965" startWordPosition="2948" endWordPosition="2949">e suffix candidates. In the Russian experiment reported by Andreeva (1963), a complete adjective paradigm was induced, with 12 different suffixes. The initial suffix candidate, -oj, has a high functional load and consequently a high text frequency: It is the most ambiguous of the Russian adjective suffixes, appearing in four different slots in the adjective paradigm, and is also homonymous with a noun suffix. In Andreev (1965b) the method is tested extensively on Russian, which is the subject of several papers in the volume, and a number of other languages: Albanian (Peršikov 1965), Armenian (Melkumjan 1965), Bulgarian (Fedulova 1965), Czech (Ožigova 1965), English (Malahovskij 1965), Estonian (Hol’m 1965), French (Kordi 1965), German (Fitialova 1965), Hausa (Fihman 1965a), Hungarian (Andreev 1965a), Latvian (Jakubajtis 1965), Serbo-Croatian (Panina 1965), Swahili (Fihman 1965b), Ukrainian (Eliseeva 1965), and Vietnamese (Jakuševa 1965). As an aside, we may note that only after the turn of the millennium are we again seeing this variety of languages in ULM work. Most of these studies are small-scale proof-of-concept experiments on corpora of varying sizes (from a few thousand words in many of the</context>
<context position="35189" citStr="Melkumjan 1965" startWordPosition="5415" endWordPosition="5416">n affix. (d) Phonological Categories and Separation: In this family of methods, the phonemes (approximated by graphemes) are first classed into categories, foremostly, vowel versus consonant. Thereafter, each word is separated into its vowel skeleton and its consonant skeleton, after which various 318 Hammarström and Borin Unsupervised Learning of Morphology Table 2 Very brief roadmap of earlier studies. Model Superv. Experimentation Learns what? Harris 1955, 1968, 1970 C T English Segmentation Andreev 1965a, Andreev 1967, C T Vietnamese to Segmentation Chapter 2, Peršikov 1965; Hungarian (I) Melkumjan 1965; Fedulova 1965; Ožigova 1965; Malahovskij 1965; Hol’m 1965; Kordi 1965; Fitialova 1965; Fihman 1965a; Andreev 1965a; Jakubajtis 1965; Panina 1965; Fihman 1965b; Eliseeva 1965; Jakuševa 1965 Gammon 1969 C T English Segmentation Lehmann 1973, pages 71–93 C T French (I) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1</context>
</contexts>
<marker>Melkumjan, 1965</marker>
<rawString>Melkumjan, M. R. 1965. Ishodnye dannye i statistiko-kombinatornoe vydelenie paradigmy pervogo morfologiˇceskogo tipa v armjanskom jazyke. In Nikolaj Dmitrieviˇc Andreev, editor, Statistiko-kombinatornoe modelirovanie jazykov. Nauka, Leningrad, pages 123–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
</authors>
<title>Automatic rule induction for unknown-word guessing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="9097" citStr="Mikheev 1997" startWordPosition="1353" endWordPosition="1354">nd Yang 2003, for example) Such approaches are excluded from the present survey, unless the required data (e.g., paradigm members) are extracted from raw text in an unsupervised manner as well. We also exclude the special case of the second approach where morphology learning means not “learning the morphological system of a language,” but rather “learning the inflectional classes of out-of-vocabulary words,” namely, approaches where an existing morphological analysis component is used as the basis for guessing in which existing paradigm an unknown text word should belong (e.g., Antworth 1990; Mikheev 1997; Bharati et al. 2001; Forsberg, Hammarström, and Ranta 2006; Lindén 2008; Lindén 2009). One of the matters that varies the most between different authors is the desired outcome. It is useful to set up the implicational hierarchy shown in Table 1 (which 4 There are, however, rare cases of languages which allow the permutation of specific pairs of prefixes, such as Kagulu (Petzell 2007), Yimas and Karawari (Foley 1991, pages 31–32) as well as Chintang, Bantawa, and possibly other Kiranti languages where prefix ordering in general is very free (Rai 1984; Bickel et al. 2007). 5 Clitics are affix-</context>
</contexts>
<marker>Mikheev, 1997</marker>
<rawString>Mikheev, Andrei. 1997. Automatic rule induction for unknown-word guessing. Computational Linguistics, 23(3):405–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marianne Mithun</author>
</authors>
<title>The Languages of Native North America. Cambridge Language Surveys.</title>
<date>1999</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<marker>Mithun, 1999</marker>
<rawString>Mithun, Marianne. 1999. The Languages of Native North America. Cambridge Language Surveys. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Monson</author>
</authors>
<title>A framework for unsupervised natural language morphology induction.</title>
<date>2004</date>
<booktitle>In ACL 2004: Student Research Workshop,</booktitle>
<pages>67--72</pages>
<location>Barcelona.</location>
<contexts>
<context position="38188" citStr="Monson 2004" startWordPosition="5876" endWordPosition="5877">j/Ralámuri/Czech Affix List Medina-Urrea 2006a, 2008 Mayfield and McNamee 2003; - -8 West European Same-stem McNamee and Mayfield 2007 languages (IR) Zweigenbaum, Hadouche, and C T Medical French Segmentation Grabar 2003; Hadouche 2002 Pirrelli et al. 2004; Pirrelli and C T Italian/English/Arabic Unclear Herreros 2007; Calderone 2008 Johnson and Martin 2003b C T Inuktitut Unclear Katrenko 2004 C T Ukrainian Lexicon+ Paradigms ´Cavar et al. 2004a, 2004b; ´Cavar, C T Child-English Unclear Rodrigues, and Schrementi 2006; ´Cavar et al. 2006 Rodrigues and ´Cavar 2005, 2007 NC T Arabic Segmentation Monson 2004, 2009; Monson et al. C T English/Spanish/ Segmentation 2004, 2007a, 2007b, 2008, 2008a, Mapudungun (I) 2008b Yarowsky and Wicentowski 2000; C/NC T 30-ish mostly European Segmentation + Wicentowski 2002, 2004 type languages Rewrite Rules Gelbukh, Alexandrov, and Han 2004 C - English Segmentation Argamon et al. 2004 C T English Segmentation Goldsmith et al. 2005; Hu et al. 2005a C/NC T Unclear Unclear Bacchin, Ferro, and Melucci 2005, C T Italian/English Segmentation 2002a, 2002b; Nunzio et al. 2004 Oliver 2004, Chapter 4–5 C T Catalan Paradigms Bordag 2005a, 2005b, 2007, 2008 C T English/Germa</context>
</contexts>
<marker>Monson, 2004</marker>
<rawString>Monson, Christian. 2004. A framework for unsupervised natural language morphology induction. In ACL 2004: Student Research Workshop, pages 67–72, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Monson</author>
</authors>
<title>ParaMor: From Paradigm Structure to Natural Language Morphology Induction.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="62490" citStr="Monson (2009)" startWordPosition="9866" endWordPosition="9867">urs in is likely not to be normally distributed. Most lexemes will occur in only one form, and only very few, if any, lexemes will occur in all |P| forms. It appears that for most languages and most paradigms, the number of lexemes that occur in i forms tends to decrease logarithmically in i (Chan 2008, pages 75–84). As an example, consider the three most common paradigms in Swedish and the frequency of forms in Table 8. Works which have attempted nevertheless to tackle the matter of paradigms, at least for languages with one-slot morphology, include Zeman 2008, 2009, Hammarström (2009b), and Monson (2009). They explicitly or implicitly make use of the following two heuristics to narrow down the search space: • Languages tend to have a small number of paradigms (where “small” means fewer than 100 paradigms with at least 100 member stems each). • Languages tend to have only small paradigms (where “small” means fewer than 50), that is, the number of affixes in each paradigm is small. Agglutinative languages, which have several layers of affixes, can be said to obey this generalization in the sense that each layer has few members, whereas conversely, the full paradigm achieves considerable size co</context>
</contexts>
<marker>Monson, 2009</marker>
<rawString>Monson, Christian. 2009. ParaMor: From Paradigm Structure to Natural Language Morphology Induction. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christian Monson</author>
<author>Jaime Carbonell</author>
<author>Alon Lavie</author>
<author>Lori Levin</author>
</authors>
<title>ParaMor: Finding paradigms across morphology.</title>
<date>2007</date>
<booktitle>Advances in Multilingual and Multimodal Information Retrieval, 8th Workshop of the CrossLanguage Evaluation Forum, CLEF 2007,</booktitle>
<pages>892--899</pages>
<editor>In Carol Peters, Valentin Jijkoun, Thomas Mandl, Henning Müller, Douglas W. Oard, and Anselmo Peñas, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Budapest, Hungary,</location>
<marker>Monson, Carbonell, Lavie, Levin, 2007</marker>
<rawString>Monson, Christian, Jaime Carbonell, Alon Lavie, and Lori Levin. 2007a. ParaMor: Finding paradigms across morphology. In Carol Peters, Valentin Jijkoun, Thomas Mandl, Henning Müller, Douglas W. Oard, and Anselmo Peñas, editors, Advances in Multilingual and Multimodal Information Retrieval, 8th Workshop of the CrossLanguage Evaluation Forum, CLEF 2007, Budapest, Hungary, September 19–21, 2007, Revised Selected Papers, pages 892–899. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Monson</author>
<author>Jaime Carbonell</author>
<author>Alon Lavie</author>
<author>Lori Levin</author>
</authors>
<title>ParaMor: Minimally supervised induction of paradigm structure and morphological analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology,</booktitle>
<pages>117--125</pages>
<location>Prague.</location>
<marker>Monson, Carbonell, Lavie, Levin, 2007</marker>
<rawString>Monson, Christian, Jaime Carbonell, Alon Lavie, and Lori Levin. 2007b. ParaMor: Minimally supervised induction of paradigm structure and morphological analysis. In Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 117–125, Prague.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christian Monson</author>
<author>Jaime Carbonell</author>
<author>Alon Lavie</author>
<author>Lori Levin</author>
</authors>
<title>ParaMor and Morpho Challenge 2008. Using unsupervised paradigm acquisition for prefixes.</title>
<date>2008</date>
<booktitle>Evaluating Systems for Multilingual and Multimodal Information Access, 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008,</booktitle>
<pages>967--974</pages>
<editor>In Carol Peters, Thomas Deselaers, Nicola Ferro, Julio Gonzalo, Gareth J. F. Jones, Mikko Kurimo, Thomas Mandl, Anselmo Peñas, and Vivien Petras, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Aarthus, Denmark,</location>
<marker>Monson, Carbonell, Lavie, Levin, 2008</marker>
<rawString>Monson, Christian, Jaime Carbonell, Alon Lavie, and Lori Levin. 2008. ParaMor and Morpho Challenge 2008. Using unsupervised paradigm acquisition for prefixes. In Carol Peters, Thomas Deselaers, Nicola Ferro, Julio Gonzalo, Gareth J. F. Jones, Mikko Kurimo, Thomas Mandl, Anselmo Peñas, and Vivien Petras, editors, Evaluating Systems for Multilingual and Multimodal Information Access, 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008, Aarthus, Denmark, September 17–19, 2008, Revised Selected Papers, pages 967–974. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Monson</author>
<author>Alon Lavie</author>
<author>Jaime Carbonell</author>
<author>Lori Levin</author>
</authors>
<title>Unsupervised induction of natural language morphology inflection classes.</title>
<date>2004</date>
<booktitle>In SIGPHON 2004: Proceedings of the Seventh Meeting of the ACL Special Interest Group in Computational Phonology,</booktitle>
<pages>52--61</pages>
<location>Barcelona.</location>
<marker>Monson, Lavie, Carbonell, Levin, 2004</marker>
<rawString>Monson, Christian, Alon Lavie, Jaime Carbonell, and Lori Levin. 2004. Unsupervised induction of natural language morphology inflection classes. In SIGPHON 2004: Proceedings of the Seventh Meeting of the ACL Special Interest Group in Computational Phonology, pages 52–61, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Monson</author>
<author>Alon Lavie</author>
<author>Jaime Carbonell</author>
<author>Lori Levin</author>
</authors>
<title>Evaluating an agglutinative segmentation model for ParaMor.</title>
<date>2008</date>
<booktitle>In Proceedings of the Tenth Meeting of ACL Special Interest Group on Computational Morphology and Phonology,</booktitle>
<pages>49--58</pages>
<location>Columbus, OH.</location>
<marker>Monson, Lavie, Carbonell, Levin, 2008</marker>
<rawString>Monson, Christian, Alon Lavie, Jaime Carbonell, and Lori Levin. 2008a. Evaluating an agglutinative segmentation model for ParaMor. In Proceedings of the Tenth Meeting of ACL Special Interest Group on Computational Morphology and Phonology, pages 49–58, Columbus, OH.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christian Monson</author>
<author>Ariadna Font Llitjós</author>
<author>Vamshi Ambati</author>
<author>Lori Levin</author>
<author>Alon Lavie</author>
<author>Alison Alvarez</author>
<author>Roberto Aranovich</author>
<author>Jaime Carbonell</author>
<author>Robert Frederking</author>
<author>Erik Peterson</author>
<author>Katharina Probst</author>
</authors>
<title>Linguistic structure and bilingual informants help induce machine translation of lesser-resourced languages.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<pages>2854--2859</pages>
<location>Marrakech.</location>
<marker>Monson, Llitjós, Ambati, Levin, Lavie, Alvarez, Aranovich, Carbonell, Frederking, Peterson, Probst, 2008</marker>
<rawString>Monson, Christian, Ariadna Font Llitjós, Vamshi Ambati, Lori Levin, Alon Lavie, Alison Alvarez, Roberto Aranovich, Jaime Carbonell, Robert Frederking, Erik Peterson, and Katharina Probst. 2008b. Linguistic structure and bilingual informants help induce machine translation of lesser-resourced languages. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), pages 2854–2859, Marrakech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taesun Moon</author>
<author>Katrin Erk</author>
<author>Jason Baldridge</author>
</authors>
<title>Unsupervised morphological segmentation and clustering with document boundaries.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>668--677</pages>
<marker>Moon, Erk, Baldridge, 2009</marker>
<rawString>Moon, Taesun, Katrin Erk, and Jason Baldridge. 2009. Unsupervised morphological segmentation and clustering with document boundaries. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 668–677, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Naradowsky</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving morphology induction by learning spelling rules.</title>
<date>2009</date>
<booktitle>In UCAI 2009, Proceedings of the 21st, International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1531--1537</pages>
<location>Pasadena, California, USA,</location>
<contexts>
<context position="39508" citStr="Naradowsky and Goldwater 2009" startWordPosition="6060" endWordPosition="6063">, 2009b Bernhard 2005a, 2005b, 2006, 2007, C T Finnish/Turkish/English Segmentation+ 2008 Related sets of words Keshava and Pitler 2005 C T Finnish/Turkish/English Segmentation Johnsen 2005 C T Finnish/Turkish/English Segmentation Atwell and Roberts 2005 C T Finnish/Turkish/English Segmentation Dang and Choudri 2005 C T Finnish/Turkish/English Segmentation ur Rehman and Hussain 2005 C T Finnish/Turkish/English Segmentation Jordan, Healy, and Keselj 2005, 2006 C T Finnish/Turkish/English Segmentation Goldwater, Griffiths, and Johnson C T English/Child-English Segmentation 2005; Goldwater 2007; Naradowsky and Goldwater 2009 Freitag 2005 C T English Segmentation Golcher 2006 C - English/German Lexicon+ Paradigms Arabsorkhi and Shamsfard 2006 C T Persian Segmentation Chan 2006, Chan 2008, C T English Paradigms pages 101–139 Demberg 2007 C/NC T English/German/ Segmentation Finnish/Turkish Dasgupta and Ng 2006, 2007a; C T Bengali Segmentation 2007b; Dasgupta 2007 De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al. 2007; C T French/Bengali/French/ Analysis Majumder, Mitra, and Pal Bulgar2008</context>
</contexts>
<marker>Naradowsky, Goldwater, 2009</marker>
<rawString>Naradowsky, Jason and Sharon Goldwater. 2009. Improving morphology induction by learning spelling rules. In UCAI 2009, Proceedings of the 21st, International Joint Conference on Artificial Intelligence, Pasadena, California, USA, July 11–17, 2009, pages 1531–1537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Neuvel</author>
<author>Sean A Fulop</author>
</authors>
<title>Unsupervised learning of morphology without morphemes.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning,</booktitle>
<pages>31--40</pages>
<location>Philadelphia,</location>
<contexts>
<context position="8439" citStr="Neuvel and Fulop 2002" startWordPosition="1253" endWordPosition="1256">ular–plural pairs,” or “all members of a paradigm” (Garvin 1967; Klein and Dennison 1976; Golding and Thompson 1985; Wothke 1985; McClelland and Rumelhart 1986; Brasington, Jones, and Biggs 1988; Tufis 1989; Zhang and Kim 1990; Borin 1991; Theron and Cloete 1997; Oflazer, McShane, and Nirenburg 2001, for example) • approaches where some (small) amount of annotated data, some (small) amount of existing rule sets, or resources such as a machinereadable dictionary or a parallel corpus, are mandatory (Yarowsky and Wicentowski 2000; Yarowsky, Ngai, and Wicentowski 2001; Cucerzan and Yarowsky 2002; Neuvel and Fulop 2002; Johnson and Martin 2003; Rogati, McCarley, and Yang 2003, for example) Such approaches are excluded from the present survey, unless the required data (e.g., paradigm members) are extracted from raw text in an unsupervised manner as well. We also exclude the special case of the second approach where morphology learning means not “learning the morphological system of a language,” but rather “learning the inflectional classes of out-of-vocabulary words,” namely, approaches where an existing morphological analysis component is used as the basis for guessing in which existing paradigm an unknown </context>
</contexts>
<marker>Neuvel, Fulop, 2002</marker>
<rawString>Neuvel, Sylvain and Sean A. Fulop. 2002. Unsupervised learning of morphology without morphemes. In Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning, Philadelphia, pages 31–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene A Nida</author>
</authors>
<title>Morphology. The Descriptive Analysis of Words, 2nd edition.</title>
<date>1949</date>
<institution>The University of Michigan Press,</institution>
<location>Ann Arbor, MI.</location>
<marker>Nida, 1949</marker>
<rawString>Nida, Eugene A. 1949. Morphology. The Descriptive Analysis of Words, 2nd edition. The University of Michigan Press, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G M Di Nunzio</author>
<author>N Ferro</author>
<author>M Melucci</author>
<author>N Orio</author>
</authors>
<title>Experiments to evaluate probabilistic models for automatic stemmer generation and query word translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Cross-Language Evaluation Forum (CLEF): Methodology and Metrics (CLEF</booktitle>
<pages>220--235</pages>
<contexts>
<context position="38691" citStr="Nunzio et al. 2004" startWordPosition="5952" endWordPosition="5955">igues, and Schrementi 2006; ´Cavar et al. 2006 Rodrigues and ´Cavar 2005, 2007 NC T Arabic Segmentation Monson 2004, 2009; Monson et al. C T English/Spanish/ Segmentation 2004, 2007a, 2007b, 2008, 2008a, Mapudungun (I) 2008b Yarowsky and Wicentowski 2000; C/NC T 30-ish mostly European Segmentation + Wicentowski 2002, 2004 type languages Rewrite Rules Gelbukh, Alexandrov, and Han 2004 C - English Segmentation Argamon et al. 2004 C T English Segmentation Goldsmith et al. 2005; Hu et al. 2005a C/NC T Unclear Unclear Bacchin, Ferro, and Melucci 2005, C T Italian/English Segmentation 2002a, 2002b; Nunzio et al. 2004 Oliver 2004, Chapter 4–5 C T Catalan Paradigms Bordag 2005a, 2005b, 2007, 2008 C T English/German Segmentation Hammarström 2005, 2006a, 2006b, C - Maori to Warlpiri Same-stem 2007b, 2009a, 2009b Bernhard 2005a, 2005b, 2006, 2007, C T Finnish/Turkish/English Segmentation+ 2008 Related sets of words Keshava and Pitler 2005 C T Finnish/Turkish/English Segmentation Johnsen 2005 C T Finnish/Turkish/English Segmentation Atwell and Roberts 2005 C T Finnish/Turkish/English Segmentation Dang and Choudri 2005 C T Finnish/Turkish/English Segmentation ur Rehman and Hussain 2005 C T Finnish/Turkish/Englis</context>
</contexts>
<marker>Nunzio, Ferro, Melucci, Orio, 2004</marker>
<rawString>Nunzio, G. M. Di, N. Ferro, M. Melucci, and N. Orio. 2004. Experiments to evaluate probabilistic models for automatic stemmer generation and query word translation. In Proceedings of the Cross-Language Evaluation Forum (CLEF): Methodology and Metrics (CLEF 2003), pages 220–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
<author>Marjorie McShane</author>
<author>Sergei Nirenburg</author>
</authors>
<title>Bootstrapping morphological analyzers by combining human elicitation and machine learning.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>1</issue>
<marker>Oflazer, McShane, Nirenburg, 2001</marker>
<rawString>Oflazer, Kemal, Marjorie McShane, and Sergei Nirenburg. 2001. Bootstrapping morphological analyzers by combining human elicitation and machine learning. Computational Linguistics, 27(1):59–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Oliver</author>
</authors>
<title>Adquisició d’informació lèxica i morfosintàctica a partir de corpus sense anotar: aplicació al rus i al croat.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Universitat de Barcelona.</institution>
<contexts>
<context position="38703" citStr="Oliver 2004" startWordPosition="5956" endWordPosition="5957">ti 2006; ´Cavar et al. 2006 Rodrigues and ´Cavar 2005, 2007 NC T Arabic Segmentation Monson 2004, 2009; Monson et al. C T English/Spanish/ Segmentation 2004, 2007a, 2007b, 2008, 2008a, Mapudungun (I) 2008b Yarowsky and Wicentowski 2000; C/NC T 30-ish mostly European Segmentation + Wicentowski 2002, 2004 type languages Rewrite Rules Gelbukh, Alexandrov, and Han 2004 C - English Segmentation Argamon et al. 2004 C T English Segmentation Goldsmith et al. 2005; Hu et al. 2005a C/NC T Unclear Unclear Bacchin, Ferro, and Melucci 2005, C T Italian/English Segmentation 2002a, 2002b; Nunzio et al. 2004 Oliver 2004, Chapter 4–5 C T Catalan Paradigms Bordag 2005a, 2005b, 2007, 2008 C T English/German Segmentation Hammarström 2005, 2006a, 2006b, C - Maori to Warlpiri Same-stem 2007b, 2009a, 2009b Bernhard 2005a, 2005b, 2006, 2007, C T Finnish/Turkish/English Segmentation+ 2008 Related sets of words Keshava and Pitler 2005 C T Finnish/Turkish/English Segmentation Johnsen 2005 C T Finnish/Turkish/English Segmentation Atwell and Roberts 2005 C T Finnish/Turkish/English Segmentation Dang and Choudri 2005 C T Finnish/Turkish/English Segmentation ur Rehman and Hussain 2005 C T Finnish/Turkish/English Segmentati</context>
</contexts>
<marker>Oliver, 2004</marker>
<rawString>Oliver, A. 2004. Adquisició d’informació lèxica i morfosintàctica a partir de corpus sense anotar: aplicació al rus i al croat. Ph.D. thesis, Universitat de Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Ostler</author>
</authors>
<title>Is it globalization that endangers languages?</title>
<date>2008</date>
<booktitle>In UNESCO/UNU Conference: Globalization and Languages: Building our Rich Heritage,</booktitle>
<pages>206--211</pages>
<location>Paris.</location>
<contexts>
<context position="25850" citStr="Ostler 2008" startWordPosition="3975" endWordPosition="3976">ith language technology resources. There are on the order of 7,000 languages spoken in the world today (Lewis 2009). Their size in number of first-language speakers is very unevenly distributed. The top 30 languages in the world account for more than 60% of its population. At the other end of the scale, we find that most languages are spoken by quite small communities: There are close to 7,000 languages in the world, and half of them have fewer than 7,000 speakers each, less than a village. What is more, 80% of the world’s languages have fewer than 100,000 speakers, the size of a small town. (Ostler 2008, page 2) On the whole, small language communities will tend to have correspondingly small financial and other resources that could be spent on the development of language technology, but the cost of, for example, constructing a lexicon or a parser for a language is more or less constant, and not proportional to the number of speakers of the language. At the same time, it has been observed over and over again that the use or non-use of a language in a particular situation—where the language could in principle be used, but where there is a choice available between two or more languages—is intim</context>
</contexts>
<marker>Ostler, 2008</marker>
<rawString>Ostler, Nicholas. 2008. Is it globalization that endangers languages? In UNESCO/UNU Conference: Globalization and Languages: Building our Rich Heritage, pages 206–211, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G I Ožigova</author>
</authors>
<title>Statistiko-kombinatornoe modelirovanie paradigmy pervogo morfologiˇceskogo tipa v ˇcešskom jazyke.</title>
<date>1965</date>
<booktitle>In Nikolaj Dmitrieviˇc Andreev, editor, Statistiko-kombinatornoe modelirovanie jazykov. Nauka,</booktitle>
<pages>89--103</pages>
<location>Leningrad,</location>
<contexts>
<context position="19397" citStr="Ožigova 1965" startWordPosition="2954" endWordPosition="2955">rted by Andreeva (1963), a complete adjective paradigm was induced, with 12 different suffixes. The initial suffix candidate, -oj, has a high functional load and consequently a high text frequency: It is the most ambiguous of the Russian adjective suffixes, appearing in four different slots in the adjective paradigm, and is also homonymous with a noun suffix. In Andreev (1965b) the method is tested extensively on Russian, which is the subject of several papers in the volume, and a number of other languages: Albanian (Peršikov 1965), Armenian (Melkumjan 1965), Bulgarian (Fedulova 1965), Czech (Ožigova 1965), English (Malahovskij 1965), Estonian (Hol’m 1965), French (Kordi 1965), German (Fitialova 1965), Hausa (Fihman 1965a), Hungarian (Andreev 1965a), Latvian (Jakubajtis 1965), Serbo-Croatian (Panina 1965), Swahili (Fihman 1965b), Ukrainian (Eliseeva 1965), and Vietnamese (Jakuševa 1965). As an aside, we may note that only after the turn of the millennium are we again seeing this variety of languages in ULM work. Most of these studies are small-scale proof-of-concept experiments on corpora of varying sizes (from a few thousand words in many of the studies up to close to one million words for Rus</context>
<context position="35218" citStr="Ožigova 1965" startWordPosition="5419" endWordPosition="5420">ories and Separation: In this family of methods, the phonemes (approximated by graphemes) are first classed into categories, foremostly, vowel versus consonant. Thereafter, each word is separated into its vowel skeleton and its consonant skeleton, after which various 318 Hammarström and Borin Unsupervised Learning of Morphology Table 2 Very brief roadmap of earlier studies. Model Superv. Experimentation Learns what? Harris 1955, 1968, 1970 C T English Segmentation Andreev 1965a, Andreev 1967, C T Vietnamese to Segmentation Chapter 2, Peršikov 1965; Hungarian (I) Melkumjan 1965; Fedulova 1965; Ožigova 1965; Malahovskij 1965; Hol’m 1965; Kordi 1965; Fitialova 1965; Fihman 1965a; Andreev 1965a; Jakubajtis 1965; Panina 1965; Fihman 1965b; Eliseeva 1965; Jakuševa 1965 Gammon 1969 C T English Segmentation Lehmann 1973, pages 71–93 C T French (I) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segm</context>
</contexts>
<marker>Ožigova, 1965</marker>
<rawString>Ožigova, G. I. 1965. Statistiko-kombinatornoe modelirovanie paradigmy pervogo morfologiˇceskogo tipa v ˇcešskom jazyke. In Nikolaj Dmitrieviˇc Andreev, editor, Statistiko-kombinatornoe modelirovanie jazykov. Nauka, Leningrad, pages 89–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amaresh Kumar Pandey</author>
<author>Tanveer J Siddiqui</author>
</authors>
<title>An unsupervised Hindi stemmer with heuristic improvements.</title>
<date>2008</date>
<booktitle>In AND ’08: Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data,</booktitle>
<pages>99--105</pages>
<location>New York, NY.</location>
<contexts>
<context position="40522" citStr="Pandey and Siddiqui 2008" startWordPosition="6201" endWordPosition="6204">tion Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al. 2007; C T French/Bengali/French/ Analysis Majumder, Mitra, and Pal Bulgar2008 ian/Hungarian Zeman 2008, 2009 C - Czech/English/German/ Segmentation+ Finnish Paradigms Kohonen, Virpioja, and Klami C T Finnish/Turkish/English Segmentation 2008 320 Hammarström and Borin Unsupervised Learning of Morphology Table 2 (continued) Model Superv. Experimentation Learns what? Goodman 2008 C T Finnish/Turkish/English Segmentation Golénia 2008 C T Turkish/Russian Segmentation Pandey and Siddiqui 2008 C T Hindi Segmentation+ Paradigms Johnson 2008 C T Sesotho Segmentation Snyder and Barzilay 2008 C/NC T Hebrew/Arabic/Aramaic/ Segmentation English Spiegler et al. 2008 C T Zulu Segmentation Moon, Erk, and Baldridge 2009 C T English/Uspanteko Segmentation Poon, Cherry, and Toutanova C T Arabic/Hebrew Segmentation 2009 Abbreviations: C = Concatenative; I = Impressionistic evaluation; IR = Evaluation only in terms of Information Retrieval Performance; NC = Non-concatenative; RR = Hand-written rewrite rules; SP = Some manually curated segmentation points; T = Thresholds and Parameters to be set </context>
</contexts>
<marker>Pandey, Siddiqui, 2008</marker>
<rawString>Pandey, Amaresh Kumar and Tanveer J. Siddiqui. 2008. An unsupervised Hindi stemmer with heuristic improvements. In AND ’08: Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data, pages 99–105, New York, NY.</rawString>
</citation>
<citation valid="false">
<authors>
<author>N A Panina</author>
</authors>
<title>1965.Opyt statistikokombinatornogo vydelenija paradigmy pervogo morfologiˇceskogo tipa v fserbohorvatskom jazyke.</title>
<booktitle>In Nikolaj Dmitrieviˇc Andreev, editor, Statistiko-kombinatornoe modelirovanie jazykov. Nauka,</booktitle>
<pages>104--109</pages>
<location>Leningrad,</location>
<marker>Panina, </marker>
<rawString>Panina, N. A. 1965.Opyt statistikokombinatornogo vydelenija paradigmy pervogo morfologiˇceskogo tipa v fserbohorvatskom jazyke. In Nikolaj Dmitrieviˇc Andreev, editor, Statistiko-kombinatornoe modelirovanie jazykov. Nauka, Leningrad, pages 104–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V F Peršikov</author>
</authors>
<title>Iz opyta statistikokombinatornogo modelirovanija albanskoj morfologii.</title>
<date>1965</date>
<booktitle>In Nikolaj Dmitrieviˇc Andreev, editor, Statistiko-kombinatornoe modelirovanie jazykov. Nauka,</booktitle>
<pages>181--188</pages>
<location>Leningrad,</location>
<contexts>
<context position="19321" citStr="Peršikov 1965" startWordPosition="2945" endWordPosition="2946">rpus appearing with all the suffix candidates. In the Russian experiment reported by Andreeva (1963), a complete adjective paradigm was induced, with 12 different suffixes. The initial suffix candidate, -oj, has a high functional load and consequently a high text frequency: It is the most ambiguous of the Russian adjective suffixes, appearing in four different slots in the adjective paradigm, and is also homonymous with a noun suffix. In Andreev (1965b) the method is tested extensively on Russian, which is the subject of several papers in the volume, and a number of other languages: Albanian (Peršikov 1965), Armenian (Melkumjan 1965), Bulgarian (Fedulova 1965), Czech (Ožigova 1965), English (Malahovskij 1965), Estonian (Hol’m 1965), French (Kordi 1965), German (Fitialova 1965), Hausa (Fihman 1965a), Hungarian (Andreev 1965a), Latvian (Jakubajtis 1965), Serbo-Croatian (Panina 1965), Swahili (Fihman 1965b), Ukrainian (Eliseeva 1965), and Vietnamese (Jakuševa 1965). As an aside, we may note that only after the turn of the millennium are we again seeing this variety of languages in ULM work. Most of these studies are small-scale proof-of-concept experiments on corpora of varying sizes (from a few th</context>
<context position="35159" citStr="Peršikov 1965" startWordPosition="5411" endWordPosition="5412">h they are related, such as an affix. (d) Phonological Categories and Separation: In this family of methods, the phonemes (approximated by graphemes) are first classed into categories, foremostly, vowel versus consonant. Thereafter, each word is separated into its vowel skeleton and its consonant skeleton, after which various 318 Hammarström and Borin Unsupervised Learning of Morphology Table 2 Very brief roadmap of earlier studies. Model Superv. Experimentation Learns what? Harris 1955, 1968, 1970 C T English Segmentation Andreev 1965a, Andreev 1967, C T Vietnamese to Segmentation Chapter 2, Peršikov 1965; Hungarian (I) Melkumjan 1965; Fedulova 1965; Ožigova 1965; Malahovskij 1965; Hol’m 1965; Kordi 1965; Fitialova 1965; Fihman 1965a; Andreev 1965a; Jakubajtis 1965; Panina 1965; Fihman 1965b; Eliseeva 1965; Jakuševa 1965 Gammon 1969 C T English Segmentation Lehmann 1973, pages 71–93 C T French (I) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T En</context>
</contexts>
<marker>Peršikov, 1965</marker>
<rawString>Peršikov, V. F. 1965. Iz opyta statistikokombinatornogo modelirovanija albanskoj morfologii. In Nikolaj Dmitrieviˇc Andreev, editor, Statistiko-kombinatornoe modelirovanie jazykov. Nauka, Leningrad, pages 181–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malin Petzell</author>
</authors>
<title>A lingustic description of Kagulu.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Göteborgs Universitet.</institution>
<contexts>
<context position="9485" citStr="Petzell 2007" startWordPosition="1417" endWordPosition="1418">ut-of-vocabulary words,” namely, approaches where an existing morphological analysis component is used as the basis for guessing in which existing paradigm an unknown text word should belong (e.g., Antworth 1990; Mikheev 1997; Bharati et al. 2001; Forsberg, Hammarström, and Ranta 2006; Lindén 2008; Lindén 2009). One of the matters that varies the most between different authors is the desired outcome. It is useful to set up the implicational hierarchy shown in Table 1 (which 4 There are, however, rare cases of languages which allow the permutation of specific pairs of prefixes, such as Kagulu (Petzell 2007), Yimas and Karawari (Foley 1991, pages 31–32) as well as Chintang, Bantawa, and possibly other Kiranti languages where prefix ordering in general is very free (Rai 1984; Bickel et al. 2007). 5 Clitics are affix-like elements that attach to words in particular syntactic positions, rather than to words of particular categories as proper affixes do. The English genitive -’s is sometimes classified as a clitic, because you can say things like The girl I met yesterday’s purse (the -’s attaches to the end of the noun phrase, regardless of the part of speech of the last word, an adverb in this case)</context>
</contexts>
<marker>Petzell, 2007</marker>
<rawString>Petzell, Malin. 2007. A lingustic description of Kagulu. Ph.D. thesis, Göteborgs Universitet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vito Pirrelli</author>
<author>Basilio Calderone</author>
<author>Ivan Herreros</author>
<author>Michele Virgilio</author>
</authors>
<title>Non-locality all the way through: Emergent global constraints in the Italian morphological lexicon.</title>
<date>2004</date>
<booktitle>In SIGPHON 2004: Proceedings of the Seventh Meeting of the ACL Special Interest Group in Computational Phonology,</booktitle>
<pages>52--61</pages>
<location>Barcelona.</location>
<contexts>
<context position="37833" citStr="Pirrelli et al. 2004" startWordPosition="5820" endWordPosition="5823">nnish/Turkish/ Segmentation Lagus 2002, 2004, 2005a, 2005b, English 2005c, 2007; Creutz, Lagus, and Virpioja 2005; Hirsimäki et al. 2003; Creutz et al. 2005 319 Computational Linguistics Volume 37, Number 2 Table 2 (continued) Model Superv. Experimentation Learns what? Kontorovich, Don, and Singer 2003 C T English Segmentation Medina Urrea and Díaz 2003; C T Chuj/Ralámuri/Czech Affix List Medina-Urrea 2006a, 2008 Mayfield and McNamee 2003; - -8 West European Same-stem McNamee and Mayfield 2007 languages (IR) Zweigenbaum, Hadouche, and C T Medical French Segmentation Grabar 2003; Hadouche 2002 Pirrelli et al. 2004; Pirrelli and C T Italian/English/Arabic Unclear Herreros 2007; Calderone 2008 Johnson and Martin 2003b C T Inuktitut Unclear Katrenko 2004 C T Ukrainian Lexicon+ Paradigms ´Cavar et al. 2004a, 2004b; ´Cavar, C T Child-English Unclear Rodrigues, and Schrementi 2006; ´Cavar et al. 2006 Rodrigues and ´Cavar 2005, 2007 NC T Arabic Segmentation Monson 2004, 2009; Monson et al. C T English/Spanish/ Segmentation 2004, 2007a, 2007b, 2008, 2008a, Mapudungun (I) 2008b Yarowsky and Wicentowski 2000; C/NC T 30-ish mostly European Segmentation + Wicentowski 2002, 2004 type languages Rewrite Rules Gelbukh</context>
</contexts>
<marker>Pirrelli, Calderone, Herreros, Virgilio, 2004</marker>
<rawString>Pirrelli, Vito, Basilio Calderone, Ivan Herreros, and Michele Virgilio. 2004. Non-locality all the way through: Emergent global constraints in the Italian morphological lexicon. In SIGPHON 2004: Proceedings of the Seventh Meeting of the ACL Special Interest Group in Computational Phonology, pages 52–61, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vito Pirrelli</author>
<author>Ivan Herreros</author>
</authors>
<title>Learning morphology by itself.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fifth Mediterranean Morphology Meeting (MMM5),</booktitle>
<pages>269--290</pages>
<location>Fréjus.</location>
<marker>Pirrelli, Herreros, 2007</marker>
<rawString>Pirrelli, Vito and Ivan Herreros. 2007. Learning morphology by itself. In Proceedings of the Fifth Mediterranean Morphology Meeting (MMM5), pages 269–290, Fréjus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised Computational Linguistics Volume 37, Number 2 morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL ’09: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>209--217</pages>
<location>Morristown, NJ.</location>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>Poon, Hoifung, Colin Cherry, and Kristina Toutanova. 2009. Unsupervised Computational Linguistics Volume 37, Number 2 morphological segmentation with log-linear models. In Proceedings of NAACL ’09: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 209–217, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M W Powers</author>
</authors>
<title>Reconciliation of unsupervised clustering, segmentation and cohesion.</title>
<date>1998</date>
<booktitle>In NeMLaP3/CoNLL ’98 Workshop on Paradigms and Grounding in Language Learning,</booktitle>
<pages>307--310</pages>
<location>Sydney,</location>
<contexts>
<context position="12198" citStr="Powers (1998)" startWordPosition="1868" endWordPosition="1869">f two words are of the same stem (if meaning is disregarded) or the same lexeme (if meaning is taken into account). The converse need not hold; it is perfectly possible to answer the question of whether two words are of the same stem with high accuracy, without having to commit to what the actual stem should be. Many recent articles fail to deal properly with previous and related work, some reinvent heuristics that have been proposed earlier, and there is little modularization taking place. Previous surveys and overviews for a general audience are Borin (1991), Batchelder (1997, pages 66–68), Powers (1998), Clark (2001, pages 80–82), Xanthos (2007, pages 95–107), Goldsmith (2001), Daelemans (2004, page 1898), Roark and Sproat (2007, pages 116–136), Hammarström (2007b, pages 10–15), Chan (2008, pages 48–60), Hammarström (2009b, pages 14–21), Borin (2009), Goldsmith (2010), and, to a more limited extent, the related-work sections of individual research papers. Kurimo, Creutz, and Turunen (2007), Kurimo, Creutz, and Varjokallio (2008a, 2008b), Kurimo and Turunen (2008), Kurimo and Varjokallio (2008), and McNamee (2008) are overviews of systems in the MorphoChallenge of the respective year. However</context>
</contexts>
<marker>Powers, 1998</marker>
<rawString>Powers, David M. W. 1998. Reconciliation of unsupervised clustering, segmentation and cohesion. In NeMLaP3/CoNLL ’98 Workshop on Paradigms and Grounding in Language Learning, Sydney, pages 307–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Novel Kishore Rai</author>
</authors>
<title>A Descriptive Study of Bantawa.</title>
<date>1984</date>
<tech>Ph.D. thesis,</tech>
<institution>Poona University.</institution>
<contexts>
<context position="9654" citStr="Rai 1984" startWordPosition="1444" endWordPosition="1445">rd should belong (e.g., Antworth 1990; Mikheev 1997; Bharati et al. 2001; Forsberg, Hammarström, and Ranta 2006; Lindén 2008; Lindén 2009). One of the matters that varies the most between different authors is the desired outcome. It is useful to set up the implicational hierarchy shown in Table 1 (which 4 There are, however, rare cases of languages which allow the permutation of specific pairs of prefixes, such as Kagulu (Petzell 2007), Yimas and Karawari (Foley 1991, pages 31–32) as well as Chintang, Bantawa, and possibly other Kiranti languages where prefix ordering in general is very free (Rai 1984; Bickel et al. 2007). 5 Clitics are affix-like elements that attach to words in particular syntactic positions, rather than to words of particular categories as proper affixes do. The English genitive -’s is sometimes classified as a clitic, because you can say things like The girl I met yesterday’s purse (the -’s attaches to the end of the noun phrase, regardless of the part of speech of the last word, an adverb in this case). This could not happen with an inflectional suffix like the plural -s: *The girl I met yesterdays cannot mean The girls I met yesterday. 311 Computational Linguistics V</context>
</contexts>
<marker>Rai, 1984</marker>
<rawString>Rai, Novel Kishore. 1984. A Descriptive Study of Bantawa. Ph.D. thesis, Poona University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Norman Redlich</author>
</authors>
<title>Redundancy reduction as a strategy for unsupervised learning.</title>
<date>1993</date>
<journal>Neural Computation,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="35752" citStr="Redlich 1993" startWordPosition="5502" endWordPosition="5503">er 2, Peršikov 1965; Hungarian (I) Melkumjan 1965; Fedulova 1965; Ožigova 1965; Malahovskij 1965; Hol’m 1965; Kordi 1965; Fitialova 1965; Fihman 1965a; Andreev 1965a; Jakubajtis 1965; Panina 1965; Fihman 1965b; Eliseeva 1965; Jakuševa 1965 Gammon 1969 C T English Segmentation Lehmann 1973, pages 71–93 C T French (I) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1992, 1994, 1995 C T+SP Spanish Segmentation Janßen 1992 C T+SP French Segmentation Juola, Hall, and Boggs 1994 C T English Segmentation Brent 1993, 1999; Brent, Murthy, C T English/Child- Segmentation and Lundberg 1995; Snover 2002; English/Polish/ Snover, Jarosz, and Brent 2002; French Snover and Brent 2001, 2003 Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation 1996 Yvon 1996 C T French (I) Segmentation Kazakov 1997; Kazakov and C T French/English Segmentation Manandhar 1998, 2001 Jacquemin</context>
</contexts>
<marker>Redlich, 1993</marker>
<rawString>Redlich, A. Norman. 1993. Redundancy reduction as a strategy for unsupervised learning. Neural Computation, 5(2):289–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>Last words: The shrinking horizons of computational linguistics.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="28471" citStr="Reiter 2007" startWordPosition="4397" endWordPosition="4398">remainder—languages with a tradition of writing—are not on the whole small language communities; in the first instance, we are talking about the few hundred most spoken languages in the world, for example, the 313 languages with at least one million native speakers (accounting for about 80% of the world’s population) surveyed by the Linguistic Data Consortium some years back in their Low-density language survey (Strassel, Maxwell, and Cieri 2003; Borin 2009). 8 Another pragmatic, less savory reason is a general downplaying of linguistic knowledge in the language technology research community (Reiter 2007). 316 Hammarström and Borin Unsupervised Learning of Morphology The hope is often expressed in the literature that ULM and other unsupervised methods could be employed in order to rapidly and cheaply (in terms of human effort) bootstrap basic language technology resources for new languages. It should be noted that, even for larger languages, because of the human effort needed to build computational morphological resources, many such implementations are not released to the public domain. Also, open domain texts will always contain a fair share of (inflected) previously unknown words that are no</context>
</contexts>
<marker>Reiter, 2007</marker>
<rawString>Reiter, Ehud. 2007. Last words: The shrinking horizons of computational linguistics. Computational Linguistics, 33(2):283–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>Richard W Sproat</author>
</authors>
<title>Machine learning of morphology.</title>
<date>2007</date>
<booktitle>In Computational Approaches to Morphology and Syntax,</booktitle>
<volume>4</volume>
<pages>116--136</pages>
<publisher>Oxford University Press,</publisher>
<location>Oxford,</location>
<contexts>
<context position="12326" citStr="Roark and Sproat (2007" startWordPosition="1884" endWordPosition="1887"> converse need not hold; it is perfectly possible to answer the question of whether two words are of the same stem with high accuracy, without having to commit to what the actual stem should be. Many recent articles fail to deal properly with previous and related work, some reinvent heuristics that have been proposed earlier, and there is little modularization taking place. Previous surveys and overviews for a general audience are Borin (1991), Batchelder (1997, pages 66–68), Powers (1998), Clark (2001, pages 80–82), Xanthos (2007, pages 95–107), Goldsmith (2001), Daelemans (2004, page 1898), Roark and Sproat (2007, pages 116–136), Hammarström (2007b, pages 10–15), Chan (2008, pages 48–60), Hammarström (2009b, pages 14–21), Borin (2009), Goldsmith (2010), and, to a more limited extent, the related-work sections of individual research papers. Kurimo, Creutz, and Turunen (2007), Kurimo, Creutz, and Varjokallio (2008a, 2008b), Kurimo and Turunen (2008), Kurimo and Varjokallio (2008), and McNamee (2008) are overviews of systems in the MorphoChallenge of the respective year. However, we will try to be more comprehensive than previous surveys and discuss the ideas in the field critically. 312 Hammarström and </context>
</contexts>
<marker>Roark, Sproat, 2007</marker>
<rawString>Roark, B. and Richard W. Sproat. 2007. Machine learning of morphology. In Computational Approaches to Morphology and Syntax, volume 4 of Oxford Surveys in Syntax and Morphology. Oxford University Press, Oxford, pages 116–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Rodrigues</author>
<author>Damir ´Cavar</author>
</authors>
<title>Learning Arabic morphology using information theory.</title>
<date>2005</date>
<booktitle>In The Panels 2005: Proceedings from the Annual Meeting of the Chicago Linguistic Society,</booktitle>
<volume>41</volume>
<pages>49--58</pages>
<location>Chicago, IL.</location>
<marker>Rodrigues, ´Cavar, 2005</marker>
<rawString>Rodrigues, Paul and Damir ´Cavar. 2005. Learning Arabic morphology using information theory. In The Panels 2005: Proceedings from the Annual Meeting of the Chicago Linguistic Society, volume 41–2, pages 49–58, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Rodrigues</author>
<author>Damir ´Cavar</author>
</authors>
<title>Learning Arabic morphology using statistical constraint-satisfaction models.</title>
<date>2007</date>
<booktitle>In Elabbas Benmamoun, editor, Perspectives on Arabic Linguistics: Papers from the Annual Symposium on Arabic Linguistics,</booktitle>
<pages>63--75</pages>
<location>Urbana, IL.</location>
<marker>Rodrigues, ´Cavar, 2007</marker>
<rawString>Rodrigues, Paul and Damir ´Cavar. 2007. Learning Arabic morphology using statistical constraint-satisfaction models. In Elabbas Benmamoun, editor, Perspectives on Arabic Linguistics: Papers from the Annual Symposium on Arabic Linguistics, pages 63–75, Urbana, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monica Rogati</author>
<author>Scott McCarley</author>
<author>Yiming Yang</author>
</authors>
<title>Unsupervised learning of Arabic stemming using a parallel corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<pages>391--398</pages>
<location>Sapporo.</location>
<marker>Rogati, McCarley, Yang, 2003</marker>
<rawString>Rogati, Monica, Scott McCarley, and Yiming Yang. 2003. Unsupervised learning of Arabic stemming using a parallel corpus. In Proceedings of the 41st Annual Meeting of the ACL, pages 391–398, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anju Saxena</author>
<author>Lars Borin</author>
<author>editors</author>
</authors>
<date>2006</date>
<booktitle>Lesser-Known Languages of South Asia: Status and Policies, Case Studies and Applications of Information Technology. Mouton de Gruyter,</booktitle>
<location>Berlin.</location>
<marker>Saxena, Borin, editors, 2006</marker>
<rawString>Saxena, Anju and Lars Borin, editors. 2006. Lesser-Known Languages of South Asia: Status and Policies, Case Studies and Applications of Information Technology. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
</authors>
<title>Toward Knowledge-Free Induction of Machine-Readable Dictionaries.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Colorado.</institution>
<contexts>
<context position="33481" citStr="Schone 2001" startWordPosition="5158" endWordPosition="5159">f some kind, where frequent long substrings with clear borders offer the optimal compression gain. The outcome of such a compression scheme gives the segmentation. In addition, for those approaches which also target paradigms, stem–suffix co-occurrence statistics are gathered given the segmentation produced, rather than all possible segmentations. (b) Group and Abstract: In this family of methods, morphologically related words are first grouped (clustered into sets, paired, shortlisted, etc.) according to some metric, which is typically string edit distance, but may include semantic features (Schone 2001), distributional similarity (Freitag 2005), or frequency signatures (Wicentowski 2002). The next step is to abstract some morphological pattern that recurs among the groups. Such emergent patterns provide enough clues for segmentation and can sometimes be formulated as rules or morphological paradigms. (c) Features and Classes: In this family of methods, a word is seen as made up of a set of features—n-grams in Mayfield and McNamee (2003) and McNamee and Mayfield (2007), and initial/terminal/mid-substring in De Pauw and Wagacha (2007). Features which occur on many words have little selective p</context>
<context position="36689" citStr="Schone 2001" startWordPosition="5645" endWordPosition="5646">olish/ Snover, Jarosz, and Brent 2002; French Snover and Brent 2001, 2003 Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation 1996 Yvon 1996 C T French (I) Segmentation Kazakov 1997; Kazakov and C T French/English Segmentation Manandhar 1998, 2001 Jacquemin 1997 C T English Segmentation Cromm 1997 C T German Segmentation Gaussier 1999 C T French/English (I) Lexicon+ Paradigms Déjean 1998a, 1998b C T Turkish/English/ Affix Lists Korean/French/ Swahili/ Vietnamese (I) Medina Urrea 2000, 2003, 2006b C T Spanish Affix List Schone and Jurafsky 2000, 2001a; C T English Segmentation Schone 2001 Goldsmith 2000, 2001, 2006; Belkin C T English (I) Lexicon+ Paradigms and Goldsmith 2002; Goldsmith, Higgins, and Soglasnova 2001; Hu et al. 2005b; Xanthos, Hu, and Goldsmith 2006 Baroni 2000, 2003 C T Child-English/ Affix List English Cho and Han 2002 C T Korean Segmentation Sharma, Kalita, and Das 2002, 2003; C T Assamese Lexicon+ Paradigms Sharma and Das 2002 Baroni, Matiasek, and Trost 2002 C/NC T English/German (I) Related word pairs Bati 2002 C/NC T Amharic Lexicon+ Paradigms Creutz 2003, 2006; Creutz and C T Finnish/Turkish/ Segmentation Lagus 2002, 2004, 2005a, 2005b, English 2005c, 2</context>
<context position="65194" citStr="Schone 2001" startWordPosition="10311" endWordPosition="10312">sensitive to continuous segments. String edit distance is the most straightforward metric for which to find pairs or sets of morphologically related words (see, e.g., Gaussier 1999; Yarowsky and Wicentowski 2000; Schone and Jurafsky 2001a; Baroni, Matiasek, and Trost 2002; Hu et al. 2005a; Bernhard 2006, pages 101–117; Bernhard 2007; Majumder et al. 2007; Majumder, Mitra, and Pal 2008). In addition, as unsupervised methods for semantic clustering (e.g., Latent Semantic Analysis) and distributional clustering became more mature, these could be included as well (Schone and Jurafsky 2000, 2001a; Schone 2001; Baroni, Matiasek, and Trost 2002; Freitag 2005). More remarkable, however, is that Yarowsky and Wicentowski (2000) and Wicentowski (2002, 2004) have shown that frequency signatures can also be used to (heuristically) find morphologically related words. The example they use is sang versus sing, whose relative frequency distribution in a corpus is 1,427/1,204 (or 1.19/1), whereas singed15 versus sing is 9/1,204 (Yarowsky and Wicentowski 2000, pages 209– 210). This way, sing can be heuristically said to be parallel to sang rather than singed, and indeed the distribution for singed versus singe </context>
<context position="77166" citStr="Schone 2001" startWordPosition="12238" endWordPosition="12239">r the (a) methods. There is also the possibility of serial combination where, for example, the (a) methods target concatenative morphology and the (b)—or (c)—methods attempt the remaining cases. Presumably because most methods so far do not produce a clean, well-defined result, various forms of hybridization of techniques by different authors have yet to be systematically explored. Lastly, there are scattered attempts to address morphophonological changes in a principled way, though so far these have been developed in close connection with a particular segmentation method and target language (Schone 2001; Schone and Jurafsky 2001a; Wicentowski 2002, 2004; Tepper 2007; Kohonen, Virpioja, and Klami 2008; Tepper and Xia 2008). 4. Discussion 4.1 Language Dependence of ULM As we mentioned in Section 3.6, most approaches have an explicit bias towards certain kinds of morphological systems, those for which we introduced the label “one-slot morphology.” This is of course not a problem, if the purpose is to bootstrap a morphology for some languages which happen to belong to the right type. If the purpose is to say something about human language acquisition or language learning, or if the aim is to 332</context>
</contexts>
<marker>Schone, 2001</marker>
<rawString>Schone, Patrick. 2001. Toward Knowledge-Free Induction of Machine-Readable Dictionaries. Ph.D. thesis, University of Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Knowledge-free induction of inflectional morphologies using latent semantic analysis.</title>
<date>2000</date>
<booktitle>In Conference on Natural Language Learning</booktitle>
<pages>67--72</pages>
<location>Lisbon.</location>
<contexts>
<context position="36644" citStr="Schone and Jurafsky 2000" startWordPosition="5636" endWordPosition="5639">ld- Segmentation and Lundberg 1995; Snover 2002; English/Polish/ Snover, Jarosz, and Brent 2002; French Snover and Brent 2001, 2003 Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation 1996 Yvon 1996 C T French (I) Segmentation Kazakov 1997; Kazakov and C T French/English Segmentation Manandhar 1998, 2001 Jacquemin 1997 C T English Segmentation Cromm 1997 C T German Segmentation Gaussier 1999 C T French/English (I) Lexicon+ Paradigms Déjean 1998a, 1998b C T Turkish/English/ Affix Lists Korean/French/ Swahili/ Vietnamese (I) Medina Urrea 2000, 2003, 2006b C T Spanish Affix List Schone and Jurafsky 2000, 2001a; C T English Segmentation Schone 2001 Goldsmith 2000, 2001, 2006; Belkin C T English (I) Lexicon+ Paradigms and Goldsmith 2002; Goldsmith, Higgins, and Soglasnova 2001; Hu et al. 2005b; Xanthos, Hu, and Goldsmith 2006 Baroni 2000, 2003 C T Child-English/ Affix List English Cho and Han 2002 C T Korean Segmentation Sharma, Kalita, and Das 2002, 2003; C T Assamese Lexicon+ Paradigms Sharma and Das 2002 Baroni, Matiasek, and Trost 2002 C/NC T English/German (I) Related word pairs Bati 2002 C/NC T Amharic Lexicon+ Paradigms Creutz 2003, 2006; Creutz and C T Finnish/Turkish/ Segmentation Lag</context>
<context position="65174" citStr="Schone and Jurafsky 2000" startWordPosition="10306" endWordPosition="10309">e grouping methods are much less sensitive to continuous segments. String edit distance is the most straightforward metric for which to find pairs or sets of morphologically related words (see, e.g., Gaussier 1999; Yarowsky and Wicentowski 2000; Schone and Jurafsky 2001a; Baroni, Matiasek, and Trost 2002; Hu et al. 2005a; Bernhard 2006, pages 101–117; Bernhard 2007; Majumder et al. 2007; Majumder, Mitra, and Pal 2008). In addition, as unsupervised methods for semantic clustering (e.g., Latent Semantic Analysis) and distributional clustering became more mature, these could be included as well (Schone and Jurafsky 2000, 2001a; Schone 2001; Baroni, Matiasek, and Trost 2002; Freitag 2005). More remarkable, however, is that Yarowsky and Wicentowski (2000) and Wicentowski (2002, 2004) have shown that frequency signatures can also be used to (heuristically) find morphologically related words. The example they use is sang versus sing, whose relative frequency distribution in a corpus is 1,427/1,204 (or 1.19/1), whereas singed15 versus sing is 9/1,204 (Yarowsky and Wicentowski 2000, pages 209– 210). This way, sing can be heuristically said to be parallel to sang rather than singed, and indeed the distribution for </context>
</contexts>
<marker>Schone, Jurafsky, 2000</marker>
<rawString>Schone, Patrick and Daniel Jurafsky. 2000. Knowledge-free induction of inflectional morphologies using latent semantic analysis. In Conference on Natural Language Learning 2000 (CoNLL-2000), pages 67–72, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Knowledge-free induction of inflectional morphologies.</title>
<date>2001</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>183--191</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="64820" citStr="Schone and Jurafsky 2001" startWordPosition="10254" endWordPosition="10257">articular Latent Dirichlet Allocation, to break the full matrix into smaller dense submatrices, which, when multiplied together, resemble the full matrix. There is only one humanly tuned threshold, namely, when to stop breaking into smaller parts. 3.3 Group and Abstract In contrast to the methods that use a heuristic for finding morpheme boundaries, the grouping methods are much less sensitive to continuous segments. String edit distance is the most straightforward metric for which to find pairs or sets of morphologically related words (see, e.g., Gaussier 1999; Yarowsky and Wicentowski 2000; Schone and Jurafsky 2001a; Baroni, Matiasek, and Trost 2002; Hu et al. 2005a; Bernhard 2006, pages 101–117; Bernhard 2007; Majumder et al. 2007; Majumder, Mitra, and Pal 2008). In addition, as unsupervised methods for semantic clustering (e.g., Latent Semantic Analysis) and distributional clustering became more mature, these could be included as well (Schone and Jurafsky 2000, 2001a; Schone 2001; Baroni, Matiasek, and Trost 2002; Freitag 2005). More remarkable, however, is that Yarowsky and Wicentowski (2000) and Wicentowski (2002, 2004) have shown that frequency signatures can also be used to (heuristically) find mo</context>
<context position="77192" citStr="Schone and Jurafsky 2001" startWordPosition="12240" endWordPosition="12243">hods. There is also the possibility of serial combination where, for example, the (a) methods target concatenative morphology and the (b)—or (c)—methods attempt the remaining cases. Presumably because most methods so far do not produce a clean, well-defined result, various forms of hybridization of techniques by different authors have yet to be systematically explored. Lastly, there are scattered attempts to address morphophonological changes in a principled way, though so far these have been developed in close connection with a particular segmentation method and target language (Schone 2001; Schone and Jurafsky 2001a; Wicentowski 2002, 2004; Tepper 2007; Kohonen, Virpioja, and Klami 2008; Tepper and Xia 2008). 4. Discussion 4.1 Language Dependence of ULM As we mentioned in Section 3.6, most approaches have an explicit bias towards certain kinds of morphological systems, those for which we introduced the label “one-slot morphology.” This is of course not a problem, if the purpose is to bootstrap a morphology for some languages which happen to belong to the right type. If the purpose is to say something about human language acquisition or language learning, or if the aim is to 332 Hammarström and Borin Uns</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>Schone, Patrick and Daniel Jurafsky. 2001a. Knowledge-free induction of inflectional morphologies. In Proceedings of the North American Chapter of the Association for Computational Linguistics, pages 183–191, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Language-independent induction of part of speech class labels using only language universals.</title>
<date>2001</date>
<booktitle>In &amp;quot;Machine Learning: Beyond Supervision,&amp;quot; Workshop at IJCAI-2001,</booktitle>
<pages>53--60</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="64820" citStr="Schone and Jurafsky 2001" startWordPosition="10254" endWordPosition="10257">articular Latent Dirichlet Allocation, to break the full matrix into smaller dense submatrices, which, when multiplied together, resemble the full matrix. There is only one humanly tuned threshold, namely, when to stop breaking into smaller parts. 3.3 Group and Abstract In contrast to the methods that use a heuristic for finding morpheme boundaries, the grouping methods are much less sensitive to continuous segments. String edit distance is the most straightforward metric for which to find pairs or sets of morphologically related words (see, e.g., Gaussier 1999; Yarowsky and Wicentowski 2000; Schone and Jurafsky 2001a; Baroni, Matiasek, and Trost 2002; Hu et al. 2005a; Bernhard 2006, pages 101–117; Bernhard 2007; Majumder et al. 2007; Majumder, Mitra, and Pal 2008). In addition, as unsupervised methods for semantic clustering (e.g., Latent Semantic Analysis) and distributional clustering became more mature, these could be included as well (Schone and Jurafsky 2000, 2001a; Schone 2001; Baroni, Matiasek, and Trost 2002; Freitag 2005). More remarkable, however, is that Yarowsky and Wicentowski (2000) and Wicentowski (2002, 2004) have shown that frequency signatures can also be used to (heuristically) find mo</context>
<context position="77192" citStr="Schone and Jurafsky 2001" startWordPosition="12240" endWordPosition="12243">hods. There is also the possibility of serial combination where, for example, the (a) methods target concatenative morphology and the (b)—or (c)—methods attempt the remaining cases. Presumably because most methods so far do not produce a clean, well-defined result, various forms of hybridization of techniques by different authors have yet to be systematically explored. Lastly, there are scattered attempts to address morphophonological changes in a principled way, though so far these have been developed in close connection with a particular segmentation method and target language (Schone 2001; Schone and Jurafsky 2001a; Wicentowski 2002, 2004; Tepper 2007; Kohonen, Virpioja, and Klami 2008; Tepper and Xia 2008). 4. Discussion 4.1 Language Dependence of ULM As we mentioned in Section 3.6, most approaches have an explicit bias towards certain kinds of morphological systems, those for which we introduced the label “one-slot morphology.” This is of course not a problem, if the purpose is to bootstrap a morphology for some languages which happen to belong to the right type. If the purpose is to say something about human language acquisition or language learning, or if the aim is to 332 Hammarström and Borin Uns</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>Schone, Patrick and Daniel Jurafsky. 2001b. Language-independent induction of part of speech class labels using only language universals. In &amp;quot;Machine Learning: Beyond Supervision,&amp;quot; Workshop at IJCAI-2001, pages 53–60, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siriwan Sereewattana</author>
</authors>
<title>Unsupervised segmentation for statistical machine translation. Master’s thesis,</title>
<date>2003</date>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="85919" citStr="Sereewattana 2003" startWordPosition="13619" endWordPosition="13620">aken part in the MorphoChallenge. Taking MorphoChallenge results and proof-ofconcept reports together, it seems that high accuracy by ULM systems is presently only achievable if the language has small amounts of one-slot concatenative morphology, whereas for morphologically more complex languages, parameter tuning and/or lower accuracy is to be expected. We are not yet in a position to assess whether there are other tasks than IR which, in general, benefit significantly from (noisy) ULM, such as Speech Recognition (Hirsimäki et al. 2003, 2005, 2006; Kurimo et al. 2006) or Machine Translation (Sereewattana 2003; Virpioja et al. 2007; Bojar, Straˇnák, and Zeman 2008; Kirik and Fishel 2008; De Gispert et al. 2009; Fishel and Kirik 2010) because almost only the Morfessor system has been tested, and results are, if positive, not completely unambiguous. One usage of noisy ULM, at least, is for smoothing language identification models (Hammarström 2007a; Ceylan and Kim 2009). Further, ULM approaches are data-hungry, which precludes their use with many low-density languages. There is much ongoing work addressing these issues, however, so we can probably expect some progress in this area (Bird 2009). 4.4 Fu</context>
</contexts>
<marker>Sereewattana, 2003</marker>
<rawString>Sereewattana, Siriwan. 2003. Unsupervised segmentation for statistical machine translation. Master’s thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Utpal Sharma</author>
<author>Rajib Das</author>
</authors>
<title>Classification of words based on affix evidence.</title>
<date>2002</date>
<booktitle>In International Conference on Natural Language Processing, ICON-2002,</booktitle>
<pages>31--39</pages>
<location>Mumbai.</location>
<contexts>
<context position="37054" citStr="Sharma and Das 2002" startWordPosition="5703" endWordPosition="5706"> French/English (I) Lexicon+ Paradigms Déjean 1998a, 1998b C T Turkish/English/ Affix Lists Korean/French/ Swahili/ Vietnamese (I) Medina Urrea 2000, 2003, 2006b C T Spanish Affix List Schone and Jurafsky 2000, 2001a; C T English Segmentation Schone 2001 Goldsmith 2000, 2001, 2006; Belkin C T English (I) Lexicon+ Paradigms and Goldsmith 2002; Goldsmith, Higgins, and Soglasnova 2001; Hu et al. 2005b; Xanthos, Hu, and Goldsmith 2006 Baroni 2000, 2003 C T Child-English/ Affix List English Cho and Han 2002 C T Korean Segmentation Sharma, Kalita, and Das 2002, 2003; C T Assamese Lexicon+ Paradigms Sharma and Das 2002 Baroni, Matiasek, and Trost 2002 C/NC T English/German (I) Related word pairs Bati 2002 C/NC T Amharic Lexicon+ Paradigms Creutz 2003, 2006; Creutz and C T Finnish/Turkish/ Segmentation Lagus 2002, 2004, 2005a, 2005b, English 2005c, 2007; Creutz, Lagus, and Virpioja 2005; Hirsimäki et al. 2003; Creutz et al. 2005 319 Computational Linguistics Volume 37, Number 2 Table 2 (continued) Model Superv. Experimentation Learns what? Kontorovich, Don, and Singer 2003 C T English Segmentation Medina Urrea and Díaz 2003; C T Chuj/Ralámuri/Czech Affix List Medina-Urrea 2006a, 2008 Mayfield and McNamee 200</context>
</contexts>
<marker>Sharma, Das, 2002</marker>
<rawString>Sharma, Utpal and Rajib Das. 2002. Classification of words based on affix evidence. In International Conference on Natural Language Processing, ICON-2002, pages 31–39, Mumbai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Utpal Sharma</author>
<author>Jugal Kalita</author>
<author>Rajib Das</author>
</authors>
<title>Unsupervised learning of morphology for building lexicon for a highly inflectional language.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Workshop of the ACL Special Interest Group in Computational Phonology (SIGPHON),</booktitle>
<pages>1--10</pages>
<location>Philadelphia.</location>
<marker>Sharma, Kalita, Das, 2002</marker>
<rawString>Sharma, Utpal, Jugal Kalita, and Rajib Das. 2002. Unsupervised learning of morphology for building lexicon for a highly inflectional language. In Proceedings of the 6th Workshop of the ACL Special Interest Group in Computational Phonology (SIGPHON), pages 1–10, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Utpal Sharma</author>
<author>Jugal Kalita</author>
<author>Rajib Das</author>
</authors>
<title>Root word stemming by multiple evidence from corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of the 6th International Conference on Computational Intelligence and Natural Computation (CINC),</booktitle>
<pages>1593--1596</pages>
<location>Cary, NC.</location>
<marker>Sharma, Kalita, Das, 2003</marker>
<rawString>Sharma, Utpal, Jugal Kalita, and Rajib Das. 2003. Root word stemming by multiple evidence from corpus. In Proceedings of the 6th International Conference on Computational Intelligence and Natural Computation (CINC), pages 1593–1596, Cary, NC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew G Snover</author>
</authors>
<title>An unsupervised knowledge free algorithm for the learning of morphology in natural languages.</title>
<date>2002</date>
<tech>Master’s thesis,</tech>
<institution>Department of Computer Science, Washington University.</institution>
<contexts>
<context position="36067" citStr="Snover 2002" startWordPosition="5551" endWordPosition="5552">) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1992, 1994, 1995 C T+SP Spanish Segmentation Janßen 1992 C T+SP French Segmentation Juola, Hall, and Boggs 1994 C T English Segmentation Brent 1993, 1999; Brent, Murthy, C T English/Child- Segmentation and Lundberg 1995; Snover 2002; English/Polish/ Snover, Jarosz, and Brent 2002; French Snover and Brent 2001, 2003 Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation 1996 Yvon 1996 C T French (I) Segmentation Kazakov 1997; Kazakov and C T French/English Segmentation Manandhar 1998, 2001 Jacquemin 1997 C T English Segmentation Cromm 1997 C T German Segmentation Gaussier 1999 C T French/English (I) Lexicon+ Paradigms Déjean 1998a, 1998b C T Turkish/English/ Affix Lists Korean/French/ Swahili/ Vietnamese (I) Medina Urrea 2000, 2003, 2006b C T Spanish Affix List Schone and Jurafsky 2000, 2001a; C T English Se</context>
<context position="75374" citStr="Snover (2002)" startWordPosition="11952" endWordPosition="11953">i|y = yi} |= |{wi|xiy = wi} |be the number of words in which y equals the last part of the split. Then the task is to find splits that maximize the following expression: 11 arg max p(xi) · p,(yi) [s1,...,s|W|] wi∈W For example, if W = {ad, ae, bd, be, cd, ce, ggg}, then the configuration of splits a|d, a|e, b|d, b|e, c|d, c|e, g|gg yields the product (2 · 3)6 · (1 · 1). Brent (1999) devises a precise, but more elaborate, way of constructing W from B and S, but at the cost of a large search space, and whose global maximum is hard to characterize intuitively. The same holds for the extension by Snover (2002). Kontorovich, Don, and Singer (2003), Snyder and Barzilay (2008), Goldwater (2007), Johnson (2008), and Poon, Cherry, and Toutanova (2009) should also be noted for containing generative models. Most approaches, of any of the kinds (a)–(d) described in Section 3.1, explicitly or implicitly target languages which have (close to) one-slot morphology, that is, a word (or stem) typically takes not more than one prefix and not more than one suffix. Many (indeed most; Dryer 2005) languages deviate more or less from this model. At first, it may seem that multi-slot morphology can be handled by the sa</context>
</contexts>
<marker>Snover, 2002</marker>
<rawString>Snover, Matthew G. 2002. An unsupervised knowledge free algorithm for the learning of morphology in natural languages. Master’s thesis, Department of Computer Science, Washington University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew G Snover</author>
<author>Michael R Brent</author>
</authors>
<title>A Bayesian model for morpheme and paradigm identification.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001),</booktitle>
<pages>482--490</pages>
<contexts>
<context position="36145" citStr="Snover and Brent 2001" startWordPosition="5560" endWordPosition="5563">exicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1992, 1994, 1995 C T+SP Spanish Segmentation Janßen 1992 C T+SP French Segmentation Juola, Hall, and Boggs 1994 C T English Segmentation Brent 1993, 1999; Brent, Murthy, C T English/Child- Segmentation and Lundberg 1995; Snover 2002; English/Polish/ Snover, Jarosz, and Brent 2002; French Snover and Brent 2001, 2003 Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation 1996 Yvon 1996 C T French (I) Segmentation Kazakov 1997; Kazakov and C T French/English Segmentation Manandhar 1998, 2001 Jacquemin 1997 C T English Segmentation Cromm 1997 C T German Segmentation Gaussier 1999 C T French/English (I) Lexicon+ Paradigms Déjean 1998a, 1998b C T Turkish/English/ Affix Lists Korean/French/ Swahili/ Vietnamese (I) Medina Urrea 2000, 2003, 2006b C T Spanish Affix List Schone and Jurafsky 2000, 2001a; C T English Segmentation Schone 2001 Goldsmith 2000, 2001, 2006; Belkin C T English (I) Lexi</context>
</contexts>
<marker>Snover, Brent, 2001</marker>
<rawString>Snover, Matthew G. and Michael R. Brent. 2001. A Bayesian model for morpheme and paradigm identification. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001), pages 482–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew G Snover</author>
<author>Michael R Brent</author>
</authors>
<title>A probabilistic model for learning concatenative morphology.</title>
<date>2003</date>
<booktitle>Advances in Neural Information Processing Systems 15.</booktitle>
<pages>1513--1520</pages>
<editor>In S. Becker, S. Thrun, and K. Obermayer, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<marker>Snover, Brent, 2003</marker>
<rawString>Snover, Matthew G. and Michael R. Brent. 2003. A probabilistic model for learning concatenative morphology. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15. MIT Press, Cambridge, MA, pages 1513–1520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew G Snover</author>
<author>Gaja E Jarosz</author>
<author>Michael R Brent</author>
</authors>
<title>Unsupervised learning of morphology using a novel directed search algorithm: Taking the first step.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning,</booktitle>
<pages>11--20</pages>
<location>Philadelphia.</location>
<marker>Snover, Jarosz, Brent, 2002</marker>
<rawString>Snover, Matthew G., Gaja E. Jarosz, and Michael R. Brent. 2002. Unsupervised learning of morphology using a novel directed search algorithm: Taking the first step. In Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning, pages 11–20, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>737--745</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="40619" citStr="Snyder and Barzilay 2008" startWordPosition="6216" endWordPosition="6219"> Lexicon+ Paradigms Majumder et al. 2007; C T French/Bengali/French/ Analysis Majumder, Mitra, and Pal Bulgar2008 ian/Hungarian Zeman 2008, 2009 C - Czech/English/German/ Segmentation+ Finnish Paradigms Kohonen, Virpioja, and Klami C T Finnish/Turkish/English Segmentation 2008 320 Hammarström and Borin Unsupervised Learning of Morphology Table 2 (continued) Model Superv. Experimentation Learns what? Goodman 2008 C T Finnish/Turkish/English Segmentation Golénia 2008 C T Turkish/Russian Segmentation Pandey and Siddiqui 2008 C T Hindi Segmentation+ Paradigms Johnson 2008 C T Sesotho Segmentation Snyder and Barzilay 2008 C/NC T Hebrew/Arabic/Aramaic/ Segmentation English Spiegler et al. 2008 C T Zulu Segmentation Moon, Erk, and Baldridge 2009 C T English/Uspanteko Segmentation Poon, Cherry, and Toutanova C T Arabic/Hebrew Segmentation 2009 Abbreviations: C = Concatenative; I = Impressionistic evaluation; IR = Evaluation only in terms of Information Retrieval Performance; NC = Non-concatenative; RR = Hand-written rewrite rules; SP = Some manually curated segmentation points; T = Thresholds and Parameters to be set by a human. frequency techniques reminiscent of those of the (a) approaches can be applied. This </context>
<context position="75439" citStr="Snyder and Barzilay (2008)" startWordPosition="11959" endWordPosition="11962">in which y equals the last part of the split. Then the task is to find splits that maximize the following expression: 11 arg max p(xi) · p,(yi) [s1,...,s|W|] wi∈W For example, if W = {ad, ae, bd, be, cd, ce, ggg}, then the configuration of splits a|d, a|e, b|d, b|e, c|d, c|e, g|gg yields the product (2 · 3)6 · (1 · 1). Brent (1999) devises a precise, but more elaborate, way of constructing W from B and S, but at the cost of a large search space, and whose global maximum is hard to characterize intuitively. The same holds for the extension by Snover (2002). Kontorovich, Don, and Singer (2003), Snyder and Barzilay (2008), Goldwater (2007), Johnson (2008), and Poon, Cherry, and Toutanova (2009) should also be noted for containing generative models. Most approaches, of any of the kinds (a)–(d) described in Section 3.1, explicitly or implicitly target languages which have (close to) one-slot morphology, that is, a word (or stem) typically takes not more than one prefix and not more than one suffix. Many (indeed most; Dryer 2005) languages deviate more or less from this model. At first, it may seem that multi-slot morphology can be handled by the same algorithms as one-slot morphology, by iterating the process us</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Snyder, Benjamin and Regina Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. In Proceedings of ACL-08: HLT, pages 737–745, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Spencer</author>
<author>M Arnold</author>
</authors>
<date>1998</date>
<booktitle>The Handbook of Morphology.</booktitle>
<editor>Zwicky, editors.</editor>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<marker>Spencer, Arnold, 1998</marker>
<rawString>Spencer, Andrew and Arnold M. Zwicky, editors. 1998. The Handbook of Morphology. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Spiegler</author>
<author>Bruno Golénia</author>
<author>Ksenia Shalonova</author>
<author>Peter Flach</author>
<author>Roger Tucker</author>
</authors>
<title>Learning the morphology of Zulu with different degrees of supervision.</title>
<date>2008</date>
<booktitle>In Spoken Language Technology Workshop,</booktitle>
<pages>9--12</pages>
<location>Goa.</location>
<contexts>
<context position="40691" citStr="Spiegler et al. 2008" startWordPosition="6225" endWordPosition="6228">s Majumder, Mitra, and Pal Bulgar2008 ian/Hungarian Zeman 2008, 2009 C - Czech/English/German/ Segmentation+ Finnish Paradigms Kohonen, Virpioja, and Klami C T Finnish/Turkish/English Segmentation 2008 320 Hammarström and Borin Unsupervised Learning of Morphology Table 2 (continued) Model Superv. Experimentation Learns what? Goodman 2008 C T Finnish/Turkish/English Segmentation Golénia 2008 C T Turkish/Russian Segmentation Pandey and Siddiqui 2008 C T Hindi Segmentation+ Paradigms Johnson 2008 C T Sesotho Segmentation Snyder and Barzilay 2008 C/NC T Hebrew/Arabic/Aramaic/ Segmentation English Spiegler et al. 2008 C T Zulu Segmentation Moon, Erk, and Baldridge 2009 C T English/Uspanteko Segmentation Poon, Cherry, and Toutanova C T Arabic/Hebrew Segmentation 2009 Abbreviations: C = Concatenative; I = Impressionistic evaluation; IR = Evaluation only in terms of Information Retrieval Performance; NC = Non-concatenative; RR = Hand-written rewrite rules; SP = Some manually curated segmentation points; T = Thresholds and Parameters to be set by a human. frequency techniques reminiscent of those of the (a) approaches can be applied. This strategy is targeted towards the special kind of non-concatenative morph</context>
</contexts>
<marker>Spiegler, Golénia, Shalonova, Flach, Tucker, 2008</marker>
<rawString>Spiegler, Sebastian, Bruno Golénia, Ksenia Shalonova, Peter Flach, and Roger Tucker. 2008. Learning the morphology of Zulu with different degrees of supervision. In Spoken Language Technology Workshop, 2008 (SLT 2008), pages 9–12, Goa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Strassel</author>
<author>Mike Maxwell</author>
<author>Christopher Cieri</author>
</authors>
<title>Linguistic resource creation for research and technology development: A recent experiment.</title>
<date>2003</date>
<journal>ACM Transactions on Asian Language Processing,</journal>
<volume>2</volume>
<issue>2</issue>
<marker>Strassel, Maxwell, Cieri, 2003</marker>
<rawString>Strassel, Stephanie, Mike Maxwell, and Christopher Cieri. 2003. Linguistic resource creation for research and technology development: A recent experiment. ACM Transactions on Asian Language Processing, 2(2):101–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tepper</author>
</authors>
<title>Knowledge-lite induction of underlying morphology: A hybrid approach to learning morphemes using context-sensitive rewrite rules. Master’s thesis,</title>
<date>2007</date>
<institution>University of Washington.</institution>
<contexts>
<context position="39914" citStr="Tepper 2007" startWordPosition="6123" endWordPosition="6124">on Jordan, Healy, and Keselj 2005, 2006 C T Finnish/Turkish/English Segmentation Goldwater, Griffiths, and Johnson C T English/Child-English Segmentation 2005; Goldwater 2007; Naradowsky and Goldwater 2009 Freitag 2005 C T English Segmentation Golcher 2006 C - English/German Lexicon+ Paradigms Arabsorkhi and Shamsfard 2006 C T Persian Segmentation Chan 2006, Chan 2008, C T English Paradigms pages 101–139 Demberg 2007 C/NC T English/German/ Segmentation Finnish/Turkish Dasgupta and Ng 2006, 2007a; C T Bengali Segmentation 2007b; Dasgupta 2007 De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al. 2007; C T French/Bengali/French/ Analysis Majumder, Mitra, and Pal Bulgar2008 ian/Hungarian Zeman 2008, 2009 C - Czech/English/German/ Segmentation+ Finnish Paradigms Kohonen, Virpioja, and Klami C T Finnish/Turkish/English Segmentation 2008 320 Hammarström and Borin Unsupervised Learning of Morphology Table 2 (continued) Model Superv. Experimentation Learns what? Goodman 2008 C T Finnish/Turkish/English Segmentation Golénia 2008 C T Turkish/Russian Segmentation Pandey and Siddi</context>
<context position="77230" citStr="Tepper 2007" startWordPosition="12247" endWordPosition="12248">nation where, for example, the (a) methods target concatenative morphology and the (b)—or (c)—methods attempt the remaining cases. Presumably because most methods so far do not produce a clean, well-defined result, various forms of hybridization of techniques by different authors have yet to be systematically explored. Lastly, there are scattered attempts to address morphophonological changes in a principled way, though so far these have been developed in close connection with a particular segmentation method and target language (Schone 2001; Schone and Jurafsky 2001a; Wicentowski 2002, 2004; Tepper 2007; Kohonen, Virpioja, and Klami 2008; Tepper and Xia 2008). 4. Discussion 4.1 Language Dependence of ULM As we mentioned in Section 3.6, most approaches have an explicit bias towards certain kinds of morphological systems, those for which we introduced the label “one-slot morphology.” This is of course not a problem, if the purpose is to bootstrap a morphology for some languages which happen to belong to the right type. If the purpose is to say something about human language acquisition or language learning, or if the aim is to 332 Hammarström and Borin Unsupervised Learning of Morphology devis</context>
</contexts>
<marker>Tepper, 2007</marker>
<rawString>Tepper, Michael. 2007. Knowledge-lite induction of underlying morphology: A hybrid approach to learning morphemes using context-sensitive rewrite rules. Master’s thesis, University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tepper</author>
<author>Fei Xia</author>
</authors>
<title>A hybrid approach to the induction of underlying morphology.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<pages>17--24</pages>
<location>Hyderabad.</location>
<contexts>
<context position="39935" citStr="Tepper and Xia 2008" startWordPosition="6125" endWordPosition="6128">aly, and Keselj 2005, 2006 C T Finnish/Turkish/English Segmentation Goldwater, Griffiths, and Johnson C T English/Child-English Segmentation 2005; Goldwater 2007; Naradowsky and Goldwater 2009 Freitag 2005 C T English Segmentation Golcher 2006 C - English/German Lexicon+ Paradigms Arabsorkhi and Shamsfard 2006 C T Persian Segmentation Chan 2006, Chan 2008, C T English Paradigms pages 101–139 Demberg 2007 C/NC T English/German/ Segmentation Finnish/Turkish Dasgupta and Ng 2006, 2007a; C T Bengali Segmentation 2007b; Dasgupta 2007 De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al. 2007; C T French/Bengali/French/ Analysis Majumder, Mitra, and Pal Bulgar2008 ian/Hungarian Zeman 2008, 2009 C - Czech/English/German/ Segmentation+ Finnish Paradigms Kohonen, Virpioja, and Klami C T Finnish/Turkish/English Segmentation 2008 320 Hammarström and Borin Unsupervised Learning of Morphology Table 2 (continued) Model Superv. Experimentation Learns what? Goodman 2008 C T Finnish/Turkish/English Segmentation Golénia 2008 C T Turkish/Russian Segmentation Pandey and Siddiqui 2008 C T Hindi Se</context>
<context position="77287" citStr="Tepper and Xia 2008" startWordPosition="12254" endWordPosition="12257">t concatenative morphology and the (b)—or (c)—methods attempt the remaining cases. Presumably because most methods so far do not produce a clean, well-defined result, various forms of hybridization of techniques by different authors have yet to be systematically explored. Lastly, there are scattered attempts to address morphophonological changes in a principled way, though so far these have been developed in close connection with a particular segmentation method and target language (Schone 2001; Schone and Jurafsky 2001a; Wicentowski 2002, 2004; Tepper 2007; Kohonen, Virpioja, and Klami 2008; Tepper and Xia 2008). 4. Discussion 4.1 Language Dependence of ULM As we mentioned in Section 3.6, most approaches have an explicit bias towards certain kinds of morphological systems, those for which we introduced the label “one-slot morphology.” This is of course not a problem, if the purpose is to bootstrap a morphology for some languages which happen to belong to the right type. If the purpose is to say something about human language acquisition or language learning, or if the aim is to 332 Hammarström and Borin Unsupervised Learning of Morphology devise a method that should work with any language, such a bia</context>
</contexts>
<marker>Tepper, Xia, 2008</marker>
<rawString>Tepper, Michael and Fei Xia. 2008. A hybrid approach to the induction of underlying morphology. In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP 2008), pages 17–24, Hyderabad.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pieter Theron</author>
<author>Ian Cloete</author>
</authors>
<title>Automatic acquisition of two-level morphological rules.</title>
<date>1997</date>
<booktitle>In Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>103--110</pages>
<location>Washington, DC.</location>
<contexts>
<context position="8080" citStr="Theron and Cloete 1997" startWordPosition="1199" endWordPosition="1202">rpheme segmentation rather than word segmentation. We prefer the term segmentation to analysis because, in general in ULM, the algorithm does not attempt to label the segments. There have been other approaches to machine learning of morphology than pure ULM as defined here, the most popular ones being: • approaches that require selective input, such as “singular–plural pairs,” or “all members of a paradigm” (Garvin 1967; Klein and Dennison 1976; Golding and Thompson 1985; Wothke 1985; McClelland and Rumelhart 1986; Brasington, Jones, and Biggs 1988; Tufis 1989; Zhang and Kim 1990; Borin 1991; Theron and Cloete 1997; Oflazer, McShane, and Nirenburg 2001, for example) • approaches where some (small) amount of annotated data, some (small) amount of existing rule sets, or resources such as a machinereadable dictionary or a parallel corpus, are mandatory (Yarowsky and Wicentowski 2000; Yarowsky, Ngai, and Wicentowski 2001; Cucerzan and Yarowsky 2002; Neuvel and Fulop 2002; Johnson and Martin 2003; Rogati, McCarley, and Yang 2003, for example) Such approaches are excluded from the present survey, unless the required data (e.g., paradigm members) are extracted from raw text in an unsupervised manner as well. W</context>
</contexts>
<marker>Theron, Cloete, 1997</marker>
<rawString>Theron, Pieter and Ian Cloete. 1997. Automatic acquisition of two-level morphological rules. In Fifth Conference on Applied Natural Language Processing, pages 103–110, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trond Trosterud</author>
</authors>
<title>Porting morphological analysis and disambiguation to new languages.</title>
<date>2004</date>
<booktitle>In SALTMIL Workshop at LREC 2004: First Steps in Language Documentation for Minority Languages,</booktitle>
<pages>90--92</pages>
<location>Lisbon.</location>
<contexts>
<context position="82715" citStr="Trosterud (2004)" startWordPosition="13139" endWordPosition="13140">morphological parser) input corpus. 4.3 Is ULM of Any Use? As we said in Section 2, there is an explicit expectation frequently encountered in the more recent literature that ULM and other unsupervised methods could be employed in order to rapidly and cheaply (in terms of human effort) bootstrap basic language technology resources for new languages. However, looking at the literature, it seems that—at least in the area of inflectional morphology—the only approaches that have so far produced substantial results are the old-fashioned, hand-coded grammar-based ones, such as the work described by Trosterud (2004), where finite-state morphological processors and constraint grammar-based disambiguation components are developed for a number of related languages. The fact that the languages are related is of great help when dealing with successive languages after the first one. The morphological component for the first language, North Sámi, required approximately 2.5 person-years of highly qualified linguistic expert work to reach the prototype stage, whereas the analogous module for the closely related Lule Sámi was completed in an additional six months (Trosterud 2006).17 This and other work in the same</context>
</contexts>
<marker>Trosterud, 2004</marker>
<rawString>Trosterud, Trond. 2004. Porting morphological analysis and disambiguation to new languages. In SALTMIL Workshop at LREC 2004: First Steps in Language Documentation for Minority Languages, pages 90–92, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trond Trosterud</author>
</authors>
<title>Grammatically based language technology for minority languages.</title>
<date>2006</date>
<booktitle>Lesser-Known Languages of South Asia: Status and Policies, Case Studies and Applications of Information Technology. Mouton de Gruyter,</booktitle>
<pages>293--315</pages>
<editor>In Anju Saxena and Lars Borin, editors,</editor>
<location>Berlin,</location>
<contexts>
<context position="83280" citStr="Trosterud 2006" startWordPosition="13219" endWordPosition="13220">es, such as the work described by Trosterud (2004), where finite-state morphological processors and constraint grammar-based disambiguation components are developed for a number of related languages. The fact that the languages are related is of great help when dealing with successive languages after the first one. The morphological component for the first language, North Sámi, required approximately 2.5 person-years of highly qualified linguistic expert work to reach the prototype stage, whereas the analogous module for the closely related Lule Sámi was completed in an additional six months (Trosterud 2006).17 This and other work in the same vein reported in the literature (e.g., by Artola-Zubillaga 2004 and Maxwell and David 2008) is characterized by deep and long-lasting involvement by linguistic expertise and further often by the creative use of digitized versions of conventional printed linguistic resources, especially dictionaries. The following observation is perhaps trivial, but bears stressing, because it is in fact often not heeded in practice: For this kind of approach to work, it is necessary that tools for providing systems with linguistic knowledge use a conceptual apparatus and not</context>
</contexts>
<marker>Trosterud, 2006</marker>
<rawString>Trosterud, Trond. 2006. Grammatically based language technology for minority languages. In Anju Saxena and Lars Borin, editors, Lesser-Known Languages of South Asia: Status and Policies, Case Studies and Applications of Information Technology. Mouton de Gruyter, Berlin, pages 293–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Tufis</author>
</authors>
<title>It would be much easier if WENT were GOED.</title>
<date>1989</date>
<booktitle>In Proceedings of the Fourth Conference of the European Chapter of the ACL,</booktitle>
<pages>145--152</pages>
<location>Manchester.</location>
<contexts>
<context position="8024" citStr="Tufis 1989" startWordPosition="1191" endWordPosition="1192">he present survey are to be understood as morpheme segmentation rather than word segmentation. We prefer the term segmentation to analysis because, in general in ULM, the algorithm does not attempt to label the segments. There have been other approaches to machine learning of morphology than pure ULM as defined here, the most popular ones being: • approaches that require selective input, such as “singular–plural pairs,” or “all members of a paradigm” (Garvin 1967; Klein and Dennison 1976; Golding and Thompson 1985; Wothke 1985; McClelland and Rumelhart 1986; Brasington, Jones, and Biggs 1988; Tufis 1989; Zhang and Kim 1990; Borin 1991; Theron and Cloete 1997; Oflazer, McShane, and Nirenburg 2001, for example) • approaches where some (small) amount of annotated data, some (small) amount of existing rule sets, or resources such as a machinereadable dictionary or a parallel corpus, are mandatory (Yarowsky and Wicentowski 2000; Yarowsky, Ngai, and Wicentowski 2001; Cucerzan and Yarowsky 2002; Neuvel and Fulop 2002; Johnson and Martin 2003; Rogati, McCarley, and Yang 2003, for example) Such approaches are excluded from the present survey, unless the required data (e.g., paradigm members) are extr</context>
</contexts>
<marker>Tufis, 1989</marker>
<rawString>Tufis, Dan. 1989. It would be much easier if WENT were GOED. In Proceedings of the Fourth Conference of the European Chapter of the ACL, pages 145–152, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ur Rehman</author>
<author>Khalid</author>
<author>Iftikhar Hussain</author>
</authors>
<title>Unsupervised morphemes segmentation.</title>
<date>2005</date>
<booktitle>Proceedings of MorphoChallenge 2005,</booktitle>
<pages>52--56</pages>
<editor>In Mikko Kurimo, Mathias Creutz, and Krista Lagus, editors,</editor>
<institution>Helsinki University of Technology,</institution>
<location>Helsinki.</location>
<marker>Rehman, Khalid, Hussain, 2005</marker>
<rawString>ur Rehman, Khalid and Iftikhar Hussain. 2005. Unsupervised morphemes segmentation. In Mikko Kurimo, Mathias Creutz, and Krista Lagus, editors, Proceedings of MorphoChallenge 2005, pages 52–56, Helsinki University of Technology, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sami Virpioja</author>
<author>Jaakko J Väyrynen</author>
<author>Mathias Creutz</author>
<author>Markus Sadeniemi</author>
</authors>
<title>Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner.</title>
<date>2007</date>
<booktitle>In Proceedings of Machine Translation Summit XI,</booktitle>
<pages>391--498</pages>
<location>Copenhagen.</location>
<contexts>
<context position="85941" citStr="Virpioja et al. 2007" startWordPosition="13621" endWordPosition="13624">rphoChallenge. Taking MorphoChallenge results and proof-ofconcept reports together, it seems that high accuracy by ULM systems is presently only achievable if the language has small amounts of one-slot concatenative morphology, whereas for morphologically more complex languages, parameter tuning and/or lower accuracy is to be expected. We are not yet in a position to assess whether there are other tasks than IR which, in general, benefit significantly from (noisy) ULM, such as Speech Recognition (Hirsimäki et al. 2003, 2005, 2006; Kurimo et al. 2006) or Machine Translation (Sereewattana 2003; Virpioja et al. 2007; Bojar, Straˇnák, and Zeman 2008; Kirik and Fishel 2008; De Gispert et al. 2009; Fishel and Kirik 2010) because almost only the Morfessor system has been tested, and results are, if positive, not completely unambiguous. One usage of noisy ULM, at least, is for smoothing language identification models (Hammarström 2007a; Ceylan and Kim 2009). Further, ULM approaches are data-hungry, which precludes their use with many low-density languages. There is much ongoing work addressing these issues, however, so we can probably expect some progress in this area (Bird 2009). 4.4 Future Directions In pra</context>
</contexts>
<marker>Virpioja, Väyrynen, Creutz, Sadeniemi, 2007</marker>
<rawString>Virpioja, Sami, Jaakko J. Väyrynen, Mathias Creutz, and Markus Sadeniemi. 2007. Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner. In Proceedings of Machine Translation Summit XI, pages 391–498, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Wicentowski</author>
</authors>
<title>Modeling and Learning Multilingual Inflectional Morphology in a Minimally Supervised Framework.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University,</institution>
<location>Baltimore, MD.</location>
<contexts>
<context position="33567" citStr="Wicentowski 2002" startWordPosition="5167" endWordPosition="5168">compression gain. The outcome of such a compression scheme gives the segmentation. In addition, for those approaches which also target paradigms, stem–suffix co-occurrence statistics are gathered given the segmentation produced, rather than all possible segmentations. (b) Group and Abstract: In this family of methods, morphologically related words are first grouped (clustered into sets, paired, shortlisted, etc.) according to some metric, which is typically string edit distance, but may include semantic features (Schone 2001), distributional similarity (Freitag 2005), or frequency signatures (Wicentowski 2002). The next step is to abstract some morphological pattern that recurs among the groups. Such emergent patterns provide enough clues for segmentation and can sometimes be formulated as rules or morphological paradigms. (c) Features and Classes: In this family of methods, a word is seen as made up of a set of features—n-grams in Mayfield and McNamee (2003) and McNamee and Mayfield (2007), and initial/terminal/mid-substring in De Pauw and Wagacha (2007). Features which occur on many words have little selective power across the words, whereas features which occur seldom pinpoint a specific word or</context>
<context position="38390" citStr="Wicentowski 2002" startWordPosition="5905" endWordPosition="5906">egmentation Grabar 2003; Hadouche 2002 Pirrelli et al. 2004; Pirrelli and C T Italian/English/Arabic Unclear Herreros 2007; Calderone 2008 Johnson and Martin 2003b C T Inuktitut Unclear Katrenko 2004 C T Ukrainian Lexicon+ Paradigms ´Cavar et al. 2004a, 2004b; ´Cavar, C T Child-English Unclear Rodrigues, and Schrementi 2006; ´Cavar et al. 2006 Rodrigues and ´Cavar 2005, 2007 NC T Arabic Segmentation Monson 2004, 2009; Monson et al. C T English/Spanish/ Segmentation 2004, 2007a, 2007b, 2008, 2008a, Mapudungun (I) 2008b Yarowsky and Wicentowski 2000; C/NC T 30-ish mostly European Segmentation + Wicentowski 2002, 2004 type languages Rewrite Rules Gelbukh, Alexandrov, and Han 2004 C - English Segmentation Argamon et al. 2004 C T English Segmentation Goldsmith et al. 2005; Hu et al. 2005a C/NC T Unclear Unclear Bacchin, Ferro, and Melucci 2005, C T Italian/English Segmentation 2002a, 2002b; Nunzio et al. 2004 Oliver 2004, Chapter 4–5 C T Catalan Paradigms Bordag 2005a, 2005b, 2007, 2008 C T English/German Segmentation Hammarström 2005, 2006a, 2006b, C - Maori to Warlpiri Same-stem 2007b, 2009a, 2009b Bernhard 2005a, 2005b, 2006, 2007, C T Finnish/Turkish/English Segmentation+ 2008 Related sets of words</context>
<context position="65332" citStr="Wicentowski (2002" startWordPosition="10330" endWordPosition="10331">ogically related words (see, e.g., Gaussier 1999; Yarowsky and Wicentowski 2000; Schone and Jurafsky 2001a; Baroni, Matiasek, and Trost 2002; Hu et al. 2005a; Bernhard 2006, pages 101–117; Bernhard 2007; Majumder et al. 2007; Majumder, Mitra, and Pal 2008). In addition, as unsupervised methods for semantic clustering (e.g., Latent Semantic Analysis) and distributional clustering became more mature, these could be included as well (Schone and Jurafsky 2000, 2001a; Schone 2001; Baroni, Matiasek, and Trost 2002; Freitag 2005). More remarkable, however, is that Yarowsky and Wicentowski (2000) and Wicentowski (2002, 2004) have shown that frequency signatures can also be used to (heuristically) find morphologically related words. The example they use is sang versus sing, whose relative frequency distribution in a corpus is 1,427/1,204 (or 1.19/1), whereas singed15 versus sing is 9/1,204 (Yarowsky and Wicentowski 2000, pages 209– 210). This way, sing can be heuristically said to be parallel to sang rather than singed, and indeed the distribution for singed versus singe (its true relative) is 9/2, that is, much closer to 1. Suppose now that groups of morphologically related words are somehow heuristically </context>
<context position="77211" citStr="Wicentowski 2002" startWordPosition="12244" endWordPosition="12245">sibility of serial combination where, for example, the (a) methods target concatenative morphology and the (b)—or (c)—methods attempt the remaining cases. Presumably because most methods so far do not produce a clean, well-defined result, various forms of hybridization of techniques by different authors have yet to be systematically explored. Lastly, there are scattered attempts to address morphophonological changes in a principled way, though so far these have been developed in close connection with a particular segmentation method and target language (Schone 2001; Schone and Jurafsky 2001a; Wicentowski 2002, 2004; Tepper 2007; Kohonen, Virpioja, and Klami 2008; Tepper and Xia 2008). 4. Discussion 4.1 Language Dependence of ULM As we mentioned in Section 3.6, most approaches have an explicit bias towards certain kinds of morphological systems, those for which we introduced the label “one-slot morphology.” This is of course not a problem, if the purpose is to bootstrap a morphology for some languages which happen to belong to the right type. If the purpose is to say something about human language acquisition or language learning, or if the aim is to 332 Hammarström and Borin Unsupervised Learning </context>
</contexts>
<marker>Wicentowski, 2002</marker>
<rawString>Wicentowski, Richard. 2002. Modeling and Learning Multilingual Inflectional Morphology in a Minimally Supervised Framework. Ph.D. thesis, Johns Hopkins University, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Wicentowski</author>
</authors>
<title>Multilingual noise-robust supervised morphological analysis using the wordframe model.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL Special Interest Group on Computational Phonology (SIGPHON),</booktitle>
<pages>70--77</pages>
<location>Barcelona.</location>
<contexts>
<context position="66343" citStr="Wicentowski (2004)" startWordPosition="10488" endWordPosition="10489">her than singed, and indeed the distribution for singed versus singe (its true relative) is 9/2, that is, much closer to 1. Suppose now that groups of morphologically related words are somehow heuristically extracted. For example, one group might be {play, player, played, playing} and another might be {bark, barks, barked, barking}. The next step would be to find what is common among several groups, not just one. Abstracting morphological alternations given a family of groups is a thorny issue. For instance, Baroni, Matiasek, and Trost (2002) leave the matter largely in the exploration phase. Wicentowski (2004) presents a finished theory based on constraining the abstraction to find patterns in terms of prefix, suffix, and stem alternations. The outstanding question for the group-and-abstract approaches, related not only to grouping but also to abstracting, is how to find one and the same morphological process (umlauting, adding a suffix, etc.) that operates over a maximal number of groups. The search space is huge, considering not only the group space but also the large number of potential morphological processes itself. The group-and-abstract approaches are also characterized by the ubiquitous use</context>
</contexts>
<marker>Wicentowski, 2004</marker>
<rawString>Wicentowski, Richard. 2004. Multilingual noise-robust supervised morphological analysis using the wordframe model. In Proceedings of the ACL Special Interest Group on Computational Phonology (SIGPHON), pages 70–77, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuly Wintner</author>
</authors>
<title>Last words: What science underlies natural language engineering?</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="89823" citStr="Wintner 2009" startWordPosition="14212" endWordPosition="14213"> are talking about linguistic form, but there has been next to no progress at all when it comes to linguistic meaning (e.g., functional labeling of affixes). In the early days of ULM, the expectation was that it should constitute—when eventually achieved sometime in the future—a formalized version of a linguistic discovery procedure, that is, a knowledge-heavy enterprise. Instead, recent successes in the area have been largely contingent upon the rapid development in computational linguistics of statistical and information-theoretic knowledge-light (but robust) methodologies. We believe (like Wintner 2009 for computational linguistics in general), however, that if ULM is to become a serious alternative to—or, equally likely, a natural component of—manually built computational morphology systems for a wide and diverse range of languages, and especially if we are to make headway in the area of semantics, we need to see more interaction between the present approaches to ULM with the computational techniques and mathematical modeling tools they can bring to bear on the problem on the one hand, and typologically informed linguistic research on morphology founded on a vast store of knowledge and met</context>
</contexts>
<marker>Wintner, 2009</marker>
<rawString>Wintner, Shuly. 2009. Last words: What science underlies natural language engineering? Computational Linguistics, 35(4):641–644.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Wothke</author>
</authors>
<title>Machine learning of morphological rules by generalization and analogy.</title>
<date>1986</date>
<booktitle>In Proceedings of the 11th Conference on Computational Linguistics,</booktitle>
<pages>289--293</pages>
<location>Morristown, NJ.</location>
<marker>Wothke, 1986</marker>
<rawString>Wothke, Klaus. 1986. Machine learning of morphological rules by generalization and analogy. In Proceedings of the 11th Conference on Computational Linguistics, pages 289–293, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Christian Wothke</author>
</authors>
<title>Maschinelle Erlernung und Simulation morphologischer Ableitungsregeln.</title>
<date>1985</date>
<tech>Ph.D. thesis,</tech>
<institution>Rheinische Friedrich-Wilhelms-Universität zu Bonn.</institution>
<contexts>
<context position="7946" citStr="Wothke 1985" startWordPosition="1180" endWordPosition="1181">ne algorithm are included. Hence, subsequent uses of the term segmentation in the present survey are to be understood as morpheme segmentation rather than word segmentation. We prefer the term segmentation to analysis because, in general in ULM, the algorithm does not attempt to label the segments. There have been other approaches to machine learning of morphology than pure ULM as defined here, the most popular ones being: • approaches that require selective input, such as “singular–plural pairs,” or “all members of a paradigm” (Garvin 1967; Klein and Dennison 1976; Golding and Thompson 1985; Wothke 1985; McClelland and Rumelhart 1986; Brasington, Jones, and Biggs 1988; Tufis 1989; Zhang and Kim 1990; Borin 1991; Theron and Cloete 1997; Oflazer, McShane, and Nirenburg 2001, for example) • approaches where some (small) amount of annotated data, some (small) amount of existing rule sets, or resources such as a machinereadable dictionary or a parallel corpus, are mandatory (Yarowsky and Wicentowski 2000; Yarowsky, Ngai, and Wicentowski 2001; Cucerzan and Yarowsky 2002; Neuvel and Fulop 2002; Johnson and Martin 2003; Rogati, McCarley, and Yang 2003, for example) Such approaches are excluded from </context>
<context position="21821" citStr="Wothke (1985" startWordPosition="3334" endWordPosition="3335">es that several parameters that Andreev provides mostly without motivation or comment in fact can be changed in a more accepting direction, leading to much increased recall without much loss in precision. Unfortunately, however, in his short paper, Cromm does not provide enough information about the algorithm or the changes that he made to it, so that the Russian original is still the only publicly available source for the details of Andreev’s approach. A very different, more practically oriented, motivation for ULM came in the 1980s, beginning with the supervised morphology learning ideas by Wothke (1985, 1986) and Klenk (1985a, 1985b) which later led to partly unsupervised methods (see the following). Because full natural language lexica, at the time, were too big to fit in working memory, these authors were looking for a way to analyze or stem running words in a “nichtlexikalisches” manner, that is, without the storage and use of a large lexicon. This motivation is now obsolete. The interest in purebred ULM was fairly low until about 1990, however, with only a few works appearing between the mid 1960s and 1990. Especially in the 1980s, the focus in computational morphology was on the develo</context>
</contexts>
<marker>Wothke, 1985</marker>
<rawString>Wothke, Klaus Christian. 1985. Maschinelle Erlernung und Simulation morphologischer Ableitungsregeln. Ph.D. thesis, Rheinische Friedrich-Wilhelms-Universität zu Bonn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aris Xanthos</author>
</authors>
<title>Apprentissage automatique de la morphologie: Le cas des structures racine-schème.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Université de Lausanne.</institution>
<contexts>
<context position="12240" citStr="Xanthos (2007" startWordPosition="1874" endWordPosition="1875">ng is disregarded) or the same lexeme (if meaning is taken into account). The converse need not hold; it is perfectly possible to answer the question of whether two words are of the same stem with high accuracy, without having to commit to what the actual stem should be. Many recent articles fail to deal properly with previous and related work, some reinvent heuristics that have been proposed earlier, and there is little modularization taking place. Previous surveys and overviews for a general audience are Borin (1991), Batchelder (1997, pages 66–68), Powers (1998), Clark (2001, pages 80–82), Xanthos (2007, pages 95–107), Goldsmith (2001), Daelemans (2004, page 1898), Roark and Sproat (2007, pages 116–136), Hammarström (2007b, pages 10–15), Chan (2008, pages 48–60), Hammarström (2009b, pages 14–21), Borin (2009), Goldsmith (2010), and, to a more limited extent, the related-work sections of individual research papers. Kurimo, Creutz, and Turunen (2007), Kurimo, Creutz, and Varjokallio (2008a, 2008b), Kurimo and Turunen (2008), Kurimo and Varjokallio (2008), and McNamee (2008) are overviews of systems in the MorphoChallenge of the respective year. However, we will try to be more comprehensive tha</context>
<context position="39983" citStr="Xanthos 2007" startWordPosition="6133" endWordPosition="6134">Segmentation Goldwater, Griffiths, and Johnson C T English/Child-English Segmentation 2005; Goldwater 2007; Naradowsky and Goldwater 2009 Freitag 2005 C T English Segmentation Golcher 2006 C - English/German Lexicon+ Paradigms Arabsorkhi and Shamsfard 2006 C T Persian Segmentation Chan 2006, Chan 2008, C T English Paradigms pages 101–139 Demberg 2007 C/NC T English/German/ Segmentation Finnish/Turkish Dasgupta and Ng 2006, 2007a; C T Bengali Segmentation 2007b; Dasgupta 2007 De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al. 2007; C T French/Bengali/French/ Analysis Majumder, Mitra, and Pal Bulgar2008 ian/Hungarian Zeman 2008, 2009 C - Czech/English/German/ Segmentation+ Finnish Paradigms Kohonen, Virpioja, and Klami C T Finnish/Turkish/English Segmentation 2008 320 Hammarström and Borin Unsupervised Learning of Morphology Table 2 (continued) Model Superv. Experimentation Learns what? Goodman 2008 C T Finnish/Turkish/English Segmentation Golénia 2008 C T Turkish/Russian Segmentation Pandey and Siddiqui 2008 C T Hindi Segmentation+ Paradigms Johnson 2008 C T Sesotho S</context>
<context position="41517" citStr="Xanthos (2007)" startWordPosition="6344" endWordPosition="6345">tion; IR = Evaluation only in terms of Information Retrieval Performance; NC = Non-concatenative; RR = Hand-written rewrite rules; SP = Some manually curated segmentation points; T = Thresholds and Parameters to be set by a human. frequency techniques reminiscent of those of the (a) approaches can be applied. This strategy is targeted towards the special kind of non-concatenative morphology called intercalated morphology11 with the observation that, empirically, in those (relatively few) languages which have intercalated morphology, it does seem to depend on vowel/consonant considerations. In Xanthos (2007), the phonological categories are inferred in an unsupervised manner (cf. Goldsmith and Xanthos 2009) whereas in Bati (2002) and Rodrigues and ´Cavar (2005, 2007) they are seen as given by the writing system. The first two, (a) and (b), enjoy a fair amount of popularity in the reviewed collection of work, though (a) is much more common and was the only kind used up to about 1997. The last two, (c) and (d), have been utilized only by the sets of authors cited therein. Let us now look at some salient questions in more detail. The following notation will be used in formal statements: • w, s, b, x</context>
<context position="71646" citStr="Xanthos 2007" startWordPosition="11302" endWordPosition="11303">ntal and long-distance phenomena, but are so far largely unexplored and not free from thresholds and parameters. 3.5 Phonological Categories and Separation These approaches specifically target the special kind of non-concatenative morphology called intercalated morphology (or templatic morphology or root-and-pattern morphology) famous mainly from Semitic languages, such as Arabic. They start out by assuming that graphemes can be subdivided into those that take part in the root, and those that take part in the pattern. For the languages so far targeted, Arabic (Rodrigues and ´Cavar 2005, 2007; Xanthos 2007) and Amharic (Bati 2002), this is largely true, or a transcription is used where it is largely true. Rodrigues and ´Cavar (2005, 2007) and Bati (2002) hard-code the transition from the graphemic representation of a word to its (potential) root and pattern parts. This can be said to constitute a strong language specific bias, tantamount to supervision. Xanthos (2007), on the other hand, starts out only by assuming that there exists a distinction between root and pattern graphemes and subsequently learns which graphemes are which. See Goldsmith and Xanthos (2009) for an excellent survey on how t</context>
</contexts>
<marker>Xanthos, 2007</marker>
<rawString>Xanthos, Aris. 2007. Apprentissage automatique de la morphologie: Le cas des structures racine-schème. Ph.D. thesis, Université de Lausanne.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aris Xanthos</author>
<author>Yu Hu</author>
<author>John Goldsmith</author>
</authors>
<title>Exploring variant definitions of pointer length in MDL.</title>
<date>2006</date>
<booktitle>In Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology and Morphology at HLT-NAACL</booktitle>
<pages>32--40</pages>
<marker>Xanthos, Hu, Goldsmith, 2006</marker>
<rawString>Xanthos, Aris, Yu Hu, and John Goldsmith. 2006. Exploring variant definitions of pointer length in MDL. In Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology and Morphology at HLT-NAACL 2006, pages 32–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the First International Conference on Human Language Technology Research,</booktitle>
<pages>1--8</pages>
<location>Stroudsburg, PA,</location>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>Yarowsky, David, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the First International Conference on Human Language Technology Research, Stroudsburg, PA, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Richard Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL-2000),</booktitle>
<pages>207--216</pages>
<location>Hong Kong.</location>
<contexts>
<context position="8350" citStr="Yarowsky and Wicentowski 2000" startWordPosition="1240" endWordPosition="1243">fined here, the most popular ones being: • approaches that require selective input, such as “singular–plural pairs,” or “all members of a paradigm” (Garvin 1967; Klein and Dennison 1976; Golding and Thompson 1985; Wothke 1985; McClelland and Rumelhart 1986; Brasington, Jones, and Biggs 1988; Tufis 1989; Zhang and Kim 1990; Borin 1991; Theron and Cloete 1997; Oflazer, McShane, and Nirenburg 2001, for example) • approaches where some (small) amount of annotated data, some (small) amount of existing rule sets, or resources such as a machinereadable dictionary or a parallel corpus, are mandatory (Yarowsky and Wicentowski 2000; Yarowsky, Ngai, and Wicentowski 2001; Cucerzan and Yarowsky 2002; Neuvel and Fulop 2002; Johnson and Martin 2003; Rogati, McCarley, and Yang 2003, for example) Such approaches are excluded from the present survey, unless the required data (e.g., paradigm members) are extracted from raw text in an unsupervised manner as well. We also exclude the special case of the second approach where morphology learning means not “learning the morphological system of a language,” but rather “learning the inflectional classes of out-of-vocabulary words,” namely, approaches where an existing morphological an</context>
<context position="38327" citStr="Yarowsky and Wicentowski 2000" startWordPosition="5894" endWordPosition="5897">Mayfield 2007 languages (IR) Zweigenbaum, Hadouche, and C T Medical French Segmentation Grabar 2003; Hadouche 2002 Pirrelli et al. 2004; Pirrelli and C T Italian/English/Arabic Unclear Herreros 2007; Calderone 2008 Johnson and Martin 2003b C T Inuktitut Unclear Katrenko 2004 C T Ukrainian Lexicon+ Paradigms ´Cavar et al. 2004a, 2004b; ´Cavar, C T Child-English Unclear Rodrigues, and Schrementi 2006; ´Cavar et al. 2006 Rodrigues and ´Cavar 2005, 2007 NC T Arabic Segmentation Monson 2004, 2009; Monson et al. C T English/Spanish/ Segmentation 2004, 2007a, 2007b, 2008, 2008a, Mapudungun (I) 2008b Yarowsky and Wicentowski 2000; C/NC T 30-ish mostly European Segmentation + Wicentowski 2002, 2004 type languages Rewrite Rules Gelbukh, Alexandrov, and Han 2004 C - English Segmentation Argamon et al. 2004 C T English Segmentation Goldsmith et al. 2005; Hu et al. 2005a C/NC T Unclear Unclear Bacchin, Ferro, and Melucci 2005, C T Italian/English Segmentation 2002a, 2002b; Nunzio et al. 2004 Oliver 2004, Chapter 4–5 C T Catalan Paradigms Bordag 2005a, 2005b, 2007, 2008 C T English/German Segmentation Hammarström 2005, 2006a, 2006b, C - Maori to Warlpiri Same-stem 2007b, 2009a, 2009b Bernhard 2005a, 2005b, 2006, 2007, C T F</context>
<context position="64794" citStr="Yarowsky and Wicentowski 2000" startWordPosition="10250" endWordPosition="10253">iques from linear algebra, in particular Latent Dirichlet Allocation, to break the full matrix into smaller dense submatrices, which, when multiplied together, resemble the full matrix. There is only one humanly tuned threshold, namely, when to stop breaking into smaller parts. 3.3 Group and Abstract In contrast to the methods that use a heuristic for finding morpheme boundaries, the grouping methods are much less sensitive to continuous segments. String edit distance is the most straightforward metric for which to find pairs or sets of morphologically related words (see, e.g., Gaussier 1999; Yarowsky and Wicentowski 2000; Schone and Jurafsky 2001a; Baroni, Matiasek, and Trost 2002; Hu et al. 2005a; Bernhard 2006, pages 101–117; Bernhard 2007; Majumder et al. 2007; Majumder, Mitra, and Pal 2008). In addition, as unsupervised methods for semantic clustering (e.g., Latent Semantic Analysis) and distributional clustering became more mature, these could be included as well (Schone and Jurafsky 2000, 2001a; Schone 2001; Baroni, Matiasek, and Trost 2002; Freitag 2005). More remarkable, however, is that Yarowsky and Wicentowski (2000) and Wicentowski (2002, 2004) have shown that frequency signatures can also be used </context>
</contexts>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>Yarowsky, David and Richard Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL-2000), pages 207–216, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>François Yvon</author>
</authors>
<title>Prononcer par analogie: motivation, formalisation et evaluation.</title>
<date>1996</date>
<booktitle>Ph.D. thesis, École Nationale Supérieure des Télécommunications,</booktitle>
<location>Paris.</location>
<contexts>
<context position="36235" citStr="Yvon 1996" startWordPosition="5576" endWordPosition="5577">English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1992, 1994, 1995 C T+SP Spanish Segmentation Janßen 1992 C T+SP French Segmentation Juola, Hall, and Boggs 1994 C T English Segmentation Brent 1993, 1999; Brent, Murthy, C T English/Child- Segmentation and Lundberg 1995; Snover 2002; English/Polish/ Snover, Jarosz, and Brent 2002; French Snover and Brent 2001, 2003 Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation 1996 Yvon 1996 C T French (I) Segmentation Kazakov 1997; Kazakov and C T French/English Segmentation Manandhar 1998, 2001 Jacquemin 1997 C T English Segmentation Cromm 1997 C T German Segmentation Gaussier 1999 C T French/English (I) Lexicon+ Paradigms Déjean 1998a, 1998b C T Turkish/English/ Affix Lists Korean/French/ Swahili/ Vietnamese (I) Medina Urrea 2000, 2003, 2006b C T Spanish Affix List Schone and Jurafsky 2000, 2001a; C T English Segmentation Schone 2001 Goldsmith 2000, 2001, 2006; Belkin C T English (I) Lexicon+ Paradigms and Goldsmith 2002; Goldsmith, Higgins, and Soglasnova 2001; Hu et al. 2005</context>
<context position="67222" citStr="Yvon (1996)" startWordPosition="10623" endWordPosition="10624">ne and the same morphological process (umlauting, adding a suffix, etc.) that operates over a maximal number of groups. The search space is huge, considering not only the group space but also the large number of potential morphological processes itself. The group-and-abstract approaches are also characterized by the ubiquitous use of ad hoc thresholds. However, there are clear advantages in that they are in principle capable of handling non-concatenative morphology and in that issues of semantics (of stems) are addressed from the beginning. The work by de Kock and Bossaert (1969, 1974, 1978), Yvon (1996), Medina Urrea (2003) and partly Moon, Erk, and Baldridge (2009) can favorably be seen as a mid-way between the border-and-frequency and group-and-abstract approaches as they rely on 15 That is, the past tense of the verb singe. 329 Computational Linguistics Volume 37, Number 2 Table 9 Example feature values for the words ngithii(I went) and t˜ugithii(we went) adapted from De Pauw and Wagacha (2007, page 1518). B=-features describe a subset at the start of the word form, E=-features indicate patterns at the end of the word, and I=-features describe patterns inside the word form. class features</context>
</contexts>
<marker>Yvon, 1996</marker>
<rawString>Yvon, François. 1996. Prononcer par analogie: motivation, formalisation et evaluation. Ph.D. thesis, École Nationale Supérieure des Télécommunications, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
</authors>
<title>Unsupervised acquiring of morphological paradigms from tokenized text.</title>
<date>2008</date>
<booktitle>In Advances in Multilingual and Multimodal Information Retrieval: 8th Workshop of the CrossLanguage Evaluation Forum, CLEF</booktitle>
<pages>892--899</pages>
<location>Budapest.</location>
<contexts>
<context position="40133" citStr="Zeman 2008" startWordPosition="6155" endWordPosition="6156"> T English Segmentation Golcher 2006 C - English/German Lexicon+ Paradigms Arabsorkhi and Shamsfard 2006 C T Persian Segmentation Chan 2006, Chan 2008, C T English Paradigms pages 101–139 Demberg 2007 C/NC T English/German/ Segmentation Finnish/Turkish Dasgupta and Ng 2006, 2007a; C T Bengali Segmentation 2007b; Dasgupta 2007 De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al. 2007; C T French/Bengali/French/ Analysis Majumder, Mitra, and Pal Bulgar2008 ian/Hungarian Zeman 2008, 2009 C - Czech/English/German/ Segmentation+ Finnish Paradigms Kohonen, Virpioja, and Klami C T Finnish/Turkish/English Segmentation 2008 320 Hammarström and Borin Unsupervised Learning of Morphology Table 2 (continued) Model Superv. Experimentation Learns what? Goodman 2008 C T Finnish/Turkish/English Segmentation Golénia 2008 C T Turkish/Russian Segmentation Pandey and Siddiqui 2008 C T Hindi Segmentation+ Paradigms Johnson 2008 C T Sesotho Segmentation Snyder and Barzilay 2008 C/NC T Hebrew/Arabic/Aramaic/ Segmentation English Spiegler et al. 2008 C T Zulu Segmentation Moon, Erk, and Bald</context>
<context position="62444" citStr="Zeman 2008" startWordPosition="9860" endWordPosition="9861">ms. The number of forms i that a lexeme occurs in is likely not to be normally distributed. Most lexemes will occur in only one form, and only very few, if any, lexemes will occur in all |P| forms. It appears that for most languages and most paradigms, the number of lexemes that occur in i forms tends to decrease logarithmically in i (Chan 2008, pages 75–84). As an example, consider the three most common paradigms in Swedish and the frequency of forms in Table 8. Works which have attempted nevertheless to tackle the matter of paradigms, at least for languages with one-slot morphology, include Zeman 2008, 2009, Hammarström (2009b), and Monson (2009). They explicitly or implicitly make use of the following two heuristics to narrow down the search space: • Languages tend to have a small number of paradigms (where “small” means fewer than 100 paradigms with at least 100 member stems each). • Languages tend to have only small paradigms (where “small” means fewer than 50), that is, the number of affixes in each paradigm is small. Agglutinative languages, which have several layers of affixes, can be said to obey this generalization in the sense that each layer has few members, whereas conversely, t</context>
<context position="85974" citStr="Zeman 2008" startWordPosition="13628" endWordPosition="13629">ts and proof-ofconcept reports together, it seems that high accuracy by ULM systems is presently only achievable if the language has small amounts of one-slot concatenative morphology, whereas for morphologically more complex languages, parameter tuning and/or lower accuracy is to be expected. We are not yet in a position to assess whether there are other tasks than IR which, in general, benefit significantly from (noisy) ULM, such as Speech Recognition (Hirsimäki et al. 2003, 2005, 2006; Kurimo et al. 2006) or Machine Translation (Sereewattana 2003; Virpioja et al. 2007; Bojar, Straˇnák, and Zeman 2008; Kirik and Fishel 2008; De Gispert et al. 2009; Fishel and Kirik 2010) because almost only the Morfessor system has been tested, and results are, if positive, not completely unambiguous. One usage of noisy ULM, at least, is for smoothing language identification models (Hammarström 2007a; Ceylan and Kim 2009). Further, ULM approaches are data-hungry, which precludes their use with many low-density languages. There is much ongoing work addressing these issues, however, so we can probably expect some progress in this area (Bird 2009). 4.4 Future Directions In practice, the near future should def</context>
</contexts>
<marker>Zeman, 2008</marker>
<rawString>Zeman, Daniel. 2008. Unsupervised acquiring of morphological paradigms from tokenized text. In Advances in Multilingual and Multimodal Information Retrieval: 8th Workshop of the CrossLanguage Evaluation Forum, CLEF 2007, pages 892–899, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
</authors>
<title>Using unsupervised paradigm acquisition for prefixes.</title>
<date>2009</date>
<booktitle>Evaluating Systems for Multilingual and Multimodal Information Access, 9th Workshop of the Cross-Language Evaluation Forum, CLEF</booktitle>
<editor>In Carol Peters, Thomas Deselaers, Nicola Ferro, Julio Gonzalo, Gareth J. F. Jones, Mikko Kurimo, Thomas Mandl, Anselmo Peñas, and Vivien Petras, editors,</editor>
<location>Aarthus, Denmark,</location>
<marker>Zeman, 2009</marker>
<rawString>Zeman, Daniel. 2009. Using unsupervised paradigm acquisition for prefixes. In Carol Peters, Thomas Deselaers, Nicola Ferro, Julio Gonzalo, Gareth J. F. Jones, Mikko Kurimo, Thomas Mandl, Anselmo Peñas, and Vivien Petras, editors, Evaluating Systems for Multilingual and Multimodal Information Access, 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008, Aarthus, Denmark,</rawString>
</citation>
<citation valid="true">
<title>Revised Selected Papers,</title>
<date>2008</date>
<pages>983--990</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="12667" citStr="(2008)" startWordPosition="1933" endWordPosition="1933">ation taking place. Previous surveys and overviews for a general audience are Borin (1991), Batchelder (1997, pages 66–68), Powers (1998), Clark (2001, pages 80–82), Xanthos (2007, pages 95–107), Goldsmith (2001), Daelemans (2004, page 1898), Roark and Sproat (2007, pages 116–136), Hammarström (2007b, pages 10–15), Chan (2008, pages 48–60), Hammarström (2009b, pages 14–21), Borin (2009), Goldsmith (2010), and, to a more limited extent, the related-work sections of individual research papers. Kurimo, Creutz, and Turunen (2007), Kurimo, Creutz, and Varjokallio (2008a, 2008b), Kurimo and Turunen (2008), Kurimo and Varjokallio (2008), and McNamee (2008) are overviews of systems in the MorphoChallenge of the respective year. However, we will try to be more comprehensive than previous surveys and discuss the ideas in the field critically. 312 Hammarström and Borin Unsupervised Learning of Morphology We will not attempt a comparison in terms of accuracy figures as this is wholly impossible, not only because of the great variation in goals but also because most descriptions do not specify their algorithm(s) in enough detail. Fortunately, this aspect is better handled in controlled competitions, </context>
<context position="75439" citStr="(2008)" startWordPosition="11962" endWordPosition="11962">e last part of the split. Then the task is to find splits that maximize the following expression: 11 arg max p(xi) · p,(yi) [s1,...,s|W|] wi∈W For example, if W = {ad, ae, bd, be, cd, ce, ggg}, then the configuration of splits a|d, a|e, b|d, b|e, c|d, c|e, g|gg yields the product (2 · 3)6 · (1 · 1). Brent (1999) devises a precise, but more elaborate, way of constructing W from B and S, but at the cost of a large search space, and whose global maximum is hard to characterize intuitively. The same holds for the extension by Snover (2002). Kontorovich, Don, and Singer (2003), Snyder and Barzilay (2008), Goldwater (2007), Johnson (2008), and Poon, Cherry, and Toutanova (2009) should also be noted for containing generative models. Most approaches, of any of the kinds (a)–(d) described in Section 3.1, explicitly or implicitly target languages which have (close to) one-slot morphology, that is, a word (or stem) typically takes not more than one prefix and not more than one suffix. Many (indeed most; Dryer 2005) languages deviate more or less from this model. At first, it may seem that multi-slot morphology can be handled by the same algorithms as one-slot morphology, by iterating the process us</context>
</contexts>
<marker>2008</marker>
<rawString>September 17–19, 2008, Revised Selected Papers, pages 983–990. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Byoung-Tak Zhang</author>
<author>Yung-Taek Kim</author>
</authors>
<title>Morphological analysis and synthesis by automated discovery and acquisition of linguistic rules.</title>
<date>1990</date>
<booktitle>In Papers Presented to the 13th International Conference on Computational Linguistics (COLING</booktitle>
<volume>2</volume>
<pages>431--436</pages>
<location>Helsinki.</location>
<contexts>
<context position="8044" citStr="Zhang and Kim 1990" startWordPosition="1193" endWordPosition="1196">urvey are to be understood as morpheme segmentation rather than word segmentation. We prefer the term segmentation to analysis because, in general in ULM, the algorithm does not attempt to label the segments. There have been other approaches to machine learning of morphology than pure ULM as defined here, the most popular ones being: • approaches that require selective input, such as “singular–plural pairs,” or “all members of a paradigm” (Garvin 1967; Klein and Dennison 1976; Golding and Thompson 1985; Wothke 1985; McClelland and Rumelhart 1986; Brasington, Jones, and Biggs 1988; Tufis 1989; Zhang and Kim 1990; Borin 1991; Theron and Cloete 1997; Oflazer, McShane, and Nirenburg 2001, for example) • approaches where some (small) amount of annotated data, some (small) amount of existing rule sets, or resources such as a machinereadable dictionary or a parallel corpus, are mandatory (Yarowsky and Wicentowski 2000; Yarowsky, Ngai, and Wicentowski 2001; Cucerzan and Yarowsky 2002; Neuvel and Fulop 2002; Johnson and Martin 2003; Rogati, McCarley, and Yang 2003, for example) Such approaches are excluded from the present survey, unless the required data (e.g., paradigm members) are extracted from raw text </context>
</contexts>
<marker>Zhang, Kim, 1990</marker>
<rawString>Zhang, Byoung-Tak and Yung-Taek Kim. 1990. Morphological analysis and synthesis by automated discovery and acquisition of linguistic rules. In Papers Presented to the 13th International Conference on Computational Linguistics (COLING 1990), volume 2, pages 431–436, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Zweigenbaum</author>
<author>F Hadouche</author>
<author>N Grabar</author>
</authors>
<title>Apprentissage de relations morphologiques en corpus.</title>
<date>2003</date>
<booktitle>In Actes de TALN</booktitle>
<pages>285--294</pages>
<marker>Zweigenbaum, Hadouche, Grabar, 2003</marker>
<rawString>Zweigenbaum, P., F. Hadouche, and N. Grabar. 2003. Apprentissage de relations morphologiques en corpus. In Actes de TALN 2003, pages 285–294, Batz-sur-mer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>